1
00:00:00,000 --> 00:00:14,480
Hello everyone and welcome to the podcast.

2
00:00:14,480 --> 00:00:18,640
If you're a regular listener of the show, I want to start out by saying thank you so much

3
00:00:18,640 --> 00:00:19,640
for your support.

4
00:00:19,640 --> 00:00:24,040
It's been really great to get your notes and feedback about the show.

5
00:00:24,040 --> 00:00:28,360
I won't go into the backstory here, but going forward, I'm going to pivot a bit in my approach

6
00:00:28,360 --> 00:00:33,960
to the show and focus on interviews with interesting folks in machine learning and AI.

7
00:00:33,960 --> 00:00:38,680
And to accompany the podcast, I'm still going to bring you the news, but now via the email

8
00:00:38,680 --> 00:00:40,080
newsletter.

9
00:00:40,080 --> 00:00:44,400
If you'd like to know more about these changes, hop over to the show notes after listening,

10
00:00:44,400 --> 00:00:51,880
which can be found at twimlai.com slash talk, T-A-L-K slash two, the number two.

11
00:00:51,880 --> 00:00:56,000
Okay, so about the interview you're about to hear.

12
00:00:56,000 --> 00:01:00,520
If you've listened to a few of my previous shows, you've probably heard me mention the name

13
00:01:00,520 --> 00:01:02,520
Siraj Ravel.

14
00:01:02,520 --> 00:01:07,960
Siraj is a machine learning hacker and educator whose machine learning for hackers and fresh

15
00:01:07,960 --> 00:01:14,160
machine learning YouTube series are fun, informative, high energy and practical ways to learn

16
00:01:14,160 --> 00:01:18,240
about a ton of machine learning and AI topics.

17
00:01:18,240 --> 00:01:24,000
I had a chance to catch up with Siraj in San Francisco recently and we had a great discussion.

18
00:01:24,000 --> 00:01:27,960
Siraj has great advice on how to learn machine learning and build confidence as a machine

19
00:01:27,960 --> 00:01:33,280
learning developer, how to research and formulate projects, who to follow on machine learning

20
00:01:33,280 --> 00:01:35,160
Twitter and much more.

21
00:01:35,160 --> 00:01:41,280
I'll include links to Siraj's shows and some of the things we discuss in the show notes.

22
00:01:41,280 --> 00:01:43,320
A quick note before the interview.

23
00:01:43,320 --> 00:01:47,000
If you're new to the show, you should know that I've partnered with O'Reilly to give

24
00:01:47,000 --> 00:01:49,840
away a ticket to their upcoming AI conference.

25
00:01:49,840 --> 00:01:56,840
I'll talk about how to enter after the interview and end the show notes and now onto the interview.

26
00:01:56,840 --> 00:02:06,680
Alright, so I'm here with Siraj Ravel.

27
00:02:06,680 --> 00:02:08,520
Siraj, it's great to meet you in person.

28
00:02:08,520 --> 00:02:14,200
I've been talking about your YouTube videos on the podcast for I've talked about a couple

29
00:02:14,200 --> 00:02:18,880
of them and like I wanted to talk about you like every week because there's so many

30
00:02:18,880 --> 00:02:23,120
great videos but I've held back a lot, you know, I got to spread the love.

31
00:02:23,120 --> 00:02:27,560
So it's great to get a chance to meet you in person and I just wanted to spend a few

32
00:02:27,560 --> 00:02:31,360
minutes kind of talking about what you're up to and how you got here.

33
00:02:31,360 --> 00:02:32,360
Sounds good, yeah.

34
00:02:32,360 --> 00:02:33,360
I'm totally down.

35
00:02:33,360 --> 00:02:34,360
Appreciate you coming over.

36
00:02:34,360 --> 00:02:35,360
Nice.

37
00:02:35,360 --> 00:02:39,040
So, you know, let's start there like how did you get into machine learning?

38
00:02:39,040 --> 00:02:47,400
I mean, ever since I was in college, I was looking for something to really put all my

39
00:02:47,400 --> 00:02:52,840
energy into and what it was for me was a robotics lab at my school at Columbia and the robotics

40
00:02:52,840 --> 00:02:56,960
lab was my first for ray into machine learning and I found that there were all these problems

41
00:02:56,960 --> 00:03:02,720
that I wanted to solve that at the time deep learning wasn't really a thing that deep learning

42
00:03:02,720 --> 00:03:08,240
would then solve later like in two years and so I was looking into like the initial types

43
00:03:08,240 --> 00:03:12,960
of machine learning like support vector machines and things like that and just gradually

44
00:03:12,960 --> 00:03:16,560
over time I realized like, hey, neural nets, deep learning, this stuff is like going to

45
00:03:16,560 --> 00:03:20,360
solve so many problems.

46
00:03:20,360 --> 00:03:25,240
So yeah, I've just always been into intelligence and solving intelligence.

47
00:03:25,240 --> 00:03:29,800
That's pretty much my main driver in life like I want to help humanity solve intelligence

48
00:03:29,800 --> 00:03:32,360
because I think it's the most important thing we can do.

49
00:03:32,360 --> 00:03:36,200
So Columbia is, you know, not San Francisco and we're sitting here in San Francisco like

50
00:03:36,200 --> 00:03:37,200
what was the path?

51
00:03:37,200 --> 00:03:38,200
How did you get?

52
00:03:38,200 --> 00:03:39,840
How did you end up here and what are you up to?

53
00:03:39,840 --> 00:03:45,480
Yeah, so yeah, was that Columbia and honestly I didn't feel like I really fit into Columbia.

54
00:03:45,480 --> 00:03:50,440
I was, you know, I fit in really well here in San Francisco in like Silicon Valley culture.

55
00:03:50,440 --> 00:03:56,520
I think because I'm, you know, I'm not so much into like going to classes in person and

56
00:03:56,520 --> 00:04:00,200
just like studying subjects that I don't care a lot about.

57
00:04:00,200 --> 00:04:03,120
Like I just wanted to just study robotics and AI.

58
00:04:03,120 --> 00:04:07,520
So once I was at the robotics lab, I felt like, okay, this is, this is like my thing.

59
00:04:07,520 --> 00:04:09,160
I'm going to keep doing this.

60
00:04:09,160 --> 00:04:12,920
But that only lasted like a year and then I had a startup called Lucid Robotics where

61
00:04:12,920 --> 00:04:16,520
I was trying to create a robot for your home like a platform where each app would be a physical

62
00:04:16,520 --> 00:04:17,520
task.

63
00:04:17,520 --> 00:04:20,000
So you'd have an app for like cleaning the dishes and stuff.

64
00:04:20,000 --> 00:04:23,640
Clearly, this was way out of scope at the time, but at the time you couldn't tell me that.

65
00:04:23,640 --> 00:04:26,480
I had to see the computer science problems myself.

66
00:04:26,480 --> 00:04:30,080
What actually ended the startup, I mean, we raised funding from Sabir Bhattia, the founder

67
00:04:30,080 --> 00:04:32,360
of Hotmail, we had a team.

68
00:04:32,360 --> 00:04:36,880
What ended the startup was we couldn't get the robot to pick up a simple novel object

69
00:04:36,880 --> 00:04:39,240
they had never seen before.

70
00:04:39,240 --> 00:04:41,320
Deep learning now solves this.

71
00:04:41,320 --> 00:04:44,720
So then, so then after the startup failed, I dropped out.

72
00:04:44,720 --> 00:04:45,720
I dropped out of Columbia.

73
00:04:45,720 --> 00:04:49,200
I just was so disenchanted with so many things.

74
00:04:49,200 --> 00:04:54,320
And I felt like San Francisco was the place where I could go to rediscover myself.

75
00:04:54,320 --> 00:04:57,000
And it's been a, it's been a, you know, quite a journey.

76
00:04:57,000 --> 00:05:01,400
And there's been a lot of uncertainty in my life about what I should be doing, the path

77
00:05:01,400 --> 00:05:03,680
I should be moving towards.

78
00:05:03,680 --> 00:05:07,760
But I'm lucky enough to have come to the conclusions that I have that intelligence is the most

79
00:05:07,760 --> 00:05:12,760
important thing for us to solve in our lifetime.

80
00:05:12,760 --> 00:05:17,240
Because if we don't solve it, then some other catastrophe could wipe out our species, whether

81
00:05:17,240 --> 00:05:22,640
it's biochemical terrorism or some natural disaster or, you know, something like that.

82
00:05:22,640 --> 00:05:24,360
We have to solve intelligence.

83
00:05:24,360 --> 00:05:25,360
Yeah.

84
00:05:25,360 --> 00:05:29,240
So how did you, how did that bring you to doing a YouTube channel?

85
00:05:29,240 --> 00:05:30,240
Yeah.

86
00:05:30,240 --> 00:05:36,040
So, so I, you know, I had a few jobs here as an engineer at CBS Interactive and at Twilio

87
00:05:36,040 --> 00:05:37,240
and they were, they were incredible.

88
00:05:37,240 --> 00:05:42,560
I love these positions, but engineering itself just, I don't know, I felt like I could,

89
00:05:42,560 --> 00:05:44,720
I could be having more impact at Twilio.

90
00:05:44,720 --> 00:05:46,120
I mean, Twilio was a great place.

91
00:05:46,120 --> 00:05:48,600
They, I was doing company, it was a great company.

92
00:05:48,600 --> 00:05:52,520
I was doing developer education and like, that was my full time role.

93
00:05:52,520 --> 00:05:53,840
So I was doing technical writing.

94
00:05:53,840 --> 00:05:56,080
It was the first time I hadn't just been doing code.

95
00:05:56,080 --> 00:05:58,560
And I felt like, okay, this is, this is my thing, like technical writing.

96
00:05:58,560 --> 00:05:59,560
This is awesome.

97
00:05:59,560 --> 00:06:02,480
I get to, I get to combine my writing ability and my coding ability.

98
00:06:02,480 --> 00:06:03,480
Yeah.

99
00:06:03,480 --> 00:06:08,720
The thing for me, like the reason that I left was that I wanted to do video documentation.

100
00:06:08,720 --> 00:06:12,640
I believe in the future of video documentation.

101
00:06:12,640 --> 00:06:14,720
And I, I feel like Twilio was going on a different path.

102
00:06:14,720 --> 00:06:19,040
So I decided, okay, you know what, I'm just going to do this full time.

103
00:06:19,040 --> 00:06:22,520
And so I started the YouTube channel on the side while I was at Twilio.

104
00:06:22,520 --> 00:06:23,520
Okay.

105
00:06:23,520 --> 00:06:27,040
So I was making one video a week, but the quality wasn't at the level that I wanted it

106
00:06:27,040 --> 00:06:28,040
to be.

107
00:06:28,040 --> 00:06:30,560
I didn't have enough, like the production equipment wasn't good enough.

108
00:06:30,560 --> 00:06:31,560
Yeah.

109
00:06:31,560 --> 00:06:33,280
I wasn't giving enough time to the technical writing.

110
00:06:33,280 --> 00:06:36,120
So the only option I had was to quit and do this full time.

111
00:06:36,120 --> 00:06:37,120
Okay.

112
00:06:37,120 --> 00:06:39,160
And so then I was just like, all right, here we go.

113
00:06:39,160 --> 00:06:41,080
And how many have you done?

114
00:06:41,080 --> 00:06:42,080
Video so far.

115
00:06:42,080 --> 00:06:43,080
Yeah.

116
00:06:43,080 --> 00:06:47,200
I think it's like, it's at least, at least like 28 videos now.

117
00:06:47,200 --> 00:06:48,200
Wow.

118
00:06:48,200 --> 00:06:49,200
Almost 30.

119
00:06:49,200 --> 00:06:52,360
It's one video a week every week since like January 1st.

120
00:06:52,360 --> 00:06:53,360
Wow.

121
00:06:53,360 --> 00:06:54,360
Nice.

122
00:06:54,360 --> 00:06:55,360
Nice.

123
00:06:55,360 --> 00:06:57,960
And you've, the original show was called Machine Learning for Hackers.

124
00:06:57,960 --> 00:06:58,960
Is that right?

125
00:06:58,960 --> 00:06:59,960
Yeah.

126
00:06:59,960 --> 00:07:00,960
Machine Learning for Hackers.

127
00:07:00,960 --> 00:07:01,960
And you, you've just launched a new one?

128
00:07:01,960 --> 00:07:02,960
Yeah.

129
00:07:02,960 --> 00:07:06,000
And does that one replace Machine Learning for Hackers?

130
00:07:06,000 --> 00:07:10,600
Or are they like two parallel tracks that continue ongoing?

131
00:07:10,600 --> 00:07:13,560
You know, it's interesting because the idea with Machine Learning for Hackers is that

132
00:07:13,560 --> 00:07:14,920
it's meant for developers.

133
00:07:14,920 --> 00:07:18,160
And Fresh Machine Learning was also meant for developers, but it was like a different

134
00:07:18,160 --> 00:07:19,160
topic subset.

135
00:07:19,160 --> 00:07:20,600
It was like newer things.

136
00:07:20,600 --> 00:07:25,840
But what I've noticed is that I have, so my subscribers, I have three different types

137
00:07:25,840 --> 00:07:27,000
of people who are watching me.

138
00:07:27,000 --> 00:07:32,640
I have the research scientists, the cool kids who are like developing the novel algorithms.

139
00:07:32,640 --> 00:07:35,720
Then there's the developers who are, honestly, they're also the cool kids.

140
00:07:35,720 --> 00:07:40,920
And those are the people I really want to, you know, they were my main motivation from

141
00:07:40,920 --> 00:07:41,920
the start.

142
00:07:41,920 --> 00:07:43,480
I want to make things for developers.

143
00:07:43,480 --> 00:07:47,120
And then there's actually the third subset which I'm learning about, which are people

144
00:07:47,120 --> 00:07:49,560
who are not really technical, but they really want to be.

145
00:07:49,560 --> 00:07:53,800
So it's like, I have to make videos that catering to each of them.

146
00:07:53,800 --> 00:07:58,120
So I'm still kind of trying to figure out like, you know, because sometimes my video is

147
00:07:58,120 --> 00:08:01,960
catered to the research scientists, sometimes the developers, sometimes the, you know, people

148
00:08:01,960 --> 00:08:03,640
who are not very technical.

149
00:08:03,640 --> 00:08:08,200
So I think for now, I'm making videos that kind of cater to all three, but eventually I

150
00:08:08,200 --> 00:08:12,160
want to get to the point where I have channels, dedicated channels for each of these subsets.

151
00:08:12,160 --> 00:08:13,160
Yeah.

152
00:08:13,160 --> 00:08:14,400
And for that, I have to grow a little bit more.

153
00:08:14,400 --> 00:08:15,400
Okay.

154
00:08:15,400 --> 00:08:22,320
It sounds like, in a lot of ways, a parallel path to mine with this podcast, I, you know,

155
00:08:22,320 --> 00:08:27,120
my initial vision was, you know, I just couldn't get enough machine learning information.

156
00:08:27,120 --> 00:08:32,520
Like I, I, you know, spend the week like opening up web browser tabs of articles that I wanted

157
00:08:32,520 --> 00:08:35,360
to read or papers that I wanted to take a look at.

158
00:08:35,360 --> 00:08:39,240
And I'd end up in a given week with like 80 to 100 of these tabs open.

159
00:08:39,240 --> 00:08:43,760
And I'm like, this is ridiculous, not, and then you spend some time going through it and

160
00:08:43,760 --> 00:08:45,120
half of it is crap.

161
00:08:45,120 --> 00:08:50,280
And like if everyone's doing the same thing, then, you know, people would appreciate, you

162
00:08:50,280 --> 00:08:53,480
know, something that tries to figure out what's good and what's not and just spend some

163
00:08:53,480 --> 00:08:54,760
time talking about what's good.

164
00:08:54,760 --> 00:08:59,320
So hey, I don't have to spend my week collecting this bag of tabs.

165
00:08:59,320 --> 00:09:02,920
And you know, it's been super rewarding, but it's like a ton of work.

166
00:09:02,920 --> 00:09:03,920
It's a ton of work.

167
00:09:03,920 --> 00:09:08,600
And then I look at your stuff and like, I can't imagine what goes into, you know, your

168
00:09:08,600 --> 00:09:12,840
videos because you're like going deep into a topic and then, you know, you're writing

169
00:09:12,840 --> 00:09:15,760
code, you're like, you know, publishing code up on GitHub.

170
00:09:15,760 --> 00:09:16,760
What's the process?

171
00:09:16,760 --> 00:09:20,240
Is it, is it the same every, every week, or are you like still experimenting?

172
00:09:20,240 --> 00:09:21,240
Yeah.

173
00:09:21,240 --> 00:09:23,640
So I've developed a methodology for this over time.

174
00:09:23,640 --> 00:09:25,920
Like I'm building the process.

175
00:09:25,920 --> 00:09:30,360
So what it is is like the first part of research, like what is the topic I want to talk about

176
00:09:30,360 --> 00:09:32,720
and let me just learn about it.

177
00:09:32,720 --> 00:09:37,160
The second part is the code, like programming it.

178
00:09:37,160 --> 00:09:40,960
Like I'm going to program some very, very simple, what I like to call the quick start

179
00:09:40,960 --> 00:09:41,960
of X.

180
00:09:41,960 --> 00:09:46,200
So the quick start of auto encoders, the quick start of support vector machines.

181
00:09:46,200 --> 00:09:47,200
Yep.

182
00:09:47,200 --> 00:09:49,440
Then it's the technical writing.

183
00:09:49,440 --> 00:09:53,800
So research, code, technical writing, then it's the production.

184
00:09:53,800 --> 00:09:57,520
So the actual video, like shooting it, and then it's editing.

185
00:09:57,520 --> 00:10:02,440
And then there's marketing and release.

186
00:10:02,440 --> 00:10:05,880
So yeah, it's like five or six things in sequential order.

187
00:10:05,880 --> 00:10:11,720
And I can, I managed to fit, I'm able to fit all these things into a single week and

188
00:10:11,720 --> 00:10:15,360
it takes around 40 to 60 hours for a single video.

189
00:10:15,360 --> 00:10:17,600
Generally, it's closer to 60 hours.

190
00:10:17,600 --> 00:10:22,960
Now I have clients, so I'm increasing the output from one video a week to two.

191
00:10:22,960 --> 00:10:24,880
So that's like 120 hours a week.

192
00:10:24,880 --> 00:10:26,520
That's a lot.

193
00:10:26,520 --> 00:10:30,800
This is actually the first week where I have to make two videos in one week.

194
00:10:30,800 --> 00:10:37,080
So I'm hiring, yeah, I'm hiring a video editor, a technical video editor, which is like

195
00:10:37,080 --> 00:10:42,880
a new role because they have to be a video editor who also knows kind of like how to code.

196
00:10:42,880 --> 00:10:46,360
Because I have code and I have to, you know, they have to point those red arrows at what

197
00:10:46,360 --> 00:10:47,360
I'm talking about.

198
00:10:47,360 --> 00:10:48,360
They have to be at least.

199
00:10:48,360 --> 00:10:49,360
They need to know what's important.

200
00:10:49,360 --> 00:10:50,600
They need to know what's important, right?

201
00:10:50,600 --> 00:10:53,400
And they have to know the cards, you know, the cards where I'm talking, like what I'm

202
00:10:53,400 --> 00:10:56,880
saying, like, oh, this is a relevant what he's talking about, like support documentaries

203
00:10:56,880 --> 00:10:57,880
or whatever.

204
00:10:57,880 --> 00:10:58,880
Yeah.

205
00:10:58,880 --> 00:10:59,880
So I'm looking for unicorns, basically.

206
00:10:59,880 --> 00:11:00,880
Yeah.

207
00:11:00,880 --> 00:11:01,880
Yeah.

208
00:11:01,880 --> 00:11:02,880
Aren't we all?

209
00:11:02,880 --> 00:11:10,000
Well, so you're doing it all in one week, you're not like, you know, researching one week

210
00:11:10,000 --> 00:11:14,160
and producing the, you know, researching in next week's video one week and then producing

211
00:11:14,160 --> 00:11:15,800
it's all self-contained in that week.

212
00:11:15,800 --> 00:11:17,440
It's all self-contained in that week, yeah.

213
00:11:17,440 --> 00:11:23,560
And how do you determine what's, you know, what you're going to talk about next?

214
00:11:23,560 --> 00:11:25,560
That's a good question.

215
00:11:25,560 --> 00:11:32,600
I, so, yeah, so I browsed the machine learning subreddit, looking at what's hot, what

216
00:11:32,600 --> 00:11:38,040
it, whatever interests me, I look at hacker news, I look at Twitter, like Twitter is actually

217
00:11:38,040 --> 00:11:39,120
a great learning tool for me.

218
00:11:39,120 --> 00:11:43,000
I just follow people who I think are really smart and, you know, young lacunes and stuff

219
00:11:43,000 --> 00:11:44,000
like that.

220
00:11:44,000 --> 00:11:52,720
And I think my, the other data source is Facebook groups, I mean, a lot of machine learning

221
00:11:52,720 --> 00:11:54,520
Facebook groups.

222
00:11:54,520 --> 00:11:58,000
So yeah, whatever is like new and hot and the intersection of what's new and hot and

223
00:11:58,000 --> 00:12:02,880
like what I'm into, generally, I can figure that out in like one day, but it takes all

224
00:12:02,880 --> 00:12:03,880
day.

225
00:12:03,880 --> 00:12:04,880
Yeah.

226
00:12:04,880 --> 00:12:05,880
Yeah.

227
00:12:05,880 --> 00:12:08,680
The curation part is, yeah, it's hard.

228
00:12:08,680 --> 00:12:12,000
I mean, there's just, there's a lot of stuff out there, you know, like I said before and

229
00:12:12,000 --> 00:12:15,600
there's a lot of stuff that, you know, looks really, there's a lot of clickbait, right?

230
00:12:15,600 --> 00:12:16,600
It looks really interesting.

231
00:12:16,600 --> 00:12:19,560
And then you get, you dig deep and it's just nothing there.

232
00:12:19,560 --> 00:12:20,560
Totally.

233
00:12:20,560 --> 00:12:21,560
Yeah.

234
00:12:21,560 --> 00:12:25,400
Or it's like just way, way too technical and you didn't even think it would be.

235
00:12:25,400 --> 00:12:26,800
It's like a lot of math.

236
00:12:26,800 --> 00:12:29,040
It's like, ah, here we go.

237
00:12:29,040 --> 00:12:33,240
Is there an example of, you know, something that you thought you wanted to take on?

238
00:12:33,240 --> 00:12:40,120
And then you just, you know, found out that it was just, that the math was just too ridiculous.

239
00:12:40,120 --> 00:12:49,200
I think, ah, well, I can't, well, I, if I've decided I'm going to do it, I'm just like,

240
00:12:49,200 --> 00:12:52,720
I literally don't have time to like, not do it because I've, you know what I mean?

241
00:12:52,720 --> 00:12:53,960
Because I just have to keep going.

242
00:12:53,960 --> 00:12:58,440
But I can tell you that the closest I was to like, not being able to finish a video

243
00:12:58,440 --> 00:13:00,600
was generative adversarial networks.

244
00:13:00,600 --> 00:13:01,600
That was a very video.

245
00:13:01,600 --> 00:13:02,600
That was a very video.

246
00:13:02,600 --> 00:13:03,600
Thank you.

247
00:13:03,600 --> 00:13:07,480
That was the hardest video I've ever had to make because make, because that stuff, yeah,

248
00:13:07,480 --> 00:13:09,680
that stuff was pretty hard.

249
00:13:09,680 --> 00:13:12,680
Was that the video where you're like, well, this really should be two or three videos,

250
00:13:12,680 --> 00:13:15,520
but I'm just going to, you know, cram it down to one and see how it goes.

251
00:13:15,520 --> 00:13:18,280
It was one of those where you said something like that, I thought.

252
00:13:18,280 --> 00:13:22,200
Um, yeah, no, no, I could definitely have more than one on, uh, Gans.

253
00:13:22,200 --> 00:13:23,200
Yeah.

254
00:13:23,200 --> 00:13:27,160
Uh, so what, um, you know, that's the topic that's come up on my podcast quite a bit.

255
00:13:27,160 --> 00:13:31,000
Why don't you talk a little bit about, um, you know, talk a little bit about Gans, what

256
00:13:31,000 --> 00:13:34,160
you learn there in, in doing that project.

257
00:13:34,160 --> 00:13:35,160
Yeah.

258
00:13:35,160 --> 00:13:41,720
So yeah, Ian Goodfellow, who's now a research scientist at OpenAI, he's, he's the guy who

259
00:13:41,720 --> 00:13:48,160
authored the paper, but it's a, it's a generative model that can create, uh, so if you give it

260
00:13:48,160 --> 00:13:53,760
some input data, it's going to, it's going to have some output data that's similar to the

261
00:13:53,760 --> 00:13:55,360
input data, but different.

262
00:13:55,360 --> 00:13:59,200
So if you feed it like a collection of faces, it's going to generate faces that look similar

263
00:13:59,200 --> 00:14:00,720
but are different.

264
00:14:00,720 --> 00:14:03,440
And at first, I was like, well, how is this going to be useful?

265
00:14:03,440 --> 00:14:08,960
But it's a tool for any kind of engineer to design, uh, so if you feed it like, you

266
00:14:08,960 --> 00:14:16,160
know, uh, collection of living rooms, it's going to be able to generate novel living

267
00:14:16,160 --> 00:14:19,880
rooms that look photorealistic, which is super cool.

268
00:14:19,880 --> 00:14:24,600
So it's a tool to help engineers like envision their ideas better.

269
00:14:24,600 --> 00:14:33,400
And yeah, yeah, I, um, I like the idea of two dueling entities, you know, uh, uh,

270
00:14:33,400 --> 00:14:40,400
how the discriminator is always trying to, uh, fool, um, fool, fool, fool its counterpart,

271
00:14:40,400 --> 00:14:44,920
or the, the counterpart is always trying to fool the discriminator, which is always trying

272
00:14:44,920 --> 00:14:47,920
to detect like, oh, is this, is this false or real?

273
00:14:47,920 --> 00:14:48,920
Right.

274
00:14:48,920 --> 00:14:51,960
And just keeps doing that until eventually, you know, it just gets better and better.

275
00:14:51,960 --> 00:14:52,960
It's a brilliant idea.

276
00:14:52,960 --> 00:14:55,440
And like, you know, deep mind has done this stuff with like AlphaGo when they trained

277
00:14:55,440 --> 00:14:59,000
two dual neural neural nets against each other to play Go, so it just got better and

278
00:14:59,000 --> 00:15:00,000
better.

279
00:15:00,000 --> 00:15:04,480
So this idea of, you know, of having this adversarial nature can be applied to a lot of other

280
00:15:04,480 --> 00:15:05,800
things in machine learning.

281
00:15:05,800 --> 00:15:06,800
Have you seen examples of that?

282
00:15:06,800 --> 00:15:12,280
I've been looking for that, uh, as well, I've come across, uh, or at least ideas of, uh,

283
00:15:12,280 --> 00:15:16,520
of, you know, where, hey, if we can pit one machine learning algorithm or one AI against

284
00:15:16,520 --> 00:15:19,200
another, you know, and let them train each other.

285
00:15:19,200 --> 00:15:23,720
Have you seen, uh, besides from the, the generous stuff that was covered in the papers,

286
00:15:23,720 --> 00:15:25,240
other examples of that?

287
00:15:25,240 --> 00:15:33,600
Yeah, um, I think, uh, there's a lot of a potential for like game AI.

288
00:15:33,600 --> 00:15:38,280
So if you have, you know, a bot versus a human, or just two bots first thing each other.

289
00:15:38,280 --> 00:15:43,840
Um, I think so, so deep mind is like really into games, which is cool.

290
00:15:43,840 --> 00:15:44,840
Uh-huh.

291
00:15:44,840 --> 00:15:51,800
And I think there's a lot of potential for combining, uh, adversarial work with what

292
00:15:51,800 --> 00:15:54,080
they're doing in 3D games.

293
00:15:54,080 --> 00:15:57,880
Um, just as like a, I mean, it was, it's kind of like a suggestion on my part.

294
00:15:57,880 --> 00:16:05,200
I'm sure they've already thought about this, but if you, um, apply gans, if you were to

295
00:16:05,200 --> 00:16:09,400
apply gans to games, I think that would be really cool.

296
00:16:09,400 --> 00:16:10,400
I haven't seen a paper.

297
00:16:10,400 --> 00:16:14,760
No, we're talking about like first person shooters type games or the types of games that

298
00:16:14,760 --> 00:16:18,000
they're playing, you know, in deep mind, the Atari game.

299
00:16:18,000 --> 00:16:19,000
No, no, no, yeah.

300
00:16:19,000 --> 00:16:20,000
Okay.

301
00:16:20,000 --> 00:16:21,000
So, okay.

302
00:16:21,000 --> 00:16:23,960
So like, what I think is really cool, um, so open AI just yesterday.

303
00:16:23,960 --> 00:16:28,920
I think released this call for, um, research scientist on four problems.

304
00:16:28,920 --> 00:16:30,680
There was number four was, what was it?

305
00:16:30,680 --> 00:16:36,040
It was like create a simulation that where all the entities get better and better over

306
00:16:36,040 --> 00:16:37,040
time.

307
00:16:37,040 --> 00:16:41,800
Like you create an entity in the simulated world and then it learns to like what kind of

308
00:16:41,800 --> 00:16:45,560
food it needs, what kind of nutrition it needs to survive better and better.

309
00:16:45,560 --> 00:16:50,840
Like I think there's a lot of potential for adversarial, uh, algorithms there, um,

310
00:16:50,840 --> 00:16:54,280
two entities versus each other in this, in this simulated world.

311
00:16:54,280 --> 00:16:57,880
So maybe not necessarily just a game, but any kind of simulated environment where you

312
00:16:57,880 --> 00:17:02,360
have a set of constraints and you want, you want it, you want some kind of AI to get

313
00:17:02,360 --> 00:17:04,280
better over time.

314
00:17:04,280 --> 00:17:10,760
I think we're going to see a lot of, um, a lot of adversarial algorithms in the future.

315
00:17:10,760 --> 00:17:12,760
And a lot of one shot learning, mm-hmm.

316
00:17:12,760 --> 00:17:16,120
I'd like to see more of that because right now, you know, all this machine learning stuff

317
00:17:16,120 --> 00:17:22,600
is, is kind of siphoned off to these big companies like Facebook and Google and Apple.

318
00:17:22,600 --> 00:17:25,800
But with, you know, if, with advances in one shot learning, anybody who is going to be

319
00:17:25,800 --> 00:17:32,360
able to, uh, create these models and, and learning algorithms from sparse data, startups,

320
00:17:32,360 --> 00:17:35,880
for example, that only have like, you know, a hundred users, but they want to imply machine

321
00:17:35,880 --> 00:17:36,880
learning to that.

322
00:17:36,880 --> 00:17:37,880
Right.

323
00:17:37,880 --> 00:17:38,880
Right.

324
00:17:38,880 --> 00:17:45,800
Uh, so you dig into a topic like this, you know, Gans, there's, you know, research papers.

325
00:17:45,800 --> 00:17:52,120
Like how do you, how do you take, how do you make the leap from that to code to getting

326
00:17:52,120 --> 00:17:53,120
code up?

327
00:17:53,120 --> 00:17:57,640
Um, you know, a lot of the folks that listen to my podcast, I've, you know, heard from,

328
00:17:57,640 --> 00:18:00,800
you know, are in the process of learning and they're trying to figure out projects to work

329
00:18:00,800 --> 00:18:05,280
on and, you know, getting from some of the things that they're reading about to, you

330
00:18:05,280 --> 00:18:09,040
know, some working example, like, and you've got that down to a science, right?

331
00:18:09,040 --> 00:18:13,960
So at this point, yeah, uh, from repetition, how do you approach it?

332
00:18:13,960 --> 00:18:23,960
Yeah, so I think, um, for me, it's, it's, it's, it's, a lot of it is what I learned

333
00:18:23,960 --> 00:18:28,640
from Tulio, like the idea of having a quick start, like a bare bone skeleton that a developer

334
00:18:28,640 --> 00:18:30,080
can then build off of.

335
00:18:30,080 --> 00:18:34,640
What is the minimum viable product for, for demoing this, this idea that you have?

336
00:18:34,640 --> 00:18:36,920
However simple you can make it, do it.

337
00:18:36,920 --> 00:18:49,360
So if I read something like, you know, um, a paper on, like, for example, ah, there's

338
00:18:49,360 --> 00:18:57,320
so much, uh, auto encoders, what's the simplest thing I can do with an auto encoder?

339
00:18:57,320 --> 00:19:02,440
An auto encoder takes them input, compresses it and then, uh, reconstructs it.

340
00:19:02,440 --> 00:19:05,160
It's only a, it's a three layer, it's a very simple neural network.

341
00:19:05,160 --> 00:19:07,160
What's the simple, simple demo I can make with this?

342
00:19:07,160 --> 00:19:11,560
And I just think about it and I'm like, okay, compression, oh, compression, just compression

343
00:19:11,560 --> 00:19:12,560
alone.

344
00:19:12,560 --> 00:19:15,320
So just use it as a compression algorithm.

345
00:19:15,320 --> 00:19:18,080
So like a zip, you know, zipping, yeah, zipping and unzipping.

346
00:19:18,080 --> 00:19:20,880
So then I was like, okay, so then I'm like, okay, so how do I code this?

347
00:19:20,880 --> 00:19:22,560
So what I first do is I search GitHub.

348
00:19:22,560 --> 00:19:26,640
So it's happened like very, you know, auto encoder and I look under Python because Python

349
00:19:26,640 --> 00:19:28,640
is awesome.

350
00:19:28,640 --> 00:19:34,160
And I see what's been done before, usually, usually something has been done before.

351
00:19:34,160 --> 00:19:37,160
So I'll take that and I'll like kind of like strip away the unnecessary things and add

352
00:19:37,160 --> 00:19:38,160
documentation.

353
00:19:38,160 --> 00:19:39,160
And that's going to be the demo.

354
00:19:39,160 --> 00:19:40,160
Okay.

355
00:19:40,160 --> 00:19:42,160
And the rare case, it's not, then I have to code it myself.

356
00:19:42,160 --> 00:19:43,160
Okay.

357
00:19:43,160 --> 00:19:44,160
Yeah.

358
00:19:44,160 --> 00:19:45,160
How often does that happen?

359
00:19:45,160 --> 00:19:50,720
That code to get yourself, like entirely, um, I'd say like off the top of my head, probably

360
00:19:50,720 --> 00:19:52,520
like 15% of the time.

361
00:19:52,520 --> 00:19:53,520
Okay.

362
00:19:53,520 --> 00:19:54,520
Yeah.

363
00:19:54,520 --> 00:19:58,000
So one of the, you know, the two lessons I got from that are, you know, simplify, simplify,

364
00:19:58,000 --> 00:20:04,120
simplify, like, you know, you know, whether it's, uh, the actual coding or the, uh,

365
00:20:04,120 --> 00:20:08,920
you know, trying to parse the research, it's like, figure out what this thing is at its

366
00:20:08,920 --> 00:20:14,640
bare essence and focus on that, uh, and then like reuse, like, figure out what's been

367
00:20:14,640 --> 00:20:16,400
done and try to use that.

368
00:20:16,400 --> 00:20:17,400
Is there anything else?

369
00:20:17,400 --> 00:20:21,440
And like any other pieces of advice that you'd give to folks that are trying to work this

370
00:20:21,440 --> 00:20:22,440
process?

371
00:20:22,440 --> 00:20:29,800
Um, yeah, just like, don't be intimidated by papers, like, there is a lot of math and

372
00:20:29,800 --> 00:20:30,800
papers.

373
00:20:30,800 --> 00:20:38,200
Really, like when I'm reading a paper, um, it's the abstract and the background, the process

374
00:20:38,200 --> 00:20:42,400
and the conclusion, which matter the most to me.

375
00:20:42,400 --> 00:20:43,880
And there's really not a lot of math.

376
00:20:43,880 --> 00:20:50,240
It's, it's, it's when they start describing, you know, certain aspects of the process that

377
00:20:50,240 --> 00:20:55,200
it can get really, really confusing if you don't know math notation.

378
00:20:55,200 --> 00:20:58,920
But math notation itself is in serious need of an upgrade.

379
00:20:58,920 --> 00:21:01,080
Mm-hmm, so it's more human readable.

380
00:21:01,080 --> 00:21:02,080
Mm-hmm.

381
00:21:02,080 --> 00:21:05,920
Right now, it's kind of siphoned off to just these research scientists who look at this

382
00:21:05,920 --> 00:21:07,160
stuff every day.

383
00:21:07,160 --> 00:21:12,400
So I think, you know, we're going to start to see, um, innovations in, in how we publish

384
00:21:12,400 --> 00:21:15,200
scientific research so that anybody can read it.

385
00:21:15,200 --> 00:21:17,360
What that's going to look like, I'm not sure.

386
00:21:17,360 --> 00:21:22,560
But they're just, they're just too much coming out right now and it's too important, uh,

387
00:21:22,560 --> 00:21:26,360
for a few people, for only a few people to be able to read it.

388
00:21:26,360 --> 00:21:32,760
So, so I would say, if you just read the abstract of a paper and you feel like you get the

389
00:21:32,760 --> 00:21:35,240
gist, that's fine.

390
00:21:35,240 --> 00:21:37,280
You can go start searching GitHub with just that.

391
00:21:37,280 --> 00:21:41,040
Don't feel like, um, you know, guilty or, or something.

392
00:21:41,040 --> 00:21:44,200
And definitely look at videos and, and what I try to do when I'm trying to learn something

393
00:21:44,200 --> 00:21:48,400
is I try to get as many different types of data sources that can into my brain.

394
00:21:48,400 --> 00:21:49,400
That always helps.

395
00:21:49,400 --> 00:21:55,560
Videos, articles, conversations with people, you know, there's, there's a lot of content

396
00:21:55,560 --> 00:21:56,560
out there.

397
00:21:56,560 --> 00:21:58,120
And it's just going to increase exponentially.

398
00:21:58,120 --> 00:21:59,120
Mm-hmm.

399
00:21:59,120 --> 00:22:00,120
Yeah.

400
00:22:00,120 --> 00:22:04,920
I find the same thing and find also that, um, sometimes it doesn't work out like you expect

401
00:22:04,920 --> 00:22:11,560
like the, I did a review of the Google research wide and deep learning paper.

402
00:22:11,560 --> 00:22:15,080
And you know, they've got this cool YouTube video that, you know, simplifies everything.

403
00:22:15,080 --> 00:22:16,720
But I watched that and I didn't, I didn't get it.

404
00:22:16,720 --> 00:22:19,360
But then I went through the paper and, uh, it made sense.

405
00:22:19,360 --> 00:22:22,320
And then I went back to the video and was like, oh, yeah, I don't know why I didn't get

406
00:22:22,320 --> 00:22:23,320
that before.

407
00:22:23,320 --> 00:22:24,320
Yeah.

408
00:22:24,320 --> 00:22:30,640
I think that, you know, having lots of different types of input can make a big difference.

409
00:22:30,640 --> 00:22:38,520
So what's, like, what's your roadmap for, for upcoming topics and research?

410
00:22:38,520 --> 00:22:40,520
Yeah.

411
00:22:40,520 --> 00:22:45,800
Um, so in terms of like the topics themselves, I kind of decide them week to week.

412
00:22:45,800 --> 00:22:51,760
But the, for the larger vision is to just focus on machine learning, kind of be like

413
00:22:51,760 --> 00:22:54,560
Khan Academy for machine learning.

414
00:22:54,560 --> 00:22:57,560
And I'm going to start needing help and from other people.

415
00:22:57,560 --> 00:23:04,360
So I'm hiring and, uh, yeah, just try to get, I'm just optimizing for subscribers.

416
00:23:04,360 --> 00:23:08,240
I want to get, you know, I want to get every developer on the planet to at least do a little

417
00:23:08,240 --> 00:23:09,680
bit of machine learning.

418
00:23:09,680 --> 00:23:11,240
I think it's super important.

419
00:23:11,240 --> 00:23:15,160
Uh, there are about 10 million developers on the planet right now.

420
00:23:15,160 --> 00:23:21,240
And not nearly, there's not nearly enough that are even aware of how important machine

421
00:23:21,240 --> 00:23:28,040
learning is, um, architecture engineering is a new, uh, feature engineering.

422
00:23:28,040 --> 00:23:35,720
And if you want to win, if you have a startup, if you, if you have, uh, an idea, if you

423
00:23:35,720 --> 00:23:39,160
want to win, you, at this point, you have to implement some sort of AI because if you

424
00:23:39,160 --> 00:23:42,040
don't, someone else will, right.

425
00:23:42,040 --> 00:23:45,640
So I want to make machine learning as, you know, democratizing, making it as accessible

426
00:23:45,640 --> 00:23:48,640
and understandable as possible to as many people as possible.

427
00:23:48,640 --> 00:23:51,320
So I'm just going to keep going down that path and do whatever it takes to, to make that

428
00:23:51,320 --> 00:23:52,320
happen.

429
00:23:52,320 --> 00:23:54,520
And that's going to be lots and lots of videos in the future.

430
00:23:54,520 --> 00:23:56,080
You've got a new project that you're working on.

431
00:23:56,080 --> 00:23:57,560
Is that something that you can talk about?

432
00:23:57,560 --> 00:23:58,560
That's going to be public as well.

433
00:23:58,560 --> 00:23:59,560
That's going to be public.

434
00:23:59,560 --> 00:24:00,560
Yeah.

435
00:24:00,560 --> 00:24:01,560
And it's not going to be on my channel.

436
00:24:01,560 --> 00:24:02,560
It's going to be on theirs.

437
00:24:02,560 --> 00:24:06,160
But a big ML, like I'm partnered with, I've, I've now, you know, I've signed a deal

438
00:24:06,160 --> 00:24:07,160
with big ML.

439
00:24:07,160 --> 00:24:11,040
So I'm going to be making a video series for them, um, about their product and it's going

440
00:24:11,040 --> 00:24:14,200
to be, it's, it's called cloud machine learning.

441
00:24:14,200 --> 00:24:20,680
And it's using big ML to do a bunch of, uh, pragmatic, real world applications.

442
00:24:20,680 --> 00:24:26,880
So the first one, uh, is going to be, uh, about climate change and how we can use machine

443
00:24:26,880 --> 00:24:29,240
learning to prevent climate change.

444
00:24:29,240 --> 00:24:30,240
Okay.

445
00:24:30,240 --> 00:24:31,280
So I'm super excited about that one.

446
00:24:31,280 --> 00:24:35,640
And then so like, because video content takes up so much of my time, I don't really have

447
00:24:35,640 --> 00:24:39,880
time to do things like client acquisition and, yeah, you know, all this, all this stuff.

448
00:24:39,880 --> 00:24:43,120
So the, the clients that I do have are the people who have come to me and right now have

449
00:24:43,120 --> 00:24:48,560
like seven or eight and they're kind of in a queue, uh, and yeah, I'm just taking on

450
00:24:48,560 --> 00:24:50,520
as much as I can handle at a time.

451
00:24:50,520 --> 00:24:57,080
And as I grow, I'm going to start, start looking at more, you know, ideally, you know, my

452
00:24:57,080 --> 00:25:02,200
goal is to one day partner with deep mind.

453
00:25:02,200 --> 00:25:08,480
I want to make videos for deep mind, um, but they're like, I consider them like the Navy

454
00:25:08,480 --> 00:25:12,680
Seals machine learning, so I've got to get, I've got to get to that level.

455
00:25:12,680 --> 00:25:18,320
You know, the Apollo program for intelligence, uh-huh.

456
00:25:18,320 --> 00:25:22,320
You know, if we solve intelligence, we can apply it to anything like just think of it as

457
00:25:22,320 --> 00:25:26,880
an objective function or X any problem you can ever think of it.

458
00:25:26,880 --> 00:25:33,080
If you have the right learning algorithm and you say solve for X, it could solve it, scientific

459
00:25:33,080 --> 00:25:37,240
research problems or even existential problems, the questions that have plagued us in

460
00:25:37,240 --> 00:25:38,240
stay one.

461
00:25:38,240 --> 00:25:39,240
Who are we?

462
00:25:39,240 --> 00:25:40,240
Why are we here?

463
00:25:40,240 --> 00:25:41,720
What's the point of the universe?

464
00:25:41,720 --> 00:25:46,560
You might not be capable of figuring this stuff out ourselves, but a highly intelligent

465
00:25:46,560 --> 00:25:47,560
AI could.

466
00:25:47,560 --> 00:25:48,560
Mm-hmm.

467
00:25:48,560 --> 00:25:50,960
We might not like the answers.

468
00:25:50,960 --> 00:25:53,600
We might, we might not like the answers.

469
00:25:53,600 --> 00:25:58,200
We might not like the answers, but where, so where do you fall on the whole singularity

470
00:25:58,200 --> 00:25:59,200
thing?

471
00:25:59,200 --> 00:26:00,200
All right.

472
00:26:00,200 --> 00:26:02,280
That's what, wake, wake, that's why I wake up in the morning.

473
00:26:02,280 --> 00:26:07,400
I want to make it a benevolent singularity happen as soon as possible, uh-huh, as soon

474
00:26:07,400 --> 00:26:08,400
as possible.

475
00:26:08,400 --> 00:26:15,120
What do you think about the, uh, the open AI research stuff that they put out a few

476
00:26:15,120 --> 00:26:18,600
weeks ago on Safe Machine Learning, have you been following that stuff?

477
00:26:18,600 --> 00:26:19,600
Mm-hmm.

478
00:26:19,600 --> 00:26:26,360
So, so specifically like, uh, ways to, they published this framework for like four or

479
00:26:26,360 --> 00:26:31,680
five different areas of research that need to be kind of dug into so that we can ensure

480
00:26:31,680 --> 00:26:36,720
the safety and, you know, benevolence as you put it of, of AI like, you know, if we've

481
00:26:36,720 --> 00:26:44,400
got a AI powered robot, um, you know, how do we, how do we ensure that, you know, it

482
00:26:44,400 --> 00:26:50,480
doesn't learn how to game the system and, um, and, you know, for example, if it's being

483
00:26:50,480 --> 00:26:52,200
programmed to clean, right?

484
00:26:52,200 --> 00:26:56,520
How do, how do we know that, how do we program it so that it doesn't sweep stuff onto the

485
00:26:56,520 --> 00:26:57,520
carpet?

486
00:26:57,520 --> 00:26:58,520
Right.

487
00:26:58,520 --> 00:26:59,520
Yeah.

488
00:26:59,520 --> 00:27:03,520
I think, um, yeah, and then Google had like the kill switch paper, which I thought was

489
00:27:03,520 --> 00:27:04,520
super cool.

490
00:27:04,520 --> 00:27:07,400
Uh, I like that open AI is thinking about this.

491
00:27:07,400 --> 00:27:10,960
I love open AI in general, the, the concept behind it.

492
00:27:10,960 --> 00:27:23,040
I think, um, yeah, it's like preventing AI from doing bad things is going to be really

493
00:27:23,040 --> 00:27:24,040
important.

494
00:27:24,040 --> 00:27:26,480
I mean, technology has always been a double-edged sword, you know, with the, starting with

495
00:27:26,480 --> 00:27:27,560
the natural fire.

496
00:27:27,560 --> 00:27:32,560
And I think that, you know, with security, I think that's going to be one of the first,

497
00:27:32,560 --> 00:27:38,640
uh, where we're going to see, uh, we're going to see the power of AI, uh, when it comes

498
00:27:38,640 --> 00:27:45,160
to protecting humans, if you have an AI and you train it to get really good at breaking

499
00:27:45,160 --> 00:27:50,000
into systems, the only thing that's going to be able to stop that is an AI that's good

500
00:27:50,000 --> 00:27:53,720
at detecting an AI that can break into systems.

501
00:27:53,720 --> 00:27:57,360
So, uh, I think it's a great thing what they're doing.

502
00:27:57,360 --> 00:27:58,640
I think it's really important.

503
00:27:58,640 --> 00:28:00,120
I think it's really important.

504
00:28:00,120 --> 00:28:05,000
And what Mary is doing as well, and, uh, you know, the ethics committee that DeepMind

505
00:28:05,000 --> 00:28:11,080
has at Google to prevent, uh, you know, malevolent types of AI, all this stuff is super,

506
00:28:11,080 --> 00:28:12,080
super important.

507
00:28:12,080 --> 00:28:14,440
Mary's the machine intelligence research institute.

508
00:28:14,440 --> 00:28:15,440
Yeah.

509
00:28:15,440 --> 00:28:16,440
Yeah.

510
00:28:16,440 --> 00:28:17,440
Yeah.

511
00:28:17,440 --> 00:28:18,440
Yeah.

512
00:28:18,440 --> 00:28:23,600
And, you know, there's always a question like, can we stop it, um, you know, who knows?

513
00:28:23,600 --> 00:28:25,360
But it's, it's, it's good to try.

514
00:28:25,360 --> 00:28:32,720
And honestly, uh, if a malevolent AI, uh, doesn't kill us, then something else likely

515
00:28:32,720 --> 00:28:33,720
will.

516
00:28:33,720 --> 00:28:39,720
So this is something that, um, just something that's really important.

517
00:28:39,720 --> 00:28:40,720
Hmm.

518
00:28:40,720 --> 00:28:48,720
So what's your maybe taking a step back like for folks that are trying, uh, do you have

519
00:28:48,720 --> 00:28:52,320
a quick like, if someone, you know, a friend comes up to you and says, okay, I really,

520
00:28:52,320 --> 00:28:54,840
you know, I really want to learn this stuff now.

521
00:28:54,840 --> 00:28:56,440
Like what's your curriculum?

522
00:28:56,440 --> 00:29:01,000
What's your, you know, one, two, three, uh, list of stuff to do is it?

523
00:29:01,000 --> 00:29:04,480
Do you think are you trying to build your videos so that someone could just follow those

524
00:29:04,480 --> 00:29:08,840
and get everything that they need or, uh, are there some set of resources that you think

525
00:29:08,840 --> 00:29:10,840
are kind of canonical?

526
00:29:10,840 --> 00:29:11,840
Yeah.

527
00:29:11,840 --> 00:29:20,360
So I think that, um, my videos are good if you know some basic Python.

528
00:29:20,360 --> 00:29:24,800
If you know Python, then my good, my videos are a great starting point.

529
00:29:24,800 --> 00:29:32,200
But I think that my videos alone are not enough, um, you know, it's, it's one of the things

530
00:29:32,200 --> 00:29:33,640
of like combining different data sources.

531
00:29:33,640 --> 00:29:41,440
So I think my videos, in addition to some long form content, I think, uh, so for me, uh,

532
00:29:41,440 --> 00:29:52,000
big ML has some great long form content, um, there's, uh, so, you know, I actually don't

533
00:29:52,000 --> 00:30:00,360
think I think that, uh, there's a deep learning course on Udacity, uh, by a Google engineer

534
00:30:00,360 --> 00:30:03,520
who works at Google Brain, I forgot what it's called, but if you, if you Google just

535
00:30:03,520 --> 00:30:06,840
like Udacity deep learning, uh, that, that course is really good.

536
00:30:06,840 --> 00:30:10,560
That to me is even the TensorFlow course or not the TensorFlow course.

537
00:30:10,560 --> 00:30:12,360
That's a great one.

538
00:30:12,360 --> 00:30:21,960
Um, but there's one specifically on deep learning in general, um, um, there's just

539
00:30:21,960 --> 00:30:22,960
just so much.

540
00:30:22,960 --> 00:30:26,440
I think it's, it's one of those things where it's like, you, okay, so if you're saying like,

541
00:30:26,440 --> 00:30:31,960
I want to learn machine learning, I would say like, okay, first learn Python, um, uh, by

542
00:30:31,960 --> 00:30:37,560
reading the book, uh, learn Python the hard way, um, uh, and then once you, once you,

543
00:30:37,560 --> 00:30:41,800
once you feel like you're comfortable with Python, just start building things, just start

544
00:30:41,800 --> 00:30:45,240
building things and, and my videos are good because it's application specific and I make

545
00:30:45,240 --> 00:30:49,080
it really easy for you to, you know, just, when you hit compile and you see your model

546
00:30:49,080 --> 00:30:55,400
train, train and then you can apply to other things, that is like super useful for, for

547
00:30:55,400 --> 00:30:59,920
your confidence as a machine learner and also just as a developer.

548
00:30:59,920 --> 00:31:06,560
So and also just go to GitHub and search for machine learning projects like search for

549
00:31:06,560 --> 00:31:12,120
like machine learning demo or machine learning simple and just look at those readmeas, download

550
00:31:12,120 --> 00:31:16,640
them, compile them, open it in a text editor and just like go through them one by one and

551
00:31:16,640 --> 00:31:21,760
like really try to understand what's happening, you know, and, and I would say start off

552
00:31:21,760 --> 00:31:26,080
at a high level because, you know, some people would say the other way, like start off at

553
00:31:26,080 --> 00:31:29,800
a low level, like learn exactly how to implement these models from scratch.

554
00:31:29,800 --> 00:31:34,200
No, no, no, I would say start off at a high level and once you get it at a high level,

555
00:31:34,200 --> 00:31:42,760
then you can start like trying to rebuild, you know, uh, you know, you know, you know,

556
00:31:42,760 --> 00:31:43,760
neural net from scratch.

557
00:31:43,760 --> 00:31:48,200
Yeah, like custom and implement from ground up or implement some research or something

558
00:31:48,200 --> 00:31:49,200
like that.

559
00:31:49,200 --> 00:31:50,200
Yeah.

560
00:31:50,200 --> 00:31:51,200
Yeah.

561
00:31:51,200 --> 00:31:52,200
Yeah.

562
00:31:52,200 --> 00:31:53,200
Keras, torch, lots of great libraries these days.

563
00:31:53,200 --> 00:31:54,200
Mm hmm.

564
00:31:54,200 --> 00:31:55,200
Nice.

565
00:31:55,200 --> 00:32:00,200
Um, what, uh, Quora is well, sorry, Quora is awesome.

566
00:32:00,200 --> 00:32:05,120
I've learned so much from Quora, just like, you know, cause I'll find one question on

567
00:32:05,120 --> 00:32:08,880
Quora on deep learning and on the side bar, it's like, oh my god, all these questions

568
00:32:08,880 --> 00:32:10,360
are amazing.

569
00:32:10,360 --> 00:32:14,240
And then you have people like Jan LeCoon answering them and like Monica Anderson and like,

570
00:32:14,240 --> 00:32:16,040
all these like really famous research scientists.

571
00:32:16,040 --> 00:32:17,040
Yeah.

572
00:32:17,040 --> 00:32:18,040
So I've learned a lot from them.

573
00:32:18,040 --> 00:32:22,200
Are there people that you, is it primarily like search, uh, based that where you find

574
00:32:22,200 --> 00:32:28,160
stuff or are you following particular people and just kind of keeping up with them there?

575
00:32:28,160 --> 00:32:29,160
It's search based.

576
00:32:29,160 --> 00:32:30,160
Search based.

577
00:32:30,160 --> 00:32:31,160
Yeah, search based.

578
00:32:31,160 --> 00:32:32,160
How about on Twitter?

579
00:32:32,160 --> 00:32:35,320
Are there, you mentioned Yana, there are other folks that you, uh, yeah, find our good

580
00:32:35,320 --> 00:32:39,160
signal to noise, uh, machine learning folks on Twitter?

581
00:32:39,160 --> 00:32:40,160
For sure.

582
00:32:40,160 --> 00:32:46,640
I think, uh, for me, uh, I think Chris Dixon, he's a partner in Andreessen Horowitz.

583
00:32:46,640 --> 00:32:50,880
He, he's, he's good for like knowing, uh, you know, what's up and coming and machine

584
00:32:50,880 --> 00:32:51,880
learning.

585
00:32:51,880 --> 00:32:53,680
I think he has a good eye for that.

586
00:32:53,680 --> 00:32:58,760
One person in general that I really respect about technology is Bology Srinivasan, who's

587
00:32:58,760 --> 00:33:01,120
also a partner in Andreessen Horowitz.

588
00:33:01,120 --> 00:33:05,320
That guy knows he lives in the future.

589
00:33:05,320 --> 00:33:10,120
And, um, yeah, board, Jan LaCoon is also like a great Twitter handle.

590
00:33:10,120 --> 00:33:14,520
I don't think I've come across that one yet, but it sounds funny.

591
00:33:14,520 --> 00:33:15,520
I really like it.

592
00:33:15,520 --> 00:33:16,520
Yeah.

593
00:33:16,520 --> 00:33:17,520
Yeah.

594
00:33:17,520 --> 00:33:18,520
Nice.

595
00:33:18,520 --> 00:33:19,520
And, yeah.

596
00:33:19,520 --> 00:33:24,080
And then, and, and, and following these big companies like Amazon and Google is really

597
00:33:24,080 --> 00:33:28,000
important because you, you can see like, oh, they just released, you know, DSST&E,

598
00:33:28,000 --> 00:33:33,360
their new machine learning library, which needed to be renamed, but, yeah, destiny.

599
00:33:33,360 --> 00:33:34,360
It's great.

600
00:33:34,360 --> 00:33:43,920
I don't think so, I'm not a fan, but, I mean, in general, I think machine learning needs

601
00:33:43,920 --> 00:33:55,800
better marketing, like a lot, a lot, you know, not, I'm not going to diss anybody, so nice.

602
00:33:55,800 --> 00:34:02,800
So for folks that aren't familiar with your, your videos, are there, you know, two or

603
00:34:02,800 --> 00:34:10,600
three that like, oh, man, these were my favorite or these were my best are, yeah, um, I think

604
00:34:10,600 --> 00:34:15,160
so the, the one that ended up being most popular was AI composer.

605
00:34:15,160 --> 00:34:18,320
That was the second video I made for machine learning for hackers.

606
00:34:18,320 --> 00:34:21,920
So AI composer, so the top three would be like AI composer.

607
00:34:21,920 --> 00:34:26,200
The one I'm most proud of is generative adversarial networks because it was the hardest.

608
00:34:26,200 --> 00:34:35,120
And the one that I thought was the dopest was a build an AI artist because I just thought

609
00:34:35,120 --> 00:34:42,080
that application was really cool, like applying some style to some novel, you know, picture.

610
00:34:42,080 --> 00:34:44,600
This like the thing that Prism is doing now.

611
00:34:44,600 --> 00:34:45,600
That Prism is doing.

612
00:34:45,600 --> 00:34:46,600
Yeah.

613
00:34:46,600 --> 00:34:47,600
Nice.

614
00:34:47,600 --> 00:34:50,480
And so what is the, what's composer?

615
00:34:50,480 --> 00:34:56,040
Composer is generating machine, like machine generated music.

616
00:34:56,040 --> 00:35:01,400
Do you feed it to music, like a data set of like, you know, 500 songs, it'll learn the

617
00:35:01,400 --> 00:35:04,960
style of that song and then it can generate new music in that same style.

618
00:35:04,960 --> 00:35:05,960
Okay.

619
00:35:05,960 --> 00:35:09,720
And I trained it in the video over British folk music, but you could apply anything to

620
00:35:09,720 --> 00:35:10,720
it.

621
00:35:10,720 --> 00:35:15,440
One idea I thought would be really cool that someone should do is take Hans Zimmer music

622
00:35:15,440 --> 00:35:18,040
and generate music in the style of Hans Zimmer.

623
00:35:18,040 --> 00:35:19,040
So Prism for music.

624
00:35:19,040 --> 00:35:23,840
And that's kind of what the magenta magenta is trying, well, I'm, they're not trying

625
00:35:23,840 --> 00:35:29,880
to do specifically that, but did you use magenta, any of their code in your, in this project?

626
00:35:29,880 --> 00:35:30,880
I didn't.

627
00:35:30,880 --> 00:35:31,880
No, no, no, no.

628
00:35:31,880 --> 00:35:41,800
This was for that, it was not that, yeah, it was, I found it on GitHub and I modified it.

629
00:35:41,800 --> 00:35:42,800
Okay.

630
00:35:42,800 --> 00:35:43,800
Yeah.

631
00:35:43,800 --> 00:35:44,800
Nice.

632
00:35:44,800 --> 00:35:45,800
Interesting.

633
00:35:45,800 --> 00:35:46,840
What was I going to ask you?

634
00:35:46,840 --> 00:35:52,720
Oh, you've done a, you've done a couple of videos on chatbots and chatbot platforms.

635
00:35:52,720 --> 00:35:54,600
That was a good one.

636
00:35:54,600 --> 00:35:58,880
What do you think about that space and like, what would you learn and, you know, over a few

637
00:35:58,880 --> 00:36:02,160
attempts at playing around with that stuff?

638
00:36:02,160 --> 00:36:03,160
Yeah.

639
00:36:03,160 --> 00:36:12,200
I think, you know, I, with the marketing effort, I expected, I expected with AI, like Facebook's

640
00:36:12,200 --> 00:36:18,200
acquisition, that, that chatbot building technology to be way better than it was.

641
00:36:18,200 --> 00:36:23,640
But what ended up happening is I found that API.ai had a much better, it was much easier

642
00:36:23,640 --> 00:36:25,680
for me to build a chatbot with API.ai.

643
00:36:25,680 --> 00:36:26,680
Yeah.

644
00:36:26,680 --> 00:36:28,880
I think that chatbots in general are going to get really popular and we're going to replace

645
00:36:28,880 --> 00:36:30,440
all of our apps with chatbots.

646
00:36:30,440 --> 00:36:32,200
This is already happening in Asia.

647
00:36:32,200 --> 00:36:37,280
So like with WeChat, like, a lot of people don't even use apps anymore, you know, in China

648
00:36:37,280 --> 00:36:42,880
and stuff because it's so easy to just say like, you know, you can even combine different

649
00:36:42,880 --> 00:36:49,920
apps together, like book me an Uber in 30 minutes at this location and take me to my favorite

650
00:36:49,920 --> 00:36:50,920
restaurant.

651
00:36:50,920 --> 00:36:51,920
And that's querying.

652
00:36:51,920 --> 00:36:57,440
Like, Yelp, your Google or whatever, you know, your preferences locally and the Uber

653
00:36:57,440 --> 00:37:01,480
app or if it was in China, quality.

654
00:37:01,480 --> 00:37:03,400
So there's a lot of potential for chatbots.

655
00:37:03,400 --> 00:37:09,760
And if you are like right now thinking about, you know, building a startup, like if it

656
00:37:09,760 --> 00:37:14,000
was me, if it was me, I would be doing some kind of chatbot, chatbot for X where there

657
00:37:14,000 --> 00:37:18,400
is no chatbot because this is just going to get more and more popular.

658
00:37:18,400 --> 00:37:25,080
Like I already used chatbots in Messenger for like, you know, like detecting like scores

659
00:37:25,080 --> 00:37:27,120
and stuff like that.

660
00:37:27,120 --> 00:37:29,360
Scores of like sports and stuff.

661
00:37:29,360 --> 00:37:30,360
Yeah.

662
00:37:30,360 --> 00:37:31,360
Okay.

663
00:37:31,360 --> 00:37:33,800
No, I mean, not that I watch sports, but like I just play around with them.

664
00:37:33,800 --> 00:37:34,800
Yeah.

665
00:37:34,800 --> 00:37:35,800
Yeah.

666
00:37:35,800 --> 00:37:36,800
Yeah.

667
00:37:36,800 --> 00:37:41,240
But I haven't found anything that's particularly useful like the thing that I haven't played

668
00:37:41,240 --> 00:37:47,600
around with a bunch of them, but you know, they're all kind of, uh, yeah, which is the

669
00:37:47,600 --> 00:37:48,800
doesn't feel like we're there yet.

670
00:37:48,800 --> 00:37:49,800
Yeah, we're not there yet.

671
00:37:49,800 --> 00:37:50,600
We're not there yet.

672
00:37:50,600 --> 00:37:54,160
But we will be in like a year.

673
00:37:54,160 --> 00:37:55,320
That's how fast the space is moving.

674
00:37:55,320 --> 00:37:56,320
Yeah.

675
00:37:56,320 --> 00:37:57,320
Yeah.

676
00:37:57,320 --> 00:37:58,320
It's just going to it's just yeah.

677
00:37:58,320 --> 00:38:02,080
Right now there's a lot of people who are like really need deep in this stuff and

678
00:38:02,080 --> 00:38:04,880
they're building, but we're going to see a lot of releases.

679
00:38:04,880 --> 00:38:08,840
And like, you know, because the bigger players haven't caught on yet, that, you know, that's

680
00:38:08,840 --> 00:38:09,840
one of the reasons.

681
00:38:09,840 --> 00:38:15,080
But deaf, I promise you Uber has a team dedicated to this Airbnb has a team dedicated, right?

682
00:38:15,080 --> 00:38:16,320
Absolutely.

683
00:38:16,320 --> 00:38:19,880
And then Facebook's releasing M, which they're training right now full time with like humans

684
00:38:19,880 --> 00:38:22,920
and machines and just getting better and better and people internally at Facebook are

685
00:38:22,920 --> 00:38:23,920
using this.

686
00:38:23,920 --> 00:38:26,320
And I talked to, you know, some of these people and they really like it.

687
00:38:26,320 --> 00:38:30,360
Um, and I was like, please give me an invite to M like, wouldn't you like to have an invite?

688
00:38:30,360 --> 00:38:32,200
I'm like, oh my God.

689
00:38:32,200 --> 00:38:35,720
If anyone at Facebook is listening, we both want invites to ask.

690
00:38:35,720 --> 00:38:36,720
Yes.

691
00:38:36,720 --> 00:38:37,720
Please.

692
00:38:37,720 --> 00:38:38,720
Please.

693
00:38:38,720 --> 00:38:39,720
Nice.

694
00:38:39,720 --> 00:38:40,720
Nice.

695
00:38:40,720 --> 00:38:41,720
Well, that's been great.

696
00:38:41,720 --> 00:38:42,720
It's been great.

697
00:38:42,720 --> 00:38:43,720
Channel with you.

698
00:38:43,720 --> 00:38:48,440
Anything that you'd want to leave folks with or point them to or, you know, help them

699
00:38:48,440 --> 00:38:49,440
to check out.

700
00:38:49,440 --> 00:38:50,440
Yeah.

701
00:38:50,440 --> 00:38:56,200
Um, yeah, I would say definitely subscribe to my channel because I'm just getting started.

702
00:38:56,200 --> 00:38:59,600
And that's where I'm putting all of my effort into right now.

703
00:38:59,600 --> 00:39:08,640
And, uh, what else, I would say, if you're a unicorn video producer, if you're a

704
00:39:08,640 --> 00:39:12,320
universe machine learning, yeah, if you're, if you're a video editor who happens to know

705
00:39:12,320 --> 00:39:17,240
how to program as well, definitely, uh, you know, message me on Twitter, uh, because

706
00:39:17,240 --> 00:39:20,200
I'm looking for you because I need you.

707
00:39:20,200 --> 00:39:28,040
And, uh, yeah, just don't, don't give up if, if, you know, machine learning is, you

708
00:39:28,040 --> 00:39:33,440
know, it's, it's kind of hard, but it's a worthwhile endeavor and you can make a lot of money

709
00:39:33,440 --> 00:39:34,680
for it from it.

710
00:39:34,680 --> 00:39:41,280
And you can learn a lot and it's going to, and if you, if you get good at learning about

711
00:39:41,280 --> 00:39:45,800
machine learning, which is one of the, it can be one of the hardest things on the planet

712
00:39:45,800 --> 00:39:50,720
to learn, like solving intelligence, like the human brain, like how do we work is equivalent

713
00:39:50,720 --> 00:39:52,640
to asking like, what is the universe?

714
00:39:52,640 --> 00:39:56,000
If you can get good at that, it's just going to train your brain to be good at so many

715
00:39:56,000 --> 00:39:57,320
different things.

716
00:39:57,320 --> 00:39:59,320
So, yeah, don't give up.

717
00:39:59,320 --> 00:40:00,320
Awesome.

718
00:40:00,320 --> 00:40:01,320
Awesome.

719
00:40:01,320 --> 00:40:02,320
Well, thanks.

720
00:40:02,320 --> 00:40:03,320
Yeah.

721
00:40:03,320 --> 00:40:04,320
Thanks.

722
00:40:04,320 --> 00:40:13,240
All right, everyone, that's it for today's interview.

723
00:40:13,240 --> 00:40:18,200
Before we go, a reminder that this week in machine learning and AI and O'Reilly have

724
00:40:18,200 --> 00:40:24,280
partnered to offer one lucky listener a free pass to the inaugural O'Reilly AI conference,

725
00:40:24,280 --> 00:40:27,720
which will be held at the end of September in New York City.

726
00:40:27,720 --> 00:40:34,120
You can enter via Twitter or the twimmalei.com website by doing one of the following three things.

727
00:40:34,120 --> 00:40:36,640
The preferred way of entering is via Twitter.

728
00:40:36,640 --> 00:40:43,960
Just follow at twimmalei, T-W-I-M-L-A-I and retweet the contest tweet that I'll pin to the

729
00:40:43,960 --> 00:40:46,080
account and post in the show notes.

730
00:40:46,080 --> 00:40:48,760
Do those two things and you'll be entered.

731
00:40:48,760 --> 00:40:53,920
If you're not on Twitter, you can sign up for my newsletter at twimmalei.com slash newsletter

732
00:40:53,920 --> 00:40:58,240
and add a note, please enter me in the additional comments field.

733
00:40:58,240 --> 00:41:03,880
Finally, if you're not on Twitter and you aren't interested in the newsletter, no problem.

734
00:41:03,880 --> 00:41:09,440
Just go to the contact form on twimmalei.com and send me a message with that form using

735
00:41:09,440 --> 00:41:12,560
AI contest as the subject.

736
00:41:12,560 --> 00:41:16,080
The drawing will be open to entries through September 1st and I'll announce the winner

737
00:41:16,080 --> 00:41:18,280
on the September 2nd show.

738
00:41:18,280 --> 00:41:20,760
Good luck and hope to see you in New York.

739
00:41:20,760 --> 00:41:27,760
Thanks again for listening.

