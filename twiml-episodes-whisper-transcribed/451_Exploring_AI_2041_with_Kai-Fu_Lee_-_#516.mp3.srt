1
00:00:00,000 --> 00:00:18,640
All right, everyone. I am here with Kaifu Lee. Kaifu is chairman and CEO of Innovation Ventures,

2
00:00:18,640 --> 00:00:25,200
the former president of Google China, an author of the New York Times bestseller AI Superpowers.

3
00:00:25,200 --> 00:00:30,640
And we're here to talk about his new book, which will be released next week, AI2041.

4
00:00:31,200 --> 00:00:38,400
Kaifu, welcome to the Twomo AI podcast. Thank you. Thanks, Sam. It is great to have an

5
00:00:38,400 --> 00:00:44,320
opportunity to speak with you. I'm looking forward to digging in and talking more about the book.

6
00:00:44,960 --> 00:00:49,040
Before we do, though, I'd love to have you share a little bit about your background and

7
00:00:50,080 --> 00:00:53,920
how you came to work in the field of AI. Sure.

8
00:00:53,920 --> 00:01:02,480
I started with my excitement in AI back in 1979 when I started my undergraduate at Columbia,

9
00:01:03,120 --> 00:01:09,760
and I worked on natural language and vision at Columbia, and then I went to Carnegie Mellon for

10
00:01:09,760 --> 00:01:17,200
my PhD, at which I developed the first speaker independent speech recognition system,

11
00:01:17,200 --> 00:01:25,840
based on machine learning, actually, one of the earlier thesis in machine learning in 1988.

12
00:01:26,640 --> 00:01:33,520
I also developed a computer program that beat the world's off-the-all champion. It's all in the 80s,

13
00:01:33,520 --> 00:01:41,680
very early years. After my graduation from CMU, I taught there for two years, then I joined Apple,

14
00:01:41,680 --> 00:01:49,760
and led a lot of apples, AI, speech, natural language, and multimedia efforts. Later, I joined SGI,

15
00:01:50,560 --> 00:01:57,440
and then Microsoft, where I started the Microsoft Research Asia in Beijing in 1998,

16
00:01:57,440 --> 00:02:05,040
which kind of became one of the best AI research labs in Asia. Later, I joined Google,

17
00:02:05,040 --> 00:02:13,040
and ran Google China for four years between 2005 and 2009. We did do a little bit for Hanei,

18
00:02:13,040 --> 00:02:20,480
but mostly it was really developing Google's presence in China. In 2009, I left Google and started

19
00:02:20,480 --> 00:02:28,080
my venture capital firm, Sinovation Ventures. At Sinovation Ventures, we invested in about 40 AI

20
00:02:28,080 --> 00:02:36,480
companies. We were about the earliest and probably invested in the most companies. We invested in

21
00:02:36,480 --> 00:02:44,240
about seven unicorns in AI alone, and with a few more yet to come. So, very excited to be in the

22
00:02:44,240 --> 00:02:51,920
era of AI. It was not so hot during much of my career, but I'm glad to be able to catch the

23
00:02:51,920 --> 00:03:00,000
recent wave and participate in it. Fantastic. Fantastic. So, let's maybe jump into the book.

24
00:03:00,000 --> 00:03:07,200
The title is AI2041. If you just read that, having heard nothing of the book, you might think that

25
00:03:07,200 --> 00:03:15,600
it's kind of a straight-up, you know, your vision for AI in 2041. But, and to some degree,

26
00:03:15,600 --> 00:03:21,200
that is the case, you're asking interesting questions on that time horizon, but there's a little

27
00:03:21,200 --> 00:03:27,440
bit of a twist. Tell us about that twist and the way the book is, you know, organized.

28
00:03:29,040 --> 00:03:37,040
Sure. The twist is, we, I call this book, Scientific Fiction, because I collaborated with a

29
00:03:37,040 --> 00:03:43,760
science fiction writer who wrote most of the book, probably three quarters. And they are 10 stories,

30
00:03:43,760 --> 00:03:51,040
we call it, 10 visions of the future. I find that the impact of AI is misunderstood by a lot

31
00:03:51,040 --> 00:03:57,920
of people. Some are too conservative, others are too optimistic, and others are just naive,

32
00:03:57,920 --> 00:04:04,720
and some explanation, I think, could be helpful. AI will change our future, and more people need

33
00:04:04,720 --> 00:04:11,680
to understand it. And having a fictional writer, right, in terms of stories, will make it all

34
00:04:11,680 --> 00:04:18,080
more accessible to people. So, the book is organized in 10 stories, of which takes a place in a

35
00:04:18,080 --> 00:04:24,640
different country, and in a different industry. So, we can see how AI will impact all countries

36
00:04:24,640 --> 00:04:31,760
and all industries. And then, after each story, I write an analysis of the technologies embedded

37
00:04:31,760 --> 00:04:40,000
in the chapter, how they will progress, and what challenges they may bring, and how we might

38
00:04:40,000 --> 00:04:46,720
solve them, or what we should do now, to deal with the externalities or potential challenges that

39
00:04:46,720 --> 00:04:55,840
they bring about. So, it's 10 stories going from relatively simple uses of AI to extremely

40
00:04:57,840 --> 00:05:03,920
challenging and somewhat futuristic uses of AI, but in the whole set of 10 stories, I try to

41
00:05:04,960 --> 00:05:12,880
at least have a high degree of confidence, like 80% or more, that this would work in the 10 to 20

42
00:05:12,880 --> 00:05:20,240
year timeframe. You mentioned that when you talk to people, you get a range of

43
00:05:22,560 --> 00:05:28,320
reactions or perspectives on AI ranging from very conservative to over optimistic. A lot of that

44
00:05:28,320 --> 00:05:36,320
has to do with the time horizon that you're thinking about. You chose 20 years as the kind of central

45
00:05:36,320 --> 00:05:43,760
time horizon for this book. Why is that? Because on the one hand, 20 years is pretty long. A lot

46
00:05:43,760 --> 00:05:50,720
can happen in 20 years. 20 years ago, we didn't have the iPhone or the mobile internet, and look how

47
00:05:50,720 --> 00:05:59,920
things have changed. So, imagine 20 years ago, if someone were to write AI 2021, it would be pretty

48
00:05:59,920 --> 00:06:07,040
interesting and fantastic 20 years ago if it accurately described today. So, it's long enough,

49
00:06:07,040 --> 00:06:14,080
futuristic enough, exciting enough, but not so long that we could hand wave and say, you know,

50
00:06:14,800 --> 00:06:22,240
bring download as possible and we become cyborgs or we're doing teleportation or time travel,

51
00:06:22,240 --> 00:06:29,040
so we stay away from that. And also, I factor in that the time it takes to develop the research

52
00:06:29,040 --> 00:06:36,560
to perfect it, to reduce the cost, to implement it, to productize it, to make it acceptable to the

53
00:06:36,560 --> 00:06:48,800
market, and also to deal with potential legal regulatory and accountability issues. So, it's not,

54
00:06:48,800 --> 00:06:53,920
so some of the stories may look like, hey, we could almost do that today, but there are a lot of

55
00:06:53,920 --> 00:07:00,560
other issues that we're coming to play. Sure, sure. I think one of the things that challenges

56
00:07:01,200 --> 00:07:11,840
folks the most on this, you know, conservative optimistic from a mass perspective is autonomous

57
00:07:11,840 --> 00:07:18,480
driving. Do you have a story in the book that talks about autonomous driving? Yes, yes, of course,

58
00:07:18,480 --> 00:07:24,240
can't write the book in 20 years without it. Of course, by then, L5 will have worked.

59
00:07:25,840 --> 00:07:33,520
It's kind of in transition, so I think that describes my view is that L5 is quite challenging,

60
00:07:34,240 --> 00:07:42,320
and the path towards L5, as I describe in the story, will be incremental. As we know,

61
00:07:42,320 --> 00:07:49,680
AI gets better with more deployment, with more data, with more learning, so basically L5 will be a

62
00:07:49,680 --> 00:07:58,480
series of increasingly more challenging environments, perhaps starting with fixed routes, like buses,

63
00:07:58,480 --> 00:08:04,720
and then trucks on highways, and then more and more cities, and that's kind of one path

64
00:08:04,720 --> 00:08:13,520
as more data experience is gathered. It will face still a lot of challenges, even in 20 years,

65
00:08:13,520 --> 00:08:21,200
and one of the predictions I make is that cities will have to modify some existing road infrastructure,

66
00:08:21,840 --> 00:08:30,480
for example, to separate very dangerous cross sections with pedestrians and cars, so that there's

67
00:08:30,480 --> 00:08:37,120
no risk of a car hitting a pedestrian in the most likely environments and crossroads in downtowns,

68
00:08:37,680 --> 00:08:45,600
such as happened with the Uber Autonomous Vehicle in Phoenix, and also roads can be smart and

69
00:08:45,600 --> 00:08:52,560
essentially work symbiotically with autonomous vehicles. Also, I predict that there will still be

70
00:08:52,560 --> 00:08:59,680
environments in which AI will be lost and need a backup driver. Yet, we will need cars that

71
00:08:59,680 --> 00:09:05,600
have no steering wheel, and really no place for a driver, so they can be smaller and

72
00:09:05,600 --> 00:09:14,480
simpler, talk to each other, and even avoiding accidents as they communicate their location and speed,

73
00:09:14,480 --> 00:09:21,360
and if you have a blown tire, you will tell cars around you, so I envision all of these will happen,

74
00:09:21,360 --> 00:09:26,720
but one part of it that my partner Stanley who wrote the science fiction stories thought was

75
00:09:26,720 --> 00:09:34,400
interesting was what would be the life of a backup driver in that case, because if the car got

76
00:09:34,400 --> 00:09:40,640
into a natural disaster where the roads have disappeared, and you have to fall back on natural

77
00:09:40,640 --> 00:09:46,640
instincts of a human driver to survive and navigate how they incredibly dangerous zone,

78
00:09:47,920 --> 00:09:51,760
and obviously the passenger couldn't do it, there's no longer a steering wheel,

79
00:09:51,760 --> 00:09:58,720
so the solution would have to be a remote center where super drivers jump in from one disaster to

80
00:09:58,720 --> 00:10:05,600
another saving people's lives, and then the other interesting dramatic element is well what happens

81
00:10:05,600 --> 00:10:11,840
to the life of such a backup driver, would it create so much stress that they can't live with

82
00:10:11,840 --> 00:10:18,800
themselves because they will be watching people die from day to day, and so how would such an

83
00:10:18,800 --> 00:10:23,600
arrangement be made, so without giving away the story that's kind of the dramatic element and

84
00:10:23,600 --> 00:10:31,600
a technical element weaved into a story. That last note about the drivers and their welfare and

85
00:10:31,600 --> 00:10:38,000
their life, even though we're talking about a scenario 20 years from now that calls to mind

86
00:10:38,640 --> 00:10:44,960
the lives of folks that are working in like content moderation farms and centers today that are

87
00:10:44,960 --> 00:10:54,080
dealing with those kinds of issues, so I imagine that part of what you're trying to do is to

88
00:10:54,080 --> 00:10:59,600
point to future issues but also tie them to contemporary issues as well.

89
00:11:02,720 --> 00:11:08,480
And also there are other interesting dramatic elements, for example, how do you recruit such

90
00:11:08,480 --> 00:11:18,240
a amazing drivers, so part of the story is games are developed and then winners of these games

91
00:11:18,240 --> 00:11:24,640
sometimes teenagers would be approached to see if they would be a backup driver, but of course

92
00:11:24,640 --> 00:11:32,960
that's too much psychological burden for a young teenager to be put into the position of

93
00:11:32,960 --> 00:11:44,320
helping and saving lives, so is it morally a problem to package a real job saving people's lives

94
00:11:44,320 --> 00:11:49,760
as a game and not disclose it to the teenager who happens to be the best backup driver that

95
00:11:49,760 --> 00:11:57,360
one can find, so we're saving lives to the purpose but can you lie to a teenager who's known

96
00:11:57,360 --> 00:12:03,680
to be saving lives but also not fair to put a psychological burden for them to know they're

97
00:12:03,680 --> 00:12:10,880
saving and not saving lives every day and do you tell them or don't you tell them it's a moral dilemma.

98
00:12:12,720 --> 00:12:17,680
And do you answer those questions or do you just raise them?

99
00:12:18,960 --> 00:12:25,600
We just raise them but I think the endings of the stories would give away how we feel

100
00:12:25,600 --> 00:12:32,400
but we don't want to but I think it's an issue where reasonable people can and will disagree

101
00:12:32,400 --> 00:12:37,440
so we don't presume that we know the right answer but I think we need to be aware such challenges

102
00:12:37,440 --> 00:12:44,960
will come up and the book is probably for the people watching this podcast the book is less

103
00:12:44,960 --> 00:12:49,760
about learning about technologies because you probably know most of what I have to say

104
00:12:49,760 --> 00:12:56,320
but but thinking ahead about the externalities and implications that are up ahead and what we

105
00:12:56,320 --> 00:13:02,320
technologists can possibly do about it to educate people and also to develop solutions.

106
00:13:05,520 --> 00:13:13,520
One more question on the autonomous driving scenario you mentioned that you fully expect and

107
00:13:13,520 --> 00:13:23,680
that the story presumes level 5 autonomy is in existence in 20 years does the the book or your

108
00:13:23,680 --> 00:13:35,040
analysis project a degree of deployment or the degree to which it is in use at that time?

109
00:13:35,040 --> 00:13:43,920
Yes yes I think the presumption is that in developed countries it's already popularly in use

110
00:13:44,480 --> 00:13:52,080
and in countries that are proactively changing its transportation ecosystem it gets deployed

111
00:13:52,080 --> 00:13:58,240
earlier and that's part of the technological prediction and hypothesis it's also predicting

112
00:13:58,240 --> 00:14:04,800
that developing countries and underdeveloped countries would need the help of developed countries

113
00:14:05,440 --> 00:14:12,960
to put this technology into place so this story takes place in Sri Lanka and in the story Sri

114
00:14:12,960 --> 00:14:20,320
Lanka gets help from a Chinese technological company that is building a business out of helping

115
00:14:20,320 --> 00:14:28,800
developed countries move from not having autonomous to autonomous and I think part of the

116
00:14:28,800 --> 00:14:35,600
implication is that large countries will continue to have more advanced AI to put into other

117
00:14:35,600 --> 00:14:43,040
countries and another assumption is the world will move towards autonomy one tier at a time

118
00:14:43,040 --> 00:14:51,840
and that I feel first tier of countries may have it in the 15 year time frame with other countries

119
00:14:51,840 --> 00:14:58,080
coming later 20 years plus and Sri Lanka was chosen at the place because not all the roads are

120
00:14:58,080 --> 00:15:05,760
yet quite ready for autonomy with some very backward environments because it would not be

121
00:15:05,760 --> 00:15:11,760
reasonable to put that scenario in US or China where by then I think the infrastructure as well

122
00:15:11,760 --> 00:15:18,800
as the technology would perhaps have a very more minimal use of backup drivers by 20 years

123
00:15:22,560 --> 00:15:30,640
you know it's hard to talk about AI in the future without raising the question of jobs and job

124
00:15:30,640 --> 00:15:37,760
displacement it's probably one of the you know issue of one of the issues he was around which

125
00:15:37,760 --> 00:15:46,960
there's the most concern when talking about the future use of AI do you take that up in you know

126
00:15:46,960 --> 00:15:52,320
one particular story in the book or is this something that cuts across various stories

127
00:15:54,080 --> 00:16:01,200
it cuts across all the stories there are probably three stories in which this covers

128
00:16:01,200 --> 00:16:09,600
the one that's squarely on the topic is a story called the job savior and it's a story that

129
00:16:09,600 --> 00:16:20,320
takes place in the US by watching phases and phases of routine jobs being taken over by AI a new

130
00:16:20,320 --> 00:16:28,960
profession arises called a job reallocator and it's a company that would be funded either out

131
00:16:28,960 --> 00:16:36,080
of government funding instead of paying social security or universal basic income the company

132
00:16:36,080 --> 00:16:43,280
would take out take the funding and basically solve the problem of retraining and redeploying

133
00:16:43,280 --> 00:16:50,640
workers whose jobs are being displaced by AI and this company faces some significant challenges

134
00:16:50,640 --> 00:16:57,120
one is that AI is improving capability so more and more routine jobs are being lost people are

135
00:16:57,120 --> 00:17:02,960
being retrained the three years later losing job again another major challenge that it faces

136
00:17:03,760 --> 00:17:11,200
is that many entry jobs are being hollowed out because AI can do jobs of an entry level accountant

137
00:17:11,200 --> 00:17:17,440
entry level architect entry level reporter but how do you advance people's careers and maintain

138
00:17:17,440 --> 00:17:25,840
their motivation to grow and learn without having entry level jobs so it brings up the possibility

139
00:17:25,840 --> 00:17:33,280
of creating a virtual job in which the person thinks he or she is working but perhaps is only

140
00:17:33,280 --> 00:17:41,040
training is more like a training wheels not creating value to the economy but the training

141
00:17:41,040 --> 00:17:46,800
will will help point out people's individual talents so they can be read that redirected later

142
00:17:46,800 --> 00:17:55,440
so again a potentially a moral question of is it okay to let people work jobs which are not

143
00:17:55,440 --> 00:18:01,840
real or maybe are real but could be done by AI and then how would the job reallocator deal with

144
00:18:01,840 --> 00:18:09,680
these challenges so that's the more direct one there is another one on education is how would AI

145
00:18:09,680 --> 00:18:18,960
be evolved so that it helps young people of home their soft skills skills like communication

146
00:18:18,960 --> 00:18:26,080
teamwork and human to human interaction as well as train their creativity and critical thinking

147
00:18:26,080 --> 00:18:32,160
which become all the more important because those are the skills AI cannot replace and also find

148
00:18:32,160 --> 00:18:39,200
the voice of each individual person so that's kind of related to routine jobs being displaced

149
00:18:39,200 --> 00:18:45,520
so people either have to find something AI cannot do or do things that only humans can do or find

150
00:18:45,520 --> 00:18:51,840
something the individual is good at so that's another story a third story has to do with human

151
00:18:51,840 --> 00:18:59,280
motivation it's it's more of a utopian outcome where so much money is being generated in an era

152
00:18:59,280 --> 00:19:07,760
of plenitude where not only does AI do our much of the routine work for us but also the energy

153
00:19:07,760 --> 00:19:13,440
costs have come down with green energy new materials the cost of goods are reduced and the

154
00:19:13,440 --> 00:19:21,520
meaning of money needs to evolve so it asks the question the work isn't the work and money isn't

155
00:19:21,520 --> 00:19:32,000
just a something to keep us busy but it's kind of people's motivation and and reason for living

156
00:19:32,000 --> 00:19:40,000
so if jobs are largely gone routine jobs are largely gone how do people remain motivated so I think

157
00:19:40,000 --> 00:19:47,600
it pushes the question into why do we live in this world perhaps it isn't just for work and pay

158
00:19:47,600 --> 00:19:58,320
but also for for self-actualization finding what our lives are about and can those be measured

159
00:19:58,320 --> 00:20:06,000
somehow can AI somehow measure whether we are improving ourselves where we're happily growing

160
00:20:06,000 --> 00:20:12,080
where we're finding where we're creating more positive energy because as you move up the

161
00:20:12,080 --> 00:20:21,200
mass law hierarchy it's not just about subsistence and not just about money for security but also

162
00:20:21,200 --> 00:20:29,440
about love and empathy and companionship so can those things be measured and can people have

163
00:20:29,440 --> 00:20:37,760
some kind of metric to improve a different metric than money and a different metric by job by

164
00:20:37,760 --> 00:20:47,120
spending their time perhaps in sustainability volunteer jobs companionship as well as all the

165
00:20:47,120 --> 00:20:55,120
creative professional jobs and it's an exploration of how that could develop in in the market in the

166
00:20:55,120 --> 00:21:03,440
country in this case Australia which is doing a pretty good job in in energy efficient energy

167
00:21:03,440 --> 00:21:11,280
that it might create enough of a small enough population to create and a lot of natural resources

168
00:21:11,280 --> 00:21:18,240
to create a the first science the first economy of where universal basic income

169
00:21:18,240 --> 00:21:24,800
plenitude and the moving people the higher purpose might be explored as an experiment kind of the

170
00:21:24,800 --> 00:21:31,520
gamification of life purpose in a sense that's right I mean our life is a gamification now I mean

171
00:21:31,520 --> 00:21:41,360
money is a virtual it's a silly virtual tool that keeps us you know in the rat race when it's

172
00:21:41,360 --> 00:21:48,720
it's you know it's a fabricated human story and we're playing a big game now in chasing fame

173
00:21:48,720 --> 00:21:56,080
and wealth so I think we need to find another which is perhaps more motivating it's interesting

174
00:21:56,080 --> 00:22:03,920
you're hearing you talk about these these stories and reflecting on the other ones the book kind of

175
00:22:03,920 --> 00:22:12,080
walks this line between you know presenting these potentially dystopic scenarios kind of like

176
00:22:12,080 --> 00:22:20,880
black mirror-esque but you know trying to pull out I think trying to pull out an optimistic note

177
00:22:21,680 --> 00:22:28,960
and for the most part you know to talk more about your your broad perspective on the book

178
00:22:28,960 --> 00:22:35,520
are you you know are you kind of going into these stories looking specifically for the the

179
00:22:35,520 --> 00:22:42,000
optimistic ending or you know does it vary do you have kind of different takes on where we'll go

180
00:22:42,000 --> 00:22:52,080
for different scenarios yeah I'm a huge fan of black mirror and if you know if the book reviews

181
00:22:52,080 --> 00:22:58,720
with find this book to be similar to the black mirror but more positive I there's nothing that would

182
00:22:58,720 --> 00:23:06,320
please me more I think the black mirror does a great job describing possible dangers and they

183
00:23:06,320 --> 00:23:13,200
usually end up with bad endings sometimes good ending so this so this is our effort to try to

184
00:23:13,200 --> 00:23:20,400
also describe the challenges that could arise and but also I want to go an extra step to say

185
00:23:20,400 --> 00:23:27,520
there could be a solution if only right if only we educated our kids differently if only we

186
00:23:28,400 --> 00:23:35,200
regulated large companies in particular ways if only we thought ahead about job displacement

187
00:23:35,200 --> 00:23:42,560
and provide the training if we deeply understand the meaning of money and how we can gradually

188
00:23:42,560 --> 00:23:52,800
move towards a substitute so and and I think probably six stories or so have a happy ending and then

189
00:23:52,800 --> 00:24:00,320
three have an ambiguous ending and one has a somewhat bad ending so I'm not I'm not being naive

190
00:24:00,320 --> 00:24:06,000
to say all the problems will be hand-waved away and solved so yeah really their challenges yeah

191
00:24:06,000 --> 00:24:14,560
yeah I think there's there are challenges with both the dystopic ending and the the utopian ending

192
00:24:14,560 --> 00:24:21,280
the optimistic ending and I think you almost want a you know not quite a choose your own adventure

193
00:24:21,840 --> 00:24:30,880
type of a story but a story that you know where there's a branch that says what the dystopic ending

194
00:24:30,880 --> 00:24:38,000
could look like and what are some of the levers that could put you in down that path and also

195
00:24:38,000 --> 00:24:44,240
presents the optimistic ending and you know what are some of the levers that would kind of drive

196
00:24:44,240 --> 00:24:51,440
society towards an optimistic perspective do you do you take on any of that in your analysis of

197
00:24:51,440 --> 00:25:01,680
the the various scenarios I do so so in the chapter about autonomous weapons it's a story it's

198
00:25:01,680 --> 00:25:08,480
an issue that I think a lot of people in the AI community feel the same way that the physics

199
00:25:08,480 --> 00:25:14,560
community felt about nuclear weapons and the chemistry community felt about chemical weapons

200
00:25:14,560 --> 00:25:24,720
and so on so it is the the the clearest challenge that we face today because the cost of making

201
00:25:24,720 --> 00:25:31,840
an autonomous drone with face recognition that can kill an individual as an automated assassin

202
00:25:32,400 --> 00:25:38,480
that has so many dangers because it lowers the cost of it for the terrorists without having

203
00:25:38,480 --> 00:25:44,800
to risk the terrorists lives and also it's very hard to to regulate because it's not like nuclear

204
00:25:44,800 --> 00:25:51,120
weapons they're the good and bad thing about nuclear weapons is there is a principle of a short

205
00:25:51,120 --> 00:25:56,960
mutual destruction so people have a deterrent countries don't do it because they're afraid of

206
00:25:56,960 --> 00:26:01,920
retaliation and mutual destruction but that isn't the case with autonomous weapons

207
00:26:01,920 --> 00:26:07,760
so so the one of the stories is about a terrorist model after the

208
00:26:08,640 --> 00:26:16,400
unabomber and who decided to take revenge on a particular group of people in this case the elites

209
00:26:16,400 --> 00:26:27,520
of the world as this model after the the unabomber so unabomber sorry so so the story ends up with

210
00:26:27,520 --> 00:26:36,480
challenges of what happens when autonomous weapons are not regulated and the outcome is

211
00:26:38,160 --> 00:26:42,720
somewhat negative one and the world isn't destroyed but it's still a somewhat negative one he

212
00:26:42,720 --> 00:26:50,240
gets away with something of course and and of course he's caught but he creates causes a lot of

213
00:26:50,240 --> 00:27:00,240
damage so in the explanation section I go into detail explaining the additional challenges

214
00:27:00,240 --> 00:27:07,360
of autonomous weapons compared to conventional weapons compared to you know as well as the

215
00:27:07,360 --> 00:27:12,560
nuclear weapons and why they really have to be regulated and what happens when you don't

216
00:27:12,560 --> 00:27:18,880
regulated and and then I point out the challenges of regulating it because unlike nuclear weapons you

217
00:27:18,880 --> 00:27:26,800
can't have UN going to a country and inspect uranium or nuclear facilities because someone can

218
00:27:26,800 --> 00:27:32,480
build this in their garage but nevertheless regulation must take place and I point out a few

219
00:27:32,480 --> 00:27:39,280
possible ways of regulating it as well as most AI scientists believe that it should be

220
00:27:39,280 --> 00:27:48,640
regulated several letters have been written and and also the consequences of not not regulating it

221
00:27:48,640 --> 00:27:56,560
so so I think that describes the both both possible paths of a negative outcome and that's an

222
00:27:56,560 --> 00:28:06,560
example where both paths are are explored. In a section like that where you're talking about

223
00:28:06,560 --> 00:28:15,680
something that's you know very clear and present danger do you is there a concrete call

224
00:28:15,680 --> 00:28:21,600
the action for folks is that part of your perspective here to tell folks how to

225
00:28:21,600 --> 00:28:26,400
you know if this is the issue that they're most passionate about where do they go?

226
00:28:26,400 --> 00:28:37,120
Yeah no it's not a direct call to action but I think it hopefully removes all ambiguity

227
00:28:38,000 --> 00:28:44,160
arguments have been made that autonomous weapons are in the early phases of development

228
00:28:44,160 --> 00:28:49,680
so it's too early to regulate them and I give counter example on why that isn't the case

229
00:28:49,680 --> 00:28:57,280
and also there are huge issues about how difficult it is to regulate it but I also point out

230
00:28:57,280 --> 00:29:04,400
that we mankind have managed to largely control and contain chemical weapons and biological

231
00:29:04,400 --> 00:29:11,040
weapons which are potentially equally difficult to to track and regulate so if we can do those

232
00:29:11,040 --> 00:29:16,880
we should be able to do this one so I think people can draw their own conclusions some are

233
00:29:16,880 --> 00:29:21,440
probably perhaps still not convinced by the argument but I wanted to to make the argument.

234
00:29:23,040 --> 00:29:30,240
I've often had these exchanges with folks where you know they kind of present this scenario of

235
00:29:31,440 --> 00:29:39,360
you know conscious AI that is belligerent in some way kind of like Terminator type of example

236
00:29:39,360 --> 00:29:48,080
and you know or like a you know a Nick Bostrom superintelligence that's potentially you know

237
00:29:48,080 --> 00:29:55,680
dangerous and you know often we'll say you know that is something that is potentially out there

238
00:29:55,680 --> 00:30:03,600
in the future but you know autonomous weapons are much more kind of clear and closer and

239
00:30:03,600 --> 00:30:12,320
concrete and scary for me personally than you know some AI that's you know acting on its own

240
00:30:12,320 --> 00:30:21,280
against human interest. Yeah absolutely I totally agree that's why by absence there is no

241
00:30:21,280 --> 00:30:28,000
singularity story in the book there is no AI with self-consciousness self-awareness that tries to

242
00:30:28,000 --> 00:30:35,040
destroy the human race in the story so by its absence I'm totally agreeing with that in the

243
00:30:35,040 --> 00:30:41,040
analysis section I do bring up the issue of singularity of why I don't rule out its possibility

244
00:30:41,040 --> 00:30:48,080
in the future but I think it's too simplistic to say the exponential growth of compute power

245
00:30:48,080 --> 00:30:53,920
also means an exponential growth that will drop the people behind in many of the stories we

246
00:30:53,920 --> 00:31:01,440
still see many parts of the human intelligence that cannot be replicated by AI the fact that

247
00:31:01,440 --> 00:31:08,160
many stories are saved by the saved by the hero or heroine of the story because of their

248
00:31:08,960 --> 00:31:16,160
emotional and beliefs and conviction and love and that's something unique to people and

249
00:31:16,720 --> 00:31:22,720
and also in stories where there are villains who do terrible things they're the ones who

250
00:31:22,720 --> 00:31:30,800
cause the disaster using AI as a tool AI never in these 10 stories become the villain in itself

251
00:31:30,800 --> 00:31:38,960
and and and then I clearly explained that singularity could happen when breakthroughs in algorithms

252
00:31:38,960 --> 00:31:47,520
enable a fully taking advantage of the exponential growth but today we still have not had breakthroughs

253
00:31:47,520 --> 00:31:54,880
that understand how our brain works why we have self-awareness and emotion and creativity

254
00:31:54,880 --> 00:32:01,520
and can do analysis and strategic thinking so to think we can replicate AI on something that we

255
00:32:01,520 --> 00:32:08,960
don't even know how we do it nor do we see AI approaching it and we know AI do not possess it

256
00:32:08,960 --> 00:32:15,760
so we we don't have to worry about extrapolating the exponential curve and seeing super

257
00:32:15,760 --> 00:32:22,400
intelligence and or singularity within the next 20 years that said I do believe the set of

258
00:32:22,400 --> 00:32:28,400
things that AI can do better than human will grow dramatically and AI will do many things that

259
00:32:28,400 --> 00:32:35,200
humans cannot imagine to do but there will always be a set that is about our core humanity

260
00:32:35,200 --> 00:32:41,200
at least in a 20 year time frame that we can hold on to and it's exactly that set that defines

261
00:32:41,200 --> 00:32:47,360
our humanity that causes are the stories the people in the stories to shine and save the day.

262
00:32:49,760 --> 00:32:56,320
Speaking of the exponential growth in compute one of the stories touches on quantum computing

263
00:32:56,960 --> 00:33:04,240
what you take on where that is in 20 years and the degree to which it enables a more powerful

264
00:33:04,240 --> 00:33:14,160
artificial intelligence. Yeah I think quantum is one of the areas where I needed to make a

265
00:33:14,160 --> 00:33:21,840
not 100% confident prediction right because there's too much understanding and variability

266
00:33:23,040 --> 00:33:29,040
but but I do think looking at the maps that IBM, Google and other companies have

267
00:33:29,040 --> 00:33:33,680
and the progress that's been made particularly in the last two years it seems like we can

268
00:33:33,680 --> 00:33:45,200
extrapolate a story where the improvements in logical cubits will reach thousands probably

269
00:33:45,200 --> 00:33:50,880
less than 20 years there are a lot of issues of how do you maintain stability and how many physical

270
00:33:50,880 --> 00:33:57,680
cubits do you need to support logical a few thousand logical cubits so I'm not an expert in the

271
00:33:57,680 --> 00:34:04,000
area but the experts seem to agree several thousand cubits are possible and there are useful

272
00:34:04,000 --> 00:34:13,120
applications by that time with the major one being insecurity that is the existing asymmetrical

273
00:34:13,120 --> 00:34:21,760
cryptography algorithms will no longer work the flip side of that is quantum computing will provide

274
00:34:21,760 --> 00:34:30,400
a new unbreakable security system so in the story I did not go into how quantum and AI work

275
00:34:30,400 --> 00:34:37,520
together because at a few thousand cubits I don't think it's enough to disrupt AI completely yet

276
00:34:37,520 --> 00:34:44,880
so 20 years would be about when the security challenges would come up so in one of the stories

277
00:34:44,880 --> 00:34:52,880
the villain achieved 4,000 cubits without anyone else knowing it and the villain went after

278
00:34:53,520 --> 00:35:01,360
stealing bitcoins which is one commonly described the largest bank that's waiting to be robbed

279
00:35:01,360 --> 00:35:08,720
so that was a part of the story but in the analysis I do go into the very nature of quantum

280
00:35:08,720 --> 00:35:15,600
that can hold uncertainty in its head and pursue paths in parallel and dramatically reducing

281
00:35:15,600 --> 00:35:22,320
the MP complete search problem will lead to a day where AI algorithms will be disrupted

282
00:35:22,960 --> 00:35:28,880
but I don't think 20 years is quite when that will happen will probably take longer with more than

283
00:35:28,880 --> 00:35:40,800
4,000 cubits so the book of course is focused on this 20 year you know 20 year forward time horizon

284
00:35:40,800 --> 00:35:49,280
but there are a lot of AI technologies around which there are very contemporary issues in the

285
00:35:49,280 --> 00:35:56,080
realm of ethics bias and others computer vision is one that comes to mind facial recognition in

286
00:35:56,080 --> 00:36:05,600
particular it's a very contemporary issue the use of facial recognition by police organizations

287
00:36:05,600 --> 00:36:14,000
the proliferation of cameras in you know quote unquote smart cities you know a lot of people

288
00:36:14,640 --> 00:36:21,120
look at the you know it would be easy to look at the situation now and the frustration that many

289
00:36:21,120 --> 00:36:27,520
people have with the situation now and find it difficult to project forward 20 years do you do that

290
00:36:27,520 --> 00:36:35,920
in the book I do not facial recognition is one I did not speculate because we're in a bifurcated

291
00:36:35,920 --> 00:36:44,800
world some countries are attempting to regulate others are not and it's it's not clear a bifurcated

292
00:36:44,800 --> 00:36:52,880
world can work hopefully will reach a universal consensus at some point I do go into many other

293
00:36:52,880 --> 00:37:01,120
aspects of externalities and I guess you can extrapolate from them to to all of the possibilities

294
00:37:01,120 --> 00:37:09,360
so for example I talk about how objective functions need to be improved to go from

295
00:37:09,360 --> 00:37:18,080
maniacally focusing on something like clickthroughs and revenue generating moving into longer term

296
00:37:18,080 --> 00:37:25,440
metrics so our social media in 20 years ought to be showing us content that is making us better

297
00:37:25,440 --> 00:37:32,240
over time that that we feel we're seeing content that is time well spent as Tristan Harris would

298
00:37:32,240 --> 00:37:39,040
say or we're seeing content that is making us improving in some metric maybe it's our wealth

299
00:37:39,040 --> 00:37:45,840
maybe it's our happiness maybe it's our how much knowledge we've gained and whether AI

300
00:37:45,840 --> 00:37:52,720
objective functions can be turned more long term and more aligned with humans that's one aspect

301
00:37:52,720 --> 00:37:59,280
I explored and I think technologists should spend more time on topics like that another is on

302
00:37:59,280 --> 00:38:09,600
bias and fairness can we ensure that have tools that ensure that AI is being trained reasonably

303
00:38:09,600 --> 00:38:17,040
balanced data so that it's not discriminating against any race gender individual etc.

304
00:38:18,240 --> 00:38:26,880
And and also can can compilers alert warnings right now can AI tools do the same

305
00:38:26,880 --> 00:38:33,840
and also can AI engineers be trained to be aware of the substantial power that they control

306
00:38:33,840 --> 00:38:40,720
and therefore the responsibilities that must come with it so that's another aspect another one

307
00:38:40,720 --> 00:38:48,480
related to privacy and and and and having our cake and eat it too can we have AI trained on a lot

308
00:38:48,480 --> 00:38:55,360
of data but not everybody giving away data privately without consent so the stories in the book

309
00:38:55,360 --> 00:39:02,720
talks about how technologies like federated learning homomorphic encryption and also

310
00:39:03,360 --> 00:39:10,560
hardware environments that are self-contained where where data does not leak can these take these

311
00:39:10,560 --> 00:39:15,920
technologies I predict in 20 years we'll be able to let us have our cake and eat it too

312
00:39:15,920 --> 00:39:24,960
so that our data stays in devices to which we permit say our phone our computer or the computers

313
00:39:24,960 --> 00:39:31,600
at the hospital which has our data but not beyond that so the models from a hospital is trained

314
00:39:31,600 --> 00:39:37,920
on all the patients in that hospital who licensed their data to the hospital but not beyond

315
00:39:37,920 --> 00:39:45,440
then the hospitals can jointly train by pulling their models together so so in the book I

316
00:39:45,440 --> 00:39:50,720
point out the technological areas that I think are promising and the possible technological

317
00:39:50,720 --> 00:39:57,760
solutions that could end up addressing many of the problems we see and I think the call to action

318
00:39:57,760 --> 00:40:04,480
is for the technologists who read the book and watch the podcast to think about whether rather than

319
00:40:04,480 --> 00:40:12,480
doing research on the next deep learning or tweaking a particular model is it useful for a sufficient

320
00:40:12,480 --> 00:40:19,840
percentage of the AI community to think about these technological solutions that solve the problems

321
00:40:19,840 --> 00:40:31,440
caused by our technology AI. On that note there are a number of labs focused on AI

322
00:40:31,440 --> 00:40:39,280
safety as a research focus you know often they take the perspective of you know trying to

323
00:40:39,280 --> 00:40:44,800
prevent the terminator scenario or making sure that we can control the terminator scenario.

324
00:40:44,800 --> 00:40:52,240
Do you have a perspective on on those efforts are they asking the right questions looking at

325
00:40:52,240 --> 00:41:01,680
the right things? Well there are risks that are clear and present danger I think those ought to

326
00:41:01,680 --> 00:41:11,600
be addressed by the largest number of people issues related to bias fairness how to have our

327
00:41:11,600 --> 00:41:18,240
cake in either two with respect to personal data there are a longer term lower likelihood

328
00:41:19,520 --> 00:41:25,120
existential questions that one could ask and I think it makes perfect sense for a small number

329
00:41:25,680 --> 00:41:34,880
of people in more like a think tank than a technology development to basically watch

330
00:41:34,880 --> 00:41:40,800
for that possibility and to alert for the rest of us so yes I do think those labs should

331
00:41:40,800 --> 00:41:46,480
continue to do what they do I don't think those existential threats will happen in the next 20

332
00:41:46,480 --> 00:41:52,960
years but I think we should have think tanks that think about them and tell us when we really do

333
00:41:52,960 --> 00:42:03,600
need to get involved and worried. In the chapter on autonomous weapons you call for regulation

334
00:42:03,600 --> 00:42:12,240
you mentioned the objective function and you know there are many contemporary calls for regulation

335
00:42:12,240 --> 00:42:21,120
of internet companies and advertising methods and you know privacy and many other things

336
00:42:21,120 --> 00:42:27,680
for internet companies but how do you see regulation evolving over the next 20 years?

337
00:42:27,680 --> 00:42:36,400
Yeah I think in the US people talk the most about breaking up companies I think that is

338
00:42:36,400 --> 00:42:44,720
a two brute force and two you know 20th century it's not something designed for this kind of

339
00:42:44,720 --> 00:42:51,200
monopoly I do think regulations are needed but I think we need to come up with newer and better

340
00:42:51,200 --> 00:42:59,280
regulations and why do you think that's an effective for modern companies? Well let's say

341
00:42:59,280 --> 00:43:06,960
Facebook got broken up into whatsapp.com and instagram.com and facebook.com it doesn't stop any of it

342
00:43:06,960 --> 00:43:13,520
it wouldn't have stopped the Cambridge Analytica issue right it wouldn't stop any one of the three

343
00:43:13,520 --> 00:43:19,600
products doing things that we don't want them to do it would reduce it but it's it's too brute force

344
00:43:19,600 --> 00:43:28,800
it was specifically addressing monopoly extension by you know standard oil moving into gas gas

345
00:43:28,800 --> 00:43:38,080
into gas stations and stop yeah and having the big bell company break broken up into baby bells

346
00:43:38,720 --> 00:43:44,400
I think those were perhaps appropriate for telecommunications or traditional industries

347
00:43:44,400 --> 00:43:54,160
but I think the issue that is fundamental is I think people the reason people go into such extreme

348
00:43:54,160 --> 00:44:00,960
measures is they've given up hope that some companies can self-manage I've not given up hope

349
00:44:00,960 --> 00:44:08,400
but I don't think today's reward and punishment systems give the large companies any incentive

350
00:44:08,400 --> 00:44:15,200
to self-manage and those need to be created for example I think an idea called the AI audit

351
00:44:15,200 --> 00:44:20,640
is something that could be pursued right it's very clear that the government can't go in and

352
00:44:20,640 --> 00:44:27,440
and look at the code and data for each of the large internet companies but when there are

353
00:44:28,320 --> 00:44:34,160
sufficiently serious complaints and repetitions of complaints there can be an audit just like

354
00:44:34,160 --> 00:44:40,880
there can be a financial audit or a tax audit so with that as a deterrent I think companies

355
00:44:40,880 --> 00:44:46,480
can be better behaved of course what are the metrics how does a complaint count should the

356
00:44:46,480 --> 00:44:51,200
government get to look at the data in a large company these are all issues that need to be solved

357
00:44:51,200 --> 00:44:57,600
but it seems more I think a more plausible way than just breaking up companies and more effective

358
00:44:57,600 --> 00:45:06,880
another I think ultimately we need to get companies to really have aligned financial

359
00:45:07,680 --> 00:45:14,880
incentives so that if they better behave for example can there be a third party watchdog

360
00:45:14,880 --> 00:45:22,240
that publishes how much fake news how much you know false advertising how much wrong search results

361
00:45:22,240 --> 00:45:27,840
or whatever things we have if we have a third party watchdog that publishes those and

362
00:45:28,640 --> 00:45:35,520
enough consumer advocacy and a corporate ESG pressure for the companies to feel like

363
00:45:35,520 --> 00:45:40,880
every quarter they have to report not just the financial results but how they do on the

364
00:45:41,600 --> 00:45:48,560
you know a fake news metric fake news ranking then they're going to form internal teams

365
00:45:48,560 --> 00:45:54,640
because they're being monitored from the outside so that would be one example but the ideally

366
00:45:56,000 --> 00:46:03,840
we want to somehow have companies that can make even more money by aligning themselves with

367
00:46:04,400 --> 00:46:14,480
user needs so as users become happier or learn more information or become wealthier or whatever

368
00:46:14,480 --> 00:46:21,840
those metrics and it can somehow be attributed to companies that have created or helped

369
00:46:21,840 --> 00:46:28,960
enable that situation then it can make more it can even make even more money so

370
00:46:28,960 --> 00:46:33,760
so in other words are we willing to pay a company a lot more money to make us

371
00:46:34,880 --> 00:46:42,560
more knowledgeable wealthier or happier in a three year horizon compared to the money that

372
00:46:42,560 --> 00:46:52,080
the company would make us clicking and buying things so looking at natural financially aligned

373
00:46:52,080 --> 00:46:58,720
metrics that connect the user and the the company this is a little bit abstract at right now

374
00:46:58,720 --> 00:47:03,120
I was going to ask you say destiny such metrics in the book I suspect answers now

375
00:47:04,320 --> 00:47:11,040
no well it's such like you know 30 years ago would people have come up with the ways that

376
00:47:11,040 --> 00:47:18,160
you know Facebook does advertising or Google does at words or at sense some smart entrepreneur

377
00:47:18,800 --> 00:47:26,720
will come up with some system that will create a new ecosystem that will create a new set of

378
00:47:26,720 --> 00:47:31,920
companies that are even more profitable than Google and Facebook unfortunately I don't I don't

379
00:47:31,920 --> 00:47:38,000
know the answer but entrepreneurs and VCs can take a step back and think about it it's not so much

380
00:47:38,000 --> 00:47:43,600
out of a question right because how do you how do we measure people's happiness well there are

381
00:47:43,600 --> 00:47:50,560
many metrics that can be used on our facial expressions micro expressions measures of our hormones

382
00:47:51,520 --> 00:47:58,800
endorphin etc that could be one beginning of such a way our our wealth can be measured over time

383
00:47:59,760 --> 00:48:06,560
and whether we've learned something and grown I think just like you know gpt3 today can remember

384
00:48:06,560 --> 00:48:11,040
millions of words that it's read and pick out the ones that are relevant for the given current

385
00:48:11,040 --> 00:48:18,080
context perhaps there will be AI that can look at all of our time spent on the internet and pick out

386
00:48:18,080 --> 00:48:24,800
the epiphany moments that have caused us to grow and and and those moments if there is a software

387
00:48:24,800 --> 00:48:31,520
technology or objective function that enable the moment they should be properly compensated for it

388
00:48:31,520 --> 00:48:36,320
so I don't think it's out of the question um the technologies can be developed but I don't know

389
00:48:36,320 --> 00:48:42,000
what the model is if I did I'd be either funding or creating that company myself I was just

390
00:48:42,000 --> 00:48:48,000
going to ask was that part of your motivation for writing the book to kind of signal to

391
00:48:48,720 --> 00:48:54,720
entrepreneurs that hey these are areas that need to be explored and if you are working in these

392
00:48:54,720 --> 00:49:03,200
areas hey reach out to reach out to me that's not the primary purpose but if that were a side

393
00:49:03,200 --> 00:49:10,560
effect I would be happy to look at those business plans awesome awesome well kayfoo thanks so much

394
00:49:10,560 --> 00:49:18,720
for taking the time to chat with us and share a bit about the book congratulations on the book it

395
00:49:18,720 --> 00:49:27,280
is really takes an interesting approach at raising some very important questions in the development

396
00:49:27,280 --> 00:49:57,120
of AI so thanks and and congrats once again on that thank you thanks Sam for having

