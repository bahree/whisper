1
00:00:00,000 --> 00:00:16,360
Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting

2
00:00:16,360 --> 00:00:21,480
people doing interesting things in machine learning and artificial intelligence.

3
00:00:21,480 --> 00:00:27,240
I'm your host Sam Charrington.

4
00:00:27,240 --> 00:00:30,600
Once again, the recording you're about to hear is part of a series of interviews

5
00:00:30,600 --> 00:00:36,440
I recorded live from the O'Reilly AI and Stratoconferences in New York City.

6
00:00:36,440 --> 00:00:41,720
My guess this time is Pascal Fung, professor of electrical and computer engineering at

7
00:00:41,720 --> 00:00:45,000
Hong Kong University of Science and Technology.

8
00:00:45,000 --> 00:00:50,800
Pascal gave a really interesting presentation at the AI conference, focused on how we teach

9
00:00:50,800 --> 00:00:57,200
computers and robots to understand human emotion and be empathetic.

10
00:00:57,200 --> 00:01:02,560
She also had some really interesting things to say about the theoretical foundations of

11
00:01:02,560 --> 00:01:06,160
the various modern approaches to speech understanding.

12
00:01:06,160 --> 00:01:10,040
And we dig into all of this in our conversation.

13
00:01:10,040 --> 00:01:14,640
As always, I'll be linking to Pascal and her research in the show notes, which you'll

14
00:01:14,640 --> 00:01:20,480
be able to find at twimlai.com slash talk slash nine.

15
00:01:20,480 --> 00:01:24,600
As is unfortunately the case with my other field recordings, there's a bit of unavoidable

16
00:01:24,600 --> 00:01:27,960
background noise, but it's not too bad.

17
00:01:27,960 --> 00:01:31,600
And now on to the show.

18
00:01:31,600 --> 00:01:44,760
Alright, hey everyone, I'm here at the O'Reilly AI conference and I'm with Pascal Fung,

19
00:01:44,760 --> 00:01:51,640
who is a professor of electrical engineering at Hong Kong University of Science and Technology.

20
00:01:51,640 --> 00:01:58,760
And I sat in on her talk earlier on emotions and AI and how do we enable computers to recognize

21
00:01:58,760 --> 00:01:59,760
emotions.

22
00:01:59,760 --> 00:02:04,320
And she graciously agreed to spend a few minutes with us to tell us a little bit about

23
00:02:04,320 --> 00:02:05,840
what she's working on.

24
00:02:05,840 --> 00:02:07,600
So welcome Pascal.

25
00:02:07,600 --> 00:02:08,600
Thank you.

26
00:02:08,600 --> 00:02:13,200
How about we start with talking a little bit about your background and the kinds of things

27
00:02:13,200 --> 00:02:14,200
you work on?

28
00:02:14,200 --> 00:02:15,200
Sure.

29
00:02:15,200 --> 00:02:20,920
So my background, I am an electrical engineer and computer scientist.

30
00:02:20,920 --> 00:02:27,360
And I've been working on speech recognition since 1988 and then move on to statistical

31
00:02:27,360 --> 00:02:30,320
machine translation in the 90s.

32
00:02:30,320 --> 00:02:36,920
And after I became a professor at the HKUST, I lead a team working on speech language

33
00:02:36,920 --> 00:02:44,040
and more recently on emotion and mood recognition or sort of using statistical modeling and machine

34
00:02:44,040 --> 00:02:45,040
learning methods.

35
00:02:45,040 --> 00:02:47,920
That's my technical background.

36
00:02:47,920 --> 00:02:51,760
I worked at different places before.

37
00:02:51,760 --> 00:03:00,200
I was a student and while working on my PhD thesis at Bell Labs, I was very lucky to work

38
00:03:00,200 --> 00:03:06,280
with some of the best people in the area and in the early 90s when I started my thesis,

39
00:03:06,280 --> 00:03:14,120
it was when the field of natural language processing was transforming from a heavily

40
00:03:14,120 --> 00:03:19,800
knowledge-based, linguistics-based field to statistical modeling.

41
00:03:19,800 --> 00:03:23,200
And at the time, statistical modeling for language was very controversial.

42
00:03:23,200 --> 00:03:24,200
Right.

43
00:03:24,200 --> 00:03:27,720
People actually didn't believe you could learn language or study language or understand

44
00:03:27,720 --> 00:03:30,760
language with just statistics.

45
00:03:30,760 --> 00:03:38,600
But 20 years later, now that is the mainstream approach or everything we do is with machine

46
00:03:38,600 --> 00:03:42,560
learning statistical modeling in natural language processing.

47
00:03:42,560 --> 00:03:48,040
One of the slides you put up had a quote from a professor who I think you mentioned you

48
00:03:48,040 --> 00:03:52,480
work with that said, for every language, like fire, or anything like that.

49
00:03:52,480 --> 00:03:57,640
He's sort of the father of statistical speech recognition.

50
00:03:57,640 --> 00:04:00,680
So the field holds a lot to him.

51
00:04:00,680 --> 00:04:03,440
He passed away a few years ago.

52
00:04:03,440 --> 00:04:09,480
And then this quote of his was controversial, but what was the quote?

53
00:04:09,480 --> 00:04:15,840
The quote was that every time I fire a linguist, the speech recognition accuracy goes up.

54
00:04:15,840 --> 00:04:26,200
So it came from at the time at IBM, he was leading the IBM group with a group of mathematicians

55
00:04:26,200 --> 00:04:32,040
and information theoreticians to work on the problems of speech recognition, which was

56
00:04:32,040 --> 00:04:37,960
previously worked on using knowledge-based approach in AI.

57
00:04:37,960 --> 00:04:41,480
And actually, his group wasn't allowed to use that approach.

58
00:04:41,480 --> 00:04:43,240
They somehow did it anyways.

59
00:04:43,240 --> 00:04:48,880
So there was always a little bit of conflict between the knowledge-based AI community and

60
00:04:48,880 --> 00:04:54,080
the at the time, the statistical-minded people.

61
00:04:54,080 --> 00:05:01,840
So there were papers written about the empiricist versus rationalists in the 90s.

62
00:05:01,840 --> 00:05:08,840
And I remember my first conference presentation on a statistical-based natural language processing

63
00:05:08,840 --> 00:05:09,840
paper.

64
00:05:09,840 --> 00:05:14,800
I was yelled at by some of the senior people in the field.

65
00:05:14,800 --> 00:05:17,120
Yeah, those were the days.

66
00:05:17,120 --> 00:05:23,280
But now it's totally non-controversial at all, because you can see machine learnings everywhere.

67
00:05:23,280 --> 00:05:31,520
And yeah, so he said that because the approach he proposed, his group proposed, was very

68
00:05:31,520 --> 00:05:37,800
radical at the time, which is not looking at how to imitate human at all, but just looking

69
00:05:37,800 --> 00:05:41,040
at the input and output of the task.

70
00:05:41,040 --> 00:05:48,760
So for example, for speech recognition, the input is speech waveforms and the output should

71
00:05:48,760 --> 00:05:49,760
be words.

72
00:05:49,760 --> 00:05:55,240
And for machine translation, the input could be French and output should be English.

73
00:05:55,240 --> 00:06:01,200
And they basically treated all these problems as the kind of information theoretic problems

74
00:06:01,200 --> 00:06:02,200
to solve.

75
00:06:02,200 --> 00:06:07,040
So a different mathematical approach from the traditional knowledge-based AI approach

76
00:06:07,040 --> 00:06:08,520
at the time.

77
00:06:08,520 --> 00:06:14,720
Can you maybe give a 30,000-foot background of information theory and how it plays into

78
00:06:14,720 --> 00:06:15,720
all this?

79
00:06:15,720 --> 00:06:16,720
Okay.

80
00:06:16,720 --> 00:06:17,720
I'll try.

81
00:06:17,720 --> 00:06:24,520
So information theory was invented by the entire field, came from a paper written by Claude

82
00:06:24,520 --> 00:06:31,280
Shannon in 1948 called the Mathematical Theory of Communication.

83
00:06:31,280 --> 00:06:36,040
So it basically looks at, you know, the information coding.

84
00:06:36,040 --> 00:06:42,520
For example, if you want to transmit telephone signal through a telephone cable, there's

85
00:06:42,520 --> 00:06:46,160
only that much information you can transmit.

86
00:06:46,160 --> 00:06:54,320
And how many simultaneous calls can you transmit in one cable is limited by physics, actually.

87
00:06:54,320 --> 00:06:55,320
Right.

88
00:06:55,320 --> 00:07:02,920
And so the information theory really is talking about, you know, how do you encode on transmit

89
00:07:02,920 --> 00:07:05,520
and decode information.

90
00:07:05,520 --> 00:07:11,000
So the earliest application was, of course, telephone systems.

91
00:07:11,000 --> 00:07:12,240
So no coincidence.

92
00:07:12,240 --> 00:07:17,120
So there was no accident that Claude Shannon was also at the labs.

93
00:07:17,120 --> 00:07:24,120
And then later on, information theory was then applied, actually, at the time when Claude

94
00:07:24,120 --> 00:07:30,600
Shannon came up with this information theory, he already had the paper on the information

95
00:07:30,600 --> 00:07:31,600
of language.

96
00:07:31,600 --> 00:07:32,600
Oh, right.

97
00:07:32,600 --> 00:07:33,600
Yes, yes.

98
00:07:33,600 --> 00:07:41,000
He actually wrote a paper very early about how language can be encoded and learned.

99
00:07:41,000 --> 00:07:46,400
And so he was one of the earlier pioneers of AI.

100
00:07:46,400 --> 00:07:50,520
Even though people don't think of him all in that way, he had no idea.

101
00:07:50,520 --> 00:07:51,520
Yes.

102
00:07:51,520 --> 00:07:57,520
And he actually also had a cent of American paper on the first chess playing game, chess

103
00:07:57,520 --> 00:08:00,160
playing algorithm in the 60s.

104
00:08:00,160 --> 00:08:01,160
So, yeah.

105
00:08:01,160 --> 00:08:06,040
So information theory became an entire field of research.

106
00:08:06,040 --> 00:08:14,720
And it's applied widely in many, many different areas, most importantly telecommunications communications.

107
00:08:14,720 --> 00:08:21,080
And then, of course, in all the statistical learning field, we also use information theory.

108
00:08:21,080 --> 00:08:34,000
For example, look at how we can learn to model a language from information theory at the

109
00:08:34,000 --> 00:08:35,000
point of view.

110
00:08:35,000 --> 00:08:42,520
So it's more like a, you can think of as message encoding kind of way, rather than linguistically

111
00:08:42,520 --> 00:08:43,840
motivated way.

112
00:08:43,840 --> 00:08:49,160
So it's a different way of looking at mathematical approach of looking at problems.

113
00:08:49,160 --> 00:08:50,160
Right.

114
00:08:50,160 --> 00:08:53,240
You mentioned that in your talk and I found that fascinating and it never occurred to

115
00:08:53,240 --> 00:08:54,240
me.

116
00:08:54,240 --> 00:09:00,320
I think you, you pose the idea of thinking of the machine translation problem as you've

117
00:09:00,320 --> 00:09:05,520
got this, you've got this message that is, you're trying to translate French to English.

118
00:09:05,520 --> 00:09:07,760
You've got this message that's in noisy English.

119
00:09:07,760 --> 00:09:08,760
Yeah.

120
00:09:08,760 --> 00:09:09,760
Exactly.

121
00:09:09,760 --> 00:09:12,040
You clean up the noise and get to the true English.

122
00:09:12,040 --> 00:09:14,920
So for example, it's exactly that.

123
00:09:14,920 --> 00:09:19,200
So for example, the word order are different in different languages.

124
00:09:19,200 --> 00:09:24,680
So it can be, you can think of that word reordering as a kind of a distortion.

125
00:09:24,680 --> 00:09:29,360
And then some of the words actually in French English, some of the words are the same.

126
00:09:29,360 --> 00:09:30,360
Like 40%.

127
00:09:30,360 --> 00:09:31,360
Yeah.

128
00:09:31,360 --> 00:09:32,360
Right.

129
00:09:32,360 --> 00:09:33,360
But other words are different.

130
00:09:33,360 --> 00:09:34,360
Right.

131
00:09:34,360 --> 00:09:38,960
So you can, again, think of a word that's different, a different language being a kind of noise.

132
00:09:38,960 --> 00:09:42,000
I mean, a kind of distortion, I'm sorry.

133
00:09:42,000 --> 00:09:46,240
But if you can learn that distortion, then you learn the translation.

134
00:09:46,240 --> 00:09:49,760
So and that is some information theoretic approach.

135
00:09:49,760 --> 00:09:52,720
That's what Google Translate is still based on.

136
00:09:52,720 --> 00:09:54,600
So interesting.

137
00:09:54,600 --> 00:10:04,120
So how, starting from what are some of the, you know, the algorithms or approaches that

138
00:10:04,120 --> 00:10:10,200
kind of come out of the information theory background, like how is it applied more concretely

139
00:10:10,200 --> 00:10:11,200
to that problem?

140
00:10:11,200 --> 00:10:12,200
Okay.

141
00:10:12,200 --> 00:10:18,640
So for example, all modern speech recognition software is based on the noisy channel model.

142
00:10:18,640 --> 00:10:25,240
So the whole idea of you can train a speech recognizer with lots of data.

143
00:10:25,240 --> 00:10:27,960
So let's say voice search and all that.

144
00:10:27,960 --> 00:10:35,240
It's all based on what other people have said and it's based on these days, millions

145
00:10:35,240 --> 00:10:40,920
of hours of speech data like Siri, for example, it's trained on this data.

146
00:10:40,920 --> 00:10:48,360
And then it uses a different kind of machine learning method.

147
00:10:48,360 --> 00:10:55,520
But all these speech recognition methodologies based on one big paradigm is still the

148
00:10:55,520 --> 00:11:01,480
noisy channel model, which is speech inwards out through this channel.

149
00:11:01,480 --> 00:11:10,040
Now the latest approaches has turned part of these methods into using deep learning to

150
00:11:10,040 --> 00:11:18,560
replace some modules, for example, replacing how phonemes can be modeled or replacing part

151
00:11:18,560 --> 00:11:23,520
of the predicting what will come after which word.

152
00:11:23,520 --> 00:11:30,880
So some of this is now enabled by deep learning, but the whole paradigm is still an information

153
00:11:30,880 --> 00:11:32,600
theory approach.

154
00:11:32,600 --> 00:11:38,080
Similarly for things like Google Translate, it's still based on the, what I just talked

155
00:11:38,080 --> 00:11:40,400
about the noisy channel model.

156
00:11:40,400 --> 00:11:48,760
And recently there's some research work on using Neuronat, but we haven't seen any commercial

157
00:11:48,760 --> 00:11:56,800
application that claims to be using Neuronat for, so end to end Neuronat for formation translation

158
00:11:56,800 --> 00:11:57,800
yet.

159
00:11:57,800 --> 00:11:58,800
Okay.

160
00:11:58,800 --> 00:12:00,800
Okay.

161
00:12:00,800 --> 00:12:05,640
So how did you, how did you kind of get it?

162
00:12:05,640 --> 00:12:10,320
It's all to mention all the encryption software, all that is based on information theory.

163
00:12:10,320 --> 00:12:11,320
Right.

164
00:12:11,320 --> 00:12:12,320
Yeah.

165
00:12:12,320 --> 00:12:13,320
Okay.

166
00:12:13,320 --> 00:12:16,960
That is not my field, but that's actually a main application for this kind of work.

167
00:12:16,960 --> 00:12:17,960
Yeah.

168
00:12:17,960 --> 00:12:18,960
Okay.

169
00:12:18,960 --> 00:12:19,960
Awesome.

170
00:12:19,960 --> 00:12:20,960
Awesome.

171
00:12:20,960 --> 00:12:24,040
How did you get into the study of emotion?

172
00:12:24,040 --> 00:12:26,560
How did I get into the study of emotion?

173
00:12:26,560 --> 00:12:33,920
Basically we noticed that for, so I've been working on spoken language understanding for

174
00:12:33,920 --> 00:12:34,920
a long time.

175
00:12:34,920 --> 00:12:42,720
And I've participated on different efforts from different generations of virtual agents,

176
00:12:42,720 --> 00:12:44,240
what we call virtual assistants today.

177
00:12:44,240 --> 00:12:45,240
Okay.

178
00:12:45,240 --> 00:12:52,680
So the earliest system in the 80s was funded by DARPA, which was a ticket booking system.

179
00:12:52,680 --> 00:12:59,760
Why you call and say I want to go from here to there and then it's trying to book a ticket

180
00:12:59,760 --> 00:13:00,920
for you.

181
00:13:00,920 --> 00:13:05,720
And from that time on, we have seen different generations of dialos systems up until today

182
00:13:05,720 --> 00:13:12,120
we have Siri and Cortana and working on these dialos systems, I've noticed that we've

183
00:13:12,120 --> 00:13:20,200
always sort of just look at literal meaning of a user query.

184
00:13:20,200 --> 00:13:26,560
So the machine just pays attention to what is the destination, the origin city, how

185
00:13:26,560 --> 00:13:31,280
I mean text you want, what kind of restaurant you're looking for.

186
00:13:31,280 --> 00:13:37,920
So very sort of literal interpretation of your query, which means that your query has

187
00:13:37,920 --> 00:13:39,400
to be very clear.

188
00:13:39,400 --> 00:13:44,240
These days people always complain, Siri doesn't work well and all that.

189
00:13:44,240 --> 00:13:50,360
To us, researchers, we can see why it doesn't work well because the system assumes users

190
00:13:50,360 --> 00:13:57,280
to be very clear and say explicitly what they really want to get.

191
00:13:57,280 --> 00:14:02,040
And unless the users are lawyers, I mean, very few people talk that way.

192
00:14:02,040 --> 00:14:06,280
We talk naturally, we expect you to understand what I mean.

193
00:14:06,280 --> 00:14:10,240
And for example, you just laughed because you know I was trying to be funny.

194
00:14:10,240 --> 00:14:16,040
And that kind of information is completely lost in our dialos systems of previous generations.

195
00:14:16,040 --> 00:14:19,800
But it is very, very important for true communication.

196
00:14:19,800 --> 00:14:30,400
If we want to go past the current sort of plateau of understanding, we have to also incorporate

197
00:14:30,400 --> 00:14:35,800
the understanding of emotion intent and all that in addition to understanding the meaning

198
00:14:35,800 --> 00:14:37,400
of the words.

199
00:14:37,400 --> 00:14:40,960
So that's when I started working on incorporating.

200
00:14:40,960 --> 00:14:46,960
So I do not recognize, I don't work on recognizing emotion for emotion sake.

201
00:14:46,960 --> 00:14:51,640
It's really recognizing emotion for communications.

202
00:14:51,640 --> 00:14:55,960
So that's what I call empathy module.

203
00:14:55,960 --> 00:15:02,480
And then when I look at the future applications, what we do, the sum of the most immediate

204
00:15:02,480 --> 00:15:07,360
applications that immediate in the sense that in the next 20 to 30 years we'll see why

205
00:15:07,360 --> 00:15:17,120
desperate application will be healthcare and elderly care, because by the year 2050 there

206
00:15:17,120 --> 00:15:22,080
will be more old people than young children in the world.

207
00:15:22,080 --> 00:15:28,360
And so elderly care is a big area and governments will be running out of resources, human resources

208
00:15:28,360 --> 00:15:30,960
to take care of these elderly.

209
00:15:30,960 --> 00:15:34,360
So we're going to need a lot of machine assistance.

210
00:15:34,360 --> 00:15:39,240
Now my mother spends a lot of time alone.

211
00:15:39,240 --> 00:15:43,120
She's very independent, she's in her ceremonies.

212
00:15:43,120 --> 00:15:48,080
She doesn't want to live with me and she wants to be left alone to do her own thing.

213
00:15:48,080 --> 00:15:54,720
And I'm always worried and in her independence, I worry about her health.

214
00:15:54,720 --> 00:16:01,880
She seems healthy when I see her, but at her age I want to make sure that she's fine

215
00:16:01,880 --> 00:16:02,880
right now for example.

216
00:16:02,880 --> 00:16:04,600
I want to know she's fine right.

217
00:16:04,600 --> 00:16:12,160
So this kind of, I want to know her health conditions but also her mental conditions.

218
00:16:12,160 --> 00:16:19,200
So if she doesn't want to live with me, now how about if I have, if we build a home

219
00:16:19,200 --> 00:16:27,880
robot who can be there at her, you know, at her home or be around her and converse with

220
00:16:27,880 --> 00:16:34,200
her, sometimes just to get a sense of how she's doing simple things, right.

221
00:16:34,200 --> 00:16:38,640
And then sends me a message so I know how she, I mean, all sends me a curve of her vital

222
00:16:38,640 --> 00:16:46,960
signs in addition to her emotional state, then that will help the guilty children, busy

223
00:16:46,960 --> 00:16:53,000
guilty children, but also help the elderly because in a lot of people have emergency problems

224
00:16:53,000 --> 00:16:58,080
like a heart attack or something that could have been, they could have been saved if someone

225
00:16:58,080 --> 00:16:59,240
knows.

226
00:16:59,240 --> 00:17:05,800
And then there are others who can get lonely and depressed and then they can also be helped

227
00:17:05,800 --> 00:17:10,760
by machine to some extent, not completely by machines, you know, living with just machines

228
00:17:10,760 --> 00:17:17,280
is also very sad, but but when there are not enough humans around people around then

229
00:17:17,280 --> 00:17:19,440
the machines can help.

230
00:17:19,440 --> 00:17:26,800
So this is why I want to work on empathetic robots, in the case, in the case of a crisis

231
00:17:26,800 --> 00:17:32,480
situation like a heart attack, where does empathy and what does that come in?

232
00:17:32,480 --> 00:17:40,360
I think heart attack is basically its emergency, then the robot has to basically alert, call

233
00:17:40,360 --> 00:17:41,440
the ambulance, right?

234
00:17:41,440 --> 00:17:47,880
That's the first thing, but what empathy comes into play is that so daily reminder to take

235
00:17:47,880 --> 00:17:48,880
medicine.

236
00:17:48,880 --> 00:17:56,680
In some of the aging studies, people have found that a lot of elderly, they don't want

237
00:17:56,680 --> 00:18:05,200
to take medicine unless somebody talks to them, like sometimes you have to sort of entice

238
00:18:05,200 --> 00:18:08,760
them, I mean some elderly select children.

239
00:18:08,760 --> 00:18:16,600
So in that case, the machine doesn't just say take your medicine and repeatedly insisting

240
00:18:16,600 --> 00:18:21,600
that you take your medicine, like with the same command, that will be extremely annoying

241
00:18:21,600 --> 00:18:23,960
and it will have the opposite effect, right?

242
00:18:23,960 --> 00:18:31,160
So the machine needs to know that the elderly is hesitant or resisting and how is the person's

243
00:18:31,160 --> 00:18:39,640
patient's emotional state and to know whether now the machine can insist or it's time to

244
00:18:39,640 --> 00:18:48,160
call a doctor or nurse or whether telling the patient a bedtime story will suit them.

245
00:18:48,160 --> 00:18:53,560
So that requires empathy, if you think about all the nurses in the hospital, a lot of

246
00:18:53,560 --> 00:19:00,640
their job, a lot of their work and their tasks are very repetitive and sometimes the better

247
00:19:00,640 --> 00:19:05,280
nurses are the ones who really have a very good bedside manner and what is the bedside

248
00:19:05,280 --> 00:19:10,160
manner other than being empathetic, just being empathetic, you know, for both doctors

249
00:19:10,160 --> 00:19:11,160
and nurses.

250
00:19:11,160 --> 00:19:15,920
So if you want doctors and nurses to be empathetic, obviously you want the healthcare robots

251
00:19:15,920 --> 00:19:16,920
to be empathetic.

252
00:19:16,920 --> 00:19:17,920
Right, right.

253
00:19:17,920 --> 00:19:24,640
You talk, I interpreted its focus on empathy recognition, but your description is also

254
00:19:24,640 --> 00:19:27,560
talking about what you might call generative empathy.

255
00:19:27,560 --> 00:19:28,560
Right, right.

256
00:19:28,560 --> 00:19:34,800
So empathy has two sides, it's the emotional recognition and then the appropriate emotional

257
00:19:34,800 --> 00:19:35,800
response.

258
00:19:35,800 --> 00:19:36,800
Okay.

259
00:19:36,800 --> 00:19:42,680
So empathy, I only, in my talk, I focus mostly on the emotion recognition part because that

260
00:19:42,680 --> 00:19:47,920
is hard until we can recognize the emotion, we don't know how to react, right?

261
00:19:47,920 --> 00:19:50,840
So I focus on that.

262
00:19:50,840 --> 00:19:56,640
But the response part, I talked a little bit at the end that we're trying to learn the

263
00:19:56,640 --> 00:20:04,280
appropriate response as well from data, so also using deep learning.

264
00:20:04,280 --> 00:20:11,560
You mentioned healthcare as a use case, I think there's often, for a while now, people

265
00:20:11,560 --> 00:20:13,760
have talked about a customer service use case.

266
00:20:13,760 --> 00:20:14,760
Right, right.

267
00:20:14,760 --> 00:20:15,760
Sure, sure.

268
00:20:15,760 --> 00:20:19,760
You know, the, the, the whole line will recognize when you're getting frustrated.

269
00:20:19,760 --> 00:20:20,760
Sure, sure.

270
00:20:20,760 --> 00:20:30,720
In fact, at AT&T Bell Labs in the 90s already, they, they, a group worked, came out with

271
00:20:30,720 --> 00:20:33,400
the system called How May I Help You?

272
00:20:33,400 --> 00:20:40,280
So when you call the AT&T line, it's a, it's a virtual system, virtual operator that

273
00:20:40,280 --> 00:20:43,360
talks to you first and says, How May I Help You?

274
00:20:43,360 --> 00:20:47,760
And you basically say whatever you want and then it goes to different categories.

275
00:20:47,760 --> 00:20:48,760
Yeah.

276
00:20:48,760 --> 00:20:50,920
And that is the intention classifier.

277
00:20:50,920 --> 00:20:51,920
Okay.

278
00:20:51,920 --> 00:20:57,640
And also at the time, AT&T, AT&T had internal programs to analyze all this call center

279
00:20:57,640 --> 00:21:01,360
data to see whether people upset, they're happy or not.

280
00:21:01,360 --> 00:21:04,400
So that is already the beginning of emotion recognition.

281
00:21:04,400 --> 00:21:08,240
That was my first contact with emotion recognition.

282
00:21:08,240 --> 00:21:12,880
And it was for, for customer service, indeed, it is a big area.

283
00:21:12,880 --> 00:21:18,600
But I don't get the sense that it's widely deployed or at least not in a way that I would

284
00:21:18,600 --> 00:21:23,200
see as an every person, maybe it's more at, yeah, it's at the back end.

285
00:21:23,200 --> 00:21:26,280
So this is the thing because it's not consumer facing.

286
00:21:26,280 --> 00:21:31,960
It's really for, it's really in the, in the area, the realm of data analytics.

287
00:21:31,960 --> 00:21:39,120
So it's more of a tool used by corporations to improve the efficiency of their call centers.

288
00:21:39,120 --> 00:21:40,640
And a performance management.

289
00:21:40,640 --> 00:21:42,880
Performance, yeah, they do do that.

290
00:21:42,880 --> 00:21:48,160
There are companies that provide technology to customer service.

291
00:21:48,160 --> 00:21:49,160
Okay.

292
00:21:49,160 --> 00:21:50,160
Yeah.

293
00:21:50,160 --> 00:21:51,160
Yeah.

294
00:21:51,160 --> 00:21:52,160
Yeah.

295
00:21:52,160 --> 00:22:07,320
So you also talked about the use of convolutional neural nets and recognizing emotion and kind

296
00:22:07,320 --> 00:22:15,080
of drew some interesting correlations across, you basically that the CNNs were able to functionally

297
00:22:15,080 --> 00:22:17,920
approximate the cochlear functionality.

298
00:22:17,920 --> 00:22:18,920
Yeah.

299
00:22:18,920 --> 00:22:23,640
And some perception system, indeed, that's what we found.

300
00:22:23,640 --> 00:22:29,880
So I think my talk, I started out by saying, okay, traditionally speech recognition, look

301
00:22:29,880 --> 00:22:35,920
at these human, human, some perception system and try to imitate that.

302
00:22:35,920 --> 00:22:42,280
But we sort of hit the bottleneck and we had to move away from that and go with the information

303
00:22:42,280 --> 00:22:47,640
theoretic approach where we actually try, we don't try to imitate human model at all.

304
00:22:47,640 --> 00:22:54,000
And what's interesting with CNN is that a lot of times CNN or other deep neural net are

305
00:22:54,000 --> 00:22:56,160
being used as a black box.

306
00:22:56,160 --> 00:23:02,920
So we know it works, we don't know why, and humans being humans are always interesting

307
00:23:02,920 --> 00:23:03,920
knowing why.

308
00:23:03,920 --> 00:23:04,920
Right.

309
00:23:04,920 --> 00:23:10,520
And in fact, there's a practical reason is that if you ever want to commercialize a technology

310
00:23:10,520 --> 00:23:16,040
like that, to provide to your customer, they want to know why, what it's learning.

311
00:23:16,040 --> 00:23:25,080
So we then took a look at the CNN, different layers of CNN and saw that it was actually,

312
00:23:25,080 --> 00:23:32,720
as I mentioned in my talk, that it's basically approximating the future bank in our cochlea

313
00:23:32,720 --> 00:23:35,240
that's connected to the auditory system.

314
00:23:35,240 --> 00:23:40,560
And then we also saw that it's picking up on the amplitude, the peaks in the amplitude

315
00:23:40,560 --> 00:23:42,880
that correspond to different emotions.

316
00:23:42,880 --> 00:23:47,440
Now we thought that was very interesting that we could actually see what's going on.

317
00:23:47,440 --> 00:23:54,160
For example, it was always, CNN was first applied to image recognition.

318
00:23:54,160 --> 00:24:01,240
They used like seven, eight, nine layers of CNN to achieve that purpose.

319
00:24:01,240 --> 00:24:05,800
And in image recognition, they were able to see that each layer, so for example, one

320
00:24:05,800 --> 00:24:12,880
layer is recognizing the edges of image and the other layer is looking nice, maybe something

321
00:24:12,880 --> 00:24:15,520
else, some features on your face and all that.

322
00:24:15,520 --> 00:24:17,720
So it's all very obvious and really nice.

323
00:24:17,720 --> 00:24:24,600
And we were never able to explain how the neural networks on speech and language.

324
00:24:24,600 --> 00:24:28,720
So it was interesting to see why works on emotion.

325
00:24:28,720 --> 00:24:32,600
We still are not able to figure out what it's doing on languages.

326
00:24:32,600 --> 00:24:35,600
So each layer is recognizing.

327
00:24:35,600 --> 00:24:41,880
We were hoping that each layer will correspond to some linguistic functions, such as syntax

328
00:24:41,880 --> 00:24:47,440
or we haven't seen anything that neat yet.

329
00:24:47,440 --> 00:24:48,440
So.

330
00:24:48,440 --> 00:24:49,440
Interesting.

331
00:24:49,440 --> 00:24:50,440
Yeah.

332
00:24:50,440 --> 00:24:52,840
So it's not blinding learning something.

333
00:24:52,840 --> 00:24:58,080
It's learning something which has a physical meaning.

334
00:24:58,080 --> 00:25:04,720
But you made the point that it's also an error to correlate it too tightly to brain function

335
00:25:04,720 --> 00:25:06,240
because that doesn't really work.

336
00:25:06,240 --> 00:25:07,240
It doesn't.

337
00:25:07,240 --> 00:25:08,240
It's not.

338
00:25:08,240 --> 00:25:09,240
Because no.

339
00:25:09,240 --> 00:25:13,640
So even though I said it approximates human perception system, it's really the hearing

340
00:25:13,640 --> 00:25:14,640
system.

341
00:25:14,640 --> 00:25:15,640
Right?

342
00:25:15,640 --> 00:25:16,640
It's not the understanding part.

343
00:25:16,640 --> 00:25:18,280
It's the hearing system.

344
00:25:18,280 --> 00:25:21,320
And we know exactly how our hearing system function.

345
00:25:21,320 --> 00:25:22,920
We know very, very well.

346
00:25:22,920 --> 00:25:23,920
We don't guess.

347
00:25:23,920 --> 00:25:30,440
But how our mind's functioning, understanding the meaning, we don't know.

348
00:25:30,440 --> 00:25:38,000
We're activating our, you know, I mentioned 100 billion neurons and 100 trillion synopsis

349
00:25:38,000 --> 00:25:42,520
to get that until we have neural network of their size.

350
00:25:42,520 --> 00:25:50,480
It's hard to come up with something similar to human cognitive ability.

351
00:25:50,480 --> 00:25:51,480
So it's not.

352
00:25:51,480 --> 00:25:52,480
We don't have that.

353
00:25:52,480 --> 00:25:59,280
So that's why I say it's no coincidence that we can use.

354
00:25:59,280 --> 00:26:03,240
We can't explain what CNN is doing for perception.

355
00:26:03,240 --> 00:26:08,640
So speech recognition and emotional recognition are both perception problems.

356
00:26:08,640 --> 00:26:16,520
Perception is actually easy because we understand human perception, how our skin feels the

357
00:26:16,520 --> 00:26:17,520
temperature.

358
00:26:17,520 --> 00:26:20,800
We actually understand the physics of that very, very well.

359
00:26:20,800 --> 00:26:25,720
But once you go into understanding, which is language understanding, which is the trans,

360
00:26:25,720 --> 00:26:31,160
you know, if I want to use a noisy channel model, it would be from words to meaning.

361
00:26:31,160 --> 00:26:36,680
Once we're getting to the realm of that, we are kind of clueless to how humans.

362
00:26:36,680 --> 00:26:41,960
So there are a lot of linguistic theories about how humans think, how humans understand.

363
00:26:41,960 --> 00:26:46,640
But you know, for every linguistic theory, there are other people who say no.

364
00:26:46,640 --> 00:26:55,400
So there's no, no, there's no scientific truth that we all share right now about how human

365
00:26:55,400 --> 00:26:59,760
mind understands language or understands anything else.

366
00:26:59,760 --> 00:27:03,240
How do you know a video of a cat is funny or not?

367
00:27:03,240 --> 00:27:05,840
How does our mind interpret humor?

368
00:27:05,840 --> 00:27:06,840
We actually don't know.

369
00:27:06,840 --> 00:27:09,160
A lot of marketers would like me to be able to.

370
00:27:09,160 --> 00:27:14,320
So yeah, how do you predict, for example, one thing we were asked since we could classify

371
00:27:14,320 --> 00:27:19,200
music, we were asked by a company and say, hey, can you predict whether a song is going

372
00:27:19,200 --> 00:27:21,200
to be a hit or not?

373
00:27:21,200 --> 00:27:26,200
Maybe we could predict that, wouldn't that be amazing?

374
00:27:26,200 --> 00:27:33,160
No, you know, because you know, even we look at big data of all the past hit songs, it

375
00:27:33,160 --> 00:27:38,640
can learn, it cannot predict what the next one, not yet, maybe.

376
00:27:38,640 --> 00:27:43,720
So all we can do right now is use engineering models to approximate input and output.

377
00:27:43,720 --> 00:27:48,800
You know what I mean, it's really a mimicry, like just looking at this input, can we come

378
00:27:48,800 --> 00:27:53,120
out with output that's similar to the truth?

379
00:27:53,120 --> 00:28:00,120
So we're not, we're not in any way near to imitating human minds and that will be, even

380
00:28:00,120 --> 00:28:05,080
though, so even the term deep learning is a new term for something that has been around

381
00:28:05,080 --> 00:28:06,080
for a while.

382
00:28:06,080 --> 00:28:08,880
So that's a particular kind of machine learning.

383
00:28:08,880 --> 00:28:09,880
Right.

384
00:28:09,880 --> 00:28:16,280
It is, it is no deeper, let's say that are the kind of machine learning methods, it's

385
00:28:16,280 --> 00:28:25,720
a terminology and also neural network, it's a very, very rudimentary kind of neural network,

386
00:28:25,720 --> 00:28:31,720
you know, for speech recognition that might be tens of thousands of neurons and for emotional

387
00:28:31,720 --> 00:28:35,160
recognition, much, much fewer.

388
00:28:35,160 --> 00:28:39,520
So that cannot compare to the human brain.

389
00:28:39,520 --> 00:28:45,840
One of the things that I noticed in your presentation is that when you're doing

390
00:28:45,840 --> 00:28:52,080
the emotional, emotional recognition, you're mapping it to kind of the, you know, these

391
00:28:52,080 --> 00:28:56,160
names, common names we have for emotion, angry sad, whatever.

392
00:28:56,160 --> 00:29:01,360
Is that model even too simple or is there an underlying more nuanced model for emotion?

393
00:29:01,360 --> 00:29:03,360
I'm not sure they're.

394
00:29:03,360 --> 00:29:09,160
Yeah, well, so there's a lot of research done on the underlying model for emotion, such

395
00:29:09,160 --> 00:29:16,120
as valence arousal, you know, and there are models that try to predict that first before

396
00:29:16,120 --> 00:29:18,320
they predict the final label.

397
00:29:18,320 --> 00:29:22,520
And to interrupt you, because I saw the slide, but these folks haven't, valence arousal is

398
00:29:22,520 --> 00:29:25,160
valence is like the strength of the emotion and-

399
00:29:25,160 --> 00:29:27,080
No arousal is the strength.

400
00:29:27,080 --> 00:29:29,760
Valence is the positive and negative of the emotion.

401
00:29:29,760 --> 00:29:36,200
And so the various, you know, angry sad, cool, it's a combination of, yeah, they are a

402
00:29:36,200 --> 00:29:41,520
combination of different values of valence and arousal, this is one emotion theory.

403
00:29:41,520 --> 00:29:47,640
So these are models that psychologists came up with to try to organize what we know

404
00:29:47,640 --> 00:29:49,680
about emotion.

405
00:29:49,680 --> 00:29:56,080
And so it's just human minds, we're symbolic animals, you know, we need to have names,

406
00:29:56,080 --> 00:29:58,160
we give names to everything.

407
00:29:58,160 --> 00:30:02,640
So it's just easier for us to give a name to emotion, so we know what we're talking

408
00:30:02,640 --> 00:30:11,200
about, rather than give this vague, you know, number, valence arousal, if I tell you, oh,

409
00:30:11,200 --> 00:30:15,080
this is valence arousal, this is this number, you wouldn't know what I'm talking about,

410
00:30:15,080 --> 00:30:20,440
if I say he's, you know, he's showing happiness on his face, you kind of, kind of, no.

411
00:30:20,440 --> 00:30:24,840
So it's kind of emotional recognition, it's kind of like speech recognition when it was

412
00:30:24,840 --> 00:30:28,960
only recognizing isolated words.

413
00:30:28,960 --> 00:30:31,640
It's oversimplified for sure.

414
00:30:31,640 --> 00:30:34,240
We're not good at all with emotion recognition.

415
00:30:34,240 --> 00:30:40,520
You saw my slides, the performance is nowhere near the ability, the performance of recognizing

416
00:30:40,520 --> 00:30:44,680
words, recognizing emotion is much harder right now.

417
00:30:44,680 --> 00:30:50,400
One reason you pointed out is that it's hard to define where emotion is, you know, for

418
00:30:50,400 --> 00:30:55,960
example, maybe it's easy to see if somebody's happy, but is he smiling because he's happy

419
00:30:55,960 --> 00:30:59,320
as he's smiling because he's trying to be polite.

420
00:30:59,320 --> 00:31:03,400
And also, what about emotions like frustration?

421
00:31:03,400 --> 00:31:05,200
How can you tell?

422
00:31:05,200 --> 00:31:11,840
Sure, some things are obvious, you know, when someone's frustrated, if they're rolling

423
00:31:11,840 --> 00:31:16,440
their eyes or something, but there are other times, you know, from the voice, from the tone

424
00:31:16,440 --> 00:31:24,680
of your voice, we can tell a lot of things, but we cannot tell everything all the time.

425
00:31:24,680 --> 00:31:28,720
So it is along with the goal, you saw the accuracy, the accuracy these days, even the

426
00:31:28,720 --> 00:31:36,640
best commercial systems, it's just like 60 percent, you know, and most 70 percent.

427
00:31:36,640 --> 00:31:39,880
And speech recognition, we're talking about above 90 percent, right?

428
00:31:39,880 --> 00:31:45,240
So there's a long way to go for emotional recognition, and especially some more complex

429
00:31:45,240 --> 00:31:57,040
emotions, like humor, sarcasm, yeah, sarcasm, humor and, and deceit, you know, is this

430
00:31:57,040 --> 00:31:59,440
person lying?

431
00:31:59,440 --> 00:32:04,040
And there are colleagues in the field who have come up with systems that, that has 70

432
00:32:04,040 --> 00:32:09,280
some percent accuracy in detecting deceit and actually performs better than humans, turns

433
00:32:09,280 --> 00:32:13,320
out we are now very good at detecting deceit, we're not good at all.

434
00:32:13,320 --> 00:32:16,720
And humans are not very good in recognizing emotions.

435
00:32:16,720 --> 00:32:17,720
Yeah.

436
00:32:17,720 --> 00:32:22,320
You know what we found with, when we did some human subject studies, is that we found

437
00:32:22,320 --> 00:32:31,400
that women are better, women are more empathetic than men, there was actually quantifiable.

438
00:32:31,400 --> 00:32:37,760
And then women can detect emotion across languages, languages, in languages they don't know.

439
00:32:37,760 --> 00:32:38,760
Interesting.

440
00:32:38,760 --> 00:32:42,560
Also better than men in the same language.

441
00:32:42,560 --> 00:32:46,680
So you can talk about why the reason, you know, we're programmed to be mothers, we must

442
00:32:46,680 --> 00:32:49,440
recognize the emotion of a baby early on.

443
00:32:49,440 --> 00:32:51,160
And I think there's some merit to that.

444
00:32:51,160 --> 00:32:59,520
So, although I'm not a anthropologist, I cannot prove that, but, yeah, so, interesting.

445
00:32:59,520 --> 00:33:04,360
And then the reasons, you know, you see there are a lot of women who are nurses, doesn't

446
00:33:04,360 --> 00:33:08,480
mean men cannot be, but just happen that way, right.

447
00:33:08,480 --> 00:33:19,520
A lot of the caregivers, women, you know, kindergarten teachers, you know, just nannies, you know.

448
00:33:19,520 --> 00:33:24,000
So if we want to build robots, we want them to be more like that, more empathetic.

449
00:33:24,000 --> 00:33:25,000
Right.

450
00:33:25,000 --> 00:33:26,000
Great.

451
00:33:26,000 --> 00:33:28,400
Well, thanks so much for taking the time to sit down with me.

452
00:33:28,400 --> 00:33:29,400
It was a great discussion.

453
00:33:29,400 --> 00:33:30,400
I really appreciate it.

454
00:33:30,400 --> 00:33:31,400
Thank you.

455
00:33:31,400 --> 00:33:32,400
Thank you.

456
00:33:32,400 --> 00:33:36,800
Would you like to share how folks can find your research, or are you on any of the social

457
00:33:36,800 --> 00:33:37,800
media networks?

458
00:33:37,800 --> 00:33:48,080
Well, it's easy to Google my name, which is Pascal Fung, P-A-S-E-A-L-E, Fung is F-U-N-G.

459
00:33:48,080 --> 00:33:54,600
If you Google my name, you come to my website, which lists all my our publications, our projects

460
00:33:54,600 --> 00:33:55,760
and all that.

461
00:33:55,760 --> 00:33:59,560
And they can, they can email me via that website as well.

462
00:33:59,560 --> 00:34:00,560
Great.

463
00:34:00,560 --> 00:34:01,560
Great.

464
00:34:01,560 --> 00:34:02,560
Well, thanks so much.

465
00:34:02,560 --> 00:34:03,560
Thank you.

466
00:34:03,560 --> 00:34:06,800
All right, everyone, that's it for today's show.

467
00:34:06,800 --> 00:34:11,240
If you enjoyed this show or have something to add to the discussion, please leave a comment

468
00:34:11,240 --> 00:34:17,960
on the show notes page at twimmaleigh.com slash talk slash nine.

469
00:34:17,960 --> 00:34:24,680
Or tweet to me at at Sam Charrington or at twimmaleigh to let me know what you think.

470
00:34:24,680 --> 00:34:44,640
Thanks so much for listening and catch you next time.

