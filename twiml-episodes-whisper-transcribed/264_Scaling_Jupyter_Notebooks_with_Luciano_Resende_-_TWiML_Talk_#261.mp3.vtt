WEBVTT

00:00.000 --> 00:16.320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

00:16.320 --> 00:21.480
people, doing interesting things in machine learning and artificial intelligence.

00:21.480 --> 00:31.160
I'm your host Sam Charrington.

00:31.160 --> 00:35.280
This week on the podcast we're featuring a series of shows that highlight just a few of

00:35.280 --> 00:39.800
the great innovations and innovators at the intersection of three very important and

00:39.800 --> 00:46.280
familiar topics, data science, the Python programming language and open source software.

00:46.280 --> 00:49.960
To better understand our listeners' views on the importance of open source and the projects

00:49.960 --> 00:54.520
and players in this space, I'm conducting a survey, which I'd be very grateful if you

00:54.520 --> 00:57.040
took a moment to complete.

00:57.040 --> 01:01.760
To access the survey, visit Twimbleai.com slash Python survey.

01:01.760 --> 01:09.800
Please hit pause now and we'll wait for you to get back.

01:09.800 --> 01:15.880
That's twimbleai.com slash Python survey.

01:15.880 --> 01:19.840
Before we dive into the show, I'd like to send a huge thanks to our sponsor for this

01:19.840 --> 01:22.200
series IBM.

01:22.200 --> 01:26.680
Speaking of open source, IBM has a long history of engaging in and supporting open source

01:26.680 --> 01:32.160
projects that are important to enterprise data science, projects like Hadoop, Spark,

01:32.160 --> 01:35.680
Jupiter and Cubeflow to name just a few.

01:35.680 --> 01:41.640
IBM also hosts the IBM data science community, which is a place for enterprise data scientists

01:41.640 --> 01:47.320
looking to learn, share and engage with their peers and industry renowned practitioners.

01:47.320 --> 01:52.440
Here you'll find informative tutorials and case studies, Q&As with leaders in the field

01:52.440 --> 01:57.800
and a lively forum covering a variety of topics of interest to both beginning and experience

01:57.800 --> 01:59.480
data scientists.

01:59.480 --> 02:05.160
Check out and join the IBM data science community by visiting IBM.com slash community slash

02:05.160 --> 02:06.160
data science.

02:06.160 --> 02:09.760
All right, everyone.

02:09.760 --> 02:12.320
I am on the line with Luciano Recende.

02:12.320 --> 02:19.600
Luciano is an open source AI platform architect at IBM and part of the center for open source

02:19.600 --> 02:24.600
data and AI technology there, also known as code eight.

02:24.600 --> 02:27.320
Luciano, welcome to this weekend machine learning and AI.

02:27.320 --> 02:28.320
Hi, Sam.

02:28.320 --> 02:29.640
Thanks for having me.

02:29.640 --> 02:33.880
So Luciano is great to have you on the show and I'm looking forward to chatting with you

02:33.880 --> 02:40.280
about a project that you spent a lot of time working on in particular Jupiter notebook

02:40.280 --> 02:42.880
and the Jupiter notebook ecosystem.

02:42.880 --> 02:47.160
But before we dive into that, I'd love to hear a little bit more about your background

02:47.160 --> 02:52.680
and how you got started working at this, the intersection of open source and data and

02:52.680 --> 02:53.680
AI.

02:53.680 --> 02:54.680
Great.

02:54.680 --> 02:55.680
Let me start.

02:55.680 --> 02:58.880
So like I've been working with open source for a long time.

02:58.880 --> 03:03.880
I've been contributing to projects that are patching in other areas on the open source

03:03.880 --> 03:11.840
AI ecosystem for over 10 years and more towards getting into this Jupiter ecosystem part.

03:11.840 --> 03:18.520
A couple of years ago, we started seeing a more and more adoption of the notebooks as

03:18.520 --> 03:24.760
a way for the interactive development for data science and recently for AI as well.

03:24.760 --> 03:33.240
And when talking to external or internal customers, we had a need to basically scale that

03:33.240 --> 03:37.360
platform for a large number of concurrent users.

03:37.360 --> 03:43.160
So like you go to a financial institution and they might have like 500 data scientists

03:43.160 --> 03:47.880
that come in the morning will launch their notebook and they need to be able to do all

03:47.880 --> 03:52.640
the crunch of the data that they have to start getting insights from that.

03:52.640 --> 03:55.440
And we didn't have a very good solution for that.

03:55.440 --> 04:02.160
We also were looking to building the IPM platform, the Watson Studio and we needed a similar

04:02.160 --> 04:03.160
solution.

04:03.160 --> 04:07.320
And that's how we started the Jupiter Enterprise Gateway.

04:07.320 --> 04:13.680
Basically, what we do in Jupiter Enterprise Gateway, we are a lightweight, multi-tenant,

04:13.680 --> 04:19.920
scalable and secure gateway that enables Jupiter notebooks to start sharing resources across

04:19.920 --> 04:23.680
an Apache Spark cluster or a Kubernetes cluster.

04:23.680 --> 04:27.680
And we focus a lot on the enterprise and cloud use gays for that.

04:27.680 --> 04:37.400
I'm starting to hear quite a bit of interest on the part of enterprises and folks that

04:37.400 --> 04:42.640
are working in building out platforms and infrastructure for data science and machine

04:42.640 --> 04:52.560
learning teams about the idea of kind of standing up Jupiter notebook instances for their teams.

04:52.560 --> 04:58.960
As you kind of think about the way Jupiter is being used in large organizations, what

04:58.960 --> 05:02.720
are some of the big challenges that folks run into?

05:02.720 --> 05:09.320
So Jupiter notebooks, they are evolution from Python console.

05:09.320 --> 05:17.440
And because of that, we see it some limitations that came from that past instances.

05:17.440 --> 05:25.600
And Jupiter by default, it's a single user kind of like environment and all the kernels

05:25.600 --> 05:34.440
which are the actually run time of the data executes the code, it's default to run as a

05:34.440 --> 05:36.520
local process.

05:36.520 --> 05:42.240
And what we see is that it doesn't scale to the needs that we need on enterprise or large

05:42.240 --> 05:43.560
deployments.

05:43.560 --> 05:49.880
Then on on the Jupiter ecosystems, we started seeing a lot of like projects that are kind

05:49.880 --> 05:55.760
of like built as an add-on to Jupiter and start solving some of those limitations.

05:55.760 --> 06:01.880
And the idea is that you can start building itself service platform for data science and

06:01.880 --> 06:02.880
AI.

06:02.880 --> 06:10.640
So when we look, for example, Jupiter gives you the ability to start providing multi-user

06:10.640 --> 06:16.480
authentication for those notebooks, and then Jupiter Enterprise Gateway starts to scale

06:16.480 --> 06:21.920
the runtime and distribute that into either a Sparkless or Kubernetes cluster.

06:21.920 --> 06:26.560
And that's the direction that we are seeing things going to the Jupiter ecosystem at this

06:26.560 --> 06:27.560
days.

06:27.560 --> 06:29.840
Okay, yeah, that was going to be one of my questions.

06:29.840 --> 06:37.800
The differences between Jupiter Hub and Jupiter Enterprise Gateway, is it all about kind

06:37.800 --> 06:42.320
of where the back end kernels are run or are there other differences?

06:42.320 --> 06:52.800
So as I mentioned, we need a way to start launching an environment for multiple users,

06:52.800 --> 06:56.000
so provide a multi-tenant support.

06:56.000 --> 07:00.880
And what Jupiter Hub does, it's kind of like a launcher of a Jupiter server.

07:00.880 --> 07:07.680
So each user that requests an environment gets a Jupiter server runtime for doing

07:07.680 --> 07:10.000
its computation.

07:10.000 --> 07:15.080
Then when you get that environment, you then start using multiple notebooks.

07:15.080 --> 07:21.080
Those notebooks require a kernel to execute to the code.

07:21.080 --> 07:26.640
And then that's when Jupiter Enterprise Gateway comes into mind so that you can have a very

07:26.640 --> 07:32.920
lean Jupiter server environment launched by Jupiter Hub.

07:32.920 --> 07:39.240
And the runtime then, it goes across a cluster and you can start having multiple notebooks

07:39.240 --> 07:40.960
at the same time.

07:40.960 --> 07:45.600
But if you close a notebook, you let go those resources and make it available to other

07:45.600 --> 07:48.480
data scientists on the platform.

07:48.480 --> 07:54.480
It also enables you to start sharing more expensive resources, like for example, you're

07:54.480 --> 07:57.640
doing AI and you need GPUs.

07:57.640 --> 08:04.560
We provided a way to containerize that and then be able to associate the number of GPUs,

08:04.560 --> 08:09.000
for example, or the resources per kernel that it gets launched.

08:09.000 --> 08:13.640
And because those are decoupled, those come and go as you're using them and not necessarily

08:13.640 --> 08:16.240
the whole time that your notebook server is up.

08:16.240 --> 08:22.760
Okay, so it sounds like then the typical organization will use both Jupiter Hub as kind

08:22.760 --> 08:28.600
of the front end and then Enterprise Gateway as a way to distribute the backend kernels.

08:28.600 --> 08:32.360
But they're not alternative products, they're complimentary.

08:32.360 --> 08:35.760
They are definitely complimentary.

08:35.760 --> 08:39.360
They work very well together.

08:39.360 --> 08:40.360
Yes.

08:40.360 --> 08:47.200
So you mentioned both Spark and Kubernetes as services that you can use to provide the

08:47.200 --> 08:55.240
backend distributed computing framework for these kernels.

08:55.240 --> 08:58.040
What's the current mix that you see of those two options?

08:58.040 --> 09:00.640
Is one more popular than the other?

09:00.640 --> 09:07.320
I think it really depends on the type of workloads that you're processing.

09:07.320 --> 09:14.240
What I see Spark and use it is more for analytics where you have a lot of data and you might

09:14.240 --> 09:18.200
start processing crushing those data and then applying some kind of like a machine learning

09:18.200 --> 09:20.240
on top of that.

09:20.240 --> 09:27.960
And with the recent release that we have, the Enterprise Gateway 2.0, we support not

09:27.960 --> 09:35.760
only if you have like a legacy Spark deployment based on like a HDFS and I do distribution,

09:35.760 --> 09:38.920
but we also support Spark on Kubernetes.

09:38.920 --> 09:43.560
So if you're doing analytics, you're most likely going to use Spark runtime and that's

09:43.560 --> 09:47.080
why we initially started supporting that.

09:47.080 --> 09:56.040
With more of the AI workloads, what we have seen is a lot of like AI platforms for training

09:56.040 --> 10:03.960
models and deploying models all based on Kubernetes and the reason I think for that is that

10:03.960 --> 10:11.840
Kubernetes provides you kind of like the right sandbox with containers and that enables

10:11.840 --> 10:19.280
you to, for example, start a Python kernel, but if you're doing a TensorFlow model, you'll

10:19.280 --> 10:26.240
just kind of say, okay, I want to Python kernel based on a TensorFlow image and you start

10:26.240 --> 10:30.920
mixing and matching your environment with the kernel that you need and then you can have

10:30.920 --> 10:38.960
a very specific environment and close it in the container and another AI engineer might

10:38.960 --> 10:45.560
completely have a completely different one and Kubernetes manage and although deployment

10:45.560 --> 10:50.840
anything that needs to happen on that, and we manage the lifecycle and give the connectivity

10:50.840 --> 10:59.880
to notebooks to be able to actually execute and train your models on a Kubernetes environment.

10:59.880 --> 11:06.880
Now, I've had several conversations with organizations, Airbnb is one that comes to mind

11:06.880 --> 11:17.440
that have invested pretty heavily in customizing Jupyter Notebook and in some ways, maybe they've

11:17.440 --> 11:23.840
duplicated what you've done or went to build it because what you've done either wasn't

11:23.840 --> 11:31.360
ready on their time frame or didn't offer features that they needed and I'm wondering

11:31.360 --> 11:41.480
in general, where do you see folks investing heavily in customizing Jupyter to build it

11:41.480 --> 11:49.080
into their workflows? What are some of the key gaps maybe in what's currently available

11:49.080 --> 11:55.440
or some of the ways that organizations might want it to be more tightly integrated?

11:55.440 --> 12:00.960
What I've been seeing in the industry is a lot of customization around your internal

12:00.960 --> 12:05.520
environment and integration with some of the tools.

12:05.520 --> 12:12.160
You might need to get some of the outputs of your notebooks to a given platform that

12:12.160 --> 12:18.560
is available only internally and that's a lot of the things that I've been seeing.

12:18.560 --> 12:25.840
When you go, I think with Jupyter Lab, which is a new UI for the Jupyter Notebook, those

12:25.840 --> 12:32.000
kinds of customizations and ability to start bringing up widgets are going to become

12:32.000 --> 12:41.360
much more flexible and easy to deploy or customize and then in terms of runtime, I think

12:41.360 --> 12:49.920
what I've been seeing is a lot of like trying to accomplish multi-tenant by deploying multiple

12:49.920 --> 12:56.720
environments for different users, but it's a duplication of that environment and not like

12:56.720 --> 13:04.080
a decoupling the actual execution of the kernel with the Jupyter Notebook and that's

13:04.080 --> 13:06.720
how Jupyter and Fifthgate became to do.

13:06.720 --> 13:12.640
Can you elaborate on that last part when you say it's duplication and what sense?

13:12.640 --> 13:23.400
So Jupyter Hub, as you were saying, was available a lot prior than enterprise data and initially

13:23.400 --> 13:28.520
what I've seen people doing before enterprise data is that they will start a Jupyter

13:28.520 --> 13:36.160
server for each user and then that instance of the Jupyter server will have to have all

13:36.160 --> 13:43.440
the resources required for any type of workload that the user needs to do.

13:43.440 --> 13:53.200
That is an okay solution, but all those resources get locked for a long time and in that case,

13:53.200 --> 13:59.280
I think a lot of those resources might be in an idle position and not be able to be reused

13:59.280 --> 14:05.600
by other members of your platform or other data scientists that wants to do some computations.

14:05.600 --> 14:12.240
You also mentioned integrating with internal systems and security and things like that

14:12.240 --> 14:15.840
as a big motivator for folks.

14:15.840 --> 14:23.840
The Jupyter Notebook and this ecosystem, APIs, do they lend themselves to folks doing this

14:23.840 --> 14:26.960
kind of integration, is it difficult?

14:26.960 --> 14:33.240
Where is it on the scale of built for this and easy to do or really difficult to do and

14:33.240 --> 14:36.000
requires a lot of jumping through hoops?

14:36.000 --> 14:37.320
That's a good question.

14:37.320 --> 14:40.360
Let me look into a different perspective here.

14:40.360 --> 14:48.400
Imagine your data scientists and you're running your workloads with Jupyter.

14:48.400 --> 14:54.400
You will not see any difference in terms of user experience if you're using enterprise

14:54.400 --> 14:57.960
gateway on the backhand or not.

14:57.960 --> 15:03.680
If you are a kind of like DevOps and you are providing that environment for your data

15:03.680 --> 15:11.160
scientists, pretty much what you need to do is customize the kernel definition and instead

15:11.160 --> 15:17.480
of saying that it's just a local kernel, you will switch to a different lifecycle manager

15:17.480 --> 15:23.320
and say, this is a remote kernel, we call it process proxy, we have different lifecycle

15:23.320 --> 15:29.680
managers for each resource manager so we have one for yarn, we will have one for Kubernetes

15:29.680 --> 15:35.880
and that is extensible so people have been coming in and providing others as well and

15:35.880 --> 15:41.800
depending on the one that you choose, the resource manager that you choose, you can then

15:41.800 --> 15:44.480
add additional information needed.

15:44.480 --> 15:50.080
So for example, if you choose yarn, you might want to pass like your Spark home information.

15:50.080 --> 15:56.280
If you choose Kubernetes, you might not need anything specific or depends on how it goes

15:56.280 --> 15:58.800
into terms of configurations.

15:58.800 --> 16:06.880
But once that is properly configured, we handle most of the hard work internally on the

16:06.880 --> 16:12.640
tool by automating all the lifecycle like the starting the kernel remotely figuring out

16:12.640 --> 16:19.640
where it actually mounted, providing the configuration back to Jupyter so that the end-to-end

16:19.640 --> 16:23.520
communication can be connected.

16:23.520 --> 16:30.880
So it's to certain extent not very hard to get an environment going and we also have

16:30.880 --> 16:39.240
been doing lots of tools like SQL scripts where you can deploy enterprise gateway from scratch

16:39.240 --> 16:47.560
or from an existing environment for both Spark and Kubernetes run types.

16:47.560 --> 16:56.720
So the work to create this remotely executable or invocable kernel, was that already in existence

16:56.720 --> 17:02.560
in the Jupyter ecosystem or is that part of the work that enterprise gateway needed

17:02.560 --> 17:07.040
to do in order to exist really?

17:07.040 --> 17:12.320
Yes, so the kernels in itself is nothing new that we created.

17:12.320 --> 17:17.680
The kernels were available before in Jupyter and today I think we have a list of like over

17:17.680 --> 17:19.680
100 kernels available.

17:19.680 --> 17:25.920
The kernels, they will abstract the languages that you are using.

17:25.920 --> 17:31.280
So for example, if you are doing Python, you have a Python kernel, if you're doing

17:31.280 --> 17:33.600
Scala, you have a Scala kernel.

17:33.600 --> 17:38.720
What enterprise gateway brings is the ability to decouple those that originally were running

17:38.720 --> 17:41.800
as local process to be run remotely.

17:41.800 --> 17:45.520
That's the main thing that enterprise gateway brings.

17:45.520 --> 17:53.960
And around that came a lot of requirements such as security, how to handle multi-tenant

17:53.960 --> 17:58.760
so that the users that are actually requesting the kernel are the ones that are running the

17:58.760 --> 18:01.000
kernels on the ground times as well.

18:01.000 --> 18:08.200
And we started leveraging the capabilities that we had to handle all of those additional

18:08.200 --> 18:10.800
requirements in enterprise gateway.

18:10.800 --> 18:18.280
So occasionally here from folks that are using Kubernetes for these types of workloads that

18:18.280 --> 18:24.080
they run into challenges with the kind of out-of-the-box Kubernetes scheduler.

18:24.080 --> 18:27.840
Is that something that you've seen for running these kernels?

18:27.840 --> 18:34.360
We haven't experienced any particular issue with the schedule at the moment.

18:34.360 --> 18:38.440
We also haven't heard from any users.

18:38.440 --> 18:44.480
I think most of the users at the moment are going more for the Kubernetes environment and

18:44.480 --> 18:48.480
we haven't heard anything specifically to that from them yet.

18:48.480 --> 18:53.600
I guess that one of the other things that I hear from folks about the way they want to

18:53.600 --> 19:04.120
use Jupyter is to integrate it with, for example, more tightly with Git repositories and shared

19:04.120 --> 19:11.400
file systems and kind of have these resources easily accessible by data scientists and machine

19:11.400 --> 19:16.120
learning engineers, kind of wherever they spin up their notebooks.

19:16.120 --> 19:18.840
Is that something that you see as well?

19:18.840 --> 19:24.720
And is it a piece of what's offered by the current notebook ecosystem?

19:24.720 --> 19:28.280
That's definitely something that I hear a lot.

19:28.280 --> 19:36.640
I think the Jupyter notebook and as well the new UI Jupyter lab have the ability to have

19:36.640 --> 19:39.480
the extensions built on top of that.

19:39.480 --> 19:46.520
Particularly on the Jupyter lab, I've seen, for example, integration with Git repositories

19:46.520 --> 19:54.000
and you can easily or transparently get things back and forth based on how you're progressing

19:54.000 --> 19:55.400
with your notebook.

19:55.400 --> 19:59.880
I think another thing that I also hear a lot, and currently I don't think it's available

19:59.880 --> 20:06.480
or at least not easily available on the open source, is the ability to start doing collaboration.

20:06.480 --> 20:12.120
That is something that, like sharing and having multiple people looking to the notebook

20:12.120 --> 20:14.280
and maybe trying to collaborate on that.

20:14.280 --> 20:18.440
I think it's one thing that really needs to come and be available on the open source

20:18.440 --> 20:21.720
of the Jupyter ecosystem and that's not available today.

20:21.720 --> 20:27.400
Yeah, I think Google Collab is an offering that comes to mind that does a pretty nice

20:27.400 --> 20:34.920
job at showing what a collaborative notebook experience might look like.

20:34.920 --> 20:40.440
And I can imagine that's kind of creating a lot of demand for folks that want to use

20:40.440 --> 20:45.120
something similar, but in a native Jupyter type of an environment.

20:45.120 --> 20:46.120
That is correct.

20:46.120 --> 20:52.480
I mean, I've seen commercial solutions that start offering those, but in the open source

20:52.480 --> 20:58.640
or more like on the academia, we haven't seen an open source solution that handles that.

20:58.640 --> 20:59.640
Okay.

20:59.640 --> 21:07.600
You mentioned that there are hundreds of kernels for Jupyter, which I actually find kind

21:07.600 --> 21:08.600
of surprising.

21:08.600 --> 21:14.760
I mean, when I click down that kernels button, it only ever says, I thought, two or three.

21:14.760 --> 21:23.720
I'm wondering, to what extent my experience is reflective broadly or in general, how much

21:23.720 --> 21:31.000
of the Jupyter ecosystem is kind of Python centric and is Python users and how much of

21:31.000 --> 21:35.960
it folks that are trying to do other things beyond Python?

21:35.960 --> 21:42.720
I think that we at least commercially like on more enterprise environments, what we have

21:42.720 --> 21:51.160
seen is Python, the number one, R, the second, and those are mostly data scientists.

21:51.160 --> 21:57.440
Then some data engineers, particularly the ones working on analytics workloads, they

21:57.440 --> 22:03.000
also use the Scala for particular integrating with Spark.

22:03.000 --> 22:08.680
And you mostly see Python because Python, like if you have an environment like an account

22:08.680 --> 22:15.720
or any other way, Python most likely will be natively available for you.

22:15.720 --> 22:22.400
The other kernels are, you can easily install all those, they will just after installation

22:22.400 --> 22:28.200
start appearing as one of the options into your Jupyter notebook environment.

22:28.200 --> 22:36.320
And particularly in academia, then you start seeing Julia, C++ and other kernels that are

22:36.320 --> 22:39.360
very used on that community.

22:39.360 --> 22:44.800
So that's why you have at least like over a hundred kernels available today.

22:44.800 --> 22:45.800
Okay.

22:45.800 --> 22:54.600
And for folks that are, can you maybe are there specific companies or examples that you

22:54.600 --> 23:01.440
can share about what folks have managed to do with the enterprise gateway?

23:01.440 --> 23:06.120
One of the examples that I can give you internally is IBM Watson Studio.

23:06.120 --> 23:11.800
In order to be able to scale the notebook environment that we provide as part of Watson Studio,

23:11.800 --> 23:18.240
we use enterprise gateway to not only remote to the kernels, but also to give you a choice

23:18.240 --> 23:20.560
on where you want to run those kernels.

23:20.560 --> 23:26.200
We have different run times that we support and you can select those run times as where

23:26.200 --> 23:33.960
you want to run where you want to have the execution of the notebook runtime.

23:33.960 --> 23:39.960
Another example, and this is just from strata, we were at strata yesterday and PayPal was

23:39.960 --> 23:46.400
giving a very interesting presentation on how they were using enterprise gateway to be

23:46.400 --> 23:51.720
able to view the internal platform for data scientists and AI.

23:51.720 --> 23:53.880
So these are two examples that comes to mind.

23:53.880 --> 23:58.160
There are probably more, but particularly because we are open source, we only hear from

23:58.160 --> 24:04.640
them when they have an issue that gets reported or maybe like to a conference or something

24:04.640 --> 24:05.640
like that.

24:05.640 --> 24:07.600
Is it currently at a production ready?

24:07.600 --> 24:11.800
State sounds like PayPal at least in IBM or using it in production.

24:11.800 --> 24:13.920
How mature would you say it is?

24:13.920 --> 24:21.560
So we have two code streams that we are maintaining at Jupyter Enterprise Gateway.

24:21.560 --> 24:29.120
We have what we call the OneX release, which supports Spark, is mostly focused on analytics

24:29.120 --> 24:35.200
workloads and that has been stable and being used internally by products and have been

24:35.200 --> 24:38.600
available in the open source for a long time.

24:38.600 --> 24:43.880
Probably over a year or around close to a year and a half.

24:43.880 --> 24:50.400
And we have maintaining that in terms of like any new bugs that we see around that or any

24:50.400 --> 24:55.360
dependencies that needs to be updated for security reasons or anything like that.

24:55.360 --> 25:02.120
In parallel, we then are working on our tool release, which two days ago, we launched

25:02.120 --> 25:06.960
the tool RC1 release candidate one.

25:06.960 --> 25:12.160
And that brings a lot of innovation that we are doing on Kubernetes environment, having

25:12.160 --> 25:18.760
said that based on the questions and issues that we are getting, we have seen a lot of people

25:18.760 --> 25:26.240
experimenting, going more towards that direction and using the tool already to view their analytics

25:26.240 --> 25:29.200
environments on top of Kubernetes.

25:29.200 --> 25:36.920
So hopefully that will also be available as a GA in the next couple of weeks, based on

25:36.920 --> 25:39.520
whatever feedback we get from the RC1.

25:39.520 --> 25:45.360
And is the Enterprise Gateway functionality independent of the kernel that's being used

25:45.360 --> 25:49.240
or there's some limitations or dependencies there?

25:49.240 --> 25:53.120
So it is supposed to be independent.

25:53.120 --> 26:00.520
What we have done, we created a layer called kernel launcher that gives us the ability

26:00.520 --> 26:08.600
to enhance the kernels functionality, particularly for startup and shutdown.

26:08.600 --> 26:16.200
So we have tested a lot with Python, DR, and Scala kernels, which are the most requested

26:16.200 --> 26:19.160
and the most used for us.

26:19.160 --> 26:23.480
And things that we do is like, for example, if you are starting a kernel and that kernel

26:23.480 --> 26:29.760
is connecting to Spark, we create, for example, a parallel thread to start the Spark context.

26:29.760 --> 26:35.720
So it gives you a better and quicker user experience for the startup of the kernel.

26:35.720 --> 26:42.840
We need the ability to provide remote signaling so you can hit stop the kernel and things like

26:42.840 --> 26:43.840
that.

26:43.840 --> 26:50.480
And we need to flow that externally to a remote kernel and the launcher does that as well.

26:50.480 --> 26:58.520
But that kernel launcher can be easily ported or used for a different kernel.

26:58.520 --> 27:03.800
So the addition of new kernels shouldn't be very hard to incorporate into Enterprise

27:03.800 --> 27:04.800
Gateway.

27:04.800 --> 27:14.200
Here's about the requirements that users have when architecting environments for Enterprise

27:14.200 --> 27:15.600
Gateway.

27:15.600 --> 27:22.880
One of the things that comes to mind, for example, is when you're running your kernel locally,

27:22.880 --> 27:25.840
you've got typically data locally.

27:25.840 --> 27:31.400
If you're starting to remote your kernel, then the kernels operating against data that's

27:31.400 --> 27:39.040
remote, I'm wondering if there are network or bandwidth considerations or any kinds of

27:39.040 --> 27:45.640
issues that an organization might want to think about as they're architecting an Enterprise

27:45.640 --> 27:54.640
Gateway environment and generally how straightforward that process is or their tools that are

27:54.640 --> 27:58.880
available for capacity planning, things like that.

27:58.880 --> 28:02.720
I completely agree with the type of considerations that you're bringing up.

28:02.720 --> 28:09.000
I just, from what I'm seeing, these are not specific to when you're trying to deploy Enterprise

28:09.000 --> 28:10.000
Gateway.

28:10.000 --> 28:11.680
And it's a request more in general.

28:11.680 --> 28:18.840
So if you're doing analytics, the data will be most likely in a Spark cluster, but in

28:18.840 --> 28:25.720
a distributed file system like HDFS or some kind of like object storage environment.

28:25.720 --> 28:30.840
And you have like shuffling of data going back and forth, independent if you're using Enterprise

28:30.840 --> 28:32.320
Gateway or not.

28:32.320 --> 28:39.280
So that should have been part of like planning the deployment of Spark in particular.

28:39.280 --> 28:47.160
If we consider the Kubernetes environment, Kubernetes we most likely have multiple microservices

28:47.160 --> 28:56.480
being together to bring up a solution and you have similar type of constraints regarding

28:56.480 --> 29:00.880
data flowing and any other things like that.

29:00.880 --> 29:07.440
The easiest way that I've been seeing in more like a Kubernetes environment is we give

29:07.440 --> 29:16.280
the ability to do mattings into volumes into the specific container and by doing that,

29:16.280 --> 29:25.200
you can have the user environment available in that kernel image and that helps you alleviate

29:25.200 --> 29:34.160
most of these constraints by maybe providing data, maybe providing environment with specific

29:34.160 --> 29:40.760
packages that you might need for a given user on that mount and that should solve some

29:40.760 --> 29:41.760
of these problems.

29:41.760 --> 29:50.600
So it sounds like there's nothing particularly exotic or unknown or that needs special attention

29:50.600 --> 29:51.600
here.

29:51.600 --> 29:57.040
It's kind of the typical considerations when architecting a distributed environment for

29:57.040 --> 29:58.920
these kinds of workloads.

29:58.920 --> 29:59.920
Exactly.

29:59.920 --> 30:07.640
Granted that this capability that we're talking about particularly with Kubernetes is something

30:07.640 --> 30:12.440
that was just announced a couple of days ago or released a couple of days ago.

30:12.440 --> 30:19.280
But are there any other kinds of issues that you hear a lot about that folks run into

30:19.280 --> 30:26.680
that maybe you wish more people would think about before they embark on this path?

30:26.680 --> 30:32.680
Maybe a little bit unrelated but in the context of the Geoctery ecosystem, what I see a lot

30:32.680 --> 30:45.360
of discussions of what is the notion of a notebook and how it incorporates on your, let's

30:45.360 --> 30:49.960
say, model training pipeline and things like that.

30:49.960 --> 30:58.320
And most cases that I've seen, people would use the notebook for kind of like evaluation,

30:58.320 --> 31:03.000
they can look at the data and play with your model.

31:03.000 --> 31:09.600
But we'll actually convert the notebook into a Python file, for example, when they are

31:09.600 --> 31:15.160
kind of like ready to do a big training or start getting close to go to production.

31:15.160 --> 31:21.040
In my mind, the notebooks can become a little bit more kind of like first class citizens

31:21.040 --> 31:27.560
on those pipelines for model training and actually be one of the steps, actually be the

31:27.560 --> 31:34.200
notebook in itself and not necessarily go to a pipeline as a converted Python file.

31:34.200 --> 31:39.040
And I think once we have that, it's going to be much more productive, the data scientists

31:39.040 --> 31:46.800
can come up with something that then goes directly into the pipeline, produces the model and

31:46.800 --> 31:52.680
then can recreate the model later on if needed without any kind of like a collaboration maybe

31:52.680 --> 31:57.120
with a system engineer to convert to a Python or anything like that?

31:57.120 --> 32:04.680
And what needs to happen in order to get there from a kind of Jupyter ecosystem perspective?

32:04.680 --> 32:07.800
We have been seeing tools that it starts to enable that.

32:07.800 --> 32:19.400
I think it's mostly in a state of mind of like how people see the notebooks and they initially

32:19.400 --> 32:25.040
were a lot used for kind of like experimenting with data, trying to understand the data

32:25.040 --> 32:31.000
and trying to build models, but not necessarily maybe because those tools were not available

32:31.000 --> 32:32.880
yet go into production.

32:32.880 --> 32:36.960
And I think today we have seen like the Netflix coming out with paper, meal, a couple

32:36.960 --> 32:42.040
of other schedulers also coming out and supporting notebooks.

32:42.040 --> 32:48.520
My hope is that I will start seeing more of that being part of the pipeline for model

32:48.520 --> 32:49.520
development.

32:49.520 --> 32:52.200
All right, Luciano, this has been really great.

32:52.200 --> 32:59.920
I'm wondering if you've got any final thoughts or tips or ideas for folks that are using

32:59.920 --> 33:06.880
Jupyter, maybe as individual data scientists and want to use it more broadly in their

33:06.880 --> 33:07.880
enterprises?

33:07.880 --> 33:08.880
Oh, definitely.

33:08.880 --> 33:15.920
I think one of the things that I've seen and it's interesting that a lot of people still

33:15.920 --> 33:22.560
use and know about the Jupyter, what I call like classic UI, but not aware of like Jupyter

33:22.560 --> 33:23.560
Lab.

33:23.560 --> 33:32.400
Jupyter Lab gives you a lot of like flexibility and a better UI to do your model development

33:32.400 --> 33:37.800
and that gives you ability to start having like time in those multiple tabs all together

33:37.800 --> 33:42.680
and provide a much more integrated development environment for the model development.

33:42.680 --> 33:43.680
Awesome.

33:43.680 --> 33:46.200
If folks should definitely check that out.

33:46.200 --> 33:49.520
Well, Luciano, thanks so much for taking the time to chat with me.

33:49.520 --> 33:52.680
Okay, thanks, Sam.

33:52.680 --> 33:55.760
All right, everyone.

33:55.760 --> 33:57.800
That's our show for today.

33:57.800 --> 34:02.480
If you like what you've heard here, please do us a huge favor and tell your friends about

34:02.480 --> 34:03.640
the show.

34:03.640 --> 34:07.720
And if you haven't already hit that subscribe button yourself, make sure you do so you

34:07.720 --> 34:11.720
don't miss any of the great episodes we've got in store for you.

34:11.720 --> 34:15.080
As always, thanks so much for listening and catch you next time.

