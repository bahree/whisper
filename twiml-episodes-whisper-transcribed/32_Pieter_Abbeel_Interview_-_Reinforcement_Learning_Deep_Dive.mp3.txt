Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting
people doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
This week we continue our exploration into industrial AI.
Is that you ask, well in my forthcoming paper on the topic, I define industrial AI as
any application of AI relating to the physical operations or systems of an enterprise.
I go on to note that the focus of industrial AI is on helping enterprises monitor, optimize
or control the behavior of these operations and systems to improve their efficiency and performance.
When people hear the phrase industrial, they quickly jump to manufacturing, but it's
more than that. Industrial AI includes manufacturing applications like robotics and using computer
vision for quality control, but also applications like supply chain optimization and risk management,
warehouse automation, the monitoring and operation of building HVAC systems and much more.
For more information about industrial AI or the report, visit twimlai.com slash industrial AI.
This week our guest is Peter Rabiel, Assistant Professor at UC Berkeley, Research Scientist at Open AI
and Co-Founder of Great Scoop. Peter has an extensive background in AI research,
going way back to his days as Andrew Ng's first PhD student at Stanford.
His work today is focused on deep learning for robotics.
During this conversation, Peter and I really dig into reinforcement learning,
which is a technique for allowing robots or other AI's to learn through their own trial and error.
Before we jump in, a quick nerd alert. This conversation explores cutting edge research with
one of the leading researchers in the field, and as a result, it gets pretty technical at times.
I try to up level it when I can keep up myself, so hang in there. I promise that you'll learn
a ton if you keep with it. I could also use your feedback here. You want more or fewer of these
kinds of conversations. Let me know in the comments, along with any feedback, comments,
or questions you have about this episode. Before we jump in, a word about our sponsor.
I introduced Bonsai last week, and once again, I'd like to thank the team over there
for sponsoring this podcast series, as well as my forthcoming industrial AI research.
Bonsai offers an AI platform that empowers enterprises to build and deploy intelligent systems.
If you're trying to build AI-powered applications focused on optimizing and controlling the systems
in your enterprise, you should take a look at what they're up to. They've got a unique approach to
building AI models that let you use high-level code to model real-world concepts in your application,
automatically generate, train, and evaluate low-level models for your project,
using reinforcement learning and other technologies, and then easily integrate those models into your
applications and systems using APIs. You should check them out at bonds.ai, bons.ai, bons.ai,
and definitely let them know you appreciate their support of the podcast. And now on to the show.
All right, hello, everyone. I have got Peter Abil on the line. Peter is an associate professor
at UC Berkeley, a research scientist at OpenAI, and a co-founder at GradeScope. Peter, how are you?
Doing pretty well. How about you, Sam? I'm doing very well and excited to jump into this conversation.
In addition to all of those positions you hold, you are also at the cutting edge of a very interesting
field in machine learning called deep reinforcement learning. And I'm really looking forward to
learning a bunch about that, talking with you today. Why don't we get started by
having you tell us a little bit about your background and kind of how you got to where you are now
in your area of research? Sure, yeah. If I go pretty far back, actually, as a high school,
my excitement was mostly about physics and math, and from there engineering was a natural
next pick. And when I was wrapping up my bachelor's in engineering, I just found so many topics so
exciting and it was hard to choose, but ultimately it seemed artificial intelligence was the area
that could drive almost all other areas. It is by making progress in AI, it might be possible to help
know how to do many, many other things. And so that's kind of what drove me into AI and then got
me started on my masters and then PhD on the artificial intelligence with Andrew at Stanford.
And then from there I became professor at Berkeley and research scientist at OpenAI.
Well, working with Andrew, that's an impressive credential. He's done a lot of amazing things in
the field. It couldn't agree more. It's very fortunate that actually his first PhD students,
I saw him start from ground zero, which was amazing. Oh wow. Wow. And so what do you focus on today?
So a lot of what drives my work is trying to get robots out in the real world,
meaning beyond repetitive tasks, as you would see in current robot deployments.
And so what I think is key to get robots out in the real world and doing more flexible things is
to give them more intelligence and key to that will be for them to be able to learn rather than
us having to program them for every possible scenario they could encounter.
And so reinforcement learning obviously comes up in that. Is that just one of a number of techniques
that you're looking into to make robots smarter or what's kind of the landscape of things that you're
pursuing there? All right. So there's there's many ways to learn. If you look at the kind of
machine learning landscape, there is supervised learning, which is recognizing a pattern between
inputs and outputs. There is unsurface learning, which is trying to make sense of just data doesn't
have any labels. There's reinforcement learning, which looks at trying to optimize the reward,
which is a really good fit for robotics. So let's say you have a robot and maybe you want it to
clean your home. You could define a reward function that says the cleaner my home, the higher the
reward. And then a reinforcement algorithm deployed on a robot would try to optimize that reward
than as a consequence, optimize the cleanliness of your home if it's successful.
So we've been hearing a lot about deep reinforcement learning of late, but is reinforcement learning
as a whole? Is it new or has it been in use for a while? And how has it done prior to deep learning?
Yeah. So reinforcement learning has been around for a long time. Ever since people start thinking
about artificial intelligence, they were thinking about things like reinforcement learning. Because
when you think about AI, you think about some kind of intelligent system supposedly required to
make a decision. And then after making that decision, there will be consequences. And then it will
need to deal with the consequences of the action. And this will repeat over and over and over.
And that is exactly the reinforcement learning setting where some systems, some AI systems
are supposed to make decisions that have impact over time and then adjust over it.
And so it's been around for a very long time. Actually, even before deep reinforcement,
there were quite a few interesting success stories. For example, Andrew, his autonomous helicopter,
I was part of the group working on that. But the helicopter at Stanford was largely driven by
a reinforcement learning. And this was just regular reinforcement learning, no deep neural nets
behind it. Russ Tedrick, professor at MIT, but during his PhD at MIT also, he build a biped walker
that learned to walk with regular reinforcement learning, no deep networks involved.
But what characterized those early successes is that it required a combination of a lot of
demand expertise with expertise in reinforcement learning. So you would carefully think about how
does the helicopter work? What are the relevant aspects of controlling helicopter? Talk to experts
in helicopter piloting what they pay attention to, what they think about. And then you would condense
that into some representation. You'd define to decide what it means to be a good helicopter
control policy. You'd say, well, helicopter control policy would look at these and these and these
aspects of helicopter state and then make a decision based on that. And leave a few free parameters
in there. Stanford, a helicopter control kit, there were 12 parameters that were not determined,
there were just real numbers that were hard to come up with by hand. But then the reinforcement
learning algorithm would find those 12 parameters better than a person could find them by hand.
And that would lead to extremely reliable helicopter flight. But the big difference now with
a deeper reinforcement learning is that it largely takes away the need for domain expertise.
So if you look at the results on Atari go or the results in robotics that we, for example,
got on Berkeley with learning assembly, you look at those and what goes into the thing that's
learning is raw sensory percepts. So it'll get raw pixels from the Atari game, it'll get
it just the raw board configuration, not some new encoding of strengths or weaknesses
about the board configuration, just the raw configuration for learning assembly. We'll get raw
pixels of what the robot is seeing and we'll need to make decisions based on that. And so
now it's the deep network that somehow makes sense of this raw sensory information and turns
it then into after a long computation into meaningful control commands, which is a very different
setup from this previous successes where you would have had to analyze ahead of time.
What is it that we need to pay attention to? How can we extract that with a separate piece of code
that then is fed into the reinforcement learner? Are we limited by our ability to incorporate
that domain expertise into the deep neural networks? And the background for that question is I'm
just imagining that if we were able to incorporate that domain expertise in, the models would be
even smarter and more accurate. This is a very interesting question. How can you kind of get the
best of both worlds? Exactly, exactly. I think it's like, how do you get both the domain expertise
and the flexibility of learning things from raw sensory information? Like, for example,
I had a conversation with Stefano Irman who you may know over at Stanford. And one of the things
that we talked about was some of his work on incorporating, for example, physics models into
machine learning models and how they were able to dramatically increase the accuracy of a
projectile trajectory model by telling a little bit about the parabolic trajectory that
things take when they're moving through free space to be a physics. And it strikes me that if we
could, again, get the best of both worlds, that would help us here. Absolutely. So that is a perfect
example. So let me maybe expand the scope of this a little bit. So if you look at prior knowledge,
I can come in in many different formats. So one thing is, for example, you might know that
physics is involved and that the laws of physics could help you in making decisions. It might also be
that certain existing algorithms could be relevant. Maybe you know that a common filter which was
actually used to track the first rocket that went to the moon, that that idea is going to be relevant
because you want to track maybe the position of your self-driving car. Or maybe you know that
a planning competition, a competition that doesn't just reactively look at the current
sensory inputs and the spit tide and action, but actually thinks ahead simulates what would happen
if you were to take a certain sequence of actions. And then based on that makes a decision.
And so there's many of those ideas out there that we know can play a role in decision-making.
And if a deep neural net is supposed to figure it out all from scratch, the data needs might be
prohibitive to get to a practical application anytime soon. And so you can do it there, which is
actually really interesting, which maybe transcends deep networks a little bit, which is actually the
automatic differentiation frameworks, such as TensorFlow, Theano, PyTorch, and so forth. These
frameworks actually can differentiate to anything. They're not specific to neural nets. And so we
can do is you can set up something more complex than a neural net, more generalized the competition
graph. What you can do with that is you can tend to set up a competition graph that encodes
the algorithm or the prior knowledge that you have. So for example, if you thought you wanted to
use a common filter, you could set up the equations of the common filter inside these frameworks.
Now typical would happen is you still need to deal with raw pixels. So you would
take your raw pixels, feed them through a deep network that feeds into these common filter
equations that then leads to some output. And so what you get then if you train this is that
you're training this big competition graph that has the flexibility of a neural net,
namely the ability to adapt to how you should process raw sensory information. But then also the
advantage of knowing that a certain competition will matter and have it built into it. And you can
optimize this all in one optimization. You don't need to separately find a neural net that you then
plug into a common filter. The same is true for the physics equations that you were referring to
that could be, for example, inside of a planner. So you could have your neural net processing
raw sensory information, feed it into another competition that has a planner in it and the planner
can rely on physics equations. And you don't fully wire up the details of how it's going to rely
on that. You let it figure that out, but it's a component that sits there ready for it to use
such that it can be more effective at learning from whatever data it's getting, what the right thing
is to do. It sounds like there's definitely a lot in there to unpack, but it sounds like in a
nutshell, what you're saying is that these, you know, our prior knowledge, whether it's a form of
in the form of governing equations or models or subject matter expertise or what have you,
you can almost think of them as like features that you're eventually going to be
using as inputs to your neural networks. And in fact, you know, these features, we've got a
tremendous amount of depth that we can express through, you know, matrix math and differential
equations, you know, they can go through the same infrastructure that we're using to train our
reinforcement learning models vis-a-vis TensorFlow and the like. Absolutely correct. And one thing
that this also reminds me of is when you think about all these equations, they've been developed
over time. Mathematics is been developed over time. If you think of, you know, there's a lot of
benefits to reinforcement learning in and of itself. And it's really powerful to be able to
learn from scratch. But the way humans often learn is actually by imitation, which is just as
important an area of deep learning for action, for systems that can take action. Because if you
think about it, I mean, imagine, you know, our intelligence now, our reinforcement and capabilities
now as humans are probably not that different from what our reinforcement and capabilities
were 100,000 years ago or even 200,000 years ago. But we live a very different life. And the
reason we live a very different life is because we don't know start from scratch. We actually build
upon what previous generations have developed, have built, and then we learn from that much more
effectively than if we had to start from scratch. And so a lot of applications of reinforcement
learning actually will rely on a combination of imitation learning and reinforcement learning.
So typical setting would be something where you say, well, I want my robot maybe to, I don't know,
one maybe stack some dishes. And then a natural thing to do would be to instead of starting from
scratch with reinforcement learning, which of course, in some sense is beautiful intellectually,
but at the same time is very ineffective given that you know what stacking dishes looks like.
The more natural thing to do would be to show how to stack a few dishes and have the system
watch you do that. And then use that as a guide to then learn its own motor skills that will
allow it to match up with what you just did. That should poses a lot of challenges. In the simplest
version, you would actually just move the robot's arms around and make the robot experience everything.
But that is not how you would want to do it in the longer and longer and you want to just do it
yourself has the robot watch you and understand what is the essence namely the objects that are being
moved around. And what is not essence namely that it's your hands versus robot hands or that maybe
you are moving your head in certain ways that are irrelevant to the task because you just might
be looking because somebody comes by and checking out what they're doing. And so a robot understanding
from watching a human what is the essence and distilling that into then understanding how to do
something themselves in their own body which is different from the human's body.
There's a lot of interesting challenges that actually will go a long way in terms of
seeding the robot's capabilities to then bring in reinforcement learning to really get fine-tuned
skills. When we're doing imitation learning what what's the underlying mechanism or what are some
of the underlying mechanisms that we're using to kind of capture what the you know what we're
training on that what we're imitating is it like we're constraining the the state space of the
ultimate solution. And so like we don't have to train a bunch of things or are we like building
representations of what the robot's seeing or some combination of all that plus other stuff.
Yeah so there's been quite a few interesting things happening over the past year that I think
are changing what's possible with imitation learning. So one piece of work that happened to come
out of OpenAI about a half year ago was a third person imitation learning. And so what that considers
is it it considers the specific problem of how to learn when you are watching from a third
person point of view what it is that you should do but then later you should do it yourself
in which case it'll look very different because it's now you with your own hands from your own
viewpoints, first person viewpoints. And so some of the ideas we we put out play there were
actually quite related to some work Stefano Armand did at Stanford who you just mentioned
on imitation learning through generative adversarial networks. And so the idea there was to
say in the original paper from Stefano's group, this was John Fennel, Stefano's student,
they looked at how can we generate a behavior that an adversarial network cannot distinguish
from the demonstration behavior because if an adversarial neural net cannot distinguish the robot
behavior from the demonstrations, then that means the robot might have captured what matters about
those demonstrations. Now the tricky part is what does behavior mean here is it is it the movements
or the outcomes? So those are things that the organ designer has a little bit of an open
and a choice there. Do you just let it look at the outcome? Do you let it look at the entire
trajectory? Most of the work let's again look at the entire trajectory and based on that make a
decision of whether this is the same or not the same as the expert. Now when when you do first
person demonstrations where you're inside the robot, this will work. But now if you have a third
person view on the demonstration, this will never work. This is always very easy to distinguish
the demonstration from the robot's execution because while the demonstration is going to be this
this human doing something and then the robot's going to do it and it's obviously there's a human
or a robot and so you can't directly apply this in a third person setting. And when you say inside
the robot, you mean controlling a robot remotely using some controller? Yeah, we're using some
controllers, but then the robot experiences it themselves as if they're doing it. And so
what you need then is something extra, something that says, I want the GAN not to be able to distinguish
between the two, but what it's looking at the GAN should not have access to certain pieces of
information. She not have access to essentially what identifies the human versus robot because then
it's obvious and it's not actually paying attention to the essence of what you care about.
Right. So in the third person imitation work, we brought everybody in another actually GAN
related idea, domain confusion. And what you do there is you you process information through
your neural net and then at some layer in the neural net, you decide that it should not be possible
to distinguish between two things. In this case, between whether it was a robot or a human doing it.
And if you're not allowed to distinguish in that layer, that means that layer, the features that
live in that layer are not allowed to contain information anymore about human versus robot,
but only about other things like the objects in the scene, which are shared between the
demonstration and the robot. And so that's a way to make sure that that information is removed
and then you start paying attention to the essence when you're trying to imitate.
That shares other applications too. For example, Richard Semmel's group at the University of Toronto
has looked at this in terms of understanding things like fairness in machine learning.
So you can imagine that you don't want to make a decision based on certain features.
Maybe you don't want to make a decision with your machine learning system based on race.
But if you just remove race from your feature set, that's not enough because the zip code might
curl it with race or anything else might curl it with race. And so as you know in the processes
things, you could decide that at some layer, the information should not be extractible anymore,
what the race was of the person being processed. And then at that point, you know that the
decisions being made by your machine learning system, while not depending on the feature that you
didn't want to depend on, not just not directly depending on it, but also implicitly not depending
on it through how it could have figured it out from other features. So that's the domain
confusion idea. And another application domain. And now how are we identifying the layers that
are encoding this feature that we don't want to be able to use?
So that's a good question and often requires a little bit of trial and error. But essentially,
you take a deep network and somewhere in the middle of that network, you pick a layer and decide
this layer is the one I'm going to use. And at this point, it's not allowed to be distinguishable
anymore, let's say between human and robot, or you know, can't recover race or yet, you know,
sometimes people would do it between simulated environment and real world environment and so forth.
And what does it mean to not allow the network to use that layer? Does it mean you're not propagating
things from that layer forward or you're not, you're changing weights or
so what that means is that at that layer, there is a network that continues that will try to make
a decision of maybe what action to take or whether it's experts versus non expert and so forth.
But then you branch off a second head of the neural network. It'll get a second head and
it could be a pretty deep head that can do a lot of competition. And that second head is on
its output classifies, let's say, between robot and person. And then instead of maximizing the
accuracy of that head, you minimize the accuracy. You make it maximally confused.
So you're basically training your network to forget about or obfuscate that information in
that network and that layer rather. Exactly. So if you wanted to be that that network can do well
and the head is trying to be accurate, but the layers before the split need to do something to
ensure that that head cannot be accurate. So the early set of layers needs to lose the information
so that that head cannot achieve what it's trying to do. That's incredible.
Interesting, interesting. So we've gotten pretty deep here. I want to maybe take a step back
to kind of RL deep reinforcement learning and maybe address, I think when most folks come into
contact with reinforcement learning, it's probably in the context of games like Atari video games
and other games that people are training RL models to try to play. Well, first, I'm wondering if
you can speak to, what's the significance of games to RLY? Are they so popular as training
vehicles? And then maybe we can dig into some of the techniques that folks are using these
are policy gradients and Q learning and what those mean and how they're applied.
Sure. So there are a few things that make games very interesting as the research environment.
So one aspect is that games are designed by humans for other humans. And so they're designed with
some kind of intelligence in mind. And so it's very interesting to see if we can build
an artificial intelligence that can also play those games. Now related to that, they're designed by
humans for other humans, but they were designed not with artificial intelligence in mind. So
it's not something where you get to design a game once you have your argument in mind. The
games already exist. And we need to see if our algorithms can tackle these existing games.
I will say there are also a few downsides to games. So one of their big upsides is of course
that simulation is easier than real world experimentation. Far safer, you can parallelize more easily
and you can often get more self-contained environments to make it easier to run large-scale
experiments through the size of the environment. And so there's a lot of benefits to simulation,
including also simulation of other things such as simulated robots. Where games can get a little
tricky is that it's not always obvious if you start looking at more advanced things like
transfer. If you learn in one game, can you learn more quickly in another game? It's not always
obvious that whether this should be possible or not, whereas if you look at things like, for example,
simulated or real robotic manipulation, it's more natural to expect that if you learn to pick up one
object, it should help you learn to pick up another object. And if your algorithm is not able
to get that kind of transfer, it's probably all of them that's that fault. Or maybe it didn't see
enough data yet. Whereas if you learn to play, let's say, Pong, and then you're supposed to learn
Montezuma as a revenge, it's not immediately obvious that there should be any transfer between the
two games. And so one of the big questions, I think, when you think about RL, are you learning
for mastery or you're learning for generalization? And so, masteries where you stay within one environment,
one game. You say, I want to master Montezuma's revenge, which probably no system has done yet.
It's one of the harder Atari games in that suite that's researched a lot. But it's still a different
question. Can you master one game versus can you learn something that then can be helping you
in the future to learn something else more quickly? Right. And so that's that's where
games can get a little trickier unless you're very careful about maybe which set of games you choose.
And at the risk of kind of going kind of arguing down into another detail.
With regards to transfer learning, is there any work looking at transfer learning?
Is transfer learning only done kind of at the level of an entire deep network? Or can you transfer
learn specific layers or architectural subsets of a network? That's a good question. I think
transfer learning is still very much an open problem to claim anybody's found like a full solution
to it. There's definitely been some progress and people have done very interesting things.
So for example, one type of transfer that's been very successful is training on ImageNet
and then fine tuning on a new data set. So this would be for computer vision. You want to do a
good computer vision on a new task where you have a small amount of data. You first train on ImageNet,
which is a data set with many, many labeled images, a thousand categories. And it turns out if
you train to be good at recognizing those thousand categories, the later layers of this deep network
contain features that are quite good. In fact, the entire network contains information that's
quite good to then reuse to train another data set. Still a vision data set of course, but one
that might not have as many labels, it might have completely different categories. So that's one
example. The idea being that the training on ImageNet teaches your network, things like edges
and textures and things like that that are transferable to other vision related tasks.
Exactly. And so I think some of the most exciting work in terms of transfer has actually been
inspired by those results. And it falls under the category of a few shot learning. And the idea
there is that at training time, you might see a lot of data that you can do all kinds of things with.
But then at test time, you'll get to see new data that has different categories. And you're
supposed to learn very quickly what to do for those new categories. For example, a standard
thing could be maybe have ImageNet, which is a thousand categories that training time you only
get to train on 800 categories. At test time, the new categories get presented to you. I need to
adapt very, very quickly. And so that's the few shot learning setup. People do for other data sets
like Omnigloth, which is a handwritten character data set. And so some of the ideas there essentially,
one example that we worked on recently, this was led by Chelsea Finnipi, as she's doing that
Berkeley was to see if it's possible to also apply this to reinforcement learning. So people had
had some success in supervised learning, but can you in reinforcement learning train in training
environments, but then somehow reuse what you learn there in new test environments that are
related. So maybe you learn to control a and simulated robot to do certain things, like maybe
running at certain speeds. But then at test time, it needs to run at a very different speed.
And the question is how quickly can it learn to run at that different speed? Can it do it with a
very small number of policy-gradient updates? And indeed, the experience found that it is possible
with a very small number of updates to adjust to a new task at test time compared to typical RL
if you were to learn from scratch would need a very large number of iterations.
Which brings us back to policy-gradients and Q learning. Yes, absolutely. So let's start with
Q learning. So what's the idea behind Q learning? What's a Q value? A Q value is the Q value of a
current state and current action is how much reward you expect to get when you start in that
current state, take that action and from then onwards act optimally. So if you have the Q
values, it's very easy to decide what to do. You look at the Q values in your current state and
you just choose the action that maximizes the Q value in that current state and that's the best
action to take. Now of course, the tricky part is how do you find your Q values? You need to
somehow know what they are for every state that might be in the world. And so there's a lot of
states. And so building just a table that tells you whatever Q value is is not practical unless
your environment is really, really tiny and not of practical interest. So for realist
environments or even just for larger, not that realist environment, like just some simple video
games, typically the Q values are represented by deep neural nets this day. And so input to the
neural net would be, let's say pixel values, what's currently on the screen and output would be
for each action that you can take the Q value of that action for the current screen configuration.
And so for the initial Atari results from DeepMind, they trained a Q network.
And from its own tron error, this network was trained to take on the right values or good enough
values such that if you use the trained Q network to choose your actions, you actually perform quite
well in the game. The intuition on how you train this Q network is as follows. You say, okay,
what does it mean to be a Q value? It's the value of current state and action, okay? That is
actually equal to the reward you're going to get in the first transition that you encounter from
current time to next time, plus the Q value at the next state that you landed. Because
how well you do from current state is how well you do in the first step, plus then how well you do
in all future steps. And so that, that then gives you actually a self consistent set of equations.
That says Q value equals reward plus Q value at next state. And so what Q learning algorithms
do is they solve this self consistent set of equations. And the way to solve it is by collecting
a lot of data from running trials in the environment. And on the states and actions that are
experienced, trying to enforce the self consistency of that set of equations. And once you've
enforced the self consistency, you end up with, if it's fully enforcing up with the correct Q
values and that will prescribe your actions. And also tell you how good it is to be in a certain
state and take a certain action. And practice the one before we made self consistent. It's a very
large set of equations. And it's not easy to make that self consistency true. But stochastic
gradient taking updates will get you closer to self consistency. And as you get closer, acting
based on those Q values will will typically lead to pretty good behavior. And so our policy
gradients, an enhancement of that basic technique, or is it a different technique altogether?
So it's very interesting. So policy gradients are a different family of techniques. But I'll get
back to how they might actually be quite similar. Most I've explained it because actually there
is some recent work showing that there might be stronger connections than people might have
initially thought. Okay. But so what policy gradients do in some senses is much simpler to explain.
What's a policy? A policy is in these days, it's a function. And these days it's typically a deep
neural net. So it's a deep neural net that takes in, let's say current pixels and outputs the
action that you're going to take. Or a distribution over actions is what often is used. So input,
current situation, output distribution over actions. So once you have a policy, you can follow that
policy by sampling from that distribution over actions. And then you're executing the policy.
Of course, most policies are not good policies. So you need to do some work to find a good policy.
And so a policy grand approach is a very, use a very simple idea. Let's say you have a current
policy, you execute a few times, you see what happens, you see how much reward you got,
and now you can use, you can perturb your policy. You can say, let me use a slightly different
policy and execute again and see what happens. Now you can compare how well did my original
policy do, how well did my new policy do in terms of how much reward they collected. Which
ever is the better one you retain and you repeat, that would be a simple way to do it.
And so that way you're gradually improving your policy as you iterate in your algorithm.
And so you said a policy is for all intents and purposes a deep neural network. When we
perturb the policy, are we changing the weights or are we randomly changing the weights,
so what does that mean specifically? So there's different ways, that's a really good
question. Different strategies to do policy perturbation. So one way can perturb the policies by
just randomly preserving the weights in the neural net. Another way you can get variation to get
gradient signal from is by ensuring that your distribution, that you have a distribution of
our actions. So a non-deterministic policy. And then what happens is you sample your actions
from that distribution. And so every rollout will lead to different behavior. And it turns out
that you can compute policy gradients from that too using something called the likelihood ratio
of policy gradient. And that effectively pieces apart and then makes actions that led to better
reward more likely and actions that led to less reward less likely and it doesn't object to your
policy that way. You can also do something like finite differences where you go per coordinate.
Of course, in high dimensions, that's a little tricky. You could go per coordinate, increase the
value of that coordinate. Cornet here is a single weight in your neural net. For each thing,
each weight in your neural net, you can increase the weight, decrease it, and just use like a high
school type derivative calculation. With finite difference ways, say I increase the x value,
decrease the x value. Now look at f of x plus minus f of x minus divided by the size of the
perturbation and that gives me my derivative. That's probably a baseline that you could check,
but that would be somewhat sampling efficient if you had a high dimensional policy.
Okay, so the summary on Q learning and policy gradients is
Q learning, you've got a neural network that's representing essentially a sequence of consistent
equations and you solve that. You solve for your model by enforcing that consistency and
coming up with a set of weights that kind of maximizes the score, if you will. Maximize the
consistency. Okay, okay. And then with policy gradients, you've got this policy that is,
well, explain for me actually the relationship between the policy neural network and the
broader neural network is one subset of the other or they just two totally different things.
So in policy gradients, the policy network is the only network that you might use and it just
represents a policy directly. Okay. Now what's interesting is that some recent words both from
DeepMind and from OpenAI. Here, John Schillman was lead author here at OpenAI.
Show that there's a very close connection between policy gradients and Q learning.
Ask Q learning is typically used. So if you look at how Q learning is typically used in practice,
look at the details because there's there's various incarnations you can have of Q and with this
typical way of using it in practice is that you collect data based on what your current
Q function prescribes. Once you do that, it becomes a lot closer to a policy gradement because
the policy gradient method also says I have a current policy at collect data and I improved the
policy based on that. If you have a Q function, a current Q function which of course doesn't
satisfy the self-consisting equations yet, we collect data based on that current Q function
and then try to get the self-consistency to be more satisfied. It turns out that that update
is extremely similar and under some additional specific assumptions that are quite practical and
people often have algorithms that match those, the two become unified and Q learning and policy
gradients end up using the same update equations as each other which is very intriguing and actually
explains in many ways some of the mystique that has been behind Q learning in the sense that
if you look at Q learning the self-consistency set of equations, it turns out that
what ends up being found doesn't really fully satisfy the self-consistency and also the values
that you find running Q learning, which are supposed to be how much reward you'll get going
forward from that state and action. The values are often way way off. They're not precise at all
and nevertheless somehow this Q learning algorithm leads to a good policy and so this connection
between the two that John Shulman figured out essentially shows why it might be that Q learning
leads to good policies, namely that it's secretly running something like a policy-grant algorithm
underneath or something very close to it. Is the idea then when you talked about collecting
additional data is that in both Q learning and policy gradients you're training some agent
to kind of navigate an environment and the agent in either case tends to perform behaviors
in a rough neighborhood of what it has previously seen and done and that's kind of the cause
of the one approximating the other. That's exactly right. The space is so big that the learning tends
to focus on where you currently would be going with what you have learned so far and once you do
that in Q learning because it's natural to restrict attention to that because why try to learn
about everything there's so much that you might be busy for too long. Once you mostly pay attention
to what your current Q function prescribes you start being extremely similar to a policy-grating
method. You've mentioned pixels a few times in your descriptions is reinforcement learning only
applied to applications that have some vision component? So reinforcement learning can take in
any type of sensory input. So raw sensory information could be pixels but it could be something else
in robotics. We also take in joint angles and joint velocities because while the motors that
are part of the robot know those values and they're very informative about what situation the robot
is currently in. So that's being fed in too. Looking ahead things that are very interesting to me are
things like tactile sensing. If you have a robot hand if you can have tactile sensing on that robot hand
that should amplify what this robot hand is capable of doing but now how do you process that
information? How do you turn this raw sensory tactile information into an understanding of how you're
holding the object? Well there are some object properties of the object that you're holding and
so forth. Those are challenging problems but also the kind of problems that I suspect deep learning
could help solve because it is the same flavor of problem as the image processing type problems.
You have high dimensional sensory information. The information is intrinsically in there but it
just somehow needs to be teased apart from these raw sensory inputs and so it should be learnable
if we set up the data collection for that if we have some supervision or if there's some reward
related to if you had a reward such that to be successful on that reward tactile would matter
then presumably that robot hand would learn how to process tactile information because that would be
the way to maximize reward. So we've talked about games we've talked about robots
focusing in on the industrial applications of reinforcement learning in and around robots and
other other use cases that would appear within an enterprise context. What use cases that we
seen success with and where are we kind of getting close? Okay that that's a great question that
actually first I think a lot of reinforcement learning right now is happening in still research
environments. So if you look at a lot of the big success tours of reinforcement learning and
that are very well known learning to play Atari games. Learning to play go which by the way there's
a combination of imitation and reinforcement learning in that case. Learning simulated locomotion
skills that was some of the work John Schumer and Sergey Leven did at at at Berkeley or learning
robot motor control but still for very simple tasks was Sergey Leven and Chelsea Finna Berkeley
how how to assemble let's say toys all of those are are still a little removed from what you
would think of as real world deployment. I think there are a few reasons for that. I think one reason
is that these algorithms are only recently have become like really something that people are
able to get to work five years ago people didn't think those things are possible but now they're
starting to work and it takes some time to transition that into applications especially since for
now it still requires a little bit of well I would say a substantial amount of reinforcement
learning expertise to make sure these things work out and so the number of people who can put this
to use is still a little limited and a lot of those people are actually excited about expanding
the research frontier rather than necessarily putting it into application so there's maybe a slight
shortage of reinforcement learning experts that are taking their expertise then and and try to
deploy them in the real world because the real world has many so man's where it could be applied
anything where you make decisions over time reinforcement learning is going to matter this could be
for your HFAC system this could be for let's say servicing demand in cues where maybe you're
providing support longer run once language understanding is better this could be part of
dialogue because the goal and dialogue is not just to spit out a statistically reasonable sentence
and reply to the previous one the goal and dialogue tends to be figuring out what the other person
wants to achieve and helping them achieve what they're trying to achieve and so in that scenario
there is a reinforcement learning problem in terms of maximizing reward is maximizing happiness
of the other side in terms of what they get out of this conversation and so forth so one of the
things we're actually doing in late august is this need to get it with a few other people so also
Vladimir from from deep mind under her path from open AI serving 11 from Berkeley Chelsea Finn
from Berkeley and then John Schillman rocket one and Peter Chen from open AI is organizing a
deep reinforcement learning boot camp in late august the incentive here is that it seems
reinforcement learning is getting ready to be deployed in various application domains in industry
but it will require more experts and so to educate experts to then start you know they they will
see the applications once their experts they'll see the applications and latch onto them and start
deploying things and so the hope is to kind of accelerate that a little bit by having a weekend
a very intense weekend with lectures but at least for about 50% lab sessions where people really
you know get things working in old environments we talked about simulated robots video games and so
forth get the essence down but then with the hope that they can take it back to their companies
or to their research efforts because it's also useful for researchers to be more productive and
get things done with reinforcement learning that sounds great I will get the URL to that from you
and we'll make sure to include it in the show notes it sounds like the summary is that the technology
is ready but there is still a shortage of expertise to help folks build out these applications and
you know start so that we can start to see you know proliferation of success stories out in
the commercial world yeah am I am I say the technology is at the cost of being ready I wouldn't say
it's like very mature it's like it's it's getting there and the first application should become
possible in the future but the technology shouldn't also still be improved a lot over the next few
years and in your experience is reinforcement learning often a an alternative to some other
technique that you know might work or is reinforcement learning kind of the only way to solve
the problems that reinforcement learning is good at and you know is there a general way to
characterize like the benefits or advantages of our own relative to you know some alternative
approaches so the typical starting point when you need to make decisions over time like
it reinforcement learning would be imitation learning because imitation learning is simpler
it's it's like supervised learning you would demonstrate what needs to happen and then you would
try to learn something that matches what you did during the demonstrations now when it's hard to
demonstrate that could be an issue or it could be that it's not too hard to demonstrate but it's
hard to demonstrate at the scale that you need to learn from and that's where reinforcement
can do autonomous data collection and so it might be able to collect a much larger amount of data
than you can get from demonstrations it could also be that you can demonstrate in the format that
you need so maybe you want a robot to do something that maybe load your dishwasher but it's very hard
to make the robot do it you can do it yes by hand yourself but that's not the exact form factor
that's easiest to learn from and it would be the third person imitation again which is
still a hard problem even though some progress has been made so the first shot typically
I would argue is you try and find a way to get imitation in place see how far you can get with that
and then take it from there and typically you'll you'll build reinforcement on top of that
that will fine tune or in some cases imitation will just not be workable because you can get
the demonstrations that that you need yeah I would imagine even if you can kind of demonstrate
the activity in a perfect environment reinforcement learning has still advantages in being somewhat
more robust to the position of the object that you're picking relative to just a pure imitation
learning is that true in general that capability of adaptation once deployed if you let your
reinforcement learner continue to learn once is deployed is very different from what you would get
from a standard imitation learning setup and so yes that's absolutely a big part that also
reminds me of the I think automation provides big opportunities and one I'm personally particularly
interested in is how to get reinforcement learning and imitation learning to start playing big
roles in how automation works for manufacturing to make that a lot more flexible than the way
things tend to be right now which is a lot more rigid than how you need to set things up
and the idea being there that as opposed to a particular you know an agent like a robot
manipulating something we're talking about now sequences of steps are we talking about kind of
the big picture manufacturing are we talking about you know still individual devices
it could be both so at the individual level it could be that instead of you know setting up a robot
for taking multiple days or weeks to set up a robot for an individual manipulation skill you just
demonstrate a few times it learns from that reinforcement learns on top of that and based on that
maybe can be deployed within hours rather than days or weeks in the bigger picture I think it's
very fascinating I mean big it wouldn't be that easy to execute on a small scale with a bigger
scale if you say I want a factory that can take in any raw materials and output any goods and the
system can just adapt itself to whatever the needs are just send your design files and outcomes
you know in goes raw materials outcomes the product I think I mean that I'm not saying that that's
going to happen tomorrow but that kind of flexibility would be really amazing and it seems it requires
a lot of flexibility a lot of adaptation the robots in such a system need to be able to do a wide
range of things you can just set them up for one thing because then something new needs to
manufactures and they need to reconfigure themselves take up a new skill and work together to get the
product made right right well that's a super compelling vision and maybe a great place to
leave off here anything else that you'd like to leave the audience with we'll definitely share
that link to the deep learning bootcamp but anything else you'd like to share
um thanks for listening uh thanks for having me Sam this is a great and fun chat yeah great well
thanks so much Peter I really appreciate it bye bye all right everyone that's our show for today
thanks so much for listening and for your continued support comments and feedback we're excited to
hear what you guys think about this show and the series I'd also like to thank our sponsor
Banzai once again be sure to check out what they're doing at Banz.ai B-O-M-S.ai another reminder
there are less than two weeks left into the O'Reilly AI conference in New York City and I just saw
an email this morning that prices go up today if you'd like to attend you can save 20% on registration
using our discount code which is PC Twimble PCT W-I-M-L we'll link to the registration page
in the show notes I'd love to meet up with listeners at the conference and as I mentioned last time
I'm planning a meetup during the event I'll share details as soon as they've been ironed up
the notes for this episode can be found at Twimble AI dot com slash talk slash 28
for more information on industrial AI my report or the industrial AI podcast series visit
Twimble AI dot com slash industrial AI as always remember to post your favorite quote or take away
from this episode or any other and we'll send you a laptop sticker you can post them as comments
to the show notes page via Twitter at Twimble AI or via our Facebook page once again thanks so
much for listening and catch you next time
