WEBVTT

00:00.000 --> 00:04.880
All right, everyone. Welcome to another episode of the Swimwell AI podcast. I am your host,

00:04.880 --> 00:10.880
Sam Sherrington, and today I'm joined by Johan Braemer. Johan is a research scientist at Qualcomm AI

00:10.880 --> 00:16.240
research in Amsterdam. Before we get going, be sure to take a moment to hit that subscribe button

00:16.240 --> 00:20.720
wherever you're listening to today's show. Johan, welcome to the podcast.

00:20.720 --> 00:25.200
Hi, Sam. Thanks a lot for having me. Super excited to have you. We're going to be talking about one of

00:25.200 --> 00:32.480
the papers that you presented at NURPS on causal representations, as well as some of the things

00:32.480 --> 00:38.000
that your colleagues presented at the conference. But before we do that, I'd love to have you share

00:38.000 --> 00:42.400
a little bit about your background and how you came to work in machine learning. I grew up as a

00:42.400 --> 00:46.800
particle physicist. So I was trying to figure out the best way that we can measure the properties

00:46.800 --> 00:50.800
of all these elementary particles around us, like the Higgs boson, from all the data collected

00:50.800 --> 00:55.200
the large header and collider. Now, that's an inherently statistical problem, right? You need tools

00:55.200 --> 00:59.600
from statistics and from machine learning to solve this problem with all the high-dimensional data

00:59.600 --> 01:03.760
and the many pyramids you're trying to measure. During that time, I figured out that I actually

01:03.760 --> 01:09.040
enjoy working with the methods with the statistics and the machine learning much more than talking

01:09.040 --> 01:13.760
about the theoretical questions I set out to solve in the first place. So from there, it was a

01:13.760 --> 01:19.360
slippery slope. I started working on statistics for particle physics. I then did some machine learning

01:19.360 --> 01:23.760
for the sciences, and suddenly I found myself an Amsterdam working on machine learning problems

01:23.760 --> 01:31.360
that have nothing to do with particle physics with some great colleagues. And initially, or immediately

01:31.360 --> 01:38.400
into communications types of problems and the kind of things that you work on at Qualcomm, or

01:38.400 --> 01:43.200
did you start in another area? So in my first year at Qualcomm, I worked on a video compression

01:43.200 --> 01:47.200
with neural networks. I think my colleague, Alke Viggas, from the team was recently on your

01:47.200 --> 01:52.080
podcast and explained all that much better than I would do justice now. But there was a nice

01:52.800 --> 01:57.440
introduction to Qualcomm and all the research that we're doing there. But then one year in,

01:57.440 --> 02:02.880
we started a new team on causality. Now relative to some of the other folks that I've talked to

02:04.400 --> 02:11.680
on the Qualcomm AI research team, causal work is much less applied than video compression,

02:11.680 --> 02:17.440
for example. Yeah, totally true. I think Qualcomm has kind of this full spectrum of research that

02:17.440 --> 02:22.800
ranges from pretty applied, like model efficiency, neural network quantization, video compression,

02:22.800 --> 02:27.200
these kind of things, to pretty fundamentally like a geometrically learning, a covariance, and now

02:27.200 --> 02:32.560
also causality. But they're all united by this idea of making AI more efficient in some way.

02:32.560 --> 02:37.440
And I think for causality, the idea is that causality may be a framework that helps us solve some

02:37.440 --> 02:41.840
problems that are currently pretty much unsolvable with the standard paradigms in machine. But it may

02:41.840 --> 02:46.960
take some years to get there. And what are some of those types of problems that you think causality

02:46.960 --> 02:52.560
can help us solve in in ways that will be much more efficient? Yeah, so machine learning systems

02:52.560 --> 02:56.480
right now are great, like just by scaling up the datasets and scaling up the models we can solve

02:56.480 --> 03:01.360
many problems much better than we thought we could. I mean, look at ChatGPT. But there's still

03:01.360 --> 03:07.440
some things that are really hard for these problems. And one case is kind of the brittleness under

03:07.440 --> 03:11.520
changes. If you train a model in one type of conditions and then deploy it under different

03:11.520 --> 03:16.080
type of conditions, then often the performance goes down quite a bit. Like the machine learning

03:16.080 --> 03:22.720
systems have a hard time with seem to real with any kind of changing conditions. Now causality

03:22.720 --> 03:28.640
on a very high level is a framework about reasoning about actions, about reasoning about changes,

03:28.640 --> 03:35.520
about robustness under changes. So on an intuitive level, I think it makes a lot of sense that

03:35.520 --> 03:40.960
this can help us address these these open problems. However, I should say that it's not totally

03:40.960 --> 03:45.280
clear concretely how this will work, right? This is an open research field. Causing machine learning

03:45.280 --> 03:51.760
has really just started gaining momentum a few years ago. So let's see where this takes us.

03:51.760 --> 03:59.440
Yeah, before we dive into the paper about your research, Qualcomm, is it entirely focused on

03:59.440 --> 04:05.440
causality or are you covering several areas as well? It's about causality and interactive learning.

04:05.440 --> 04:09.360
So it kind of focuses on a lot of topics that bother on reinforcement learning. We have some

04:09.360 --> 04:13.520
projects on imitation learning. Maybe we'll get into that a little bit later. We have some projects

04:13.520 --> 04:18.640
on unsupervised reinforcement learning. What can you learn in a setting where you have an

04:18.640 --> 04:23.040
interactive environment, but no reward function, and no expert demonstrations, and then there's

04:23.040 --> 04:29.040
kind of the more classical causality. Awesome. And so this particular paper that we wanted to spend

04:29.040 --> 04:34.560
some time talking about is called weekly supervised causal representation learning. unpack that

04:34.560 --> 04:40.720
title a little bit and the goals you have for the paper. Yes, excellent. Let me start a little bit

04:40.720 --> 04:45.040
from the end of it, right? So the paper title ends with representation learning. And one of the

04:45.040 --> 04:52.240
two goals of this paper is to learn meaningful representations of data. What do I mean with that?

04:52.240 --> 04:57.040
If you have data presented to you in some kind of low level format, think the pixels of an image

04:57.040 --> 05:02.720
or maybe of a video feed or anything that we can record with sensors 3D, we want to train

05:02.720 --> 05:08.160
into a network to take that as an input and output a much smaller number of high level variables

05:08.160 --> 05:13.200
that capture the meaningful aspects of a system similar to how humans reason about system.

05:13.200 --> 05:17.040
When I show an image, you're not thinking, oh, these are some really nice collection of pixels

05:17.040 --> 05:22.720
here, right? You think this image shows, for instance, a car on a road in front of a traffic light.

05:22.720 --> 05:26.720
And the relevant variables are probably something like the position of the car, maybe the speed of the

05:26.720 --> 05:31.040
car, and then the state of the traffic light, whether it's red or green. So this is a much smaller

05:31.040 --> 05:35.760
number of meaningful variables and we want to train a new network to put this out. So that's the

05:35.760 --> 05:40.720
representation learning part. Now going one word more to the beginning, there's a causal in there.

05:40.720 --> 05:46.000
The second goal in our work is that we also want to learn a causal model between these high level

05:46.000 --> 05:52.320
variables. And its causal model is about the interactions between concepts. So again, in this

05:52.320 --> 05:57.040
example of a car before a traffic light, as humans, it's very intuitive to us that we think that

05:57.040 --> 06:02.320
the traffic light state, whether it's green or red, influences causes the behavior of the driver,

06:02.320 --> 06:07.520
whether they accelerate the car or break the car. So there's a causal effect from the traffic

06:07.520 --> 06:12.880
light state to the velocity of the car, but not the other way around. And this is important

06:12.880 --> 06:16.640
because it allows us to reason about what if questions. As a human, it's very easy for us to say,

06:17.600 --> 06:22.400
if the traffic light would turn to red, the car would break. If the car would break, it wouldn't

06:22.400 --> 06:27.280
necessarily make the traffic light go red. Now, machine learning systems, at least the majority of

06:27.280 --> 06:32.320
them, do not really reason about this kind of cause and effect relations. Most machine learning

06:32.320 --> 06:37.680
systems are stuck at the level of describing correlation patterns. So for instance, looking at a

06:37.680 --> 06:43.280
data set that shows you images of cars on traffic lights, machine learning systems could figure out

06:43.280 --> 06:50.880
that green lights are correlated with fast velocities and red lights are correlated with braking cars.

06:51.600 --> 06:55.520
But this doesn't allow the machine learning system to answer a question, what happens if the car

06:55.520 --> 07:02.240
breaks? Does the traffic light then suddenly change? So this is really a new kind of capability that

07:02.240 --> 07:09.120
doesn't exist in most kind of machine learning systems. Now, there's the weekly supervised part in

07:09.120 --> 07:14.320
the title. In this paper, what we do is we learn a neural network that that learns that

07:14.320 --> 07:19.280
represents data from pixel level into this kind of low number of meaningful high level variables

07:19.280 --> 07:25.840
and also learns the causal relations between them. But we do this without explicit labels in the

07:25.840 --> 07:32.880
training data. So there's no label that tells us in this image, the velocity of the car is 30 kilometers

07:32.880 --> 07:40.000
per hour, that's 20 miles per hour, or what have you, anything like that. So we just train this from

07:40.000 --> 07:45.760
example on the pixel level. But it's impossible to do this fully unsupervised. There are some papers

07:45.760 --> 07:51.520
that prove that this problem is really underspecified if you have just IID data just images.

07:52.400 --> 07:57.040
And the way that we solve this is by by breaking the ID assumption, we consider non-ID data,

07:57.040 --> 08:02.400
so kind of data with some structure in it. And concretely, we consider the case where we observe

08:02.400 --> 08:07.360
the system kind of pairs of before and after images. So we take a picture of the street,

08:08.000 --> 08:13.760
then something changes in the scene, like some effect is applied to it. For instance,

08:13.760 --> 08:18.880
you switch the traffic light state from red to green and let all the causative effects of that

08:18.880 --> 08:22.720
play out, and then you take another picture of the scene. And we just need this kind of before and

08:22.720 --> 08:27.040
the after image, but no further labels, no information on what happened before and after. If we have

08:27.040 --> 08:33.680
this kind of paired data, then we can show that this is enough to really learn the true causal

08:33.680 --> 08:38.080
variables in the scene and the true causal effects, the true causal graph, all the mechanisms that

08:38.080 --> 08:44.880
come on the scene. Now, I'd love to maybe start talking about how far along you are and how far

08:45.520 --> 08:53.200
the method that you've identified can take us and ask that in the context of this one diagram

08:53.200 --> 08:58.720
that you have at the very beginning of the paper that kind of articulates what you're hoping to do.

08:58.720 --> 09:06.480
It's a picture of some standing dominoes that you're before picture and then you have

09:06.480 --> 09:11.360
some dominoes tipped over and presumably there's an intervention between the two, someone

09:11.360 --> 09:17.760
tipped a domino. And you know, the grand idea is that you, with enough of these domino pictures,

09:17.760 --> 09:22.240
you can start to learn that if you've got some kind of domino tipping intervention,

09:23.520 --> 09:28.720
what the outcome will be after you've applied that intervention.

09:28.720 --> 09:33.200
Yeah, that's exactly right. And yeah, we're not at the point where I can, you know,

09:33.200 --> 09:38.400
giving your trained model, take a picture of some dominoes and apply this intervention somehow

09:38.400 --> 09:45.120
or say that, okay, give me the picture after the dominoes have fallen and get that resulting

09:45.120 --> 09:50.320
picture, correct? That's unfortunately correct. Yes, I think we make a pretty strong claim.

09:50.320 --> 09:55.360
In this paper, right, we have this really strong claim that we can identify the causal structure

09:55.360 --> 10:00.000
and the right variables and everything correctly. But to get such a strong result, we also need

10:00.000 --> 10:04.480
strong assumptions. And the strongest assumption is I think there's this kind of data regime that we

10:04.480 --> 10:09.600
need this data in this before and after pairing. And we also need to assume that nothing else

10:09.600 --> 10:14.320
changes between the before and the after image, just like one thing is intervened upon one action

10:14.320 --> 10:19.360
happens. But we also need to make a couple of more technical assumptions. And unfortunately,

10:19.360 --> 10:24.480
many real world examples violates some of these technical assumptions that we need for our theory,

10:24.480 --> 10:30.080
at least. So in particular, in this domino case, if you have two domino pieces, if you push over one,

10:30.080 --> 10:33.520
it would knock over the second one. If you push over the second, it could knock over the first

10:33.520 --> 10:38.800
one. So they're kind of the graph here in some sense is cyclic, right? Like each domino,

10:38.800 --> 10:44.560
like they affect each other pairwise. And this is something that standard causal frameworks

10:44.560 --> 10:50.160
can't really deal with so easily because we always assume that the graph of causal interactions

10:50.160 --> 10:55.040
needs to be a cyclic. Something can be the cause of something else, but that can't also be the

10:55.040 --> 11:00.560
effect of that variable. There's no back and forth relation in classical causal models. Now,

11:00.560 --> 11:05.440
there's some ways to resolve that. But in this work, we just assume that the causal structure is

11:05.440 --> 11:10.320
indeed a cyclic. So there's a clear causal ordering of mechanisms. The traffic light causes the

11:10.320 --> 11:14.560
velocity, but the velocity doesn't cause the traffic light, just one of these two things is possible.

11:14.560 --> 11:22.400
And so this is one setting that that that domino example violates. But I think it's really good

11:22.400 --> 11:25.600
to look at these cases that we can't do with yet because ultimately what we want to do is we want

11:25.600 --> 11:29.600
to solve real world problems. So we really need to kind of look at what what our theory can't

11:29.600 --> 11:35.040
describe yet and push there. And I think the way forward maybe to give up a little bit on the

11:35.040 --> 11:42.080
theoretical rigor to maybe not aim for theorems and proofs, but aim for just algorithms that empirically

11:42.080 --> 11:48.400
work ultimately. But in this current paper, we are kind of at the closer to the fundamental side

11:48.400 --> 11:52.720
of things. We have some theory. I think we understood some things better, but we don't have this.

11:52.720 --> 11:58.160
One size fits all algorithm that you can deploy to real world examples. And certainly not we haven't

11:58.160 --> 12:05.280
solved autonomous driving yet. Yeah, for sure, for sure. But what you've done is you've demonstrated

12:05.280 --> 12:14.480
that it is possible to identify these causal variables from, for example, from just pixels,

12:14.480 --> 12:20.160
which is pretty impressive in and of itself. Does the paper just demonstrate that it is possible

12:20.160 --> 12:24.080
or does the paper demonstrate how to derive the causal variables themselves?

12:25.040 --> 12:29.200
It does both. So I think a large fraction of the contribution of this paper is the theory. So

12:29.200 --> 12:34.720
that it's basically mostly one theorem and the proof that it accompanies it. But we also have a

12:34.720 --> 12:39.520
practical implementation. And essentially it consists of a variational autoencoder. So there's

12:39.520 --> 12:43.920
an encoder that takes as input pixels and it outputs some latent variables. And these latent

12:43.920 --> 12:47.920
variables are the causal variables of the system. And then there's a decoder that knapset back to the

12:47.920 --> 12:53.360
pixel space. And what's new about this kind of variation autoencoder is that we describe some causal

12:53.360 --> 12:59.360
structure in the latent space. So the latent variables are not just described by some, you know,

12:59.360 --> 13:04.960
IID Gaussian prior, like most fear is to it. But we have a prior that really incorporates

13:04.960 --> 13:09.520
the causal structure, the graph between the variables, this graph is learned during training.

13:10.640 --> 13:14.480
And also the exact mechanism, how each variable affects each other very well.

13:16.080 --> 13:21.440
And yeah, this kind of VAE, we trained this on these paired data sets before and after

13:22.160 --> 13:27.600
the intervention images. And then we show in a series of experiments with some, we start with

13:27.600 --> 13:32.160
some very similar toy data sets as we worked towards a simple image data sets that this works in

13:32.160 --> 13:39.760
practice. Is it interesting or kind of obvious and not interesting that the parallels between

13:39.760 --> 13:44.640
the video compression stuff that we talked about before that might use an encoder decoder type

13:44.640 --> 13:51.360
of architecture and that you're able to do the same thing here. You're kind of compressing

13:51.360 --> 14:02.640
the dynamics of the system happening before and after the pixel space into some kind of reduced

14:02.640 --> 14:10.080
dimensionality, you know, causally semantic space. Yeah, that's a good point. It definitely has

14:10.080 --> 14:15.600
some parallels like the overall architecture is very similar. But I think there's one important

14:15.600 --> 14:19.600
difference and that is that the kind of goal that we are trying to achieve, right? And compression

14:19.600 --> 14:24.720
is really about this aspect of compression, but then kind of how we use these different bits,

14:24.720 --> 14:28.320
how we use the representation of the latent space, doesn't really matter like the model can do

14:28.320 --> 14:33.680
whatever at once. And if you visualize the latent representation that compression autoencoder

14:33.680 --> 14:38.560
learns, you'll see that it's really not interpreted by humans. Like varying one latent variable

14:38.560 --> 14:45.440
leads to some very weird outputs in image space. And contrast, our model doesn't really care so

14:45.440 --> 14:49.120
much about the bit rate used in the end. I mean, it does play a role in the loss function a little

14:49.120 --> 14:55.680
bit, but it's not really the goal. The goal is really that the variables in the end have some

14:55.680 --> 15:01.360
meaning. The latent variables are exactly the true latent variables up to some permutations

15:01.360 --> 15:06.560
and rescalings. There's definitely some kind of machine learning knowledge that we gained

15:06.560 --> 15:10.800
on this compression work that we could use in the causality work, but I think the interpretation

15:10.800 --> 15:14.720
of the results is very different. Can you maybe talk a little bit broadly about

15:14.720 --> 15:19.520
the, you mentioned that there's kind of this one theorem that's at the heart of the paper. Can

15:19.520 --> 15:25.920
you talk a little bit more detail about that theorem? Yeah, I'm happy to. So this basically says,

15:25.920 --> 15:32.560
if you have two models, and with model, I now mean kind of a causal model between some variables,

15:32.560 --> 15:37.440
and then a map that takes these causal variables and maps them to some data space, so think pixels.

15:38.080 --> 15:43.280
So we call this a latent causal model. Now, what the theorem says, if you have two latent causal

15:43.280 --> 15:47.600
models, such that both of these latent causal models give rise to the same kind of data set,

15:47.600 --> 15:53.840
if you look at them. So if you kind of look at what kind of images you would get from the first

15:53.840 --> 15:57.440
latent causal model and what kind of images before and after images you'd get from the second causal

15:57.440 --> 16:02.800
model, they are the same. There's the same distribution. Then the two latent causal models are the

16:02.800 --> 16:06.720
same in the sense that they have the same latent variables, they have the same causal structure

16:06.720 --> 16:10.080
up to some equivalence class. This equivalence class basically tells you that for instance,

16:10.080 --> 16:17.360
we can't resolve permutations. If one model has variable one and two, and then the second model

16:17.360 --> 16:22.960
has variable two and one in the opposite order, then there's just something we can't further resolve

16:22.960 --> 16:28.960
because we never get labels. But up to this kind of small, and one might say irrelevant effects,

16:29.520 --> 16:35.120
those two models need to be the same. Now, why does this theorem matter? Because if you think of one

16:35.120 --> 16:40.640
of these two models as the generative process, the ground truth process, the generated observed data

16:40.640 --> 16:46.160
in some setting, maybe it's like the physical laws of the universe or it's like the rules of

16:46.160 --> 16:50.400
the traffic scene or maybe there's some human psychology and there would have some ground truth

16:50.400 --> 16:55.520
process that generates a data set. And the second model is some neural implementation of

16:55.520 --> 17:00.480
the latent causal model. So we use a VAE, we use neural permutations of the causal structure,

17:00.480 --> 17:06.960
and we train it to maximize our training objective. And then another some additional assumption

17:06.960 --> 17:10.080
that we have a good optimizer that we have enough data and so on, like the usual machine learning

17:10.080 --> 17:15.280
assumptions, then our theorem implies that in the end our learned model will recover the ground

17:15.280 --> 17:19.600
truth causal variables and the ground truth causal structure. Of course, the hidden assumption

17:19.600 --> 17:25.680
here is that indeed nature operates as such a causal model. And as we've already discussed

17:25.680 --> 17:30.640
with this domino example, there's sometimes some subtleties where our assumptions are not satisfied.

17:31.600 --> 17:38.080
And what are the mechanisms that you use to prove this theorem?

17:39.920 --> 17:44.480
Oh, that's a fun question and I like to talk more about this, but maybe I try to keep this

17:44.480 --> 17:49.440
kind of short. My two collaborators, my two main collaborators on this paper, Pim de Han and

17:49.440 --> 17:56.640
Takako and are both believers and practitioners of a field of mathematics called category theory.

17:56.640 --> 18:03.760
This is like the most abstract of mathematics. I really try to find like the most general structures

18:03.760 --> 18:08.000
that exist and try to unify many different fields of mathematics. We are not really using

18:08.000 --> 18:13.360
category theory, but we're using one tool, one particular graphical language developed in

18:13.360 --> 18:19.520
category theory, called string diagrams. And we use string diagrams to kind of represent

18:19.520 --> 18:24.400
different probabilistic equations, kind of relations between different probabilistic distributions

18:24.400 --> 18:29.520
graphically. And then you can use some nice manipulations on these and then use that to prove

18:29.520 --> 18:34.240
the theorem. I have to say, when I started out of this project, I was a bit skeptical about this

18:35.120 --> 18:40.560
and I like to make fun of my colleagues. And category theory in general? No, I think category

18:40.560 --> 18:43.920
theory is beautiful, but I was skeptical that it would give us practical advantages for this

18:43.920 --> 18:50.960
concrete project. But then it did simplify the proof here quite a bit. And I say by colleagues

18:50.960 --> 18:58.000
really showed me the way a little bit there. That's awesome. Did that just happen to be the

18:58.000 --> 19:04.240
approach that worked or was the project conceived with that as part of the approach that you

19:04.240 --> 19:11.840
would, was the project conceived as, you know, with that solution in mind? No, I would say we started

19:11.840 --> 19:16.400
out just from the actual kind of question. Can we, can we identify, cause a variables, can we

19:16.400 --> 19:21.040
identify, cause a structure just from pixelabid data? And then our first version of the proof

19:21.040 --> 19:26.160
looked much longer and much less elegant and did not involve any fancy diagrams, but then that

19:26.160 --> 19:31.680
evolved over time into something prettier. So it was more of a later stage of the project.

19:31.680 --> 19:38.160
Okay, okay. And you're careful to distinguish the causal variables and the causal structure

19:38.160 --> 19:45.760
or the causal relationships. Can you kind of more carefully distinguish the two and really talk

19:45.760 --> 19:50.640
about the relationships and how, how causal relationships are expressed? Oh, yeah, that's

19:50.640 --> 19:57.440
a great question. So in causality, one of the really important things is that we really don't

19:57.440 --> 20:01.840
just reason about variables, but reason about variables and the mechanisms that generate them.

20:01.840 --> 20:06.000
And what I mean with that is that the causal model, or there's different frameworks, but the one

20:06.000 --> 20:10.960
that's most widely used these days, describes the relation between variables is kind of factorized

20:10.960 --> 20:15.920
into a number of, if you want cause and effect relations, each of them takes the form of some,

20:15.920 --> 20:21.200
some, some mechanism. So in the, the root example that we had, one mechanism could, for instance,

20:21.200 --> 20:26.640
be how the, the traffic light is programmed, right? It probably has some frequency, how often it's

20:26.640 --> 20:30.880
green, how often it's red, how the changes, that's, that's one mechanism. And then a separate

20:30.880 --> 20:36.480
mechanism is how the car behavior adapts to, to the traffic light and maybe also the other things

20:36.480 --> 20:42.320
it sees on the road. So that kind of determines the car velocity as a function of all the other

20:42.320 --> 20:46.480
inputs, in particular, as the function of the traffic lights date. This is something that depends

20:46.480 --> 20:52.080
now on, on the particular driver. And what's kind of cool about this is that these mechanisms

20:52.080 --> 20:56.480
are independent in the sense that if you think about a different version of the scene, let's say we

20:56.480 --> 21:01.600
move to a different city, then maybe the, the traffic light frequency is totally different. So you

21:01.600 --> 21:06.000
can kind of change this one mechanism, but the way that humans react to traffic light will stay

21:06.000 --> 21:10.480
the same, right? That's kind of universal. So we only have to swap one mechanism when we change

21:10.480 --> 21:15.760
the scenery, why can keep, why we can keep the other one. This as fast mechanism shift hypothesis

21:15.760 --> 21:21.520
at its call sometimes is the essential reason why people believe that causality can help us with

21:21.520 --> 21:25.280
things like domain shift. Because we can think that if everything is composed of mechanisms,

21:25.280 --> 21:30.560
it's quite likely that when you change conditions, if you go from seem to real or from one country

21:30.560 --> 21:34.080
to another one, just a few of these mechanisms change, while other things are more universal,

21:34.080 --> 21:37.840
like the laws of physics are the same everywhere. Some human behaviors are the same everywhere.

21:39.040 --> 21:42.720
Yeah, but other things change, like go to the UK and suddenly you have to drive on the left side

21:42.720 --> 21:48.800
of the road. Now going back to the exchange we had about the parallel between compression and

21:48.800 --> 21:56.560
what you're doing here, identifying causal representations. You mentioned that in the case of

21:56.560 --> 22:06.240
compression, the goal is to find kind of the minimalist representation of this function, if you will.

22:06.880 --> 22:14.320
In this case, you're trying to identify a representation that is true to the causal nature that

22:14.320 --> 22:25.280
you're trying to model. I don't know if it's related to the, I guess intuitively, it strikes me

22:25.280 --> 22:32.560
that the causal representation would also be minimal. In the case of the traffic scenario we're

22:32.560 --> 22:38.800
talking about, really, the only things that matter are the traffic light and the cars and everything

22:38.800 --> 22:45.440
else is noise that might be captured by some other model, but the fundamental things that are

22:45.440 --> 22:51.440
causing the system to behave the way it's behaving are the traffic light and the car behavior.

22:51.440 --> 22:58.000
If you can model those and just those as a set of causal relationships, that would seem like it

22:58.000 --> 23:04.800
should be minimal. Where does that break down? Yeah, that's a really interesting point. I am actually

23:04.800 --> 23:11.760
not sure if anyone has tried exactly this, but it's true that most people find it likely that

23:11.760 --> 23:16.880
the causal description of the system has the most or relatively simple components as opposed to

23:16.880 --> 23:20.720
a non-causal description of the system which could have very complicated probability distributions.

23:20.720 --> 23:27.760
So, yeah, that's a great question. It is true that in some sense causal mechanisms are generally

23:27.760 --> 23:32.480
assumed to be simple, especially for humans. We assume that if we factor a probability

23:32.480 --> 23:35.840
distribution along the causal mechanisms, so if we describe everything with a kind of

23:35.840 --> 23:41.920
causal mechanisms, it's a much simpler thing than if we parameterize it in some other way.

23:41.920 --> 23:47.200
Now, I think where this breaks down is that ultimately what's simple to humans and what's

23:47.200 --> 23:51.600
simple to machine learning algorithms, at least right now, isn't quite the same yet. So,

23:53.040 --> 24:01.520
the way that machine learning capacity works translates into different notions of simplicity

24:01.520 --> 24:07.440
for a totally neural prior and then a neural decoder, then notions of simplicity for kind of,

24:07.440 --> 24:12.160
when humans read an expression, what do they think of the complexity of something?

24:12.160 --> 24:18.640
But I do think it's very interesting to explore this further and think more about things like

24:18.640 --> 24:23.200
the minimal description length principle and how that could be applied maybe to compression.

24:24.000 --> 24:30.480
Maybe indeed there is a deeper relation how we can use some of these ideas of simplicity that

24:30.480 --> 24:36.560
pop up both in causality and in compression. There was some prior work that attempted to do

24:37.360 --> 24:44.160
something similar, identify causal representations, but found that it was not possible.

24:44.800 --> 24:51.200
Is that the case? That's exactly right. There's a very well done paper by Francesco Locatello.

24:51.200 --> 24:57.200
I think it won a best paper award at ICML, I want to say 2020. They basically show that if you just

24:57.200 --> 25:03.200
have IID data, if you just have individual images, but kind of just one image from each scene,

25:04.240 --> 25:08.400
that then you can't even identify the variables, even if you assume that the causal relations are

25:08.400 --> 25:13.360
in some sense trivial and that kind of implies that for more complicated causes structures you can

25:13.360 --> 25:18.880
also not do it. So, the way that we get around this is by introducing this non-IID setting,

25:18.880 --> 25:23.120
where we have these pairs of observations. And actually for full fairness here, I would point out

25:23.120 --> 25:26.800
that this is not a totally novel idea. There was another paper by Francesco Locatello

25:26.800 --> 25:33.360
at ICML 2021, I think, where they already think about this weekly supervised setting in observing

25:33.360 --> 25:38.000
a system before and after something has changed. But they focus on the trivial case where

25:38.960 --> 25:42.800
trivial sounds are negative. What I mean is the case where all the causal variables are

25:42.800 --> 25:46.800
independent. So, there's no causal relations between anything. You just have statistically

25:46.800 --> 25:51.840
independent latent variables and they shall show that those can be identified. What we do in our

25:51.840 --> 25:55.440
papers, we extend that to the setting where there's actually non-trivial causal structure

25:55.440 --> 25:59.600
and that we can even then learn the variables and even more that we can also identify the causal

25:59.600 --> 26:06.560
structure. Is there an experimental component to your paper at all? So, in the sense, yes,

26:06.560 --> 26:11.680
that we evaluate our via algorithm on synthetic data. We start with some simple toy data sets and

26:11.680 --> 26:19.200
then we scale up to image data sets. But also these data sets are not real photos of some real

26:19.200 --> 26:22.800
scenes or anything like that. We start with a standard benchmark for causal representation

26:22.800 --> 26:27.840
learning, the causal 3D-ident data set or a variation of that was introduced in a paper by

26:27.840 --> 26:33.120
Julius van Kugelgen and others. And then we introduce our own data set. We call the causal

26:33.120 --> 26:37.840
circuit because we weren't quite satisfied with the available benchmarks. And in this causal

26:37.840 --> 26:44.480
circuit data set, you see a robotic finger operating a bunch of buttons that are connected to lights

26:44.480 --> 26:48.560
in a way. And we show that in this data set, you can correctly identify kind of which pixels

26:48.560 --> 26:53.120
map to the buttons and to the robotic finger and how everything is related. I guess the

26:53.120 --> 27:01.680
salient point in this data set is that the robot, when the robot interacts with a button and it

27:01.680 --> 27:07.200
turns on that light, that doesn't impact any of the other lights, for example. Is that true?

27:08.000 --> 27:12.160
Yeah, kind of. So we actually, just for fun, we added a little causal relation there that like if

27:12.160 --> 27:17.600
you press one of the lights, it turns on one of the other ones as well. But in principle, you could

27:17.600 --> 27:21.600
also have the kind of independent thing. But I think one very obvious thing is like the robot

27:22.640 --> 27:26.240
which is also in the scene that causes the lights, right? It's not the other way around. You could

27:26.240 --> 27:30.880
in principle also think there's some kind of magnet on the table, if whenever one light activates

27:30.880 --> 27:34.960
that magnet pulls the robot, I'm closer to it. Just from looking at individual pictures, there's

27:34.960 --> 27:40.240
no way of telling these two apart. But the, yeah, with these kind of paired data,

27:42.400 --> 27:46.160
this data setting that we introduced and we show that we can actually resolve this and we can

27:46.160 --> 27:50.880
learn that the robot causes the lights to go on another way around. I think there's a really

27:50.880 --> 27:56.960
interesting paper and I'm personally interested in following along with kind of how that work evolves

27:56.960 --> 28:02.960
and how far we can push this idea of just pulling causal representation from images and

28:05.200 --> 28:11.040
you know, woodling down the set of constraints that need to be applied to the model that you create

28:11.040 --> 28:17.440
from them. But I also wanted to talk to you about some of the papers that your colleagues presented

28:17.440 --> 28:25.120
at NURBS and there were several. A couple of those were on combinatorial optimization broadly.

28:25.120 --> 28:30.320
Can you talk a little bit about those? Yeah, I'm happy to. So my colleagues wrote two papers on

28:30.320 --> 28:35.760
combinatorial optimization that we accepted at NURBS, one that was written by Mukul Gagrani and

28:35.760 --> 28:42.080
Karado Rhinona and some others on neural topological ordering for computation graphs. And a second

28:42.080 --> 28:46.960
that was on a batch-based optimization over permutations led by Chang-Yung-Oh,

28:48.960 --> 28:53.200
Cuba students who student at the University of Amsterdam in a Qualcomm sponsored lab there.

28:54.080 --> 28:59.120
What both of these papers have in common is that they are about the problem of finding some

28:59.120 --> 29:03.840
optimal ordering of some objects under constraints. So where this shows up, for instance,

29:03.840 --> 29:09.120
is in computation graphs when you have some, for instance, neural network that has a number of

29:09.120 --> 29:13.440
operations that need to be performed and you want to execute them on your hardware in the right order,

29:13.440 --> 29:18.800
where right order means you want to be as fast as possible, you want to use as little memory as

29:18.800 --> 29:24.320
possible, but you also have to satisfy some precedence constraints that you don't execute an

29:24.320 --> 29:29.520
operation while all the inputs aren't available yet. And this is a hard problem because the number

29:29.520 --> 29:34.800
of operations can be large and the number of possible orders of operations grows very quickly

29:34.800 --> 29:42.160
with the number of operations. In this neural topological ordering paper, my colleagues develop a new

29:42.160 --> 29:47.600
attention-based graph neural network with a suitable message passing algorithm to solve this problem.

29:47.600 --> 29:54.160
It's called a topoformer. I think what's really key about this idea is that this graph-based

29:54.160 --> 30:02.480
transformer combines both the local topology of the computation graphs, so kind of for each

30:02.480 --> 30:07.280
operation you have the information kind of what are the preceding operations that you require and

30:07.280 --> 30:12.080
for which other operations is the output required, but also in the computation you're readily available,

30:12.080 --> 30:17.040
you have readily available global information about the structure of the graph. So each node can

30:17.040 --> 30:21.120
kind of talk to each part of the graph at the same time in this message passing algorithm.

30:21.120 --> 30:26.080
Now there was a bit technical, but the bottom line is that this works really well. If you just

30:26.080 --> 30:31.840
compare to similarly fast algorithms, this is really state of the art, both on synthetic graphs

30:31.840 --> 30:36.880
and on a few real-world examples. And I think it's still competitive with algorithms that are some

30:36.880 --> 30:43.920
orders of magnitude slower than this method. In this context, when you compare synthetic graphs

30:43.920 --> 30:56.240
versus real-world, elaborate on that, real-world problems will be kind of represented as a graph,

30:56.240 --> 31:02.960
the graph itself isn't real-world necessarily. That's a good point, but what I mean with

31:02.960 --> 31:06.000
the real-world problem here is more something like you have some neural network that somebody

31:06.000 --> 31:11.920
really trains, so usually people use some kind of standard neural networks. And then if you look

31:11.920 --> 31:16.160
at the structure of the neural network, you can extract the computation graph from that. So

31:17.360 --> 31:22.720
the graph is as much real-world as the neural network is. It's not that you can see it on the

31:22.720 --> 31:26.880
street, of course, but there's kind of some consensus in the field of what these networks look like.

31:27.760 --> 31:32.800
So this is a problem that appears for real-world compilers when these networks are executed.

31:33.680 --> 31:37.200
While synthetic graphs is really you want to generate a graph with, you say, I don't know,

31:37.200 --> 31:43.360
I want 10,000 nodes and I want like each edge to exist with the probability of 0.1 and I want

31:43.360 --> 31:47.040
the graph to be a cyclic and those are kind of the constraints you put in. And then you just

31:47.760 --> 31:50.960
put on your random number generator. So typically these have a little bit less structure than

31:52.480 --> 31:56.480
real-world graphs. Got it. You're not generating synthetic

31:57.120 --> 32:01.280
programs and extracting the graphs from those programs or models or something like that.

32:01.280 --> 32:04.880
How about the other paper? The other paper, batch-based optimization on

32:04.880 --> 32:09.760
permutations using acquisition weighted kernels is essentially on the same problem. So

32:09.760 --> 32:14.800
it's still about finding the right order for a number of objects. But now they consider the case

32:14.800 --> 32:20.640
where computing performance for each order, so the cost function or whatever you want to call it,

32:21.600 --> 32:26.960
is really expensive. I see you can only run it a few times. So this setting you have to be smart

32:26.960 --> 32:32.240
about how you perform your training, which different configuration, which permutations you query.

32:32.240 --> 32:36.720
And Bayesian optimization is one framework that allows you to do this in a principled way. So you

32:36.720 --> 32:43.520
kind of at each step you try to find that permutation that allows you to gain the most information

32:43.520 --> 32:50.480
on the optimal configuration. Without getting into too many details here, I think my colleagues

32:50.480 --> 32:57.120
here introduced the first batch-based optimization method for optimizing over permutations.

32:57.120 --> 33:01.920
This batch means that you can kind of evaluate the number of configurations in parallel,

33:01.920 --> 33:05.760
which is really practical if you, for instance, have multiple devices, multiple GPUs during training.

33:06.560 --> 33:11.680
And again, like this other paper, I think what really stands out is after developing the algorithm

33:11.680 --> 33:17.520
that this is just state-of-the-art, it beats existing methods on standard benchmarks.

33:18.720 --> 33:22.400
As we mentioned earlier, a number of your colleagues work on

33:22.400 --> 33:30.240
a core variant deep learning, and a couple of the papers at this year's NURBS were on that topic.

33:30.240 --> 33:31.920
Can you talk a little bit about those?

33:31.920 --> 33:38.800
Yeah, we have two papers on a key variant deep learning at NURBS. The first is led by Gabriela

33:38.800 --> 33:43.520
Chesa, and three of the four authors on that paper were actually previously on your podcast.

33:43.520 --> 33:50.400
I believe there was Arash, Taco, and Max were all colleagues, or in Max's case, former colleague

33:50.400 --> 33:56.880
of mine, and who you talked at some point. Anyway, this paper is about a cryo-electron microscopy.

33:56.880 --> 33:57.920
Have you heard about that?

33:57.920 --> 34:00.080
No, tell me more about that.

34:00.080 --> 34:04.400
Yes, it's a really cool and pretty new imaging method for structural biology,

34:04.400 --> 34:09.680
and it has really revolutionized this field a little bit, and I think it also has one

34:09.680 --> 34:16.320
Nobel Prize in 2017. So the idea is, you're biologist, you have some molecules, say a protein

34:16.320 --> 34:19.600
or something, and you want to reconstruct the 3D structure of that.

34:19.600 --> 34:24.480
Now that's hard to do because molecules are kind of small. So what you can do is take a bunch

34:24.480 --> 34:30.000
of these molecules, solve them in some solution, freeze the solution in a very thin layer,

34:30.000 --> 34:36.560
and then bombard it with electrons, and image kind of the size of this. This gives you a bunch

34:36.560 --> 34:43.680
of really noisy two-dimensional images. Now the task is, of course, you have kind of have to combine

34:43.680 --> 34:49.440
these noisy two-dimensional images into the 3D structure of the molecule that you started with,

34:49.440 --> 34:54.320
and that's really difficult, one, because of the noise, and two, because you don't know the

34:54.320 --> 34:59.440
orientation of the molecules, right? They are frozen in some liquid, but you have no idea which

34:59.440 --> 35:02.960
way around. So you need to infer the pose for each of these molecules first.

35:03.760 --> 35:07.280
And this is the problem that my colleagues tackle here. They develop a new algorithm

35:07.280 --> 35:14.240
for inferring the pose of three-dimensional molecules from these trial EM images,

35:14.240 --> 35:20.560
and they tackle this using some strategy from group theory. I think it's called a group synchronization

35:20.560 --> 35:26.640
framework. It's quite technical, but the bottom line here is that they incorporate the structures,

35:26.640 --> 35:32.400
the geometric symmetries of the problem, into the algorithm as much as possible, and then they,

35:33.200 --> 35:38.080
those are smart people, they figure out that you can infer the 3D poses of these molecules

35:38.080 --> 35:44.640
from these 2D images a little bit better than before. This is the starting point to then plug

35:44.640 --> 35:50.240
the residing post-estimates into some 3D reconstruction algorithm and then get better

35:50.240 --> 35:54.560
reconstructions of these molecules. Maybe on a slightly higher level, I think this is another

35:54.560 --> 35:59.600
example for how when you take into account the geometric structure of your problem, the symmetries

35:59.600 --> 36:04.880
of a problem can really make your machine learning algorithms or classical algorithms more efficient.

36:04.880 --> 36:11.840
And that's a lot of what the group focusing on the aquarium and stuff focuses on those symmetries

36:12.480 --> 36:17.280
applied in all different places. Yeah, that's right, and maybe that's a good transition to this

36:17.280 --> 36:24.560
other paper by Arash BÃ©berodi, also a Gabriele and others called a PEC Bayesian generalization bound

36:25.280 --> 36:32.880
for aquarium networks. Now here, they really study how much you can win when you include

36:32.880 --> 36:37.280
in corporate aquarium variants or kind of the geometric structure into the architectural

36:37.280 --> 36:44.400
choices in NREL network. And they do this using a framework called PEC Bayes, so the idea is

36:44.400 --> 36:49.680
basically that you can develop a theoretical upper bound on how much your loss can become worse

36:49.680 --> 36:56.160
when you go from training data to test data. And what they do is that they show that this bound

36:56.160 --> 37:03.760
on the loss, so this kind of generalization error becomes better, the more symmetry constraints

37:03.760 --> 37:09.040
you incorporate in the architecture of your neural network. So in some sense, they theoretically show

37:09.040 --> 37:12.400
that aquarium neural networks generalize better than non-aquivariant neural networks.

37:13.120 --> 37:17.840
What I really like about this paper is that the result of this this theoretical bound also depends

37:17.840 --> 37:22.240
a little bit on the architectural choices you're making your neural network, so you can use this

37:22.240 --> 37:26.480
to provide some guidance for the design of aquivariant neural networks.

37:27.200 --> 37:30.080
How are the architectural choices parameterized?

37:30.960 --> 37:37.120
So I think this is all about what kind of representations of your group you use in the hidden

37:37.120 --> 37:42.240
layers of your neural network. So representations of a group are kind of a classification of

37:42.240 --> 37:48.000
the different ways that groups can manifest themselves on mathematical objects

37:48.000 --> 37:53.760
in particular on vector spaces. So if you have some latent variables, some hidden variables

37:53.760 --> 38:00.400
in a neural network, you can think about them, for instance, as vectors or as tensors or higher

38:00.400 --> 38:07.440
order objects, and they all transform differently under group transformations. Now, that was

38:08.160 --> 38:12.720
quite dense sentence I just said, but basically if you think about, for instance, translation

38:12.720 --> 38:17.040
symmetries, you kind of the idea that if you shift the input a little bit, the output should also

38:17.040 --> 38:22.480
shift a little bit in some way. Then one representation of this is kind of just shifting in the same

38:22.480 --> 38:27.280
way, the standard representation. Another representation is kind of the trivial representation,

38:27.280 --> 38:33.040
where you just stay constant, no matter what happens. This is exactly invariant. So if the input

38:33.040 --> 38:37.200
shifts your hidden layer or your output would kind of stay at the same point. And then there is

38:37.200 --> 38:43.760
higher order representations where when you shift something, the hidden variables or the outputs,

38:43.760 --> 38:47.360
we have in a more complicated way, but still in a way that reflects the symmetry structure of your

38:47.360 --> 38:55.680
problem. And what my colleagues find here is that these representations are that you choose for

38:55.680 --> 38:59.920
for the design of your network are related to the bound on the generalization error. So they give

38:59.920 --> 39:06.720
you some guidance on how you can build a query networks in a way that have as small as possible

39:06.720 --> 39:14.880
bound on the generalization error. The question again. There was a paper on quantization as well.

39:14.880 --> 39:21.920
Yes. Our last paper at NURBs called FP8 quantization, the power of the exponent. This was led by

39:21.920 --> 39:26.480
my colleagues Andrei Kusmin and Mart van Bealen. And I think one of the authors' time has also been

39:26.480 --> 39:32.320
on your podcast before is indeed about a neural network quantization. As you know, a neural

39:32.320 --> 39:38.560
quantization is maybe the most efficient way of making neural networks more efficient. And with

39:38.560 --> 39:45.520
that, I mean, run faster, useless memory, useless power are these things. That's quite important

39:45.520 --> 39:49.360
when you run to run things on device. And that's something that Qualcomm cares a great deal about.

39:49.360 --> 39:53.120
But it's also maybe just important from a sustainability perspective.

39:54.800 --> 39:59.680
Now, one problem with quantization is that many different formats for the representation of

39:59.680 --> 40:04.080
weights and activations have been proposed. So there are integer representations and there are

40:04.080 --> 40:07.600
floating point representations, for instance. And for floating point representations, there's also

40:07.600 --> 40:14.240
many different ways you could construct the representation. What my colleagues do in this paper

40:14.240 --> 40:22.080
is that they compare 8-bit representations in theory and practice. In particular, they compare 8-bit

40:22.080 --> 40:28.480
integer representations and 8-bit floating point representations. And they train neural networks

40:28.480 --> 40:34.080
both in a quantization-aware way, so they kind of take into account how your quantitative

40:34.080 --> 40:38.240
already doing training. And for post-training quantization, where you run the quantization as an

40:38.240 --> 40:44.240
afterburner after your neural network has converged. The result of this paper is a very clear

40:44.240 --> 40:52.960
it depends. So if you have, if you want to use post-training quantization, and especially if you have

40:52.960 --> 40:57.360
large neural networks, let's say large transformer, then a floating point quantization can be

40:57.360 --> 41:02.560
beneficial. But they also find that if you use quantization-aware training, so if you already

41:03.280 --> 41:06.560
include the knowledge that you're going to quantize your neural networks during training,

41:06.560 --> 41:12.080
then the difference becomes much smaller. And another point they make is that the performance

41:12.080 --> 41:18.560
of floating point quantization really depends on the hyper parameter structures. And this is

41:18.560 --> 41:24.720
much more sensitive to the choice of hypermills than int8 quantization. Finally, I think it's important

41:24.720 --> 41:30.240
to say that if the performance of the things is kind of equal, integer quantization has the

41:30.240 --> 41:35.840
advantage that the hardware that is compatible with running that is often much more energy efficient.

41:35.840 --> 41:42.160
So all of the things being equal, I think the go-to recommendation is maybe still int8 quantization here.

41:42.160 --> 41:48.800
Awesome, awesome. How about we quickly run through workshops and demos? Were there any of those as

41:48.800 --> 41:55.040
well this year? Yeah, I'm happy to talk about those as well. We had a couple of workshop papers.

41:55.040 --> 41:58.880
I'm a little bit biased here, but I'd like to cherry-pick just one of them because I was involved

41:58.880 --> 42:04.480
with it personally. And this is a paper called Deconfirmed Immutation Learning, led by my colleague

42:04.480 --> 42:11.760
Risto for Oreo, and presented at the deep reinforcement learning workshop. Now, you know

42:11.760 --> 42:16.800
imitation learning is about the problem of training a policy to not just maximize the reward

42:16.800 --> 42:21.040
function, but imitate behaviors in some offline data set, generate by some experts, right?

42:21.040 --> 42:24.720
And this is advantageous because it doesn't require you to come up with a reward function,

42:24.720 --> 42:28.480
but also it can be beneficial for safety, for instance, in autonomous driving.

42:28.480 --> 42:33.040
And where does the confounding problem that's apparent in the title come in?

42:33.040 --> 42:39.280
Yeah, yeah, exactly. We're getting to that. The confounding problem appears when the expert

42:39.280 --> 42:44.000
and the imitator don't see exactly the same data. Let's say the expert has access to more data

42:44.000 --> 42:49.120
in his inputs than the imitator. Maybe let me give you a silly example for this. So,

42:49.120 --> 42:54.480
let's say we are trying to solve autonomous driving and we are doing this through imitation

42:54.480 --> 42:58.800
learning where the data set is generated by some human drivers. It's a reasonable setting,

42:58.800 --> 43:02.720
but these human drivers generally will have lots of additional information that then later the

43:03.440 --> 43:07.440
autonomous agent will not have. For instance, imagine a human driver listens to the weather

43:07.440 --> 43:12.240
forecast on the morning of generating the training data. And maybe on the weather forecast,

43:12.240 --> 43:16.400
they hear that the road conditions are going to be icy. And then even if you don't see anything

43:16.400 --> 43:20.400
on the road, I bet that their driving will be a little bit slow and a little bit more careful.

43:20.400 --> 43:26.640
A little more cautious. Yeah. Exactly. And that's a problem because then later the agent in the data

43:26.640 --> 43:31.920
set observed multiple examples where the inputs look the same or the behavior of the agent is different.

43:31.920 --> 43:35.600
So what are they going to choose? Are they going to drive slowly? Are they going to drive fast?

43:35.600 --> 43:40.000
They have no way of inferring this. So they must make some random guess here. And then it gets

43:40.000 --> 43:46.800
worse because once you make an initial guess, later the agent can use its own past behavior

43:46.800 --> 43:52.240
as evidence for what the correct behavior is. This is something known as causal delusions and was

43:52.240 --> 44:00.960
pointed out in a series of papers by Pedro Ortega who used to be at DeepMind. So if you just train

44:00.960 --> 44:06.320
an imitation learning agent with a standard algorithm and it kind of decides to go fast initially

44:06.320 --> 44:11.440
and then a few time steps later, it can kind of look back and see, okay, I used to drive fast

44:11.440 --> 44:16.080
and in all the training examples where early on the driver drives fast, he continues to drive fast.

44:16.080 --> 44:20.240
So then this is kind of evidence for continuing with a high speed. Of course, that's not

44:20.240 --> 44:25.520
ideal if the road is potentially icy. So this is a slightly exaggerated example. Of course,

44:25.520 --> 44:30.080
autonomous vehicles right now do not really have exactly this problem, but maybe you get the idea.

44:30.080 --> 44:34.960
And what we do with this paper and what particular is to us in this paper is study this problem

44:34.960 --> 44:40.240
theoretically and characterize the different conditions under which we can solve it in imitation learning.

44:40.240 --> 44:47.600
And mostly limited kind of simple toy scenarios, but we also develop an algorithm that we can run

44:47.600 --> 44:52.880
on reinforcement learning or imitation learning settings and we demonstrate it on a few

44:53.920 --> 45:02.400
continuous control problems. Interesting. How toy? So for instance, we have one environment,

45:02.400 --> 45:07.840
it's one of these standard open eye gym environments where you have like a robotic arm with a few

45:07.840 --> 45:13.840
degrees of freedom and it can strike a ball. And there are some parameters that the agent doesn't know

45:13.840 --> 45:19.920
like the weight of the ball or how much it slows down later kind of the friction.

45:21.920 --> 45:25.360
And the expert knows these things. You can imagine that the expert maybe picked up the ball and

45:25.360 --> 45:30.400
placed it on the table before they hit it to play some mini golf or whatever you want to call it.

45:30.400 --> 45:35.440
So this is something if you just do it once, there's just, if you just train it on imitation learning

45:35.440 --> 45:39.360
and out there in your test environment, you have no way of knowing how hard to hit the ball.

45:39.360 --> 45:44.880
But we learn a policy that kind of does it twice and based on kind of observing how the

45:44.880 --> 45:49.520
ball behaves the first time, then then figures out the value of the friction and the mass in these

45:49.520 --> 45:55.920
things and corrects the behavior for that in the next time. So it's still pretty toy, but I think it's

45:55.920 --> 46:01.360
at least not a kind of tabular data set anymore, but it's slowly moving towards bigger problems.

46:01.360 --> 46:07.280
Awesome. How about demos? Yeah, it was a great fun. It was the first time for me to be at the

46:07.280 --> 46:12.960
Qualcomm booth at least physically. And a lot of people came by and looked at the four demos

46:12.960 --> 46:19.040
that we had there. I talk to you about what exactly they were in a second, but I think what they all

46:19.040 --> 46:27.520
have in common is kind of maybe Qualcomm AI research core competency, which is really

46:29.120 --> 46:33.760
making AI ubiquitous in the sense that we want to run machine learning systems on any kind of

46:33.760 --> 46:40.960
device. So we don't just run our systems on GPUs or even bigger systems, but we had two phones

46:40.960 --> 46:48.880
there. We had one tablet there and we had one AR goggle there and we were running our AI systems on

46:48.880 --> 46:54.400
all of these devices and of course all of these included Qualcomm ships inside. Now more

46:54.400 --> 47:02.400
concretely, we had one demo on super resolution. And here the main novelty was that we quantized

47:02.400 --> 47:07.760
the super resolution network to in four to four bit integers, which is quite a bit small and

47:07.760 --> 47:13.120
quite a bit more efficient than what has been done before at least in terms of real world demos.

47:15.120 --> 47:22.240
We have another demo on conditional compute for video understanding on device. So here the idea

47:22.240 --> 47:27.440
is that we want to solve an action recognition problem. So you see some video data and you want to

47:27.440 --> 47:32.560
recognize what is being done in that video. And most methods do this by kind of parsing the full

47:32.560 --> 47:38.080
video all frames and then classifying the action. But my colleagues developed this method called

47:38.080 --> 47:44.080
frame exit. I think it was presented in CVPR last year that it basically just looks at a few frames

47:44.080 --> 47:48.160
and then the size of it is seen enough and can make the decision already. And once it is seen enough

47:48.160 --> 47:53.520
it kind of stops the computation. It doesn't require processing of all frames and thus makes

47:53.520 --> 47:58.560
a still very efficient decision but exactly with much less computation time also running on

47:58.560 --> 48:05.120
device yet. And we had one demo on multi-model future learning on teach your AI. So we had a

48:05.920 --> 48:11.760
cute simulated robot doc on a tablet and you try to teach gestures that we are performing

48:11.760 --> 48:16.960
to this dog. So you talk to the dog and at the same time you perform some gesture to tell the

48:16.960 --> 48:22.720
dog I don't know something with your hands and and it would kind of after seeing it on your

48:22.720 --> 48:27.680
few times it would kind of learn what these gestures meant by matching what you said and what

48:27.680 --> 48:34.160
what kind of past as audio input with with the video feed. Also running on device.

48:35.200 --> 48:43.600
And then finally we had a demo on a 3D reconstruction on AR goggles. This is about the problem

48:43.600 --> 48:48.560
of depth estimation in AR. So when you have your AR goggles on to do anything with your environment

48:48.560 --> 48:52.160
you really need to kind of reconstruct the 3D scene around here right you need to know how far

48:52.160 --> 48:56.880
away are these pixels that the cameras recording from you. And in particular you want to be able to

48:56.880 --> 49:03.520
do this with very strict hardware constraints. Ideally even just from a single camera feed.

49:04.720 --> 49:10.720
So here my colleagues developed a monocular depth estimation network trained with

49:10.720 --> 49:17.440
a self-supervised learning and then quantized that and ran it on device in a different way.

49:17.440 --> 49:21.840
The processor and the AR goggles roughly equivalent to something that might be in a mobile phone.

49:21.840 --> 49:26.800
I think it's a little bit more restricted. I'm not an expert on this but I believe that this

49:26.800 --> 49:32.400
thing was a Qual Snapdragon 888 chip in there and I think these are less powerful than the

49:32.400 --> 49:36.400
you know the Snapdragon 8 Gen 2 that we just announced that was running in our one of our phone

49:36.400 --> 49:42.240
demos. Okay. Awesome. So I would like to add one thing to this and I think what's really

49:43.200 --> 49:47.200
unites these four demos and what what makes them work in addition to kind of these these ideas

49:47.200 --> 49:53.840
on the method sides is this philosophy of full stack optimization. So at Qualcomm and I research

49:53.840 --> 49:58.960
we do everything from developing the architectures thinking about the training algorithm

49:58.960 --> 50:04.320
to quantizing the neural network. We use this AMAT AI model efficiency toolkit that's

50:04.320 --> 50:09.520
open source and then finally also running it on our chips actually that you can hold in your hand

50:09.520 --> 50:14.800
at the demo booth. I think that's quite neat. It's awesome and I'll link to an interview that I

50:14.800 --> 50:21.920
did with one of your colleagues Morale on that full stack philosophy as well as some of the other

50:21.920 --> 50:28.320
interviews that you've mentioned as well. With all that said it sounds like quite a successful

50:28.320 --> 50:35.680
nerves for you and your colleagues so congratulations. I'd love to have you maybe close us up by

50:35.680 --> 50:43.680
talking a little bit about kind of where your research goes in the in the future and kind of what

50:43.680 --> 50:48.320
you're most excited about across the various things that we've talked about. Yeah thanks for

50:48.320 --> 50:53.360
the question. So this causal representation learning paper that we talked about at length

50:53.360 --> 51:00.400
earlier is really one example of how action and perception should inform each other. In the

51:00.400 --> 51:04.080
sense that in this paper we show that if we can observe the effect of actions it gives us a

51:04.080 --> 51:09.760
principled way of learning perception learning representations. But I think this this relation

51:09.760 --> 51:14.320
between action and perception goes goes further than the justice one direction and in the future

51:14.320 --> 51:18.320
my colleagues and I would really like to continue working on this and maybe also going in the

51:18.320 --> 51:25.680
other direction. Can we learn principled ways of of learning to act learning policies in an

51:25.680 --> 51:32.320
interactive environment based on ideas about causality based on ideas about structure of a system

51:32.320 --> 51:38.880
based on maybe ideas of how representations should behave under actions and that's one direction

51:38.880 --> 51:44.000
I'm very excited about. More broadly we talked about this already before but I really think we

51:44.000 --> 51:51.040
should try to move away from identifiability theorems and more towards thinking about downstream

51:51.040 --> 51:58.320
applications. But I you know what I've had really beautiful in machine learning is the following

51:58.320 --> 52:03.440
like sometimes you have this hammer based research where you have some elegant idea and some

52:03.440 --> 52:08.560
beautiful theory and then you just try to make this theory somehow work in some way and sometimes

52:08.560 --> 52:12.480
you have this nail based research where you have a really well problem you really want to solve it

52:12.480 --> 52:16.320
and the working is the only way that matters and often it ends up being more of an engineering

52:16.320 --> 52:20.800
effort than then really kind of beautiful. But every now and then these two things come together

52:20.800 --> 52:25.360
and you can really kind of hit a hammer with an air you can really use something elegant

52:25.360 --> 52:29.120
from the theory side to solve a really well problem and that's that's where the beauty is

52:29.120 --> 52:34.880
that's that's where machine learning is really fun. Awesome. Awesome. Well Johan thanks so much for

52:34.880 --> 52:40.320
taking the time to walk us through your paper and some of your colleagues work at NURPS.

52:40.320 --> 53:10.160
Yeah thanks a lot thanks for having me this was fun.

