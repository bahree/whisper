1
00:00:00,000 --> 00:00:15,440
Welcome to the Tumel AI Podcast. I'm your host, Sam Charrington.

2
00:00:15,440 --> 00:00:27,440
Alright everyone, I am here at CubeCon in San Diego and I'm with Bob Killan. Bob is

3
00:00:27,440 --> 00:00:33,520
a research cloud administrator at the University of Michigan. Bob, welcome to the Tumel AI Podcast.

4
00:00:33,520 --> 00:00:42,400
Thank you. Awesome. So we met yesterday. You were on the AI, ML, media and analyst panel

5
00:00:42,400 --> 00:00:48,240
talking a little bit about your experiences using Kubernetes to support the researchers

6
00:00:48,240 --> 00:00:52,400
there at the University of Michigan. And you're also one of the other roles you play as

7
00:00:52,400 --> 00:00:59,440
your co-chair of the CNCF's research user group. So really interested in hearing a little bit about

8
00:00:59,440 --> 00:01:05,600
your experiences with sounds like you work with Kubernetes kind of to support a broad portfolio

9
00:01:05,600 --> 00:01:10,800
of applications, not just ML and AI, is that right? We run a wide variety of applications,

10
00:01:11,600 --> 00:01:16,880
both, you know, AI and ML focused like Qflow, but we have a whole slew of other supporting

11
00:01:16,880 --> 00:01:22,720
services that our researchers consume generally on our larger HPC cluster. Tell us a little bit

12
00:01:22,720 --> 00:01:29,760
about your background. How did you get started working with Kubernetes? I moved over to ArcTS,

13
00:01:30,640 --> 00:01:34,320
Microbe Advanced Research Computing and Technology Services about three years ago,

14
00:01:34,320 --> 00:01:40,640
but before that I worked for the University of Michigan Hospital. And in 2015, we started,

15
00:01:40,640 --> 00:01:44,880
you know, going down the container route for a lot of our clinical workflows. This is sort of

16
00:01:44,880 --> 00:01:51,360
before Docker was a thing. So we were predominantly like Alexi containers. And we were first looking

17
00:01:51,360 --> 00:01:56,720
at Kubernetes to orchestrate that, but Kubernetes wasn't mature enough at the time and we went

18
00:01:56,720 --> 00:02:04,800
for mesos. From there, we then shipped it back and forth and also started supporting our research

19
00:02:04,800 --> 00:02:09,680
workloads, sort of alongside our clinical workflows on there. They sort of saw, you know, some of the

20
00:02:09,680 --> 00:02:14,720
benefits of containerization and just wanted to take advantage of the broader service that we were

21
00:02:14,720 --> 00:02:21,680
offering. From there, I was hired by ArcTS to sort of build out the same thing, but be completely

22
00:02:21,680 --> 00:02:28,480
research focused, as well as like managing a virtualization system. And Kubernetes has really

23
00:02:28,480 --> 00:02:34,160
exploded when it comes to the broader management and adoption of containers. And you know, we have

24
00:02:34,160 --> 00:02:40,240
things like the Cube Sponor in Jupyter Hub that just integrates directly with it. And from there,

25
00:02:40,240 --> 00:02:47,200
it's just you said the Cube Sponor. Cube Sponor. I'm sorry. Cube Sponor in Jupyter Hub. So I can

26
00:02:47,200 --> 00:02:53,680
spin up a notebook as a container itself. Can you maybe talk a little bit about the various

27
00:02:53,680 --> 00:03:01,520
use cases that you're supporting? What are some of your researchers doing? Oh man, it's a pretty

28
00:03:01,520 --> 00:03:08,320
broad spectrum. On top of Kubernetes, we support a bunch of social science stuff, various databases

29
00:03:08,320 --> 00:03:14,720
that are consumed by other applications, and people running workloads in our larger HPC cluster.

30
00:03:15,600 --> 00:03:23,360
And we do support, we have a significantly large Jupyter environment, a lot for classes,

31
00:03:23,360 --> 00:03:30,000
and other researchers that are spinning stuff up. Bioinformatics, physics, I think those are

32
00:03:30,000 --> 00:03:37,520
most of the big ones. Okay. And so is the primary user experience among your researchers, the

33
00:03:37,520 --> 00:03:42,800
Jupyter notebook, and the ability to spawn off containers from the notebook environment? Yep.

34
00:03:43,520 --> 00:03:48,480
Over time, we've seen a gradual shift of people, you know, sort of the moving away from the classic

35
00:03:48,480 --> 00:03:54,160
HPC style system where you would log in to like, there'd be a sage and a login node and you'd run

36
00:03:54,160 --> 00:03:58,880
your system or you'd queue something up and run it off. Now there's sort of an expectation

37
00:03:59,600 --> 00:04:04,880
for to have the Jupyter notebook or to have some other sort of science gateway for the user to

38
00:04:04,880 --> 00:04:10,080
consume. It's a lot, you know, friendlier than having to dig in and write like a, you know,

39
00:04:10,080 --> 00:04:16,640
batch script to automate some of the stuff for you. One of the things that's different between kind

40
00:04:16,640 --> 00:04:23,280
of the classical HPC scale-up systems and the more distributed environments that we see now and

41
00:04:23,280 --> 00:04:32,400
that are common with Kubernetes is that in order to take advantage of these distributed environments,

42
00:04:32,400 --> 00:04:38,240
the users in your case, the researchers have to know about them and kind of know how to use them

43
00:04:38,240 --> 00:04:42,160
in a lot of cases right there, you know, code to take advantage of distributed computing.

44
00:04:42,800 --> 00:04:48,720
Are your researchers, you know, doing that or are there, you know, things that you've put in place

45
00:04:48,720 --> 00:04:55,440
that provide abstraction so that they don't have to think about that? We do have some abstractions

46
00:04:55,440 --> 00:05:00,800
in play that make it much easier. Argo workflows and things of that nature have simplified things

47
00:05:00,800 --> 00:05:04,960
greatly for people. Then, of course, there are some power users that, you know, we'll just want

48
00:05:04,960 --> 00:05:11,440
to dive in and do everything themselves. But in general, we have a pretty good suite of tools and

49
00:05:11,440 --> 00:05:18,960
libraries to make that easy for them. The social scientists isn't kind of the target user for a

50
00:05:18,960 --> 00:05:24,320
cube flow. You know, yet they're, you know, using Kubernetes cube flow in your case, it sounds like

51
00:05:24,320 --> 00:05:30,720
the Argo workflows, you know, are what you didn't say that necessarily the social scientists were using

52
00:05:30,720 --> 00:05:36,400
the Argo workflows and cube flow. But, you know, I'm curious to hear a little bit about how cube flow

53
00:05:36,400 --> 00:05:42,160
in particular is used in this environment. Cube flow itself, we don't have any social science people

54
00:05:42,160 --> 00:05:48,960
using cube flow. The cube flow stuff is mostly used by a small subset of people. And right now,

55
00:05:48,960 --> 00:05:56,240
it's a more in an experimentation stage. But the aspects of it really sort of make, especially

56
00:05:56,240 --> 00:06:01,280
like the model lifecycle of things, significantly easier for people. And we sort of see much more

57
00:06:01,280 --> 00:06:07,520
people shifting and adopting it going forward. Maybe talk a little bit about that user experience.

58
00:06:07,520 --> 00:06:11,760
Like, what did they have to do to get their apps up and running in the cube flow environment?

59
00:06:11,760 --> 00:06:15,680
They don't have to do too much. They can do most of it from the Jupyter notebooks that

60
00:06:15,680 --> 00:06:23,360
cube flow spins up and manages for them. And then there's, I forget the Argo or the cube flow

61
00:06:23,360 --> 00:06:28,400
CLI tool. But that allows them to create some workflows pretty easy too. And there's some other

62
00:06:28,400 --> 00:06:33,200
things being built that allows for like better like designing building and building better pipelines

63
00:06:33,200 --> 00:06:41,520
and more consumable workflows. Maybe talking generally what from your participation in this panel,

64
00:06:41,520 --> 00:06:47,360
I got the impression that you're pretty excited about Kubernetes as a platform generally for

65
00:06:47,360 --> 00:06:52,720
these types of research workloads. Maybe talk a little bit about why you're excited about it.

66
00:06:52,720 --> 00:07:00,320
Sure. Kubernetes provides a lot of like proper abstractions for just managing a variety of

67
00:07:00,320 --> 00:07:06,720
workloads. And the things that it cannot handle, it offers the extension points to easily extend

68
00:07:06,720 --> 00:07:14,080
and augment. So like Kubernetes itself does not have any capability of running a like standard MPI

69
00:07:14,080 --> 00:07:19,840
job or a, you know, gang schedule job. But because the extension points are there, it's easy enough

70
00:07:19,840 --> 00:07:25,040
to sort of write your own scheduler to handle that. And it's just like a little, you know,

71
00:07:25,040 --> 00:07:30,960
one variable changed, then use that scheduler to spin up and manage that workload. And by using

72
00:07:30,960 --> 00:07:38,320
Kubernetes itself and this extension mechanism, you gain the accessibility of using everything else

73
00:07:38,320 --> 00:07:43,360
that's being developed on top of Kubernetes. Whereas in the past, you know, again, going back to

74
00:07:43,360 --> 00:07:50,400
the classic HPC system, there wasn't any of these other, this other tooling and APIs to really do

75
00:07:50,400 --> 00:07:56,960
any of this stuff. So now we can get, you can do a lot more such as like a venting and triggering

76
00:07:56,960 --> 00:08:00,960
different workflows off different things happening within the system. We're definitely seeing

77
00:08:00,960 --> 00:08:07,440
them a much more broader adoption of things like data streaming and flink and, and you know, this

78
00:08:07,440 --> 00:08:14,080
stuff you can really, you know, do well on a classic HPC system. But it works and fits quite

79
00:08:14,080 --> 00:08:17,920
well on top of Kubernetes. And you can integrate it with a whole slew of other things that are being

80
00:08:17,920 --> 00:08:23,120
built on top of it. Are you supporting users that are doing things like building out their own

81
00:08:23,120 --> 00:08:28,000
schedulers or are these things that the broader community is doing and they can just kind of take

82
00:08:28,000 --> 00:08:32,640
off the shelf and take advantage of? Right now, it's mostly from the broader community and our users

83
00:08:32,640 --> 00:08:38,480
can then take that off the shelf. Cubeflow itself has a MPI operator built into it so that, you know,

84
00:08:38,480 --> 00:08:46,000
makes it easier for the people consuming Cubeflow to spin up an MPI job. And then there's other things

85
00:08:46,000 --> 00:08:51,920
like a volcano which are trying to offer much more of the classic batch computing things that we'd

86
00:08:51,920 --> 00:08:56,800
find in HPC for Kubernetes directly. Joe down a little bit deeper into that. What is volcano

87
00:08:56,800 --> 00:09:05,280
specifically? Volcano is a open-source project that is trying to be sort of the classic HPC

88
00:09:05,280 --> 00:09:10,800
scheduler workload managers thinking like Slurm but built for Kubernetes. So it is offering

89
00:09:10,800 --> 00:09:19,200
things like fair share, cues, backfill, as well as offering things like, you know, being able to

90
00:09:19,200 --> 00:09:27,600
support game jobs and things of that nature. So the idea there being that as a research cloud

91
00:09:27,600 --> 00:09:31,360
administrator, you've got some pool of resources, you've got some pool of people that want to

92
00:09:31,360 --> 00:09:39,600
take advantage of these resources, how do you fairly or consistently give them access to the resources

93
00:09:39,600 --> 00:09:45,680
and so, you know, fair share being, you know, one idea there, gang scheduling is more like,

94
00:09:45,680 --> 00:09:49,680
I've got these five things that need to go at the same time. Exactly. How do I ensure that they're

95
00:09:49,680 --> 00:09:54,400
happening in concert with one another? Yep. And then like, if a worker happens to die, you don't want

96
00:09:54,400 --> 00:09:59,040
to necessarily kill the entire job, whereas, you know, if sort of the controller dies, you want to

97
00:09:59,040 --> 00:10:04,880
then, you know, kill and re-cute the entire job for us and for sort of the larger sites that are,

98
00:10:04,880 --> 00:10:10,560
you know, still primarily on-prem and focused on like bare metal, you know, we have a finite

99
00:10:10,560 --> 00:10:16,560
amount of resources. We aren't necessarily bursting up to the cloud. So being able to backfill and

100
00:10:16,560 --> 00:10:22,640
set priorities on things is very useful for us as well as, you know, cues. For us, it is like,

101
00:10:22,640 --> 00:10:27,360
we want to backfill with, you know, jobs from students. They might be able to run those for free,

102
00:10:27,360 --> 00:10:31,600
but if we have a, you know, researcher that's queuing something, we want to, you know, have those

103
00:10:31,600 --> 00:10:37,600
take priority and sort of bump off those lower priority jobs. And we think about in the context of

104
00:10:37,600 --> 00:10:44,880
machine learning, some of these jobs, like neural network training, deep learning training,

105
00:10:45,600 --> 00:10:50,640
they can take, you know, many, you know, days or weeks. But that's not a new thing in the context

106
00:10:50,640 --> 00:10:59,200
of HPC are the workloads that you're attending to support also, kind of these long running jobs,

107
00:10:59,200 --> 00:11:05,360
but not necessarily deep learning training. Yeah, we have, we support, again, being a,

108
00:11:05,360 --> 00:11:10,480
a, that handles the research needs for the entire university. We have a little bit of everything.

109
00:11:12,000 --> 00:11:18,400
And most of our jobs would probably fit into that category where they, you know, might take,

110
00:11:18,400 --> 00:11:23,440
you know, a week to complete or something or something of that nature. I think our max wall time

111
00:11:23,440 --> 00:11:28,160
is 21 days. Oh, wow. And can you say what that is? I don't remember off the top of my head.

112
00:11:28,960 --> 00:11:34,320
But we've had several that are just like cranking through, you know, terabytes and terabytes of

113
00:11:34,320 --> 00:11:40,560
data. And if they have something that just, you know, it's the wall time, you know, it's unfortunate

114
00:11:40,560 --> 00:11:46,400
for the researcher, but it will kill it off. Oh, so this max wall time isn't the biggest job

115
00:11:46,400 --> 00:11:51,840
that you've seen. It's the limit that you've seen. Sorry. They have something that runs longer

116
00:11:51,840 --> 00:11:56,240
than that. We're going to give them. Yeah, nice, nice, or not nice. Yeah.

117
00:11:57,840 --> 00:12:02,960
Ask this question yesterday that you disagreed with, or that you disagreed with the premise,

118
00:12:02,960 --> 00:12:08,960
and that that was that, you know, one of the things that I've, you know, noted here in

119
00:12:08,960 --> 00:12:13,440
this conference, the conference has gotten huge. I was at, I don't think I was at the first one

120
00:12:13,440 --> 00:12:21,120
in San Francisco. I may have been, but I was definitely at the second US one in Seattle. And

121
00:12:21,120 --> 00:12:28,560
that was 1500 people. And now it's 12,000. It is massive. And, you know, the kind of thought that

122
00:12:28,560 --> 00:12:35,200
I positive was, you know, that growth is not in machine learning and AI users. It's in like

123
00:12:35,200 --> 00:12:41,520
cloud native, you know, I'm building web apps users, right? And those users have very different

124
00:12:41,520 --> 00:12:50,000
concerns, potentially, you know, than the ML and AI users. And I kind of pose as a straw man.

125
00:12:50,000 --> 00:12:55,280
Like, is it possible? Or is there, you know, any concern that, you know, that growing

126
00:12:55,280 --> 00:13:02,240
kind of user base and momentum, but focused on kind of a different use case would have negative

127
00:13:02,240 --> 00:13:08,880
impacts on the ML and AI users, like, hey, you know, to do X, Y, Z and Q flow, you need feature

128
00:13:08,880 --> 00:13:14,400
or API, ABC, but that's a super low priority because none of the web app people need it.

129
00:13:14,400 --> 00:13:19,520
Yeah. And you were like, yeah, no, that's, I'm not worried about that. So yeah, reactive that

130
00:13:19,520 --> 00:13:24,960
a little bit. Some of that stuff, like, is there. So there's like still some issues with, like,

131
00:13:24,960 --> 00:13:34,240
Numa and some of the more, yeah, it's the essentially mapping of RAM to CPU core. Okay. And when

132
00:13:34,240 --> 00:13:40,480
you're scheduling a job, you really don't want to like cross Numa domains. So you really want to,

133
00:13:40,480 --> 00:13:44,640
you know, and there'll be like GPUs and things like that that are also sort of tied to that. So

134
00:13:44,640 --> 00:13:51,360
you don't want to have a pod scheduled on like this other core that then has to sort of jump and go

135
00:13:51,360 --> 00:13:56,080
through a round to a different like Numa domain and path to talk to a GPU. And so some of that stuff,

136
00:13:56,080 --> 00:14:01,280
those parameters aren't exactly there. But at this point, there's, you know, some pretty strong

137
00:14:01,920 --> 00:14:06,480
drivers for that, a lot from Intel and Vidya and those groups to sort of, you know,

138
00:14:06,480 --> 00:14:12,240
firm up those lower end components. And then some of the other things, there's like enough sort

139
00:14:12,240 --> 00:14:17,280
of extension, extension points where the core of Kubernetes might not be able to, you know,

140
00:14:17,280 --> 00:14:24,240
cater that well to a machine working AI and machine learning workloads. But an outside developer

141
00:14:24,240 --> 00:14:29,360
could, you know, hook into it or replace that component fairly easily and then enable it for,

142
00:14:29,360 --> 00:14:35,360
again, more research workloads. Yeah. Basically, the whole panel said essentially, yeah,

143
00:14:35,360 --> 00:14:39,440
no, you have to, there's nothing to look at here and don't worry about that, which was a little

144
00:14:39,440 --> 00:14:47,120
less unsatisfying. But I think the key takeaway was that the belief was that the Kubernetes core

145
00:14:47,920 --> 00:14:56,960
is general enough and has enough extension points that the specific use case, you know,

146
00:14:56,960 --> 00:15:03,360
the, the dominance of the web use case, you know, isn't necessarily going to derail its use in,

147
00:15:03,360 --> 00:15:06,800
in other cases. And I think you or someone else mentioned it and even, you know, there's a lot

148
00:15:06,800 --> 00:15:12,160
of attention paid to, it's not just the web use case isn't as homogenous as I'm making it sound.

149
00:15:12,160 --> 00:15:19,120
There are a bunch of use cases there. And like, there's enough diversity in workloads that the,

150
00:15:19,120 --> 00:15:25,840
the core is kind of staying general and extensible. You're shaking your head. Sorry. I probably should be

151
00:15:25,840 --> 00:15:32,960
answering. No, no, go ahead. Go ahead. Yeah. The core, the people behind the core, you know,

152
00:15:32,960 --> 00:15:39,520
Google board going all the way back, definitely built it in mind to be extendable. They knew from

153
00:15:39,520 --> 00:15:46,000
the get go that, you know, lots of things might come and go in and if you sort of really tied it

154
00:15:46,000 --> 00:15:51,440
to one, you know, specific design decision, you'd limit yourself in the future. Because you really

155
00:15:51,440 --> 00:15:56,400
wouldn't know, you know, what's going to happen in a couple of years. You know, who knows, you know,

156
00:15:56,400 --> 00:16:02,560
um, co scheduling or, you know, gang might actually make its way into Kubernetes core, but right now

157
00:16:02,560 --> 00:16:08,800
that's not really a priority, but it's easy enough to extend and add through, you know, the various

158
00:16:08,800 --> 00:16:16,720
extension points. What are the biggest gaps or things missing, you know, as you are trying to support

159
00:16:17,360 --> 00:16:25,840
these AI and ML workloads? The biggest gap from a general adoption standpoint, I would say is

160
00:16:25,840 --> 00:16:31,280
documentation of best practices. There's, you know, some blog posts and things like that out there,

161
00:16:31,280 --> 00:16:38,640
but there is no concise guide when it comes to how do we design and build a cluster? What setting

162
00:16:38,640 --> 00:16:45,120
should we set for this sort of thing? As part of the research user group, we've pulled, you know,

163
00:16:45,120 --> 00:16:50,880
wholesale of users and, you know, we pulled them actually like live the other day during the session.

164
00:16:50,880 --> 00:16:56,640
And documentation and just guided best practices was the number one request, both times,

165
00:16:56,640 --> 00:17:04,640
and by a wide variety of groups. I've heard similar, by the way, and one particular point

166
00:17:04,640 --> 00:17:11,920
that was raised was, you know, for the documentation that does exist and the tutorials that do exist,

167
00:17:12,560 --> 00:17:19,200
it's all very GCP focused. And so, you know, there's a whole other set of mental exercises and

168
00:17:19,200 --> 00:17:24,960
translations you need to do to get it up and running on AWS, for example. Yeah. So besides from

169
00:17:24,960 --> 00:17:32,720
documentation, besides documentation from, I would say, broader standpoint, it's a lot of those,

170
00:17:32,720 --> 00:17:39,200
the things that like volcano we're trying to add from a base functionality standpoint. So again,

171
00:17:39,200 --> 00:17:46,320
things cues, backfill, better pod priority and preemption capabilities and things like that,

172
00:17:46,320 --> 00:17:54,080
at least to support a better multi-tenant or I would say a multi workload environment.

173
00:17:55,280 --> 00:18:03,120
And what's the state of GPU support for in Kubernetes? Is it as flexible as you need it to be

174
00:18:03,120 --> 00:18:08,720
to some kind of a virus? Right now, you can't share GPUs like you can in some other environments.

175
00:18:09,840 --> 00:18:14,000
So that's that's a little bit of a problem. I don't know the stats of it. I know there's

176
00:18:14,000 --> 00:18:21,600
like an open issue on it, meaning you may support fractional CPUs or, you know, cores,

177
00:18:21,600 --> 00:18:28,080
you'd like to be able to offer users, fractional GPUs. And then there's a like on some of the

178
00:18:28,080 --> 00:18:33,680
NVGA GPUs, you can sort of like carve them up and give like, you know, half of it to a container.

179
00:18:34,320 --> 00:18:37,840
But that flexibility still isn't like, isn't really firmed up yet.

180
00:18:37,840 --> 00:18:42,160
It sounds like that's something that would have to come through Kubernetes core as opposed to an

181
00:18:42,160 --> 00:18:48,800
extension. Or I think that would be probably through that might be through Nvidia and their

182
00:18:48,800 --> 00:18:54,240
driver. I'm not 100% certain. Okay. I'd have to go back and look. There's there's a long issue

183
00:18:54,240 --> 00:19:00,320
that's been open. I think since early 2018, where there's been a lot of discussion on in sort of,

184
00:19:00,320 --> 00:19:05,920
you know, who should handle this and where should it be done? Okay. I can dig it up later too.

185
00:19:05,920 --> 00:19:12,960
Sure. Yeah. I mean, you know, curious, be curious to kind of read through its evolution. So GPU

186
00:19:12,960 --> 00:19:19,680
support, your role is kind of managing the Kubernetes cluster, kind of independent of these

187
00:19:19,680 --> 00:19:24,720
use cases. How many kind of machines, containers, nodes, are you dealing with?

188
00:19:25,520 --> 00:19:31,600
Right now, we have about 20 different research groups that are using our clusters.

189
00:19:31,600 --> 00:19:38,480
We have three separate clusters, one that we sort of use that has a bunch of our persistent

190
00:19:38,480 --> 00:19:42,720
services that we consume. And they provide services to the other clusters, so things like

191
00:19:42,720 --> 00:19:49,680
identity management and central point for logging. And then we have another cluster that is for

192
00:19:49,680 --> 00:19:58,640
our non-restricted data workloads. That's our relatively general use cluster. And I don't remember

193
00:19:58,640 --> 00:20:02,960
the stats off the top of my head, but that's about like, honestly, it's actually pretty small,

194
00:20:02,960 --> 00:20:12,240
about 14 nodes and probably 800 gigs of RAM. I don't remember the cores off the top of my head.

195
00:20:12,240 --> 00:20:18,800
Okay. And then we have another, our restricted data cluster. We do have, it's a little bit smaller.

196
00:20:19,680 --> 00:20:23,040
And we don't want the end users like talk to that one or consume that one directly,

197
00:20:23,040 --> 00:20:29,200
but we will spin up and manage workloads for them. That's mostly for healthcare.

198
00:20:29,200 --> 00:20:39,280
I kept a type of, okay, got it. When you think about the AI ML workloads that your users are asking

199
00:20:39,280 --> 00:20:45,520
for, where do you see the, where do you see things going from a user perspective? What do they

200
00:20:45,520 --> 00:20:50,000
want to do relative to where they are now? What are the things that they're asking about? What are

201
00:20:50,000 --> 00:20:55,200
the things they're curious about, excited about? Honestly, it's mostly extending in better

202
00:20:55,200 --> 00:21:02,560
integration like notebooks into the various workflows. Because we also have a classic HPC system,

203
00:21:03,440 --> 00:21:08,400
it's very hard to integrate that with the Kubernetes native ecosystem of tools.

204
00:21:10,240 --> 00:21:13,840
We've had and sort of been experimenting with a little bit of bridging the two worlds,

205
00:21:13,840 --> 00:21:21,120
because our HPC system has like 30,000 cores and dramatically larger than our Kubernetes environment.

206
00:21:21,760 --> 00:21:26,400
But it doesn't have like the, well, it sort of does, but it doesn't have like the ease of use

207
00:21:26,400 --> 00:21:30,880
that Kubernetes and our front end does. So we've been doing a little bit of like

208
00:21:31,920 --> 00:21:37,520
syncing POSIX identities, sort of the Linux UID and GID and mapping to our parallel shared

209
00:21:37,520 --> 00:21:43,360
file system underneath the hood that's consumed by both services. And for our users that has been

210
00:21:43,360 --> 00:21:49,120
sort of like the the next big evolution. And I know like some of the like national labs and

211
00:21:49,120 --> 00:21:54,960
other larger sites are also having that same issue. And maybe kind of last question,

212
00:21:54,960 --> 00:21:59,680
going back to this, you know, documentation being the the biggest gap, like how do you think

213
00:21:59,680 --> 00:22:06,640
that gets solves in a community like this? Well, there are several groups that have some internal

214
00:22:06,640 --> 00:22:12,240
documentation and they've sort of out some best practices and there's been a couple of good blog

215
00:22:12,240 --> 00:22:19,120
posts. The open AI blog post, I think from about a year ago regarding like scaling Kubernetes and

216
00:22:19,120 --> 00:22:25,440
running ML workloads on 2500 nodes was a really good one on some of the the issues that you'd run

217
00:22:25,440 --> 00:22:31,920
into that sort of thing. Yep. And I did an interview with one of the folks on that team will drop a

218
00:22:31,920 --> 00:22:37,520
link in the show notes, but continue. Cool. And like I've been talking to some of the folks that

219
00:22:37,520 --> 00:22:43,600
like certain they have some they've teased out a lot of really good best practices. For now,

220
00:22:43,600 --> 00:22:49,280
it's mostly been like time and sort of bringing it all concisely together. For now, it'll probably

221
00:22:49,280 --> 00:22:54,640
be, you know, scrubbing it of any, you know, related, you know, internal related documentation and

222
00:22:54,640 --> 00:23:01,440
then like dumping into the gate, get repo. There's a get repo for the CNCF research user group. And

223
00:23:01,440 --> 00:23:06,080
that's sort of, you know, our collection point of all all this various stuff. And once it's there,

224
00:23:06,080 --> 00:23:10,400
we'll sort of try and, you know, hit it again and make it a bit more concise.

225
00:23:10,400 --> 00:23:16,080
It sounds like a lot of the need is broader than, you know, just research users. It's everything.

226
00:23:16,080 --> 00:23:22,240
Yeah. Right. And there's a lot of people that still like don't necessarily how to run Kubernetes

227
00:23:22,240 --> 00:23:28,400
well. And then definitely heard that a lot here at CubeGun. Yeah. Yeah. You mentioned Sarn. They're

228
00:23:28,400 --> 00:23:35,520
kind of noted for using OpenStack for the infrastructure management. You use OpenStack.

229
00:23:35,520 --> 00:23:45,520
We do not use OpenStack. We use it's a proprietary virtualization product that was donated to

230
00:23:45,520 --> 00:23:51,360
the University of Michigan. It's by a local Michigan company. Cool. Well, Bob, thanks so much for

231
00:23:51,360 --> 00:23:54,080
taking a few minutes to chat with us. Sure. Thank you. Thank you.

232
00:23:57,360 --> 00:24:02,160
All right, everyone. That's our show for today. To learn more about today's guests or the

233
00:24:02,160 --> 00:24:09,280
topics mentioned in the interview, visit twomelai.com slash shows. For more information on either

234
00:24:09,280 --> 00:24:14,960
of our new study group offerings, call zone modeling and machine learning or the IBM enterprise AI

235
00:24:14,960 --> 00:24:23,040
workflow, visit twomelai.com slash learn 2020. Of course, if you like what you hear on the podcast,

236
00:24:23,040 --> 00:24:29,120
be sure to subscribe, rate, and review the show on your favorite pod catcher. Thanks so much for

237
00:24:29,120 --> 00:24:39,120
listening and catch you next time.

