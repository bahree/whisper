WEBVTT

00:00.000 --> 00:15.440
Welcome to the Tumel AI Podcast. I'm your host, Sam Charrington.

00:15.440 --> 00:27.440
Alright everyone, I am here at CubeCon in San Diego and I'm with Bob Killan. Bob is

00:27.440 --> 00:33.520
a research cloud administrator at the University of Michigan. Bob, welcome to the Tumel AI Podcast.

00:33.520 --> 00:42.400
Thank you. Awesome. So we met yesterday. You were on the AI, ML, media and analyst panel

00:42.400 --> 00:48.240
talking a little bit about your experiences using Kubernetes to support the researchers

00:48.240 --> 00:52.400
there at the University of Michigan. And you're also one of the other roles you play as

00:52.400 --> 00:59.440
your co-chair of the CNCF's research user group. So really interested in hearing a little bit about

00:59.440 --> 01:05.600
your experiences with sounds like you work with Kubernetes kind of to support a broad portfolio

01:05.600 --> 01:10.800
of applications, not just ML and AI, is that right? We run a wide variety of applications,

01:11.600 --> 01:16.880
both, you know, AI and ML focused like Qflow, but we have a whole slew of other supporting

01:16.880 --> 01:22.720
services that our researchers consume generally on our larger HPC cluster. Tell us a little bit

01:22.720 --> 01:29.760
about your background. How did you get started working with Kubernetes? I moved over to ArcTS,

01:30.640 --> 01:34.320
Microbe Advanced Research Computing and Technology Services about three years ago,

01:34.320 --> 01:40.640
but before that I worked for the University of Michigan Hospital. And in 2015, we started,

01:40.640 --> 01:44.880
you know, going down the container route for a lot of our clinical workflows. This is sort of

01:44.880 --> 01:51.360
before Docker was a thing. So we were predominantly like Alexi containers. And we were first looking

01:51.360 --> 01:56.720
at Kubernetes to orchestrate that, but Kubernetes wasn't mature enough at the time and we went

01:56.720 --> 02:04.800
for mesos. From there, we then shipped it back and forth and also started supporting our research

02:04.800 --> 02:09.680
workloads, sort of alongside our clinical workflows on there. They sort of saw, you know, some of the

02:09.680 --> 02:14.720
benefits of containerization and just wanted to take advantage of the broader service that we were

02:14.720 --> 02:21.680
offering. From there, I was hired by ArcTS to sort of build out the same thing, but be completely

02:21.680 --> 02:28.480
research focused, as well as like managing a virtualization system. And Kubernetes has really

02:28.480 --> 02:34.160
exploded when it comes to the broader management and adoption of containers. And you know, we have

02:34.160 --> 02:40.240
things like the Cube Sponor in Jupyter Hub that just integrates directly with it. And from there,

02:40.240 --> 02:47.200
it's just you said the Cube Sponor. Cube Sponor. I'm sorry. Cube Sponor in Jupyter Hub. So I can

02:47.200 --> 02:53.680
spin up a notebook as a container itself. Can you maybe talk a little bit about the various

02:53.680 --> 03:01.520
use cases that you're supporting? What are some of your researchers doing? Oh man, it's a pretty

03:01.520 --> 03:08.320
broad spectrum. On top of Kubernetes, we support a bunch of social science stuff, various databases

03:08.320 --> 03:14.720
that are consumed by other applications, and people running workloads in our larger HPC cluster.

03:15.600 --> 03:23.360
And we do support, we have a significantly large Jupyter environment, a lot for classes,

03:23.360 --> 03:30.000
and other researchers that are spinning stuff up. Bioinformatics, physics, I think those are

03:30.000 --> 03:37.520
most of the big ones. Okay. And so is the primary user experience among your researchers, the

03:37.520 --> 03:42.800
Jupyter notebook, and the ability to spawn off containers from the notebook environment? Yep.

03:43.520 --> 03:48.480
Over time, we've seen a gradual shift of people, you know, sort of the moving away from the classic

03:48.480 --> 03:54.160
HPC style system where you would log in to like, there'd be a sage and a login node and you'd run

03:54.160 --> 03:58.880
your system or you'd queue something up and run it off. Now there's sort of an expectation

03:59.600 --> 04:04.880
for to have the Jupyter notebook or to have some other sort of science gateway for the user to

04:04.880 --> 04:10.080
consume. It's a lot, you know, friendlier than having to dig in and write like a, you know,

04:10.080 --> 04:16.640
batch script to automate some of the stuff for you. One of the things that's different between kind

04:16.640 --> 04:23.280
of the classical HPC scale-up systems and the more distributed environments that we see now and

04:23.280 --> 04:32.400
that are common with Kubernetes is that in order to take advantage of these distributed environments,

04:32.400 --> 04:38.240
the users in your case, the researchers have to know about them and kind of know how to use them

04:38.240 --> 04:42.160
in a lot of cases right there, you know, code to take advantage of distributed computing.

04:42.800 --> 04:48.720
Are your researchers, you know, doing that or are there, you know, things that you've put in place

04:48.720 --> 04:55.440
that provide abstraction so that they don't have to think about that? We do have some abstractions

04:55.440 --> 05:00.800
in play that make it much easier. Argo workflows and things of that nature have simplified things

05:00.800 --> 05:04.960
greatly for people. Then, of course, there are some power users that, you know, we'll just want

05:04.960 --> 05:11.440
to dive in and do everything themselves. But in general, we have a pretty good suite of tools and

05:11.440 --> 05:18.960
libraries to make that easy for them. The social scientists isn't kind of the target user for a

05:18.960 --> 05:24.320
cube flow. You know, yet they're, you know, using Kubernetes cube flow in your case, it sounds like

05:24.320 --> 05:30.720
the Argo workflows, you know, are what you didn't say that necessarily the social scientists were using

05:30.720 --> 05:36.400
the Argo workflows and cube flow. But, you know, I'm curious to hear a little bit about how cube flow

05:36.400 --> 05:42.160
in particular is used in this environment. Cube flow itself, we don't have any social science people

05:42.160 --> 05:48.960
using cube flow. The cube flow stuff is mostly used by a small subset of people. And right now,

05:48.960 --> 05:56.240
it's a more in an experimentation stage. But the aspects of it really sort of make, especially

05:56.240 --> 06:01.280
like the model lifecycle of things, significantly easier for people. And we sort of see much more

06:01.280 --> 06:07.520
people shifting and adopting it going forward. Maybe talk a little bit about that user experience.

06:07.520 --> 06:11.760
Like, what did they have to do to get their apps up and running in the cube flow environment?

06:11.760 --> 06:15.680
They don't have to do too much. They can do most of it from the Jupyter notebooks that

06:15.680 --> 06:23.360
cube flow spins up and manages for them. And then there's, I forget the Argo or the cube flow

06:23.360 --> 06:28.400
CLI tool. But that allows them to create some workflows pretty easy too. And there's some other

06:28.400 --> 06:33.200
things being built that allows for like better like designing building and building better pipelines

06:33.200 --> 06:41.520
and more consumable workflows. Maybe talking generally what from your participation in this panel,

06:41.520 --> 06:47.360
I got the impression that you're pretty excited about Kubernetes as a platform generally for

06:47.360 --> 06:52.720
these types of research workloads. Maybe talk a little bit about why you're excited about it.

06:52.720 --> 07:00.320
Sure. Kubernetes provides a lot of like proper abstractions for just managing a variety of

07:00.320 --> 07:06.720
workloads. And the things that it cannot handle, it offers the extension points to easily extend

07:06.720 --> 07:14.080
and augment. So like Kubernetes itself does not have any capability of running a like standard MPI

07:14.080 --> 07:19.840
job or a, you know, gang schedule job. But because the extension points are there, it's easy enough

07:19.840 --> 07:25.040
to sort of write your own scheduler to handle that. And it's just like a little, you know,

07:25.040 --> 07:30.960
one variable changed, then use that scheduler to spin up and manage that workload. And by using

07:30.960 --> 07:38.320
Kubernetes itself and this extension mechanism, you gain the accessibility of using everything else

07:38.320 --> 07:43.360
that's being developed on top of Kubernetes. Whereas in the past, you know, again, going back to

07:43.360 --> 07:50.400
the classic HPC system, there wasn't any of these other, this other tooling and APIs to really do

07:50.400 --> 07:56.960
any of this stuff. So now we can get, you can do a lot more such as like a venting and triggering

07:56.960 --> 08:00.960
different workflows off different things happening within the system. We're definitely seeing

08:00.960 --> 08:07.440
them a much more broader adoption of things like data streaming and flink and, and you know, this

08:07.440 --> 08:14.080
stuff you can really, you know, do well on a classic HPC system. But it works and fits quite

08:14.080 --> 08:17.920
well on top of Kubernetes. And you can integrate it with a whole slew of other things that are being

08:17.920 --> 08:23.120
built on top of it. Are you supporting users that are doing things like building out their own

08:23.120 --> 08:28.000
schedulers or are these things that the broader community is doing and they can just kind of take

08:28.000 --> 08:32.640
off the shelf and take advantage of? Right now, it's mostly from the broader community and our users

08:32.640 --> 08:38.480
can then take that off the shelf. Cubeflow itself has a MPI operator built into it so that, you know,

08:38.480 --> 08:46.000
makes it easier for the people consuming Cubeflow to spin up an MPI job. And then there's other things

08:46.000 --> 08:51.920
like a volcano which are trying to offer much more of the classic batch computing things that we'd

08:51.920 --> 08:56.800
find in HPC for Kubernetes directly. Joe down a little bit deeper into that. What is volcano

08:56.800 --> 09:05.280
specifically? Volcano is a open-source project that is trying to be sort of the classic HPC

09:05.280 --> 09:10.800
scheduler workload managers thinking like Slurm but built for Kubernetes. So it is offering

09:10.800 --> 09:19.200
things like fair share, cues, backfill, as well as offering things like, you know, being able to

09:19.200 --> 09:27.600
support game jobs and things of that nature. So the idea there being that as a research cloud

09:27.600 --> 09:31.360
administrator, you've got some pool of resources, you've got some pool of people that want to

09:31.360 --> 09:39.600
take advantage of these resources, how do you fairly or consistently give them access to the resources

09:39.600 --> 09:45.680
and so, you know, fair share being, you know, one idea there, gang scheduling is more like,

09:45.680 --> 09:49.680
I've got these five things that need to go at the same time. Exactly. How do I ensure that they're

09:49.680 --> 09:54.400
happening in concert with one another? Yep. And then like, if a worker happens to die, you don't want

09:54.400 --> 09:59.040
to necessarily kill the entire job, whereas, you know, if sort of the controller dies, you want to

09:59.040 --> 10:04.880
then, you know, kill and re-cute the entire job for us and for sort of the larger sites that are,

10:04.880 --> 10:10.560
you know, still primarily on-prem and focused on like bare metal, you know, we have a finite

10:10.560 --> 10:16.560
amount of resources. We aren't necessarily bursting up to the cloud. So being able to backfill and

10:16.560 --> 10:22.640
set priorities on things is very useful for us as well as, you know, cues. For us, it is like,

10:22.640 --> 10:27.360
we want to backfill with, you know, jobs from students. They might be able to run those for free,

10:27.360 --> 10:31.600
but if we have a, you know, researcher that's queuing something, we want to, you know, have those

10:31.600 --> 10:37.600
take priority and sort of bump off those lower priority jobs. And we think about in the context of

10:37.600 --> 10:44.880
machine learning, some of these jobs, like neural network training, deep learning training,

10:45.600 --> 10:50.640
they can take, you know, many, you know, days or weeks. But that's not a new thing in the context

10:50.640 --> 10:59.200
of HPC are the workloads that you're attending to support also, kind of these long running jobs,

10:59.200 --> 11:05.360
but not necessarily deep learning training. Yeah, we have, we support, again, being a,

11:05.360 --> 11:10.480
a, that handles the research needs for the entire university. We have a little bit of everything.

11:12.000 --> 11:18.400
And most of our jobs would probably fit into that category where they, you know, might take,

11:18.400 --> 11:23.440
you know, a week to complete or something or something of that nature. I think our max wall time

11:23.440 --> 11:28.160
is 21 days. Oh, wow. And can you say what that is? I don't remember off the top of my head.

11:28.960 --> 11:34.320
But we've had several that are just like cranking through, you know, terabytes and terabytes of

11:34.320 --> 11:40.560
data. And if they have something that just, you know, it's the wall time, you know, it's unfortunate

11:40.560 --> 11:46.400
for the researcher, but it will kill it off. Oh, so this max wall time isn't the biggest job

11:46.400 --> 11:51.840
that you've seen. It's the limit that you've seen. Sorry. They have something that runs longer

11:51.840 --> 11:56.240
than that. We're going to give them. Yeah, nice, nice, or not nice. Yeah.

11:57.840 --> 12:02.960
Ask this question yesterday that you disagreed with, or that you disagreed with the premise,

12:02.960 --> 12:08.960
and that that was that, you know, one of the things that I've, you know, noted here in

12:08.960 --> 12:13.440
this conference, the conference has gotten huge. I was at, I don't think I was at the first one

12:13.440 --> 12:21.120
in San Francisco. I may have been, but I was definitely at the second US one in Seattle. And

12:21.120 --> 12:28.560
that was 1500 people. And now it's 12,000. It is massive. And, you know, the kind of thought that

12:28.560 --> 12:35.200
I positive was, you know, that growth is not in machine learning and AI users. It's in like

12:35.200 --> 12:41.520
cloud native, you know, I'm building web apps users, right? And those users have very different

12:41.520 --> 12:50.000
concerns, potentially, you know, than the ML and AI users. And I kind of pose as a straw man.

12:50.000 --> 12:55.280
Like, is it possible? Or is there, you know, any concern that, you know, that growing

12:55.280 --> 13:02.240
kind of user base and momentum, but focused on kind of a different use case would have negative

13:02.240 --> 13:08.880
impacts on the ML and AI users, like, hey, you know, to do X, Y, Z and Q flow, you need feature

13:08.880 --> 13:14.400
or API, ABC, but that's a super low priority because none of the web app people need it.

13:14.400 --> 13:19.520
Yeah. And you were like, yeah, no, that's, I'm not worried about that. So yeah, reactive that

13:19.520 --> 13:24.960
a little bit. Some of that stuff, like, is there. So there's like still some issues with, like,

13:24.960 --> 13:34.240
Numa and some of the more, yeah, it's the essentially mapping of RAM to CPU core. Okay. And when

13:34.240 --> 13:40.480
you're scheduling a job, you really don't want to like cross Numa domains. So you really want to,

13:40.480 --> 13:44.640
you know, and there'll be like GPUs and things like that that are also sort of tied to that. So

13:44.640 --> 13:51.360
you don't want to have a pod scheduled on like this other core that then has to sort of jump and go

13:51.360 --> 13:56.080
through a round to a different like Numa domain and path to talk to a GPU. And so some of that stuff,

13:56.080 --> 14:01.280
those parameters aren't exactly there. But at this point, there's, you know, some pretty strong

14:01.920 --> 14:06.480
drivers for that, a lot from Intel and Vidya and those groups to sort of, you know,

14:06.480 --> 14:12.240
firm up those lower end components. And then some of the other things, there's like enough sort

14:12.240 --> 14:17.280
of extension, extension points where the core of Kubernetes might not be able to, you know,

14:17.280 --> 14:24.240
cater that well to a machine working AI and machine learning workloads. But an outside developer

14:24.240 --> 14:29.360
could, you know, hook into it or replace that component fairly easily and then enable it for,

14:29.360 --> 14:35.360
again, more research workloads. Yeah. Basically, the whole panel said essentially, yeah,

14:35.360 --> 14:39.440
no, you have to, there's nothing to look at here and don't worry about that, which was a little

14:39.440 --> 14:47.120
less unsatisfying. But I think the key takeaway was that the belief was that the Kubernetes core

14:47.920 --> 14:56.960
is general enough and has enough extension points that the specific use case, you know,

14:56.960 --> 15:03.360
the, the dominance of the web use case, you know, isn't necessarily going to derail its use in,

15:03.360 --> 15:06.800
in other cases. And I think you or someone else mentioned it and even, you know, there's a lot

15:06.800 --> 15:12.160
of attention paid to, it's not just the web use case isn't as homogenous as I'm making it sound.

15:12.160 --> 15:19.120
There are a bunch of use cases there. And like, there's enough diversity in workloads that the,

15:19.120 --> 15:25.840
the core is kind of staying general and extensible. You're shaking your head. Sorry. I probably should be

15:25.840 --> 15:32.960
answering. No, no, go ahead. Go ahead. Yeah. The core, the people behind the core, you know,

15:32.960 --> 15:39.520
Google board going all the way back, definitely built it in mind to be extendable. They knew from

15:39.520 --> 15:46.000
the get go that, you know, lots of things might come and go in and if you sort of really tied it

15:46.000 --> 15:51.440
to one, you know, specific design decision, you'd limit yourself in the future. Because you really

15:51.440 --> 15:56.400
wouldn't know, you know, what's going to happen in a couple of years. You know, who knows, you know,

15:56.400 --> 16:02.560
um, co scheduling or, you know, gang might actually make its way into Kubernetes core, but right now

16:02.560 --> 16:08.800
that's not really a priority, but it's easy enough to extend and add through, you know, the various

16:08.800 --> 16:16.720
extension points. What are the biggest gaps or things missing, you know, as you are trying to support

16:17.360 --> 16:25.840
these AI and ML workloads? The biggest gap from a general adoption standpoint, I would say is

16:25.840 --> 16:31.280
documentation of best practices. There's, you know, some blog posts and things like that out there,

16:31.280 --> 16:38.640
but there is no concise guide when it comes to how do we design and build a cluster? What setting

16:38.640 --> 16:45.120
should we set for this sort of thing? As part of the research user group, we've pulled, you know,

16:45.120 --> 16:50.880
wholesale of users and, you know, we pulled them actually like live the other day during the session.

16:50.880 --> 16:56.640
And documentation and just guided best practices was the number one request, both times,

16:56.640 --> 17:04.640
and by a wide variety of groups. I've heard similar, by the way, and one particular point

17:04.640 --> 17:11.920
that was raised was, you know, for the documentation that does exist and the tutorials that do exist,

17:12.560 --> 17:19.200
it's all very GCP focused. And so, you know, there's a whole other set of mental exercises and

17:19.200 --> 17:24.960
translations you need to do to get it up and running on AWS, for example. Yeah. So besides from

17:24.960 --> 17:32.720
documentation, besides documentation from, I would say, broader standpoint, it's a lot of those,

17:32.720 --> 17:39.200
the things that like volcano we're trying to add from a base functionality standpoint. So again,

17:39.200 --> 17:46.320
things cues, backfill, better pod priority and preemption capabilities and things like that,

17:46.320 --> 17:54.080
at least to support a better multi-tenant or I would say a multi workload environment.

17:55.280 --> 18:03.120
And what's the state of GPU support for in Kubernetes? Is it as flexible as you need it to be

18:03.120 --> 18:08.720
to some kind of a virus? Right now, you can't share GPUs like you can in some other environments.

18:09.840 --> 18:14.000
So that's that's a little bit of a problem. I don't know the stats of it. I know there's

18:14.000 --> 18:21.600
like an open issue on it, meaning you may support fractional CPUs or, you know, cores,

18:21.600 --> 18:28.080
you'd like to be able to offer users, fractional GPUs. And then there's a like on some of the

18:28.080 --> 18:33.680
NVGA GPUs, you can sort of like carve them up and give like, you know, half of it to a container.

18:34.320 --> 18:37.840
But that flexibility still isn't like, isn't really firmed up yet.

18:37.840 --> 18:42.160
It sounds like that's something that would have to come through Kubernetes core as opposed to an

18:42.160 --> 18:48.800
extension. Or I think that would be probably through that might be through Nvidia and their

18:48.800 --> 18:54.240
driver. I'm not 100% certain. Okay. I'd have to go back and look. There's there's a long issue

18:54.240 --> 19:00.320
that's been open. I think since early 2018, where there's been a lot of discussion on in sort of,

19:00.320 --> 19:05.920
you know, who should handle this and where should it be done? Okay. I can dig it up later too.

19:05.920 --> 19:12.960
Sure. Yeah. I mean, you know, curious, be curious to kind of read through its evolution. So GPU

19:12.960 --> 19:19.680
support, your role is kind of managing the Kubernetes cluster, kind of independent of these

19:19.680 --> 19:24.720
use cases. How many kind of machines, containers, nodes, are you dealing with?

19:25.520 --> 19:31.600
Right now, we have about 20 different research groups that are using our clusters.

19:31.600 --> 19:38.480
We have three separate clusters, one that we sort of use that has a bunch of our persistent

19:38.480 --> 19:42.720
services that we consume. And they provide services to the other clusters, so things like

19:42.720 --> 19:49.680
identity management and central point for logging. And then we have another cluster that is for

19:49.680 --> 19:58.640
our non-restricted data workloads. That's our relatively general use cluster. And I don't remember

19:58.640 --> 20:02.960
the stats off the top of my head, but that's about like, honestly, it's actually pretty small,

20:02.960 --> 20:12.240
about 14 nodes and probably 800 gigs of RAM. I don't remember the cores off the top of my head.

20:12.240 --> 20:18.800
Okay. And then we have another, our restricted data cluster. We do have, it's a little bit smaller.

20:19.680 --> 20:23.040
And we don't want the end users like talk to that one or consume that one directly,

20:23.040 --> 20:29.200
but we will spin up and manage workloads for them. That's mostly for healthcare.

20:29.200 --> 20:39.280
I kept a type of, okay, got it. When you think about the AI ML workloads that your users are asking

20:39.280 --> 20:45.520
for, where do you see the, where do you see things going from a user perspective? What do they

20:45.520 --> 20:50.000
want to do relative to where they are now? What are the things that they're asking about? What are

20:50.000 --> 20:55.200
the things they're curious about, excited about? Honestly, it's mostly extending in better

20:55.200 --> 21:02.560
integration like notebooks into the various workflows. Because we also have a classic HPC system,

21:03.440 --> 21:08.400
it's very hard to integrate that with the Kubernetes native ecosystem of tools.

21:10.240 --> 21:13.840
We've had and sort of been experimenting with a little bit of bridging the two worlds,

21:13.840 --> 21:21.120
because our HPC system has like 30,000 cores and dramatically larger than our Kubernetes environment.

21:21.760 --> 21:26.400
But it doesn't have like the, well, it sort of does, but it doesn't have like the ease of use

21:26.400 --> 21:30.880
that Kubernetes and our front end does. So we've been doing a little bit of like

21:31.920 --> 21:37.520
syncing POSIX identities, sort of the Linux UID and GID and mapping to our parallel shared

21:37.520 --> 21:43.360
file system underneath the hood that's consumed by both services. And for our users that has been

21:43.360 --> 21:49.120
sort of like the the next big evolution. And I know like some of the like national labs and

21:49.120 --> 21:54.960
other larger sites are also having that same issue. And maybe kind of last question,

21:54.960 --> 21:59.680
going back to this, you know, documentation being the the biggest gap, like how do you think

21:59.680 --> 22:06.640
that gets solves in a community like this? Well, there are several groups that have some internal

22:06.640 --> 22:12.240
documentation and they've sort of out some best practices and there's been a couple of good blog

22:12.240 --> 22:19.120
posts. The open AI blog post, I think from about a year ago regarding like scaling Kubernetes and

22:19.120 --> 22:25.440
running ML workloads on 2500 nodes was a really good one on some of the the issues that you'd run

22:25.440 --> 22:31.920
into that sort of thing. Yep. And I did an interview with one of the folks on that team will drop a

22:31.920 --> 22:37.520
link in the show notes, but continue. Cool. And like I've been talking to some of the folks that

22:37.520 --> 22:43.600
like certain they have some they've teased out a lot of really good best practices. For now,

22:43.600 --> 22:49.280
it's mostly been like time and sort of bringing it all concisely together. For now, it'll probably

22:49.280 --> 22:54.640
be, you know, scrubbing it of any, you know, related, you know, internal related documentation and

22:54.640 --> 23:01.440
then like dumping into the gate, get repo. There's a get repo for the CNCF research user group. And

23:01.440 --> 23:06.080
that's sort of, you know, our collection point of all all this various stuff. And once it's there,

23:06.080 --> 23:10.400
we'll sort of try and, you know, hit it again and make it a bit more concise.

23:10.400 --> 23:16.080
It sounds like a lot of the need is broader than, you know, just research users. It's everything.

23:16.080 --> 23:22.240
Yeah. Right. And there's a lot of people that still like don't necessarily how to run Kubernetes

23:22.240 --> 23:28.400
well. And then definitely heard that a lot here at CubeGun. Yeah. Yeah. You mentioned Sarn. They're

23:28.400 --> 23:35.520
kind of noted for using OpenStack for the infrastructure management. You use OpenStack.

23:35.520 --> 23:45.520
We do not use OpenStack. We use it's a proprietary virtualization product that was donated to

23:45.520 --> 23:51.360
the University of Michigan. It's by a local Michigan company. Cool. Well, Bob, thanks so much for

23:51.360 --> 23:54.080
taking a few minutes to chat with us. Sure. Thank you. Thank you.

23:57.360 --> 24:02.160
All right, everyone. That's our show for today. To learn more about today's guests or the

24:02.160 --> 24:09.280
topics mentioned in the interview, visit twomelai.com slash shows. For more information on either

24:09.280 --> 24:14.960
of our new study group offerings, call zone modeling and machine learning or the IBM enterprise AI

24:14.960 --> 24:23.040
workflow, visit twomelai.com slash learn 2020. Of course, if you like what you hear on the podcast,

24:23.040 --> 24:29.120
be sure to subscribe, rate, and review the show on your favorite pod catcher. Thanks so much for

24:29.120 --> 24:39.120
listening and catch you next time.

