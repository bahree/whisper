WEBVTT

00:00.000 --> 00:17.040
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

00:17.040 --> 00:22.040
people doing interesting things in machine learning and artificial intelligence.

00:22.040 --> 00:25.080
I'm your host Sam Charrington.

00:25.080 --> 00:30.720
Thanks so much to everyone who sent in their favorite quote from last week's podcast.

00:30.720 --> 00:32.720
Your stickers are on the way.

00:32.720 --> 00:36.720
We had a bunch of fun with this contest and we've decided to continue it while our sticker

00:36.720 --> 00:38.320
supplies last.

00:38.320 --> 00:42.200
So definitely send us your favorite quote from today's show as well.

00:42.200 --> 00:47.880
You can do that via a comment or post on Facebook, Twitter, YouTube or SoundCloud as well as

00:47.880 --> 00:51.160
the show notes page for any episode of the show.

00:51.160 --> 00:54.520
Okay, and now about today's show.

00:54.520 --> 00:56.520
I've got a very special guest this week.

00:56.520 --> 01:01.520
His name is Evan Wright and he's principal data scientist at Anomaly.

01:01.520 --> 01:06.600
If that name sounds familiar, it's because Evan was the winner of our O'Reilly strata

01:06.600 --> 01:10.360
Hadoop World ticket giveaway earlier this month.

01:10.360 --> 01:14.440
Evan and I met up at the conference last week and got to chat about a number of topics

01:14.440 --> 01:18.240
in the realm of machine learning and cybersecurity.

01:18.240 --> 01:22.600
We discussed the three big problems in cybersecurity that ML can help with.

01:22.600 --> 01:27.480
The challenges of acquiring ground truth and cybersecurity in some ways to do it and

01:27.480 --> 01:33.440
the use of decision trees, generative adversarial networks and other algorithms in that field.

01:33.440 --> 01:36.960
I think you really enjoy this show.

01:36.960 --> 01:41.160
Speaking of cybersecurity, that's just one of the many topics that will be covered in

01:41.160 --> 01:46.880
the future of data summit event that I'm hosting May 15th and 16th at the inter-op conference

01:46.880 --> 01:48.760
in Las Vegas.

01:48.760 --> 01:54.600
At the event, Diana Kelly, a global executive security advisor with IBM, will be talking

01:54.600 --> 01:58.760
about the future of securing cloud, IoT and big data systems.

01:58.760 --> 02:04.240
We'll also be giving you a glimpse into the future of machine learning in AI, so-called

02:04.240 --> 02:10.720
fog or edge computing, augmented and virtual reality, blockchain, algorithmic IT operations

02:10.720 --> 02:12.440
and much more.

02:12.440 --> 02:18.040
You can learn more about the summit at twimmolai.com slash future of data.

02:18.040 --> 02:20.040
And now on to the show.

02:20.040 --> 02:29.280
All right, hey, everyone.

02:29.280 --> 02:33.520
I am here with Evan Wright, Principal Data Scientist with Anomaly.

02:33.520 --> 02:39.960
You may remember Evan's name because we announced him as the winner of our O'Reilly Stratta

02:39.960 --> 02:42.840
Hadoop ticket giveaway.

02:42.840 --> 02:46.840
Evan and I decided to get together and record a little show.

02:46.840 --> 02:48.640
Evan, say hi.

02:48.640 --> 02:49.640
Good morning, everybody.

02:49.640 --> 02:53.320
Good afternoon or good evening as it applies.

02:53.320 --> 02:54.320
Awesome.

02:54.320 --> 03:00.920
Evan, why don't we start by having you talk a little bit about the company you're at

03:00.920 --> 03:02.240
and what you do there?

03:02.240 --> 03:03.240
Sure.

03:03.240 --> 03:04.240
Sure.

03:04.240 --> 03:07.120
And Anomaly, we are focused on threat intelligence.

03:07.120 --> 03:12.680
I think there's really three big problems in cybersecurity that are really ripe for using

03:12.680 --> 03:15.400
machine learning to improve.

03:15.400 --> 03:18.680
One problem is malware detection.

03:18.680 --> 03:23.680
Another problem is threat intelligence, which is really the focus of our work.

03:23.680 --> 03:29.000
And another one is sort of stream detection of when you've got a series of events like

03:29.000 --> 03:33.960
for example, network traffic, trying to sift through that and find the security relevant

03:33.960 --> 03:36.200
concerns out of that stream.

03:36.200 --> 03:42.880
So we're focused on the latter two the most, especially as far as understanding threat

03:42.880 --> 03:44.400
intelligence better.

03:44.400 --> 03:51.000
So threat intelligence is this idea of imagine you're conducting an investigation like if

03:51.000 --> 03:56.040
you're a law enforcement, you would collect pieces of evidence and try and stitch together

03:56.040 --> 03:58.640
the story of what happened.

03:58.640 --> 04:04.000
And if possible, try and stop things, stop bad things happening before they actually

04:04.000 --> 04:05.000
occur.

04:05.000 --> 04:08.160
So something similar is the case in organizations.

04:08.160 --> 04:14.000
They run these security teams and security teams want to know things like of all the things

04:14.000 --> 04:19.920
that our network is talking out to on the internet are some of those malicious, right?

04:19.920 --> 04:21.400
And they want to be able to separate that.

04:21.400 --> 04:27.760
So threat intelligence, fundamentally, is understanding the tools and the infrastructure

04:27.760 --> 04:34.640
out on the internet and in your network to be able to separate those, right?

04:34.640 --> 04:39.880
So very pragmatically, it's something like a list of IPs or domains to start with,

04:39.880 --> 04:47.800
which if your network has activity going to these IPs or domains, then it would be malicious.

04:47.800 --> 04:52.720
So a lot of our main focus at anomaly is we create a threat intelligence platform.

04:52.720 --> 04:56.800
The idea is there's all these security vendors out there which kind of give us this ground

04:56.800 --> 04:57.800
truth.

04:57.800 --> 05:00.280
And in security, ground truth is so, so important.

05:00.280 --> 05:06.880
It is really the fundamental problem and why machine learning is hard in cybersecurity

05:06.880 --> 05:08.960
because ground truth is really expensive.

05:08.960 --> 05:16.840
We're not exactly spoiled in some sense, like having image data, like having video data.

05:16.840 --> 05:22.200
Because in those spaces, you have a very clear sense of what data should I collect and

05:22.200 --> 05:23.560
what data do I care about.

05:23.560 --> 05:27.760
If you look at an image, it's easy to tell if it's a cat or not to have that ground truth

05:27.760 --> 05:29.880
for labeling images.

05:29.880 --> 05:32.280
In cybersecurity, it's much, much harder.

05:32.280 --> 05:36.760
Instead of just being a layman who can look at an image and tell you it's a cat, in the

05:36.760 --> 05:41.280
case of cybersecurity, usually you need a very specialized expert to be able to tell you

05:41.280 --> 05:44.480
if you want really good confidence.

05:44.480 --> 05:52.560
They often need to be even more specialized because the best ground truth comes from analyzing

05:52.560 --> 05:55.920
reverse engineering pieces of software.

05:55.920 --> 06:01.760
So reverse engineering how it works, maybe there's, maybe the software calls out to components

06:01.760 --> 06:06.280
out on the internet that are malicious.

06:06.280 --> 06:08.520
And you need to be able to pull those out.

06:08.520 --> 06:13.080
So all of these indicators are essentially pieces of evidence that we try and put together

06:13.080 --> 06:18.920
and organizations want to alert on and stop or at least investigate if it's happening

06:18.920 --> 06:20.120
in their organization.

06:20.120 --> 06:24.640
So our company provides a stream of this threat intelligence.

06:24.640 --> 06:27.920
And that product is incidentally called threat stream.

06:27.920 --> 06:29.640
Okay.

06:29.640 --> 06:37.040
So it sounds analogous to maybe the kind of signatures that an antivirus type of company

06:37.040 --> 06:42.640
would produce, is that a fair assessment or maybe even another way of asking that.

06:42.640 --> 06:46.400
Like what makes the stream streamy?

06:46.400 --> 06:49.800
So different people can get this ground truth in different ways.

06:49.800 --> 06:53.680
So some of them are, in fact, antivirus companies.

06:53.680 --> 07:00.560
Some of them are organizations that do sort of a forensic investigation on incidents that

07:00.560 --> 07:03.400
happen after the fact.

07:03.400 --> 07:07.920
Some of them are, in fact, applying machine learning given their different data sets.

07:07.920 --> 07:17.480
So everyone has sort of a different strategy, both in their data collection and in how they

07:17.480 --> 07:18.560
validate it.

07:18.560 --> 07:22.800
Some of it is extremely qualitative by hiring cybersecurity experts.

07:22.800 --> 07:27.440
Some of it is extremely data driven and quantitative and pretty much everything you can imagine

07:27.440 --> 07:28.440
in between.

07:28.440 --> 07:32.880
And organizations really struggle from this problem of which one do I pick?

07:32.880 --> 07:38.000
And then if they can decide that, then they still have this issue of how reliable is that

07:38.000 --> 07:39.320
stream.

07:39.320 --> 07:45.400
So one of the things that in our platform we do, our machine learning use case, is to give

07:45.400 --> 07:51.400
people an assessment of how confident are we in this indicator from the stream.

07:51.400 --> 07:59.600
So we're able to, you buy sort of a big database of, this is what all the badness is.

07:59.600 --> 08:02.560
You pay a particular provider.

08:02.560 --> 08:12.480
Then we run our own largely regression based, like ensemble regression, on the input feeds

08:12.480 --> 08:14.240
that they give us.

08:14.240 --> 08:18.720
And then we use that to sort of augment a lot of the incoming feeds, especially when

08:18.720 --> 08:22.800
we go out to the internet, collect free and open source data.

08:22.800 --> 08:27.120
Then we pull back data and how reliable is this particular indicator?

08:27.120 --> 08:29.840
How reliable is this feed in general?

08:29.840 --> 08:35.760
So, so that's where we use a lot of our scoring algorithms to help separate, you know, how

08:35.760 --> 08:41.280
seriously should you take an action on this particular indicator from this feed?

08:41.280 --> 08:45.040
Because there's a drowning in data problem in cybersecurity.

08:45.040 --> 08:50.800
Yes, I'd like to get more concrete on what the specific feeds are.

08:50.800 --> 08:59.440
I know some examples of things that I've seen folks doing are, you could have, you know,

08:59.440 --> 09:03.760
blacklisted IPs, for example, you know, so that may be one, is that an example of a feed

09:03.760 --> 09:04.760
in this case?

09:04.760 --> 09:10.600
Sure, yeah, so you could have, so we scrape a bunch of open source data.

09:10.600 --> 09:14.160
So that's part of it that we just sort of provide with the platform.

09:14.160 --> 09:18.560
Because, you know, intuitively, it kind of makes sense that that needs more vetting, you

09:18.560 --> 09:24.800
know, and so the ML scoring is more important in that role, because if it's just an open source

09:24.800 --> 09:31.600
intelligence feed that comes down, having the scoring is particularly important, whereas

09:31.600 --> 09:37.080
if you're paying for a particular feed, then you intuitively probably have a bit more

09:37.080 --> 09:38.080
confidence in it.

09:38.080 --> 09:45.320
So certainly, you may be tipped off by honeypots, right?

09:45.320 --> 09:50.040
So that's what honeypots are one example of how to collect this data, and that's basically

09:50.040 --> 09:55.720
we put a very vulnerable looking machine on the internet and just try and advertise that

09:55.720 --> 09:58.760
we're vulnerable and get people to attack us.

09:58.760 --> 10:03.720
So we have an open source project called the Modern Honeynet, which is MHN, which is

10:03.720 --> 10:10.800
used to collect a lot of these, no matter what type of honeypot you have, then we can

10:10.800 --> 10:15.200
take this data and sort of centralize it, and it simplifies the data collection piece,

10:15.200 --> 10:18.360
no matter how many different types of honeypots you have.

10:18.360 --> 10:20.360
So that's an example of one of them.

10:20.360 --> 10:28.360
So we've got the kind of the IP, the black listed IPs, like what are some other data

10:28.360 --> 10:36.800
sources that you're collecting, cleaning, providing that feed into, you know, making ultimately

10:36.800 --> 10:42.120
you're trying to make a determination whether, you know, some activity is, you know, likely

10:42.120 --> 10:43.360
to be a threat.

10:43.360 --> 10:45.160
Yeah, absolutely.

10:45.160 --> 10:48.560
And so some organizations may choose to immediately block it.

10:48.560 --> 10:52.400
Other organizations may just want to monitor it and see what happened.

10:52.400 --> 10:57.960
Some organizations get breached and then need to do a retrospective evaluation of how

10:57.960 --> 10:58.960
did this get in?

10:58.960 --> 10:59.960
How did this happen?

10:59.960 --> 11:03.480
What was our whole, what was our chink and our armor?

11:03.480 --> 11:08.840
And so with the retrospective evaluation, we've got another, another tool, anomaly enterprise

11:08.840 --> 11:15.440
that's very focused on applying, basically taking the work off of your existing information

11:15.440 --> 11:19.360
aggregation tools, call the SIM.

11:19.360 --> 11:25.640
Taking the workload off that and applying some ML, applying these known pieces of evidence

11:25.640 --> 11:34.200
like IPs and domains and URLs and file hashes when they occur in the computer network.

11:34.200 --> 11:42.520
Yeah, I'm still trying to wrap my head around like concretely the specific, the specific

11:42.520 --> 11:49.480
types of data that you're ultimately be running machine learning algorithms over.

11:49.480 --> 11:50.480
Right.

11:50.480 --> 11:51.480
Sure.

11:51.480 --> 11:52.480
So there's two kind of main buckets.

11:52.480 --> 11:55.880
Host-based data and the other is network-based data.

11:55.880 --> 12:01.360
So on the host-based side, we have, the most common is something like file hashes, right?

12:01.360 --> 12:06.760
So unique identifier of a file because it's easy to rename a file if it's a piece of malware

12:06.760 --> 12:08.400
that's a really bad indicator.

12:08.400 --> 12:15.000
So we hash the contents of the file or in more sophisticated cases, we hash subsets of

12:15.000 --> 12:20.760
the file to indicate maybe that there's a piece of the malware that's, you know, getting

12:20.760 --> 12:23.600
passed around, for example.

12:23.600 --> 12:29.960
So another kind of lower volume, host-based piece of information that we track is things

12:29.960 --> 12:38.440
like, you know, function names and regaxes and so these types of sub pieces of a piece

12:38.440 --> 12:45.040
of software that could be used in a piece of malware because it's, when you start tracking

12:45.040 --> 12:49.280
by a file hash, so much of cybersecurity is an evolution, right?

12:49.280 --> 12:54.240
So we have an active adversary trying to elude us, which is part of what makes it a bit

12:54.240 --> 12:56.120
more of a unique space.

12:56.120 --> 13:02.040
And so one way this manifests is the type of data that we collect, for example, hashing

13:02.040 --> 13:03.520
files.

13:03.520 --> 13:07.880
When adversaries know that we do that to track their malware, they're going to try and

13:07.880 --> 13:10.440
find methods to get around it.

13:10.440 --> 13:21.760
So one way to get around hashing of files is to just randomly add a little bit of gibberish

13:21.760 --> 13:25.320
in a non-executed portion of the file.

13:25.320 --> 13:31.920
And you can get into, in the ecosystem, like the economic ecosystem of malware, you can

13:31.920 --> 13:42.160
have automatic systems to just add, pad, garbage in the end of the file, just to change the

13:42.160 --> 13:45.800
file hash, just to mislead you.

13:45.800 --> 13:49.920
And is the idea with that to just render tools like yours not useful because there's

13:49.920 --> 13:54.400
so much noise out there, or are you trying to replicate the hash of, like, replicate

13:54.400 --> 13:55.400
the known good hash?

13:55.400 --> 14:03.160
Well, the idea is that file hashes are a very commonly used strategy in cybersecurity

14:03.160 --> 14:04.240
in general.

14:04.240 --> 14:08.480
And so it's a very popular way to uniquely identify a file.

14:08.480 --> 14:13.840
And so adversaries, they're not doing to elude us, they're doing it to elude everyone.

14:13.840 --> 14:18.120
And so then we get into smaller parts of the file.

14:18.120 --> 14:27.440
So there's, in like a PE32, like an executable on Windows, you might have different sections.

14:27.440 --> 14:33.760
If you break down the assembly code, you might hash the import table for all the libraries

14:33.760 --> 14:36.040
that you import.

14:36.040 --> 14:41.840
So these more granular notions of hashing make it harder for the adversaries to avoid,

14:41.840 --> 14:48.480
but it requires a larger context of piecing together the different pieces to the puzzle.

14:48.480 --> 14:54.200
So you've got the host base data, you've got the network, net flow data, presumably,

14:54.200 --> 14:58.440
and perhaps other types of network data.

14:58.440 --> 15:05.840
That stuff is all coming off of ultimately routers, firewalls, network devices.

15:05.840 --> 15:13.320
And then you are talking about the difficulty of kind of coming to identifying ground truth.

15:13.320 --> 15:23.320
Like you have all this data that you've collected, how do you go about labeling or using unsupervised

15:23.320 --> 15:30.040
techniques machine learning wise to try to identify the bad actors?

15:30.040 --> 15:35.280
So we're using, most of our work is definitely in the supervised space.

15:35.280 --> 15:41.880
So like XG boost, random forest, that sort of a space, ensembles, we see a lot of benefit

15:41.880 --> 15:43.360
from.

15:43.360 --> 15:47.760
What's interesting in the threat and intelligence space, I've tried, I've personally looked

15:47.760 --> 15:52.880
at a few different problems and pretty reliably ensemble decision trees or ensemble regression

15:52.880 --> 15:56.520
trees end up being the best strategy.

15:56.520 --> 16:00.120
So maybe can you talk a little bit about some of the different types of problems that

16:00.120 --> 16:01.520
you've looked at?

16:01.520 --> 16:09.640
And how do you go about defining the problems in a way that leads you to being able to solve

16:09.640 --> 16:11.440
them using ML?

16:11.440 --> 16:12.440
Sure.

16:12.440 --> 16:13.440
Sure.

16:13.440 --> 16:17.560
So one of the important rules of machine learning, I think, is only use machine learning

16:17.560 --> 16:22.560
when you actually need it, because most machine learning has false positives and false negatives.

16:22.560 --> 16:27.640
If you can come up with maybe logical expressions that don't have those, then you should use

16:27.640 --> 16:29.480
other methods.

16:29.480 --> 16:36.680
And so when I think of the order of the tools that we use, if we can use the very first

16:36.680 --> 16:42.360
most naive version of a tool, a sort of a white list and black list, just big, big lists

16:42.360 --> 16:47.600
of exact matching, then the next level of tool is a bit more fuzzy matching, starting

16:47.600 --> 16:54.880
to get into space of reg X's and maybe signatures that are capturing patterns with the typical

16:54.880 --> 16:57.920
sort of programming language style of logical expressions, right?

16:57.920 --> 17:03.720
If we see these group of things, possibly followed by another group of things.

17:03.720 --> 17:08.840
If those two methods fail, then your problem is really ripe for machine learning.

17:08.840 --> 17:12.400
And that's really the space we're interested in.

17:12.400 --> 17:18.200
I feel like you need a little bit of intuition to believe that machine learning would work.

17:18.200 --> 17:23.040
So one example is domain generation algorithms.

17:23.040 --> 17:30.400
So for a long time, the strategy is when you have malware infecting your computer network

17:30.400 --> 17:36.240
and it calls out to maybe an IP or domain, then you block that and you're good.

17:36.240 --> 17:40.760
So first we did that with IPs and then the bad guys became aware of it.

17:40.760 --> 17:46.320
So then they started calling out to domains and then once those were getting blocked and

17:46.320 --> 17:51.480
the strategy was successful, then the bad guys evolved yet again.

17:51.480 --> 17:58.400
And so the innovation after that was in one sense fast flux, which is sort of moving

17:58.400 --> 18:04.080
around IPs, but the one that really stuck is something called domain generation algorithms.

18:04.080 --> 18:09.040
So what that is is in a piece of malware itself, you have a pseudo random function that

18:09.040 --> 18:15.760
is known to the computer that's infected with the malware and it's known to the adversary

18:15.760 --> 18:17.360
maybe out on the internet, right?

18:17.360 --> 18:20.320
The guy that's running the whole botnet.

18:20.320 --> 18:28.760
So both of those two points know what the pseudo random algorithm will predict for tomorrow.

18:28.760 --> 18:32.200
And the problem is in between those two points, no one knows.

18:32.200 --> 18:36.560
So the guys that are responsible to defend the network have no idea what the domains might

18:36.560 --> 18:39.840
be tomorrow or the next day.

18:39.840 --> 18:48.280
And so this asymmetry that gets created between the infected malware and the bad guy in

18:48.280 --> 18:52.200
the internet know what's going to happen and the good guys at the perimeter that have

18:52.200 --> 18:56.440
to stop it creates a pretty big disadvantage, right?

18:56.440 --> 19:01.720
Because there's no way they're going to know what domains need to be blocked.

19:01.720 --> 19:09.880
So the problem gets compounded, the asymmetry is compounded specifically because you not

19:09.880 --> 19:12.400
only have one domain like per day, right?

19:12.400 --> 19:16.960
These domains would change every day, for example, but it's not just one.

19:16.960 --> 19:22.600
Maybe that the malware calls out to 1,000 domains each day.

19:22.600 --> 19:27.640
And the bad guys only have to find one chink in your armor and the good guys have to defend

19:27.640 --> 19:29.760
100%.

19:29.760 --> 19:36.400
And so exploiting that asymmetry, the bad guys just maybe pick one or two of the domains

19:36.400 --> 19:38.880
that the malware will call out to tomorrow.

19:38.880 --> 19:44.200
They register that ahead of time and that's how you utilize this communication mechanism.

19:44.200 --> 19:50.080
Because all malware in order to be useful needs to have some network connectivity, right?

19:50.080 --> 19:51.840
There's very few exceptions.

19:51.840 --> 19:55.520
And so, you know, if you're stealing someone's credit card information, you need to send

19:55.520 --> 19:59.160
that back so it can be resold and you can monetize it, right?

19:59.160 --> 20:03.480
If you're doing economic espionage, you need to get the data out of the network and back

20:03.480 --> 20:05.920
into your organization.

20:05.920 --> 20:13.320
So this is a sort of fundamental rule of attacks is that nearly every type of attack is about

20:13.320 --> 20:17.520
some sort of taking information.

20:17.520 --> 20:23.000
So then since the bad guys are able to use this domain generation algorithm to get the

20:23.000 --> 20:30.560
information out of the network, then this is clearly a weak point in the defense of

20:30.560 --> 20:33.080
the good guys perimeter, right?

20:33.080 --> 20:40.280
And so since it's a pseudo random algorithm, the intuition is that humans can look at it

20:40.280 --> 20:43.240
and say, hey, that domain looks like it's gibberish, right?

20:43.240 --> 20:47.280
It might be XLJQBZF2, right?

20:47.280 --> 20:50.720
Absolutely.

20:50.720 --> 21:01.360
And so it's usually the case that these very random sort of high entropy domains are affiliated

21:01.360 --> 21:03.280
with domain generation algorithms.

21:03.280 --> 21:07.280
There's a few exceptions, but the vast majority is like that.

21:07.280 --> 21:12.840
And so what's fascinating is that human analysts, when they see it, they're like duh.

21:12.840 --> 21:14.640
I can pick that out really easy.

21:14.640 --> 21:16.920
Why is this really a problem?

21:16.920 --> 21:21.680
Well because these algorithms may sort of tune down how aggressively they call out, start

21:21.680 --> 21:23.600
blending in with other traffic.

21:23.600 --> 21:28.080
And if you have to look through maybe hundreds of millions of domains that your organization

21:28.080 --> 21:32.160
is called out to yesterday, are you going to be able to look through all of them and

21:32.160 --> 21:33.160
pick this out?

21:33.160 --> 21:34.760
The answer is no.

21:34.760 --> 21:36.840
We need to automate.

21:36.840 --> 21:42.600
Because you can't really automate this very well with logical expressions.

21:42.600 --> 21:49.040
And even if you did, you might be able to reconstruct maybe like a regax that describes

21:49.040 --> 21:51.080
this one particular algorithm.

21:51.080 --> 21:54.000
Oh, but by the way, there's different types of malware.

21:54.000 --> 21:56.240
They all use very different algorithms.

21:56.240 --> 22:00.400
They're often have different seeds to their randomization.

22:00.400 --> 22:05.760
And so you really need an effective way to be able to distinguish between these random

22:05.760 --> 22:11.960
looking domains and normal user traffic and activity.

22:11.960 --> 22:20.160
So this space is really what motivated my first project in this space of machine learning

22:20.160 --> 22:22.760
applied to cybersecurity.

22:22.760 --> 22:24.240
So we saw these things happening.

22:24.240 --> 22:27.520
This was in 2009 actually.

22:27.520 --> 22:33.400
So in 2009, I was talking to some colleagues.

22:33.400 --> 22:38.000
And they, we had realized this was a problem that all these domains were happening out

22:38.000 --> 22:39.600
there in the internet.

22:39.600 --> 22:46.880
And so we put together a supervised prototype and turned out it was able to detect them

22:46.880 --> 22:49.440
pretty effectively.

22:49.440 --> 22:52.360
So that was in about 2009.

22:52.360 --> 23:00.320
And then I think an interesting story that's kind of unique to security is that in a little

23:00.320 --> 23:08.600
over a year later, we, someone, about a year and a half later, someone published a very,

23:08.600 --> 23:13.480
very similar strategy, a little less scalable, but a very similar strategy.

23:13.480 --> 23:14.960
So this was focused.

23:14.960 --> 23:21.040
This was, I can give you the name, this was from Texas AMU.

23:21.040 --> 23:24.200
And they published this paper, which was some good work.

23:24.200 --> 23:29.000
But we intentionally chose to not publish our strategy at the time.

23:29.000 --> 23:32.960
Because there's this question of, if you tell an adversary, you can detect them.

23:32.960 --> 23:34.760
What will happen?

23:34.760 --> 23:39.120
So the upside to the situation is, we had been monitoring them.

23:39.120 --> 23:44.040
We had been monitoring the overall activity of these domain generation algorithm domains

23:44.040 --> 23:46.000
out on the internet.

23:46.000 --> 23:55.680
And so when the report was released at UsNix, it took about two to three weeks or so.

23:55.680 --> 24:01.120
And you can clearly see the points when the information from the conference got back

24:01.120 --> 24:03.520
to the adversaries.

24:03.520 --> 24:07.760
And we can see this, first it drops off, nearly completely.

24:07.760 --> 24:11.960
And that happened maybe lasted about two months or so.

24:11.960 --> 24:16.760
And then after that, you see a significant change in the variance of these.

24:16.760 --> 24:22.920
So it might have been something like, you know, maybe like the weekend activity, you

24:22.920 --> 24:26.920
know, was much higher, but the weekday was lower, something, something like that.

24:26.920 --> 24:30.280
You see a big change in the variance of day-to-day activity.

24:30.280 --> 24:33.920
Then before the paper was released, I have a chart of this, too.

24:33.920 --> 24:34.920
All right.

24:34.920 --> 24:45.880
So, so you did this project, tell, tell us about how, how you wanted about solving the problem.

24:45.880 --> 24:51.400
You know, what techniques did you use and what you learned in the process of, of deploying

24:51.400 --> 24:52.400
them?

24:52.400 --> 24:53.400
Sure.

24:53.400 --> 24:59.800
So, one thing that was very interesting was, I thought was interesting, was understanding

24:59.800 --> 25:03.880
how many, which different models were more effective.

25:03.880 --> 25:16.000
And so, I remember at the time I was using WECA APIs and instrumenting various type of algorithms.

25:16.000 --> 25:22.720
I was very interested in a set-up across fold validation and figure out which algorithms

25:22.720 --> 25:25.680
are going to work better.

25:25.680 --> 25:33.040
The one that ultimately I found worked the best was a mix of adabust with adabust on

25:33.040 --> 25:34.040
Ripper.

25:34.040 --> 25:37.320
So if you've ever heard of the Ripper algorithm was made by William Cohen.

25:37.320 --> 25:42.560
It's a rule-based learner, very similar to, very similar to a decision tree.

25:42.560 --> 25:47.360
But it's a non-boosted algorithm and then boosting it after the fact ended up being more

25:47.360 --> 25:51.560
effective than something like, you know, maybe rent and forest out of the box.

25:51.560 --> 25:53.080
Well, let's dig into all of this.

25:53.080 --> 25:57.840
So, WECA, I've heard of a bunch of times.

25:57.840 --> 26:00.400
I don't know a whole lot about it.

26:00.400 --> 26:06.880
I get the impression that it has declined in popularity relative to newer things, but

26:06.880 --> 26:09.320
maybe tell, what's your take on that?

26:09.320 --> 26:15.600
Let me, you know, what it is to you and the role that, you know, where would you, you know,

26:15.600 --> 26:17.600
what kind of situations would you turn to it?

26:17.600 --> 26:21.520
I think the biggest sweet spot for WECA in my opinion is people who aren't real comfortable

26:21.520 --> 26:23.480
in programming.

26:23.480 --> 26:27.680
So it has a really good point and click GUI.

26:27.680 --> 26:29.280
And so it does have APIs.

26:29.280 --> 26:30.280
It certainly does.

26:30.280 --> 26:34.000
They're Java APIs and it has a lot of algorithms implemented.

26:34.000 --> 26:40.680
So it's going to be much less than or implemented in R, but for being a machine learning framework,

26:40.680 --> 26:48.240
it's got a pretty good selection of APIs, includes feature selection, it includes clustering,

26:48.240 --> 26:51.000
it includes a bit of neural nets.

26:51.000 --> 26:57.800
They've got a sort of repository for prototype code, which includes a lot of bioinformatics

26:57.800 --> 26:59.400
work.

26:59.400 --> 27:04.040
But I think in my perspective, I think it's the, all the tools I've seen, I think it's

27:04.040 --> 27:07.400
my favorite for someone who doesn't do programming.

27:07.400 --> 27:09.480
If you do do programming, you can also use it.

27:09.480 --> 27:12.280
So there's a nice sort of stepping out, right?

27:12.280 --> 27:17.400
So you can go from just using it in GUI to, it's also got a command line where you can

27:17.400 --> 27:25.360
call, you know, like, you know, call it C45 classifier, for example, just from the command

27:25.360 --> 27:27.880
line with the CSV file.

27:27.880 --> 27:32.160
So it's got a nice sort of transition path to programming, and of course, it's also got

27:32.160 --> 27:34.120
Java APIs as well.

27:34.120 --> 27:39.120
So I feel like the transition path from, I don't program to like a program a little bit,

27:39.120 --> 27:42.320
I think in WECK is really strong.

27:42.320 --> 27:46.200
I think like, if you want to compare it to some, like, it's a little bit similar to orange.

27:46.200 --> 27:50.840
So I, what I liked about orange is that it has a canvas format, so you can have sort

27:50.840 --> 27:54.760
of a drag and drop GUI, where you can assemble a pipeline.

27:54.760 --> 28:01.480
The orange is primarily orange is Python based, and WECK is Java based.

28:01.480 --> 28:06.640
With orange, you can assemble sort of a pipeline of what operations you want to do with purely

28:06.640 --> 28:08.320
click and drop.

28:08.320 --> 28:11.120
You can also do this with RapidMiner.

28:11.120 --> 28:12.120
Okay.

28:12.120 --> 28:17.520
And all right, so that's WECK, and then you mentioned Addabust.

28:17.520 --> 28:18.520
Yeah.

28:18.520 --> 28:19.520
Yeah.

28:19.520 --> 28:22.000
Actually, boosted decision trees in general.

28:22.000 --> 28:26.720
Yes, talk through kind of those in the role that they play and soften problems like

28:26.720 --> 28:27.720
that.

28:27.720 --> 28:28.720
Yeah.

28:28.720 --> 28:36.120
So I think that when you're getting introduced to machine learning, I think a very sort

28:36.120 --> 28:42.400
of helpful progression is to start with an ID3, which is a really basic decision tree,

28:42.400 --> 28:47.640
and then kind of move to like a C45, which is a little bit more of a robust decision tree,

28:47.640 --> 28:51.960
and then to understand a bit more about ensembles.

28:51.960 --> 28:57.840
And so ensembles became a pretty hot topic in around the early 2000s, something like

28:57.840 --> 28:58.840
that.

28:58.840 --> 29:02.200
Well, maybe we should take a step back and talk about decision trees for folks that

29:02.200 --> 29:05.560
don't might not know about even a decision tree.

29:05.560 --> 29:06.560
Sure.

29:06.560 --> 29:07.560
Sure.

29:07.560 --> 29:13.160
So the general intuition with the decision tree is that we have a metric like information

29:13.160 --> 29:20.320
gain, so that might be when we, so so when we say decision trees, we actually don't

29:20.320 --> 29:21.320
mean decision trees.

29:21.320 --> 29:24.640
They're actually decision tree induction algorithms.

29:24.640 --> 29:30.720
So they take the data and from the data, they induce a decision tree.

29:30.720 --> 29:34.080
And that's what's a little bit confusing, because when you talk to maybe folks in the

29:34.080 --> 29:38.760
business side, you'll say a decision tree, and it's all about like action, and then

29:38.760 --> 29:43.920
there's an arrow for decisions, and then which way should we go?

29:43.920 --> 29:47.800
That is the end product of these decision tree algorithms.

29:47.800 --> 29:53.760
So the ability to induce those decision trees is really the key observation.

29:53.760 --> 29:58.040
So if you're really in the ML space and you say, oh, you know decision trees, right?

29:58.040 --> 30:02.320
It may mean a completely different thing than you think it means, right?

30:02.320 --> 30:06.400
So so there's special ways these trees are constructed.

30:06.400 --> 30:13.160
I mean, I think a good, a good conceptual understanding is take a metric like information

30:13.160 --> 30:21.440
gain, where we play around with drawing out a bunch of leaves in the tree.

30:21.440 --> 30:29.920
And then at the end of each leaf, we calculate how many, you know, so it's maybe like if

30:29.920 --> 30:32.000
feature a is above three, right?

30:32.000 --> 30:33.000
And that's one leaf.

30:33.000 --> 30:39.200
And if feature b is below two, right, and if feature three is true.

30:39.200 --> 30:45.920
So we build out these levels of of trees, and we assign a, you know, information gain

30:45.920 --> 30:50.560
the idea is if the likelihood is very high.

30:50.560 --> 30:59.200
So how many, how many instances do we label correctly versus inc, how many instances

30:59.200 --> 31:03.240
do we label correctly because of this branch?

31:03.240 --> 31:09.400
And when you have a high metric of usefulness with these, particularly with an individual

31:09.400 --> 31:13.080
branch, then it tends to sort of move up the tree.

31:13.080 --> 31:20.960
So to make this more concrete in the example we were discussing earlier, you may have,

31:20.960 --> 31:28.320
for example, you know, feature a is is IP on such, such and such blacklist feature b

31:28.320 --> 31:33.240
might be, you know, is, you know, hash on x file incorrect.

31:33.240 --> 31:38.240
And so you have some, some grouping of, of these features, and you're basically trying

31:38.240 --> 31:47.040
to create a tree that uses these features to determine whether a, whether the likelihood

31:47.040 --> 31:50.760
of a particular pattern is likely malicious, right?

31:50.760 --> 31:58.400
And then the information gain metric is, you know, as your, your decision tree induction

31:58.400 --> 32:06.320
algorithm is basically trying a bunch of permutations of these different features, and information

32:06.320 --> 32:11.600
gain is essentially asking the question, you know, is this tree adding anything that,

32:11.600 --> 32:15.240
you know, all the other ones that I've looked at, you know, didn't tell me.

32:15.240 --> 32:22.640
Yeah, yeah, so, so you draw out the tree, and then what's important, right, so so much

32:22.640 --> 32:28.080
of data science is then about how do we control overfitting, right?

32:28.080 --> 32:34.920
And I feel like overfitting needs, I think it would be helpful in the field if people

32:34.920 --> 32:40.280
would spend some thought on more precise subgroups of overfitting, because I think there

32:40.280 --> 32:48.400
are subcategories of how things can be overfit, and it's not quite as, not always necessarily

32:48.400 --> 32:51.400
such a binary observation.

32:51.400 --> 32:59.480
So tree pruning on the decision tree is usually one of the parameters that one would tune,

32:59.480 --> 33:04.840
and when you use, when you use tree pruning, the idea is you're, you're trying to appreciate

33:04.840 --> 33:09.360
this trade off between model fit versus model complexity, right?

33:09.360 --> 33:15.880
Since we have a data set that we're using to teach our model, then we could, if we could

33:15.880 --> 33:21.840
build out our tree infinitely, well, we could memorize the data effectively, and that's

33:21.840 --> 33:22.960
not what you want to do.

33:22.960 --> 33:29.240
It's important to generalize, so you don't overfit, and so one of the, one of the important

33:29.240 --> 33:34.920
measurements to tweak with the decision tree is pruning, and so essentially what you're

33:34.920 --> 33:41.800
doing is adding sort of a regularization or a penalty to when you grow out the tree

33:41.800 --> 33:42.800
too much.

33:42.800 --> 33:47.280
So you're trying to find the sweet spot between having a tree that can correctly classify

33:47.280 --> 33:53.280
everything according to the data that it saw, versus having a really complex tree that

33:53.280 --> 33:57.240
might have just memorized all the data.

33:57.240 --> 34:02.040
And so that's, that's a bit of an overview and decision trees, and then when we start

34:02.040 --> 34:05.840
talking about data boosting, we're talking about ensembles.

34:05.840 --> 34:11.320
And I think in the space of decision trees, Leo Bryman's work is really the seminal work

34:11.320 --> 34:12.320
here.

34:12.320 --> 34:15.360
He's, he's the creator of random forests.

34:15.360 --> 34:21.600
And so the, I like to try and explain ensembles in a more general way.

34:21.600 --> 34:25.840
There's, there's variations, so bagging and boosting and stacking are specific types

34:25.840 --> 34:27.680
of ensembles.

34:27.680 --> 34:32.200
But the general idea with ensembles is you have some sort of classifier.

34:32.200 --> 34:35.880
For example, it could be a decision tree, could be an SVM.

34:35.880 --> 34:41.960
Some of the recent work is actually looking at ensembles of deep learning models.

34:41.960 --> 34:50.560
And so, so the idea with an ensemble is that you construct multiple different trees.

34:50.560 --> 34:57.480
So in our decision tree case, you would construct a number of different estimators.

34:57.480 --> 35:05.360
And effectively, they, and you're able to combine the knowledge from the different estimators.

35:05.360 --> 35:08.880
For example, one strategy is to use sort of a voting, right?

35:08.880 --> 35:17.320
And so every different estimator gets a vote toward the ultimate prediction.

35:17.320 --> 35:19.200
And so we talk about ensembles of these.

35:19.200 --> 35:23.680
And so you might use, you know, dozens or hundreds or maybe even thousands, but usually

35:23.680 --> 35:29.800
like low hundreds is, you know, or a few dozen is kind of a good starting place for how

35:29.800 --> 35:32.200
many ensembles to use.

35:32.200 --> 35:36.520
And it's almost always the case that when you combine these ensembles, you get better

35:36.520 --> 35:40.280
performance than you would with the decision tree alone.

35:40.280 --> 35:43.720
And why is that?

35:43.720 --> 35:48.360
It's because some of them, you can imagine sort of variations of some trees might overfit

35:48.360 --> 35:49.680
just a little bit.

35:49.680 --> 35:52.080
Some might underfit just a little bit.

35:52.080 --> 35:57.760
And when you vote, I think it exploits more of a law of large numbers.

35:57.760 --> 36:00.720
And so you get more of an aggregate function, right?

36:00.720 --> 36:06.000
You see in society, you see these interesting things like in crowd voting, where maybe

36:06.000 --> 36:08.880
the intelligence, like wisdom of the crowds, right?

36:08.880 --> 36:16.800
The intelligence of the group is more valuable than the intelligence of any one individual.

36:16.800 --> 36:21.240
So I think that's some of the intuition of why ensembles are better.

36:21.240 --> 36:30.880
And how are the different trees in the ensemble different, are they different sets of features,

36:30.880 --> 36:36.720
are they different hyperparameters, are they, you know, different, I know, when I've

36:36.720 --> 36:42.640
seen ensembles, I've seen them in the case of different models, like you might have

36:42.640 --> 36:47.880
a linear regression, you know, that's good at figuring out one piece of your problem

36:47.880 --> 36:52.800
and, you know, some other type of model that's figuring out another subset of your problem

36:52.800 --> 36:58.800
and then you kind of ensemble them together or at least create a hierarchical model.

36:58.800 --> 37:03.480
When you're doing ensembles of decision trees, like what differentiates tree A from tree

37:03.480 --> 37:04.480
B?

37:04.480 --> 37:05.480
Sure.

37:05.480 --> 37:08.640
And it, of course, depends on the parameters that are set, right?

37:08.640 --> 37:14.120
So it could be that you send different, different data to certain trees.

37:14.120 --> 37:21.160
It could be that you use different initialization parameters, right?

37:21.160 --> 37:25.040
And so, you know, especially if you're creating your own algorithms, there's really no limit,

37:25.040 --> 37:30.640
you know, and some of them you could use information gain as the metric of what is good and

37:30.640 --> 37:35.440
others, you could use, you know, like genie entropy, for example.

37:35.440 --> 37:36.440
What's that?

37:36.440 --> 37:41.240
I don't think I really want to try and define genie entropy.

37:41.240 --> 37:44.800
Like, I've looked at a bunch of definitions, and I'm not surprised I can like succinctly

37:44.800 --> 37:52.640
and clearly describe it with, yeah, it's another metric like information gain that can be

37:52.640 --> 37:58.680
used to rate how good a model is, right, or contributions to a model R.

37:58.680 --> 37:59.680
Yeah.

37:59.680 --> 38:00.680
Okay.

38:00.680 --> 38:02.080
So, add a boost.

38:02.080 --> 38:07.720
You've got a bunch of trees that you've created from sending them different subsets of

38:07.720 --> 38:18.760
your data, different initialization parameters, what does the add a boost algorithm do, and

38:18.760 --> 38:23.240
how's that different from, well, you were talking about actually boosting and bagging and

38:23.240 --> 38:24.240
all that.

38:24.240 --> 38:25.240
Yeah.

38:25.240 --> 38:26.240
Kind of walk us through all that.

38:26.240 --> 38:27.240
Yeah.

38:27.240 --> 38:32.280
So, bagging, boosting, stacking, I think very, very practically.

38:32.280 --> 38:36.560
These are different ways to squeak a little bit of better performance out of your models.

38:36.560 --> 38:42.040
So you have these initial models that you believe are pretty good, and I think it's one

38:42.040 --> 38:44.840
of the single most straightforward ways.

38:44.840 --> 38:48.600
How do I improve my accuracy without changing my data?

38:48.600 --> 38:50.680
Do ensembles, right?

38:50.680 --> 38:58.760
So try different parameters, try boosting, try bagging, try stacking, and you're nearly

38:58.760 --> 39:05.480
guaranteed to improve your performance, whether you're measuring error rates or accuracy

39:05.480 --> 39:09.400
or MCC or anything, AUC.

39:09.400 --> 39:18.800
Yeah, I know, I don't remember the exact stats, but some very large portion of recent

39:18.800 --> 39:27.240
Kaggle contest winners are all gradient boosted decision trees of one sort, you know, ensembles

39:27.240 --> 39:28.680
of one sort or another.

39:28.680 --> 39:29.680
Yeah.

39:29.680 --> 39:35.200
So, XG Boost is really, I think actually the Kaggle is one of the greatest practical sources

39:35.200 --> 39:38.360
to go to for learning about this stuff.

39:38.360 --> 39:43.560
I mean, we all need multiple tools, ensembles, if you will, ensembles of information that

39:43.560 --> 39:45.160
we can pull in, right?

39:45.160 --> 39:50.920
Podcasts like yours, and I think the Kaggle forums are a really valuable space.

39:50.920 --> 39:52.640
That's, you know, a few years back.

39:52.640 --> 39:58.360
That's where, you know, I got first exposed to XG Boost and how sort of it was able to

39:58.360 --> 40:05.800
add a gradient boosted component to our existing strategy of an ensemble decision tree.

40:05.800 --> 40:12.760
And in many cases, XG Boost performs better than random forest.

40:12.760 --> 40:14.880
But it involves a bit more tuning, right?

40:14.880 --> 40:19.160
So it involves basically all the tuning you would have in random forests, you know, maybe

40:19.160 --> 40:24.920
you've got something like a half a dozen to a dozen parameters in random forest.

40:24.920 --> 40:31.640
But when you're tuning XG Boost, you know, you're really looking at one to maybe three dozen

40:31.640 --> 40:33.800
different parameters to tune.

40:33.800 --> 40:38.120
And that can really take some time if you're doing sort of a grid search to try out a bunch

40:38.120 --> 40:42.520
of different levels, you know, it's sort of a combinatorial problem, right?

40:42.520 --> 40:46.520
It's if you have three different settings for Feature One, three for two, three for three,

40:46.520 --> 40:49.680
then it's all multiplicative of how long it will take.

40:49.680 --> 40:54.640
And at the same time, you often want to do it with pretty large data sets because usually

40:54.640 --> 40:59.640
you'll get better performance, if you're doing something like cross validation, cross

40:59.640 --> 41:04.000
whole validation, you'll do, you'll probably get better performance out of it.

41:04.000 --> 41:08.040
So that can really take some time to compute.

41:08.040 --> 41:16.200
So we were kind of walking through applying this stuff to the cyber security use case.

41:16.200 --> 41:22.360
And in that project you were describing, you ended up using decision trees to kind of

41:22.360 --> 41:28.200
get to basically to e-couch your increase performance.

41:28.200 --> 41:37.000
Yeah, how do you even think about performance in this context like what, you know, what,

41:37.000 --> 41:41.560
I don't know if you can quote specific like, you know, error rates or something like that

41:41.560 --> 41:46.560
or what are, what are the key metrics, you know, business metrics, I guess.

41:46.560 --> 41:51.040
And then how do those tie to, you know, like metrics that you would be thinking about

41:51.040 --> 41:52.560
as a data scientist?

41:52.560 --> 42:01.440
Well, actually I can come back to describing that for how we score indicators and just

42:01.440 --> 42:05.400
pass along the confident indicators to our users.

42:05.400 --> 42:09.440
But I think there's an interesting story there when talking about, when talking about the

42:09.440 --> 42:16.000
domain generation algorithm use case because we have this situation of, you have a few

42:16.000 --> 42:17.320
different families of malware.

42:17.320 --> 42:23.080
Let's say you have three families of malware and they generate domain names, right?

42:23.080 --> 42:26.960
It's the domains that they'll call out to in the next day or two or whatever, right?

42:26.960 --> 42:32.040
And so intuitively the first thought you would have is, okay, well, I'm going to measure,

42:32.040 --> 42:38.040
you know, my performance metric, maybe it's, maybe it's accuracy, maybe it's a UC or

42:38.040 --> 42:39.040
whatever.

42:39.040 --> 42:43.200
I'm going to measure that, I'm going to train on some of those domain names, but then

42:43.200 --> 42:47.720
I'm going to test on other domain names and, you know, cross-fold validation and that's

42:47.720 --> 42:49.560
very straightforward.

42:49.560 --> 42:56.640
That's not the right metric to describe it, no matter what, whether you use AUC or Accuracy

42:56.640 --> 43:04.360
or MCC or whatever metric you're using, that's simply the wrong problem you're measuring.

43:04.360 --> 43:10.920
This is what I was meaning about kind of defining, I think there's value to us as a community

43:10.920 --> 43:18.480
to think about better definitions of overfitting, because if I can predict new domains from

43:18.480 --> 43:23.280
the same malware that I've already looked at, what is the value?

43:23.280 --> 43:31.040
Even if it's 100% confidence or 100% accuracy, that's great, but I'm predicting different

43:31.040 --> 43:33.800
domains from the same malware families.

43:33.800 --> 43:40.080
What you want to be doing is predicting new domains from different malware families.

43:40.080 --> 43:44.880
And so when the research was coming out in the space, I was sort of surprised that all

43:44.880 --> 43:51.680
the metrics that was being done were always on detecting new domains from the same malware

43:51.680 --> 43:57.120
families and people weren't measuring other malware families and the accuracy there.

43:57.120 --> 43:59.200
Does that point make sense?

43:59.200 --> 44:05.920
So it's maybe sort of that you need to look in a larger business case to observe what

44:05.920 --> 44:07.600
you should be measuring.

44:07.600 --> 44:15.120
Yeah, so what you're saying is more like, it's almost like a different approach to cross

44:15.120 --> 44:16.120
validation.

44:16.120 --> 44:21.120
You're cross validating across malware families instead of just cross validating within

44:21.120 --> 44:22.600
one malware family.

44:22.600 --> 44:27.520
Yeah, I think that's a great assessment of it, is to think about the scope of the cross

44:27.520 --> 44:32.560
validation and what you're ultimately trying to solve.

44:32.560 --> 44:39.960
Any other interesting use cases or other projects that you guys are working on in the cybersecurity

44:39.960 --> 44:41.440
domain?

44:41.440 --> 44:47.960
Well, I think one thing that's an interesting thing that we are tracking is that how

44:47.960 --> 44:54.640
well are adversaries sort of evolving to our methods, right?

44:54.640 --> 44:59.120
We could talk about this in the space of adversarial machine learning, right?

44:59.120 --> 45:05.600
The idea I think with adversary machine learning is the, so in the cybersecurity domain,

45:05.600 --> 45:12.600
it's pretty common that good guys will first find strong signals of how to detect the bad

45:12.600 --> 45:13.600
guys.

45:13.600 --> 45:19.840
Eventually, the bad guys will know that the good guys are using that particular strong

45:19.840 --> 45:23.120
signal and they'll try to avoid it.

45:23.120 --> 45:28.240
We see this case, you know, this back and forth game happening.

45:28.240 --> 45:34.520
And so what machine learning allows us to do is potentially pick up on a lot of weak

45:34.520 --> 45:39.440
signals that we didn't quite realize were there.

45:39.440 --> 45:43.560
And then use those weak signals to detect the adversary.

45:43.560 --> 45:45.240
And that's kind of the state we're at now.

45:45.240 --> 45:51.280
There's a lot of cyber security companies trying to employ machine learning on obviously,

45:51.280 --> 45:56.560
you know, weak signals to try and get good predictability to detect the bad guys, especially

45:56.560 --> 46:01.440
invariants, right, invariant mechanisms that the bad guys aren't changing and moving

46:01.440 --> 46:03.080
around.

46:03.080 --> 46:09.560
Now the concern is are the bad guys using machine learning, right, or will the bad guys

46:09.560 --> 46:13.720
use machine learning to try and blend in, right?

46:13.720 --> 46:17.640
The most straightforward way is, you know, maybe they would try to model how we do it, for

46:17.640 --> 46:19.880
example.

46:19.880 --> 46:27.040
If the bad guys are modeling how the good guys do it, then constructing an outlier example

46:27.040 --> 46:28.040
isn't too hard.

46:28.040 --> 46:30.320
I mean, that's just the space of generative models.

46:30.320 --> 46:37.360
So if you can fit a generative model to approximate the algorithm the good guy is using, then

46:37.360 --> 46:44.640
this is, you know, this is a great way to sort of evade what the good guys are using.

46:44.640 --> 46:51.080
And that's one way that the space of generative adversarial networks, I think, is a little

46:51.080 --> 47:01.160
bit exciting because most of the generative adversarial networks, the GANs, are used,

47:01.160 --> 47:05.800
so Ian Goodfellow was really a big pioneer in the space.

47:05.800 --> 47:13.760
And so he had some interesting observations and was able to put together a system where

47:13.760 --> 47:17.240
you have sort of a generator function as a generative model.

47:17.240 --> 47:20.960
And then you have a discriminator function.

47:20.960 --> 47:28.920
The discriminator function is trying to detect when, in the case of maybe classifying images.

47:28.920 --> 47:32.640
So you have a generative function that's trying to generate an image and the discriminative

47:32.640 --> 47:36.520
function is looking at the generative model.

47:36.520 --> 47:41.240
And then, or the images that it produces, right, exactly, yeah, it's looking at the

47:41.240 --> 47:44.640
images that the generative model produces, right?

47:44.640 --> 47:50.400
And then the discriminative function, its goal, is to determine, was this determined

47:50.400 --> 47:53.400
by like a human or a computer, right?

47:53.400 --> 47:58.560
So most of this gets used because there's really impressive results.

47:58.560 --> 48:03.040
You end up seeing things like, you know, you can use this in a few different ways.

48:03.040 --> 48:08.880
You can use it to, like, as a better mechanism to interpolate.

48:08.880 --> 48:15.360
So for example, if you have a gap in an image and you want to fill in the image gap, you

48:15.360 --> 48:19.560
could use interpolation and try and sort of guess what it would be.

48:19.560 --> 48:22.240
The point is you've lost some information.

48:22.240 --> 48:26.240
And so when you try and algorithmically fill it in, it looks like it's algorithmically

48:26.240 --> 48:27.240
filled in.

48:27.240 --> 48:28.240
Right.

48:28.240 --> 48:33.120
Meaning if you're averaging pixels or something like that, it looks it just doesn't look

48:33.120 --> 48:34.120
right.

48:34.120 --> 48:35.120
Exactly.

48:35.120 --> 48:40.760
In case you're training a generator or you have a generator that's generating, you

48:40.760 --> 48:44.960
know, proposed, this is a proposed way that I might fill this image in.

48:44.960 --> 48:48.760
And your discriminator is saying, ah, that doesn't look good, ah, that looks good.

48:48.760 --> 48:55.680
And so using the generative adversary on network approach works tends to produce better

48:55.680 --> 49:02.080
looking in fill than, you know, your algorithmic approach is your, you know, averaging and

49:02.080 --> 49:03.080
stuff like that.

49:03.080 --> 49:06.360
That's a, yeah, that's, that's a good assessment of it.

49:06.360 --> 49:10.520
And so a lot of times how it's used is just to try to produce something that's somewhat

49:10.520 --> 49:12.320
convincing.

49:12.320 --> 49:18.120
And the cybersecurity space, we can, we can imagine how that could be used for a type

49:18.120 --> 49:21.600
of like poisoning and evasion.

49:21.600 --> 49:30.400
So, so if we want to generate maybe network data that looks believable, but also gets

49:30.400 --> 49:36.000
around your model, we can use the generator, generator and the discriminator together

49:36.000 --> 49:43.640
to, um, to optimally have the right sort of balance of these two components.

49:43.640 --> 49:47.800
And so in the long term, this is definitely a concern in the security space, you know,

49:47.800 --> 49:53.200
as far as the maturity of the field, you know, we're really just starting to get production

49:53.200 --> 49:58.200
machine learning to do a lot of the, the detection for network and malware and so forth,

49:58.200 --> 50:03.120
you know, the past two, three, four years have been pretty hot in this space.

50:03.120 --> 50:08.880
And so we're really just getting, maturing our process of machine learning, deployed

50:08.880 --> 50:11.240
in cybersecurity.

50:11.240 --> 50:16.360
And so it's a little bit premature to talk about adversaries evading our model, but we

50:16.360 --> 50:20.520
definitely see the adversaries when I was talking about strong signals and weak signals.

50:20.520 --> 50:25.080
We definitely see the adversaries picking up on the strong signals that the humans are

50:25.080 --> 50:27.800
finding and evolving.

50:27.800 --> 50:33.040
And so it's just the case that the way that the adversaries are evolving is suboptimal

50:33.040 --> 50:37.120
because if they had, you know, if they were using machine learning, they could learn

50:37.120 --> 50:41.000
how, learn more optimal strategies to avoid the good guys.

50:41.000 --> 50:42.520
Interesting.

50:42.520 --> 50:47.720
So the good guys all hire these black headers to try to, you know, act like the bad guys

50:47.720 --> 50:55.160
and, and, you know, break them to the bad guys all hire white headers to try to, um, give

50:55.160 --> 50:59.320
them information about what the, the good guys might know about.

50:59.320 --> 51:05.960
So so far, um, as far as what we see adversaries doing to avoid good guys machine learning detection

51:05.960 --> 51:10.800
for the ones we actually catch, right, because we're limited to just what we can catch, um,

51:10.800 --> 51:16.000
we haven't really seen deep model understanding coming from the bad guys.

51:16.000 --> 51:19.520
It's really still at that sort of subtle signal point.

51:19.520 --> 51:26.600
We do in a less ML type, in a less machine learning type of way, we do see bad guys, you

51:26.600 --> 51:31.680
know, testing their malware code, maybe by running it against a bunch of antivirus, right,

51:31.680 --> 51:36.280
because that's, frankly, that's just QA to the bad guys, right?

51:36.280 --> 51:37.280
Right.

51:37.280 --> 51:38.280
Right.

51:38.280 --> 51:39.280
Right.

51:39.280 --> 51:49.000
Um, so Gans, I've only ever seen that in the context of, um, you know, images and deep

51:49.000 --> 51:56.640
neural nets, um, have you seen Gans in, is it, does Gans make sense, uh, in the context

51:56.640 --> 51:59.120
other than deep neural nets?

51:59.120 --> 52:06.800
Um, so yeah, so yeah, well, yeah, that's what's interesting about it is, um, so you, Ian

52:06.800 --> 52:12.080
Goodfeld pretty much invented this space and, uh, I think it was at, I think was at nips

52:12.080 --> 52:13.080
this year.

52:13.080 --> 52:15.480
He gave a pretty famous tutorial on it.

52:15.480 --> 52:20.560
And in that, he mentions, um, you can plug in any supervised algorithm.

52:20.560 --> 52:29.000
Um, but I, we, before there was Gans, there was work in adversarial machine learning.

52:29.000 --> 52:31.240
There was some folks at Berkeley that worked on it.

52:31.240 --> 52:37.280
There was folks, um, at university of, as it pronounced, Caligari, it's that Island

52:37.280 --> 52:40.960
in, uh, the middle of the Mediterranean.

52:40.960 --> 52:41.960
Yeah.

52:41.960 --> 52:48.600
So I think it's the university of Caligari, um, they created a framework for just testing,

52:48.600 --> 52:54.040
um, you know, uh, simple algorithms like SVMs and, you know, decision trees and these types

52:54.040 --> 52:55.040
of things.

52:55.040 --> 53:00.080
So they created a framework for testing those and creating sort of an adversarial space.

53:00.080 --> 53:05.400
So in, in my mind, those types of testing frameworks are examples of adversarial machine

53:05.400 --> 53:07.040
learning that are not Gans.

53:07.040 --> 53:10.160
Yeah, I wasn't aware of any of that stuff, so that's super interesting.

53:10.160 --> 53:21.360
Um, and so in your context, um, are you applying Gans per se or non neural network, uh, uh, models

53:21.360 --> 53:28.520
to, to this and, uh, are you applying neural nets elsewhere in the stuff you're doing?

53:28.520 --> 53:34.520
So we are watching pretty closely as far as do we need to start thinking of an adversarial

53:34.520 --> 53:35.520
component.

53:35.520 --> 53:40.000
And if we do, we'll certainly, the first indication we get, we're going to reorient

53:40.000 --> 53:47.040
our work, um, because it could be a month out, it could be 10 years out, right?

53:47.040 --> 53:52.840
So, um, at the moment, it's, there's not enough data to convince me that it's really worth

53:52.840 --> 53:57.040
that big investment, but it's definitely worth kind of watching and keeping on the radar.

53:57.040 --> 54:05.800
Um, as far as neural nets, um, they're less, uh, advantageous for us for the most part.

54:05.800 --> 54:12.120
Um, I really tend to value interpretable models and I, and I think model interpretability

54:12.120 --> 54:18.440
and cybersecurity is uniquely important because so much of cybersecurity is about, you have

54:18.440 --> 54:24.240
the right data, um, when you understand how your model works, it gives you insights into

54:24.240 --> 54:29.600
what current features are useful, you get a sense of how your model is working, which

54:29.600 --> 54:32.440
you don't really get with neural networks.

54:32.440 --> 54:37.760
And when you get that insight, then you can, um, if you've got some subject matter and

54:37.760 --> 54:42.080
knowledge about how networks work and how operating system work, systems work, you can

54:42.080 --> 54:46.640
start to have the conversation of, well, we should instrument at this level or collect

54:46.640 --> 54:48.880
this new type of data.

54:48.880 --> 54:54.160
So I think the biggest innovations in cybersecurity space are being able to interpret a model

54:54.160 --> 55:00.720
to go back to the data collection, make suggestions of new data to collect and then, uh, improve

55:00.720 --> 55:06.480
the process and start all over there because, um, you know, we've kind of seen a pretty

55:06.480 --> 55:11.160
long history of, for the most part, having good data generally tends to be better than

55:11.160 --> 55:13.080
having really good algorithms.

55:13.080 --> 55:18.760
Of course, you want both, if you have bad data, good algorithms, don't do that much, right?

55:18.760 --> 55:23.800
And even deep neural nets usually require tremendous amounts of data.

55:23.800 --> 55:29.440
And so one of the nice things about our, um, about our company is that, um, when we're

55:29.440 --> 55:34.720
rating all these different feeds that come in, um, in order to help our end users, you

55:34.720 --> 55:38.640
know, we can, we can learn from all these different sources of labeled malicious data,

55:38.640 --> 55:42.600
which includes, you know, all sorts of different techniques for, you know, how do they detect

55:42.600 --> 55:43.600
malware, right?

55:43.600 --> 55:48.920
How much of it is an automated malware of maybe executing and watching the execution patterns

55:48.920 --> 55:54.600
versus, um, statically analyzing a piece of code and reverse engineering and walking

55:54.600 --> 56:03.560
through it or, you know, um, or, or methods like setting up honeypots or, um, other types

56:03.560 --> 56:07.680
of like network-based measurements may be monitoring the collection outside of a tornoid

56:07.680 --> 56:10.080
and just generally tracking this maliciousness.

56:10.080 --> 56:16.000
There's so many different ways to get insight into what malicious is.

56:16.000 --> 56:18.760
We do actually have a nice amount of data.

56:18.760 --> 56:24.560
So for that reason, I think deep networks, deep neural nets might be on the horizon,

56:24.560 --> 56:32.040
but really understanding what data to use to judge the incoming streams of data is very,

56:32.040 --> 56:33.360
um, is important.

56:33.360 --> 56:35.640
So model interpretability is huge for us.

56:35.640 --> 56:36.640
Right.

56:36.640 --> 56:37.640
Right.

56:37.640 --> 56:43.160
Yeah, that comes up in so many conversations I have with folks about, um, as being, uh,

56:43.160 --> 56:51.360
you know, real challenge for employing neural nets right now, at least, um, although, you

56:51.360 --> 56:55.880
know, as those, as that technique matures, you know, there are some signs that will start

56:55.880 --> 57:02.880
to, you know, that it's not necessarily, you know, as I'm not necessarily, uh, antithetical

57:02.880 --> 57:05.040
to have interpretability with neural nets.

57:05.040 --> 57:07.040
It's just, we're not there yet.

57:07.040 --> 57:08.040
Absolutely.

57:08.040 --> 57:09.040
Absolutely.

57:09.040 --> 57:12.520
So, I mean, I think once, once we can cross that threshold of interpretability, I think

57:12.520 --> 57:16.960
it's, um, going to be much more productive in the security space, at least for the companies

57:16.960 --> 57:17.960
that have enough data.

57:17.960 --> 57:18.960
Yeah.

57:18.960 --> 57:19.960
Yeah.

57:19.960 --> 57:23.160
Do you want to touch on before we wrap it up?

57:23.160 --> 57:31.640
So how I got into machine learning, uh, was that in late 2008, I saw this, like, 60

57:31.640 --> 57:36.280
minutes news story, uh, it was titled Reading Your Mind.

57:36.280 --> 57:41.200
And it was, uh, some work happening at Carnegie Mellon, uh, with Tom Mitchell and Marcel

57:41.200 --> 57:51.240
Just and they were using some of the early folks to, to put people in FMRI images, FMRI

57:51.240 --> 57:59.080
brain imaging systems and then use machine learning algorithms to uniquely identify various

57:59.080 --> 58:00.080
thought patterns.

58:00.080 --> 58:02.440
So the FMRI measures blood flow in your brain.

58:02.440 --> 58:04.040
So it's your data source.

58:04.040 --> 58:09.120
And then the ML algorithms were able to separate, um, particular words and concepts.

58:09.120 --> 58:13.920
So they were able to show them a picture of a house or a hammer or something like that.

58:13.920 --> 58:20.200
And then FMRI brain image, the blood flow in the brain and then using the ML to predict

58:20.200 --> 58:22.760
what the person was thinking about.

58:22.760 --> 58:26.920
And so I saw that, it's a little creepy, but it's really cool.

58:26.920 --> 58:32.360
And I just got to thinking, wow, we should, we should be using this for cybersecurity.

58:32.360 --> 58:38.360
And so, um, that was a little bit earlier than a lot of the hot, um, hot, exciting times

58:38.360 --> 58:41.640
when people got, you know, started getting into, so I was a little, a little early to the

58:41.640 --> 58:47.880
game in some, in, in some extent, um, but I ended up a few years later end up publishing

58:47.880 --> 58:53.400
a paper with Tom Mitchell as well and, uh, and Ellen Ritter, uh, at, who's now at Ohio

58:53.400 --> 58:54.400
State.

58:54.400 --> 58:55.400
Okay.

58:55.400 --> 59:01.600
And what we were doing is we had, um, sort of a special, so we applied expectation regularization

59:01.600 --> 59:06.360
to, uh, to detect, um, detect patterns in Twitter stream.

59:06.360 --> 59:12.800
So there's this space of weak supervision in machine learning and the idea is doing supervised

59:12.800 --> 59:16.240
learning when you have very few examples.

59:16.240 --> 59:22.680
And so it was mostly an NLP problem and we were harvesting from data from the Twitter

59:22.680 --> 59:24.480
verse.

59:24.480 --> 59:31.640
And, um, what we were trying to do was predict security, um, pull out the security events.

59:31.640 --> 59:38.160
So create sort of a security news source out of just the general Twitter stream.

59:38.160 --> 59:43.040
And so with just a few hand labeled events, we were able to pull out, you know, events

59:43.040 --> 59:45.480
on like DDoS and account hijacking.

59:45.480 --> 59:50.920
I think we labeled like a couple dozen events and it had reasonably, um, reasonably high

59:50.920 --> 59:59.400
accuracy given the, given the weak supervision space, right, and very few labeled examples.

59:59.400 --> 01:00:06.400
So then we were able to sort of create these news streams of, um, these news streams of

01:00:06.400 --> 01:00:07.840
security events, right?

01:00:07.840 --> 01:00:12.520
So this, like, these are the events of people getting, uh, people's accounts getting hijacked

01:00:12.520 --> 01:00:13.680
today.

01:00:13.680 --> 01:00:16.440
And so able to track, um, news stories.

01:00:16.440 --> 01:00:21.560
And I just thought that might be useful in security socks, right, to know a security operation

01:00:21.560 --> 01:00:26.080
centers to know, uh, you know, what are the big events of the internet?

01:00:26.080 --> 01:00:31.960
Because in cybersecurity, you go into any fancy high-end, um, cybersecurity operation center

01:00:31.960 --> 01:00:34.720
and they've always got like CNN on the view screen.

01:00:34.720 --> 01:00:35.720
Why?

01:00:35.720 --> 01:00:38.680
Because if 9-11 happens, it's really going to change the network characteristics, right?

01:00:38.680 --> 01:00:41.800
So you just need a basic exposure to the news.

01:00:41.800 --> 01:00:47.240
And so the thinking with some of this work was, um, if we can pull out the security news

01:00:47.240 --> 01:00:51.880
from the Twitterverse, then we can track some of these cyber events that are happening

01:00:51.880 --> 01:00:54.640
near the cyber news from Twitter.

01:00:54.640 --> 01:00:55.920
Mm-hmm, right.

01:00:55.920 --> 01:00:58.920
And the technique that you mentioned using was what?

01:00:58.920 --> 01:01:01.160
Expectation, regularization.

01:01:01.160 --> 01:01:07.520
So we're, um, we're looking at the, the log likelihood, um, so we, we do a bunch of sort

01:01:07.520 --> 01:01:12.800
of n-gram analysis of looking at the tweet and the particular words that are involved.

01:01:12.800 --> 01:01:24.040
And, um, we're able to, um, take, take the log likelihood and then have a term for regularizing,

01:01:24.040 --> 01:01:28.600
um, the label, so sort of a, uh, a penalty for being wrong, right?

01:01:28.600 --> 01:01:34.800
Um, and then also use, um, use L2 regularization as well.

01:01:34.800 --> 01:01:40.560
So two types of regularization as a penalty, um, the sort of basic ideas in the same way

01:01:40.560 --> 01:01:45.120
we don't want to grow our decision tree really large, we want to prune it back.

01:01:45.120 --> 01:01:50.040
So we don't fit too tightly to the data by having a lot of strong regularization.

01:01:50.040 --> 01:01:54.400
The intuition is, so we'll try and pick up on, on the key patterns.

01:01:54.400 --> 01:01:59.400
And in this case, the key patterns were like nearby words in the tweet, um, we used hash

01:01:59.400 --> 01:02:01.360
tags as, as well.

01:02:01.360 --> 01:02:07.080
Um, and so from that, we're able to sort of create, uh, security news service of sorts.

01:02:07.080 --> 01:02:09.360
Cool, does that still exist?

01:02:09.360 --> 01:02:12.240
Um, it existed about a year ago.

01:02:12.240 --> 01:02:15.760
I could try and email Allen and let you know if it gets back up.

01:02:15.760 --> 01:02:22.240
We, we had the security tweets domain and security tweets.org and, uh, that just pointed

01:02:22.240 --> 01:02:27.040
to like one of his test servers at Ohio State, um, right now it's down.

01:02:27.040 --> 01:02:29.040
So I could ping him, ask if you could turn it back on.

01:02:29.040 --> 01:02:30.840
Oh, it sounds like interesting work.

01:02:30.840 --> 01:02:33.800
Maybe we'll be able to include a link to the paper or something like that.

01:02:33.800 --> 01:02:34.800
Yeah.

01:02:34.800 --> 01:02:40.680
So it was presented at the, uh, the World Wide Web Conference in 2015.

01:02:40.680 --> 01:02:41.680
Okay.

01:02:41.680 --> 01:02:42.680
Yeah.

01:02:42.680 --> 01:02:43.680
Great, great.

01:02:43.680 --> 01:02:51.240
Uh, well, Evan, thanks so much for, uh, for meeting with me for taking the time to join

01:02:51.240 --> 01:02:52.240
us here on the show.

01:02:52.240 --> 01:02:57.520
I think this is that this may be the first time I've had a kind of a diehard Twoma listener

01:02:57.520 --> 01:02:58.520
on the show.

01:02:58.520 --> 01:03:01.720
So it's, it's super exciting for me from that perspective.

01:03:01.720 --> 01:03:07.480
Um, and I'm so glad you're able to join us here at the, uh, at the strata event.

01:03:07.480 --> 01:03:12.320
Um, maybe before we go, uh, have you, what have you learned so far at the conference?

01:03:12.320 --> 01:03:13.320
Anything interesting?

01:03:13.320 --> 01:03:14.320
Yeah.

01:03:14.320 --> 01:03:15.320
So, uh, so there was two talks.

01:03:15.320 --> 01:03:20.280
This is pretty early in the conference, but, uh, two talks was sitting in, um, uh, some

01:03:20.280 --> 01:03:26.760
folks from Microsoft were talking about, um, they were talking about how they use machine

01:03:26.760 --> 01:03:28.800
learning and their security operations.

01:03:28.800 --> 01:03:33.800
Um, in the Q and A section, we had a little bit of discussion of data imputation because

01:03:33.800 --> 01:03:36.760
that's, that's an area that's been on my radar lately.

01:03:36.760 --> 01:03:41.320
So in cybersecurity, we often have to make like a bunch of external API calls because

01:03:41.320 --> 01:03:43.360
that's part of our feature space.

01:03:43.360 --> 01:03:50.200
And so for whatever reason, if those API calls fail, then you're forced to impute that data.

01:03:50.200 --> 01:03:54.360
And can that impute data then cause you to make significant errors?

01:03:54.360 --> 01:03:59.880
For example, if you were an adversary and you knew how to get me to impute my data, then

01:03:59.880 --> 01:04:03.160
I could misclassify because of that.

01:04:03.160 --> 01:04:07.960
So, um, so we ended up chatting about that a little bit in the Q and A section.

01:04:07.960 --> 01:04:09.360
So valuable so far?

01:04:09.360 --> 01:04:10.360
Yeah.

01:04:10.360 --> 01:04:11.360
Okay, absolutely.

01:04:11.360 --> 01:04:15.520
I love that there's, um, a little bit of an, uh, there's multiple sessions that are

01:04:15.520 --> 01:04:22.000
covering the cybersecurity use case now, um, because it used to just be, um, basically

01:04:22.000 --> 01:04:27.000
media studies of video, audio, or images, or NLP, right?

01:04:27.000 --> 01:04:31.600
And so kind of the, the minority of us that are outside of those two areas, um, I don't

01:04:31.600 --> 01:04:35.400
think it quite as much attention in the machine learning space.

01:04:35.400 --> 01:04:36.400
Awesome.

01:04:36.400 --> 01:04:37.400
Awesome.

01:04:37.400 --> 01:04:41.400
Thanks again, and, uh, thanks for being on the show.

01:04:41.400 --> 01:04:42.400
Great.

01:04:42.400 --> 01:04:43.400
Thanks for having me.

01:04:43.400 --> 01:04:44.400
All right.

01:04:44.400 --> 01:04:45.400
Bye.

01:04:45.400 --> 01:04:47.400
All right, everyone.

01:04:47.400 --> 01:04:48.880
That's our show for today.

01:04:48.880 --> 01:04:53.320
Once again, thanks so much for listening and for your continued support.

01:04:53.320 --> 01:04:59.080
Don't forget to share your favorite quotes for one of our hot, new, tumult stickers.

01:04:59.080 --> 01:05:03.960
You can share them via the show notes page, via Twitter, via our Facebook page, or via

01:05:03.960 --> 01:05:07.320
a comment on YouTube or SoundCloud.

01:05:07.320 --> 01:05:13.520
The notes for this week's show will be up on twomlai.com slash talk slash 16, where you'll

01:05:13.520 --> 01:05:18.320
find links to Evan and the various resources mentioned in the show.

01:05:18.320 --> 01:05:46.320
Thanks so much for listening and catch you next time.

