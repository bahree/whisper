All right, everyone, I am here with Hulieta Martinez. Hulieta is a senior research scientist
at Wabi. Hulieta, welcome to the Twoma AI podcast. Thank you. Thanks for having me.
I am really looking forward to diving into our conversation. We're going to talk all about
some of your research into computer vision. To get us started, why don't you share a little
bit about your background and how you came to work in the field? Yeah, of course.
So I started, so I was born and raised in Mexico and around 2011, 2012, I finished my undergrad
there, and I really wanted to go somewhere else for my for grad school. So I applied to a bunch
of universities in Canada. Then I got accepted into the UBC, the University of British Columbia,
and then I started when my master's there then eventually went on to do a PhD with the same
institution, some supervisors. And then in 2018, I was grabbing my PhD, but I just had to defend,
so I started looking for jobs, and I ended up moving to Toronto to Uber ATG, which was this
lab that had just been founded and self-driving. Then in earlier this year, that lab shut down,
and a few weeks later, a little bit later after that, I joined Wabi, which is what I'm working
at right now. Awesome. Tell us a little bit about Wabi. What is Wabi doing?
So Wabi is a company dedicated to build an ex-generation of self-driving.
And so what would we believe is that self-driving is the most important and exciting technological
innovation that we will see in the next few years. And Wabi is that Wabi's deal is trying to build
upon the lessons of the last 20 years, and try to build an approach that is AI first. So not just
something that is hand-engineered, and then we try to put AI to make it better, but something
that right from the get-go can really put AI in the center stage. Awesome. And so when you say
when you say kind of AI first versus bolt-on AI after, what are the implications of that approach?
Where do you see it? Express itself. I imagine that everybody says that, but it means different
things. So I guess it means that when you're building a system, you really have this choice of
going for a more traditional approach, and then trying to say alter some parts of the stack
with learned parts, I would say. But if you want to, it actually takes a lot of engineering
effort, and there's a lot of overhead in trying to do this replication on a system that is
in production. But if you build a system that from the start takes this into consideration,
then it should be easier to say, for example, experiment on which parts should be learned,
or which parts should not, or which parts should be trained and to end, and which parts should be
trained separately. There are all these questions that arise when you are trying to decide where
what the role of AI should be. And so I think it's really something that we're trying to do
from the very start. That makes sense. Got it, got it. Yep. And so you, actually this week,
as we're recording this, are presenting at CVPR. You've got a couple of papers at the conference
that we'll talk about, and you're also doing a keynote talk at the Latinx and AI workshop.
You know, let's start there. Tell us about the talk that you're doing at the workshop.
Right. So first of all, I want to thank the organizers of the workshop for reminding me.
It's funny. We have, so there's not a lot of people from Latin America in the field,
let's say, ML and computer vision in particular. So we have been, there's a couple of us,
like maybe five of us, six of us, that we have been trying to meet up every time there's a conference.
We started with, like, you know, hanging out, the conference, like sometimes someone would come
to your poster and be like, you notice that maybe they're like, Latin Americans, you'll be like,
maybe it's big Spanish and so on. And then we would start, like, say, hanging out, like going for lunch,
or trying to build a little bit of a community amongst ourselves. And then there's this Latinx
in AI organization started. I think it was at Newrips where it started. And so then it started
branching out to other conferences. And now there's a more formal presence, I would say,
and more, and there's a workshop where, like, submissions from people from Latin America are
encouraged. And where we want to also highlight tap speakers from Latin America, or Latinx living
in the US or Canada, or, you know, another part of the world. So I think this is the first or
second time that we are doing this official thing with official sponsors and official canals
and posters and so on. Okay. Right. So that's how that happened. Got it. And the presentation that
you're talking, what's the title of your talk? So I decided to name my talk what those large
kill visual search and network compression have in common. Because I think the answer to that
shouldn't be obvious. And I didn't encourage people to come to the talk.
So let's dig into that. What do large-scale visual search and neural network compression
have in common? So the argument that I tried to make there is that, so first of the motivation,
is that we know that there's large neural networks that are that currently are achieving
state-of-the-art performance on all sorts of tasks in computer vision, natural language processing
and so on. And so they're the big elephant in the room. In particular, there is evidence,
and I think this is starting in the NLP side, but now there's some papers that are showing this
for computer vision tasks too, that the performance of these models scales log linearly with how
many parameters they have, which eventually requires more compute and requires more data. So everything's
we are at a moment where we have kind of known for a couple of years that this was going to happen,
but now we have like very, very clear evidence, very clear and critical evidence that larger
models are something that we're going to need. And so this means that we now have models that have
billions of parameters and models that have trillions of parameters are coming. And so this means
that if you want to deploy these models into a system, you have to compress them, like there is
no going around that, unless you are in a very high-end lab where you have access to a super
computer, even if you're trying to even if you have a powerful desktop or a powerful laptop or
a powerful server for these models that have billions of billions of parameters, you're going to
have to compress them anyway. So that is, you know, it sounds like it's a new problem that we are
facing and it is kind of new in the context of neural networks, at no point in history how we
ever build these trainable systems that are so large. But it's also true that we have seen
similar problems in other areas. And we had similar problems before deep learning to
center stage in the computer vision community. So in particular, myself, when I was doing my PhD,
I worked on this problem called, it's called a large scale approximate in just neighbor search.
Okay. And so this problem arises when, say, for example, you have a database of images
and you want to find objects. And so you want to do, you want to match something in them.
So the way that the way you do this and with the pipelines we had would be, say, from a
from a 1080p image, you would run this through a key point detector and then you would find a bunch
of features. And, and fairly high resolution image, but not super high, like, say, 720p 1080p,
it would give you around 10,000 descriptors. And so then when, when an image comes, you have to
extract these descriptors and then you have to find the nearest neighbors in these database.
And so that means that if we have a data set of, say, like, like, say, a thousand images or, sorry,
if you have a data set of a million images, then you're dealing with billions of descriptors.
And so we have these database, these databases that are so large that we
can't really keep them in memory. And then if we want to deploy them, we have to compress them.
We have to somehow get around them. So I think what these two problems have in common is that
they have very large computational requirements, particularly in terms of memory.
And the fact that things are so big that we cannot even keep them in memory,
we have to be clever about how we compress them such that we can still use them while they are
compressed. It's a common team that I try to approach in this talk.
It strikes me that while there's thematic commonality between these two problems,
dealing with large databases and dealing with
high dimensional many parameter neural networks are very different.
And how do you, does the connection extend to technical approaches or is it more of this thematic
connection? It's both. So there's definitely the thematic connection where the database,
let's call them databases, but some of them call them data sets.
They're so large, but they're also high dimensional, right? So usually these descriptors that
we would extract from images, even if you go back 20 years, people were already using
descriptors that had like hundreds of dimensions. So the sort of high dimensional problem was already
there. Now, how do we approach now? Yeah, so I guess the talk kind of builds up upon to motivate
that maybe we are facing the same problems, we can try to apply some of the same solutions. So
in particular, in this talk, we are talking about this paper a little bit more in detail.
The paper is being presented in the main conference where the workshop has been held on that
CVPR. And so here we argue that we can use this particular approach called I guess vector
quantization or some people might know it as product quantization. So product quantization is a
very nice approach. It's not something I developed. It's an idea that originally came
from a researcher that is currently at Facebook. I think his name is Erveje Gu. It's a paper that
was published around 2011. And so it is motivated very much by this need to find nearest neighbors
in high dimensional spaces very fast and with a compressed memory budget. And is it specifically
in the context of neural networks or is it outside of that? Yeah, so this was 2011. This was dealing
with the database problem before neural networks were widely used. So they actually have the
they motivate this with the computer vision setting and experiments. It's a tip on the paper
like transactions in pattern analysis and machine intelligence. It's one of the big computer vision
journals. So the data sets are from computer vision and so on. So in particular, there was this one
descriptor that everyone was using in the day called SIFT that describes a small patch of an
image. And if you have a bunch of them, you can do a lot of interesting things with the database.
So the approach is actually very, very simple. And I think that's part of what makes it so nice.
So we're going to have, so after we extract all these descriptors,
we are going to stack them on a big matrix that is going to have dimensionality D on one side.
That's going to be the tiny side and then the size N where N is the size of the database. And the
thing is that D can be like 100 or something like that, but N can be 100 million or so on.
The simplest way I like to explain this is you can cut these data sets into say four smaller data
sets. If the dimensionality is 100, then each sub data set is going to have dimensionality 25.
And so then we're going to run k-means on each of these little data sets or a small representative set
of that. That is going to give us two things. So the output of k-means is on the one hand the cluster
centers or we're going to call that the codebook. And then it's also going to give us an assignment
for each point in that sub data set to the cluster center that is closest to it.
So now what we're going to do is we're going to run these k-means with a very low k,
so say k equals something like 256 is like the standard value we use.
So that means that the codes are going to be numbers between 0 and 255.
And that means that we can store them with a single byte.
Now the way, now the nice thing is that once we have these groups of codes in codebooks,
we're going to throw away the regional database. The regional database that was so large,
we couldn't fit in memory. And these codes that are small, we can load that in RAM.
This is of course it's going to be an approximation of the regional data set.
But because you can load it in RAM, then you can do clever things with it.
So the first thing is now you have extra space and something that you can manage.
So you can come up with certain data structures that allow you to say do filtering.
Because when you have a query, even if it's compressed, even if it's very nice,
easy to search, you still have hundreds of millions. And you don't want to compute hundreds of
millions of distances. So you want to have some data structure that filters good candidates
that are likely to be the nearest neighbor. But the other thing you can do is that you can use
this code to approximate the distance to the regional vector. So when the query comes,
you can also split it into four chunks. And then you can compute four tables from each little chunk
to the code book, to the code book that we computed before. So that gives us four tables of
255 entries each. And we can just do lookups. So if the element 2,000 in the database is represented
by five numbers, called 5, 200, 120, and it's 255, then you can look up table one in
index five, they will do an index 100, they will tree index, whatever number you had and so on.
Just to make sure I'm following you, is it fair to compare this to or make the analogy of like
a hashing table or hashing algorithm where your hashing algorithm is nearest neighbors
or related to nearest neighbors? Yeah, so hashing is also an approach that will give you
you it's also used to to make nearest neighbors faster. It often counts with nice theoretical
guarantees. And so people like hashing because of that. One of the things that that
product acquisition showed was that even though hashing hashing approaches tend to have
nice theoretical guarantees in practice, they were very much outperformed by doing this
quantization approaches. So yeah, this was 2011, now the literature has moved a lot on both sides.
So, you know, I might be wrong now, I don't follow this closely, but back in the day, that was
pretty much what the evidence was saying.
And so we've got this methodology that we can use on the database side to it sounds like
get more efficient ways to filter the data and you know, based on the reference to
visual search, search as well. What's kind of the next step in your argument, how do we get
closer to the application of neural network compression? Yeah, so I think this is the
last obvious point and it's something that I was very proud of when these ideas started going.
So again, to be fair, like other people had experimented with vector quantization of neural nets.
There are papers on 2014, 2016 and so on, but it is considerably much less explored. So the standard
approach to compressed neural nets is usually scalar quantization or things like network, there's
a lot of focus on network pruning or say low rank approximations. Vector quantization, you can count
the number of papers that do that for neural networks, it's one or two hands.
Can you maybe talk a little bit about scalar quantization versus vector quantization and
how those approaches are worked to set us up for your application?
Yeah, so they're kind of similar. So scalar quantization is what happens when you try to look at each
value in your network or in general in any data that you have and you're going to look at every
number that you have and you're trying to map this real flooring point in practice values
to a discrete subset. And so it's kind of similar to what we're doing in vector quantization.
Just in scalar quantization, you're going to have one code for each scalar.
In vector quantization, you're going to have one code for each vector and of course the question
is how you define a vector, what is going to be grouped together and so on. So theoretically, scalar
quantization is going to be limited in what compression ratios it can achieve because let's say that
you're representing your numbers with 32-bit floating points. Even in the more extreme case,
you're going to reduce that to a binary value, one or zero. So that's going to give you a 32-X
compression rate. It's pretty extreme, though. Which is pretty good, right? It's pretty good.
But when you have a network with trillions of parameters, that might still mean that you need a
machine with 200 gives of RAM or something like that. So now you're saying if you represent
a vector of eight quantities with a single binary, then you've multiplied that out.
Yes, yes. Or if you take like eight values, 16 values, and so on, the bigger the vector,
the more sort of bang for your buck you get. Yeah, exactly. Got it. Okay.
And so then that is, you mentioned that there are only a handful of papers that have explored
vector quantization. Any sense for why that is? Is it just hard? Did it not seem promising?
Did other more promising ideas jump to the fore?
Yeah, it's a good question, actually. I am not entirely sure why that is. I think
a scalar quantization is very intuitive. One thing that might have happened is that
when you have a scalar quantized network in particular, the way how you can use that to
accelerate inference, for example, is more straightforward with current hardware.
Whereas if you start mapping multiple values to a single code, then how you would accelerate that
is not necessarily obvious. And so I think that might be. And so if you don't really have the
the memory limitation being really the main constraint, which if your network has
50 million parameters, 100 million parameters, you can probably get away with scalar quantization
and still run that in, you know, decent desktops or fairly high end phones, for example.
So it might be just now that we're seeing these networks, it's really, really huge
that there's not going to be a way around it.
Right. Right. And so kind of going back to the method that we discussed in the context of
database, I forget you said product. Product quantization product quantization. Yeah. So the
is a fair simplification to say that in scalar and vector quantization, you're only looking at
the specific quantities that you're quantizing. Whereas with product quantization, now you're
looking at the data that you're going to be quantizing it kind of at large. I think that is kind
of fair to say. So the thing with vector quantization is that then you have to design, then you have
to define, you really have to think about what your definition of a vector is going to be.
Mm-hmm. Because for example, before we were saying we have this database that has a hundred
dimensions and we were saying, let's just split it into four groups. But the performance of vector
and sorry, and if you're doing something like scalar quantization, you can take any permutation
of your values. You're going to get the same compression rates. It doesn't really matter for
terms of compression in which order you look at your data. But if you're doing something like
vector quantization, you might say, hey, maybe if I group the first five dimensions and then
dimensions from 55 to 60 and then the last ten dimensions and I make that a single subgroup,
that's going to be much easier to compress than to just take whatever dimensions happen to be
together. Mm-hmm. Yeah, I think that what I was maybe trying to get at is when you're typically
applying vector quantization, you're coming up with this map. Are you also considering the entirety
of your data and using that to create your quantization scheme or is that unique to product quantization?
It sounds like that that may not be. Oh, vector versus product. So I think,
no, so in both cases you have to quantize everything. The gain that you get with product is that
you have this freedom of having subgroups. Okay. Whereas if you're just doing like say more
just taking your whole data set as each entry as a full vector, you wouldn't really have that.
Okay. And so how do you characterize the advantages that you get with product quantization?
So the nice thing is that because you have multiple subgroups in example that I was giving say for,
you have, you can think of it as you're expressing for example, you're expressing your vector as
four numbers. And those four numbers can take each 256 values. So in a sense, you can think of having
a combinatorial sense of clusters to compress your data because you're going to have 256 times 256
times 256 times 256 times four times. Whereas if you, so that allows you to have an implicit code
book that is very, very large, which is good for compression. Whereas if you were looking at the
data as a single vector, then having a code book of that size might not be even feasible for example.
Okay. And then practically, how does this play out when you're,
when you're using this to compress a neural network?
You want to know how to efficiency and accuracy and all those kinds of things, right?
Let's get into it. So the question of how we do it, I think that's a nice point.
So like I was saying, the main question is, what are you going to group together?
So let's say you have a matrix in your neural net that is of size a thousand by a thousand.
You can choose to split this into subsets of four numbers each and then treat those of vectors,
run k-means, and then get a compression that, get a representation that is much more compressed.
So the first thing that we do is, that's why we call the paper permute, permute, then quantize
and find you. So the permute step is, it's a nice, it's a nice observation. So neural nets have
this nice property that if you have two adjacent layers, let's say a linear layer, some non-linear
activation and another linear layer, some non-linear activation, you can express the same
function by having different permutations of the weights. So let's say that you choose one
permutation of this 1024 elements and on the first layer you're going to permute the rows according
to that permutation and then in the second layer you're going to permute the columns. If you pass
data through it, the first thing you're going to notice after the first layer is that your output
is going to be permuted and then when you process it through the second layer because you
permuted the rows, that is going to undo the permutation. The same thing is true for convolutional layers.
So convolutional layers are parameterized by, say, a one-dimensional array of a lot of
little three-dimensional arrays, which are the convolutional filters. The only thing that determines
the order of your output, the order of the channels of the output of the first layer is in which
order you apply these filters. So that is one dimension you can permute and just alter the order
of the output. And then to the layer that's downstream you can just say, oh, you should switch
the filters. You should switch the channels of your filters and then that will undo the permutation,
right? So you're saying that you can permute, but you haven't said yet why you want to permute?
Exactly, exactly, exactly. And that's what we've been hinting at, right? Because the key question
for vector quantization to work and for product quantization to work is how am I going to form
these little subgroups? If I just go and take whatever four numbers happen to be together
because a stochastic gradient descent took these values there, that might not be very easy to
compress. But if I have this other, this whole permutation, it gives me more degrees of freedom.
I can, I can find, I can search over all the permutations of the data. Well, if you search
over all of them, that's going to take you a while because there's a factorial number of permutations.
But you can find efficient algorithms to say stochastically look at look at a subset of them
and then try to find one permutation that is going to make your data easier to compress.
In this case, the data is going to be the weights of the network itself.
And so then the next question is if you have, if I give you to permutations, how do you know
which one is better? That's a natural question. The thing is that vector quantization was a very
hot topic. I'm talking about the 80s, late 80s, early 90s because there was this big push to
digitize a lot of the infrastructure that we had to go from analog to try to make it more digital.
And so the limitation that we had in doing that was that we had very little bandwidth.
So we could only transmit so much binary data over the SL connections or whatever we had.
So there is a lot of papers, usually older papers, that deal very neatly with satioretical guarantees
of how to get good code books, how to get good compression rates. So one thing people studied
in particular, because back then getting real data was really hard to get in. So there's a lot of
papers on how to impress samples from a Gaussian, samples from a Laplacian, samples from other
canonical say distributions. So for example, for a Gaussian, we know that there are well-known
lower bounds, well-known upper bounds. Actually, upper bounds are harder to find. Lower bounds
are lower bounds. And so if you look at expression for this, you will see that there is some constant
numbers based on how big your code book is, how high-dimensional your data is. And then there is
something, there is a term, a linear term based on the determinant of the covariance of the data
that you have. And intuitively, this makes a lot of sense, because geometrically, the determinant
of the covariance gives you a sense of how spread your data is in higher dimensions.
So given two permutations of the data, you can compute your covariance,
compute your determinant, and choose whichever one has lowest, lowest determinant of covariance.
Got it, got it. And that's how you, that's how you create your mappings for your product quantization.
Yes, that's how you create, that's how you decide exactly which things to compress together.
Okay. Yes. And so, yeah, we're pretty proud of that idea, yeah. We're not the first to notice
that, that this permutation invariance exists, like that has been known since the 90s or so on,
probably since people started doing neural nets, I think it's kind of obvious and retrospect.
And there's papers that try to use this to, to make optimization easier, or for example, there's
there's a guy at Google Brain called Simon Cornbullet. He has some papers where
he says, okay, if you have two networks, you train them same data, but different initializations.
How can you tell if they learn the same features? And then the question that is that the features
could be the same, but they're gonna be permuted differently because initialization is different.
So you have to find some sort of canonical permutation to make these things comparable.
And so they do a similar search to what we were doing with a very different end goal,
what exploit in the same properties. Okay, okay, nice. And so then zooming out and looking broadly at
approaches that folks are taking for neural compression or compressing deep neural nets,
how does this approach compare in the grand scheme of things?
Right, so in the paper we compare against a bunch of mostly scalar quantization baselines,
because it's it's the dominant approach in the literature. Okay, and there was a paper that we
are building upon that was doing vector quantization without say, without this permutation inside
in particular. And when you plot this saying in the in the x-axis, you put the compression ratio,
so 10x, 20x, 30x, and in the y-axis, you want to put how much accurate your network is on whatever
task you have. You see that vector approaches sort of create an envelope around all the scalar
ones. And our method just kind of pushes that I would say by a fairly large margin,
especially for high compression ratios.
In terms of implementation, difficulty or efficiency, did you look at any comparisons
along those dimensions? Yeah, that is that is a question we get very often.
So unfortunately not, so it's it takes a lot of engineering effort to to make these things
actually run fast. We did some preliminary experimentation where we noticed that a lot of these
libraries that people often use to do inference, for example, like Kula, Kudian, and so on, they are
kind of hidden, but ones start poking them. They're very optimized for certain settings,
and which means they're really bad for other settings. So they tend to be really good
for particular batch sizes, for particular sizes of data that people tend to use a lot in practice.
But once you take them out of that range, they tend to perform not so great.
And so yeah, the question of how do we actually make this run fast? I think there's two ways.
Either we find very talented engineers, and they do it for us, but another very promising
approach is I think looking at automatic compilers, and compilers that can actually
analyze the whole graph ahead of time and come up and maybe run some experiments, benchmark
these things ahead of time for you, and then try to find a set of instructions that run nicely,
such that the action implementation will be fast on actual hardware. That's not easy.
There's a bunch of efforts, for example, Apache's TVM, I think, is very promising.
And so the promises that there's potentially a greater engineering effort, but
if you put in that effort, this is a potentially higher performance.
Well, is it higher performance at a given compression level or greater compression at a given
performance level or both? I get some thinking of what I'm trying to articulate is one objective
could be to maximize your compression within some given performance bound. Another way you might
approach the problem is to given a required compression level, what's the best performance you're
going to get? If you can fit the model, if you have a hard limit into the model size that you need
to get to, what's the best performance you can get? It's not clear to me that the same
approach necessarily checks both boxes, and that's what I was trying to get at.
Yeah, so I think in practice, you can definitely set the compression rate ahead of time.
And what we see is that when we do that, then in this compression to performance chart,
you tend to be a little bit, so that this would be a cross-section on the x-axis.
So this method tends to be a little bit higher up. It's harder to say I want this performance,
then give me the smallest model I can get, because yeah, a hair of time, networks are hard to
predict. You have to train them, and then you have to hope for the best.
So what we do is we just compress for a bunch of say code book sizes, and that gives us like a
nice, a nice curve that tells you what the trade-offs are. And in practice, that happens to be an
envelope that is pretty optimal with respect to other approaches.
And so the paper that we've been discussing as far per mute quantize and fine-tune is one of a
couple that you presented at or are presenting at CVPR. There's another deep multitask learning
for joint localization, perception, and prediction. Can you give us a high-level overview of that one?
Yeah, of course. So first of all, I want to say I am first author on the per mute quantize paper,
but it was joint work with a bunch of other people. In particular, there's two interns,
Joshan, Shabra Kamani, and Thingway Liu. They did a bunch of the of the work in this
in this part, and you know, shout out to them and my other colleagues at UberHG.
The joint localization paper is, I am not first author there, that is the work
of John Phillips, who was also an intern with us. He's finishing his undergrad right now. He's
he's one of those interns that are doing their undergrad in there, you know,
already publishing at the venues. Oh wow. Yes, it's more common. It's just getting more and more common.
So that's that's a paper actually like a lot too, because it's asking a question that doesn't get
asked very often. So if you go to there's so localization is given a sensor reading for a bunch of
sensor readings and a map, you want to know what in the map you are. And so this is if you want to
build a robot that goes around your house or you want to build an autonomous driving car or
anytime you want to build an autonomous system, there is always going to be some sort of map,
some sort of map of the world that is going to make it easier for the agent to navigate.
And I guess self-driving cars are just the prime example. Their whole thing is that they have
to navigate and that they have to do it safely. So there are a lot of papers in localization,
this has been a problem that has been going on for decades. And there's a lot of approaches,
there's approaches that assume that the map has certain composition, the map is high definition,
low definition, did you build it online, there's memory trade-offs, computational trade-offs,
it's a huge area. And the way people motivate this work in that, they usually will throw on like
say you want to localize and holiday image that you took three years ago and you don't remember
what it was taken. Or you want to or say or you have an autonomous robot or a satellite car.
But the thing that they're actually optimized for is they have this measure called localization error,
which is basically how far you are from given some better sensors telling you, you know you are.
Which makes a lot of sense, right? It's kind of the task that you're trying to solve.
But when you are building say an autonomous system, an autonomous self-driving car,
the thing you really care about is is this right comfortable and is this right safe?
I don't really care if the car thinks that I am five centimeters ahead of what I'm supposed
of where it actually is or 10 or 20 as long as it's safe, it's really fine. And so that was the
question we asked, like how can we link these localization errors? How is that going to affect
autonomous systems? In particular, how is that going to affect things like perception, prediction
and motion planning? By perception, I mean object detection, so detecting the cars, the pedestrians,
the bikes, the motorcycles around you. If my position is if I get that wrong, am I going to be
unable or less good at detecting cars? And am I going to be worse at planning my way around them
or trying to predict what they're going to do? What we notice is that if there is a small enough
error, so say five, then below 20 centimeters, basically your perception system, your motion
planning system doesn't care. It doesn't seem to matter. And then based on that, we're saying, well,
maybe we can build a localization system that runs much faster, but it still gives us the accuracy
that that is good enough for our autonomous system. So we end up designing a neural net that
has a branch that is dedicated to localization, and that branch can be very, very tiny.
And then we can leverage a big backbone that is doing the perception and the motion,
and sorry, the perception and the prediction, and sort of kind of piggyback on top of those
features, try to reuse that computation that's already there, so that we can have a very, very
tiny network that runs in like two milliseconds, three milliseconds, so that and it's still acceptable.
And so we have experiments to show that this is actually possible.
Nice, nice. In the title of the paper is deep multitask learning, after you observe that you
don't need your localization to be quite as accurate, then you've built a multitask network that
has localization as one part of it, as opposed to localization being a standalone system.
Yeah, so that will be the more traditional approach where like you will have some,
either some part of the stock or its own network, or often this would reflect into having
some group of engineers being a set task force or a group whose task is just to improve localization.
So we're kind of doing this thing, joining, and another thing we know is besides the performance
improvements is that if you are deploying this in the real world, this is we argue easier to
deploy as well, because then your training times are much slower, so you can iterate faster
and you don't have to think of two systems at the same time. You can just have
sort of one system that you, after it's trained, you can push it to the car and just get going.
Got it, got it. Awesome, awesome.
I'm curious what you're, you know, when you look at everything that's happening and autonomous
vehicles, just your personal take, what you're excited about, where you think the field is going.
So I am really excited about why I am working here.
I think, so I don't know if, so our co-founder, Professor Hurtason, she has 20 years of experience
within AI, 10 years of experience in self-driving and four years working in industry and just in this
problem. It's a lot of experience, a lot of lessons learned, and I think that having those
insights and that belief that we can build something from scratch that is going to put AI center
stage is not, we're not just saying that, like that's something that's backed up by a lot of
experience and by seeing what limitations current approaches have.
Awesome, awesome. Well, Hulieta, thanks so much for taking the time to chat with us and share
a little bit about what you're up to and your recent presentations at CVPR.
Thank you so much for having me. Thank you, bye bye.
