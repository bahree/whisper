1
00:00:00,000 --> 00:00:15,840
All right, everyone. I am here with Kim Branson. Kim is senior vice president and global head

2
00:00:15,840 --> 00:00:22,480
of the artificial intelligence and machine learning at GSK. Kim, welcome to the Twomo AI podcast.

3
00:00:22,480 --> 00:00:29,040
Thanks, Sam. Good to be here. Looking forward to our conversation. Let's jump right in. You come

4
00:00:29,040 --> 00:00:37,280
to GSK by way of Genentech, which kind of this traditional or this historical Silicon Valley

5
00:00:37,280 --> 00:00:44,160
challenger. Before that, you spent a lot of time in Silicon Valley and now you're at a large

6
00:00:44,160 --> 00:00:50,800
pharma. Tell us about that journey and how it came to be. Sure. So, I guess my background,

7
00:00:50,800 --> 00:00:55,440
as you know, I did my PhD in machine learning, really looking at

8
00:00:55,440 --> 00:01:01,120
trying machine learning to actually drug discovery. So, small molecule drug discovery and design

9
00:01:01,120 --> 00:01:07,040
was what I started off a bit. And then says, I'm dating myself, but like, you know, in 2003,

10
00:01:07,040 --> 00:01:10,400
around then, and where we're, you know, the cutting edge techniques with things like support

11
00:01:10,400 --> 00:01:15,280
back to machines and random forests and things like that. But doing a lot of work on doing simulation,

12
00:01:15,280 --> 00:01:18,880
a physics simulation, things like that, or small molecules, spying to proteins and all the things

13
00:01:18,880 --> 00:01:24,880
around with that. From there, I ended up doing, you know, a very sort of academic thing of spending

14
00:01:24,880 --> 00:01:29,200
time at Stanford and others. There's a lot of sudden earlier work on machine learning applied

15
00:01:29,200 --> 00:01:33,680
there into graph convolution networks. That sort of stuff was happening. And a lot of my time

16
00:01:33,680 --> 00:01:37,760
was spent actually in, you know, I spent some time at Vertix Pharmaceuticals, but then a favorite

17
00:01:37,760 --> 00:01:43,120
time, so I've been start up and so things like, you know, was involved in early search startup,

18
00:01:43,120 --> 00:01:47,360
there was acquired by Twitter, another company doing large scale medical records aggregation

19
00:01:47,360 --> 00:01:51,440
and differential privacy machine learning on that sort of stuff. And although, like all things,

20
00:01:51,440 --> 00:01:55,840
90% of your time is building all the stuff to do ETLs, extract tables out of PDFs and all those

21
00:01:55,840 --> 00:02:00,800
wonderful things. And that was acquired by Apple. And then a lot of large scale machine learning

22
00:02:00,800 --> 00:02:04,480
on claims, other types of data, so all about like predicting like what's the probability of

23
00:02:04,480 --> 00:02:08,880
predicting disease X at time T given given someone's past medical history. So you could intervene

24
00:02:08,880 --> 00:02:13,680
early and things like that. And from there, you know, many friends at Genentech, I've always been

25
00:02:13,680 --> 00:02:20,640
involved in computational chemistry. So joined Genentech and really was, I guess, really recruited

26
00:02:20,640 --> 00:02:26,080
from Genentech to come to GSK, which, you know, it's one of the things that wasn't really on my radar

27
00:02:26,080 --> 00:02:30,160
as a place to be. And I had some friends there on, you know, I'd known of the company for a while,

28
00:02:30,160 --> 00:02:33,520
but they sort of committed me to come in and I realized that actually it's something very different

29
00:02:33,520 --> 00:02:40,080
happening here. So they put it in a new head of R&D, so how Baron had joined and, you know, how it

30
00:02:40,080 --> 00:02:44,000
was worked at Calico, with definitely Colin people like that. And I was sort of like, well,

31
00:02:44,000 --> 00:02:47,440
there's a guy who really, you know, actually understands what machine learning looks like and what

32
00:02:47,440 --> 00:02:52,480
it requires to be done properly almost. And sort of, you know, really explained to me that like,

33
00:02:52,480 --> 00:02:57,040
it's going to be such a core part of GSK, right, that like they're really serious about it. It's not

34
00:02:57,040 --> 00:03:01,600
just a company like I want to build a 10 person team or double and kind of half resource you,

35
00:03:01,600 --> 00:03:05,680
but it was going to be such a fundamental thing. So that's what sort of led me here and has

36
00:03:05,680 --> 00:03:10,640
led me to create the create the group and then scale as we do now. And give us some context for

37
00:03:10,640 --> 00:03:17,200
GSK. What's the core business and where does ML and AI fit in? Yeah, sure. So we're obviously a

38
00:03:17,200 --> 00:03:22,080
you know, pharmaceutical company, you know, we make, you know, medicines and vaccines across a

39
00:03:22,080 --> 00:03:27,600
bright range of things. And so GSK is an old company, right? It's been about 300 years, right? So

40
00:03:28,240 --> 00:03:31,440
all these companies, every now and then, they go through these sort of revolutions and they,

41
00:03:31,440 --> 00:03:37,040
you know, they sort of turn themselves inside out and reposition themselves. And, you know,

42
00:03:37,040 --> 00:03:43,040
what was really apparent is that we have this increasing body of sort of genetic data. So these

43
00:03:43,040 --> 00:03:48,560
are these large genetic databases. So where we have lots of people, we cost the sequencing has gone

44
00:03:48,560 --> 00:03:53,520
down. So, you know, members, I guess 20 years, we have the human genome, one, one, one, one genome,

45
00:03:53,520 --> 00:03:58,240
now we can do lots, lots of people, right? So you can see sequence lots of people you know about

46
00:03:58,240 --> 00:04:02,720
their medical histories. And you have, you can basically do it, answer a question like, you know,

47
00:04:02,720 --> 00:04:05,680
he's a hero bunch of people that got a disease, hero bunch of people like Brooklyn that don't get

48
00:04:05,680 --> 00:04:10,000
the disease. And ask a question, well, what's different in the genetics, right? The idea is that

49
00:04:10,000 --> 00:04:14,480
the genetics kind of points at a clue as to what you might want to do medicine for, how, what's

50
00:04:14,480 --> 00:04:19,920
involved in that disease. So a really increasing amount of just data being generated in the genetics

51
00:04:19,920 --> 00:04:26,160
side of things. And the other side of things is happening is really these technology for

52
00:04:26,160 --> 00:04:30,480
function genomics, right? So maybe the first wave you can think of as molecular biology, right?

53
00:04:30,480 --> 00:04:34,160
Restriction enzymes, we have passwords, we can like, you know, do genetic engineering and

54
00:04:34,160 --> 00:04:39,200
make a cell make a protein. The next, this is really the continuation of the evolution of those

55
00:04:39,200 --> 00:04:45,680
those tools is now with CRISPR and talons and those sorts of things where you can actually perturb

56
00:04:45,680 --> 00:04:50,480
a specific gene, right? We can turn it up, we'll turn it down in a particular cell type almost

57
00:04:50,480 --> 00:04:54,320
at an almost sort of single cell level. So you now you've got this other set of technologies where

58
00:04:54,320 --> 00:05:00,320
you can start to like generate huge amounts of data at scale. And what it turns out with is that

59
00:05:00,320 --> 00:05:04,960
we're just biology can measure so much more now. There's just this massive amount of measurement

60
00:05:04,960 --> 00:05:09,440
and it's multimodal. So it's, you know, looking at RNA-C kind of single cell levels that's looking

61
00:05:09,440 --> 00:05:14,880
at messenger RNA made as you're doing these edits. You can do cellular imaging, you can do proteomics,

62
00:05:14,880 --> 00:05:20,400
you've got all, you know, all the omics as we call them, you're always explosion of data. And so

63
00:05:20,400 --> 00:05:24,160
really you need machine learning in the middle of it to sort of make sense of the data,

64
00:05:24,160 --> 00:05:27,600
but also even to help you kind of make sense of all the literature you've gotten into plan

65
00:05:27,600 --> 00:05:32,000
the sort of next experiments. And that's in the discovery phase. So it's it's really core,

66
00:05:32,000 --> 00:05:36,800
we have this, this three-pronged strategy that's sort of this, the, you know, genetics, right?

67
00:05:36,800 --> 00:05:40,240
And function dynamics on the side and the AIML in the middle to sort of integrate together

68
00:05:40,960 --> 00:05:44,480
to help us really, you know, find the time that's in design better medicines.

69
00:05:44,480 --> 00:05:50,640
And so is it, is it accurate to think of genetics as the kind of the data source there,

70
00:05:50,640 --> 00:05:56,880
what's happening in the gene genomics as a control point, the way you influence what's happening,

71
00:05:56,880 --> 00:06:03,040
and then AI is telling you what influence to exert based on what you, you know, the patterns

72
00:06:03,040 --> 00:06:09,600
you see. Yeah, so I think the way to think about is, genetics gives us, so this genetic databases

73
00:06:09,600 --> 00:06:15,840
give us a clue of what to start looking at, right? And we know it's really important because we've

74
00:06:15,840 --> 00:06:20,000
done studies, and others have done this as well, we're showing that if you've got a medicine that

75
00:06:20,000 --> 00:06:25,040
works on a target, so remember genes, we call it messenger RNA, messenger RNA,

76
00:06:25,040 --> 00:06:31,280
from proteins, proteins is the thing that does the things in ourselves. And the proteins are,

77
00:06:31,280 --> 00:06:34,480
what we would call it, it's a type of the thing we want to modulate, typically, and that might

78
00:06:34,480 --> 00:06:38,880
would be with a small molecule, things like clinical, things like penicillin or aspirin,

79
00:06:38,880 --> 00:06:43,280
or it could be something like an antibody, right? Which usually extracellular, not usually inside

80
00:06:43,280 --> 00:06:48,480
cells, then it's sort of blocks, blocks something. And so the genetics is giving us the hint of

81
00:06:48,480 --> 00:06:53,120
what to look for, but it doesn't, it's not the whole story, you still need to actually kind of,

82
00:06:53,120 --> 00:06:56,640
it gives you a clue, but you need to sort of go and do further experiments and understand like,

83
00:06:56,640 --> 00:07:01,520
well, you know, is this the correct target, or is it something else that's in this pathway that's

84
00:07:01,520 --> 00:07:05,680
operating in? That's the function genomics coming in, where the function genomics allows us to

85
00:07:05,680 --> 00:07:11,520
basically, what happens when we turn this particular gene off, like lower the level of that protein,

86
00:07:11,520 --> 00:07:15,600
does that look like it has the right effect, right? And it's sort of mimicking the effect of

87
00:07:16,640 --> 00:07:20,640
making a medicines, we don't have to make a medicine to work out whether it works, we can use

88
00:07:20,640 --> 00:07:25,200
that kind of thing to inform it. So it sort of starts to put these things together and

89
00:07:26,000 --> 00:07:30,480
all these experiments now, because the cost of measurement has gone down, because we can take a

90
00:07:30,480 --> 00:07:36,480
single cell and make a change in a single cell, right? And then do RNA sequencing, like look at it,

91
00:07:36,480 --> 00:07:40,800
all the changes in messenger RNA on those single cell levels, it generates a whole bunch of data

92
00:07:40,800 --> 00:07:45,680
that's at a large scale. And that's where we sort of bring machine learning in,

93
00:07:45,680 --> 00:07:53,360
maybe I can illustrate the point, probably the one of the most, one of the key problem we work on

94
00:07:53,360 --> 00:07:59,360
is that in these large genetic databases, we really only want to know what to do with about

95
00:07:59,360 --> 00:08:07,920
probably 15 to 20% of what we call the variance, and a variance is where, you know, your DNA sequence

96
00:08:07,920 --> 00:08:11,760
will differ from mine, right? Maybe I get the disease and you don't, and I've got a particular

97
00:08:11,760 --> 00:08:17,760
mutation, right? If that mutation falls into what we call the open reading frame of a gene, or the

98
00:08:17,760 --> 00:08:21,840
bit that encodes for the protein, that gets translated, mRNA encodes the protein, we know what

99
00:08:21,840 --> 00:08:24,960
affects the protein, and we can go and look at that protein and work out what's happening, you know,

100
00:08:24,960 --> 00:08:29,520
is it not fold, is it fold, but it's missing, it's not as active, that kind of thing.

101
00:08:30,400 --> 00:08:35,200
A whole bunch of them fall outside of that, right? They forms of the regulatory regions of DNA,

102
00:08:35,200 --> 00:08:39,120
and you can imagine there's a whole bunch of control structures in DNA saying what's

103
00:08:39,120 --> 00:08:44,400
the turn on what proteins and what conditions. And so we spend a lot of time in building machine

104
00:08:44,400 --> 00:08:51,040
learning models, really to understand what genes those variants are regulating, right? The ones

105
00:08:51,040 --> 00:08:56,000
that are in the control area, right? It's not always like the closest gene, it can they can have

106
00:08:56,000 --> 00:09:00,640
quite long range effects, it might be different proteins, right? And so one thing that if we can

107
00:09:00,640 --> 00:09:05,920
sort of understand what things are regulating, it gives us a whole lot of sort of potential

108
00:09:05,920 --> 00:09:10,080
targets to go and look for. So that's the sort of thing we use machine learning on, right? So we

109
00:09:10,080 --> 00:09:14,800
use it on discovering kind of, you know, things to make medicines against, so better targets,

110
00:09:14,800 --> 00:09:19,120
and the genetically valid, so we know they're more like to work, and then actually maybe things

111
00:09:19,120 --> 00:09:25,600
on the closer to the clinical side of the world, right? So I've got a medicine, how best to use

112
00:09:25,600 --> 00:09:29,520
a who's going to respond, respond, how would we measure the response and things like that?

113
00:09:29,520 --> 00:09:34,000
And that's things like computational pathology or other types of things in the clinical domain.

114
00:09:34,000 --> 00:09:40,080
One of the things that I heard in your explanation, this is maybe a little bit of a tangent, was

115
00:09:41,200 --> 00:09:48,720
suggesting that, you know, I think of, you know, variants as, you know, one of the genes gets,

116
00:09:48,720 --> 00:09:52,720
you know, flipped from a A to a G or something like that, right? So you've got these four,

117
00:09:52,720 --> 00:09:58,560
and one of them gets flipped. But I heard in your description that, you know, that's a oversimplification,

118
00:09:58,560 --> 00:10:03,040
and maybe it's not just, you know, which gene is encoded. Am I hearing that correctly?

119
00:10:03,040 --> 00:10:08,800
Yeah, so, you know, our cells are sort of a, you know, they're a network. We have lots of

120
00:10:08,800 --> 00:10:12,000
different proteins to be made, and they, you know, they communicate to other proteins and

121
00:10:12,000 --> 00:10:19,200
form up various functions. And sometimes it's literally the amount of the protein, right,

122
00:10:19,200 --> 00:10:24,800
is important, right? And so, you know, there are classic diseases, and you can think of these,

123
00:10:24,800 --> 00:10:29,360
what we call a rare genetic disorder, where you've got a single mutation, right? And it makes

124
00:10:29,360 --> 00:10:33,280
the protein functionalist, you know, a canonical example might be, I mean, you think of hemoglobin,

125
00:10:33,280 --> 00:10:38,240
the sickle cell, or you can think of factor 10A deficiencies, you don't make it effective factor

126
00:10:38,240 --> 00:10:43,600
10A, you know, you need to put that protein back. Some things are about like the level of the

127
00:10:43,600 --> 00:10:49,040
proteins it is, it can influence the, the behavior. Some things are about, some of these mutations

128
00:10:49,040 --> 00:10:52,320
means that maybe the protein doesn't get made at all. Something that means the protein gets made,

129
00:10:52,320 --> 00:10:56,720
but it doesn't, it's not as stable, so it gets turned over rapidly, so you just don't have as much.

130
00:10:56,720 --> 00:11:01,520
And sometimes you might have a mutation that makes the protein always on, so it's always

131
00:11:01,520 --> 00:11:04,720
conforming its fate, so it's not regulated anymore, right? That's another thing, and because it's

132
00:11:04,720 --> 00:11:10,000
not regulated, it's driving the behavior other pathways, and it's leads to, you know, abhorrent

133
00:11:10,000 --> 00:11:14,880
function, which leads to pathology and disease, right? And so, it's not just as simple as like,

134
00:11:14,880 --> 00:11:19,600
I've got a mutant, right, you know, what does the mutant, this mutant tells me I need to make a

135
00:11:19,600 --> 00:11:22,960
drive against that, it's actually what's the effect of the mutant? And this is the kind of the

136
00:11:22,960 --> 00:11:27,280
thing before I was talking about that kind of the variant gene problem, as we phrase it.

137
00:11:27,280 --> 00:11:31,840
Okay. Another missing piece is that kind of the gene-to-function problem, right? And that's the

138
00:11:31,840 --> 00:11:38,720
other, that's another key thing. So you've, you've got these data sources that come from genetics

139
00:11:38,720 --> 00:11:47,120
and genomics, and you're applying AI, ultimately trying to develop new drugs, new interventions.

140
00:11:47,120 --> 00:11:53,360
Can you, you talk about some of the specific use cases or problems?

141
00:11:53,360 --> 00:11:58,160
Yeah, so I think that, you know, there's a lot of things, first of all, thinking about,

142
00:11:59,920 --> 00:12:03,440
obviously, you know, taking your clues from genomics and things to come up with those targets,

143
00:12:03,440 --> 00:12:07,600
right? And then you have to think about, well, what's the effect of what we're just talking about

144
00:12:07,600 --> 00:12:11,120
like that mutation on that target, right? So there might be, is it more stable or not?

145
00:12:11,120 --> 00:12:17,280
And then it might be, well, I think about, how do I, you know, what a thing is to take the,

146
00:12:19,120 --> 00:12:23,920
the target and that sort of silly context and know that you've kind of made something better or not,

147
00:12:23,920 --> 00:12:27,680
right? Like, how do I know that I've actually found a good target that's moving, you know,

148
00:12:27,680 --> 00:12:32,240
it's going to, it's going to become a good medicine in people. And that's where actually, so we do,

149
00:12:33,440 --> 00:12:37,040
all of the things we build, all the models we build actually have sort of a large sort of

150
00:12:37,040 --> 00:12:43,520
experimental feedback loop, right? And it's actually, we, for example, that variant to gene model

151
00:12:44,160 --> 00:12:47,120
that has sort of an experimental feedback loop where we're actually doing what we call

152
00:12:47,120 --> 00:12:50,720
experiments as code. So we're asking the model what it needs and it's really sort of adaptive

153
00:12:50,720 --> 00:12:55,120
sampling under uncertainty constraints. So, you know, rather than having data being generated by

154
00:12:55,120 --> 00:12:59,600
some other process at GSK and I'm trying to build a model of that where I might have, you know,

155
00:12:59,600 --> 00:13:03,680
900 examples a week or something I'm very good at, right? And I'd be like, I really like more

156
00:13:03,680 --> 00:13:07,840
examples of things I'm not good at. We actually sort of use a lot of sort of automated, like,

157
00:13:07,840 --> 00:13:12,240
like biology. So this is biology. I'm with robot robotic automation to generate data and things

158
00:13:12,240 --> 00:13:16,960
that sort of feed back into these models. And it's the model that becomes the tool that helps us

159
00:13:16,960 --> 00:13:24,080
solve the problem, right? So we can use the, we can use that, um, that model we've built to help

160
00:13:24,080 --> 00:13:28,560
us map more of those variants of those genes. But then we still need to understand that gene

161
00:13:28,560 --> 00:13:34,560
to function for. And again, there we use automated experimentation. But in this case, we're doing

162
00:13:35,120 --> 00:13:38,480
things with these various cellular models. So they can be what we call induced

163
00:13:38,480 --> 00:13:43,520
peripheral cells, right? These IPSC cells. So these like stem-like cells from patients that have

164
00:13:43,520 --> 00:13:48,160
a disease, patients that don't have a disease. And then we actually sort of want to work out,

165
00:13:48,800 --> 00:13:54,480
you know, how, what we want to basically take the disease tissue and make it look more like the

166
00:13:54,480 --> 00:13:58,480
normal tissue. And when we say look like it could be measured by a bunch of different ways,

167
00:13:58,480 --> 00:14:02,080
it could be measured by looking at imaging data, it could be measured looking at gene expression

168
00:14:02,080 --> 00:14:08,000
pattern or protein or some kind of functional consequence. And typically these assays are

169
00:14:08,000 --> 00:14:11,920
complicated. You can't do, you know, with CRISPR and things like that. Sometimes you can just do

170
00:14:11,920 --> 00:14:15,520
what we call a genome-wide screen. I'll just do all the things, right? All 20,000 genes.

171
00:14:16,240 --> 00:14:20,880
These ones are so complex that you can't do that. And you sort of need to do an adaptive

172
00:14:20,880 --> 00:14:25,440
experimentation thing. So you can take your clues, you start with from genetics and from the literature,

173
00:14:25,440 --> 00:14:30,800
for example. And you sort of see the model for that. And then the model sort of makes an experiment,

174
00:14:31,440 --> 00:14:34,080
right? And we do everything as a sequential learning product kind of problem. We make,

175
00:14:34,080 --> 00:14:38,960
we do an experiment. We perturb a few genes. We look at how it moves things. We feed that data back

176
00:14:38,960 --> 00:14:43,360
here and we say, okay, what have we learned? Based on that, my next best experiment is to do,

177
00:14:43,360 --> 00:14:47,520
is to do this other thing. So then I make another mutation. I do another sort of round of like

178
00:14:47,520 --> 00:14:51,760
interventions on that cellar system and then sort of iterate and anything about it's kind of like

179
00:14:51,760 --> 00:14:56,480
an optimization problem. I'm trying to find the best target to move it to that, right? And so

180
00:14:57,840 --> 00:15:01,760
and you want to get there with the fewest number of steps, right? And try and find the best things.

181
00:15:02,400 --> 00:15:06,160
And at the same time, why you're trying to make it look like, you know, it's

182
00:15:07,680 --> 00:15:11,760
affecting the disease. So you have to, like your ground truth is at cellular models. You also do

183
00:15:11,760 --> 00:15:17,120
other things like making sure that like, you know, there are certain proteins in the body that you

184
00:15:17,120 --> 00:15:22,480
can't touch, right? That like have toxicity associated with it, right? So a classic one is in,

185
00:15:22,480 --> 00:15:26,320
you know, cardio toxicity, right? Like it's no good making something into your rA drug if it's

186
00:15:26,320 --> 00:15:30,240
going to give you cardio toxic, right? So the certain things that we know we can't hit this,

187
00:15:30,240 --> 00:15:34,720
there's toxicity from what we call on target toxic. If I hit this protein, but something bad happens

188
00:15:34,720 --> 00:15:39,040
and then there are other sorts of toxicology, we can also at the same time have an AI

189
00:15:39,040 --> 00:15:43,280
system that's learning to learn, which targets a toxic, which targets not to touch based on

190
00:15:43,280 --> 00:15:47,760
prior data and things like that, which things are, which targets are easier to make a small molecule

191
00:15:47,760 --> 00:15:51,600
not. It is a multi objective optimization because I can come up with targets that are really great,

192
00:15:52,320 --> 00:15:56,800
right? And we sit around, well, we know no idea how to make a selective medicine against that,

193
00:15:56,800 --> 00:16:02,320
right? So an example of one of those things is a protein involved in cancer called K-RAS.

194
00:16:02,880 --> 00:16:06,400
You know, it took people years to come up with a selective K-RAS inhibitor. It was a

195
00:16:06,400 --> 00:16:11,120
always a great target, but it just was really what we call intractable. So it's this optimization

196
00:16:11,120 --> 00:16:16,320
of finding something that moves your, your model of the disease in the right area. It's kind of

197
00:16:16,320 --> 00:16:21,040
tractable. It's not toxic, right? And then we can put forward. So that's that sort of the thing

198
00:16:21,040 --> 00:16:24,640
there. So again, we use machine learning in that sense to help design those experiments that

199
00:16:24,640 --> 00:16:33,920
carry that out. So can we, can we maybe take a step back. Have you talked through a specific

200
00:16:33,920 --> 00:16:39,040
concrete example of a project, whether you're talking about the cancer one you just mentioned and

201
00:16:39,040 --> 00:16:45,280
you know, what the data sources are, what the, the valuation criteria are and then talk through

202
00:16:45,280 --> 00:16:50,560
how this sequential learning idea plays out in some concrete context. Sure, let's, let's,

203
00:16:50,560 --> 00:16:55,040
let's talk about the variant to gene one, because I think that's, that's something that everybody

204
00:16:55,040 --> 00:17:02,400
has more of a sense of that now, especially. So what we do in that model is we have some genetic

205
00:17:02,400 --> 00:17:06,560
variants from, from standard GWAS analysis, these genetic wide association studies. And these

206
00:17:06,560 --> 00:17:11,440
are saying these variants in this region of DNA, important in the role of the disease,

207
00:17:11,440 --> 00:17:15,200
but we don't know which gene it is. It could be gene A, it could be gene B, it could be gene C.

208
00:17:16,400 --> 00:17:21,920
And so what we actually do is the, the system that looks at that, it treats the whole thing sort

209
00:17:21,920 --> 00:17:27,600
of a bit as a ranking problem, right? So top level model is, is like a ranking model, right? And

210
00:17:27,600 --> 00:17:30,960
there are a whole bunch of different features that feed into that. So think of it as equivalent

211
00:17:30,960 --> 00:17:36,640
to web search, you're saying for this, for this disease and this variant, what is the gene, right?

212
00:17:36,640 --> 00:17:40,080
And you're coming up with your ranked list of genes, right? And you'd want your top ranked

213
00:17:40,080 --> 00:17:46,160
list gene to be first, you first on the page, right? It'd be like the causal gene. So that system

214
00:17:46,160 --> 00:17:50,960
actually has a bunch of different models, right? They're feed into that. So one of these models

215
00:17:50,960 --> 00:17:55,680
are things that are like sort of these stacked encoder models that feature as of raw DNA. So look

216
00:17:55,680 --> 00:18:00,640
at the raw DNA sequence and then they predict like which, where a transcription factor and which type

217
00:18:00,640 --> 00:18:05,760
may bind, whether that sequence of DNA is what we call open or close, which is when it's packed up

218
00:18:05,760 --> 00:18:12,400
and open or closed chromatin. There are other features that talk about whether those, these

219
00:18:12,400 --> 00:18:16,720
particular genes are expressed or not in that, in a particular tissue type, because not all genes

220
00:18:16,720 --> 00:18:20,720
are turned on in all different tissue types, right? You know, cardiac cells are different from

221
00:18:20,720 --> 00:18:25,920
neuronal cells or different from skin cells. There are other features that come out of, like, knowledge

222
00:18:25,920 --> 00:18:29,520
graphs are sort of like these node embeddings. And I can talk about how we have a pretty large

223
00:18:29,520 --> 00:18:33,760
knowledge graph we use behind things. All these different types of models and there are some of

224
00:18:33,760 --> 00:18:37,920
them are neural networks and are in right, some of them are different types of things. I will sort

225
00:18:37,920 --> 00:18:43,600
of features that go into this other model. And it's again a neural network type model. But again,

226
00:18:43,600 --> 00:18:47,680
it's supervised in the form that we say, okay, here's a variant and here what we think is the gold

227
00:18:47,680 --> 00:18:53,200
standard gene for that variant is, right? And then we go away and you learn how to weight those

228
00:18:53,200 --> 00:18:58,160
particular features, right? Just much like you would train everything else. The challenge we have,

229
00:18:58,160 --> 00:19:03,440
right, is there isn't a massive amount of gold, canonical, you know, variant to gene features,

230
00:19:03,440 --> 00:19:08,160
because I just told you that we only know what to do with 15% of them. So then we have to do the

231
00:19:08,160 --> 00:19:12,320
experiment part and experiment is where we bring in the function genomics. So what we actually do

232
00:19:12,320 --> 00:19:16,720
is take cell type, depending on what we're doing, it could be different types of cell types,

233
00:19:16,720 --> 00:19:23,280
but you can say it's a primary T cell from a human donor. We do the edit and then we actually

234
00:19:23,280 --> 00:19:27,360
sequence those cells. We look at the mRNA levels and we say, okay, we know what it was before

235
00:19:28,000 --> 00:19:32,560
and we know what was afterwards. It was the differential gene expression. And you say, I think

236
00:19:32,560 --> 00:19:36,240
this variant affects this gene. So then we go and look at that gene and look at the change in

237
00:19:36,240 --> 00:19:40,160
gene expression, right? And if we get it right, you know, that gene expression falls, right?

238
00:19:41,120 --> 00:19:45,680
And only that particular gene, right? And that becomes a training data. So then that kind of feeds

239
00:19:45,680 --> 00:19:52,560
back in, you know, we rebuild the model and off we run again. And so what it means is that

240
00:19:53,360 --> 00:19:59,520
the different teams that run those different sort of submodels, you know, they can, they also

241
00:19:59,520 --> 00:20:02,960
have different data sources that they will bring in and some of them are generated from external

242
00:20:02,960 --> 00:20:06,560
data, some of them internal data to build their kind of their feature factories that feed into this

243
00:20:06,560 --> 00:20:15,440
thing. But that's how we train the whole model. And it's, it's a really interesting scenario because

244
00:20:15,680 --> 00:20:22,000
probably I would say 45% of the time, right? A simple model, right? Which is like the variant

245
00:20:22,000 --> 00:20:26,720
affects the closest gene. We'll get you there and it'd be right there. The problem is is that the

246
00:20:26,720 --> 00:20:30,480
rest of the time that model doesn't work. And it's not the closest gene. It could be something

247
00:20:30,480 --> 00:20:34,640
quite far away, right? And one third of the time from doing this experiment, it's really, really

248
00:20:34,640 --> 00:20:38,560
far away. And so not what we expected. So we've been running this learning loop, which basically,

249
00:20:38,560 --> 00:20:43,360
like, build some model in general, it's a trained data at the same time. And as a result, the overall

250
00:20:43,360 --> 00:20:47,440
model that maps variants of genes gets better. And so we track that over time. So when we started

251
00:20:47,440 --> 00:20:51,920
off, you know, we were mapping like, they were like 15% of the unexplained things in the UK by

252
00:20:51,920 --> 00:20:57,920
when we could map them up to 24%. And they were at 40%. So we know what to do now with 40% of

253
00:20:57,920 --> 00:21:02,160
our genetic variants in this database, right? And that gives us a whole bunch of new potential

254
00:21:02,160 --> 00:21:05,280
targets to go and explore with some of the other systems I talked about.

255
00:21:05,280 --> 00:21:10,800
Got it. So is the, again, sequential learning loop? And in some ways, you describe it and it

256
00:21:10,800 --> 00:21:15,440
sounds like this, you know, automated thing that's kind of continuing to iterate over time,

257
00:21:15,440 --> 00:21:20,080
performing an optimization across, you know, all of the experiments you're doing. In other

258
00:21:20,080 --> 00:21:25,040
ways, it sounds like your applying machine learning, it's given you some, you know, some features,

259
00:21:25,040 --> 00:21:30,720
some signals. And then it, you know, goes into a scientist brain that determines, you know,

260
00:21:30,720 --> 00:21:37,760
what the next step is. How close does that loop? Yeah. So the tools I'm talking about, the models

261
00:21:37,760 --> 00:21:44,080
we build, you know, they're obviously scientists involved in running the, you know, the experiments

262
00:21:44,080 --> 00:21:48,000
as code, right? Sort of sort of like, you know, tending to the robots, depending on how complicated

263
00:21:48,000 --> 00:21:53,120
experiment and the throughput we're doing, involved in that. It's really where the, where the

264
00:21:53,120 --> 00:21:56,320
human science is getting involved is sort of the output of this sort of thing. You know,

265
00:21:56,320 --> 00:22:01,680
another experiment we have is involved in discovery of an air of cancer drugs, right? And it's,

266
00:22:01,680 --> 00:22:07,440
it's a concept, something called synthetic lethality. And all that means is basically all cells,

267
00:22:07,440 --> 00:22:12,080
most, both biology we have redundant pathways are really important things, right? Tumor cells

268
00:22:12,080 --> 00:22:17,760
grow really rapidly and they tend to sometimes like, because they divide so rapidly, they can tend

269
00:22:17,760 --> 00:22:22,800
to like break and only have one functional copy of it or something. And there's a, you know,

270
00:22:22,800 --> 00:22:28,720
and so what we know then is if we can identify which things are likely to break and I can make a

271
00:22:28,720 --> 00:22:33,280
drug against that sort of thing, because it doesn't have a backup anymore, it's selectively kills

272
00:22:33,280 --> 00:22:38,880
the tumour cell over your normal, your normal tissue, right? So a GSK drug like niraporib,

273
00:22:38,880 --> 00:22:44,240
right? It's a, it's a, it's a class of drug called a pop inhibitor, but pop inhibitors are involved

274
00:22:44,240 --> 00:22:50,880
in DNA repair, right? So basically if you stop the DNA repair, it then basically, you know,

275
00:22:50,880 --> 00:22:57,760
acts to sort of kill the cancer cell. So what we actually do is have another system and again,

276
00:22:57,760 --> 00:23:02,400
this one basically looks and tries to come up with what we call synthetic lethal pairs, right?

277
00:23:02,400 --> 00:23:07,360
If you see this mutation, then you can target this other particular gene, right? And so we have a

278
00:23:07,360 --> 00:23:12,640
commons of the things we know tumour cells like, you know, mutations they get and say well,

279
00:23:12,640 --> 00:23:17,440
what things pair with that? So again, we do experiments, we knock that thing out, we turn it

280
00:23:17,440 --> 00:23:21,280
out, we turn it down, we look at this effect on viability in a whole bunch of different tumour cell

281
00:23:21,280 --> 00:23:26,800
lines, right? But then the output of that doesn't automatically become like this is the target

282
00:23:26,800 --> 00:23:30,400
go away and make it anybody against it and make a small molecule against it. That's where we

283
00:23:30,400 --> 00:23:35,920
interface with our experimental colleagues, because there's a whole lot of, these are all narrow

284
00:23:35,920 --> 00:23:41,040
purpose ML systems we're building, right? So there's a whole lot of of data and things like that

285
00:23:41,040 --> 00:23:45,040
and things that they bring into bed to think about to work out like how all that happens and

286
00:23:45,040 --> 00:23:49,440
different experiments that they will then go and design to really, it's really a hypothesis

287
00:23:49,440 --> 00:23:53,840
that's suggested by this machine learning algorithm, for example. So, you know, all the things we do,

288
00:23:54,800 --> 00:23:58,480
you know, to surface the information to work with our colleagues and this sort of thing,

289
00:23:59,280 --> 00:24:02,960
and really sometimes the production of the ML model is a lot more automated, but then the use

290
00:24:02,960 --> 00:24:07,520
of the model is where the human scientists, right? These are tools for the scientists, right? We cannot

291
00:24:07,520 --> 00:24:12,400
encode all the background knowledge of biology and things in the way we want. And also,

292
00:24:12,400 --> 00:24:17,600
the scientific literature is really messy, right? Not everything in it is correct, right? So there's

293
00:24:17,600 --> 00:24:23,040
certainly a role for domain expertise in that. And did I hear you earlier reference work that you've

294
00:24:23,040 --> 00:24:29,120
done to apply machine learning to mind the scientific literature itself? That's right. So,

295
00:24:30,880 --> 00:24:34,000
one of the ways you think about if you're doing sequential learning and you're running all these

296
00:24:34,000 --> 00:24:38,800
big learning loops is like, you know, humans, we've been doing medicine for a long time, right? We

297
00:24:38,800 --> 00:24:43,280
know a lot about biology and medicine and things like that. It would be foolish to start from a,

298
00:24:43,280 --> 00:24:47,200
you know, a clean slate and have to learn all that and we'll take a lot more samples. So,

299
00:24:48,000 --> 00:24:51,280
there are ways to think about that. Like, how do I have structured priors if you're a Bayesian

300
00:24:51,280 --> 00:24:59,120
and things like that? So, we have another group who does, you know, one of the great advances you

301
00:24:59,120 --> 00:25:03,920
see obviously is in NLP, right? So, we have all these journal articles that are either in, you know,

302
00:25:03,920 --> 00:25:09,440
open source like on the bio archive or, you know, Elsevere or PubMed etc. Right? There's lots of

303
00:25:09,440 --> 00:25:15,600
those sources and there's also in, there's also data sets published as well. And so, we have a,

304
00:25:15,600 --> 00:25:22,320
a group that sort of builds a, an NLP type model and it's based on like birth type architectures

305
00:25:22,320 --> 00:25:26,400
again, we're seeing encoders sort of appear everywhere. And what that does is that, that sort of

306
00:25:26,400 --> 00:25:30,640
pulls out things, right? So, it entities, right? So, it's entity in relationship extraction. So,

307
00:25:30,640 --> 00:25:35,200
we put what we call a semantic triple, which is really a thing, a type of relation and another thing,

308
00:25:35,200 --> 00:25:41,280
or a subject, predicate object pairs, right? The predicates we care about are a limited set of

309
00:25:41,280 --> 00:25:45,760
things. And luckily, the scientific literature isn't, has free form as the rest of the thing,

310
00:25:45,760 --> 00:25:50,720
people writing things, right? So, you didn't say A has function in B, right? X does Y, X does not

311
00:25:50,720 --> 00:25:56,160
do Y, right? And where X is and Y could be genes, diseases, small molecules, you name it,

312
00:25:56,160 --> 00:26:01,360
when there's a set of things where we're interested. So, we mine all that out, we run all the sort of

313
00:26:01,360 --> 00:26:05,760
thing over the literature, we pull out all those semantic triples, and we stick that into a really

314
00:26:05,760 --> 00:26:10,960
big store. So, we have a graph that's like 500 billion nodes, right? It's huge.

315
00:26:10,960 --> 00:26:16,160
I mean, you were referring to earlier. Yeah, yeah. So, we don't use that whole thing, what we do is

316
00:26:16,160 --> 00:26:20,560
we pull out things. So, maybe I'm interested particularly in just genes, diseases, and you know,

317
00:26:20,560 --> 00:26:24,960
protein, protein products, which is genes and things like that. And I can pull out those subgraphs.

318
00:26:24,960 --> 00:26:30,640
And we might do some link prediction on that. So, actually, it's not known, but we're pretty sure

319
00:26:30,640 --> 00:26:34,880
that X does do Y, or maybe there's some weak evidence for it. You know, you can apply again,

320
00:26:34,880 --> 00:26:39,200
you're applying ML algorithms on top of that to build that knowledge base. And then we actually

321
00:26:39,200 --> 00:26:43,040
use, usually, node embedding. There's a different way to think about, how do I represent that data

322
00:26:43,040 --> 00:26:47,360
into my algorithm? I don't, it doesn't have to learn that, like, gene X does gene Y, we've really

323
00:26:47,360 --> 00:26:50,880
told it that this happens, right? So, how do you represent that structure knowledge that you're

324
00:26:50,880 --> 00:26:56,000
confident in? And this means that we can be kind of more sample efficient, right? You view all

325
00:26:56,000 --> 00:27:00,000
ML algorithms as sort of information engines. We can be more sample efficient with the data we're

326
00:27:00,000 --> 00:27:04,080
doing. So, we learn the things we don't know, right? Rather than relearn the things we do know,

327
00:27:04,800 --> 00:27:10,560
it is a key. So, that's another big area. And once you've built these knowledge graphs,

328
00:27:11,280 --> 00:27:14,560
you know, there's lots of other uses you can put them to, rather than just like machine learning

329
00:27:14,560 --> 00:27:19,280
group, right? And finally, I just can say, oh, you know, what's new about my protein? Okay,

330
00:27:19,280 --> 00:27:23,200
here's all the facts about my protein. Like, does X do what? I've got a hypothesis that X is involved

331
00:27:23,200 --> 00:27:28,000
in Y, right? No one holds the scientific literature in their head anymore, right? It's, you know,

332
00:27:28,000 --> 00:27:33,760
it's too complicated. So, you can go on query that. But the interesting thing is, like, you know,

333
00:27:33,760 --> 00:27:38,960
said before, a big 300-year-old company, like, there are things that have been in Gone and GSK

334
00:27:38,960 --> 00:27:42,720
that people have forgotten, right? Like, is that right? You know, so, like, you actually can

335
00:27:42,720 --> 00:27:46,720
mind your own data and go, oh, wow, we did experiment about that. Or we thought we thought, you know,

336
00:27:46,720 --> 00:27:50,720
is it interesting protein someone's got, by the way, you know, 20 years ago, we worked on this,

337
00:27:50,720 --> 00:27:54,000
someone worked in a related thing and they found a molecule that affects it. It wasn't what they

338
00:27:54,000 --> 00:27:58,160
were looking for at the time, but we've got it on a shelf somewhere. So that kind of thing becomes

339
00:27:58,160 --> 00:28:03,840
like, you know, the brain of GSK. And because, you know, it's not just scientific papers, it's also

340
00:28:03,840 --> 00:28:08,640
the data sets aside with papers, you can start putting whole data sets where people are doing these

341
00:28:08,640 --> 00:28:13,120
big experiments at scale and industrial scale into these knowledge graphs as well. So, there are

342
00:28:13,120 --> 00:28:17,440
lots of experiments where people are doing screens for a particular functional things like that,

343
00:28:17,440 --> 00:28:21,280
and they come up with lists of genes that are known to be involved in things. You can import

344
00:28:21,280 --> 00:28:25,680
that knowledge as well into the knowledge graph. So, it becomes a sort of, like, growing reference

345
00:28:25,680 --> 00:28:32,640
space to use. One thing I'm curious about is you think of kind of how your team operates against

346
00:28:32,640 --> 00:28:37,840
the quadrant of kind of innovating on the biology and innovating on the machine learning. I'm curious

347
00:28:37,840 --> 00:28:45,280
where you find them and where you want them to be. You know, this space is moving so quickly,

348
00:28:45,280 --> 00:28:50,320
oftentimes you may have to innovate on the machine learning to make it work for your application.

349
00:28:51,680 --> 00:29:00,720
Is that the case here? Yeah, so I think that we have, in general, we don't work on very many

350
00:29:00,720 --> 00:29:06,400
things as a group. So we're about a 120-person research group, and we're quite globally distributed,

351
00:29:06,400 --> 00:29:12,240
I've been in San Francisco, I have people in team members in Boston, Philly, London,

352
00:29:12,880 --> 00:29:16,480
Tel Aviv, Heidelberg, Switzerland, right? There's where we're kind of everywhere.

353
00:29:17,600 --> 00:29:22,240
But a lot of things we work on, like, there isn't a solution, right? There isn't a variant

354
00:29:22,240 --> 00:29:26,000
to gene off the shelf, piece of software algorithm, right? Because the data's not there,

355
00:29:26,000 --> 00:29:30,160
you've got to build the whole thing. But there are cases where we can borrow things,

356
00:29:30,160 --> 00:29:34,240
and there are cases where we have to do research. So we do a lot of research into causal

357
00:29:34,240 --> 00:29:39,600
machine learning, because obviously we want to come up with things that are causal for the disease.

358
00:29:39,600 --> 00:29:44,400
So like, if I, you know, and what I would say have a small level of clinical hysteresis,

359
00:29:44,400 --> 00:29:48,160
and that all that means is basically, I small change in this particular, like, your drug

360
00:29:48,160 --> 00:29:51,600
against this with thing like, I don't have to knock it down 100%, if I knock it down 10%,

361
00:29:51,600 --> 00:29:55,360
has a larger effect on the cause of disease. That's the easier medicine to make. There's something

362
00:29:55,360 --> 00:30:00,480
like, well, if you take this down to 99% of its level in the body, then you might see a clinical

363
00:30:00,480 --> 00:30:03,760
effect. That's probably not a good candidate for medicine. So we do a lot of work on like

364
00:30:03,760 --> 00:30:07,440
causal, reconstruction of causal data from network data and things like that.

365
00:30:08,160 --> 00:30:14,320
And there are some areas where, you know, you might start on off with just taking things that

366
00:30:14,320 --> 00:30:19,840
have worked really effectively. So computational pathologies is a great example. So pathology is

367
00:30:19,840 --> 00:30:23,760
when you have, you might have seen those, those ready purple slides of a tumor and things like

368
00:30:23,760 --> 00:30:28,320
that or a biopsy or things like that. And so typically they were like, you know, they were

369
00:30:28,320 --> 00:30:31,920
their human toxic ears and stained slides, and they looked at biohuman pathologists,

370
00:30:31,920 --> 00:30:36,640
who looked at things and those things like old stage one, two or three cancer, for example. We know

371
00:30:36,640 --> 00:30:43,040
some information in the image, higher than number, worse it is, right? Now, what happened was,

372
00:30:43,920 --> 00:30:48,160
you know, we got digitization happening. So we got people started to scan these slides,

373
00:30:48,160 --> 00:30:52,800
a high resolution, right? And these are big images, right? These like four terabyte images,

374
00:30:52,800 --> 00:30:58,240
okay? Gigapixel images. And then the other side, we've got confidence and units and things like

375
00:30:58,240 --> 00:31:02,240
that sort of sitting around. So the natural thing was like, well, I'll just take a unit. I'll take

376
00:31:02,240 --> 00:31:05,920
a confnet and I'll see what I can, what I can do. And I might want to segment things. I might count

377
00:31:05,920 --> 00:31:09,360
the number of types of cells in the slide, rather than having a pathologist go through and do that,

378
00:31:09,360 --> 00:31:14,480
right? I might want to say, well, what's the tumor stroma ratio? Like so, when you take a tissue

379
00:31:14,480 --> 00:31:17,920
block, you might have the tumors growing here and there's normal tissue around it, right? How we

380
00:31:17,920 --> 00:31:24,000
give that area, for example, what are the characteristics? And so it started off with people doing those

381
00:31:24,000 --> 00:31:29,520
types of things, right? And those technologies work. But then almost everywhere, when you go, we start

382
00:31:29,520 --> 00:31:34,480
ask more advanced questions, you innovate on my methodology, right? So some of the things we do now are,

383
00:31:35,120 --> 00:31:41,280
well, can I predict the genetic status of a tumor just from the image alone, right? And you ask

384
00:31:41,280 --> 00:31:44,720
pathologists, they're like, well, could you tell me whether this tumor has this particular mutation

385
00:31:44,720 --> 00:31:49,120
from looking at it? They're like, it's crazy. There's no way I can do that on the human, right? And

386
00:31:49,120 --> 00:31:53,680
you think, okay, well, but you can actually build models. And we've done this, they can actually predict

387
00:31:53,680 --> 00:31:57,760
like the genetic status of the tumor. So there are subtle microchanges in environment and stained

388
00:31:57,760 --> 00:32:02,480
density and things like that due to like the changes in the biological processes, right? A human

389
00:32:02,480 --> 00:32:07,040
eye can't be, can't obviously be sensitive to train that, but you can sometimes do be retrospective

390
00:32:07,040 --> 00:32:11,440
and see what features has the model learn. But suddenly you're taking the comb nets and these

391
00:32:11,440 --> 00:32:15,680
types of things and res nets, sort of frequently. And then you're starting to push, push them in

392
00:32:15,680 --> 00:32:19,280
into different areas and you start to do, start to tinker with them again. And then you're finding

393
00:32:19,280 --> 00:32:23,280
different architecture. So we're again, we're seeing that, we see that also in, you know,

394
00:32:23,280 --> 00:32:27,360
cellular imaging as well, where we use the same types of things. We're looking at cells and how

395
00:32:27,360 --> 00:32:31,520
they're phenotypes have changed, what they look like pictureally, as they change when we give them

396
00:32:31,520 --> 00:32:36,640
a drive or don't treat them the drive, for example. So it doesn't take much before you've taken some

397
00:32:36,640 --> 00:32:40,000
off the self-technology, but typically you're, you're starting with an architecture or something

398
00:32:40,000 --> 00:32:46,880
else like that that you will then adapt to a use case. So that big, you know, bearing to gene

399
00:32:46,880 --> 00:32:52,240
algorithm, right? You know, well, I cast as a ranking problem. There's been lots of machine learning

400
00:32:52,240 --> 00:32:56,080
research into ranking problems for a long time, right? There's lots of self-tooling and things

401
00:32:56,080 --> 00:33:00,960
like that and ways to think about things. So we, those are things we start with, right? We bring in.

402
00:33:00,960 --> 00:33:05,600
And, you know, but there are some things where it's, it's wholly new algorithms and architectures

403
00:33:05,600 --> 00:33:12,880
and the things that, you know, we were sort of having to invent as well. And did your team publish

404
00:33:12,880 --> 00:33:20,320
in those areas? Yeah. So, yeah, I mean, so we, we publish, you know, we publish all our code

405
00:33:20,320 --> 00:33:24,160
and our work and it's kind of really important you do that because you're talking before about,

406
00:33:24,160 --> 00:33:29,920
like, how do you find people, right? So I think what's happened is there's a lot of people that

407
00:33:29,920 --> 00:33:35,760
have realized that, you know, there's now lots of this data appearing in biology, right? I mean,

408
00:33:35,760 --> 00:33:40,240
since post-COVID-19, it's really interested in human health, right? And you want to find an

409
00:33:40,240 --> 00:33:46,400
environment where you have the computation resources and people look to do that. But nobody wants to

410
00:33:46,400 --> 00:33:50,640
join somewhere and then connect vanish into a black hole, right? And we use, the models you just

411
00:33:50,640 --> 00:33:55,200
talked about, like, you know, we're only essentially able to, I would apply ResNet, some

412
00:33:55,200 --> 00:33:58,480
to computation mythology and start with that because it's out there in the literature and the domain.

413
00:33:58,480 --> 00:34:04,160
And that's why AI is so fast, is that free exchange of ideas and test sets and study out benchmarks

414
00:34:04,160 --> 00:34:08,720
and we level those things as well? So, you know, we will, we publish that code, right? Because it's

415
00:34:08,720 --> 00:34:14,240
the data that's really important, right? So, you know, we will publish, if it's a model that we've

416
00:34:14,240 --> 00:34:18,240
built, we've got a code and there's a public data set and we can build a model, we would also

417
00:34:18,240 --> 00:34:24,000
publish that model build public data, right? Because it's the sort of the data GSK's

418
00:34:24,000 --> 00:34:28,400
generate rate and allows us to build a model at much higher quality, right? That's our kind of strategic

419
00:34:28,400 --> 00:34:33,200
advantage, right? So, it's, again, it's all about the data and then that's similar to other industries,

420
00:34:33,200 --> 00:34:36,800
right? You know, Facebook publishes lots of really cool graph algorithms and things like that

421
00:34:36,800 --> 00:34:41,520
that don't give you their social graph, right? Data. Similar for us. But it also means that we can contribute

422
00:34:41,520 --> 00:34:48,400
to the community. We're running a challenge, I think, at ICLR this year as well on gene discovery

423
00:34:48,400 --> 00:34:53,520
and causal discovery from networks and things like that. I think we've got two or three papers

424
00:34:53,520 --> 00:34:58,880
in Europe this year out of the group as well. So, yeah, we publish and, you know, in both in

425
00:34:58,880 --> 00:35:02,960
the conference proceedings and in other scientific literature as well. It's very important for us.

426
00:35:03,760 --> 00:35:12,240
Nice, nice. Do you think much about tooling and infrastructure platforms? I have read that you're,

427
00:35:12,240 --> 00:35:21,280
I've read about your AI hub a bit. Yeah, the answer is yes, but yeah, absolutely. So, you know,

428
00:35:21,280 --> 00:35:25,680
there's a few, so, the infrastructure, I mean, there's one thing I learned from doing start.

429
00:35:25,680 --> 00:35:28,960
It's like, you know, the, you start building infrastructure now on the next best way to start

430
00:35:28,960 --> 00:35:32,480
implementing infrastructures tomorrow because it allows you to scale and if you suddenly have

431
00:35:32,480 --> 00:35:37,360
infrastructure problems, it's really difficult to solve once you're in that phase. In the AI team,

432
00:35:37,360 --> 00:35:43,520
we have a whole group that's really an AML platform organization and they build all the kind of

433
00:35:43,520 --> 00:35:48,400
tooling and infrastructure for us to kind of, you know, deal with data containers and running

434
00:35:48,400 --> 00:35:53,120
things and scanning algorithms and other kind of those aspects. And it's about not only just,

435
00:35:54,320 --> 00:35:58,080
you know, GPUs, computing things, you know, we do a lot of pie talk extensively,

436
00:35:59,040 --> 00:36:04,080
is our, is our preferred platform of choice. But some of it even also comes down to the things

437
00:36:04,080 --> 00:36:08,880
we're looking at, we end up needing kind of like novel compute, right? So, we've had a strategic

438
00:36:08,880 --> 00:36:12,640
partnership with a company called Cerribris, which you may have, some of you that you may be familiar

439
00:36:12,640 --> 00:36:17,520
with, right? Cerribris have one of these companies they built like, you know, a really

440
00:36:17,520 --> 00:36:23,600
really amazing piece of hardware. And so we use Cerribris for a particular type of problem where

441
00:36:23,600 --> 00:36:29,040
we're building these encoder models on, on DNA. Now, what's interesting is these encoder models

442
00:36:29,040 --> 00:36:33,360
is we want to have a really, really large window size, right? And so you get to this, it's really

443
00:36:33,360 --> 00:36:39,280
challenging to build model parallel and data parallel kind of algorithms at the sort of scale. And

444
00:36:39,280 --> 00:36:43,360
the data sets we're passing over are really, really large as well, right? It's for these

445
00:36:43,360 --> 00:36:49,840
genomic data sets. So that was a really interesting problem that the Cerribris system is like,

446
00:36:49,840 --> 00:36:53,680
it's got massive throughput. It could be a really big model because of the scale of the chip

447
00:36:53,680 --> 00:36:59,520
products and the latency between that and the memory was really large. So, you know, we started

448
00:36:59,520 --> 00:37:04,720
working with them. We've got us, we have a CS1 system. We'll have our CS2 soon. And, you know,

449
00:37:04,720 --> 00:37:09,360
that becomes a strategic thing that we can start to build new algorithms and play new things on

450
00:37:09,360 --> 00:37:14,000
and actually build models for. And then, you know, so the, so the, the compute is really important.

451
00:37:14,000 --> 00:37:19,600
For us, it's a, it's key to be unconstrained by, by computer, right? To be like, you know, to think

452
00:37:19,600 --> 00:37:24,960
of like, you know, what's the best way to solve the problem. You know, we usually constrain by data,

453
00:37:24,960 --> 00:37:30,720
like the data I love to have, right? And so this is also why, you know, we also work with Nvidia, where

454
00:37:30,720 --> 00:37:36,000
we're pushing the bounds of CUDA. So we have an a strategic ground arrangement with Nvidia, where

455
00:37:36,000 --> 00:37:40,160
we have people on site in London, where, you know, we're making changes to low-level, you know,

456
00:37:40,160 --> 00:37:45,040
LibDNN and things like that or working with them on those types of things. So, you know, how do we,

457
00:37:45,040 --> 00:37:49,120
how do we, how do we make it easier for us to do it focus on like, you know, only one problem,

458
00:37:49,120 --> 00:37:52,640
which is the science problem rather than two problems, like the science problem and engineering

459
00:37:52,640 --> 00:37:57,520
problem, right? Because the, the challenge we face are hard enough, right? So, so that's, that's a key,

460
00:37:57,520 --> 00:38:02,720
is a key component for us. And, you know, the other thing is like, we want to think about how

461
00:38:02,720 --> 00:38:06,480
many iteration cycles a machine learning person can do per day, right? I don't want to be someone

462
00:38:06,480 --> 00:38:09,680
sitting there like, I've got an idea, I've kicked it off. Well, I guess that'll be done in two

463
00:38:09,680 --> 00:38:13,200
days time. Yeah, I'll sit here and read a thing. I want to be able to like look at something,

464
00:38:13,200 --> 00:38:16,960
have an idea or have a bunch of ideas, kick them off and then actually get the results back

465
00:38:16,960 --> 00:38:20,640
that afternoon and think about it and then, you know, run on to other sets. That's also really key

466
00:38:20,640 --> 00:38:26,640
for us is to be, you know, so as we grow, we've needed to add, you know, every every every new AML

467
00:38:26,640 --> 00:38:32,880
high requires, you know, a bunch of A 100s or whatever we need, like added to the stack, right? It's a cost.

468
00:38:32,880 --> 00:38:39,120
Yeah, you mentioned earlier, you reference a feature factory. So you're developing these features

469
00:38:39,120 --> 00:38:44,640
and you reference a feature factory, is that a concept or an idea or is that a physical thing,

470
00:38:44,640 --> 00:38:50,720
like a feature store? So you can imagine for, you know, if I've got, you know, that section of DNA

471
00:38:50,720 --> 00:38:56,560
and I've got my very antenna, right? Those, there are different models that can like tell you different

472
00:38:56,560 --> 00:39:01,120
things about it. So analogy with the web page, I would have the title of the web page, the links to

473
00:39:01,120 --> 00:39:05,600
it, the text and those links, you know, the content of the web page, the word count, author, the date,

474
00:39:05,600 --> 00:39:12,240
those are all features about the website, right? And, you know, that you have tools and code that

475
00:39:12,240 --> 00:39:16,400
could pull those things out and represent that and a featureisation to some kind of AML model.

476
00:39:16,400 --> 00:39:23,120
Similar in this case, there are, we look at that whole stretch of DNA and the disease that's

477
00:39:23,120 --> 00:39:27,520
in the similar context and there are algorithms. In this case, there are models themselves

478
00:39:28,000 --> 00:39:31,840
that work on how to featureize that, to represent that to that whole ranking algorithm.

479
00:39:31,840 --> 00:39:38,000
So we term those sort of things as a feature factory, right? So the ones that look at the raw DNA

480
00:39:38,000 --> 00:39:41,840
sequence and will say, well, this is open and closed chromatin, the other one will say, well,

481
00:39:41,840 --> 00:39:46,160
this gene isn't on in this cell type, right? And it might imagine the model when it's trying

482
00:39:46,160 --> 00:39:50,480
to, when it had ranked the importance of those things to give you, you know, it's candidates

483
00:39:50,480 --> 00:39:54,640
of genes might say, well, well, this gene isn't even on in this cell type that's involved in this

484
00:39:54,640 --> 00:39:59,920
disease. So this is probably a low unlikely thing, right? This one's in closed chromatin, that's

485
00:39:59,920 --> 00:40:03,520
unlikely. This one's in open chromatin, it's involved in disease and things like that. So it's

486
00:40:03,520 --> 00:40:08,320
becomes a good candidate. So it learns how to rank all these different things and how to combine

487
00:40:08,320 --> 00:40:14,880
them. And so it's, it's not a, it's not a sort of a physical thing, but it is a sort of like,

488
00:40:14,880 --> 00:40:19,040
it's a featureisation type aspect, right? So it's basically a featureisation by other

489
00:40:19,040 --> 00:40:27,280
sort of submodels themselves. Yeah, when you think about all of the, the various algorithms

490
00:40:27,280 --> 00:40:34,320
and tools and things that kind of factor into and enable what you're doing and look forward,

491
00:40:34,320 --> 00:40:42,960
what are the areas that, either you need the most innovation happening or you're excited

492
00:40:42,960 --> 00:40:48,080
because you see the innovation happening, you know, whether, you know, we're talking about algorithms

493
00:40:48,080 --> 00:40:55,280
or tooling infrastructure, that kind of thing. Yeah, I think one of the things that I'm deeply

494
00:40:55,280 --> 00:41:04,080
interested in is robustness and reliability constraints, right? And, and this plays into a,

495
00:41:04,080 --> 00:41:08,320
you know, a debate with, you know, in regulation and things like that, is that as we start to build

496
00:41:08,320 --> 00:41:12,800
sort of probabilistic reasoning systems and imagine, let's take the pathology example where I have images

497
00:41:12,800 --> 00:41:18,720
and things like that. And we've, maybe, you know, we've designed it well, so we've made sure we've

498
00:41:18,720 --> 00:41:21,840
got like, you know, bunch of people different backgrounds, you know, we've got a lot of,

499
00:41:21,840 --> 00:41:25,520
lots of training day, we'd have underrepresented groups, we've done the best case to do that,

500
00:41:25,520 --> 00:41:30,480
that's very important. And we built a model and the model has good performance characteristics,

501
00:41:30,480 --> 00:41:35,360
you know, maybe 80% of the time it's correct and it predicts someone's got to, as a, you know,

502
00:41:35,360 --> 00:41:39,440
it gets a genetic status right, so we know then who to sequence or not, for example, I'm just

503
00:41:39,440 --> 00:41:45,680
using a hypothetical scenario. How do we know how that model behaves when, you know, what's the

504
00:41:45,680 --> 00:41:50,000
episode image for like a, you know, for a pathology thing like it's like, maybe the area is out of

505
00:41:50,000 --> 00:41:55,440
focus, right, or it's got a pen mark, or it isn't enough tumistrum. How do we know that this model

506
00:41:55,440 --> 00:42:00,080
fails gracefully? How can we define as its balance of operation and things like that? And, you know,

507
00:42:00,080 --> 00:42:05,280
for some other methods and things, it's easier to define and construct that, you know, for a new

508
00:42:05,280 --> 00:42:11,040
network, it's harder to know how, you know, some of those changes result in, you know, the decision

509
00:42:11,040 --> 00:42:16,640
boundary actually happening. And so knowing that you've trained a model that's, um, for some

510
00:42:16,640 --> 00:42:21,840
of these scenarios, you would train off some, you know, some performance for robustness characteristics,

511
00:42:21,840 --> 00:42:26,960
right? But it is hard to know how those robustness and reliability characteristics happen. So,

512
00:42:26,960 --> 00:42:30,560
we're doing a lot of, we do a lot of research in that area and we have various groups we interact

513
00:42:30,560 --> 00:42:36,560
with and PhD students, we sponsor. But that's a really active area that's, you know, it's not just,

514
00:42:36,560 --> 00:42:41,600
you know, our organization, I have to particularly, it's, it's across the industry in various things,

515
00:42:41,600 --> 00:42:46,320
where people want to know, um, know those, those sorts of aspects and that's where you get into

516
00:42:46,320 --> 00:42:52,560
monitoring other sorts of things. But for us, it's, it's all about, um, knowing how I can measure,

517
00:42:54,080 --> 00:42:59,600
that I found a good robust solution and it's not brittle, right? The small changes that

518
00:42:59,600 --> 00:43:06,640
import don't lead to large changes in the activation. Um, that's, that's one key area. Uh, I think for

519
00:43:06,640 --> 00:43:14,400
us, the, you know, looking at, um, simpler transformer architectures that could lead to the same

520
00:43:14,400 --> 00:43:18,560
kind of performance is another really key thing is we can train them faster and things like that.

521
00:43:18,560 --> 00:43:24,480
So I understand a little bit of that sort of, you know, just the, you know, um, model parameters

522
00:43:24,480 --> 00:43:29,120
based performance trade-offs and sorts of sorts of things, you know, um, where you think, well,

523
00:43:29,120 --> 00:43:34,320
maybe there is a simpler architecture that can do just as well. Uh, that's, that's a, a common

524
00:43:34,320 --> 00:43:42,880
error research. And, you know, I've been a lot of it actually comes down to really, uh, biology is

525
00:43:42,880 --> 00:43:50,080
all about, um, low, low and high dimensionality, right? And, uh, sort of biases and time series,

526
00:43:50,080 --> 00:43:56,800
right? So a lot of our data, if you think about it, that idea where I've edited my variant in,

527
00:43:56,800 --> 00:44:04,640
I'm looking to see which genes change, right? Well, I can measure that six hours after I've made

528
00:44:04,640 --> 00:44:10,480
my edit 12 hours or 24 hours and the whole thing is changing over time, right? And so actually

529
00:44:10,480 --> 00:44:14,640
begins to start to integrate those, those, those temple dimensions. So this is where we have a lot

530
00:44:14,640 --> 00:44:19,120
of time series data and we can generate that in biology. And this is where I think that's another

531
00:44:19,120 --> 00:44:25,760
area that, of key research and probably the, the final one is really sort of multimodal, um,

532
00:44:25,760 --> 00:44:31,520
and end to end, and, and learning multimodal. So they're the classic examples. I have some

533
00:44:31,520 --> 00:44:36,160
cells in a dish. I'm taking images of them and I might pull them out and do RNA seek. And I

534
00:44:36,160 --> 00:44:39,680
wasn't at my time series, that's why they're in for good measure. And I want to start to look at

535
00:44:39,680 --> 00:44:45,360
that and I want to build a model that can classify when a perturbation has made a cell look like the

536
00:44:45,360 --> 00:44:49,920
wild type cell, the healthy tissue, and made its gene expression look like that, right? And I'd

537
00:44:49,920 --> 00:44:55,440
like to go to end to end learn that right now we typically learn the gene expression model,

538
00:44:55,440 --> 00:44:59,280
the cellular imaging component together. We'd maybe take the top two layers of those sorts of

539
00:44:59,280 --> 00:45:03,120
things and you throw that into another model learns to integrate them. We don't propagate the

540
00:45:03,120 --> 00:45:07,120
error back down through all of that, just because the complexity of the thing. But that's an area where

541
00:45:07,120 --> 00:45:15,840
I think that, you know, could be really useful. And, you know, typically, you know, you mean, the

542
00:45:15,840 --> 00:45:18,640
convolution size, right? That's that you could use attention on that. You can have these

543
00:45:18,640 --> 00:45:22,080
dilated and flexible convolutions where you could adapt that because it might not, you know,

544
00:45:22,080 --> 00:45:25,280
picking the one that looked good for that, it could be a very specific one that could work better

545
00:45:25,280 --> 00:45:29,200
for that problem, right? When you want to, you know, so that that's a that's a massive error

546
00:45:29,200 --> 00:45:35,600
research for us. And because we have that in medicine, right? We have, um, you know, your biopsy,

547
00:45:35,600 --> 00:45:40,080
your pathology biopsy might be done once, right? When you're diagnosed. But we can do clinical

548
00:45:40,080 --> 00:45:45,520
imaging every seven or eight weeks. It's like a CT scan or MRI, for example. But I might do your,

549
00:45:45,520 --> 00:45:50,000
your blood work, right? I can sample it. You take a file of blood and I might look for circulating

550
00:45:50,000 --> 00:45:54,880
tumor DNA, right? It's the things like grail or, or free gnome or those sorts of things, um,

551
00:45:54,880 --> 00:45:59,440
garden health, right? Those sorts of assays we're looking at literally DNA in the blood that's

552
00:45:59,440 --> 00:46:04,160
come from the tumor cells, right? And look sequencing that. And, and that could be done. They could,

553
00:46:04,160 --> 00:46:08,080
uh, they were done at different timescales, but they're all multimodal things about that

554
00:46:08,080 --> 00:46:12,560
particular patient and you're trying to integrate all those together to say, what's your particular

555
00:46:12,560 --> 00:46:15,680
outcome going to be? Are you responding to this therapy? Where are you going to go? When, what,

556
00:46:15,680 --> 00:46:21,120
what therapy should we give you next, for example? Mm hmm. And to some degree, that brings us back

557
00:46:21,120 --> 00:46:26,720
to compute because the scale required to integrate all this together is significant. Yeah, it's

558
00:46:26,720 --> 00:46:30,960
computing data, right? Because, um, you can, we can measure so many things, right? But we're

559
00:46:30,960 --> 00:46:37,600
measuring, pre-offent, often what you see in biology is we can measure more data on things that

560
00:46:37,600 --> 00:46:42,320
are wrestling, wrestling and people. So I can learn lots of data on a, on cells in a dish, right?

561
00:46:42,320 --> 00:46:46,240
But cells in a dish aren't going to give me the effect of like, you know, a whole person, right?

562
00:46:46,240 --> 00:46:51,440
I can't ever get information about whole organ failure from single cell culture of hepatocytes.

563
00:46:51,440 --> 00:46:56,080
But I mean, we're starting to see more complex innovations in biology. So things like organoids

564
00:46:56,080 --> 00:47:02,000
and things like that where they start to have more of the complexity and you see, and where we

565
00:47:02,000 --> 00:47:06,160
end up finding machine learning is actually building a bridging model from the, the thing that we can

566
00:47:06,160 --> 00:47:11,440
perturb a measure at scale and then how well does that correlate to, you know, to humans where we

567
00:47:11,440 --> 00:47:15,680
can only really like, we can't perturb humans. We can treat humans if we, if we've got a really

568
00:47:15,680 --> 00:47:22,080
good thing, we can measure things about us, right? So, um, that's sort of another area.

569
00:47:22,080 --> 00:47:26,320
Um, that's one of the things that we were actually doing with this sort of this King's College

570
00:47:26,320 --> 00:47:31,520
collaboration that was recently in the press where what we'll be doing is, and is, as we're taking,

571
00:47:31,520 --> 00:47:37,360
um, tumor samples from, from patients, right? And we can culture their tumor, and it's not,

572
00:47:37,360 --> 00:47:41,120
and it's not organoid. So it's the tumor, but it's passed their immune system and it's actually,

573
00:47:41,120 --> 00:47:46,320
quickly, it's that immune system combines from that particular patient. And then we can start to

574
00:47:46,320 --> 00:47:50,000
see how that responds, right? With various drugs, with influence, and we can measure very things

575
00:47:50,000 --> 00:47:54,480
about that and look at that over time. And the idea there is to sort of build a model of,

576
00:47:55,040 --> 00:47:58,320
you know, how best to treat that patient, one of the characteristics. So we even want to,

577
00:47:58,320 --> 00:48:02,240
what is the risk? Lung cancer, for example, you might resect it. You're hoping that you've

578
00:48:02,240 --> 00:48:06,080
got it all, and there's no secondary metastasis, but there are some people that will see a higher

579
00:48:06,080 --> 00:48:10,720
rate of secondary metastasis than others, right? So could you, how can you identify that, for example?

580
00:48:10,720 --> 00:48:15,920
So it's just a really interesting interplay between the development of experimental

581
00:48:15,920 --> 00:48:20,560
biological techniques, the ability to generate data at scale, right? And the ability to build

582
00:48:20,560 --> 00:48:25,760
models to kind of connect them back to humans. You mentioned robustness as being important,

583
00:48:25,760 --> 00:48:31,760
and before that, you talked a little bit about explainability. I'm curious how you think about that,

584
00:48:31,760 --> 00:48:38,240
and, and how you approach machine learning problems with those concerns in mind. Do you,

585
00:48:38,240 --> 00:48:44,160
you know, drive for performance and then back off to the explainability requirements,

586
00:48:44,160 --> 00:48:48,160
is it the other way around? Is there some kind of hybrid? It's, it's a really interesting

587
00:48:48,160 --> 00:48:55,280
debate, because a lot of the times, you know, I think people kind of use interpretability

588
00:48:55,280 --> 00:49:00,000
or these types of things as a proxy for I don't trust or understand sufficiently the engineering

589
00:49:00,000 --> 00:49:05,360
validation, right? And so, you know, I mean, I had the question, I was like, okay, I can give you a

590
00:49:05,360 --> 00:49:09,040
very, what was the simple model we like? I'm like, oh, I like a logistic regression with like six

591
00:49:09,040 --> 00:49:13,680
parameters. I'm like, okay, if I give you a logistic regression with six parameters, right?

592
00:49:13,680 --> 00:49:18,800
And maybe I only allow like, you know, positive non-zero coefficients, right? It's a huge number

593
00:49:18,800 --> 00:49:22,000
of functional forms can be known, but most people can't look at it in their head and really

594
00:49:22,000 --> 00:49:27,120
understand how that works, it makes the decision, right? Or have been said to the threshold. So,

595
00:49:27,120 --> 00:49:34,000
and we use technology and systems everywhere day to day without knowing how they work, right?

596
00:49:34,000 --> 00:49:39,840
Like everyone in the lab, where it comes down to is actually where you want reliability constraints

597
00:49:39,840 --> 00:49:45,440
and how it performs, right? And that's that's the sort of the trade-off, but there's also there's

598
00:49:45,440 --> 00:49:50,160
a trade-off between when do we really need to have secondary checks and things like that? We're

599
00:49:50,160 --> 00:49:56,720
making really big decisions, right? You know, you know, avionics for flying my plane or maybe doing

600
00:49:56,720 --> 00:49:59,760
I'm going about to diagnose someone with something else, you know, I really need to, how do I,

601
00:49:59,760 --> 00:50:03,600
how do I have a functional sources data? How to make sure it's robust? And there's sort of things

602
00:50:03,600 --> 00:50:09,200
in the discovery phrase where do we need to have some in explains to a human scientist? These are

603
00:50:09,200 --> 00:50:13,360
the key features and why we think this, this cell type is more like this cell type and others and

604
00:50:13,360 --> 00:50:18,000
this, this target's pushing it in the right direction versus and there's a trade-off between like,

605
00:50:18,000 --> 00:50:23,760
well, we're asking the machine to do tasks that a human can't. We're putting in so much data,

606
00:50:23,760 --> 00:50:29,200
and we're going to take those results and then check them in other ways, right? There, I would rather

607
00:50:29,200 --> 00:50:33,200
harness the full power of machine learning and not hamstring the system by saying, well, okay,

608
00:50:33,200 --> 00:50:37,280
here's a saliency map and having to us, I agree with that or not, or the functional form. So,

609
00:50:37,280 --> 00:50:41,520
depending what we're doing, it's a trade-off, but what we always care about is making sure we build

610
00:50:41,520 --> 00:50:46,240
a robust reliable model. So, like, understanding how you're assessing it, right? How you measure

611
00:50:46,240 --> 00:50:50,880
the performance? How do you understand them? Like, you know, you haven't somehow an information

612
00:50:50,880 --> 00:50:56,560
leakage and those sorts of things come through. So, it's attention, but where you do find things is where

613
00:50:57,520 --> 00:51:01,920
you have to make sure that you, you know, if you're doing, you're placing what someone would be

614
00:51:01,920 --> 00:51:06,160
doing manually, right? So, there's a whole thing of like, well, is this thing better than me? How do I

615
00:51:06,160 --> 00:51:10,320
know it's working? How is that quality aspect? And usually, once you want to say is like, look,

616
00:51:10,320 --> 00:51:14,400
I'm here to automate the boring, right? So, you can actually do and go to high-level science,

617
00:51:14,400 --> 00:51:19,200
then spend your time, you know, looking and analyzing this, right? And also giving them sort of an

618
00:51:19,200 --> 00:51:22,400
audit trail, they can go back and look at the data that went into the system and maybe look at it

619
00:51:22,400 --> 00:51:28,880
from a self or diagnosis tools, right, on the model's performance as well, you know? So, things like

620
00:51:28,880 --> 00:51:33,360
is the input vector within a vector space that's well bounded by the training set and the test set?

621
00:51:33,360 --> 00:51:38,480
What does the error manifold look over that thing? Is it uniform or is there a spiky regions where

622
00:51:38,480 --> 00:51:42,720
it, you know, because we, a lot of our performance measures are global measures or model performance,

623
00:51:42,720 --> 00:51:48,720
but you could easily take the input vectors, you can title them on a 2D plane, you can work out the

624
00:51:48,720 --> 00:51:53,120
error function over those things, right? And you can say, oh, wow, overall, it's a pretty good model,

625
00:51:53,120 --> 00:51:57,680
but like, you know, small molecules that look like this, this thing's lousy at, right? So, having

626
00:51:57,680 --> 00:52:01,840
uncertainty bounds, those sorts of things, they go a long way to actually putting these things in

627
00:52:01,840 --> 00:52:07,120
production. And, you know, another thing is that it's really important not just to have any model

628
00:52:07,120 --> 00:52:10,880
over spit out a number, anything we put in production has to give you both a number and a confidence

629
00:52:10,880 --> 00:52:16,480
bound and also has to, it also has to refuse to return a value. It's saying, I don't have that.

630
00:52:16,480 --> 00:52:21,520
This is so far out what I've seen, boss, I have no idea. We can collect those, we can log those,

631
00:52:21,520 --> 00:52:24,800
maybe we've got enough of them, we can build another model and over time almost every model becomes

632
00:52:24,800 --> 00:52:28,080
a cascade function. It's like, well, this is a global model for this one, this model for this,

633
00:52:28,080 --> 00:52:32,160
maybe eventually we can unify them again, but that's really important. Those are all the sort of

634
00:52:32,160 --> 00:52:37,440
the functional things because it's honestly, it's quick to make a model and once you give people

635
00:52:37,440 --> 00:52:41,360
a tool and it's very quick for them to use it, but it's the opportunity cost of the downstream

636
00:52:41,360 --> 00:52:44,800
decisions that they can make with it, right? They decide to do experiment A and not experiment B,

637
00:52:44,800 --> 00:52:51,440
for example. So, we think a lot about those sorts of things. And, you know, when you start off,

638
00:52:51,440 --> 00:52:54,720
it's always, I want to know how it works, what's the model thinking and things like that.

639
00:52:54,720 --> 00:53:01,680
But, you know, a lot of those theories of mind are not really truly how the model's thinking,

640
00:53:01,680 --> 00:53:05,920
even if we do distillation, right, and things like that. That's not really it is, it's sort of a

641
00:53:05,920 --> 00:53:12,960
hack on those things. Getting models to predict or describe their confidence, that's an active

642
00:53:12,960 --> 00:53:20,480
research area itself. Does that requirement that models, your model spit that out? Does that put

643
00:53:20,480 --> 00:53:26,800
a limit on the types of architectures you use, or is it itself kind of an area where you have to

644
00:53:26,800 --> 00:53:33,680
question reliability and trust of that confidence element? You know, we do a lot of work on that

645
00:53:33,680 --> 00:53:38,080
internally. I think, I guess, Francesco Ferrin has got some pretty good papers coming out on

646
00:53:38,080 --> 00:53:44,560
that. Well, he's published that we've done on that sort of aspect. We try and have some general

647
00:53:44,560 --> 00:53:48,080
genetic methods we can bolt onto any architecture. So, you can kind of separate the problem a little

648
00:53:48,080 --> 00:53:52,000
bit. Some architectures also give you, you can give you at the same time estimates of that,

649
00:53:52,000 --> 00:53:58,400
depending on what you're doing. But we also, depending where we are in development and where we're

650
00:53:58,400 --> 00:54:05,600
using it, you can, you might have great or less requirements for that kind of aspect of things,

651
00:54:05,600 --> 00:54:12,160
right? So, there's a bit of a, certainly once you're making big important decisions and you're

652
00:54:12,160 --> 00:54:16,640
putting things in production and it's going into other processes, you definitely need to have

653
00:54:16,640 --> 00:54:20,800
those aspects. But I wouldn't say every model of good at GSK by every group will always have

654
00:54:20,800 --> 00:54:25,280
those characteristics. For us, where we're making these big things that have, you know, big downstream

655
00:54:25,280 --> 00:54:31,040
consequences and decisions, you need to have that sort of place. And I think that, you know,

656
00:54:31,840 --> 00:54:37,680
I think just in the community globally, people are realizing that, right? You know, and that's also

657
00:54:37,680 --> 00:54:42,640
about sort of monitoring things in production, right? You know, of checking things. And, you know,

658
00:54:42,640 --> 00:54:48,640
there's plenty of examples of models going rogue and, you know, that kind of thing. And so,

659
00:54:48,640 --> 00:54:56,960
for us, it's a key thing. But we, depending on the stage, is how stringent we are on those

660
00:54:56,960 --> 00:55:02,320
requirements for it. Maybe kind of one more direction to briefly explore, kind of zooming out.

661
00:55:02,320 --> 00:55:12,480
The, can you speak a little bit about the, you know, kind of building an organization like yours

662
00:55:12,480 --> 00:55:20,080
in the context of a large, large company, large pharmaceutical company, you know, transformation

663
00:55:20,080 --> 00:55:27,840
implications, organizational receptivity to probabilistic models. That kind of thing is it,

664
00:55:27,840 --> 00:55:34,160
you know, it's a, it's a research organization or a scientific organization at its core. So,

665
00:55:34,160 --> 00:55:39,040
do you, you know, not have the resistance that, you know, places have or...

666
00:55:40,320 --> 00:55:44,800
Yeah, I mean, I wouldn't have come here if it wasn't for having, like, you know,

667
00:55:44,800 --> 00:55:48,560
how is the head of R&D? Because you've got someone who really gets it. And you know, you

668
00:55:48,560 --> 00:55:54,560
got to go, all right, by, by himself, like you said, engineer as well. It's really important to

669
00:55:54,560 --> 00:55:59,680
have an organization that, because it's core to the strategy, like people like, oh, we're going

670
00:55:59,680 --> 00:56:04,320
to have AIML. We're going to do it. And sort of when I came in, I'm like, look, we get, like,

671
00:56:04,320 --> 00:56:08,240
the normal you hire and do people who doesn't work anymore, right? I want any of you people,

672
00:56:08,240 --> 00:56:10,800
and I'm going to give an offer in a few days, like, like, three days. We're going to do it.

673
00:56:10,800 --> 00:56:13,360
We're going to use hacker rack. We have all these different things we bring people in, and we're

674
00:56:13,360 --> 00:56:17,200
not going to do the usual process, so you, they get offered in three months time or three,

675
00:56:17,200 --> 00:56:22,080
you know, a month, because they've got other places to be, right? We want to show them that we're

676
00:56:22,080 --> 00:56:27,280
not this giant, usified organization that we can move fast and do things, right? And, you know,

677
00:56:27,280 --> 00:56:30,320
even to the way we're working, you know, we're going to use Slack. We're going to be distributed

678
00:56:30,320 --> 00:56:34,560
everywhere, we're going to go with talent ears, you know, we use a laptop and a thing we can work

679
00:56:34,560 --> 00:56:39,040
that way. You know, post COVID, I think the rest of the organizations caught up to that. So we came

680
00:56:39,040 --> 00:56:46,000
in and like, you know, did things in a very, very different way, right? And like from the way we

681
00:56:46,000 --> 00:56:51,840
interviewed HR hiring, but also, you know, we always max like this HPC requirements are we're going

682
00:56:51,840 --> 00:56:56,480
to do things and like, and so we built like a whole new process to do things, right? That are

683
00:56:56,480 --> 00:57:00,000
the way we work, right? We work in two big sprints. We have these different types of things.

684
00:57:00,960 --> 00:57:07,920
What's interesting is actually seeing the wider organization kind of been given permission to

685
00:57:07,920 --> 00:57:11,040
think and innovate and like some of them picking up some of those tech techniques and things like

686
00:57:11,040 --> 00:57:14,720
that and the methods of managing science, right? Because I first looked at my biological colleagues,

687
00:57:14,720 --> 00:57:19,040
they're like, you don't stand, but it doesn't work that way. You can't plan things out for like this

688
00:57:19,040 --> 00:57:22,080
could take longer or shorter. I'm like, well, you know, computer science doesn't work that way either,

689
00:57:22,080 --> 00:57:26,080
right? Like we actually like, you know, it's, I love to think that like we know the thing and we

690
00:57:26,080 --> 00:57:29,680
write down all the steps and we just do it. That's not how it works, right? So it's always a garden

691
00:57:29,680 --> 00:57:34,240
of walking paths. So doing that, stopping every two weeks, going what work, what did work? Okay,

692
00:57:34,240 --> 00:57:38,560
now we'll do this. Let's have a really good way of planning and working across really complex

693
00:57:38,560 --> 00:57:44,560
teams as we do. So, you know, we broaden a lot of that culture. You know, there are always people

694
00:57:44,560 --> 00:57:48,320
that are true believers, right? They're like, oh my gosh, ML and AI could do everything. You're like,

695
00:57:48,320 --> 00:57:52,240
well, slow down, you know, let's just talk about what we're doing here. And then there are people

696
00:57:52,240 --> 00:57:56,560
that are super skeptical, right? That are like, well, you know, how is this anything different from

697
00:57:56,560 --> 00:58:01,120
before? You know, this stuff or to the, no, you'll never place a human science, this human

698
00:58:01,120 --> 00:58:05,600
ingenuity. You know, I've got a better way of picking tech. I can synthesize this on my head.

699
00:58:05,600 --> 00:58:10,240
And maybe they really do have an alpha value. Maybe they really do have that. But sometimes a lot of

700
00:58:10,240 --> 00:58:14,880
I'm like, yeah, I think you've got a bit of biorecal bias there as well. So it's attention to trade off.

701
00:58:16,080 --> 00:58:19,680
And like all these things, we'll work out where these, some of these tools, based on the

702
00:58:19,680 --> 00:58:26,480
technology maturity stack, where they're best used and where we're too early, right? Or where we don't

703
00:58:26,480 --> 00:58:31,840
have enough data or the right data, that sort of thing. But it is, it's been fun and it's been fun

704
00:58:31,840 --> 00:58:37,360
just sort of, you know, as the organizations grow and more people have come in. And I think what

705
00:58:37,360 --> 00:58:43,360
people have realized is that, you know, if you're interested in doing machine learning in bio and

706
00:58:43,360 --> 00:58:50,080
those sorts of things that come is like GSK actually have lots of compute. So if people that you,

707
00:58:50,080 --> 00:58:53,920
like, join me with the MIT is like, now I've got more compute and more data than I ever had, right?

708
00:58:53,920 --> 00:58:57,760
And I don't have to write grants and spend all my time doing these sorts of things. And if you

709
00:58:57,760 --> 00:59:02,560
get it right, you've got a whole machine that will translate to an impact, right? To patients,

710
00:59:03,280 --> 00:59:06,880
to really do those sorts of things. And that's, that's really important to a lot of people as well,

711
00:59:06,880 --> 00:59:11,280
is that is that connection to the part of the whole thing, rather than, okay, I'm an academic,

712
00:59:11,280 --> 00:59:15,040
I built some good idea. Now I have to make a start up into the whole thing, and this can take

713
00:59:15,040 --> 00:59:20,800
like so long for my work to get out there and actually influence the world. So those are all

714
00:59:20,800 --> 00:59:26,800
good things. You know, there are certain pluses and minuses to doing things in large corporations

715
00:59:26,800 --> 00:59:31,360
to smaller ones, right? A small start up, we can raise capital, you know, we're all in the same room

716
00:59:31,360 --> 00:59:34,400
or, you know, place that we know what we're doing, we're all in charge of scale, we don't have to

717
00:59:34,400 --> 00:59:39,120
have all this overhead big things. Large corporation takes some time to turn the ship, but once you

718
00:59:39,120 --> 00:59:44,560
can focus that whole thing on something, man, you can really drive, drive on it. So different skills,

719
00:59:45,280 --> 00:59:49,680
you know, certainly the largest corporation sort of thing I've sort of worked in. And but

720
00:59:50,800 --> 00:59:55,200
the organization has to want to do it. I think is, is the less than I've learned. Like, if they're not

721
00:59:55,200 --> 00:59:59,200
really into it, or the senior leadership aren't really into it, or a large fraction, it's not

722
00:59:59,200 --> 01:00:04,480
a core strategy. It's a very difficult thing, right? And different companies are in different stages,

723
01:00:04,480 --> 01:00:08,400
right? Some of them are externalizing it, but we decided to build a really large in-house team.

724
01:00:08,400 --> 01:00:13,120
What are the, what are the couple three top three things that keep you up at night? Like, what are

725
01:00:13,120 --> 01:00:22,400
you most worried about in your role? The first thing is being able to generate the data at the

726
01:00:22,400 --> 01:00:28,640
right kind of cadence, right? So one of the great, you know, if you're in different domains, right,

727
01:00:28,640 --> 01:00:33,600
you can, depending what you're doing, you can get, you know, high frequency lots of new data generated

728
01:00:33,600 --> 01:00:38,240
quickly. For what we have to do, you know, for example, if you think of like reinforcement learning,

729
01:00:38,240 --> 01:00:42,320
right? You know, I've got a simulator of the game or things like that. I get many, many samples

730
01:00:42,320 --> 01:00:47,120
that can run lots of experiments, right? I'm in BioLand, right? Like, I don't get, you know, 12

731
01:00:47,120 --> 01:00:53,200
million data points, I get like 300 data points every four to six weeks, and they cost. And by

732
01:00:53,200 --> 01:00:57,280
the way, it costs us a lot of money to generate the data points. So then, you know, you start to ask

733
01:00:57,280 --> 01:01:02,320
this question of like, you know, what's my information gain? What's my model performance gain per data

734
01:01:02,320 --> 01:01:06,160
point for time? Where am I? Am I linear? Am I a plateauing? Like, you know, how many cycles do I need

735
01:01:06,160 --> 01:01:11,920
to run? I can run 12 cycles a year. I can get so many data points. Is that enough? Right? So it's

736
01:01:11,920 --> 01:01:17,520
all about, for me, a lot of it is about a, can I'm, you know, am I ever going to be in a generator

737
01:01:17,520 --> 01:01:22,720
enough data to solve this problem? We're going to have the right data. And another thing is the

738
01:01:22,720 --> 01:01:26,960
cost of the experiment. But, you know, people will ask like, well, how much data do you need to build

739
01:01:26,960 --> 01:01:32,080
this model? I'm like, I don't really know yet. You know, I need more, but then we'll start to see

740
01:01:32,080 --> 01:01:38,000
a trend. But, you know, or am I collecting the right data? So it's really about those learning

741
01:01:38,000 --> 01:01:43,600
cycles and those sorts of aspects. I'm, I'm really, the other thing that sort of keeps me up

742
01:01:43,600 --> 01:01:54,240
enough is thinking about the best ways and the ways that we have other data sources and things

743
01:01:54,240 --> 01:02:01,040
that we generate data, there's lots of historical data and GSK that we pull it together and use it

744
01:02:01,040 --> 01:02:07,760
in the right fashion, right? And, you know, that we remove, it's really important to, you know, there's

745
01:02:07,760 --> 01:02:12,240
a patient data about individual people, right? But when we run a trial and we do things with people,

746
01:02:12,240 --> 01:02:15,520
they're contributing to medical research. And if you talk to a lot of people with the trials,

747
01:02:15,520 --> 01:02:19,120
they're like, you say, well, we ran your trial and we put it into a box and we, the medicine

748
01:02:19,120 --> 01:02:23,600
was successful or not, right? And like no one else can touch that data, right? I'm like, that's

749
01:02:23,600 --> 01:02:27,280
criminal, right? They contributed medicine. There's other things we can learn about it. So what I

750
01:02:27,280 --> 01:02:31,360
concern about is when we generate data and we're doing things as an organization, how do we make

751
01:02:31,360 --> 01:02:35,440
stackable data sources to build this like a long-weigh children compass, right? An individual medicine

752
01:02:35,440 --> 01:02:39,280
for a particular, for say, roomside arthritis, it may fail, right? Which is terrible for patients

753
01:02:39,280 --> 01:02:43,120
and terrible for us. I drug didn't work. We didn't work as well as we hoped. But the question is

754
01:02:43,120 --> 01:02:48,080
what do we learn from it? And how do we build data sets that have the same common longitudinal

755
01:02:48,080 --> 01:02:52,000
characteristics, you know, maybe a common course, I can join them up together and I can build this

756
01:02:52,000 --> 01:02:57,200
longitudinal corpus of data. And so a lot of time when people are doing an experimental lab,

757
01:02:58,000 --> 01:03:02,320
they do an experiment and then, you know, they'll analyze that for that particular use case

758
01:03:02,320 --> 01:03:06,480
and they lose the metadata or it's lost and things like that. So I have to tell people,

759
01:03:06,480 --> 01:03:10,560
like, you know, you know, build data for future use so you can use it again and also like,

760
01:03:11,360 --> 01:03:14,880
you know, collect those other data points at additional marginal cost, right? Like they're

761
01:03:14,880 --> 01:03:23,200
really useful. So those are the things that really keep me up. You know, you know, the final

762
01:03:23,200 --> 01:03:27,360
thing is really about like, from building models in pathology, we're starting to do things that are

763
01:03:27,360 --> 01:03:31,760
really like, you know, doing patient prognosis, doing prediction, doing sorts of things, saying

764
01:03:31,760 --> 01:03:35,520
these persons like their benefit from this medicine or not, as we started going to learn that,

765
01:03:35,520 --> 01:03:42,480
it's like, we have to be right. And the other thing is we also, we make medicines for everybody.

766
01:03:42,480 --> 01:03:49,920
The challenge we have is if we're using lots of prior information or I'm relying on a system

767
01:03:49,920 --> 01:03:54,720
where, you know, I can get digitized medical records or pathology can be uploaded, we can raise

768
01:03:54,720 --> 01:03:58,640
a risk of building really great AAR advances, but only work for people that are in, you know,

769
01:03:59,280 --> 01:04:03,040
countries that have the data infrastructure and things like that. So we think a lot about, you know,

770
01:04:03,040 --> 01:04:07,760
what are the data, what's the data culture? We have culture in the vaccines and medicine.

771
01:04:07,760 --> 01:04:10,640
What's the equivalent of that? You know, we don't want to build these great advances and say,

772
01:04:10,640 --> 01:04:14,880
oh, I'm sorry, you know, that's, you know, it's going to take five to 10 years for another country

773
01:04:14,880 --> 01:04:21,360
to be able to have access to it, right? Those are my top three things. Well, Kim, thanks so much

774
01:04:21,360 --> 01:04:28,160
for joining us and taking the time to share a bit about what you're working on and how you think

775
01:04:28,160 --> 01:04:33,120
about the problems in your space. Thanks, Sam. It's been a lot of fun. I'm a big fan of the

776
01:04:33,120 --> 01:04:43,600
cobs. Cheers. Thank you so much. Thanks so much.

