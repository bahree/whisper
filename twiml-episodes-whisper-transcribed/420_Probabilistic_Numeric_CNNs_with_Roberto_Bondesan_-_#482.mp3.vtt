WEBVTT

00:00.000 --> 00:17.440
All right, everyone. I am here with Roberto Bandizan. Roberto is an AI researcher at Qualcomm.

00:17.440 --> 00:20.080
Roberto, welcome to the Twomo AI podcast.

00:20.080 --> 00:26.960
Hi, Sam. Thank you. I'm really looking forward to our chat. We'll be talking about your paper

00:26.960 --> 00:35.040
at ICLR, probabilistic numerical, numeric CNNs. But before we dig into that, I'd love to hear

00:35.040 --> 00:41.600
you share a little bit about your background and how you came to work in AI. Sure. So my background

00:41.600 --> 00:49.440
is in physics, and that's how I got into AI by basically applying deep learning to some physics

00:49.440 --> 00:56.800
problems. So before joining Qualcomm, I was working on characterizing new phases of matter and

00:56.800 --> 01:03.920
their potential for quantum computation. So phase of matter is basically one of the states

01:05.280 --> 01:10.800
you can find matter or materials around you. And one classical example is provided by the

01:10.800 --> 01:17.360
Easy Model. So in the Easy Model, you have some binary units, some bits which can be either

01:17.360 --> 01:25.520
up or down. And you can find this system into phases. One order phase, which is at a low

01:25.520 --> 01:31.360
temperature, where all these units, we call them spins, point in the same direction, and a high

01:31.360 --> 01:37.680
temperature disorder phase, where these units point in random directions. It turns out that

01:37.680 --> 01:43.200
characterizing phases of matter is a very challenging problem. And so researchers in the field

01:43.200 --> 01:51.600
they started to use AI for that. So that's how I got into AI. And as I was, you know, just learning

01:51.600 --> 01:58.080
about the techniques that were being developed, if this was around 2017-2018, for these problems,

01:58.080 --> 02:03.360
I got very interested in AI per se, and then I decided a career shift and to move

02:03.360 --> 02:10.800
in there to do research in AI. At this point, it was also when the paper by Taco Cohen and Max

02:10.800 --> 02:16.720
Welding were my colleagues at Qualcomm. Their paper on spherical CNNs came out. And so at that

02:16.720 --> 02:23.760
point, I understood that, you know, studying neural networks using insights from physics was a very

02:23.760 --> 02:30.800
useful and interesting thing. And that's what I wanted to work on. So did you get, were you familiar

02:30.800 --> 02:37.120
with the spherical CNN paper before you came to work with Taco and Max? Yes, yes, definitely.

02:37.120 --> 02:43.840
That's indeed one of the links that brought me to Qualcomm, yeah. That's fantastic. So tell us

02:43.840 --> 02:52.320
a little bit about your research interests. Yeah, so after I joined Qualcomm, I got involved

02:52.320 --> 03:00.080
into quite a few exciting projects. So one of them has to deal with the neural data compression.

03:00.080 --> 03:09.040
So here, the problem is that you want to compress data to, you know, send them over to some receiver.

03:09.040 --> 03:15.120
And you can actually use neural networks in particular, some forms of auto encoder to do that.

03:15.120 --> 03:19.920
So that's a quite, you know, interesting problem from the theoretical point of view. It involves

03:19.920 --> 03:24.320
generative modeling and things like that. But it is also super impactful because, you know,

03:24.320 --> 03:30.960
it can immediately translate into different ways to compress data. So that's a very interesting

03:30.960 --> 03:39.040
direction. Otherwise, I'm also very excited about Quantum AI. So after joining Qualcomm, I also

03:39.040 --> 03:45.040
had the chance to develop some ideas around quantum deep learning. So this is the, you know,

03:45.040 --> 03:50.400
intersection between quantum computation and the NDI. And so here is where also my background

03:50.400 --> 03:56.720
in quantum thesis was quite useful to, you know, get started quickly on this field, which is

03:56.720 --> 04:01.440
a rapidly developing field, which I think is one of the most interesting, you know, directions,

04:01.440 --> 04:10.480
which can disrupt AI in the future. And finally, more recently, I got interested in also applying

04:10.480 --> 04:15.680
machine learning to combinatorial optimization problems. So, you know, in industry, we have a lot

04:15.680 --> 04:21.840
of combinatorial optimization problems to solve. And so the potential of using machine learning

04:21.840 --> 04:26.800
to, you know, improve on the classical techniques is very interesting. And again,

04:26.800 --> 04:32.800
this is a very interesting, you know, area between, you know, very theoretical mathematical work,

04:32.800 --> 04:39.040
which, you know, is interesting to me and also very impactful work. So, yeah, in summary,

04:39.040 --> 04:47.760
I had the chance, you know, of both doing impactful work for society, you say at large,

04:47.760 --> 04:56.160
but also, you know, being able to do some long-term research like quantum AI. So I'm very excited

04:56.160 --> 05:02.560
about, you know, 15s that are going on at Qualcomm. I know. Nice. And when you say combinatorial

05:02.560 --> 05:07.200
optimization, these are problems like traveling salesmen and map coloring and that kind of thing,

05:07.200 --> 05:13.120
like classical combinatorial problems. Yeah, precisely. So, in fact, you know, in the last few

05:13.120 --> 05:22.240
years, maybe three, four years, there's been quite, you know, good progress on using deep learning

05:22.240 --> 05:29.280
for these problems. So, you know, traveling salesmen indeed is the standard paradigm of combinatorial

05:29.280 --> 05:33.840
optimization problem, right? When you have a salesman, we need to find the optimal tool to visit

05:33.840 --> 05:39.120
the series of cities. And so it has, of course, also direct applications in industry, for example,

05:39.120 --> 05:44.560
vehicle routing, right? Where a company has to deliver stuff and so we need to find the optimal

05:44.560 --> 05:51.040
route. And so, yeah, traveling salesmen is certainly a great example. And it also historic,

05:51.040 --> 05:55.680
I just driven most of the research combinatorial optimization and it is also in the intersection

05:55.680 --> 06:02.640
with machine learning. But also, I'm thinking about other problems like cheap design or, you know,

06:02.640 --> 06:08.000
some problems, combinatorial problems in wireless. So, these are also problems that are, you know,

06:08.000 --> 06:14.160
hard. So, in fact, there are some empty complete, you know, problems there. And, you know,

06:14.160 --> 06:17.200
these are also very interesting application areas for us.

06:18.400 --> 06:24.080
So, and cheap design, that would be things like routing traces on a circuit board or on a

06:24.080 --> 06:28.800
chip. Yeah, so there are indeed a few stages of cheap designs and all of them, in fact,

06:28.800 --> 06:34.480
involve different combinatorial optimization problems. And, yeah, so as you say, routing,

06:34.480 --> 06:40.880
so finding the optimal connections between the different logical gates for memories on a

06:40.880 --> 06:47.440
cheap canvas. And also, you can think about indeed also placing these components on a cheap

06:47.440 --> 06:52.960
in the optimal way to minimize area and stuff like that. Indeed, this is all very interesting

06:52.960 --> 06:56.960
combinatorial problems where, you know, I could disrupt.

06:56.960 --> 07:03.120
And on the wireless side, I'm imagining that's like frequency allocations and things like that,

07:03.120 --> 07:07.840
or what are some of the applications there? So, I have in mind, you know, things like

07:08.640 --> 07:16.320
some coding problems where, you know, you basically have sent some signal over and then you want to,

07:16.960 --> 07:24.880
you know, retrieve what was the bit string that was sent. But, you know, your signal has been

07:24.880 --> 07:30.080
corrupted by a noisy channel and you want to retrieve that, that these strings, that's one

07:30.080 --> 07:36.160
example of things that one could do or some other error correcting problems and things like that,

07:36.160 --> 07:43.040
yeah. So, there's a bit of a relationship between the combinatorics and kind of information

07:43.040 --> 07:48.240
theoretical types of problems and oppression, which is where you spend a bulk of your time.

07:48.240 --> 07:53.840
Yeah, absolutely. It is also fair to say indeed that all of these problems, you know,

07:53.840 --> 08:01.600
compression, also other problems that we work on, like quantization. These are all combinatorial

08:01.600 --> 08:07.520
nature. So, these are certainly possible use cases for this nesting field of ML for combinatorial

08:07.520 --> 08:14.720
optimization. Yeah. Nice, nice. It's an important area. Yeah. So, you're probabilistic numeric CNN's

08:14.720 --> 08:23.680
paper. What's the problem area that you're addressing there? Sure. So, there, the problem area

08:23.680 --> 08:33.680
is the application of deep learning to, you know, signals that are not necessarily

08:34.560 --> 08:41.440
sampled on a grid. Think about, you know, in many applications of deep learning, you, for

08:41.440 --> 08:48.400
example, want to model some time series and these time series, you know, have not been,

08:48.400 --> 08:55.760
you know, sampled uniformly. So, that's one possible, you know, use case for the models we

08:55.760 --> 09:02.320
want to develop in probabilistic numeric CNN's. But more generally, the motivation there

09:02.320 --> 09:10.080
is really, you know, trying to think about the signals that we want to model, you know,

09:10.080 --> 09:14.480
in machine learning, in their continuous formulation, not in their discrete formulation,

09:14.480 --> 09:21.280
which is, you know, the natural formulation you observe, you know, when you measure something.

09:21.280 --> 09:29.120
And so, basically, just to unpack a little bit of the title, right? So, the title starts

09:29.120 --> 09:34.960
with probabilistic numeric. So, the inspiration for this work is really this field of probabilistic

09:34.960 --> 09:39.680
numeric. So, you certainly have a media with a convolution on your networks, so I do not need to

09:39.680 --> 09:44.960
introduce that. But so, let me just spend a couple of words on probabilistic numerics, which

09:44.960 --> 09:52.000
might not be familiar to everyone. So, probabilistic numeric is a recent field in statistics,

09:52.000 --> 09:57.840
which tries to, I mean, not necessarily recently, but recently, I think there's been a

09:57.840 --> 10:06.800
quite big developments. And so, it tries to quantify the uncertainty that a numerical program

10:06.800 --> 10:14.160
has, due to the discreteness of the sampling procedure of its input. To make this concrete,

10:14.160 --> 10:19.760
let's think about the problem of computing a numerical integral. You know, the function that you

10:19.760 --> 10:25.120
want to integrate analytically, you can have evaluated everywhere on a continuous range in its

10:25.120 --> 10:31.760
domain, but necessarily, you need to sample that function on a discrete sample of points,

10:31.760 --> 10:38.240
because your memory and time to compute that integral is finite. And so, probabilistic numeric

10:38.240 --> 10:44.480
tells you a way to derive uncertainty from this sampling procedure. And the way it is done

10:44.480 --> 10:51.840
is via Bayesian inference. So, in Bayesian inference, one has an agent, a machine learning model,

10:51.840 --> 10:59.520
that has a prior. Here, the prior will be over the set of possible functions that you want to

10:59.520 --> 11:06.240
integrate. And in technical terms, it is a Gaussian process. So, a Gaussian prior on this set of

11:06.240 --> 11:11.680
functions. And then, upon measuring that function, you want to integrate on some points in your

11:11.680 --> 11:18.640
domain, you update your prior to a posterior. And this allows you to immediately translate to some

11:18.640 --> 11:24.960
uncertainty on the result of your integral. So, now, your numerical program will not just return

11:24.960 --> 11:30.080
a number, which will return a probability distribution. And this probability distribution will be

11:30.080 --> 11:35.520
picked around some value, and there will be an uncertainty. So, that's your discretization error.

11:36.400 --> 11:40.640
So, we thought that this is quite interesting also for machine learning. And so, we,

11:41.360 --> 11:47.200
indeed, start from the same philosophical standpoint, you know, the images that we want to model

11:47.200 --> 11:52.000
in machine learning, the time series that we want to model, are in fact, continuous signal.

11:52.000 --> 11:57.920
And so, we necessarily need to discretize them because we want to put them in a computer.

11:57.920 --> 12:03.120
And, but, you know, this procedure will come with some uncertainty with some errors. And so,

12:03.120 --> 12:09.440
it is important to quantify those. And so, that's basically the motivation behind this work.

12:09.440 --> 12:22.160
If I could replay that to make sure I'm understanding with classical numerical programs, like I'm thinking

12:22.160 --> 12:28.400
back to Fortran, numerical computing and undergrad, like you've got some function, and you want to

12:28.400 --> 12:33.760
compute an integral for it. And you just, you do that. There are established outcomes for doing that.

12:33.760 --> 12:40.560
What probabilistic layers on top of that is allowing you to look at your, the function that you're

12:40.560 --> 12:47.920
integrating, not as a single function, but as a distribution of functions. And what you have then,

12:49.520 --> 12:57.120
you know, then your kind of classic quantization error now becomes a probabilistic quantization error.

12:57.120 --> 13:03.920
Yeah, that's a good summary. Thanks. And maybe just to clarify, so here we are really looking at the

13:03.920 --> 13:10.080
quantization, right, in the domain of the function. So, the value that the function takes

13:10.800 --> 13:17.120
are still continuous. And so, that's the kind of quantization we are looking at. And so, indeed,

13:17.120 --> 13:27.680
in our probabilistic numeric CNN, so we start from this idea. And then we develop on top in your

13:27.680 --> 13:34.400
network. I also want to interrupt to say that when you say the quantization is in the domain of

13:34.400 --> 13:40.880
the function, meaning as opposed to the range, which is your think about your vertical amplitude,

13:40.880 --> 13:45.600
here we're talking about you're taking different points in time that may or not be, may or may

13:45.600 --> 13:51.280
not be, well, are not uniform. And so, that's where your quantization is coming in. So, you've got a

13:51.280 --> 13:57.440
time series, but you're not getting data in every second or millisecond or whatever it comes

13:58.880 --> 14:05.120
irregularly. And you're trying to figure out quantization error based on that irregularity.

14:05.840 --> 14:12.800
That's precisely it. Yeah. Okay. Yeah. All right. Cool. So, and indeed, maybe just to give you a little

14:12.800 --> 14:24.320
more about the paper, so we develop the idea of using probabilistic numeric for deep learning.

14:24.320 --> 14:31.600
So, the first step in this procedure is that we start indeed from a regular sample time series,

14:31.600 --> 14:38.240
for example, or even from an image, which has been sub-sampled in a regular way. And what we do

14:38.240 --> 14:43.680
is that we interpolate that. So, we interpolate that in a probabilistic way. So, like it, is that

14:44.240 --> 14:52.480
like a probabilistic numerical programs do. And that gives us my posterior distribution over

14:52.480 --> 14:59.280
our input. And what we do then on this posterior distribution is that we apply a neural net. So,

14:59.280 --> 15:04.560
now this posterior distribution is a distribution of a continuous functions. And so, the technical

15:04.560 --> 15:11.280
contribution that we make in this paper is to devise a neural network on continuous functions.

15:11.280 --> 15:18.640
So, typically, your CNN, we lack on vectors, right? There's some array of numbers.

15:18.640 --> 15:24.560
Here, our probabilistic numeric CNN is defined directly in the continuum. And that turns out

15:24.560 --> 15:30.800
to be quite powerful. And also unlocks, you know, new models and new mechanisms for learning.

15:30.800 --> 15:38.080
Which have, what does that exactly mean? I think, yes, I'm so used to thinking about the input

15:38.080 --> 15:44.160
to a CNN being a vector. I'm not even sure how to unpack it being continuous.

15:44.160 --> 15:50.560
Right. So, indeed, you're not going to store that function in your computer, because by definition,

15:50.560 --> 15:59.200
indeed, you're going to need to, you know, have, you know, an infinite number of points if you

15:59.200 --> 16:04.880
want to store all the values. What you're going to store is just some function of form,

16:04.880 --> 16:10.080
some code that allows you to evaluate that function, right? And so, that's somehow the input to,

16:10.080 --> 16:16.720
to your, you know, neural network. And so, to be precise, indeed, about what happens,

16:16.720 --> 16:22.880
we still have a neural network which works by interliving linear, nonlinear layers. But now,

16:22.880 --> 16:27.760
and so the nonlinear layer, you can actually morally understand that it's going to be very

16:27.760 --> 16:32.800
similar to what you're used to do at each point of your function applied nonlinearity. But now,

16:32.800 --> 16:40.320
the real, you know, new part of the work is about the linear layer. So, we devise actually a

16:41.520 --> 16:46.560
new convolutional layer, which is defined in terms of a linear PD, the partial differential

16:46.560 --> 16:52.720
equation. So, this partial differential equation is a linear operation on an input function,

16:52.720 --> 17:00.480
which is the input sum out to the PD, namely the value that you have the initial condition

17:00.480 --> 17:09.200
to your differential equation. So, what happens is that, you know, if you want to impose actually

17:09.200 --> 17:14.960
translation equivalence that you have, you know, in convolutional layer, this restricts the forms

17:14.960 --> 17:20.400
of differential equations that you can input in your neural network. And interestingly,

17:20.400 --> 17:27.040
you know, one of the simplest things you can do is to use the kind of generalized diffusion

17:27.040 --> 17:32.880
equation. So, diffusion, you know, is a process from physics, which you can understand, you know,

17:32.880 --> 17:37.600
for example, when you have a glass of water, you put some dye into it and this dye diffuses

17:38.240 --> 17:44.320
over time. And so, similarly here, you know, we have our image, which is encoded, you know,

17:44.320 --> 17:50.640
in some function. And that function gets blurred, similar to the diffusion process over time.

17:50.640 --> 17:57.280
So, that's really what we mean, you know, by the layer on continuous functions. So,

17:57.280 --> 18:03.600
it is defined formally. And it turns out that for certain choices, indeed, of layers,

18:03.600 --> 18:09.600
we can do computations analytically. So, we can actually propagate these, you know,

18:09.600 --> 18:15.920
functional forms in our code analytically. And so, that's a pretty cool.

18:17.840 --> 18:25.040
And so, we can, you know, ultimately, we can devise a practical procedure to, you know,

18:25.040 --> 18:32.320
start from our input signal, which was, you know, this sub-sampled signal, then interpolate it,

18:32.320 --> 18:38.400
then we apply this convolutional layer as PDEs. We interleave with some non-linearities.

18:38.400 --> 18:43.520
And what we get out, after some of these layers, and perhaps the pooling and so on,

18:44.400 --> 18:49.200
we get out, you know, a prediction. Like, you know, we want to classify this time serious,

18:49.200 --> 18:55.760
for example, this input image. And so, we want to get out a class label, right? As we do usually,

18:55.760 --> 19:00.800
but on top of that, we also get out an uncertainty. And actually, this uncertainty is also there

19:00.800 --> 19:06.400
at T-EV intermediate layer. And it's really an uncertainty that is related to the, you know,

19:06.400 --> 19:12.880
entity, the input signal didn't have maybe information in certain regions of space or time.

19:14.000 --> 19:18.720
So, in this way, we know, you know, we can characterize indeed what is the error that we make.

19:18.720 --> 19:24.320
And more interesting, we can also choose where to sample the signal in order to reduce uncertainty.

19:25.440 --> 19:30.000
So, these are all the interesting applications that we can think about with this model.

19:30.000 --> 19:42.080
Is that, is that ladder point choosing where to sample? Is that a, like a byproduct of

19:42.960 --> 19:47.280
going through the process in the same direction, or is it more like going through the process

19:47.280 --> 19:52.160
backwards? I don't know if that question makes it. It's going to the process backwards, you're right,

19:52.160 --> 19:59.840
it's a small, you basically, you know, find a certain uncertainty and this uncertainty will be a

19:59.840 --> 20:04.320
function of where your value to your input. So, you can compute some kind of derivative of that

20:04.320 --> 20:09.760
to the spread to the inputs to minimize the uncertainty. And that's, you know, that can be

20:09.760 --> 20:15.600
useful, you know, when it is, for example, costly to get data points. You can optimize the points

20:15.600 --> 20:20.560
that are most informative. Or, you know, when you have maybe some data on meshes and things like that,

20:20.560 --> 20:24.880
you know, where discretization errors are important. So, there are a lot of interesting, you know,

20:24.880 --> 20:33.760
use cases. So, in this paper, actually, we focus mostly on a benchmarking this model on a couple

20:33.760 --> 20:42.640
of data sets. So, one is the super pixel classification of images. So, super pixel, you know, is just

20:42.640 --> 20:50.560
an image, which is sub-sampled, but again, the points are not on a grid. Before we get to the

20:50.560 --> 20:58.400
benchmark, another question about the architecture here. So, you, one of the key innovations or

20:58.400 --> 21:06.080
contributions, it sounds like it's this PDE layer. Yes. And PDE's arise in physics all the time,

21:06.080 --> 21:12.720
like you can, I'm imagining the inspiration of that was thinking about the problem, like the

21:12.720 --> 21:17.280
closed form problem and how you might solve it. And then, you know, that involves PDE's.

21:19.920 --> 21:26.560
Yes. So, PDE, yeah, good. Sorry. No, no, I was, I was going to, you know, but then you get to that

21:26.560 --> 21:32.400
so that your PDE, you have this PDE layer that you think needs to be involved in here, but it,

21:32.400 --> 21:39.120
you have the constraints of translation, invariance from CNNs, and then suddenly you're like,

21:39.120 --> 21:44.880
okay, diffusion is the answer. And like, where did that come from? Was that? Did you,

21:46.400 --> 21:55.120
did you recognize diffusion as like a translational independence by thinking about a glass of water,

21:55.120 --> 22:02.240
or is that like a known physics thing? Yeah. So, yeah. So, the, the way we got there, and actually,

22:03.360 --> 22:09.360
I would like at this point to amend the one of the big omissions that I've done in the beginning,

22:09.360 --> 22:14.640
which is not to acknowledge that the first author of this paper is Mark Fincy, who was doing

22:14.640 --> 22:20.400
an internship with us last summer. So, he's really the main driver driving force in this project.

22:20.400 --> 22:27.600
And so, you know, Mark came up with this proposal, and I guess it was a bit of a mixture of two things.

22:27.600 --> 22:32.160
One thing was intuition, and the other thing may become from physical reasoning, and the other

22:32.160 --> 22:37.200
thing was just a mathematical formalism. So, we wrote down, you know, the most general,

22:37.840 --> 22:45.360
basically, local, you know, linear layer in the former PDE, and then, you know, basically,

22:45.360 --> 22:51.280
this turned out to be diffusion when you imposed translation in variance.

22:51.280 --> 22:57.280
And actually, we also, you know, did something a bit more general. So, we also consider the,

22:57.280 --> 23:01.520
you know, symmetries like rotation and things like that. So, we thought a little bit about

23:01.520 --> 23:07.360
spherical CNN at the beginning. So, there is, you know, interest in the community in characterizing

23:07.360 --> 23:11.840
equipment against under more general symmetries. And so, it turns out that, you know,

23:11.840 --> 23:16.960
beautifully, also, in this context, we can get, you know, PDE's, which are, you know,

23:16.960 --> 23:22.160
equivalent under more general transformations, like rotations at things like that.

23:22.160 --> 23:28.160
And that's actually quite interesting, I believe, because, you know, one of the problems with,

23:28.160 --> 23:34.640
you know, getting to work, also, the equivalent under rotations, say, is that you necessarily need

23:34.640 --> 23:41.840
to discretize things on a lattice. And so, at that point, you know, the rotation for a certain angle

23:41.840 --> 23:47.360
becomes, you know, pretty tricky to get it to work well and necessarily, you know, you will have

23:47.360 --> 23:53.360
some error, which is due to the, basically, mesh of your lattice and so on. In this context,

23:53.360 --> 23:58.560
we avoided this problem. So, our model is defining the continuum, and you know, it is basically

23:58.560 --> 24:05.280
equivariant under arbitrary rotations. So, that's a pretty cool, I think, feature,

24:06.000 --> 24:11.440
and also equivalent on the arbitrary translation. So, that's, I think, a pretty cool feature too.

24:12.000 --> 24:16.880
Right. Yeah. I'll just interject really quickly that the, this whole idea of

24:16.880 --> 24:23.760
equivalence and spherical CNNs and gauge equivalence is a big focus of the AI research team

24:23.760 --> 24:30.400
there at Qualcomm. And for folks that want to dig in more, that's probably the best place to

24:30.400 --> 24:35.200
start as the first interview I did with Max Welling on gauge equivalence CNNs, or we talk about

24:35.200 --> 24:40.960
a lot of this, what equivalence is and why it's important. And we'll drop a link to that in the show

24:40.960 --> 24:49.680
notes. Yeah, thanks. I also listened to that. It was a great episode. Yeah. Awesome. Awesome.

24:49.680 --> 24:56.560
So, you were talking about benchmarking? Yeah, indeed. So, I was talking about the fact

24:56.560 --> 25:03.440
that we benchmarked on a couple of data sets. So, the first one was this super pixel images. So,

25:03.440 --> 25:09.920
you start from an image. And, you know, suppose it is defined on a grid and then you sub-sample it.

25:09.920 --> 25:14.720
So, you take away some of these points. It's such a way that then it becomes, you know,

25:14.720 --> 25:20.640
the grid structure is lost. And, you know, at this point, your regular CNN will not work well

25:20.640 --> 25:27.200
for this data type. There are a few other competitors out there, but it turns out that our model

25:27.200 --> 25:34.480
basically established a new state of the art for this task. So, three times reduction in the

25:34.480 --> 25:43.600
test error. And so, this was quite encouraging. And we also applied, you know, the model to

25:43.600 --> 25:49.200
medical time series. So, in this case, you know, you can think about, you know, patients

25:49.200 --> 25:56.960
goes to the hospital. And then, you know, for example, the doctor measures, you know, blood pressure,

25:56.960 --> 26:01.760
things like that. And this is done at the regular times, right? So, this is also a good, you know,

26:01.760 --> 26:06.160
case of a regular time series. And then, based on these measurements, you want to predict,

26:06.160 --> 26:12.160
you know, if the patient will recover things like that. And in fact, so, this is our,

26:12.160 --> 26:18.400
these are pretty important data sets to look at. And so, we also applied our model to these

26:18.400 --> 26:28.480
data sets and show competitive results there too. Yeah, so, we basically, you know, think that

26:28.480 --> 26:37.520
this point of view is very powerful. And, yeah, in fact, we have a few, you know, future directions

26:37.520 --> 26:47.040
in mind that came after this, this work. And what are those? Right. So, one of the interesting

26:47.040 --> 26:53.840
directions for this work, in my opinion, is to connect it to quantum computation. In fact,

26:54.960 --> 27:01.120
one of the promising platforms for quantum computation is a so-called quantum optical computer.

27:01.120 --> 27:09.600
So, here, optical means that you use light as the, you know, basically unit of information that

27:09.600 --> 27:16.320
you want to manipulate. And it turns out that, you know, there are states of light that, you know,

27:16.320 --> 27:24.400
people know how to build in a lab, which are closely related to Gaussian processes.

27:24.960 --> 27:29.440
So, there is a beautiful connection here between states of light and Gaussian processes.

27:29.440 --> 27:37.600
And they immediately disperse a connection also between, you know, our model and a possible

27:37.600 --> 27:46.960
generalization to a quantum model, so a quantum neural network. And so, I mean, started about this

27:46.960 --> 27:54.640
because, you know, it seems to me a very natural, basically, way to encode the data via this

27:54.640 --> 28:00.880
relationship between Gaussian processes and certain states, Gaussian states of light. So,

28:00.880 --> 28:06.080
that's, I think, a very natural way to encode data in a quantum computer, which operates on

28:06.640 --> 28:13.280
with optical elements. And therefore, you know, there is also a nice way to interpret, basically,

28:13.280 --> 28:18.080
the probabilistic numeric, new and network from this point of view of quantum information,

28:18.080 --> 28:25.520
quantum optics, quantum mechanics. So, there is a big, basically, direction here that I'm

28:25.520 --> 28:30.560
excited about, which spurred out of this, of this paper, actually, and this new way to think

28:30.560 --> 28:34.240
about inputs to neural networks. Think about neural networks.

28:35.520 --> 28:41.200
Is the primary connection there thinking about continuous functions, or is there also this

28:41.200 --> 28:49.760
aspect of missing or irregularly sample data? Yeah, I would say both. So, the fact that indeed,

28:49.760 --> 28:58.720
you have continuous function relates to what, you know, physicists call, basically, kind of model

28:58.720 --> 29:03.920
that physicists use, which is called quantum field theory. So, the quantum field theory is also

29:03.920 --> 29:09.360
a continuous field. And so, your continuous field classically, right, which is your function,

29:09.360 --> 29:13.440
which is continuous, becomes now a quantum function, so it becomes a quantum field.

29:14.080 --> 29:18.400
And this is quite exciting, I think, because this, you know, these quantum fields are relevant

29:18.400 --> 29:24.800
for describing, you know, elementary particles. So, this kind of experiments that you see,

29:24.800 --> 29:32.000
you know, in this, they collide as a lecture and so on. So, this, you know, continuous formulation

29:32.000 --> 29:38.080
allows you to make a very interesting connection between very different fields, which are described

29:38.080 --> 29:43.920
in a very similar formalism. And so, this is a very interesting thing. And also the, you know,

29:43.920 --> 29:50.560
sampling, the irregularly sample nature of the inputs is also, I think, naturally captured

29:50.560 --> 29:56.880
in terms of these Gaussian states of light that I was talking about. So, yes, I would say both,

29:57.440 --> 30:03.840
I, in my opinion, are very natural candidates, you know, that allow you to, I think, propose

30:03.840 --> 30:12.960
the interesting models for quantum neural networks. And what are some of the, you talked about kind

30:12.960 --> 30:22.960
of the, the, going back to the benchmarks, the sub sampled images and the, the healthcare time

30:22.960 --> 30:33.760
series, is there also a compression application for this paper as well? No, I would say that

30:33.760 --> 30:40.560
a compression was not our main focus, but I can see maybe where you're going. So, if you can

30:40.560 --> 30:46.240
condense, you know, your input in some mean and covariance of the Gaussian, that's maybe also

30:46.240 --> 30:51.520
way to think about if compressed it to a few numbers. So, that's an interesting spin that I didn't

30:51.520 --> 30:57.360
think about. So, it was not really our, our focus here, but yeah, it might be. Got it.

30:57.360 --> 31:05.680
Cool. So, again, this is a paper that you're presenting at ICLR. What else's Qualcomm

31:05.680 --> 31:13.600
have going on at the conference? Sure. So, another paper is from my colleagues,

31:14.560 --> 31:23.840
Tis, Farazindal, and Iris Halben and Taco Cohen. And so, this paper is about adaptive

31:23.840 --> 31:30.320
neural compression. So, here, the, so we thought a little bit before, you know, about what is the

31:30.320 --> 31:36.320
idea of neural compression? So, neural codex. And so, typically, there is a problem, which is the

31:36.320 --> 31:41.360
problem that, you know, you want to have a small neural network, because you want, you know, to,

31:41.360 --> 31:48.160
to have a low computational burden to do, to do this codec process. But at the same time, a low

31:48.160 --> 31:54.160
neural network, low complexity neural network, we will typically, you know, not generalize well.

31:56.160 --> 32:04.800
And so, the idea here of the authors in this paper that we present at ICLR was to do adaptive

32:04.800 --> 32:10.400
or fine-tuned compression. So, the idea is that, okay, you have trained your models on training

32:10.400 --> 32:15.280
data, but now you deploy it. But it turns out that, you know, the test data can be different

32:15.280 --> 32:20.720
from the training data. As I said, you can suffer from generalization problems. But you can,

32:20.720 --> 32:25.520
you know, imagine now that in the scenario where, for example, your sender, you know, is some,

32:26.480 --> 32:32.080
you know, at your sender side of the data, you can, you're willing to spend compute time.

32:32.080 --> 32:36.880
And, you know, you're sending this data to this compressed data to some low power devices,

32:36.880 --> 32:42.480
like mobile phones. And in this scenario, it makes sense, you know, that at your sender time,

32:42.480 --> 32:51.200
you can fine-tune your, your, basically, codec on the test data. And then, on top of sending,

32:51.200 --> 32:58.320
you know, the transmitted image or video to your mobile phone, you also send some bits that are

32:58.320 --> 33:05.120
related to the difference in the weights of your, you know, adaptive neural network codec.

33:05.840 --> 33:11.440
And so, the authors in this show that, you know, you can indeed reserve some bandwidth

33:11.440 --> 33:18.720
for this delta in the weights, on top of the bandwidth for the stream of the video that you want

33:18.720 --> 33:25.280
to send. And it turns out that if you do that, if you jointly optimize the model to do the best

33:25.280 --> 33:31.360
possible thing, so to minimize the rate, the number of bits if you transmit and optimize the

33:31.360 --> 33:36.400
reconstruction accuracy, you actually can do better if you do this procedure, you know,

33:36.400 --> 33:41.360
of sending over some of the bits for your daily weights, then if you're just in occupied

33:41.360 --> 33:45.520
the whole bandwidth for your, for your stream. So that's a pretty up-promising, I think,

33:45.520 --> 33:50.640
direction, which can have a few interesting, you know, applications, a direct application,

33:50.640 --> 33:58.960
in fact, also for PACOM. Now, I think this one was counterintuitive for me. I maybe misremembering

33:58.960 --> 34:05.440
the information theory, but I thought like Nyquist or Hammond or Heming or something like that

34:05.440 --> 34:11.600
said that it doesn't matter how you chop up your channel, you know, if you have a fixed bandwidth,

34:11.600 --> 34:18.000
there's some certain maximum throughput that you'll be able to get, but here you're kind of

34:18.000 --> 34:23.680
splitting your channel into kind of in-band and out-of-band or something like that and getting

34:23.680 --> 34:30.800
better results. Yeah, so here the idea is really that, you know, you want to basically send

34:30.800 --> 34:39.440
the certain number of bits, right? That's your somehow, the rate that you're willing to

34:40.880 --> 34:48.240
send. So that's somehow the amount of information that you would like to send. And at that

34:48.240 --> 34:54.400
point, you would like to do, you know, the best possible job given that constraint. So what is the,

34:54.400 --> 35:03.200
you know, choice of my codec, you know, encoder, right, to give me the best description of my input

35:03.200 --> 35:09.840
in such a way that when I decode it, I get the highest reconstruction quality. And so that's the

35:09.840 --> 35:17.040
setup. And so in this setup, you know, what we showed is that you can actually reserve some of

35:17.040 --> 35:25.680
these bits that you transmit for the, you know, weights. And so that was a new idea that, you know,

35:25.680 --> 35:31.920
people have not thought about. And yeah, in this setting, this is beneficial. But you're right,

35:31.920 --> 35:38.320
there are certain some terrific bounds to the, you know, rate distortion performance that you

35:38.320 --> 35:43.040
can achieve, but you know, we are certainly within these bounds. And yeah.

35:43.040 --> 35:49.280
Mm hmm. Okay. So yeah, we're talking about the different, the, the thing that I was thinking

35:49.280 --> 35:56.080
about it applies to the theoretical bounds, but I would not say that. So we have some ability to

35:56.080 --> 36:03.120
operate within that envelope. I would say closer to the boundary with this procedure than without.

36:03.760 --> 36:11.680
Got it. Got it. Cool. Any other papers? Qualcomm AR research papers at ICLR?

36:11.680 --> 36:20.480
Yeah. Yeah, certainly. There are a few other papers. I can highlight a paper by my colleague,

36:21.200 --> 36:29.600
Pim, the one who is a PhD student of Max. And he has a paper on mesh CNN. So here the idea

36:31.200 --> 36:40.240
is that, you know, you have tasks or meshes like, you know, shape segmentation or 3D shape

36:40.240 --> 36:46.240
reconstruction and things like that. And so you would like to use a graph neural network for

36:46.240 --> 36:51.920
these tasks, but graph neural networks suffer from the problem that they are, they are oblivious

36:51.920 --> 36:56.480
to geometry. So by definition of the graph structure, they do not have information about the

36:56.480 --> 37:02.960
geometry. So in particular, if you have, you know, a vertex with two edges connected to it,

37:02.960 --> 37:07.840
you know, it doesn't matter if you move these edges around, basically because the graph neural

37:07.840 --> 37:13.920
networks do not, convolution neural networks do not see the angle between these edges. And so the

37:13.920 --> 37:21.040
idea of this mesh CNN is to use, you know, gauge equivariance tools to build this geometry

37:21.040 --> 37:26.160
into graph neural networks. It turns out that if you do that, you get, you know, much better results

37:26.160 --> 37:42.320
for these tasks or meshes. So we have also other works on, you know, temporal localization

37:42.320 --> 37:58.000
of actions. And also we have a, we are organizing together with UC Irvine and Disney research.

37:58.000 --> 38:05.360
We are organizing a workshop at ICLR, a workshop on neural compression. So that's a certain

38:05.360 --> 38:12.240
and excited opportunity to, you know, get together with the experts in information theory, communication,

38:12.240 --> 38:18.480
and deep learning to indeed afford the advance of this effort of getting better codex using neural

38:18.480 --> 38:28.720
networks. Nice, nice. So you've talked, we spoke earlier on kind of where you saw the

38:28.720 --> 38:37.200
probabilistic numeric research going kind of more broadly when you think about your research area

38:37.200 --> 38:42.240
and the area of your team. What, what are you most excited about? Where do you see that going?

38:42.960 --> 38:48.720
Yeah. So indeed, we spoke earlier and I was hinting at quantum neural networks. So this is

38:48.720 --> 38:55.760
certainly something that I believe would be a drive for innovation in AI in the future. You know,

38:55.760 --> 39:02.080
recent years, last couple of years, I've seen a tremendous, you know, excitement in the quantum

39:02.080 --> 39:08.160
computing community, you know, with the supremacy experiments. So we are really at an era where

39:08.160 --> 39:14.080
we are starting, you know, to seriously think about, you know, these things. And so it's really

39:14.080 --> 39:19.360
timely, I think, to get serious about, you know, thinking about how can you use quantum

39:19.360 --> 39:25.840
information of quantum computation to enhance machine learning. So that's I think a very exciting place

39:25.840 --> 39:33.840
to be. It is true that, you know, it is still a recent deal and, you know, there is certainly a lot

39:33.840 --> 39:41.760
to do and it is still an open question to figure out what's the best way to use quantum computers

39:41.760 --> 39:47.280
for machine learning. So that's why I think it's a very exciting area. So I've been thinking

39:47.280 --> 39:53.760
a little bit about this over the last year. And so one of the things also, I've been thinking

39:53.760 --> 40:00.640
about was the problem, you know, of benchmarking these models. So we talked a little bit about

40:00.640 --> 40:06.240
this direction where you can use the quantum optical computers and the relation to, you know,

40:06.240 --> 40:11.200
probabilistic numerics, CNN and so on. But, you know, in general, the problem here is that we do

40:11.200 --> 40:16.880
not have these devices to run the quantum neural networks as scales that we would like right now,

40:16.880 --> 40:22.240
right? So what do we do? Certainly, we can indeed base on intuition and small experiments,

40:22.240 --> 40:28.880
figure out what are the most promising models. And I think that I worked on was to also try to

40:28.880 --> 40:34.960
find an interpolation between, you know, your classical neural net and a quantum neural net.

40:34.960 --> 40:42.400
So basically, we came up with this quantum deformed neural networks, which is, you know,

40:42.400 --> 40:51.120
is some work I did also with Maxwell. And so the idea here is that, you know, you can think

40:51.120 --> 40:57.280
about your classical neural network as embedded in a quantum computer. And in fact, the architecture

40:57.280 --> 41:02.480
we are thinking about now is the qubit architecture, which is an interesting relation, you know,

41:02.480 --> 41:07.200
to binary neural nets, because you know, a qubit is basically the, you know, quantum equivalent

41:07.200 --> 41:12.480
of a bit. And so there is a relation with quantization of neural net, so that you can also explore

41:12.480 --> 41:18.480
using quantum mechanics and so on. But the most interesting thing is that indeed we managed to

41:18.480 --> 41:23.600
map this binary neural net or probabilistic binary neural net. In fact, on a quantum computer,

41:23.600 --> 41:31.520
using qubits. And then we started to deform this model, so to introduce gates, which, you know,

41:31.520 --> 41:37.920
use pure quantum effects like entanglement and superposition. And so we do that in a way that,

41:37.920 --> 41:43.840
you know, we slow interpolate between a regime, which we can simulate classically, which is basically

41:43.840 --> 41:50.400
the classical neural net regime. And the regime, you know, which is basically intractable classically,

41:50.400 --> 41:56.480
which will require a quantum computer. And along the way, there is some, some, you know,

41:56.480 --> 42:01.520
intermediate regime where you can still do some classical simulations using some tools from

42:01.520 --> 42:07.200
quantum physics, which are called tensor networks. And basically, this allows you to start from a good,

42:07.200 --> 42:12.320
you know, prior somehow for your model, which is this classical network, the format by doing

42:12.880 --> 42:17.760
this, indeed, these modifications. And then you can, you know, use the classical simulations of

42:17.760 --> 42:23.920
the quantum model to learn that. So that you can implement as a differential program. And so we

42:23.920 --> 42:30.240
show actually some modest gains with respect to, you know, the classical neural net by introducing

42:30.240 --> 42:35.840
this quantum gates. And we can actually provide, you know, the first example where you can simulate

42:36.480 --> 42:41.920
quantum model at the scale of, you know, real world data. So that was interesting for us.

42:42.560 --> 42:46.720
But yeah, more generally, you know, there are many, indeed, the different directions at the moment

42:46.720 --> 42:51.600
are also related to optimization problems. So we talked about computer optimization, right?

42:51.600 --> 42:56.800
And machine learning and also, you know, quantum computing is also an exciting area for exploring

42:56.800 --> 43:02.560
new algorithm for combinator optimization. So yeah, to summarize, indeed, the quantum AI, I think,

43:02.560 --> 43:08.240
is a very interesting direction. And also, I also think that the machine learning for combinator optimization

43:08.240 --> 43:14.800
is a very interesting direction. So this is also pretty recent. And I believe that here we will see,

43:14.800 --> 43:22.560
you know, large scale adoption of this technique, because it has been shown recently that, you know,

43:22.560 --> 43:29.760
you can actually enhance your classical solvers for, you know, integer linear programs or stuff

43:29.760 --> 43:35.040
like that, which, you know, people use routinely for solving their problems in the industry. You can

43:35.040 --> 43:42.480
actually, you know, augment with neural networks, these solvers in such a way that the decisions

43:42.480 --> 43:48.480
that these solvers make are better informed and basically are faster. And the idea here is that,

43:48.480 --> 43:54.080
you know, you can adapt, basically, your, your solver to the data distribution that you really

43:54.080 --> 44:00.240
care to solve, you know, a delivery company which will routinely solve, you know, the driving

44:00.240 --> 44:05.200
assessment problem in the same city, you know, if you not want to deal with the most arbitrary

44:05.200 --> 44:08.960
hard instances of a traveling assessment problem. And this is where machine learning, I think,

44:08.960 --> 44:14.000
we really put an edge. So it will allow you, you know, to tailor your optimization algorithm

44:14.000 --> 44:19.760
to the instances that you care about. And ultimately, in ultimately this, you know, leads to,

44:20.320 --> 44:26.240
you know, better performance for combinator optimization solvers. And, you know, and also

44:26.240 --> 44:31.680
potentially, you know, it allows you to discover new strategies, like using reinforcement learning

44:31.680 --> 44:38.800
as we have seen, you know, you know, alpha-go alpha-fold. So super excited, I think. Awesome. Awesome.

44:38.800 --> 44:44.880
Overbirdo, thanks so much for sharing a bit about what you're working on. And what some of the

44:44.880 --> 44:50.080
folks in your team are working on at ICLR. It's been really great chatting with you.

44:50.080 --> 45:01.040
Thank you, Sam. Pleasure for me, too. Thank you.

