All right, everyone, welcome to our AI Trends 2023 series.
Each year, we invite friends of the show
to join us to recap key developments of the year
and anticipate future advancement
in the most interesting subfields in AI.
And today, we're joined by Samir Singh.
Samir is an associate professor
in the Department of Computer Science at UC Irvine
and a fellow at the Allen Institute
for Artificial Intelligence for AI2.
To talk through some of the key research developments
in NLP.
Of course, before we get going,
take a moment to hit that subscribe button
wherever you're listening to today's show.
And you can also follow us on TikTok and Instagram
at Twimble AI for highlights from every episode.
All right, let's jump in, Samir.
Welcome back to the podcast and our Trends series.
Yeah, thank you for having me, Sam.
It's great to be back.
It's super excited to have you back.
We were joking a little bit before we got rolling
that we picked big years to have you on.
The last one was our 2020 right in the wake of GPT-3,
a big year, and of course,
this has been a huge year for NLP
with the relatively recent release of chat GPT.
Yeah, it's always kind of crazy
when you have these big changes happening in the year
where there is research still going on in parallel
and people are exploring research questions
and a lot of them either become obsolete
or have to be revisited and things like that
in the middle of the year.
And in this year, especially,
it was much closer to the end of the year.
So looking back at the year,
it's always the first thing to think about the trajectory
and what ideas will still be for a citizen, what do you want?
Yeah, yeah, as a great point, chat GPT
happened right at the end of the year.
Do you think we'd have this same sense
that this was a huge year in NLP
if it wasn't for that late year release of chat GPT?
Oh, I definitely.
I think this year has been really impressive.
I would say even bigger, even if you take out chat GPT,
overall, this year has been really big for NLP,
even compared to the ERGPT-3 came out.
So yeah, I think there have been,
I feel like it took us a while to come in terms
with what these large language models are capable of
or what they clearly fail at and what they are good at
and try to build better two-wing around it,
build better support systems around it.
And so yeah, I think this year has been good,
even if you don't take it out on chat GPT, yeah.
Awesome.
Well, we're going to dig into chat GPT
in a fair amount of detail,
as well as some of the other advances you just hinted at.
But before we do, I'd love to have you take a few minutes
to just introduce yourself to our audience
with a focus on kind of your research focus
and what your interests are.
Cool, yeah.
So I've been working in NLP for a long time now,
but my focus has mostly been looking at
when these language models or machine learning
in general gets interface with real users,
what are the needs that sort of are there.
So a lot of my work has been in explanations
and interpretability, but also in robustness,
both from an adversarial perspective,
but also from out of domain generalization perspective.
And also in terms of evaluation,
how do we know whether the models are doing well,
how well are they doing?
And in general, be able to understand
and predict when the models would work
and when the models would work.
Mm-hmm.
And I'm imagining that the advent of large language models
and the kind of the dominance of that approach
to NLP modeling is, well, it certainly changed the tools
and the approach that you take.
Has it changed kind of the fundamental way
that you approach the problem?
To some degree, yes and no,
I think it has made a lot of my work obsolete
in the sense that we were doing a really good job
of finding fundamental faults
in a lot of these language models
and turned out a lot of them go away
when you have a lot more data or a lot larger size.
And so the specific observations and insights
we had, not all of them have persisted.
But the other differentiation we had in our work
was always being somewhat model agnostic
or try to use a black box approach to the model
rather than looking inside what's going on.
And that is something that you can use
in this world of only access to API.
A lot of those tools can still work out.
So yeah, so it's been a mix, but it's been exciting
to sort of continue to do that.
Well, you've identified some themes that from your purview
have been some of the key topic areas
and research that have emerged in the field
over the past year, let's start there.
And maybe before we dive into any of the individual items,
what's your take on 2022 broadly
and some of the areas that you are most excited about
in the year?
Yeah, so I think broadly speaking
and we'll delve deeper into a bunch of these topics.
But broadly speaking, I think the importance of data
and the importance of looking at what might be
in the pre-chaining data has sort of brought up
back into the focus in a way that I feel earlier years,
we were a lot more agnostic of what the model was being
trained on and just more data was better and a thing.
This year it's been a lot more sort of thinking
about what goes in the models.
And also thinking of ways to use the models
not just by simply prompting it with a simple thing,
but trying to get it to reason,
trying to get it to break down the problem into pieces
and try to evaluate how much the language models can do that.
And that I think is key when you start thinking
about taking language models to more higher level decision
making or higher level reason.
Awesome, awesome.
What's the first area you'd like to dig into?
So let's actually start with a chain of thought prompting.
This is work coming out of Google
that came out earlier this year.
And I guess the easiest way to summarize is
to say let's think step by step.
The idea here is to have the model not just
generate the answer directly, but try
to have it go through the reasoning process
and then arrive at the answer.
This ended up being quite a strong like a quite an effective
method to get the model to do a lot of things, especially
when it comes to mathematical reasoning
and sort of where you can break down the problems
into a bunch of different steps.
Chain of thought prompting did extremely well
compared to what we had before.
And part of the difference, I guess,
was you're not just prompting with questions and answers,
but you're also prompting with something
that is much more detailed.
So the prompt itself has a bunch of examples
of breaking the reasoning down.
And then you have the model being
able to walk through that reasoning and get to the answer.
And in that work is the idea that the user of the model
should break the prompts down into more detail
or that the model should learn how to kind of show its work
and give in a course screen prompts, break it, break
the prompt down itself.
Yeah, so I think the initial paper focused on the user
providing a few examples of this big down.
So if you're saying, like, you know,
here's a mathematical word problem.
You have two op apples.
And then somebody gives you double of that.
How many apples do you have?
Breaking it down into a double means times two
and two times two is four.
This is a very simple example.
But this kind of giving an example or two
of breaking this down can be quite powerful for that.
Especially I think one of the key insights here
and we can talk about other papers that sort of showed
later things, but this is a very emergent property
that seems to exist for really large language models.
And as if you have smaller language models,
it's kind of difficult to get them to do this kind of piece.
And so that's also been exciting to see.
Did the results there?
Did you find them surprising?
Were they counterintuitive that that would work?
I think how well they worked were, I think
it surprised everyone because it's a very simple idea
to just break it down a little bit.
Everybody kind of assumed that the transformers
are sort of either doing this internally
or completely not doing this internally, right?
And by showing you that if you actually write
out a bunch of examples, these transformers models
are able to do this to the extent that they are
was quite surprising and the gains were quite impressive.
Can you talk a little bit about the evaluation
of that method?
Yeah, so the evaluation was mostly focused
on mathematical world problems.
So there's this GSM in, GSM 8 dataset,
and then there's this A-A-W-Q-S MOPS, I guess, dataset as well.
These are mathematical world problems.
And this first evaluation was mostly looking at
how well you can do a reason through some of those.
And yeah, it was much, much better
than anything that we had before.
And then there were, they had some evaluations
on symbolic reasoning as well.
So if you give them sort of tasks which have, like,
you know, like, let have a bunch of,
so like finding a character inside a long string.
Like what is the fifth character or something like that?
You can break it down into a bunch of steps
and if you give it a few examples, it can do it.
If you don't give it a few examples
of how to break it down the models are very bad.
And have you seen any work that looks to extend this
beyond the kind of math and symbolic domain?
So beyond, I'll talk a little bit about some of related ideas
and sort of question answering a little bit later.
But there is one work that is related that I like.
This is called algorithmic prompting.
And this is stuff that came out of Google brain as well.
So, you know, a lot of the,
this stuff is coming out of Google brain
because you need really large language models
to be able to work with this or even bigger than GPD3, for example.
So in this algorithmic prompting paper,
this was kind of interesting where they had,
essentially the same idea as Jane of Thought,
except that they go really detailed
into what those reasoning steps would be.
So they mostly focus on, you know, things that can be
described more as an algorithm rather than us
just breaking it into a few pieces.
So you can say things like,
if I had to add 12 plus 24, right?
How would you do that?
They literally break it down into digits.
Oh, you take the ones place that's two in one case four
and the other, you add them up, you get six,
there is no carry.
Okay, that's that's one.
This is the second step is looking.
Now look at the next 10th place.
It's one and two, add them up three.
Look at the carry or the carry is zero.
So it's just three and then 36, right?
So all of this, this very detailed breakdown,
which looked like extremely detailed.
But what was really impressive to me about that paper
is they showed that you can give examples
of really low digit operations.
So like maybe two or three digit operations
when you're talking about addition
or multiplication, any of these things.
But at test time, you can firstly,
even on two or three digit stuff,
it was much, much accurate compared to regular chain of thought.
Like, you know, 20% going from 80% of for chain of thought
to something that's 100%.
I'm kind of making up numbers.
And this is relative to asking for the model
to solve the same problem without any intermediate steps.
No, so without any steps is even worse, right?
So this is asking the model to,
so like 12 plus 24,
I don't know exactly what the chain of thought would be,
but it would be something that would be
at a higher granularity, let's just say, right?
And so when you go, when you give this detail from
the models are more accurate, which is not so surprising.
But what was surprising was that they kept increasing
the size of the number at the test time.
So started adding more and more digits
and even up to 18 digit numbers,
the model is able to do these operations
much, much more accurately.
Because even though the problems were only
on two or three digits, sort of numbers, right?
And so does this, does this type of work?
Answer definitively whether, you know,
this is already happening inside the model versus
there's some other effects like, in a sense,
it's really counterintuitive that it would work at all.
Like, you know, there's no registers inside the model
that are tracking digits, you know,
the ones place and attend the place.
Like, why should that work?
Yeah, so I think there are, there are,
people are still trying to come to terms
with white end of thought reasoning works.
Is there something in the pre-gaining data?
Is there something in the model?
And there's been some interesting work there.
But no, I think the tricky thing here
is you're making all of these things explicit.
So you're not relying on the model to keep these bits
somewhere latent in its sort of memory, right?
Like you're making it explicit
and of course it's attending to all of that.
And so the chances of it sort of going away
into a wrong place is much lower.
So, you know, scratch pad and a bunch of other papers
had similar ideas of like, hey, let's give some model,
some space to think about things, right?
So it's possible that this is just letting the model
actually think things through.
So it's somehow more computation
that the model is getting.
And then there've been some papers showing that, yeah,
that might be the difference, the fact that you're
generating a single number, but you're letting,
not just asking the model to give it one shot,
but letting it think about it.
And it's not so much the fact that you're giving
these examples breakdowns that helps.
But I think, you know, as many of these things,
I'm sure the answer is complicated
and it's some combination of things.
The last thing you said almost sounds like
the kind of multitask argument.
It's not that, you know, the specific other thing
that you're asking the model to do matters,
but that you're asking it to do another thing
and that kind of, you know, on the traditional side,
like has some kind of regularization effect
or some kind of effect that causes your results
to be better just by overloading the model
a little bit.
Yeah, yeah, exactly, right?
So you're letting, in some sense,
you have more activations, you have more slated states,
you just have giving model and more things to do.
And so it has space to explore through more reasoning.
So maybe that's one explanation for why this kind of stuff
works, but yeah.
Amazing, amazing.
And I should have mentioned earlier on,
but I will mention it now.
All of the papers that we're referring
to will be available on the show notes page,
so folks can check them out.
So the next thing that you had on your list
was decomposed reasoning.
It sounds like it's in a similar vein.
Yeah, so I think this is, that's why I kind of put them
together, but I think fundamentally
this is a very different approach to the same idea.
So yes, I think terminology is something
that the field is going to be revisiting
and decomposed using is kind of something
that I give up with.
I don't even know if people use it.
But the idea here is that there have been
a bunch of papers here, and I'm just
going to sort of put here on through some of them.
But the idea here is that you shouldn't rely on the language
model alone to do the whole task.
So suppose I give it a mathematical word problem,
or if I give it a question answering problem,
that's a lot complicated.
I shouldn't rely on the model and its parameters
to be able to carry everything out.
Maybe the model needs to use a calculator.
Maybe the model needs to do a web search.
Maybe the model needs to even write a small Python script
and actually run it to get the answer that I want.
And so there's this whole idea of language models
getting what you need, but not just relying on its own
parameters, but breaking down your problem
and figuring out, oh, I need to call something else.
And this is what I'm going to do to call it.
It's an idea that sort of claim came out
post chain of thoughts sort of middle of the year,
but they've been a bunch of papers all the way
to the end of the year that have been doing a lot of this.
So yeah, it's kind of been exciting.
A lot of them have been on the QA side of things.
So the two I'll mention is success at prompting that came
out of my group, but there's also decomposed prompting
that came out of AI2.
And the idea behind both of these
was to take a complex question, break it down
into simpler ones, and then have the language model
sort of call another language model
that is answering each of these simple questions.
So if a simple question is a mathematical operation,
then you would use a calculator.
If a simple question is a very simple lookup question,
then you would use something that is like a squad style
question answering system, things like that, right?
So being able to take what the user wants
and breaking down into pieces and then
composing the answers together to give you the actual answer,
this is the answer.
Can you talk a little bit in a little bit more detail
the difference between successive prompting
and decomposed prompting?
How did the settings for those differ?
They came out pretty much around the same time.
So it's difficult to sort of, and they sort of appeared
at the same conference as well.
I think, yeah, so I think some of it
depended on sort of which data set they use.
So decomposed prompting used specifically multi-hop data
sets and sort of trying to decompose it that way.
Successive prompting focused a little bit more
on calculations and symbolic operations as well.
So yeah, I would say the difference is between them.
But kind of same idea, different data sets,
slightly different tooling.
Right, right, yeah.
And we'll see, in some of these cases,
other pairs of papers also that are very similar
that came out around the same time because that's where we are.
How about tool augmented?
So tool augmented stuff.
So there was a paper coming out of Google, I believe,
called tool augmented language models.
So down is a paper.
And this is one of the papers that was essentially showing
that you can have, instead of just calling a calculator
explicitly or just having a fixed set of things,
you can create a description of API is that the language model
has access to.
And have the language model itself generate
example calls to that API when it's doing an output, right?
So if I want to say like, hey, GPT-3 or whatever,
how hot is it going to get today, right?
Or how hot is it going to get today in Irvine?
The language model is going to say, okay,
this is a question about the weather in Irvine.
So I'm going to compose an API called
to a weather service.
That's going to say, what's the weather in Irvine?
And then it'll return some JSON object that says,
oh, the high is this, low is this,
probability of rain is this.
And then the language model will kick in again
and take that output and say, oh, it's going to be
pretty hot today, as since it is Southern California.
And yeah, you know, something.
Seems like this research is heading in the direction
of how would you kind of rebuild Siri or, you know,
Alexa or something like that with LLMs.
Yes, yeah.
And I think this is one of the key sort of advantages
of these language models is not that they can do
additions and subtractions internally.
Like I think that's interesting from an intellectual point
of view.
But when you're making actual products,
you want this language model to,
language is a way to interface with things
that are external to you, right?
So the language models should take in the user queries,
but also be the interface to other things outside
and be able to query it.
I think we will talk a little bit about that later.
But one of the reasons I like this is you can also somehow
now attribute the answer that you're getting,
not to some internal parameter in the language model,
but to say, look, this is the API call I made.
And this is the answer I got.
And now that's what that's the answer I gave you.
So in some sense, it becomes a little bit more
after you attribute it.
The idea of the language model writing a program
to figure out the answer to a question
is a fascinating one.
And it almost feels like if anything around LLM
is going to be the path to AGI, it's like it's that.
What was your reaction to that research?
Yeah, I think it seems quite like to me
from a practical point of view,
it seems quite exciting.
From a code generation point of view,
and things like that, it's useful as well.
But the nice thing about the code writing code
is that it's unambiguous.
So it's making some calls to an external database.
If I want to update the language model or update
this whole system, I can just update my knowledge directly.
The knowledge is external somehow
to the parameterization of the language model.
That makes it super convenient to delete things,
or to add things, or to get attributions, and all these things.
And the interface to that data source
is always programs, either it's like a simple API call
or a more complex one.
And I think I really like this idea
because it allows the language models
to do things that it should be doing,
which is to understand language, or let's not call it understand,
would be able to parse language, be able to sort of transform it.
But it doesn't necessarily have to know
the temperature of Irvine every day, or things like that.
That's not something I necessarily want,
but I mean, this is like this model.
Yeah.
So just very subtly in there, you kind of addressed
another big conversation that's happening in the community
now in this idea of do language models understand.
You call this decomposed reasoning.
The thing is writing programs that kind of requires
some kind of reasoning.
Like, what's your take on these broader questions
about reasoning and understanding in LLMs?
Or would you like to defer that?
Is there a natural point later for us to talk about that?
Come back to it a little bit later.
Maybe even in the next section or sort of,
we are trying to sort of question what reasoning is
and trying to evaluate that in some sense.
But yeah.
The semantic argument around understanding,
like that's not that interesting, but like how a language model
can reason, and the extent to which it's reasoning
versus like cutting, pasting at some level beyond
at an impressive, in an impressive way,
like that's kind of really interesting.
Yeah, definitely.
So I would say, and then even pushing it a little bit further,
like what are the consequences of the fact
that it is cutting, pasting versus its reasoning, right?
So how should we calibrate what things
these should be deployed for and what things
this should not be deployed for based on the situations?
Those are the kind of things that I'm really, really interested.
Awesome, awesome, awesome.
Is that your next section?
Yes, and that sort of ties in very well
with what I think is exciting next, which is,
I'm going to call it sort of understanding
the relationship between the data, the pre-training data,
and the output of the model.
And I feel like there is, again, a few different threads here,
but there is one that came out of my group
that I think is a simple idea that really sort of captures
exactly what you said, the cutting, pasting versus
a reasoning thing.
So this paper is called impact of pre-training term
frequencies on few short reasoning.
And the idea here is we were looking only
at numerical reasoning right now.
So we started looking at all of these examples of,
oh, a GPT-3 can do addition and multiplication
and things like that.
And we started looking at the instances
and turns out that it doesn't always do it, right?
It's not 100% at those.
It's 80% or 90% or whatever the number is.
So we started looking at, OK, what differentiates the one
that gets correct and do things that it doesn't get corrected.
So for example, we saw that if you ask it, what is 24 times
18, the model gets it right.
It says 432.
If you say, what is 23 times 18, the model gets it wrong.
So 24 times 18 is correct.
23 times 18 is not correct.
Is this random, like what's going on here?
And did you just enter up there?
Did you find that consistent across invocations?
I've run into that kind of thing.
We've all run into that kind of thing
playing with chat GPT and other things.
And sometimes it gets certain things consistently wrong.
Other times, it gets the thing wrong.
Sometimes and not wrong.
Other times, I get to random seed kind of thing
or something else going on in the model.
Did you explore that at all?
Yeah, we definitely saw that.
So it's both like if you're doing few short prompting,
which examples you put in the prompt would sometimes
change to the output or how you phrase it.
Like you do you say, what is 24 times 18 or what is 24x18?
You know, things like that definitely made a difference.
But even after averaging these things out,
we saw that 24 times 18 was in general more accurate
than 23 times 18.
And even more than that, we did even further analysis.
And it turns out that all of our instances
that involved 24, the model was much more accurate on
than all of the instances that involved 23.
Well, so we decided to do this for everything
from 0 to 100.
So all two data numbers, essentially,
single and two data numbers.
And no, it's a whole spectrum.
And we didn't see a clear reason why
some things are low accuracy, some things are high accuracy.
And so then what we decided to do,
this is the part that I think I quite excited about.
We started to count how many times
do each number, each of these numbers
appear in the pre-getting data.
And turns out, and you can see the plot in the figure.
If you plot the log of the frequency of these terms,
and now how accurate the models are,
it is pretty much exactly like a.
Which is intuitive.
The model does better on things that it sees a lot of, right?
Yes, yeah.
But yeah, so it's also expected yet disappointing
because you don't want it to be such a nice strong curve,
like you want it to do.
Like if it's doing mathematical reasoning,
it should know that 23 is one less than 24,
and all of these things, right?
So I think it's one of these things
where it was expected that the model would be better
on things it has seen before.
But you also, at the same time,
hold this thing of like, oh, it is able to reason,
it is able to do these things,
and it's kind of difficult to resolve both of those, right?
So this was one example.
I think we are barely scratching the surface,
but this was an example of paper
that sort of started looking at some of these pre-training
statistics, or not just single term frequencies,
but diagram frequencies and things like that,
and show that the model is quite sensitive
to what these things should be, right?
And I don't want to sort of make a claim
that there is cutting, wasting, going on,
or any of these things.
But this effect is so strong that at least
when we think about reasoning,
and when we are evaluating reasoning
in these language models,
we should be taking this effect into account.
And this may be a side note.
It looks like the model that you evaluated with GPTJ,
and clearly that's a model.
So an open source model that you had access
to the pre-training data kind of asks questions about,
how do you get the same kind of insight
into these models that are behind APIs?
Yeah, so I think the question is,
I kind of don't mind that models are behind APIs
to some degree that's commercially
that that kind of makes sense.
I feel a little bit disappointing
that the training data also is behind
sort of close wall, right?
So I know that there is a lot in the training data,
but if we want to be able to understand
why GPTJ works, or why GPTJ works,
or even generally, when do language models work,
when are they safe to deploy?
All of these get a question.
I think it's okay if the language model
we only have a black box access to,
but it would be good to have access to the training data.
It would be good to have access to a bunch
of these other things that can help us
sort of do simple kind of analysis like this,
and maybe more complex ones.
And actually be able to sort of decide
what to do with the model, okay?
So I think this whole direction of trying to understand
what's in the pre-training data, I think is key
and then something that will persist
for the next couple of years.
Do you think we have the right tools to do that at scale?
I'm imagining that was not an easy task
to do just for simple mathematical problems.
That's true, but training GPT-3 is also not a simple problem
and people have solved it, right?
So I think the tooling is something that everybody right now
is sort of excited about building tools
that actually give information and insights
into these language models.
And I think even at the idea,
we are at sort of early stages of trying to do these things
of building some tooling that can support this kind of analysis.
But you know, if the data set is available,
I think people will do amazing things.
And I thought this would be impossible
and it seemed like crazy.
Like, hey, this is almost a terabyte of text.
Like how can you do anything with that?
And it was not trivial, but it was easier than impossible.
How do you identify,
you know, so you identified some behavior,
the relationship between accuracy
and frequency in the training data.
How do you identify what that is a consequence of?
Meaning is it specific to the way GPT-J was trained?
Is it all transformer-based language models?
Is it, you know, maybe something about that particular data set?
Like, have you, are you able to say that it is
a broad characteristic of LLMs in general
based on the work that you've done thus far?
That's a little bit difficult to sort of.
Yeah, that's a little bit difficult to measure,
partly because we don't have data set available
for too many models, right?
So at least we tried the whole slew of the Luther models
that were trained of the same data set
and we saw similar effects on different model sizes, essentially.
And yeah, as data sets become pre-training data sets
become more standard, it's fairly trivial to sort of extend this stuff.
Since this paper, we also have sort of an online demo
where we have a bunch of more tasks
that try to go beyond mathematical reasoning.
It's a little bit difficult to sort of even define
what these sort of terms are
and what you should be computing frequency of.
But yeah, I think we should be able to do this stuff
for other tasks and for other models.
And to me, I think this is somehow a consequence
of a language modeling loss that encourages us in some sense, right?
So yes, the model has seen more and it'll be more accurate,
but even the ones that it has seen less,
like it has still seen billions of times.
So there is no reason for it to be wrong on it,
except for the fact that the language modeling loss would sort of
want you to be more right on the ones that have seen more.
Mm-hmm.
Yeah, another paper identified out of Yav Goldberg's group.
Oh, yeah.
So this is work led by, I think I'll quickly talk about this.
So this had a similar sort of intuition
for trying to look at things in the data
and trying to figure out why the model has certain biases
or has certain errors.
And this was sort of a little bit more
on trying to identify when two entities are related, right?
So if you say where was Barack Obama born,
the model tends to say Chicago or in some sense,
it can say Washington and depending on how you praise it.
And like, why does it give the wrong answer?
It's kind of a question.
Why does it not say Hawaii or something?
And I think to be able to answer this question,
you have to go back to the pre-training data
and try to see like, okay, what did it even see?
So what I like about this paper is it kind of tries
to build use causality tools
and builds a whole causal graph
for where these kind of predictions might have come from
and then tries to estimate all of the edges
in those causality graphs
and tries to do some causal inference
to sort of attribute it to specific
statistics of the big data.
So in this causal graph would each individual document
in the pre-training data be an intervention of sorts?
So they sort of worked, they worked at the level of,
I guess, triples or something like that, right?
So let's say you see Obama in Chicago
being a senator there or something, right?
So this is kind of a triple.
And so they work on statistics of those triples
of the pre-training data to sort of make it tractable
and make it sort of allow this inference to work.
But in applying the causality machinery
like are each of those interventions relative to
some prior relationship between the things the triples?
Yeah, so there is the true relationship
between these triples
and then there is the observed relationship
between these triples and how many of these things,
how many times it appeared in the pre-training data.
And so the idea would be when you're doing it
over many different entities and many different relations,
do you, so those kind of become your whole data set
in some sense.
So Obama has appeared with Chicago,
but Hillary Clinton has appeared elsewhere
and on all of these things.
And then together, which of these relations
seem to affect a specific prediction the most?
That counts.
Awesome, awesome.
Kind of continuing on in the data theme,
there's been a ton of work looking at the need for clean data.
I think maybe one of the most surprising things for me
is like the return of supervision
at the scale of LLMs.
Talk a little bit about this category.
Yeah, so this was somehow the most surprising category
for me for this year.
I will say that like after GBT's
it came out and at the end of last year,
everybody was kind of excited about language models,
but the solutions for what's next always seem to be like,
hey, let's get more data and let's get larger models
and let's train, train longer.
And those are still sort of useful things nobody's denying.
But this year has shown that like,
okay, you can actually do a lot
if you're a little bit careful about your data, right?
And maybe if you'll start cleaning up your data
and try to think a little bit about where the data,
your pre-gaining data should come from,
your pre-gaining data itself, you know,
that could be quite interesting.
So when you think of like RLHF as an example,
do you think of that as fundamentally
just cleaning up your data, being more careful
about your data as opposed to?
Yeah, so no, I think I was thinking more
what happened with the loom language model
which was trained on sort of a lot more
thoughtful process of gathering the data set
because partly because they documented it
and we know what sort of they went through.
But now like RLHF and those kind of things,
I think our examples of showing that the language models
are not quite ready for use case
just based on pre-training on sort of large data
that has been gathered.
You need to read, like you can call it like,
hey, cleaning up the data, but I think of it as like
maybe reinforcing some of the nice signals in the data
by having these examples.
Or in some sense, you know, people have been fine-tuning
on these sort of supervised data as well.
And the gains that you get from RLHF
have become extremely evident this year, right?
So somehow that has become the secret source
of opening eye in all of these companies
that want to have really strong language models
rather than scale and just draw a pre-training.
And for completeness, we've talked a little bit
about RLHF on the show, but what's,
how do you think about it as a researcher?
I think it's quite exciting.
I think it sort of addresses a lot of
my concerns with language models.
I don't think pre-training data can be trusted, right?
And you shouldn't just train something
and expect the model to have clean output
or have, you know, your values
and any of these kind of things,
whatever that means is the context of large language models.
But essentially, if you want real users
to be interfacing with language models,
you need to make sure that there is some sort of check.
And RLHF is not a solution, like a full solution,
but at least there is a way to sort of say, okay,
this is the actual task.
Your actual task is to be interfacing with humans,
not just regurgitating what you've seen
in the pre-training corpus, right?
And so that intuition sort of is captured by this RLHF.
And do you, do you remember offhand any of the,
if they were even published the stats
in terms of the number of prompts,
like human generated prompts that were used
in chat GPT or in struct GPT?
Yeah, I don't think they were published
as far as I know.
Yeah, I don't remember exactly what they are.
I think until GPT had the documentation
of sort of how they were gathered,
but the size was like, you know,
how many of them were sort of generation does,
was just classification does, things like that.
But I don't think the exact dataset is good.
Do you, do you have a guess
as to like the relative cost of, you know,
gender of collecting the human feedback
relative to the cost of training the models?
Oh, the rate of cost of training the more it was like,
I think it's much cheaper.
Order of magnitude or is it like much, much, much cheaper?
Because we always say like, you know,
collecting the data label data
is the most expensive part of machine learning.
Is that still true at the scale of LLMs?
Or is it that RLHF is like extremely efficient
and you just need a little bit of guidance
on top of the, you know, the pre-training data?
I feel the true answer is somewhere in between.
So I don't think it's like, it's nowhere
very little data, like I think you need a lot of data
to be able to do it,
but I don't think it comes close,
at least the way these are trained right now,
I don't think it comes close to sort of training
the model itself, right?
So, but like when you think about, you know,
charge GPT, it's been released publicly
and a lot of people are using it.
A lot of that data is gonna go into,
in some form, back into the model and improve it.
So was that expensive to collect in some sense
because they had to run charge GPT, but, you know,
they'll probably pay some managers to clean that up,
but I don't think that's gonna prepare to actual training.
It's also a really interesting example
of like bootstrapping, like there's a certain amount
that they collected themselves, you know,
the instruction GPT work, and then they, you know,
created something that was good enough
to set loose in the world,
and now they've got this virtual cycle
where I'm imagining it's a lot cheaper
for some annotator to clean up,
what, you know, millions of people are creating
then for them to create that themselves.
And I think like, I think this year has also shown,
maybe even to people at OpenAI,
that the value of these things, right?
Like when they released GPT-3,
they probably didn't realize how valuable it would be,
and then they sort of collected data released in start GPT,
and yeah, on their benchmarks, it was good,
but when people started using it,
you realize how much better it is.
I think similarly with chat GPT,
they probably knew how good it was,
but they probably didn't realize
how good it actually is, right?
And I think this idea of human feedback,
being a secret source that is proprietary,
I think, will continue to be a bigger piece in the future.
Talk a little bit about Roots.
Yeah, so the Roots is this nice data set
that was gathered by the big science group,
and I've been following the big science group,
and a bunch of interesting things there.
I guess I'll jump in to refer to the interview
that I did with Thomas Wolf,
that I don't think Roots came up explicitly,
but we talked about that work,
and that eventually resulted in Bloom,
which we'll talk about a little bit more as well.
Yeah, so Roots, I like because I think I really like
what Luther have done with the pilot dataset,
by releasing the dataset that was used
to train all the GPTJ models,
and I think the big science group sort of took their intuition
and sort of went further with it,
where they have a really well-documented,
and not just well-documented,
I would say a very thoughtful process
of gathering this dataset.
It's multi-lingual over many, many different languages,
they've been careful about sort of listing which sources
they want to even crawl in the first place before.
So it's not like a post hot cleanup of the data,
it's very sort of thinking about it.
They gathered a dataset that is huge,
and we talked a little bit about this data,
also the hugging phase has sort of built tools
on top of it to be able to quickly search it,
to see what's in it, and stuff like that.
And I kind of like that approach to life language models.
So I think getting the right dataset
is crucial for these language models,
and doing this documentation and stuff
is good for in the long term.
So your next category is decoding only.
Talk a little bit about what that means.
Yeah, so this is a theme that I like about some of the work
that has come out here.
And partly it's because we have these language models,
where we have this black box of interface to them.
And a lot of it is just prompting,
so changing things on the input side
to see what the model generates,
and the only thing most people are changing
on the output side is like, oh,
we let's change the temperature a little bit,
and we get part of different things.
But there has been a bunch of work looking at,
okay, let's not just do that.
Let's actually think about what's happening
in the output of the model during decoding of the text.
And maybe we can do smart things there
that actually sort of change the output considerably, right?
So some of these sort of came out sort of late last year.
So there was this work on Newtius sampling,
that's a little bit older,
but then there was this stuff on sort of constrained decoding
as well, where the constrained decoding paper
came out of semantic machines.
They showed that you can have,
suppose you want to want the language model
to generate programs, right?
So the programs come with a certain grammar, right?
Like there is a syntax that they need to follow.
So you could actually constrain the output of the language model
as it's generating token by token
to sort of adhere to that syntax in some sense, right?
And just by doing this constraint,
you can get, firstly, obviously,
you will get programs that are syntactically correct,
but you can actually get the right things out of the model.
And so there have been a lot of sort of works looking at
how can we decode by having some constraints on the decoding.
So one of the papers that came out this year,
that we've got the best paper award as well,
is called Neural Logic ASTAR, ASTAR-esque decoding.
And the idea here is that instead of just doing
left to right decoding where you're being greedy
or where you're being doing some kind of beam search
or sampling or any of these process,
why don't you actually use some of the computer science ideas
that we have like ASTAR search
and try to find the best possible decoding.
And then when you're doing this kind of thing,
you can also think about constraints
that you might want to put on the decoding.
So you want to say, look, I want the decoding
to have these three words in it, right?
Like, hey, you're generating a recipe,
make sure that it has these five ingredients, right?
Somewhere in the generated text.
You can also flip it around, hey, generate whatever text
you generate, make sure it doesn't have these specific words,
like that.
And this paper sort of uses ASTAR
during decoding to generate that text,
that sort of, you know, your constraints are satisfied.
And this paper showed that, yeah,
once you do that properly,
you can actually do a lot of the tasks much better
just by controlling decoding rather than changing much
on the inputs.
It seems like this is another example
where it's predicated on having open access
to the model internals,
and you potentially lose a lot if you don't.
Yeah, I think so from what I understand,
you can still do these kind of things
with GPT-3 to some degree.
I think what you need, you can, okay,
so you can do this with black box model
as long as you get the probabilities
of all of the tokens at every step, right?
So I don't think GPT actually does that, right?
But, you know, you could imagine an API
that says, okay, the next,
here's the distribution over all of the tokens.
And you should be still be able to do these kind of items.
So, you know, some of the concerns is like,
if you want decoding to be fast,
then it's difficult to use some of these ideas.
The A-star-1 specificities is a lot slower,
but it's able to satisfy your constraints.
So it can be where you're okay to trade off some time,
but let the model take more time
in making sure the output is seen
and satisfy your constraints,
it could be really cool.
And now, yeah, often you see,
hey, we applied one method, A-star, in this case,
let's go back to the computer science toolkit
and apply everything else.
Have we seen that here?
Not yet.
I think it came out late enough in the year.
But I guess it came out sort of early in the year,
but yeah, we haven't seen that much yet
because, but I think, yeah,
that's the kind of thing that will happen next.
It's like, okay, now this is, yeah,
this is attracting a whole different kind of thinking
where people were not thinking about decoding at all,
and now they will be in this light,
which is all this kind of good paper.
Awesome, awesome.
Well, those are great themes to kind of reflect on
as we think about the past year and LP research.
Our next category is to talk about
some of the new tools and open source projects
that we saw in the year.
We've already talked a little bit about data sets,
which is kind of related.
But I think the first thing you have here is OPT.
That's all about OPT.
So yeah, I think OPT came out fairly early in this year.
And I think it kind of surprised everyone
because the sort of looking back at last year,
there weren't that many open source reproductions
of large sites, right?
So I think Luther AI was sort of leading it.
GPTJ was six billion and, you know,
they were sort of growing it slowly and slowly
and they had got to 20 billion parameters.
And then OPT sort of came into the scene
and they, yeah, there were a bunch of nice things.
They documented a lot of their whole training process
in a log work with sort of all kinds of insights
about what training a log.
Yes, it was released better, right?
And that was also not to say too much against better,
but it was also surprising that reproducibility
and open source seemed to be key aspect of OPT as well.
So that was kind of nice.
And they also released a lot of models and, you know,
like all different sizes, including 175 billion,
which hadn't been available at all.
And even right now, I think it's probably the most useful model
if you want to do stuff with 175 billion
is to use the OPT model, right?
So I think the idea of documenting the whole training data
gathering process, documenting the whole training
of the model process,
and then releasing all of these models available
for research, I think has helped the research community a lot.
And I expect that if there are people
who want to build models and potentially find you
in language models and do all of these things,
the OPT would be a pretty big source.
Have you seen much in terms of benchmarking it
against GPT-3?
Yes, so I think people have been benchmarking it.
And I think it performs reasonably well.
The tricky thing is, of course,
there is instruct GPT, which is, you know,
when you call it GPT-3 on the API right now,
it's often defaults to the instruct one.
And that one is a lot more difficult to beat,
but for all of the purposes, I think of it as like,
yeah, OPT-8 is basically same as GPT-3.
We talked a little bit about the big science project
and one of its outputs and other is Bloom.
What did you, what was your take on Bloom?
Bloom was, again, a really big data model.
That was, I think, 180 billion parameters.
So similar sizes, GPT-3,
believes to become a daily open source.
It's like we talked about completely well-documented
data process and sort of training process.
Combined with the fact that this was done by a group of people
it's kind of just volunteering their time to do so.
And then being able to reproduce to a large degree,
what OpenAI has done was quite amazing, right?
And then sort of, again, like both OPTs,
releasing all of these things is kind of a sign
for other big tech companies to say like,
hey, you can do this because we've done this kind of thing.
But Bloom has shown is that a bunch of people enthusiastic
and excited folks that are enterprising can actually do things
that maybe even a year or two ago would have seemed impossible.
It may have been in our trends conversation
from last year or maybe it was prior.
But in these kinds of conversations,
there was a point in time where we were lamenting
the loss on the part of the individual academic researcher
to contribute to fundamental model research
because of the resources that were required.
And to hugging face and the big sciences team,
like they showed that not necessarily not so fast, right?
Right, right, exactly.
And the other thing I like about this,
the blue method is and the corpus that came with it,
they were also focused on being a lot more inclusive
in terms of having a global perspective.
So they were trying to cover many, many different languages.
Very principled in the way they pulled the data together.
Yeah, yeah.
And also multilingual in a way
that none of the existing models have been.
So yeah, it's quite quite exciting.
And so like conceptually,
this is a great example of how one model
at 175 billion primers and another model,
you know, the same number of primers
could be very different, you know,
at least in the data that they were trained on
and you would expect that to result in
very different results using the model.
To what extent have we characterized that?
Like at that scale of data,
it's still a lot of data,
still a lot of like raw internet data.
Does it all kind of fall out in the wash
and all their efforts at being principled,
you know, kind of just get lost?
Or do we know how to compare that?
Yeah, so there have been a bunch of benchmarks
and including in their papers,
but in general also.
And that's where sort of the, you know,
don't hold me to this,
but I would say like Bloom is not the go-to language model
for people if they want to do English things, right?
All right, so I think maybe some of the trade-offs
that made in collecting the data
or even just having all languages
result in the model that's definitely really good
for multilingual things.
But that's not what our benchmarks have been designed for
unfortunately.
And so if you just look at the benchmarks,
like, you know, which are traditionally designed
for English Bloom, I don't think quite is at par
with OPD or GPDC and definitely not with Instruct.
And when I mention benchmark, you know,
there's that aspect of kind of applying
the traditional performance benchmarks,
you know, for LLMs to bloom and comparing their results
to the others.
But I'm also curious about.
How we characterize like qualitative differences
between the way Bloom responds
and the way GPT responds.
For example, you know, in terms of,
you know, like the kind of fairness considerations
or that kind of thing,
or, you know, are there qualitative differences
in the kinds of responses that you get
that aren't picked up by the traditional benchmarks
or are the traditional benchmarks like so, you know,
expansive at this point, we've kind of
characterized a lot of that stuff explicitly.
Yeah, again, I think the answer is somewhere in between.
So I don't know if people have thoroughly compared
the due to see like it was the level of toxicity
and people like that, you know, like,
I think when OPD came out,
they did a lot of disanalysis in their paper of like,
hey, how toxic is their model, how safe is their model?
And they realized that yeah, in some things,
they were worse off than some of the existing models.
But I think with loom, specifically,
I don't know off the top of my head,
how it sort of compared in terms of these,
these other sort of other aspects.
Okay.
Talk about the inverse scaling competition.
Yeah, so this was a pretty nice thing that gave out
and I think I suppose it's still going on
even though the submissions are down.
So I'm kind of hoping to see
what the actual effect of this was.
But this was sort of introduced sort of
in the middle of the year.
And the idea here is the thinking of things
like what sort of scaling laws was showing, right?
Like when you scale up your models,
performance goes up for everything.
And that's kind of exciting to see.
But it also tells us that okay,
there are many, many things that just the models
would just get better on as time goes by
because they'll get bigger, they'll have more data set.
The inverse scaling was this intuition to see,
okay, what are, can we characterize the phenomenas
that don't have the same trend, right?
So other aspects, you know, you created a data set
which is something everybody will agree
is a reasonable data set.
But when you give them to larger models,
they actually get worse.
And so this, this price and this competition
is an effort to identify what those,
what those tasks would be.
And sort of the better your inverse scaling is.
So the worse, the bigger models are on the data set
that you've contributed, the more likely you are
to build this competition.
And so yeah, they've had the submissions
and they're kind of evaluating them as opposed
and they haven't quite announced it.
But I think a lot of the stuff on, you know,
a lot of the interesting things could come out of this effort.
So one thing I could imagine is sort of deeper levels
of misinformation, whether the model is relying so much
on what it has seen in its training data.
Now let's not call it misinformation,
just not being able to update its information in some sense,
right?
So these large language models have memorized so much
about the pre-telling data that they kind of reject
evidence against that, right?
Maybe if they're smaller, there's less memorization
and more generalization.
But I think it could be pretty exciting to see
what are those things that actually get worse with scale.
I think it's quite an interesting question.
And next up, you have the Galactica,
can we call it a debacle?
Okay.
So Galactica is this L alone that meta released
that was tuned to generate scientific and research text.
And research text.
And was it even up for three days?
It got pulled down pretty quickly, right?
Yeah, I think maybe a little bit more than that,
but yeah, they're about, yeah.
And I think to me, it's a story about how not
anything in terms of what the Galactica team did itself,
like I think the model training it,
everything was the right thing to be doing.
The tricky thing was just how it was pitched
and how, you know, there was just not clear caveats
about what this model is capable of doing
and what it's not capable of doing,
that led to such a backlash, right?
So I think it was a language model training
like language model training on a lot of science papers.
So it's going to produce papers
that look like scientific text.
I think that was an expected thing,
but again, the backlash it got and stuff like that.
Essentially, it tells everyone,
and I hope the message is not to demo language models anymore,
but I think the message should be how to make sure
that you're not hyping things up more than they should be.
If you reflect on chat GPT, which came not very long after
Galactica and the launches of those respective products,
are there clear, is there a clear like,
do don't do list?
So I will say that chat GPT itself was also not,
you know, not completely without hype attached to it,
even sort of how they, right?
Somehow they managed it, it was a lot of hype.
Right, right, right.
I will say that they were fairly clear about the fact
that like, hey, don't trust, you know,
maybe they could have been clearer,
but like don't trust the factual stuff and things like that.
Like it's not a lookup engine.
I think they kind of could have done a lot more of that,
but you know, they at least had some caveats.
But more than that, they part of their RLHF stuff
was to make sure that the model is not producing,
at least obviously sexist, don't say.
Yeah, there was a lot, and maybe we're jumping into chat GPT,
which actually is the next thing we're gonna talk about,
but there was definitely a lot of,
especially early on, things that it just would not a pine on.
Like yeah, no, you're not gonna sucker me in and go in there.
Right, right, right, yeah.
And I think when you're building something that's public
facing that you're selling as a tool, as a product,
that is necessary, right?
Like I don't think you should be doing otherwise.
Galactica should not have been a public facing tool
for every scientist to start using to write their papers.
It should be a language model, right?
And then what the product is or what the tool is,
is a gap that other people can help fill in, right?
So that was sort of the missing piece
when I think about chat GPT versus galactica.
It's like, yeah, chat GPT has some of the caveats
about what it's doing.
Has some of the caveats about oh,
it's a language model, not a product to some degree.
And galactica was missing it, right?
So, yeah.
Now, we're there.
We were talking about open source.
Next up is kind of commercial developments.
Top of that list is chat GPT.
Right, yeah, let's talk about it.
It's, you said early on that,
hey, even without chat GPT, you know, this was a huge year
that's clearly not to say that chat GPT
wasn't a huge contribution to the year.
I mean, certainly one of the things
that I found most interesting was the degree to which
it kind of broke out of the MLA echo chamber
to, you know, just talking to random friends
and are like, hey, have you tried this chat GPT thing?
Well, yeah, I have.
Yeah, so that that's been the, I guess, the most surprising
and in some sense, the longest,
longest term impact for chat GPT is going to be the fact
that it made it commoditize.
It made it mainstream in a way that nothing before it had,
right?
And whether it deserved it or not,
what the actual innovations are and all of these things
is a different question, right?
Like it is clearly, even for research,
point of view, qualitatively better than GPT's tree.
Whether it met some threshold for becoming the big thing
that it did, it's sort of difficult to sort of
in hindsight try to evaluate that.
But I think it was, yeah, it is definitely something
that became mainstream and MVP is talking about it.
There is still a question in my mind whether that's a good
thing or not in the long run.
Because, you know, we can talk about some of the problems
with chat GPT, the biggest one being,
we know it's a language model.
Like, to some degree, we've been figuring out
last couple of years what these things are capable of
and what these things are not.
And I can say that in like a couple of minutes,
come up with tons of examples where it would fail.
That's not quite the case when you sort of start putting it
out in the public.
So, most people don't know what a language model is.
And I have played around with,
you know, I've got a bunch of my family to try it
and things like that.
And the biggest thing I've had,
the biggest difficulty I've had conveying to them
is the fact that it's not looking up anything
when you ask it something, right?
Like, that is a conceptual jump that is very, very
difficult for people to get over.
Yeah.
And so people, like, yeah, of course,
it should know about these things because it happened
yesterday and for such a big news items.
Like, why would it not know?
And now, like, no, actually, it doesn't know anything
beyond a certain time.
And even saying it knows anything from then
is a little bit difficult, right?
So I think the best analogy that I've, you know,
this applies to my research as well,
but the best analogy I've had in trying to explain
people what chat GPT does is do not think of it
as a stochastic pattern or anything like that.
But if you have to think in terms of animals,
think of it as like a chameleon,
like, it's trying to sort of fit in
to a bunch of humans, right?
And it's trying to just write things that will make it pass
as if it sort of knows all of those things, right?
I was in a Twitter exchange about I'd ask chat GPT
to explain RLHF and it came up with this acronym
it came up with this acronym that was like,
oh, I forget it and it was really funny.
It was like something leaderboard, you know,
humans, human something.
It was like, it was so far off.
Interestingly enough, I'd asked it about,
I'd had conversations, you know, interactions with it
about RLHF and then knew what it was.
Like to your point, it's about kind of where it sits
in the context of the prompt.
And I just kind of posted, you know,
is it trolling me or is it just trying to BS me?
And one of the responses that I got that, you know,
and reflecting on it was like really insightful,
like it's always trying to BS you.
That's all it's doing is trying to BS you
to produce some text that you will think is reasonable.
And, you know, to its credit, a lot of times it's right
but that's all that's trying to do.
Right, right, yeah.
And especially when it comes to factual stuff
or even like, you know, it is a very useful bullshitter
in some sense, right?
So because when it's right or when it's partially right,
that's still useful because, you know, it is what it is.
But that when you put that label,
like if they had sold it as like,
hey, we have built a really good bullshitter, right?
Like and it sold that as a product,
then people would know, okay,
not to use it for a bunch of tasks
that they're currently thinking of using it, right?
And so, yeah.
So that's the sort of divide that in messaging
that somehow researchers and NLP folks know,
oh, yeah, language model loss.
Obviously all that is doing is blah, blah, blah, right?
And yes, RLHF can help to some degree,
but clearly it's not going to be able
to do these bunch of other things all the time.
And that kind of thing is missing from general public,
but also how a lot of people are planning to use it
for example, right?
So I think that aspect is the part
that we need to think a little bit more about.
You've got Palm and Arpa Flan down.
You know, tell me a little bit more about your take there
because I hear of them, you know,
in a vague research context
because no one really has access to these,
but Google as much more so than, you know,
something that is huge from a commercial perspective.
Is this a prediction or is this a reflection?
Yeah, so no, I think of this as a palm
was a huge commercial development this year.
Like Google built this really, really large model.
Now there are, obviously they haven't released it, right?
So what's the ideal situation?
They completely release it open source.
Everybody gets access to it.
That's not gonna happen.
Another possible thing is they put an API on it
and charge people from a Google perspective
that doesn't make sense, right?
So it is something that they've built.
It's valuable internally.
I'm sure it sort of has, you know,
there are reasons not to make it public,
but it also has a lot of research insights
because nobody else has such a big language model
trained in a similar way.
And I guess I want to give them props
for at least publishing and evaluating
and doing things like that with palm.
Because it is doing, it is oversized that
we will not see for maybe another year or two
to be sort of publicly available,
but yet we get to hear about some insights,
what to expect, what are the emergence behaviors
coming out of those language models, right?
So yeah, it would be ideal if we could audit it
and all of us and I could contribute
in finding out what the problems are
and when it works and it doesn't work.
But given that, I think they did a good job.
Specifically, what I will say is that that size
has brought up a bunch of capabilities
like the whole chain of thought thing
that we talked about at the beginning.
That somehow became possible at that size,
but wasn't possible at other size, right?
So that's why that research is also all coming out of Google
because it applies mostly to, you know,
to the language, to the LLM's of that size.
Palm is 540 billion parameters?
Something, yeah, five, four years, yeah, yeah.
So I think, you know, they have access to it
and they can produce a string of papers
and yes, nobody else can write those papers,
but from a consumer of research as well as producer, right?
So from my consumer side, I love to read research
and I'm glad that they're writing those papers
because there's a lot of interesting stuff
in all of the papers.
So yeah, there's a whole string of papers
that I would recommend and I can point you to them offline.
But yeah, there's stuff that, you know,
we'll see happen publicly next year
or maybe another year after that
when those models become so much better.
So yeah, no, I think that's been kind of key.
So I'd say for that commercial, but not commercialized.
Yes, right, right, right, yeah, yeah, yeah.
I'll soon to be commercialized, I'm sure,
but maybe not for I could do.
Yeah, awesome.
Next up kind of the intersection between search and LLM's.
What do you see in there?
Yeah, so I think that's been kind of an interesting,
it's been a commercial development again,
questionable to some degree,
but because I don't think the research is,
and these models are quite up to stuff,
but this somewhat coincided with ChatGPT.
Well, ChatGPT certainly raised a ton of questions about,
hey, is this a Google killer?
Right, right, right, right.
Yes, exactly.
And along the same time,
there were at least three search engines that I know of.
There was publicity.ai that I don't think existed before.
What the product they came up with,
which is a search engine, which sort of gathers
all of the results from a typical search engine,
but then uses GPT-3 like models
to summarize the content of those links
and produces a paragraph that actually answers your query.
U.com is again a search engine that has been around for a while,
but they brought this whole Chat aspect to their search
where you're sort of chatting
and trying to come up with an answer.
And again, it's sort of not just showing you a bunch of links,
but composing information into text.
That's Richard Social's company
and we'll drop a link to my interview with him
in the show notes as well.
Okay, cool, yeah, yeah.
And Niva is another, it's a private search company.
It's a startup that also has an AI agent
that you can talk to with and things like that, right?
So I haven't played around with all of them.
I've played around with them a little bit.
And again, it's very easy to find problems
and sort of realize that, okay, these language models are,
you know, this interface is great
and it would be great to get the right paragraph
if it could get there,
but oftentimes it don't quite work
because of sort of fundamental issues with language models,
but I think from a commercial development,
I'm pretty excited about what search would look like in the future
and where language models would fit into that for much.
Yeah, one of my thought experiments
with this in the context of chat GPT,
I'm not that it was particularly deep,
but like there was this early meme,
you know, along the lines of,
hey, Google searches crap, now it's all ads,
chat GPT, you know, I love this interface,
you know, it's gonna kill Google.
And so I asked chat GPT to basically build response
with an ad in it.
It works, it can do it.
I wouldn't be so sure that your, you know,
LLM based search won't have any ads.
Right, right, yeah.
Yeah, no, I think, yeah,
where the ads would come in and how subtle the ads
will be once you throw in language model into it.
Yeah, that's kind of interesting question.
And I guess next up on your list of commercial developments
is what I might call the LLMing of all the things.
Yeah, so I think, you know,
it's been two years or so since GPT three came out.
And it's the question of like, okay,
where is the world changing products
that are using GPT when it came out,
hey, it was gonna change everything,
has it changed everything?
And I would say like for the most part, no.
The products that did seem to show some promise
and some of these are ones that will appear in the future
but have been kind of semi-announced.
It's the notion of writing assistantships, right?
So I think notion, notion AI is the one I think about
where a lot of people, it's a mainstream product,
anybody can use notion.
And notion has this GPT three thing built in
where it can write to do lists for you
and things like that.
So I think that is a pretty strong first version
of GPT three as a commercial product
that anybody can use that I'm quite excited about.
I feel like the timing there is very chat GPT influence.
Obviously, they've been working on it, you know,
they saw it when GPT three came out
but I think they made it available right after chat GPT
and a lot of these, you know,
Jasper's been around for a while
but there's a lot of new kind of writing assistant
types of things and it just does seem like
there's a step function increase in kind of energy
in the space of using LLAMs since chat GPT.
Even though they're all based on GPT three
which has been around for two years, right?
Yeah, so I don't know exactly why that thing seemed
to align well, right?
So it's like, yeah, GPT three was announced
but it was a while before the API was rolled down
to everybody and you know, and maybe after that
it takes a while to make the business case for these things.
So yeah, maybe it's just timing of why it worked
or there were people like already kind of working on it
in a sort of on the side and they were like,
hey, now we gotta sort of write this wave
and sort of introduce things, right?
So I don't know exactly what that looks like
but yeah, no, I think the fact that it aligns
also gets a lot more excitement and people know,
okay, chat GPT is something I've played around with.
This is now a chat GPT that's working on something
that I do and there's a lot of value in that.
Am I detecting an underlying pessimism maybe
about like, you know, kind of, you know,
where's the flying car that I was promised
all I have is this GPT three thing?
Well, it's not so much the pessimism
because when I saw GPT three, it became evident to me
that this is a great language model
but it's not clear as it is how it can be made
into a product, right?
But it still came with a lot of hype
and yeah, it can generate a bunch of things
but we quite haven't quite seen
what the product version of those look like.
I think the language models are extremely powerful
not just as language models
but they can be converted into products.
I don't quite feel like we are at a stage
where it's just going to be through prompting
and, you know, let's just tweak it a little bit.
I think there are a bunch of products
that will come out of just by doing that
but there's a whole slew of product
where the language models need to know a lot more
about the context where they're gonna be
to be able to be effective, yeah, effective tools.
And you mentioned Microsoft, what do you have in mind there?
Yeah, this was sort of a news that came out recently
where they're trying to have a bigger stake in OpenAI
but also just generally thinking of having OpenAI
like tools available in Word, available in PowerPoint
and all of these things, they don't have it yet
but I think those kind of things are just some of the comments.
Do you think a chat GPT-based Bing is a Google killer?
Oh, I don't think with that branding
they would have to call it something else or at this point, yeah.
I mean, that seemed to be the suggestion, right?
Chat GPT comes out, they're gonna take a big stake
and it was mentioned, if not in the official announcement,
it seemed to be the conjecture that it was gonna be
some tie up with Bing explicitly to target search, right?
I think there needs to be a lot more fundamental work
and we can talk about this in the future predictions
but there needs to be a lot more fundamental work
before we sort of are able to kill search
just by putting a language model, right?
Like I think that gap is not as simple as replacing something
or just augmenting existing search.
I think you would have to think about what kind of things
can language models actually do
and you still want to rely on sources and things like that
but yeah, so I think it's going to happen at some point
but it's going to be like search as a,
it won't be replacing search because it'll be a different thing,
right? Like it'll be, it's not gonna be search
because not in the way we think about search.
It literally means, yeah.
Exactly.
It'll be question-answering or it'll be something else, right?
Like it'll be a helper or whatever,
but searches may not be anything.
Well, one quick thing before we jump into predictions,
you kind of reflected on your top use case for the year
and that was code pilot.
Tell me a little bit more about how you thinking about that.
I think co-pilot came out probably not exactly
in this calendar, but I feel like it got a lot more
adoption this year and started becoming part of the tools
where people are coding and firstly,
I started using co-pilot this year,
so I'm gonna put it in top use case this year.
And I will say before notion.ai,
co-pilot was probably the only use
of large language models that I saw anywhere.
So from that point of view, it was interesting
that you could use to came out and there's nothing,
nothing that can fill co-pilot.
But from a use case point of view,
it has been incredibly useful, right?
So I've been able to, you know,
do things that has made me a lot more effective as a coder,
not that I code much, but when I do,
I want to do a lot and co-pilot has let me sort of do that.
And that's been amazing, I feel the right combination
of having a nice user interface,
having the right data that is trained on
to be able to sort of really help people
in what they want to do.
Now of course, co-pilot has issues,
it's producing, you know, code that can be dangerous,
that can be buggy, and of course,
there are the questions of copyright and plagiarism exactly.
So I feel like I hope those things will get resolved,
but those are again, when you start using a language model,
these are the issues that you have to solve.
And then I'm glad that co-pilot is bringing all of these things
into the discussion by having, by it being out there.
Yeah, I've had the same experience with it.
I think I've shared this on social or in the podcast
in a conversation.
I saw all the co-pilot demos played around with it
with kind of the toy problem things,
but I don't do a lot of coding necessarily,
but I do tend to binge on coding everyone's in law.
Like, and usually like that end of year holiday thing,
I'll have some projects, and I did that this year
and used co-pilot, it was amazing.
Like the productivity, you can, it helps,
the productivity helps create for you, attacking,
a new problem with new tools,
without the context switching of going to Google
and Stack Overflow, like it's incredible.
I'm a total believer.
Yeah, and I think that exactly is the kind of thing
I expect language models to be useful for.
They are not going to, and you know,
which at GPD going back a little bit,
people are talking about, hey, if people are going to lose jobs
and it's going to change everything,
and you know, we'll replace XYZ with Giat GPD.
And I don't quite see that happening,
but I do expect a lot of people in many different areas
becoming a lot more productive because of Giat GPD.
And co-pilot is, you know,
is an example of how language models
can make you a lot more productive without replacing,
I don't think it's replacing specific programmers,
it's just making, allowing them to do a lot more.
And that I think is the best use of technology.
Awesome, awesome.
Well, let's jump into predictions.
What are you most excited about kind of looking
into your crystal wall?
So I think the Giat GPD is the one that sort of,
everybody knew language models,
they just trained on data and making predictions.
What Giat GPD really did was remind everyone, like,
okay, even if the language modeling part is
quote unquote, salt, right?
Even if you get a really, really large language model,
that doesn't mean you're done, right?
And I think one of the biggest aspects of that was
making sure that what you're generating is not just BS,
it's somehow valid, somehow the truth,
somehow something that you can cite and rely on, right?
They definitely signed a light on how challenging that is.
Right.
Exactly.
So I don't think this is going to be a prediction
necessarily for 2023, maybe 2023 is when we'll start
seeing the first attempts at this,
but being able to generate text that's,
that's not have misinformation,
that differentiates factual from, you know,
creative hallucinations that is able to cite its sources
and sort of point to like,
hello, this is the piece of paragraph that I'm based on,
which I'm generating a piece of text.
I think those things are needed and it's probably going
to be the next aspect of language models
that's going to be a big topic of research.
Do you have a sense for where,
how we get there is it kind of applying the same tools,
RLHF, for example, attacking this specific problem,
or do you think is, you know,
we don't have the tools and it's going to need to be
kind of new invention that gets us there.
I think it's going to have to be new inventions
and I want to sort of think of it as not just,
you know, how do we attribute it to specific pieces of text,
but I kind of think of it as like being able to use
other tools, being able to use other things
available to the language model
when it's being trained as well, right?
So it should not rely on memorizing facts to any degree.
It should just rely on using existing tools,
including search, including maybe calculations,
maybe even Python, interpreter, whatever else it needs to do,
but still be able to do the language modeling task, right?
So I think there is some combination of being able
to refer to external stuff and still do language modeling
that we quite haven't quite extracted
and that would be something that I think will come into picture.
I'll give you an example of how sort of some people
have been thinking about it.
There's this whole idea of retrieval-based language modeling
where you're still generating the text
token by token, but you're always retrieving
some set of documents and you're conditioning on them
when you're generating each token.
That's sort of one step towards what I'm talking about,
where at least you're trying to look at retrieved documents
when you're generating, but that doesn't guarantee
what you're generating is actually based on.
So you just spoke earlier about the decomposed reasoning.
Is this prediction that those ideas become more real
in some way in 2324?
Is it that what we're doing with a trained model
to kind of get decomposed reasoning,
we're gonna push even deeper into the fundamental creation
of the model like at train time and other things?
Yeah, so more of the latter, right?
So right now we are expecting the model
to be able to do decouples reasoning,
but we only do it at test time in some sense, right?
Let's actually try to start thinking,
putting that stuff during training, right?
So like, again, I don't want to make this analogy too much,
but when you think about,
when you're training a human on how to do things,
you don't just give it pairs of input and output,
you give it a little bit more of a decomposition
and then based on that, they're able to do what they do.
If you want them to use the Python interpreter,
you don't just expect them to finish it on their own,
they can use the interpreter when needed, get a thing, right?
So I just think of language models as,
yeah, maybe they're still doing the language modeling task,
but they have access to a bunch of other tools.
And maybe this is more far-fetched than 2023,
but I think in the long run,
you want a system that's able to do those things.
You got your next prediction is around diffusion models.
It's kind of surprising that that term hasn't come up yet so far.
Yeah, I guess it is surprising,
but also in NLP in general,
I feel like we are barely scratching the surface
of what diffusion models can do.
So, yeah, I think,
clearly in the image generation space,
we've seen a lot of progress with diffusion models
and we've seen some in NLP, but not enough.
I guess what I find attractive about diffusion model
is that it's trying to generate more than just
a single thing at a point point, right?
So when diffusion models are applied to text,
the way it would look like is not just
producing one token at a time,
it will try to produce a whole sentence
or whatever we decide is the right guarantee.
And that idea of a model that is trained
not to do one token at a time,
but to do something bigger really appeals to me
because I feel like a lot of the issues
we talk about with language models
fundamentally come from the fact that
it's trained to do one token at a time
and sort of, and that's kind of the loss, right?
So if we can have the model be trained to generate more
and then give it a loss,
I think that's fundamentally interesting
in the previous model, so provide one way of doing it.
Do you, would you kind of visualize this
as a model like in a first iteration,
spitting out bullshit and then successively
like iterating towards truth?
I guess that one way that this could play out.
Yes, well, I mean, probably not,
probably it's gonna be somewhere in the latent space.
But I think the way I think about it is
like if we were doing this token by token thing
for images, it just wouldn't make sense, right?
Like, pretty certainly would produce the images
that we see coming out of stable diffusion and then yeah.
Or even what it's going to learn
is going to be something different,
what it's going to learn is given the image
that I have seen so far,
let me predict the next pixel or the next piece, right?
That somehow feels like a fundamentally different task
than being able to generate an image fully, right?
And so I feel like thinking about the same idea
for text just kind of makes sense,
like you write the summary in one shot
and realize how wrong it is,
feels like there's something fundamentally different
than hey, you got a bunch of tokens correct,
but you also got a bunch.
And in some sense, there are some analogies
to our LHF and using PPO for training,
for example, where you try to make sure it's fluent
and things like that.
These are all losses designed on not just token
by token basis, but something that's longer.
And so we've known how useful they've been.
So I feel like there may be something
in taking that idea and applying it to appreciating
something that's interesting, interesting.
I expect a lot of people will be wanting
to figure out how to do that.
Great.
And online updates to models.
Yeah, so I think one of the problems with language models,
so let's keep aside the grand vision
of how language models will use search
and all of these other things.
But one of the fundamental problems with language models
is that the word changes, but they don't.
And this seems to be a fundamental sort of issue
with language models, right?
So I think thinking about how we can update
language models every month or every week or every day,
I think is an interesting problem to be thinking about
and becomes increasingly relevant,
where Bert doesn't know anything about COVID,
so it's not useful for a bunch of applications,
even though otherwise fundamentally,
there's nothing wrong with it, right?
That kind of stuff is just not fun.
And I think there would be research on trying to fix that.
What's the current, not necessarily state of the art,
but the current approach for doing this at the scale
of a GPT-3, is it collect more data
and retrain from scratch?
Or how did they approximate or approach
some kind of incremental training ability, if at all?
Yeah, so there hasn't been that much work on that front,
I would say, this is something that needs a lot more attention.
Yeah, but I think there'd be parameter efficient training
on how can we slightly improve the change the model,
but not completely change it.
Find the set of parameters that we should update,
so that it's not updating the whole parameters,
but updating a little bit of it,
things like that I feel are around,
but it needs a lot more work.
You kind of, one way to think about the fundamental problem
is with the transformer,
it's not like a layered architecture, like a CNN,
where you can just chop off the N layers and retrain
from that point, it's just a much more complex
and interconnected model,
so that kind of incremental updating doesn't work.
Not so easily, yeah, I think there's been some work
on sort of taking like one percent of the parameters
sort of spread over the transformer
and updating them with new text,
but I think solving this problem
is going to be something that needs to happen pretty quickly.
And so to be clear, taking a step back,
like this is all the looking forward section,
those three things, kind of misinformation
and attributable generations,
diffusion models, and online updates
for specifically in your category
of the greatest, most exciting opportunities in the field.
Areas where we're likely to see a lot of research attention
and possibly some really interesting results
coming up in the next year or two.
And also sort of fundamental problems
that need to be addressed by language models.
And so that brings us to your top three predictions
for the field's proper, what do you see there?
Yeah, so I think, and maybe some of it is little
with a disappointment as well.
So the first one here is multiple modalities.
I think there's been a lot of exciting work,
so I don't want to sort of think that away.
But to me, after GBT3 came out and then you saw a clip
when Dali and Whisper and now there's video models
and things like that, to me fundamentally,
I don't understand technically why they're not the same model,
but it's still a little bit disappointing
that they're not the same model.
It's like, why is there not the same model
that change over the same data?
GBT3 is trained on, but also on the Lyon dataset
that does all the images and text and audio
and video and stuff like that, right?
And I think this is a sort of near future prediction
is that we are going to see ways for pre-gaining models
that cuts across multiple modalities.
And I think clip was a good example,
sort of early example of what you can do
when you have a lot of text and images.
But I think it still didn't have access
to a lot of text-only data.
And I want a model that can do chat GBT-like things,
but also generate images for me and maybe read them out
and things like that, right?
So I feel like multiple modalities is an exciting sort of
kind of an opportunity, but definitely something
that's going to happen soon.
When I first heard you describe this,
I thought, well, multi-modal, that was the big thing
we were talking about in these trends, conversations last year,
but you're going a level deeper.
You don't want multi-modal use cases or outputs.
You want a single architecture to do multi-modal things.
That's what I want.
My prediction is going to be a little bit more grounded,
so to say, but yeah, like video, for example,
is a more concrete one.
Like text to video, we've seen some initial versions
of those that's probably where a lot of initial stuff would go in.
But I've been really excited about sort of the mind dojo
world of like playing with text and Minecraft
and having an agent that can do a bunch of things
in Minecraft.
I feel like there are things that modules can learn
from images, even for language modeling,
it would benefit to see a lot of images in some sense.
There are just a bunch of things in images
that we never talk about in text.
So from an AI agent, I think it's useful to think about something
that has access to everything.
But yeah, more concretely, we're just going to be pushing them
sort of fair-wise.
Like, yeah, it's going to be audio and images.
And there's going to be a bunch of other pairs that will happen first.
But eventually, I think having multi-actual multiple
modalities, not just greater than one, but altities would be exciting.
Awesome, awesome.
Next up.
Next, I'm kind of excited about better training and better
inference and better in the sense of being
more computationally efficient.
I think this is an exciting work that a bunch of people
are already doing.
But I think this is just going to become increasingly
important from a sustainability point of view,
but also from university surviving and doing interesting things.
And small companies are contributing to research.
I think it's important to be able to train these models,
to be able to run these models.
And there's going to be a lot of research
in trying to do those things.
And you've got a few examples that we'll link to in the show notes.
Any anything that you want to point out?
Yeah, so let me mention two that I saw recently.
One of them is this paper called Cramming.
And the idea here is to think about the scaling laws paper,
like, hey, what can you do when the models get larger
and stuff like that?
The Cramming paper sort of turns it on his head
and decides, OK, what if I have just one GPU for one day?
What's the most I can do with that?
And it's a very different question,
but it somehow is a lot more relevant to many more people,
because a lot more people have a single GPU for a single day.
And they show that you can get almost sort of bird level
performance if you make the right choices
and this sort of detail what those choices might look like.
It's a paper, but I think I like that idea of like, hey,
what if we were scrappy about training these models?
How far can we get?
I think that's a very interesting question
that Google and OpenAI is not going to be asking,
but might be relevant for a lot of other research.
So the other one I want to talk about
is this Petals work that came out of the big science thing.
I haven't read too much about this,
but it seems like a really interesting idea
of the problem of running really large language models.
So even if OPT releases 175 billion model,
how do you actually run it?
It doesn't really help most people,
even if you have a big cluster, it's kind of difficult to run it.
So what this Petals does is they're
building this framework for using the ideas
behind BitTorrent of sort of distributed computing
and bringing it to language models.
So like, hey, you should be able to run
these 100 billion size language models,
distribute it over a bunch of commodities
of consumer computers.
So yeah, I think this is an interesting idea.
I haven't played around with it to see how far you can push it.
There's partly, you need a bunch of people also running Petals.
But once we get there, I think that could be a pretty exciting way
to run language models.
Interesting, interesting.
So your third prediction is editing and revising models.
What do you mean there?
So these are these family of models that
are not so much interested in generating text,
but taking existing text and editing it.
And I think this is a very interesting idea
that can become increasingly important.
And in some sense, this could be the way
you fix language model output, but actually,
is to have another model that takes the output of the language model
and fixes it.
So some of the work here, there was a paper out of Julias.
A group from YouTube now that sort of looked at summarization.
And there are systems that generate summaries.
How can you take the generated summaries and edit it
to correct all the factual mistakes it has made?
And editing is somehow much, let's not say definitely
a simpler problem, but it's a, in some sense,
it could be a simpler problem than writing the whole summary
from scratch, especially when you do the writing,
you do left to right generation.
So you can't go back and revisit something
that you've done before.
With these editing models, they have the whole picture
to some degree, and all they have to do
is fix it so that the picture is consistent.
And so this idea seems like potentially simpler than generation.
So you could generate something, and maybe this
is also attached to diffusion models,
where you write something that's maybe not so correct,
but you revise it, and it becomes better.
So there is a bunch of work along these directions
that came out essentially this year,
and maybe second half of this year.
Some of it early on, that tries to gather data sets
where you have edits, or try to maybe even generate
data sets where you have edits, and create these models
that are able to fix those edits in some sense.
And so the prediction specifically
is that teams will build on this and produce models
that can actually kind of deliver on the ability
to do editing and revising.
And I think this could be, for example,
there'll be an editing model that can fix bias issues.
There'll be an editing model that fixes toxicity.
There'll be an editing model that fixes factuality.
And these editing models can make web searches
and sort of take that information and edit the output.
So I could imagine that this could be a practical way
of solving many of the issues.
It is a really interesting idea that,
I don't know if it's like a separation of concerns
or something like the language model
doesn't necessarily need to do everything
if we can compensate.
So in a way, it's like decomposition as well.
Like, let it generate, and if the way
to get something that's not toxic that's accurate
is to have another type of model support it.
Great.
Yeah, I think that's right.
And for at least for summarization
and things where it's supposed to be factual and stuff
like that, I could see it sort of addressing those problems.
Of course, if it's generating a long text
and there are longer range sort of consistency issues
and stuff like that, it might be a little bit difficult
for editing models to come into feature there.
What I like about editing is also,
it's something that we can imagine not only working
on language model output, but working on a human output
or a text that's been written with the writing assistant
and things like that, right?
You can still go back and do a post-processing editing step
to polish it up and I think that would be great as well.
So our last category in the NLP predictions
is top people's companies, organizations, teams
to watch in the field 2023.
Of course, the caveat of you're not any emissions here
are not just like the work of any particular team,
but who's got your mind share
and who are you expecting to see interesting things
from in the upcoming year?
Yeah, so this has been a little bit difficult
question I think every year, but one thing I will say
and this is maybe the most obvious answer
is to keep an eye on OpenAI and what they're up to, right?
I think once they do something,
people always come back and say,
look, what they've done is not so exciting,
oh, they only scale it up or oh, they only did this
and this new thing.
But the fact is that they are the first ones to do it,
that the first ones to bring it out, make it available
and that is, and get people excited
about language models in a way that they weren't before,
so that happened with GPT-2, GPT-3 and JGPD
and I'm sure GPT-4 will have the same thing.
I'm sure retroactively we will all talk about
what the problems with GPT-4 are
and how it's incrementally only training on more data
or has more parameters or whatever it is,
but I think qualitatively it'll bring something
interesting to the table and I'm really curious
about what that next interesting thing is going to be.
Do you think the general predictions
that are kind of floating around,
basically spring and 100 trillion parameters?
Is your money on those?
I mean, to sort of have a completely different perspective,
I think this is also a nice model
that came out, this nice paper that came out
a little earlier called the Chinchilla paper.
This was a paper that showed that these models
are extremely under-trained and they are data hungry.
So one version of GPT-4 could be potentially
not even a different architecture,
not even more parameters, like exactly,
let's keep it 175 billion and let's just somehow
get 10 times the data if you can potentially get
that spot somewhere, right?
I could totally use it.
But then everybody that shared the image
with the little dot and the big dot would be totally wrong.
Well, yeah, they'll just sort of replace that with data
and it might still be true, right?
For those not on Twitter, that is dominated LLM Twitter
over the past couple of days.
Is there even,
I think that when GPT-3 came out,
the kind of colloquial articulation
of what they did was like train this language model
on the entire internet.
Like, is there 10X more data to train on?
Yeah, I don't know how much they've trained on
and how much there is.
I mean, there's definitely 10X more data.
There is a lot of stuff on the proprietary, right?
proprietary.
Maybe even proprietary, right?
Like, transcribe a bunch of videos and audio and books
and I guess, yeah.
They do have that whisper model
that that's really good transcribing.
So they could use that.
They didn't create that for no reason.
Right, yeah.
They also can go into scientific papers
and like, I don't think the 48 million
that I'm papers that Galactica was trained on
was something GPT-3 was trained on.
And I think that is a pretty valuable resource.
That Galactica paper also showed that even
on mathematical reasoning and things like that,
they were actually better.
So these scientific papers may be useful
for a bunch of other things than that we don't realize.
So yeah, I think where that data comes from is unclear to me,
but it's clear that more data is somehow
maybe even more interesting than more parameters.
And more data could include more RLHF style things, right?
Like, I don't know what to open it, I guess.
Okay.
The other company too, I would say again,
continue taking a look at is hugging face.
I've been constantly sort of amazed by how much
they've been doing.
One of the sort of key insights is like E-MNB,
which is the stock conference in NLP
has this demo track where they highlight sort
of not research papers but products of demos
that are relevant for research.
And for the last three years, I think,
at E-MNB, hugging face has got the best demo paper award.
And that's that kind of thing sort of shows
how they've been doing very different things,
but also doing things that are impactful and interesting.
So the two, I want to highlight this year is,
again, they've done many, many things,
but the one I want to highlight is the evaluate system
where they had this whole evaluation framework
for reproducing evaluations and evaluating models
and making all of this stuff really easy.
So you can introduce a new metric,
evaluated in thousands of models, and so that.
Make it really easy to compare models,
make it really easy to reproduce papers.
And I think that that's a really valuable service
to do research.
And the other one that I sort of we also started with this
of like, hey, what's happening inside the pre-training data?
One of the tools they have is this roots search tool
that takes the roots pre-training data,
but allows you to search it and find all kinds of things
that are happening inside that pre-training data.
So if you have a specific prediction,
then you want to be like, hey, is there anything
in the training data that looked exactly like this?
You can do that search and get some results.
So I think they are just being pretty creative
and thoughtful about what is useful and building tools.
And that's been exciting.
And the last one that I'll bring up, and this is something
that was on top of my head this week
with it can change.
It's a group called Ott.
It's o-u-g-h-d.
I believe it's Ott.org, it's a website.
And this is sort of a research nonprofit.
And they've been doing sort of interesting things
related to sort of building tools.
So they have this tool called Primer.
And this is going back to decomposed reasoning.
This tool called Primer, sort of you can give it a question
and it tries to come up with an answer.
But in the process of coming up with an answer,
it can do a web search or it can write a small program
and it can do all of these things.
And they're very sort of nice tool
to be able to visualize what the decompositions are
and what sort of things are being done.
So it's a really interesting use case of language models.
And then they also have another tool called Elisit,
which is in some sense, it's a little bit like Galactica,
but it's not so much interested in generating papers for you,
but helping you do research for your paper.
So you have a specific question.
It's going to find a bunch of relevant papers,
take out snippets from those papers,
and be able to do that.
So I don't know, they've had a bunch of tools
that when I'm looking at decomposed reasoning,
it comes up and I'm looking at, OK,
assist, select, assist, it sort of comes up.
And so it's been interesting to see you
and I'm curious what those will be next.
I'm really curious about that.
And I'm going to look into that in more detail.
Awesome, awesome.
Well, I think we are done.
Like, you've been a champ.
This has been awesome.
It's been fun, yeah.
Yeah, now, I mean, you rose to the occasion
of kind of capturing an amazing year in NLP, for sure.
So thanks so much for joining us.
Yeah, thanks for inviting me.
I think the time sort of justifies
how much this year had in NLP this year.
And I'm really curious to see where NLP is going to go.
I will mention that ChatGPD came out right before,
or I think maybe even during Eurips.
So I attended Eurips.
And I saw the first-hand experience
of the whole machine learning community there.
Then I flew to Abu Dhabi to attend the NLP.
And that's where I saw the reaction of the whole NLP community.
And it's been interesting to see sort of how the reactions
have sort of spanned both optimism and excitement,
which is kind of where I am, like to see like,
hey, what can we bear with this stuff?
To pessimism where they're like, oh,
it doesn't really, yeah, it's not going to change anything.
It's just a bigger language model.
All the way to essentially, I want to say
some form of denial, where it's like,
look, it's behind a proprietary closed-off system.
And therefore, it doesn't matter to research.
And that's definitely not the day I agree with.
So yeah, it's been exciting.
And there's also a fourth, which maybe is less so.
And I don't know, maybe less so in the research community
than in the general sphere, which is fear of the implications
of it.
Did you find out less so on the research side?
I guess less so, definitely less so on the, yeah.
Because I think we've been, there is
a little bit of fear becoming a little bit more obvious.
But I think the community, because of a lot of people
who've been sort of pointing out problems
in the bit of life, like the models for a while,
we are kind of, we know what not to.
As a community, we should know what not to do.
But it is a little bit scary, where
people are using it for things that,
clearly, at the onset, should be like,
hey, why are you doing this straight?
Yeah, yeah, yeah.
Awesome.
Well, once again, Tamir, thanks so much.
Really great session and conversation.
And I appreciate all the work you put into prepping for it.
Yeah, thank you so much, it's fun.
