WEBVTT

00:00.000 --> 00:16.320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

00:16.320 --> 00:21.480
people, doing interesting things in machine learning and artificial intelligence.

00:21.480 --> 00:34.320
I'm your host Sam Charrington. For those challenged with promoting the use of machine learning

00:34.320 --> 00:40.760
in an organization and making it more accessible, a key to success is to support data scientists

00:40.760 --> 00:46.320
and machine learning engineers with modern processes, tools and platforms.

00:46.320 --> 00:51.480
This is a topic we're excited to address here on the podcast with the AI Platforms podcast

00:51.480 --> 00:56.000
series that you're currently listening to, as well as a series of e-books that we'll

00:56.000 --> 00:58.080
be publishing on the topic.

00:58.080 --> 01:02.760
The first of these e-books takes a bottoms up look at AI platforms and is focused on the

01:02.760 --> 01:08.160
open source Kubernetes project, which is used to deliver scalable machine learning infrastructure

01:08.160 --> 01:12.840
at places like Airbnb, booking.com and open AI.

01:12.840 --> 01:17.640
The second book in the series looks at scaling data science and ML engineering from the top

01:17.640 --> 01:23.720
down, exploring the internal platforms, companies like Airbnb, Facebook and Uber have built,

01:23.720 --> 01:26.600
and what enterprises can learn from them.

01:26.600 --> 01:31.280
If these are topics that you're interested in, and especially if part of your job involves

01:31.280 --> 01:36.880
making machine learning more accessible, I'd encourage you to visit twimbleai.com slash

01:36.880 --> 01:44.640
AI platforms and sign up to be notified as soon as these books are published.

01:44.640 --> 01:49.800
In this episode of our AI Platforms series, we're joined by Lima Nasseri, senior engineering

01:49.800 --> 01:54.000
manager and head of the recommendations team at Comcast.

01:54.000 --> 01:58.520
Lima spoke at the strange loop conference a few months ago on resurrecting a recommendations

01:58.520 --> 01:59.920
platform.

01:59.920 --> 02:05.040
In our conversation, Lima and I discuss how she and her team resurrected Comcasts Xfinity

02:05.040 --> 02:11.760
X1 recommendations platform, including rebuilding the data pipeline, the machine learning process,

02:11.760 --> 02:15.320
and the deployment and training of their updated models.

02:15.320 --> 02:20.400
We also touch on the importance of A-B testing and maintaining their rebuilt infrastructure.

02:20.400 --> 02:24.120
This is a very fun interview, which I know you'll enjoy.

02:24.120 --> 02:26.280
And now on to the show.

02:26.280 --> 02:32.120
Alright everyone, I am on the line with Lima Nasseri.

02:32.120 --> 02:35.640
Lima is a senior engineering manager at Comcast.

02:35.640 --> 02:38.640
Lima, welcome to this week in machine learning and AI.

02:38.640 --> 02:40.800
Hi Sam, thanks for having me.

02:40.800 --> 02:44.800
Also, why don't we get started by talking a little bit about your background.

02:44.800 --> 02:52.000
You've been at Comcast for nine years, and as of about 14 months ago, you have been leading

02:52.000 --> 02:54.640
the recommendations group there.

02:54.640 --> 02:57.600
What are you doing in and around recommendations at Comcast?

02:57.600 --> 02:58.600
Yeah, sure, definitely.

02:58.600 --> 03:04.360
Yeah, so I've been here for a while, most recently, so the past 14 months, like you've said,

03:04.360 --> 03:06.840
I switched over to leading the recommendations team.

03:06.840 --> 03:09.800
So when I did that switch, it was a really small team.

03:09.800 --> 03:13.040
Other companies, their recommendations team, or really their personalization team, is

03:13.040 --> 03:17.800
one of the most important and biggest part of their content discovery platforms or content

03:17.800 --> 03:18.800
discovery teams.

03:18.800 --> 03:21.920
At Comcast, that wasn't quite the case.

03:21.920 --> 03:24.080
About 14 months ago, it was about three people.

03:24.080 --> 03:29.920
The main focus, unfortunately, at the time was just training a model and then deploying

03:29.920 --> 03:35.120
that model in production to surface predictions or to surface what people should watch on

03:35.120 --> 03:37.480
cable in one way or another.

03:37.480 --> 03:42.320
It was a one size fits all model, so essentially, we had one model that did everything.

03:42.320 --> 03:47.520
So regardless of the context in which the quote unquote predictions were being served,

03:47.520 --> 03:52.680
we still use the same model, and we didn't tweak it as much as we should, based on where

03:52.680 --> 03:58.400
it was being surfaced on X1, X1 being our content discovery platform for cable.

03:58.400 --> 03:59.560
So anyway, so it was a small team.

03:59.560 --> 04:02.960
So it was pretty much like an engineer on the team, less management, more engineering,

04:02.960 --> 04:07.480
which was fun for me, and it's tripled in size since then, which is great.

04:07.480 --> 04:12.920
But the goal of when I switched over to that team was to essentially resurrect the recommendations

04:12.920 --> 04:19.200
platform, which sounds kind of morbid, but I like the eliteration, so that's the name

04:19.200 --> 04:22.240
of my talk, strangely, resurrecting a recommendations platform.

04:22.240 --> 04:23.240
Nice.

04:23.240 --> 04:30.080
And so the focus of your talk is, what, how you kind of went in, a commando style and

04:30.080 --> 04:36.480
like, essentially, essentially, so the focus of the talk is really building a end-to-end

04:36.480 --> 04:38.440
machine learning platform.

04:38.440 --> 04:42.240
And coming from my background, I majored in computer science, and then I didn't obviously

04:42.240 --> 04:43.960
start in AI and machine learning.

04:43.960 --> 04:48.200
I instead started in the web service tier, so I built web services that got millions

04:48.200 --> 04:54.000
of requests per day, as you can imagine, like we get the scale at Comcast is pretty high.

04:54.000 --> 04:58.440
So having that background of building web services and platforms that scale really helped

04:58.440 --> 05:04.160
this team to go from simply having this really old school platform where it was a bunch

05:04.160 --> 05:09.480
of machine learning jobs that ran on physical infrastructure that took hours and hours to

05:09.480 --> 05:15.200
complete, to building that to be more event driven and real time updates, and also like

05:15.200 --> 05:20.760
owning the end-to-end data collection, you know, requests from the client, everything.

05:20.760 --> 05:22.080
That's essentially what we did.

05:22.080 --> 05:27.160
We went from owning just the model to owning, surfacing the recommendations, training the

05:27.160 --> 05:30.520
model, evaluating the model, and then having multiple models in production.

05:30.520 --> 05:35.840
I thought it was interesting when you describe the world that you walked into when you

05:35.840 --> 05:40.760
transitioned into this role as, unfortunately, we just had a model and we're making some

05:40.760 --> 05:45.560
recommendations with machine learning, like a lot of companies are, you know, trying

05:45.560 --> 05:46.560
to get to that.

05:46.560 --> 05:47.560
Right.

05:47.560 --> 05:48.560
That's a really good point.

05:48.560 --> 05:54.080
I think the unfortunate aspect of that was, as an engineer, the code and the infrastructure

05:54.080 --> 05:56.960
was not where I wanted it to be.

05:56.960 --> 06:01.560
For example, and you'll hear this in so many machine learning talks where they talk about

06:01.560 --> 06:04.840
the end-to-end, the infrastructure is probably the most important part.

06:04.840 --> 06:13.160
And at the time, we had like a 13-node cluster running community addition of our Hadoop distribution.

06:13.160 --> 06:16.440
So if there were any problems, we couldn't call anyone.

06:16.440 --> 06:21.320
And again, it was MapReduce, which, you know, one significant benefit of MapReduce is

06:21.320 --> 06:22.320
all I owe.

06:22.320 --> 06:24.400
So as long as you have space, you really fail.

06:24.400 --> 06:26.960
But again, that meant that the jobs ran forever.

06:26.960 --> 06:32.520
I mean, we had a job that we recently deprecated, which was exciting, that literally took 18

06:32.520 --> 06:33.520
hours.

06:33.520 --> 06:38.840
So it's still responsibility was to sessionize data so that it could feed our model.

06:38.840 --> 06:39.840
Wow.

06:39.840 --> 06:40.840
Yeah, 18 hours.

06:40.840 --> 06:44.240
I mean, imagine what you can get done in 18 hours, and this job still wasn't done.

06:44.240 --> 06:45.240
Wow.

06:45.240 --> 06:46.240
Right.

06:46.240 --> 06:50.280
So walk us through this process of resurrecting this recommendation platform.

06:50.280 --> 06:51.280
Yeah.

06:51.280 --> 06:52.440
That sounds good.

06:52.440 --> 06:57.280
So kind of in my mind, when I first took this on, I broke it down to four main parts,

06:57.280 --> 06:58.520
the data platform.

06:58.520 --> 07:02.120
So we wanted to build a proper data pipeline to consume the data into our ecosystem so

07:02.120 --> 07:06.280
that we could easily aggregate those usage events of what customers are doing on our

07:06.280 --> 07:09.040
platform so that it could feed a model.

07:09.040 --> 07:13.520
And you'll ask anyone that does anything with machine learning, the data is sometimes

07:13.520 --> 07:15.520
the hardest part.

07:15.520 --> 07:20.160
Sometimes it actually hurts me to hear, I'll hear researchers at Comcast say that they're

07:20.160 --> 07:21.160
working on a model.

07:21.160 --> 07:25.360
And then I hear them talk about the pain that they take into creating the training data

07:25.360 --> 07:26.360
set.

07:26.360 --> 07:27.800
So that's what we were trying to abstract.

07:27.800 --> 07:29.960
So the data platform was the first part.

07:29.960 --> 07:36.320
And this is without going too deeply into this part before you walk through them.

07:36.320 --> 07:46.040
I'm imagining that much of what you're dealing with is kind of the traditional telco, cable

07:46.040 --> 07:51.600
service provider type of data sources, provisioning systems, and that kind of thing.

07:51.600 --> 07:53.600
Or are you dealing with any of that stuff or is this?

07:53.600 --> 07:54.600
Definitely not.

07:54.600 --> 07:59.640
No, it's actually, I think it was a lot of, it's what people are doing on the platform.

07:59.640 --> 08:03.320
So it's just the interactive side, you don't have to talk about that.

08:03.320 --> 08:07.560
Yeah, like what they're clicking on, what they're watching, what they're not watching,

08:07.560 --> 08:10.120
what they get surfaced, but then are ignoring.

08:10.120 --> 08:11.880
So it's our content discovery.

08:11.880 --> 08:14.040
So which do they watch on TV?

08:14.040 --> 08:15.040
Some more of that side.

08:15.040 --> 08:16.440
Definitely not the old school cable side.

08:16.440 --> 08:17.440
Okay.

08:17.440 --> 08:20.240
I try to stay away from that part.

08:20.240 --> 08:24.480
So yeah, so breaking it up into four pieces, there's the data, the data tier.

08:24.480 --> 08:27.080
And then there was just orchestrating the machine learning aspect.

08:27.080 --> 08:31.800
So easily orchestrating the key steps that are obviously required to build a model and

08:31.800 --> 08:33.960
then foresee that model into production.

08:33.960 --> 08:38.160
That was actually one of my goals which we achieved was to have more than one model.

08:38.160 --> 08:43.560
And that's not for a big company like this, that's not a crazy thing to have, to have

08:43.560 --> 08:49.560
a model that's fine tuned to suggest content that is available for rent versus having a

08:49.560 --> 08:54.880
model that's fine tuned for suggesting music recommendations for customers that have

08:54.880 --> 08:58.600
a higher propensity to watch music videos on on Comcast.

08:58.600 --> 09:04.160
So orchestrating the process and training, evaluating, and then of course deploying.

09:04.160 --> 09:07.680
The model was the second part that we had to do because previously we lived in a world

09:07.680 --> 09:11.280
where we just had one and we just had to make sure we had to keep the lights on that

09:11.280 --> 09:15.720
one machine learning, like quote unquote, that one.

09:15.720 --> 09:19.640
So yeah, so the ability to experiment and deploy was the second thing that we wanted

09:19.640 --> 09:20.640
to tackle.

09:20.640 --> 09:23.240
And the third part was AB testing.

09:23.240 --> 09:29.000
So before I switched to the recommendations team, I actually worked on a big data platform

09:29.000 --> 09:31.240
which I could use that experience for this team.

09:31.240 --> 09:32.960
So it worked out fairly well.

09:32.960 --> 09:36.600
But while also on that team we built an AB testing platform which meant we essentially

09:36.600 --> 09:40.160
created a system that tagged accounts.

09:40.160 --> 09:44.440
And then if you tag like or if you tag users then you should be able to give them different

09:44.440 --> 09:49.000
types of experiences based on that tag, you know, super high level, AB testing logic.

09:49.000 --> 09:54.000
But what was ironic is we built that and then we had nothing to AB test.

09:54.000 --> 09:59.120
So the luxury of moving to the recommendations platform is there's so much to AB test.

09:59.120 --> 10:01.800
So I pulled that over with me when I made this switch.

10:01.800 --> 10:05.960
And then last but not least was the infrastructure problem that I mentioned earlier.

10:05.960 --> 10:09.440
As we grew as a company, the infrastructure did not grow.

10:09.440 --> 10:14.640
So the more customers that we were getting onto the X1 platform, our infrastructure stayed

10:14.640 --> 10:15.640
the same.

10:15.640 --> 10:19.640
So that's somewhat mind boggling if you think about that.

10:19.640 --> 10:24.280
Like the more data you get, the more you need to scale out your platform.

10:24.280 --> 10:29.280
And one of the changes we made to support that was of course moving to the cloud.

10:29.280 --> 10:34.120
So yeah, at high levels I think those were the four key pieces needed to resurrect a

10:34.120 --> 10:35.440
recommendations platform.

10:35.440 --> 10:36.440
Okay.

10:36.440 --> 10:37.440
Nice.

10:37.440 --> 10:38.440
Well, let's dig in.

10:38.440 --> 10:39.440
Yeah.

10:39.440 --> 10:40.440
We can start at the beginning.

10:40.440 --> 10:43.320
The data platform building out this data pipeline.

10:43.320 --> 10:49.120
We're starting with interaction data, click stream data, that kind of thing.

10:49.120 --> 10:54.640
What do you need from a pipeline perspective to support building these kinds of models

10:54.640 --> 10:55.640
of scale?

10:55.640 --> 10:56.640
Right.

10:56.640 --> 11:00.400
So the biggest issue that we saw with what I call the legacy platform, which is the one

11:00.400 --> 11:04.920
that ran on the physical infrastructure, the MapReduce specific platform.

11:04.920 --> 11:11.360
What I noticed was that we had so much code to get the data in the right format.

11:11.360 --> 11:14.800
And that's I'm sure you'll hear that all the time with machine learning.

11:14.800 --> 11:17.880
It's always getting the data in the right format at the right time.

11:17.880 --> 11:20.040
So what we did is we decided to go the serverless route.

11:20.040 --> 11:25.280
So we have a battalion of Lambda functions that consume the events in real time.

11:25.280 --> 11:30.640
So as the event happens, we eventually consume that into our into our data platform.

11:30.640 --> 11:35.760
And then right at that moment, we transform it to meet the specific requirements of the

11:35.760 --> 11:36.760
platform.

11:36.760 --> 11:41.240
So, fortunately, at Comcast, and I know this is a problem outside of Comcast, allow

11:41.240 --> 11:43.720
the data is originally in plain text.

11:43.720 --> 11:47.720
And plain text obviously doesn't play well with any data processing framework.

11:47.720 --> 11:52.360
So the first step typically is transforming those into a proper schema, which is usually

11:52.360 --> 11:55.000
Avro or some binary format.

11:55.000 --> 11:58.720
And then once we have that, then we start enriching the data so that it has more context,

11:58.720 --> 12:03.640
like simple things, like if they've watched this TV show in rich, it was some metadata.

12:03.640 --> 12:10.200
This TV show is the genre or the year it was made or what language it was in.

12:10.200 --> 12:15.280
And then once we've enriched it with that metadata, the next Lambda function was aggregating

12:15.280 --> 12:17.280
it for the user.

12:17.280 --> 12:21.720
So it was kind of, the goal was to build this seamless event-driven platform that we could

12:21.720 --> 12:25.000
kind of close our eyes and it would just do what we expected.

12:25.000 --> 12:26.400
It wasn't like scheduled.

12:26.400 --> 12:32.240
It wasn't based on scheduling jobs and some ask a ban or oozee or some job scheduler

12:32.240 --> 12:36.920
was, when the event happens, something is triggered and it goes through the process.

12:36.920 --> 12:40.600
I really was not expecting to hear serverless come up in this conversation.

12:40.600 --> 12:42.440
That's really, sorry about that.

12:42.440 --> 12:48.080
I mean, typically pipeline, you know, we're talking about like ETL or workflow engines,

12:48.080 --> 12:50.640
like Airflow or something like that.

12:50.640 --> 12:54.120
This is super interesting that you want the serverless route.

12:54.120 --> 12:58.840
When you are ready in AWS for this application.

12:58.840 --> 13:02.840
No, we weren't at all, which was, it took far longer than I thought, because again,

13:02.840 --> 13:07.240
it wasn't like a lift and shift to AWS, this is our first introduction, specifically

13:07.240 --> 13:09.800
my, at least my first introduction to AWS.

13:09.800 --> 13:14.160
So the learning curve was somewhat steep, but it really paid off.

13:14.160 --> 13:15.360
And I mean, you mentioned Airflow.

13:15.360 --> 13:20.480
So we do utilize Airflow for the second part of the four key parts to building a machine

13:20.480 --> 13:22.320
learning platform.

13:22.320 --> 13:25.200
And that was for the orchestration of when we do what.

13:25.200 --> 13:29.200
But yeah, so I kind of, when I hear the word ETL, it kind of, I get like, it kind of

13:29.200 --> 13:33.560
irks me because it's so much more than ETL, you know, if you build a data platform that,

13:33.560 --> 13:39.240
you know, is avoiding the construct of a pipeline jungle, it could be like somewhat kind

13:39.240 --> 13:40.840
of beautiful, you know.

13:40.840 --> 13:47.040
How specifically does serverless help you avoid the pipeline jungle?

13:47.040 --> 13:48.680
Oh, yeah, that's a good question.

13:48.680 --> 13:54.040
Well, specifically in the Lambda route, what was nice about Lambda's is that very, very

13:54.040 --> 13:59.160
well-managed in AWS, meaning like when, so obviously, data flux should work.

13:59.160 --> 14:04.680
So it's depending on the day of the week, when it comes to content consumption on TV, you'll

14:04.680 --> 14:08.080
see Sundays and Thursdays, the usage is far higher.

14:08.080 --> 14:11.560
So Lambda's scaled in and out for us, which was awesome.

14:11.560 --> 14:17.240
And just the ability to have them be triggered based on some other, like, some other action

14:17.240 --> 14:21.400
was also critical, which so that, yeah, so that's the route.

14:21.400 --> 14:27.680
That's the reason why I think serverless in the Lambda context worked for this data pipeline.

14:27.680 --> 14:33.440
Now, I haven't done a ton with serverless hands-on, but the attempts that I've made

14:33.440 --> 14:38.240
to play with it, it just seemed like a configuration mess.

14:38.240 --> 14:41.800
It was a little bit actually, you know, hard to kind of wrap my head around, like how

14:41.800 --> 14:48.480
you manage these kind of function artifacts, you know, the way that you would manage, you

14:48.480 --> 14:53.240
know, version control and, like, how you tie everything together, like, was there a big

14:53.240 --> 14:54.560
learning curve either?

14:54.560 --> 14:55.560
There was.

14:55.560 --> 15:00.880
So we utilized Terraform, and that helps the ton Terraform and Jenkins to handle the whole

15:00.880 --> 15:06.360
version control and putting the artifact into a place where AWS could reference it.

15:06.360 --> 15:11.960
But I think the incorporation of Terraform made everything that you said a lot easier.

15:11.960 --> 15:12.960
But it did take its time.

15:12.960 --> 15:14.400
We didn't start with Terraform.

15:14.400 --> 15:18.920
We started with, you know, just AWS CLI and it was a big mess.

15:18.920 --> 15:20.560
So going to Terraform route helps a ton.

15:20.560 --> 15:22.080
Now, that's kind of blowing my mind.

15:22.080 --> 15:28.440
And I think of Terraform as like laying down images and containers and stuff like that.

15:28.440 --> 15:30.200
What does that have to do with serverless?

15:30.200 --> 15:33.480
I mean, so you could deploy your land of functions using Terraform, right?

15:33.480 --> 15:36.600
Or you could configure them using Terraform.

15:36.600 --> 15:37.600
Okay.

15:37.600 --> 15:42.400
So yes, I mean, a lot of what we do, it utilizes, like, the goal is never to put anything

15:42.400 --> 15:46.160
in AWS without using Terraform in one way or another.

15:46.160 --> 15:47.680
Got it.

15:47.680 --> 15:53.640
So your functions are essentially configuration, that's the point and you're using Terraform

15:53.640 --> 15:58.800
as configuration management and it's managing the like battalion of functions that need

15:58.800 --> 15:59.800
to be triggered.

15:59.800 --> 16:00.800
Yeah.

16:00.800 --> 16:01.800
Right.

16:01.800 --> 16:06.000
I'm kind of trying to figure out how far to geek out on serverless, like, how far you use.

16:06.000 --> 16:11.880
How did you get your developers comfortable from a tooling perspective with Lambda functions?

16:11.880 --> 16:17.240
Like, is it to the point now where there's kind of IDE support or it's?

16:17.240 --> 16:21.880
So I think the biggest struggle we've had is testing changes locally.

16:21.880 --> 16:24.160
I think that and we're still struggling with that.

16:24.160 --> 16:26.000
I know there are frameworks out there that we could use.

16:26.000 --> 16:30.600
We just haven't had the, you know, time or the resources to experiment with them.

16:30.600 --> 16:35.520
But at some point, we were essentially deploying and making changes in production in our,

16:35.520 --> 16:37.000
you know, quote unquote production platform.

16:37.000 --> 16:40.800
We did live in a space where we had both the legacy and the production at the same time.

16:40.800 --> 16:46.200
So that was a kind of a luxury in that we can make changes to our data pipeline.

16:46.200 --> 16:50.120
There are all these Lambda functions in our AWS production platform.

16:50.120 --> 16:53.800
And then if any, if we mess anything up, we could always rely on our physical infrastructure

16:53.800 --> 16:57.600
to copy the data that we may have, you know, dropped because we made a mistake.

16:57.600 --> 17:01.680
But yeah, I think the biggest struggle was being careful for what we did.

17:01.680 --> 17:03.840
And also the other biggest role was the cost.

17:03.840 --> 17:09.920
I'm there was this one time where we enabled CloudWatch to test one of our Lambda functions.

17:09.920 --> 17:12.000
And we had it on debug mode.

17:12.000 --> 17:16.800
And unfortunately, like after just like three or so hours, it resulted in 20K.

17:16.800 --> 17:17.800
Wow.

17:17.800 --> 17:18.800
Expenses.

17:18.800 --> 17:19.800
Yeah.

17:19.800 --> 17:20.800
It's a lot.

17:20.800 --> 17:21.800
It's a ton.

17:21.800 --> 17:22.800
And it was just four hours.

17:22.800 --> 17:24.400
I remember frantically figuring out, like, how do we turn this off?

17:24.400 --> 17:27.040
Because it wasn't one of my Lambda functions.

17:27.040 --> 17:28.040
What caused that?

17:28.040 --> 17:30.600
You shifted over too much traffic to it or something like that?

17:30.600 --> 17:32.400
Well, we just turned on debug logging.

17:32.400 --> 17:33.400
Ah, okay.

17:33.400 --> 17:34.400
Okay.

17:34.400 --> 17:40.800
And so we're sending a ton of logs to invoke this, you know, we're invoking these Lambda functions.

17:40.800 --> 17:44.120
It was just like the logs that we specifically wrote.

17:44.120 --> 17:47.200
It was all the logs that were created by all the dependencies that are pulled in.

17:47.200 --> 17:51.560
So we were seeing, you know, logs from Apache, logs, everything and its mother was being

17:51.560 --> 17:52.560
logged.

17:52.560 --> 17:53.560
So 20K.

17:53.560 --> 17:54.560
Yeah.

17:54.560 --> 17:55.560
That's a good story.

17:55.560 --> 17:56.560
Wow.

17:56.560 --> 17:57.560
Yeah.

17:57.560 --> 17:58.560
Wow.

17:58.560 --> 17:59.560
Yeah.

17:59.560 --> 18:00.560
Now we're really careful.

18:00.560 --> 18:01.560
But it could have been way worse.

18:01.560 --> 18:02.560
I was, I made a joke with the team that if that were me, it probably would have been like 60K.

18:02.560 --> 18:03.560
Wow.

18:03.560 --> 18:04.560
Because I lost a ton.

18:04.560 --> 18:12.040
And so how about the stringing together of the Lambda functions?

18:12.040 --> 18:17.920
It sounds like you were, you're essentially creating these pipeline in Lambda functions is

18:17.920 --> 18:21.760
presumably AWS makes that pretty easy if you stay within the Lambda environment.

18:21.760 --> 18:22.760
Is that right?

18:22.760 --> 18:23.760
Yeah.

18:23.760 --> 18:24.760
Definitely.

18:24.760 --> 18:26.840
So it's, it's very like action oriented.

18:26.840 --> 18:32.200
So these Lambda functions are triggered based on these defined actions that we specify.

18:32.200 --> 18:37.680
So for example, it all starts with consuming data from Kinesis or Kafka or some event stream.

18:37.680 --> 18:42.760
The Lambda function gets triggered to, you know, pull or automatically Kinesis triggers

18:42.760 --> 18:43.760
itself.

18:43.760 --> 18:47.920
And it reads data from the event stream and then we say, OK, this is the raw because, you

18:47.920 --> 18:52.760
know, data 101 never get rid of the raw data because you may, you know, mess up how

18:52.760 --> 18:53.960
you transform it.

18:53.960 --> 18:57.560
So we stick the raw data into our bucket, our s3 bucket.

18:57.560 --> 19:01.640
And then there's another Lambda function that's, you know, waiting for these events to

19:01.640 --> 19:03.360
be written to that path.

19:03.360 --> 19:04.600
So it gets triggered on that.

19:04.600 --> 19:08.920
So every time there's like a new file that's created, another subsequent Lambda function

19:08.920 --> 19:11.560
gets triggered and it does the enrichment aspect.

19:11.560 --> 19:16.680
And then once, once we've enriched that file, the usage events in that file, we write

19:16.680 --> 19:20.160
that back out to a different part of that s3 bucket.

19:20.160 --> 19:24.080
And then there's a subsequent, different Lambda function that, you know, starts aggregating

19:24.080 --> 19:29.720
the data to start building the feature data set that the models eventually consume, if

19:29.720 --> 19:30.720
that makes sense.

19:30.720 --> 19:32.920
Like events mean, it does make sense.

19:32.920 --> 19:37.600
So I'm going to ask again, a question that I asked earlier, you referred to, I forget

19:37.600 --> 19:42.760
actually the specific wording, but like avoiding a pipeline hell or pipeline spaghetti or

19:42.760 --> 19:44.240
something like that.

19:44.240 --> 19:48.000
You've essentially created some number of pipelines here.

19:48.000 --> 19:49.000
Yeah.

19:49.000 --> 19:52.680
What specifically do you avoid, you know, that you had to deal with before?

19:52.680 --> 19:57.880
Before the data was discopied from one cluster to another cluster.

19:57.880 --> 20:04.440
Not often that discopy would fail or there would be, you know, the data would lack in its

20:04.440 --> 20:05.440
full sense.

20:05.440 --> 20:10.120
Like they had dropped events and we wouldn't know until we realized like, until there was

20:10.120 --> 20:15.440
until our QA team was like, this box clearly watched the sopranos, but we're not, we're

20:15.440 --> 20:19.880
not seeing that in the usage data that's being fed into the training model.

20:19.880 --> 20:24.800
So it was just a lack of visibility is what we gained visibility of the most important

20:24.800 --> 20:28.480
part of a machine learning platform, our data pipeline.

20:28.480 --> 20:32.840
And just being able to, you know, have alerts when we see like all of a sudden, a drop

20:32.840 --> 20:38.040
in events, is it because of our infrastructure or is it because of upstream clients?

20:38.040 --> 20:42.920
We never had that before because again, we didn't, we relied on another quote unquote data

20:42.920 --> 20:45.040
like to provide us that data.

20:45.040 --> 20:52.960
So the kind of, at, at a missity for lack of a better term of the, like ownership, the

20:52.960 --> 20:56.760
functional environment gave you a lot more transparency and ownership of the different

20:56.760 --> 20:57.760
pieces.

20:57.760 --> 21:04.120
And kind of forced you to think about transforming an individual piece of data at a time like

21:04.120 --> 21:08.760
a log entry as opposed to doing big batch runs of transformations or something.

21:08.760 --> 21:09.760
Right.

21:09.760 --> 21:10.760
Right.

21:10.760 --> 21:11.760
Right.

21:11.760 --> 21:12.760
Right.

21:12.760 --> 21:13.760
Yeah.

21:13.760 --> 21:16.400
So avoiding the 18 hour MapReduce job for just doing it in somewhat real time.

21:16.400 --> 21:18.400
And so that's the data platform.

21:18.400 --> 21:24.120
Yeah, so then there's the second part, the like orchestrating of the machine learning machine

21:24.120 --> 21:25.720
learning side.

21:25.720 --> 21:29.160
And that's where we did you, we just, we have started to use airflow, which is great

21:29.160 --> 21:30.400
that you mentioned that.

21:30.400 --> 21:35.640
But yeah, so the key part to that was before it was, there was no way for us to introduce

21:35.640 --> 21:37.360
a new model into our platform.

21:37.360 --> 21:40.840
There was just the one, the one machine learning model and that's it.

21:40.840 --> 21:41.840
And it did everything for us.

21:41.840 --> 21:42.840
It was a super high level.

21:42.840 --> 21:45.000
It was ALS matrix factorization.

21:45.000 --> 21:50.360
We were find it in different ways and we, we somewhat tested those changes, but once

21:50.360 --> 21:55.560
we made a change to like our similarity matrix or anything specific to the model itself,

21:55.560 --> 22:00.520
there was no way of partitioning subscribers to only get this new algorithm.

22:00.520 --> 22:02.040
It's everyone gets it.

22:02.040 --> 22:08.520
So just being able to train a new or new model or change and then evaluate that in an offline

22:08.520 --> 22:11.160
fashion and then push that into production.

22:11.160 --> 22:16.120
And only for like a subset of subscribers was the next big effort that we focused on.

22:16.120 --> 22:17.280
And how did you get there?

22:17.280 --> 22:18.280
Yeah, definitely.

22:18.280 --> 22:23.840
So one part we started utilizing the AB testing platform that I brought from my other team.

22:23.840 --> 22:29.400
And then two, making the, the way that what we had to introduce, which also led to like

22:29.400 --> 22:34.600
a reason why my previous like work history was perfect for this team, we introduced our

22:34.600 --> 22:36.240
own service layer.

22:36.240 --> 22:41.800
So before it was our logic was super really tightly coupled with the client.

22:41.800 --> 22:45.560
So if we think about it in a very high level fashion, this is obviously not how it's

22:45.560 --> 22:46.560
implemented.

22:46.560 --> 22:51.800
When you watch TV, you see a bunch of options on your TV saying, you know, these are

22:51.800 --> 22:55.280
the TV shows you should watch here, the top movies for you.

22:55.280 --> 22:57.560
We were super tightly coupled to that infrastructure.

22:57.560 --> 23:04.600
So if we wanted to introduce a new model, there was no way to tell the, the cable box or

23:04.600 --> 23:08.440
the set up box, hey, go, go get this model for just this account.

23:08.440 --> 23:13.400
So adding a service layer in between us allowed us to be more independent in serving what

23:13.400 --> 23:16.240
we wanted to serve when, if that makes sense.

23:16.240 --> 23:23.600
So you're able to encapsulate the, the business logic of what model to use to respond to a specific

23:23.600 --> 23:29.240
prediction requests at the service layer, as opposed to just kind of serving up just

23:29.240 --> 23:30.560
raw model results.

23:30.560 --> 23:31.560
Yep.

23:31.560 --> 23:33.040
Just in serving raw predictions, regardless.

23:33.040 --> 23:34.040
Yep.

23:34.040 --> 23:37.280
So this is the next layer, which is we started to get context.

23:37.280 --> 23:42.640
We started asking the client to pass this context into what, where this is being served so

23:42.640 --> 23:48.760
that we can later, specifically if we're looking at, you know, a view on X1 or on, on your

23:48.760 --> 23:51.640
cable box, it's like, here, here's all the music videos.

23:51.640 --> 23:54.920
Let's go to the model that's finally tuned for music.

23:54.920 --> 23:56.640
So yeah, so it was a combination of a few things.

23:56.640 --> 24:00.560
It was one being able to get the data in the right format so that we could easily train

24:00.560 --> 24:02.480
it on different data.

24:02.480 --> 24:08.760
So different feature sets to introducing that service layer in between the producing the

24:08.760 --> 24:12.560
predictions and serving the predictions to the client.

24:12.560 --> 24:18.960
And three, we beefed up our evaluation metrics before when we had this model, we really

24:18.960 --> 24:21.720
kind of was flying blind, I think is the phrase.

24:21.720 --> 24:22.720
I'm not sure.

24:22.720 --> 24:25.960
But we had really no idea how our models were performing.

24:25.960 --> 24:30.960
We would run, and this is before I joined the team, I used to say they, some getting

24:30.960 --> 24:36.200
used to saying we, we would train our model and then our evaluation metrics were essentially

24:36.200 --> 24:37.200
just precision.

24:37.200 --> 24:41.600
And it would run once a month or so, which is not ideal, especially for a customer-facing

24:41.600 --> 24:42.600
platform.

24:42.600 --> 24:45.240
So we did spend a lot of time, this is kind of like a weekend project for me.

24:45.240 --> 24:52.280
I spent a lot of time on building apps that surface what are like our recall or precision,

24:52.280 --> 24:54.080
what that was at an hourly basis.

24:54.080 --> 24:58.240
So we could see this historical view of how our predictions are performing.

24:58.240 --> 25:01.440
And then we could use that as a baseline to make changes.

25:01.440 --> 25:06.080
And was that easy to do with your pre-existing models, meaning were they sufficiently

25:06.080 --> 25:13.240
instrumented for you to be able to even report on their performance, or did you have to do

25:13.240 --> 25:15.680
a lot of work to instrument them?

25:15.680 --> 25:17.160
That's a good question.

25:17.160 --> 25:22.240
So it did take a good amount of work to be able to easily grab what we would predict

25:22.240 --> 25:25.920
right now, because what I wanted to do is I wanted to build, or what I did do.

25:25.920 --> 25:34.440
What I built was an online offline recall platform, which essentially all it did was right

25:34.440 --> 25:40.400
now, I'm going to check all the events that people are like, all the TV shows that a subset

25:40.400 --> 25:42.120
of subscribers are watching.

25:42.120 --> 25:46.280
And then right in this moment, I want to check what our recommendations platform is suggesting

25:46.280 --> 25:49.400
and do they match up, you know, basic recall evaluation.

25:49.400 --> 25:55.280
The hardest part was being able to easily be able to check right now what we are predicting

25:55.280 --> 26:00.640
because it was encapsulated like deep in this, you know, legacy platform, which utilized

26:00.640 --> 26:01.640
couch-based.

26:01.640 --> 26:04.000
So introducing that service layer made it easy.

26:04.000 --> 26:08.320
So essentially the evaluation platform that we built, all it does is it hits our service

26:08.320 --> 26:09.320
layer.

26:09.320 --> 26:11.720
We pass it, you know, context for who the user is.

26:11.720 --> 26:16.280
And then it returns for this specific model what we would recommend.

26:16.280 --> 26:18.800
And then we just check that with what they're actually watching.

26:18.800 --> 26:25.240
You mentioned couch-based, presumably you, you know, ran your 18-hour map, or

26:25.240 --> 26:29.960
reduced jobs, created a model, and it started in a database.

26:29.960 --> 26:35.880
Are you hitting a model based in code now, or are you also kind of caching it in some

26:35.880 --> 26:37.440
database data structure?

26:37.440 --> 26:38.440
Yeah.

26:38.440 --> 26:43.400
So right now we're at, we're going to routes, they're, we're going one route where there's

26:43.400 --> 26:48.280
a model that's in base in a, you know, a recommender backend that we incorporate into

26:48.280 --> 26:52.560
our web service, that's work in progress, all the work that we've done since moving

26:52.560 --> 26:57.440
from our legacy platform has been pre-computing recommendations and writing it into Redis.

26:57.440 --> 26:59.280
We've moved away from couch-based.

26:59.280 --> 27:04.800
And only because, again, Redis is more of a AWS managed service, so it's easy to stand

27:04.800 --> 27:07.320
up and scale out.

27:07.320 --> 27:08.320
So yes, that's the goal.

27:08.320 --> 27:13.280
The goal is to also go the route of, of having the model be encapsulated in code and then

27:13.280 --> 27:18.120
also put into our web service, but yeah, we'll see how that goes.

27:18.120 --> 27:25.560
And would you envision in doing that, putting the actual predictions behind a Lambda function

27:25.560 --> 27:28.240
or does inference take too long for?

27:28.240 --> 27:33.520
Yeah, I, I think I would probably keep our service serverless side or a Lambda route in

27:33.520 --> 27:34.520
the back.

27:34.520 --> 27:40.120
I think for now, keeping, I'm a bit nervous on the performance of Lambda functions with

27:40.120 --> 27:42.480
anything that's like customer impacting.

27:42.480 --> 27:53.080
So you built out the capability to use multiple models and benchmark those, the models relative

27:53.080 --> 27:55.440
to the previous system.

27:55.440 --> 28:01.640
You mentioned earlier that part of what you wanted to do on the machine learning side

28:01.640 --> 28:08.920
is just make it easier to experiment with models and with the data and to deploy.

28:08.920 --> 28:10.920
What have you done in that regard?

28:10.920 --> 28:12.760
Yeah, definitely.

28:12.760 --> 28:19.160
So in that regard, to be able to easily experiment deploy, we started Ulyse Redshift.

28:19.160 --> 28:24.840
So one of the big parts of machine learning and any model is your model is as good as

28:24.840 --> 28:26.640
the data that you train it on.

28:26.640 --> 28:30.360
So we started to experiment with what if we, and this is basic constructs that I'm sure

28:30.360 --> 28:33.920
other content discovery platforms already went down.

28:33.920 --> 28:37.200
What if we started to train our model based on different criteria?

28:37.200 --> 28:40.520
So, and this is before we went down the deep learning route where we're playing with

28:40.520 --> 28:41.520
hyper parameters.

28:41.520 --> 28:46.560
This is just the basic our similarity matrix and everything that's like just super simple

28:46.560 --> 28:48.760
machine learning, let's start playing with that.

28:48.760 --> 28:52.720
So what we started to do is first, let's put our data in a place that we can easily access

28:52.720 --> 28:58.000
so that we could train models quickly and not have to spend days babysitting them.

28:58.000 --> 29:05.360
So we put all our training data in Redshift, which is like a big database, I think Oracle,

29:05.360 --> 29:07.080
but for big data.

29:07.080 --> 29:13.760
And what that allowed us to do was start segmenting subscribers based on attributes.

29:13.760 --> 29:17.920
So I could explain one of the models that we recently deployed to production at a high

29:17.920 --> 29:18.920
level.

29:18.920 --> 29:22.280
What we first did is we group subscribers based on their location.

29:22.280 --> 29:26.640
So people in Philly, versus people in DC, versus people in New York.

29:26.640 --> 29:30.840
And then just on those small clusters, we started grouping them based on what they're

29:30.840 --> 29:32.960
similar to watch to each other.

29:32.960 --> 29:35.840
First we started clustering based, segmenting based on locality.

29:35.840 --> 29:38.880
Then we started clustering within that based on what they watched.

29:38.880 --> 29:44.240
And then we fed those accounts into our training, our models to produce recommendations.

29:44.240 --> 29:48.760
We found what that was, the model trained a lot quicker.

29:48.760 --> 29:53.400
We could do them in parallel, so instead of waiting hours and hours for one model to train

29:53.400 --> 29:58.840
based on all of our subscriber base, we had multiple models training at the same time.

29:58.840 --> 30:04.000
And then where we leveraged Redis and our web services that we could easily, based on

30:04.000 --> 30:08.720
the key, write those predictions to Redis for this web service to pick up.

30:08.720 --> 30:13.840
I'm hearing the folks talking about doing training directly against the data warehouse,

30:13.840 --> 30:18.280
whether it's Redshift or BigQuery or something else.

30:18.280 --> 30:24.240
A lot more than I used to, and I don't know if that's because it's just more business

30:24.240 --> 30:33.480
as opposed to academic or more production as opposed to toy problems or what.

30:33.480 --> 30:39.640
I think it's this evolution of engineers, because at least what I've observed is before

30:39.640 --> 30:43.080
there was, you know, research, traditional researchers, and then there were engineers

30:43.080 --> 30:47.560
who kind of had it played a blind eye to everything that went on with machine learning.

30:47.560 --> 30:51.480
I think we're like starting to build machine learning engineers.

30:51.480 --> 30:56.080
And that's like the sweet spot where they can understand the intricacies and the detail

30:56.080 --> 31:00.080
oriented that's required for training models and evaluating them.

31:00.080 --> 31:06.000
But then they also know how to build platforms that can, you know, manage terabytes and petabytes

31:06.000 --> 31:09.600
worth of data to be able to access, if that makes sense.

31:09.600 --> 31:16.640
And so the data warehouse enabled you to kind of slice and thus easily access this data

31:16.640 --> 31:20.280
query language, yep.

31:20.280 --> 31:27.520
Create kind of new features on the fly, presumably, did you do anything in terms of creating

31:27.520 --> 31:31.840
a feature store or trying to achieve feature reusability across models?

31:31.840 --> 31:32.840
Yep.

31:32.840 --> 31:33.840
So we did a few things.

31:33.840 --> 31:37.760
One, we tried to go down the route of, when we created a feature store, we would create

31:37.760 --> 31:38.760
a view.

31:38.760 --> 31:42.120
For some reason, we found difficult creating a view in Redshift at the time.

31:42.120 --> 31:48.240
So what we ended up doing was we would output that data to a different part of S3.

31:48.240 --> 31:54.000
So we have our, you know, our partition, our path in S3 where our bigger training data

31:54.000 --> 31:59.040
set exists and, you know, that data's offloaded or unloaded into Redshift.

31:59.040 --> 32:02.320
Once we came to a point where we've evaluated a model and then we're like, okay, we wanted

32:02.320 --> 32:05.720
to play that production, but we want to save the training, the feature set.

32:05.720 --> 32:11.960
We would output that into a specific, you know, training data set part of our S3 bucket.

32:11.960 --> 32:16.840
And if you're developing new features for a model and you need to backfill, is that

32:16.840 --> 32:21.000
something you do manually or do you have some kind of automation in there?

32:21.000 --> 32:27.400
So the only automation that we've gotten to at this point is utilizing Airflow to continuously

32:27.400 --> 32:33.240
update the tables to the tables in our, what we call our training tables in Redshift.

32:33.240 --> 32:38.000
We haven't gotten to the point where we are automated to offloading the feature data

32:38.000 --> 32:41.000
set, but that's definitely something we'd like to look into.

32:41.000 --> 32:45.520
And then kind of back to this idea of experimentation.

32:45.520 --> 32:53.360
How are you managing experiments in terms of, you know, recording that iterative process?

32:53.360 --> 32:54.360
Right, right.

32:54.360 --> 32:57.880
So there's a lot of metadata that goes along with experimenting, right?

32:57.880 --> 33:02.480
It's the version of the model, the, you know, the time frame in which you extracted the

33:02.480 --> 33:05.680
training data set, you know, was it the last three months or the last nine months or

33:05.680 --> 33:10.680
the last 13 months, the instance type that you use, the GPUs.

33:10.680 --> 33:16.440
So we offload that metadata into dynamo DB at the time, or currently.

33:16.440 --> 33:20.400
And we often reference that when we have to deploy the model's production.

33:20.400 --> 33:26.320
But right now are what we call our, our model metadata store, it's reutilizing dynamo.

33:26.320 --> 33:29.720
Is that a manual process to keep that up to date as you're experimenting?

33:29.720 --> 33:36.240
Yeah, it's one of the steps in our Airflow workflow is once we've went before we start

33:36.240 --> 33:41.360
training a model, we first insert a record that describes like what the model is, here's

33:41.360 --> 33:45.960
the training data sets going after it's typically part of every, it's the first step before

33:45.960 --> 33:46.960
we start training it.

33:46.960 --> 33:51.680
And then the last step is it outputs the time, like how long it took.

33:51.680 --> 33:55.560
And then, you know, the recall about the high level evaluation, the recall evaluation

33:55.560 --> 33:59.720
or the precision that was determined after the predictions were produced.

33:59.720 --> 34:02.440
It's part of the end and flow.

34:02.440 --> 34:06.800
But I didn't, I didn't think I made that connection earlier part of the way you're using

34:06.800 --> 34:10.800
Airflow is to manage the training workflow.

34:10.800 --> 34:11.800
Yep, yep, yep.

34:11.800 --> 34:16.040
And it took us a while to get there before we were super manual.

34:16.040 --> 34:20.480
You know, we would execute a job, wait, and then I'll put this data into a Google Excel

34:20.480 --> 34:21.480
sheet.

34:21.480 --> 34:26.280
You know, like it was, and then we realized, wait, we should do something about this.

34:26.280 --> 34:31.400
And so where there are multiple steps to get to using Airflow, or did you realize the

34:31.400 --> 34:36.840
problem and then, you know, big bang, you've put the Airflow solution in place.

34:36.840 --> 34:39.760
It was fairly straightforward.

34:39.760 --> 34:44.720
In the past, we've, I mean, we've dealt with things like oozy workflows that were traditional

34:44.720 --> 34:46.120
with the Hadoop infrastructure.

34:46.120 --> 34:50.000
Airflow was really straightforward from my perspective, at least.

34:50.000 --> 34:52.680
And it seems to be the best practice what everyone else is using.

34:52.680 --> 34:56.880
I remember we were going down the route of looking at something else and then we had a few

34:56.880 --> 35:00.520
blog posts that were suggesting stick with Airflow, so it's what we did.

35:00.520 --> 35:07.520
For training, do you have multiple workflows for different types of training jobs, or have

35:07.520 --> 35:13.000
you kind of abstracted training as a workflow and you've got different ways to parameterize

35:13.000 --> 35:14.000
that?

35:14.000 --> 35:16.480
We have different, so at the moment, that's a good question of parameterizing.

35:16.480 --> 35:18.680
That's actually the dream would be to parameterize that.

35:18.680 --> 35:23.120
Right now, we have different workflows, but we also have like a lot of different types

35:23.120 --> 35:28.960
of algorithms, because you know, like there's the typical recommendations isn't just producing

35:28.960 --> 35:33.800
recommendations, there's a relevancy aspect to it too, so our relevancy models require

35:33.800 --> 35:38.760
a different workflow than our core recommendations models.

35:38.760 --> 35:40.520
So right now, there are different workflows.

35:40.520 --> 35:45.000
It'd be great to get to the point where they're parameterized, as we'll see, hopefully

35:45.000 --> 35:46.000
one day.

35:46.000 --> 35:53.640
Last bit on this experimentation piece, have you incorporated any aspect of automated hyperparameter

35:53.640 --> 35:55.040
tuning?

35:55.040 --> 35:56.040
That would be great.

35:56.040 --> 35:57.640
Actually, that's one of, that's on our to-do list.

35:57.640 --> 36:00.000
One, our research team would love that.

36:00.000 --> 36:06.000
So we are, as I mentioned earlier, in the previous world, or 14 months ago, we had this

36:06.000 --> 36:12.800
one machine learning algorithm that was one size fits all, it was at super high levels

36:12.800 --> 36:18.440
and ALS, matrix factorization, and what we wanted to do more, especially with everyone's

36:18.440 --> 36:22.520
talking about AI and ML, we've started to go down three routes.

36:22.520 --> 36:26.560
So we mentioned the utilizing, clustering, and recommendations together, so that's where

36:26.560 --> 36:28.440
out that's already in production.

36:28.440 --> 36:32.400
Our research team is going down the route of utilizing deep learning models that are somewhat

36:32.400 --> 36:37.080
based off of word-to-vec, and that's where we want to do exactly what you say with the

36:37.080 --> 36:38.080
hyperparameters.

36:38.080 --> 36:39.080
We're not quite there yet.

36:39.080 --> 36:44.040
We're still very much in the experimentation phase with this deep learning model that's

36:44.040 --> 36:46.200
similar to word-to-vec.

36:46.200 --> 36:47.200
So we'll see.

36:47.200 --> 36:52.640
If it gains momentum, maybe we'll start abstracting that, but as of now, we've seen some limitations,

36:52.640 --> 36:54.040
so we'll see how far this goes.

36:54.040 --> 36:59.440
So if you hit experimentation, deployment, you've talked about server-side and redis,

36:59.440 --> 37:06.520
and then you've alluded to this next step, which is A-B testing previously, but I get the

37:06.520 --> 37:09.360
impression that there's more to it than what we've talked about so far.

37:09.360 --> 37:10.360
Yeah, definitely.

37:10.360 --> 37:14.760
So A-B testing is my sweet spot, just because I built this platform prior to joining

37:14.760 --> 37:16.920
the recommendations team.

37:16.920 --> 37:22.840
And the A-B testing platform that we built, what it does is it allows for in partnership

37:22.840 --> 37:28.000
with our web service, it allows for a certain set of customers to get one model, and then

37:28.000 --> 37:30.360
another subset of customers get a different model.

37:30.360 --> 37:33.720
And then, of course, it always takes a new account breaking out the control.

37:33.720 --> 37:36.560
One thing that I always say when we talk about A-B testing, it's quite expensive.

37:36.560 --> 37:42.200
It does take time, so when in doubt, try to do as much offline evaluation as you can.

37:42.200 --> 37:46.000
And it's also, at least with TV viewing behavior, there are sometimes where you just

37:46.000 --> 37:49.760
you can't run an A-B test, like on Comcast, like working at Comcast.

37:49.760 --> 37:53.600
You can't run an A-B test during the Olympics, the data fluctuates so much.

37:53.600 --> 37:59.240
It's so skewed to Olympics that for that entire month, you can't experiment, right?

37:59.240 --> 38:02.080
So we get that a lot in the TV world.

38:02.080 --> 38:07.600
So when I mean A-B testing is expensive, when we find a good time where data is inflectioning,

38:07.600 --> 38:12.320
the usage patterns are, you can predict, then we execute an A-B test.

38:12.320 --> 38:15.120
And what that platform entails is a few parts.

38:15.120 --> 38:20.800
It's the tagging of putting essentially traits onto accounts, and then associating those

38:20.800 --> 38:25.240
similar traits or tags that are on counts to the models so that the models are only surfaced

38:25.240 --> 38:29.800
for those accounts, or those customers, sorry, I'm talking such cable lingo.

38:29.800 --> 38:34.880
So yeah, so it facilitates that, it orchestrates allowing different models to be available for

38:34.880 --> 38:36.520
different customers.

38:36.520 --> 38:42.520
And then the second part is the metrics aspect, which is deriving analytics on how the

38:42.520 --> 38:47.160
customers are actually interacting with those new rows or those new content, the new content

38:47.160 --> 38:48.160
that's being surfaced.

38:48.160 --> 38:54.320
So your A-B testing is primarily kind of at scale across the client base.

38:54.320 --> 38:55.320
Yeah.

38:55.320 --> 39:00.480
The luxuries that we have millions of subscribers, so the sample size, we could do something

39:00.480 --> 39:03.160
like we're A-B testing something where it's like a million.

39:03.160 --> 39:08.400
So there's never a doubt whether this meets statistical significance because our sample

39:08.400 --> 39:09.800
sizes are so huge.

39:09.800 --> 39:14.080
We talked a little bit about kind of evaluation metrics before in the context of when you

39:14.080 --> 39:19.440
kind of instrumented your previous model, what kind of instrumentation do you have on the

39:19.440 --> 39:22.960
current model and how does that play into your A-B testing system?

39:22.960 --> 39:23.960
Right.

39:23.960 --> 39:27.240
So they're actually, they're very tangential to each other.

39:27.240 --> 39:34.560
The A-B testing framework is more so on the, I guess the front office side of our platform

39:34.560 --> 39:40.760
whereas the orchestration of our models is more of our back office side because if at some

39:40.760 --> 39:45.480
point one of the models isn't performing as we expect, we need to be able to easily turn

39:45.480 --> 39:48.760
that off for customers.

39:48.760 --> 39:55.160
So utilizing what the web service to be able to gate us from doing that, to enable us

39:55.160 --> 40:00.960
to do that to gate different features from different customers, I guess is the key difference.

40:00.960 --> 40:03.480
That makes sense.

40:03.480 --> 40:09.560
So meaning you're collecting metrics on model performance and primarily consuming, consuming

40:09.560 --> 40:16.040
those metrics within the web service tier so that you can take action if predictions

40:16.040 --> 40:17.040
start to degrade.

40:17.040 --> 40:18.040
Yeah, quicker action.

40:18.040 --> 40:19.040
Right.

40:19.040 --> 40:24.760
And it's not like a historic, like for the evaluation of models and typically what we call

40:24.760 --> 40:29.320
the back office side, we have like historical usage, right?

40:29.320 --> 40:33.000
For A-B testing, we really just care of what just happened the past day or the past two

40:33.000 --> 40:34.520
days or the past week.

40:34.520 --> 40:40.680
So keeping that small subset of usage from the recent time frame is a lot easier on the

40:40.680 --> 40:44.600
web service side than it would be in the, if we were keeping historical data.

40:44.600 --> 40:45.600
Got it, got it.

40:45.600 --> 40:49.840
So you're the A-B testing is pure kind of online.

40:49.840 --> 40:55.000
You've got two models in flight, you've labeled traffic and you're serving up different models

40:55.000 --> 40:59.920
and you're just comparing very short-term results between those models.

40:59.920 --> 41:00.920
Exactly.

41:00.920 --> 41:06.600
What do you tend to see in terms of model, shelf life, model degradation, that kind of thing

41:06.600 --> 41:10.880
for these kind of recommendation models that you're building?

41:10.880 --> 41:13.160
That's a good question.

41:13.160 --> 41:20.600
So the biggest, I guess, issue that I've seen is the frequency in which we need to train

41:20.600 --> 41:22.000
these models.

41:22.000 --> 41:26.240
If the customer keeps seeing the same exact pre-computer recommendations over and over

41:26.240 --> 41:28.440
again, it gets kind of old, right?

41:28.440 --> 41:35.640
So being able to frequently update the pre-computed recommendations that are produced by our models

41:35.640 --> 41:39.000
is something that we're trying to experiment on how we could do more often.

41:39.000 --> 41:44.000
For example, waiting higher, what you just just watched for something that you've watched

41:44.000 --> 41:45.920
six months ago.

41:45.920 --> 41:50.120
That's something that we're trying to evaluate how we could not overemphasize something

41:50.120 --> 41:55.040
that you just watched, but some sweet spot where it's just enough where you see the impact

41:55.040 --> 41:56.640
and what you're being served.

41:56.640 --> 41:58.880
The last piece of the puzzle was infrastructure.

41:58.880 --> 41:59.880
Right.

41:59.880 --> 42:04.360
We kind of discussed that a bit with everything that we mentioned with the AWS or public

42:04.360 --> 42:05.360
cloud.

42:05.360 --> 42:09.520
But it was fairly straightforward as the legacy platform that we'd built was primarily

42:09.520 --> 42:14.440
running on this physical infrastructure, and we weren't at the point where we could scale

42:14.440 --> 42:18.240
that out where we could add more nodes to the Hadoop cluster.

42:18.240 --> 42:22.640
So the easiest decision was moving up to the public cloud, AWS.

42:22.640 --> 42:27.440
Of course, with every new decision where you make or you're dramatically changing something,

42:27.440 --> 42:31.520
some aspect of your platform, you find new problems, your old problems go away, but you

42:31.520 --> 42:32.520
get new problems.

42:32.520 --> 42:36.360
And one of those is, it's very easy to spend money in AWS.

42:36.360 --> 42:41.480
It's very easy to lose money in AWS, so that was one of the struggles that we've been

42:41.480 --> 42:42.480
working through.

42:42.480 --> 42:46.960
And another aspect is it's very easy just to start picking new technologies.

42:46.960 --> 42:52.400
There's something new in AWS every month that feels like maintaining the, yeah, at least

42:52.400 --> 42:57.240
maintaining the key tech stack, and then not trying to move too far from it.

42:57.240 --> 43:01.720
Otherwise, you're just going to build this, this ginormous technical debt where you're

43:01.720 --> 43:05.240
continually slaying the learn and then update and then you just realize, like, wait, like

43:05.240 --> 43:10.440
there's this example, we went down the path of utilizing, before we went down lambdas,

43:10.440 --> 43:14.320
we decided to use Kafka Connect for consuming data from Kafka.

43:14.320 --> 43:19.360
And because that wasn't specifically managed by AWS, there was a lot of inherent difficulties

43:19.360 --> 43:23.600
with monitoring and getting the logs and just seeing how it's performing that we realized

43:23.600 --> 43:25.320
we can't just pick any technology.

43:25.320 --> 43:29.280
We have to really think of the technology that we picked since we're in this public cloud

43:29.280 --> 43:30.280
space.

43:30.280 --> 43:34.400
So yeah, so I was, there's actually whenever I talk about infrastructure and I ever make

43:34.400 --> 43:40.320
the case for more budget, I often referenced this, there was this paper that was, that was

43:40.320 --> 43:44.600
published at the NIPPS conference by these Google developers, it was something on the lines.

43:44.600 --> 43:49.320
I've read the exact titles, like hidden technical debt in machine learning systems or machine

43:49.320 --> 43:50.320
learning platforms.

43:50.320 --> 43:53.560
Machine learning is the high interest credit card of technical debt.

43:53.560 --> 43:54.560
Yeah, exactly.

43:54.560 --> 43:55.560
Yeah, exactly.

43:55.560 --> 43:57.040
It's my favorite paper.

43:57.040 --> 43:58.040
It's a great paper.

43:58.040 --> 44:02.720
So I guess that I often reference it when I have to make the case for our cloud infrastructure

44:02.720 --> 44:08.520
needs more budget or you need to do this because it essentially explains that it's quite,

44:08.520 --> 44:13.800
it's really easy to start building technical debt when implementing a platform like this,

44:13.800 --> 44:15.880
regardless of any platform.

44:15.880 --> 44:20.280
And in that paper, they go into detail, the risk factors, you know, data dependency and

44:20.280 --> 44:23.320
configuration issues, things that we've talked about today.

44:23.320 --> 44:28.080
What I also really like about that paper is that it like specifically states that like machine

44:28.080 --> 44:32.560
learning systems have a special capacity for like acquiring this technical debt.

44:32.560 --> 44:37.360
I mean, a lot of platforms that, you know, that was written with code do this, but then

44:37.360 --> 44:42.920
there's an additional set of debt that can be gained from machine learning issues.

44:42.920 --> 44:48.320
And I think moving our infrastructure to AWS, we realize that really quickly.

44:48.320 --> 44:56.120
And so how do you think that the machine learning in the cloud kind of accentuates that?

44:56.120 --> 45:02.000
Well, of course, there's the budget aspect and then even there's just there are easy ways

45:02.000 --> 45:07.520
where we, it's really easy to deploy to the cloud.

45:07.520 --> 45:12.280
And whereas before there were always constraints where, oh, we don't have enough space, like

45:12.280 --> 45:15.320
we're just, we're just going to keep collecting data and then we're going to just delete

45:15.320 --> 45:20.080
the last, you know, the most historical data we have that's not a problem in AWS.

45:20.080 --> 45:24.840
Like, of course, there's glacier and there are ways to archive data, but you can kind

45:24.840 --> 45:29.200
of keep, you know, adding and picking up the latest and greatest, you know, version of

45:29.200 --> 45:33.720
Spark and then just releasing with that and not realize that you're, you're kind of building

45:33.720 --> 45:38.680
this platform that has so many different pieces because it's easy to just hop on and,

45:38.680 --> 45:45.640
you know, spin up an EC2 instance do that and or, you know, spin up an EMR instance to run

45:45.640 --> 45:49.200
this clustering algorithm and then you kind of just forget about it, whereas in a physical

45:49.200 --> 45:53.520
infrastructure, you can only do as much as you're allowed to, right?

45:53.520 --> 45:54.520
Otherwise, right?

45:54.520 --> 45:55.520
Right, right.

45:55.520 --> 45:56.520
You have backlog of jobs waiting.

45:56.520 --> 45:57.840
Yeah, that's really interesting.

45:57.840 --> 46:02.720
There's a story in there somewhere like the double edged sort of agility in the cloud

46:02.720 --> 46:03.720
like you.

46:03.720 --> 46:04.720
Right.

46:04.720 --> 46:09.960
There's friction in the real world, not the real world because cloud is real for sure

46:09.960 --> 46:15.760
at this point, but the traditional, there's friction and inefficiencies, I guess, that

46:15.760 --> 46:21.800
makes you think about the way you do things and cloud in taking away some of that makes

46:21.800 --> 46:26.120
it easy to not think about some of the things you're doing, which leads to debt.

46:26.120 --> 46:27.120
That's really interesting.

46:27.120 --> 46:28.120
Very well said.

46:28.120 --> 46:29.120
Thank you.

46:29.120 --> 46:32.880
Those are essentially the four parts of resurrecting a recommendation platform.

46:32.880 --> 46:33.880
Nice.

46:33.880 --> 46:37.160
How do you characterize the business impact of all this?

46:37.160 --> 46:38.320
That's a great question.

46:38.320 --> 46:43.040
So that was actually the biggest question I had in all honesty when I first switched over

46:43.040 --> 46:47.760
this team about 14 months ago is I just couldn't see the value that these recommendations were

46:47.760 --> 46:48.760
producing.

46:48.760 --> 46:54.600
I mean, I saw that we are recommending content, but we weren't really measuring it.

46:54.600 --> 46:59.120
Were we taking to account what the business wants us to recommend?

46:59.120 --> 47:00.480
So that's something that we've been playing with a lot.

47:00.480 --> 47:04.960
We've been specifically building models that are taking into account what the business

47:04.960 --> 47:05.960
wants.

47:05.960 --> 47:11.560
For example, promoting content that is specifically given to us by a provider.

47:11.560 --> 47:15.800
Let's see if we can figure out the customers who are likely to watch that and include

47:15.800 --> 47:19.920
that in their recommendations, whereas previously it would have just been at the bottom of the

47:19.920 --> 47:20.920
barrel.

47:20.920 --> 47:21.920
We would have never looked at it.

47:21.920 --> 47:26.280
So definitely taking into account what the business goals are we've been trying to consider

47:26.280 --> 47:30.560
on the recommendations team, and also from a business perspective, how they're being

47:30.560 --> 47:33.760
surfaced, there's something that we've been abtesting.

47:33.760 --> 47:37.200
It's just the names of the rows we're serving content.

47:37.200 --> 47:38.360
We're going down the path.

47:38.360 --> 47:43.600
We are on the path in surfacing because you watched Wonder Woman, how does that play against

47:43.600 --> 47:48.280
a row that's surfacing the same content, but it's called top movie picks for you.

47:48.280 --> 47:53.520
So leveraging how the business works and what works best and taking that into account

47:53.520 --> 47:58.720
in our machine learning platform and utilizing abtesting is kind of the big picture to see

47:58.720 --> 48:04.160
if we can increase hours watched or increase engagement by making small tweaks this platform.

48:04.160 --> 48:05.680
Well, Lima, thank you so much.

48:05.680 --> 48:08.000
This is a really, really fun conversation.

48:08.000 --> 48:09.000
Thank you.

48:09.000 --> 48:10.160
Thanks for having me.

48:10.160 --> 48:14.200
All right, everyone.

48:14.200 --> 48:15.800
That's our show for today.

48:15.800 --> 48:20.880
For more information on Lima or any of the topics covered in this episode, head over

48:20.880 --> 48:25.760
to twimmelai.com slash talk slash 201.

48:25.760 --> 48:32.200
To learn more about our AI platform series or to download our eBooks, visit twimmelai.com

48:32.200 --> 48:34.560
slash AI platforms.

48:34.560 --> 49:00.440
As always, thanks so much for listening and catch you next time.

