WEBVTT

00:00.000 --> 00:15.920
Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting

00:15.920 --> 00:20.880
people doing interesting things in machine learning and artificial intelligence.

00:20.880 --> 00:23.400
I'm your host Sam Charrington.

00:23.400 --> 00:27.920
This week on the podcast, we're featuring a series of conversations from the Nips conference

00:27.920 --> 00:30.240
in Long Beach, California.

00:30.240 --> 00:34.120
This was my first time at Nips and I had a great time there.

00:34.120 --> 00:37.360
I attended a bunch of talks and of course learned a ton.

00:37.360 --> 00:43.520
I organized an impromptu roundtable on building AI products and I met a bunch of wonderful

00:43.520 --> 00:47.680
people, including some former Twimble Talk guests.

00:47.680 --> 00:52.480
I'll be sharing a bit more about my experiences at Nips via my newsletter, which you should

00:52.480 --> 00:59.160
take a second right now to subscribe to at twimblei.com slash newsletter.

00:59.160 --> 01:04.840
This week through the end of the year, we're running a special listener appreciation contest

01:04.840 --> 01:09.560
to celebrate hitting one million listens on the podcast and to thank you all for being

01:09.560 --> 01:11.480
so awesome.

01:11.480 --> 01:16.200
Tweet to us using the hashtag Twimble1Mill to enter.

01:16.200 --> 01:20.840
Everyone who enters is a winner and we're giving away a bunch of cool Twimble swag and

01:20.840 --> 01:22.880
other mystery prizes.

01:22.880 --> 01:29.640
If you're not on Twitter or want more ways to enter, visit twimblei.com slash twimble1Mill

01:29.640 --> 01:32.120
for the full rundown.

01:32.120 --> 01:36.480
Before we dive in, I'd like to thank our friends over at Intel Nirvana for their sponsorship

01:36.480 --> 01:39.680
of this podcast and our Nips series.

01:39.680 --> 01:44.720
While Intel was very active at Nips with a bunch of workshops, demonstrations and poster

01:44.720 --> 01:50.480
sessions, their big news this time was the first public viewing of the Intel Nirvana

01:50.480 --> 01:54.200
neural network processor or NNP.

01:54.200 --> 01:59.160
The goal of the NNP architecture is to provide the flexibility needed to support deep learning

01:59.160 --> 02:04.240
primitives while making the core hardware components as efficient as possible, giving

02:04.240 --> 02:09.720
neural network designers powerful tools for solving larger and more difficult problems

02:09.720 --> 02:14.360
while minimizing data movement and maximizing data reuse.

02:14.360 --> 02:20.200
To learn more about Intel's AI products group and the Intel Nirvana NNP, visit Intel

02:20.200 --> 02:30.200
Nirvana.com and now on to the show.

02:30.200 --> 02:31.560
All right, everyone.

02:31.560 --> 02:37.160
I am here in Long Beach, California at the Nips conference and I am seated with Juan

02:37.160 --> 02:43.400
Bruna, Assistant Professor at the current Institute at NYU and also at the Center of Data

02:43.400 --> 02:49.080
Science at NYU and Michael Bronstein, who is currently on sabbatical at Harvard.

02:49.080 --> 02:53.000
Juan and Michael, welcome to this week a machine learning and AI.

02:53.000 --> 02:54.000
Thank you.

02:54.000 --> 02:58.440
But why don't we get started by having each of you introduce yourselves and tell us a

02:58.440 --> 03:02.920
little bit about how you got involved in machine learning and artificial intelligence?

03:02.920 --> 03:10.560
Yes, so I did my masters and my PhD in France and actually at the beginning my training

03:10.560 --> 03:17.120
was more on the single processing and applied math and I was not so much into machine learning

03:17.120 --> 03:26.400
up until I started to mutate during my PhD and enter machine learning through the ideas

03:26.400 --> 03:28.800
and tools from single processing.

03:28.800 --> 03:33.640
So that was five years ago and ever since I have been getting more and more interested

03:33.640 --> 03:39.800
in ways and problems in which we can maybe leverage some of the more techniques and ways

03:39.800 --> 03:45.640
of mathematical tools from applied math into some of the current modern problems in

03:45.640 --> 03:46.640
deep learning.

03:46.640 --> 03:49.960
Yeah, that's kind of how I arrived here.

03:49.960 --> 03:50.960
Awesome.

03:50.960 --> 03:56.160
And Michael, so I did my studies in Israel and the Technion and my main background is in

03:56.160 --> 04:01.480
geometry, so different types of geometry, metric geometry, spectral geometry, mostly applications

04:01.480 --> 04:07.360
to computer vision and computer graphics problems and in recent years, well, probably with

04:07.360 --> 04:12.200
like many other people, I wouldn't say everybody, but the following a little bit, the

04:12.200 --> 04:16.800
hype of deep learning, we are trying to apply some of deep learning methods to geometric

04:16.800 --> 04:22.400
data, which comes with many of its challenges that are, there are many similarities and

04:22.400 --> 04:29.040
many dissimilarities from the more traditional Euclidean soul to say data, okay?

04:29.040 --> 04:33.600
And I mentioned you're on sabbatical at Harvard, where are you otherwise affiliated?

04:33.600 --> 04:37.920
So otherwise I explain, I spend most of my time between Switzerland and Israel, full

04:37.920 --> 04:42.600
professor at the University of Lugano in Switzerland, Tel Aviv University in Israel, and I also have

04:42.600 --> 04:48.280
a position as a principal engineer at Intel, perceptual computing.

04:48.280 --> 04:56.200
You sound like a very busy guy, don't tell the way wife.

04:56.200 --> 04:59.480
The two of you delivered a tutorial, was it today, right?

04:59.480 --> 05:00.480
Yes.

05:00.480 --> 05:03.680
You did a tutorial, tell us a little bit about the tutorial you did.

05:03.680 --> 05:09.360
Okay, so the tutorial is called geometric deep learning on graphs and manifolds, okay?

05:09.360 --> 05:17.160
So the, in one sentence, the goal of what the tutorial is about is how can we leverage

05:17.160 --> 05:24.760
the successful techniques that the deep learning developed to process images, text and speech

05:24.760 --> 05:31.160
into data and tasks where you might have, where your input might be a bit more exotic.

05:31.160 --> 05:37.840
For example, proteins, or it could be data from a social network, or it could be data captured

05:37.840 --> 05:42.920
from a, let's say, a Kinect, where you have a bunch of point clouds in, in, in 3D.

05:42.920 --> 05:47.680
And so what the tutorial is trying to do is to say, basically give a picture of our current

05:47.680 --> 05:53.720
understanding of how deep learning models can be used and developed in this regime and

05:53.720 --> 05:59.800
also try to describe some of the future and current open directions.

05:59.800 --> 06:05.800
And Michael, in your introduction, you drew one of the key distinctions here, which is

06:05.800 --> 06:09.160
Euclidean space versus other geometric spaces.

06:09.160 --> 06:10.960
Can you elaborate on that distinction?

06:10.960 --> 06:11.960
Sure.

06:11.960 --> 06:16.000
So basically, the difference between Euclidean and non-Euclidean data, one of the main

06:16.000 --> 06:19.920
differences is that you don't have the possibility of vector space operations.

06:19.920 --> 06:25.000
So in Euclidean space, you can take your sample, your point, and you can add or subtract

06:25.000 --> 06:26.000
two points.

06:26.000 --> 06:29.440
You cannot do the same thing on a graph, for example, you cannot add or subtract two vertices

06:29.440 --> 06:30.440
on a graph.

06:30.440 --> 06:36.240
So basically, you have more general structures, but also less operations that you, you can

06:36.240 --> 06:37.240
do with these structures.

06:37.240 --> 06:42.840
So you need to reinvent many of the building blocks that, basically, such as convolutions

06:42.840 --> 06:48.800
or pudding, or that are commonly used in deep learning architectures when you need

06:48.800 --> 06:51.120
to deal with these data.

06:51.120 --> 06:55.280
So Zohan, why don't you walk us through the general structure of the tutorial?

06:55.280 --> 06:56.320
How did you set the context?

06:56.320 --> 06:57.560
How did you get started?

06:57.560 --> 07:03.040
Well, I mean, historically, yeah, so there's a little clique of researchers that we started

07:03.040 --> 07:07.040
to look into this problem a few years ago, maybe like three, four years ago.

07:07.040 --> 07:12.280
So that was at the time where I was a postdoc in Yalekunz Lab in New York, like at the

07:12.280 --> 07:13.600
time of the postdoc.

07:13.600 --> 07:18.280
So there we, and together with Arthur Slam, who is another organizer of the tutorial,

07:18.280 --> 07:25.760
we came up with a very simple, I would say, first model that was trying to set up the

07:25.760 --> 07:31.520
techniques that later on we realized, and Michael and his group also helped us understand

07:31.520 --> 07:35.640
that they were far too naive, right, that there were a lot of things that could be improved

07:35.640 --> 07:37.200
upon this first idea.

07:37.200 --> 07:41.720
So I guess that since the very beginning, we kind of saw that there was a very interesting

07:41.720 --> 07:47.520
exchange of ideas, and also leveraging the fact that we came from slightly different backgrounds,

07:47.520 --> 07:48.520
right?

07:48.520 --> 07:52.840
Michael's group, they have a lot of expertise in geometry and understanding manifolds,

07:52.840 --> 07:57.880
and we came up with an expertise maybe where we had been working hands-on with convolutional

07:57.880 --> 08:03.360
neural networks and some of the ideas that I also worked under in my PhD that involve

08:03.360 --> 08:07.400
more the harmonic analysis of this convolution neural networks.

08:07.400 --> 08:09.800
And so the tutorial came quite naturally.

08:09.800 --> 08:16.640
We have been also collaborating in a variety of different projects, yeah, I mean, that's

08:16.640 --> 08:17.840
how it came out.

08:17.840 --> 08:22.800
So for us, basically, the paper that Ron mentioned that he also, as a postdoc, was actually

08:22.800 --> 08:27.920
an inspiration, and that's where we started looking into these kind of problems.

08:27.920 --> 08:31.360
So we've been working, of course, on spectral geometry for a long time, and in computer

08:31.360 --> 08:36.120
graphics, geometry, processing, community, these or similar ideas have been around for

08:36.120 --> 08:41.360
a while, but basically combining it with learning and redefining operations, for example,

08:41.360 --> 08:45.960
in the spectral domain, as they did in their influential paper, was, to some extent,

08:45.960 --> 08:48.160
eye-opening, and basically we took it from there.

08:48.160 --> 08:53.560
So we rolled together a review paper that appeared in a triple-esignal processing magazine

08:53.560 --> 08:58.240
just this summer, that for us, it was also doing some homework.

08:58.240 --> 09:03.400
We discovered many works in other communities that existed for quite some time, or maybe

09:03.400 --> 09:08.080
we're almost forgotten, or not given sufficient attention.

09:08.080 --> 09:12.880
And maybe even in different communities, people calling the same things with different

09:12.880 --> 09:19.160
names, or to some extent even, I wouldn't dare say reinventing approaches just without

09:19.160 --> 09:22.320
probably being aware of what is done in other communities.

09:22.320 --> 09:23.320
So that never happens.

09:23.320 --> 09:29.600
Well, of course, it always happens, but in this way, because it's new, it's a new field.

09:29.600 --> 09:33.840
We can probably already call it a new field, or a new trend, or a new niche in machine

09:33.840 --> 09:39.520
learning, and because it's new, then basically it's an effort of, there are several seeds

09:39.520 --> 09:46.600
that exist in other domains, and I think now people start getting, maybe, a uniform

09:46.600 --> 09:50.480
picture of what exists, and what's the relation between different methods.

09:50.480 --> 09:53.360
So I think the tutorial is very timely.

09:53.360 --> 09:55.720
So you mentioned spectral analysis.

09:55.720 --> 09:59.800
When I think of that, I think of things like fast freer transforms, and the like, how

09:59.800 --> 10:06.680
does that fit into graphs and manifolds, and non-uclidean spaces, and all these things?

10:06.680 --> 10:10.120
One way you can think about it is that it's like your dictionary, right?

10:10.120 --> 10:14.600
And we should probably even take a step back and explain what an FFT is for a folks who

10:14.600 --> 10:16.800
might not be familiar with that.

10:16.800 --> 10:21.960
Yes, so maybe, if we take a step sufficiently back, we can maybe start with physics, right?

10:21.960 --> 10:28.560
So, so, so, yes, so there's a very fundamental equation in physics that governs how things

10:28.560 --> 10:29.560
oscillate, right?

10:29.560 --> 10:33.560
But like in a guitar, or you know, in a domain, like if you take, you know, like a drum,

10:33.560 --> 10:37.480
and you hit it, there's some, some equation that is going to determine how things are going

10:37.480 --> 10:38.480
to oscillate.

10:38.480 --> 10:42.800
And so the modes of oscillation, as you, as you might suspect, they are related to the

10:42.800 --> 10:44.040
Fourier analysis, right?

10:44.040 --> 10:49.960
So, so the, and the equation that, and the operator that governs this behavior is called

10:49.960 --> 10:50.960
Laplacian, right?

10:50.960 --> 10:56.840
So, so the Laplacian is the, the mathematical, it's a differential operator that in the

10:56.840 --> 10:59.360
occlusion domain might look very inoffensive, right?

10:59.360 --> 11:03.320
It's just the, you take the partial derivatives with respect to all directions, and you just

11:03.320 --> 11:04.760
sum everything.

11:04.760 --> 11:09.440
But it turns out that, that from this operator, you can use this as a vehicle to generalize

11:09.440 --> 11:10.440
things, right?

11:10.440 --> 11:15.280
Because now, if you want to generalize convolutions from the occlusion domain to an

11:15.280 --> 11:17.800
unequally in domain, you will have a hard time, right?

11:17.800 --> 11:21.120
Because they are defined through something that doesn't exist in the nodule domain.

11:21.120 --> 11:25.480
But if you, if you take one step back and they say, okay, maybe I cannot directly use

11:25.480 --> 11:29.960
the convolution, but maybe I can step back and, and use the, this Laplacian operator as

11:29.960 --> 11:33.080
a tool to maybe go from one world to the other.

11:33.080 --> 11:37.640
And as it turns out, the Laplacian is a, is a operator that is intrinsically defined with

11:37.640 --> 11:38.640
very weak assumptions.

11:38.640 --> 11:43.200
In it, what it means is that even on, on a graph or on a manifold, it's an operator that,

11:43.200 --> 11:49.080
that exists, and it's, well, it's, it has a very complete and rich structure and status.

11:49.080 --> 11:54.520
And these, the application of the Laplacian to these different domains, this is all work

11:54.520 --> 11:58.280
that, you know, was done with regard to that domain, independent of what we're trying

11:58.280 --> 11:59.960
to do here with machine learning.

11:59.960 --> 12:00.960
Yeah, absolutely.

12:00.960 --> 12:04.400
I mean, I think that, I mean, I, I would not claim that I know exactly when these things,

12:04.400 --> 12:10.360
but I'm sure that many famous physicists from even like the 90th century were aware of,

12:10.360 --> 12:13.800
you know, the distance and the generality of this operator, right?

12:13.800 --> 12:19.720
And this is way, I think I believe that when people refer to spectrograph theory, it's

12:19.720 --> 12:25.280
most of it, or a big part of it, is in developing the properties of Laplacian and related operators

12:25.280 --> 12:26.280
in graph.

12:26.280 --> 12:28.560
As applied to graphs, yes, okay.

12:28.560 --> 12:35.360
And so, so in that respect, the Fourier transform is now a concept that you can, of course,

12:35.360 --> 12:40.440
if you analyze it and the occlusion domain, it has a very, again, a very rich structure

12:40.440 --> 12:43.360
and it's useful beyond our imagination, right?

12:43.360 --> 12:47.200
It's going to be used to do fast, mostly thanks to the fast Fourier transform.

12:47.200 --> 12:53.480
But it's also an object that exists in general graphs, perhaps not with the only detail

12:53.480 --> 12:57.280
that is like a single detail, but might be very important, is that it doesn't come with

12:57.280 --> 12:58.760
a fast algorithm, right?

12:58.760 --> 13:04.200
That's the, I would say that this is the main, or one of the main technical differences.

13:04.200 --> 13:10.760
So, when I think of the Fourier transform or the FFT, I think of, you have some input

13:10.760 --> 13:15.880
signal and you pass it through this Fourier transformation and it basically decomposes

13:15.880 --> 13:18.400
it into its frequency parts.

13:18.400 --> 13:20.200
How does that apply to a graph?

13:20.200 --> 13:22.120
What does that even mean?

13:22.120 --> 13:25.840
So, on the graph, basically, in the Euclidean case, you can, as you said, you can represent

13:25.840 --> 13:29.840
a function or a signal as a superposition of signs or co-science, right?

13:29.840 --> 13:31.600
Basically, some harmonic functions.

13:31.600 --> 13:36.720
So, these functions turn to be eigenfunctions or eigenvectors of the Euclidean operator,

13:36.720 --> 13:40.880
in the Euclidean case and the one-dimensional case, it's just the second order derivative.

13:40.880 --> 13:45.320
So, basically, if you replace this Euclidean operation with a non-Euclidean operation, with

13:45.320 --> 13:49.360
a graph operation, or in a manifold, that would be what is called differential geobotid,

13:49.360 --> 13:53.200
a plus-biltrami operator, basically, you get exactly the same thing.

13:53.200 --> 13:58.240
You get eigenfunctions of this operator that turn to be an orthogonal basis, basically,

13:58.240 --> 13:59.840
it's a self-adjoint operator.

13:59.840 --> 14:01.840
So, it has orthogonal eigen decomposition.

14:01.840 --> 14:07.400
The eigenvalues play a role of frequencies that are exactly, as Ron mentioned, vibration

14:07.400 --> 14:13.440
modes of, yeah, basically, the eigenvalues that you obtain in the heliports or the spatial

14:13.440 --> 14:17.840
part of the wave equation that governs vibration of a domain.

14:17.840 --> 14:22.760
Ron, you were saying, no, it's just that maybe one way where you can maybe picture this

14:22.760 --> 14:28.000
thing in your head is, yeah, if you take like a spherical plate that would correspond

14:28.000 --> 14:34.240
to a drum, as we understand it, right, just a drum, and then you could imagine like deforming

14:34.240 --> 14:39.960
somehow the drum and just playing it, you would, intrinsically, you would still be playing

14:39.960 --> 14:45.320
things that can be seen as a superposition of like fundamental waves that would look

14:45.320 --> 14:49.640
a bit more funny, right, that would probably adapt, and they would be like the formation

14:49.640 --> 14:54.840
of the original science and coscience, depending on how you have to form the original drums.

14:54.840 --> 14:55.840
Ron, interesting.

14:55.840 --> 14:57.240
This is interesting stuff.

14:57.240 --> 15:03.640
It's bringing me back to my DSP class and I don't think eigenvalues and eigenvectors

15:03.640 --> 15:07.720
has really come up much on the podcast either, but we're not going to go into the linear algebra.

15:07.720 --> 15:10.440
We'll assume a bunch of that.

15:10.440 --> 15:19.840
So basically just to kind of, to catch us up, this is all kind of background to the tutorial.

15:19.840 --> 15:25.080
Maybe let's take a moment to talk about applications of this to make it a little bit more concrete

15:25.080 --> 15:26.080
for everyone.

15:26.080 --> 15:30.400
How was the, some of the stuff that you're doing applied?

15:30.400 --> 15:34.960
So maybe, yeah, we can illustrate a couple of very different applications.

15:34.960 --> 15:39.000
So one that I'm personally involved in is Article Physics.

15:39.000 --> 15:44.080
So the sort of experiments that people do in a large collider and like in particle accelerators

15:44.080 --> 15:49.720
where they're there, the goal of the game is to say very precise things about how the

15:49.720 --> 15:53.120
standard model, like some very specific properties of the standard model.

15:53.120 --> 15:58.680
So the way it works is that you do like a very, an experiment where you clash two particles

15:58.680 --> 16:03.360
certain, you know, with very, very large speed and then they produce what they call a jet.

16:03.360 --> 16:08.400
And a jet is like a shower of little events that can be detected through a very expensive

16:08.400 --> 16:11.800
and very sophisticated detector, like looks like a cylinder.

16:11.800 --> 16:15.720
So and the goal of the game is to say, well, given this observation that looks like this

16:15.720 --> 16:21.360
point cloud in my detector, how can I infer what was the underlying physical theory, right?

16:21.360 --> 16:26.800
And so it's really a machine learning and their machine learning is a very, very fundamental

16:26.800 --> 16:30.080
step for this experimental physicist.

16:30.080 --> 16:34.960
And so there you use this techniques that we described in the tutorial to essentially

16:34.960 --> 16:41.920
learn a data driven model that looks as input a set of like a point cloud in model in the

16:41.920 --> 16:47.760
color limiter and tries to predict if it was due to theory A or theory B.

16:47.760 --> 16:52.640
Before we go to the other application, when I think about when I visualize this, I'm

16:52.640 --> 16:58.320
visualizing, you know, something that, you know, is very well within the bounds of Euclidean

16:58.320 --> 16:59.320
stuff, right?

16:59.320 --> 17:04.880
It's a three-dimensional point cloud, like why do we need to apply the stuff that you've

17:04.880 --> 17:07.600
done as opposed to the usual stuff?

17:07.600 --> 17:08.600
Very good question.

17:08.600 --> 17:11.960
So the answer is that you're not forced to apply what you do.

17:11.960 --> 17:18.480
So if you decide to stay in the Euclidean route, you would have to somehow quantify your

17:18.480 --> 17:21.240
measurements, such that they look like a regular grid.

17:21.240 --> 17:24.520
And so we thought there are two potential risks.

17:24.520 --> 17:30.040
The first one is that if the precision that you need, if the little blobs that the particles

17:30.040 --> 17:35.680
create are very small, you might need to sample at, you know, you will need a sampling rate

17:35.680 --> 17:38.760
that will make inputs incredibly large, right?

17:38.760 --> 17:44.600
That you will basically be paying a huge price in order to transform your input into something

17:44.600 --> 17:46.320
that is not regular grid.

17:46.320 --> 17:50.640
Maybe another way to put that is in order to solve this in Euclidean space, you have

17:50.640 --> 17:56.640
like a quantization noise that you have to sample more than otherwise if you were to use

17:56.640 --> 17:57.640
a different.

17:57.640 --> 18:02.760
So that's a trade-off, right, between, so once you choose a quantization step, there's

18:02.760 --> 18:07.560
a trade-off between how large and how sparse your input is going to look like versus how

18:07.560 --> 18:09.680
much resolution you are going to lose.

18:09.680 --> 18:15.560
So in that respect, the models that we propose here, they are an alternative that don't have

18:15.560 --> 18:16.720
this limitation, right?

18:16.720 --> 18:22.200
That can stay at the native, let's say they don't lose any information because we don't

18:22.200 --> 18:25.280
need to quantize, we don't need to go into this regular grid.

18:25.280 --> 18:31.000
And I would say that there's another potential advantage is that when you look at the experiment

18:31.000 --> 18:35.400
that comes from a detector, as if it was an image, right, in a regular grid, the underlying

18:35.400 --> 18:40.760
assumption is that the statistics, you know, the statistics of the input, they are following

18:40.760 --> 18:45.400
the same assumption as the statistics of natural images, namely that everything is stationary

18:45.400 --> 18:50.920
and that there's a, I would say, a canonical way to compare, like, to measure distance between

18:50.920 --> 18:52.720
points, right?

18:52.720 --> 18:59.000
Whereas in physicists, they have a very good and sophisticated notions of how one should

18:59.000 --> 19:00.000
be comparing particles.

19:00.000 --> 19:05.960
Okay, so there's a, in other words, there's a more physics-aware version of a neural network

19:05.960 --> 19:11.240
that, I mean, another way to say it is that for physicists, it's very important to have

19:11.240 --> 19:15.640
a model where you can infuse and you can incorporate, like, prior information about how things

19:15.640 --> 19:17.520
should be compared into the model.

19:17.520 --> 19:24.280
And so the graph formulation that we are working on is allowing you to incorporate this thing

19:24.280 --> 19:25.280
into the model.

19:25.280 --> 19:29.440
Whereas if you use just the quantization and the standard convolution of your network,

19:29.440 --> 19:33.760
it doesn't seem so easy to incorporate and to accommodate for this.

19:33.760 --> 19:34.760
Okay.

19:34.760 --> 19:40.960
So when I go from my picture of the three-dimensional Euclidean for this application to maybe

19:40.960 --> 19:46.160
something that's a little bit less traditional, I think, of, like, maybe looking at your

19:46.160 --> 19:49.000
points in some kind of spherical coordinate system.

19:49.000 --> 19:53.360
Is that kind of, is that what you're doing or is it, because we keep coming back to graph

19:53.360 --> 19:55.960
and I'm trying to wrap my head around where graph.

19:55.960 --> 19:56.960
Yes.

19:56.960 --> 20:03.240
So here, the graph enters the game as just like a data structure that you use to relate

20:03.240 --> 20:04.240
particles.

20:04.240 --> 20:05.760
Okay, so I think that your input is discreet.

20:05.760 --> 20:10.480
It contains a number of particles that can be variable, depending on each event.

20:10.480 --> 20:17.000
And what you learn is a network that learns how to relate, how to relate and propagate

20:17.000 --> 20:21.080
information across the set of particles.

20:21.080 --> 20:24.680
And so it doesn't see it into this extrinsic Euclidean space.

20:24.680 --> 20:30.680
So of course, you could say, well, you know, it's just a, I mean, yeah, I can see my input

20:30.680 --> 20:32.360
as being n particles.

20:32.360 --> 20:36.160
I can also see it as, you know, as you say, just put everything in the sphere and then

20:36.160 --> 20:39.200
I just see it as a function, like it's an image in that sphere, right?

20:39.200 --> 20:42.600
And then the number, the notion that I have in particles completely disappears, right?

20:42.600 --> 20:44.680
And then you can connect everything with everything.

20:44.680 --> 20:49.320
I would say that it's not that there's one approach that is systematically better than

20:49.320 --> 20:50.320
the other.

20:50.320 --> 20:53.680
It's really, there's context in which you might want to use one formulation versus the

20:53.680 --> 20:54.680
other.

20:54.680 --> 20:58.920
So what I see here is that we are just providing another tool that now our practitioners

20:58.920 --> 21:02.360
can use and maybe then they can combine with the other one.

21:02.360 --> 21:08.240
And we also have reasons to believe that in some contexts, one formulation might scale

21:08.240 --> 21:09.680
better than the other.

21:09.680 --> 21:15.320
For example, if you're having a moment of four dimensions, I had a moment of, you know,

21:15.320 --> 21:21.400
like eight dimensions, well, in our case, the architectural changes to the model model

21:21.400 --> 21:22.560
would be minimal, right?

21:22.560 --> 21:29.280
There's nothing in our scaling that depends on the, like it depends very quickly on the

21:29.280 --> 21:31.040
dimension of the input.

21:31.040 --> 21:36.760
Whereas if you had to do the quantization on the domain, you would pay a huge price for

21:36.760 --> 21:39.240
this small change.

21:39.240 --> 21:40.240
Interesting.

21:40.240 --> 21:42.080
So you're going to give a second example.

21:42.080 --> 21:45.280
I can give a second example or Michael can give an example as you prefer.

21:45.280 --> 21:46.800
Michael, why don't you give one?

21:46.800 --> 21:47.800
Yeah.

21:47.800 --> 21:50.640
So I can give you as an example, a paper that actually will be presenting tomorrow here

21:50.640 --> 21:51.640
at NIPS.

21:51.640 --> 21:52.640
Okay.

21:52.640 --> 21:54.080
And the example is recommended systems.

21:54.080 --> 21:58.560
So, you know, probably the most famous example is the Netflix problem, you know, the

21:58.560 --> 22:04.400
Netflix is a movie rental company and they have probably tens of millions of users and

22:04.400 --> 22:07.160
probably hundreds of thousands of different movies.

22:07.160 --> 22:11.360
So the users can give different scores to movies, whether they like the movie or not,

22:11.360 --> 22:14.040
let's say, on a scale between zero and nine.

22:14.040 --> 22:18.200
So basically yet, you can describe this information as a huge matrix that is very sparse

22:18.200 --> 22:23.040
example because, of course, even if you watch continuously the movies throughout your entire

22:23.040 --> 22:27.120
lifetime, you'll probably watch just a small percentage of what they have.

22:27.120 --> 22:30.560
So they want to feel in the missing entries of these huge metrics, basically they try

22:30.560 --> 22:31.960
to interpolate it.

22:31.960 --> 22:35.640
And the standard approach is user algebraic techniques, basically try to fit a low-dimensional

22:35.640 --> 22:41.960
model to these data, minimizing the matrix rank or more correctly, basically the convex

22:41.960 --> 22:45.320
proxy of the rank, the so-called nuclear norm.

22:45.320 --> 22:49.280
So this problem formation doesn't have any geometric structure, for example.

22:49.280 --> 22:53.040
You can shuffle the columns and the rows of the matrix.

22:53.040 --> 22:56.880
If you remove one of the columns, there are infinitely many ways you can fill it in.

22:56.880 --> 23:01.000
So if you have some either side information that you can construct it from the data, some

23:01.000 --> 23:05.840
notion of affinity between users or items or actually both, that you can represent as

23:05.840 --> 23:07.720
a graph, for example.

23:07.720 --> 23:12.280
So think of a social network and maybe a little bit naive model that friends have similar

23:12.280 --> 23:13.280
tastes.

23:13.280 --> 23:18.240
So this already allows you to think of the matrix as some kind of space with geometric

23:18.240 --> 23:19.240
structure.

23:19.240 --> 23:22.880
You can talk about notions like smoothness, so basically you can say that the values in

23:22.880 --> 23:28.120
these metrics vary significantly or insignificantly when you go from one vertex on a graph to

23:28.120 --> 23:32.520
another vertex on a graph, and you can actually learn optimal filters, spectral filters.

23:32.520 --> 23:36.760
For example, on these graphs, actually it's a product of two graphs, so you can, the best

23:36.760 --> 23:41.400
analogy from single processing will be a two-dimensional free-attent form, like free-attent form and

23:41.400 --> 23:42.400
image.

23:42.400 --> 23:44.400
Okay, two-dimensional free-attent form.

23:44.400 --> 23:45.400
Yeah.

23:45.400 --> 23:47.200
So think of, and the free-attent form of an image, right?

23:47.200 --> 23:50.200
You apply free-attent form to the columns and rows of the image.

23:50.200 --> 23:55.000
So here instead of Euclidean structured rows and columns have matrix that leaves on two

23:55.000 --> 23:56.000
different graphs.

23:56.000 --> 24:00.760
Right, so obviously the rows for example represent items or movies, and the columns represent

24:00.760 --> 24:01.760
users.

24:01.760 --> 24:06.120
So these are two different graphs, and you can apply filters in this frequency domain

24:06.120 --> 24:11.680
that is now basically characterized by the eigenvalues of the two operations of the rows

24:11.680 --> 24:13.960
and the column and the column graphs.

24:13.960 --> 24:18.560
And basically this way you can have a better way of feeling in the missing elements of

24:18.560 --> 24:23.920
the matrix that accounts for the similarities between different users and movies.

24:23.920 --> 24:31.120
How specifically does the frequency domain, the Fourier transform, tie into the graph itself

24:31.120 --> 24:33.760
and in this context?

24:33.760 --> 24:37.680
So it's very similar to what JoÃ£o described before, basically the spectral definition

24:37.680 --> 24:38.680
of convolution, right?

24:38.680 --> 24:41.440
You can do a convolution in the spectral domain, right?

24:41.440 --> 24:45.720
The classical convolution, this is what we call the conversion theorem in single processing

24:45.720 --> 24:50.880
that you can implement filtering or convolution in the frequency domain just as a product,

24:50.880 --> 24:52.960
point-wise product of free-attent forms, right?

24:52.960 --> 24:56.960
And this is what we usually do in standard single processing because we can efficiently

24:56.960 --> 24:59.840
compute the free-attent form using F50.

24:59.840 --> 25:01.600
And you can do the same thing for images, right?

25:01.600 --> 25:05.200
You can do two-dimensional filters in the frequency domain.

25:05.200 --> 25:09.240
So basically extend this analogy to a matrix data.

25:09.240 --> 25:12.800
Basically you can think of it as an image, but the domains where the rows and the columns

25:12.800 --> 25:19.320
of these image leave, they have graph structure, sort of standard Euclidean structure.

25:19.320 --> 25:26.120
If someone has a problem where they've got some set or sets of entities that have some

25:26.120 --> 25:33.080
inherent structure, one relative to the other, as opposed to some of the more traditional

25:33.080 --> 25:39.000
image, an image or other data types, that's an area where they could be looking at an

25:39.000 --> 25:42.640
approach like this and it might give them some advantage.

25:42.640 --> 25:43.640
Absolutely.

25:43.640 --> 25:50.720
So you mentioned a setting that is now approached and defined through different names, so now

25:50.720 --> 25:54.400
there's something that is becoming very popular recently called a meta-learning, right?

25:54.400 --> 26:01.360
Where is this idea that maybe rather than using a traditional supervised learning, where

26:01.360 --> 26:06.320
I have very few number of samples and then very few number of labels and then the environment

26:06.320 --> 26:09.440
is non-stationary, so it changes all again and again.

26:09.440 --> 26:15.280
Maybe what I can learn is a rule that by looking at how maybe the inputs relate to each other,

26:15.280 --> 26:20.680
even if the individual distribution of the labels and the images change, maybe the underlying

26:20.680 --> 26:26.040
relationship between labels and images that I need to learn is something that I can leverage

26:26.040 --> 26:28.160
by exploring more and more data.

26:28.160 --> 26:34.440
So once you start setting up problems using this formulation, you end up with a task where

26:34.440 --> 26:37.960
you have to learn how to relate different things, very different objects.

26:37.960 --> 26:43.360
And so this is a terrain where it's very natural to look at our models, so we have a recent

26:43.360 --> 26:49.160
paper that is currently under review where we think about what's called a few-shot learning

26:49.160 --> 26:52.360
problem using this model, using a graphical network.

26:52.360 --> 26:57.840
And so what is interesting is that somehow it generalizes and it includes, as particular

26:57.840 --> 27:02.640
cases, some of the models that people have been using and developing in the recent year

27:02.640 --> 27:05.080
or so to attack this problem.

27:05.080 --> 27:11.400
And so this is just to say that there are many tasks that you could imagine across AI

27:11.400 --> 27:16.040
and across sciences, the underlying thing that you need to learn is how to relate objects

27:16.040 --> 27:17.040
to each other.

27:17.040 --> 27:22.880
And so once you have to do this relational task, then the natural data structure for that

27:22.880 --> 27:25.440
is a graph, just a point cloud.

27:25.440 --> 27:30.480
So I expect that they will see more and more applications of these technology in the

27:30.480 --> 27:32.320
near future.

27:32.320 --> 27:37.440
So one of the things that this conversation brings to mind is a recent conversation I

27:37.440 --> 27:38.440
had.

27:38.440 --> 27:43.280
In fact, here it nips with Sri Ram Natharajan, who studies statistical relational AI.

27:43.280 --> 27:47.760
Are you familiar with that line of research and how that relates to this?

27:47.760 --> 27:48.760
Not really.

27:48.760 --> 27:49.760
Okay.

27:49.760 --> 27:50.760
Just check it out.

27:50.760 --> 27:58.640
Similar thinking he's also looking at graph-based approaches and applying them to the healthcare

27:58.640 --> 28:01.360
domain and other domains.

28:01.360 --> 28:07.560
So in the case of the, you know, any of the examples we've talked about, I can, you know,

28:07.560 --> 28:12.000
we've already talked about some of the advantages of this approach over traditional approaches.

28:12.000 --> 28:19.000
One of them is that in the case of the particle physics work, it's, you know, maybe more intuitive

28:19.000 --> 28:23.400
for the physicists because it maps more closely to the tools and the way they're used to thinking

28:23.400 --> 28:25.400
about the domain.

28:25.400 --> 28:27.720
What are the other advantages of this approach?

28:27.720 --> 28:33.240
Are there performance advantages in terms of either computational or model accuracy or

28:33.240 --> 28:34.240
things like that?

28:34.240 --> 28:35.240
Yeah.

28:35.240 --> 28:40.200
So what I would say that the main advantage is definitely the fact that it's more general

28:40.200 --> 28:44.760
so you can, there are problems in which it might be the only, your only choice, right?

28:44.760 --> 28:48.040
That there's no, there's no, including alternative to do so.

28:48.040 --> 28:53.840
I would say that whenever you're, I mean, you could ask the question, well, if I just forget

28:53.840 --> 28:57.920
about the grid structure of an image and just treat it as a graph, right?

28:57.920 --> 29:01.880
And I run my model, this model is it going to do better than the CNN?

29:01.880 --> 29:02.880
Right.

29:02.880 --> 29:03.880
I would say that the answer is no.

29:03.880 --> 29:04.880
Yeah.

29:04.880 --> 29:05.880
That's not really the point, right?

29:05.880 --> 29:06.880
It's not really the point, right?

29:06.880 --> 29:12.680
So I would say that the main strength of, of the model is really to, to respect the

29:12.680 --> 29:17.000
invariance of the data, for example, if you are treating a, you know, if you are just

29:17.000 --> 29:20.960
observing a, you know, a point cloud, you know, that the order in which I give you the

29:20.960 --> 29:23.240
point cloud is completely relevant, right?

29:23.240 --> 29:28.480
So therefore, you're, you're, if you can certify that your model is going to exactly, if

29:28.480 --> 29:33.760
exactly the same output independent of how the, the input are permitted, then you are kind

29:33.760 --> 29:36.760
of respecting that this kind of natural invariant of the data.

29:36.760 --> 29:41.640
So I would say that, that relative to models that are, for example, maybe using sequential

29:41.640 --> 29:46.840
networks, like, for example, Rekord neural networks, here, it could be that, that eventually

29:46.840 --> 29:51.400
these models based on graphs and sets are going to be more sample efficient and maybe

29:51.400 --> 29:56.480
they can get you better performance precisely because they are a bit more tailored to the

29:56.480 --> 30:00.440
data format, like to the input data format.

30:00.440 --> 30:04.720
Maybe a good example would be applications from the domain of graphics and computer vision

30:04.720 --> 30:07.960
and in particular analysis of deformable 3D shapes.

30:07.960 --> 30:12.440
So this is actually, I think it's a good illustration because you can treat such objects in two

30:12.440 --> 30:13.440
different ways.

30:13.440 --> 30:16.800
You can treat them as Euclidean objects, basically, they are things that live in three-dimensional

30:16.800 --> 30:17.800
space, right?

30:17.800 --> 30:22.600
You can apply standard, let's say, convolutional neural networks, maybe, on volumetric representations

30:22.600 --> 30:27.320
of, of these objects, or you can think of them intrinsically from the perspective of differential

30:27.320 --> 30:30.320
geometry, basically, model them as many folds.

30:30.320 --> 30:35.960
And what you gain in this way by resorting to these kind of architectures is, you gain

30:35.960 --> 30:37.480
a deformation invariance.

30:37.480 --> 30:41.480
So basically, your model is by construction invariant to inelastic deformations of the

30:41.480 --> 30:42.480
shape.

30:42.480 --> 30:45.720
And if you're a task, for example, is deformation invariant correspondence or deformation

30:45.720 --> 30:51.320
invariance similarity, it means that you can deal with way less training examples because

30:51.320 --> 30:55.120
you don't need to show to the network all the possible deformations that the shapes can

30:55.120 --> 30:59.040
undergo in order to learn these deformations from examples.

30:59.040 --> 31:02.160
You basically have these invariance built into the model.

31:02.160 --> 31:03.880
And the difference can be very dramatic.

31:03.880 --> 31:08.000
The difference can be in orders of magnitude, less training samples.

31:08.000 --> 31:16.640
So in other words, the approach you're taking to model, because it's more tailored to the

31:16.640 --> 31:21.360
problem domain, it kind of restricts your search space and you don't have to provide examples

31:21.360 --> 31:26.400
for things that wouldn't really exist in the domain itself, but do exist geometrically

31:26.400 --> 31:28.000
in Euclidean space.

31:28.000 --> 31:29.000
Exactly.

31:29.000 --> 31:34.080
Or maybe a better way to say it is that you try to model axiomatically as much as possible

31:34.080 --> 31:38.460
or as much as make sense in your specific problem and everything that cannot be modeled

31:38.460 --> 31:42.320
axiomatically because, of course, there is a limitation to what you can model basically

31:42.320 --> 31:48.320
in handcrafted way, everything that deviates from your model you learn.

31:48.320 --> 31:49.320
Okay.

31:49.320 --> 31:53.840
Other other topics that you covered in tutorial that we haven't touched on yet?

31:53.840 --> 32:00.400
So just very briefly, so one other potential area of application that we are currently exploring

32:00.400 --> 32:05.560
to what extent, if you now have a language to learn our graph-based structure, you can

32:05.560 --> 32:10.040
use it for a combinatorial optimization problems that are naturally defined over graphs.

32:10.040 --> 32:14.240
So this is a completely different domain of application, because there the goal is not

32:14.240 --> 32:19.240
so much to solve a task that you don't know how to solve, is more to solve or to approximate

32:19.240 --> 32:20.240
a task faster.

32:20.240 --> 32:23.480
We're talking like traveling salesmen and these kinds of graphs.

32:23.480 --> 32:24.480
Exactly.

32:24.480 --> 32:25.480
Exactly.

32:25.480 --> 32:29.160
So we briefly touched upon one of such problems in the tutorial, which is the quadratic

32:29.160 --> 32:30.160
assignment problem.

32:30.160 --> 32:31.160
But what?

32:31.160 --> 32:37.160
The quadratic assignment, which contains the travel statement problem is that you can think

32:37.160 --> 32:40.360
it as a particular case of that one.

32:40.360 --> 32:46.840
So there the general setup is really an instant of this trend that you can always have this

32:46.840 --> 32:51.680
analogy between an algorithm to solve a task and a neural network.

32:51.680 --> 32:56.080
And this analogy works by looking at the algorithm and then unraveling typically the algorithm

32:56.080 --> 33:01.720
involved a series of iterative steps, iterations, so you can just see these iterations as being

33:01.720 --> 33:04.400
different layers of a network.

33:04.400 --> 33:09.000
And then once you have this analogy, then you can try to study like a trade-offs between

33:09.000 --> 33:14.680
a computation and accuracy, by just replacing the guarantees that the algorithm gives you

33:14.680 --> 33:19.800
by just a data driven approach where you just feed the parameters of the network to a

33:19.800 --> 33:21.680
dataset of solved problems.

33:21.680 --> 33:28.880
So this is an interesting and potentially also useful because in many domains, especially

33:28.880 --> 33:33.080
when it comes to combinatorial optimization, there are problems in which it's still an

33:33.080 --> 33:38.680
open research area, how to come up with a vision approximation of intractable problems.

33:38.680 --> 33:44.760
So here, one of the potential uses of what we presented is, well, now we are providing

33:44.760 --> 33:50.880
a family of, let's say, trainable architectures that can be used to guide and to provide good

33:50.880 --> 33:55.120
trade-offs between accuracy and complexity for problems such as the travel experience,

33:55.120 --> 33:57.520
salesman, or other interesting things.

33:57.520 --> 33:58.520
Interesting.

33:58.520 --> 34:02.600
So the paraphrase is that what you've done is your research is kind of providing a way

34:02.600 --> 34:10.600
to express graph-oriented problems in terms of neural networks, traveling salesmen, map

34:10.600 --> 34:15.320
coloring, all these other combinatorial kind of graph problems.

34:15.320 --> 34:20.560
They're typically very difficult to solve exactly and so there are all kinds of approximations

34:20.560 --> 34:25.320
and heuristics, but for a certain level of complexity, those don't work very well.

34:25.320 --> 34:31.200
So now, your research applied to them gives you a way to solve these using neural networks.

34:31.200 --> 34:32.200
Well, yes.

34:32.200 --> 34:33.720
So I would not say potentially.

34:33.720 --> 34:34.720
Potentially.

34:34.720 --> 34:35.720
Potentially, exactly.

34:35.720 --> 34:39.720
It's a question mark and I think it's a question mark that it's worth exploring, right?

34:39.720 --> 34:40.720
That's correct.

34:40.720 --> 34:41.720
Sure.

34:41.720 --> 34:48.600
Because in some applications, it could be useful that, for example, not just have a single

34:48.600 --> 34:56.520
algorithm with a single aristic, but to have a toggle that you can select between how

34:56.520 --> 35:00.600
many cycles do you want to spend versus how much accurate do you want the solution to

35:00.600 --> 35:01.600
be, right?

35:01.600 --> 35:07.560
And be able to learn adaptive trade-offs and all these things are interesting.

35:07.560 --> 35:12.840
Then there's another declination of this area of research that is a bit more going into

35:12.840 --> 35:18.560
the theoretical computer science, namely to what extent the models that we learn could

35:18.560 --> 35:22.000
be interpreted as algorithms that we still don't know.

35:22.000 --> 35:27.240
This might be, it could well be that it doesn't work because it relies on this fact that

35:27.240 --> 35:31.080
can we interpret or can we uncover what the neural network is learning?

35:31.080 --> 35:33.320
And we know that this is typically a hard thing to do, right?

35:33.320 --> 35:37.440
Even for a convolutional neural network, we don't really know how the network figures

35:37.440 --> 35:39.000
out how to solve the problem.

35:39.000 --> 35:44.000
But what I'm saying is that in some context, it could be interesting to try to understand

35:44.000 --> 35:47.560
and analyze kind of things that the network learned.

35:47.560 --> 35:50.360
So we have these graph problems.

35:50.360 --> 35:53.560
There are graph problems in computer science as well.

35:53.560 --> 35:58.560
Your research allows us to express those as neural networks if we could then peer into

35:58.560 --> 36:02.560
the neural network that might give us some insight into these computer science problems

36:02.560 --> 36:04.960
that we're trying to model in a first place.

36:04.960 --> 36:05.960
Yes.

36:05.960 --> 36:06.960
Interesting.

36:06.960 --> 36:07.960
Interesting.

36:07.960 --> 36:08.960
How about implementation?

36:08.960 --> 36:12.760
Like I'm imagining, like at this point in time, you can't just do, you know, TensorFlow

36:12.760 --> 36:13.760
TF.

36:13.760 --> 36:16.800
You know, graph solver and do this.

36:16.800 --> 36:18.000
How does that work?

36:18.000 --> 36:22.520
So to some extent, we are trying to leverage existing tools not to reinvent the wheel.

36:22.520 --> 36:25.120
Basically, the underlying framework is a standard one.

36:25.120 --> 36:27.680
We use TensorFlow, for example.

36:27.680 --> 36:32.880
We just create some custom things that then boil down again to some standard operations

36:32.880 --> 36:35.080
like matrix multiplication.

36:35.080 --> 36:39.160
So basically the short answer is yes, it is that easy.

36:39.160 --> 36:42.040
It is built on top of some standard frameworks.

36:42.040 --> 36:43.040
Okay.

36:43.040 --> 36:51.040
Some minor but potentially profound differences in the fact that the scaling up, like using

36:51.040 --> 36:56.920
these models on large graphs or large domains involves, you know, matrix multiplication

36:56.920 --> 36:58.200
with matrix that are large.

36:58.200 --> 37:02.400
And so the structure that we have is that these matrices are sparse.

37:02.400 --> 37:08.800
And so hopefully we see more and more integration of sparse linear algebra into PyTorge and

37:08.800 --> 37:15.560
TensorFlow, etc. but there's a fundamental difference now that maybe the hardware, like

37:15.560 --> 37:22.720
GPUs, they are excelling at a specific form of operation that is not very friendly with

37:22.720 --> 37:24.800
sparse matrix multiplication.

37:24.800 --> 37:25.800
Okay.

37:25.800 --> 37:31.160
But again, I'm not an expert in this low-level implementation, but this, I would say,

37:31.160 --> 37:35.640
is one of the main differences between, you know, running a complement or running a

37:35.640 --> 37:37.640
graph convolution.

37:37.640 --> 37:44.000
So you've released some code that works on their TensorFlow, but it isn't necessarily

37:44.000 --> 37:46.880
amenable to scaling up just yet.

37:46.880 --> 37:49.240
There's stuff that needs to be figured out.

37:49.240 --> 37:54.640
Maybe a one way to get a sense for the complexity of this has anyone like beyond the, you know,

37:54.640 --> 37:58.360
the two of you and folks that are like deep in this research use this.

37:58.360 --> 38:04.520
Are you aware of any like external arms length applications?

38:04.520 --> 38:07.680
So people, different people from different communities try to use.

38:07.680 --> 38:13.640
I wouldn't say that it's extremely popular yet, but it probably, it starts to become.

38:13.640 --> 38:18.480
So many different domains, many different applications can be, the problems in these domains

38:18.480 --> 38:23.520
can be formulated using graphs, graphs at the end are very generic and very convenient

38:23.520 --> 38:28.720
representation of any kind of relations or interactions you can think of.

38:28.720 --> 38:33.360
So that's really, very generic framework of describing certain types of data.

38:33.360 --> 38:38.520
So yeah, people that are even not experts in machine learning that come from different

38:38.520 --> 38:44.200
domains that bring certain applications, they try to basically, they see that graphs allow

38:44.200 --> 38:49.640
to formulate their problems in a natural way and they are curious to try out these approaches.

38:49.640 --> 38:50.640
Okay.

38:50.640 --> 38:51.640
Great.

38:51.640 --> 38:52.640
Great.

38:52.640 --> 38:52.640
This is really exciting stuff.

38:52.640 --> 38:59.240
Where can folks go to learn more about it, download the TensorFlow code or read some

38:59.240 --> 39:00.240
of the papers?

39:00.240 --> 39:06.200
So we have a dedicated website that is easy to remember, it's geometricdiplearning.com.

39:06.200 --> 39:07.720
geometricdiplearning.com also.

39:07.720 --> 39:11.520
Where, yeah, where I think the idea is to have all the tutorial material.

39:11.520 --> 39:14.400
We have the review paper that Michael mentioned.

39:14.400 --> 39:20.880
We have also the recent literature by not just us, but other groups that use the tools.

39:20.880 --> 39:24.800
And then we are also going to have in two months, we're going to have an IPAM workshop here

39:24.800 --> 39:30.760
in Los Angeles, where I think I'm also looking forward to it because it's what you are

39:30.760 --> 39:36.040
saying that people from different domains and people from different disciplines will

39:36.040 --> 39:42.560
come together and essentially present their data problem or their information.

39:42.560 --> 39:48.000
And what I'm expecting is that we are going to see more and more this realization that

39:48.000 --> 39:55.480
actually the models and the tools can be used across more domains than maybe we are expecting.

39:55.480 --> 39:56.480
Okay.

39:56.480 --> 39:57.480
Awesome.

39:57.480 --> 39:58.480
Great.

39:58.480 --> 40:01.320
Well, Joanne, Michael, thank you so much for taking the time to chat.

40:01.320 --> 40:02.320
I enjoy the conversation.

40:02.320 --> 40:05.320
Thank you very much.

40:05.320 --> 40:10.720
All right, everyone, that's our show for today.

40:10.720 --> 40:15.840
Thanks so much for listening and for your continued feedback and support.

40:15.840 --> 40:22.800
To follow along with the NIP series, visit twimmelai.com slash NIPS 2017.

40:22.800 --> 40:29.440
To enter our Twimmel 1 Mill contest, visit twimmelai.com slash Twimmel 1 Mill.

40:29.440 --> 40:35.160
Of course, we'd be delighted to hear from you either via a comment on the show notes page

40:35.160 --> 40:40.360
or via a tweet to add Twimmelai or add Sam Charrington.

40:40.360 --> 40:44.880
Thanks once again to Intel Nirvana for their sponsorship of this series.

40:44.880 --> 40:49.600
To learn more about the Intel Nirvana NNP and the other things Intel's been up to in

40:49.600 --> 40:53.840
the AI arena, visit intelnervana.com.

40:53.840 --> 40:58.600
As I mentioned a few weeks back, this will be our final series of shows for the year.

40:58.600 --> 41:03.840
So take your time and take it all in and get caught up on any of the old pods you've been

41:03.840 --> 41:05.440
saving up.

41:05.440 --> 41:07.840
Happy holidays and happy new year.

41:07.840 --> 41:10.120
See you in 2018.

41:10.120 --> 41:14.240
And of course, thanks once again for listening and catch you next time.

