1
00:00:00,000 --> 00:00:16,320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

2
00:00:16,320 --> 00:00:21,480
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:21,480 --> 00:00:35,440
I'm your host Sam Charrington. A big thanks to everyone who participated in, voted

4
00:00:35,440 --> 00:00:42,560
in and supported our My AI contest. I'm excited to announce that our first place winner is,

5
00:00:42,560 --> 00:00:54,240
and I get a drum roll, Mohit Nalavati. Mohit sent in the video personal health care applications,

6
00:00:54,240 --> 00:01:01,380
which you can find along with the other entries at Twimbleai.com slash My AI. Second prize

7
00:01:01,380 --> 00:01:09,000
goes to Julian Ford, for his entry, dad and Eleanor. And third prize goes to Matt Taylor

8
00:01:09,000 --> 00:01:16,080
for his poetic and ominously named entry, let Earth not become our tomb. Thank you all

9
00:01:16,080 --> 00:01:22,680
for entering and stay tuned for our next contest. A special shout out to Lighthouse and Anki

10
00:01:22,680 --> 00:01:31,760
for donating the Lighthouse and Cosmo prizes. Check them out at lighthouse.ai and Anki.com.

11
00:01:31,760 --> 00:01:37,320
In this episode, I'm joined by Raja Chitilla, director of intelligence systems and robotics

12
00:01:37,320 --> 00:01:43,000
at Pierre and Marie Curie University in Paris, an executive committee chair of the IEEE Global

13
00:01:43,000 --> 00:01:49,000
Initiative on Ethics of Intelligent and Autonomous Systems. Raja and I had a great chat about

14
00:01:49,000 --> 00:01:54,600
his research, which deals with robotic perception and discovery. We discussed the relationship

15
00:01:54,600 --> 00:01:59,840
between learning and discovery, particularly as it applies to robots in their environments,

16
00:01:59,840 --> 00:02:04,880
and the connection between robotic perception and action. We also dig into the concepts

17
00:02:04,880 --> 00:02:11,000
of affordances, abstract teachings, meta-reasoning, and self-awareness as they apply to intelligent

18
00:02:11,000 --> 00:02:17,720
systems. Finally, we touch on the issue of values and ethics of these systems. Before

19
00:02:17,720 --> 00:02:22,240
we jump into the interview, this is your final reminder that our next Twimmel Online

20
00:02:22,240 --> 00:02:29,840
Meetup is tomorrow, Tuesday March 13th at 5pm U.S. Pacific time. This time, our focus

21
00:02:29,840 --> 00:02:35,000
will be on the Google Deep Mind Paper, playing Atari with deep reinforcement learning,

22
00:02:35,000 --> 00:02:40,920
in a presentation by Sean Devlin. Head on over to twimmelai.com slash Meetup to learn

23
00:02:40,920 --> 00:02:47,680
more or register. Okay, let's get to it.

24
00:02:47,680 --> 00:02:54,120
Alright everyone, I am on the line with Raja Chitilla. Raja is the executive committee chair

25
00:02:54,120 --> 00:03:00,760
of the IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems, as well

26
00:03:00,760 --> 00:03:07,640
as an IEEE Fellow. He's also a professor at Sorbonne University, Pierre and Marie Curie,

27
00:03:07,640 --> 00:03:12,640
and Director of the Institute of Intelligent Systems and Robotics there. Raja, welcome

28
00:03:12,640 --> 00:03:14,960
to this weekend machine learning and AI.

29
00:03:14,960 --> 00:03:20,400
Hello everyone, thanks for inviting me. Absolutely. Why don't we get started by having

30
00:03:20,400 --> 00:03:25,880
you tell us a little bit about your background and how you got involved in Intelligent Systems,

31
00:03:25,880 --> 00:03:33,960
Robotics and Ethics? Well, let's see, I'll make a long story short because I've been

32
00:03:33,960 --> 00:03:43,520
working in robotics and artificial intelligence since a number of years now. Actually, I defended

33
00:03:43,520 --> 00:03:55,160
my PhD on Mobile Robot Navigation in 1981. So that's quite a long ago. And I've been

34
00:03:55,160 --> 00:04:02,280
working therefore in robotics and intelligence systems, intelligent robotics since that

35
00:04:02,280 --> 00:04:12,640
time. Working on several aspects of those problems, including planning, action planning,

36
00:04:12,640 --> 00:04:22,680
motion planning, perception, decision-making, learning and human robot interaction. So,

37
00:04:22,680 --> 00:04:30,800
it's a field that I've been familiar with since a long time and which is evolving very,

38
00:04:30,800 --> 00:04:40,120
very fast, especially in the past few years because the technologies that were developed

39
00:04:40,120 --> 00:04:48,840
over the years have come to some maturation. And we understand much better now, a lot

40
00:04:48,840 --> 00:04:57,880
of processes and problems to be able to integrate them and get them to real world solutions.

41
00:04:57,880 --> 00:05:05,480
And there have been also some progress, of course, which is enabling those technologies

42
00:05:05,480 --> 00:05:11,800
to become much more efficient, which is the increasing computer speed, the availability

43
00:05:11,800 --> 00:05:20,480
of memory, of data, enormous amounts of data. So that fostered the whole field recently.

44
00:05:20,480 --> 00:05:33,480
Great. And what's your current focus? Well, currently I'm focusing on learning processes,

45
00:05:33,480 --> 00:05:43,760
how to get a robot to interact with its environment. This is a basic issue, but the point is that

46
00:05:43,760 --> 00:05:54,720
we want the robot to learn through his or its perception, its physical interaction to

47
00:05:54,720 --> 00:06:03,600
connect those two together without giving it much information a priority, so that it discovers

48
00:06:03,600 --> 00:06:12,880
actually what is its environment made of. We don't want to pre-program things. We don't

49
00:06:12,880 --> 00:06:19,080
want to supervise its learning process. We want this to be unsupervised and we want

50
00:06:19,080 --> 00:06:25,240
the robot to connect its perception and action, because it's the only way, we think, to

51
00:06:25,240 --> 00:06:32,760
actually grasp an understanding of the environment. If you take a perception in general, it's

52
00:06:32,760 --> 00:06:40,960
considered and developed as an observation process. You have images and they process the

53
00:06:40,960 --> 00:06:49,600
images, you make some classification and you come up with identifying objects in the

54
00:06:49,600 --> 00:06:57,040
scene. And this can be based, of course, on learning processes, such as deep learning.

55
00:06:57,040 --> 00:07:08,000
But I think that this, of course, is very efficient and has proven efficiency lately, but

56
00:07:08,000 --> 00:07:15,800
by doing this, the robot doesn't really understand what it's looking at. To understand, it means

57
00:07:15,800 --> 00:07:26,840
that the robot has to develop a knowledge about what it can do, what it's action scope,

58
00:07:26,840 --> 00:07:33,760
its action possibility on the object in this environment. And therefore, it has to connect

59
00:07:33,760 --> 00:07:40,840
its perception with its action capacities. And this cannot be done separately. It has

60
00:07:40,840 --> 00:07:46,920
to be done simultaneously. So this learning process, which is based, of course, on processing

61
00:07:46,920 --> 00:07:52,720
data from sensors, but it's also based on reinforcement learning of interaction with

62
00:07:52,720 --> 00:08:02,000
this environment, would enable the system, the robot, to really connect its action capacities

63
00:08:02,000 --> 00:08:07,960
with its perceptual capacities. And this is what I call really understanding the environment.

64
00:08:07,960 --> 00:08:17,000
Interesting. And now is the robot learning all of this from the ground up, meaning you're

65
00:08:17,000 --> 00:08:23,960
firing up a system that has no a priori knowledge of its capabilities and it's exercising

66
00:08:23,960 --> 00:08:31,360
stepper motors and figuring out what degrees of freedom it has or what's the starting

67
00:08:31,360 --> 00:08:38,360
point for this understanding in your model? Well, that's the main difficulty, excellent

68
00:08:38,360 --> 00:08:45,920
question, I would say. What I want to do is to achieve this process with a minimal initial

69
00:08:45,920 --> 00:08:55,360
information. Now, what is minimal, that's the point. It's not very clear. But if we define

70
00:08:55,360 --> 00:09:04,640
very basic and very specific minimal knowledge that the robot has when it's manufactured, I

71
00:09:04,640 --> 00:09:12,480
would say, what is the initial programming that it has? And then starting from this, develop

72
00:09:12,480 --> 00:09:19,240
the whole process, but reducing more and more these basic knowledge. So for example,

73
00:09:19,240 --> 00:09:25,560
if we start from really zero knowledge, just as you said, exercising the stepping motors,

74
00:09:25,560 --> 00:09:32,280
this will lead to a very big complexity because there are many degrees of freedom. And there

75
00:09:32,280 --> 00:09:39,120
is quite a wide complexity also in the perception of the system. So we have to define a minimal

76
00:09:39,120 --> 00:09:47,800
set of processing capacities. For example, the system is able to determine verticals,

77
00:09:47,800 --> 00:09:56,000
horizontals, diagonals, it's able to identify some intersections, it's able to define some

78
00:09:56,000 --> 00:10:08,480
regions which have a common perceptual background. So these are minimal processing capacities

79
00:10:08,480 --> 00:10:15,360
that the systems come with, come with. And in terms of actions, it is able to move

80
00:10:15,360 --> 00:10:22,320
its arm, for example. So we are not going to start with the basic stepping motor or

81
00:10:22,320 --> 00:10:28,360
it's basic motor, but more a little bit more complex capacities that enable, for example,

82
00:10:28,360 --> 00:10:37,120
to move all arm to touch something. So starting from that, the development of the process

83
00:10:37,120 --> 00:10:46,600
can be made to understand how we connect perception and action together. Then we can get back

84
00:10:46,600 --> 00:10:54,440
to reduce maybe those initial assumptions, which are already small, but what we want

85
00:10:54,440 --> 00:11:00,440
to prove is the process itself. Do you want to prove the process or improve? Approve

86
00:11:00,440 --> 00:11:08,400
that it can work, that we can actually arrive to do this understanding. The system has

87
00:11:08,400 --> 00:11:14,280
to discover what is called affordances of the objects, of the elements of the environment

88
00:11:14,280 --> 00:11:19,880
that it's interacting with. And then we want to manipulate those affordances, which

89
00:11:19,880 --> 00:11:29,160
are those perceptual action representations, so that the system is able to extract more

90
00:11:29,160 --> 00:11:37,080
and more information about what it can do with them through this interaction.

91
00:11:37,080 --> 00:11:48,280
And in order to prove or even measure the robot's understanding of its environment, you

92
00:11:48,280 --> 00:12:00,120
need some kind of representation for how, I guess, for these affordances, how does the

93
00:12:00,120 --> 00:12:07,960
robot represent affordances and how do you measure, I guess, the scope of the affordances

94
00:12:07,960 --> 00:12:15,400
that it has determined with an environment relative to the total number of affordances

95
00:12:15,400 --> 00:12:26,480
or space of affordances? Okay, yeah, very good question. So the point is, some affordances

96
00:12:26,480 --> 00:12:34,040
are discovered initially, because there are some initial simple interactions like pushing

97
00:12:34,040 --> 00:12:44,400
or holding or tapping. And this provides a first base, a representation and affordance.

98
00:12:44,400 --> 00:12:52,840
It's not, of course, the complete representation. So then the process, which is based on reinforcement

99
00:12:52,840 --> 00:13:05,160
learning, tends to build more complex affordances. So this is guided, if I may say, through values,

100
00:13:05,160 --> 00:13:14,800
meaning that the system has a basic set of motivations that translates into rewards,

101
00:13:14,800 --> 00:13:25,400
that it has when it discovers some representation that afford for those motivations and reinforce

102
00:13:25,400 --> 00:13:35,960
therefore, this kind of representation. But by repeating this process with those basic

103
00:13:35,960 --> 00:13:48,360
motivations, the idea is to build upon the basic affordances more complex ones. So let

104
00:13:48,360 --> 00:13:57,960
me give an example. When the robot pushes a given object, well, suppose this pushing will

105
00:13:57,960 --> 00:14:05,800
be actually pushing on a switch and then there is light that as a result. And one of the

106
00:14:05,800 --> 00:14:14,960
motivations of the robot is to have a light, to have a clearer environment. So then this

107
00:14:14,960 --> 00:14:23,080
object that it has pushed on, which is the switch, is identified as a forwarding light.

108
00:14:23,080 --> 00:14:33,760
But the initial representation was just a kind of box or a very simple object, which had

109
00:14:33,760 --> 00:14:39,120
other affordances. For example, the robot could be able to hold it or push it. But now

110
00:14:39,120 --> 00:14:50,400
it's associated with something new, which is light. And so it has a more abstract property,

111
00:14:50,400 --> 00:14:58,240
which is not just its geometrical shape or the fact that it was pushable and so on.

112
00:14:58,240 --> 00:15:06,960
Now it's associated with light. And I can then use it for building up on more complex

113
00:15:06,960 --> 00:15:15,720
motivations, which for example are for fetching other objects I need first to put on the

114
00:15:15,720 --> 00:15:26,400
light. And therefore, this object has become a, I don't really dare to pronounce the word

115
00:15:26,400 --> 00:15:34,560
symbolic, but that's what it is. It's not just described in its perceptual features.

116
00:15:34,560 --> 00:15:42,120
It's more described in some more abstract properties. It's related to this property of light

117
00:15:42,120 --> 00:15:47,840
and the idea is that then we can build upon that again and again.

118
00:15:47,840 --> 00:15:56,560
Yeah, I'm thinking a little bit of Josh Tenenbaum's work at MIT and trying to relate some

119
00:15:56,560 --> 00:16:08,560
of what you're doing to what I've heard of that work. And it's interesting in that I

120
00:16:08,560 --> 00:16:14,480
think what the way I think of what he's trying to do is like come up with this notion of

121
00:16:14,480 --> 00:16:21,320
common sense for like how things work. And what you're doing is starting from almost

122
00:16:21,320 --> 00:16:27,600
no common sense in trying to have the robot figure everything out on the fly. I don't

123
00:16:27,600 --> 00:16:35,440
know if you're familiar with the stuff that he's doing, but I'm curious if you can maybe

124
00:16:35,440 --> 00:16:38,240
help me contextualize how they might fit together.

125
00:16:38,240 --> 00:16:47,240
Yes, of course, I know about Josh Tenenbaum's work and work of many others in this area,

126
00:16:47,240 --> 00:16:54,920
which is not recent, of course. And there have been a lot of efforts and research to try

127
00:16:54,920 --> 00:17:06,600
to provide common sense to robots, to computer systems and so on. And so it's absolutely related.

128
00:17:06,600 --> 00:17:15,680
And I think the approach to building common sense should really be grounded in the physical

129
00:17:15,680 --> 00:17:25,760
interaction with the world. So this is how we build common sense by trying to discover

130
00:17:25,760 --> 00:17:32,320
the effects of our actions, our, I mean, I'm speaking about the robot, of course, the

131
00:17:32,320 --> 00:17:44,120
effects of the actions of the robots action on the work. So the without this knowledge

132
00:17:44,120 --> 00:17:52,040
common sense would be just a very abstract representation. It wouldn't be really something

133
00:17:52,040 --> 00:18:02,680
that the robot is able to sense in its own actions. So that's, I mean, it's, of course,

134
00:18:02,680 --> 00:18:12,680
in this general paradigm of developing common sense. And the approach is developmental again

135
00:18:12,680 --> 00:18:21,240
similarly to what children do. They have some basic capacities, basic knowledge, basic

136
00:18:21,240 --> 00:18:29,920
sensing, basic action capacities. But when they start to experiment with the environment,

137
00:18:29,920 --> 00:18:35,040
this is where they discover that there is something called gravity. Of course, they don't

138
00:18:35,040 --> 00:18:43,680
call it like that. But they discover that when they hold something and they just let it

139
00:18:43,680 --> 00:18:50,520
go, it will drop to the ground. So it means there is a specific effect from their action

140
00:18:50,520 --> 00:18:58,560
that is observed. And the connection between this perception and action is the component,

141
00:18:58,560 --> 00:19:05,120
the major component of what is going to become common sense, which is the description of

142
00:19:05,120 --> 00:19:14,880
the laws of physics of what actually is the effect of actions on the environment. Then we

143
00:19:14,880 --> 00:19:20,800
will need something else, which is generalization, because what I'm speaking about is just the

144
00:19:20,800 --> 00:19:28,000
interaction of the system with its environment. But what what happens when you observe, observe

145
00:19:28,000 --> 00:19:39,320
other agents, for example, doing things and how do you develop from that, also representations

146
00:19:39,320 --> 00:19:47,480
that you can embed with your own actions. That's another step, which is, I think, also

147
00:19:47,480 --> 00:19:52,560
very important. But we are not doing this yet.

148
00:19:52,560 --> 00:20:01,920
As opposed to having the robot learn from base principles, everything about the world,

149
00:20:01,920 --> 00:20:08,160
shouldn't it have the benefit of the knowledge that we have? And for example, are there

150
00:20:08,160 --> 00:20:18,240
ways to or should we move towards ways of representing the laws of physics, for example,

151
00:20:18,240 --> 00:20:25,920
and being able to provide that as a, you know, an input model or something to the robot

152
00:20:25,920 --> 00:20:31,880
that it can kind of discover and use affordances with that knowledge as opposed to needing to

153
00:20:31,880 --> 00:20:34,960
figure that all out from scratch?

154
00:20:34,960 --> 00:20:44,120
If I can answer bluntly, I would say the more knowledge you input to the system, the

155
00:20:44,120 --> 00:20:53,160
less it will be able to understand its environment. It has to, to some point, it has to build this

156
00:20:53,160 --> 00:21:04,960
knowledge by itself. Then it can maybe generalize or learn something that it can relate to its

157
00:21:04,960 --> 00:21:16,240
own experience. But let me know if the following example is good. You can explain to me theoretically

158
00:21:16,240 --> 00:21:27,320
with the laws of physics how I can swim or how I can ride a bike. If I don't do it myself,

159
00:21:27,320 --> 00:21:36,280
I wouldn't really understand what you are talking about. So then if I do it myself and then

160
00:21:36,280 --> 00:21:47,120
I read or I learn the laws of physics about flotation, about the conservation of energy,

161
00:21:47,120 --> 00:21:53,320
of the moments and so on, well, then I can really theorize and understand what's going

162
00:21:53,320 --> 00:22:04,160
on when I ride the bike. So that's the idea. If we cannot anchor or ground on actual experience,

163
00:22:04,160 --> 00:22:12,320
these notions, then we wouldn't really be able to understand these notions. So providing

164
00:22:12,320 --> 00:22:23,040
initial knowledge or information, especially if it's described in theoretical ways or ways that

165
00:22:23,040 --> 00:22:31,160
relate to how the humans actually understand this, I don't believe it will be much of help.

166
00:22:31,160 --> 00:22:39,680
So one of the things that you mentioned earlier that I thought was interesting was that you put all

167
00:22:39,680 --> 00:22:49,040
of this or the robot is exploring all of this in a framework, a values-based framework, right?

168
00:22:49,040 --> 00:22:56,240
The robot has some values or motivation and it strikes me that perhaps this is one of the hooks

169
00:22:56,240 --> 00:23:08,000
into the broader idea of ethics. Is that correct? Yes, well, values. Of course, it relates to some point

170
00:23:08,000 --> 00:23:13,440
to ethics because the values I'm speaking about when you start to interact with the environment are

171
00:23:13,440 --> 00:23:26,960
more values that will in the beginning relate to what is able to answer the motivation of the

172
00:23:26,960 --> 00:23:37,520
robot. For example, just for the sake of simplicity, if the motivation of the robot is to have more power,

173
00:23:37,520 --> 00:23:48,640
to plug into a power device, to have more energy, then this would lead the robot to search for those

174
00:23:48,640 --> 00:24:01,760
actions and those perceptual inputs that can lead it to a better reach this goal, which is

175
00:24:01,760 --> 00:24:08,720
actually achieved this motivation. But then the idea is exactly to make those motivations more

176
00:24:08,720 --> 00:24:15,600
and more complex, starting with basic things like energy, for example, but then you can formulate

177
00:24:16,880 --> 00:24:28,720
motivations in a more complex way because, for example, in order to reach the power plug,

178
00:24:28,720 --> 00:24:38,720
I might have to go to somewhere else because it's not rotated here. Therefore, as another motivation,

179
00:24:38,720 --> 00:24:48,160
there is the motivation to move, to have this motion to reach somewhere else where I can

180
00:24:48,960 --> 00:24:57,520
indeed be able to have my initial motivation fulfilled. This is just an example to say that

181
00:24:57,520 --> 00:25:06,320
we can build more and more global or abstract motivations. I mean, that's my purpose. To the point

182
00:25:06,320 --> 00:25:16,000
that motivations can maybe lead to ethics, in this case, because when I mentioned values,

183
00:25:16,000 --> 00:25:23,360
initially the values are those that are going to lead some to some rewards. So starting from

184
00:25:23,360 --> 00:25:34,720
basic rewards to more complex rewards, is it rewarding to have, for example, a positive

185
00:25:34,720 --> 00:25:46,240
interaction with another agent? How is it rewarding to have an interaction that is satisfying

186
00:25:46,240 --> 00:25:55,120
another agent's own motivations? Now, if you build on that, you can reach a point where this notion

187
00:25:55,120 --> 00:26:03,920
of motivation is going to, and the values guarded, is going to lead the robot to respect some

188
00:26:03,920 --> 00:26:20,000
maybe ethical values. Ethics is built eventually when the Greek philosophers, when Plato, Aristotle,

189
00:26:22,000 --> 00:26:31,680
came up with the notion of ethics. The idea was to live the good life. The good life is what enables

190
00:26:31,680 --> 00:26:42,960
society to hold together, what enables people to live in a society in which they have the ability

191
00:26:42,960 --> 00:26:55,120
to reach a form of happiness. That's what is the initial grounding of ethics. How can we live

192
00:26:55,120 --> 00:27:08,800
together and live the good life? So it means that this comes from an ability to respect some values,

193
00:27:08,800 --> 00:27:20,240
which make this social life possible instead of each one trying to kill their neighbors.

194
00:27:20,240 --> 00:27:28,000
And so this can be translated, although it's not what I'm doing now in terms of scientific

195
00:27:28,000 --> 00:27:35,200
research. This can be translated if we want our robots to respect, to develop an ethical

196
00:27:39,280 --> 00:27:49,920
behavior or an ethical common sense. This can be also input as a motivation of the robot system.

197
00:27:49,920 --> 00:28:01,840
However, personally, I don't believe that at least for some time we can build robots that can

198
00:28:01,840 --> 00:28:10,240
grasp the meaning of some abstract notions, such as human dignity, for example. The notion of

199
00:28:10,240 --> 00:28:22,320
dignity, which also is very important in ethics. I don't believe that we can really define this or

200
00:28:23,920 --> 00:28:32,720
I don't see a way for a robot, which is a machine, to discover what this means exactly. So

201
00:28:32,720 --> 00:28:42,400
there will be some order maybe that we will have to cross one day to try to achieve this. But

202
00:28:42,400 --> 00:28:53,440
today I don't see how we can really define or learn by a robot. These abstract notions of human

203
00:28:53,440 --> 00:29:07,280
dignity, dignity, freedom, human rights. I'm just curious, there are abstractness is a spectrum.

204
00:29:07,280 --> 00:29:12,880
I'm not sure if I'm not sure that it's a two-dimensional spectrum. But I'm wondering,

205
00:29:12,880 --> 00:29:18,000
do you have a sense for what's the most abstract thing we've been able to teach a robot?

206
00:29:18,000 --> 00:29:24,160
Or any examples of things that are kind of towards the abstract side of the spectrum?

207
00:29:24,160 --> 00:29:32,240
Not sure, I can answer this. Well, it's an abstract question.

208
00:29:32,240 --> 00:29:41,520
Yes, no, because it's quite difficult to define what the robot has actually learned when we say

209
00:29:41,520 --> 00:29:52,160
what does the robot know exactly. Of course, I can program the robot with some very abstract

210
00:29:52,160 --> 00:29:58,560
notions. For example, speaking about emotions, I can program a robot that says, I love you,

211
00:29:58,560 --> 00:30:06,080
but I don't think the robot understands what you love is. It's just the result of a specific

212
00:30:06,080 --> 00:30:15,280
programming, some parameters that have been input and that the system can measure and react

213
00:30:15,280 --> 00:30:22,560
according to those values of the values of those parameters. But understanding what love means

214
00:30:22,560 --> 00:30:29,760
exactly, I'm not sure, I'm even sure of the contrary, that a robot can grasp this.

215
00:30:29,760 --> 00:30:36,720
So this is why I'm saying, it depends what you mean by teaching and what you mean by understanding

216
00:30:38,160 --> 00:30:41,760
by a robot system of those abstract notions.

217
00:30:42,800 --> 00:30:50,800
I guess the things that came to mind were, I think we've been able to teach robots

218
00:30:51,600 --> 00:30:59,280
an abstract notion of shape, for example, that is at a higher level than point locations.

219
00:30:59,280 --> 00:31:05,200
And an abstract notion of relative position, like something being on top of another thing that,

220
00:31:05,200 --> 00:31:13,440
again, is abstracted away from stepper motor rotations and point locations and things like that.

221
00:31:14,160 --> 00:31:20,960
And it just occurred to me that there's a spectrum of abstractness. I don't know if those are

222
00:31:20,960 --> 00:31:28,400
pretty close to the physical side of on that spectrum. And maybe we're just kind of stuck around

223
00:31:28,400 --> 00:31:34,240
abstracting these base physical things. And I don't even know how to characterize the line between

224
00:31:34,240 --> 00:31:39,600
that and the next set of things. I was just curious if this was something that you've come across

225
00:31:39,600 --> 00:31:45,840
and thought of. Yeah, well, I still believe that the examples you've provided, which are mostly

226
00:31:45,840 --> 00:31:51,520
geometrical examples, for example, or biological examples, so it's above something else.

227
00:31:51,520 --> 00:31:59,520
I still believe that even if these sound as abstract notions that we have defined,

228
00:32:01,360 --> 00:32:07,840
we, I don't believe we have really made a robot understand what this means exactly.

229
00:32:09,600 --> 00:32:18,400
Of course, you can describe what it means box A on top of box B or whatever

230
00:32:18,400 --> 00:32:32,400
by defining some relationships between the shapes. But this is this really defined in any situation.

231
00:32:33,600 --> 00:32:43,040
Is it that abstract? What what will happen, for example, if you have a robot in outer space

232
00:32:43,040 --> 00:32:50,880
and there is no gravity, what does it mean something on top of something else?

233
00:32:52,160 --> 00:32:57,840
I'm not sure robot can grasp the profound notion of something on top of something else

234
00:32:59,600 --> 00:33:07,840
because it doesn't even have this notion of gravity. And notion of up, down, and so on is

235
00:33:07,840 --> 00:33:16,960
very much related to our relationship with the world. So it's on top because my I'm vertical,

236
00:33:16,960 --> 00:33:23,440
for example, and something is on top of something else because it's following this verticality,

237
00:33:23,440 --> 00:33:33,920
for example. But when you change the referential, I'm not sure the robot can grasp what this means.

238
00:33:33,920 --> 00:33:43,360
So I believe our definitions are actually very much grounded in some knowledge that is not

239
00:33:43,360 --> 00:33:53,120
expressed, not explicit in the robot system. So that kind of begs two questions for me. One is

240
00:33:54,160 --> 00:33:59,440
what does understanding even mean and what is what is the test for robot understanding?

241
00:33:59,440 --> 00:34:10,800
And secondarily, it also causes me to question this idea of having robots learn things from scratch

242
00:34:10,800 --> 00:34:20,480
and in particular, if the robots are going to interact with and communicate with us as humans

243
00:34:20,480 --> 00:34:26,320
and so many fundamental things about the way we perceive the world, like, you know, something

244
00:34:26,320 --> 00:34:32,800
being on top of it is fundamentally connected to our humaneness. In this case, our verticality,

245
00:34:32,800 --> 00:34:39,360
like, don't we have to give the robots a lot in order for them to develop an understanding of

246
00:34:39,360 --> 00:34:48,320
the world that even makes sense to us? Yeah. Well, that's exactly the point. I mean, if we want to

247
00:34:48,320 --> 00:34:57,040
build a robot that is useful, that we want to use tomorrow, that we want to use it for achieving

248
00:34:57,040 --> 00:35:05,360
tasks with which we want to communicate and say, put this book on top of this desk, for example.

249
00:35:05,920 --> 00:35:13,760
Of course, we have to program things into those robots. But again, the robot is just a kind of zombie.

250
00:35:13,760 --> 00:35:18,560
It doesn't understand what it's doing. It's doing it, but it doesn't understand what it's doing.

251
00:35:18,560 --> 00:35:28,480
It's just repeating or executing some program that has embedded what is needed for it to do this

252
00:35:28,480 --> 00:35:38,160
task. If we truly, I mean, from a scientific point of view, not from an utilitarian point of view,

253
00:35:38,160 --> 00:35:45,120
if I may say, if we want really to understand what this means, what does it mean for a system,

254
00:35:45,120 --> 00:35:53,120
for a machine to develop those notions? Well, then it's a different story. And this is what

255
00:35:53,120 --> 00:36:01,760
actually AR robotics in terms of scientific research is trying to deal with. And we are still

256
00:36:01,760 --> 00:36:11,440
miles and years from understanding how we are going to do that. So two different things.

257
00:36:11,440 --> 00:36:16,080
And just to be a little bit more provocative, I would say, my personal

258
00:36:17,840 --> 00:36:26,400
view is that to achieve this understanding, you said what understanding is, to achieve this notion

259
00:36:26,400 --> 00:36:37,520
of understanding. I believe something is necessary. It's the notion of self-awareness.

260
00:36:39,200 --> 00:36:48,640
If the machine does not have this notion, I don't believe it will be able to really understand.

261
00:36:48,640 --> 00:36:58,000
Because understanding is related to me, I mean, the self, being at the center, at the core of a

262
00:36:58,000 --> 00:37:09,680
process of interaction with the environment. And the capacity of reflecting on one's own capacities,

263
00:37:09,680 --> 00:37:16,240
one's own reasoning, reasoning on what I'm doing. Meta reasoning, if you wish.

264
00:37:16,240 --> 00:37:26,320
So I believe that if we are not able to develop these meta reasoning capacities, this self-evaluation

265
00:37:26,320 --> 00:37:36,560
capacity that enables to reflect on one's own place in the environment, in the universe,

266
00:37:36,560 --> 00:37:47,040
in the world, this notion of understanding would be just artificial, not genuine. And which leads us

267
00:37:47,040 --> 00:37:55,280
to this notion of are we able to build self-aware machines, conscious machines, self-aware machines?

268
00:37:56,240 --> 00:38:03,920
That's another issue that is much discussed as well. But I think this is fundamental.

269
00:38:03,920 --> 00:38:14,880
Otherwise, we will always have systems that just act like zombies, not like entities that

270
00:38:16,560 --> 00:38:24,000
grasp what their environment and therefore what they are about, what they are doing, really.

271
00:38:25,360 --> 00:38:32,640
Is there anything that you've come across in the research that is showing promising

272
00:38:32,640 --> 00:38:36,320
results at demonstrating some degree of self-awareness?

273
00:38:39,280 --> 00:38:46,720
There are tons of publications on self-awareness. But frankly, I think we are just

274
00:38:48,160 --> 00:38:56,880
rotating around this notion like the planets are rotating around the sun,

275
00:38:56,880 --> 00:39:03,440
except that the sun we can see it and the consciousness or self-awareness we can't, and we

276
00:39:03,440 --> 00:39:13,440
don't have a clear definition of what it is. So this is actually the basic motivation between my work

277
00:39:13,440 --> 00:39:20,640
is to try to understand if and how, and of course the question is still quite open, if and how

278
00:39:20,640 --> 00:39:28,080
we can develop this notion of self-awareness through the interaction of the system with its

279
00:39:28,080 --> 00:39:36,080
environment, the interaction of the system with other agents, other systems. The idea is that

280
00:39:36,080 --> 00:39:42,400
when you interact with the environment, you have to relate things to you. You have to recognize

281
00:39:42,400 --> 00:39:51,360
yourself in this environment. This is my action, this is my arm doing this, and this is therefore

282
00:39:51,360 --> 00:40:01,520
my evolution, my will, my decision to act in this way. This my places the self in the center.

283
00:40:02,720 --> 00:40:07,520
This means something to me because I can do something with it.

284
00:40:07,520 --> 00:40:17,040
If the system is not able to grasp this relationship between itself and its environment,

285
00:40:17,040 --> 00:40:21,920
it will not have self-awareness. So the self-awareness will be developed through this interaction

286
00:40:21,920 --> 00:40:26,800
with the environment. It strikes me that some of the work that's happening in and around

287
00:40:26,800 --> 00:40:35,680
reinforcement learning, playing Atari games and the like, it strikes me that some of that is

288
00:40:35,680 --> 00:40:44,320
approaching self-awareness. The agent is clearly aware, well, I guess we have to define aware,

289
00:40:44,320 --> 00:40:52,800
but there's a clear relationship between the actions of the agent and the environment and the

290
00:40:52,800 --> 00:41:00,000
whole premise of the reinforcement learning environment is that the agent can manipulate this

291
00:41:00,000 --> 00:41:07,920
environment and maximize some objective. Absolutely. The reinforcement learning, in my opinion,

292
00:41:07,920 --> 00:41:16,960
is a core to this, and this is a technique which is absolutely related to try and

293
00:41:16,960 --> 00:41:24,800
need to build, reinforce this understanding. The point is, of course, that in your

294
00:41:24,800 --> 00:41:31,280
reinforcement learning, you have to learn the value function. It should be something that is

295
00:41:31,280 --> 00:41:40,000
discovered as well. This means that I have to discover what is of value tuning. This gets us back

296
00:41:40,000 --> 00:41:51,680
to motivations. There should be some minimal motivations upon which more complex motivations

297
00:41:51,680 --> 00:41:57,520
are going to be built, but there should be some minimal motivations. However, most systems

298
00:41:57,520 --> 00:42:05,120
doing reinforcement learning are focused on solving a specific problem. You have systems with,

299
00:42:05,120 --> 00:42:10,400
of course, the concept is quite wide and quite general, but you have a system that's going to play

300
00:42:10,400 --> 00:42:16,240
Atari, as you said, or that's going to assemble boxes and so on, with the same process. Again,

301
00:42:16,240 --> 00:42:27,280
the process is core, but the values and the way they translate into achieving the motivations of

302
00:42:27,280 --> 00:42:36,320
the system, that's a big point. For example, you achieve a system that is able to play, say,

303
00:42:36,320 --> 00:42:45,440
a reinforcement learning is going to play Go or to play Atari, whatever, but what about a system

304
00:42:45,440 --> 00:42:52,320
that says, I've had enough playing this game, let's do something else. Then it's going to

305
00:42:53,920 --> 00:42:59,600
actually reason differently. It's not just doing something that it has been designed to do.

306
00:42:59,600 --> 00:43:09,600
It's going to decide something else, which maybe is related to a different motivation. That's

307
00:43:11,600 --> 00:43:24,320
a point in achieving a system that is not just doing what I have or learning to achieve what

308
00:43:24,320 --> 00:43:33,200
I programmed to do. It has to be able from a wider set of motivations or a wider set of

309
00:43:36,000 --> 00:43:49,280
self-reflection capacities to jump, to initiate maybe other decisions that were not explicitly

310
00:43:49,280 --> 00:43:58,880
what I asked it to do at the moment. I'm not sure I'm clear, but the point is if the system is not

311
00:43:58,880 --> 00:44:07,120
able to reflect on its own motivations and to change that, then it will remain prisoner

312
00:44:08,320 --> 00:44:18,000
in the compartment of actions and objectives that the programmer has completely defined.

313
00:44:18,000 --> 00:44:25,360
I'm not sure it's possible to get out of that compartment, but that's the issue. That's the point.

314
00:44:26,240 --> 00:44:34,960
The notion of, I guess we can call it self-agency, that you just describe being able to come up

315
00:44:34,960 --> 00:44:43,280
with its own motivations. Do you think that's inseparable from the idea of self-awareness?

316
00:44:43,280 --> 00:44:52,080
Like, could a machine be both self-aware and yet unable to determine its own motivations?

317
00:44:52,880 --> 00:45:02,400
Well, that's just a belief. I'm not sure. I think it's related, maybe to some degree.

318
00:45:04,080 --> 00:45:10,480
I'm not sure we can define. Self-awareness is not necessarily something you have or we don't.

319
00:45:10,480 --> 00:45:18,880
You might have several degrees of self-awareness. There are studies about this for some animals,

320
00:45:18,880 --> 00:45:32,800
for example, for the dolphins or for some apes, but there might be some degrees of self-awareness

321
00:45:32,800 --> 00:45:42,560
with, so I'm not sure there is only one degree, but eventually, yes, I think that this is related

322
00:45:42,560 --> 00:45:57,040
to this capacity of being able, ultimately, to have a kind of free will to step out of the enclosure.

323
00:45:57,040 --> 00:46:04,960
So, and this might lead more explicitly to the notion of ethics, actually, although we are trying

324
00:46:04,960 --> 00:46:10,800
to, in the artically initiative, to work on the ethical issues of today's autonomous systems,

325
00:46:10,800 --> 00:46:17,280
of today's intelligent systems. Those which don't have those capacities,

326
00:46:17,280 --> 00:46:33,040
how do we reach a development or a design process of those systems so that they are compliant

327
00:46:33,040 --> 00:46:42,000
with human values. So, this is a design and methodology, for example, this is something that

328
00:46:42,000 --> 00:46:50,800
has to be considered and taken into account when people, researchers, engineers, designers

329
00:46:50,800 --> 00:47:00,480
develop such systems, but on the more research scientific side, of course, having an autonomous system

330
00:47:00,480 --> 00:47:12,640
or a system that is capable of reflecting on its own, on its own about ethical considerations.

331
00:47:12,640 --> 00:47:20,640
Well, that will be the ultimate thing, but as I mentioned earlier, I don't believe we are

332
00:47:20,640 --> 00:47:31,360
even close to that. It is certainly a fascinating topic in one with many, many kind of interrelated

333
00:47:31,360 --> 00:47:36,400
layers. You know, we're coming up on the top of the hour, we've hit the top of the hour,

334
00:47:36,400 --> 00:47:42,000
I think we're going to have to schedule a part two at some point to go into the ethics of

335
00:47:42,000 --> 00:47:49,840
autonomous systems. There's a lot there to chat about as well, but I really enjoyed learning about

336
00:47:49,840 --> 00:47:57,760
your research and some of the ways you're approaching this idea of abstract thought and

337
00:47:57,760 --> 00:48:04,160
understanding for robotics. Well, thank you. Yes, I would be delighted to have another chat,

338
00:48:04,160 --> 00:48:10,400
because indeed we just alluded very, very briefly to that, but the conversation was very,

339
00:48:10,400 --> 00:48:19,120
very interesting. Great. Well, thank you so much, Raja. Thank you, Sam. All right, everyone, that's our show

340
00:48:19,120 --> 00:48:25,040
for today. For more information on Raja or any of the topics covered in this episode,

341
00:48:25,040 --> 00:48:32,000
you'll find the show notes at twimlai.com slash talk slash 118. If you have any questions for

342
00:48:32,000 --> 00:48:37,680
Raja, please post them there and we'll make sure to bring them to his attention. If you're new to

343
00:48:37,680 --> 00:48:43,280
the podcast and like what you hear or you're a veteran listener and haven't already done so,

344
00:48:43,280 --> 00:48:49,200
head on over to your podcast app of choice and leave us your most gracious and glowing review.

345
00:48:49,840 --> 00:48:55,520
It helps new listeners find us, which helps us grow. Thanks in advance. And of course,

346
00:48:55,520 --> 00:49:09,040
thanks so much for listening and catch you next time.

