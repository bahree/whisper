WEBVTT

00:00.000 --> 00:06.600
All right, everyone. Welcome to another episode of the Twimal AI podcast. I am your host, Sam

00:06.600 --> 00:12.920
Charrington. And today, I'm joined by Vinod Purpakran. Vinod is a senior research scientist

00:12.920 --> 00:18.080
at Google Research Vinod. Welcome to the show. Thank you, Sam. Thank you so much for having

00:18.080 --> 00:23.440
me here. I'm looking forward to digging into our conversation. We'll be talking about some

00:23.440 --> 00:29.840
of your research, which looks at both the way that AI impacts social disparities, but also

00:29.840 --> 00:35.520
how AI can be used to study social disparities. To get us started with that, I'd love to have

00:35.520 --> 00:39.200
you share a little bit about your background and how you came to work in the field.

00:39.200 --> 00:47.040
Absolutely. So, yeah, I come from a more traditional computer science background, like my PhDs

00:47.040 --> 00:55.120
in computer science, where I worked on sort of more research at the intersection of

00:55.120 --> 01:02.720
natural language processing and social sciences or society. So, that's where I kind of started.

01:02.720 --> 01:10.080
So, I come from a more kind of technical background in that sense. But over time, I've sort of

01:10.640 --> 01:17.920
got to this place where researching at the intersection of AI in general and society. And

01:17.920 --> 01:25.040
where I kind of have two different research profiles. One is where I use AI or NLP

01:25.040 --> 01:32.480
or machine learning technologies to sort of look at societal disparities, use them as tools to

01:32.480 --> 01:39.280
look at societal disparities. And that's the kind of work that I was doing mostly in the PhD

01:40.160 --> 01:48.400
work, as well as afterwards as a postdoc at Stanford. But afterwards, like in the last three or

01:48.400 --> 01:55.120
four years, Google as a research scientist, I've been mostly looking at how social disparities

01:55.120 --> 02:01.920
influence these tools or how these social disparities are captured, reflected and maybe even

02:01.920 --> 02:07.600
propagated or amplified through these machine learning and natural language processing tools.

02:07.600 --> 02:12.560
So, it's kind of like looking at both directions of this intersection.

02:12.560 --> 02:19.200
And you referenced some of the work you were doing for your PhD. What are some examples of the

02:19.200 --> 02:26.640
way that you used AI and NLP to look at social or societal disparities? Yeah. So, I mean,

02:27.200 --> 02:32.960
some of the work was like looking at how workplace interactions, how social power manifest,

02:32.960 --> 02:37.760
you know, like workplace interactions, like that was my PhD work. So, looking at email conversations

02:37.760 --> 02:42.960
at a workplace and like looking at just by looking at a language used and the structure of

02:42.960 --> 02:48.880
these conversations, can you tell who is the superior or who is the subordinates in that conversation?

02:48.880 --> 02:53.840
Like how does power manifest in like how people interact with one another? So, that was my kind

02:53.840 --> 03:02.880
of PhD research thesis topic. That sort of got me into this work, which is kind of I'm really excited

03:02.880 --> 03:08.560
about I continue to be excited about the work is I don't actively work on it, but right now,

03:08.560 --> 03:16.960
but that was where we used this NLP tools to look at social racial disparities in police

03:16.960 --> 03:22.720
community interactions. This is work I did as a postdoc at Stanford with a bunch of amazing

03:22.720 --> 03:29.120
researchers there in collaboration with social psychologists and linguists and computer scientists

03:29.120 --> 03:36.320
where we had access to body camera videos of about one year worth of data from the Auckland

03:36.320 --> 03:42.240
police department. So, it was in collaboration with the police department and we used the NLP tools

03:42.240 --> 03:49.360
to sort of look through this lot of data. This usually this data is looked at as like, you know,

03:49.360 --> 03:55.760
something goes wrong. It's looked at as evidence whereas we were kind of demonstrating that it could

03:55.760 --> 04:02.240
be used as like data to learn from and like understand what is going wrong potentially in the

04:02.240 --> 04:09.360
in the in the department or in that particular city. So, yeah, we used NLP tools to sort of look at

04:09.360 --> 04:15.440
like things like the level of respect, the level of politeness, the way the conversations are

04:15.440 --> 04:22.240
structured when police officers stop community members for traffic jobs, for instance.

04:22.240 --> 04:28.240
So, presumably the body camera videos were transcribed and that's what you applied NLP tools to.

04:28.240 --> 04:33.280
Yes. So, the first paper that came out of that work like we did look at the transcribed data like

04:33.280 --> 04:40.000
we had them manually transcribed and looked at like signals for politeness and respect and so on.

04:40.000 --> 04:45.760
But there's also work on looking at the audio signals like the prosody. I was not actively involved

04:45.760 --> 04:52.720
in it but the the the kind of frequency and pitch and all those factors might be signals that

04:53.360 --> 04:58.320
so this is a huge project with like, you know, so many people looking at interesting signals there.

04:58.880 --> 05:03.360
I also had some work where we are actually looking at the structure of these conversations like

05:03.920 --> 05:09.040
where police officers or when did the police officers like give the reason for the stuff,

05:09.040 --> 05:14.080
right? Like when you stop someone, do you start by saying, hey, I'm stopping you for this

05:14.080 --> 05:19.120
in this reason, where is your diverse license and registration versus basically

05:19.120 --> 05:24.240
stopping someone and saying like, hey, give me your license and registration and then telling.

05:24.240 --> 05:30.320
So, that sets the conversation in a very different kind of path and that also be empirically

05:30.320 --> 05:36.880
we're able to analyze and look at these kind of differences in this kind of subtle ways,

05:37.600 --> 05:43.120
subtle ways in which like conversations can be different and what how that can affect

05:43.120 --> 05:48.480
like the later parts of the conversation. So, this is ongoing work like the researchers

05:48.480 --> 05:54.800
at Stanford continue to work on this and I work on like I kind of wrapping up it's been like four

05:54.800 --> 06:00.960
years now, but some of the work I'm still in in in in bolden. So, there's like some work that's

06:00.960 --> 06:06.080
going to be coming out from that like in the near future, hopefully. But yeah, that's that's an

06:06.080 --> 06:14.080
example of a place where these AI tools are used to sort of look at social disparities.

06:14.080 --> 06:20.640
I'm curious how much of the social science side of your of this work you were

06:22.240 --> 06:28.960
involved in or kind of got into or were you collaborating with social scientists and working

06:28.960 --> 06:33.520
on the more machine learning technical sides of it? Yeah, I love this question because it's

06:33.520 --> 06:38.640
something that that particular project was such a learning experience for me to sort of having

06:38.640 --> 06:44.400
deeper collaborations with social scientists like to and also like having that deep respect for

06:44.400 --> 06:51.600
each disciplines like you know engaging with the social scientists. It was a very close collaboration

06:51.600 --> 06:55.680
to answer your question. It was a very close collaboration. We had like weekly meetings and like

06:55.680 --> 07:02.000
I went to the you know it was not that it's a typical computer science project that I have

07:02.000 --> 07:07.040
been part of like prior to that. You get this data from somewhere and then you go into your lab and

07:07.040 --> 07:11.440
like you know go through munch through the numbers and like try to come up with like you know some

07:11.440 --> 07:16.800
results and then like you know put it in a paper. But this was a very um involved collaboration

07:16.800 --> 07:21.360
with social scientists but also with the police department and communities. I didn't actively

07:21.360 --> 07:25.280
involve with and interact with the communities there say but like you know our collaborators did

07:25.280 --> 07:31.760
have like community workshops and stuff. But I did go to the police you know precinct and like

07:31.760 --> 07:37.040
you know talk to the police department, police chief and like sort of understood like you know

07:37.680 --> 07:46.400
the context of this data and I even went on like right along to sort of like be part of to see

07:46.400 --> 07:53.840
how these conversations happen in reality. That I mean that really changed or shapes the way I

07:53.840 --> 07:58.240
kind of look at this data. It's not no longer just like ones and zeros like it actually

07:58.240 --> 08:03.920
have has a lot more brings a lot more meaning to like the data that we are working with and

08:05.040 --> 08:10.720
so yeah it was and to answer your question about like the disciplinary collaboration also like

08:10.720 --> 08:17.120
it was a very as I said like we had weekly co-a meetings and we had like multiple papers I was

08:18.720 --> 08:25.520
working with like social psychology PhD students and yeah and we were in another podcast recently

08:25.520 --> 08:31.200
me and another social psych substance colleague of one of these papers about like how interdisciplinary

08:31.200 --> 08:35.360
works like in these kind of collaborations right like said I think in that particular project like

08:35.360 --> 08:40.320
the I think one thing that we both mentioned was that there was a deep respect for these disciplines

08:40.320 --> 08:45.120
rather than like you know the computer scientist coming and like taking away like the you know

08:45.120 --> 08:50.160
the data and like going and doing magic like it was not that we were very actively involved in

08:50.160 --> 08:55.920
framing the research questions how we ask the research questions how we interpret the results

08:55.920 --> 09:01.840
all of that was like in deep collaboration and so the more recent work that you're doing

09:03.440 --> 09:11.840
how did the work that you were doing previously kind of take you to the more recent work?

09:11.840 --> 09:18.400
I think the work that I was doing at Stanford especially in this particular context kind of got me

09:18.400 --> 09:28.960
a lot more sort of aware of like a lot more sort of yeah aware of like the kind of the

09:28.960 --> 09:33.280
justice angle of like these social science work like they know the disparities and like that

09:33.280 --> 09:40.320
that human angle to it and that sort of like and it was around the time that there was conversations

09:40.320 --> 09:45.840
around fairness and bias in machine learning models that was just like you know beginning in 2015

09:45.840 --> 09:53.600
1617 period so I was in that like I just happened to be in that space where like I was already working

09:53.600 --> 10:00.160
on understanding social disparities and this is like an important or like a really cool sort of

10:00.160 --> 10:05.840
like way of like looking in the other direction how does this disparities get into or get captured

10:05.840 --> 10:12.000
into these machine learning models and that's what like sort of like pivoted or like it wasn't

10:12.000 --> 10:16.560
a really pivot because and I still kind of like have this I still work on like social science

10:16.560 --> 10:26.000
see kind of flavored work but that realization is like how I go into the Google research

10:26.640 --> 10:33.200
job that I'm where I am right now where kind of like the focus at that time was on sort of

10:33.200 --> 10:37.520
understanding like this how these disparities get captured in machine learning models and that

10:37.520 --> 10:42.320
was like the transition because I was already working at that space in the other direction

10:43.200 --> 10:50.000
and since joining Google most of my Google work as I said have been on the other direction where

10:51.360 --> 10:59.600
you know looking at how disparities in data kind of get captured in these models and

10:59.600 --> 11:07.680
you know clearly one key place where some of those disparities will get introduced into

11:07.680 --> 11:16.480
models is through the that human interface which is data labeling and your more recent work is

11:16.480 --> 11:26.960
kind of focused on on that in particular can you introduce us to your paper your 2021 paper the

11:26.960 --> 11:33.360
issues aggregating human labels what was the kind of the broad set of issues that you were exploring

11:33.360 --> 11:40.320
there absolutely so I think yeah there's a lot of work within the machine learning and OP field

11:40.320 --> 11:46.240
in the last few years looking at like various sources of biases very sources through which like

11:46.240 --> 11:50.960
you know biases get creeped into the models it could be coming through data it could be coming

11:50.960 --> 11:59.120
through the humans who are building these models and their perspectives or you know limited

11:59.120 --> 12:07.200
perspectives about society but in this particular so I was always curious about how diverse

12:07.200 --> 12:12.720
perspectives in data beat like you know data beat and like you know people who are building these

12:12.720 --> 12:18.320
like models how does like diverse perspectives like get captured in these like pipeline right you

12:18.320 --> 12:24.400
know there's no that's not always like this like one single answer I mean we live in a world of like

12:25.520 --> 12:30.160
you know pluralistic world with like so many value systems if you think about like across the

12:30.160 --> 12:36.320
world there's no one single answer for many of these like questions and ethics and fairness

12:37.040 --> 12:42.960
what is fair for me may not be what is fair for like someone in a different cultural context and

12:42.960 --> 12:48.160
having a different cultural history so that kind of motivated that motivates my current work like

12:48.160 --> 12:53.760
largely most of my research currently isn't that kind of motivated from that question of like you

12:53.760 --> 12:58.160
know how do we different perspectives like all disagreements between like what is right and wrong

12:58.160 --> 13:04.000
gets captured in these kind of interventions so it's like even when like you know we intervene or

13:04.000 --> 13:10.800
like making things fairer like how do we do that like in a more pluralistic way so that that's

13:10.800 --> 13:16.480
where like this particular paper work on this particular paper that you referenced I came about

13:16.480 --> 13:25.360
and so we had a paper in 2021 where we looked at this one core thing as you said data labeling

13:25.360 --> 13:30.800
which becomes like a such a core thing for the machine learning pipeline where you actually

13:30.800 --> 13:38.880
you know have human writers annotate like you know be it like images or text content like for like

13:38.880 --> 13:43.840
whatever whatever social constantly whatever construct that you're trying to model in cases where like

13:43.840 --> 13:49.200
you know whether labeling whether something is a cat or a dog you can see that like most people

13:49.200 --> 13:53.440
would agree like you know there might be cases where like you know people disagree whether something

13:53.440 --> 13:59.760
is a cat or not but like it's it's a it's a relatively more objective kind of task but when

13:59.760 --> 14:06.640
you ask like somewhere is this piece of text offensive that's you're actually leaning on a lot of sort

14:06.640 --> 14:13.360
of aspects of that individual humans like you know social cultural context and their lived

14:13.360 --> 14:20.000
experiences shape how they feel or how they perceive something to be offensive and not and so

14:20.000 --> 14:28.320
we looked at like a bunch of datasets which kind of capture or attempts to capture such sort of

14:29.120 --> 14:35.120
subjective or relatively more subjective tasks such as like sentiment whether this is a

14:35.120 --> 14:42.320
positive sentiment or a negative sentiment or offenseiveness like or hate speech or emotions

14:42.320 --> 14:47.600
like whether this is like something expressing happiness or sadness or that sort of so

14:47.600 --> 14:54.320
this like a we had around three datasets around seven or eight different like tasks like these

14:54.320 --> 15:01.600
kind of like social constructs that were like labeled and we looked at how people's like different

15:01.600 --> 15:06.800
annotators like perspectives like matched or not so the traditional way of dealing with like when

15:06.800 --> 15:13.680
people disagree for for a long time like you know in machine learning traditionally how you deal

15:13.680 --> 15:19.280
with it when people disagree is that you take a majority vote like you you have especially when you

15:19.280 --> 15:23.680
collect data from the crowd from the crowd work kind of platforms like mechanical talk and all

15:23.680 --> 15:28.960
that you it's like relatively cheap so you get like three annotations for all in or three or five

15:28.960 --> 15:36.160
or ten depending on how much money you have and you basically get the more number of annotations

15:36.160 --> 15:41.520
and then you take a majority vote to say that like oh this is the majority of people in our pool

15:41.520 --> 15:47.120
agreed and stuff we basically were questioning like how does what does that mean for like

15:47.120 --> 15:53.360
perspectives or people with specific social cultural backgrounds which maybe underrepresented

15:53.360 --> 16:00.160
within the annotator pool so we had a paper like kind of like precursor paper first where we kind

16:00.160 --> 16:05.600
of did this analysis on this eight different datasets or eight different tasks annotation tasks

16:05.600 --> 16:12.320
like where we looked at like if you just take the majority vote and then compare or look at like how

16:12.320 --> 16:16.880
does each individual annotator agree with this majority vote like because we also have like the

16:16.880 --> 16:22.640
annotations that this individual voters gave like how many times like their vote really made it to

16:22.640 --> 16:29.440
the majority right so we looked at like that or was equal to the majority right like we looked at that

16:30.640 --> 16:35.920
for these tasks like that varied significantly across tasks and sometimes it is like all over the

16:35.920 --> 16:42.400
place some some tasks like such as hate speech if I remember correctly was a lot more kind of like

16:42.400 --> 16:46.720
you know most people agree and most people with the majority vote like agreed with like a most

16:46.720 --> 16:53.840
people but like the things like disgust is an emotion like it had like very wide range of like

16:53.840 --> 16:59.200
interpretations and like people disagreed on that or one thing that jumps out at me just hearing

16:59.200 --> 17:08.400
you describe that setting is in some context you might want to you might be tempted to look at the

17:09.520 --> 17:16.240
degree to which a particular annotator annotates along with the majority as like a measure of

17:16.240 --> 17:22.720
quality but here you're pointing out that it's in fact you know maybe a measure of diversity or

17:22.720 --> 17:28.960
different context or something like that absolutely I think predominantly machine learning community

17:28.960 --> 17:35.680
have been using that quality kind of framing in doing the majority vote like the reason why they

17:35.680 --> 17:41.840
do majority vote is to actually remove the noise right so I think it probably comes from the

17:41.840 --> 17:48.640
place that you know machine learning researchers like you know I want to ensure the quality of

17:48.640 --> 17:53.360
the data that they're working with and this is like comes across as like noise but there's

17:53.360 --> 17:59.040
there's even there's two different things there's like in the context of an individual label

17:59.040 --> 18:05.600
data point the you know taking the majority allows you to eliminate the noise but then

18:05.600 --> 18:14.480
from the context of the kind of the labeling operations right you know often you're looking at the

18:14.480 --> 18:22.160
label or over time and and you're trying to understand you know which of the labelers that are

18:22.160 --> 18:29.040
best I've seen work that also looks at like this kind of agreement with majority as a way to like

18:29.040 --> 18:35.760
you know assess the quality of the greater themselves right that's that's what I'm alluding to and

18:35.760 --> 18:46.480
that this now the this additional angle that hey it's not just quality depending on the type of

18:46.480 --> 18:53.520
question that we're asking it's also a measure of you know different degrees of diverse perspectives

18:53.520 --> 19:02.000
yeah yeah and I think one I do want to note that like yeah it's it's possible that it could be

19:02.000 --> 19:08.080
a unreliable reader but if that reader is like internally consistent with their their own annotators

19:08.080 --> 19:12.640
like if they see similar things like again if they're like so then then they are actually bringing

19:12.640 --> 19:18.800
in a different perspective like that is kind of like it's not that it is lower quality it is just

19:18.800 --> 19:25.040
a different value system maybe is reflected in their annotations so that's what we were going

19:25.040 --> 19:29.520
after right like so this is an ongoing project it's not just one paper would not answer all the

19:29.520 --> 19:35.920
questions so in this particular case we were basically trying to tease a part or disentangle

19:35.920 --> 19:42.720
these disentanglements and sort of understand what factors might be contributing to why people

19:42.720 --> 19:47.600
are disagreeing right like is it just this person being unreliable just like you know randomly

19:47.600 --> 19:54.160
selecting things or is there a systematicity to the way they disagree with the majority right

19:54.160 --> 20:01.200
and one and one way in that paper that we looked at was looked at like there was one dataset where

20:01.200 --> 20:08.080
we also had access to the social demographic characteristics of these annotators so we looked

20:08.080 --> 20:14.720
at whether there is a difference in across like different genders this particular dataset had

20:14.720 --> 20:20.560
only binary gender no I think there was only one non binary gender person so it wasn't like

20:20.560 --> 20:28.240
big enough to like analyze all the different kind of gender categories but it was a you know

20:28.240 --> 20:37.280
gender was one thing and then a political affiliation or ethnic ethnicity or race

20:39.040 --> 20:42.560
and in this particular study we show that there was no difference in gender there's

20:42.560 --> 20:49.840
in like men and women had like or male and female annotators had similar rates of sort of like

20:49.840 --> 20:56.720
having rate rateers who disagreed with the majority with the in across annotations but it was

20:56.720 --> 21:03.760
interesting to note that like when it came to race African-American writers annotators like

21:03.760 --> 21:10.400
who identified as African-American in that dataset had significantly lower agreement with the

21:10.400 --> 21:18.160
majority rating than white American this was the dataset collected in the US so white American

21:18.160 --> 21:25.680
Asian-American Raiders so that is a problem that is a problem when a from a fairness perspective

21:25.680 --> 21:30.720
when you take majority then you're actually sidelineing a particular perspective like potentially

21:30.720 --> 21:36.400
there is like a you know perspective that is being sideline what was the particular

21:36.400 --> 21:45.440
the specific dataset and task so this is a dataset for determining sentiment like positive negative

21:45.440 --> 21:51.120
neutral sentiment leveling I do want to note though that this dataset was collected with the

21:51.120 --> 21:56.240
intention to study by so this was not a case where like there was a dataset and someone just like

21:56.240 --> 22:01.680
did majority vote and then moved on with like in the in an incorrect way this dataset had the

22:01.680 --> 22:08.480
social demographic information in them precisely because they wanted to study these this is a

22:08.480 --> 22:16.400
dataset built by Mark Diaz my colleague for his PhD thesis so he now works with me so that's how

22:16.400 --> 22:21.200
this dataset had this information usually machine learning researchers when they collect data

22:21.200 --> 22:27.360
for various reasons like do not collect the social demographic information because it's a much

22:27.360 --> 22:34.240
harder to get that information there's all sorts of risk involved with it yeah so we were able

22:34.240 --> 22:41.440
to see these these disagreements with the majority that differs across different social demographic

22:41.440 --> 22:49.840
kind of groups which is even a bigger problem from a fairness perspective because this is like a

22:49.840 --> 22:55.600
human or like a researcher's decision to sort of take the majority vote and by doing that you're

22:55.600 --> 23:02.480
actually actively sidelineing certain perspectives like in your data labels so yeah that was the work

23:03.280 --> 23:08.720
that you referenced and then we had a follow up paper that I can talk about where we kind of look

23:08.720 --> 23:14.720
at like how we can sort of deal with that like how we can constructively sort of deal with this

23:14.720 --> 23:21.600
like diverse perspectives so let's dig into the let's dig into that second paper you've identified

23:21.600 --> 23:28.800
this challenge and the second paper wanted to propose some potential solutions yeah so what

23:29.520 --> 23:38.240
we did there was basically rather than sort of trying to find this single ground truth for like

23:38.240 --> 23:44.560
these kind of subjective tasks ahead of the time like you know ahead of training these models

23:44.560 --> 23:49.600
at the data collection stage itself like taking the majority out vote rather than doing that

23:49.600 --> 23:56.240
we built a sort of like an approach or like we built a kind of a mission learning pipeline where

23:56.240 --> 24:03.120
we use a multi task approach where it's it's traditionally used for like you know you want to train

24:03.120 --> 24:09.440
a model for like multiple tasks that are similar so use use the same mission learning network like

24:09.440 --> 24:14.880
you know the network that you train like you use the same data but then you kind of like have this

24:14.880 --> 24:21.520
final layers that are trained for specific tasks so that you have like a shared embedding or shared

24:21.520 --> 24:27.680
kind of network for the most of the part and then like you have like this specific task specific

24:27.680 --> 24:35.760
like parts of that network so we looked into whether we can actually model separate annotators

24:35.760 --> 24:41.840
and their systematicity in which like they annotate like using this sort of like shared network

24:41.840 --> 24:46.960
like a multi annotator model so that like you have all the data is being used for training like

24:46.960 --> 24:52.720
you know this model but like this model does not output one label it outputs like okay if it was

24:52.720 --> 25:00.080
this person that person would have said the label X or so you have like if you had like 10 different

25:00.080 --> 25:04.960
labels a labelers like 10 different Raiders you actually are modeling these 10 different

25:04.960 --> 25:12.000
lip perspectives in the output and then in the output time you can actually take the choice of like

25:12.000 --> 25:16.240
if you still want to do majority voting like you can do the majority voting at the output like

25:16.240 --> 25:20.560
you know the machine learning model gave like 10 outputs and you can just take the majority

25:20.560 --> 25:29.440
vote there or you could say that oh we want to take the kind of outputs produced by these

25:29.440 --> 25:35.920
subset of annotators or subset of like these predictions that models like annotators from a

25:35.920 --> 25:40.080
particular background it this you know you can imagine a scenario where like you have annotators

25:40.080 --> 25:43.920
from like different countries let's say there are three countries that we have annotators from

25:43.920 --> 25:50.240
India, US and like say Germany and then when you are actually rolling out like products

25:50.240 --> 25:56.720
into the societies like you may want to actually use the majority vote or like use the vote from

25:56.720 --> 26:03.440
a particular region or like you know or the machine learning models predictions that

26:04.320 --> 26:15.120
reflect that annotator those annotators to kind of to be used in that region right you know so

26:15.120 --> 26:19.440
there are like so many different ways that you can be used you can use this approach so you

26:19.440 --> 26:24.800
basically captures this like multitude of perspectives in the prediction pipeline rather than

26:24.800 --> 26:31.600
sort of like suppressing it in the in the beginning of the pipeline it also gives you an additional

26:31.600 --> 26:38.320
ability to sort of know when there is uncertainty like when there is disagreement like because if you

26:39.520 --> 26:44.480
make a label as a bit majority labels if you call that okay this sentence is offensive

26:44.480 --> 26:49.120
just based on like in a majority vote like in the beginning of the data collection itself

26:49.120 --> 26:55.280
we would never know whether that was like you know agreed on by 10 people or like you know two people

26:56.640 --> 27:02.480
right like and it was an unanimous decision or not whereas in this case like you have these

27:02.480 --> 27:07.600
like multiple perspectives at the prediction time so you kind of know that oh multiple annotators

27:07.600 --> 27:12.720
would have disagreed on this particular case so maybe the machine should not predict here

27:12.720 --> 27:17.280
it should be like a human label or from that particular social context should like make a call

27:17.280 --> 27:22.560
whether this particular piece of text is offensive or not so that's another way I could imagine

27:22.560 --> 27:28.240
being used this being used is that like it gives you a handle on how much uncertainty is potentially

27:28.240 --> 27:33.840
here how much diverse perspectives potentially is here for this given piece of text it's kind of

27:33.840 --> 27:41.840
consistent with the general idea in dealing with neural networks to just give it all the data and

27:41.840 --> 27:48.720
don't try to clean it up too much because you're not hiding what you think might be noise that

27:48.720 --> 27:54.080
the network can actually find some signal in probably I mean that was definitely not the motivation

27:54.080 --> 28:01.520
for our like as approaching it but I think yeah the current neural networks are powerful

28:03.760 --> 28:09.520
in sort of like you know bringing in these signals like you know because prior to this sort of

28:09.520 --> 28:15.680
multi-task architectures which is like a two or three years maybe a little bit longer old prior to

28:15.680 --> 28:21.680
that like you would have had to like train if you have like five different like set you know annotators

28:21.680 --> 28:25.280
you have to train like five different models like you there's no way that like they have a shared

28:25.280 --> 28:30.640
network and they can take signals like even the smaller signals like from each of these

28:30.640 --> 28:35.360
raters to be like you know model differently or separately that would have not been possible so

28:35.360 --> 28:43.200
I think the current neural networks ability to sort of like pick up on these signals are definitely

28:43.200 --> 28:51.280
contributing to being able to sort of capture this diversity without like affecting like sort of

28:51.280 --> 29:00.240
like the end performance if that's what you're going for. And so is the the model that you're

29:00.240 --> 29:07.920
creating based on the kind of the annotator signal is this then kind of use separately as part of

29:08.640 --> 29:16.960
a downstream training task or are you kind of embedding it end-to-end in the ultimate task.

29:16.960 --> 29:21.760
That's a great question. I think that there is potential to actually use this as kind of like a

29:21.760 --> 29:29.200
lead-in for like other downstream tasks and so to take a step back like so this is ongoing work

29:29.200 --> 29:34.480
like you know we have not sort of like rolled it out and like into any products or anything this is

29:34.480 --> 29:40.240
like I work in research so this allows in in Google research that allows us to sort of like do

29:40.240 --> 29:45.760
this foundational research without having it to be tied to any particular products. So in that

29:45.760 --> 29:52.400
sense like we have not sort of like rolled it out yet on any of the products but like I think

29:52.400 --> 29:57.520
I envision this both as like a potential kind of like first step to sort of like you know

29:57.520 --> 30:06.240
tease apart these differences but it could also work as a sort of end-to-end scenario where like you

30:06.240 --> 30:14.480
know I could imagine like an online platform using this model to sort of have multiple answers for

30:14.480 --> 30:22.000
like multiple perspectives for sort of you know choosing whether something some particular content

30:22.000 --> 30:28.000
needs to be removed or art for instance like it could it could help in the content moderation kind

30:28.000 --> 30:38.000
of pipeline to sort of queue content for review appropriate like you know reprioritize content

30:38.000 --> 30:42.400
and so I could see this being used in like that kind of production scenario as an end-to-end

30:42.960 --> 30:49.840
when I say end-to-end like I often envision like a human label or a human human sort of like

30:49.840 --> 30:55.840
intervention it's a human in the loop or machine in the loop kind of situation rather than sort of

30:55.840 --> 31:01.520
like button click and like you kind of like you know get the label and like that's that decide

31:01.520 --> 31:07.040
like whether something is offensive or not. So there's often a human in the loop is needed

31:08.880 --> 31:14.720
and yeah in those settings like I could imagine this model being useful. How did you evaluate the

31:14.720 --> 31:21.360
the performance of the model? Traditionally for this dataset like if you had built like a single

31:21.360 --> 31:27.760
kind of label model like you would basically go and evaluate you know the typical machine learning

31:27.760 --> 31:32.800
evaluation pipeline like you know you have like a test set and you basically look at like the accuracy

31:32.800 --> 31:41.520
or precision recall and so on. So we evaluated first in terms of basically if you choose to just

31:41.520 --> 31:45.840
still do the majority word like if you had taken the majority in the beginning versus like

31:45.840 --> 31:52.080
this whole pipeline of like this multi multi task or multi annotator architecture which has like

31:52.080 --> 31:57.520
these multiple perspectives and then you take the majority word and looking at like sort of

31:59.040 --> 32:05.360
how well this does you know in that evaluation. So using the traditional pipeline to evaluate

32:05.360 --> 32:14.320
it and in that case we were able to see that like the performance was almost the same or sometimes

32:14.320 --> 32:19.760
in some data it's even better because better in the sense that like because it's it's modeling

32:19.760 --> 32:24.960
each annotator kind of like individually and it allows it to sort of like look at internal

32:24.960 --> 32:32.000
consistency of their labels better than when you actually merge them. So it doesn't happen across

32:32.000 --> 32:37.520
board but like in in in in some tasks like it performed even better in that particular evaluation

32:37.520 --> 32:44.960
strategy. But we also looked at sort of how well are we doing on calculating this uncertainty?

32:44.960 --> 32:53.360
We looked at are we able to or the the sentences where the model said something is offensive and

32:53.360 --> 32:57.840
like you know there was a lot of disagreement in the end like in the end predictions are multiple

32:57.840 --> 33:04.960
there's more disagreement. We compared that with like how much did annotators for this particular

33:04.960 --> 33:11.600
text disagreed with each other right? Like so we compared when the model has this high disagreement

33:11.600 --> 33:16.880
is that also the cases where like the annotators originally disagreed and that turned out to be the

33:16.880 --> 33:25.360
case there was a significant correlation between sentences where the annotators disagreed

33:25.360 --> 33:31.520
and the model disagreed and that kind of and that was a lot more than traditional ways of calculating

33:31.520 --> 33:37.760
uncertainty. So this is another kind of way we evaluated so that was not about model performance

33:37.760 --> 33:44.320
because our focus is not getting like the model performance from like 72% to 74% like that's like

33:44.320 --> 33:51.760
the typical sort of like the mission learning kind of research kind of objective and for good

33:51.760 --> 33:58.560
reason like you know it's it is important to improve the performance but in this work we are

33:58.560 --> 34:03.040
we are interested in like how well we are capturing diverse perspectives and are when we are

34:03.040 --> 34:08.400
kept having these disagreements in the predictions are they reflecting disagreements in the actual

34:08.400 --> 34:15.360
data and which turned out to be significantly correlated. So yeah has this research led you to

34:15.360 --> 34:23.600
kind of a set of axioms that or that you would recommend to folks as they're kind of building

34:23.600 --> 34:31.920
these labeling pipelines and you know wanting to create models that are as robust as possible

34:31.920 --> 34:37.200
as fair as possible like how should they think about all these issues? Absolutely yeah I was

34:37.200 --> 34:44.880
this like so this work is ongoing and there's like a lot more work that needs to be done and this

34:44.880 --> 34:52.160
particular model that I proposed is not going to solve all the problems in this space and so we had

34:52.160 --> 34:58.880
like laid out like a bunch of like sort of recommendations in the first paper actually that like

34:58.880 --> 35:03.680
you know the traditional practice is that you take a majority vote and sort of like that's what

35:03.680 --> 35:07.840
is in the data set most people when they collect labels from like multiple people they just take

35:07.840 --> 35:12.480
the majority vote and that one label is the only thing that's even in the data set. So you know we

35:13.680 --> 35:18.400
recommend or we argue that people should like when people collect data should sort of

35:20.160 --> 35:26.160
release annotator level that sorry individual annotator level kind of labels so that like you know

35:26.160 --> 35:32.000
you're not treating these annotators as interchangeable anymore also where possible like you know

35:32.000 --> 35:38.240
sort of being able to collect social demographic information about annotators as much as possible

35:38.240 --> 35:44.480
to do it responsibly being able to sort of like release that along with the data set would only

35:44.480 --> 35:50.640
enrich that and sort of the downstream users of this data set could basically use that information

35:50.640 --> 35:55.920
to account for any fairness failures or any of these kind of analysis which is very hard for us to

35:55.920 --> 36:02.160
do because most data sets in our community do not have even annotator level labels or like social

36:02.160 --> 36:09.360
demographic information. In addition we also argued in favor of sort of like documenting about

36:09.360 --> 36:15.200
the recruitment selection and assignment processes that are followed for this annotation kind of

36:15.200 --> 36:20.000
pipeline so that like it gives more information about like how these annotation labels like you know

36:20.000 --> 36:26.160
what was the diversity of Raiders like you know what like social demographic kind of factors or like

36:26.160 --> 36:30.800
groups were represented and what was underrepresented so those information is super useful

36:30.800 --> 36:38.160
for the downstream user of the of these data sets. So on that note like I want to also give a call

36:38.160 --> 36:48.480
out to the recent paper by my colleague Mark Diaz on sort of providing a sort of comprehensive

36:48.480 --> 36:56.480
framework for communicating these things about annotator diversity and annotator these processes

36:56.480 --> 37:03.520
through kind of a transparency artifact called crowd work sheets which capture these kind of

37:03.520 --> 37:10.000
information and that just got published like a couple of months ago at the fairness and accountability

37:10.000 --> 37:15.680
and transparency conference. So I encourage people to check that check out that work as well and I

37:15.680 --> 37:21.360
also want to call out that like this. The two papers that I discussed at length is work led by

37:21.360 --> 37:27.680
my intern Ida Davani who was at USC at that time last summer and so this is like almost a year

37:27.680 --> 37:32.880
long of work after internship like she continued as a researcher student researcher with us

37:33.600 --> 37:39.280
and she has now joined back as our team as a full-time researcher so this work is an ongoing

37:39.280 --> 37:45.120
effort like and one of the core sort of themes of my research going forward. Awesome awesome

37:45.120 --> 37:49.600
well Vinod thanks so much for joining us share a little bit about what you've been working on.

37:50.400 --> 37:55.760
Absolutely this was a pleasure to chat about this research and it was lovely chatting

37:55.760 --> 38:23.920
with you Sam. Same thanks Vinod.

