Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting
people, doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
This week on the podcast we're featuring a series of shows that highlight just a few of
the great innovations and innovators at the intersection of three very important and
familiar topics, data science, the Python programming language and open source software.
To better understand our listeners' views on the importance of open source and the projects
and players in this space, I'm conducting a survey which I'd be very grateful if you
took a moment to complete.
To access the survey, visit Twimbleai.com slash Python survey.
Please hit pause now and we'll wait for you to get back.
That's twimbleai.com slash Python survey.
Before we dive into the show, I'd like to send a huge thanks to our sponsor for this
series IBM.
Speaking of open source, IBM has a long history of engaging in and supporting open source projects
that are important to enterprise data science, projects like Hadoop, Spark, Jupiter and
Cubeflow to name just a few.
IBM also hosts the IBM data science community, which is a place for enterprise data scientists
looking to learn, share and engage with their peers and industry renowned practitioners.
Here you'll find informative tutorials and case studies, Q&As with leaders in the field
and a lively forum covering a variety of topics of interest to both beginning and experience
data scientists.
Check out and join the IBM data science community by visiting IBM.com slash community slash
data science.
Alright everyone, I am on the line with Inez Montani.
Inez is the co-founder of Explosion, a co-developer of the popular NLP open source library
Spacey and lead developer of Prodigy.
Inez, welcome to this weekend machine learning and AI.
Yay, thanks.
I'm really happy to be here.
I'm excited to have you on as well.
I've been looking forward to speaking with you for quite some time now.
I'd love to hear a little bit about how you got started on this path of working at this
confluence of open source and AI and Python.
How did it all happen?
Yeah, so I actually had to try out quite a few things to kind of end up at this point
where I found kind of like the perfect job that combines all of the things I like doing
and all of kind of my skills and things I'm good at.
So I initially, I've always been into programming.
So I grew up on the internet.
I've made websites where we got our first computer when I was like 11.
So I was really into that.
So I spent most of my teenage years building websites basically.
But I didn't actually choose to go into computer science.
So I did my degree in communication science and linguistics, which are also things I was
really interested in.
I worked in media for a while.
But I was always kind of programming on the side and doing those kinds of things.
I eventually, I met my co-founder and also the initial Spacey author Matthew Hannibal
in 2014 here in Berlin.
And yeah, we just started working together and we quickly realized that we had very
similar ideas about like building software, making the technology accessible, making it
usable.
And so yeah, so we eventually, we decided to find a company together, explosion.
Initially we bootstrapped with a bit of consulting and then focused 100% on product.
And development for developer tools.
And yeah, Prodigy was our first product that we launched.
Awesome.
And so Spacey came before Prodigy, correct?
Prodigy is relatively new, right?
Yeah, Prodigy, I think we launched, we went on sale in December 2017.
So it's actually been out for quite a while and we've been really, really happy about like,
you know, response to it and how well, like how many people have been adopting it.
But yeah, Spacey's been there before, Matt basically, he left academia when he realized
that like, wow, you know, the technology is becoming a lot more mature.
People, companies wanted to use his research code and asked him about licensing terms and
he was like, well, you know, if I actually focus on this and write a library that's really
focused on doing all of this in production and getting stuff done, that could be, you
know, really useful to people.
So yeah, he developed Spacey, it was, you know, released open source and I kind of started
working on it pretty much kind of around the time I was first released, but that's when
we started working together.
And so as I mentioned in the intro, Spacey is a very popular library.
I hear it come up all the time in the context of NLP, but for those that aren't familiar
with it, can you maybe share a little bit about what makes it unique and the philosophy
behind it relative to the many other libraries kind of in the NLP landscape?
Yeah, sure.
So Spacey is a library for natural language processing in Python.
So basically, if you have lots of text and pretty much any company, any kind of use case,
you always end up with lots of text.
And at some point, you want to find out more about that text that also goes beyond like
what you can personally read and also goes beyond a bit of like keyword search.
So you know, we want to find out like what companies I mentioned, what people, who says
what and to whom and how are the people, how are there all the things and concepts and
ideas related.
And so Spacey is a library that can help you do that using rule-based methods, but also
with using machine learning models and by training.
And you know, that also helps you train your own machine learning models to do these things.
And the focus is really specifically on industry and production use cases.
So I would say that's also where Spacey is a bit different from a lot of other libraries
that focus a lot more on research.
And you know, research is clearly like also a very important field, but we say okay, instead
of giving you lots of different ways to do one thing so you can, you know, compare
them and you know, compare different model architectures, we give you one way to do things.
And you know, one API, you know, we focus a lot on having a concise API and also having
like one implementation that does one thing and, you know, that and also I think I guess
one another thing Spacey is kind of famous for is that it's very fast.
So that's another very important kind of goal we set ourselves that like whatever we build
and whatever we ship to people has to run fast enough that you can process millions, billions
of documents, you know, in a time that's feasible for a production use case.
I imagine the 900 pound gorilla in the space is NLTK, is that kind of the, the, the
the fact of standard for, you know, folks doing NLP and Python or is there something else
that comes to mind for you?
So NLTK is definitely very popular library and it's also one, a library that many people
have started working with when they, you know, learned NLP, it's very, it was initially
developed for teaching and research, so it still has, you know, very wide adoption, but
it's also very much research focused, like a lot of other, you know, libraries, we now
have, you know, also there are lots of implementations that use PyTorch or TensorFlow to do NLP, but
still there's a very, you know, but a lot of focus is on, you know, really building the
model architectures and we kind of start like kind of on the other side where we say,
okay, we give you APIs and basically the whole like, you know, container objects and the
whole like infrastructure plus optimized statistical models to solve your NLP problems.
And you mentioned the, that part of it is that it offers kind of facility for rule-based
processing in addition to models.
Can you elaborate on that a little bit?
Yeah, obviously a lot of the excitement is around machine learning and models, but
I just did an interview yesterday, it hasn't been published yet, but we were talking about
how, you know, in the real world, you know, in these production use cases, especially at
scale, sometimes, you know, it just makes sense to do things with rules.
Yeah, absolutely, yeah, I would definitely agree with that, like that's also something
we see a lot.
So, you know, what space your office is kind of imagine regular expressions, but with
like, you know, a lot of additional features that you can take advantage of because, you
know, we can now predict a lot of things about a sentence that holds a lot of deeper information.
For example, you can find things in, but only if they attach to a verb or you can, you
know, use a lot of those like linguistic attributes to build a very, very complex or
also a very simple straightforward set of rules that lets you extract content or information
that you're looking for.
And especially actually if combined with statistical models, rule-based systems can be incredibly
powerful.
And also something we see a lot in a lot of industry use cases that actually, yeah, it's
kind of, you know, the fun part is training all your hip neural network models and fiddling
with the hyperparameters, but actually in production, what often really makes a difference
is a really good set of rules that's been tested, validated, and build up over a long
period of time and is really, really specific to the use case.
And in that enhanced, with some machine learning can actually often be much more effective
than like an end-to-end approach.
Are there any use cases that come to mind as, you know, particularly exciting or even
surprising why I never would have imagined someone would have done this with this code
that we wrote?
I mean, in general, like I'm one thing I'm always very, you know, excited about, or like
what that was very eye-opening was that like, it's really, there's, there's no like boundaries
in like the industries that use NLP, like, you know, often people think, oh, well, of course
that the large like tech companies or anyone's doing something with like tech modern stuff
will be using NLP, but it's like a lot of from aerospace to like, I don't know, energy
companies, like everyone has text and everyone uses NLP.
So often also when, you know, we have like new companies who are like, who start using
prodigy or annotation tool, we're like, oh, wow, you use spacey in production and you
do NLP.
Yeah, I guess it makes sense, but it can still, it's basically like, you know, it's
not, it's everywhere.
And in terms of, I mean, use cases, it's, I do, I do have to say that the things that
work best are probably not the most like, oh my god, exciting use cases.
Like, you know, people love to talk about the really like, you know, like cutting edge,
like, I don't know, things you could have never guessed, but actually the stuff that works
well is just like, really good old like information extraction.
You have some problem, you want to, for example, pre fill a database with information from
natural language text.
And that's actually, that's the stuff that actually works really, really well.
And luckily, it's also kind of the stuff that like offers the biggest, I guess, return
and monetary value to like companies doing that.
When you say the stuff, that's the stuff that works really, really well, a library on
that is that mean that it's the stuff that the library does best or the stuff that folks
have the most success in actually implementing or, or something else.
I would say actually we're just, we're just with NLP in general, whether technology or
doing machine learning with text actually, it has proven to work.
Like there are other fields like, you know, we are seeing a lot of like, you know, really
cool like achievements in even conversational stuff, but like it's just that, and you
know, that that just doesn't yet work as well as, you know, people would maybe, you know,
people on the outside would maybe imagine it to work.
Like you still don't have like a magical computer that can answer any question, whereas,
okay, yeah, but about the kind of, the more information extraction stuff, which also
is something we kind of focus on a bit because it works so well is really what's, you know,
that works, we can, you know, we can predict the right things, we can augment, you know,
the predictions with rules, and that's actually, yeah, it's just more successful than other
more speculative areas.
Is explosion you and Matt, or is it a bigger company than that today?
So it started out with only us, and it was only us for quite a while.
We're now working with a few other people on different types of projects.
So, you know, we currently have one person working full time together with us on cool new
features for Spacey, which is really exciting.
And then we have a few developers working on kind of an extension product to Prodigy,
which we're currently finishing, but we still, we still have a small company and a
very small team, and we also plan on staying a very small team.
So that's definitely an important part of kind of our strategy.
And so when you think about Spacey as an open source project, is the contribution
and the code from that primarily contributions made by the explosion team, or do you have,
there's clearly a broad community there, but is it a broad user community, or contributing
community, or a little bit of both, how is that aspect of the project then?
So I would say compared to other more community open source projects, we still have a fairly
small number of my core contributors.
That's true, so a lot of the sort of direction is driven by us, but it's also something
that works well because, you know, users use Spacey because they want to have like, you
know, a good implementation, and they're like, okay, you guys do the implementation,
and we'll use it, and we'll report bugs, and that's okay, that's fine, that's something
we accept, but where we see a lot of contributions, especially like recently, is all the language
specific stuff.
So Spacey ships with a bunch of rules and a bunch of kind of base, basically the kind of
basic setup for all kinds of languages that we support, and that's really where people,
even people who are kind of new in the field can very easily help out, like, you know,
you could, if you speak, or if you know some language, you could maybe add some rules
to improve the way Spacey can tell what a word is and what's not a word and what's punctuation,
for example, or, you know, add some other rules for limitization, or, I don't know, just
add some more tests.
So that's really, that's where we see most of the community contributions, and that's
also where the community contributions are most valuable.
Given the focus on rules and some of the fundamental NLP techniques, do you also track closely
the more cutting edge stuff, like how does Spacey relate to, you know, these new models
like Bert and GPT2, do you try to implement those and ship those with Spacey, or are
they kind of separate?
Yeah, so we, I mean, basically, so our mission has always been, we take what's proven to
work in research and bring it into production so people can views it reliably.
So that's always been our focus, of course, we track what's going on, and then the next
challenge is, okay, we have to see how are we implementing this in a way that it makes
sense for people, because, you know, often what's a bit unintuitive to people is that,
well, you can't just, you know, you can't just pip install Bert, and then it will just
like run magically in your production application, but it's like, you know, sometimes people
have like this idea, it's like, oh, what can be so hard about like just, you know, giving
us all of these models, but, you know, so what we do is we see, okay, how can we make
this work?
Also, how can we make this work with the performance targets we have?
Like, you know, if you have a system, you know, like Bert, where we, basically, we predict
the next word given, you know, the context and the previous words, that's like, those
models are very, very large, and they're also not necessarily very fast.
And at Spacey, we have like, you know, performance target of like 10,000 tokens per second.
That's pretty fast.
So like, for example, to be able to implement this sort of idea, we came up with kind
of our own way of doing this.
So what we're doing in Spacey is we're actually predicting the vector of the next word,
which makes our models much, much smaller and makes it much, much faster.
But actually, yeah, with the latest version of Spacey, we were able to ship a pre-training
feature that basically, you know, let's, let's people use the way, those very new transfer
learning techniques in NLP that I've made the headlines recently, that's a, that's a
very cool thing.
Yeah, can you elaborate a little bit more on that distinction, the distinction of predicting
the vector of the next word as opposed to the next word and how that gives you the performance
increases?
Well, basically, I mean, if you, you know, if you predicting the vector, you're only predicting
kind of an approximation and we can also take advantage of pre-trained word vectors that
we already have.
So like, you know, something like word-to-vec, we also ship like a pre-trained package,
and you can actually, so instead of always keeping track of, you know, all of the individual
words and having representations of all of these words, we actually only have like kind
of the rough meeting representation, the word vector.
So, you know, we can predict that, we can use take, you know, we can take advantage of
a pre-trained word vectors we already have, and it basically makes, it makes the overall
artifact much smaller, and we can rely on that at runtime, so it makes it, it makes it
faster and smaller, and like, you know, there's some other tricks that we have in spacey in
a way we, you know, store those vectors and, you know, cache the data, that also, you
know, contribute a bit to the performance.
And so, yeah, we tried this out, we actually, we had this idea for a while, but we're like,
we weren't sure if it would work, we ran some tests, it looked good, and actually what
was really great was that, like, while we were trying this out, someone actually published
a paper showing that it worked with exactly kind of the same idea, and we were like, yay,
they did, like, of course, of course, they did like much better experiments, and we could
have ever done, and, you know, they really did this well, so we were like, okay, that gave
us a lot of confidence.
And yeah, so we actually, yeah, we just shipped that with spacey 2.1, and there are a lot
of other things we also were like, you know, working on, and spacey does have like its own
neural network model implementations, and we actually have kind of our own little library
for that.
So it is, you know, it's, we're obviously baking to keep up with what works, but it's also
not like, you know, if you want to try out some deletus like model architecture by, you
know, in some paper that was recently published and compare that and hack around with it, that's
maybe, spacey is maybe not the best choice for that, because, you know, we give you one,
the implementation once it's ready and once it's usable.
And so is spacey, is it pure Python, or is it written in C, or underneath, or, yeah,
so it's Python with C extensions, yeah.
Okay.
And the NLP stuff that you're doing, are you writing that in, Sython, are you writing
that, like, are you, do you ever like popping down in the C to get speed, or is it all done
in Python?
I mean, it depends, like some aspects of the library were actually, you know, that, that
really matters, is written in Sython, other parts, we can write in pure Python.
I mean, it's kind of a nice, that's the nice thing about Sython is that you can actually
write Python, but can take advantage of the C whenever you need that, and it still all
looks like Python, which, you know, makes it, at least makes it a bit more approachable.
It still means that, okay, maybe for some contributors, it's a bit of a barrier because
it's just like, I don't know, you have to know some arbitrary stuff, many needs to be compiled,
so it's maybe not as approachable as a pure Python project, but yeah, so it's, I'm
not even sure, I'm not sure it's 5050, it's like probably a bit less, like the most of
the models, of course, the model implementations, obviously, and Sython, then some other, you
know, all the stuff that, all the objects, container objects, all that, like, kind of infrastructure
that needs to be really fast, yeah, and then some Python around it.
Okay.
So all that familiar with, with Sython, I assume that it was Python with an underlying
C implementation, but it sounds like you're a bit more exposed to the C part with Sython.
If you're doing compiling and stuff like that, well, yeah, you, in the end, you compile
it, but it's more like what I meant was like, okay, the syntax, like, I don't know, if
you, if you look at, okay, if you, if you look at spacey source and you look at, look
at the code and what it looks like, it's, you know, you'll be able to read it because it
looks, it's Python.
Okay.
Like, the syntax is way, you know, it will look way familiar.
They're just like some, you know, some little things that look a bit different.
Okay.
And is, is spacey as well a commercial product for explosion or is, is prodigy the first
commercial product?
prodigy is, prodigy is our first commercial product, and we also, it was very important
to us to kind of have a clear distinction between like the open source library and how
we make money.
Um, I mean, at least in terms of, okay, what, you know, what you can use like spacey will
always, spacey is a free library and the code we believe is always, you know, that, that's
always going to be free and open source.
And instead, we think that actually it's much nicer to monetize all the, you know, the
space around it.
Like, for example, um, you know, we can say, Hey, if you, if you power use of spacey and
you really like our open source software, we have something else that, you know, you
might be interested in, which our first product of, our first example of this is prodigy.
So, um, you know, we, we actually think it's like, you know, there's, there's a lot of
talk around like, Oh, how do you monetize open source?
And we think that having a very clear separation between, okay, here's our free library.
Here's our free product and here's our paid product.
Um, it's actually very useful and very good.
Mm-hmm.
Do you also provide commercial support for spacey?
No.
So we've actually, that's, that's another thing.
Like, you know, we, of course, it's, this is also a common, but they come in like business
model and it's, it is what a lot of people do.
And, um, in, you know, in some cases, it works, it actually works quite well because you
can have like, you know, especially in, like, infrastructure cloud staff, there's, there's
a big space for like that sort of stuff.
But for us, we always felt like you end up, if you also, if you're the vendor, um, and
you also like the author of, um, inner the product, uh, you often end up in this really
weird situation where you want to, you want to provide like as much free support as possible.
Like, you want to have really great documentation, you want to have all of, um, that stuff, because
you want people using the library, but at the same time, you sort of want, you want people
to pay you to help them with it.
And if you, you know, if you, a software is like quite easy to use, um, well, then, you
know, you're making less money because people don't actually need you.
Um, if you make your dog shit, cool, people will actually come and want to pay you for,
uh, providing the dogs, but then, you know, you, you're soon running out of customers because
you're, you know, you're product is so shit that always going to use it.
So that's, that's kind of, there's a real dilemma and it's like, to be fair, okay, machine
learning, there's a lot more, actually any, you know, support wouldn't necessarily be
support in terms of, hey, how do I install this?
So how do I write this kind of logic?
It's more like, how should I structure my, you know, P projects?
So how should I do that kind of stuff?
But, um, we actually think there's, you know, you can, uh, people, people say, oh, you
can run lots of companies, but like, in reality, you could run one company, like, you
could really only run one company and we were like, okay, the company, we run one, where
we have the best, like, edge and what, you know, where we actually, you know, like, also
what we enjoy doing is building developer tools, it's not running a support company.
Can you talk a little bit about prodigy and the focus there?
Is it, uh, specifically focused on annotation for NLP types of problems or, or textual data
sets or, uh, is it broader than that?
I mean, in general, I would say, um, more generally machine learning, but of course, I would
like, I think our, of course, the NLP support is probably, um, the best because that's
also, I mean, compared to the other, um, areas of machine learning, also just because that's
what we also know best.
So I would say, um, okay, maybe we have, we have a bit more, um, in there for, that's useful
for NLP compared to, uh, computer vision, um, but the idea is basically, yes, it's an annotation
tool for data science and machine learning and, um, it was basically inspired by us, like,
you know, very early on the company for a few months, we took on some consulting projects
and we saw that like one topic that always came up in every single, or everyone we
talked to, um, was labeling data and also this all like, okay, you know, a lot of companies
would either do it in like Excel spreadsheets or they just send it off to mechanical Turk,
then they'd get it back, train a model, wouldn't work, um, people with a, yeah, we're not very
happy with their workflows around that.
And there was, we kind of saw a gap for like, um, actually a tool that lets the data scientists
get a bit closer to that sort of data collection process because, um, if you just, a lot, if
you outs, just outsource that completely, you're actually outsourcing a lot of the decisions
about how your model is going to perform and what, what your model is going to do because
the, you know, ultimately the labels are, uh, what you're going to predict and how your
application is going to work.
Right.
And that's actually quite ineffective.
So instead, um, what we, what we're suggesting is a much, much closer workflow, um, where
the data scientists can, or the, um, developers, engineer, whatever, uh, can we bait them involved
in the initial, uh, labeling process, um, SIT, you know, you can, you can write little
Python scripts that queue up the work.
You can try out concepts, um, much more quickly because we've built a very, very efficient,
very fast interface.
So let's you move through the examples quickly, um, we have some approaches where you can
try and put a model in the loop and, um, instead of, you know, doing everything from scratch
correcting the model's predictions and BIDs, sure, we're not saying all the data scientists
just have to do it all themselves by hand, but you can actually, you know, you can label
a few thousand examples in an hour and then train your model, see if your idea worked.
Most likely you won't work because nothing ever works on the first try or most things.
We'll never work at all in data science and that's just the reality of things.
And you say, okay, you need to, you know, if you can find what fails quicker and find
what succeeds quicker as well, um, you're actually going to be more successful.
And that was what motivated, um, Prodigy and the way we set it up as a developer tool and
not just as some like platform.
It's interesting that you describe it like that as a developer tool and not just some
platform, because when I think about the things that you've done, it, in my head, it sounds
a lot more like a platform than a developer tool in a sense of, you know, for example,
you talk about how you can put the model in a loop and I've heard Prodigy come up in
a number of conversations in the context of active learning and that starts to sound
to me a lot more like platform than, you know, tool like in Excel or some other kind of
desktop thing that's just a lie me to like crank through a bunch of text or images and
put labels next to stuff.
Yeah.
I mean, I think that one of the big differences as well is that like, okay, we build developer
tools and all our tools are very privacy, day privacy focused.
Like, you know, space you in full that you run it on your machine.
Um, it's not just like some software as a service API and also it's, you know, it's
just like an open source, um, and the same with, uh, Prodigy is, it's not like, you
know, an open, free open source product, but it's a product and it's a library as well
and you can download it, you install it on your own machine.
It works completely offline and it just like runs on your hardware.
And so in that sense, you know, it feels a bit more, you know, like, I was often compared
to like, you know, this, remember Adobe Creative Suite before they went all cloud, you
know, you could actually, you could buy Photoshop and then you would download Photoshop and
then you would have it.
Right.
And then they wouldn't take it away from you and you could use it.
And then if in two years, you pick up your design project, it's all still there because
you still have Photoshop and that's also, that's kind of, that's, that's also how we see
these things and that's also how Prodigy functions, at least, uh, you know, the developer tool
version.
Got it.
And, yeah.
And also because, you know, people, yeah, another thing is actually, so I'll, um, we
call this let them write code.
It's basically developers can program and they like to program and often instead of coming
up with like very complex arbitrary configuration languages and APIs to do stuff, often the best
way to configure a program or configure something that you want to do is to write a few lines
of code.
So, um, Prodigy can be configured fully in a Python script.
So if you, you want to let load some data from some arbitrary database that you have and
then like filter it that way, we're like, cool, can you write that in Python?
Yes.
Cool.
You can use it.
So, that's, that's another thing where I think the developer tool, um, angle comes in
and also it's something that people actually really, really like about this.
Developers generally like tools that don't lock them in and tools that they can interact
with via a programming language naturally.
Can you elaborate on that last, uh, example and maybe, or maybe provide an example of,
you know, is this the user experience of, of Prodigy and, you know, for a given use case
and what are, what's the code that you might need to write and how you might integrate
a model in a loop?
I had all the pieces fit together.
Yeah.
So, we have, um, we do have a few like built-in workflows, so it's like, you know, you can
use it and maybe you never, you know, you don't often have to really write your own code
from scratch, but the idea is at the center of it are recipes, we call it which are Python
functions.
So there are Python functions that we turn into, um, a command.
So one of them could be, okay, let's say you have some raw text you want to load in and
you have a pre-trend model that you've downloaded from Spacey that can predict, uh, person names.
But because your text is like obviously quite specific and maybe you're working in like
finance or something and it has like person names or organization names that are quite
different from like some generic pre-trend model.
So you're like, okay, you want to see, hmm, can I improve that pre-trend model on my data?
So it performs a bit better.
So you can run one of these command line scripts, you, um, load pass in the path to your data
file, you pass in the pre-trend model you want to use, you pass in some labels and then
you start that up and then you get the annotation interface and it will highlight a prediction
from the model and you can say, yep, that's correct or you can say no, that's wrong.
And what under the hood, what we do there is we load up the model and we'll get all possible
predictions, um, for your, for each example and then, um, all of these, uh, possible predictions
will come with a score and then we've, um, by default, we'll focus on the scores that
are closest to 0.5.
So the ones where, um, the model is most uncertain, um, basically so that's, that's also what's
referred to us, uh, uncertainty sampling in active learning.
So the idea is if we focus on the ones where like, um, the model isn't sure, that's also
where you decision will have the highest impact.
If you're only labeling things where the model is like super confident, that is correct.
The labels and the annotations you provide have like, um, you know, the gradient from
that is just less significant than if you focus on the ones, um, where the model, um, yeah,
isn't sure at all whether it's like A or B or nothing or, um, and so yeah.
So that's, that's one possible workflow that you could use and then, you know, that's
actually super fast, you know, you're just clicking yes or no, you're focusing on one
thing at a time can easily collect a few hundred annotations in like, uh, 10, 15 minutes.
And then you can run a little training, uh, command, update your model and just see, um,
what the results look like.
They're obviously not going to be like the most, you know, they're not going to be the definitive
results that you can report at the end, but they give you some idea whether what you had
in mind, there is good or not, but you know, if you see that like the model is not learning
anything, then you have at least some idea that like maybe, um, my idea wasn't that great
or maybe it's just more difficult than I thought it would be.
Or if you're like, Oh, accuracy is going up.
That looks promising.
I should spend a bit more time on that.
One of the things I hear from folks is about the need to kind of drive repeatability
into the process of collecting and labeling data and producing models does the fact that
prodigies a standalone tool, get in the way of doing that or are there ways to kind of fit
it into a broader pipeline or workload that allow you to establish, uh, some degree of
repeatability or consistency?
Yeah.
No, I think, um, in the end, you know, if you, for example, if you're writing your own
functions, you can actually have, you know, the script that you can, you know, you can
commit your script to your GitHub repository and you have that and everyone can always,
you know, reproduce the logic you use to, for example, select your examples, um, and
also, I mean, another thing, um, that we think is quite important is that a lot of problems
people have down the line often come down to, um, problems with, uh, you know, the label
scheme and how the task is defined in the first place.
Um, so, uh, you know, often you're like, Oh, I want a label like, you know, I want my
model to predict this.
So my system needs to do X and often people approach this from the very end to end viewpoint
and often that's not necessarily with the one that like, uh, works best on any label scheme,
any, anything, you know, basically defining what you want your model to predict often
needs lots of iterations.
And once you, you know, once you get that right, you're often, um, that's where it really
starts working.
Is there a specific example of that, uh, that comes to mind?
Um, so for example, I mean, one, um, one thing we see a lot is that like people, people
often start, you know, a machine or a data science project often starts with like, um,
you know, so a defined set of categories that you want to, uh, for example, uh, categorize
your text into so you might have like something like, you know, um, different type, different
types of clothing, for example, and you want to, you know, extract, uh, that from like,
your customer emails.
And so you have like, you know, you start, the data scientist starts off with like a very
broad catalog of like every type of clothing we offer and every type of clothing that
like the system needs to categorize.
And then often, you know, one approach could be, okay, well, we need to take a few million
examples or, you know, emails and have someone label them and select out of our, um, hundreds
of clothing types, what clothing types are mentioned.
And I mean, it sounds like a very straightforward approach, but often probably, you know, at
some point down the line, you'll notice that like your machine learning model is actually
really, really bad at distinguishing adult shoes from kids shoes, uh, for example, in,
you know, like brand names or, um, you know, summer code and winter codes, you know, that's
something.
Maybe, maybe you can, you know, but like that's probably something that's very quite,
that could be quite difficult to predict just on the basis of like the raw text and the
surrounding words.
So, you know, one, one approach that you could then after, you know, you try this out,
you realize our model's not learning anything.
And then, you're like, okay, how could I improve that?
One approach could be, you start with a bunch more broader definition.
You start, okay, first, I'll let my model label whether something is clothing, you know,
you can start, okay, start collecting some data for that, um, and then train it.
Okay, actually looks pretty good.
Um, next step.
Okay, maybe we can add another model component that then given a clothing, um, item or clothing
brand can then, um, you know, assign it to one of our, um, you know, entries in a knowledge
base or to a more fine grained, uh, category and so on.
Like, that's just one, one example that just came, that came to mind based.
And in the end, you know, that, that, that stuff really matters.
Like, you know, you end up at the same results, but, um, the process, how you work there
is very, very different.
And it takes, it can easily take a few iterations, uh, to get to the point, uh, where you,
you kind of, you have a good idea for what, what can my model learn and, um, what's actually
feasible and what machine learning tasks do I have to break this larger, they abstract
business goal into?
I'm curious, what are some of the thing, you know, the, the NLP space as, with all of, uh,
machine learning is, you know, changes rapidly, lots of exciting stuff happening.
We talked about, uh, some of that in terms of these new language models.
I'm curious, what are you most excited about in this space?
Um, so definitely, I mean, the transfer learning stuff was definitely like a big, um, breakthrough
because we've always, for quite a long time, we're like, okay, this should, this should
be possible and, um, it was kind of, you know, I think for, for many people in the space,
it was kind of clear, like, okay, at some point, hopefully someone's going to show here's
how we should do it.
And, um, especially the fact that like, um, you know, we can potentially pre-trained, um,
embeddings that would, um, allow us to get much, much better results, much more quickly
with, um, significant fewer examples.
And that doesn't mean that like, um, you know, labeling examples becomes less relevant,
actually, um, you know, I think it, it means it, you know, you can try out more and
it becomes a lot more relevant because, um, you know, cool, if you only need like 200 labeled
examples to really get a, um, very definitive idea of whether, um, what you're trying to train
works or not, that means you can like, you know, you can train much more, more quickly
and you can train so many more models, so much faster, um, you can try out so many more
ideas.
Um, and I think, I think that's like, that's super exciting.
And, um, yeah, it's really cool to see this already working quite well.
Is there other things happening that are, that you're particularly paying attention to?
Mm, let me think, um, so I mean, one, one thing we, like, I can, I can talk about one
thing we're working on at the moment is entity linking, which I also think is really
cool.
So basically, you know, the concept of you have like a person name and you want to link
that back to a knowledge base.
And it sounds, um, you know, the kind of concept sounds, sounds quite simple, but it's
actually, you know, there's a lot you have to consider and you actually, you know, you
want to, you have a mention of, um, Apple or like, actually, I'm trying to think of it
better, like more, um, you know, ambiguous example, but like there are a lot of these,
like names and their different options, it depends on the context and, you know, you
want to assign those back to large knowledge bases.
Yeah, that's a feature, um, that's, yeah, maybe not as like, oh, shiny, um, new cutting
edge stuff, but it's actually another one that like people really need, people really
want, and that's going to make a huge difference for people using NLP, uh, for variety of like
business use cases and industry use cases.
A feature like that is, are you typically or not typically, but in this case, are you
attacking that using traditional rules based or heuristic types of approaches or machine
learning or neural nets, like we're in a spectrum, uh, you know, what, which of the tools
do you use to, you know, build out a feature like that?
Um, I mean, well, it depends on the feature, like, um, you know, in this case, it's, it
is a combination of machine learning and, um, kind of, uh, kind of ties in, uh, with
the rule based approaches, it also ties in with like the neural network models we already
have, uh, which are the models that actually predict entities in the first place.
Um, so, I mean, it really, like, it always depends on the, um, on the use case, but like,
what, you know, for us, what matters is, well, what works best and what's, what's fast,
what's efficient and what actually, um, works best, what generalizes best and what's, um,
customizable because, you know, ultimately, it's nice if you can provide people with
a pre-train model that happens to work quite nicely on, you know, some academic benchmark
task, but what people really want to do is they want to plug in their own data and they
want to plug in their own stuff and they want to plug in their own, like ideas and, uh,
goals.
And so that, that's also kind of how we decide how to approach a thing.
Like, you know, we're not, we're in, you know, we're not in position where we have to,
you know, come up with like, what's, what's a good paper to write?
Like, that's, you know, that's another way, um, and I'm not even, you know, dismissing
that.
That's a very like, um, you know, genuine, like, um, you know, valid motivation that
people in research would have, but what's, you know, good papers, not necessarily what
like, you know, people actually will find like the most useful as a, in a practical application.
If you were starting over with spacey or they're things that you do very differently.
Oh, that's, that's a really good question, actually.
I'm not, I'm not sure if I thought about this much.
I mean, they're just, they obviously some API decisions that like, um, uh, I would have,
I would have done differently or we, we should have, we would have probably, um, done
differently.
Just like, I mean, it's mostly just like small, small stuff where, you know, you design
something one way and then you realize, ah, that's actually super, that's kind of confusing
or like, kind of taking on this life of its own and sensor on, doesn't send a right message,
um, math, we should have done this differently.
Otherwise, you know, it, of course, it depends on like, it always depends on, okay, what
you have like available and, you know, what was like possible at the time.
But actually, I think I'm quite happy with like, you know, the progression, like sure,
there's sometimes I wish like, you know, we would have had like more time, uh, that we
could have like, you know, spend on the project, uh, to work on that because, okay, you
know, it was very important for us to stay independent and be independent.
And so, um, you know, while we were like initially, like bootstrapping the company, um,
yeah, we had a bit less time, um, but yeah, like in hindsight, like, I'm very happy with
how things turned out and, um, yeah, um, on that theme of independence and you previously
mentioned that you intend for, uh, explosion to remain a small company.
We talk a little bit about that, uh, and your motivation or philosophy there and, um, you
know, maybe, you know, any thoughts for folks that are also interested in kind of contributing
in the space, um, you know, but aren't necessarily excited about doing in the context of a large
company.
Yeah.
I mean, for us, like we've, um, basically, I mean, you already, you already see this in
all kinds of other software projects that like most, a little software is actually written
by a very, very small number of people, like even if you look at very large, like libraries,
it's, um, you know, it's not like, you know, thousands of people kind of wrote on, you
know, worked on the same piece of code.
Um, it's like often development teams are very small and like for a reason because, you
know, it's like, you know, writing a novel with like 200 people.
That's like, you know, that's a fun art project, but that's not, that's not necessarily
like, you know, that's not going to be the best novel, you know, right?
So, um, you know, software teams, I often by definition are quite small.
And, um, another thing is that actually by being quite small, you can really take a lot
of advantage of, um, you know, very diverse skill sets that, that large companies can't
necessarily.
So, you know, large companies often hire in a very particular way, um, because you have
to hire in a way that makes your team more interchangeable.
Like you, you know, you need, you need a lot of people that are quite similar because,
you know, if you're scaling this up, um, that's kind of the most efficient, but if you,
a small team, you can actually, um, you know, hire a lot of people with way, um, specific
skills that work for, um, you know, specific tasks, then you can have people with kind of
overlapping skills, people with a broad foundation in one, um, technology, but who also have
done like other things, like it's typically, it's often referred to as t-shaped skills
because, you know, kind of like a t-shirt.
So, you have like, I've always found these a super weird metaphor because like, it's
a t-shirt, but like, you know, you're brought kind of a broad foundation and then like
these like arms, but, um, I've, I've started calling it more like, you know, I think of it
more as like tree-shaped skills, you know, you have like the stem and then you have all
these like branches and you can have like, um, you know, you can have small branches, you
can have big branches, you can have like different branches and different directions, branches
can overlap overlap with like some other tree, you can grow new branches, which actually
this is another reason like, yeah, I hate the t-shirt metaphor because it's like a t-shirt
that can only like get worse once it's like there for a tree, you know, a tree can grow,
this is like life in it, but basically that's sort of, that's the idea we've had like,
um, for our team, we can actually, you know, really have like, um, you know, complimentary
skills, um, in a team, um, and you know, really take advantage of that and it's not like,
you know, it's not like large companies are not doing that because they're like stupid
and don't see this, they like, aren't because of, you know, they operate slightly differently.
So, um, yeah, like we don't, we are small, but we're still, you know, we can still do things
and it's still, I, yeah, there's one anecdote that like I sometimes tell in this kind of
context, which is, um, once we were asked, um, whether our company would pass the bus test
because for someone who wanted to buy our software, this was like, um, this was important
and yeah, to, yeah, if someone, yeah, for anyone who doesn't know what the bus test is,
it's like, it's a varying number of people, but it's basically if four people were run
over by a bus tomorrow, would your company still exist? And at that time, we were like
two people were like, no, we'd be minus two. But it's also, it's a very weird concept.
The person ended up buying prodigy anyways, but like, um, it's, it's an interesting framework
that some people use and I don't think makes much sense because essentially you're always
asking like, well, how interchangeable are like your people and, um, you know, who could
you afford to like spare in a bus accident? Mm-hmm. And yeah, yeah, so we, I've always
find this kind of weird and like we don't pass, we probably do not pass the bus test. Um,
we kind of pass it now, but like, we'd be in a pretty bad state. And interesting anecdote
along those lines, I was just this morning reading, uh, an article about, uh, long story
this lawsuit that, uh, hurts is apparently suing Accenture for botching this, uh, digital
transformation project. It was going around on Twitter. And if you read the, as a $35 million
project to build some websites, uh, that kind of integrates into, uh, their reservation
systems, kind of, you know, update their website. Uh, and you read the lawsuit and all of
the things that, you know, could have been done so much better on, on both sides, but,
you know, in particular, there's this one point where they talked about how kind of Accenture
had full ownership of this project. And there were two people that knew the project and
they pulled them off and put them onto something else. And that was one of the reasons why this
thing got delayed by two years and never got finished. Yeah. And then, I mean, they built
a website. There wasn't responsive and like everything was, we were so insecure. It
was like unusable when like, wow, that's like, that's almost, that's a skill. Like, you
know, it's, you can build so much stuff nowadays that like works okay, even if it's kind
of crap. So there's so many lessons there, but the thing that, that connected to what
you were speaking was, you know, even Accenture, you know, in this case, couldn't pass the
bus tests. Yeah. Hold these two people off. And, you know, that was a significant contributor
to the failure of this project. So, yeah. So I think it's a very bizarre metric. And yeah,
so that was one of the things we saw this early on. We're like, well, we could, you know,
we already do it. We were already doing great stuff when we're two people. Okay. If we
have a few more people and they're the right people, we can do even more stuff. And that's
like fine, we also get to, you know, focus a lot more on the core work because we have fewer
distractions that, you know, you'd normally have if you, you know, not like a few, for example,
if you're running at a loss, which is also something we said, okay, we don't want to be doing
we actually, because I think, you know, for there are many legitimate reasons why your company
would want to start and run at a loss or at a significant loss fairly on and why you would
have to, you know, where you need capital and why you would have to make the decision to sell
equity in order to function. But we're always in a, you know, very good position that we didn't
have to do that. And so also we thought, okay, for what we're doing, there's absolutely no logical
reason we would have to be running at a loss. We can actually run at a profit because we can have
a product and people can buy our product. And then, yeah, we can sell more products and we can
make more products and do more things. And then people can, you know, give us money and then we
can spend less money than that. And then we'll have some money. It's kind of, it's a very crazy
concept, but like, you know, it works. Virtua cycle. Yeah. Yeah. So, so that was, that was another
thing that really helped us, you know, stay focused and do our things and also being able to really,
you know, it's one thing you can validate your ideas. Like, by, I don't know, talking to people,
doing like surveys and doing like all kinds of things, but like in the end, making a profit and
making money is like a very, very good way to validate what you're doing. And it's also a very
honest way of validate. Like, you know, you, money doesn't really lie. Like, if nobody wants to buy
your product, then well, nobody wants to buy your product. And then, you know, that's, that's a
pre-clear sign. And on the other hand, okay, if you, if you see cool, what I'm doing works, people
like it so far, people are interested in buying that. That's a very good, you know, that's a very
good way to validate your ideas. And yeah, yeah, so it's like, you know, we see this, that's,
that's a much better KPI than a lot of other things. Well, you know, thank you so much for
taking the time to share what you're up to is great getting to know you and explosion and
spacey and prodigy a little bit better. I really enjoyed our chat. Yeah, thanks.
All right, everyone. That's our show for today. If you like what you've heard here,
please do us a huge favor and tell your friends about the show. And if you haven't already
hit that subscribe button yourself, make sure you do so you don't miss any of the great episodes
we've got in store for you. As always, thanks so much for listening and catch you next time.
