WEBVTT

00:00.000 --> 00:10.980
Alright everyone, welcome to another episode of the Twimble AI podcast. I am your host Sam

00:10.980 --> 00:17.840
Charrington, and today I'm joined by Jeff Gailhar, Vice President of Technology at Qualcomm.

00:17.840 --> 00:22.120
Jeff, welcome back to the podcast. Thank you very much. It's great to be back again.

00:22.120 --> 00:25.640
Great to have another conversation with you, Sam, about what's going on in AI.

00:25.640 --> 00:30.960
I am looking forward to digging into our conversation. It's always a lively one. We were talking

00:30.960 --> 00:37.560
a little bit about what was going on in your world in our pre-call, and one of the points

00:37.560 --> 00:41.960
that really jumped out at me that we'll be exploring in the conversation today is kind

00:41.960 --> 00:46.680
of this divergence that's happening in the industry between model complexity, increasing

00:46.680 --> 00:52.440
and device power in some cases decreasing, in the case of like IoT devices. But before

00:52.440 --> 00:55.680
we do that, it's been a while since you've been on. I'd love to have you share a little bit

00:55.680 --> 00:58.440
about your background and role with our audience.

00:58.440 --> 01:05.040
Sure. Thank you very much again for having me. Look, so I'm VP Technology. I had Qualcomm's

01:05.040 --> 01:10.280
AI software portfolio, and so we'll of course talk about the Qualcomm AI stack, and some

01:10.280 --> 01:16.480
of the challenges you and I have talked about. But my role really is to provide a platform

01:16.480 --> 01:22.000
of software solutions on top of our AI hardware that spans Qualcomm Silicon. Then

01:22.000 --> 01:28.400
enables us to address market segments from cloud all the way to IoT, including automotive,

01:28.400 --> 01:32.760
mobile and so on. So that's really what my team is focused on doing.

01:32.760 --> 01:40.560
Just for context, talk a little bit about the relationship between your focus and that

01:40.560 --> 01:47.520
of Qualcomm AI research. As you know, I'm fortunate to have the opportunity to talk to a lot

01:47.520 --> 01:54.000
of your colleagues and research about their latest projects and papers at conferences.

01:54.000 --> 01:56.920
How does that work relate to what you do?

01:56.920 --> 02:01.400
Really the way to think about it is that there's sort of three prongs for success for AI

02:01.400 --> 02:07.480
at Qualcomm. One is hardware. Of course, we're a Silicon First company in a lot of ways.

02:07.480 --> 02:12.240
Software, which we just talked about. And then the research, which really is, you know,

02:12.240 --> 02:16.920
drives our innovation. And so my job is to work with Qualcomm research, to work with

02:16.920 --> 02:22.360
the hardware teams, to kind of bring a lot of those innovations forward, either in the

02:22.360 --> 02:26.160
form of, you know, hardware software code design, where we're maybe changing hardware

02:26.160 --> 02:32.840
and software together to address a new challenge, a new innovation in AI, say, Transformers.

02:32.840 --> 02:38.600
And this kind of thing that's a new, relatively newer technology. Or we're bringing techniques

02:38.600 --> 02:43.760
for optimizing networks, you know, they have a big focus on power optimization and optimality,

02:43.760 --> 02:49.280
bringing those techniques into the software stack so that practitioners, developers and

02:49.280 --> 02:54.080
practitioners in general can take advantage of those techniques in a way which doesn't

02:54.080 --> 02:59.360
require them to read a paper or write their own code or whatever. That's part of my

02:59.360 --> 03:04.960
job is to kind of bring that, you know, mass market kind of way to our end users.

03:04.960 --> 03:10.360
One of the things that surprised me recently in, you know, having the benefit of conversations

03:10.360 --> 03:16.800
with the folks on the research side and the folks on the, the enterprise side, the product

03:16.800 --> 03:22.600
side is how quickly things can move from one to the next. I'm thinking in particular

03:22.600 --> 03:31.120
of conversation that I had about using neural nets to, you know, on the research side,

03:31.120 --> 03:37.920
using neural nets to kind of tune radio parameters that was, that was with Joseph Soriaga.

03:37.920 --> 03:47.080
And it seemed like, you know, just a couple of months later, I was speaking with folks

03:47.080 --> 03:53.920
about that being, you know, kind of in the wild or starting to make its way into the hardware.

03:53.920 --> 03:59.920
Any, any perspective that you can share on that is has that kind of velocity shifted over

03:59.920 --> 04:05.400
the years that you've been at Qualcomm or are there things that are impacting it or changing

04:05.400 --> 04:06.400
it?

04:06.400 --> 04:09.880
I think it's an area, thank you for the compliment because sometimes we feel like we don't move

04:09.880 --> 04:16.480
fast enough. I think that it's an area we have focused on really kind of tightening, trying

04:16.480 --> 04:24.880
to tighten the, the innovation cycle from research to product. And I think it's just a pleasant

04:24.880 --> 04:30.960
product of us focusing on it and us really trying to be more strategic about this hardware

04:30.960 --> 04:37.120
software co-design, which is kind of foundational to bringing these innovations quickly. We,

04:37.120 --> 04:43.880
we've had this sort of virtuous three way, you know, cycle going for a couple of while

04:43.880 --> 04:48.800
we're probably into our seventh rate generation product now. And that really bears fruit because

04:48.800 --> 04:54.320
we have a much clearer perspective on the use cases that our customers want in driven

04:54.320 --> 04:59.120
by the innovation and research. And then when time, you know, ripens to put one of these

04:59.120 --> 05:03.920
innovations into practice, we have found that we're pretty well prepared to make that happen

05:03.920 --> 05:09.600
quickly, right? So it's that whole sort of internal ecosystem we've developed and we have

05:09.600 --> 05:14.960
focused deliberately on trying to increase the cadence by which we can take research and

05:14.960 --> 05:16.720
bring it to the marketplace.

05:16.720 --> 05:22.200
So let's talk a little bit about the, this idea that I mentioned earlier, kind of increasing

05:22.200 --> 05:31.720
model complexity and kind of decreasing device power. I guess, you know, some devices in any

05:31.720 --> 05:36.440
particular class of devices, they're getting more powerful, but the diversity of the types

05:36.440 --> 05:43.240
of devices that you're trying to run machine learning models on is increasing, including

05:43.240 --> 05:49.560
lower power devices. Can you just talk a little bit about what you're seeing out in the ecosystem?

05:49.560 --> 05:54.840
Yeah. So, so I think, you know, traditionally, if you kind of rewind a little bit, you know,

05:54.840 --> 06:01.880
a lot of focus on CNNs. And of course, in their heyday, CNNs were, you know, very demanding

06:01.880 --> 06:06.120
workloads, right, for the hardware and the software that we had at the time. And it's not like

06:06.120 --> 06:10.760
that demand has gone away, but as you know, the industries move towards higher complexity,

06:10.760 --> 06:17.000
CNNs, higher resolution inputs, transformers, which are, you know, very powerful, but are

06:17.000 --> 06:23.800
higher complexity in a lot of ways than CNNs. And the, we've gone from, from delivering AI

06:23.800 --> 06:30.200
into mobile devices, which is kind of where Qualcomm, you know, has its heart and soul into IoT

06:30.200 --> 06:35.880
devices, which are obviously really constrained. So think about home robot, think about a ring doorbell,

06:35.880 --> 06:41.800
this kind of thing. And markets like XR are particularly challenging because you have high

06:41.800 --> 06:47.560
complexity networks involving hand tracking or gestures or super resolving the displays and

06:47.560 --> 06:53.240
stuff, right, foviated views, that kind of thing. And that's a thing, that's a device you want

06:53.240 --> 06:59.240
to be compact, you want it to sit on your head, you can't, you know, make your face or head too hot.

06:59.240 --> 07:04.520
So there's power constraints, right? So there's a lot of concurrency going on, which drives complexity

07:04.520 --> 07:10.440
and their battery limits, right, and thermal limits. And so at the margins of these, you know,

07:10.440 --> 07:16.360
let's call them simpler devices. We, you know, we find an increasing challenge to take these

07:16.360 --> 07:22.440
complicated and, and concurrent workloads and pack them into these devices. And at the other

07:22.440 --> 07:27.800
extreme, you can think about an automobile is having similar characteristics, tons of cameras,

07:27.800 --> 07:34.600
tons of concurrent workloads. You think relatively a lot more power, but still there, thermal and

07:34.600 --> 07:39.720
power is a consideration. And of course, other factors like safety, right, did you have to also factor

07:39.720 --> 07:45.880
into your workloads? So kind of a explosion of diversity, where we're trying to build a common,

07:46.760 --> 07:52.360
we'll talk about this, I hope a common architecture that lets the practitioners move workloads

07:52.360 --> 07:58.040
one the other. So you think XR, but you know, in cabin camera and an automobile that's tracking your

07:58.040 --> 08:02.120
eyes and making sure you're not texting when you're socially looking out the windshield is not in

08:02.120 --> 08:08.200
some way so dissimilar from an sort of outside in XR workload, right? So you see kind of analogies

08:08.200 --> 08:15.640
across these markets for different kinds of applications, right? So we want to build a common

08:15.640 --> 08:21.560
kind of architecture that lets us do that. From the developer perspective is the common

08:21.560 --> 08:30.280
architecture in place because you see the same types of applications across one device to the

08:30.280 --> 08:36.360
next or, you know, one use case like XR to audio or is it not so much the same applications,

08:36.360 --> 08:41.640
but you want the developer to be able to reuse their skills in the different environment.

08:42.520 --> 08:48.840
It's a little bit of both. So we can talk about it. We recently announced the Qualcomm AI stack

08:49.320 --> 08:54.760
at the kind of heart of that kind of the center of it is this Qualcomm AI engine direct layer,

08:54.760 --> 09:01.480
which provides a API abstraction that spans across all of our chips from our cloud product all

09:01.480 --> 09:06.680
the way to our, you know, IoT products. And like I said, all the sort of products in the middle.

09:06.680 --> 09:13.480
And that is directly aimed at, you know, developers, practitioners. And I use that word broadly because

09:13.480 --> 09:20.120
that could mean a hands at OEM. It could mean an application developer and so on. And that is the

09:20.120 --> 09:26.280
contract really that we're building between the runtimes and the training frameworks to get

09:26.280 --> 09:32.280
on to Qualcomm silicon. So we're going to provide sort of out of the box as part of that our own

09:32.280 --> 09:37.560
Snapdragon nor processing SDK, which is our traditional runtime that we provide on our silicon.

09:37.560 --> 09:44.840
It's built on this common block. We're of course providing like TensorFlow Lite delegate plugins

09:44.840 --> 09:50.600
where we're going to Microsoft and Onyx runtime plugins. And that API will be open to anybody who

09:50.600 --> 09:58.040
wants to build sort of a direct to Qualcomm silicon, you know, metal runtime of their own,

09:58.040 --> 10:03.880
whether their runtime is just an application or whatever. Now we do also see, you know,

10:03.880 --> 10:10.440
application providers that want to provide basically the same application on like compute devices.

10:10.440 --> 10:15.960
So think Windows on Snapdragon, I think ARM powered Windows devices and be able to move that,

10:15.960 --> 10:22.760
let's say to like a handset device, okay. And they want to be able to have basically the same

10:22.760 --> 10:27.640
application, right. And so we're providing the same silicon acceleration and now we're providing

10:27.640 --> 10:33.960
the same software platform and they can very easily move their application if you will from cloud

10:34.520 --> 10:41.480
or from device to device onto the device. And we've demonstrated that with one of our leading

10:41.480 --> 10:47.800
partners and and shown how the same application can be sort of cloud powered or device powered.

10:47.800 --> 10:51.560
And then you can split the workload using the same, you know, abstraction.

10:51.560 --> 10:57.320
Can you kind of walk us through it? You know, as you're talking to these developers,

10:58.680 --> 11:04.120
yeah, what are kind of the real real world challenges that they run into when they're trying to

11:04.120 --> 11:11.640
take advantage of neural networks on device? I mean, we talk about some of them, you know, power and

11:12.200 --> 11:19.560
the kind of compute constraints of those devices, but kind of take us to the next level of detail.

11:19.560 --> 11:24.040
Like what really, what are the pain points that the developers run into?

11:24.040 --> 11:29.400
Yeah, I think I think one of the key ones is when we think about network optimality,

11:29.400 --> 11:34.280
you know, there's a lot of dimensions to it. There's the size of the network.

11:34.280 --> 11:38.120
And it can take storage. It can take time to download it. It's kind of, of course,

11:38.120 --> 11:41.880
size is directly relevant. Let's say to the number of parameters, the number of weights and so on.

11:42.760 --> 11:47.000
Power like you mentioned, right? You know, other factors, performance, total performance can be a

11:47.000 --> 11:51.960
factor of model size, right? Of course. So all these factors are really, let's put them in a bucket

11:51.960 --> 11:59.080
of like achieving network optimality, right? And we have demonstrated, in fact, we just finished

11:59.080 --> 12:04.840
a project with one of our biggest customers where we were able to demonstrate that even their

12:04.840 --> 12:11.720
most advanced use cases can be quantized. We can quantize the networks to 8 bits or 16 bits,

12:12.520 --> 12:18.040
whereas their internal kind of research had shown that they needed like floating point operations

12:18.040 --> 12:24.040
in order to achieve the accuracy that they were, you know, targeting. And through our advanced

12:24.040 --> 12:28.840
techniques, which are all part of the, you know, the software offering that we're developing,

12:29.640 --> 12:33.480
we were able to work with them and show them, hey, look, you can get to 8 bit on some of these

12:33.480 --> 12:38.520
networks. You can get to 16 bit fixed point on some of these networks. The result of that,

12:39.080 --> 12:44.920
and we're, we have a lot of research in 4 bit as well. The result of that is tremendous,

12:44.920 --> 12:50.920
you know, improvements in power, performance, the ability to run things concurrently,

12:50.920 --> 12:57.640
because it can run faster, right? These are all kind of, I think, the most, the biggest pain

12:57.640 --> 13:01.960
points, because what's happened is it used to be that CNNs were sort of over-paramortized,

13:01.960 --> 13:07.560
and so it was relatively simpler to compress them or relatively simpler to sparsify them or whatever.

13:07.560 --> 13:13.080
And as sort of complexity has gone up, and the concurrency we talked about has gone up,

13:14.600 --> 13:19.720
getting them all jammed into a device and getting them to work efficiently and concurrently at

13:19.720 --> 13:23.400
whatever frame rate you need them to get it, becomes harder and harder. And so this bridge,

13:23.400 --> 13:28.920
if you will, from ML ops, like what happens if you will in the cloud, in the laboratory,

13:28.920 --> 13:36.520
and a data scientist invents a new network architecture or a new, whatever, and they say, hey,

13:37.160 --> 13:43.000
now deploy it, the deployment part turns out to be really pretty hard. And now you have this sort

13:43.000 --> 13:47.960
of impedance mismatch between that data scientist who says, hey, I've got this perfect model,

13:47.960 --> 13:52.280
and it solves a problem perfectly. Why can't you make it run on the device? That's really,

13:52.280 --> 13:57.160
I would say, one of the key pain points right now, and we're working very aggressively to address

13:57.160 --> 14:04.600
that and bringing tools that can allow more easily take that model out of the data scientist's

14:04.600 --> 14:11.640
hands and bring it to the device in a way that does not compromise the integrity, the basic design

14:11.640 --> 14:20.280
of that of that model. And are you, do those tools apply only to kind of custom models that

14:22.360 --> 14:27.880
ML engineer might be creating, or do they also apply to kind of the lighter weight models

14:27.880 --> 14:32.520
that someone might download off of hugging face or something like that? Sure. So hugging faces

14:32.520 --> 14:37.960
is a particular example where again, we can touch on Transformers. Quantizing Transformers is a

14:37.960 --> 14:43.080
bit of a new art, which we have quite a bit of research on. And you will see that kind of flow

14:43.080 --> 14:49.000
into our tools. But if we take the more general part of your question, no, these tools apply

14:49.000 --> 14:53.640
to all call it run of the mill, you know, models, resnets and those kinds of things that people

14:53.640 --> 15:01.240
are maybe familiar with, as well as these custom ones, right? And so we're working very hard to

15:01.240 --> 15:06.760
make sure that if you've got a model in TensorFlow or PyTorch, whether it's a stock resnet or you

15:06.760 --> 15:11.160
pull it off of GitHub or whether it's, you know, a proprietary model, that you can run it through

15:11.160 --> 15:17.720
this workflow and you can get, you know, consistent results to bring that to, in our case, Qualcomm,

15:17.720 --> 15:22.920
you know, Silicon and Qualcomm software. We think that's one of the challenges and I think it's

15:22.920 --> 15:30.360
evidenced by the fact that there are, you know, new companies surfacing all the time to deal with

15:30.360 --> 15:36.120
ML ops issues. And ML ops has its own set of complexities, managing the data, managing the

15:36.120 --> 15:40.520
pipeline, scaling up training. I mean, there's a lot of challenges there, right? We don't want to

15:40.520 --> 15:47.000
be one more challenge in the pipeline. We want to go, okay, you solve your ML ops problem or your

15:47.000 --> 15:51.480
training thing. That's fine. We don't want to be the, oh, but now it doesn't work on device. We're

15:51.480 --> 15:55.240
working really hard to make that transition, you know, as smooth as possible. So can you talk a

15:55.240 --> 16:01.320
little bit about the workflow that you just mentioned, kind of what are the steps or tools that

16:01.320 --> 16:07.320
you're providing that enable folks to do this? Yeah. So I think you've had some of our, some of my

16:07.320 --> 16:11.800
other colleagues talk about like the AI model efficiency toolkit. That's kind of at the center

16:11.800 --> 16:17.880
of this, right? So, so think about the workflow being like this. There are kind of maybe two aspects

16:17.880 --> 16:23.640
the model efficiency toolkit. One is an aspect that that pieces of it can plug directly into your

16:23.640 --> 16:28.520
training pipeline. So think quantization where training is a good example. So this is the process

16:28.520 --> 16:36.920
where during the training cycle itself in your training loop, you are conditioning the network to

16:36.920 --> 16:41.000
understand what the effects of quantization are going to be on that particular network. So it

16:41.000 --> 16:48.280
learns the noise, if you will, that gets introduced during that quantization process instead of waiting

16:48.280 --> 16:55.160
until you get all the way to the deployment phase and you now realize, hey, this model isn't very

16:55.160 --> 17:00.520
resilient to quantization, right? So some models really require quantization or training and we've

17:00.520 --> 17:06.200
got tools that plug into PyTorch and plug into TensorFlow, add a little bit of extra to your

17:06.200 --> 17:12.520
training pipeline and then condition the model during training to be quantization aware.

17:12.520 --> 17:17.000
Now, there's a whole category of models where you don't need that sophisticated

17:17.640 --> 17:23.240
approach. You can do what we call post training quantization, okay? So you've trained the model in

17:23.240 --> 17:29.720
your favorite framework. We've got tools that will read that native file format.

17:30.360 --> 17:35.000
We run it through a series of post training techniques. Again, a lot of this we have done

17:35.000 --> 17:41.480
quite a bit of innovation on and studying how do you do this in a way that preserves the integrity

17:41.480 --> 17:47.880
of the model and also reduces the precision which reduces the size, you know, reduces power

17:47.880 --> 17:52.840
consumption and so on, which are objectives. So you would run it through post training quantization.

17:52.840 --> 18:00.520
This produces a representation of the model in this Qualcomm AI engine direct format. Think

18:00.520 --> 18:07.160
about it like an intermediate representation. So it's a common representation. Any graph that comes

18:07.160 --> 18:12.520
from PyTorch or TensorFlow will be quantized and then reduced into this intermediate representation.

18:12.520 --> 18:18.040
From that intermediate representation, you can then as a practitioner, direct where you want

18:18.040 --> 18:23.160
that model to run. Do you want it to run in our AI hexagon accelerator? Do you want it to run

18:23.160 --> 18:28.600
on the GPU? Do you want it to run on the CPU? Once we have it in that common representation,

18:28.600 --> 18:32.920
you can then run it wherever you want. And a lot of our customers will go, oh, I want to run this

18:32.920 --> 18:38.120
one on the GPU and that one on the accelerator and so on. That one on the low power sub-system and

18:38.120 --> 18:45.560
so on. That's kind of the basic workflow. And then if your deployment is, you know, a smartphone,

18:45.560 --> 18:52.360
you can run the AI accelerator. If your deployment is a cloud device or a heavy-edged device,

18:52.360 --> 18:56.440
it's using our cloud processor, you can take the same representation and run it there.

18:57.240 --> 19:01.160
And that's a multi-core device. So you have a lot of performance so you can run there. So that's

19:01.160 --> 19:07.880
where this idea of having a common representation across our silicon ties in with that sort of

19:07.880 --> 19:13.000
developed workflow that a day of scientists might pursue. You've also been doing work in network

19:13.000 --> 19:17.880
architecture. So how does that plan to this workflow? You know, network architecture, again,

19:17.880 --> 19:23.880
is sort of an evolving technology, right? There was a big rush and Google was an early innovator

19:23.880 --> 19:29.000
and we're happy to be partnered with Google on our offering. Now, the way to think about it is,

19:29.000 --> 19:37.640
I put it in this sort of very advanced category, right? But again, it's one of the tricks when

19:37.640 --> 19:44.600
one of the tools that we're offering to our customers to bridge that gap from what a data

19:44.600 --> 19:50.760
scientist might want to do sort of in the laboratory, if you will, when they're designing a network

19:50.760 --> 19:57.800
and tying the design of our network to hardware-specific characteristics. So that's kind of one

19:57.800 --> 20:04.840
attribute. We're working with Google to make sure that Vertex NAS understands Snapdragon,

20:04.840 --> 20:09.080
if you will, that's the way to put it. So that the network architectures that are produced

20:09.080 --> 20:15.640
from neural architecture search are sort of inherently hardware aware. So they're inherently

20:15.640 --> 20:24.040
optimized for Snapdragon, okay? And so by tying those two things together, again, we are providing

20:24.040 --> 20:32.680
a way for the data scientists to produce a result that is sort of prepared to run in one of these,

20:32.680 --> 20:37.240
say, power constrained environments. And we've had good success with some XR and automotive

20:37.240 --> 20:44.600
workloads where we're seeing the, we're seeing NAS produce in a much quicker way than a data scientist

20:44.600 --> 20:50.920
could iterate on their own innovative network architectures that are tuned for Snapdragon

20:50.920 --> 20:55.560
that reduce the latency, increase the frame rate, reduce the network size, all, you know,

20:55.560 --> 21:00.440
characteristics that we're trying to achieve when we're helping like an XR customer or an automotive

21:00.440 --> 21:06.360
customer, you know, get a very demanding set of workloads onto, you know, our platforms.

21:06.360 --> 21:12.120
So it's another tool there, but I think that for the listeners, the key takeaway is this linking

21:12.920 --> 21:18.040
the speed at which you can search a lot of, you know, run a lot of experiments, if you will,

21:18.040 --> 21:23.560
automatically, and linking it to hardware awareness so that the final output

21:23.560 --> 21:30.120
is preconditioned to do well on our silicon. You talked about frame rate, are the primary

21:30.120 --> 21:35.720
workloads that these are being used for vision workloads? Yeah, so I mean, I think of that because

21:35.720 --> 21:39.240
that's the predominant workload. Of course, if this were like natural language processing,

21:39.240 --> 21:43.560
you know, you might say like how fast can it, you know, parse a sentence or whatever,

21:43.560 --> 21:48.840
right? So that has to do more with like, you know, word length, right? Or per length. But typically,

21:48.840 --> 21:55.480
you know, so many of the applications which are demanding are, you know, we can go across the

21:55.480 --> 22:00.440
whole spectrum. So of course, in mobile, it's typically, you know, low light video, let's say,

22:00.440 --> 22:05.960
is a pretty demanding workload, super resolution, right? We've been to, we've shown some super

22:05.960 --> 22:11.880
resolution in games, right? So I'm doing gaming, and I want to super resolve, you know, the action

22:11.880 --> 22:20.520
scene in the game to increase the, you know, quality and the experience XR. So maybe two kinds

22:20.520 --> 22:25.720
of workloads, not only the rendering part, right? And the sort of you're, you're doing

22:26.520 --> 22:31.240
some kind of augmentation or whatever. And also things like hand gesture, right? Which again,

22:31.240 --> 22:35.720
we talk about frame rate there because you want to sample it often enough that it's a smooth

22:35.720 --> 22:41.400
experience, right? That you don't miss gestures, you don't miss motion, right? So we oftentimes use

22:41.400 --> 22:46.440
frame rate a little loosely, meaning kind of sampling interval, right? For, for an experience.

22:46.440 --> 22:51.080
And like we said, the car, it's looking down the road, you don't want it to look down the road,

22:51.080 --> 22:55.880
you know, once a second, you want it to look down the road, you know, 30 times a second or 25 times

22:55.880 --> 23:04.520
a second, right? Do you have a sense for when, when folks know that they need to start exploring

23:04.520 --> 23:08.760
network architecture search? Like what are, what's the wall that they're bumping up against that?

23:08.760 --> 23:16.360
Oh, if you're experiencing that, and maybe now you need to go to this advanced level.

23:16.360 --> 23:21.720
Yeah, so that's kind of a complicated surface. You know, we didn't talk about, we didn't talk

23:21.720 --> 23:27.400
about, for example, mixed precision, right? Is another kind of technique of like you don't have

23:27.400 --> 23:31.640
to just do stuff in one precision, you can mix different layers and different precision,

23:31.640 --> 23:36.680
and we support that. You know, I think there's a couple drivers, the idea that

23:36.680 --> 23:39.640
that you can sort of algorithmically

23:42.040 --> 23:48.360
imbue this algorithm with an awareness of the hardware is something which is hard to sort of

23:48.360 --> 23:53.480
learn at scale. And in some cases, we don't want to tell, you know, all of our customers,

23:53.480 --> 23:57.720
all of our secrets, right? So there's a little bit of that going on. But it's also hard to learn

23:58.840 --> 24:03.960
a lot of the subtlety that something like a NAS system can learn on its own and can leverage.

24:03.960 --> 24:08.760
The other thing too is just raw experimentation. I mean, if NAS can produce a thousand

24:08.760 --> 24:15.000
candidate models in some unit time, how long would it take a data scientist to think out the same

24:15.000 --> 24:20.360
thousand experiments, right? And so some of it's just kind of a brute force, like search

24:21.000 --> 24:27.320
the solution space in an automated way, but it's directed, right? It's got considerations for

24:27.320 --> 24:34.280
hardware, it's got considerations for size, and so on, kind of baked into it. So it's a nice tool to

24:34.280 --> 24:39.800
increase the rate at which a data scientist can explore different approaches. So it sounds like

24:40.760 --> 24:50.600
given the relative cost and complexity of using it, there's a space of high value problems where

24:51.480 --> 24:56.600
they're close enough that they think that there's a solution, but not quite far enough that they're

24:56.600 --> 25:01.720
at the solution that, you know, this is something to try. Yeah, you can think about it kind of maybe

25:01.720 --> 25:06.920
in two areas. You can think about it like if you have kind of no idea how to solve a problem,

25:06.920 --> 25:12.680
right? Your search space is very large. This might be a way to kind of get to an idea, you know,

25:12.680 --> 25:18.360
automatically through basically, you know, wrote experimentation to solve your problem.

25:18.360 --> 25:23.560
The experience we've done so far have focused more on, I've got a network kind of to your point.

25:23.560 --> 25:28.600
I've got a network, and you know, if I apply this technique, how much better can I get it, right?

25:28.600 --> 25:34.360
And sometimes you go, oh, I saved, you know, 15% in latency or something, right? Which can translate

25:34.360 --> 25:38.760
into power or frame rate, depending on what you're trying to, you know, what effect you want.

25:39.320 --> 25:43.480
And that could mean, oh, I could run one more network with the same unit time because this

25:43.480 --> 25:48.520
one's going to finish sooner, right? So, you know, these projects, because they have so many

25:48.520 --> 25:53.720
networks and there's so much complexity are really sort of a puzzle fitting experience. You can't

25:53.720 --> 25:58.840
generally, at priori say, oh, this network absolutely has to run in such a amount of time,

25:58.840 --> 26:03.080
or use so much power because you're looking at a collection of networks that are together

26:03.080 --> 26:07.960
are solving a problem, right? Again, something people might, your audience might be experienced with

26:07.960 --> 26:12.360
this is, you know, virtual reality, you know, metaverse XR, whatever term you want.

26:13.720 --> 26:17.400
They realize, oh, it's following my hands and it's rendering the screen and they don't really

26:17.400 --> 26:22.920
think about all the things it's doing. But if it doesn't do all those things fast enough together,

26:22.920 --> 26:28.760
you just don't have that immersive experience. Right, right, right. You mentioned mixed precision.

26:29.880 --> 26:36.840
Can we dig into that a little bit more? Where is mixed precision now on kind of the adoption

26:36.840 --> 26:43.160
cycle? Is it being broadly used, do you think, or folks just starting to explore it?

26:43.160 --> 26:49.240
So I would say that there's been a period of exploration, say, maybe the last product cycle,

26:50.600 --> 26:56.520
where we enabled it in our products. But again, a little bit sort of to the nastery,

26:56.520 --> 27:02.440
there weren't good tools to sort of automate it. So it was a way to address point problems.

27:02.440 --> 27:07.640
Oh, this layer is having a problem quantizing, or we need more output resolution here. So we're

27:07.640 --> 27:13.160
going to use a higher precision layer that produces higher resolution activations or something.

27:13.800 --> 27:20.200
Now we're introducing automatic mixed precision. So again, bridging that the friction points

27:20.200 --> 27:26.360
from taking a model that works great on a bench somewhere for a data scientist and getting it

27:26.360 --> 27:31.320
onto device. Sometimes, again, thinking about post-training, quantization techniques,

27:31.320 --> 27:38.040
this would be one of them. I'm not really retraining the model. I'm looking, I'm analyzing the model

27:38.040 --> 27:43.880
and saying, oh, look, I can see that layer 42. That's the universal number of everything, right?

27:43.880 --> 27:52.200
Layer 42. That's the one that's causing the trouble if it's in say 8-bit integer precision.

27:52.200 --> 27:59.560
Let me move it to integer 16. And boom, my quantization problem goes away, right? That's the source

27:59.560 --> 28:05.880
of my accuracy. So again, providing tools for practitioners to automate that so that it's more

28:05.880 --> 28:12.120
push button. That's the road we're on. And well, those features are arriving now in our tool chain.

28:12.120 --> 28:19.320
What are the features that you are expecting to see looking forward? So again, I think we're early

28:19.320 --> 28:28.840
in NAS. I think we're still somewhat early in sort of quantization of transformers and all their

28:28.840 --> 28:34.520
variants, right? So we're seeing kind of an emergence of vision transformers. We're seeing

28:34.520 --> 28:39.880
transformers for like multi-modal kinds of applications, right? And so on. I would say we're,

28:39.880 --> 28:43.160
you know, we've done a bunch of research there, but I would say that that's an emerging

28:44.360 --> 28:48.760
area where we will see, you know, practitioners moving towards

28:49.400 --> 28:54.360
want to apply those and facing some of the same hurdles that they're facing now on sort of

28:54.360 --> 29:03.160
traditional architectures. And I would also say that, you know, there's a lot of smaller friction

29:03.160 --> 29:11.160
points in moving from, you know, the data scientists part of the ecosystem to the, you know,

29:11.160 --> 29:19.880
final deployed product. And so we can think about streamlining like the model conversion steps

29:19.880 --> 29:28.360
better. Int 4 is a really exciting, even more exotic data format. You know, we recently announced

29:28.360 --> 29:34.040
some discussions around FP8, which is another data format. So there's a lot of things coming with

29:34.040 --> 29:39.560
the continued, you know, challenges as, you know, like we started off this discussion. I don't see

29:39.560 --> 29:46.360
the challenge of increasing model complexity faced with, you know, struggles to reach optimality.

29:46.360 --> 29:51.640
In various forms, data format is away, NAS is away there, right, the tools we're providing. These

29:51.640 --> 29:59.800
are all roads to the same goal, which is more, you know, more complexity in, you know, packages that

29:59.800 --> 30:04.600
will also get more capable, but there'll be limits, right, that will always face kind of a

30:04.600 --> 30:09.400
innovation challenge. So I would say the tooling is really going to be directed at continuing to

30:09.400 --> 30:15.880
make that easier for practitioners. And at the same time, continuing in the direction of things

30:15.880 --> 30:21.560
like more NAS, lower precision and so on, right, which will pose our own challenges going forward.

30:21.560 --> 30:28.840
And do you see the tools themselves getting easier to use or are they based on what I've seen

30:28.840 --> 30:33.160
thus far? They're pretty low level tools. How did those, how did those evolve?

30:33.160 --> 30:38.600
Yeah, so I think, so the way they're evolving is we're not ready sort of sort of fully

30:38.600 --> 30:44.680
announced stuff, but you can think about we will also be introducing a graphically oriented tool.

30:44.680 --> 30:51.160
So again, moving from the, I'll call it advanced practitioner who maybe is part data scientists

30:51.160 --> 30:56.440
and part deployment engineer, right, to where, again, we want to make this simpler and simpler

30:56.440 --> 31:03.560
and more and more mass market. And so yes, we've started with, you know, with command line tools

31:03.560 --> 31:09.080
and sophisticated, you know, profilers and debuggers. What you will see from Qualcomm is,

31:09.080 --> 31:15.240
I think, increasingly steady set of announcements around, oh, now we've got a, you know,

31:15.240 --> 31:20.200
profiler or visual profiler for this. Oh, now we have a way to do, you know, network

31:20.200 --> 31:26.040
inspection. And so the way I think for the audience, I think about it is like source code debugging,

31:26.040 --> 31:30.280
if people, you know, still write code and they remember what that's like, you know, you want to,

31:30.280 --> 31:35.080
you want to write your code and you do not want to see it your debugger in a semi language.

31:35.080 --> 31:40.360
Like you don't want to see what the compiler turned your code into. You want to debug your code

31:40.360 --> 31:46.360
in the context that you wrote it that you understand it. And so the analogy, the way I think about

31:46.360 --> 31:51.880
it is the same here. A data scientist designed a network and they visualize it like a set of

31:51.880 --> 32:00.120
connected nodes. They don't want any, I think, hardware or compiler company to go, oh, now you have

32:00.120 --> 32:08.840
to debug your network that you, you know, conceptualize as a graph in a semi language. And so we're

32:08.840 --> 32:14.440
going to do the same sort of thing. We're going to provide debuggers that put that performance and

32:14.440 --> 32:20.920
data that the developer needs to understand how their network is performing on our silicon in

32:20.920 --> 32:26.600
the context of the original graph as it arrived, you know, at the start of that pipeline I described,

32:26.600 --> 32:31.240
right? And that's really the link we're trying to build is you think as a data scientist about

32:31.240 --> 32:36.920
your network and TensorFlow, and we want to show it to you on the device. Sure, with device-specific

32:36.920 --> 32:42.600
performance metrics and accuracy metrics, but in the context of this graph, the way you, the way

32:42.600 --> 32:47.640
you designed it, right? Not in some low level kind of, you know, a semi language, if you will,

32:47.640 --> 32:53.800
right? And that's what you'll see is more and more visually oriented tools that make the,

32:53.800 --> 33:00.520
you know, ML ops part of the equation and the deployment part a lot more equitable.

33:00.520 --> 33:06.120
When you talk about the relationship between kind of the higher level tools and programming

33:06.120 --> 33:11.480
languages that a data scientist or developer might want to use in the lower level hardware,

33:12.120 --> 33:18.120
it calls to mind comparisons to things like CUDA. Is that, you know, how should we think about the,

33:18.120 --> 33:25.640
what you're offering with SDKs relative to what someone might be familiar with in the GPU world?

33:25.640 --> 33:32.280
Yeah, so I think, I think, you know, I don't really want to do a bake off, but I think the analogy

33:32.280 --> 33:36.760
runs like this. You could think of CUDA and CUDA is a lot of things, right? It's a parallel

33:36.760 --> 33:41.800
programming language, it's a, you know, and so on. But you could think about our Qualcomm

33:41.800 --> 33:49.480
Engine Direct as being sort of Qualcomm's answer to CUDA in that it provides a common way

33:49.480 --> 33:54.760
for practitioners to either use existing runtimes, right, to deploy their solutions,

33:54.760 --> 33:59.240
or if they're advanced, and that's what their application demands, they can sort of program

33:59.240 --> 34:05.080
directly. So the analogy is Nvidia provides UDNN, which sits on top of CUDA, right? That's an

34:05.080 --> 34:10.120
example of a library. You know, our stack provides very similar kinds of capabilities.

34:10.120 --> 34:15.480
If you want to roll up your sleeves, you want to write a CUDA kernel for the GPU. Of course,

34:15.480 --> 34:21.560
there's a way to do that. The analogy is, if you want to write a custom application or a custom

34:22.120 --> 34:29.320
operation or layer for the Qualcomm AI stack, we provide a way to plug that into the Qualcomm

34:29.320 --> 34:35.000
Engine Direct. We have an API for that. We provide a debugger, we provide a compiler and tools

34:35.000 --> 34:42.680
for hexagon. We provide the same for our mobile GPU. So it's a very analogous offering. And then

34:43.400 --> 34:49.320
one thing which a lot of people like are these visual consoles that Nvidia provides that sit

34:49.320 --> 34:56.600
on top of their GPUs. We have offered that for like our gaming customers for a long time.

34:56.600 --> 35:01.960
We are expanding that kind of offering to the AI portfolio and really to provide

35:01.960 --> 35:07.880
the debug ability of Snapdragon as a whole. So you will see us working to integrate

35:09.400 --> 35:14.680
our debug tools, not just in the AI space and there will be AI-specific tooling, but more generally

35:15.240 --> 35:20.040
as Snapdragon is a development platform, you will see us kind of expand that offering.

35:21.320 --> 35:27.960
So to sort of maybe put a bow on it, I think at the library level and at the sort of programming

35:27.960 --> 35:35.880
and lower levels, we have a one-for-one corresponding kind of offering to Nvidia. And then you will

35:35.880 --> 35:42.280
see us kind of put the top of the story onto that offering in the next period. Over the years,

35:42.280 --> 35:49.320
we've talked a lot about the ecosystem and the way your tools support and the way you work with

35:49.320 --> 35:56.440
other ecosystem partners. You've already mentioned things like Onyx and PyTorch TensorFlow.

35:56.440 --> 36:04.360
Can you give us an update on where the ecosystem priorities are and any new and exciting updates?

36:04.360 --> 36:07.960
So yeah, I can maybe tease a little bit without getting into too many details.

36:07.960 --> 36:15.160
So you know the way I think about it is this, we continue and I think the Qualcomm Edge and

36:15.160 --> 36:25.160
Direct is a good concrete example of where we are trying to provide an abstraction of API surface

36:25.160 --> 36:33.480
onto which all these runtimes can plug in. So we want maximum access to our silicon and our

36:33.480 --> 36:39.160
software. We don't just leave it to them. We've invested working with Google on TensorFlow

36:39.160 --> 36:43.320
delegates working with Microsoft on Onyx runtime. So these are ways in which we're partnering with

36:43.320 --> 36:48.120
the broader ecosystem. Some of the work we do, some of the work they do, we share the work.

36:48.120 --> 36:55.880
An exciting thing to think about is the Cloud to Edge story. The idea that you could

36:57.000 --> 37:05.880
have like a Snapdragon Windows compute device and maybe also an XR device that run very

37:05.880 --> 37:11.240
similar applications or share and experience. These kinds of multi-device

37:11.240 --> 37:20.040
experiences are I think only really possible when you've got this common abstraction that we're

37:20.040 --> 37:25.480
building and you've got common runtimes. So that's why it's so important for us to work with

37:26.040 --> 37:32.440
like the Microsoft of the world and the Google of the world so that if you were Adobe or something

37:32.440 --> 37:37.080
and you're building Photoshop and you want a Photoshop experience on an Android device and you

37:37.080 --> 37:45.400
want a Photoshop experience on a Snapdragon ARM powered PC device, you don't have to write

37:45.400 --> 37:49.640
complete different pieces of software because underneath it the silicon is Qualcomm silicon.

37:50.440 --> 37:54.440
And so we want to kind of make that easier. So that's an example of the kind of thing we have in mind.

37:55.000 --> 38:00.600
And we're partnering with the operating system providers if you will, the main drivers of

38:00.600 --> 38:08.520
the broader ecosystem, but also like these ISVs, right? So as our portfolio of use cases expands,

38:08.520 --> 38:14.600
then the partners aren't just our traditional handset OEMs, they become Microsoft, Adobe,

38:14.600 --> 38:19.400
you know, these other kind of companies that have really compelling applications that they can bring

38:20.360 --> 38:25.960
to an assortment of Qualcomm devices. So that's kind of where I think we will see our ecosystem

38:25.960 --> 38:31.480
efforts expand. We've touched on throughout the conversation, some of the work happening in

38:31.480 --> 38:38.360
automotive as kind of this, you know, touchstone for, you know, a set of use cases that are kind of

38:38.360 --> 38:48.120
pushing the, pushing the edge. Can you talk a little bit about what you're seeing in automotive

38:48.120 --> 38:53.800
nowadays and kind of what's new and interesting in that world? Yeah, so new and interesting is

38:53.800 --> 39:00.920
automated cars are coming, but they're not here yet, right? So, you know, we have moved from,

39:00.920 --> 39:06.600
you know, connectivity, which is our traditional business. So this is like, you know,

39:08.360 --> 39:12.600
your car is connected to the internet so that you can, you know, get emergency services and stuff,

39:12.600 --> 39:19.080
to digital cockpit, right? So think about in cabin, right? And so we have very strong offerings there,

39:19.080 --> 39:24.280
and we talked a little bit about like gaze detection, you know, cameras inside the cockpit that

39:24.280 --> 39:30.200
are monitoring the driver as part of a safety protocol. Eight as, we recently want to

39:30.200 --> 39:36.040
a bid with with with BMW, for example, you know, to provide BMW, you know, eight as solutions.

39:36.040 --> 39:44.040
Think about this is, this is, you know, semi-autonomous safety oriented systems, right?

39:44.040 --> 39:49.640
This is lane to very advanced lane departure warning, lane keeping,

39:51.320 --> 39:57.880
automatic parking, backup. I mean, everything, everything related to making the automobile

39:57.880 --> 40:03.640
safer and more automated experience, right? And we're on this sort of trajectory towards

40:03.640 --> 40:07.720
sort of full automation, but before we get there, there's a lot of problems as all. So

40:07.720 --> 40:15.720
we want the car to be surrounded by cameras. So we're talking 10 cameras, 15 cameras, right?

40:15.720 --> 40:21.640
Completely surrounding. We're talking about immersive in cabin experiences. So

40:23.160 --> 40:31.080
displays, right? Perseete displays, perseete audio experiences, right? We think, oh,

40:31.080 --> 40:35.320
well, a car doesn't have to have NLP, but you could talk to your car. It could talk back to you,

40:35.320 --> 40:40.120
right? I mean, there's just layers and layers and layers of AI in an automated vehicle,

40:40.120 --> 40:46.120
both for the comfort of the passengers and the safety of the passengers. And so we're looking at

40:46.680 --> 40:49.800
all of those kinds of problems. And the most challenging ones are

40:51.640 --> 41:01.400
safety critical time critical kinds of problems. So viewing the car in the context of all the

41:01.400 --> 41:08.840
other cars on the road, very precise lane positioning for, you know, for lane keeping and so on.

41:09.960 --> 41:16.840
But things like, things like drive policy, like I want to tell the car, I want to go from here to

41:16.840 --> 41:22.200
there and I want to know it needs to know where I'm going, but it needs to be aware of other cars

41:22.200 --> 41:26.600
making lane changes. It needs to plan lane changes. It needs to do that in a safe and predictable

41:26.600 --> 41:33.400
manner, right? These are all very intensive, basically computer vision, you know, AI power

41:33.400 --> 41:39.880
computer vision problems that have to be completely seamless, you know, to the driver and are

41:39.880 --> 41:44.680
very demanding from a predictability point of view, you cannot be like, oh, I'm late, you know,

41:44.680 --> 41:48.840
with that frame, like that is could be a problem. And you have to, of course, design all that

41:48.840 --> 41:55.400
software in a safety critical kind of way. So that's really the challenge. And to come sort of full

41:55.400 --> 42:01.320
circle on the discussion, there's a lot of compute power that our platform brings to the automobile.

42:02.120 --> 42:07.800
But it's proportional to the demand of the workload, right? So the challenges that we see in XR,

42:08.360 --> 42:15.960
we see sort of analogous challenges in, in a vehicle, because the number of cameras is higher

42:15.960 --> 42:20.840
or the resolution is higher or the modalities are different, like a mix of LiDAR and radar and

42:20.840 --> 42:26.520
camera all working together to solve a problem. This introduces, you know, multimodal kinds of

42:26.520 --> 42:32.040
challenges. So there's a lot of very exciting challenges. And I think you'll see Qualcomm continue

42:32.040 --> 42:36.840
to, I know you will see Qualcomm continue to make strides on a mode of market. It's a very

42:36.840 --> 42:43.320
important market for us. Again, we do fundamental research there that drives our innovation. And

42:43.320 --> 42:49.800
it's a similar kind of cycle that we talked about with Qualcomm AI research. We do fundamental

42:49.800 --> 42:54.920
research in these automotive networks. And that drives, you know, our product line.

42:56.360 --> 43:03.880
Do you take a strong position on vision first type of automotive experience? Or do you

43:05.640 --> 43:11.880
also support and work with kind of these with other sensors and kind of a sensor fusion type of

43:11.880 --> 43:19.400
environment? Yeah. So we we're very agnostic. We do what our customers want, right? And so we're

43:19.400 --> 43:23.560
looking, like I said, it kind of three, the three primary modalities camera, which I think is sort

43:23.560 --> 43:30.200
of camera first in a lot of ways. I think pretty much for everybody. And then radar and LiDAR.

43:30.200 --> 43:35.160
So we look at sort of all, I'll call them the three major modalities, four sensors.

43:35.160 --> 43:44.920
Awesome. Any final thoughts, you know, as we close out, you know, what do you think we'll be talking

43:44.920 --> 43:50.440
about next year when we get together? Well, I hope, I hope, you know, thinking about your sort of

43:50.440 --> 43:56.200
practitioner, you know, audience, I hope, let's put this way, I am planning that we will be able to

43:56.200 --> 44:01.640
talk and a lot more concretely about let's say the tools that we talked about, right? These sort of

44:01.640 --> 44:07.000
visually oriented tools. We hope to be able to show them publicly by then. That's our plan.

44:08.600 --> 44:13.880
And I would like to talk a lot more about automotive probably. We will be another year into that,

44:13.880 --> 44:20.600
you know, evolution and hopefully, you know, of course, talk about a few more customers concretely,

44:20.600 --> 44:25.880
but we expect to, you know, win more business and make more advances there. I say that because I

44:25.880 --> 44:32.120
feel like it's perhaps one of the most compelling and demanding markets, you know, for AI,

44:32.120 --> 44:36.200
generally speaking, right? I think you can't build one of these safe vehicles without AI. I don't

44:36.200 --> 44:42.120
think there'll be a lot of debate about that statement. And I really hope we will be able to showcase

44:42.120 --> 44:48.760
more of these multi device use cases, right? We're working pretty hard on that. And I think we will

44:48.760 --> 44:55.320
see in the next year, the fruits of that come to bear like in a real kind of context that I can

44:55.320 --> 45:00.360
talk about publicly, but just know that a lot of work in that direction. And we're excited about

45:01.560 --> 45:06.760
we're excited about Windows on Snapdragon and, and, you know, putting that power of, you know,

45:06.760 --> 45:11.800
an all day device and people's hands that can also do AI, right? Because we take for granted

45:11.800 --> 45:16.280
background blur and these other things, but they can be even better with AI, right? And they can

45:16.280 --> 45:22.280
be done in a way that does not compromise battery life and, you know, makes the, the experience that

45:22.280 --> 45:28.520
we all, you know, live with every day a lot more, a lot more seamless. So I hope that these are

45:29.160 --> 45:32.680
a forecast of some of the things we'll talk about that would be more publicly, you know,

45:32.680 --> 45:38.840
announced in the next, you know, period. Awesome. Awesome. Well, Jeff, it's always a pleasure to

45:38.840 --> 45:43.720
reconnect with you. Thanks so much for joining us. I appreciate it. And you have a good rest of your day.

45:43.720 --> 46:11.720
I appreciate it. Thank you.

