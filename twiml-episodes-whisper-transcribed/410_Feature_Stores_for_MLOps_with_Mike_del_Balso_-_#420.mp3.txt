All right, everyone. I am here with Mike DelBalso. Mike is the co-founder and CEO of Techton.
Mike, welcome to the Twomo AI podcast or I should say welcome back to the Twomo AI podcast.
Yeah, thank you. Happy to be back after a couple of years now.
A couple of years. So we recorded our first interview in March of 2018. So
it's like two and a half years. It was episode number 115 and we just published four
17. So 300 episodes ago. Wow. I'm not even considering 2020 years which make it seem way
longer. Yeah, that's like three decades ago. That's nuts. Well, I'm happy to be back and
congrats on having what 415, 417. Wow, that's a that's quite an accomplishment.
Yeah, happy to be back. Thanks for having me. Absolutely, absolutely. And you've been busy
yourself. When we first spoke, you were at Uber and we were talking about Michelangelo, the platform
that you built there to help scale and operationalize machine learning and you left Uber to help
other folks do that. When did you how long has it been? Yeah, so beginning of 2019 right at the
end of 2018, that's when myself and some of the other folks that helped build the Michelangelo
system, we kind of split off to help other folks solve similar problems. And yeah, so companies
called Techton, we build an enterprise feature store for machine learning and happy to tell folks
about that today. But yeah, that started. So we've been doing that almost two years into that.
Wow, wow. I'm curious about one thing I'm curious about is, you know, Michelangelo kind of
encompassed a ton of features not to overload that term. And you could have done a lot of things
in space, but you chose to focus on the kind of feature store part of it. Maybe share a little bit
of that to help contextualize the way you think about the overall problem of operationalizing machine
learning. Yeah, for sure. Well, so I've been doing this a long time now. And before building this
stuff at Uber, I actually was at Google and I worked on the machine learning systems that power
that adds auction at Google. And they had been doing this for many years and they had really great
systems to power these core processes that run Google's business, right, determining what you
had to show. And it's very financially important and super highly productionized. And you know,
at Uber, we were starting from, you know, not zero, but we just had a handful of models and
production. It was kind of early days for machine learning at that time. And we went through this
journey at Uber over a period of like two and a half, three years where we brought in the right
tooling and we really unlocked the ability for the data science and the analytics teams to really
build machine learning systems and deploy them in production. And so during that time, I was,
you know, thinking about, hey, what is, you know, how does this look when it's done right? And
I was thinking a lot about my time at Google and with these really amazing large models updated
all the time, you know, really, we didn't use the term ML ops at that time. But things were highly
productionized and had a very DevOps and modern ML ops feel. And so, you know, we had to build the
whole stack at Uber. We started that 20 in 2015. And there was not a lot of good ML infrastructure
and all tools at that time. And going through that journey, I had the chance to really kind of
understand the value of every single component to the ML stack as we added it, right? We added
model serving layer and, you know, how many use cases did that unlock and how much, how much
did it make it easier for teams that were trying to put ML in production? And we built a model
training system and then we connected them. We built all the different components. And
what we found was that the, so we always hear, you know, team struggle with data and data scientists
spend 85% of their time cleaning data. You know, I'm sure every single, you know, podcast guests
has said that at one point. So we also found though that after kind of solving spending 85% of their
time on that, there's another kind of like hidden 85% of the time it takes to get something into
production as well. And so, and I get that that adds up to more than 100%, but it's also like kind
of the point. And the core of that was these data challenges that were preventing people from
getting to production. And as we built out, what at the time was just kind of like data infrastructure
that we were building into the machine learning system and we were building in these kind of core
data workflows into the machine learning platform at Uber. We found out that that unlocked a lot
of value. And so what, what does that mean? More specifically, it allowed teams to get
into production, to go to production really quickly because before they would have to, they would
be really like coming up with some cool model prototype and then have a bunch of data engineering
that they would have to do to get that model in production. And a lot of the kind of future
store capabilities, we didn't even call it future store right away, are things that unlocked that
path to production really quickly for data scientists who are just trying to build their models.
And the second element there was making just in terms of in terms of machine learning as a
like an organization thing, the future store really allowed for different teams to be able to reuse
each other's work. And so it really kind of like led to the scaling of machine learning across
the organization really quickly because you know, we would have teams that have a variety of
models that they want to build, but they're all kind of similar. They're all kind of using
similar and related data. And so they probably have very high overlap in the number of,
in which features they want to use across those models. And so this provided a way for teams to
share and reuse and kind of have this canonical catalog of these features that allowed for like
kind of like a Cambrian explosion of machine learning at the company. And it wasn't even really
something that we realized at the time, but kind of looking back on it and doing reflections and
reviews, you're like, oh, that was really the component that was the most useful and unlocking
machine learning at the company. And so we spend a lot of time formalizing like what is a
feature store? And what are the bounds of it? And how, you know, how does it use? And that's why
we really focused on the area. Does that make sense? Yeah, no, it does. It does. Yeah, it makes me think
of a question that I've asked a number of folks. We may have even talked about this a couple of
years ago, but when, you know, talking to folks about a feature store, there's, you know,
there's often a set of folks that are super excited about it and get this idea of feature
reusability and wanting to make it, you know, easy for, you know, the next data scientist that
has to deal with customers or products or some of these core ideas that, you know, that a given
business deals with over and over again, you know, wanting to make it easy for them to use pre-built
features. You know, there's a core set of folks that get excited about that. There are other
folks that are somewhat hesitant and, you know, worry about the burden it puts on a data scientist
to, you know, they're essentially then if they publish a feature into a feature store kind of
owning a product, that feature that they have to then maintain and, you know, they may be confronted
with requirements from other data scientists that are not really in line with what they're trying
to do. There's a little bit of, you know, tension between, you know, just making it really easy
for folks to solve their own problem, you know, and then you put in this kind of reuse infrastructure
that promises to make the overall ecosystem faster, but it does require, you know, potentially
sacrifices on the part of individual data scientists. And I'm curious, is that something, you know,
that you run into is that, you know, how was that evolved kind of over the past couple of years
in terms of in practice and the way you see folks use these kind of tools? Yeah, I mean, this is,
I guess that's a core collaboration problem, right? And why do teams want to do this in the first
place? Well, the reason is because these efficiencies that you mentioned, you know, without this,
we, you know, talk to companies every day, see the first hand, there will be two data scientists
who sit right next to each other and they're building the same 10 features or the same 100 features
and either they don't know that the other person is building the same features or they know about it
and then they don't have a way to reuse it. They don't have, they don't have that kind of path
to build that into their model. And then there's that kind of third element of like,
hey, I actually do have a way to reuse this, but you know, I don't know if I'm going to be able
to trust that this person is going to maintain this data pipeline, this feature at the quality,
the level of quality that I care about, is this person going to be on call for this pipeline?
Is, does this person think of it as seriously as I do? You know, I could probably work something
out with them, but you know, it's probably just easier for me to build my own thing even though
it's going to be a hassle. I want to, I want to save this future larger hassle. And so these are
kind of problems, collaboration problems that can be solved by a central platform, right? So
within a feature store, of course, the feature store tracks the kind of data, the feature values
and the functions or the pipelines that generate these features. But there's also like a variety
of metadata, which is like a pretty important component of the feature store metadata that are
tracked for these features to allow different organizations to kind of apply policies for reuse,
right? Who's the owner of this feature? And what level of kind of productionization or SLA,
are they promising for this feature? Are they promising? Is this used in like a tier one system
or a tier two system? And different organizations have different ways that they think about this
kind of stuff. Is it an experimental feature, a production feature, or a completely dev feature?
And so having this kind of metadata and this transparency of this metadata is the first step
to allowing these teams to collaborate. But one kind of pattern that we're beginning to see
in many organizations also is there is this notion of these kind of like analytical data pipelines
and then these operational data pipelines. Think of analytical as being data that doesn't actually
get used in my product or in production in some way, but it's just something I've built for
exploration or reporting or just like a one-off analysis, hey, I want to estimate what the
sales forecast is going to be at the end of the year or something like that. And then the operational
pipelines are things that run every day or potentially real time, they're using the product,
they're productionized, they have SLAs, you want to be on call for them. And so
another kind of pattern we're starting to see in organizations is that beyond, they recognize
that individual data scientists need to be able to get stuff in production on their own.
But as what they have built becomes useful to beyond just their single use case,
there tends to be these kind of central, most a lot of companies have these ML platform teams
and the ML platform teams often dedicate some resources to like managing the core feature pipelines
for the company. So rather than just having data scientists now have to worry about owning this
feature that other people will consume and then they'll have all these extra expectations of them,
a central kind of like feature team as a sub team of the ML platform team is a common pattern
we're beginning to see. And they'll kind of take over the most use and the highest value features
to guarantee their correctness, et cetera. And that's actually the pattern we use in the Michael
Angelo team as well. Now you're talking about a level of rigor and sophistication that's
you know fairly common in the kind of large Silicon Valley companies, but for traditional enterprises
is certainly starting to see more and more platform teams forming,
but it's not nearly as common in my experience. I'm curious if that's your experience as well.
And if you can kind of maybe compare and contrast what you're seeing in traditional enterprises,
both with regard to feature stores, but more broadly kind of their journey to operationalizing
machine learning. Yeah, I mean, it really depends on the company, but there's a lot of companies
who are being told, hey, we need to we need to figure out ML. We need to kind of that's
core to our strategy. And they're spinning up an ML platform team and ML infrastructure team,
or it could be kind of like an advanced analytics infrastructure group. And it's hard, you know,
often it's hard to find people who the people who like know know this stuff well, or it's hard
to like learn this stuff in the first place. And a big challenge that a lot of these companies
have is that they're still not kind of at like data maturity. So then building like ML maturity
on top of that is a tricky spot to be probably the biggest kind of elements. And technically,
that's the challenge is you have to be in a pretty good spot with your data infrastructure.
First and a lot of these companies, you know, they're trying to hop on the AI, the machine learning
wave while they're still mid migration to cloud, or they have a ton of data silos, or they're
they have they're trying to figure out how to adopt streaming data at the same time, but they want
to build machine learning using streaming data. So kind of like that core infrastructure and the
core data is something that I would say is like one of the top priorities to focus on and really
work on as you're building out the ML platform team. But then also kind of on top of that,
you want to build ML ops processes. And it's a super fragmented space right now. And I think it'll
stay like that for a while. The ML ops, all the different tools that can fit in the ML ops pipelines.
But that's stuff you can figure it out and get started with. There's a lot of good options out
there and you know, just goes start using cube flow. A lot of teams can just pull that in and get
going. But a specific challenge that teams face there is kind of the boundary between that
infrastructure data and and the kind of ML ops tooling. And we see a lot of kind of,
for example, example, like see a lot of auto ML systems that do really nice demos where
you know, they'll say, okay, let me just drag in this training data dot CSV into my auto ML
system. And then, you know, I have this amazing model now and it's productionized. But it's like,
where did that training data dot CSV come from? That's actually the whole hard part here.
And then how do I use that in production? And so, you know, practically like I want to
calculate a feature on a stream. I want to share share something with my colleague. A lot of
these elements are challenging. And and I think like getting started with the infrastructure.
And then some of the just like core ML ops frameworks like cube flow is a great option. It's just
a great way to get started. You know, kind of digging into the feature store and and you know,
kind of what it really means technically. Yeah, I'm curious your sense for, you know,
what are the kind of core components or or capabilities. I think at the highest level,
like you can kind of group it into online and offline. And you know, those have different
requirements. But I've, you know, refer, I think we actually talked about this at one point.
Like, you know, there's, you know, different, you know, feature stores will do like automatic
backfilling and, you know, have all different kinds of abilities. I'm curious what you,
you know, and maybe the way to lay it out is in terms of, you know, what do you need when
you're just getting started? And, you know, gives you kind of the, you know, 60% of the bang for
your buck or your 80% and what are the capabilities that more mature teams tend to look for or need to
build? Yeah, it's, it's very interesting because the space of needs is really large. So different
teams need totally different things. And, and it really comes down to kind of what are their data
needs for their models and the ML things that they're trying to build. So if you're a company where
you have, you, if you're a company that is only doing kind of batch analyses, there's no real time
component to your, to your company. There's no interaction, live interactions with the customer.
The concept of a batch, batch feature stores, probably sufficient. And that's, it's not so much
different from what you would get from a standard data warehouse and some kind of typical data
pipeline and tools where things start to get more complicated is when you have, you really have
this operational environment, this production environment on top of the analytic environment
or as alongside it. And then you have to manage your machine learning development process
in such a way that it interacts with that it plays well with the operational or the production
environment. And so a feature store kind of, what is it? It's a data system built for supporting
ML ops workflows. And so it operates the data pipelines that generate feature values,
it persists and manages the feature data itself. And then it serves this feature data consistently
across production, you know, online and development offline workflows. And so we think of it kind
of as like a central hub for feature data and the metadata that is used across ML models,
lifecycle and especially for sharing and crossing organization. So some specific things, you know,
you mentioned, you mentioned that kind of backfilling. So, you know, feature stores are unique in
that they map across the development environment and the production environment. And so when they
have some special capabilities to allow when you add a new feature to automatically backfilling
and calculate historical values of a feature. So when you're training a model on all
logins on the past six months or all purchases in the last year or something like that,
you don't have to wait another year for a feature to be calculated, right? You don't have to
log that feature for a year. You can generate, you can continually register new features and have
all of that training data be instantly available. And so that's a pretty big component
of feature stores. But we break down the capabilities of a feature stores roughly into five
components, right? A transformation layer, which takes your raw data and generates feature
dollars, a storage layer. So that's kind of like feature store, the storage layer, which
organizes those feature values and persists them for use, for retrieval online and offline.
And then a serving layer that serves them online. So for real time serving less than
so like low latency serving, monitored, etc. And to and to also power training data set feature
retrieval to build a model to generate a training data set. So kind of a unified retrieval interface
across the serving, across online and offline. So those are kind of the three main components that
touch the data. And then a central registry that defines all of these features and contains
that data and metadata, which is kind of like an immutable record of what was what was I using
in production? What was available analytically at this time or that time? And a monitoring layer to
ensure the correctness of features and the operational, you know, track the operational metrics of all
of the data pipelines that are powering my model in production. Now a lot of the elements that you
talked about have kind of traditional analogs within the enterprise data ecosystem. Data catalogs
have gotten more popular. You know, certainly there's data warehouses, you know, snowflake,
IPO, make sure that we all know about data warehouses, although that's been part of the enterprise
data landscape for a very long time. But there's, you know, all of these kind of independent
tools that have played a role in helping enterprises do similar kinds of things. Is it, is it, you know,
easy to kind of put a finger on the difference between, you know, those standalone components
and a feature store or are you maybe even seeing organizations, you know, take their existing databases
and data warehouses and data catalogs and kind of build a feature store out of those things. Yeah,
so those components make up, those components are reused by a feature store. So feature store
really coordinates across, like across a common orchestration data transformation orchestration
system or a warehouse for storage or a data lake for storage. And we don't reuse anything,
we've run our own kind of serving layer. But the goal, it's kind of important to recognize that
the goal of the feature store is to provide really, really good access to data in the ways that
ML ops workflows need that data. It's not to replace existing data infrastructure. So it's actually
quite important for the feature store to plug in and integrate quite nicely with what data
infrastructure, a team company already has today that they're happy with. And, you know,
this kind of goes back to actually like a lesson that I learned, I learned building out
Michelangelo, you know, we would go to, we'd, we'd built Michelangelo, we would go talk to
different teams internally, hey, you guys are building this kind of model. Can Michelangelo help
your team out? And it was never the case that a team would say, hey, I want to migrate this
existing thing that's working for me onto this other system that you're coming and telling me about.
It was always, hey, we have some pain points and maybe we can build V2 of what we're building
on your new system because that system solves all of these additional pain points and makes use
of what is already working. And so this concept of being gradually adoptable and reusing as much
of the company's existing data infrastructure such that there's as little duplication as possible,
was quite important for a feature store. I think the biggest distinction though is the concept of
now we're talking about operational environment as well. And so that's the big shift that a
feature store also has to take into account. It's, it's, there's this concept of a catalog of
operationally available, vetted, productionized signals features for use in models.
And on the note of kind of this evolution, what does it typically look like to
deploy one of these? So that conversation that you had with team at Uber, I imagine you're having
various versions of that conversation with teams today. And what are you telling them? It looks
like to eventually kind of make their way to, you know, dynamic, always available feature store
in Irvana. Yeah, there's, so a tech con we have a couple of deployment models. And one of, so
they range from a fully hosted cloud service to a, a managed cloud service that lives actually
in your cloud account. So tech con is completely cloud based. And the, the distinction between
those deployment models that I just mentioned is that in one, it's a little bit more similar to
a snowflake model where tech con manages the whole tech con cluster, the whole feature store,
we manage all the SLAs for it. You pass your data into it, it's storing the features and it
will serve features to you. And so your data comes into tech con's account and we, and we manage the
whole thing end to end. You don't need to have any engineers do anything internally to kind of
maintain or support the tech on deployment. There's a separate deployment model, which is preferred
by some organizations, which actually has tech on run in there, Amazon account, but still have
our team manage that software. And this is becoming a more and more common deployment model for
enterprise data infrastructure assets. It's, it's really, you know, have, have a company from the,
from the outside, have their control plan talk into connect to a data plan that lives within the
customers, AWS account or whatever it is. And within that, that there's a VPC where all that
software runs and talks to the data sources internal to the customers account processes that data
and serves that data all within the customers account. So their data never leaves their own account.
And so you can imagine there's some larger enterprises that prefer deployment models like that.
And maybe let's talk a little bit about what you're seeing in terms of the, you know, for folks that
decide to go this route, what does the ecosystem look like? You've got some open source out there,
particularly in the Kubernetes community in Feast. There are kind of rumblings that, you know,
some of the cloud providers will incorporate feature store capabilities into their offerings.
You know, what else are you seeing out there and what, you know, your, your offering is
cloud-based. Does that, you know, does that disadvantage you when the cloud vendors decide that
they want to offer this as a, you know, feature store as a feature? I mean, it's going to be a
battle, but, you know, it's definitely not worried that they're going to build a better product or
better feature store than we will. The, you know, the space right now, there's a handful of
systems called feature stores, the most notable beyond. So we kind of coined the term feature store
a couple of years ago when we published a blog post at Uber about Michelangelo and we talked a
lot about this notion of a feature store. There's a couple of different projects that have come out
and a number of kind of talks at conferences where different companies have, have talked about,
hey, this is how we implemented a feature store internally. One of the most notable is,
is Gojack on G, on GCP, they open sourced pretty lightweight, but very powerful feature store
that is quite good and people should check it out. It's really easy to get started with that.
That is based on Google right now. I'm sure it's going to be in other clouds quite soon as well.
And then TechCon offering is more of like an enterprise-based offering. So, you know,
with SLAs, hosting, you know, being on call all of the enterprise capabilities. And I think we're
going to see the cloud providers and the big data companies come in this space quite strong.
I believe that 2021 is going to be the year of the feature store. I can't tell you how many
companies come to us asking for a feature store without, you know, having a great understanding,
like originally about what a feature store is, what problems directly it solves for them,
because they have a variety of problems and they know just, hey, I have so many data problems.
How can the feature store help me out? And we have a lot of those discussions and just to help
them get started in this space. Yeah, it's been an interesting evolution of the MLOB space
in general. You know, just in doing this interview and kind of reflecting on the fact that this
was two and a half years ago that we were talking about feature stores. The first wave of products
in the space, and it's more nuanced in this, but a lot of them were kind of these workflow
end to end. We're going to try to slurp up your entire process and automate it.
And yet, when in my conversations with folks that were, you know, building and running these
systems, you know, at Facebook and Google and Airbnb and others, one of the most important
elements of what they're doing was this, you know, this feature store. Airbnb has Zipline,
which is, you know, their kind of central repository. And I forget what it was called in Facebook,
but they had theirs and Google had theirs. And yet, it's taken quite a while before, you know,
you have started to see kind of commercial offerings in the space. Your company's relatively
new in terms of go-to-market. And yet, it seems like I read 2021 is going to be this year where,
you know, we're starting to see a lot more activity. And I'm curious to your take on why that is.
Yeah, I think it's interesting because you think of machine marketing.
It's going to be great for you. Like, you have this market that you know that you know is important
and no one seems to be in it for a really long time, or I should say few seem to be in it because
there are, you know, some folks, but a lot of what I've seen thus far has been folks taking, you
know, older technologies or technologies that weren't necessarily purpose-built and trying to
apply them to solving this problem as opposed to taking a purpose-built feature store approach.
Yeah, I think part of the trickiness here in one factor is just when you think of machine learning,
you think of models, and you think of, you know, when people get started, what's the sexiest thing
to work on? I want to get started on these cool models. And, you know, some people just jump straight
to deep learning, which you know, I'm sure you've had a ton of people on the podcast say the way
to do it is actually start as simple as possible with the simplest algorithm and then you get more
complicated after that, right? And so what we see is the teams, there's just kind of like this
anti-pattern in industry where teams kind of get started with kind of focusing on stuff that's
slightly more advanced in what their needs actually are. And they kind of over-invest in some of the
model stuff at first, which ends up being actually operationally easier to manage than the
data pipelines that power these models. And so the teams that come to us, or they kind of say,
hey, we actually thought we could repurpose a lot of our existing data infrastructure,
just like plug it in directly to the models. But what actually happened was that's now where we're
having a ton of pain, a ton of friction. And that's the core thing that's grinding the innovation
from our data science teams to a halt. So, you know, we see a feature store to help us out there,
but we already have this investment in this model stuff and we probably should have
done it the other order or kind of done those in parallel. And so it's not a super obvious,
there's all these kind of roadblocks and challenges you face that are not super obvious up front.
But when you hit them, it kind of just grinds things to a halt.
I had a follow-on question to that.
You know, one of the things that we have that I think we see that is like actually quite challenging
for people to kind of makes, it really makes it obvious what some of the data challenges are when
putting machine learning into production is that there's kind of just a variety of elements,
you know, the development and production environments are not the same. So you kind of have to map
between those. And then we see teams that have kind of constraints from what can be done in
production that affect what the data scientists are even allowed to experiment with. And that
really kind of just like constraints, you know, what they're able to do. Deploying these systems
is really complex as well. And even just like monitoring and validation of these systems,
not a solve problem. So when things break in machine learning, it tends to be kind of the data
pipelines that break and investing in. And so these problems are super hard to debug also.
So it kind of investing in that layer ends up going paying off in a big way when you're really
trying to depend a core business process on these systems. And the question that I was looking for
earlier was right along those lines, do you you mentioned platform teams earlier as folks that
are kind of owning these production feature pipelines. Is that the primary person that is that
you see kind of leading the the feature store charge or does it does it ever come from data
engineering and the data infrastructure side of the house or good question. So the so there's
a couple of things like why do people adopt feature stores in the first place. And so it kind of
comes. There's kind of two paths. There's teams who are just trying to get this one model,
you know, they have this important use case where this fraud system and we need to be able to
calculate features in real time and use our historical features. And it's just like a crazy
engineering problem for us and we just want a system that can handle that for us. So that's that
kind of component. Another path for people who are adopting feature stores is when they are
trying to build out their ML infrastructure properly and they're doing their research and they
identify, you know, I was going to conversation with a big bank the other day and they're showing
their stack and right in the center they're kind of ideal stack and there's kind of data infrastructure
feature store feature catalog right in the center and then some of the modeling elements
and the application elements on top. So they're just trying to take a very thoughtful approach to
building the right infrastructure and the right tooling. What we want to enable the goal is to
enable data scientists or analysts who ever are building these features to be able to to go
end to end and get their models all the way in production without requiring without throwing
things over the wall to the engineering teams for every single change that they have. You shouldn't
need production or data engineers really have to know or be involved in any way when a data scientist
is making a change to their model and production. When that happens, we've seen a real kind of shift
in almost like the almost like the roles it kind of makes data scientists much more owners of their
work in production rather than rather than being in a position where yeah I handed it off to that
team and they're handling it. I don't know why it's not working right now and so that has been
like a really big cultural shift in teams that have adopted this kind of adopted this technology
that allows data scientists to kind of get stuff into production on their own. So we like to kind
of have the people who build the systems be the owners of those systems and then there's just
instances where you you have so many people who depend on these systems that you might want to
centralize some components of that ownership. Is it now like us to the transformation has been
happening over the past 10 years or so with traditional software engineering and DevOps and
having these pizza box teams that you know exactly like cycle of their services. Exactly and
yeah and people you know sometimes company will ask okay what's the the right operational ml stack
and you know I don't think the space of needs is so so large I don't think there is one and
to me it's kind of like what's the right software stack period in question mark right and it's
just like you know it depends and the answer is always it depends and so you know I kind of like
give guidance to companies or when you're trying to figure out what the right approach is talk to
someone who's done it before and they can kind of walk you through a lot of the challenges that
you're likely to encounter there's a lot of a lot of bottlenecks that you will hit when you have
you know data scientists and engineers starting to collaborate for the first time or when you're
trying to buy some software that you've never had to buy before you don't have an owner for the
software there's just like a number of elements that come to cause a lot of challenges without
being kind of thoughtful about things ahead of time and on the data side we think obviously we
think feature stores are like the right way to kind of unlock a lot of the core ability to put
models in production features in production and so that's kind of the key reason why we got started
with the feature store yeah yeah I mean there's an interesting paradox in there where you know earlier
you were alluding to folks that were choosing more complex technology than they needed for the
thing that they were doing the thought in my head as you were saying that was yeah I need to deploy
you know you know single you know team internal web app I need Kubernetes right and you know at
the same time your advice is talk to folks who have kind of gone down the road and
consider things that you're likely to run into as you're making your plans and and you know
that in many cases is what you know leads folks to kind of overbuild right there kind of thinking
far into the future at least that's an optimistic you know view rather than kind of playing with
cool what actually mean like talk to someone who knows what they're doing who can tell you hey don't
overbuild someone who can someone who can say you probably shouldn't be investing in that because
you don't even have you don't even have your data in the right place or I know where I'm where
I'm the question and I'm trying to get to is like you know how does someone know if they should
even be thinking about a feature store at all or if it's you know a step or or or five ahead of
them and it sounds like the first question is you know do you have your data house in order you know
if you don't worry about that first right yeah I think the core kind of like the two biggest
things are are you building models that need to interact in real time so are do you have some online
component to your machine learning application to your operational machine learning application
that's when you that's just like a key like a pretty good indicator that okay I'm going to have
I'm gonna have this production environment I'm gonna have to have a real time real time serving
on this machine learning application very likely includes kind of an online data storage
element for my features which I'm gonna have to manage and it's a data scientist probably haven't
done that before so that's kind of one pretty important kind of path and then second is do we have
do we have more than a handful of models and more than a handful of features powering those right
uh especially if they're being shared across those models it it goes from I've seen teams kind of
like manage their collaboration with you know a Google spreadsheet and they have a list of
features that is okay this pipeline is here and this feature does this and just ask this person
if you want to use it or if you want the code for it and uh and so I've talked to them and said
hey this is working pretty well for us now and uh that's great when that works for you and then a
month later they call me up and then they were like okay so now we're at uh you know a couple hundred
rows of this thing and things are kind of going crazy and we need a better way to manage this so
it's almost like when you get that scale and you need some kind of like you need to start thinking
about collaboration between your team and you want to have some efficiencies of scale as well.
Features you know one of the uh you you brought up uh kind of deep learning earlier in the
conversation is kind of a side comment um you know deep learning is kind of notable for not being
as dependent on features and feature engineering as traditional ML models um is a feature store still
relevant in that world or no not as well. Yeah good question um so we see uh teams use feature
stores in two ways for when they're doing deep learning. One is uh to to host pre-computed parts
of uh their model you can think of it as like use a feature store to do my embeddings properly
and uh and pre-computed my embeddings and then look them up in production so that's a really big
element of a feature store is kind of like making that stuff operationally possible and really easy
and then a second component is making sure the data is available so you know you may have a
uh deep learning model that let's say it's a uh recommendation model and it does uh it needs
some input from for example your current search query right that's one one input to the model
and then some also some information about the user itself or users themselves and maybe some
information about the current page or the current product that they're looking at that model needs
to have access to all that information that historical information about the user item etc and
it's really the feature store's responsibility to get that right information at the right time
and deliver that information the those data uh to that model and they're effectively features
at that point that we're passing into the model though they're typically less processed features
than uh you might have in a non deep learning uh kind of model does that make sense you got
you get what I'm saying where you know you bring that data up and make that available to the
model and that's really um uh that's really kind of the role of the feature store for deep learning
models typically yeah well what it made me think of is um you know presentations that I've seen
from folks at like Google for example where they talk about one of the main issues that they see
in production is um you know for any kind of model deep learning or otherwise is the feature data
just changing in semantic or being semantics or being missing or um you know before you
you know had you know something as nulls and then you change it to empties or whatever it's just
random things that that happen in a pipeline that no one sees and it's um it sounds like what you're
saying is that people use the feature store to kind of manage uh that uh that infrastructure that
that process and and specifically the the data part yeah having a common way to reference like
a common way to literally specify what data does this model need what data does it need in
production yeah I'm making a prediction for this user and this item how do I know which data to
pass into the model for that purpose for this this data for this user and this data for this item for
example um but then all of that data that does you know we do use for that model you know we think
of it as kind of making it like operationally ready for for machine learning consumption for your
actual ML application where there's a bunch of things you want to do uh in terms of like validating
that data monitoring it for drift when we talk about monitoring it for drift it's like monitoring
it compared to you know how the data looks today compared to how it looked last week and make
sure the data doesn't look completely different but also does this data look similar to the data
that the model was trained on in the first place and you know is there are there any indications that
this model is um starting starting to get a completely different data than it expects in which case
we don't really have any guarantees about its behavior and it's you can't really expect non
erratic behavior from a model in in that situation mm-hmm awesome awesome uh well great stuff here
any uh kind of parting thoughts or you know for folks that want to dig in deeper to this where they
should look or um words of wisdom being uh a couple of years into this journey well or more than
a couple years into this journey yeah i would say i would say start simple and um yeah for
your interest in learning more about feature stores um or getting started with them uh come to
tecton.ai and uh and just leave us you know get in touch and uh and we can chat and see how we can
help awesome well mike uh wonderful to catch up with you and yeah i want to see all the cool things
you guys are up to yeah thanks a lot it's been fun awesome thank you
