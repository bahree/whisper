Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting
people, doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
This week's shows are drawn from some of the great conversations I had at the recent Nvidia
GPU technology conference and they're brought to you by Dell.
If you caught my tweets from GTC, you may already know that one of the announcements this
year was a new reference architecture for data science work sessions powered by high
NGPUs and accelerated software such as Nvidia's Rabbids.
Dell was among the key partners showcased during the launch and offers a line of workstations
designed for modern machine learning and AI workloads.
To learn more about Dell precision workstations and some of the ways they're being used by customers
in industries like media and entertainment, engineering and manufacturing, healthcare
and life sciences, oil and gas and financial services, visit Dell EMC.com slash precision.
Alright everyone, I am on the line with Paul Moller.
Paul is a senior data scientist and technical product manager for machine learning at Nvidia.
Paul, welcome to this week in machine learning and AI.
Thanks for having me.
Absolutely.
I'm looking forward to jumping into our conversation which will be focused on what Nvidia is doing
around rapids and QML and all of the interesting stuff in that area.
But before we do that, you were a philosophy major.
How did you make your way to working in machine learning?
I was a philosophy major and two years before I was set to graduate, I added economics as
a major because I read the economist magazine and thought that it was a fascinating collection
of a bunch of different articles about different aspects of the world.
So I figured if that's what economists read, I wanted to be an economist.
I went on to do a master's degree in economics where I mostly focused on quantitative methods.
In an earlier life, I went out to Washington, D.C. where I worked as an economist.
I began my career at the World Bank, serving on health and human welfare issues in sub-Saharan
Africa.
And then I worked for the office of the chief economist at Fannie Mae.
Now one day, while waiting for the bus to get home from Fannie Mae, there was an article
from the New York Times about a gentleman from SUNY Buffalo that had written an algorithm
that was offering notes on screenplays.
And I always had like a hobby interest in the different creative arts aspects of our
culture.
And when I saw that somebody had written essentially a big block of math and code that was telling
writers how to write their screenplays better, I immediately decided that I needed to switch
into data science or big data, which was the more popular term at the time, because that
was where some of the most interesting things in the world were going on.
That's a great story.
You remember the moment, literally, the bus ride that triggered the, that set you off
down this path.
Yeah, it was raining at the time.
Awesome.
And so, what do you do now?
I've been doing for the last year.
I had been at a couple of startups, and I joined in the video to work on what we've been
calling the Rapids project or the Rapids ecosystem.
Now what that began as is that our director of engineering, who I'd worked with previously
at Accenture, for years had been saying that we see this great acceleration in neuro-acceleration
neural network methods as a result of getting them on GPUs.
But you are not seeing any of those advantages for more of the bread and butter data science
that happens at a lot of places like Fannie Mae or the World Bank, where they may have large
data sets and questions they want to address through machine learning, but we aren't talking
about convolutional neural networks to understand images.
So, you know, a lot of the time when I was working at a data scientist at a couple of startups,
I like to joke that I was really a bar trivia champion, because while I was waiting for
my code to finish running and spit out my result, I had plenty of time to read all the news
of the world on the internet.
And I guess, you know, unfortunately, for me and my compatriots in data science, what
we've done with QML and QDF in particular is, you know, if somebody knows pandas or they
know the Pi data ecosystem, they can immediately jump right in and start seeing just crazy
speed ups, like, you know, 50X, like, sometimes more than that on doing their end-to-end workflows.
And that includes, you know, reading from a disk to GPU memory, doing all your data
munging and merging and variable creation through actually executing your algorithm and, you
know, making inferences.
So the idea was that since all of this, well, I mean, some of it is not obviously tractable
to GPUs, we are able to process strings in the latest iteration of QDF, which to me seems
like a miracle, but it really, I like to joke that it's kind of like before, you know,
the team that I work with had delivered these big pieces of QDF, it's like I could drive
a car, and now suddenly I can fly a plane.
And I don't need to be an expert in CUDA or parallel algorithms or anything except the
tools that I've worked with most of my career.
Now, let's take a step back, maybe.
When I introduce you, I mentioned QML, you've mentioned QDF, mentioned Rapids.
Can you kind of paint a picture of the broader ecosystem of software and libraries and
tools that comprise our makeup, Rapids, and, you know, how they all fit together?
Yeah.
So Rapids is the overall name of the project, and that's made up of smaller sub libraries
that all start with CUDA, because that's inherited from CUDA.
Rapids is built on Nvidia CUDA.
And CUDA is the, for anyone who's not familiar with the underlying library or API for
doing things on Nvidia GPUs.
Right.
It's the general purpose computing library for Nvidia GPUs.
Okay.
And if you think about the more broad, pie data ecosystem, I think a lot of people do
a lot of their initial data cleaning and exploration in pandas.
And so that's what CUDA is meant to replace for people that are moving their workload
on to GPUs.
And so the API is very, very close, and you're able to, in some cases, just change the import
statements at the top of your program, and it will just work.
And so pandas has this kind of core abstraction of a data frame, and so CUDA is just a kind
of, you can think of it as a CUDA-powered data frame.
Yeah.
I think that's the best way to think about it.
Yeah.
QML is our machine learning toolkit, and we aspire to, one day have almost all the functionality
that exists in Scikit Learn.
Scikit Learn is an eminent package built by some of the world's greatest developers.
So we've got a ways to go there.
But we've been rapidly adding algorithms in the last release, for example.
We have stochastic gradient descent regression, ordinary linear regression, ridge regression,
principle components analysis, and some other things like Kalman filtering.
What we're trying to do is start with the things that are the real workhorses of day-to-day
machine learning in business and other parts of industry.
And it's been exciting to watch the package grow.
In fact, when we launched back in October, we had, I think, four algorithms in QML, and
we've doubled that over the last couple of months.
It was very exciting to present at the GPU developer's conference in San Jose, California,
a couple of weeks ago, to the wider community, all the things that we had been able to deliver
in such a short amount of time.
You mentioned that you kind of aspired to Scikit Learn.
Does that mean that QML replaces Scikit Learn?
It sounds like it does, for folks that are trying to take advantage of the GPU.
And was there an opportunity to rather than replacing Scikit Learn kind of fit in underneath
it so folks can that use that API or that have existing work that uses Scikit Learn could
take advantage of the GPU acceleration without having to rewrite their apps?
I mean, at least with the algorithms we've delivered, we've tried to keep the API one-to-one.
For any of your listeners, I would encourage them to take a look at the API and just see
how close it is to Scikit Learn.
I'd also like to add that we've partnered with Enrea, the French research institution
that does a lot of work on Scikit Learn and over the next few months and few years we're
going to be building that collaboration with them.
I don't think we'll ever replace Scikit Learn because there are still problems where I
don't think it's big enough or the use case is right to necessarily go to full GPUs.
So I think of certain analyses I did as an economist which would look like machine learning
but we're maybe a few thousand rows and this was much more traditional frequentist statistics.
I think that there's always going to be a lot of that work being done and I think that
with any data science work it's about finding the right tool for the job.
But I will tell you when I was testing out our code earlier in the summer, our demo workflow
involved reading in, I think around like a gigabyte of CSV data to pandas and on my MacBook
Pro it took like five and a half minutes and on a single GPU it took like 15 or 20 seconds.
When you talk about data analysis is an iterative interactive process and the faster you can
move the more fluid your conversation with the data will feel to you as a user.
There won't be the long wait times to see results or see if you made a coding error in
my case.
That's an opportunity to become a bar trivia master.
Yeah, it's a shock.
So before we dig into that because that's an interesting point there, we're kind of
talking about the landscape.
You mentioned Kudieff, Kuh-A-Mal.
Are there other major pieces that we should be keeping track of in this conversation?
Yeah, we are working on a graph analytics package called Kuh-Graph and yeah, our minds
are so fixated on accelerating the algorithms, we're totally out of bandwidth for fancier
names.
But everyone knows that Jensen does all the naming it in videos, why would anyone else
spend any time thinking about that?
Kuh-Graph is embarrassing in a sense that we compare it to a graph analytics package
in Python and it's one of those things where you see the numbers, you just really want
to double check them, like 10,000 time speed ups over network X for certain algorithms.
Well, that was my reaction to the loading the data frame.
And I still want to get through the kind of broad landscape before we dig deep into
that, but that's the first place I'm going to come back to once we do.
So you've got a graph analytics piece in Kuh-Graph, any other major components here?
Some of the things that began as major components are now under the hood.
So we put a bunch of effort into building a string reader.
So you could directly parse data sets with strings, it's a very common thing that happens
in data science, GPUs do not like strings.
But now you can do things like just easily create your dummy variables from strings on
the GPU, which sounds kind of ho-hum, but it's actually a pretty major achievement.
It's just part of the whole speeding things up.
I don't think our case would be as compelling if we said that it could only be numerical
data in the Kuh-Graph data frame.
That simply will not work for many use cases.
So we also have a package called Kuh-Cross filter, it's written as Kuh-X filter, and we're
going to be building out some GPU-accelerated visualizations.
So if you think of the workflow from munging to analysis to insight through visualizations,
we want to be able to offer every piece of that puzzle.
The other thing, we're heavily using a software called DASC.
And DASC is a package that handles distributed computing.
It has been used to scale out pandas, for example, to multiple cores.
And we were lucky enough that the creator of DASC has joined our team, and is helping
us use that as a way to distribute workloads when we're talking about moving beyond single
node of GPUs.
Let's maybe go back to the initial example you gave of loading the pandas data frame.
We're loading the data frame.
You said it was a terabyte?
It was a gigabyte.
It's pretty easy to choke pandas, and I'm sure a lot of your listeners have experienced
this before.
The workflow was Fannie Mae makes loan-delinquency data going back, I think, 16 years available
for free.
And this is the whole payment history for a subset of all the loans that Fannie Mae has
acquired.
And as a demo workflow, what we wanted to do is read in however many quarters of data
we could fit, or were relevant, and then apply XGBoost to predict default, which reminds
me of another sort of under-the-hood improvement that we've made.
It's not really under-the-hood, but we have made contributions to the DMLC XGBoost library,
and we'll continue to do so.
That has appeared in a lot of our early presentations and webinars.
I think XGBoost is almost like magic, and it's a good, broad workhorse for the first thing
that we were going to introduce.
But we are working with the community to make certain changes to XGBoost that make it more
amenable for the rapid ecosystem, but then giving those back to the community.
So pardon that sidebar, but before any real data processing had happened, just bringing
in this dense, large data set, at a rule of thumb, we couldn't do more than a couple of
quarters of data.
And then you would really see the time to load and go through the data preparation and
execute the algorithm increase substantially.
So I think that the largest we could do on pandas quarter-wise was like two or three quarters
of data.
They were small.
So I think the biggest we tried was about one and a half gigabytes.
And that's where you saw those really kind of frustrating load times of more than five
minutes.
And so what's happening here in these two different scenarios is on the panda side, you've got a gig
and a half on both sides, you've got a gig and a half of information on a disk, probably
a CSV file, perhaps, and you're loading it into a data frame on the panda side.
It's a data frame that's located in RAM.
And on the rapid side, it's a QDF frame that's located on the GPU itself.
Is that right?
Yep.
In GPU memory.
Okay.
And that's where the kind of the five minute versus 15 seconds differentiation come from.
Yeah.
And this is where this really is more for like our hardcore C guys, but there's some
problems with pandas that have been known for a while.
It's great.
I can't thank West McKinney and the community enough for open sourcing it because it's put
food on my plate for five or six years.
But it's also single threaded on the CPU.
So even in the world of CPUs, you started to see people look for things like desk to help
better leverage.
And the multiple cores of CPUs you might have on your MacBook Pro.
I think it's, you know, I think it's kind of funny, you know, every, every data science
gig I've had, they've given me like a shiny MacBook Pro and I mostly work in Jupyter
notebooks.
And most of that stuff is only taking advantage of a single core of the processor.
Right.
Right.
I mean, I, I don't want to exaggerate too much, but it's almost like you still want
the next MacBook Pro.
Yeah.
Well, wait and see.
I want to see what they do with that touch bar.
I got some thoughts on the touch bar, but so let's not even go there.
But you know, and so the other thing is that when we, when we move past the, you know,
the massive parallelism that you can get from using GPUs, when you get to the side,
I know better.
This is all for the most part, matrix algebra and GPUs love matrix algebra.
They are designs to do it and, you know, in our algorithms like doing, um, Ridge regression,
which can take a some time to run on, uh, in the conventional, pydated ecosystem, um,
I gave a, uh, tutorial at the GPU, uh, developers, uh, GPU technology convention, I was just
like, we're going to just do a hyper parameter search and run through like a thousand Ridge
regressions on this Black Friday data set because it's just fast enough that we can kind
of brute force hyper parameter search on certain data set sizes.
And, uh, do you have comparative results for that particular scenario?
We aren't baking off every algorithm these days.
Ridge regression is fast.
I don't know.
Like doing a Ridge regression, I think like 800,000 rows took me like, uh, less than a second
couple of seconds.
I remember being fast enough where I could do a live demo and run through a hundred iterations
of it.
Normally doing two or three would have taken quite some time.
Let's jump into the, uh, to this, this part of it, the QML library.
Can you maybe talk us through the, uh, technical underpinnings of this?
I mean, is it as simple as, hey, these things love matrix multiplication and we're just
doing matrix multiplication using kuda and it's just faster or they're kind of interesting
nuances to the way some of these algorithms work that, uh, might be worth chatting about.
Um, well, to start with a little bit about the, uh, architecture of QML.
Um, so QML is built on top of what we're calling ML prams.
Um, these are, uh, primitive functions that are composed of even lower level math libraries
or various things that have been developed at Nvidia for certain linear algebra purposes.
And so, um, we take these primitives and they are, uh, delivered in C++.
So then when we need something new, um, like, uh, I have a colleague working on, um, doing
massively parallel arena regressions.
And so when he began working on that, we already had, uh, Kalman filter primitive and an
OLS primitive.
And so the amount of new work that he needed to begin composing a prototype was dramatically
reduced.
One day in the future, I actually want to see these ML prams wrapped in Python.
So, um, different, you know, machine learning researchers or graduate students that aren't
experts in parallel programming would be able to, um, mock up the new algorithms they're
inventing and be able to, uh, leverage the advantages of the GPU.
Yeah, that sounds like a, uh, that sounds like a no brainer.
The ML Prams open source or they locked up in a binary or something.
I mean, even if they're in a binary, they want to be able to, yeah, that's a mix.
I mean, so some of the stuff is, uh, Nvidia proprietary stuff and we've tried to wrap it
in a binary, but other ones are more open source.
And it's a, it's a discussion that we're going to continue to have.
But wherever that ends up, oh, sorry, go ahead.
Is it a well documented, uh, you know, set of primitives or is it kind of internal, nobody
really knows about them outside of the company?
It's something that, you know, um, I've tried to mention publicly, whatever we speak about
it.
It's not super well documented right now.
So you'd need to, uh, you need to be able to go into, uh, our GitHub folder and look
at the primitives folder and be able to read a little bit of C C plus plus got it.
Okay.
Um, but, uh, yeah, I hopefully, uh, want to, you know, wrap these in Python and introduce
them to the greater development community to see what else they can do with them.
Um, now you've mentioned, are there, are there some algorithms that are more or less
tractable to that?
I mean, right now we're working on building a lot of, uh, different solvers for more exotic
kinds of, um, regressions and one of the challenges in developing those has been, um, and this
really is where, you know, I'm, uh, you know, all of some of the, the thinking of my colleagues,
they're essentially sequential, uh, algorithms, the way that they originally designed, right?
If you look at the most basic version of like, uh, a gradient descent, you, you know, start
some place and you keep taking little steps until you're satisfied and then you stop.
Now that's, um, uh, sequential operation, um, when we've been doing some early inspections
on different solvers, um, this morning, in fact, um, a colleague told me that he was disappointed
that we were only getting a 3x speed up because he was still trying to think around how to
make the algorithm less sequential.
So there will be things just by the way that the underlying math works that aren't going
to necessarily be another 10,000 x speed up, but a two or three x speed up, I think is still
pretty great.
And the other, you know, really heavy intellectual work that, um, is going on right now, um, that
we hope to wrap up by the end of the summer or the fall is going to be on, um, multi-node,
multi-GPU algorithms.
Um, that's using DASC.
Uh, in some cases, it'll use DASC and we're currently working our way through what, um,
other kind of, uh, communications layers could be helpful in trying to, you know, block
up this data and distribute it across, uh, a cluster of GPUs in a way that creates, uh,
you know, a wow moment for the user.
Mm-hmm.
I know that's a little marketing, but that's what I'm, I'm looking at, I'm looking at what,
you know, we're doing with QML, like that's what I'd really like.
You know, we've, we've been lucky so far that, um, we have gotten some wows with what
we deliver, but the, the underlying, uh, you know, algebra and algorithms of breaking
some of these things into parallel jobs is, uh, very far from trivial.
You mentioned that, uh, some of the things that you're doing are allowing you to load
your strings onto the GPU.
Are you able to utilize the GPU for kind of, uh, heavy, heavy kind of NLP types of algorithms?
I guess for a lot of those, you're kind of numericalizing the textual data anyway, but,
are there any limitations there, one way or the other?
There are some limitations, and we do want to do ML, uh, NLP, um, we're not quite there
yet.
So, um, we're on a, uh, implementation of word to veck, um, in terms of, uh, preparing
or understanding your data, we have a lot of, um, ordinary string functions like, uh, token
izers, um, we have a regular expressions engine, so you can, um, search for regular expressions
on strings and substrings and use that to create variables.
I think probably closer towards the end of the year would be the soonest I expect us
that we're going to deliver that, but it's certainly something that we're interested
in and are currently working on.
And in just in terms of like the, the less, the kind of the pre-processing to NLP is,
is where we started.
Um, and so, uh, now that we have, you know, released and continue to iterate on our, our
string manipulations package, that's going to lay the foundation for, um, NLP practitioners
to be able to work in a way that they're used to and have algorithms like word to veck,
um, or LDA or other things like that, um, open and available to them.
One of the things I'm curious about is in the Clomel library are all of these algorithms
and everything you're doing with that library only dependent on the GPU.
In other words, are you doing all of the compute in the GPU, uh, or are you using the, the
CPU, you know, as needed or, or where appropriate and, you know, and I'm wondering more, you
know, more broadly is it kind of an all or nothing kind of thing or is, is the focus
really on doing a given operation in the best place for that operation?
I think it's doing, uh, a given operation, um, where it's best suited, but, um, we're
really just looking at things that are, are suited to the GPU, uh, that's actually,
so doing everything where it best, where it's best suited, but everything is best suited
in the GPU.
Our work has been focused on, um, you know, once the, especially on the Clomel side, once
the data is in GPU memory, we don't want to move it around.
Um, that's kind of one of the big advantages that we have by doing int and data science.
We're dramatically cutting down on these read rights.
So like the data will come in and it'll sit as a coup df data frame and I can immediately
pass it into my algorithm and it all stays in one place.
So, um, we're able to cut down some of the overhead by thinking really hard about, um,
reducing, uh, the copies that happen in the course of doing this and all of those.
I imagine that there's a bit of a dichotomy or decision point around, you know, do you
kind of optimize around these NN workflows and then everyone who, you know, wants to do
anything that utilizes the GPU and takes advantage of what you're doing needs to wait
for you to build their algorithm, you know, or, you know, is there some way, you know,
if I want to do something that, you know, that QML offers an optimized version of, but
I also need to do stuff that, uh, for which there's not a QML optimized version, do I
need to load, you know, do the five minute pen to load in the RAM and the load into the
GPU and kind of go back and forth each time or, you know, is there some kind of scenario
where I can load into the GPU and keep the data there, but also do CPU based operations
against it.
I don't even know if technically that makes any sense, you know, or is feasible.
We have a bunch of different formats you can export data from QDF.
So if you had an algorithm that we haven't built at, um, and I'd say anybody is welcome
to join in.
This is an open source project.
We'd love to have, uh, any help we can get building these algorithms out, but if you have
something that you need to do for work today and you're like, could you have sounds like,
like a great way to speed up the pandas part of my workflow, but I am going to do a bunch
of algorithms, for example, um, I want to do a bunch of Bayesian stuff.
Um, we can export data from the GPU data frame into a pandas data frame.
If that's what we'd like to work with it, um, into an empty array, um, we support the
arrow data format, um, and we also just introduced support for DL pack, which plays nicely with
a handful of the deep learning packages like PyTorch.
So, um, while we're working as hard as we can to add more algorithms to it for the user
community, um, you can, uh, pick and choose what's, what's most useful to you at this time.
I'd also like to add that, um, QML can, uh, take in Numpy arrays.
So these packages were designs to be closely used together, but, um, we know that that's,
uh, not going to cover all users and, um, that's not, uh, necessarily a requirement.
The other piece of this is QGraph, and I gather that's, uh, a newer, kind of more emerging
part of the rapid ecosystem.
Uh, yeah it is.
Um, I don't have the deepest knowledge of, uh, QGraph on the team.
Uh, I'm not, uh, really a, uh, graph guy, but, um, I know that their benchmarks have been
fabulous and we're hoping to make more graph algorithms available to people that heavily
rely on graph theory in their, uh, day to day work.
And then maybe switching gears a little bit beyond the work that's happening, uh, in
rapids.
Uh, one of the things that was mentioned at this recent GTC was some, uh, announcements,
uh, and partnerships around, uh, creating a reference architecture for data science
workstations.
What can you tell us about that initiative?
Yeah.
The reference architecture for, um, data science workstations is, uh, very new, but, um,
what I think is exciting about it is that, uh, for, for people that are able to get an
Nvidia data science workstation, um, we are going to have the, uh, the software that's
going to be heavily based on rapids laid out, um, ideally so once you get the data science
workstation, um, it's loaded with the software that you need to have that, that reference
architecture will also refer to what we think the best, uh, hardware lay out is, um, and
we're just trying to, in another way, make GPU data science more, um, accessible to people.
Um, sometimes, uh, and it's a common project in data science circles to try to build your
own deep learning rig.
I think that's a great exercise, but it's not for everybody.
And I've been in some very, you know, um, serious corporate environments where IT is not
going to let you bring in the computer that you built to start working on their proprietary
data.
Right.
Right.
And the, the data science workstation initiative is, um, really about making it as easy
as possible for an organization that wants to dive into GPU data science to get started.
Cool.
Any parting thoughts from you on rapids or a QML or advice for folks who haven't really,
you know, been exposed to, uh, what Nvidia is doing on the software side that want to
explore more?
I would just really encourage, uh, everybody to take a look.
If you go to, um, rapids.ai, that's, uh, kind of a portal, uh, landing page that will
do somebody to everything that, um, I haven't covered here as well, some of the things that
I have, um, links to documentation, links to GitHub.
We've begun, uh, Google group, uh, we encourage everybody that, uh, touches rapids and find
something that they don't like or that doesn't work to file a ticket.
You can see, um, our roadmaps and our current work on GitHub.
And we really want, um, the community to be involved like, uh, you know, as I think about
the machine learning algorithms that I'm going to road map next for the team to develop
on, a lot of that has been informed by customer and community feedback.
And it's going to continue to be informed by customer and community feedback.
And so, um, I would just, you know, uh, ask anybody that, uh, is interested in taking
a look, um, you know, uh, please try to get involved with this because that's, that's
really what's going to measure the success of our project.
Uh, there's a real open source project and, um, we've done a great job of building community
so far.
We've got lots of stars and forks, but we want to see more of those and, uh, we're always
happy to see issues opened on GitHub.
Awesome.
Well, Paul, thanks for taking the time to share with us what you're up to.
Well, uh, thanks for having me.
All right, everyone, that's our show for today.
For more information on any of the shows in our GTC 2019 series, visit twimmaleye.com
slash GTC19.
Thanks again to Dell for sponsoring this series.
Be sure to check them out at delemc.com slash precision.
As always, thanks so much for listening and catch you next time.
