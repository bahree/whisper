Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting
people, doing interesting things in machine learning and artificial intelligence.
I'm your host, Sam Charrington Hey, what's up everyone? As many of you know, my work
involves understanding the way large companies are adopting machine learning, deep learning
and AI. While it's still fairly early in the game, we're at a really interesting time
for many companies. With the first wave of ML projects that early adopter enterprises
starting to mature, many of them are asking themselves how they can scale up their
ML efforts to support more projects and teams. Part of the answer to successfully scaling
machine learning is supporting data scientists and ML engineers with modern processes, tooling
and platforms. And now if you've been following me or the podcast for a while, you know that this
is one of the topics I really like to geek out on. While I am super excited to announce that we'll
be exploring this topic in depth here on the podcast over the next few weeks. You'll hear
from folks building and supporting machine learning platforms at companies like Facebook,
Airbnb, OpenAI, Comcast, Shell and more. And we'll be digging deep into the technologies
they're deploying to accelerate data science and ML development in their companies,
the challenges they're facing, what they're excited about and much, much more.
In addition, as part of this effort, I'm publishing a series of e-books on this topic.
The first of them takes a bottoms up look at AI platforms and is focused on the open source
Kubernetes project, which is used to deliver scalable machine learning infrastructure at OpenAI,
booking.com, matroid and many more companies. It'll be available soon on the Twoma website
and will be followed shortly thereafter by the second book in the series, which looks at
scaling data science and ML engineering from the top down, exploring the internal platforms
companies like Facebook, Uber and Google have built, the process disciplines that they embody
and what enterprises can learn from them. If this is a topic you're interested in,
I'd encourage you to visit TwomaLAI.com slash AI platforms and sign up to be notified as soon
as these books are published. All right, on to the main event. In this, the kickoff episode
for our AI platform series were joined by Aditya Calro, engineering manager at Facebook
to discuss their internal machine learning platform FB Learner Flow. Introduced in May of 2016,
FB Learner Flow is the workflow management platform at the heart of the Facebook ML engineering
ecosystem. In our conversation, Aditya and I discussed the history and development of the platform
as well as its functionality and its evolution from an initial focus on model training
to supporting the entire ML life cycle at Facebook. Aditya also walks us through the data science
tech stack at Facebook and shares his advice for supporting ML development at scale.
And now on to the show. All right, everyone. I am on the line with Aditya Calro. Aditya is an
engineering manager at Facebook. Aditya, welcome to this weekend machine learning and AI.
Thank you so much. It's great to be here. Awesome. Awesome. Why don't we get started by having
you tell our audience a little bit about your background? Sure. So I'm an engineering manager
on the AI infrastructure team at Facebook and I support a platform called FB Learner. I've been here
about three years and this is the project that I started with when I joined Facebook. So it's
something that's pretty near and dear to my heart. Fantastic. Fantastic. And as our audiences
probably guessed, FB Learner is really going to be the topic of our conversation today.
Why don't we start by having you tell us a little bit about the history of the project?
Sure. Like everything at Facebook, it grew organically. Facebook started using machine learning
as a way to provide a better experience to all of our users. And we realized that there was
certain really common patterns that we were seeing among the developers. It started with
binaries, which were hand-coded by developers and they were running on their dev boxes,
which meant that these developers couldn't do anything else while the training was running.
That's where the basic idea of FB Learner flow came from. It was to create a cloud of machines
that ML engineers could use and schedule their jobs on. And they didn't have to take care of
the machines. So they actually focus on actually making ML better. Now, what we ended up with was
something that evolved further and further into the platform that it is today. But that was the
basic idea. We wanted to make it easier for machine learning engineers to do what they did best.
And is FB Learner flow a particular feature or subset of a broader FB Learner or do you use
those interchangeably? It's the initial and the heart of the FB Learner ecosystem. It is a
little bit broader than just that. But it's what runs pretty much everything.
Okay. And it sounds like the initial focus was to provide an environment to get training off
of the developer workstations to centralize cloud or cluster. Does training remain the primary
focus of FB Learner today? It's the majority of the work that we do. That is correct. But there
is a lot more that it's actually doing today. So training just happens to be one of the things
that we're working on. It's expanded further into a generic workflow engine that does
stuff like workflow management across even build and push. So weirdly enough, we actually use
flow to push flow. And I know that sounds really meta. But the entire point of it is that we wanted
to make a genetic system and we succeeded in doing that. Are there specific types of workloads
more specific than generically machine learning that FB Learner is designed to accommodate or
does it span all of the ML workloads at Facebook? So it spans more than just the ML workloads.
One of the things that we do, I can't say that we have a specific target, right? It's meant to be
genetic. One of the things that we actually have, which in my opinion is really cool is that we
actually build Android, the Android app, the Facebook Android app on FB Learner Flow for regression
testing. We do actually target towards machine learning, but it's much bigger than that.
What drove, for example, this Android app to use it, was it kind of an organic, I've used this
for machine learning. It's here, it does interesting things, and maybe let's try to do it for
this Android app, or was there some specific set of features that have provided that weren't
available elsewhere in the kind of Facebook engineering ecosystem. I imagine Facebook has
very well defined builds, infrastructure build processes, that kind of things. What drove
this non-machine learning app to use this platform? Actually, it's a really good question. So ML
algorithms are typically workflows, right? There's some data prep followed by some training,
followed by some evaluation, followed by metric generation. The idea behind flow was to make
these workflows really flexible. So machine learning engineers could do whatever they wanted.
The other thing was that we wanted to be able to run any binary. We wanted to be able to expand
it to use any framework. It turned out that workflows are actually the most common way for pretty
much anything at Facebook or any batch workload at Facebook to be expressed, including build and push.
So it turned out that because we were able to run binaries, because we had a really good API
for workflow management, because we were able to run large workflows, workflows, they found it
really easy to expand their use case to use our Python APIs and just get off to a running start.
That's really interesting, really interesting. So you rattled off four distinct stages of the
data science workflow and those were data prep training and I missed what you said the third one
was? Well, evaluation and metric generation. I'm imagining then as a generic workflow engine,
that FP learner is used to support all four of these different steps.
And more, so the idea essentially is that if you are a developer, you can extend it to create your
own workflow step and that's exactly what the Android build system did. So the regression testing
system, they actually extended it to be able to write their own operator that could be executed
inside of the workflow. But I imagine given the system's roots as a tool for machine learning
developers and engineers, there are some specific features and capabilities that
lend themselves to those types of workloads. Is that correct? And that is correct. There are two
specific things. One is experiment management. So this is a significant part of where our UI
is helpful. Typically, what ends up happening for machine learning is that you start with a baseline
and then you keep doing experiments in order to improve it. You need to compare back to the
baseline and you need to be able to keep track of all of your experiments. Now, one of the other
initial things that we noticed was that developers were using Excel sheets to keep track of all of their
experiments. And we want to like a little bit better and to give them a mechanism to just point
and click at the experiments and do comparisons. And that's where the experiment management comes in.
The second part of it is because of the way that FP learner is involved, it think of it as
open source within Facebook. So pretty much whatever machine learning developers or the Android
developers or anybody in infrastructure writes, it's available for other people to use. So we've
actually got a pretty rich library of operators that people can reuse and build on top of.
And many of those are built by data engineers and by machine learning engineers.
On this experiment management point, what specific functionality does it provide? Can you walk us
through that in a little bit more detail? For example, there's capturing the results of different
tests and evaluation runs. There's possibly integrating in with code repositories and
versioning different models and the whole model management thing. There's an element to this
is possibly snapshotting data sets that are trained against. So you can kind of compare
the results relative to the training data sets. What's the scope of the experiment management
capabilities of FP learner? Sure. I think the easiest way to describe this is actually by one of
our guiding principles, which was flexibility. We basically wanted to make the system as flexible
as we could. So we created a mechanism for people to put in plugins. And you can actually do
comparisons across your workflow outputs. Now, one of your workflow outputs potentially could be,
let's say an AUC curve, right? Or for the Android engineers, it's the amount of time that it takes
to build the app. Or for somebody else, it could be potentially the size of the model. Now,
what we want to be able to do is say, okay, these are all outputs. And you get to choose this particular
experiment, which and compare it to another experiment that you've already done. That's one.
The other part of it. And this is exactly where the baseline and experimentation I was telling you
about earlier comes in. We want people to be able to clone things, clone an experiment from their
baseline. So you already have something that you started with. You've got a bunch of input
parameters that you've got built in there. And you want to be able to clone from the previous
things that you don't have to redo everything. That's one of the things that we provided.
This helps save a significant amount of time for engineers who are using the system.
The other thing that we also did was a bunch of input validation. So like I said, you have a lot
of input parameters, typically for machine learning. So for example, the number of trees,
how deep a tree should be of a neural networks, how what kind of model architecture you want. And
then it parameters all over the place to that. So one of the things that we built was a type system
that allows you to or allows us to be able to define this is an integer or a categorical feature
or something else. And we don't, if somebody puts in some, let's say a character or a name where
they're supposed to be an integer, the UI will actually want them right up front rather than letting the
the training or the experiment start and then want them later on. In order for FB learner to
manage the experiments and perform type checks against the different features, it is clearly
managing this process, the training process in this case for the developers. And I'm curious
how are the developers submitting their job parameters to FB learner? Is it via some UI? Is it via
you know, JSON files or configuration files? Some place, how does that interface work?
So I think that the process goes in two phases. There's one which is a Python API that we provide.
Now everything in flow is a workflow and workflows are composed of operators. Each operator is
has resource requirements, but it also defines in a large part what it's supposed to be doing.
So for example, it could be fetching or shuffling the amount, shuffling the data. It could be
actually training. It could be generating metrics and these are all different operators.
This, let's say I'm a developer, I'm actually going to write an operator for data fetching and
I'm going to pass it on to an operator for training and then I'm going to pass that operator on
to metric generation. This whole thing together is a workflow. We developers will check this in.
This shows up inside of the flow UI as something that you can invoke as you
when you start. You can invoke this directly from the UI.
Now the reason that we took this two step approach was because we wanted people to be able to
write their workflows and people on their teams or on other teams to be able to use it. So as you
can imagine, we're all about sharing and we actually do want teams to be able to share the work
that they've done with other teams. So as a user or flow, you can invoke my workflow from the UI
and you can point it to your data and you can point it. You can maybe even extend my workflow to
be able to generate the metrics that you want to be able to generate. Man, I imagine that that sharing
piece is a large part of the reason why you need this robust type checking. If you've got another
team that's using a workflow developed by a different team, they might not be as intimately
familiar with what the workflow expects in terms of input. That's actually one of the major reasons
that we need a type checking. But it's also we understand that systems and even algorithms are going
to live for a really long time. Teams change, people are added and that's one of the reasons that
we actually wanted to be able to provide a robust ecosystem for people to be able to use.
One of the possible, one of the things I alluded to earlier was the idea of tracking different
versions of these models or of training data sets in the context of managing experiments.
To what extent does FB learner get involved in that or is it delegating that out to
traditional repositories like get repositories or whatnot?
It's a little bit of both. What we do is create packages. For example, you promote
your package saying this is now ready to be production. That's when it shows up inside of
FB learner flow is a production package. There it does get versioned. We save some number of
versions for this. This depends on individual teams is to how far back they down the path they
want to go. That's one. If I remember correctly, you said versioning models. We actually version
on the basis of experiments. Each experiment has an ID that's associated with it and that's how
we versioned models. Do you do anything in terms of versioning the training data set associated
with a given experiment? We don't not specifically. This is typically done by the teams that are
using this themselves. The reason that we do that is because we have, like I said, we have a variety
of use cases. We're not just limited to one and each team has a different mechanism of doing.
Going back to the four steps in the machine learning process that you outlined, starting with
the data preparation step, there are a bunch of repetitive steps that fall under data preparation.
Are there standardized operators or I guess you could call them operators in this context for
different types of data preparation or, for example, data augmentation. Is there a standard
you said of data augmentations that a developer can pull in off the shelf or is each team
defining these by hand? This is exactly where that operator library that I was telling you about
comes in. People have written operators that can just be reused. A lot of teams want additional
augmented functionality that they write on top of these operators. There are some that we support
as the AI infrastructure team as well. This is something that is going to work with, let's say,
the data infrastructure team, infrastructure that they've developed, and we're going to be the
guarantee terms of that. There are, again, different places. Like I said, FPL and Open Source within
Facebook, the operator library grows significantly with, I can't see each passing day, but I can
have definitely 50,000 week a month. The operators are the primary function that you can plug in
to support this data prep. We can imagine things like off the shelf, data augmentation, or fetching
from different supported data repositories, things like that. How about on the training side? Can
you walk us through a little bit more detail on the training part of this process? It sounds like
a lot of this is wrapped up in the idea of experiment management, but what as a data scientist
or machine learning engineer, what specific requirements do I have for training that the platform
can take care of for me? Typically, this is more around resource management than anything else.
For example, let's say that one of the things that we've spoken about before is, let's say you
have a boosted decision tree that's piping into a logistic regression layer. Now, each of these
have different resource requirements. For boosting, you may require, let's say, a significantly
beefier machine. And for logistic regression, it's something that's a little bit lighter. So you
can specify the resource requirements for your boosting operator for your trees and say that,
okay, I need like a beefy machine that I need the entire machine versus for logistic regression,
I need a tiny machine that is capable, but I need it for a longer period of time. That's something
that we take care of right off the bat, but also it's moving data from one place to another.
You don't actually have to worry about which machines this is running on. This is our responsibility.
It's on our cluster. We will figure out the right place so that you get the machine and the computing
power that you need, but also so that we can stack additional jobs on the system as well.
Is the resource management built on top of an existing framework or platform like a Kubernetes
or is it built from scratch at Facebook? It's built from scratch at Facebook. We have our own
internal scheduling and resource procuring mechanism, and it's something that we've grown and
extended as necessary for Facebook's game. So we've talked about the data preparation phase,
we've talked a little bit about training and resource management for the different
training jobs. How about on the evaluation side, what are the key requirements there that the
platforms providing? It's actually very similar to training. It's primarily resources, but the other
thing that we do is we plug into a variety of backends so that we can actually distribute the load.
So you can use something like MapReduce or distributed evaluation in order to be able to run
things quicker. The other thing is that because of the scale that we have, you could potentially
end up running it very, very quickly, completely burn through a significant amount of compute,
but use it for a very short time so you can get your evaluation results quickly.
Is this kind of a cost optimization thing, whether they want their job to go as quickly as
possible, but burn a lot of compute resource or take longer on a best effort kind of basis?
Is that what we're talking about here? Yeah, that's exactly it. So you can end up bucketizing
your evaluation into significantly larger number of buckets. Okay, just based on time and business
factors. Exactly. Okay. And when we're talking about evaluation, are we talking about
evaluation as the tail end of a training cycle where you're evaluating the model that
the training has spit out against a broader set of data? Or are we talking about a model that
has been promoted into production and kind of managing the ongoing performance and
evaluation of the in production models or both? We're talking about both. So I'm going to try and
keep it, try and give you better nomenclature, at least nomenclature that we use. One is
evaluation for the test set. The other one is what we call back inference. When you say evaluation,
are you including both test set evaluation and inference evaluation? No, generally, I say
test set evaluation when I refer to the training process. We do have batch inference and real time
inference that are batch inference built on top of the system, real time inference based on
a different system. And we do support both of those as well. Well, we'll come back to that.
So I've got this model. I've trained it. I've evaluated it against the test set for both training
and evaluation. You're managing the compute resources dedicated to those tasks. And then we get
to metrics generation. What is metrics generation in this context? There is some standard metrics
that you always get. Learning curves, AUC, so on and so forth. The thing that is actually the
most powerful about flow, in my opinion, is the fact that this is entirely extendable.
So for example, let's say that you wanted to create a completely new type of metric and you
want to be able to plot that. We give you the resources necessary. So either you can use something
like sci-fi or one of the other Python based plotting mechanisms or you can write your own
JavaScript based plotting mechanism and just use our UI to surface it. So for example, when you're
looking at your experiment, this is all there for you. The other thing that we want, and this is,
if you wanted to write something completely esoteric, that is specific to you. If you wanted to
just use a standard graph, you can provide us all of your data and we'll plot it for you,
giving you the ability to actually do comparisons between your current experiment and compare it to
preview of previous baseline. I think that the power here is more in terms of its, again,
its flexibility in terms of its extendability and metrics change from team to team. So we can't
actually say we have a less set of metrics that are probably the common base set, but there are
tons of people who build on top of this. So you've talked quite a bit about the example of this
Android application. Are there some machine learning specific examples that you can walk us through
and how they take advantage of the various features of the platform? Sure, I think that the,
so for example, I don't know if you remember, but sometime ago we had this thing Facebook for
the blind, which was a computer vision based application. And we wanted to be able to recognize
trees, snow, skis, things like that. And there were a lot of metrics that were specific to this
that we built into the system. There were also a few things around training. So this was a deep
learning application. There were specifics around training that they did when they were using
flu. Maybe from an architectural perspective, how is the platform architected? How is it set up?
Is it and maybe a little bit more on the technology stack? It sounds like the API and a lot of the
components are in Python. Is it highly distributed or are there kind of centralized components? How
does that tend to work? Sure. I think that the best way to think about this is as a standard cloud.
Right. So at the top level, you have, okay, let's actually start from the bottom. At the bottom,
you have the core execution mechanism. And this is built on top of the scheduler on top of our
distributed package management and distribution framework. This is also built on top of several
storage layers that we have so that we can actually move data from one place to another place.
Above the core execution mechanism is where you have the API. This is what workflow authors
use significantly. How they describe workflows and operators. And then the translation layer in
between these, which is taking workflows and creating the DAG out of it so that the scheduler can
actually run it. We also have agents that run on our machines to keep track of which operator is
running and what state it's in and to make sure that the operator is running fine.
Above the workflow author level is the UI and experiment management layer. This is the place
where it'll significantly amount to the business logic clips. And this is based on our own metadata
store, which is in my sequel to keep track of all of the experiments. I think you alluded to Python.
And yes, a significant portion of this is written in Python. This we chose this language for
specific reason, both for the workflow authors API and for the system itself. This is something
that most machine learning engineers are very comfortable with. It's a language that I think
with PyTorch, with Cafe and with the circuit we've seen machine learning engineers get more and
more comfortable with it. That's the reason that we chose this language. Are there specific areas
that the platform decidedly doesn't address? I'm not entirely sure. Is there a specific area
that you're referring to? Because we're trying to be as generic as possible so it can be extended
to address most of not all areas. Okay. I guess I was thinking about the context of
opinionated versus not. Clearly you're targeting being very flexible and it sounds like
less opinionated, but I'm wondering if things have come up where the team has made a call to say,
no, while this comes up, it's not really in scope for this particular platform.
I think one of the things that we've historically done is made sure that we can make the platform
as robust and reliable as possible. So yes, there have been certain situations in which we said,
okay, we're going to punt on this particular thing for a little while. There are things that we definitely
know that we're going to support. Like for example, certain storage mechanisms that we know that
we're going to support certain others that don't meet our SLA requirements that we say that we
won't. Those are the decisions that we typically make. For the operators in this operator library,
I'm envisioning kind of like an app store for machine learning, data engineering, other
elements of these processes. What's the user experience for that and how do you,
I imagine discoverability and findability is a bit of a challenge if this library has gotten
quite large. How do you address that? Actually, your analogy is quite very, very apt. It is an
app store for operators. We've actually created an index. We index every operator that comes in
and we created a mechanism where we auto generate documentation from the code when somebody writes
an operator. We index all of that and fortunately, you're unfortunately that the operator library
has gotten very, very large. People do end up having to search through a significant portion of it.
We're hoping that a search and indexing mechanisms actually helps them quite a bit.
The other thing is that we do have internal forums where people discuss this and discuss how they
can use each other's work. Do you find situations where a team develops, I guess you may be alluded
to this earlier. You alluded to the notion of operators that are developed by individual teams
and operators that are officially supported. I imagine there have been situations where a team
developed an operator. They used it for their purpose but other teams had a adjacent needs and
maybe your team took it over, generalized it a bit and supports it. There are some
situations like that. I think they're relatively few. We specifically don't want to do that because
like I said, we aren't experts in machine learning. We are experts in building systems and that's
the reason that we try and make sure that whatever it is that's there on the system is supported by
the team that originally built it. We built a bunch of functionality to keep track of this. We
built a bunch of functionality so that if somebody does want to use it, they know who built it and
can get in touch with them instantly. From a systems perspective, if you are an enterprise or
other organization that is starting to think about how to industrialize machine learning operations,
doesn't already have a platform like this. What are the principles separate from maybe the details
of what you've done and how you've built it? What are the principles that they should be thinking
about when looking to support machine learning at scale? I can tell you what our principles were
and then hopefully that will generate really, really well. The first is actually make sure that
it's completely reusable. I've seen this happen several times where we built something the first
specific use case and then we realized that when the use case changed, we couldn't use it anymore
and that's one of the reasons that we made the system as generic as possible so that today maybe
the flavor of the week is to build your own binary. Tomorrow, the flavor of the week may be to use
PyTorch to expose neural networks. That's the current flavor of the week. The day after that,
it may be something completely different. We wanted to make the system as reusable as possible.
That's one. The second is to make it as comprehensive as possible. That's the reason that we actually
built the UI and the workflow operator library so that everything that we do inside of this system
is catalogued and it's tracked so that we can build on Docker better. That's the second one.
The third one, actually, you already said it, which is scale. We wanted to make sure that we were
at Facebook scale. We did several things including making sure that we were able to distribute
packages and distribute workflows and the data at a very, very large scale or at Facebook scale.
The other thing that we did was caching. We want what typically happens is especially when
you're doing experimentation, there is a possibility that you might reuse the results from a previous
experiment. We just cached the results of a previous experiment so that you don't have to run
it over and over again. Often the needs for pre-production, model development and training
are very different from the needs of production. It sounds like you're supporting both
development and prod with this platform. Can you talk about how you've managed the varying
requirements for those two modes? In principle, you're right. They're very different, but at the basic
level, they're about the same. We actually have a mechanism for dealing with canaries differently
from dealing with production packages. We don't go through all of the same steps as we do with
production packages. For example, with production packages, we're the ones that cached and distributed
across the entire cluster. With canaries, it is actually copied from the dev machine of the
developer that's building it onto the flow machine directly. It doesn't go through the entire
large full-blown step of going everywhere. This is a simple example of changes that we've done.
Are there examples of challenges that you've added the level above that kind of categories of
challenges that you've run into along the way in putting this platform together that someone who's
getting started down the path of building an environment to support data science and machine
learning engineering should be aware of? I think there are lots of challenges that we fix
one of the biggest challenges is actually when you were doing something that's
this flexible, make sure that you have all of the pieces for robustness built right into it.
This is one of the things that we've seen has been our biggest challenge because
we've grown significantly from three years ago and today there are portions of the system
that we redesign for robustness. Make sure that you're thinking about that up front.
That's the one piece of advice that I'll give anybody who's starting down this path.
So building for robustness and thinking about those kinds of issues from the very beginning.
Yeah, definitely. Any other thoughts in terms of challenges?
I think talk to your customers. If you're an enterprise application,
you typically are typically people who are going to be talking, who are going to be using your system.
One of the challenges that we've had is that we've created a mechanism for people to use and
then we realized that we had become the bottleneck for that mechanism.
So we have to constantly reinvent ourselves and disrupt ourselves.
So in the best way for us that we found to do that is to actually talk to our customers a lot
more often. Is there a specific example of that kind of reinvention or disruption that comes to mind?
So one of the biggest things is that people tend to use Excel sheets. I know I keep coming back to
that, but to use Excel sheet for end documents a lot. And as teams grow larger, they're paradigm changes.
When you're a single engineer who's working on one particular problem, you have all of the
context in your head. But when you go to a team, that becomes a hugely different proposition.
And we actually noticed that when as teams got larger, they had different interaction mechanisms.
So now we ended up having to talk to them saying, okay, how are you actually using this system?
And why are you using Excel sheets to be able to keep track of everything? Why are you sharing it?
This is where the experiment management actually comes in. I tend to think of our job as
making ML engineers more productive. And in order to do that, sometimes it's about robustness and
scale, but sometimes it's about their workflow, how they use the system. And that's exactly what we
should and will be focusing on. Fantastic. Any other final thoughts or words of wisdom for folks that
are thinking about these types of platforms? I think platforms in general are as useful as
the customers, as how the customers use them. So keep track of that. And this is a growing field.
Be ready to be disrupted and to disrupt yourselves. That's the only thing that I would say.
Fantastic. Well, I did you. Thank you so much for taking the time to share with us a bit of what
you're up to there. It's a really interesting stuff. Thank you so much. Thank you for having me.
All right, everyone, that's our show for today. For more information on Editia or any of the topics
covered in this episode, visit twimmalai.com slash talk slash 197. To learn more about our AI
platform series or to download our ebooks, visit twimmalai.com slash AI platforms. As always,
thanks so much for listening and catch you next time.
