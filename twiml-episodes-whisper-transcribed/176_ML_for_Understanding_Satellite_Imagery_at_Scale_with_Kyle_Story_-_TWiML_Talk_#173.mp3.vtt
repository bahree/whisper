WEBVTT

00:00.000 --> 00:16.320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

00:16.320 --> 00:21.480
people, doing interesting things in machine learning and artificial intelligence.

00:21.480 --> 00:31.480
I'm your host Sam Charrington.

00:31.480 --> 00:36.880
Today we're joined by Kyle's story, computer vision engineer at Descartes Labs.

00:36.880 --> 00:41.920
Kyle and I caught up after his recent talk at the Google Cloud next conference titled

00:41.920 --> 00:47.320
How Computers See the Earth, a machine learning approach to understanding satellite imagery

00:47.320 --> 00:48.960
at scale.

00:48.960 --> 00:52.520
Kyle and I discussed some of the interesting computer vision problems he's worked on

00:52.520 --> 00:58.480
at Descartes, including custom object detectors and the company's geo-visual search engine.

00:58.480 --> 01:02.520
We cover everything from the models they've developed and the platform they've built

01:02.520 --> 01:06.200
to the key challenges they've had to overcome in scaling them.

01:06.200 --> 01:07.200
Enjoy.

01:07.200 --> 01:10.760
Alright everyone, I am on the line with Kyle's story.

01:10.760 --> 01:15.160
Kyle is a computer vision engineer at Descartes Labs.

01:15.160 --> 01:18.160
Kyle, welcome to this weekend machine learning and AI.

01:18.160 --> 01:19.680
Thank you so much for having me on.

01:19.680 --> 01:20.680
How are you?

01:20.680 --> 01:21.680
I am wonderful.

01:21.680 --> 01:28.400
You are joining an esteemed group of physicists who have been on this podcast.

01:28.400 --> 01:34.800
It seems like there are quite a number of folks that come out of cosmology like you and

01:34.800 --> 01:40.520
astronomy and particle physics and make their way to machine learning.

01:40.520 --> 01:42.200
How did you do that?

01:42.200 --> 01:50.480
Yeah, it's actually a fairly natural transition so my background is in cosmology as you said

01:50.480 --> 01:54.720
which is a branch of astrophysics and so I spent almost a decade in graduate school

01:54.720 --> 02:01.280
and then as a postdoc at Stanford working on a mostly on it with data from a telescope

02:01.280 --> 02:06.800
that's in Antarctica where we were measuring what's called the cosmic microwave background.

02:06.800 --> 02:10.240
So this is radiation left over from the big bang.

02:10.240 --> 02:14.840
You can think of it as an afterglow of the big bang and you can actually map it and what

02:14.840 --> 02:21.840
you see in those maps are basically a baby picture of the early universe or variations

02:21.840 --> 02:27.160
in density, about 100,000 years after the big bang is the furthest back you can see

02:27.160 --> 02:34.520
in time and so I spent a decade studying this using the information and the statistics

02:34.520 --> 02:41.840
in those data to try to understand the universe as a system, model the universe as a whole

02:41.840 --> 02:47.520
and then relatively recently this past fall came out of academia to join day cart labs

02:47.520 --> 02:53.880
and basically instead of looking upwards using a satellite I'm now using a telescope on

02:53.880 --> 02:59.160
the ground and kind of inverted that problem so I'm now using data from satellites to look

02:59.160 --> 03:06.080
downwards at the earth and so it's a lot of there are a lot of similarities and a number

03:06.080 --> 03:12.360
of important differences as well so in one sense they are both remote sensing problems

03:12.360 --> 03:20.080
so they are both trying to learn something about phenomenon purely based on the light

03:20.080 --> 03:28.200
that you see or the signals that you're receiving from that source and then on the machine

03:28.200 --> 03:34.840
learning side before we go too far I've got to ask have you did you ever get to spend

03:34.840 --> 03:40.360
time in Antarctica like I'm envisioning I forget the movie but like the you know the

03:40.360 --> 03:45.320
one camp where you go basically under I don't know if it's on the ground probably not

03:45.320 --> 03:51.200
under under eyes but you go into this into this encampment was it anything like that

03:51.200 --> 03:58.080
it's it's not quite like that I have spent just shy of a year of time total in Antarctica

03:58.080 --> 04:03.480
that was spread across six different trips and so that the telescope that I worked at

04:03.480 --> 04:08.920
is aptly named the South Pole telescope and it is literally at the South Pole and so

04:08.920 --> 04:14.000
I would go there during the austral summer season to maintain the telescope set it up

04:14.000 --> 04:20.120
for observations and then most of our observations were taken during the austral winter so when

04:20.120 --> 04:25.280
you're at the pool there is one day and one night per year so I like to say that I've

04:25.280 --> 04:32.120
spent almost a year in Antarctica and I've never seen the sunset just kind of interesting

04:32.120 --> 04:37.880
but it it's it's a base that's funded by the National Science Foundation and the South

04:37.880 --> 04:42.080
Pole Station is fairly small during the summer it usually has a hundred to a hundred and

04:42.080 --> 04:47.640
fifty people who live there we do have a building that's heated so kind of very small

04:47.640 --> 04:52.760
dormitory style rooms and a cafeteria and then where there works we just work all the

04:52.760 --> 04:59.400
time but it's it's a really interesting environment to be in it's just a super cool place to

04:59.400 --> 05:05.200
be and I really value having had the opportunity to spend time down there it sounds like an

05:05.200 --> 05:11.960
incredible experience so you were recently at the Google next conference talking a little

05:11.960 --> 05:20.200
bit about the work you're doing at Descartes and one of the things that you made sure to

05:20.200 --> 05:25.920
touch on is really the types of problems that you encounter the computer vision problems

05:25.920 --> 05:31.160
that you encounter at Descartes and some of the unique characteristics of those problems

05:31.160 --> 05:34.080
can you maybe review those for us?

05:34.080 --> 05:41.840
Sure so the types of problems that we're working on in Descartes labs is trying to understand

05:41.840 --> 05:50.160
the world using machine learning and using satellite imagery and other geospatial or like

05:50.160 --> 05:57.560
georeference data sets so it's it's really is a big data problem and I know that's kind

05:57.560 --> 06:03.240
of a you know thrown around as a phrase that people like to use but the the size and scale

06:03.240 --> 06:10.800
of the data are are quite large and because of that it really does require automated analysis

06:10.800 --> 06:16.440
approaches and this is where the machine learning comes in is trying to instead of just

06:16.440 --> 06:21.480
saying we have a bunch of pictures let's set somebody down in front of computer to go

06:21.480 --> 06:28.440
look it through of all of those pictures that that doesn't really scale to the globe right

06:28.440 --> 06:34.760
so that's where that's where we take a machine learning approach where you say let's instead

06:34.760 --> 06:42.000
build an algorithms that can detect either objects or patterns and then we can automatically

06:42.000 --> 06:49.160
scale these to large regions so and just to give you a concrete or two concrete examples

06:49.160 --> 06:54.640
one of the places that Descartes labs started as a company was trying to understand corn

06:54.640 --> 07:01.360
production in the United States so being able to measure and predicts corn production

07:01.360 --> 07:07.040
over the the Midwest United States and then another example just to have something concrete

07:07.040 --> 07:13.080
is mapping infrastructure in a higher resolution is satellite imagery so for example being

07:13.080 --> 07:17.720
able to find just simply find buildings be able to map make maps of buildings and you'd

07:17.720 --> 07:23.920
be surprised how incomplete open source data sets are even on on basic infrastructure

07:23.920 --> 07:31.120
like this so the idea with that latter example is we don't have good lists of where the

07:31.120 --> 07:37.960
nuclear reactors are or the power stations or things like that so we need to actually regenerate

07:37.960 --> 07:44.240
those lists using satellite imagery so some of the specific types of buildings like nuclear

07:44.240 --> 07:50.800
power plants are pretty well listed but even just mapping you know residential buildings

07:50.800 --> 07:57.920
and industrial buildings and simply just knowing where they are the fast open source data

07:57.920 --> 08:02.520
set that I'm aware of his open street map and so some of your listeners may be familiar

08:02.520 --> 08:09.480
with that and it's an incredible data source but even in the United States when I'm looking

08:09.480 --> 08:14.680
at that data set I can go and pull up and see entire neighborhoods that just are blank

08:14.680 --> 08:20.720
just don't have anything filled in and that's largely because that community has primarily

08:20.720 --> 08:25.280
been based on people hand labeling things and largely volunteer effort and it's really

08:25.280 --> 08:31.240
impressive what they've been able to do but being able to scale that to an up-to-date

08:31.240 --> 08:35.720
map of buildings across the United States or even better across the world is really

08:35.720 --> 08:42.040
just not it's not up to the task of hand labeling all of you know segmenting out each building

08:42.040 --> 08:47.720
in a neighborhood and so so when you step outside of the United States of course it the

08:47.720 --> 08:53.800
problem changes as well in that there's just a lot less information in a lot of countries

08:53.800 --> 08:59.960
around the United States and so by training convolutional neural nets or other machine-land

08:59.960 --> 09:06.480
approaches we can take a first pass at being able to map out where buildings are and that

09:06.480 --> 09:11.400
provides an incredible amount of information that would just be unfeasible to achieve

09:11.400 --> 09:16.600
in a so person sitting in front of a computer screen hand labeling type of way.

09:16.600 --> 09:24.280
I think many listeners of the podcasts may be familiar with for example the planet

09:24.280 --> 09:32.600
data set on Kaggle kind of understanding the Amazon from space and I know for those

09:32.600 --> 09:39.240
of us who have worked through the fast AI course you kind of spend some time playing

09:39.240 --> 09:46.680
with that data set and learning to you know automatically classify tiles that are forested

09:46.680 --> 09:52.600
from tiles that contain water features things like that and so we may like come to this

09:52.600 --> 09:58.040
conversation say oh yeah sounds easy pretty standard stuff like bring us to the real world

09:58.040 --> 10:03.200
what's the you know what are some of the challenges that you deal with that you know the

10:03.200 --> 10:10.480
folks doing this on Kaggle don't have to worry about yeah sure so I so I come at this

10:11.120 --> 10:17.600
from you know from my physics background as it's all about the data and the information

10:17.600 --> 10:24.480
that's in that data and so the stepping away from machine learning a little bit at first

10:24.480 --> 10:32.480
you simply need to be able to access the data in an efficient way so that's and you know these

10:32.480 --> 10:37.200
datasets are quite large you know when you when you come to a Kaggle competition that that step

10:37.200 --> 10:42.320
has pretty much already been taken care of for you but there was a huge amount of effort in being

10:42.320 --> 10:49.440
able to get the satellite imagery and then have it quickly accessible in you know a way that you

10:49.440 --> 10:54.800
can incorporate into even starting to take a machine learning approach and so this is actually

10:54.800 --> 11:00.240
one of the things that Descartes labs realized fairly early on was that a lot of the power for

11:00.240 --> 11:05.200
being able to work with these datasets is going to come from having a platform that allows

11:05.200 --> 11:11.840
uniform access and quick access to these types of data so we one of the things that we have done

11:11.840 --> 11:17.440
and it has been a primary goal and continues to be a primary objective of the company is to

11:17.440 --> 11:25.520
build out a platform that provides access quick access to a huge amount of data we do so this is

11:25.520 --> 11:30.960
why we are at the Google next conference we've built out this entire platform within Google Cloud

11:30.960 --> 11:37.440
so it's so we pretty much don't own any hardware beyond our own laptops and are working almost

11:37.440 --> 11:43.200
exclusively within the cloud and what that gives us is being able to host a large amount of data

11:43.200 --> 11:48.880
in a way that we can access very quickly so that's that's one problem then another so then a more

11:48.880 --> 11:55.920
machine learning specific problem is the is the ground truth question right so when you come to

11:55.920 --> 12:02.560
a cargo competition oftentimes you have a lot of well labeled ground truth and how basically

12:02.560 --> 12:07.200
how much ground truth you have will dictate what types of algorithms you're able to use and then

12:07.200 --> 12:14.160
what types of problems you're able to solve so those are at least just two problems that

12:14.160 --> 12:20.560
present present themselves when you start dealing with questions in the real world on that first

12:20.560 --> 12:29.360
point the platform talk about I'm curious what that means for you and or rather I'm curious what

12:29.360 --> 12:36.880
that all that that encompasses you know is the platform you know just some set of Google Cloud

12:36.880 --> 12:46.720
services that you're using off the shelf or have you built a lot of capability on top of kind of

12:46.720 --> 12:53.440
raw level hardware that you're using via Google Cloud you know walk me through like what the

12:53.440 --> 13:00.720
the elements of this platform are sure so it's all the above everything you mentioned okay

13:00.720 --> 13:09.120
uh the so the first step is getting access to these data the so there are a number of openly

13:09.120 --> 13:15.360
available sources like Landsat or Sentinel generally government sources and they all have their own

13:15.360 --> 13:23.360
unique ways of accessing the data so in principle it's it's open freely available but each one has

13:23.360 --> 13:28.800
it you know each one has a slightly different format or has slightly different image registering

13:28.800 --> 13:35.040
or has been tiled in a different way so the first big step is ingesting all of those data and

13:35.040 --> 13:40.880
then hosting them in the cloud so figuring out how to access the data stream pulling out all of

13:40.880 --> 13:47.040
that data in uploading it into the cloud the next the the next step that the platform accomplishes

13:47.040 --> 13:54.800
is tiling those images across the globe so in so I as a because I'm working within the you know

13:54.800 --> 13:59.760
within our platform I don't have to think too carefully about how to stitch together a bunch of

13:59.760 --> 14:05.440
different images in order to make a picture that covers the region that I'm interested in that's

14:05.440 --> 14:12.560
all handled in an automatic way so a predetermined way of tiling the data uh that just simply removes

14:12.560 --> 14:18.880
one of the steps that I would otherwise have to deal with then there's the search question right

14:18.880 --> 14:24.480
so let's say I want to let's continue working with the building model example since that's the one

14:24.480 --> 14:32.080
that I've personally worked on directly so let's say I want to collect a set of imagery over

14:32.080 --> 14:39.760
California to use as training data how do I figure out which exactly which images I need to pull

14:39.760 --> 14:45.280
from you know from the giant database so we have a search capability that has where each

14:45.280 --> 14:52.160
each image that gets uploaded has a list of metadata and then we can using the google's big query

14:52.160 --> 14:57.280
we can or actually this is the last search we can search through that metadata and pull down

14:57.280 --> 15:03.200
exactly which images over which region over what time in a really efficient way and then turning

15:03.200 --> 15:09.760
those images from so it's from whatever format compress format they've been saved into into an

15:09.760 --> 15:15.840
actual image that I can applaud on my computer and then do things with and so this actually brings

15:15.840 --> 15:23.200
up a point that maybe we can get into a little bit later of how we think about these types of analysis

15:23.200 --> 15:30.720
and what I mean by that is thinking about imagery instead of something you pull up on your computer

15:30.720 --> 15:35.600
and then you flip through images kind of hand-level things or look at things thinking about that

15:35.600 --> 15:41.760
imagery as input to algorithms input to machine-learning algorithms and when you when you you start

15:41.760 --> 15:48.400
thinking in in those terms it's you definitely take a different approach to how you work with that

15:48.400 --> 15:54.240
data how you store it on disk how you make it available interesting so for example is that

15:54.240 --> 16:02.720
latter point meaning it as opposed to storing the images on disk as you know in a native image

16:02.720 --> 16:08.880
format your form your storing them as like TensorFlow records or things like that close it's that's

16:08.880 --> 16:16.320
right yes so we actually chose to store the majority of our data in jpeg 2000 and it's a the

16:16.320 --> 16:26.240
reason for doing so is it that format basically decomposes the images by scale so and and then

16:26.240 --> 16:32.080
allows you to use the correlations between different bands so these images you know can have

16:32.080 --> 16:38.160
spectral bands like red, green, blue, and year and red, swirr etc so you can take advantage of

16:38.160 --> 16:42.480
some of the fact that these bands are correlated to be able to compress them significantly

16:43.120 --> 16:51.120
and so when you say store them by scale is that in a sense of like a progressive image where you've

16:51.120 --> 16:58.560
got kind of a low resolution version of the entire tile and then like progressively more detailed

16:59.280 --> 17:04.880
tiles on a yeah yeah so it it allows you to do that without having to save duplication duplicate

17:04.880 --> 17:10.800
copies of the data right so if you're so if then you so this is one way that we're able to zoom in

17:10.800 --> 17:16.160
and out you know so on the visual side when a person is actually looking at it being able to zoom

17:16.160 --> 17:23.200
in and out quickly is taking advantage of that kind of layered structure of the way that the data

17:23.200 --> 17:32.800
is saved and then sticking to this theme of platform you kind of talked about these three steps I

17:32.800 --> 17:43.840
guess this ingest and the indexing and then the way you manage and manipulate these images I don't

17:43.840 --> 17:48.560
know if I captured exactly as the way you described it but you know looking at like the ingest

17:48.560 --> 17:54.480
side is that are you what are the different ways that you're getting the imagery like are you

17:54.480 --> 18:02.400
I don't recall if Google has a you know ship a hardware box type of feature like a AWS snowball

18:02.400 --> 18:09.520
but is that an issue for you are you mostly getting them online and landing them in cloud storage

18:09.520 --> 18:15.120
or something similar yeah it's the second it's the second option so we're pretty much always

18:15.120 --> 18:22.480
getting them online and then depending on the most of the data sources that we're interested in

18:22.480 --> 18:28.320
are data sources that are continually updating right because when you're thinking about what

18:28.320 --> 18:32.320
types of problems you actually want to use these data for a lot of it is going to be looking for

18:32.320 --> 18:38.320
change or monitoring what's what's new on the surface of the earth and so it's important to have

18:38.320 --> 18:45.360
as you know it's close to real-time data as the sources can provide so what so we're stepping

18:45.360 --> 18:51.600
a little bit outside my specific expertise but what those in general look like are pipelines

18:51.600 --> 18:58.240
that are built within the cloud that can at check for new imagery as that new imagery becomes

18:58.240 --> 19:05.200
available pulled into the cloud and then using the you know using parallel parallel compute they

19:05.200 --> 19:12.320
can process those imagery filter out all the metadata get that get that all set up and then store

19:12.320 --> 19:18.880
it into our cloud storage system so it can then be searched and accessed by a user like myself

19:18.880 --> 19:26.160
and then so from a machine learning platform once you've got all of this imagery in place what are

19:26.160 --> 19:31.600
some of the types of models that you're building and the specific problems that you're trying

19:31.600 --> 19:38.880
to solve so let's say a little bit in general about how I think about the data and then we can

19:38.880 --> 19:46.560
talk about some specific problems so with these data and there are kind of three dimensions sort of

19:46.560 --> 19:53.920
that are that provide at least me a useful framework for thinking about how to put together analyses

19:53.920 --> 19:59.920
by that I mean a spatial dimension a spectral dimension a temporal dimension so the spatial

19:59.920 --> 20:07.120
dimension is just where are you in xy and perhaps z position on the surface of the earth the

20:07.120 --> 20:14.080
spectral dimension is for each image you may have taken data in different bands or different

20:14.080 --> 20:21.760
frequencies so optical frequencies red and blue like a ccd camera would take near infrared imagery

20:21.760 --> 20:28.880
there's thermal bands so that that type of spectral information can be really important

20:28.880 --> 20:33.680
and then there's the temporal dimension so what I mentioned about seeing how things change

20:33.680 --> 20:39.440
over time for a given place on the earth and then given these three you can combine them in

20:39.440 --> 20:43.600
different ways to address different types of problems so to be concrete about that

20:43.600 --> 20:50.000
usually a computer vision problem where I've spent most of my time working is generally combining

20:50.000 --> 20:56.640
spatial and spectral information so you're trying to say find where things are on the earth by

20:57.520 --> 21:04.240
looking at these images you can also combine spectral and temporal information to say given a

21:04.240 --> 21:12.880
place I know so let's say like a crop field if I want to measure how that field is doing over time

21:12.880 --> 21:17.680
I'm probably going to be primarily using spectral and temporal information and then obviously

21:17.680 --> 21:23.120
you'll pull all these all together in different in different ways so should I jump into a

21:24.080 --> 21:31.680
specific example or two sure please yeah so on the computer vision side where one of the places

21:31.680 --> 21:38.080
that we're kind of starting is with mapping what you can see on the ground so this is related to

21:38.080 --> 21:44.480
your earlier comment about the the cargo competition on planet data so that you can think of that as

21:46.320 --> 21:51.520
basically making masks of where's forest where's water where other types of things like that

21:53.600 --> 21:59.200
taking that to the next step would be like like the building example that I said so being able to

21:59.200 --> 22:05.520
define an algorithm that you can run over imagery and it will return result of that algorithm

22:05.520 --> 22:12.880
will be a probability map where each just says for each pixel are you in a building or not so

22:12.880 --> 22:17.920
this is a semantic segmentation problem and we can use pretty pretty standard semantic segmentation

22:17.920 --> 22:25.280
tools and then there are also if you don't need to segment out entire image you just want to say

22:25.280 --> 22:33.280
where are all of the x in the world so an example of something we've done is found electric

22:33.280 --> 22:40.160
substations right so saying we even in the United States we do not there are not comprehensive

22:40.160 --> 22:46.400
lists of all the electric substations across the United States but you can see them in high

22:46.400 --> 22:51.600
resolution satellite imagery so in this case we were working with the freely available

22:51.600 --> 22:59.280
nape that's the national agriculture imagery program and so we can use an object detection approach

22:59.280 --> 23:05.440
to find where all the substations are if you want I can go into more detail about what that looks like

23:05.440 --> 23:12.720
yeah please sure so for let's just keep running with the the electric substation example

23:13.920 --> 23:20.080
for that one there are so from a machine learning approach you have you need to define your data

23:20.080 --> 23:25.280
you define your inputs they need to find the model architecture train and then run that model

23:25.280 --> 23:31.280
right this is kind of the life cycle of these types of problems so for here the input data as I

23:31.280 --> 23:39.040
said is high resolution imagery from nape so this is either one meter or 60 centimeters on a side

23:39.040 --> 23:45.680
per pixel so it's enough to actually be able to see the kind of the structure of an electric

23:45.680 --> 23:51.040
substation where you see the the wires coming off and you see the the stands that are holding

23:51.040 --> 23:56.560
pulling all the wires and the transformers and things like that there are enough substations

23:56.560 --> 24:02.880
that are labeled in open street map and so for this problem that makes it makes the ground truth

24:02.880 --> 24:09.200
fairly fairly straightforward we can just grab the locations of substations in open street map and

24:09.200 --> 24:12.560
for training we don't need to know where all of them are we just need enough to be able to train

24:12.560 --> 24:21.920
a good model and then you know using the platform I can create images and then I can burn those polygons

24:21.920 --> 24:28.800
that say where the the known substations are into a similar target image so I now have an image

24:28.800 --> 24:36.400
and a target pair that I can use for a supervised supervised learning approach and then we picked

24:36.400 --> 24:42.720
ssd model it's a single shot multi box detector model as our object detection model of choice

24:43.520 --> 24:48.400
and one of the reasons for this is also kind of an interesting that we might talk a little bit more

24:48.400 --> 24:58.640
about is this is taking advantage of transfer learning ideas so with ssd the so in two sentences

24:58.640 --> 25:03.120
for listeners who aren't familiar with it the way that algorithm works in general is you take an

25:03.120 --> 25:10.720
image you split it up into a set of predefined boxes for each of those small boxes you use a

25:10.720 --> 25:17.120
compost from a neural net to create a set of features and then the final step is basically

25:17.120 --> 25:22.480
predicting whether or not each box contains the object you care about so in my case an electric

25:22.480 --> 25:30.720
substation and we don't necessarily have enough examples to train something all the way from scratch

25:30.720 --> 25:38.960
but we can take advantage of in this case ResNet to be able to generate those features and then we

25:38.960 --> 25:43.920
only have to train you to fine tune those those features and train a final classification stage

25:43.920 --> 25:50.160
so even with a relatively small number of examples we're able to train an effective algorithm to

25:50.160 --> 25:57.680
locate these electric substations and so when you say ResNet ResNet trained on what image net or

25:57.680 --> 26:05.440
yeah exactly yeah exactly so actually I'm sorry I misspoke this one is the VGG network

26:06.160 --> 26:13.040
we do use ResNet in in similar contexts but in other models yeah so this one is using VGG as

26:13.040 --> 26:19.840
the neural net to produce features and then we are just fine tuning the last I guess training the

26:19.840 --> 26:24.800
last classification step to say whether or not each box contains the object you care about

26:24.800 --> 26:33.600
with VGG and ResNet and the like typically these networks are trained particularly the ones that

26:33.600 --> 26:38.720
are pre-trained on image net and the like are typically looking at relatively small

26:39.600 --> 26:46.880
image sizes and like three channel images you know 240 pixel square or something along those lines

26:47.520 --> 26:53.360
or what approaches do you use to apply those to these you know super high resolution images that

26:53.360 --> 27:01.680
you have yeah so for for this particular model we used 512 by 512 images and week so one

27:01.680 --> 27:08.160
one advantage even though we're trying to use this over large regions the earth is still in

27:08.160 --> 27:12.880
in some sense an embarrassingly parallel problem right as long as your objects are small enough that

27:12.880 --> 27:21.120
they fit into a tile you can chop up a region into you know almost an arbitrary a number of large

27:21.120 --> 27:28.240
number of small tiles so it so it still works quite well we even even with the constraints that you

27:28.240 --> 27:34.960
said and then in this case we are just taking so you mentioned the three bands that's actually

27:34.960 --> 27:39.680
that is an important difference between a lot of standard computer vision problems where you're

27:39.680 --> 27:45.680
just using a you know RGB image versus what I've out what I've mentioned here with satellite imagery

27:45.680 --> 27:52.560
in this case we are still just taking the I think I think I'm using an NIR and R&G bands in this

27:52.560 --> 28:00.080
case but in other approach and other algorithms that we worked on we have used basically data

28:00.080 --> 28:05.680
compression so you know PCAD composition things like that to be able to compress a larger number

28:05.680 --> 28:11.760
of bands down into three bands that will then work with these pre-trained feature generators

28:11.760 --> 28:20.240
huh that's interesting so in that case you've got you know some larger number of bands you've

28:20.240 --> 28:25.520
got your you know I don't know if for the satellite images the color bands a visual bands or

28:25.520 --> 28:31.280
three or more but you've got some number of visual bands and then you've got like these infrared

28:31.280 --> 28:38.480
bands how many total do you start within some of those cases it depends on the satellite so for

28:38.480 --> 28:45.200
example the sentinel satellite has 12 bands that extend from visible to a near infrared all the

28:45.200 --> 28:51.120
up into sphere bands that are sensitive to thermal emission so it yeah it can vary a lot

28:51.840 --> 28:59.920
so you've got you've got 12 bands and you're able to use PCA or some some kind of alternate

28:59.920 --> 29:09.600
projection of these bands down to three bands and yet a model that's trained on kind of visual

29:10.480 --> 29:18.880
image net objects still works yes and no it's it's it's surprising I mean because I'm

29:18.880 --> 29:24.800
imagining that the these visual I guess in one sense like that you know the the result of this PCA

29:24.800 --> 29:33.120
thing I'm not imagining that to really mean anything visually right and so your model that's been

29:33.120 --> 29:37.840
trained on this highly visual data you know in one sense I wouldn't expect to work on the other

29:37.840 --> 29:43.040
sense like all it's doing is learning a bunch of different kinds of patterns that's what you're

29:43.040 --> 29:47.920
counting on is that it learns a bunch of different types of patterns that themselves aren't necessarily

29:47.920 --> 29:53.760
visual so maybe it should work yeah exactly and it's it's a lot of trial and error right it's a lot of like

29:53.760 --> 30:01.360
hey this I don't know this might work let's you know let's do a small test and just see what happens

30:01.360 --> 30:08.240
and in some cases this type of transfer of compression has worked well and then in other in

30:08.240 --> 30:13.760
other cases I hopefully we'll talk about a geobigial search in a little bit there was insufficient

30:13.760 --> 30:19.600
and we had to do some further fine tuning to be able to get effective features so it just you

30:19.600 --> 30:25.040
know it's it's it's a lot of a lot of trial and error and a lot of simply digging into the data and

30:25.760 --> 30:31.120
doing a few tests seeing what works seeing what doesn't and then moving on from there you mentioned

30:31.120 --> 30:38.720
that you to get your your ground truth you start with this open street map data that has identified

30:38.720 --> 30:45.040
some number of electrical substations how many approximately are there that are identified

30:45.040 --> 30:51.040
so in the the ground truth set that I ended up with I had about a few thousand okay so it kind of

30:51.040 --> 30:55.600
gives you a gives you an order of magnitude yeah so you've got a few thousand of these electrical

30:55.600 --> 31:05.520
substations and you said you burn them into the images meaning you like your yeah just simply saying

31:05.520 --> 31:14.080
that I the output from OSM is a polygon that has lat long coordinates for to trace out a boundary

31:14.080 --> 31:19.360
and I'm just simply turning that into an actual image of zeros and ones ones with inside that

31:19.360 --> 31:26.880
polygon and zeros outside that line up with the corresponding image from the satellite and is

31:26.880 --> 31:35.360
that type of training data arrangement while you know we're on you've you've essentially colored on

31:35.360 --> 31:42.560
a pixel by pixel basis is that kind of what SSD is looking for so then for SSD there is one final

31:42.560 --> 31:49.920
step of then drawing a bounding box around the the ground truth object and so I guess I we

31:49.920 --> 31:56.240
probably could skip the burning the raster step to make it slightly more efficient but then the

31:56.240 --> 32:02.240
actual training is done against the coordinates of the well I guess at that point they're actually

32:02.240 --> 32:07.520
in pixel coordinates so we do need to go through the image to to get the pixel coordinates of the

32:07.520 --> 32:14.640
bounding box for the object of interest did that make sense yeah I mean it sounds like you could

32:14.640 --> 32:24.400
potentially do some kind of math going from a polygon from the OSM data set to pixel coordinates but

32:24.400 --> 32:30.720
the way you did it is by just manipulating the pixels on the image themselves yeah that's right and

32:30.720 --> 32:35.200
ends you know with a lot of these things it's a question of where do you invest your time

32:35.920 --> 32:41.120
absolutely and yeah so something like that yeah you could probably make that better but there's

32:41.120 --> 32:46.960
a thousand other things that are more important to to put my time in at my desk into

32:46.960 --> 32:58.480
so you've trained this model and I'm curious like are you with you mentioned some places you

32:59.120 --> 33:07.520
use VGG other places you use ResNet are you you know is it kind of you you know for some projects you

33:08.400 --> 33:13.520
do have you built up in an intuition around where you use specific things that works or is it

33:13.520 --> 33:19.280
more like kind of looking at the the rings in a tree like the you know this project is older and

33:19.280 --> 33:23.600
so you use VGG which was state of the art at the time and now if you were to do it again you'd use

33:23.600 --> 33:30.480
ResNet yeah there's there's a lot of that right exactly kind of exactly what you said you yeah

33:31.120 --> 33:38.400
you build up models as you go and then as new tools come come online and depending whether or not

33:38.400 --> 33:44.160
it's worth going back to fix up a model we'll dictate whether or not you go back and try out new

33:44.160 --> 33:48.960
tools or new feature generation I hope we'll talk about GeoVigil search in a bit and that's an

33:48.960 --> 33:54.400
example of that where I we've done something that's worked pretty well but then are now the process

33:54.400 --> 33:59.440
of going back using new tools using new data trying to make it better okay well let's do that before

33:59.440 --> 34:06.400
we do that have we covered all of the the important bits on the the substation identification

34:06.400 --> 34:11.920
but maybe one last thing just because it really with this being the google next series and relating

34:11.920 --> 34:18.480
to the cloud an important part of these models is once you're done with the model you need to be able

34:18.480 --> 34:24.720
to run it efficiently over large regions and so it's you know not machine learning specific

34:24.720 --> 34:31.360
necessarily but it's definitely cloud related so as I mentioned a few minutes ago the earth is

34:32.160 --> 34:35.760
somewhat of an embarrassingly barrel problem where you can just chop it up into a lot of little

34:35.760 --> 34:42.800
pieces and this really lends itself very naturally to a cloud environment so once we have a model

34:42.800 --> 34:48.960
then say I want to run over the United States I can just grab all of the imagery that to cover

34:48.960 --> 34:55.280
the entire United States chop it up into a little 512 by 512 pixel pieces and then hand a set of

34:55.280 --> 35:03.120
those and the model to a worker in the cloud and then I can spawn effectively an arbitrary number

35:03.120 --> 35:08.880
of workers to go process all of these data through the model and then save those results directly to

35:08.880 --> 35:17.360
the cloud so just by by working in that highly parallel fashion that's how we're able to automate

35:17.360 --> 35:22.320
analyses and run them in a recently you know a reasonably quick amount of time so like I've been

35:22.320 --> 35:27.840
able to I mentioned the building detector I was able to run that over the entire state of California

35:28.480 --> 35:32.720
kind of overnight and so I come back and work in the morning and I have a map of all the buildings

35:32.720 --> 35:40.320
across the state so that's just the kind of the final piece of the model life cycle and I even with

35:40.320 --> 35:49.520
that final you know that specific piece and you know the the constraints you mentioned of taking

35:49.520 --> 35:54.960
this model handing it to something that lives in the cloud and having it run in a distributed fashion

35:54.960 --> 36:03.360
and even within the confines of Google's cloud offerings there are still probably I can think of

36:04.080 --> 36:10.000
five ways off the top of my head to do that specific thing like do you get involved in the

36:10.640 --> 36:17.360
infrastructure pieces of putting that all together or maybe more generally how have you

36:17.360 --> 36:24.400
architected this distributed execution or training is it like is it one of the higher level you know

36:24.400 --> 36:32.720
the machine learning engine type services or is it all running on just cloud engine workers

36:32.720 --> 36:41.040
that you guys build up and manage or is it running on Kubernetes yeah so we so the engineers and

36:41.040 --> 36:49.120
our team have built up a system in Kubernetes that are using primarily preemptible VMs and then

36:49.760 --> 36:55.680
being then they have we've written our own task scheduling software that will and then

36:55.680 --> 36:59.520
so we've written our own task and new software that will launch jobs into the into

36:59.520 --> 37:05.840
criminal VMs all of the work environment so capturing the code and then all you know all the

37:05.840 --> 37:11.120
different dependencies things like that using Docker and then inside of Kubernetes and so then

37:11.120 --> 37:18.240
you can set up a environment in each of those workers run the code save your results to the cloud

37:18.240 --> 37:25.440
and then break that all down so it's all it's all Kubernetes-based we're now working on getting

37:25.440 --> 37:31.920
Istio which is one of the new monitoring packages out of Google to be able to monitor all of these

37:31.920 --> 37:39.200
processes in an effective and effective way so and then we're we we're not using the

37:39.920 --> 37:45.200
ML engine yet but it's something that we're looking into and are you also looking at the

37:45.920 --> 37:52.000
Qflow package for Kubernetes yeah so that is that that's also something that we're looking into

37:52.000 --> 37:57.440
at this point so far we're running you're basically running our own models we're largely

37:57.440 --> 38:02.960
cares models but yeah Qflow is something that we're starting to look into now how long has the

38:02.960 --> 38:10.400
platform been around how long have you been working with Kubernetes so the company in total has

38:10.400 --> 38:18.080
been around for about three years so we're pretty pretty company and I'm not on the engineering team

38:18.080 --> 38:22.560
but we've been working with Kubernetes for for a while so I think most of that time

38:22.560 --> 38:30.080
cool yeah well we won't drill into necessarily the Kubernetes details here but I may be interested

38:30.080 --> 38:36.320
in talking to someone on the engineering side as well just for background we talk through the

38:37.040 --> 38:42.480
substation detection problem and you've been chomping at the bit to tell us about the geovisual

38:42.480 --> 38:48.160
search tell us about that problem sure yeah so this kind of chomping at the bit just because I think

38:48.160 --> 38:56.080
it's cool and interesting and a little bit unique so the idea with visual search is to be able to

38:56.640 --> 39:03.040
click on anywhere on the earth and then be able to see other areas that look like this so at a high

39:03.040 --> 39:10.240
level you can think of it as kind of a generalist model that hasn't been trained to find anything

39:10.240 --> 39:16.960
specific but is able to you know is able to search over a large region for lots of different

39:16.960 --> 39:24.160
types of things on so then the way it actually works under the hood is kind of cool so maybe I'll

39:24.160 --> 39:30.400
take a few minutes to walk through that now yeah please cool so the the way it works under the hood

39:30.400 --> 39:35.520
pretty actually fairly similar to many of the ideas that we've been talking about so far in this

39:35.520 --> 39:42.400
podcast take a set of data so in our first pass we've worked with either Nape that I've mentioned

39:42.400 --> 39:49.120
before over the United States or compositive Landsat over the entire globe chop that imagery up

39:49.120 --> 39:58.560
into a bunch of 128 by 128 images and then use a ResNet based feature generator to create a set of

39:58.560 --> 40:05.440
features and then simply search for similarity between features for each tile so so when you click

40:05.440 --> 40:11.680
on a tile it's doing a similarity search to say what other tiles have similar visual features

40:11.680 --> 40:17.440
so this this touches a number of the things that we've already mentioned on this podcast so far

40:17.440 --> 40:23.680
the first one being whether or not the ResNet features are sufficient so in this case they we found

40:23.680 --> 40:30.480
that the features that are generated by ResNet were not good enough to be able to have a really

40:30.480 --> 40:36.800
clean visual search and so we did actually do some fine fine tuning of those weights basically

40:36.800 --> 40:43.840
for Nape where it's high resolution enough to you'll see a lot of objects we took an approach of

40:43.840 --> 40:49.840
grabbing a bunch of different objects from osm again from the open street map and then doing a

40:50.800 --> 40:57.600
supervised fine tune training of the features to be able to match to osm objects that just kind of

40:57.600 --> 41:03.760
cleaned up what types of features that the network was generating and so specifically to

41:03.760 --> 41:12.000
drill in on that point for a second what you mean is that the features that the pre-trained

41:12.000 --> 41:21.440
presumably on ImageNet model was had as opposed to the architecture itself you weren't ever

41:21.440 --> 41:26.720
training from scratch this was all we're still talking about pre-trained ImageNet models exactly

41:26.720 --> 41:33.120
that's exactly right so yeah taking advantage of all of that that computation and pre-training

41:33.120 --> 41:40.480
that went into creating the ImageNet based ResNet weights and then just fine tuning those

41:40.480 --> 41:48.000
and I'm curious can you provide us some scope or order of magnitude of the fine tuning effort

41:48.000 --> 41:54.160
just to get a sense for you know how much you benefit from someone else doing the heavy lifting

41:54.160 --> 42:00.480
of training ImageNet I don't really have a good way of quantifying that at the moment yes

42:00.480 --> 42:11.840
are so you've you start with the pre-trained ResNet model on ImageNet you you fine tune and then

42:11.840 --> 42:19.440
when you talk about using the features of this ResNet model are you basically chopping off the

42:19.440 --> 42:26.480
classifier at the end and using that last layer or you're using intermediate layer activations or

42:26.480 --> 42:33.840
something like that yeah so we're we're basically just chopping off that last classification layer

42:35.040 --> 42:43.200
and then an additional step there is if we store all of those features the features numbers in

42:43.200 --> 42:48.720
their full size I would take up a future amount of disk space so we've actually binarized those

42:48.720 --> 42:55.760
features by injecting a bunch of noise at the end of the training process to basically force the

42:55.760 --> 43:02.000
those features to choose between either zero one and then in the you know in the final network we

43:02.000 --> 43:11.200
threshold it so the output of running a tile through the trained network is a list of 512 bits

43:11.200 --> 43:18.800
zero ones and that's so that's at the cost of the you'd be basically the precision of being

43:18.800 --> 43:25.120
able to differentiate between so we lose a little bit of differentiation power but we gain a lot

43:25.120 --> 43:31.040
in both speed for the search later and in terms of having it take up a reasonable amount of

43:31.040 --> 43:37.600
disk space can you elaborate on on that process the noise injection process and how that translates

43:37.600 --> 43:44.880
to allowing you to isolate these individual pixels yeah sure so at the so basically at the

43:44.880 --> 43:50.240
the last step we injected noise during training with an apple too that was comparable to the

43:50.240 --> 43:56.560
width of the layers activation function and so this means that the the network needs to either

43:56.560 --> 44:01.920
basically either decide you know the activation needs to either be one or zero otherwise there's

44:01.920 --> 44:06.800
enough noise to kind of destroy the information that the layer is trying to pass on so it forces

44:06.800 --> 44:13.120
the network to learn to output either very close to one or very close to zero and then once we have

44:13.120 --> 44:18.800
that then we can just once that's trained then we can in the final model we can just add a

44:18.800 --> 44:25.520
thresholding step does that make sense and you're injecting that noise at the last layer itself as

44:25.520 --> 44:32.240
opposed to in the input is by manipulating the image or that's right at the okay yeah it's just at

44:32.240 --> 44:38.000
that last step oh it's interesting um is there a name for that technique not that I'm aware of

44:38.000 --> 44:44.640
they're they're very well maybe okay I'm not aware of a specific name okay at that last layer of

44:44.640 --> 44:52.080
resonant how many features are there so we're we're ending up with 512 features per input image

44:52.880 --> 44:57.120
so you're what you're what you're trying to do is it is not you're not trying to necessarily

44:57.120 --> 45:04.080
reduce the number of features you're trying to basically compress them on off yeah that's right

45:04.080 --> 45:10.720
got it exactly okay and then just to complete because it's also interesting idea for the

45:10.720 --> 45:18.240
Landsat dataset this same fine tuning process but instead of using osm uh there aren't really

45:18.240 --> 45:23.360
uh enough useful information in osm around the across the entire globe so we used took an auto

45:23.360 --> 45:31.520
encoder approach basically um and use that as a way to to fine tune the um satellite imagery

45:31.520 --> 45:38.880
future generation for the Landsat dataset and what's the relationship between the fine tuning process

45:38.880 --> 45:45.760
and osm or the you know we'll get to the auto encoder but how does the osm data help you with

45:45.760 --> 45:51.920
the fine tuning it just makes the the features that you're generating more responsive to the

45:51.920 --> 45:56.080
the types of things you've seen satellite imagery so it kind of exactly what you mentioned

45:56.080 --> 46:02.000
earlier where image net is trained on you know a bunch of pictures that may or may not

46:02.000 --> 46:07.040
have the size and shapes of the types of things that you're interested in in satellite imagery

46:07.040 --> 46:13.040
this just allows us to fine tune those weights a little bit to uh make them more descriptive about

46:13.040 --> 46:20.160
the the types of visual features that we see you know in satellite imagery so does that mean

46:20.160 --> 46:26.640
I mean you've got satellite imagery in your original data um it is what you're doing

46:26.640 --> 46:31.760
are using osm to give you landmarks so that you can train on specific tiles that you know are

46:31.760 --> 46:36.800
interesting as opposed to training on a bunch of ocean or something like that or is it more

46:36.800 --> 46:42.960
nuanced than that it's partially that but it's more recognizing the fact that

46:42.960 --> 46:49.360
up overhead picture of you know a city park and a golf course that has doesn't look exactly the

46:49.360 --> 46:54.960
same as a picture of a cat or a dog and so the the features that a network learns to consider

46:54.960 --> 47:00.640
interesting from one of those images won't be exactly the same as the features that a network

47:00.640 --> 47:05.360
learns to consider interesting from the other one and so it's it's really more accounting for

47:05.360 --> 47:10.560
that type of difference knowing that we're going to be applying it to satellite imagery so trying

47:10.560 --> 47:17.040
to just fine tune a little bit uh what information the network is pulling out of those images

47:17.040 --> 47:24.400
okay uh yeah so you're you're doing this before you take off the classifier and you're training

47:25.360 --> 47:32.160
on osm tiles because you've got labels for them exactly got it I missed that part I was thinking

47:32.160 --> 47:38.320
we'd already chopped off the the classifier and so how do you use an auto encoder to do that

47:38.320 --> 47:44.800
you're basically doing it in more of an unsupervised kind of way yeah exactly that's right so because

47:44.800 --> 47:53.440
the Solanza imagery is a lot lower resolution 15 15 meters per pixel instead of one meter per pixel

47:53.440 --> 47:59.120
so the labeling from the osm simply wasn't as useful and then additionally there weren't as many

47:59.120 --> 48:06.000
labels around other go in other continents around the globe and so we still need to so we use an

48:06.000 --> 48:12.720
auto encoder to uh basically compress those images so compress the final image net features down

48:12.720 --> 48:19.920
into the same number of 512 feature bits so you so in that case the auto encoder is we're basically

48:19.920 --> 48:27.680
using it as an compression algorithm to get from the output image net features to the small number

48:27.680 --> 48:33.840
of binary features that we want to use for the visual search cool so yeah so then the result of

48:33.840 --> 48:41.520
that process is we've now tiled up the United States and the earth into small tiles and we've trained

48:41.520 --> 48:48.560
a algorithm to create visual features for each of those tiles and so now the the search part of

48:48.560 --> 48:56.800
the visual search comes in uh so the basic idea there is to you want to look for tiles that are

48:56.800 --> 49:03.680
close in feature space and so we took kind of a two-step approach to making this work first

49:03.680 --> 49:10.400
we used a hamming distance as kind of a first pass filter so this is just simply looking at the

49:10.400 --> 49:14.160
difference you're just lining up the bits and looking for differences between bits and that

49:14.160 --> 49:21.120
allows you to zoom into a smaller number of of candidates that may be visually close to the tile

49:21.120 --> 49:31.120
that you're searching for and then as a final step for the Landsat data the that first first step

49:31.120 --> 49:37.920
was enough to limited it down to where we could simply do a direct search of tag because it's just

49:37.920 --> 49:45.200
comparing bit by bit a group for search over all the the close images and then return return the

49:45.200 --> 49:52.320
image that's closest in feature space for the NAPE data set which starts with about two billion

49:52.320 --> 49:58.560
of these little tile images that was it still a director works but it's too slow for interactive

49:58.560 --> 50:03.840
use so we needed to do something a little more creative to be able to make this so you could click

50:03.840 --> 50:09.600
on something on a website and have have information come back to you in real time and so we used

50:09.600 --> 50:17.280
the approximation method called bit sampling which is basically takes a set of 32 hash functions

50:17.280 --> 50:26.000
and then it hashes sets of bits to reduce down that 512 feature bit vector into smaller chunks

50:26.000 --> 50:33.440
and then looks for images that have similar outputs of that hash function and then you can do a

50:33.440 --> 50:40.960
group for a search of that second filtered set of images did did that make sense it does I'm

50:41.760 --> 50:50.000
curious whether the notion of using some kind of projecting this feature space into some kind of

50:50.000 --> 50:56.240
embedding layer is that does that make sense in this context and using like the distance in

50:56.240 --> 51:03.600
an embedding layer to do image similarity that I think that approach could make sense yeah this

51:03.600 --> 51:09.760
this type of search is basically what we landed on first but an embedding approach could also work

51:09.760 --> 51:20.480
well okay interesting so yes so then the output of that is we now have a search capability where

51:20.480 --> 51:27.040
you can click on a little tile and get back results in other tiles that look visually similar

51:27.040 --> 51:32.400
basically in real time it's you know public available on our website at search.dakerlabs.com

51:32.400 --> 51:40.720
you can go play around with it if you want and so it forms a it's not a it's as I said in intro

51:40.720 --> 51:45.040
it's kind of a generalist model that's looking for things that look visually similar

51:45.040 --> 51:52.960
and then if so for some types of problems that's enough for other types of problems where you

51:52.960 --> 51:57.200
you want a higher precision recall in the objects that you're detecting or the objects aren't

51:57.200 --> 52:04.080
quite as visually striking then you need to take a more you know expensive more but more standard

52:04.080 --> 52:09.040
computer vision approach with like the electric substation as I mentioned before for example

52:09.040 --> 52:16.160
I'm curious with the the geovisual search have you developed a way to

52:17.120 --> 52:25.920
characterize the accuracy of the end to end process like you're you know the performance metrics

52:25.920 --> 52:30.880
that you would typically look at for a training process are you know don't really apply because

52:30.880 --> 52:37.040
you're kind of shifting the the use domain and I'm wondering if there's a way to measure

52:37.040 --> 52:44.320
the extent to which this feature similarity maps to what people think is similar or what people

52:44.320 --> 52:49.280
want to see like you know is it figuring out built you know tiles that have the same number of

52:49.280 --> 52:54.000
buildings or tiles that have some you know maybe there's something that's not really you know

52:54.000 --> 53:01.040
that doesn't jump out at us that causes feature similarity but it's not but it's not necessarily

53:01.040 --> 53:05.600
what an analyst wants to see when they're clicking around how do you characterize that performance

53:05.600 --> 53:12.960
so we haven't done anything quantitative in what you described you certainly could say I have

53:12.960 --> 53:18.400
a class I care about let's see whether or not a visual search you you could calculate a precision

53:18.400 --> 53:24.800
recall for if you had basically a labeled data set that you could check against and here it's

53:25.360 --> 53:31.520
you know we we haven't done that type of quantification of the process and because that'll also

53:31.520 --> 53:37.600
depend a lot on which object you choose you know for example so we're still we thought about

53:37.600 --> 53:43.840
this more as just kind of a general first pass search rather than a quantifiable like precision

53:43.840 --> 53:47.760
recall toward right right if that makes sense but yeah it's I mean it's definitely the type of

53:47.760 --> 53:53.920
thing that you you could do we just haven't performed that exercise with any specific examples

53:53.920 --> 54:02.000
and you mentioned that that's you know once a user cares about that type of precision you would lean

54:02.000 --> 54:08.160
more on the type of process that we described earlier where you're modeling around a specific

54:08.160 --> 54:14.480
type of entity or object and it sounds like you know presumably you what you have found is that

54:14.480 --> 54:19.440
you know when people click around in this map they tend to get results that are appealing to them

54:19.440 --> 54:26.800
that are that satisfy the base promise of the system yeah exactly so it's it's it's it's kind

54:26.800 --> 54:32.880
of an interesting I think of it as an interesting starting place or you can even say if if I'm

54:32.880 --> 54:39.760
thinking about investing time and therefore money into a more exact model do you will are there

54:39.760 --> 54:44.160
enough features that will look you know visually distinctive do I have a good chance of being able

54:44.160 --> 54:51.120
to build an effective model but then yeah for for a solution where you really want a quantified

54:51.120 --> 54:56.560
precision then taking one of the other approaches or object detection or semantic segmentation

54:57.280 --> 55:01.920
and building a specialist model for the exact thing you're looking for has almost always

55:01.920 --> 55:07.680
been more much more effective great well just as we kind of wind down I appreciate you being

55:07.680 --> 55:12.080
flexible with time here run over a little bit but this has been really interesting are there any

55:12.080 --> 55:17.280
other things that you'd want to leave folks with I just thanks for having me on it's been

55:17.280 --> 55:22.160
fun to discuss these things and if people want to play around with some of these things go check

55:22.160 --> 55:26.400
us out awesome well thank you Kyle thanks so much for taking the time to chat with me about this

55:26.400 --> 55:36.160
stuff all right thank you all right everyone that's our show for today for more information on Kyle

55:36.160 --> 55:43.680
or any of the topics covered in this episode head over to twimmol ai.com slash talk slash 173

55:43.680 --> 56:07.840
as always thanks so much for listening and catch you next time

