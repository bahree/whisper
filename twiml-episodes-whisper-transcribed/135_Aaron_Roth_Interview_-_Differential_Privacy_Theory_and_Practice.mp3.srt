1
00:00:00,000 --> 00:00:16,320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

2
00:00:16,320 --> 00:00:21,480
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:21,480 --> 00:00:32,640
I'm your host Sam Charrington.

4
00:00:32,640 --> 00:00:36,920
This week on the podcast, I'm excited to present a series of interviews exploring the emerging

5
00:00:36,920 --> 00:00:40,160
field of differential privacy.

6
00:00:40,160 --> 00:00:44,200
Over the course of the week, we'll dig into some of the very exciting research and application

7
00:00:44,200 --> 00:00:47,800
work happening right now in the field.

8
00:00:47,800 --> 00:00:53,120
In this, our first episode of the series, I'm joined by Aaron Roth, Associate Professor

9
00:00:53,120 --> 00:00:58,400
of Computer Science and Information Science at the University of Pennsylvania.

10
00:00:58,400 --> 00:01:03,200
Aaron is first and foremost a theoretician, and our conversation starts with him helping

11
00:01:03,200 --> 00:01:07,800
us understand the context and theory behind differential privacy, a research area he

12
00:01:07,800 --> 00:01:11,240
was fortunate to begin pursuing at its very inception.

13
00:01:11,240 --> 00:01:15,560
We explore the application of differential privacy to machine learning systems, including

14
00:01:15,560 --> 00:01:19,000
the costs and challenges of doing so.

15
00:01:19,000 --> 00:01:23,720
Aaron discusses as well quite a few examples of differential privacy in action, including

16
00:01:23,720 --> 00:01:28,560
work being done at Google, Apple, and the US Census Bureau, along with some of the major

17
00:01:28,560 --> 00:01:34,160
research directions currently being pursued in the field.

18
00:01:34,160 --> 00:01:38,080
Thanks to George and Partners for their continued support of the podcast and for sponsoring

19
00:01:38,080 --> 00:01:39,680
this series.

20
00:01:39,680 --> 00:01:44,160
George and Partners is a venture capital firm that invests in growth stage companies in

21
00:01:44,160 --> 00:01:46,400
the US and Canada.

22
00:01:46,400 --> 00:01:50,960
Post investment, George and works closely with portfolio companies to accelerate adoption

23
00:01:50,960 --> 00:01:56,720
of key technologies, including machine learning and differential privacy.

24
00:01:56,720 --> 00:02:01,400
To help their portfolio companies provide privacy guarantees to their customers, George

25
00:02:01,400 --> 00:02:07,080
and recently launched its first software product, Epsilon, which is a differentially private

26
00:02:07,080 --> 00:02:09,360
machine learning solution.

27
00:02:09,360 --> 00:02:14,120
You'll learn more about Epsilon in my interview with George and Chang Liu later this week,

28
00:02:14,120 --> 00:02:18,120
but if you find this field interesting, I'd encourage you to visit the differential

29
00:02:18,120 --> 00:02:26,880
privacy resource center they've set up at gptrs.vc slash twimmel ai.

30
00:02:26,880 --> 00:02:30,040
And now on to the show.

31
00:02:30,040 --> 00:02:35,600
Alright everyone, I am on the line with Aaron Roth.

32
00:02:35,600 --> 00:02:40,600
Aaron is Associate Professor of Computer Science and Information Science at the University of

33
00:02:40,600 --> 00:02:41,600
Pennsylvania.

34
00:02:41,600 --> 00:02:44,400
Aaron, welcome to this week in machine learning and AI.

35
00:02:44,400 --> 00:02:45,920
Thanks, I'm glad to be here.

36
00:02:45,920 --> 00:02:49,120
Why don't we get started by having you tell us a little bit about your background and

37
00:02:49,120 --> 00:02:53,400
how you got involved in the machine learning field?

38
00:02:53,400 --> 00:03:00,200
Sure, so I'm a computer scientist and I guess I would describe my background as really

39
00:03:00,200 --> 00:03:02,720
coming from theoretical computer science.

40
00:03:02,720 --> 00:03:10,080
So as someone who sits down and tries to understand things by thinking mathematically and proving

41
00:03:10,080 --> 00:03:19,000
theorems and the way I came to machine learning in general is well from a background in learning

42
00:03:19,000 --> 00:03:27,600
theory and in particular the flavor of problems that I've studied both sort of historically

43
00:03:27,600 --> 00:03:34,920
and now have to do with the way that machine learning as a technology interacts with things

44
00:03:34,920 --> 00:03:38,840
that you might more normally think of as societal concerns.

45
00:03:38,840 --> 00:03:45,240
So things like privacy, things like fairness and things that maybe more typically an economist

46
00:03:45,240 --> 00:03:51,600
would think about like how do machine learning algorithms work in strategic situations?

47
00:03:51,600 --> 00:03:58,720
You're also very involved in the work happening around the application of differential privacy

48
00:03:58,720 --> 00:04:01,880
to machine learning.

49
00:04:01,880 --> 00:04:04,920
How did you get started down that route?

50
00:04:04,920 --> 00:04:12,040
Well, so I started my PhD in 2006 which is the same year that the first paper on differential

51
00:04:12,040 --> 00:04:18,200
privacy was published by Cynthia Dwork and Cobine Sim and Adam Smith and Frank McCherry.

52
00:04:18,200 --> 00:04:24,480
And so this was a very new topic at the time that I was just starting to think about

53
00:04:24,480 --> 00:04:34,320
research and it struck me as timely and important and at the same time when I was just starting

54
00:04:34,320 --> 00:04:36,800
to think about it, not much was known.

55
00:04:36,800 --> 00:04:42,440
So it was sort of at the sweet spot for PhD thesis where you can study an important problem

56
00:04:42,440 --> 00:04:46,320
and have a lot of impact without having to do anything to clever.

57
00:04:46,320 --> 00:04:53,320
Well, maybe a good place to start in our exploration of differential privacy and machine learning

58
00:04:53,320 --> 00:04:59,000
would be to have you define differential privacy and tell us a little bit about the context

59
00:04:59,000 --> 00:05:01,120
in which it was created.

60
00:05:01,120 --> 00:05:10,800
Sure, so privacy has a relatively long history in computer science, but until people started

61
00:05:10,800 --> 00:05:16,880
thinking about differential privacy, what people meant when they said privacy, with some

62
00:05:16,880 --> 00:05:23,680
kind of syntactic constraints on what the output of a computation could look like.

63
00:05:23,680 --> 00:05:32,920
And it turns out these kinds of syntactic privacy guarantees don't have a strong meaning

64
00:05:32,920 --> 00:05:39,440
in terms of privacy and there was sort of a cat and mouse game in which people would

65
00:05:39,440 --> 00:05:45,280
attempt to share data sets with some kind of privacy protections and then some clever

66
00:05:45,280 --> 00:05:50,040
person would come around and figure out how to get around those privacy protections and

67
00:05:50,040 --> 00:05:51,360
this would iterate.

68
00:05:51,360 --> 00:05:57,800
Can you give us an example of those types of syntactic constraints and a little bit of

69
00:05:57,800 --> 00:05:59,960
how that cat and mouse game evolved?

70
00:05:59,960 --> 00:06:06,000
Sure, yeah, so maybe the simplest thing that you might imagine is you might think to

71
00:06:06,000 --> 00:06:13,520
yourself, well, if I just remove any identifying attributes from a data set.

72
00:06:13,520 --> 00:06:19,520
So for example, if I've got a data set of medical records, if I just remove things like

73
00:06:19,520 --> 00:06:25,400
name and zip code and maybe a few others, that'll anonymize the data and it'll be safe

74
00:06:25,400 --> 00:06:28,280
to release the data set in the clear.

75
00:06:28,280 --> 00:06:32,240
And unfortunately, that turns out not to be the case.

76
00:06:32,240 --> 00:06:39,120
So there's a bunch of examples of the sort, but maybe the first one was a demonstration

77
00:06:39,120 --> 00:06:41,120
by Latanya Sweeney.

78
00:06:41,120 --> 00:06:45,760
At the time she was a PhD student at MIT, now she's a professor at Harvard.

79
00:06:45,760 --> 00:06:52,840
And the state of Massachusetts had released a supposedly anonymized data set of patient

80
00:06:52,840 --> 00:06:53,840
medical records.

81
00:06:53,840 --> 00:06:57,840
So it didn't have people's names attached.

82
00:06:57,840 --> 00:07:03,680
But what Latanya figured out was that there was another data set that was publicly available.

83
00:07:03,680 --> 00:07:11,080
That was the voter registration records in Cambridge, Massachusetts.

84
00:07:11,080 --> 00:07:17,920
And when you've got two data sets and you know something about an individual, for example,

85
00:07:17,920 --> 00:07:24,000
Latanya knew that the governor of Massachusetts at the time, William Weld, lived in Cambridge.

86
00:07:24,000 --> 00:07:26,360
I knew a few other things about him.

87
00:07:26,360 --> 00:07:32,760
You can basically cross-reference these two data sets and reattach names.

88
00:07:32,760 --> 00:07:40,400
So that's sort of the simplest example for why attempting to remove identifying attributes

89
00:07:40,400 --> 00:07:41,400
doesn't work.

90
00:07:41,400 --> 00:07:45,160
You know, it seems like a good idea in isolation, but in the real world, there's all of this

91
00:07:45,160 --> 00:07:49,240
information out there that you can attempt to cross-reference with existing data sets.

92
00:07:49,240 --> 00:07:55,880
I think another example along those lines was when Netflix released their anonymized

93
00:07:55,880 --> 00:08:02,520
recommendation data set for I think it was the Netflix prize, someone or a set of people

94
00:08:02,520 --> 00:08:07,880
cross-reference that to IMDB and found that they were able to de-anonymize a pretty significant

95
00:08:07,880 --> 00:08:09,400
portion of those records.

96
00:08:09,400 --> 00:08:14,040
Yeah, that was another high-profile example using a more sophisticated technique that

97
00:08:14,040 --> 00:08:18,640
was done by Arvind Narayanaan, who is now a professor at Princeton and Videlish Mottikov,

98
00:08:18,640 --> 00:08:21,000
who's at Cornell Tech.

99
00:08:21,000 --> 00:08:26,960
And that was another example where names were removed, so all that was made available was

100
00:08:26,960 --> 00:08:33,200
sort of a big data set where for each person now identified by supposedly anonymous numeric

101
00:08:33,200 --> 00:08:38,960
identifier, all you saw about them were which movies they watched, what they rated them,

102
00:08:38,960 --> 00:08:40,640
and approximately when they watched them.

103
00:08:40,640 --> 00:08:46,440
And as you say, by cross-referencing this data set with IMDB, they were able to reattach

104
00:08:46,440 --> 00:08:47,440
names.

105
00:08:47,440 --> 00:08:53,680
So differential privacy came about kind of in the wake of the broader realization of

106
00:08:53,680 --> 00:08:56,520
the failure of anonymization, it sounds like?

107
00:08:56,520 --> 00:08:57,520
Exactly.

108
00:08:57,520 --> 00:09:02,320
So I think the key insight that the creators of differential privacy had was that, you

109
00:09:02,320 --> 00:09:08,320
know, if you want to speak rigorously about what someone can infer about an individual

110
00:09:08,320 --> 00:09:13,960
and given what they observe about an algorithm, you shouldn't be trying to put syntactic

111
00:09:13,960 --> 00:09:18,720
constraints on the output of that algorithm, but rather you should be putting information

112
00:09:18,720 --> 00:09:23,560
theoretic constraints on the algorithmic process itself on the computation.

113
00:09:23,560 --> 00:09:27,160
And so that's exactly what differential privacy does.

114
00:09:27,160 --> 00:09:32,320
What differential privacy means, informally, it's a constraint on an algorithm, and it

115
00:09:32,320 --> 00:09:39,880
says small changes in the input to an algorithm, for example, adding or removing the record

116
00:09:39,880 --> 00:09:46,920
of a single individual should have only small changes on the distribution of outputs that

117
00:09:46,920 --> 00:09:48,480
the algorithm produces.

118
00:09:48,480 --> 00:09:55,680
So if I remove your record entirely from a data set, that shouldn't cause any observable

119
00:09:55,680 --> 00:10:00,760
event, anything that the algorithm might do when computing on the data set to become too

120
00:10:00,760 --> 00:10:03,400
much more or less likely.

121
00:10:03,400 --> 00:10:09,960
And this kind of probabilistic information theoretic constraint turns out to have a really strong

122
00:10:09,960 --> 00:10:16,760
semantics about what, you know, an adversary, an attacker can infer about your data.

123
00:10:16,760 --> 00:10:21,880
One of the subtleties in the way you describe that is that different, and maybe it's not

124
00:10:21,880 --> 00:10:27,600
so subtle, but differential privacy isn't an algorithm itself, it's a constraint on an

125
00:10:27,600 --> 00:10:28,600
algorithm.

126
00:10:28,600 --> 00:10:30,960
Am I hearing that correctly?

127
00:10:30,960 --> 00:10:31,960
That's right.

128
00:10:31,960 --> 00:10:32,960
Yeah.

129
00:10:32,960 --> 00:10:37,200
So differential privacy is a property that an algorithm might or might not have, any particular

130
00:10:37,200 --> 00:10:41,040
algorithm might be differentially private or might not be.

131
00:10:41,040 --> 00:10:46,640
And a lot of the definition of the constraint, it's relatively simple, but a lot of the

132
00:10:46,640 --> 00:10:53,000
science that goes into studying differential privacy asks the question, you know, if I've

133
00:10:53,000 --> 00:10:58,480
tied my hands in this way in what kinds of algorithms I can use, what type of algorithm

134
00:10:58,480 --> 00:11:00,640
tasks can I still perform?

135
00:11:00,640 --> 00:11:05,280
And as you say, differential privacy is a family of, you know, it's a constraint, it's

136
00:11:05,280 --> 00:11:06,280
not an algorithm.

137
00:11:06,280 --> 00:11:12,240
So to show that you can do something subject to differential privacy, it's sufficient

138
00:11:12,240 --> 00:11:17,480
to exhibit a differentially private algorithm that does that thing, but to prove lower bounds

139
00:11:17,480 --> 00:11:21,640
to show that for some problem, you cannot solve it subject to differential privacy.

140
00:11:21,640 --> 00:11:22,840
That's another matter entirely.

141
00:11:22,840 --> 00:11:28,040
You have to write down a mathematical proof that no algorithm could solve it subject to

142
00:11:28,040 --> 00:11:29,040
the constraint.

143
00:11:29,040 --> 00:11:30,040
Hmm.

144
00:11:30,040 --> 00:11:37,680
So how does that relatively simple sounding constraint lead to the benefits of privacy?

145
00:11:37,680 --> 00:11:44,480
I guess most basically it provides a guarantee of plausible deniability.

146
00:11:44,480 --> 00:11:50,080
So let me, maybe to make things less abstract, let me give you an example of a very simple,

147
00:11:50,080 --> 00:11:52,960
intuitive, differentially private algorithm.

148
00:11:52,960 --> 00:12:00,320
So suppose that I want to conduct a poll and I want to find out amongst all of the citizens

149
00:12:00,320 --> 00:12:04,880
in Philadelphia, how many of them voted for Donald Trump in the last election?

150
00:12:04,880 --> 00:12:05,880
Okay.

151
00:12:05,880 --> 00:12:11,200
You know, the most straightforward way to do this is I would call up some random sample

152
00:12:11,200 --> 00:12:13,120
of individuals on the phone.

153
00:12:13,120 --> 00:12:18,640
And I'd ask them, you know, did you vote for Donald Trump in the 2016 election?

154
00:12:18,640 --> 00:12:22,440
And I'd write down their answer on a piece of paper.

155
00:12:22,440 --> 00:12:27,240
And then when I was all done, when I'd called, you know, sufficiently many people, I'd

156
00:12:27,240 --> 00:12:28,520
tally up their answers.

157
00:12:28,520 --> 00:12:32,080
I'd find that, you know, 15% of people voted for Donald Trump.

158
00:12:32,080 --> 00:12:35,200
I'd do some statistics to attach error bars to that.

159
00:12:35,200 --> 00:12:38,960
And then I'd publish the, uh, publish the statistic.

160
00:12:38,960 --> 00:12:39,960
Okay.

161
00:12:39,960 --> 00:12:44,840
Now, note that like the thing that I wanted to find out was just this property of the

162
00:12:44,840 --> 00:12:48,280
distribution, what fraction of people voted for Donald Trump.

163
00:12:48,280 --> 00:12:56,240
But like incidentally along the way, I accumulated this large body of potentially sensitive

164
00:12:56,240 --> 00:13:01,200
information, what individual people voted for, who, who individual people voted for.

165
00:13:01,200 --> 00:13:02,200
Right.

166
00:13:02,200 --> 00:13:03,200
Okay.

167
00:13:03,200 --> 00:13:07,840
So think about the following alternative polling procedure, which turns out to be differentially

168
00:13:07,840 --> 00:13:14,800
private and will allow us to figure out the distributional statistic we care about,

169
00:13:14,800 --> 00:13:19,960
like fraction of people voted for Donald Trump without having to collect, you know, sensitive

170
00:13:19,960 --> 00:13:21,400
information about individuals.

171
00:13:21,400 --> 00:13:22,400
Okay.

172
00:13:22,400 --> 00:13:25,880
So I'm going to, again, call up some large collection of people.

173
00:13:25,880 --> 00:13:30,400
But now instead of telling them to, instead of instructing them to tell me whether they

174
00:13:30,400 --> 00:13:35,280
voted for Donald Trump, I'll tell them the following thing, I'll say flip a coin.

175
00:13:35,280 --> 00:13:40,080
If it comes up heads, tell me truthfully whether you voted for Donald Trump or not.

176
00:13:40,080 --> 00:13:44,640
If it comes up tails, though, tell me a random answer by which I mean, flip the coin

177
00:13:44,640 --> 00:13:50,600
again and tell me Trump if it came up heads and tell me not Trump if it came up tails.

178
00:13:50,600 --> 00:13:54,800
So and importantly, you're not going to tell me how the coin came out.

179
00:13:54,800 --> 00:13:55,800
Right.

180
00:13:55,800 --> 00:13:56,800
Okay.

181
00:13:56,800 --> 00:14:00,360
So I hear Trump or not Trump, but I don't know how your coins were realized.

182
00:14:00,360 --> 00:14:03,760
So I don't know whether you're telling me the truth or whether you're lying simply because

183
00:14:03,760 --> 00:14:05,200
of how the coins were flipped.

184
00:14:05,200 --> 00:14:06,200
Okay.

185
00:14:06,200 --> 00:14:13,080
So on the one hand, you now have a significant amount of plausible deniability.

186
00:14:13,080 --> 00:14:14,080
Okay.

187
00:14:14,080 --> 00:14:23,000
If all of a sudden the country collapses into a police state and my polling records are

188
00:14:23,000 --> 00:14:28,760
subpoenaed and you're called in front of the truth commission and that's suggested that

189
00:14:28,760 --> 00:14:36,400
you voted in one way or the other, you can reasonably say, no, I didn't.

190
00:14:36,400 --> 00:14:40,920
The answer that you're reading was simply the result of a coin flip because I was following

191
00:14:40,920 --> 00:14:42,080
this randomized protocol.

192
00:14:42,080 --> 00:14:43,080
Right.

193
00:14:43,080 --> 00:14:44,080
Okay.

194
00:14:44,080 --> 00:14:45,080
So you have plausible deniability.

195
00:14:45,080 --> 00:14:46,080
That's the guarantee of privacy.

196
00:14:46,080 --> 00:14:51,080
On the other hand, even though I cannot form strong beliefs about what any individual's

197
00:14:51,080 --> 00:14:57,480
data looks like, in aggregate, I can, I can, I can figure out to a high degree of accuracy

198
00:14:57,480 --> 00:14:59,680
what fraction of people voted for Donald Trump.

199
00:14:59,680 --> 00:15:05,280
And that's because I understand the process by which noise was injected into these measurements.

200
00:15:05,280 --> 00:15:06,280
Okay.

201
00:15:06,280 --> 00:15:14,360
In aggregate, the noise behaves in a very structured easy to understand way and I can subtract

202
00:15:14,360 --> 00:15:19,280
that noise out of the average, even though I cannot figure out for any individual who

203
00:15:19,280 --> 00:15:20,280
they voted for.

204
00:15:20,280 --> 00:15:25,480
That strikes me as somewhat counterintuitive in that, you know, thinking about the coin flip,

205
00:15:25,480 --> 00:15:31,040
you know, almost half of your data could be corrupted or quarter, maybe a quarter exactly.

206
00:15:31,040 --> 00:15:35,200
So if you think about it, right, it's a simple calculation.

207
00:15:35,200 --> 00:15:40,040
If I know that for every individual, three quarters of the time they're telling me the truth

208
00:15:40,040 --> 00:15:46,800
and a quarter of the time they're telling me the opposite of the truth, then say 15%

209
00:15:46,800 --> 00:15:51,120
of people in Philadelphia truly voted for Donald Trump, you know, I can write out on a piece

210
00:15:51,120 --> 00:15:57,720
of paper what percentage of people in expectation should report to me under this protocol voted

211
00:15:57,720 --> 00:15:59,040
for Donald Trump.

212
00:15:59,040 --> 00:16:00,040
Right.

213
00:16:00,040 --> 00:16:04,760
And because I've got a bunch of independent samples, the actual number of people who end up

214
00:16:04,760 --> 00:16:08,120
telling me this will be highly concentrated around its expectation.

215
00:16:08,120 --> 00:16:09,120
Right.

216
00:16:09,120 --> 00:16:13,400
And therefore, you know, what I need to do as the pollster is work backwards.

217
00:16:13,400 --> 00:16:18,840
What I know is the number of people who actually told me they voted for Donald Trump, but

218
00:16:18,840 --> 00:16:22,680
because I can compute this one-to-one mapping between the number of people who really did

219
00:16:22,680 --> 00:16:26,840
and the number of people I expect to tell me this, I can invert the mapping and figure

220
00:16:26,840 --> 00:16:33,320
out what the underlying population statistic was to a high degree of accuracy.

221
00:16:33,320 --> 00:16:34,320
Okay.

222
00:16:34,320 --> 00:16:41,320
Sounds like, you know, the method you're describing could be used both prior to data collection

223
00:16:41,320 --> 00:16:47,720
by instructing your respondents to follow this algorithm, or if you're an organization

224
00:16:47,720 --> 00:16:51,920
collecting data, you could collect the actual responses and then apply this algorithm

225
00:16:51,920 --> 00:16:53,720
before publishing the data.

226
00:16:53,720 --> 00:16:54,720
That's right.

227
00:16:54,720 --> 00:17:01,240
So the interaction that I described to you was what's called the local model of differential

228
00:17:01,240 --> 00:17:07,560
privacy and, you know, in this scenario, people's privacy was protected even from the pollster.

229
00:17:07,560 --> 00:17:08,560
Okay.

230
00:17:08,560 --> 00:17:10,880
He never wrote down the data.

231
00:17:10,880 --> 00:17:14,180
And of course, if that's what you want, then you have to apply these protections when the

232
00:17:14,180 --> 00:17:15,720
data is being gathered.

233
00:17:15,720 --> 00:17:16,720
Right.

234
00:17:16,720 --> 00:17:22,160
But as you say, if I'm an organization that's already got the data, I can apply privacy

235
00:17:22,160 --> 00:17:27,800
protections to the output of my computations to anything I release.

236
00:17:27,800 --> 00:17:33,200
So then obviously, privacy isn't protected from me, the organization that has the data.

237
00:17:33,200 --> 00:17:37,960
I've already got the data in the clear, but assuming there are no data breaches and

238
00:17:37,960 --> 00:17:45,200
no subpoenas, differential privacy can guarantee something about what any outside observer

239
00:17:45,200 --> 00:17:51,200
can learn about individuals in the data set by observing the outcomes of computations.

240
00:17:51,200 --> 00:17:53,320
And you're careful to say guarantee something.

241
00:17:53,320 --> 00:17:58,040
What exactly does differential privacy guarantee and what does it not guarantee?

242
00:17:58,040 --> 00:17:59,040
Yeah.

243
00:17:59,040 --> 00:18:03,840
So one thing that has been alighted in this discussion is that there's a quantitative

244
00:18:03,840 --> 00:18:09,880
parameter that comes with differential privacy called Epsilon.

245
00:18:09,880 --> 00:18:16,800
But what differential privacy guarantees formally is that no event becomes much more likely

246
00:18:16,800 --> 00:18:20,640
if your data is in the data set compared to if it's not.

247
00:18:20,640 --> 00:18:25,280
But what does much more likely mean, that means more likely by some multiplicative factor

248
00:18:25,280 --> 00:18:27,680
that depends on this parameter Epsilon.

249
00:18:27,680 --> 00:18:31,000
So suppose Epsilon is small.

250
00:18:31,000 --> 00:18:35,680
This is a pretty good guarantee because it says whatever it is that you're worried about

251
00:18:35,680 --> 00:18:41,800
from the perspective of privacy, whatever harm you're worried that the use of your data

252
00:18:41,800 --> 00:18:47,560
will cause to befall you, that harm is even though I don't know what you're worried

253
00:18:47,560 --> 00:18:54,760
about, I can promise you that the risk, the increased risk of that harm befalling you can

254
00:18:54,760 --> 00:18:58,080
be bounded as a function of this parameter Epsilon.

255
00:18:58,080 --> 00:19:03,240
But of course, if Epsilon is very big, then that's not a very strong guarantee.

256
00:19:03,240 --> 00:19:08,240
So it doesn't really mean anything if someone tells you that their algorithm is differentially

257
00:19:08,240 --> 00:19:11,400
private, unless they also tell you what this privacy parameter is.

258
00:19:11,400 --> 00:19:16,000
As the privacy parameter goes to infinity, differential privacy is no constraint at all,

259
00:19:16,000 --> 00:19:20,000
as it goes to zero, it becomes a very strong constraint.

260
00:19:20,000 --> 00:19:27,360
Going back to this fundamental constraint, it's that within the bounds of Epsilon, adding

261
00:19:27,360 --> 00:19:35,960
or removing an individual piece of data won't change the statistics of your overall distribution.

262
00:19:35,960 --> 00:19:36,960
Is that correct?

263
00:19:36,960 --> 00:19:37,960
That's right.

264
00:19:37,960 --> 00:19:40,960
It won't change the behavior of your algorithm.

265
00:19:40,960 --> 00:19:48,520
So adding or removing a single data point won't cause your algorithm to do something that

266
00:19:48,520 --> 00:19:52,080
is very different from the point of view of an observer.

267
00:19:52,080 --> 00:20:00,800
And so how does, how do we get from there to, again, the notion of privacy?

268
00:20:00,800 --> 00:20:06,520
And I guess you were setting that up in talking about the plausible deniability example.

269
00:20:06,520 --> 00:20:07,520
Yes.

270
00:20:07,520 --> 00:20:10,400
So there's a couple of interpretations of differential privacy.

271
00:20:10,400 --> 00:20:12,200
And I can walk through a few of them.

272
00:20:12,200 --> 00:20:16,160
So one is this plausible deniability guarantee.

273
00:20:16,160 --> 00:20:24,720
So if someone accuses you of having some particular data record, and the piece of evidence

274
00:20:24,720 --> 00:20:30,560
they have at their disposal is the output of a differentially private computation, you

275
00:20:30,560 --> 00:20:34,760
have plausible deniability in the sense that you can say that the piece of evidence they

276
00:20:34,760 --> 00:20:41,680
have in hand is essentially, as likely to have been observed, again, up to this factor

277
00:20:41,680 --> 00:20:46,200
relating to the privacy parameter, if your data had been different.

278
00:20:46,200 --> 00:20:47,200
Okay.

279
00:20:47,200 --> 00:20:51,640
Another way of saying this is, you know, suppose they've got some prior belief about what

280
00:20:51,640 --> 00:20:55,920
your data point looks like.

281
00:20:55,920 --> 00:20:59,080
And then they observe the output of a computation.

282
00:20:59,080 --> 00:21:04,720
And so they update their belief to some posterior belief using Bayes' rule, for example.

283
00:21:04,720 --> 00:21:09,240
And what differential privacy promises is that they would have performed nearly the

284
00:21:09,240 --> 00:21:14,560
same update, and therefore had nearly the same belief had your data been different if

285
00:21:14,560 --> 00:21:17,560
we hold the rest of the data set fixed.

286
00:21:17,560 --> 00:21:24,800
Another interpretation is this model of harm, and you can view this as a sort of utilitarian

287
00:21:24,800 --> 00:21:31,480
definition of privacy, you know, like how hard is it for me to convince you to contribute

288
00:21:31,480 --> 00:21:35,760
your data to my data set, if I'm going to use it for some statistical analysis?

289
00:21:35,760 --> 00:21:39,840
Well, why wouldn't you want to contribute your data?

290
00:21:39,840 --> 00:21:44,280
There's any number of reasons, and I might not know what they are, but, you know, presumably

291
00:21:44,280 --> 00:21:49,360
you're worried that as the result of the use of your data, something bad is going to happen

292
00:21:49,360 --> 00:21:50,360
to you.

293
00:21:50,360 --> 00:21:52,640
Maybe your health insurance premiums are going to rise, or maybe you're going to start

294
00:21:52,640 --> 00:21:55,480
getting drunk phone calls during dinner.

295
00:21:55,480 --> 00:21:59,560
And what differential privacy can promise is no matter what event that you're worried

296
00:21:59,560 --> 00:22:04,600
about, and I really mean no matter what event, so we can talk about the probability that

297
00:22:04,600 --> 00:22:09,920
your health insurance premiums rise for the probability that you get spam phone calls.

298
00:22:09,920 --> 00:22:17,920
This event will be, will have almost the same probability up to, again, this privacy parameter.

299
00:22:17,920 --> 00:22:23,400
In the following two hypothetical worlds, in the one world, you don't contribute your data

300
00:22:23,400 --> 00:22:27,840
to the computation in the other world you do, and everything else is held constant between

301
00:22:27,840 --> 00:22:28,840
these two worlds.

302
00:22:28,840 --> 00:22:32,240
That's the difference in differential privacy, right?

303
00:22:32,240 --> 00:22:37,640
So if I look at the two different worlds that are identical, except for this one fact,

304
00:22:37,640 --> 00:22:42,920
whether you contributed your data to my analysis or whether you did not, then the events that

305
00:22:42,920 --> 00:22:46,680
you're worried about, whatever they are, become almost no more likely when you contribute

306
00:22:46,680 --> 00:22:48,680
your data.

307
00:22:48,680 --> 00:22:53,960
And does that interpretation, it sounds like that assumes some anonymization?

308
00:22:53,960 --> 00:22:59,240
It doesn't assume anything, that follows sort of directly from the definition of differential

309
00:22:59,240 --> 00:23:02,360
privacy, that if you like, that is the definition of differential privacy.

310
00:23:02,360 --> 00:23:07,480
I guess I think I'm maybe I'm thinking of this in some perversely, but if I include

311
00:23:07,480 --> 00:23:15,400
my data and my data, includes my phone number, how does differential privacy address that?

312
00:23:15,400 --> 00:23:19,800
Oh, well, your data can include anything you like, including your phone number, but a

313
00:23:19,800 --> 00:23:23,280
differentially private algorithm, certainly can't look at your data record and publish your

314
00:23:23,280 --> 00:23:24,280
phone number.

315
00:23:24,280 --> 00:23:25,280
Right.

316
00:23:25,280 --> 00:23:31,040
And so is the idea that we're applying the coin flip, for example, to the publishing, you

317
00:23:31,040 --> 00:23:34,760
know, maybe it would randomly spit out phone numbers or something like that?

318
00:23:34,760 --> 00:23:40,720
Yeah, I mean, I think I'm getting stuck in a rat hole here, but so maybe one thing that's

319
00:23:40,720 --> 00:23:45,280
useful to keep in mind, you know, you can try to write down a differentially private algorithm

320
00:23:45,280 --> 00:23:50,480
for anything you like, but it's only for certain kinds of problems for which differentially

321
00:23:50,480 --> 00:23:53,480
private algorithms are going to be able to do anything useful.

322
00:23:53,480 --> 00:23:59,320
And those are our statistical problems, where the thing that you want to estimate is some

323
00:23:59,320 --> 00:24:04,360
property of the underlying distribution, so it's very good for sort of machine learning

324
00:24:04,360 --> 00:24:08,400
if I want to learn a classifier that on average makes correct predictions.

325
00:24:08,400 --> 00:24:12,840
But if I want to learn what your phone number is, you know, it's all well and good that

326
00:24:12,840 --> 00:24:17,840
I want to learn that, but by design, you know, there is no differentially private algorithm

327
00:24:17,840 --> 00:24:22,960
that will give me a non trivial advantage over essentially random guessing.

328
00:24:22,960 --> 00:24:27,600
Differential privacy isn't compatible with answering the kinds of questions that have

329
00:24:27,600 --> 00:24:30,600
to do with just a single individual, and that's by design.

330
00:24:30,600 --> 00:24:38,600
So that's a great segue to the applications of differential privacy in machine learning.

331
00:24:38,600 --> 00:24:45,280
Can you maybe start by talking about the specific machine learning, you know, problems or examples

332
00:24:45,280 --> 00:24:49,440
that differential privacy is, is trying to address in that application and maybe talk

333
00:24:49,440 --> 00:24:53,240
through some of the specifics of how that's done?

334
00:24:53,240 --> 00:24:54,240
Sure.

335
00:24:54,240 --> 00:24:58,040
So there's a couple of things that you might want to do subject to differential privacy

336
00:24:58,040 --> 00:25:00,120
when you're doing machine learning.

337
00:25:00,120 --> 00:25:04,280
So one is that just that you might want to solve a single machine learning problem subject

338
00:25:04,280 --> 00:25:05,280
to differential privacy.

339
00:25:05,280 --> 00:25:10,640
So maybe you've got some, for example, supervised classification task, you've got some label

340
00:25:10,640 --> 00:25:15,600
data you'd like to learn, you know, a support vector machine or a neural network of some

341
00:25:15,600 --> 00:25:22,480
sort that will minimize some loss function, maybe my classification error, when I apply

342
00:25:22,480 --> 00:25:24,240
it to new data.

343
00:25:24,240 --> 00:25:25,240
Okay.

344
00:25:25,240 --> 00:25:30,360
So that's the standard machine learning problem and differential privacy is extremely

345
00:25:30,360 --> 00:25:35,760
amenable to this kind of problem, essentially, because well, there's several reasons,

346
00:25:35,760 --> 00:25:41,080
but maybe the most fundamental reason is that this is inherently a statistical problem

347
00:25:41,080 --> 00:25:48,240
in the sense that the thing that I already for reasons of overfitting wanted to avoid

348
00:25:48,240 --> 00:25:53,120
when I'm solving a machine learning problem, depending too heavily on any single data

349
00:25:53,120 --> 00:25:54,120
point, right?

350
00:25:54,120 --> 00:25:59,040
So, so machine learning and privacy, they're sort of aligned in the sense that they're

351
00:25:59,040 --> 00:26:04,440
both trying to learn facts about the distribution without overfitting to the particular data

352
00:26:04,440 --> 00:26:09,760
set I have on hand overfitting is closely related to privacy violations and we can talk

353
00:26:09,760 --> 00:26:10,760
more about that.

354
00:26:10,760 --> 00:26:12,960
The connection turns out to go both ways.

355
00:26:12,960 --> 00:26:18,400
Another thing that you might want to do is more ambitious, you might want to construct

356
00:26:18,400 --> 00:26:23,280
a synthetic data set by which I mean, like rather than solving a single machine learning

357
00:26:23,280 --> 00:26:28,360
problem, maybe you want to construct a data set that looks like the real data set with

358
00:26:28,360 --> 00:26:33,840
respect to some large number of statistics or machine learning algorithms, but it's

359
00:26:33,840 --> 00:26:38,160
nevertheless differentially private, so I can construct this synthetic data set with

360
00:26:38,160 --> 00:26:42,120
a private algorithm and then release it to the world and then other people can go and

361
00:26:42,120 --> 00:26:46,360
try to apply their machine learning algorithms to this synthetic data set, the hope being

362
00:26:46,360 --> 00:26:51,120
that insights that they derive classifiers they train on the synthetic data set would

363
00:26:51,120 --> 00:26:53,880
also work well on the real data set.

364
00:26:53,880 --> 00:26:58,360
And then finally, and this relates back to the connections between differential privacy

365
00:26:58,360 --> 00:27:03,640
and overfitting, it might be that you don't care about privacy at all in your application,

366
00:27:03,640 --> 00:27:14,560
but you know, you want to, for example, repeatedly test hypotheses or fit different kinds

367
00:27:14,560 --> 00:27:19,480
of machine learning models while reusing the same holdout set over and over again, maybe

368
00:27:19,480 --> 00:27:22,240
because it's too expensive to get more data.

369
00:27:22,240 --> 00:27:28,040
Now normally this would be a really bad idea, sort of the standard test train methodology

370
00:27:28,040 --> 00:27:35,040
and machine learning entirely falls apart, basically, if you reuse the holdout set as part

371
00:27:35,040 --> 00:27:40,080
of an iterative procedure by which you're choosing your model.

372
00:27:40,080 --> 00:27:47,600
But as it turns out, when you perform your entire pipeline of statistical analyses subject

373
00:27:47,600 --> 00:27:53,680
to the guarantees of differential privacy, you can't overfit, so if you can be accurate.

374
00:27:53,680 --> 00:27:58,840
In sample, you can be guaranteed that you learn an accurate model out of sample, even

375
00:27:58,840 --> 00:28:01,320
if you've repeatedly reused the data.

376
00:28:01,320 --> 00:28:07,280
And when you say perform your entire pipeline subject to the guarantees of differential

377
00:28:07,280 --> 00:28:13,000
privacy, does that mean you are enforcing those constraints at every individual step

378
00:28:13,000 --> 00:28:18,200
or the, you know, relative to the inputs and outputs of the entire pipeline?

379
00:28:18,200 --> 00:28:24,560
It means that, you know, everything you shouldn't have touched the data in any way using a non-differentially

380
00:28:24,560 --> 00:28:25,560
private algorithm.

381
00:28:25,560 --> 00:28:29,280
So differential privacy has a very nice property that it composes.

382
00:28:29,280 --> 00:28:33,680
If I have two algorithms, you know, the first one is epsilon one, differentially private,

383
00:28:33,680 --> 00:28:36,400
and the second one is epsilon two, differentially private.

384
00:28:36,400 --> 00:28:41,480
Then if I run the first one and based on the outcome decide what I want to do at the second

385
00:28:41,480 --> 00:28:46,760
step and then I run the second algorithm, this whole computation is in aggregate, epsilon

386
00:28:46,760 --> 00:28:49,200
one plus epsilon two, differentially private.

387
00:28:49,200 --> 00:28:54,600
So, you know, if at every decision point I'm making my decision about what to do next

388
00:28:54,600 --> 00:28:59,320
as a function only of differentially private access to the data, then you've got these

389
00:28:59,320 --> 00:29:02,000
strong safety guarantees about overfitting.

390
00:29:02,000 --> 00:29:09,520
So maybe to make it a little bit more concrete, I've heard a few examples of scenarios that

391
00:29:09,520 --> 00:29:14,720
pop up in the machine learning world, and I'm vaguely recalling them, maybe you can provide

392
00:29:14,720 --> 00:29:23,360
a bit more detail, but one of them was an example of a, it's almost, it was like reverse

393
00:29:23,360 --> 00:29:29,280
engineering and object detector to determine whether an individual or a face detector to

394
00:29:29,280 --> 00:29:33,080
determine whether an individual face was in the training data set.

395
00:29:33,080 --> 00:29:38,960
Okay, so you're talking about maybe an attack on a classifier that wasn't trained in

396
00:29:38,960 --> 00:29:43,160
a differentially private way and the kind of thing that you might, the kind of reason

397
00:29:43,160 --> 00:29:49,400
why you might want to have privacy guarantees when you're training a learning algorithm

398
00:29:49,400 --> 00:29:51,800
that they don't come for free, if I'm.

399
00:29:51,800 --> 00:29:52,800
Exactly, exactly.

400
00:29:52,800 --> 00:29:58,080
Yes, so I think there's a couple of these kinds of attacks now, and I don't know the details

401
00:29:58,080 --> 00:30:04,640
of the specific one you're referring to, but you might have, you know, a priori before

402
00:30:04,640 --> 00:30:10,720
you started worrying about privacy, I think that, you know, okay, maybe if I'm, you know,

403
00:30:10,720 --> 00:30:15,880
releasing individual data records like in the Netflix example, I have to worry about privacy

404
00:30:15,880 --> 00:30:21,800
violations, but if I'm, if I'm just training a classifier, how could, how could releasing

405
00:30:21,800 --> 00:30:26,400
only the model parameters, you know, the weights in the neural network possibly violate

406
00:30:26,400 --> 00:30:27,400
anyone's privacy?

407
00:30:27,400 --> 00:30:28,400
Exactly.

408
00:30:28,400 --> 00:30:34,560
Yeah, and that intuition is wrong and you're describing the kind of attacks that, that

409
00:30:34,560 --> 00:30:41,360
show that it's wrong, but the basic, I think, premise underlying these attacks is that when

410
00:30:41,360 --> 00:30:48,440
you train a model without differential privacy, it'll tend to overfit a little bit, even

411
00:30:48,440 --> 00:30:51,440
if, even if this doesn't really affect the model performance.

412
00:30:51,440 --> 00:30:57,360
But what you find is that, you know, when you try to classify a face, for example, that

413
00:30:57,360 --> 00:31:04,360
was in the training data set, the model will tend to have higher confidence in its classification

414
00:31:04,360 --> 00:31:07,920
than when you try to classify an example that was not in the training data set.

415
00:31:07,920 --> 00:31:08,920
Okay.

416
00:31:08,920 --> 00:31:13,120
And it's sort of natural that you would expect that because the model got to update itself

417
00:31:13,120 --> 00:31:14,920
on the examples in the training set.

418
00:31:14,920 --> 00:31:15,920
Right.

419
00:31:15,920 --> 00:31:21,360
And by taking advantage of that, you can therefore figure out whether a particular person's

420
00:31:21,360 --> 00:31:26,560
picture was in the training data set or not by examining what is the confidence in

421
00:31:26,560 --> 00:31:28,560
the model's prediction.

422
00:31:28,560 --> 00:31:29,560
Okay.

423
00:31:29,560 --> 00:31:35,760
Are other examples that come to mind of, you know, where the notion of distributing

424
00:31:35,760 --> 00:31:41,840
models or, you know, more generally, I guess, statistical aggregates can fail to be privacy

425
00:31:41,840 --> 00:31:42,840
preserving?

426
00:31:42,840 --> 00:31:48,360
So, maybe the most obvious example is sort of naive training of a support vector machine.

427
00:31:48,360 --> 00:31:56,040
So the simplest way to, you know, the most concise way for me to describe to you the model

428
00:31:56,040 --> 00:32:01,680
of a trans support vector machine is by communicating to you the support vectors.

429
00:32:01,680 --> 00:32:06,640
But the support vectors are just examples from the training data set.

430
00:32:06,640 --> 00:32:13,440
So the most straightforward way to distribute a trained support vector machine is to transmit

431
00:32:13,440 --> 00:32:17,520
to you some number of examples from the training data set in the clear.

432
00:32:17,520 --> 00:32:21,720
So that's sort of obvious once you realize it, but, you know, it is one of the things that

433
00:32:21,720 --> 00:32:25,920
you might not have thought of initially when you, if you're coming from this position,

434
00:32:25,920 --> 00:32:29,920
but things that represent just aggregate statistics like trained models shouldn't be

435
00:32:29,920 --> 00:32:30,920
disclosed.

436
00:32:30,920 --> 00:32:31,920
Okay.

437
00:32:31,920 --> 00:32:32,920
Okay.

438
00:32:32,920 --> 00:32:43,320
What I'm hearing is, you know, I guess granted some in some classes of problem, maybe privacy

439
00:32:43,320 --> 00:32:47,200
isn't, you know, the greatest concern.

440
00:32:47,200 --> 00:32:52,680
But if differential privacy were free and easy to apply everywhere, you know, I might

441
00:32:52,680 --> 00:32:58,680
do that. What are some of the, you know, the issues or costs of applying differential

442
00:32:58,680 --> 00:33:02,680
privacy that come up when trying to apply it in the machine learning context?

443
00:33:02,680 --> 00:33:03,680
Yeah.

444
00:33:03,680 --> 00:33:05,680
So it definitely doesn't come for free.

445
00:33:05,680 --> 00:33:07,680
And I think there's costs of two sorts.

446
00:33:07,680 --> 00:33:14,200
So the first is sort of, maybe it's difficult to just acquire the expertise to implement

447
00:33:14,200 --> 00:33:19,000
all of these things at the moment, you know, a lot of the knowledge about differential

448
00:33:19,000 --> 00:33:22,840
privacy at the moment is contained in hard to read, you know, academic papers.

449
00:33:22,840 --> 00:33:26,760
There's not that many people who are trained to read these things.

450
00:33:26,760 --> 00:33:33,800
So if you're addressed some random company, it can be hard to even get started.

451
00:33:33,800 --> 00:33:39,640
But maybe the more fundamental thing is that although differential privacy is compatible

452
00:33:39,640 --> 00:33:44,920
with machine learning by which I mean, in principle, anything that you can, any statistical

453
00:33:44,920 --> 00:33:49,720
problem that is susceptible to machine learning absent differential privacy is, you know,

454
00:33:49,720 --> 00:33:54,200
can also be solved with machine learning with differential privacy guarantees.

455
00:33:54,200 --> 00:33:59,240
The cost is that if you want strong differential privacy guarantees, you'll need more data.

456
00:33:59,240 --> 00:34:04,240
And if you want the privacy parameter to be small, this thing that governs the strength

457
00:34:04,240 --> 00:34:08,760
of the privacy guarantee, you might need a lot more data to achieve the same accuracy

458
00:34:08,760 --> 00:34:10,000
guarantees.

459
00:34:10,000 --> 00:34:17,720
So as a result, it can be a tough sell to apply privacy technologies in a setting in which,

460
00:34:17,720 --> 00:34:22,840
you know, developers and researchers already have direct access to the data set because

461
00:34:22,840 --> 00:34:25,000
the data set's not getting any bigger.

462
00:34:25,000 --> 00:34:31,560
So if yesterday you could do your statistical analyses on your data set of 10,000 records

463
00:34:31,560 --> 00:34:36,720
and today I say, now you've got to do it subject to differential privacy.

464
00:34:36,720 --> 00:34:45,680
The accuracy of your analyses is going to degrade the place in which I've seen it successfully

465
00:34:45,680 --> 00:34:54,120
deployed overcoming sort of this kind of objection in industry has been in settings where because

466
00:34:54,120 --> 00:34:59,640
of privacy concerns, developers previously didn't have access to the data at all.

467
00:34:59,640 --> 00:35:05,720
And they're now, you know, once privacy, strong privacy guarantees are built in, are

468
00:35:05,720 --> 00:35:11,640
able to start collecting it, so it's a much easier sell if the privacy guarantees are

469
00:35:11,640 --> 00:35:16,060
going to give you access to new data sets that previously you couldn't touch because

470
00:35:16,060 --> 00:35:22,600
of privacy concerns, then it is to sort of add on X post when previously you were able

471
00:35:22,600 --> 00:35:29,560
to ignore privacy or the cost of privacy will tend to come in the form of less accuracy

472
00:35:29,560 --> 00:35:33,320
in terms of your, you know, costification error, for example.

473
00:35:33,320 --> 00:35:40,480
Okay, some of the known uses of differential privacy are places like Google, Apple, Microsoft,

474
00:35:40,480 --> 00:35:46,800
the US Census Bureau, are you familiar with those examples and what they're doing and

475
00:35:46,800 --> 00:35:49,400
can you talk about the ones that you are?

476
00:35:49,400 --> 00:35:58,000
Sure, so Google and Apple are both using differential privacy in the local model, this model

477
00:35:58,000 --> 00:36:05,160
of the polling agency trying to figure out how many people voted for Donald Trump in the

478
00:36:05,160 --> 00:36:06,360
example that I gave.

479
00:36:06,360 --> 00:36:12,800
Okay, so both of them are collecting statistics, you know, Google in your Chrome web browser

480
00:36:12,800 --> 00:36:19,480
and Apple on your iPhone in which the privacy protections are added on device.

481
00:36:19,480 --> 00:36:25,880
And what they're trying to do are collect simple statistics, population-wide averages.

482
00:36:25,880 --> 00:36:32,720
So if you look at the Apple paper, for example, they're collecting statistics on, you know,

483
00:36:32,720 --> 00:36:39,000
like what are the most frequently used emojis in different countries or for different websites?

484
00:36:39,000 --> 00:36:46,440
What fraction of people like it when videos automatically play as opposed to requiring

485
00:36:46,440 --> 00:36:47,440
some human intervention?

486
00:36:47,440 --> 00:36:52,920
So they're trying to collect population-wide statistics that allow them to improve user

487
00:36:52,920 --> 00:36:59,800
experience or improve things like predictive type and things like that.

488
00:36:59,800 --> 00:37:08,160
The US Census is doing something more ambitious and the US Census collects all the data in

489
00:37:08,160 --> 00:37:09,160
the clear.

490
00:37:09,160 --> 00:37:14,240
So they're not trying to protect the privacy of your data from the census, they're collecting

491
00:37:14,240 --> 00:37:15,240
it.

492
00:37:15,240 --> 00:37:20,320
Instead they're using differential privacy in the centralized model.

493
00:37:20,320 --> 00:37:24,880
But they release huge amounts of statistical information.

494
00:37:24,880 --> 00:37:34,120
So you can go on existing census websites and figure out the answers to questions like,

495
00:37:34,120 --> 00:37:38,760
how many people live in this collection of zip codes and work in this other collection

496
00:37:38,760 --> 00:37:39,920
of zip codes?

497
00:37:39,920 --> 00:37:40,920
Okay.

498
00:37:40,920 --> 00:37:48,120
And they're going to continue releasing these large amounts of statistical information

499
00:37:48,120 --> 00:37:52,520
about the US population, but for the 2020 census, they're going to release it subject

500
00:37:52,520 --> 00:37:54,800
to differential privacy protections.

501
00:37:54,800 --> 00:37:55,800
Interesting.

502
00:37:55,800 --> 00:38:03,400
And so they're releasing not individual data records, but more of these statistical aggregates

503
00:38:03,400 --> 00:38:05,040
subject to differential privacy.

504
00:38:05,040 --> 00:38:06,040
That's right.

505
00:38:06,040 --> 00:38:12,520
So in all of these applications, what's being released or statistics rather than actual

506
00:38:12,520 --> 00:38:13,840
synthetic data sets.

507
00:38:13,840 --> 00:38:14,840
Right.

508
00:38:14,840 --> 00:38:18,640
As far as I know, I don't know the details of what the census plans to do.

509
00:38:18,640 --> 00:38:22,680
And I'm not sure that's even been pinned down.

510
00:38:22,680 --> 00:38:29,120
In an academic setting, I have a former student, Stephen Wu, who's now a postdoc at Microsoft

511
00:38:29,120 --> 00:38:30,120
Research.

512
00:38:30,120 --> 00:38:34,560
But when he was here, he worked with colleagues in the medical school, a professor named

513
00:38:34,560 --> 00:38:41,680
Casey Green, to construct a differentially private synthetic medical data sets.

514
00:38:41,680 --> 00:38:49,280
So medicine is a field that's got a big problem in that there's a lot of data and it's starting

515
00:38:49,280 --> 00:38:56,000
to be susceptible to yielding all sorts of wonderful insights if we apply the latest machine

516
00:38:56,000 --> 00:38:58,000
learning technologies to it.

517
00:38:58,000 --> 00:39:04,000
But medicine is a domain where there are serious privacy concerns and legal regulations.

518
00:39:04,000 --> 00:39:06,240
And so it's very difficult to share data sets.

519
00:39:06,240 --> 00:39:11,080
Ideally, you'd like to share your data sets with other researchers, allow them to reproduce

520
00:39:11,080 --> 00:39:15,640
the kinds of garments you did on the data, combine data sets.

521
00:39:15,640 --> 00:39:20,000
And so what Stephen and his colleagues did was it gave sort of a proof of concept that

522
00:39:20,000 --> 00:39:28,120
you could use techniques for privately training neural networks and combine those with techniques

523
00:39:28,120 --> 00:39:34,320
for generating synthetic data for training gans that would let you create synthetic data

524
00:39:34,320 --> 00:39:39,600
sets that you could release to the public, but that would look like the original data

525
00:39:39,600 --> 00:39:43,120
set with respect to a very large class of machine learning algorithms.

526
00:39:43,120 --> 00:39:50,040
So you could train the algorithms on the synthetic data and then find that when you evaluated

527
00:39:50,040 --> 00:39:52,280
them on the real data, they did pretty well.

528
00:39:52,280 --> 00:39:53,280
Okay.

529
00:39:53,280 --> 00:39:54,280
Interesting.

530
00:39:54,280 --> 00:39:55,280
Interesting.

531
00:39:55,280 --> 00:40:00,440
This is sort of the more ambitious kind of technology that I think, as far as I know,

532
00:40:00,440 --> 00:40:06,640
has so far been the domain of only academic research, but maybe in the coming years

533
00:40:06,640 --> 00:40:11,160
will find industrial and government applications.

534
00:40:11,160 --> 00:40:17,000
Can you maybe share a brief word on the current research areas and around differential privacy

535
00:40:17,000 --> 00:40:18,360
and machine learning?

536
00:40:18,360 --> 00:40:24,560
So there are many in diverse and there are people focused on more practical problems

537
00:40:24,560 --> 00:40:26,800
and more theoretical problems.

538
00:40:26,800 --> 00:40:33,080
I myself, you know, just through my natural activities tend to focus on sort of the more

539
00:40:33,080 --> 00:40:34,800
theoretical problems.

540
00:40:34,800 --> 00:40:40,960
But I think that it remains, even though it's an old problem, it remains an important

541
00:40:40,960 --> 00:40:47,400
and unsolved problem to figure out sort of practical ways to generate useful synthetic

542
00:40:47,400 --> 00:40:49,880
data for large collections of tasks.

543
00:40:49,880 --> 00:40:55,320
We know, we've known for a while, you know, since my PhD thesis that these kinds of problems

544
00:40:55,320 --> 00:41:00,760
are possible in principle, there are inefficient information theoretic, you know, kinds of algorithms

545
00:41:00,760 --> 00:41:03,320
that accomplish them, but we don't get to have practical algorithms.

546
00:41:03,320 --> 00:41:06,080
I think that remains very important.

547
00:41:06,080 --> 00:41:11,320
You know, another important direction is that a lot of the academic literature to date

548
00:41:11,320 --> 00:41:15,560
has really focused on the central model of data privacy where there's a trusted database

549
00:41:15,560 --> 00:41:20,240
curator who gathers all the data in the clear in part because you can do more stuff in

550
00:41:20,240 --> 00:41:21,240
that model.

551
00:41:21,240 --> 00:41:23,720
So it's attractive to study it.

552
00:41:23,720 --> 00:41:28,440
But as we've seen differential privacy move from theory to practice, you know, two

553
00:41:28,440 --> 00:41:34,200
days, it's two largest scale deployments that Google and Apple are both in the local model.

554
00:41:34,200 --> 00:41:39,360
And there's a lot of things, I think, that we understand in the central model of differential

555
00:41:39,360 --> 00:41:42,760
privacy that we still don't understand in the local model.

556
00:41:42,760 --> 00:41:46,400
And that's too bad because the local model's turning out to be very important.

557
00:41:46,400 --> 00:41:52,960
So I think understanding basic tasks in the local model continues to be very important.

558
00:41:52,960 --> 00:41:58,120
And I mentioned briefly this sort of research agenda showing that you can use differential

559
00:41:58,120 --> 00:42:03,600
privacy to avoid false discovery and overfitting even when you don't care about privacy.

560
00:42:03,600 --> 00:42:11,160
I think this is one of the most general promising directions to, you know, attack the statistical

561
00:42:11,160 --> 00:42:13,400
crisis in science.

562
00:42:13,400 --> 00:42:18,840
But so far, we're just in early days, you know, we understand the basic sort of proof

563
00:42:18,840 --> 00:42:23,680
of concept kinds of results for why techniques from differential privacy might be useful.

564
00:42:23,680 --> 00:42:28,000
But we're a pretty far way off from having practical tools that, you know, working data

565
00:42:28,000 --> 00:42:33,960
scientists can use to prevent overfitting and with practically sized data sets.

566
00:42:33,960 --> 00:42:34,960
Okay.

567
00:42:34,960 --> 00:42:35,960
Great.

568
00:42:35,960 --> 00:42:41,960
We will share a link to your website so folks can take a look at some of your recent

569
00:42:41,960 --> 00:42:44,800
work and publications in this area.

570
00:42:44,800 --> 00:42:48,480
Before we close up, would you, is there anything else that you'd like to share with the

571
00:42:48,480 --> 00:42:49,480
audience?

572
00:42:49,480 --> 00:42:57,280
No, thanks for, thanks for listening and, you know, I'm always happy to hear about interesting

573
00:42:57,280 --> 00:42:59,320
new applications of differential privacy.

574
00:42:59,320 --> 00:43:05,680
So feel free to send me emails when I was just getting started and writing my PhD thesis.

575
00:43:05,680 --> 00:43:10,720
You know, all of this was a theoretical abstraction and it's been great fun, you know, hearing

576
00:43:10,720 --> 00:43:16,120
about and consulting with companies that are actually putting this into practice.

577
00:43:16,120 --> 00:43:20,520
So it's been a fun ride and I like to hear what's going on out there.

578
00:43:20,520 --> 00:43:21,520
Fantastic.

579
00:43:21,520 --> 00:43:22,520
Well, thanks so much, Aaron.

580
00:43:22,520 --> 00:43:23,520
Thank you.

581
00:43:23,520 --> 00:43:29,720
All right, everyone, that's our show for today.

582
00:43:29,720 --> 00:43:34,800
For more information on Aaron or any of the topics covered in this episode, head on over

583
00:43:34,800 --> 00:43:40,040
to twimla.com slash talk slash one thirty two.

584
00:43:40,040 --> 00:43:44,360
Thanks again to our friends at Georgian for sponsoring this series and be sure to visit

585
00:43:44,360 --> 00:43:52,400
their differential privacy resource center at gptrs.vc slash twimla for more information

586
00:43:52,400 --> 00:43:55,120
on the field and what they're up to.

587
00:43:55,120 --> 00:44:23,640
And of course, thank you so much for listening and catch you next time.

