Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting
people doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
What you're about to hear is the third of a series of shows recorded at the Georgian
Partners Portfolio Conference last week in Toronto.
My guess this time is Graham Taylor, professor of engineering at the University of Guelph
who keynoteed day two of the conference.
Graham leads the machine learning research group at Guelph and is affiliated with Toronto's
recently formed Vector Institute for Artificial Intelligence.
Graham and I discussed a number of the most important trends and challenges in artificial
intelligence, including the move from predictive to creative systems, the rise of human in the
loop AI and how modern AI is accelerating with our ability to teach computers how to learn
to learn.
Georgian Partners is a venture capital firm whose investment thesis is that certain tech
trends change every aspect of a software business over time, including business goals, product
plans, people in skills, technology platforms, pricing and packaging.
Georgian invests in those companies best positioned to take advantage of these trends and then
works closely with those companies to develop and execute the strategies necessary to make
it happen.
Applied AI is one of the trends they're investing in as our conversational business and security
first.
Georgian sponsored this series and we thank them for their support.
To learn more about Georgian, visit twimmolai.com slash Georgian where you'll also be able to
download white papers on their principles of applied AI and conversational business.
Before we jump in, if you're in New York City on October 30th and 31st, we hope you'll
join us at the NYU Future Labs AI Summit and Happy Hour.
As you may remember, we attended the inaugural summit back in April.
The fall event features more grade speakers including Karina Cortez head of research at
Google New York, David Venturelli, science operations manager at NASA Ames Quantum AI
Lab, and Dennis Mortensen, CEO and founder of startup x.ai.
For the event homepage, visit aiSummit2017.futurelabs.nyc, and for 25% off tickets, use the
code twimmol25.
For details on the Happy Hour, visit our events page at twimmolai.com slash events, and now
onto the show.
Alright everyone, I am here at the Georgian Partners portfolio conference and I've got
the pleasure of being seated with Graham Taylor.
Graham is a professor at the University of Guelph here in Canada, and he did a really
interesting talk today on some of the challenges and opportunities associated with machine learning
in AI and particularly around his research area and deep learning.
And we're here to spend a little bit of time chatting about that.
Graham, welcome to the podcast.
Thanks for having me on the program.
I told you this is my first time doing a podcast, so I'm really excited to being a consumer
of podcasts to actually get back to the podcast community.
So thanks for having me.
Nice, absolutely, absolutely.
Why don't we get started by having you tell us a little bit about your background and
how you got involved in machine learning in AI and what you're up to nowadays?
Sure, so currently I'm working as a professor at the University of Guelph.
I'm also a member of the new Vector Institute for Artificial Intelligence, which has started
up and getting ready to move in in November here in Toronto.
I'm the academic director of a program called Next AI, which is a founder development program
for startup specifically working on AI technologies.
So I wear a number of different hats, but they're all focused on artificial intelligence
and machine learning.
So let me tell you a little bit about how I entered that space.
I often get asked this question of how I got into AI, and fortunately I can point at
one specific point in my life, which really convinced me.
And that was an inspiring professor when I was an undergrad student at the University
of Waterloo.
So I had a course.
I believe the course was called Machine Intelligence, and the way that course was set
up was to actually encourage us all the students to write AI programs to play each other's
AI programs in this game called Abelone.
And it's amazing looking at this effectively.
I would say it's been at least 15 years now since we did this.
But with all the news last week, in terms of this new AlphaGo system by Google Deep
Mind, playing the game of Go, and how it was trained entirely by self-play.
This is exactly what we were trying to do on this assignment.
It was an easier game, but it was really inspirational to build these agents, played against each
other, and our team ended up winning the competition, made us really proud and excited and eager
to do more work.
Nice.
Yeah, so that's what started all off, okay?
And so what's your path been so far?
So from that point, as an undergrad, I got so excited about the potential of AI.
No, I wouldn't say at all I was sort of going to predict what would happen up to this
time and how huge it's growing.
But I just observed being a technical person.
I was really excited about building those tools and wanting to learn more.
So I went to the University of Toronto.
I knew they had a good machine learning program.
They had a number of faculty there who I was aware of, their work, lots of graduates,
students.
It seemed like a great place to be.
It was not too far away from Waterloo and London, Ontario, where I grew up.
Okay.
It seemed like a natural choice.
Now I had no idea how big that group would become and the influence that Toronto machine
learning group would have on deep learning today.
So some people asked me about this and I said, well, I know I kind of stumbled upon this
group.
It seemed the right place to be for all the right reasons, but it ended up being an amazing
time to be there.
This is for me 2004 to 2009, really the start of the deep learning movement.
So a lot of the individuals that are leading the major industrial research labs or even
the nonprofit efforts like OpenAI, for example, they were students in the group at the time.
A lot of the key papers and key ideas that were published and disseminated at that time
have gone on, right, to be sort of the foundations of A&M.
And a lot of it came out of that group and the groups that we were having close collaboration
with, including the Montreal, now they call it Montreal Institute for Learning Algorithms
run by Yasha Bengeo, and then the group at NYU, which at that time was led by you on
the can.
And mentioning NYU, that's where I went immediately after PhD.
We had a good relationship, working relationship between these three labs, Montreal, Toronto
and NYU.
I considered both as postdoc options, but ultimately decided to go to New York for a couple
years and work there as a postdoc, but felt the pull to come home after that.
It was really exciting working in New York with Jan with Rob Fergus, another professor named
Chris Breggler, and that really actually got me working in computer vision more.
And then I came back in 2011, and my heart was really pulling me not just towards coming
back to Canada, but also towards an academic position.
So I had the opportunity to join the faculty at Guelph in 2012, and that's when I started.
It's been five years.
Nice, nice.
And you mentioned a bunch of names there, but you didn't mention that your advisor for
your PhD was Jeff Hinton.
I should, yeah.
I didn't mention a bunch of names, but I didn't credit my PhD advisor, I'm sorry, Jeff,
I'm sorry.
But I've talked to so many people who he's, you know, impacted via advising and other
ways, so.
That's right.
I was co-advised by Jeff and another individual named Sam Royce, who was really influential
in machine learning.
He passed away actually in 2009, right when I started at NYU.
It was tragic to lose him, but I certainly wanted to note his influence on my PhD as well.
It was really amazing having one very senior advisor, Jeff, who really experienced having
worked in the field, but also someone quite junior Sam had started as a faculty, I think,
around 2005 or so.
And he was full of energy and also helped me along the way.
Awesome.
Awesome.
So you did a talk here this morning.
Tell us a little bit about what your talk was about.
Sure.
So I broke my talk up into three parts.
The first part was just introducing myself and telling people a little bit about the work
that we do in Guelph and the types of machine learning problems we're interested in.
The second part of the talk was focused on the challenges and also the opportunities
in AI, and that was more of a technical discussion of what was coming around the corner.
And then the third part of the talk was about some of the barriers, some startups might
be facing.
We're here at the George and Partners portfolio conference, so there's many startups in
the audience.
I've done a lot of work with startups, and I tried to focus on, again, some of the barriers
they might face building companies.
So why don't we start with kind of a rundown of the challenges and opportunities as you
see them?
Sure.
So I started by talking about the technological changes coming towards us, and I think the
first one that I started with was the move from what I would call largely predictive systems
to creative systems.
So when I say predictive systems, I mean these systems that were used to interacting
with on a daily basis, the systems that might give us a temperature forecast tomorrow,
or we might get up our mapping application and would tell us an estimated time to get
it from A to B, or the people on the financial side might be interested in forecasting the
price of a particular financial instrument the next day.
But those types of inputs, they're either a category or they're a number, they're pretty
low dimensional, and there's usually a single right answer.
And so when I talk about the movement towards creative systems, I'm talking about systems
that produce high dimensional output, and where there's no single right answer.
So examples of this sort of more on the creative side would be art creation or poetry or music.
And while these are some of the more culturally flavored activities, which gets some attention,
there's also some real commercial applications such as automatic email reply or conversational
dialogue systems, or I showed a proposed design for a robot that's creating meals and serving
to them to you every day.
And so I talked about some of the challenges in building those kinds of systems.
I thought that one was pretty interesting.
Folks that listen to the podcast will be pretty familiar with the idea of generative
networks and style transfer and creating all the efforts we've seen to create movie
scripts and poetry and all this kind of stuff.
And so the art example has been the front and center for me for a while.
But then when you kind of describe the recipe creation, that's a totally different domain
than one that we hear about all the time, maybe because there are a bunch of different disciplines
that need to come together for us to really explore, you know, or fulfill that, the Jetsons
vision.
Totally.
Yes.
But we're, you know, quickly moving towards an area where a lot of opportunities and value
exist around generative, using neural networks specifically in AI in general for generative
purposes.
What are the key challenges there in your mind in that transition?
I'm particularly interested in this as an engineer working in engineering school.
I build things and I think we're seeing design migrate over from purely human design
to at least at this the next few years being machines and humans working together on design
and building objects, whether they be recipes or they be parts for vehicles or aircraft.
So I think the major challenges in working towards a more algorithmic design would be what
I pointed out this morning, namely the fact that there's no single right answer for design.
You have potentially infinite number of solutions to a problem or designs that would be acceptable.
And this makes it very hard to come up with reward signals or what we would call objectives
for machine learning systems depending on how they're trained, right?
So for us, when we think about a simple task, like image classification, image goes in,
category comes out.
We compare it with the ground truth category, there's usually a single right answer.
For a system going back to this recipe example, how do you sort of, how do you measure the
output of the system that cooks you a meal? I mean, you can, I guess, get some sort of
subjective judgment of the person eating the meal, but it's not really calibrated with
other people.
And it's also just that single reward, these are the types of rewards maybe that are being
used in reinforcement learning systems, they're very weak signals as well.
So it'd be nice to maybe find some sort of medium between this like weak subjective reward
or the explicit guidance that works for supervised learning systems.
The other path for a long way in that, against that challenge, like that particular approach.
Yeah, I think we've seen, as you've mentioned, the listeners of the podcast are going to
be familiar with examples, particularly on the visual side of generative systems.
That's kind of where we're stuck right now is evaluating generative systems, coming
up with quantitative metrics, one to evaluate them, but also maybe as a way of feeding
this kind of quantitative metric back into the system to make them better, right?
How do we improve the objectives to train them?
So then we've also seen a lot of progress really recently on the reinforcement learning
reward side, but we aren't really anywhere, I think, on the sort of merger of those two
systems.
I think there's a lot more to be done.
And particularly, we've seen a lot of nice examples in the generative space of language,
so machine translation systems, conversational dialogue systems, but I think we're still
stuck in coming up with the right kinds of metrics, so do you have, say, an English sentence
going in, a French translation coming out, again, there's so many possible valid translations.
We're still, in most cases, stuck at measuring a single, maybe I would say a canonical example
given in some data set with the output of the system rather than really considering
the space of the potential answers it could give.
Okay.
One of the examples that you use was inbox by Google, which I also use and you went a little
further.
You have a percentage in your mind of the time that you use that for responses, not quite
there yet, but I do use the responses every once in a while.
But you also talked about, you talked about a bunch of concepts, you know, transfer
learning, you know, meta learning, feshut learning, and one of the questions that I had
as you were kind of going through this was, you know, what are the, kind of the mechanisms
and approaches for using, you know, in a large scale system, the feedback that you provide
by selecting, you know, one of these responses in conjunction with the, like, the broader model
that's trained for everybody.
And what is that, what's that problem called, what are the approaches, like, how far are
we along in, you know, developing a body of thinking around that?
Right.
So I think I was referring to this Google inbox client and I'm a big fan of it, and the
user of it.
And like you said, some percentage of the time, the auto email reply feature, it's what
I would call a human and a loop system.
And what I was saying earlier this morning is essentially, I wouldn't want to necessarily
hand over all my email to an automatic reply system.
Sure.
AI is not at that stage where I could stop writing emails and people would just interface
with me through this, this agent, but it's working at the level where it can propose several
candidate replies, and I can still execute judgment over there.
I can decide not to send the email at all.
I can decide to not accept the proposals and write an email myself.
I'm still completely in control, but it's making me more efficient when it, once in a
while, proposes something that I can just click on and it will send.
So I would say this is a system, it's a human and a loop system.
It's where I maintain the judgment over what goes out, and I see this as an effective paradigm
of humans and machines working together over the near term.
But I also really like this idea of the transition from sort of full human control over a particular
task all the way to fully automated performance, but this gray area in between, and a company
that I co-founded in Toronto named Kindred, they're actually exploring this for robotics,
where essentially the company is teaching robots to perform tasks that are very difficult
to automate by allowing a human operator to control one or more robots.
So the robot will be autonomous, but when it gets into trouble, it can be taken over
by a remote operator who sort of gets it out of that, whatever it's stuck to, and the
hope is again to, if this happens enough times, the robot learns about the way that the
human assisted in getting out of that particularly difficult situation so that it becomes more
and more autonomous.
So again, it's not 0% or 100% automation, we're sort of exploring that gray area in between.
So I really like this paradigm.
Because you're using inbox, it's presenting you these possible emails that you might want
to respond with, it gives you three, you choose one.
That's potentially augmenting the set of training, label training data that the system has.
And one way for Google in particular, or someone building a system like this in general,
is to kind of throw that all in and continuously update the model and produce better models
that are trained on more data.
It strikes me that another way for a system like this to operate is that there's a general
component of the model, but then there's a subcomponent of the model that's personalized
to me and the way I respond.
And the question is really, is anyone doing that?
Does that have a name?
Are there architectures for that?
Have you ever come across that?
Well, I would say this fits into the idea of personalization.
And I think it's important for a product like inbox to have some element of personalization.
A colleague actually told me that he doesn't use inbox because it makes him sound like
a California dude.
He said it puts exclamation marks on everything he says and uses terms like awesome exclamation
mark, which he wouldn't say himself.
So interesting.
And he also claims that when I send him emails, he can tell when it's coming from the
Google inbox, got a reply system.
So again, what would fix something like this and maybe make him an adopter is a system
that would adapt to his own style of writing emails.
So it also ties back to the bias element of the conversation in a more subtle way than
we sometimes think about it.
Exactly.
So why is it making him sound like a California dude?
Well, maybe it was a bunch of emails from people in California, right?
So it certainly ties back to that.
I think in terms of how to talk about this and you even raised the idea of having a model
that's been built from a whole lot of data, sort of a master model, and then personalize
models for each of the people and sort of adapting.
I think we do see it.
It's an instance of transfer, right?
So I was talking today about how do you deal with these problems where you have a very limited
amount of labeled data, and I said, well, it's very popular right now to train a model
in a big generic data set and then cut off the top of it and then replace that with something
more specific and then train on a very much smaller set of data.
So you can take something like generic object recognition, a big image net style data
data set and then tackle a task like bird species classification, which is fine grain,
but you have much less data.
But that works because in the big system, there are birds in it, right?
The data set image net has birds, so you can learn about feathers and wings and colors
of birds and beaks and those sorts of features.
That works when there's good match between the two different domains.
And so this is what you're saying in terms of personalization, it can be viewed that way
as well.
You have a large data set of a whole bunch of different speakers and you can learn a
model on that, but then you want to transfer or adapt this system to a particular individual
where you have a smaller subset of data.
And it makes sense, you wouldn't want to necessarily have a model for each individual person
that work in isolation because there's probably not enough data there to generalize well.
In that case, you want to capitalize from the all of the email that Google is holding
in its service with people using Gmail.
Okay.
Okay.
So additional challenges that you were describing.
Oh, yeah.
So I think we had gotten, we'd really only gotten across the sort of the two opportunities
coming across.
I could move into challenges or I could tell you about another couple of things that are
coming across as sort of trends.
So maybe I'll try to finish those off.
The two trends that I hadn't mentioned yet, one was this idea of moving from careful human
construction to learning to learn.
So right now, like these, the systems are like the output of the hard work of graduate
students and the faculty members advise in them and researchers and practitioners.
I mentioned about the migration from feature engineering to architecture engineering, right?
The way people describe deep learning often is that, oh, it's the end of feature engineering.
We no longer have domain experts who craft very specific features.
We can learn all the features with deep learning.
Right.
And I saw you had a picture of Stephen Merritti's article on your slide, which I've talked
about on the podcast a while ago.
Okay.
Fantastic.
So I love that blog post and it really, I think it's totally accurate.
It moves into the world of architecture engineering.
And so one way of getting out of this is essentially having these metal learning style algorithms.
I mentioned a specific example in our lab where we're dealing with multimodal data.
So in this case, we might have video and audio and we had motion capture coming in.
And so we're figuring out how to actually merge those different modalities.
I mean, the nice thing about deep learning models is with multimodal learning, you have
so many opportunities of how to extract representations from the different modalities and how many
levels of representations you should go for each of those.
And when they should be merged together and which modalities should be merged, but there's
all these decisions to be made.
And so you can either have a grad student like we had who we just really skilled at figuring
this all out and spends a year working towards a competition.
But ultimately, we'd like to hand that over to an algorithm that figures that out.
And that's what we've done.
So that's one instance of learning in architecture.
I know Google Brain has been working on this with reinforcement learning.
They worked on learning optimizers.
They're now working on, and other people are also working on learning activation functions.
So really like it's, yeah, hand it over to the algorithm and this metal learning is really
exciting.
There's some great work that was done at Twitter for you go right now, show Google
over to Google Brain on learning an algorithm that's just good at few shot or one shot
learning.
Also an instance of metal learning.
So there's, it's an exciting area.
I think after Steven's article came out, I spent a long time trying to, through my interviews,
trying to understand the process of architecting deep neural networks.
And I guess it took me longer to, you know, retrospectively, it took me longer than
it should have to figure out, you know, the gradient descent by graduate student there
with a couple of different versions of this, and graduate student descent.
And you know, to, at least, you know, it seemed like a year ago or so, like that was the
state of the art.
But since then, you know, you described a number of methods for kind of automating architecture.
One of the ones that you mentioned was a Bayesian based approach and there were some
others.
Can you go into a little bit more detail on the various ones that you mentioned?
Sure, so we explored a couple of different approaches in, in my lab for the multimodal
learning problem.
One approach is Bayesian optimization, which a lot of people in the deep learning field
are familiar with from the point of view is doing model search or hyper parameter optimization.
So these are the decisions that we all need to make about how many layers and how many
units per layer and what kind of activation function and then on the, the learning algorithm
side, how long do we train for?
Should we use atom optimizer or should we use RMS prop or what should our regularization
coefficients be?
There's all these decisions and, and with deep learning, there's more of these decisions
in classical machine learning models.
So people have proposed Bayesian optimization as a, a suitable tool and it, it's actually
been very successful in automating some of the hyper parameter search.
So we, in, in our first example, we just viewed architecture as another hyper parameter
and we proposed essentially a, a search space of potential architectures in which this
modality fusion could happen.
And then we had a, Bayesian optimization algorithm with a, the main technical achievement
was a, a, a, a, a, a, a, a way of assessing similarity between different architectures.
And that was a building block for the Bayesian optimizer to basically search over that
space of potential fusion architectures.
Okay.
And it came up with one that would beat the graduate student descent method.
It in about 30 or so proposals, report different architectures.
Okay.
The downside to that system is when I'm saying 30 different architectures, each of those
had to be trained and evaluated.
And then that result given to the Bayesian optimizer such that it could propose the next
one.
So it's this iterative method in which you're training full architectures to convergence,
you're evaluating them, you're choosing another one going back and evaluating.
And so it gets quite slow.
We explored a second approach in which we do, we view architecture searches to CASTIC
regularization.
It's kind of a meaty thing to say, but it's what you see in methods like dropout where
people just knock out activities randomly.
In neural networks, there's also drop connect where people knock out weights.
And this is done on an example by example basis.
So every time you present a new example to the model, you knock out a different subset
of hidden activities where you knock out a different subset of weights.
So they call this CASTIC regularization and it's been shown to make networks generalize
better.
And it was very popular until some things like batch norm came along and people started
working with that.
But still, it's a general principle for us.
We did this kind of block wise, knocking out certain weights inspired by an approach by
a graduate student at CMU called block out.
And what this student found with this block out is that if you knock out certain blocks
of weights, this can give you very different architectural patterns made through this
weight structure.
So you can have sort of mergers of groups of hidden units or splits or you can just completely
ignore certain features that are being discovered in the network.
And we basically propose a modality aware version of this.
So as this training, explore many different multimodal fusion architectures and then eventually
converge to one that worked pretty well.
So that ended up being more efficient than the basic optimization approach.
Okay.
Okay.
That's pretty technical.
No, that's great.
That's great.
That's great.
And then there was another challenge.
Yes.
So I talked about the idea, which is both an opportunity and a challenge of explainability
in AI.
Right?
And I don't know if you've talked much about explainability yet.
It's a little bit about it.
I don't think you mentioned it in your talk, but I did an interview with Carlos Guestrin
who has a paper called Lime, which seeks to do explainability.
I appreciated the, you were quoting someone else, I believe, and you took issue with, you
know, we often talk about neural networks as black boxes.
And I think you, you, well, you can, yeah, sure, I can talk about that.
It was actually a quote by Cuyengan Cho, a researcher at NYU, and he came to Toronto last
summer very graciously to be part of this next AI program for startups.
So part of that program, we bring in world-class individuals like Cho, and they talk about
various things he was doing in course on NLP.
But he criticized people calling neural networks black boxes and said that they're actually
white boxes.
And that was kind of neat because this, or after my talk, Nikola Paprano talked about
black box versus white box attacks.
And it's the same concept here.
And in some sense, you can open them up, you can look at their parameters, you just happen
to have hundreds of millions of parameters most of the time, and they're just uninterpretable,
right?
So they're not, I mean, if you're accessing them through an online service, like in Paprano's
work, they were trying to attack a method that had been deployed, I think one was on
Metamine services, one was on Amazon, one was on Google.
And if you're interacting through the predictions, yes, it's black box.
But if you're the person evaluating the machine learning system, or maybe you're
your model, it's black box, right?
And generally, still uninterpretable.
It's still uninterpretable.
So that's why we're keen to move to more interpretable systems or explainable systems
in certain setups.
So we've looked at it sort of in the medical space, we've looked at it in the financial
prediction space forecasting, and then we've looked at sort of the classical vision problems
on the benchmark data sets that everybody else benchmarks on.
And yeah, I guess it's like when you teach about software and you're talking about requirements
gathering, and how much money you're going to spend on each stage of the software development
lifecycle, the same thing, assess the risks, like some problems require more careful consideration
of risks, others don't.
And I'd say that the same thing about interpretability and explainability, I mean, let's see what
the application is, who are the users, what are the risks involved.
And in many cases, we likely want to make the system more explainable.
Okay, then opportunities.
Yeah, we've arrived to opportunities.
Yeah, actually, we've gone, sorry, we've gone through a lot of opportunities and I'll
move more into sort of barriers.
Let's say that the last part was some of the barriers.
And when I talked about barriers, it was, I'll just go quickly through them, data.
The next one was talent.
And then the third one I talked about was building trust.
So we can maybe go in and dissect each of these.
And first of all, for data, I think in deep learning, we've seen a tremendous number
of really cool examples of deep learning working in practice, but I would argue that it's
been done in fairly limited set of domains.
The ones I mentioned were the big three vision, speech and audio processing, and then natural
language.
Right.
And these are generally unstructured domains where there's a lot of data, label data in particular.
These are the sorts of applications being pursued by the commercial internet giant threat.
And actually, it's something I've had a struggle with in my lab just motivating some students
to tackle other kinds of problems where benchmark data sets are not available.
So I actually mentioned today, for example, some agricultural applications that we've
worked on, but again, you're rewarded more as a researcher to conduct your experiments
reasonably quickly, get your papers out, and compare to other people in the literature.
And you know, so you download ImageNet, you propose a new architecture, you publish paper
on it.
At the end of the day, if you want to solve a problem that actually, a really important
problem like growing food in an environmentally friendly way and sustainable way, and that
gives decent yields for the farmers, and you want to explore, say, deep learning for
remote sensing in agricultural fields, this involves a crazy amount of data collection.
It takes a lot of work to get in the field, do those flights, say UAV flights, do the
ground truthing, which involves actually collecting samples, say you're looking at soil properties
or nitrogen properties of plants.
So this might take a summer, it might take multiple growing seasons, and you actually
don't see the effects of any interactions until the end of the growing season when you
actually can measure neat yields.
So this is not the same time frame of a lot of the experimentation that happens in machine
learning.
So again, going back to the AlphaGo example, the system is able to play two and a half
million games against itself because it can carry out a game and get the reward in less
than a second, in an agricultural situation you can't get a reward in less than a second,
it's six months, right, or longer.
So anyways, this is a bit of a ramble just saying that we're fairly limited in the ways
that we're applying deep learning right now, and it's a lot about the data, how do you
collect the data, where do you get the data, how hard is it to gather that process it?
And so anyways, there's a quote by Christix and then I gave it at the end, which was
data is really the key ingredient to AI because it's the missing ingredient.
So we publish our algorithms, like there's great algorithms out there, they're available
to people.
Compute power has really grown and it's become cheaper, so we have access to great compute.
So it's really the data that we don't have, that's the missing ingredient for these sorts
of problems I'm talking about.
And it's also for companies, it's the proprietary ingredient.
So people aren't publishing data sets as quickly as they're publishing papers on algorithms.
What's one of the challenges that I see is data.
Well, if you can maybe quickly summarize the other two and leave us with any final thoughts
as we wrap up.
Sure.
So in terms of the other two, one is talent, and I think this is the idea of companies
face with, well, how are we going to fill these positions where we need really skilled
people in machine learning, and whether, you know, questions around, oh, do we need PhDs,
our masters graduate, it's good enough.
Can we take somebody training in a different area and move them into machine learning field?
I think there's a lot of amazing stuff going on here, particularly in, not just Canada,
but Toronto.
There's an announcement last week by the provincial government to fund Vector Institute
with $30 million to work with the Ontario universities to develop professional graduate
programs and AI and machine learning.
And in five years, we're going to be looking at graduating 1,000 students per year.
That will be the goal for steady state in this province.
I think we're addressing that issue with talent right now.
But as I mentioned today, we also have a lot of professors leaving academics, going to
work at industry part time or full time, and we need to work on retaining those individuals.
I think decisions like starting the government supporting AI initiatives, like Vector and
Meela and Amy and Alberta, those are all working toward making this, making academics attractive
in this field versus industry, but I think we need to do more to encourage the people
staying in academics or moving into academics to continue to train the next generation.
And then the final topic was on trust, right, building trust.
And actually, we've already touched on a couple of those issues.
Explain to you as one, bias and fairness.
I tend to like the idea of using technology as actually as Nikola Papernos said in his
talk following mine, these ideas are on differential, privacy preserving algorithms to increase
people's trust in machine learning systems, rolling out technology like fair representations
for removing bias from algorithms that make predictions.
And so I think I feel pretty good about the future of AI, I guess I'll summarize it
that.
It's a great field to be working in.
This Toronto area is a great place to be working on these technologies.
There's a lot more to come.
And I think in terms of the problems, I do see a diversification in the future of the types
of tasks we're solving.
And I think they're at least working with the startups in the next AI program.
I also see a lot of interest, both from the companies building these technologies, but
also from the investment side, and companies that are doing social good as well.
So building both profitable companies, but also solving real important issues.
Right.
And so that's why I look forward to.
Awesome.
Well, Graham, thanks so much for taking the time to sit with me and share all that you
shared about your kind of vision for this and how you see it.
Thanks a lot to my pleasure.
Great to meet you.
Great.
Thank you.
All right, everyone, that's our show for today.
Thanks so much for listening and for your continued feedback and support.
For more information on Graham or any of the topics covered in this episode, head on over
to twimlai.com slash talk slash 62.
To follow along with the Georgian partner series, visit twimlai.com slash GPPC 2017.
Of course, you can send along feedback or questions via Twitter, at twimlai, or at Sam
Charrington, or leave a comment on the show notes page.
Thanks once again to Georgian partners for their sponsorship of the show.
Be sure to check out their white papers, which you can find by visiting twimlai.com slash
Georgian.
Thanks again for listening and catch you next time.
