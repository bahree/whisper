1
00:00:00,000 --> 00:00:10,000
All right, everyone. I am here with Samir Singh. Samir is an assistant professor in the

2
00:00:10,000 --> 00:00:16,240
Department of Computer Science at UC Irvine. Samir, welcome to the Twimal AI podcast.

3
00:00:16,240 --> 00:00:22,400
Thank you, Simon. Excited to be here. I'm excited to chat with you. Why don't we get started

4
00:00:22,400 --> 00:00:26,800
by having you share a little bit about your background? What got you started working

5
00:00:26,800 --> 00:00:32,720
in machine learning? So yeah, I've been sort of thinking about AI as a goal for a long time,

6
00:00:32,720 --> 00:00:37,840
you know, since teenage years reading all these science fiction books and wanting to solve AI.

7
00:00:38,720 --> 00:00:45,120
And in my naivety, I thought the solution was to build robots initially. And so I didn't

8
00:00:45,120 --> 00:00:50,080
undergrad in electrical engineering, trying to do hardware sort of things, build manipulators,

9
00:00:50,080 --> 00:00:55,440
and sort of small vehicles and things like that. And then when I started reading research

10
00:00:55,440 --> 00:01:01,120
papers, I kind of realized that at least at that time, people were spending their whole PhDs

11
00:01:01,120 --> 00:01:07,680
just trying to grasp like a cup of something, right? And that didn't seem like AI to me.

12
00:01:07,680 --> 00:01:13,600
So I quickly sort of shifted more towards the software side of things. And at that time,

13
00:01:13,600 --> 00:01:20,400
again, a lot of AI was doing a star search and things like that, which seemed a lot more cooler.

14
00:01:20,400 --> 00:01:26,160
But didn't seem to quite get at the learning or the intelligence aspect of it. So slowly,

15
00:01:26,160 --> 00:01:32,160
I made my way and found machine learning and you know, been working in machine learning for a long

16
00:01:32,160 --> 00:01:39,600
time now. And within machine learning, I think NLP ended up being this sort of really attractive

17
00:01:39,600 --> 00:01:45,360
application for me. It seemed a good mixture of being very, very practical, especially with internet

18
00:01:45,360 --> 00:01:51,440
and everybody posting stuff online, but also addressing some of the fundamental things that excited

19
00:01:51,440 --> 00:01:56,480
me about AI being able to communicate, being able to understand humans and things like that. So

20
00:01:57,200 --> 00:02:00,880
that's kind of how I made my way into doing machine learning.

21
00:02:02,160 --> 00:02:06,720
Nice. Nice. There's some out there that think that embodied applications like robotics are

22
00:02:07,360 --> 00:02:14,240
really the only way we're going to get close to to AGI. You know, that may be only the

23
00:02:14,240 --> 00:02:19,760
roboticist that think that though. Well, what you're taking on that is that's, you know, that was

24
00:02:19,760 --> 00:02:26,560
the goal that you started out with. Yeah, I think embodied AI is such a like concrete thing that

25
00:02:26,560 --> 00:02:32,800
I feel like that there is something very attractive about that. But you know, everybody's struggling

26
00:02:32,800 --> 00:02:39,200
with what is AI? How do we define it and things like that? I think of learning being a key component

27
00:02:39,200 --> 00:02:47,440
and learning doesn't have to be grounded necessarily. And if something is learning from internet and

28
00:02:47,440 --> 00:02:54,800
from YouTube videos, audio streams, that might be sufficiently intelligent, right? Like who knows?

29
00:02:55,600 --> 00:03:00,960
Our kids right now really grounded or are they completely in the internet? And you know,

30
00:03:00,960 --> 00:03:05,840
it's difficult to say even us as humans how grounded we are going to be in the future.

31
00:03:05,840 --> 00:03:15,360
In the matrix, are we is anyone grounded? Exactly. So tell us a little bit about your research focus.

32
00:03:15,360 --> 00:03:19,760
What are some of the areas that you're spending your time on nowadays?

33
00:03:20,480 --> 00:03:27,680
Yeah, so for a long time, I was doing an LP, sort of a lot of different tasks in an LP. And this is

34
00:03:27,680 --> 00:03:34,240
sort of, you know, about, I guess I want to say eight to 10 years ago, where I was so focused on

35
00:03:34,240 --> 00:03:40,160
the task that I was working on that sort of the deep learning stuff kind of went by me. And I was

36
00:03:40,160 --> 00:03:45,840
working on sort of more graphical models and log linear kind of stuff. And by the time I started

37
00:03:45,840 --> 00:03:51,600
doing deep learning things, it was really, really good at everything I was doing. But it wasn't using

38
00:03:51,600 --> 00:03:56,800
any of the cool stuff that I was working on. So that made me feel like, okay, why is it doing what

39
00:03:56,800 --> 00:04:03,360
it is doing? Whether it is, it is actually doing what it is doing. So you could say it was almost

40
00:04:03,360 --> 00:04:09,760
by spite that my research agenda got started. But you know, sort of playing around with these

41
00:04:09,760 --> 00:04:16,480
models, you know, I did my postdoc at University of Washington with Carlos. And with one of his

42
00:04:16,480 --> 00:04:21,760
students at the time, we started looking at, okay, these deep learning models really seem to be

43
00:04:21,760 --> 00:04:28,560
quite good. But we have very little idea of what's going on in them. And so maybe let's try to

44
00:04:28,560 --> 00:04:34,640
find a way to get to some place that we are actually familiar with. So this took me back to my

45
00:04:34,640 --> 00:04:40,480
sort of linear classification or log linear model days where I used to spend a lot of time

46
00:04:40,480 --> 00:04:45,200
looking at feature weights and trying to figure out, okay, is there a problem here? Can I fix

47
00:04:45,200 --> 00:04:52,160
something? Or in my worst days, can I even change some of these weights to try and get it to do

48
00:04:52,160 --> 00:04:58,240
something? And I think, but since none of that was possible in deep learning, we sort of started

49
00:04:58,240 --> 00:05:04,480
thinking about how we could try to get into the internals of deep learning and at least provide

50
00:05:04,480 --> 00:05:09,600
some intuition as to what's going on. And that's sort of where I started working on explainability.

51
00:05:09,600 --> 00:05:15,920
And we came up with this line algorithm in order to explain black box models. And since then,

52
00:05:15,920 --> 00:05:21,600
I think, you know, explainability has been really interesting. But initially we were hoping for

53
00:05:21,600 --> 00:05:29,600
explainability to tell us why deep learning models are working. But often they ended up telling us

54
00:05:29,600 --> 00:05:36,960
why the deep learning models were not working. And so my tragically sort of has become a lot more

55
00:05:36,960 --> 00:05:44,880
on looking at why are these explanation methods telling us that they are not working when all

56
00:05:44,880 --> 00:05:50,000
of these leaderboards and New York Times articles are telling us that they are working. There seems

57
00:05:50,000 --> 00:05:56,080
to be this big mismatch. And so why does this mismatch exist? And what can we do about it?

58
00:05:56,720 --> 00:06:00,400
So in the recent couple of years, I've been thinking a lot more about evaluation,

59
00:06:00,400 --> 00:06:04,400
about debugging and things of that nature for machine learning.

60
00:06:04,400 --> 00:06:12,560
Nice. Nice. Yeah. I'll say that the line paper, which goes back four or five years,

61
00:06:13,520 --> 00:06:17,680
well, first of all, as you noted, we're still struggling with the same problem with deep learning

62
00:06:17,680 --> 00:06:26,400
generally. But the paper itself and that method itself seems to have had pretty strong staying power.

63
00:06:26,400 --> 00:06:33,680
Like people are still talking about it. People are still using it. By the time this show comes out,

64
00:06:33,680 --> 00:06:39,040
we will have had our recent event model explainability forum where we dig into

65
00:06:40,800 --> 00:06:47,120
a lot of the current issues in model explainability. But when people are evaluating explainability methods,

66
00:06:47,120 --> 00:06:56,240
Lyme is one that still comes up. Any thoughts or reflections on that work? Did you expect it to be

67
00:06:56,240 --> 00:07:01,760
as important as it has turned out to be when you were working on it? That's a good question.

68
00:07:01,760 --> 00:07:07,040
I guess you hope that every paper you're working on, I would hope more people think that every paper

69
00:07:07,040 --> 00:07:13,840
they're working on is actually going to be world changing and get people excited. But it usually

70
00:07:13,840 --> 00:07:21,600
doesn't happen with Lyme, of course, people got excited about it a lot more than we anticipated.

71
00:07:21,600 --> 00:07:28,640
And there are both good and the bads of it. We are close to Lyme, so we know what it's doing in a way

72
00:07:28,640 --> 00:07:34,160
that we know what it's capable of and what it's not capable of. And we've been more recently

73
00:07:34,160 --> 00:07:38,960
with the collaboration with Hema, who I think you had on your podcast a few weeks ago. We've also

74
00:07:38,960 --> 00:07:43,840
been working on going back and looking at Lyme and saying, okay, can it be manipulated if they were

75
00:07:43,840 --> 00:07:47,920
like bad agents and stuff? And we show that it's actually not that difficult to do that.

76
00:07:49,200 --> 00:07:56,400
And I think just like machine learning is considered to be this thing that will come and solve

77
00:07:56,400 --> 00:08:01,680
all your problems. Lyme seems to be well. Whatever problems machine learning has,

78
00:08:01,680 --> 00:08:05,840
now Lyme is going to solve it for you and it's not like that right now. It's one of the

79
00:08:05,840 --> 00:08:13,040
explainability techniques. It is a useful tool, I believe, especially for evaluation, but it's not

80
00:08:13,040 --> 00:08:19,040
something that has solved the model explainability problem. And if anything, it initiated the discussion,

81
00:08:19,040 --> 00:08:23,520
and I'm glad that people are talking about it because it's a really, really difficult problem,

82
00:08:23,520 --> 00:08:29,040
even to define what explainability is, much less solved with in a single paper.

83
00:08:29,040 --> 00:08:39,520
And so that paper set the stage for the paper we'll be spending some time talking about today,

84
00:08:39,520 --> 00:08:48,080
and that is beyond accuracy, behavioral testing of NLP models with checklists, which was the

85
00:08:48,080 --> 00:08:58,240
ACL 2020 best paper winner. Congratulations. Thank you. Tell us a little bit about that paper and how

86
00:08:58,240 --> 00:09:05,280
the some of your prior research led you to that problem. And so I think a lot of my research,

87
00:09:05,280 --> 00:09:11,360
especially in collaboration with Marco, has been sort of looking at this mismatch between what

88
00:09:11,360 --> 00:09:19,200
accuracy seems to suggest and what we know models are actually capable of. And accuracy is

89
00:09:19,200 --> 00:09:24,480
something that's very quantitative. It's on the leaderboard out there. You can say 10% better

90
00:09:24,480 --> 00:09:30,560
or worse or whatever it is. But on the other side, what models are good at and what models are

91
00:09:30,560 --> 00:09:37,040
bad at is not something that's quantified or formalized or even well-defined as far as the

92
00:09:37,680 --> 00:09:45,840
community is concerned. So what we wanted to do was try to look at this mismatch and say,

93
00:09:45,840 --> 00:09:52,720
okay, there should be some way of thinking about this that goes beyond just creating another leader

94
00:09:52,720 --> 00:09:59,040
board or introducing a different data set. So last couple of years, we this work started maybe two

95
00:09:59,040 --> 00:10:06,160
years ago with this work we did called Sears. Where the idea was very simple, what we're going to do

96
00:10:06,160 --> 00:10:11,920
is we're going to think of doing a serial attacks on models, but instead of doing this adding some

97
00:10:11,920 --> 00:10:16,240
noise that people don't understand, we're going to take the original instance and we're just going

98
00:10:16,240 --> 00:10:23,600
to paraphrase it. So it should be exactly the same sentence in terms of its meaning, but the actual

99
00:10:23,600 --> 00:10:29,040
form might be different. And we had an automated tool for doing this building upon a lot of other

100
00:10:29,040 --> 00:10:34,320
people who've been working on back translation and things like that. And paraphrasing was what you

101
00:10:34,320 --> 00:10:39,760
called the adversarial attack in this case? Yes, because it was a paraphrase that was getting the

102
00:10:39,760 --> 00:10:48,480
model to change its prediction. Got it, right? And so, you know, like, why did China take some action?

103
00:10:50,000 --> 00:11:00,320
Or let me think of a different. So what is that thing on the table versus what's that thing on the

104
00:11:00,320 --> 00:11:05,680
table? This is like a very simple version of a paraphrase. Of course, this means the same to us,

105
00:11:05,680 --> 00:11:10,480
but at that time, there were models that got, like, gave a different prediction for these two

106
00:11:10,480 --> 00:11:17,920
instances. So that was one way to start characterizing what the problems were in the models.

107
00:11:17,920 --> 00:11:22,800
We continue this work. So we can call this sort of robustness to paraphrases, I guess. We had

108
00:11:22,800 --> 00:11:30,320
another follow-up work that was looking at consistency. So if you had, is there a bird in the image?

109
00:11:30,320 --> 00:11:36,400
Or let's say how many birds are in this image? The model might say two. And then we say,

110
00:11:37,120 --> 00:11:43,280
are there two birds in this image? The model should clearly say yes, but the model did not often.

111
00:11:44,320 --> 00:11:49,520
And what that meant was model was not even being consistent with its own sort of what things

112
00:11:49,520 --> 00:11:57,360
are going on. And so that also sort of started thinking as about how can we characterize what the

113
00:11:57,360 --> 00:12:03,520
pitfalls in these models are. And that all sort of culminates in this work in checklist,

114
00:12:03,520 --> 00:12:11,120
where we kind of create a tool to allow users to think a lot more about testing in a structured way,

115
00:12:11,120 --> 00:12:22,240
essentially. Okay. Yeah. Identifying the pitfalls or possible failure modes is

116
00:12:22,240 --> 00:12:30,160
fairly different from identifying the causes of those pitfalls. How well do we understand

117
00:12:30,160 --> 00:12:34,560
for the various models that you're looking at what's actually causing the problems?

118
00:12:36,320 --> 00:12:44,000
Very little, I think. So you would hope that, so just to sort of clarify what checklist does is it

119
00:12:44,000 --> 00:12:49,120
creates essentially helps you create a bunch of tests for your model. And when the tests fail,

120
00:12:49,120 --> 00:12:53,760
similar to traditional software engineering, sometimes the test fails and you're like,

121
00:12:53,760 --> 00:13:00,720
oh, I forgot to use that variable instead. I use some other variable. But sometimes it fails and

122
00:13:00,720 --> 00:13:05,600
you're like, oh, wait, the whole my whole piece of code, the way I was structuring it is completely

123
00:13:05,600 --> 00:13:10,960
wrong. Right. And so even in machine learning, when something fails, either it could be some artifact

124
00:13:10,960 --> 00:13:18,320
of the training data, or it could be, it could lead to like a string of five research papers that

125
00:13:18,320 --> 00:13:23,920
eventually solve their problem, hopefully. Right. So, so for example, fairness is a good example

126
00:13:23,920 --> 00:13:31,200
of that. Right. If your model is being unfair, there are no easy. I'm going to replace all

127
00:13:31,200 --> 00:13:36,400
males with females or something like that and solve the problem. That doesn't do it. But

128
00:13:37,200 --> 00:13:42,560
checklist focuses more on at least do you know if this is a problem or not? And can you get an idea

129
00:13:42,560 --> 00:13:53,840
of how much this is a problem? Okay. So, you describe checklist as analogous to kind of behavioral

130
00:13:53,840 --> 00:14:02,240
testing in software engineering, you know, kind of riff on that analogy a little bit. Yeah. So,

131
00:14:02,240 --> 00:14:06,800
that's actually where we started. We were like, people have been building all these complex

132
00:14:06,800 --> 00:14:11,360
software systems. Machine learning is not the first one to come up and start predicting the

133
00:14:11,360 --> 00:14:17,360
sentiment. There are way other more complicated software that do much more complicated things.

134
00:14:17,360 --> 00:14:21,840
How do they approach testing? And this is something that that all of us know and we wanted to see

135
00:14:21,840 --> 00:14:28,160
how many of those lessons can be taken and applied to machine learning. So, the idea was we we create

136
00:14:28,160 --> 00:14:32,560
a bunch of different kinds of tests and I can sort of walk you through what they look like.

137
00:14:32,560 --> 00:14:39,600
But the easiest one to understand is something we call minimal functionality text at minimum functionality

138
00:14:39,600 --> 00:14:46,080
test where it basically looks like a unit test in software engineering, right? So, I'm not going to

139
00:14:46,080 --> 00:14:51,760
think too much about what the everything that the model is supposed to be doing, but I'm going to

140
00:14:51,760 --> 00:14:59,200
pick a single phenomena. So, say we are trying to do sentiment analysis and we just want a very simple

141
00:14:59,200 --> 00:15:09,680
like does it understand negation or not, right? If I say I this movie is not good, is my sentiment

142
00:15:09,680 --> 00:15:15,520
classifier able to do that or not? Right. So, that's a very simple unit test. That by itself,

143
00:15:15,520 --> 00:15:21,200
if the model does it or not can be useful, but we sort of have this whole templating engine that

144
00:15:21,200 --> 00:15:29,920
says I can say something like this blank is not blank where we fill blanks with the first

145
00:15:29,920 --> 00:15:35,680
blank with a bunch of nouns and the second blank with a bunch of positive adjectives and negative

146
00:15:35,680 --> 00:15:40,960
adjectives and then we have an expectation over whether the model will predict positive or

147
00:15:40,960 --> 00:15:46,960
negative review for it. And so, in this case, we create many test cases as they would say in software

148
00:15:46,960 --> 00:15:52,480
engineering and we test a machine learning model by just running the model output through these

149
00:15:52,480 --> 00:15:59,120
things. And what this does is it hasn't told you like does this model, if you pass all the test,

150
00:15:59,120 --> 00:16:05,600
you don't know whether the model has definitely learned how to do negation or not, but if it does

151
00:16:05,600 --> 00:16:11,040
fail most of this test, it would be a red flag and you would say okay, the model doesn't even

152
00:16:11,040 --> 00:16:16,560
understand such a simple negation, it probably doesn't understand negation. So, based on this intuition,

153
00:16:16,560 --> 00:16:20,800
we created a bunch of different kinds of tests and I can walk you through those a few things.

154
00:16:20,800 --> 00:16:30,080
Yeah, before we do that though, you gave sentiment analysis as an example, what is the scope of

155
00:16:30,080 --> 00:16:36,080
checklist? For example, we spent a lot of time talking about language models nowadays and

156
00:16:36,640 --> 00:16:44,480
transformers and things like that, does it address those kinds of tasks as well or is it limited to

157
00:16:44,480 --> 00:16:52,720
classification? What are the boundaries of this work? Yeah, so I think in the paper itself,

158
00:16:52,720 --> 00:16:58,240
we had a bunch of tasks that go beyond sentiment analysis or just simple classification, I guess.

159
00:16:58,240 --> 00:17:03,200
And just to sort of motivate why we picked sentiment analysis, this was also one of the system

160
00:17:03,200 --> 00:17:09,600
that one of the tasks that research papers are constantly looking at and trying to do better on

161
00:17:09,600 --> 00:17:16,240
and by some metrics, we are better than humans on sentiment analysis, which may not be a surprise

162
00:17:16,240 --> 00:17:21,840
to many people. But also, it was one of those tasks where there were a lot of commercial products.

163
00:17:21,840 --> 00:17:29,360
So, like Microsoft and Google and Amazon make a ready to purchase sentiment analyzer.

164
00:17:29,360 --> 00:17:35,120
And what we wanted to do was apply checklist, not just to some research models that we had lying

165
00:17:35,120 --> 00:17:39,840
around, which were all transformable based like Word and Roberta, but also apply checklist to

166
00:17:39,840 --> 00:17:47,520
these commercial systems to see what they were lacking in and sort of be able to compare across

167
00:17:47,520 --> 00:17:52,800
research and industrial models. But in the paper, we also did things like question answering,

168
00:17:52,800 --> 00:17:57,360
where you're given a question, you have to come up with an answer, given a paragraph,

169
00:17:57,360 --> 00:18:03,120
we did a paraphrase detection. So, there's a Korak question pair dataset where you have pairs of

170
00:18:03,120 --> 00:18:08,080
questions and you want to detect whether their paraphrases are not. We didn't directly do language

171
00:18:08,080 --> 00:18:13,520
modeling, but that's something that we've been working on. And it's a little bit trickier to

172
00:18:13,520 --> 00:18:19,840
define what even a test is for language models, but I think there are ways to use checklist for that as

173
00:18:19,840 --> 00:18:30,880
well. Okay. Okay. So, walk us through the kind of the array of tests. Yeah. So, the idea here is that

174
00:18:30,880 --> 00:18:38,640
you want to figure out what are the capabilities of the model. So, we conceptualize it as a matrix

175
00:18:38,640 --> 00:18:43,760
where there are a bunch of rows, where the rows are sort of what you want to test about the model.

176
00:18:43,760 --> 00:18:49,120
Right. So, suppose simple negation is something that you might want to test, and the easiest way

177
00:18:49,120 --> 00:18:56,640
to test that is to create a bunch of minimum functionality tests, like unit tests. For some other

178
00:18:56,640 --> 00:19:05,280
ones, we wanted to test things like robustness to location names. Right. So, my flight to US was great.

179
00:19:06,560 --> 00:19:13,920
I want to see how much, whether the model is robust to changes in the country name. If I replace US

180
00:19:13,920 --> 00:19:18,960
with a different country, or if it's a city, then replace it with a different city, the sentiment

181
00:19:18,960 --> 00:19:24,800
doesn't change, but other models robust to that or not. That doesn't quite fit into the unit test.

182
00:19:24,800 --> 00:19:30,880
So, we had a second category of tests, which we call invariance tests, where we are,

183
00:19:30,880 --> 00:19:36,400
and this is also something that's similar in software engineering, where we are essentially mutating

184
00:19:36,400 --> 00:19:42,240
the input or perturbing the input, in ways that we know shouldn't affect the sentiment.

185
00:19:42,960 --> 00:19:49,760
In this case, replacing US with China or any other country, and then trying to see how often

186
00:19:49,760 --> 00:19:55,280
does that change the actual output of the model. And this was one of the ones which was quite

187
00:19:55,280 --> 00:20:01,280
surprising to us, because many times when you change the location, the output of the model

188
00:20:02,000 --> 00:20:11,200
changes, which was surprising and disappointing. Some of the other kind of capabilities you might

189
00:20:11,200 --> 00:20:17,760
want to check was, you're like, okay, I don't care what the review was, but if I explicitly add a

190
00:20:17,760 --> 00:20:24,880
strongly negative statement to it. So, if I say blah, blah, blah review ends, I really hated it

191
00:20:24,880 --> 00:20:32,320
at the end. You would hope that the model will not become more positive towards this review,

192
00:20:32,320 --> 00:20:37,760
right? Now, this is also not invariance, because in case, in this case, you're changing the input,

193
00:20:37,760 --> 00:20:42,880
but you're trying to test the directionality of the output. So, this is more of a directional

194
00:20:42,880 --> 00:20:50,880
test. And we just want to make a test where you've got, you use a similar kind of strong,

195
00:20:50,880 --> 00:20:56,800
definitive statement, and you want to see if the model even picks up on that, even if there's

196
00:20:56,800 --> 00:21:03,760
other things that are more ambiguous or even positive. Is there, do you have a test kind of

197
00:21:03,760 --> 00:21:12,320
blown those on? Not really, because it's always like, we wanted to define tests where we wanted

198
00:21:12,320 --> 00:21:20,960
it was a failure rate of 0. We didn't want something where 80% would be still, sorry, 20% failure

199
00:21:20,960 --> 00:21:27,360
would still be okay, right? So, with these strong statements, you can never be sure whether

200
00:21:28,080 --> 00:21:34,720
they contradict enough with the review text, right? So, in this case, we wanted to keep it simple,

201
00:21:34,720 --> 00:21:40,960
right? Like, we wanted to say at least this thing, the model should be able to get 0% failure on.

202
00:21:40,960 --> 00:21:47,360
Right? So, it shouldn't become more positive is a very simple statement. That's why it was a

203
00:21:47,360 --> 00:21:54,000
surprise to us, where many times it did like more than a third of the time, it just became more

204
00:21:54,000 --> 00:21:59,920
positive when you added phrases like that, right? And these are some of these word sort of commercial

205
00:21:59,920 --> 00:22:08,000
systems as well. So, yeah, so these kind of tests are in some sense, the way we describe it is

206
00:22:08,000 --> 00:22:14,160
like that, I guess mathematically more of a necessary condition for a model to get deployed,

207
00:22:14,160 --> 00:22:19,920
just because you get 0% failure doesn't mean that the model is safe to deploy, just because

208
00:22:19,920 --> 00:22:24,720
it's similar to software testing, right? Just because all your tests pass doesn't mean you don't

209
00:22:24,720 --> 00:22:30,320
have any more work to do. You probably either need to write more tests or write more code and

210
00:22:30,320 --> 00:22:38,400
it's right, but at least if a test fails, there is a red flag and you know exactly what the model

211
00:22:38,400 --> 00:22:43,520
is not able to do. And just by breaking these things down, I think that's the main contribution

212
00:22:43,520 --> 00:22:47,760
of checklist is making, hopefully making people think a little bit more about these different

213
00:22:47,760 --> 00:22:58,560
dimensions of the problem than just the single number. Got it. Is there an analogy to code coverage

214
00:22:58,560 --> 00:23:08,080
in these types of tests? That's a good question. There has been some work that tries to do these

215
00:23:08,080 --> 00:23:15,120
things where they, the analogy to code coverage is to look at the neurons inside the transformer

216
00:23:15,120 --> 00:23:21,120
or whatever and try to make sure there are enough inputs in your test case or whatever,

217
00:23:21,120 --> 00:23:27,120
at the same time, whatever you're planning to do that go through all the neurons at some point.

218
00:23:27,120 --> 00:23:34,080
We haven't looked at things like that. Yeah, we are sort of going back to like it almost like a

219
00:23:34,080 --> 00:23:40,560
black box model way of thinking about this, right? You are someone who cares about sentiment analysis

220
00:23:40,560 --> 00:23:45,440
or you care about whatever you're trying to use machine learning for and presumably with that,

221
00:23:45,440 --> 00:23:53,040
you have a set of capabilities that you expect someone who's claiming it can do sentiment analysis

222
00:23:53,040 --> 00:23:58,480
should be able to do, right? So we are sort of, I guess, delegating that to the users,

223
00:23:58,480 --> 00:24:06,000
rather than thinking about the internals of the model. And so from a practical perspective,

224
00:24:06,000 --> 00:24:18,080
if I read the paper, download the code, are you suggesting that this is a kind of point

225
00:24:18,080 --> 00:24:24,880
to set your model and run it and it's a full, it kind of throws the book at your model and tells

226
00:24:24,880 --> 00:24:34,320
you where it's weak or do I have to adapt checklist with my particular model in mind and the things

227
00:24:34,320 --> 00:24:43,760
that maybe the things I'm concerned about or how engaged as a user have to be in taking advantage of

228
00:24:43,760 --> 00:24:49,920
this. Yeah, so that's a good question and we've been thinking a lot about this. And I guess the answer

229
00:24:49,920 --> 00:24:55,920
is it's somewhere in between depends specifically on what you're working on. So I guess the easiest one

230
00:24:55,920 --> 00:25:03,520
would be if you are creating a sentiment analysis system in your company, then it's trivial. You can

231
00:25:03,520 --> 00:25:10,480
just download our code and run it. If you are doing one of the tasks that are not already supported

232
00:25:10,480 --> 00:25:16,640
in checklist, then it's pretty easy and we've tried to like Mark was written really nice documentation

233
00:25:16,640 --> 00:25:21,200
and sort of walk through it. It's pretty easy to get started and start thinking about okay,

234
00:25:21,200 --> 00:25:27,920
what are the capabilities for this specific task and how do I write it down in a way that we can

235
00:25:27,920 --> 00:25:35,680
generate a lot of tests. And checklist is really, really useful for use cases like that. I would say

236
00:25:35,680 --> 00:25:39,600
we did some experiment also to sort of make sure that this is the case and we can talk about that

237
00:25:39,600 --> 00:25:46,800
a little later. But what we also want to try to do and we are sort of getting there slowly

238
00:25:46,800 --> 00:25:52,320
is to make it incredibly easy for people to contribute the tests or for us to keep

239
00:25:53,200 --> 00:25:59,600
growing the set of tests that are available in checklist. So if you're doing a task that is either

240
00:25:59,600 --> 00:26:05,360
slightly adjacent to what we already have. For example, you're doing paraphrasing but not question

241
00:26:05,360 --> 00:26:11,520
paraphrasing, then you may have additional tests and we make it really easy to include it back in

242
00:26:11,520 --> 00:26:18,320
checklist. We also want to make it really easy to evaluate any new model that comes out. So

243
00:26:18,320 --> 00:26:23,680
we want to, for example, if hugging phase has a new transformers model, we want to make it

244
00:26:23,680 --> 00:26:29,440
incredibly easy to also test and generate sort of almost a report card of okay. On negation,

245
00:26:29,440 --> 00:26:36,000
we got this much failure rate on something else, we got this much and so on. Got it. Yeah. The

246
00:26:36,000 --> 00:26:44,720
task that the user has to do to adapt it to their model is it writing tests or writing some kind

247
00:26:44,720 --> 00:26:51,200
of meta test or test recipe that checklist then uses to generate a bunch of tests. I kind of heard

248
00:26:51,200 --> 00:26:56,800
both in your description. Yeah, so it kind of supports both, but I think the way we've been

249
00:26:56,800 --> 00:27:02,160
approaching this is for the first part, you just have to think a lot. So you have to think about

250
00:27:02,160 --> 00:27:07,680
what are the different capabilities and checklist can not quite help you with that. Maybe it helps a

251
00:27:07,680 --> 00:27:14,640
little bit by some of the automated exploration tools, but the idea is so we can take any specific

252
00:27:15,440 --> 00:27:21,360
safe fairness or something like that. I want my model to be fair. Well, how do we answer something

253
00:27:21,360 --> 00:27:29,200
like this and say I'm doing question answering, right? So okay, how can I test whether the model is

254
00:27:29,200 --> 00:27:38,160
being fair and let's be more specific? Let's say gender fairness, right? So if I think the model

255
00:27:38,160 --> 00:27:43,360
is not fair, one way to test whether it's fair or not is to come up with a really simple

256
00:27:43,360 --> 00:27:53,840
context for the question answering system where we can say John is not a doctor, but Mary is

257
00:27:54,960 --> 00:28:02,160
and then ask the question, who is the doctor, right? This thing is something we can easily write out

258
00:28:02,160 --> 00:28:07,440
and we know the answer should be Mary because it's obvious from this sentence and that's a test,

259
00:28:07,440 --> 00:28:14,800
right? Now we can say, well, if the model fails or not, it could be dependent on the word choice

260
00:28:14,800 --> 00:28:21,360
of John or Mary. So I might maybe I want to replace John or Mary with any other names, right? So we

261
00:28:22,080 --> 00:28:27,440
John with any other male name, Mary with any other female name and checklist has some of these

262
00:28:27,440 --> 00:28:32,320
lexicon built in. So you can easily just say, okay, I'm going to create a template that says

263
00:28:32,320 --> 00:28:42,640
male first name is not a doctor, but female first name is, who is the doctor, female first name,

264
00:28:42,640 --> 00:28:47,920
right? That's one level of templating. Then you can say, well, why should it be just doctor?

265
00:28:47,920 --> 00:28:54,080
Maybe there are other professions that we also want to test. So we have a tool that just you

266
00:28:54,080 --> 00:28:59,200
can hide doctor and it suggests a bunch of professions and you can say, okay, I want to

267
00:28:59,200 --> 00:29:04,720
say pick a bunch of these as part of the test and by doing all of these things, you can essentially

268
00:29:04,720 --> 00:29:11,520
create thousands of use cases automatically, even just for the single statement that I talked about

269
00:29:11,520 --> 00:29:18,240
and then just quickly test it whether the model your model is actually passing them with a sufficient

270
00:29:18,240 --> 00:29:27,760
tolerance or not. Is there, do you envision a way to use checklist or maybe some future

271
00:29:27,760 --> 00:29:37,600
version or evolution of this work? You're not just to give you kind of a binary pass fail or

272
00:29:37,600 --> 00:29:45,360
sufficient or necessary, I forget which of those conditions you mentioned, but necessary.

273
00:29:46,960 --> 00:29:53,040
But rather to give you insights into your model that you can then turn around into the

274
00:29:53,040 --> 00:29:57,760
take into the training loop, the training process, for example, you mentioned these,

275
00:29:59,360 --> 00:30:10,080
you know, the gender bias example. Is there, can it tell you more than you have a problem and maybe

276
00:30:10,080 --> 00:30:16,080
how you might go about fixing the problem? Yeah, how you go about fixing a problem is a much more

277
00:30:16,080 --> 00:30:22,640
difficult step, but I would say the first step of going about fixing a problem is to know

278
00:30:22,640 --> 00:30:28,560
all the problems that your model might have, right? So yeah, I guess the short answer is no,

279
00:30:28,560 --> 00:30:35,600
that's not something we focus on. In the future, we are thinking of ways to do it. The tricky thing here

280
00:30:35,600 --> 00:30:44,560
is I think of these tests almost as like you should think of the test set in machine learning,

281
00:30:44,560 --> 00:30:49,520
right? So test set is supposed to be this hidden test set just because you get some error on the

282
00:30:49,520 --> 00:30:53,520
test set doesn't mean you just add those instances to your training data, right? If you do that,

283
00:30:53,520 --> 00:30:59,840
you've lost the value of the test, right? So in the in similar sense, these tests or what checklist

284
00:30:59,840 --> 00:31:06,400
is doing should be treated as something slightly external to the training process because you've come

285
00:31:06,400 --> 00:31:12,480
up with the one way of phrasing the negation and you want to just test that as a proxy for other

286
00:31:12,480 --> 00:31:18,400
ways of phrasing the negation. And so if you put this check this test somehow back into the

287
00:31:18,400 --> 00:31:24,320
training loop, you've lost that advantage you had and now you have to come up with either a different

288
00:31:24,320 --> 00:31:31,440
formulation of negation or assume that the model has learned negation and both of those might be

289
00:31:31,440 --> 00:31:37,280
difficult or wrong things to do. So I think at least for now, we are thinking of checklist a little bit

290
00:31:37,280 --> 00:31:45,600
as a test set that's a lot more fine grained and essentially customizable to your specific task.

291
00:31:45,600 --> 00:31:54,240
Got it, got it. Tell us a little bit about the evaluation process for as you built this out.

292
00:31:54,880 --> 00:32:03,200
Yeah, so we built this tool and we sat down on a bunch of tasks and we did sentiment classes

293
00:32:03,200 --> 00:32:09,360
and QQP and squad and these are very different from each other, classification,

294
00:32:09,360 --> 00:32:16,400
paraphrasing and question answering and we were feeling pretty confident about how useful checklist was

295
00:32:16,400 --> 00:32:24,080
but we are biased because we developed it. So we decided to evaluate it and evaluating a

296
00:32:24,080 --> 00:32:29,200
testing pipeline is a little bit tricky but Mark was spent a lot of time thinking about what

297
00:32:29,200 --> 00:32:34,640
would be the right way to do this and we essentially converged on two separate evaluations,

298
00:32:34,640 --> 00:32:41,360
both of them involved involving users. So the first one was since we are working with these

299
00:32:41,360 --> 00:32:47,520
commercial models already, let's try to go to a commercial team that's actually responsible

300
00:32:47,520 --> 00:32:53,360
for one of these products and not only sort of find out what their testing methodology is

301
00:32:54,480 --> 00:33:01,200
but also propose checklist to them and see what they think of it. So we had I think like a five

302
00:33:01,200 --> 00:33:07,200
hour session with one of the senior developers in that team and we were like here's checklist,

303
00:33:07,200 --> 00:33:15,040
here's how it works, here's how it give them a walkthrough and then say like go crazy,

304
00:33:16,240 --> 00:33:22,560
do what you guys do and see if you find this thing useful. And what was surprising was that

305
00:33:23,280 --> 00:33:28,400
not only did they were they able to quickly confirm some of the bugs that they knew

306
00:33:28,400 --> 00:33:34,480
were in the products which is nice but they were also able to find a bunch of new problems that

307
00:33:34,480 --> 00:33:40,160
they didn't know was already in their model just by the use of checklist right and they were like

308
00:33:40,160 --> 00:33:44,480
oh yeah now I need to get people to fix this stuff because this is a problem right and that's

309
00:33:44,480 --> 00:33:51,600
exactly the use case we expect checklist to be useful for. And so that was one of the things and

310
00:33:51,600 --> 00:33:56,880
we got pretty good feedback and I think Mark was working with a bunch of people to help

311
00:33:56,880 --> 00:34:02,480
integrate checklist into the existing pipelines. The other set of evaluation we did was

312
00:34:03,440 --> 00:34:10,000
we went to sort of university students and people who have at least taken an NLP course so they

313
00:34:10,000 --> 00:34:16,160
know NLP but they probably and they most of them hadn't really worked with the QQP data set,

314
00:34:16,160 --> 00:34:22,080
this question paraphrasing data set. So we wanted to test is checklist useful even if you're not

315
00:34:22,080 --> 00:34:28,640
already a domain expert right and we explained to them what QQP was, we explained to them

316
00:34:29,200 --> 00:34:35,840
what checklist is and it was a slightly cut down version of checklist because we didn't have

317
00:34:35,840 --> 00:34:42,080
five hours to spend with each user and we said okay now can you find bugs in this system

318
00:34:43,360 --> 00:34:50,400
and essentially you know we I can refer to the paper for exact numbers but people who were using

319
00:34:50,400 --> 00:34:56,800
checklist the full capability of checklist were able to find many more bugs or many more different

320
00:34:57,360 --> 00:35:03,280
problems with the model but even for each problem they were able to figure out a lot more test cases

321
00:35:04,000 --> 00:35:10,800
so hundreds or on that order instead of writing just a few sentences so you got more confident

322
00:35:10,800 --> 00:35:15,360
that those problems were actual problems because they were being tested on so many more instances

323
00:35:15,360 --> 00:35:20,640
and so this both these combination of evaluation made us feel like this could be a pretty useful

324
00:35:20,640 --> 00:35:28,720
tool for the community whether it's someone who's an expert and is trying to deploy machine learning

325
00:35:28,720 --> 00:35:34,880
for practical purposes or it's some research project that's trying to be illegal.

326
00:35:34,880 --> 00:35:48,320
Got it. There are groups that have developed for analogous problems in computer vision like

327
00:35:48,320 --> 00:35:54,240
bias testing tool kits I'm thinking of you know things along the lines of gender shades like

328
00:35:54,240 --> 00:36:03,440
trying to determine whether a given model or service can you know do what it does whether it's

329
00:36:03,440 --> 00:36:09,920
you know predicting gender or predicting age or things like that independent of the ethnicity of

330
00:36:09,920 --> 00:36:20,400
the sample data. What you're doing with checklist you know A strikes me as like a meta level above

331
00:36:20,400 --> 00:36:30,320
those kinds of tests but I'm wondering if you are aware of or can envision something similar

332
00:36:30,320 --> 00:36:38,320
applied to computer vision or other domains beyond NLP. Yeah so that's a good connection

333
00:36:38,320 --> 00:36:45,520
and I would say we are quite inspired by a bunch of those papers when we were developing checklist

334
00:36:45,520 --> 00:36:49,920
specifically in NLP also there have been a bunch of tools where they're looking at a specific

335
00:36:49,920 --> 00:36:55,280
phenomenon and trying to look at okay is sentiment analysis system being fair or not and things

336
00:36:55,280 --> 00:37:01,760
like that right so yeah checklist is very much inspired and related to all of that work

337
00:37:01,760 --> 00:37:08,240
and in some sense tries to unify this whole thing like you said in a meta way. We actually did

338
00:37:09,040 --> 00:37:14,880
play around a lot with computer vision and that's not something and I'm an expert in so partly

339
00:37:15,520 --> 00:37:23,360
I would say that's on me but I think there are definitely cases where you can use checklist style

340
00:37:23,360 --> 00:37:28,560
approach to computer vision. We played around a little bit with visual question answering

341
00:37:28,560 --> 00:37:35,200
and we would do things like put up the image and things like that to try and see if the output

342
00:37:35,200 --> 00:37:41,280
remains the same but it's a little bit trickier to do so yes I think there are ways to extend

343
00:37:41,280 --> 00:37:46,400
checklist to apply it to computer vision and videos and things like that but it would require

344
00:37:46,400 --> 00:37:54,480
a little bit more research than we had time for. Do you have a sense for some examples of what

345
00:37:56,000 --> 00:37:58,640
minimum functional tests might be in the vision domain?

346
00:38:03,200 --> 00:38:11,600
Yeah so the ones that I think of are maybe too simple and in some sense might be easy to do so

347
00:38:11,600 --> 00:38:18,160
for example we can imagine creating an image right like taking a something that we know is a dog

348
00:38:18,160 --> 00:38:23,280
and placing it in different parts of the image to see if it gets confused right that's probably

349
00:38:23,280 --> 00:38:30,240
too simple and I'm sure most classifiers would work but then I can take dogs and combine them with

350
00:38:30,240 --> 00:38:36,400
a typical background so I can take of course a scene from the park and you put a dog in it

351
00:38:36,400 --> 00:38:41,920
is probably okay but can I take a scene of the sky and put a dog in it I still want the classifier

352
00:38:41,920 --> 00:38:49,200
to predict dog can I take a slice of pizza and put a dog on it and things like that and so you

353
00:38:49,200 --> 00:38:55,040
can imagine these kind of tests would be easy to create and maybe there's a computer vision

354
00:38:55,040 --> 00:39:00,800
paper that's doing things like that but would give you an idea of okay does the background confuse

355
00:39:00,800 --> 00:39:07,120
the classifier or not right and I imagine that some of these tests might actually be pretty might

356
00:39:07,120 --> 00:39:16,160
show problems in existing computer vision systems awesome awesome any additional thoughts that you

357
00:39:16,160 --> 00:39:20,720
would want to share with folks that are kind of interested in what they're hearing in and you

358
00:39:20,720 --> 00:39:29,600
might want to explore more or might want to you know build on on this work you know what should folks

359
00:39:29,600 --> 00:39:36,000
be thinking about you know as they're thinking about this problem yeah I would say part of the

360
00:39:36,800 --> 00:39:42,560
slightly danger which checklist is a lot of the things we do are in some sense seem very obvious

361
00:39:42,560 --> 00:39:47,040
right like especially once you see the examples you're like okay clearly we should be

362
00:39:47,040 --> 00:39:53,280
testing for these things and and that kind of makes it a little bit dangerous because people might

363
00:39:53,280 --> 00:39:59,280
easily overlook it right and so you know somebody might go into their team and like hey we have a

364
00:39:59,280 --> 00:40:05,840
sentimental analysis team are we doing testing and the answer could come back as yes but testing is

365
00:40:05,840 --> 00:40:11,840
not a yes or no question it's like are you doing a thorough job of testing it and so what we're

366
00:40:11,840 --> 00:40:19,120
really hoping is checklist would inspire people to do a lot more thorough testing and in some sense

367
00:40:19,120 --> 00:40:26,400
when they make accuracy judgment for a model they try to qualify it with some of the sort of things

368
00:40:26,400 --> 00:40:32,160
that checklist produces right and hopefully they go much beyond checklist and do much more finer

369
00:40:32,160 --> 00:40:41,760
grain reporting on what model does just like we wouldn't want the performance of a human employee

370
00:40:41,760 --> 00:40:47,040
to be reduced to a number in some sense but you know a performance review involves a lot of

371
00:40:47,040 --> 00:40:52,320
different dimensions to it as these machine learning models are becoming more involved in our

372
00:40:52,320 --> 00:40:57,360
pipelines I think it's useful to think about them as like what are their capabilities and what

373
00:40:57,360 --> 00:41:02,400
are their weaknesses and be able to say something about them rather than just saying I'm going to use

374
00:41:02,400 --> 00:41:09,440
this model because it's the top of the leaderboard and therefore best. You mentioned that the

375
00:41:10,640 --> 00:41:17,040
some of the examples that are covered by these minimum functional tests are you know obvious at

376
00:41:17,040 --> 00:41:24,720
least in retrospect do you think that part of what the paper offers is kind of concrete language

377
00:41:24,720 --> 00:41:31,040
to refer to these so that teams can say what we fail you know xyz test we fail the negation test

378
00:41:31,040 --> 00:41:40,880
we fail the strong you know the strong final statement test. Is the paper even concrete in defining

379
00:41:40,880 --> 00:41:45,840
these you know an establishing language there do you think that that becomes a way the people

380
00:41:45,840 --> 00:41:51,520
use this that's a good point we didn't think of that use case but yes that that could be a

381
00:41:51,520 --> 00:41:57,280
contribution although I would say that that would be a contribution to maybe people who haven't

382
00:41:57,280 --> 00:42:05,040
seen sort of more linguisticy aspects of NLP right so a lot of our test names are heavily inspired

383
00:42:05,040 --> 00:42:10,960
by stuff that has already been studying in linguistics we're just testing phenomena of language

384
00:42:10,960 --> 00:42:16,640
and we're not the first one to think about what are the different phenomena and so the names we

385
00:42:17,760 --> 00:42:23,680
use in checklist are a lot our common knowledge do a lot of NLP researchers but I can imagine

386
00:42:23,680 --> 00:42:29,200
to someone who's been designing for example sentiment classifier they may not know what the

387
00:42:29,920 --> 00:42:36,080
sort of what does semantic role labeling have to do with sentiment right whereas we have a bunch

388
00:42:36,080 --> 00:42:43,520
of tests that check specifically for semantic role labeling and things like that got it got it

389
00:42:43,520 --> 00:42:49,440
awesome well Samir thanks so much for taking the time to share with us a little bit about what you're

390
00:42:49,440 --> 00:42:56,640
doing awesome work and congrats once again on the best paper award yeah thanks a lot Sam this was

391
00:42:56,640 --> 00:43:10,080
a lot of fun thank you

