WEBVTT

00:00.000 --> 00:13.400
Welcome to the Twimal AI Podcast.

00:13.400 --> 00:26.160
I'm your host Sam Charrington.

00:26.160 --> 00:30.320
Have you been enjoying our Twamokon coverage, but want more?

00:30.320 --> 00:35.640
Twamokon video packages are now available for advanced purchase over at twamokon.com

00:35.640 --> 00:37.160
slash videos.

00:37.160 --> 00:42.560
The package features over 13 hours of content, including all the awesome keynotes and panels

00:42.560 --> 00:49.120
you've heard on the podcast, all of the breakout sessions, including 9K study and 8 tech track

00:49.120 --> 00:56.000
sessions, as well as the highly regarded team tear down panels with Airbnb and SurveyMonkey.

00:56.000 --> 01:01.680
Again, visit twamokon.com slash videos for more info.

01:01.680 --> 01:06.720
And now on to the show.

01:06.720 --> 01:11.560
Alright everyone, I am on the line with Phoebe DeVries and Brendan Mead.

01:11.560 --> 01:16.160
Phoebe is a postdoctorate fellow with the Department of Earth and Planetary Sciences

01:16.160 --> 01:20.720
at Harvard, as well as an assistant faculty at the University of Connecticut.

01:20.720 --> 01:26.480
And Brendan is professor of Earth and Planetary Sciences at Harvard, and an affiliate faculty

01:26.480 --> 01:28.840
in computer science as well.

01:28.840 --> 01:32.640
Phoebe and Brendan, welcome to this week in machine learning and AI.

01:32.640 --> 01:34.480
Thanks so much for having us Sam.

01:34.480 --> 01:35.480
Thank you.

01:35.480 --> 01:40.520
Let's get started by having the two of you introduce yourselves.

01:40.520 --> 01:42.280
Brendan wanted to go first.

01:42.280 --> 01:43.280
Thanks Sam.

01:43.280 --> 01:49.000
Yeah, I'm a professor of Earth and Planetary Sciences at Harvard, where I've been for the

01:49.000 --> 01:51.000
past 13 years prior to that.

01:51.000 --> 01:55.080
I got undergraduate degree in history of science from Johns Hopkins University, and I got

01:55.080 --> 02:00.840
my PhD in Earth and Planetary Sciences from MIT.

02:00.840 --> 02:05.000
And here at Harvard, we do a lot of work on computational science with a particular

02:05.000 --> 02:07.840
emphasis on understanding earthquakes.

02:07.840 --> 02:08.840
Fantastic.

02:08.840 --> 02:09.840
And Phoebe?

02:09.840 --> 02:10.840
Yeah.

02:10.840 --> 02:15.160
Well, I did my undergrad at Harvard and applied math.

02:15.160 --> 02:20.400
And then I spent a year at Cambridge doing a master's degree in glaciology, and then came

02:20.400 --> 02:24.720
back to work in Brendan's research group for my PhD.

02:24.720 --> 02:27.520
And now I'm a postdoc with him.

02:27.520 --> 02:32.400
And for most of my PhD, we worked on sort of large-scale forward modeling problems of

02:32.400 --> 02:36.800
time-dependent stress changes after large earthquakes, but now we're really focusing

02:36.800 --> 02:39.560
on machine learning for earthquake science.

02:39.560 --> 02:46.160
So Brendan, the description of your lab is that you are focusing on deconvolving, tectonic

02:46.160 --> 02:52.320
and earthquake cycle signals across Japanese islands to identify the coupled, subduction

02:52.320 --> 02:59.000
zone interface that ruptured during the Great to Hoku-Oki earthquake of 2011.

02:59.000 --> 03:02.000
Yeah, that's true.

03:02.000 --> 03:04.920
That's certainly something that we worked on.

03:04.920 --> 03:07.200
Let me tell you how we got into this problem.

03:07.200 --> 03:08.200
Okay.

03:08.200 --> 03:12.640
And our lab, what we've been really interested in, is trying to say as much as we can about

03:12.640 --> 03:15.680
earthquakes before they happen.

03:15.680 --> 03:20.360
And one of the great revolutions of the past 20 to 25 years now has been the ability to

03:20.360 --> 03:28.360
measure using very high precision GPS measurements how the earth's surface moves at scales where

03:28.360 --> 03:33.160
we can actually see the strain accumulation in the Earth's crust, the elastic strain that

03:33.160 --> 03:35.840
will be released in a future-grade earthquake.

03:35.840 --> 03:40.320
And so for a long time, our goal has been to try and map this type of behavior around

03:40.320 --> 03:45.000
the globe, and try and use this information to say something about the future sizes and

03:45.000 --> 03:47.760
locations of large earthquakes.

03:47.760 --> 03:55.680
To do this, we have employed a lot of HPC approaches ranging from visco-elastic modeling to boundary

03:55.680 --> 04:02.760
element modeling, and more recently, our interests have drifted more towards the more direct

04:02.760 --> 04:09.240
applications of ML and AI to these problems where there are vast data sets waiting to be

04:09.240 --> 04:10.240
explored.

04:10.240 --> 04:12.720
And that's what got us to the paper we wrote recently.

04:12.720 --> 04:13.720
Awesome.

04:13.720 --> 04:17.760
And Phoebe, you were the lead author on this paper, which was called Deep Learning of

04:17.760 --> 04:21.400
Aftershock Patterns Following Large Earthquakes.

04:21.400 --> 04:22.400
Tell us about the paper.

04:22.400 --> 04:24.840
What was the main thrust of the work?

04:24.840 --> 04:30.800
Well, we were interested in Aftershock's from a machine learning perspective, or Aftershock

04:30.800 --> 04:37.080
locations specifically, because there are these very well-established empirical relationships

04:37.080 --> 04:47.440
that can describe the time decay of Aftershock frequency, and also the likely maximum magnitude

04:47.440 --> 04:49.640
of Aftershocks as well.

04:49.640 --> 04:55.000
But the locations of Aftershocks are a lot more difficult to explain.

04:55.000 --> 05:00.960
So that's really what got us interested in this problem from a machine learning perspective.

05:00.960 --> 05:03.440
There are a lot of data about Aftershocks.

05:03.440 --> 05:10.480
So it's really a sort of good, you know, it's a very good application for machine learning

05:10.480 --> 05:12.840
just off the bat.

05:12.840 --> 05:14.960
What are some of the available data sources?

05:14.960 --> 05:20.280
Is it primarily the GPS data that Brendan was describing?

05:20.280 --> 05:26.800
Yes, there are a lot of earthquake catalogs, just records of where and when earthquakes

05:26.800 --> 05:30.680
have occurred, just thousands and thousands of them.

05:30.680 --> 05:35.520
And what we did in this study was we combined data from two separate catalogs.

05:35.520 --> 05:40.440
One was a catalog of very large earthquakes, most of them larger than six or so.

05:40.440 --> 05:44.040
And the other was a catalog of Aftershock's following those two.

05:44.040 --> 05:48.400
And it was the combination of those two catalogs that enabled us to analyze the relationship

05:48.400 --> 05:52.640
between the main shocks themselves and the Aftershocks that followed.

05:52.640 --> 05:59.400
And is there, is it a challenge to differentiate earthquakes from Aftershocks?

05:59.400 --> 06:03.160
Is that an issue or not really?

06:03.160 --> 06:05.560
That is discussed a lot in the literature.

06:05.560 --> 06:10.760
We decided to take a very sort of broad or simple approach to it.

06:10.760 --> 06:17.760
So we just defined Aftershocks as the earthquakes that take place sort of within a year after

06:17.760 --> 06:23.120
these large main shocks and within 100 kilometers horizontally and down to 50 kilometers depth,

06:23.120 --> 06:26.960
just to keep things simple for this kind of first approach, first try.

06:26.960 --> 06:28.640
Yeah, I think Phoebe is exactly right.

06:28.640 --> 06:34.080
In the literature, you can find arguments that Aftershocks are extremely well-defined phenomena.

06:34.080 --> 06:39.120
And you can find arguments that Aftershocks are nothing more than another part of a generic

06:39.120 --> 06:43.720
earthquake sequence that has been ongoing for hundreds of not thousands of years.

06:43.720 --> 06:48.720
And I absolutely agree with Phoebe that we took a very simplified approach to classifying

06:48.720 --> 06:50.280
them on this study.

06:50.280 --> 06:58.200
You mentioned that your lab has previously spent a lot of time on high-performance computing

06:58.200 --> 07:03.400
based analysis of these earthquakes and Aftershocks.

07:03.400 --> 07:09.560
I'm curious if you can describe that a little bit more as a segue to talking about what's

07:09.560 --> 07:14.440
been new and working with machine learning based approaches.

07:14.440 --> 07:19.000
Phoebe, do you want to talk about your thesis?

07:19.000 --> 07:26.200
Well, I could talk about the visco-elastic modeling piece of what Brendan's group has been

07:26.200 --> 07:27.200
doing.

07:27.200 --> 07:35.640
So we implemented a three-dimensional code to calculate time-dependent stress changes

07:35.640 --> 07:42.480
in the crust after large earthquakes due to visco-elastic relaxation.

07:42.480 --> 07:51.600
And we were using these models to try to get at questions of whether or not or look at

07:51.600 --> 07:56.040
questions of possible delayed earthquake triggering.

07:56.040 --> 08:01.080
One of the applications we are one of the examples that we looked at was the North Anatolian

08:01.080 --> 08:06.160
fault in Turkey where there's been this remarkable sequence of large earthquakes that have

08:06.160 --> 08:11.080
just been marching to the west over time since 1939.

08:11.080 --> 08:20.400
And so we used these visco-elastic models to try to look at what effects visco-elastic

08:20.400 --> 08:29.240
relaxation may have had in terms of loading the hypersenters of the subsequent earthquakes

08:29.240 --> 08:30.840
in the sequence.

08:30.840 --> 08:36.200
So not to get too deep into the science, but this visco-elastic study is essentially modeling

08:36.200 --> 08:42.160
the Earth or maybe the surface of the Earth as kind of using fluid dynamics types of

08:42.160 --> 08:46.160
approaches to try to anticipate earthquakes and aftershocks?

08:46.160 --> 08:49.760
Yeah, so it's a quasi-static model.

08:49.760 --> 08:57.040
So we used Berger's realities to model the behavior of the lower crust and upper mantle.

08:57.040 --> 09:02.120
And it was sort of purely phenomenological, it just seems to explain the data really

09:02.120 --> 09:05.240
well and that's what motivated the models.

09:05.240 --> 09:13.160
The previous approach to calculating these models or doing this analysis was based on

09:13.160 --> 09:14.480
high-performance computing.

09:14.480 --> 09:17.080
What did that entail?

09:17.080 --> 09:20.920
In terms of the high-performance computing behind the visco-elastic models?

09:20.920 --> 09:26.920
Or I guess I'm trying to get at ways that the modeling process may have differed between

09:26.920 --> 09:35.320
what you did before in machine learning and was it this specific problem that said,

09:35.320 --> 09:41.120
hey, this needs to be machine learning and not high-performance computing or for the

09:41.120 --> 09:47.520
class of problems that you tend to see in this space, could you choose either?

09:47.520 --> 09:51.040
And machine learning was kind of the new fancy thing, so we try that.

09:51.040 --> 09:55.800
How do they qualitatively differ in terms of solutions to these kinds of problems?

09:55.800 --> 09:57.680
Those are the kinds of things I'm curious about.

09:57.680 --> 09:59.640
Yeah, Sam, that's a great question.

09:59.640 --> 10:02.760
It turns out one led to the other.

10:02.760 --> 10:10.840
So when Phoebe did the work using this HPC code to try to see if we could explain the

10:10.840 --> 10:15.280
delayed triggering of earthquakes along the North and Italian fault, that ended up taking

10:15.280 --> 10:18.840
about 2.5 million CPU hours.

10:18.840 --> 10:19.840
Wow.

10:19.840 --> 10:25.600
And that was how long it took to try and explain the triggering of about seven or eight

10:25.600 --> 10:26.600
earthquakes.

10:26.600 --> 10:30.880
And we said, we'd like to do this elsewhere, but that's taking a long time.

10:30.880 --> 10:38.080
And so together we came up with the idea to see if we could train a neural network to emulate

10:38.080 --> 10:40.240
our HPC code.

10:40.240 --> 10:46.240
And so what we did was with the HPC code, we were able to generate lots of training data.

10:46.240 --> 10:51.160
And then we trained a neural network to simply map inputs to outputs, all the nonlinear

10:51.160 --> 10:52.160
ones.

10:52.160 --> 10:56.720
And then once we had done all the nonlinear ones, we could just combine the linear part

10:56.720 --> 10:57.720
easily.

10:57.720 --> 11:03.480
And so when we did that, we were able to replace this HPC code with an incredibly compact

11:03.480 --> 11:09.680
neural network, which is able to give almost exactly the same answer and run in 500 times

11:09.680 --> 11:10.680
faster.

11:10.680 --> 11:12.720
Oh, that's awesome.

11:12.720 --> 11:19.720
And that just one of the reasons why the description of your lab that I saw on your Harvard

11:19.720 --> 11:23.800
page caught my interest was it mentioned deconvolving.

11:23.800 --> 11:29.680
I'm assuming that that is referring to motion of tectonic plates and not convolutional

11:29.680 --> 11:30.680
neural nets.

11:30.680 --> 11:34.920
And there's an overlap there.

11:34.920 --> 11:39.920
I would have been way ahead of my time if I had been not clever.

11:39.920 --> 11:40.920
Yeah.

11:40.920 --> 11:47.960
So when we got into the work of interpreting these GPS data, the world was kind of divided

11:47.960 --> 11:48.960
into two halves.

11:48.960 --> 11:53.240
There were the people who looked at the data and thought it told you about strain accumulation

11:53.240 --> 11:55.240
in the earthquake cycle.

11:55.240 --> 11:59.120
And people who thought it told you about long-term tectonic motions.

11:59.120 --> 12:00.600
Neither group was wrong.

12:00.600 --> 12:03.360
In fact, both groups were right.

12:03.360 --> 12:09.120
And it was the ability to integrate and deconvolve those two signals together that really

12:09.120 --> 12:10.760
allowed for progress to be made.

12:10.760 --> 12:15.280
And that involved developing a class of large scale models.

12:15.280 --> 12:18.920
And that's what allowed us to really isolate the part we care about, which is the

12:18.920 --> 12:20.600
strain accumulation signal.

12:20.600 --> 12:23.400
So let's talk a little bit about that modeling process.

12:23.400 --> 12:25.200
How did you go about that?

12:25.200 --> 12:26.200
Yeah.

12:26.200 --> 12:34.400
So the core idea was to think about earthquakes, not as random phenomena, not as things that

12:34.400 --> 12:38.000
popped up in the middle of nowhere, but rather to think about earthquakes as a byproduct

12:38.000 --> 12:39.880
of plate tectonics.

12:39.880 --> 12:46.240
So most earthquakes occur simply because large pieces of the earth's rocky crust are

12:46.240 --> 12:50.440
moving past each other, and they're temporarily stuck together, and they're stuck together

12:50.440 --> 12:54.640
along these localized surfaces called faults that occasionally fail.

12:54.640 --> 12:59.720
And when they do fail, because the frictional resistance to motion is overcome by the stresses

12:59.720 --> 13:03.280
that are accumulated, they rupture in large earthquakes.

13:03.280 --> 13:09.480
So to make progress, what we found we had to do was we had to integrate both the physics

13:09.480 --> 13:13.760
of what was going on at large scales, that is, the motion of tectonic plates, with the

13:13.760 --> 13:18.240
physics of what was going on around faults, that is earthquake cycle physics.

13:18.240 --> 13:22.320
And we were able to do that and link them together, and by linking them together, we finally

13:22.320 --> 13:28.240
had a computational tool that allowed us to tease apart the competing effects due to

13:28.240 --> 13:30.080
do the both sets of physics.

13:30.080 --> 13:33.240
And with that, we were able to isolate the strain accumulation signature that we were

13:33.240 --> 13:34.240
looking for.

13:34.240 --> 13:35.240
Okay.

13:35.240 --> 13:39.760
So you started that by saying, something to get left me with the impression that the

13:39.760 --> 13:47.360
idea that earthquakes are caused by these tectonic shifts or plates was, you know, contentious

13:47.360 --> 13:55.760
in some way, or not, let's start with contentious in some way in the field.

13:55.760 --> 14:02.000
And yeah, it sounds like there's, so one model is this tectonic plate model, and the

14:02.000 --> 14:06.240
other is more this local phenomenon?

14:06.240 --> 14:10.120
I would say it was more just a case of what people were interested in.

14:10.120 --> 14:13.600
They were interested in the tectonic problem, or they were interested in the earthquake

14:13.600 --> 14:14.600
problem.

14:14.600 --> 14:20.440
And I think the realization that a lot of people had, and this started back in the 80s,

14:20.440 --> 14:24.680
and the late 80s and through the 90s and early 2000s, was that those two scales had to

14:24.680 --> 14:25.920
be linked.

14:25.920 --> 14:30.240
The small scale and the large scale had to be linked to really make sense of either.

14:30.240 --> 14:35.820
And that was a computational challenge, in particular, due to the complex geometry of

14:35.820 --> 14:39.520
fault systems of plate boundaries, that that was where the real challenge was.

14:39.520 --> 14:43.600
How did you represent those geometries and integrate them into a computational model

14:43.600 --> 14:45.760
that included the physics for both?

14:45.760 --> 14:52.520
And the specifics of that model, how do you go about developing that?

14:52.520 --> 15:03.040
Well, that takes us back a long time, that takes us back to my PhD, and so the ideas have

15:03.040 --> 15:10.000
been in the air that these scales had to be linked for more than a decade.

15:10.000 --> 15:13.760
And people had done a very nice local scale work on it.

15:13.760 --> 15:19.040
And what I attempted to do was to generalize that work so that we could put in high fidelity,

15:19.040 --> 15:24.480
very geometrically complex representations of fault systems, like those in Southern California,

15:24.480 --> 15:26.480
which are unbelievably complex.

15:26.480 --> 15:32.680
And yet at the same time, have all the motions on the faults in Southern California be consistent

15:32.680 --> 15:36.360
with what's going on between the two large plates, the North American and the Pacific

15:36.360 --> 15:40.240
plate, on either side of the Southern California fault system?

15:40.240 --> 15:45.680
And the hardest part of that was actually not any of the physics part, but the most interesting

15:45.680 --> 15:53.440
part was thinking about the problem very algorithmically, and figuring out how to efficiently

15:53.440 --> 16:00.880
specify how the fault system geometry would be related to the motions of the plates on

16:00.880 --> 16:04.160
either side of these faults.

16:04.160 --> 16:10.120
It was an extremely geometrically complex problem involved mapping back and forth between

16:10.120 --> 16:12.960
four different reference frames.

16:12.960 --> 16:18.320
And I'm really glad I finished it so that I don't have to work on that again.

16:18.320 --> 16:20.840
That's the macro scale piece.

16:20.840 --> 16:27.080
How is that then linked to the micro scale piece, which is if I caught you correctly, that's

16:27.080 --> 16:29.560
more looking at these earthquakes as a time series?

16:29.560 --> 16:30.560
Is that right?

16:30.560 --> 16:31.560
What do you think about it?

16:31.560 --> 16:39.040
Yeah, I think Phoebe's work is what really linked that in, because she added time-dependent

16:39.040 --> 16:44.080
evolving motions due to the coupling between the Earth's crust and the mantle.

16:44.080 --> 16:47.600
And so I think Phoebe can speak to the time-dependent part of that story.

16:47.600 --> 16:48.600
Awesome.

16:48.600 --> 16:49.600
Well, yeah.

16:49.600 --> 16:55.600
Well, we haven't actually totally linked Brendan's block models.

16:55.600 --> 17:04.160
Well, yeah, so we built in some time dependence to the framework that Brendan built over his

17:04.160 --> 17:05.160
PhD.

17:05.160 --> 17:10.400
And since then, using these visco-elastic models.

17:10.400 --> 17:14.800
And that just allowed us to sort of add a time perturbation throughout the earthquake cycle

17:14.800 --> 17:21.280
to sort of incorporate information about when the most recent earthquake had occurred on

17:21.280 --> 17:22.280
that fault.

17:22.280 --> 17:27.840
All right, so the picture of starting to emerge from me, you've got this model that kind

17:27.840 --> 17:35.960
of depends on two time scales, this physical geometric perspective, and this more time-oriented

17:35.960 --> 17:38.560
perspective or local perspective.

17:38.560 --> 17:46.560
And one big challenge is linking these two and kind of driving some consistency between

17:46.560 --> 17:48.040
them.

17:48.040 --> 17:54.840
And then, once you've got this model, you identified the computational complexity of actually

17:54.840 --> 18:03.000
using it and set out to build a deep learning-based approach that would essentially model your

18:03.000 --> 18:04.000
model.

18:04.000 --> 18:07.080
Is that a decent summary?

18:07.080 --> 18:12.360
Sam, that's like the best abstract of what we've done for the past 10 or 15 years I've

18:12.360 --> 18:13.360
ever heard.

18:13.360 --> 18:14.360
That's fantastic.

18:14.360 --> 18:18.000
I really like that way of describing it.

18:18.000 --> 18:25.040
And that was our entry into the machine learning and the machine learning world.

18:25.040 --> 18:32.760
And from that, we eventually found these datasets that we could start making slightly U.S.

18:32.760 --> 18:36.520
model-dependent predictions with, and that's what led us to the paper.

18:36.520 --> 18:37.520
Nice, nice.

18:37.520 --> 18:43.280
So Phoebe, as you set out to explore this machine learning, deep learning world, what were

18:43.280 --> 18:49.200
the things that you found that you were able to apply to your particular problem?

18:49.200 --> 18:53.880
We were just, well, so after we worked on sort of accelerating this computationally intensive

18:53.880 --> 18:59.920
code using a train neural network, we, well, at least from my perspective, I just started

18:59.920 --> 19:05.000
to think of all the different problems in Earth science that involve sort of inputs and

19:05.000 --> 19:10.800
outputs and figuring out what the relationship is between those inputs and outputs.

19:10.800 --> 19:17.200
And so that's really what got us thinking about this aftershock problem because we could

19:17.200 --> 19:23.520
calculate stress changes in the crust and upper mantle after large earthquakes and then

19:23.520 --> 19:30.360
see if we could use a neural network to map those stress changes to aftershock locations.

19:30.360 --> 19:33.680
So it just got us thinking about all these different problems.

19:33.680 --> 19:42.080
What was the nature of the deep learning model that you used for these types of problems?

19:42.080 --> 19:47.720
Did you, was it kind of an off-the-shelf type of a model or was it something that you

19:47.720 --> 19:52.760
crafted from scratch based on the specifics of your problem?

19:52.760 --> 19:55.840
No, it was very, very simple.

19:55.840 --> 19:57.360
We implemented it in Keras.

19:57.360 --> 20:01.880
It was a fully connected network about as simple as you can get.

20:01.880 --> 20:04.240
And it just seemed to work really well.

20:04.240 --> 20:10.240
And do you have any intuition about the fact that it works so well, so quickly give you

20:10.240 --> 20:16.280
any intuition about the problem, like does it say anything about latent characteristics

20:16.280 --> 20:23.800
of the problem or the relationships between the data that you had and the actual physical

20:23.800 --> 20:28.400
phenomena or is it, it just happens to work well because these things are good at picking

20:28.400 --> 20:29.720
out patterns and data.

20:29.720 --> 20:35.320
It turns out for this problem there's a remarkable amount of physical insight that we could

20:35.320 --> 20:36.760
gain from this.

20:36.760 --> 20:39.280
So let me set the stage a little bit.

20:39.280 --> 20:44.720
When we modeled the aftershocks in the study, we didn't just take the location of the

20:44.720 --> 20:48.840
main shock and say go predict the aftershocks.

20:48.840 --> 20:52.280
Instead what we did was we took the location of the main shock and we took information

20:52.280 --> 20:57.280
about large earthquakes in particular how much they slipped and where they slipped.

20:57.280 --> 21:03.200
And we put that information through a forward model which predicted stresses, the changing

21:03.200 --> 21:07.720
stress everywhere in the earth's crust as a result of the large earthquake.

21:07.720 --> 21:13.680
Those were the raw inputs that we actually put into that served as essentially the features

21:13.680 --> 21:15.760
for a machine learning model.

21:15.760 --> 21:20.880
And then the labels in our machine learning model were the locations of the earthquakes.

21:20.880 --> 21:27.880
And so what this did was we provided a sort of physics-based regularization of the features

21:27.880 --> 21:32.360
in a way that ensures that conservation of mass is satisfied, conservation of linear

21:32.360 --> 21:34.640
momentum is satisfied.

21:34.640 --> 21:39.520
And because of this, when we were able to look at the neural network and do inference

21:39.520 --> 21:45.000
with it after we had already trained in everything, we were able to look at synthetic examples

21:45.000 --> 21:49.680
and we were able to interpret those synthetic examples in the context of some of the basic

21:49.680 --> 21:51.560
physics that we had put in.

21:51.560 --> 21:56.120
And what it led us to discover was that in fact the neural network had not learned some

21:56.120 --> 21:59.280
absolutely out there pattern.

21:59.280 --> 22:04.200
The neural network had actually come very close to finding a physical quantity that we had

22:04.200 --> 22:05.840
heard of before.

22:05.840 --> 22:11.040
And that was essentially something called the Von Mises yield criterion.

22:11.040 --> 22:17.240
And this is a metric that exists in the literature and is very commonly used to explain the transition

22:17.240 --> 22:20.600
from elastic to plastic behavior in particular.

22:20.600 --> 22:23.400
And so that's one of the things that was really interesting about this study.

22:23.400 --> 22:26.880
We didn't just get a neural network, we didn't just develop a neural network that had greater

22:26.880 --> 22:27.880
predictive power.

22:27.880 --> 22:33.320
And it turned out we learned that the physics that might be controlling the triggering

22:33.320 --> 22:37.080
of aftershocks was different from what we thought it was before.

22:37.080 --> 22:39.480
That's really interesting.

22:39.480 --> 22:44.360
Well how did you go from the predictions that you were seeing, the inferences that you

22:44.360 --> 22:49.240
were seeing, to backing that into this Von Mises yield criterion?

22:49.240 --> 22:52.200
How did you see that that was there?

22:52.200 --> 22:55.080
Yeah, go ahead Phoebe.

22:55.080 --> 22:56.080
Oh yeah.

22:56.080 --> 23:01.280
So we looked at synthetic examples because it was kind of daunting to think about interpreting

23:01.280 --> 23:06.760
the predictions of the neural network for these very complex slip distributions that

23:06.760 --> 23:09.040
we got from these catalogs.

23:09.040 --> 23:14.920
So we looked at just for idealized earthquakes, what the neural network was predicting.

23:14.920 --> 23:22.040
And then we just simply compared that spatial pattern to the spatial patterns of a big suite

23:22.040 --> 23:29.000
of different stress metrics to see sort of which ones it was most highly correlated with.

23:29.000 --> 23:34.120
And it turned out that something like 98% of the variance in the neural network output

23:34.120 --> 23:37.080
could be explained by the Von Mises yield criterion.

23:37.080 --> 23:45.800
Okay, so you already had this criterion in mind as a contributor or a way to model these

23:45.800 --> 23:50.560
aftershocks going into this problem.

23:50.560 --> 23:56.760
I would say that there were a huge number of candidate functions that we would have

23:56.760 --> 23:57.760
considered.

23:57.760 --> 24:03.040
And my bet, I don't know about Phoebe, but my bet was that it was going to be some really

24:03.040 --> 24:09.240
complicated combination of four or five things that we're going to have to be combined

24:09.240 --> 24:15.200
in some weird nonlinear way to mimic what the neural network was doing.

24:15.200 --> 24:19.840
And as Phoebe mentioned to our surprise, what we found was that there was essentially

24:19.840 --> 24:24.880
one quantity that the neural network was coming close to discovering on its own.

24:24.880 --> 24:30.040
And in that sense, it's really interesting because a neural network is obviously incredibly

24:30.040 --> 24:34.120
good at finding very complicated nonlinear functions.

24:34.120 --> 24:39.920
And what it told us in this case was that to the extent that we trained it, it couldn't

24:39.920 --> 24:45.280
find anything that did too, too, too much better than a physical, a single physical quantity

24:45.280 --> 24:48.920
that actually exists.

24:48.920 --> 24:53.600
So Phoebe, what was the most challenging aspect of applying neural networks to this

24:53.600 --> 24:54.600
problem?

24:54.600 --> 25:02.400
The neural network really wasn't that challenging, it was the sort of data assembly, which was

25:02.400 --> 25:04.640
really, really a lot.

25:04.640 --> 25:10.560
And I think the generation of that, the generation of the data set was really the heavy lift

25:10.560 --> 25:18.560
of this because we had to read in all these complex slip distributions from this online

25:18.560 --> 25:21.640
catalog that many different authors had contributed to.

25:21.640 --> 25:28.720
So there were lots of sort of slight inconsistencies in the way that these data files were written.

25:28.720 --> 25:33.200
And so it had to be sort of very robust to those little inconsistencies.

25:33.200 --> 25:38.680
And then for each slip distribution, we had to calculate the stress changes in the vicinity

25:38.680 --> 25:44.040
and within 100 kilometers of each of these large main shocks and then incorporate another

25:44.040 --> 25:47.200
catalog of the aftershocks.

25:47.200 --> 25:51.800
So it was just a very large scale data assembly problem.

25:51.800 --> 25:59.040
The aftershocks that you were predicting, you were predicting those in two dimensional

25:59.040 --> 26:05.160
space, as opposed to kind of radio distance from the earthquake, presumably.

26:05.160 --> 26:09.920
And were you also trying to predict the magnitude of the aftershocks as well?

26:09.920 --> 26:11.880
No, that's what we're working on now.

26:11.880 --> 26:19.600
But we were doing it in the volume around the fault.

26:19.600 --> 26:24.280
So we actually, we decided to make this problem very easily tractable to start out with.

26:24.280 --> 26:30.240
We decided to frame this problem of explaining aftershock locations as a binary classification

26:30.240 --> 26:31.240
problem.

26:31.240 --> 26:37.600
And the way that we did that was we discretized the volume around each of these main shocks.

26:37.600 --> 26:42.200
And then framed to the problem is trying to classify each grid cell as either containing

26:42.200 --> 26:46.080
or not containing aftershocks.

26:46.080 --> 26:51.000
So that is the sort of way that we approached it, but we did do it not in two dimensions.

26:51.000 --> 26:53.200
We did it in the whole volume around the fault.

26:53.200 --> 26:55.440
Was that binary classification approach?

26:55.440 --> 26:59.840
Was that the first thing you decided to try and it worked or did you kind of iterate

26:59.840 --> 27:01.400
or evolve to that?

27:01.400 --> 27:02.880
That's the first thing we tried.

27:02.880 --> 27:05.160
And the results were just so interesting that we went with it.

27:05.160 --> 27:10.440
But we're certainly now trying lots of different other approaches to see what we can learn.

27:10.440 --> 27:12.000
And what are some of those?

27:12.000 --> 27:17.200
Well, we're looking at aftershock magnitudes right now.

27:17.200 --> 27:21.480
And it's all very preliminary, but it's really interesting to look at the kind of patterns

27:21.480 --> 27:26.840
that the neural network outputs and sort of use these approaches as kind of pattern synthesizers

27:26.840 --> 27:28.720
almost.

27:28.720 --> 27:34.600
And then I think going forward, also looking at aftershock density and eventually aftershock

27:34.600 --> 27:35.600
timing as well.

27:35.600 --> 27:44.080
I'm so curious about this, about how you incorporated the Von Mise's yield criterion.

27:44.080 --> 27:50.520
Was that incorporated into the neural network itself or the machine learning aspect of

27:50.520 --> 27:57.480
this project or was that secondary analysis that you just apply to the results?

27:57.480 --> 27:58.480
Option two.

27:58.480 --> 27:59.480
Got it.

27:59.480 --> 28:00.480
Okay.

28:00.480 --> 28:04.560
And that's why it's interesting because it's not a quantity that we gave it.

28:04.560 --> 28:09.120
And one could compute that quantity from the inputs, you essentially have to solve an

28:09.120 --> 28:15.040
eigenvalue problem, but it's that's why it was such a surprise to us.

28:15.040 --> 28:22.760
Yeah, it reminded me a little bit of a conversation with I'm forgetting his name.

28:22.760 --> 28:29.240
I believe at the University of Washington, where he was basically the outputs of this network

28:29.240 --> 28:35.680
were coefficients of a bunch of factors, so kind of linear kind of what you thought you

28:35.680 --> 28:39.360
would see like a linear combination of a bunch of different factors.

28:39.360 --> 28:46.720
And he used that to basically to derive mathematical equations for physical input parameter.

28:46.720 --> 28:47.720
Input parameters.

28:47.720 --> 28:48.720
Yeah.

28:48.720 --> 28:50.360
It was being used for laser tuning.

28:50.360 --> 28:51.360
That's great work.

28:51.360 --> 28:56.240
It's coming out of the University of Washington, Brunton is one of the people who's doing it.

28:56.240 --> 29:01.680
And that offers the prospect of using machine learning based techniques to constrain the

29:01.680 --> 29:06.160
physics of systems, even when we don't know the physics of a priori.

29:06.160 --> 29:10.280
I think that's a really promising technique.

29:10.280 --> 29:17.040
Not maybe for earthquake problems, but in particular for a lot of problems involving movement

29:17.040 --> 29:24.040
on the Earth's surface, whether it's groundwater or ice or things like that that are very complex

29:24.040 --> 29:25.040
media problems.

29:25.040 --> 29:28.800
I think the techniques, those sorts of techniques are going to be deeply powerful.

29:28.800 --> 29:29.800
Yeah.

29:29.800 --> 29:30.800
Yeah.

29:30.800 --> 29:37.120
It was Nathan Coots at a Coots at University of Washington for that particular one.

29:37.120 --> 29:38.360
Where do you see this going?

29:38.360 --> 29:44.960
Phoebe, you're continuing this looking at, you mentioned the magnitude of the aftershocks.

29:44.960 --> 29:50.480
You mentioned as well the time dependency of the aftershocks.

29:50.480 --> 29:56.320
Do you have a sense for how the modeling approach will need to change as you take on these

29:56.320 --> 29:57.320
new problems?

29:57.320 --> 30:02.000
I mean, we're very much in exploration mode right now.

30:02.000 --> 30:08.760
So I think we'll go to more complex network architectures if we need to.

30:08.760 --> 30:12.160
But yeah, it's just exciting to think about.

30:12.160 --> 30:17.680
And aftershock forecasting as a problem is sort of exciting to think about from a machine

30:17.680 --> 30:23.800
learning perspective because we've taken into account only static elastic stress changes.

30:23.800 --> 30:27.880
These sort of features of the network are only these static stress changes due to the

30:27.880 --> 30:29.040
main shocks.

30:29.040 --> 30:33.480
But there are a lot of other phenomena that may affect aftershock behavior, everything

30:33.480 --> 30:38.680
from the locations of existing geological structures in the region to poor or elastic

30:38.680 --> 30:41.760
stress changes, visco-elastic stress changes.

30:41.760 --> 30:46.440
So I think, I don't know if I'm going to agree with this, but I feel like the most important

30:46.440 --> 30:50.680
implication of this paper from my perspective is kind of the approach.

30:50.680 --> 30:54.600
Yeah, I think I agree with that entirely.

30:54.600 --> 30:59.680
I think I even zoom out a little bit and I think the approach is really important in

30:59.680 --> 31:06.680
so much as it provides a window into how we can rebuild a lot of earth science and potentially

31:06.680 --> 31:13.680
a lot of other sciences of complex systems in a way that doesn't require us to know everything

31:13.680 --> 31:15.880
about those systems beforehand.

31:15.880 --> 31:22.360
And what I mean is we spend a lot of time doing bottom-up modeling of all sorts of phenomena.

31:22.360 --> 31:25.960
And that's an excellent approach, but it requires that we know what's going on from the bottom

31:25.960 --> 31:26.960
up.

31:26.960 --> 31:29.800
And I think for a lot of problems that we face in earth science and in the world, what

31:29.800 --> 31:34.120
we really want to do first is try to make predictions.

31:34.120 --> 31:36.760
And that's where we can make a lot of progress here.

31:36.760 --> 31:40.640
We can try to make predictions, improve predictions of weather systems, climate systems, the

31:40.640 --> 31:44.800
earthquake system, environmental systems.

31:44.800 --> 31:51.440
And if we can exploit the ability to make predictions even in the absence of knowing what the

31:51.440 --> 31:57.360
first principles are, I think that deeply motivates us to go try and find out what those

31:57.360 --> 31:59.080
first principles are.

31:59.080 --> 32:01.280
That's one of the jobs of the scientist.

32:01.280 --> 32:06.360
And what we now have are tools that enable us to do that, not just by sitting around

32:06.360 --> 32:10.360
and thinking about what the first principles ought to be, but by giving us these networks

32:10.360 --> 32:14.560
that we can probe into and try to understand why they're predicting what they are.

32:14.560 --> 32:22.120
One of the questions that remains for me is in using this neural network approach, you've

32:22.120 --> 32:29.720
been able to pretty significantly reduce the time it takes to get new predictions to make

32:29.720 --> 32:37.680
new results by training a network to emulate the output of this traditional model that

32:37.680 --> 32:41.920
was hard to build and expensive to run.

32:41.920 --> 32:52.600
So you're using neural networks to model a model, so you still have to have the model.

32:52.600 --> 32:58.680
And I guess the question is, do you still need to develop these traditional models for

32:58.680 --> 33:02.280
each of the new problems that you want to try and solve?

33:02.280 --> 33:09.360
And does that remain a limiting factor in your ability to accelerate innovation as you

33:09.360 --> 33:11.000
were describing?

33:11.000 --> 33:14.960
I think that's a great question, and we've done it both ways.

33:14.960 --> 33:21.360
So when we accelerated this big HPC code, that did require Ford model.

33:21.360 --> 33:27.560
And in that sense, its primary utility is not insight, but simply rather speed.

33:27.560 --> 33:31.920
And it's nice too, because anyone can download this model and run it kind of on a laptop,

33:31.920 --> 33:35.400
instead of needing a data center to run it in, which is nice.

33:35.400 --> 33:39.720
But for the aftershock study that we did more recently, in that case, we didn't have

33:39.720 --> 33:43.520
to develop anything other than a kind of a classical machine learning model.

33:43.520 --> 33:48.600
And so I think insights are going to be gained in both ways, and it's going to help more

33:48.600 --> 33:50.800
people do a lot of science in both ways.

33:50.800 --> 33:58.560
Okay, and so just so that I understand that last point, the inputs to the aftershock model,

33:58.560 --> 34:03.440
I thought those features came from the traditional HPC model.

34:03.440 --> 34:05.200
Did I misunderstand that?

34:05.200 --> 34:08.240
Yeah, those are two separate studies by large.

34:08.240 --> 34:10.600
So got it, okay.

34:10.600 --> 34:14.600
One could have put in that HPC model, that's a really good idea, Sam.

34:14.600 --> 34:17.400
We should probably do that study.

34:17.400 --> 34:20.440
Hold on, I got to write that down.

34:20.440 --> 34:25.920
But we used a simpler version of it for this particular study, which was not time dependent.

34:25.920 --> 34:29.480
It was just a step function change in stress due to the earthquake itself.

34:29.480 --> 34:34.640
I'm sure I'll make even more sense when I listen to it again.

34:34.640 --> 34:40.960
But this has been really great. I appreciate you taking the time to share with me a bit

34:40.960 --> 34:42.560
of what you're working on.

34:42.560 --> 34:45.160
Any final words or thoughts?

34:45.160 --> 34:46.160
Not for me.

34:46.160 --> 34:47.160
Thank you so much for having us.

34:47.160 --> 34:48.760
Oh, Sam, this has been great.

34:48.760 --> 34:49.960
Thanks for having us.

34:49.960 --> 34:54.600
And I hope for a lot of earth and environmental science problems, there can be continued and

34:54.600 --> 35:00.440
greater dialogue between the ML community and the earth science community, because we

35:00.440 --> 35:04.120
have a lot of data from complex systems that we don't understand, and we really want

35:04.120 --> 35:06.880
to predict those to live in a better world.

35:06.880 --> 35:07.880
Awesome.

35:07.880 --> 35:09.880
Thanks so much, Phoebe and Brendan.

35:09.880 --> 35:10.880
Thanks, Sam.

35:10.880 --> 35:11.880
Thanks.

35:11.880 --> 35:17.800
All right, that's our show for today.

35:17.800 --> 35:23.800
To learn more about today's show, visit twomolai.com slash shows.

35:23.800 --> 35:29.800
Make sure you visit twomolcon.com slash videos to secure your access to Twomolcon video content

35:29.800 --> 35:30.800
now.

35:30.800 --> 35:40.800
Peace.

