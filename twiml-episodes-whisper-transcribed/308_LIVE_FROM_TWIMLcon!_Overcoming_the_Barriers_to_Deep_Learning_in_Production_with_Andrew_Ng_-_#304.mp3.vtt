WEBVTT

00:00.000 --> 00:05.960
We're excited to kick off our coverage of TwilmoCon AI platforms with an interview

00:05.960 --> 00:12.480
recorded today on the TwilmoCon stage featuring none other than Andrew Ng, founder of Landing

00:12.480 --> 00:17.320
AI among so many other ventures.

00:17.320 --> 00:21.920
Before we get to the show though, I'd like to acknowledge TwilmoCon sponsors.

00:21.920 --> 00:26.400
We couldn't have done this without our founding sponsors who supported us in a big way as

00:26.400 --> 00:31.760
we pulled this event together. A huge thanks to all of our friends at Sigopt, IBM and

00:31.760 --> 00:33.760
DotScience.

00:33.760 --> 00:38.680
We're also grateful to CloudDera who joined us as a gold sponsor. CloudDera was our first

00:38.680 --> 00:43.240
ever sponsor for this podcast and we thank them for their continued support.

00:43.240 --> 00:47.600
Our silver sponsors include Fiddler Labs and Virtua, two innovators you'll undoubtedly

00:47.600 --> 00:52.680
be hearing more about in the near future. And last but not least, we are overwhelmed by

00:52.680 --> 00:58.520
the outpouring of support by our community sponsors including Apple, Etsy, Figurate,

00:58.520 --> 01:05.120
Georgian Partners, Imerit, LinkedIn, Logical Clocks, Neuromation, NYU Future Labs,

01:05.120 --> 01:11.280
Valahai and Waits and Biasis. Thanks so much to all of these great companies. Please be

01:11.280 --> 01:17.640
sure to check them out at twilmocon.com slash sponsors and thank them for supporting us.

01:17.640 --> 01:23.320
And now on to the show.

01:23.320 --> 01:30.120
My first guest really needs no introduction but I will try anyway. Through the podcast,

01:30.120 --> 01:36.600
I get to have a lot of conversations with a lot of incredible people and learn about

01:36.600 --> 01:43.400
how they got their start in machine learning. And it probably wouldn't surprise you that

01:43.400 --> 01:50.720
many of the folks that I talk to got their start with my first guest courses, his Stanford

01:50.720 --> 01:58.320
University courses, Coursera and more recently deep learning AI. In fact, I got my start

01:58.320 --> 02:02.680
in the space taking one of his courses and I can't think of an individual who's helped

02:02.680 --> 02:10.040
more people enter this space than our first guest, Andrew Ang. Andrew?

02:10.040 --> 02:18.520
So it's great to see all of you here. It is great to have you here. So my first question

02:18.520 --> 02:26.120
is, I don't know if you saw it on the document that had your autograph but I lost 4.85 points

02:26.120 --> 02:30.520
on homework assignments in your course. What was that all about?

02:30.520 --> 02:35.040
Yeah, I'm sorry, I think you are perfect. It must have been a bug and it's a great

02:35.040 --> 02:43.680
question. Awesome, awesome. So you also launched your most important learning machine,

02:43.680 --> 02:47.760
Nova, back in February. How's fatherhood treating you?

02:47.760 --> 02:53.600
Yes, so Nova is now seven months old. But you know, there's actually a story about how

02:53.600 --> 02:58.160
we chose her name. We wanted her to have the initials, you know, neural networks taking

02:58.160 --> 03:03.360
off. So we wanted her initials to be NN, so Nova. But one layer deeper than that, you know,

03:03.360 --> 03:08.000
thinking, all right, have a new baby. I think every person on the planet is a unique person,

03:08.000 --> 03:14.000
is a unique human being. So people on, no one is a number. So we gave her the middle name,

03:14.000 --> 03:19.600
Athena. So the full name is Nova Athena. Does the initials are NAN?

03:23.040 --> 03:27.040
That doesn't burden, she'll have to carry it for her whole life. Nice, nice.

03:27.840 --> 03:32.720
You are prolific. You are up to so many things. Can you give us an overview of what you're

03:32.720 --> 03:37.440
working on nowadays? Yeah, the teams, the rise of AI, the rise of machine learning means there

03:37.440 --> 03:42.560
are a lot of pieces needed for it to reach this full potential. So right now, the three teams,

03:43.200 --> 03:48.240
swimming most time with them, leading our landing AI, which is helping companies jumpstart AI

03:48.240 --> 03:55.440
adoption, deep learning.ai, which is our educational producers content, a lot of it is on Coursera,

03:55.440 --> 04:00.880
also a weekly newsletter called The Batch, which is subscribed to get weekly news about machine

04:00.880 --> 04:06.160
learning, and then also AI funds, which is a startup studio that builds AI power startups from

04:06.160 --> 04:10.720
scratch. The rise of machine learning creates a lot of new opportunities. So building these three

04:10.720 --> 04:15.680
teams which work together in the ecosystem, trying to build many pieces that, you know, let's build

04:15.680 --> 04:21.200
AI power future. Let's dig into landing AI a bit. I associate that with doing work in the

04:21.200 --> 04:25.840
manufacturing space. Give us an overview of the company and what it's up to. Is that the case?

04:25.840 --> 04:33.920
So I think I saw with my own eyes how an injection of modern AI can make a company much more

04:33.920 --> 04:40.240
effective and valuable. I think, you know, building up Google Brain and then leading AI at by-do,

04:40.240 --> 04:45.120
I saw, right, with my own eyes, a couple of great companies become modern AI companies,

04:45.120 --> 04:48.960
have become much more effective and valuable along the way. But if you look at what we've done

04:48.960 --> 04:54.000
in machine learning world, I think we've transformed the software internet sector. So many of the

04:54.000 --> 04:59.360
companies represented, you know, at this conference and many companies in Silicon Valley and Beijing,

04:59.360 --> 05:04.160
even outside the top small handful, you kind of have a giving a lot of traction in AI.

05:04.960 --> 05:09.840
I think the next step for AI is first to transform all of the other industries as well,

05:09.840 --> 05:16.880
outside software internet. And so landing AI works with many companies from manufacturing to

05:16.880 --> 05:22.880
agriculture, to healthcare to others. And we can act as a partners outsource chief AI officer

05:22.880 --> 05:28.720
to help a partner build a AI function, channel team, develop IP. And all of those we help partners,

05:29.600 --> 05:34.480
we will cook for you, but also teach you how to cook so that after a couple of years, you can

05:34.480 --> 05:40.960
insource the function and be an AI-enabled business in your vertical, which we think can

05:40.960 --> 05:45.200
help a lot of people and help a lot of companies become more effective and more valuable.

05:45.200 --> 05:51.760
And frankly, we are go, there's more to life than your financials, but I think our impact,

05:51.760 --> 05:55.840
we hope actually has a mature impact on the market cap of companies we work with.

05:56.480 --> 06:01.120
Can you give us an example of some of the types of problems that you're helping customers solve?

06:02.240 --> 06:07.440
Let's see, one of our engagements has been with a large agriculture machine in your company.

06:07.440 --> 06:13.280
And I think if you can help a company, you know, reposition from being a traditional agriculture

06:13.280 --> 06:18.640
company to being an AI-enabled agriculture machinery company, then you can build smart agriculture

06:18.640 --> 06:24.800
machinery. We're going to have the same farmer, same farm, but with automation suggestions for how

06:24.800 --> 06:29.840
to control the machine better, you get more crop from the same farm, the same farm. And so this

06:29.840 --> 06:35.760
is direct impact on the farmer as well as not surprisingly on the company building these types of

06:35.760 --> 06:44.320
things. I think, you know, I was just in Latin America actually last week in Colombia visiting

06:44.320 --> 06:49.360
companies in different industries as well, from logistics to manufacturing. And what I'm seeing

06:49.360 --> 06:56.800
is that there's very strong CEO level interest to help companies in all sorts of industry sectors

06:56.800 --> 07:00.720
become AI-enabled. And it's not that, it's not that if you're a, you know,

07:00.720 --> 07:04.240
manufacturing company, you want to become an AI company. It's just that, you know, let someone

07:04.240 --> 07:10.240
else do that. But I think in the future, an AI-enabled manufacturing company can be much more effective

07:10.240 --> 07:16.640
and valuable than one that doesn't. Maybe one lost disruptive ways to technology was to rise

07:16.640 --> 07:23.600
to the internet. And we saw that if you have a shopping mall plus a website, you know, yes,

07:23.600 --> 07:29.200
everyone has to build a website, but that doesn't turn you into Amazon. Or if you're a taxi company

07:29.200 --> 07:35.200
and you build a website, you know, you're not an internet company, instead Uber, Lyft, Grab,

07:35.200 --> 07:43.760
you know, DD are true internet companies. AI arguably is as disruptive as the internet. And so

07:43.760 --> 07:49.200
there will, and then it changes the core of how different companies will compete. What are the

07:49.200 --> 07:52.640
things that help you build a defensive business? What are the things that generate value?

07:52.640 --> 07:56.000
Where do you play? Where do you not play? What is the new strategy? And I think companies able

07:56.000 --> 08:01.200
to figure that out will become, you know, what will survive and thrive. One of the myths we

08:01.200 --> 08:05.520
tell Silicon Valley, which is not true, is that whenever there's a disruptive technology, it's always

08:05.520 --> 08:09.920
a style that's that wouldn't. That's just not true. Where they rise the internet, some startups

08:09.920 --> 08:15.120
that did well include Google, Facebook, and Amazon, but some incumbents that did well include

08:15.120 --> 08:20.000
Microsoft and Apple, which were not internet companies, but became great internet companies.

08:20.000 --> 08:24.400
So what the rise of modern machine learning, and the exciting work that many of you in this

08:24.400 --> 08:29.680
community are doing to, you know, land these technologies, to bring them to fruition, I think

08:29.680 --> 08:34.880
this is very, that the races again on, where their great opportunities for startups,

08:34.880 --> 08:40.240
but incumbents also have a lot of advantages. And if they play their cards right, they can become

08:40.240 --> 08:46.560
very valuable, very effective AI-enabled businesses in their verticals. So what's involved in

08:46.560 --> 08:52.160
playing those cards right? One of the hardest things for companies to embrace AI is to scope

08:52.160 --> 08:59.040
the right set of projects. And so we spend a lot of time, I think we've actually become very

08:59.040 --> 09:03.520
good at working with companies to figure out what you should and should not use machine learning

09:03.520 --> 09:08.400
to do. I think, you know, some people advise a good company is one that's not small.

09:08.400 --> 09:13.440
Maybe actually here's story. When I was, some companies tried to do the biggest, most glamorous

09:13.440 --> 09:18.320
project as probably number one, and that's usually a mistake. At least the failure that then causes

09:18.320 --> 09:22.800
you lose faith, actually says to company back, because you need to regain the faith.

09:22.800 --> 09:29.760
One story, early days of Google Brain, people in Google, where Sophie didn't know how to use deep

09:29.760 --> 09:36.000
learning, or even skeptical about it. So my first internal customer was the Google speech team.

09:36.000 --> 09:39.840
It wasn't the most, you know, it's not web search advertising, right? Speech recognition is a

09:39.840 --> 09:44.720
nice project, but it's not web search advertising. But by making Google speech more accurate,

09:44.720 --> 09:50.720
it helped other teams within Google gain faith in our ability to deliver results. It also taught

09:50.720 --> 09:56.880
the company how to use, you know, deep learning. I remember when I first GPU server, it was just

09:56.880 --> 10:01.440
a server sitting under some guy's desk with a nest of wires. But that told us important lessons

10:01.440 --> 10:07.920
about how to train models on GPUs. After the first successes, second internal customer was Google maps,

10:07.920 --> 10:14.000
where we use a OCR, photo OCR to read house numbers to more accurately place houses on Google maps,

10:14.000 --> 10:18.560
the improved quality map data. So only after those two successes that I then started the most

10:18.560 --> 10:25.200
serious conversation with the advertising team. So, so one lesson from this is I think start small.

10:25.200 --> 10:29.040
It's more important that your first project comes something like speech recognition, you know,

10:29.040 --> 10:35.280
back in the day to help the company learn what it feels like and then to use that to build momentum.

10:36.400 --> 10:41.200
And then I think it's important to form cross functional teams with machine learning experts

10:41.200 --> 10:47.120
and business application experts to brainstorm projects together. One tip I offer a lot of

10:47.120 --> 10:54.080
the companies kind of approach it. Often the number one project that the CEO gets excited about

10:54.080 --> 11:00.320
does actually not the project you should work on. So I recommend to companies to brainstorm at

11:00.320 --> 11:05.440
least half a dozen projects and spend a few weeks deeply evaluating, is it technically feasible,

11:05.440 --> 11:10.400
is it actually valuable and do that before investing, you know, several few months worth of resources

11:10.400 --> 11:17.280
to do that. And I think it's also a quick answer. Several months ago published online

11:17.280 --> 11:22.240
an AI transformation playbook which talks about the sequence of steps from scoping pilot projects

11:22.240 --> 11:26.320
to building a team, to providing training, to thinking through your strategy, to even

11:26.320 --> 11:31.520
communications, including some of the pre IPO companies you work with, you know, value,

11:32.480 --> 11:39.440
mature, communicating their AI value thesis creation clearly. But that type of AI transformation

11:39.440 --> 11:42.160
playbook which can find online tell the company to become AI Naval.

11:42.160 --> 11:48.720
Yeah, one of the things that I find fascinating in speaking to folks that are leading these

11:48.720 --> 11:55.520
efforts is the challenge of managing the portfolio of projects. They need to start small and kind of

11:55.520 --> 12:02.560
pick off the easy wins, but they also need to maintain that vision, the overall promise of AI

12:02.560 --> 12:09.920
and what it can lead to so that they can continue to drive enthusiasm. Do you see that as well?

12:09.920 --> 12:18.080
That's interesting. I think the panel of the company, some of the things that is important

12:18.080 --> 12:24.880
is like go-de-locks principle for AI, right, to not be over the optimistic. So HGI is not around

12:24.880 --> 12:34.000
the corner, at least unless Elon Musk has a secret lab somewhere. But also not to. But also not be too

12:34.000 --> 12:40.080
pessimistic because there's a lot it can do if you under-ame then your competitors or someone else,

12:40.080 --> 12:48.720
you know, you're missing out on opportunities. So I think it's important that not just engineers

12:48.720 --> 12:54.480
know how to do this, but that company management and executives also learn how to do this.

12:54.480 --> 12:59.040
Actually, we're running an event in Columbia. We run these events around the world called

12:59.040 --> 13:05.680
Pine AI, but it was running an event in Columbia and I met this engineer who came up to me and

13:06.400 --> 13:11.680
she said, Hey Andrew, I love your AI for everyone course. And this is a very technical person,

13:11.680 --> 13:15.920
right, AI for everyone was scope for a non-technical audience and she said, love your AI

13:15.920 --> 13:20.240
for everyone course, not for myself, but I'm getting tons of non-engineers to take it and just

13:20.240 --> 13:25.520
making them much easier for me to work with. And I find that in a company, if the management

13:25.520 --> 13:29.840
structure, the product managers, the executives, even the C suite executives have a basic on the

13:29.840 --> 13:33.440
survey AI, then the machine learning team or the data science team is actually much better

13:33.440 --> 13:41.360
set up for success. When organizations do make that commitment and initial investment, spin up

13:41.360 --> 13:48.480
teams, how well do you think enterprises are doing and getting value out of the other end of

13:48.480 --> 13:54.240
their ML investments? I think it's really hard. We've seen the launch companies, you know,

13:54.240 --> 13:59.680
the mature, relatively sophisticated technology companies are getting pretty good at building

13:59.680 --> 14:06.480
and deploying machine learning systems, but I think a couple of challenges. One, AI grew up

14:06.480 --> 14:12.320
in consumer internet companies, which just have a lot of data, 100 million users or billion users,

14:12.320 --> 14:18.960
you have a lot of data. In other industries, you often don't have that much data. So if you're

14:18.960 --> 14:25.280
manufacturing plant, doing visual inspection, using computer vision to tell if smartphone is

14:25.280 --> 14:29.280
scratch or not, you don't have a million pictures of scratch smartphones because you just

14:29.280 --> 14:35.760
fortunately did not manufacture a million scratch smartphones. So I think how do you get

14:36.400 --> 14:41.680
machine learning to work with small data instead of big data is an important technical challenge.

14:41.680 --> 14:46.240
And then it turns out, some of the processes we use in big companies, I mean, so when I was deploying,

14:46.240 --> 14:52.320
actually, actually, so, you know, deploying a launch speech recognition system, right? Well,

14:52.320 --> 14:57.840
what happens? You deploy the model and then the world gives you data that's different than what you

14:57.840 --> 15:03.120
had in your test set, stored in your hot disk, right? So again, in the early days, the conversations

15:03.120 --> 15:08.400
we go like this, the machine learning is to say, wow, look, I do so well on the test set,

15:08.400 --> 15:13.040
and then the business people will say, no, look, the customers are doing these crazy things,

15:13.040 --> 15:18.320
they're talking a car, there's background noise, your speech recognition engine doesn't work.

15:18.320 --> 15:22.880
And then the machine learning says, look, I did so well on the test set. What are you talking about?

15:24.000 --> 15:29.120
And I think, I think, you know, the machine learning world, I think now, more machine learning

15:29.120 --> 15:33.520
people realize our job is not to do well on the test set you have on the hot disk. The job is to

15:33.520 --> 15:39.680
build a product, move the product or the business forward. But if you look at how we use to solve

15:39.680 --> 15:44.400
these problems, what we used to do was, if we ship a speech system and for whatever reason,

15:44.400 --> 15:50.400
the performance degrades, then we would monitor it, alarm it, you know, and then page at UT,

15:50.400 --> 15:55.360
whatever, and then someone like me would go, hey, you know, you 20 engineers, there's a problem,

15:55.360 --> 15:59.920
please go and fix it. Now, the launch companies could do that. Every time there's a, you know,

15:59.920 --> 16:04.480
stick them a problem for a major product, we could find 20 engineers to then go and, you know,

16:04.480 --> 16:09.920
page at UT people and fix it. But for other applications where you just cannot afford to do that,

16:10.640 --> 16:15.680
we need better, more systematic ways to monitor, alarm, mitigate. So I think the whole

16:15.680 --> 16:21.200
machine learning world, including I think many of you, you know, coming up with better, I guess

16:21.200 --> 16:27.040
in this community, MLO seems to be a growing term to figure the processes to manage that.

16:27.040 --> 16:33.920
So many organizations, including many of the folks here, are in this transition point that I

16:33.920 --> 16:40.800
described earlier. They've launched successful proof of concepts, they've generated excitement

16:40.800 --> 16:45.840
within their organizations, and now they need to scale up so they can get more models out into

16:45.840 --> 16:53.040
production. What have you seen working in those organizations that are similarly situated,

16:53.040 --> 16:57.360
that are scaling up their ability to, you know, drive real value out of these models?

16:58.880 --> 17:04.960
I think that, I know that this conference talks about platforms, and I think platforms are

17:04.960 --> 17:10.400
important, but the hopes of the technical aspects of scaling these up, I think what makes

17:10.400 --> 17:18.560
scaling up hard is even more the people, business aspects of it, although the technical aspects

17:18.560 --> 17:24.320
of it are really important to, you know, actually after Sam and I were chatting a couple days ago,

17:24.320 --> 17:29.440
and after our conversation, it was very interesting. I went back and reread the 30-year-old paper

17:29.440 --> 17:36.240
by Fred Brooks titled No Silver Bullet, and it made the point that even as we improve

17:36.240 --> 17:41.760
programming languages, software engine is still hot, right? Now I'm really glad that I could

17:41.760 --> 17:48.880
code in Python and not, you know, Fortran or Assembly or something. I think we're all happy about that.

17:50.000 --> 17:55.680
Even though we now have Python and these fancy tools, TensorFlow PyTorch, software engine

17:55.680 --> 18:02.880
is still really hot, and that's because the tools we have did not remove the essential complexity,

18:02.880 --> 18:06.160
this is Fred Brooks' term, the essential complexity of software engineering, which is a

18:06.160 --> 18:11.120
thing through clearly what you want to do to write the specification, to then express it, and then

18:11.120 --> 18:18.000
to test it. And the hot part of software engineering is not, you know, do you use a go-to statement,

18:18.000 --> 18:23.440
or do you use a fancy wild loop or whatever. It is thinking through clearly what is the problem,

18:23.440 --> 18:28.720
and what are the steps needed to solve it. Now, one of the beautiful things about

18:28.720 --> 18:35.040
modern machine learning is that it removed essential complexity from a task. As in, you know,

18:35.040 --> 18:40.160
10 years ago, if you're building a computer vision system, it was really complicated. You would

18:40.160 --> 18:46.560
download these, now, maybe, arguably, obsolete algorithms, you know, sift, surf, hog,

18:46.560 --> 18:52.160
have these features, then you do something crazy about colonization, where you feed it through,

18:52.160 --> 18:56.160
you know, some software library, say OpenCV, you find it doesn't work, the walk the image,

18:56.160 --> 19:00.160
well, it's just crazy complicated set of steps. And you did it on an octave.

19:01.120 --> 19:06.480
Oh, yeah. Actually, OpenCV, which you'll see at the time, I think.

19:06.480 --> 19:14.320
But what deep learning allowed us to do is to say, forget all these steps, let's get a lot of data,

19:14.320 --> 19:19.120
trade in your network on it, and then, and so the workflow changed dramatically with the rise

19:19.120 --> 19:25.600
of deep learning, which is why we're so many more things now. Now, I think just as I'm happy to

19:25.600 --> 19:31.360
use Python instead of, you know, Fortrad, we've been seeing a lot of improvement in developer tools

19:31.360 --> 19:37.760
for building a deploy machine learning. Some of this removes, you know, accidental complexity,

19:37.760 --> 19:42.960
Fredbroxist term. And I think we need more thought on how to remove the essential complexity

19:42.960 --> 19:48.640
at the work we still have, which is, and a lot of my work is meeting with a company, meeting with,

19:48.640 --> 19:53.600
you know, CEOs, leaders, brainstorming, what are the things actually valuable for the business,

19:53.600 --> 19:59.440
thinking about where do you get data, and do they have data, and how do you organize the data,

19:59.440 --> 20:05.520
and then those things are essential complexity things that are still difficult to, to relieve with

20:05.520 --> 20:11.440
today's tools. Which is not to say tooling is not important. The game Python is great, but I think

20:11.440 --> 20:16.400
the heart of what makes a machine learning problem difficult is still thinking through clearly,

20:16.400 --> 20:21.760
what does the problem want to solve, and where to get the data, even though I'm grateful for the

20:21.760 --> 20:26.000
much better ways we have today for expressing the neural network architecture I want to train.

20:26.000 --> 20:32.240
Talking about that, what you refer to as accidental complexity versus kind of essential complexity,

20:33.040 --> 20:43.200
very similar theme is the core mission and motivation behind Airbnb's platform team. They

20:43.200 --> 20:51.920
refer to it as, as their mission is being eliminating the incidental complexity of machine learning,

20:51.920 --> 20:57.680
so that their engineers can focus on the unavoidable complexity that essential complexity that you

20:57.680 --> 21:04.240
refer to, and that has kind of power their development of a whole set of, you know, platforms and

21:04.240 --> 21:11.360
tools to allow them to move more quickly. And when we spoke about this a couple of days ago,

21:11.360 --> 21:17.200
you made an interesting point about one of the success factors being an organization's ability

21:17.200 --> 21:23.760
to iterate quickly. Are there particular things that you've seen organizations do that allow

21:23.760 --> 21:29.440
them to iterate very quickly and do more experiments, understand what's working, what's not, you know,

21:29.440 --> 21:34.880
with a shorter time like? Yeah, so here's one example of a slightly unusual process that some of

21:34.880 --> 21:40.880
my teams use. We're all used to agile, you know, two-week sprints. Some of my teams use a one-day

21:40.880 --> 21:46.000
sprint, so there were up those like this. We wake up in the morning and look at the experiment results

21:46.000 --> 21:51.280
that we had run overnight, and then we do our analysis, gap analysis, our analysis to figure out,

21:51.280 --> 21:55.200
you know, what are the shortcomings of the algorithm? So we do that in the morning. And then,

21:55.200 --> 21:59.520
based on that, we brainstorm what to do, do collect more data, chase their architecture,

21:59.520 --> 22:03.600
our regularization, whatever, brainstorm a bunch of stuff, and then different tasks,

22:03.600 --> 22:07.600
different team members take on different tasks. I get more data. You try that regularization

22:07.600 --> 22:12.560
hyperparameter. You try that, you know, data augmentation, whatever. We write code in the afternoon,

22:12.560 --> 22:18.720
or write code in the morning through, you know, afternoon evening. And then, we run the experiment,

22:18.720 --> 22:23.760
next experiment overnight. And then the next morning, we wake up and look at experiments from

22:23.760 --> 22:29.200
last night, and then iterate the next day, our analysis, plan what to do, run code overnight.

22:29.200 --> 22:34.240
So now, this workflow isn't for everyone. It tends to work best for when the machine learning

22:34.240 --> 22:40.000
job takes, you know, like four or five hours to train, because that workflow fits in well,

22:40.000 --> 22:43.440
with this type of cadence where you code during the day and run experiments overnight.

22:43.440 --> 22:49.280
But I found that if you could, in agile development, we were used to this idea that you should

22:49.280 --> 22:54.640
replan every two weeks, or whatever is your sprint cycle. But in machine learning, it turns

22:54.640 --> 23:00.000
out a lot of workflow of building a machine learning system. It feels more like debugging than

23:00.000 --> 23:08.160
development. And as you know, it's a key insight. And with that, and so this daily sprint cycle,

23:08.160 --> 23:13.280
we found to be good practice for quickly driving down errors of certain types of workflows.

23:14.160 --> 23:18.880
I think a lot of us are still making up, you know, are still indenting these new processes.

23:18.880 --> 23:25.200
I think of how long did it take our community? We went through a lot of versions of version control,

23:25.200 --> 23:33.760
right? From emailing each other, code, email, to get, you know, I guess CVS version get, right?

23:33.760 --> 23:37.840
So I think we're still in the early phases of making up the processes that are suitable for

23:37.840 --> 23:45.200
machine learning workflows. Yeah, I think the kind of contemporary version of emailing files for

23:45.200 --> 23:50.560
version control is probably putting hyper parameters in file names to track experiments.

23:54.400 --> 24:00.160
Yeah, and actually, let me describe to you another, another strange process that we use,

24:00.160 --> 24:04.720
which is talking about version control. So we're pretty good tools for editing and

24:05.360 --> 24:11.040
versioning code. I think we're still, you know, indicating some companies are working on this,

24:11.040 --> 24:18.000
but I think there's still nascent tools for editing and versioning data. And, you know,

24:18.000 --> 24:24.160
and this is one thing you, I think there's a gap between what is done in academia versus what,

24:24.160 --> 24:27.280
you know, we need to do it to build production machine learning systems. Here's one example.

24:27.280 --> 24:34.160
Say we have a test set, train a system, it does poorly on the test set. What do you do? Well,

24:34.160 --> 24:39.840
sometimes we go in and edit the test set, right? Because we, now you can't do that. If you go

24:39.840 --> 24:44.400
to the publisher paper, it's not legit to edit the test and look, I edited the test set and got

24:44.400 --> 24:49.680
great results. You don't want to publish that paper. But from a, from a, from a business production

24:49.680 --> 24:53.200
point of view, sometimes you realize the test set labels are wrong. They don't, they're not

24:53.200 --> 24:57.120
longer business objective. And so you go and edit the test set. And I think that needs to become

24:57.120 --> 25:01.120
part of our accepted workflow. And, and then we need better tools for doing the editing and

25:01.120 --> 25:06.560
versioning it and having multiple people collaborate on editing data. I think these are things that,

25:06.560 --> 25:11.040
I think I feel like, you know, some of my teams, land AI, deep learning AI, I find we're making

25:11.040 --> 25:15.200
about our own processes, but hopefully as we as a committee learn from each other and come up

25:15.200 --> 25:20.640
much better ways to do this. One of the challenges I see is that academia has a much

25:20.640 --> 25:26.160
easy time working on certain problems where progress is, you know, measurable and repeatable.

25:26.160 --> 25:30.720
And if every team edits their test set and changes the labels in a different way, it's,

25:30.720 --> 25:34.960
you know, it's much harder to benchmark, right, who's better at editing test set labels.

25:35.840 --> 25:41.360
So, so there are certain category problems that I think are difficult to study systematically.

25:41.360 --> 25:48.000
And so for good or bad reason, they get less attention in academia. Maybe one, one of the example,

25:48.000 --> 25:52.960
I was at ICML International Conference of Machine Learning, there was a lot of discussion

25:52.960 --> 25:59.200
on robustness, but maybe what happens is a lot of times is let's say, let's say we train the

25:59.200 --> 26:04.800
deep learning system to diagnose from X-rays. And we train on high quality, high-res, X-ray images

26:04.800 --> 26:10.000
taken off of a Stanford University X-ray machine. And we post your paper saying this does really well.

26:10.000 --> 26:15.440
But if you ship your model to, you know, all the hospital down the street with a blurrier X-ray

26:15.440 --> 26:20.240
machine, whether radiologists has a different protocol for how the organ, the patient, it doesn't

26:20.240 --> 26:26.400
generalize, right, doesn't generalize well. So I think we know that this robustness or

26:26.400 --> 26:33.360
generalization problem, it is a problem. But how do you systematically study how well your

26:33.360 --> 26:39.040
learning algorithm can do on a brand new distribution of data, you know, that the algorithm's never seen,

26:39.040 --> 26:46.000
right, so that's just been a relatively more difficult problem to study systematically. So even

26:46.000 --> 26:50.480
though this is a problem that we've seen production all the time, and we have, you know, practical

26:50.480 --> 26:56.800
solutions solving it, I think the amount of attention in academia to this problem is underweighted.

26:56.800 --> 27:02.480
And there are some academic papers, but they tend to be framed in, you know, like domain adaptation

27:02.480 --> 27:07.600
or framed in certain ways that makes it easier to study systematically, whereas sometimes the world

27:07.600 --> 27:13.600
fills you some random data, and so it's been more difficult to formulate benchmarks to drive for

27:14.160 --> 27:22.160
systematic study of some of these issues. Along the lines of robustness when we were chatting

27:22.160 --> 27:27.600
a couple of days ago, you mentioned some interesting things that we're happening at landing around

27:27.600 --> 27:32.880
managing the risk of machine learning deployments once they get in the production. Can you elaborate

27:32.880 --> 27:39.360
on that for us? Yeah, so we've been doing, I feel like my team's doing a lot of strange things

27:41.920 --> 27:48.880
one of the processes I've used before is called the FME analysis, it actually does a set of

27:48.880 --> 27:53.280
processes that grew up in the hardware world where, you know, if you're building an airplane and you

27:53.280 --> 27:58.080
want to make sure the airplane is saved, then you do an analysis on the risk of what could go wrong,

27:58.080 --> 28:04.160
and typically you categorize, what's the chance of this thing going wrong, what's the chance of,

28:04.160 --> 28:10.320
you know, this error on not moving or something, what is the severity, how bad is it if it

28:10.320 --> 28:15.440
does happen, and how detectable is it, right? So those are the three things you try to categorize

28:15.440 --> 28:19.920
each risk again. So one of the things we've been doing is when we look through our machine learning

28:19.920 --> 28:24.320
projects, when you think about how to deploy these projects, you know, in a factory or elsewhere,

28:24.320 --> 28:31.360
my teams tend to, we like going through a very rigorous exercise where we, you know, pre-mortem

28:31.360 --> 28:35.520
or think through all the risks and have a team discuss in debate, but each of the things we could

28:35.520 --> 28:40.160
go wrong, what's the probability, what's the severity, and what's the detectability,

28:40.160 --> 28:45.040
so that we could do a better job seeing around corners and plan for things rather than be surprised

28:45.040 --> 28:52.160
by them later. I think we're seeing a lot of startups, a lot of, there are a lot of machine learning

28:52.160 --> 28:58.240
projects that end up at the proof of concept stage, but unable to go beyond, and I think, you know,

28:59.600 --> 29:06.160
landing AI, putting a lot of thought into how to see around corners so you can

29:06.160 --> 29:12.080
take all these things all the way to deployment and production. So I think we've become pretty good

29:12.080 --> 29:18.640
at that due to processes like these. Are there specific examples of issues that this, the

29:18.640 --> 29:25.280
figure mode and effects analysis process has allowed you to avoid? I would say the one number one

29:25.280 --> 29:33.360
most common bucket is planning for the ways that the real world test sets may be different than

29:33.360 --> 29:37.920
the test sets stored on your heart disc, and whether the consequences, what's your mitigation,

29:37.920 --> 29:42.960
what's your escalation process, and what's your response time, so you can give reasonable SLAs

29:43.600 --> 29:47.680
for what happens to the shipping machine and the system, the world does something crazy and then what?

29:47.680 --> 29:54.240
How broadly applicable do you think this, this way of thinking about deploying

29:55.280 --> 30:00.080
models and AI products is, do you think it's something that everybody should be, you know,

30:00.080 --> 30:06.000
pulling up the Wikipedia page and figuring out how to do, or is it very specific to the kinds of

30:06.000 --> 30:10.800
scenarios you're looking at at landing? I think, I think, my analysis, and then you can look

30:10.800 --> 30:17.440
along with Wikipedia, is a relatively heavyweight process, but maybe for most of my life, most of parts

30:17.440 --> 30:26.720
that I work on, I feel like, for most parts that work on, I tend to like to identify the risks

30:26.720 --> 30:34.080
up front, you know, actually when those ads on Polsera, one of my direct reports gave me,

30:34.080 --> 30:39.040
you know, gave me direct feedback, and she said, hey Andrew, I find my one-on-ones would

30:39.040 --> 30:43.200
you very depressing, because every time we meet, you want to talk about all the things that could

30:43.200 --> 30:47.040
go wrong, but look at all the things going right, why are you always, you know, so depressing,

30:47.040 --> 30:52.880
and then I actually told her, yes, I understand, vision for the bright future, but I tend to spend

30:52.880 --> 30:56.400
most of my time planning for the scenarios that could go wrong so that we don't hit those,

30:56.400 --> 31:02.560
then she was okay after, I gave her that context, but most of my project planning, you know,

31:02.560 --> 31:07.600
is the discipline of having a vision for a bright future, but then also being very explicit in

31:07.600 --> 31:12.400
listing out all the things that could go wrong, so that, so that hopefully we're not, you know,

31:12.400 --> 31:17.440
as surprised as something, so to avoid those outcomes. Having just planned an event,

31:17.440 --> 31:21.760
I can definitely relate to trying to anticipate all the things that could possibly go wrong.

31:21.760 --> 31:28.000
Yeah, and just say, I think, you know, this community, I find the work being done in this

31:28.000 --> 31:33.520
community, really exciting. I think over the past few years, you know, machine learning research

31:33.520 --> 31:38.480
has really raised ahead, our ability to build machine learning models has really raised ahead,

31:38.480 --> 31:45.760
but if this is a set of things, if it's a small set of things needed to do a good machine learning

31:45.760 --> 31:53.440
model, the set of software you need to write to a super product is so much bigger than the work

31:53.440 --> 32:00.800
of building the machine learning model, so the advances in our ability to build high accuracy

32:00.800 --> 32:05.520
machine learning models has raised ahead, and this whole technology improvement is driving a

32:05.520 --> 32:11.040
lot of value, but there are also a lot of other things that I think our whole community is still

32:11.040 --> 32:17.920
figuring out, and I think, you know, the, the ability of organizations like, like you guys

32:17.920 --> 32:23.120
told us now, as well as all of us in this community sharing our learnings with each other,

32:23.120 --> 32:28.160
I think we have important work to do to move the machine learning world forward.

32:28.160 --> 32:32.880
And I think this is especially important if you want machine learning to have an impact

32:32.880 --> 32:38.960
outside the software into that industry. I feel like, you know, the rise of open source tools,

32:38.960 --> 32:46.640
things like TensorFlow, PyTorch, many others, well, the rise of archive papers, right,

32:46.640 --> 32:51.680
knowledge can download for free. That's done a lot to hold machine learning break outside

32:51.680 --> 32:57.200
the software into the industry, which it's going to be a big part of the next wave of

32:57.200 --> 33:02.560
where machine learning needs to go. Well, Andrew, thank you so much for joining us,

33:02.560 --> 33:06.720
and helping me kick off our very first Twimmelcon AI platforms.

33:06.720 --> 33:08.320
That's good. Thank you, Zaz. Thank you.

33:17.360 --> 33:23.040
All right, everyone. That's our show for today. For more information about this and every show,

33:23.040 --> 33:29.680
visit twimmelai.com. Thanks again to all the great sponsors of Twimmelcon AI platforms.

33:29.680 --> 33:35.360
Head over to twimmelcon.com slash sponsors to learn more about each and every one of them.

33:36.080 --> 33:41.440
While you're there, be sure to click on over to the Twimmelcon news page to follow along with

33:41.440 --> 33:56.240
all of the updates coming out of Twimmelcon. Thanks so much for listening and catch you next time.

