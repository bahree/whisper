1
00:00:00,000 --> 00:00:16,320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

2
00:00:16,320 --> 00:00:21,480
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:21,480 --> 00:00:34,040
I'm your host, Sam Charrington, Hey Twimble listeners, if you're a fan of our AI

4
00:00:34,040 --> 00:00:39,680
platforms coverage and especially if you download our first AI platforms ebook, Kubernetes

5
00:00:39,680 --> 00:00:44,800
for machine learning, deep learning and AI, then today is your day.

6
00:00:44,800 --> 00:00:49,440
While we've been working tirelessly on Twimblecom, I have also been working on Book 2 in the

7
00:00:49,440 --> 00:00:53,440
series, the definitive guide to machine learning platforms.

8
00:00:53,440 --> 00:01:00,080
And I am happy to say that it is finally available to download now.

9
00:01:00,080 --> 00:01:04,880
We created the book as a resource for ML, AI and data science leaders and innovators

10
00:01:04,880 --> 00:01:09,120
to help guide their efforts in scaling and industrializing machine learning and deep learning

11
00:01:09,120 --> 00:01:11,320
in their organizations.

12
00:01:11,320 --> 00:01:16,440
In this book, we address questions such as why invest in increasing your organization's

13
00:01:16,440 --> 00:01:20,160
capacity to deliver machine learning and deep learning models.

14
00:01:20,160 --> 00:01:23,720
What are the key barriers to delivering ML and DL models?

15
00:01:23,720 --> 00:01:28,560
How of organizations like Facebook, Airbnb and LinkedIn overcome these challenges and

16
00:01:28,560 --> 00:01:32,240
how can their learning be applied to your organization?

17
00:01:32,240 --> 00:01:35,840
What are the state of the art, machine learning platforms and tools and how can you put

18
00:01:35,840 --> 00:01:37,480
them to use?

19
00:01:37,480 --> 00:01:42,280
How can you develop an AI platform strategy to support your organization's goals?

20
00:01:42,280 --> 00:01:48,000
To access the definitive guide to machine learning platforms, visit TwimbleAI.com slash

21
00:01:48,000 --> 00:01:50,200
AI platforms.

22
00:01:50,200 --> 00:01:54,760
And now on to the show.

23
00:01:54,760 --> 00:01:57,400
All right, everyone.

24
00:01:57,400 --> 00:01:59,560
I am on the line with Angela Bassa.

25
00:01:59,560 --> 00:02:03,160
Angela is the director of data science at IROBOT.

26
00:02:03,160 --> 00:02:06,080
Angela, welcome to this weekend machine learning and AI.

27
00:02:06,080 --> 00:02:07,080
Thanks so much, Salem.

28
00:02:07,080 --> 00:02:08,520
I'm happy to be here.

29
00:02:08,520 --> 00:02:14,880
I am super excited to have you on the show and to get into a little bit of what you're

30
00:02:14,880 --> 00:02:17,720
up to there at IROBOT.

31
00:02:17,720 --> 00:02:24,040
To kind of get us started, I'd love to hear a bit about your background and how you came

32
00:02:24,040 --> 00:02:28,960
to be working at the intersection of robotics and data science.

33
00:02:28,960 --> 00:02:37,920
Yeah, so my background is mathematics and I came to the intersection of robotics and

34
00:02:37,920 --> 00:02:43,240
data science through an enormous amount of luck.

35
00:02:43,240 --> 00:02:49,440
Most people who come into data scientific practice either come from sort of the analytical,

36
00:02:49,440 --> 00:02:53,560
mathematical, physics sort of background or they come from computer science and software

37
00:02:53,560 --> 00:02:56,560
development and software engineering.

38
00:02:56,560 --> 00:03:01,440
And I am very much on the first camp and I am trying to learn as much as possible from

39
00:03:01,440 --> 00:03:04,680
the second camp's firehose.

40
00:03:04,680 --> 00:03:08,760
My academic training is in applied math.

41
00:03:08,760 --> 00:03:10,400
This was back at MIT.

42
00:03:10,400 --> 00:03:15,840
Since then I left, my sort of journey into data science was always through a heavy use

43
00:03:15,840 --> 00:03:21,800
of data, but under many different disciplines, so I worked in investment banking, in strategy

44
00:03:21,800 --> 00:03:27,080
consulting and agricultural technologies, and marketing and energy trading.

45
00:03:27,080 --> 00:03:33,680
I noticed that you had an incredibly broad background set of experiences before you found

46
00:03:33,680 --> 00:03:37,040
your way into data science.

47
00:03:37,040 --> 00:03:44,160
Yeah, I think it's sort of reflective of my personal and family history as well, so I'm

48
00:03:44,160 --> 00:03:49,840
a third generation immigrant and my family is sort of very transient and almost nomadic

49
00:03:49,840 --> 00:03:55,240
and I think that has translated into my career as well.

50
00:03:55,240 --> 00:03:59,120
And it's also, I mean, all jokes aside, it's actually something that I find comes quite

51
00:03:59,120 --> 00:04:05,720
easily to make so growing up, we spoke several languages at home and I find it very easy

52
00:04:05,720 --> 00:04:10,560
now in a professional capacity to understand that some people are speaking HREs, whereas

53
00:04:10,560 --> 00:04:15,560
some people are speaking engineers or some people are speaking analytics ease and to be

54
00:04:15,560 --> 00:04:22,160
able to provide a translational step has a huge amount of value, especially in a discipline

55
00:04:22,160 --> 00:04:28,880
like data science, where there's sort of this division between the tools and the mechanics

56
00:04:28,880 --> 00:04:35,160
of how you solve problems and the domain expertise of knowing which questions to ask and which

57
00:04:35,160 --> 00:04:37,800
problems are worthy of being solved first.

58
00:04:37,800 --> 00:04:38,800
Right.

59
00:04:38,800 --> 00:04:45,840
And so playing that midfield there has proven incredibly valuable and transferable, thank

60
00:04:45,840 --> 00:04:46,840
goodness.

61
00:04:46,840 --> 00:04:49,240
Nice, nice.

62
00:04:49,240 --> 00:04:55,440
You mentioned that your academic background was in applied math, what any particular focus

63
00:04:55,440 --> 00:04:56,440
or flavor?

64
00:04:56,440 --> 00:04:59,880
I liked graph theory quite a bit.

65
00:04:59,880 --> 00:05:07,960
I spent a lot of time in that domain and also logic and first sort of logic, sort of

66
00:05:07,960 --> 00:05:14,520
model theory, but I learned very early on that academia wasn't going to be my thing.

67
00:05:14,520 --> 00:05:22,480
I just don't have the personality fit for that actually more than a personality fit,

68
00:05:22,480 --> 00:05:27,000
I actually don't have the diligence to focus that long on a single problem.

69
00:05:27,000 --> 00:05:28,000
Yeah.

70
00:05:28,000 --> 00:05:34,680
So I ended up going into industry where my particular flavor of ADHD is actually quite valuable.

71
00:05:34,680 --> 00:05:39,200
I mentioned that you are at iRobot as did you.

72
00:05:39,200 --> 00:05:44,040
I imagine folks are familiar with iRobot, but maybe you should share a little bit

73
00:05:44,040 --> 00:05:52,360
about the company and the focus of the company just to levels it.

74
00:05:52,360 --> 00:05:53,680
Yeah, happy to.

75
00:05:53,680 --> 00:05:59,160
So iRobot is, a lot of folks actually don't know, but iRobot is almost 30 years old.

76
00:05:59,160 --> 00:06:01,680
So we've been around for quite a while.

77
00:06:01,680 --> 00:06:07,360
I've been here for about two and a half years now and I started the data science practice

78
00:06:07,360 --> 00:06:08,360
here.

79
00:06:08,360 --> 00:06:12,640
We've been growing quite a bit over the last two and a half years.

80
00:06:12,640 --> 00:06:20,800
But obviously iRobot builds robots and so we have lots of fantastic algorithms and machine

81
00:06:20,800 --> 00:06:26,120
learning engineers, many of whom predate my tenure here.

82
00:06:26,120 --> 00:06:35,160
But now we have sort of a focused competency whose mandate it is to look at fleet data

83
00:06:35,160 --> 00:06:41,360
and to make sure that we are using that information and feeding that back into the development

84
00:06:41,360 --> 00:06:46,840
of products into the customer experience and into the strategy.

85
00:06:46,840 --> 00:06:54,280
So at iRobot, we're well known for our flagship product, which is the Robotic Vacuum Cleaner.

86
00:06:54,280 --> 00:06:57,040
And we have a whole line of robotic vacuums.

87
00:06:57,040 --> 00:07:01,800
There's not just one, so depending on what specifically a customer might be looking

88
00:07:01,800 --> 00:07:08,960
for, we have different offerings with different levels of data driven features and autonomy.

89
00:07:08,960 --> 00:07:13,360
And we also have the Brava line, which is a robotic mobs and we just announced earlier

90
00:07:13,360 --> 00:07:16,560
this year the ptereline of robotic lawn mowers.

91
00:07:16,560 --> 00:07:22,040
So more and more we have this portfolio of autonomous products that can take care of your

92
00:07:22,040 --> 00:07:23,760
home so you can do other stuff.

93
00:07:23,760 --> 00:07:26,640
If you're like me, you can be a nerd.

94
00:07:26,640 --> 00:07:31,080
I'm one would call an endorsement so I don't use my free time to go outside but if you

95
00:07:31,080 --> 00:07:35,320
like that then you can do that while your robots are back home making sure that everything

96
00:07:35,320 --> 00:07:37,000
is sparkly clean.

97
00:07:37,000 --> 00:07:42,000
You mentioned that you started the data science group there at iRobot.

98
00:07:42,000 --> 00:07:47,840
Did you do that from internally where you already at iRobot or did you join the company

99
00:07:47,840 --> 00:07:50,640
to start the group?

100
00:07:50,640 --> 00:07:51,640
The latter.

101
00:07:51,640 --> 00:07:58,440
So I was at a company called Annernock before and I was also brought in to run their

102
00:07:58,440 --> 00:08:02,320
data science team, although they already had the team in place when I joined.

103
00:08:02,320 --> 00:08:11,680
They had two or three people working in data science there at the time and here at iRobot

104
00:08:11,680 --> 00:08:15,080
I was the first data science hire that they had.

105
00:08:15,080 --> 00:08:22,520
There's always pros and cons as to whenever you're starting a team especially in established

106
00:08:22,520 --> 00:08:23,520
organizations.

107
00:08:23,520 --> 00:08:26,480
So it's very different if your product is data scientific at that point you probably

108
00:08:26,480 --> 00:08:32,240
have data science challenge that's built in from either the founding team or at least very

109
00:08:32,240 --> 00:08:33,240
close to inception.

110
00:08:33,240 --> 00:08:38,440
Whereas when you're coming into an organization that is established and that has a product

111
00:08:38,440 --> 00:08:45,680
or an industry that they work within and you're starting to infuse that organization with

112
00:08:45,680 --> 00:08:49,080
data and data informed decision making.

113
00:08:49,080 --> 00:08:52,520
There are certain times where it makes sense to start with merging your talent and just

114
00:08:52,520 --> 00:08:56,840
flunking and figuring out what value you can extract from that data or you can start

115
00:08:56,840 --> 00:09:04,440
very top heavy and have more of a strategic or even a structural approach to it.

116
00:09:04,440 --> 00:09:09,320
And iRobot chose the latter looking for me and I've been very diligently making sure

117
00:09:09,320 --> 00:09:14,680
that they never think that that was a bad decision on their part.

118
00:09:14,680 --> 00:09:21,080
I had the company even come to know that it needed data science, what precipitated creating

119
00:09:21,080 --> 00:09:26,600
the opening and opportunity for you to join the company.

120
00:09:26,600 --> 00:09:31,400
So I like to believe that it's because some people with a lot of foresight who were

121
00:09:31,400 --> 00:09:38,400
here before me saw that as part of the vision as a compelling part of the story of what

122
00:09:38,400 --> 00:09:39,400
we're trying to do.

123
00:09:39,400 --> 00:09:44,920
So when you read about what iRobot is doing right now we're talking very openly about

124
00:09:44,920 --> 00:09:49,880
this this bet that we're making on the smart home and what we think that smart home ecosystem

125
00:09:49,880 --> 00:09:52,600
is going to mean going forward.

126
00:09:52,600 --> 00:09:58,440
But it all started with a small step towards connecting our products to the cloud and allowing

127
00:09:58,440 --> 00:10:04,400
those products to leave artifacts for us to be able to inspect.

128
00:10:04,400 --> 00:10:11,080
And one of the things that has always been at the forefront here has been the utmost

129
00:10:11,080 --> 00:10:13,800
stewardship that we have of that data.

130
00:10:13,800 --> 00:10:20,480
This data is not being generated from pings that your mobile phone is making to the cloud

131
00:10:20,480 --> 00:10:21,480
that you carry on you.

132
00:10:21,480 --> 00:10:23,080
This is coming from inside your home.

133
00:10:23,080 --> 00:10:30,960
This is one of your most precious havens and to allow somebody to allow a company to come

134
00:10:30,960 --> 00:10:39,400
in and autonomously, a proof that environment and report back is something that has to

135
00:10:39,400 --> 00:10:41,320
be done with a lot of trust.

136
00:10:41,320 --> 00:10:48,720
So I think part of the strategy internally was to make sure that all of that was in place

137
00:10:48,720 --> 00:10:54,600
before letting a whole bunch of nerds loose for linking all that data.

138
00:10:54,600 --> 00:10:55,600
So we had good.

139
00:10:55,600 --> 00:11:01,000
I was just going to ask you to elaborate a bit on what that data is and where it comes

140
00:11:01,000 --> 00:11:02,000
from.

141
00:11:02,000 --> 00:11:03,000
Yeah.

142
00:11:03,000 --> 00:11:11,000
So in late 2015 we had our first connected product and we started aggregating that information.

143
00:11:11,000 --> 00:11:16,560
So the information that we get always with the permission of our customers has to do with

144
00:11:16,560 --> 00:11:18,080
the functioning of the robot.

145
00:11:18,080 --> 00:11:24,800
So we're interested in identifying bugs before our customers are bothered by them before

146
00:11:24,800 --> 00:11:29,440
they impact the functioning of the functionality of the robot.

147
00:11:29,440 --> 00:11:34,280
We're interested in understanding how our customers use their robots.

148
00:11:34,280 --> 00:11:39,440
So one thing that's always important to keep in mind is that I'm not the person who should

149
00:11:39,440 --> 00:11:45,520
be asking the questions because I am a very specific type of nerd who has very nerdy questions

150
00:11:45,520 --> 00:11:49,160
and there's only one of me and millions and millions of all of us.

151
00:11:49,160 --> 00:11:55,120
So we as a company want to make sure that we're answering the questions of all of our customers.

152
00:11:55,120 --> 00:12:00,160
And so the data that we're collecting helps inform what our customers want from our products

153
00:12:00,160 --> 00:12:04,720
that maybe we haven't ideated yet and then it helps us develop that.

154
00:12:04,720 --> 00:12:10,360
But one thing to keep in mind is that we never sell that data, that data is not the thing

155
00:12:10,360 --> 00:12:16,320
that we make money off of the data is essentially the thing that allows us to do what our customers

156
00:12:16,320 --> 00:12:19,360
want from their product better than anybody else.

157
00:12:19,360 --> 00:12:20,360
Yeah.

158
00:12:20,360 --> 00:12:27,440
One of the things that I found, so I was at the AWS Remar's conference a few weeks ago.

159
00:12:27,440 --> 00:12:32,440
And I robot had a big presence at that event as well.

160
00:12:32,440 --> 00:12:38,320
And one of the things that I realized was that I just had a very dated view of the company

161
00:12:38,320 --> 00:12:39,320
and the products.

162
00:12:39,320 --> 00:12:47,160
Like, I remember when the Rumba first came out, and you can correct me if I'm wrong, but

163
00:12:47,160 --> 00:12:48,760
I think it was pretty dumb.

164
00:12:48,760 --> 00:12:53,440
Like, you know, you turn this thing on and it would like bounce in a wall and kind of randomly

165
00:12:53,440 --> 00:12:54,440
change direction.

166
00:12:54,440 --> 00:12:59,680
And the idea was that like, if you kept this thing charged up enough and let it do it

167
00:12:59,680 --> 00:13:04,000
enough, it would eventually clean up, you know, a room, right?

168
00:13:04,000 --> 00:13:10,280
As opposed to, you know, what I saw today was, you know, there were some visualizations

169
00:13:10,280 --> 00:13:18,120
that were like a total, like a slam type of, you know, process where these robots have

170
00:13:18,120 --> 00:13:23,360
all kinds of sensors and they're like mapping out their environments dynamically and, you

171
00:13:23,360 --> 00:13:30,320
know, employing some sophisticated, relatively sophisticated, certainly relative to, you

172
00:13:30,320 --> 00:13:34,880
know, what I recall, algorithms to make sure that they're cleaning up all the parts of

173
00:13:34,880 --> 00:13:35,880
your house.

174
00:13:35,880 --> 00:13:42,640
And then you're getting into this notion of like collaborative cleaning with either multiple,

175
00:13:42,640 --> 00:13:47,840
like the, you know, how does the broom robot work with the mop robot to clean a house?

176
00:13:47,840 --> 00:13:53,160
Like, it's, it's a lot more as like, it's not your, you know, not your father's room

177
00:13:53,160 --> 00:13:57,760
but anymore or something like that is a lot more sophisticated than I remembered.

178
00:13:57,760 --> 00:14:05,760
Well, I think 15, 20 years ago, a lot of things in hindsight look dumber, I certainly

179
00:14:05,760 --> 00:14:09,560
looked umber when I think of myself 15, 20 years ago.

180
00:14:09,560 --> 00:14:13,960
But yeah, I think, I think that's, that's apt.

181
00:14:13,960 --> 00:14:20,920
So the, the robots today, I mean, the thing that happens with a company that has been

182
00:14:20,920 --> 00:14:26,840
around for almost 30 years is you have all of these co-bases with a lot of institutional

183
00:14:26,840 --> 00:14:30,160
knowledge and a lot of love and passion embedded in there.

184
00:14:30,160 --> 00:14:35,040
And so it's hard to just say, yeah, we're going to start over, yeah, no problem.

185
00:14:35,040 --> 00:14:39,240
All the people who came before, what could have, what could they have possibly known?

186
00:14:39,240 --> 00:14:40,240
And it's hard to do that.

187
00:14:40,240 --> 00:14:42,800
But that's sort of a little bit of what it takes.

188
00:14:42,800 --> 00:14:47,720
It takes discharging a lot of technical debt and, and starting fresh to be able to do all

189
00:14:47,720 --> 00:14:53,600
of these things and today, you know, the, the Roomba's and the Bravas, they can, some

190
00:14:53,600 --> 00:14:55,120
of the models, they can work together.

191
00:14:55,120 --> 00:15:03,040
So you can tell your Roomba and your Brava to, to tackle, to tag team on, on a task.

192
00:15:03,040 --> 00:15:07,920
And so your Roomba might know exactly where it is.

193
00:15:07,920 --> 00:15:12,760
And so if you want the kitchen and the dining room to get cleaned and mopped, then the Roomba

194
00:15:12,760 --> 00:15:17,480
can dispatch and go to the kitchen, finish as it's moving to the dining room and communicates

195
00:15:17,480 --> 00:15:21,960
with the Brava mop, that the kitchen is all done, that the, the, the mobbing can start

196
00:15:21,960 --> 00:15:24,080
there and it moves into the dining room.

197
00:15:24,080 --> 00:15:27,400
And then when it's finished with a dining room, it lets the Brava know that it can follow

198
00:15:27,400 --> 00:15:28,400
along.

199
00:15:28,400 --> 00:15:34,280
And so then both of those rooms get, uh, vacuumed and mopped and you don't have to lift

200
00:15:34,280 --> 00:15:35,600
a finger.

201
00:15:35,600 --> 00:15:41,040
And with the Roombas, those, um, also have the clean base now, so the, the, the top models

202
00:15:41,040 --> 00:15:42,280
have the clean base.

203
00:15:42,280 --> 00:15:49,280
So the Roomba can dock itself and the base vacuums, the bin of the robotic vacuum and stores

204
00:15:49,280 --> 00:15:50,280
that.

205
00:15:50,280 --> 00:15:53,080
So you don't have to touch it for sometimes weeks or months at a time until you have

206
00:15:53,080 --> 00:15:55,240
to empty the clean base.

207
00:15:55,240 --> 00:16:01,400
So this, this idea of autonomy is something that, uh, at least our modern product line,

208
00:16:01,400 --> 00:16:06,440
uh, is already delivering on and, uh, the whole product line is always evolving towards.

209
00:16:06,440 --> 00:16:10,520
But what we're going towards really, uh, is beyond autonomy, which is something that

210
00:16:10,520 --> 00:16:16,800
Colin, uh, Angle or CEO talked about are remars as well, which is intelligence is an autonomy,

211
00:16:16,800 --> 00:16:17,800
right?

212
00:16:17,800 --> 00:16:20,400
Um, if you autonomously can do something, that's great.

213
00:16:20,400 --> 00:16:25,480
That's better than, than not doing it and, and to require, uh, active direction.

214
00:16:25,480 --> 00:16:31,840
But, um, if you can coordinate and, um, if you can be responsive and collaborative and

215
00:16:31,840 --> 00:16:36,800
if you can be part of a system that enables, uh, all of that collaborative response of

216
00:16:36,800 --> 00:16:41,960
autonomy to take place, uh, then, then you're really talking about the, the transformative

217
00:16:41,960 --> 00:16:47,560
power of data, which is sort of what we're building our platform to be able to leverage.

218
00:16:47,560 --> 00:16:54,560
Is there an example of that in the context of, uh, Roomba in particular, or robotics

219
00:16:54,560 --> 00:17:00,600
or I robot in general, like, how intelligence is, you know, greater than autonomy, however.

220
00:17:00,600 --> 00:17:07,880
Yeah, I think the, the one example would be, um, if you can send, uh, a robot to, to

221
00:17:07,880 --> 00:17:11,520
perform a mission, right in the parlance, if you can send a robot to go and clean the

222
00:17:11,520 --> 00:17:13,560
room, um, that's great.

223
00:17:13,560 --> 00:17:21,000
But if you can tell that robot where in that room to, uh, spend more of its time, uh, either

224
00:17:21,000 --> 00:17:25,560
because, um, you know that there's a particular stain or you know that there is a particular

225
00:17:25,560 --> 00:17:35,160
area that needs to be, um, focused on or if you, um, know that the, uh, that another robot

226
00:17:35,160 --> 00:17:37,360
in the family should follow along.

227
00:17:37,360 --> 00:17:41,480
And so if you can report back that you've identified, there's something, uh, amiss in this

228
00:17:41,480 --> 00:17:42,480
region or that region.

229
00:17:42,480 --> 00:17:46,320
So the way that Colin sort of articulated it, which was really clever, uh, not just because

230
00:17:46,320 --> 00:17:53,280
he's my CEO actually thought this was very clever, um, is, um, if you think of, of the example

231
00:17:53,280 --> 00:17:54,520
of astronauts, right?

232
00:17:54,520 --> 00:17:58,120
If you have somebody who is able to make it to the moon and back, that's great.

233
00:17:58,120 --> 00:18:02,800
But if they're, or make it to Mars and back, but if, if they're there and they have gone

234
00:18:02,800 --> 00:18:06,840
through a battery of, of training to make sure that they can withstand the journey, they

235
00:18:06,840 --> 00:18:10,440
don't have infinite RAM so they can't know all of the things.

236
00:18:10,440 --> 00:18:14,400
So if you have somebody back in Houston who can tell them, hey, that rock over there that

237
00:18:14,400 --> 00:18:18,880
you're not paying any attention to because you're not a geologist, but I am, pick it up

238
00:18:18,880 --> 00:18:22,440
and bring it home, bring a sample because I want to know what that is, right?

239
00:18:22,440 --> 00:18:28,200
So if, if that relationship goes beyond just a relationship of autonomy, but a relationship

240
00:18:28,200 --> 00:18:33,280
of responsiveness so that you can act on that information while it is relevant rather

241
00:18:33,280 --> 00:18:39,440
than only after the payload has been delivered, um, that's really where, where we're driving

242
00:18:39,440 --> 00:18:40,440
towards.

243
00:18:40,440 --> 00:18:48,400
And so when you think about applying data science in this context, how do you break

244
00:18:48,400 --> 00:18:55,360
down the different roles and, and places that you're applying data science?

245
00:18:55,360 --> 00:19:00,200
Um, and I guess I'm, you know, I'm thinking of it kind of crudely in terms of use cases

246
00:19:00,200 --> 00:19:08,640
maybe like, you know, I'm imagining you could apply this to, you know, certainly the autonomy

247
00:19:08,640 --> 00:19:16,680
of the robots within the home, there are like predictive maintenance types of use cases,

248
00:19:16,680 --> 00:19:21,480
there are like business things like how many of the people that buy these things actually

249
00:19:21,480 --> 00:19:27,600
use them and how many of the people, you know, um, that, you know, buy them and use them

250
00:19:27,600 --> 00:19:28,600
a lot.

251
00:19:28,600 --> 00:19:33,160
Probably want to see like the next bigger version because, you know, their current

252
00:19:33,160 --> 00:19:38,040
one, you know, isn't doing something as good as it could be or like what are all the,

253
00:19:38,040 --> 00:19:42,800
like how do you think about not what are all of them, but how do you taxonomize the different

254
00:19:42,800 --> 00:19:47,840
ways that data science plays at, at I robot?

255
00:19:47,840 --> 00:19:55,120
I think the, the way that I like to think about it is in terms of the, the utility of each

256
00:19:55,120 --> 00:20:03,640
of the different projects and, um, and their priority within the, the greater, greater strategy

257
00:20:03,640 --> 00:20:08,040
that we have both in R&D and, uh, in I robot as a whole.

258
00:20:08,040 --> 00:20:13,720
So, um, especially as I was saying before, when, when you have a company that has a product

259
00:20:13,720 --> 00:20:18,840
that is data scientific, right, the thing that you are selling is the knowledge that you

260
00:20:18,840 --> 00:20:21,320
have about the data that you collect.

261
00:20:21,320 --> 00:20:26,320
It's a completely different ball game, but, uh, in the sense of I robot where our product,

262
00:20:26,320 --> 00:20:33,040
uh, is this physical, uh, robot and the data really plays a supporting role to that.

263
00:20:33,040 --> 00:20:38,520
I think the, the thing that you want to, uh, focus on is the fact that data science is

264
00:20:38,520 --> 00:20:42,320
not a, uh, a cheap investment.

265
00:20:42,320 --> 00:20:46,200
And so as, as I stood up to practice here, the first thing that was top of mind is making

266
00:20:46,200 --> 00:20:50,800
sure that we demonstrate or return on that investment because, uh, the thing that I have

267
00:20:50,800 --> 00:20:58,200
seen happen, uh, in, in similar cases is that there's this, this image of the potential

268
00:20:58,200 --> 00:21:05,040
of what, uh, what this practice could deliver and then, um, that potential isn't realized

269
00:21:05,040 --> 00:21:09,960
in the very near term and everybody gets disenchanted and it becomes the self-fulfilling

270
00:21:09,960 --> 00:21:12,240
prophecy that it was never going to work.

271
00:21:12,240 --> 00:21:17,720
Uh, and that happens over and over, uh, when you're talking outside of sort of the, the,

272
00:21:17,720 --> 00:21:22,560
the Silicon Valley data driven, uh, startup ecosystem.

273
00:21:22,560 --> 00:21:27,600
And that's the thing that I was most cognizant of is what are the things that we can deliver

274
00:21:27,600 --> 00:21:31,080
in the very short term with the information that we already have.

275
00:21:31,080 --> 00:21:32,840
I don't want to re-architect anything yet.

276
00:21:32,840 --> 00:21:39,840
I don't want to change how our software gets, uh, developed or how, uh, data gets, uh,

277
00:21:39,840 --> 00:21:45,640
encoded and, and ingested and transformed process stored all of that, uh, based on the

278
00:21:45,640 --> 00:21:50,760
things that we've inherited, what, what exists of value that we can deliver in the very

279
00:21:50,760 --> 00:21:52,080
short term.

280
00:21:52,080 --> 00:21:58,160
And then based on, uh, the, the moment that you can generate there, um, you can start recruiting

281
00:21:58,160 --> 00:22:04,200
champions and you can start recruiting stakeholders that can help you make the more foundational,

282
00:22:04,200 --> 00:22:08,560
uh, changes that enable really big bets.

283
00:22:08,560 --> 00:22:13,640
So here at IROBOT, you know, one of the big bets that we're making is, um, on the smart

284
00:22:13,640 --> 00:22:20,520
home and what that will mean and, uh, who will win in that space and, and our bet is that,

285
00:22:20,520 --> 00:22:26,840
uh, we will win because of, uh, both our, our capability and our expertise and, uh, our

286
00:22:26,840 --> 00:22:30,480
commitment to customer privacy and all of these things rolled into one.

287
00:22:30,480 --> 00:22:34,680
But that's not something that could have been accomplished in the first six months, right?

288
00:22:34,680 --> 00:22:39,720
And so I really try to think, uh, in terms of sort of your near term horizon and your long

289
00:22:39,720 --> 00:22:40,720
term horizon.

290
00:22:40,720 --> 00:22:43,760
So your near term horizon are the things that your data is already bringing to you that

291
00:22:43,760 --> 00:22:46,680
if only somebody were to pay attention to it, right?

292
00:22:46,680 --> 00:22:50,520
You could, um, you could make smarter decisions.

293
00:22:50,520 --> 00:22:51,520
And so those are easy.

294
00:22:51,520 --> 00:22:59,360
You just need to dedicate the time and effort to look at the tactical to, to, to chat across

295
00:22:59,360 --> 00:23:03,800
the organization to figure out who could do, who could make better decisions if only,

296
00:23:03,800 --> 00:23:07,760
uh, somebody could help them understand how things are getting used.

297
00:23:07,760 --> 00:23:11,360
And that's all already, uh, instrumented.

298
00:23:11,360 --> 00:23:17,520
But, uh, that's really sort of quote unquote bootstrapping, the really big, uh, play, which

299
00:23:17,520 --> 00:23:24,400
then requires, uh, you know, improved data architecture and, um, all of the things that

300
00:23:24,400 --> 00:23:30,280
come, that, that, that, that come downstream from that in terms of, uh, data quality, lineage,

301
00:23:30,280 --> 00:23:35,320
governance, um, all of those other, other things that, that are important.

302
00:23:35,320 --> 00:23:41,960
Yeah, I find this, this conversation really interesting and it's one that I, when I talk

303
00:23:41,960 --> 00:23:47,040
to folks that are, you know, running data science organizations or, you know, machine learning

304
00:23:47,040 --> 00:23:54,280
AI, organization centers of excellence, things like this, this, the whole concept of how

305
00:23:54,280 --> 00:23:58,320
they manage and balance their portfolios.

306
00:23:58,320 --> 00:24:06,280
Also as to kind of demonstrate short-term value, you know, enough short-term value, um,

307
00:24:06,280 --> 00:24:13,440
while also kind of keeping their eye on, you know, a broader vision and, you know, selling

308
00:24:13,440 --> 00:24:19,320
that or making progress to that, like it's a very delicate balance in a lot of places

309
00:24:19,320 --> 00:24:23,320
and one that, you know, a lot of energy is put towards.

310
00:24:23,320 --> 00:24:27,840
Yeah, and it's really easy to sort of hire a team.

311
00:24:27,840 --> 00:24:35,160
I mean, it's not easy, but it's quite possible to hire a team of a hundred people, uh, all,

312
00:24:35,160 --> 00:24:42,320
you know, experts and, and, and really fantastic professionals and throw them at a problem.

313
00:24:42,320 --> 00:24:47,160
And then they will, you know, come up with all of these, I mean, they really are fantastic

314
00:24:47,160 --> 00:24:53,880
ideas that will require two years of additional instrumentation and, um, further investment.

315
00:24:53,880 --> 00:24:59,480
And, um, I like to joke that if we were to do the things that I want to do, we go bankrupt

316
00:24:59,480 --> 00:25:04,480
because I'm not, uh, the target audience for this, right?

317
00:25:04,480 --> 00:25:12,320
There, there are lots of, uh, robotics companies that find it very difficult to, to stay, uh,

318
00:25:12,320 --> 00:25:18,320
in the market for as long as I robot has, uh, to, to do, uh, these things in a cost effective

319
00:25:18,320 --> 00:25:19,320
way.

320
00:25:19,320 --> 00:25:26,080
Uh, and, and that's the game for us is how can we, uh, deliver on the promise of consumer

321
00:25:26,080 --> 00:25:32,320
robotics in a way that isn't going to extinguish itself because, you know, we've, we've been

322
00:25:32,320 --> 00:25:33,880
not more than we can chew.

323
00:25:33,880 --> 00:25:40,760
So this very measured approach, um, can be quite frustrating at times because it, it can

324
00:25:40,760 --> 00:25:47,480
feel like you're, you're funding the, uh, the least interesting parts of the journey.

325
00:25:47,480 --> 00:25:52,000
And in fact, it's quite the opposite, um, what, what I feel like I'm doing is I'm clearing

326
00:25:52,000 --> 00:26:00,880
the midfield so that, uh, my, my key players can then be able to take amazing shots, uh,

327
00:26:00,880 --> 00:26:06,760
without having this, this constant oversight and this demanding voice that, uh, why haven't

328
00:26:06,760 --> 00:26:09,160
we, uh, paid for ourselves yet?

329
00:26:09,160 --> 00:26:10,160
Mm-hmm.

330
00:26:10,160 --> 00:26:11,760
Sounds like you're a football fan.

331
00:26:11,760 --> 00:26:17,800
I am, and I call it football, for me, it's soccer.

332
00:26:17,800 --> 00:26:21,240
So for, for folks who might be listening in, who don't know, I was actually born and raised

333
00:26:21,240 --> 00:26:27,680
in Brazil, and I have just started to learn, uh, American football rules.

334
00:26:27,680 --> 00:26:31,040
So, um, now it gets really confusing.

335
00:26:31,040 --> 00:26:32,040
Nice.

336
00:26:32,040 --> 00:26:37,560
Uh, so, how, how big is the data science team there now?

337
00:26:37,560 --> 00:26:41,360
Um, so we have, uh, a blended team.

338
00:26:41,360 --> 00:26:47,080
So we don't have, uh, just data scientists in our data organization, which is also something

339
00:26:47,080 --> 00:26:51,240
that I think is, is, uh, fundamental to our success so far.

340
00:26:51,240 --> 00:27:00,280
Uh, we started with, uh, data scientists, uh, but as we've, um, grown, uh, the, the, the,

341
00:27:00,280 --> 00:27:05,520
the portfolio of, of solutions that we offer internally and in our production environment,

342
00:27:05,520 --> 00:27:08,920
we've, we've been, um, adding specializations.

343
00:27:08,920 --> 00:27:15,400
So we have a blended team of, uh, data architects, data stewards, data engineers, data analysts

344
00:27:15,400 --> 00:27:20,520
and data scientists that, that help us cover all of the different things that we have in

345
00:27:20,520 --> 00:27:21,800
our road now.

346
00:27:21,800 --> 00:27:24,600
How many on the data team there?

347
00:27:24,600 --> 00:27:29,920
So on the data team per se, I think we have 12 people right now.

348
00:27:29,920 --> 00:27:38,880
One of the things that I wanted to dig into a little bit is how you have built out processes

349
00:27:38,880 --> 00:27:47,120
and platforms to support delivering models in the production and, and, you know, doing

350
00:27:47,120 --> 00:27:50,240
the work of the, the team.

351
00:27:50,240 --> 00:27:55,800
Can you talk a little bit about the, you know, both the philosophy there, but you also like

352
00:27:55,800 --> 00:28:01,520
try to get us to kind of concrete details around, you know, some of the things you're doing.

353
00:28:01,520 --> 00:28:06,520
So for a company like I robot, one of the things that we had to do was we had to discharge

354
00:28:06,520 --> 00:28:08,080
a lot of technical debt.

355
00:28:08,080 --> 00:28:15,160
So, um, a lot of our products, uh, originally were, uh, each one independent platform.

356
00:28:15,160 --> 00:28:20,320
And so they had, uh, a code base, uh, and all of the ancillary things that go with that

357
00:28:20,320 --> 00:28:22,520
that were independent of each other.

358
00:28:22,520 --> 00:28:27,400
And that made it so that each platform was robust and, and there are lots of benefits

359
00:28:27,400 --> 00:28:29,440
with that in terms of manufacturing.

360
00:28:29,440 --> 00:28:34,480
But one thing that, uh, proved, uh, less than helpful was the fact that there was very

361
00:28:34,480 --> 00:28:40,800
little modularization, so it was hard to be able to reutilize learnings, uh, and to shift

362
00:28:40,800 --> 00:28:44,280
priorities, uh, as they needed to, to happen.

363
00:28:44,280 --> 00:28:50,200
So one of the things that, uh, we've recently underwent was, was a reorganization where, um,

364
00:28:50,200 --> 00:28:57,040
we brought data science specifically, uh, closer to engineering and closer to product management.

365
00:28:57,040 --> 00:29:03,920
And we've also, um, created a new design language and a shared code base that's modularized

366
00:29:03,920 --> 00:29:12,000
so that, uh, the different pieces of the software can be interoperable.

367
00:29:12,000 --> 00:29:17,520
And so things like navigation and mapping, things like Wi-Fi connectivity and all of those

368
00:29:17,520 --> 00:29:26,440
parts of, of the, the, the platform can now, um, be, uh, plot, be almost plug and play

369
00:29:26,440 --> 00:29:29,960
with the different products that, that we develop.

370
00:29:29,960 --> 00:29:39,320
And what that does is it, it really allows us to, to be much faster in our ability to,

371
00:29:39,320 --> 00:29:41,000
to write software.

372
00:29:41,000 --> 00:29:47,840
And so what we're, uh, moving towards are these robots that become smarter over time.

373
00:29:47,840 --> 00:29:50,560
So, um, hardware is very different than software.

374
00:29:50,560 --> 00:29:55,880
You can't just release an OTA and get new hardware and new injection molded plastic in,

375
00:29:55,880 --> 00:29:58,600
in people's hands, but you can do that with software.

376
00:29:58,600 --> 00:30:04,800
And so if we can have a smart platform, um, that, uh, is part of the hardware that can

377
00:30:04,800 --> 00:30:13,880
receive improved, uh, direction over time, then that allows us to be able to, to solve

378
00:30:13,880 --> 00:30:17,840
the problems that our customers have, uh, in more intelligent ways.

379
00:30:17,840 --> 00:30:18,840
Mm-hmm.

380
00:30:18,840 --> 00:30:25,200
And so we, uh, we essentially embarked on that, uh, not too long ago and we've been operating

381
00:30:25,200 --> 00:30:32,440
under that model for the, for all of 2019, essentially, where, um, we have now a, uh,

382
00:30:32,440 --> 00:30:38,560
data, uh, a DevOps culture and we have a cloud, uh, culture and we have a data-driven

383
00:30:38,560 --> 00:30:44,480
culture that, uh, is imbued into all of the different teams so that they can leverage

384
00:30:44,480 --> 00:30:50,680
the power, uh, of this new design language that, that is shared across all of our products.

385
00:30:50,680 --> 00:30:59,360
Mm-hmm. And so when you say design language, what is that, what does that really mean?

386
00:30:59,360 --> 00:31:04,360
So, um, it means a lot of things, one of the things, for instance, is we used to have

387
00:31:04,360 --> 00:31:11,680
a, um, homegrown, uh, coding language for some of our robots because of the restricted

388
00:31:11,680 --> 00:31:17,800
nature of the compute that was available to them, to them, which is perhaps what might

389
00:31:17,800 --> 00:31:23,920
have made them feel less than intelligent 15, 20 years ago, um, moving forward into, into

390
00:31:23,920 --> 00:31:28,840
the present and the future, uh, that just wasn't going to cut. And so a lot of our code

391
00:31:28,840 --> 00:31:34,200
base right now is in Python and, uh, that helps both in terms of attracting talent, but

392
00:31:34,200 --> 00:31:38,000
also in terms of shifting talent around and shifting learnings around.

393
00:31:38,000 --> 00:31:42,640
And I was talking about code on the robot or code somewhere else.

394
00:31:42,640 --> 00:31:51,800
Yes, and so, um, uh, not all of the parts are in, uh, the, the new paradigm yet because

395
00:31:51,800 --> 00:31:56,760
this is, this is a transition, but a lot of it, uh, both on robot and off. And also in

396
00:31:56,760 --> 00:32:02,960
terms of the, uh, the types of things that can happen on the robot, um, you know, there's

397
00:32:02,960 --> 00:32:07,960
a lot more, uh, power that the robots have, uh, you know, for, for the things that, that

398
00:32:07,960 --> 00:32:12,600
they're capable of doing right now. So if you're going to have teaming, take place, um,

399
00:32:12,600 --> 00:32:16,880
all of that needs to happen. And it needs to happen on the edge so that that information

400
00:32:16,880 --> 00:32:22,600
doesn't necessarily need to get, uh, sent back home for things to work. Sorry that they

401
00:32:22,600 --> 00:32:23,600
should.

402
00:32:23,600 --> 00:32:28,280
Sure. Yeah, it strikes me that there's, you know, platform and, and this conversation

403
00:32:28,280 --> 00:32:35,960
is going to be a little overloaded in the sense of the robot itself is a platform. It's

404
00:32:35,960 --> 00:32:40,400
a hardware platform. And then you've got a software platform sitting on that hardware

405
00:32:40,400 --> 00:32:48,480
platform. And then you've got, that's presumably connected to the cloud, right? AWS, that's

406
00:32:48,480 --> 00:32:57,080
a platform. And so, you know, in that kind of environment, where does DevOps come in

407
00:32:57,080 --> 00:33:04,200
the play and how, how is that used presumably to help tie all these things together?

408
00:33:04,200 --> 00:33:12,000
So the way that it comes into play is twofold. I think one is just as a, uh, mentality

409
00:33:12,000 --> 00:33:18,080
that all of the software developers, uh, need to have in terms of, of the level of ownership

410
00:33:18,080 --> 00:33:23,560
of code. But on top of that, I think, uh, the, the, the, as you describe the different

411
00:33:23,560 --> 00:33:31,360
layering, um, so we want to have, um, folks work on the things that they're good at, the

412
00:33:31,360 --> 00:33:36,120
things that they're passionate at and not on the things that are unnecessary. So the

413
00:33:36,120 --> 00:33:39,280
same kind of mentality applies to data science. And I'm going to speak to data science because

414
00:33:39,280 --> 00:33:43,400
that just comes more easily to me. But in both data science and machine learning, you,

415
00:33:43,400 --> 00:33:48,400
you want to have that same DevOps culture where you don't want to spend 80% of your time

416
00:33:48,400 --> 00:33:52,880
cleaning data. You want to spend 80% of your time figuring out how your data is dirty.

417
00:33:52,880 --> 00:33:57,160
And then writing code that solves it. So the next person doesn't have to do that as well.

418
00:33:57,160 --> 00:34:03,960
So, um, that has to come individually from each member of our team. But we also have, uh,

419
00:34:03,960 --> 00:34:09,600
a team dedicated to maintaining, uh, all of those different layers, uh, both on the, the

420
00:34:09,600 --> 00:34:13,280
hardware platform, the software platform and the cloud platform. I don't know if that

421
00:34:13,280 --> 00:34:17,560
answers your question. Uh, kind of, it starts to get us there. What is that team called?

422
00:34:17,560 --> 00:34:26,560
So, um, that team is the cloud ops team. I guess one, one question that I've got is,

423
00:34:26,560 --> 00:34:33,120
you know, as, as a company that kind of ships these products that are inherently platforms,

424
00:34:33,120 --> 00:34:43,120
uh, I'm wondering if you also have like horizontal, you know, platforms or tools or processes

425
00:34:43,120 --> 00:34:53,400
that are, you know, just how you develop models at Irobat or how you, you know, do experimentation

426
00:34:53,400 --> 00:35:00,440
or how you do deployments that are kind of independent of the individual, uh, robotics

427
00:35:00,440 --> 00:35:07,320
platforms or, you know, because you're developing these like highly integrated things, you

428
00:35:07,320 --> 00:35:15,000
know, is everything very specific to one product. Well, that's the crux of it. So we were handicapped

429
00:35:15,000 --> 00:35:20,760
by the fact that, um, we were faced with with exactly what you're describing where we had

430
00:35:20,760 --> 00:35:28,200
all of these different, uh, distinct platforms. And so, um, as you well know, and as your audience

431
00:35:28,200 --> 00:35:35,800
knows, machine learning requires a lot of training data. Uh-huh. And so how can we train, uh, these

432
00:35:35,800 --> 00:35:43,560
models, uh, with, with the, the, the ultimate objective of having them be as robust as possible.

433
00:35:43,560 --> 00:35:50,840
And so there's a lot of, of information that, um, that these different, uh, formerly distinct

434
00:35:50,840 --> 00:35:55,720
platforms were collecting that weren't distinct at all. So that information should be able to

435
00:35:55,720 --> 00:36:02,680
all be used, but it couldn't because they were coming in through disparate, uh, mechanisms

436
00:36:02,680 --> 00:36:08,440
using different schemas, using, uh, different co-basis. And so part of this rearchitecture has really

437
00:36:08,440 --> 00:36:15,960
been to have a unified, uh, and we call it a design language because it applies both to the,

438
00:36:15,960 --> 00:36:24,520
the industrial design of the hardware, but also the, uh, code plasticity underlying all of that.

439
00:36:24,520 --> 00:36:32,120
And so now that we have this, this unified, uh, shared, uh, platform, all of the information

440
00:36:32,120 --> 00:36:39,400
that comes in, uh, is much more, uh, readily available for, for utilization in different models.

441
00:36:39,400 --> 00:36:46,760
And so part of what we're doing, um, is increasing the personalization of our products. And to do that,

442
00:36:46,760 --> 00:36:51,800
you know, if I, if you're interested in ever increasing autonomy, you don't want to have to worry

443
00:36:51,800 --> 00:36:57,240
about actually sending your robot out, uh, to clean your home. You want that to just automatically

444
00:36:57,240 --> 00:37:03,240
happen, and you don't want to have to worry about it. And so one of the things that, uh, we imagined

445
00:37:03,240 --> 00:37:11,400
we should do, and now we do, is as you are, um, using your robot over time, we learn how you like

446
00:37:11,400 --> 00:37:15,240
to clean your home. When you like to clean your home, are there specific rooms that you like to clean,

447
00:37:15,720 --> 00:37:22,600
um, with different rates? And so we can make recommendations for you on when you should run your

448
00:37:22,600 --> 00:37:28,760
robot or during what times in what rooms so that you can just say, yes, please do this for me

449
00:37:28,760 --> 00:37:33,400
and subplugging me. And then the robot will take over. And if you have one with a clean base,

450
00:37:33,400 --> 00:37:40,200
then the robot will take over and it will, um, run its, its cleaning missions, uh, on the frequency

451
00:37:40,200 --> 00:37:45,800
that you've specified. And you only touch it when the clean base is full. And so one of the things

452
00:37:45,800 --> 00:37:50,200
that's important to keep in mind with something like that is you're actively telling your customer

453
00:37:50,200 --> 00:37:54,680
to not engage with your product. So you have to make sure that you've covered all your bases

454
00:37:54,680 --> 00:38:01,160
so that your, your product is working as intended, uh, in an unsupervised way for as long as it does.

455
00:38:01,800 --> 00:38:09,800
So what's the experience from the data scientist perspective? Like they,

456
00:38:09,800 --> 00:38:19,480
they, there's not now post this rearchitecture. They, do they, is there a centralized place like a

457
00:38:19,480 --> 00:38:26,600
data warehouse or something where they access all of this data and then, um, are there specific tools

458
00:38:26,600 --> 00:38:32,760
that they are able to use now to build models that are independent of, you know, where those models

459
00:38:32,760 --> 00:38:41,240
are ultimately going to run from a robot perspective. Like how, what is that? What services

460
00:38:42,200 --> 00:38:47,880
or experiences does that layer provide for the data scientists that need to work on these

461
00:38:48,680 --> 00:38:54,200
different models for these different robots? It really depends on the use case, right? So if you

462
00:38:54,200 --> 00:39:01,000
have a robot and your robot, uh, has sent you a message for whatever reason. And so it's blinking

463
00:39:01,000 --> 00:39:05,400
and you go when you look in your app and if your app is not reflecting exactly what your robot

464
00:39:05,400 --> 00:39:10,280
is saying, you're going to think that we don't know what we're doing. So that's one use case where it's,

465
00:39:10,280 --> 00:39:17,160
you know, it has to be extremely low latency and fairly high concurrency. So that, uh, use case for

466
00:39:17,160 --> 00:39:23,640
how, for our data management is very specific. Um, whereas for the data scientists, as you ask, we

467
00:39:23,640 --> 00:39:28,760
don't really have a need for, for extremely low latency because it's a, it's sort of our research

468
00:39:28,760 --> 00:39:34,520
environment that as we discover things that we would like to do, um, we can then go into our data

469
00:39:34,520 --> 00:39:41,320
lake and swim in it. So there are these different architectures that serve different purposes as

470
00:39:41,320 --> 00:39:47,000
what they should. So in our data lake, uh, environment where our data scientists are doing a lot of

471
00:39:47,000 --> 00:39:55,720
their, their researching, that's now a centralized place, um, that has, um, reduced a lot of the overhead

472
00:39:55,720 --> 00:40:02,120
that our data scientists need. So a lot of the processing is taking place centrally and, um,

473
00:40:02,120 --> 00:40:08,200
and we've abstracted a lot of the, the complexity of, of aggregating and transforming all of that

474
00:40:09,000 --> 00:40:17,240
in our lake so that the data scientists are, are, um, essentially dealing with, with derived data sets

475
00:40:17,240 --> 00:40:22,680
that already are in the format that is shared across all of our different products and all of our

476
00:40:22,680 --> 00:40:30,520
different, uh, platforms, so to speak. That data lake, um, is that, what is that, is that S3

477
00:40:30,520 --> 00:40:38,840
or Redshift or something else or some combination of, you know, other things? It's a combination

478
00:40:38,840 --> 00:40:46,200
because of the, the different requirements of how that, uh, uh, those different data stores are

479
00:40:46,200 --> 00:40:52,280
traversed. So, um, for the data scientific case in particular, we rely quite a lot on Athena.

480
00:40:52,280 --> 00:41:01,560
Okay. Um, for, for the, the queering and, and the, the creation of, of, uh, extracts, uh, for,

481
00:41:01,560 --> 00:41:07,000
for research, uh, one important thing to note as well is that there are, uh, significant access

482
00:41:07,000 --> 00:41:12,920
controls to all of these different parts of the environment and, um, there are also the, the,

483
00:41:12,920 --> 00:41:20,040
the different parts of the, the data lake that can be, uh, hashed or tokenized, um, so I don't

484
00:41:20,040 --> 00:41:25,640
necessarily need to know what you call a particular room. I just need to know that you care enough

485
00:41:25,640 --> 00:41:30,680
that you've named things, right? Like I don't need to know what you call your robot, um, but I would

486
00:41:30,680 --> 00:41:38,680
like to know that you love it enough that you've named it. Um, so, uh, those types of, of data, uh,

487
00:41:39,720 --> 00:41:45,320
care and, and maintenance are the things that, that we've centralized so that you don't have to

488
00:41:45,320 --> 00:41:49,720
worry whether the data scientist knows enough to not go spolunking whether or not supposed to,

489
00:41:49,720 --> 00:41:52,200
and we're really trying to protect our data scientists from themselves.

490
00:41:53,080 --> 00:42:02,520
And so you've got the, this data lake environment that is, uh, Athena and other things,

491
00:42:02,520 --> 00:42:10,200
depending on the use case, what are the types of tools that data scientists are typically using

492
00:42:10,200 --> 00:42:15,320
there? Are you doing a lot of deep learning types of things with, uh,

493
00:42:15,320 --> 00:42:22,760
uh, tension flow and that kind of stuff or, uh, psychic learn or what does the modeling experience

494
00:42:22,760 --> 00:42:29,800
look like from a data scientist's perspective at a robot? I tried not to be too dogmatic, uh,

495
00:42:29,800 --> 00:42:35,080
just because different tools, uh, are appropriate for different use cases. So when we are still in

496
00:42:35,080 --> 00:42:44,280
the, uh, ideating and, and researching phase, uh, we've used things, uh, that run the gamut, um,

497
00:42:44,280 --> 00:42:48,920
and one of the things that I mentioned is once these processes are actually, uh, running,

498
00:42:49,720 --> 00:42:54,600
on the robot, they're running in, in a resource constrained environment. So when we're learning

499
00:42:54,600 --> 00:43:01,880
things about the information that we, uh, have access to, those are running on our own, uh, hardware,

500
00:43:02,760 --> 00:43:07,240
our own compute, uh, not necessarily our own hardware, sometimes the Amazon owns the hardware.

501
00:43:07,240 --> 00:43:13,000
But still, those are, uh, we have a lot more flexibility with what we use and how it will be

502
00:43:13,000 --> 00:43:19,400
appropriate. And then it's on us, uh, for whatever algorithms need to run on board, the, the robot,

503
00:43:20,120 --> 00:43:24,760
to make sure that they're, uh, optimized so that they can run in the restricted compute environment.

504
00:43:24,760 --> 00:43:31,400
So as far as what we're using, I mean, yes, we do use sensor flow. We use, um, uh, different

505
00:43:31,400 --> 00:43:38,280
libraries. We certainly use psychic learn as well. Um, and in terms of, uh, what kind of, uh,

506
00:43:38,280 --> 00:43:43,320
methodologies we're using internally. I mean, you mentioned slam, uh, you know, iRobot has,

507
00:43:44,040 --> 00:43:50,600
a pretty strong, uh, published presence, uh, in the development of eSlam, um, but there are, uh,

508
00:43:50,600 --> 00:43:56,760
myriad other, um, algorithms that, that we use as well. Um, and we have teams that focus on deep

509
00:43:56,760 --> 00:44:01,480
learning. We have a team that focuses on reinforcement learning, um, for all sorts of different

510
00:44:01,480 --> 00:44:08,040
use cases that I can't actually describe yet until we launch, uh, but they're imminent. So, um,

511
00:44:08,040 --> 00:44:12,920
yeah, we have a, a pretty strong bench, uh, in terms of, of the different methodologies that

512
00:44:12,920 --> 00:44:18,520
we make use of. And I, I mentioned, we both mentioned slam at this point and I did not define it

513
00:44:18,520 --> 00:44:23,640
earlier. It's simultaneous. Simultaneous is that position in mapping, local addition in mapping.

514
00:44:23,640 --> 00:44:28,360
Yeah. So how the robots able to go out into the world and kind of figure out where it is and

515
00:44:28,360 --> 00:44:33,880
map it, it's environment. And if you've never seen it, uh, it's pretty remarkable. It's a pretty

516
00:44:33,880 --> 00:44:42,760
long demo. Yeah. It is. And I have, uh, I mean, I have a million robots. Um, it's, uh, it's one of the

517
00:44:42,760 --> 00:44:48,840
perks of the job having to test all of these things. But, um, I actually have, uh, I also have a

518
00:44:48,840 --> 00:44:54,920
young son. And so we have, uh, child gates, sort of baby gates all over the house. And it is

519
00:44:54,920 --> 00:45:00,760
remarkable to see the robots sort of figure out, oh, nope, I can't go this way. And it goes all

520
00:45:00,760 --> 00:45:05,000
the way around the floor map to find another point of entry to the room that I wanted to clean.

521
00:45:05,560 --> 00:45:11,400
So it's actually quite smart. And it's, you know, we forget to we who, who, who, who do this

522
00:45:11,400 --> 00:45:17,080
professionally. Um, you live in the zeros and ones and the code and to actually see a piece of

523
00:45:17,080 --> 00:45:23,000
hardware, uh, take that knowledge and apply it, it is actually really, really cool, which is

524
00:45:23,000 --> 00:45:28,680
something that in previous, in previous roles, I didn't necessarily have. But, but to see that

525
00:45:28,680 --> 00:45:35,320
mobile sensor platform take direction and utilize it and, and tangibly move around its, its environment

526
00:45:35,320 --> 00:45:41,000
and understand its, its spatial components. Right. It's actually really fun.

527
00:45:41,720 --> 00:45:47,800
So we've talked a little bit about data access. We've talked a little bit about modeling. I guess

528
00:45:47,800 --> 00:45:57,720
kind of last question in this vein, uh, deployment. How are you deploying models, uh, so that they're

529
00:45:57,720 --> 00:46:04,600
kind of accessible for inference? Like I'm thinking about AWS has like this greengrass thing where

530
00:46:04,600 --> 00:46:11,880
you can like deploy models out to the edge or I'm assuming that your, the, the I robot platforms

531
00:46:11,880 --> 00:46:18,120
aren't like greengrass endpoints or that you're running. I don't know. You tell me are you running

532
00:46:18,120 --> 00:46:23,960
like serverless model deployment on the robots and that kind of thing or how funky does it get?

533
00:46:23,960 --> 00:46:32,440
Yeah. So we, um, we're pretty bought into the serverless architecture paradigm. And so, uh, that

534
00:46:32,440 --> 00:46:42,520
is essentially how most of the, the platforms run on our side. But in terms of deployment, um, for,

535
00:46:43,400 --> 00:46:51,080
for the algorithms that run on the edge, we actually have, um, an OTA pipe that, um, because,

536
00:46:51,080 --> 00:46:59,160
because the OTA, like how you, thank you. Yes. Right. Exactly. Um, I forget my TLAs aren't everybody's

537
00:46:59,160 --> 00:47:06,920
TLAs, three-letter acronym. So thank you. Um, yeah. So, um, we, because these things are running,

538
00:47:07,800 --> 00:47:14,200
on a autonomous vehicle. I mean, these aren't, you know, on the streets. They're in your homes

539
00:47:14,200 --> 00:47:22,520
always. And so once, uh, once we, we introduce those models into the, the, the platform software

540
00:47:22,520 --> 00:47:26,280
and we compile it and we ship it, that there's a, there's an extensive amount of testing that needs to

541
00:47:26,280 --> 00:47:31,800
happen. So that, you know, if, if you're a customer and we're sending you these really cool new

542
00:47:31,800 --> 00:47:37,720
features, the thing you want this robot to do is vacuum your home. That's what you went and,

543
00:47:37,720 --> 00:47:43,240
and got this robot for. So that's the thing that we never want to compromise. And we make sure that

544
00:47:43,240 --> 00:47:49,400
the, the, any new features that, that we introduce, be them, uh, data scientific or not,

545
00:47:49,400 --> 00:47:53,720
is we have to go through this extensive, uh, battery of testing to make sure that the robot

546
00:47:53,720 --> 00:47:57,800
still functions and it's not going to, you know, act all crazy because it's hardware.

547
00:47:57,800 --> 00:48:01,880
It's not just that the zeros and ones are going to fly through the screen. No, the robot, uh,

548
00:48:01,880 --> 00:48:06,520
is, is a thing that can hit your puppy. So don't we definitely don't want that to happen ever?

549
00:48:06,520 --> 00:48:16,120
So, um, we don't have an over the air pipe that delivers just new model, uh, features. But what we do

550
00:48:16,120 --> 00:48:23,000
is we have this robust system for delivery both to factories and to customers. And that's all

551
00:48:23,000 --> 00:48:27,880
delivered through AWS, but it's not, uh, greengrass because that would be specifically just for model

552
00:48:27,880 --> 00:48:33,960
deployment. Well, Angela, thanks so much for taking the time to chat with me about, uh, just a little

553
00:48:33,960 --> 00:48:38,360
bit of what you've got going on there. It sounds like really interesting stuff and, and probably

554
00:48:38,360 --> 00:48:44,280
that we need to like follow up at some point to go into more detail. Yeah, that would be great.

555
00:48:44,280 --> 00:48:48,360
I love that. And thank you so much for the opportunity to turn around. Absolutely. Thanks Angela.

556
00:48:52,760 --> 00:48:57,160
All right, everyone. That's our show for today. If you like what you've heard,

557
00:48:57,160 --> 00:49:01,400
please do us a favor and tell your friends about the show. And if you haven't already

558
00:49:01,400 --> 00:49:06,280
hit that subscribe button yourself, make sure you do so you don't miss any of the great episodes

559
00:49:06,280 --> 00:49:36,120
we've got in store for you. As always, thanks so much for listening and catch you next time.

