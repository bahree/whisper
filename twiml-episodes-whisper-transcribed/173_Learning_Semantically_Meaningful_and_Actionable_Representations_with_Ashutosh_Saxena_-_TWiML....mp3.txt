Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting
people, doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
In this episode I'm joined by Ashtotash Saxena, a veteran of Andrew Eng, Stanford Machine
Learning Group, and co-founder and CEO of Casper AI.
Ashtotash and I discuss his Robobrain project, a computational system that creates semantically
meaningful and actionable representations of the objects, actions, and observations
that are robot experiences in its environment, and allows these to be shared and queried
by other robots to learn new actions.
We also discuss his startup Casper, which applies these principles to the challenge of creating
smart homes.
Enjoy.
Alright everyone, I am on the line with Ashtotash Saxena.
Ashtotash is co-founder and CEO of Casper.
Ashtotash, welcome to this week in machine learning and AI.
Yeah, sounds good, looking forward to the chat today.
Absolutely.
Let's get started by having you tell us a little bit about your background and how you got
to get working in ML and AI.
So I joined Stanford University back in 2005 and met Andrew Eng.
Andrew Eng was a professor in machine learning and doing some interesting stuff with flying
helicopters upside down using machine learning.
So that's how I got started into the area of AI and machine learning that my PhD there.
Since then, I've been applying it to robotics, computer vision, natural language processing,
and more recently into the smart home areas.
It turns out AI is a very powerful tool, so it increases the impact and the things that
you can do with it.
Talk a little bit maybe about how, you know, where you see the implications of AI and
smart homes.
It's not a connection that we hear about too frequently.
So it turns out AI is very powerful when the situations and what you're trying to do become
too complex to program manually.
So earlier before joining CASPER, I was a professor at Cornell University for a couple of years
trying to program robots to do things.
And it turns out when you have robots in people's environments, it turns out the home environments
are very complex.
Everyone's home is different, people do activities differently.
There's a lot of variation in the way people speak and the way people do things.
And just the visual information that you get from the cameras.
So for a robot, you're trying to understand what people are doing, how people are speaking,
what the robot can do for them.
Now similarly in the smart home space, if you think about some of the devices such as
the Nest Thermostat built by Google or Amazon Alexa or Google Home built by Amazon and Google
for conversational purposes, they have to, they have a really challenging problem because
you are trying to control the environment in people's house such as climate.
And then what you're dealing with is people who have a lot of variability.
So you have a speaking conversational system which is trying to speak with the person and
every person would speak differently.
Similarly in Casper homes, we control lighting, shades, cameras, microphone for conversations,
sensing for elderly and senior.
So there are a lot of things that we do in the homes.
Such a big variation in the data that it becomes very hard to manually program these rules.
So just to give a very simple example, let us say you talk to a home and say, I am watching
TV too much sun.
So close the curtains.
This is what your intent is.
Now people will say it in a variety of ways.
So you cannot write a computer program saying if person said these words, then do this.
If person said something else, then do this.
And the situation is also a lot of variability.
If you're watching TV and there is too much sun, probably means that the sun is shining
on that particular day.
So you cannot see the TV and the right action is to roll up the shades or close the curtains
and turn on some lights in the background and stop the music which was playing.
So all of this home applications with Alexa, with robots, with Google home and with Casper,
it requires a lot of data processing and understanding of the context to be able to do things.
This is where the AI becomes extremely powerful because it's data driven and one can create
actionable items to improve and do things for the people living in the homes.
We've talked about the challenges presented by smart home environments in terms of the
variability of the different types of data and the way questions are answered and all
the different contextual information that a system needs to keep track of to do what
a user wants.
One of the projects that you've been working on to address all this is something called
Robobrain.
What is that?
Yeah, so it turns out that in some of the areas, it's variety of data and collaboration
through data helps a lot.
So in the area of computer vision, a lot of researchers have been collecting images and
putting image labels like if there is a car in the image, if there is a person in the
image and with this data, it really helps to really help the computer vision algorithms
to train and develop better techniques and use things such as deep learning to build
these classifiers.
Now it turns out in the area of robotics and homes, such progress hasn't been made that
well.
There have been some attempts, but lack of data means that every researcher, every university
was redoing the same things over and over again.
So that's why we started this Robobrain project as a collaboration between Cornell University
where I was, Stanford University where I did my PhD and couple of other places such as
Brown and so on and so forth.
And what we did is we had robots set all these places and the idea was when robot is performing
and learning in these individual environments, for example, a robot at Cornell, maybe trying
to listen to verbal commands and trying to make a simple kitchen recipe like like Afogato
or trying to operate a coffee machine.
A robot at Brown University could be trying to learn how to pick up things and put them
back and a robot at Stanford University may be focusing on image classification tasks
and how to figure out how to see things.
And the idea is if we can pull all this information together in a good learning representation
way, then suddenly this project enables all the robots and not only just the robots at
these three universities, but in many other places more powerful.
Historically, I think ten years back when I was doing my PhD at Stanford, our group was
developing a lot of these robots and one thing that came out of this effort in Enduring's
group was called robot operating system.
So that was an effort of a similar kind where the robot software was open sourced through
Stanford, Velokaraj and many other universities, and it allowed a lot of different groups
to collaborate sharing their software.
And it realized that Ross came out of Enduring's lab.
Well, it was a collaboration that started out from Morgan Quigley who was doing PhD with
Enduring.
And then Velokaraj really helped to expand it with a variety of new people joining at Velokaraj.
Okay.
Cool.
Ross at that time was a very small project where we were just trying to make sure that
a couple of universities could actually connect our software to each other.
And now like ten years later or eight years later, I saw a very similar situation where
people were developing machine learning algorithms individually in different places.
And centralized effort did not exist to bring all these learning skills for robots together
in one place.
And that's why we started this robot brain project.
So if I can maybe paraphrase that what I think I heard is that you've got, you're trying
to train robots on a variety of different tasks.
And you want a robot learning thing B to have some benefit, to be able to learn from
what, another robot learning task A, you know, learned, you know, one way to do that
might be to kind of share the model that it learned.
Like if you're training a deep learning model, you share that from robot A to robot B and
vice versa.
But it sounds like you're proposing something a little different than that.
Is that right?
Right.
So some part of it is one way to share things is you share a learned model from one robot
to another.
So a part of robot brain is to allow to do that.
And a part of that is also to become a platform such that it becomes a good representation
to figure out what model to share to go deeper into one example.
Let us say you are trying to make a kitchen recipe like afogato, which is take the ice cream
out, put some coffee in it.
Now for this, you require machine learning or deep learned models for computer vision where
you are doing classifications.
You need machine learning models for grasping where you are telling how and where to touch
the objects in order to pick it up and then for pouring that recipe and also models that
allow you to parse language into actionable items.
So you need a variety of these models.
So you first need a representation that allows you to compose these different deep learning
models together, which is what robot brain does.
And is it fair to think of it as metadata for describing different situations or is
it representations more in the mathematical sense where there's maybe some vectorized or
semantic representation?
So I think one way to think of that is, let's take a simpler example like a dictionary.
What a dictionary does or a Thessarist does is or even Wikipedia, for example, is that
it has topics or words and it tries to explain the meaning of existing words in relations
to other words through some qualifiers.
So there could be a word called running and the way you would explain running is it's
walking but with a faster speed.
So now running is explainable with two other terms walking and faster in a speed, right?
Similarly, now let us take the example of robot brain.
So you're right.
It is essentially one can think of that as a graph.
In the graph, there are not just words, but also physical situations such as how to move
your arm or robots arm.
How does visually some neurons look like?
What is the spatial relation between two objects?
And these items become nodes in the graph.
And then there are the relations between these edges is an explanation of that concept.
So to give an example, if someone says, oh, I want to pick up a cup.
So the semantic node of the cup would be connected to the table because table is cup is usually
on a table on a countertop, but it would also explain in this case that cup is usually
on top of table and it would include information about x, y coordinates, z coordinates.
That cup is usually two inches away or the bottom of the cup is touching the touching
the table and that helps the robot quite a lot because if it can find the table in the
environment, then it becomes much easier to find the cup.
And then the cup was also be connected to the handle with the information that the handle
is usually used to pick up the cup especially when they're the fluid inside the cup.
So there would be a heat map of sorts on this cup that looks like a neuron, which if
you run that machine learning model, it would find that handle.
So it becomes like a Wikipedia for robots where you're not just linking words.
You're actually linking semantic labels with physical locations through these different
models.
Interesting.
I often hear when we talk about one of the challenges of deep learning or AI more generally
this idea that we can train a model to do things, but there's this whole broader concept
of common sense that helps us as humans navigate within a world and do different things.
It sounds like part of what you're trying to do with robo brain is find a way to encode
all of this common sense, for example, that cups usually sit on top of surfaces, that
when they have liquid in them, that liquid can spill, that you want to hold the handle
in a certain angle so as to not do that.
And then that becomes this robo brain that can be shared between robots doing different
tasks.
It's almost like the Google knowledge graph for textual data, but with, you know, between
not just textual data, but also actions and kind of these semantic relationships.
Exactly.
So this is what it is.
That's why sometimes we call it Wikipedia for robots or knowledge graph for robots because
in addition to symbols and hyperlink documents, it basically contains 3D information about
the real world and appearance models and so on and so forth or actions that are relevant
for robots like how to pick up and move the arms and it's a graph, but the variables here
are semantic spatial and things that are relevant for the real world.
And so when I think of the full power of something like this and even the example or
the analogy to Wikipedia, this thing is most powerful when there's a single instance of
it with lots of people contributing to it.
Does, you know, how well defined are the interfaces to this thing such that, you know, lots of
people can contribute to it and is that, is that happening yet?
So we, it's a university project just like a robot operating system with more than quickly
and enduring was a university project in the very beginning to prove a point.
So at this stage, it is still a collaboration with just two, three groups, a university level
project, Robobrain where we showed that a robot being trained at Cornell with the Robobrain
hosted at Stanford could be used to have a robot at Brown University in a different part
of the country pick up objects and do something.
So we actually showed that loop being completed through these interfaces, the interface here
is the graph.
So you can query the graph, get all the information from different models and make it happen.
So it's not in an industry-ready state, but I think academically it proves a point.
So maybe walk us through before you move on, walk us through that specific proof point.
You know, it strikes me that you can, without the details, that can either be really interesting
or not all that interesting at all, like as is often the case to devils and to details.
What specifically is happening in this scenario and what's the graph that's being transferred?
How is it transferred or all those kind of, those are kind of the questions swirling around.
What's the best place to start with that?
Yeah, so maybe that's a good idea to dig deeper into this particular example that we did
a couple of things, but this is, we can take up one example on how the graph would look
like and go a bit deeper here.
So let us see.
So let's take a very simple example of, well, it's not simple from aerobotics point of
view, it's something that is very simple, just pick up the cup, it's actually very hard
for a robot to execute, but things are getting better.
So the scenario is that there is a robot, two robots actually at Cornell University and
one of them is trying to match the object parts to actionable items.
So this project was done by my PhD student called Jason and the project name is Robobarista.
It essentially goes around in different buildings on campus and uses something in which is known
in computer vision, community as a part-based model, like object part-based model, but more
focused towards robotics control and manipulation.
So just to give some examples, it goes around to a light switch and in 50 buildings and it
operates them now, it has a model that if the light switch looks like this, then it can
touch it from the top or the bottom in this particular way and turn on the light, right?
It learns how to operate the switch.
Then it goes around and finds all the buttons on the campus and it tries to keep pressing
them and then it correlates how the button looks in 3D and visually and tactile and how
do you press them.
And then it goes around and touches all the handles or levers and tries to operate them.
Now it comes back and it does some machine learning and it produces some data in models
and the data looks like the following.
This kind of input data which it represents using a deep learning model correlates to
this action for the symbol buttons.
For the symbol switch, this is how it looks like.
So these become some notes in the graph.
The information is not complete yet because it's not usable at this point in time.
Now another PhD student was focusing on a project called Telmediv called Deepynth Mishra
at Cornell and he's focusing on how to now use this 3D and joint angle information and
link them semantically.
So his focus was on natural language processing and mapping them to symbolic actions.
So his project is, oh, how do I make Fogato?
So Fogato, you go to a coffee container, try to use a lever to pour coffee in the cup
and then you take a cup and try to, it has some ice cream in it, then you try to put
some raspberry syrup in it and coffee.
Now let's think about it.
It is actually reusing similar actions from the other project which is how to operate
the lever, how to hold a container and move it around.
So in this case, the graph gets appended with additional nodes and additional edges.
So the nodes here become cup lever and that if you press the lever, then something comes
out and there is an online recipe.
So now you're basically extending the graph here, right?
So this is what the graph that was created at Cornell and then we went to Stanford.
There was a group with Silvio Severus, a professor in computer vision group and they
focus on computer vision appearance and part-based models and so on.
So there with a couple of other people like in the group, PhD students like Ozan, Sandher
and Ashish Jen, we built this 3D model which is a cup is on the top of a table.
So let's collect a lot of data about cups on the top of the table and from the data you
would learn that a cup when it is found is at a certain pixels or certain inches away
from the table and you would try to pick up this cup many times.
So now this graph has a lot of information.
It has information about that the cup is on the top of a table, how to use levers and
buttons and so on.
Information may not be perfect, that's okay.
Now what happens at Brown University that we have a different robot called Backster in
Stephanie Telex's group and what they are trying to do is really make grasping work very
well.
But it turns out that the grasping the way you grasp as in like pick up an object actually
differs from object to object like a like a whiteboard eraser or a pen lying on a table
you would pick it up from the top and a cup you would try to pick it up from the side
because you don't want to affect the liquid in the cup.
So they are focusing on this robotic action of picking and closing the gripper.
Now what we showed is that if you query the graph with whether it's a cup or a whiteboard
marker or a certain type of object, then you would receive the information that these
are more probabilistically likely areas for doing the grasping.
So that backster robot queried the graph through an API and then we showed that within without
that information, the grasping performs better.
Within that explanation, there are so many little details that individually are difficult
problems in computer vision and robotics.
Navigating around the campus is, you know, there are still lots of interesting problems
around there, even identifying what might be a switch so you can start to poke at it
and determine what might be the actionable surfaces, you know, and then to kind of abstract
away from that to identify that the switch has a top and a bottom.
There are so many individual challenging problems in robotics and AI, what?
Maybe, you know, rewinding that description a little bit.
What are some of the simplifying assumptions that were made in these different kind of
sub-problems?
I think we realized that so within a couple of these groups that we were working with,
that was actually the biggest discussion point because each of these problems like navigation,
manipulation, computer vision, language understanding and so on and so forth, we involved I think
like five different research groups, natural language research group at Stanford, machine
learning group here, computer vision group there and so on and so forth.
The biggest discussion point was what is the representation that became the biggest
question because if, and that actually also highlighted the difference in the different
specific fields, people get a little bit too narrow like language people think about things
in symbolically like a cup and tea and mug and drink and they forget that drink and cup
has a physical meaning it's actually a 3D thing and the people in navigation only think
about 2D, I took the robot from here to there but it's not my job to and the robot was not
close enough to the door and it turns out that if it's not in the right position, you cannot
operate the arm to open the door, it's just kinematically impossible and so on and so forth.
So this was a good exercise in a way to come up with these representations so some good
outcomes came out of it so one good outcome was relation between the natural language symbols
and 3D and spatial information.
So when you start having things like pick up or grasp or on top of like a cup is on top
of table, correlated with the actual 3D or spatial world, it becomes very powerful because
now you can say cup is on top of table, well next time a mobile phone could be on top
of a table but you don't need to observe it with 10,000 images, you can reuse the semantic
notion of on top of to now extend the 3D information that you just learned that cup is on top
of a table which means that the bottom part of the cup is on touching the top part of
the table, it's very powerful 3D information, right?
It goes to the mobile phone now and laptop and so on and so forth and suddenly this transmission
of information happens and then you can start extending the knowledge, I'm not saying
this is complete but that's the idea that we were going towards.
So other than this extension and correlation of different pieces, one other good thing
came out of it is in the area of deep learning, it was a CVPR paper that we wrote in 2014
at computer vision and pattern recognition, it was a best student paper over there and
what we did, what we found in deep learning area is that most of the models are put a
vector in and get something out, they are not interpretable, I mean there are exceptions
but majority of the work in deep learning is not interpretable, you cannot put common
sense or existing knowledge into these models and that was a little bit orthogonal to what
we were trying, many people were trying to do in robotics where there is information which
you want to reuse.
So what we developed was called structural neural networks which essentially what to do
is instead of trying to learn a big giant model which can only take one type of data and
produce one type of output, you start producing these composed, recomposable models.
So each module would focus on specific thing and they would dynamically connect to each
other depending on slightly different situation and that allows it to extend really, really
well to different data sets, different input modalities and different situations.
Your description of the effort around identifying the representation reminds me of, I don't
know if you've come across the term master data management from kind of the enterprise
computing side of things but is basically like a large company will have these different
divisions and they all have customers but they all think of the customer slightly differently.
So they go through this MDM effort to really net out what is a customer, what attributes
does a customer have, how to independent of where they represent a customer whether it's
a CRM system or a ERP system, whatever and they do this for all of the entities that
they have to track in all of these different systems and it sounds like part of what
you're doing here is like creating a master data management system for the objects that
these robots will have to manipulate.
Actually, I did not know about this MDM but looks like we are doing something like that
except that instead of when we think of data we actually mean learnable representation.
So I would call it master representation management for robots or something so that would be
a little bit more apt analogy and so you were careful to say learnable representations,
what are the specific implications of that learnable element?
So this learnable part is actually quite important so what happens in a usual like a dictionary
sort of graph, I mean you can represent a dictionary as a graph right, it links words
to words through some explanations and synonyms and so on that it becomes fixed and the links
do not adapt because the variability in a language dictionary is finite but when you are
working with more complex models such as Google knowledge graph or other elements, the
variability is very large and it has to adapt much faster.
So just a simple example like oh well let us say you are you observed that a switch can
be flipped up to down but a switch can come in three different forms, a up down switch,
a press button switch or this toggle switch and robot may not have observed all the three
switches on the on the first data set collection.
So therefore you need to have a representation that given more data it can add to this information
right.
It learned the model about the switch and the finger that oh whenever a robot finger
goes to the switch you can turn it on and it learns something but it cannot be fixed.
Next time a more data comes in from another place maybe we connect to Japan and they have
slightly different switches there which they do then it should be adaptables.
So that is how I think the representation would improve over time and become with every
action or every learning that any happens anywhere for robots the representation will keep
getting better for everyone.
And I'm curious practically speaking what how is this representation system that is ultimately
queried, how has it been implemented, is it like graph databases or you know a simple
relational database or you know something else.
Oh yeah I think it's basically implemented as a graph and it's a simple it's actually
a sparse graph right because not everything else connected to everything else.
So it is stored as a star sparse graph and one thing we have is what that we call as robot
query language or robot query library and this graph provides a certain types of certain
type of interface so you can query the graph saying that oh give me everything that is
connected to this node or give me everything related to the tactile links related to this.
So I want to operate it up but I only care about tactile information so it allows certain
type of queries it also has other complex queries like give me the most probable answers
because sometimes it has each query may result in a sequence of answers like oh for
grasping a cup there are these five locations usually but some people robots may want to
query all five so that they can optimize according to the situation.
On the other hand some other situation may just want the top one mixed probability query.
So that's how it is implemented.
Do you use this Robobrain system at Casper or have you built a separate set of systems
at Casper?
So I think what happened about two years back this is more of my personal story as compared
to Robobrain or Casper's story is I was looking to see how can I take these ideas and
this opportunity to a bigger impact and I realized that in order to make people's lives better
one can if one starts thinking about people's homes it's a really big impact situation
because a lot of people have homes and not many people have robots.
So I started talking to real estate developers for elderly housing and for normal housing
as well and I wanted to create a similar sensing scenario so it turns out at Casper it's
a robot inside out so it's a robot in which people live inside.
So our home has same level of sensing in fact more than a typical robot.
It has cameras, microphone, motion sensors, thermal sensors so you can see where people
are and what they are doing and locally I respect privacy and then it starts to learn
in a similar way that some elderly person fell down on the bathroom floor what to do about
it and so on and so forth.
So it turns out that the techniques and ideas are similar that you have to learn from
homes and share this information and so on but in technical detail we are doing different
things as compared to Robobrain.
In the Casper scenario you mentioned for example learning that someone's fallen and what
needs to happen how would a robot or a robotic home do that without some specific training
and guidance.
So the ideas are similar to what we talked about the details, algorithm and techniques
are quite different but let me give an example about the falling.
So it turns out that one can start thinking of that as a recomposable deep learning representation.
So there may be a model that can detect objects, there may be a model online that can detect
human poses and track people, there may be a model that knows about geometry.
Now in a home situation you take these recomposable models, put them to detect falling.
So this is how you are reusing information and in some of the houses what happens is after
people fall they ask for help and the expectation is that someone may be available to come there
and help.
So Casper knows that whenever a person fell down in these couple of houses it was an important
scenario that someone may have been called.
So it starts to adapt and now we are building a whole 132 senior home complex near vehicles
and now it has learnt a lot of such things that if people don't get up on time in the
morning then there is something wrong about it that sometimes seniors don't get up in
the morning.
People have a certain pattern for drinking water and it turns out that many elderly people
forget to drink water and that's how it keeps on adapting from these situations by looking
at data from different sources and tries to address those problems.
I'm trying to come to terms with a very high degree of skepticism and it's really I think
around this idea of undirected robotic learning meaning how does the thing know what its objective
function is for example.
What is it trying to optimize without some operator telling it that these are the things
that it's trying to optimize does that make sense as a kind of a half question?
Right.
So let us take an example which is a little bit more concrete toy example.
It may not be as impactful as senior falling for discussion purposes.
So we can discuss some of these important points like objective function and action variables
and so on.
And this example could be let us say Casper robotic home is trying to learn what people
trying to adapt the wake up function in the home.
So usually in alarm clock you just beep the alarm clock and people either wake up or not.
But in Casper home Casper can make a sound to wake you up play music open curtains or
shades for natural lighting in different rooms.
It can turn on the lights.
It can even flash the lights and so on.
And even before you go further down this example there is an assumption that someone a developer
of Casper decided that Casper should be trying to worry about when people wake up.
It's not like it just saw that people tend to wake up and so it's maybe I should optimize
this.
This is a feature of the system that you've built.
Right.
You are right.
So I think there is a little bit of human guidance involved there.
It's not a free AGI.
So certainly I think the level of representation that we manually guide the system is that there
are these things called activities that people do.
And there are these things called these devices that you can operate in a certain way and
you can do crazy things with it like don't open close the curtain five times in 10 seconds
otherwise.
So there are certain limits that and constraint that is given to the system.
So in this particular case what happens there are let us say five variables, music, sound,
curtains, fast curtain, slow and lights, bright lights, dim and so on and so forth.
So these are the actions.
The input variables are how many people in the house, when was the person sleeping last
time?
Did he explicitly set an alarm or not?
Is it weekend versus weekdays, summer, winter, so on and so forth?
And now what happens is you have input variables and output actions which have to be mapped
to each other and let us for the time being assume that they are perfect and they exist.
The part which becomes interesting is how do we take feedback and the objective function.
So here in this case what happens is the feedback is sort of an imitation learning because
Casper doesn't do it the first day.
It kind of observed.
So it sees when people open the curtain.
So it turns out surprisingly that there are there is kind of a clustering of behavior of
two types of people, one set of people like natural lighting and wake up very fast within
a few minutes and they are active like the jump up from the bed and move around.
And the second type of people are kind of just stay in the dark sluggish until they get
them on in coffee and they like it right.
There is spectrum of people but we have found that there are these clusters.
So Casper starts to learn that this person is like this because he took the action of
opening curtains manually.
So so a couple of times he may have opened the curtain manually Casper open curtains Casper
turn on the lights and he may have done it more on the weekdays when he wanted to go to
work versus weekends.
Those become the signals or are supervisory signals.
And that starts to get enough data to map the input to the output which is activity wake
off wake up and the situation of number of people in the house morning time weekend versus
weekday to what to do curtains music, bulbs and so on.
Okay.
So what I heard I might explain as you know part of what you're trying to do is build like
if you think of the nest thermostat right it's got you know a couple of sensors proximity,
temperature and like the controller the dial thing and it's got an effector to that
controls you know the HVAC system and it's trying to you know what differentiates it from
a traditional thermostat is that it's taking this input from this dial but also projecting
that to a number of other features like you know whether it's a weekend or a weekday
or whether someone's home and all this other thing and creating a you know a more rich
or complex relationship between the explicit input and the temperature at any given point
in time and it sounds like what you're aspiring to do with Casper is you know almost create
like a generalized platform for these kinds of relationships between sensors and effectors
that might exist in a smart home.
Yeah precisely so nest is doing is has a very important vision currently they are limited
by that they only have as you said a couple of proximity sensors and one dial or something
like that and possibly as they combine with Google Home a little bit of more supervisory signal.
In Casper homes we partner with builders so we have extremely rich data and just like Robobrain
it all boils down to what data and what level of representation that you can operate at.
So if you operate at oh if there is a motion sensor then I will do this then then this kind
of doesn't represent the real world situation which is quite complex like motion sensing can
happen for many reasons not because someone woke up or someone came home and and there are too
many variables so there's a certain level of representation and data that needs to be
included to enable some of these interesting features.
Okay interesting stuff have you by any chance come across a company called Ariel?
I may have heard of it but not on the top of my mind.
So I did an interview with with them not too long ago and basically what they do that maybe
really interesting in this Casper sense is they turn Wi-Fi signals they get they can pull some
metrics off of your standard Wi-Fi access point and through data science machine learning
uh turn that into very rich representations of you know what's happening within the space like
the speed at which people are moving localizing people the rate at which they're breathing
like really interesting stuff if you haven't already looked at that it may be a rich source of
new data about what's happening in the space that you can you can incorporate.
Oh that's a good pointer I will look it up.
Yeah it's ariel.ai is there A-E-R-I-A-L I think I always misspell it but uh you should be able
to find it Ariel Wi-Fi. Cool well this has been a really interesting conversation I I appreciate
you taking the time Ashutosh is there anything else that you'd like to add to close the circle on
on this? Yeah I think one just last couple of comments on AI and then academia and industry
I think for folks who are in industry and building a product I think this is this is one thing
that I personally found when moving from Stanford Cornell to Casper is there is this
interesting substantial impact that AI is making it possible and it's opening up new challenges
in software engineering product management and so on so it is surprisingly become powerful to
make products that were not possible before because the scale at which these learning models can
operate upon and we haven't even yet tried to yet understand the general principles of software
engineering and product management with AI so the questions that you ask like is it a general
level question or how a designer is limiting it to wake up why are they choosing wake up versus
sensors and then how do you even build a testing framework for AI which operates probabilistically
so these are very interesting challenges that as AI becomes more and more mainstream into products
that we have to address absolutely absolutely in fact just earlier this week I was at an event
I was speaking at an event down in Dallas the capital one hosted and after my talk I was
approached by someone who you know raised a very similar point right he said I'm you know I'm a
product person I don't necessarily need to know all of the details here you know around deep learning
and some of the other things that I was speaking about but there is a whole other set of issues
that I need to be aware of in terms of how we take advantage of data generally and machine learning
in particular to build products that you know meet the needs of some set of some set of users and
so there are definitely huge issues there and I've got on my my short list some conversations
that I want to bring on to the podcast to you know start us thinking about how we approach product
from a ML and AI perspective or ML and AI from a product perspective you know more specifically
sounds like it's something a point that you're starting to see as well uh definitely because
cash per is now deployed in real homes with real customers across the globe and and now now this
AI and real world product it comes up awesome awesome well as you touch thank you yeah thanks a lot
for the interview interesting conversations all right everyone that's our show for today for more
information on Aschitash or any of the topics covered in this episode head over to twimlai.com
slash talk slash 170 if you're a fan of the pod we'd like to encourage you to pop open your apple
or google podcast app and leave us a five star rating and review your reviews go a long way
in helping new listeners find the podcast as always thanks so much for listening and catch you next time
