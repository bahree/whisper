All right, everyone. Welcome to another episode of the Twimmel AI podcast. I am of course
your host Sam Charrington. And today I'm joined by Bill Bass, VP of Engineering at Amazon
Web Services. Before we get going, be sure to take a moment to hit that subscribe button
wherever you're listening to today's show. Bill, welcome to the podcast.
Happy to be here, Sam. Thanks for having me.
Absolutely. I'm looking forward to digging into our conversation. We'll be talking about
all things synthetic data. But before we dive into that, I'd love to have you share
a little bit about your background, your role at AWS and kind of how you came into all
this ML stuff.
Yeah. So, so I run about 42 services here at AWS, but I have the advantage of being the
AWS for, for about, almost eight years now and have run many different things, whether
it be things like S3 and Kinesis and other things like that or CloudWatch. Our team really
focuses right now on emerging technologies like quantum computing, robotics, autonomous
systems, high performance computing, gaming simulation, mapping, IoT edge computing, connected
car, those kinds of things. My background, I've been a software engineer, Michael,
life basically started working in 1978 as an engineer working on actually on autonomous
systems back then, interesting enough in ocean-going autonomous systems. And then worked a lot
of different places. I was a CIO, the Pentagon and the CTO for the Army and CIO at some
microsystems. And I ran a robotics and autonomous system company called Liquid Robotics before
I came to AWS. So, a pretty diverse background.
Oh, that's awesome. That's awesome. Was Liquid Robotics the company that had the small submarine
that they used to check undersea cables? No, no. Liquid Robotics did long-term surface
vehicles, right? So, we won the Guinness record for the first autonomous surface vehicle
to cross the Pacific. Oh, wow. Which was a big accomplishment for you.
That sounds like quite a fun experience vehicle. Yeah, it went through two typhoons on its
way between San Francisco and Australia. That was exciting as well. Interesting. I'm wanting
to ask you about the relative complexity of this one complex system versus the very distributed
types of systems that you work on now at AWS. Does that question resonate? Do you have any
thoughts on that? Yeah, it does. So, the robots were, they generated their energy for
a motive force with waves and they got their electrical energy from the sun. So, every
millawatt mattered. So, they basically were a little arm server racks and arm because it
used a lot less power. And they could also swarm. So, they would operate in swarms. And
you can imagine a diamond shape swarm of a bunch of robots moving together and changing
direction together, sweeping the ocean floor or doing searching for things. At one point,
we also tracked sharks and icebergs and also did oil and gas exploration and a lot of
work for the military as well. So, these were very long-duration vehicles. The ability
to operate for a year at a time on its own offshore, which is really hard because you combine
electricity, salt, water, and metal along with biofowling or like the three worst things
you can put together. And really rough environments. It's interesting. We had a number of people who'd
worked on the Mars rover on the team and they just talked about how much harder it was to
deal with the ocean than Mars. So, it's a very different environment. But that company is
still around. It's called Liquid Robotics. It was purchased by Boeing to do defense operations
and things like that. So, it's a... And then I left that to come here. We were kind of building...
We kind of looked at it as the AWS of drones, if you like, because you could get them as a
service in swarms and they're a lot cheaper than operating a ship with people on it. So,
next to out for really long times and go through, I think before I left, we'd been through about
30 hurricanes. And so, the ability to have a vehicle operate through hurricanes is really unusual.
Most ships don't do that, at least intentionally. So, awesome, awesome. Now, our conversation
comes on the heels of the AWS Remars conference where you made several interesting announcements.
The ones that come to mind is interesting context for our conversation were
Astro, this home robot that kind of runs around your house. I'm sure that will come up.
But also some new synthetic data generation capabilities. And as we were talking about this beforehand,
one of the terms that jumped out is really key to the way you think about this need for synthetic
data is the idea of data density or dense data. And machine learning and deep learning,
we often think about just more data. But dense data seems to suggest that it's not just more data,
it's about some kind of quality of the data. Why don't you elaborate a bit on that?
Yeah, so if you kind of look back to what I was doing in 1978, we were doing undersea
autonomous robots. These are the ones I used to do at Liberty Robot for Surface robots in the ocean.
And we used Neuron Network to help it navigate. And back then, this was before GPS, this was
using Laurence when it was on the surface, but when it was under the surface, it was using differential
sonar and compass to navigate. And the robots got lost all the time. My boss used to call it artificial
stupidity instead of artificial intelligence. He was a mechanical engineer, so he didn't
respect software engineers at all. So this was one of those back and forth things.
And the reality is a lot of the way we build these Neuron Networks hasn't really changed that much.
I mean, there's certainly been advancements in deep learning so that markers can be automatically
identified and created and the different types of models and things like that have improved
significantly. But the thing that really makes it different from 1978 to today is the massive
amount of compute and storage that you can apply to the problem. That's what's made, you know,
Alexa's work, that's what's made, you know, autonomous vehicles work. All these things is the
amount of density of storage data and the amount of GPUs that you can apply and CPUs you can
apply to building your models and training models. And so one of the big challenges in that is
getting enough high quality data to train. And so starting early in our fulfillment centers,
you'd think that we would have enough volume of packages and pictures of packages to train
robots to be able to identify packages along with being able to do a grass plan on packages and
items. But even with all of the packages that we have a grass plan, yeah. So basically,
to do a plan, I mean, each time a robot does a movement, it has to initiate a plan on how it's
going to use its actuators or move around a room or where it happens to be. That's my
Astro in the background. He's listening. Yeah, he's looking at me right now. Anyway,
yeah, I'm a little bit fascinated watching him navigate around the house, too. But the these
plans require, you know, the more data and high quality that you can provide, the more accurate
the planning becomes in your models. And so even with all the billions of packages we ship,
we still don't have enough pictures of packages in enough random positions and items in enough
random positions. So for Amazon Robotics, we initially built a product. It was codenamed B12.
I can go into why it was codenamed B12. Yeah, well, it started. So a Robo maker was codenamed
B9. And the reason for that was if you know anything about robotics, you'll know that's the
Lost in Space robot, which is, which is this robot. If you remember, the danger will arrive
as a robot. Anyway, that was that was the codenamed for for Robo maker. And so the
you know, this B12 was just here. We did B10 and B12, whatever. And so we
started this need to generate enough data to train grasping plans for picking things up. And
so the way we ended up doing is first we tried taking a lot of pictures, right? That's generally
what you do is you take pictures with the same density of sensors because you need your
sensor resolution to match the pictures, right? If you don't have the sensor resolution matched,
then then you get a mismatch in the the training models. And so are you saying are you referring to
matching the sensor and the when you say sensor, are you talking about cameras and you're talking
about matching the image dimensions and other characteristics with what you're using to train on
or are you referring to some other sensor? Yeah, so you want to have, for example, if you're training
an iRobot to go around a house, which they do with WorldForge, they wrote, they generate that
resolution at 720 DPI because that's their cameras, right? So our cameras are 1020 or 4K
on our robots. And so we train with those and then you have to take the same training that you
would use with LiDAR or same training you use with radar, you need to match the sensor, right? If
you don't match the sensor, again, you're not going to have the right training model. Robots get
very, our computer vision gets very sensitive to the sensor in the loop, if you like. So we have
another product called Kinesis Video Streams or KVS that we use all over Amazon,
and it will stream any frame series time data, radar, LiDAR or video data. It's how we
use security in all our data centers is how the Amazon Go stores work, which is, you know,
all object computer vision as well. It's how we stream the data off our robots and things like that.
And so what we ended up with is just not having enough data to do very good grass
plants. And so we sat around thinking about, well, how could we generate enough data that's
very high quality? And so we ended up with this idea of starting to do synthetic data generation.
And then we found out a lot of customers had this problem of not having that data. We had one
customer who was trying to detect defects in their engines, car automotive engines. And they
were training with 300 photos. It's the most number of defect photos they could generate.
And they kept complaining that their models were only about 80 or 90% accurate in detecting
defect. And a lot of defects were going through. And we realized that you really need about 30 or
40,000 pictures of each defect to really have a dense model training. And you need to have that
picture in all different lighting conditions from all different directions. So you could hire someone
to sit around for three years taking all those pictures. I'm trying to kind of correlate the
example that you just gave this kind of a fault detection kind of example where you've got,
you've got kind of this clear long tail of defects where you just don't see them very often. They're
infrequent that kind of thing. And I'm trying to refer that relate that back to your warehouse example
where you've got kind of these billions of packages and you can take pictures of all these
packages. Is it the same kind of long tail effect but just at a much bigger scale or is there
something slightly different happening there that's causing you to not have the images you need to
train? Yeah, it's the same issue because even with all those images, you only get a certain number
of images of the package in each position, right? If you imagine a package falling and it falls,
it can fall in many different positions. And you just don't get enough images of a single package
of a specific size in every possible combination, right? And so those outliers where it would fall
in a strange way, it almost never happens. And then when it does happen, the robot doesn't know what
to do, right? So in this case, with the engine blocks, you're taking a look at the engine blocks as
they go by and you just don't have enough defects physically you can create with enough images
to recognize defects, right? So what we did is we took the CAD, the AutoCAD of the model,
and then we used a game engine, actually we used Unreal to generate photo-realistic synthetic data
that matched the camera resolutions for those engine block detectors and then trained it
successfully to identify defects. And then in our packaging, grasping plans, we're generating
the same thing. We are taking all the packages as they drop. We actually put physics in it so you
can see if you watch the videos from re-invent them bouncing on the conveyor belt and they look
like packages. I mean if a human watch is that they think that it's really packages but it's just
the images of packages, right? And it's a synthetically generated conveyor belt and synthetically
generated package images. And then what we do is we start with the CAD, the AutoCAD for those,
and we actually do have an artist in the loop that does the initial work and then the
we roll through an algorithm of an infinite series of combinations that we generate. So we generate
literally tens of thousands of pictures of each package falling in each different
possible direction. And then from that those images are then set into the machine learning
model which allow the robot to learn. And then we do the same thing with, and that's the
astro again when you heard robot. Anyway, and so when you have the the grasping plans for all the
items that you want, so the the arm to pick up, you do the same thing. You take you know like a
a coat bottle and you take photographs of it from all different directions and with all different
amounts of Coca-Cola and it let's say and with its sweating and not sweating and with different
lighting and all that other stuff. And you just it would take you you know a huge amount of time
and effort to do that manually. With SageMaker synthetics you can just create the initial model and
say generate you know 30, 40,000 photos if you like using the machine learning. I mean using
the the game engine generation and then you just feed that into the model. And so this gets you
much a much denser model with a lot more interconnects and allows you to more accurately execute your
machine learning. So it's it's it's pretty amazing how much how low it works. You know that there
are certain things you have to worry about as far as drift from reality right that you have to worry
about things like that. And then we sort of extend that with our world simulations as well. So so
so SageMaker synthetics is very much about generating both defect or non-defect individual objects
for both grass plan and defect detection. And then WorldForge is for generating synthetic worlds.
So it's the same idea is just expanded into a 3D synthetic world and it's being generated also
by a game engine. You can pick your different game engines for it. And so for this case with
with Astro and or IROAT with it they're robots and things like that what you do is you choose to
generate synthetic houses. So so what what they do is they'll generate a whole bunch of synthetic
houses and then they'll do years worth of testing in those synthetic houses and do their
machine learning models and training there. And we do the same thing with Astro we will will
generate these synthetic houses as well. And that's what WorldForge is about. Again if you had to
hire an artist to do it it would cost a lot of money. You can define all the parameters for the
houses like you can define different types of furniture different types of textures how many
bedrooms how many bathrooms how many kitchens. And it will spawn out and guarantee that each
house is unique. And then you can do an accelerated test where you know you can test say 100 houses
and do a year or worth of driving around the houses in an hour. Right. And do training on that
or also do testing on it to make sure if they get stuck in a corner or something like that.
In the case of generating the houses do you is there an off-the-shelf kind of house generation
like you want some kind of parametric thing like you describe number of bedrooms number bathrooms
floor types you know and then you populate it with objects does that that framework for generating
house structures virtually does that exist or did you have to build that. Yeah we had to build it
it's it's part of it's a procedural generation system that is built into a world forage.
And so you can go out and generate generate different houses it's you can say a one bedroom
or studio or a three bedroom two bath or whatever you want and then you can pick from
assortments of furniture and you can pick for assortments of items to be on the floor and you can
pick different wall coverings you can pick different floor coverings and then the the
procedural generation is smart enough to do things like not put a table in front of a doorway
for example and things like that but it'll you know populate bedrooms it'll populate
living rooms dining rooms all of those kinds of things for you and then you have these synthetic
houses that you then drop your robotic model into and then you run your synthetic training in it.
It's quite effective I mean when the first thing the astro did when when I got it home was to
map my house. What sensors does the astro have on it? Is it just cameras or does it have some other
sensor? It's got a whole suite of sensors it's got an infrared sensor system an acoustic
sensor system and cameras on it so it's basically so that way you can see in the dark with the
infrared it also has a camera on a a I don't know how you describe it basically a pole that can
be raised up so if if it needs to look over something that it can't see over it can raise the
camera up and look around and put down again or you have a mobile app where you can you know tell
it to go different places and it'll go different places in the house or if you're your remote you
can say you know go look see if I left the stove on or go look you know it says it heard something
go check it out that sounds like glass break and go check it out and then you can point where
you want it to go and it'll drive there and you can watch it on your phone it's it's pretty neat
yeah that uh that I just left or you know I'm in another you know on a trip and oh man that I
leave the stove on that that one resonated maybe to go a little bit further into the the astro
uh direction um is it running it's like a local slam type of model for mapping the house or
what's the relationship between the the robot and the the cloud for that yeah so it runs both it
uh so the first thing it does when you when you uh after you pair it to your phone and connected
to your network uh the first thing it will do is it will start mapping the house and so we'll go
through a really interesting pattern where it will start at its docking station and go to the first
room return to its docking station go to the second room return to its and it'll just keep doing
that until it maps out the entire house um and then it'll ask you to go on a tour and so it'll
follow you around because well actually before that it does a facial recognition system so it can
recognize you so you go through and do your face from different angles uh and your voice so it
does voice and face recognition for you and now it can follow you and then it asks to go on a tour
of the house so then it goes to each room and then you say this is the living room or this is the
kitchen or this is the dining room or in this case this is the office and then um uh you can then
tell it to go to those rooms and it'll navigate there on its own even you know with my dogs running
around other things like that it'll recognize staircases to not accidentally drive down a staircase
and things like that uh it's got a uh a vertical sensor as well to make sure it can't do doesn't
drive downstairs um and um you know the wheels are quite large so it can go over carpets and floor
carpets and things like that that pretty easily it weighs about 27 pounds um and then it's got a
battery last all day it can go find its charging station and so I'm in charge itself
um too as well and it can you know you can tell it to go places it's got actually a drink carrier
in it so you can have it like bring drinks to people from the kitchen and stuff like that but
but I'm betting no grasping plan for opening the fridge and getting the drink
no not yet yeah well you know I think the you know these things you have to kind of take them in
stages so um I was actually amazingly impressed with how good a job the team did in making sure
that it could figure out how to navigate on its own and and even with obstacles moving around
in front of it the dogs moving around in front of it it would still uh it wouldn't freeze it would
uh it would and not hit things right that was another thing but you can actually see it doing
it's uh the same kind of thing as a grasping plan you can see it do it's it's plan uh to navigate
around something you can watch it stop for a second and almost see this software running as it
says okay how am I going to navigate this um and uh and then that gets stored in the cloud there's
a map that it produces of your house as you can look at on the on your phone um and uh and and so it's
constantly going back and forth and when you talk to it of course it selects and poly interface uh
lambda lexin poly interface that goes back to the cloud to to do like when when I I mentioned
the robotic company it looked it up for me as you heard but I thought it'd be interesting to have
it here with us in the interview I've been having it it's been in here all day with me for other
things too so going back to the warehouse and the defect detection example yeah I think of this
you know running into the kinds of problems that you ran into you and the customer ran into I
think of there being a uh series of steps that you might take to try to overcome that you know
on one end is maybe the synthetic data um maybe somewhere in the middle is more of a traditional
data augmentation uh I'm curious did you you know to what extent did you try that what kind of
results you you saw with that um it is as a precursor to going to full synthetic data to continue
to augment you you really just add still adding more images right I mean that that's really
what you're kind of adding more images of the stuff that you of the the fat tail as opposed to
the long tail yeah yeah yeah or another one of our customers basically has a robot looking at
conveyor belts and picking off bad chicken nuggets so we had to generate a bunch of
bad chicken nuggets and good chicken nuggets with synthetics right or another examples another
customer that's uh looking for a distressed uh parts that are being manufactured in titanium for
airplanes and the the way they had done that try to do that in the past is they would take
these thirty thousand dollar parts and beat them up with a hammer and then take pictures of them
right um and now what they do is they go into the CAD model and beat it up virtually and they
can do it a lot of different ways uh and then generate those synthetically and then train on
those synthetically so I think um I think it's going to be quite a boom for accelerating model
training specifically for defect detection and for object recognition and grasp plans um and then
I think uh world forage uh as it continues to accelerate and have more options in there
is going to be a huge accelerator for anyone who wants to build something that has an
navigate around a house and then in the future we'll do warehouses we did a hospital setting
so the ability to build hospitals offices warehouses uh and then uh extending it to outside spaces
as well to pursue or generate outside spaces intersections and other things like that so
you talked a little bit about this just now they the in the case of the titanium
part manufacturer they created the defects via CAD what um in the case of the um the other part
manufacturer creating those deep like how do you characterize a defect and like what does that
process look like in the in the general case yeah so what they do imagine like in a a sand mold
for a uh an engine block um that uh you could take each cylinder and show gaps in the cylinders
or anything that that causes the cylinder not look round before it goes into the machine process
or any you know they they basically go through and create these defects manually which they
used to have to do very so you just never get enough defects usually uh fortunately uh to train a
model on and that that's been the challenge and so but it sounds like that it sounds like there's
some subject matter expert there you know maybe an engineer or something that kind of understands
the generation process behind defects and can produce them via is it always via CAD or is it
well um you can also use things like Maya and Blender you know the the other 3D tools uh so
so we offer it as kind of a complete service so if you have an engineer slash artist that can
generate the defects for you you can do that and then upload the models and then generate
synthetics a few clicks of a button but we also offer sort of these artists and engineers that
are available on demand if you like to do it for you so if you don't have the the artist who knows
how to do the defect track creations uh you sit with one of your engineers and the artist will
sit say well it looks like this it looks like that and they'll have an interactive session
of creating those and is that artist generating or the engineers in the the case the the user
uh is doing it are they generating uh kind of static examples of defects or are they
generating some parameterized thing that's a defect generator um that's part of this the process
yeah it's both it's both so so you can generate them uh procedurally after you've defined them
uh or you can generate them you know manually if you like saying here's the the different
different defects I want to have on my on my uh uh you know you can imagine you could create a dent
and move it around randomly right under different surfaces but you need you know the algorithms
to do that properly um and then after you've generated those models then then the uh it goes into
synthetics and it generates you know images at all different lighting that match your sensors
uh from all different angles um and then and then that that's how it goes it just starts mass
generating of the images so that that way you get this sort of mass generation and then then then
you have now a library of 30,000 or 100,000 images that you then can feed into your training model
yeah and and of course that's that's combined combined with real images as well so you you know you've
got your like in that engine block example they had about 300 images that they'd made um to start
but now they have 30, 40,000 images plus the 300 so for those folks that want to do the procedural
generation of these defects what's the what what kind of tools are you using or what's the
interface uh for creating that yeah so they can log into uh sage maker synthetics um and uh
describe you know that there's there's there's choices right there on the console on what they're
interested in doing um and in that case uh we you know through the console you get connected to an
artist if you have the artists or the engineer and you just want to upload your 3D models and say
generate and then you set the parameterization just like with world fours you'd be saying I want
this furniture and and these services and this many bedrooms and this many houses and it will
procedurally generate it once you have your models in there uh and you describe your sensor
suites and you can choose from existing sensor suites because there's a lot of common sensor suites
or or specifically setting up your sensor suites then you automatically generate it from there um so
we you you have all this uh synthetically generated data defect data and the examples that we talked
about you combine that with your kind of regular uh images um and the sounds like the next step
and uh at least some of these cases is simulation can you talk a little bit about you know how and
where you see simulation coming into play yeah so so certainly if you take a look at the videos from
from re-invent you'll see the simulation of the packages falling on the conveyor belt as it's
moving right and so um and then the packages there's actually in that uh simulation there's
physics as well so the packages bounce and so you're defining your you know part part of the setting
up the simulation is you're defining the physics of it right um in in world forage when you're
generating worlds or in roblemaker you're defining uh with world forage the you know the the houses
that you want to do and the physics is earth physics right um we worked with uh JPL on the Mars
rover for example with roblemaker in that case we made a Mars landscape and you've got Mars physics
involved in that landscape with roblemaker and then we're currently working on the uh the lunar
outpost uh training the lunar rover to to drive all over the moon and so we've built uh moon
landscapes right we have another company that uses roblemaker uh actually it's a it's a a UV
sterilization robot that goes around hospitals UV sterilizing surfaces so it's got to not do that
when people are around because UV lights are dead for your eyes uh and it has to know how to
for example call an elevator going inside the elevator sterilize it go to the next floor sterilize
the floor you know call an elevator go to the next floor and so uh what we did there is we uh created
a simulation of the hospital all the floors in in different hospitals uh that were randomly generated
for for the robot to learn that um and then there's also a um a space simulator uh for simulating um
space vehicles right so you know zero g maneuvers with thrusters and those kinds of things uh
so you can simulate say for example a docking procedure of those kinds of things and so all of those
are available as different models inside roblemaker or there's a um uh a drone simulation system
so to train drones to fly between buildings and things like that and land the specific places or
there's a uh an eight-ass simulator um to use some of the core eight-ass models where you can take
it a bit further and and and generates synthetic simulations for eight-ass training for autonomous
vehicles and so it's a pretty broad range and and what we're working with worldforges and
and roblemakers just make it simpler and simpler and simpler to do that right so that you can
you don't have to be uh an ML expert to run a simulation and and train your ML models right uh or
you don't have to be an HPC high performance computing expert to do that right in a lot of cases
today you need to be an HPC expert and understand how to set up clusters and and and you know how
how to load batches of your simulation out and spawn them out and manage them and all those other
things our goal would be that uh you know uh the ML experts could concentrate on the ML model on the
compute right and and so it's getting being able to import in a 3D map of a city uh synthetically
generate uh uh say 10,000 intersections with with items you know people walking and cars going um
and then simulate you know driving through that city uh and simulate going through all those
intersections with a few clicks of a button it would be our goal and then um you can use that
to train your models synthetically and you can use it to test your models and test your your
algorithms out in these virtual worlds and then when you're happy with them then you can use our
over-their update push it out to the vehicles or the robots or the drones or whatever it
where whatever is your you're working with are the simulations are you using uh or interfacing with
off-the-shelf simulators it's some one might already use for robotics or is it kind of
custom built stuff that's part of world forged in these other platforms yeah so so it's a little
both um so for example um in uh robo maker you can choose unity or unreal or gazebo you know as
you're as your engines right um and pretty soon oh 3de as well um and then uh for the robotic arms
you could choose a Drake simulator for example and then you know there's a number of really good
you know autonomous vehicle simulators that are already out there that you can choose from as well um
so so uh you can choose your your rendering engine you can choose your simulator you can choose
your physics engine um and then you can then you you choose your your environments like generate
the houses or generate the streets or generate whatever is your you're interested in uh and then
you can say things like well I want to do you know uh a hundred houses and I want to do a year's
worth of training and then what it'll do is figure all all those parameters that it was just spawn
out that amount of compute necessary to do it right and then and then if you have I have the robot
for example get stuck in a corner uh you'll get a cloud watch alarm that it's stuck in a corner
and then you can go physically watch that one simulation and see where your lines of code are
are going through and what's wrong with your model for example uh and then you can make changes
and then just do it again and do it again until you're happy with it so it gives you you know
the a lot of flexibility to do that all automatically whereas we're we're sure you know right now
people would actually build physical environments and they'd physically load stuff out on their
their robot in the past and they'd physically um uh you know uh uh run the simulations and then
they'd see the robot get stuck and then they you know so it would be clock time that they would do
right and so clock time takes forever to do this you know you just can't you know if you look at
like one of our customers or Rora uh who's doing an autonomous systems um eight-ass type applications
you know they they drive tens of millions of miles every day in the simulation right and you
just can't physically do that in the real world they're just you know you need you know
thousands of cars driving thousands of miles every day to do that um which is just not practical
or economic or anything so I think the only way we're going to get to the level of autonomy
using machine learning that we need is these synthetic simulated environments and then
it extends even further as you start people start looking at business simulations and machine learning
related to that uh so training simulations of logistics systems so that you can train the models
to adapt to changes in unpredictable changes in your logistics system uh you can imagine that
it's also you know the factory digital twins like with our twin maker product where you can
today you can simulate uh individual components and then you can apply machine learning to that
like look out for equipment to look for anomalies um or you can apply say Siemens or ANSI simulations
to each of those individual components where they have the the the local expertise or the manufacturer
simulation for the compressor or the motor or whatever it is in your factory uh in the future you'll
be able to to since we know the relationship between those and we have the real-time data coming
into twin maker uh you'll be able to simulate the entire factory process to begin to optimize it
uh and apply machine learning at a broad broader range uh than people do today I think and so
um it's an exciting time it's like even you see in in high performance computing uh more and more in
uh you know fluid dynamics calculations and things like that we're doing things like like
training models on outcomes for different three-dimensional surfaces um and then not having to
do the computational dynamics for it fluid dynamics for it uh we can skip that because the models
can start to predict the outcome as you change the surface without having to recalculate everything
and so that's going to be a transformation I think in combining high performance computing and
machine learning and and these simulations I think it's it all begins to build on itself as we
virtualize the world whether it be sage maker synthetics or or world forage or robo maker or or
twin maker and all these ml models integrated with them um that's how things are going to continue
to advance in my opinion all this 3d processing that's necessary to do that physics that are
necessary to do that oh bill thanks so much for taking the time to share a bit about uh your
recent news and what you've been up to uh very very fascinating stuff yeah well thank you thanks for
having me
