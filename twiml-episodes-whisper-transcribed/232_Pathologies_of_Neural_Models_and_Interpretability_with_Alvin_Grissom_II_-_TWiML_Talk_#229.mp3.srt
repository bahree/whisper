1
00:00:00,000 --> 00:00:16,320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

2
00:00:16,320 --> 00:00:21,480
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:21,480 --> 00:00:31,400
I'm your host Sam Charrington.

4
00:00:31,400 --> 00:00:36,480
While it nerfs this past December, I attended the second annual Black and AI workshop, which

5
00:00:36,480 --> 00:00:41,680
gathered participants from all over the world to showcase their research, share experiences

6
00:00:41,680 --> 00:00:43,560
and support one another.

7
00:00:43,560 --> 00:00:47,680
This week I'm excited to continue our Black and AI series with this interview with Alvin

8
00:00:47,680 --> 00:00:53,160
Grissom II, Assistant Professor of Computer Science at Ersinus College.

9
00:00:53,160 --> 00:00:57,600
Alvin's research is focused on computational linguistics, and we begin with a brief chat

10
00:00:57,600 --> 00:01:02,040
about some of his prior work on verb prediction using reinforcement learning.

11
00:01:02,040 --> 00:01:06,440
We then dive into the paper he presented at the workshop, pathologies of neuro models

12
00:01:06,440 --> 00:01:09,360
make interpretations difficult.

13
00:01:09,360 --> 00:01:13,120
We talk through some of the quote unquote pathological behaviors he identified in the

14
00:01:13,120 --> 00:01:18,160
paper, how we can better understand the overconfidence of trained deep learning models in certain

15
00:01:18,160 --> 00:01:23,560
settings, and how we can improve model training with entropy regularization.

16
00:01:23,560 --> 00:01:28,080
We also touch on the parallel between his work and the work being done on adversarial examples

17
00:01:28,080 --> 00:01:30,480
by Ian Goodfellow and others.

18
00:01:30,480 --> 00:01:31,480
Enjoy.

19
00:01:31,480 --> 00:01:35,200
Alright everyone, I am on the line with Alvin Grissom II.

20
00:01:35,200 --> 00:01:39,840
Alvin is an Assistant Professor of Computer Science at Ersinus College.

21
00:01:39,840 --> 00:01:43,520
Welcome to this week in machine learning and AI.

22
00:01:43,520 --> 00:01:45,000
Absolutely.

23
00:01:45,000 --> 00:01:50,600
It was really great meeting you at the recent NERPS conference and in particular the Black

24
00:01:50,600 --> 00:01:56,320
and AI workshop where you presented the paper that we'll be talking about today pathologies

25
00:01:56,320 --> 00:01:59,800
of neuro models makes interpretations difficult.

26
00:01:59,800 --> 00:02:04,240
But before we jump into that, I would love to hear a little bit about your background

27
00:02:04,240 --> 00:02:09,320
and how you ended up doing research at the intersection of computational linguistics

28
00:02:09,320 --> 00:02:10,320
and machine learning.

29
00:02:10,320 --> 00:02:11,320
Sure.

30
00:02:11,320 --> 00:02:18,960
So I have a kind of winding path, I mean, so as an undergraduate I was actually very interested

31
00:02:18,960 --> 00:02:26,640
in AI and this was before it sort of blew up into what it has recently become.

32
00:02:26,640 --> 00:02:31,040
And I was always interested in language as well, just in general even before I was in

33
00:02:31,040 --> 00:02:33,040
college.

34
00:02:33,040 --> 00:02:36,960
And actually not a lot of people know this about me, but I considered becoming an English

35
00:02:36,960 --> 00:02:39,000
teacher when I was in high school.

36
00:02:39,000 --> 00:02:40,000
Oh, wow.

37
00:02:40,000 --> 00:02:41,000
Yeah.

38
00:02:41,000 --> 00:02:42,000
So that didn't happen.

39
00:02:42,000 --> 00:02:45,000
But yeah.

40
00:02:45,000 --> 00:02:53,840
So when I was a master student, I was getting a master's in computer science and I was doing

41
00:02:53,840 --> 00:02:56,600
language processing stuff.

42
00:02:56,600 --> 00:03:02,920
And my thesis was on actually some sentiment analysis, like qualitative and quantitative

43
00:03:02,920 --> 00:03:07,440
analysis of sentiment in Japanese.

44
00:03:07,440 --> 00:03:10,960
So I really enjoyed doing that, although I'm not sure I'd want to go back and read my

45
00:03:10,960 --> 00:03:14,120
master's thesis at this point.

46
00:03:14,120 --> 00:03:21,240
And after that, I was actually a PhD student for a year in linguistics.

47
00:03:21,240 --> 00:03:27,440
Decided that that wasn't really for me, at least at that time.

48
00:03:27,440 --> 00:03:32,720
And eventually I worked for a year and then I went back into sort of the computational

49
00:03:32,720 --> 00:03:38,800
linguistics area and that's where I am now.

50
00:03:38,800 --> 00:03:41,560
So several many years later.

51
00:03:41,560 --> 00:03:42,560
Awesome.

52
00:03:42,560 --> 00:03:50,400
And I'm curious about the decision, was that a decision that linguistics wasn't for you,

53
00:03:50,400 --> 00:03:54,280
but computational linguistics was or was it just the timing around the PhD?

54
00:03:54,280 --> 00:03:56,280
Well, it wasn't the timing.

55
00:03:56,280 --> 00:04:05,000
I think there were a lot of things going on, I was, I'm not sure how deeply I want to

56
00:04:05,000 --> 00:04:13,800
go down this rabbit hole, but there are some contentious issues in linguistics and I didn't

57
00:04:13,800 --> 00:04:15,560
realize this at the time.

58
00:04:15,560 --> 00:04:22,440
I mean, I had some sense of it, but I mean, there are a lot of things going on, but I still

59
00:04:22,440 --> 00:04:27,640
am very interested in linguistics, but I wanted to get kind of a broader view.

60
00:04:27,640 --> 00:04:34,280
And I was, it was more satisfying or at least, maybe satisfying isn't the word, but more,

61
00:04:34,280 --> 00:04:35,280
it was easier.

62
00:04:35,280 --> 00:04:42,120
I felt better about the intellectual claims I was making and I could make and computational

63
00:04:42,120 --> 00:04:46,480
linguistics than I did and some of the linguistic stuff I was interested in.

64
00:04:46,480 --> 00:04:50,480
And I'm, I say this with some trepidation because I don't want it to be misinterpreted

65
00:04:50,480 --> 00:04:56,040
as saying linguistics isn't like, isn't real or I'm not saying that, I'm saying that

66
00:04:56,040 --> 00:05:01,400
for me personally, and maybe it's just because of my background, but there are some other

67
00:05:01,400 --> 00:05:08,840
epistemological reasons I found the computational approach to be more to my liking as a F for

68
00:05:08,840 --> 00:05:09,840
a PhD.

69
00:05:09,840 --> 00:05:13,800
I'm going to study something for five plus years, I want to be comfortable with it and

70
00:05:13,800 --> 00:05:16,160
I found that I personally wasn't.

71
00:05:16,160 --> 00:05:22,920
Yeah, linguistics is one of those fields that I've done some kind of cursory, cursory reading

72
00:05:22,920 --> 00:05:27,640
of and I just, I find it fascinating and so when you said that, I kind of got the sense

73
00:05:27,640 --> 00:05:31,720
that they was something there that was kind of, kind of interesting.

74
00:05:31,720 --> 00:05:33,520
So thank you for elaborating on that.

75
00:05:33,520 --> 00:05:34,520
Sure.

76
00:05:34,520 --> 00:05:37,200
I mean, actually, it's funny because a lot of the things that bothered me at the time

77
00:05:37,200 --> 00:05:44,960
don't bother me so much anymore, but I had to kind of get some distance from it, I think,

78
00:05:44,960 --> 00:05:50,440
because I was kind of in fight or flight mode for a whole year when dealing with a lot

79
00:05:50,440 --> 00:05:55,720
of that and then I got away from it and was able to more leisurely sort of think about

80
00:05:55,720 --> 00:06:03,680
the issues and now I'm not so perturbed anymore, but at the time I was not a happy person.

81
00:06:03,680 --> 00:06:10,280
So then your research more recently is focused on the computational side as we've discussed.

82
00:06:10,280 --> 00:06:12,840
Can you elaborate on the kinds of things you've been looking at?

83
00:06:12,840 --> 00:06:21,440
My PhD work which, so I finished my PhD in 2017, I've continued to extend that a little

84
00:06:21,440 --> 00:06:25,400
bit and so that's on simultaneous interpretation.

85
00:06:25,400 --> 00:06:29,160
So trying to make computers do simultaneous interpretation.

86
00:06:29,160 --> 00:06:35,000
So like at the UN or something, you know, you're someone speaking in one language and you

87
00:06:35,000 --> 00:06:39,040
want to translate that in real time without necessarily waiting for the whole sentence

88
00:06:39,040 --> 00:06:46,640
to be uttered and just because that problem is so easy, of course, and things are passing.

89
00:06:46,640 --> 00:06:51,120
You know, we wanted to do this for what's what are called SOV or subject object-verb

90
00:06:51,120 --> 00:06:54,240
languages, to subject-verb-object languages.

91
00:06:54,240 --> 00:06:58,480
So a language like Japanese, for example, the verb comes at the end, but in English it

92
00:06:58,480 --> 00:07:00,040
comes after the subject.

93
00:07:00,040 --> 00:07:05,000
So if you want to translate, you know, I to the store went, you have to wait until you

94
00:07:05,000 --> 00:07:10,000
see when at the end of the sentence before you can say I went to the store.

95
00:07:10,000 --> 00:07:12,000
So that's a problem and so we-

96
00:07:12,000 --> 00:07:13,760
It's a problem for humans too, right?

97
00:07:13,760 --> 00:07:14,920
It is a problem for humans.

98
00:07:14,920 --> 00:07:19,960
So one of the things we had to ask ourselves was how do humans do this and they do a lot

99
00:07:19,960 --> 00:07:20,960
of things.

100
00:07:20,960 --> 00:07:26,320
So they'll reorder sentences, they'll use more vague words sometimes, but the first approach

101
00:07:26,320 --> 00:07:31,040
we sort of looked at and which I'm still working on with some other people is predicting

102
00:07:31,040 --> 00:07:32,040
those verbs.

103
00:07:32,040 --> 00:07:36,880
So humans are, when they do this, it's actually very cognitively taxing and they can only

104
00:07:36,880 --> 00:07:39,040
do it for so long at one time.

105
00:07:39,040 --> 00:07:45,720
I've noticed that at events that the transcriptionists kind of work in teams and they'll swap off

106
00:07:45,720 --> 00:07:48,760
every half an hour or hour or something like that.

107
00:07:48,760 --> 00:07:53,800
And I've had people tell me it's because it gets really taxing for them.

108
00:07:53,800 --> 00:07:58,520
Yeah, it's really, I mean, if you think just think about what they're doing, right?

109
00:07:58,520 --> 00:08:04,760
They're processing two languages simultaneously, translating between them and predicting the

110
00:08:04,760 --> 00:08:07,360
future essentially.

111
00:08:07,360 --> 00:08:11,400
And you know, your brain can only take so much for that long.

112
00:08:11,400 --> 00:08:18,240
And so what I worked on, one of the things was trying to see how computers could do this.

113
00:08:18,240 --> 00:08:22,920
And so it's like, well, let's see if we can predict these verbs, which in terms, it can

114
00:08:22,920 --> 00:08:23,920
Japanese.

115
00:08:23,920 --> 00:08:26,960
Let's see if we can predict the final verb and then just put it in the sentence earlier.

116
00:08:26,960 --> 00:08:33,160
It turns out that's very difficult, but we've probably worked on that and we use what's

117
00:08:33,160 --> 00:08:37,440
called reinforcement learning to try to teach a system.

118
00:08:37,440 --> 00:08:39,680
We were actually the first people to do this.

119
00:08:39,680 --> 00:08:43,400
Some of the people have done it before since then with neural networks and other things,

120
00:08:43,400 --> 00:08:49,840
but let's see if we can learn under which situations we can trust these predictions.

121
00:08:49,840 --> 00:08:55,600
So you have like a prediction of a verb, you have like a translation and let's see when

122
00:08:55,600 --> 00:08:59,520
should we trust each and what should we do with that information.

123
00:08:59,520 --> 00:09:03,800
And so this sort of spawn this other project we're just taking on the life of its own,

124
00:09:03,800 --> 00:09:07,840
which is just verb prediction and predicting that kind of stuff incrementally, which

125
00:09:07,840 --> 00:09:14,560
just it turns out is a very interesting problem, both for cycle linguistics and for machine

126
00:09:14,560 --> 00:09:17,320
learning computer science.

127
00:09:17,320 --> 00:09:24,720
And so since then, I've worked on some other stuff, but the paper you brought up was the

128
00:09:24,720 --> 00:09:29,000
pathologies of neural models, make interpretations difficult.

129
00:09:29,000 --> 00:09:30,000
And so that looks at...

130
00:09:30,000 --> 00:09:35,640
Actually, if you don't mind me hitting Paul, I know it's going pretty fast.

131
00:09:35,640 --> 00:09:44,720
No, you touched on something that is potentially interesting, this use of reinforcement learning

132
00:09:44,720 --> 00:09:48,160
as part of this verb prediction problem.

133
00:09:48,160 --> 00:09:53,320
So it sounds like you made a distinction between reinforcement learning and folks that came

134
00:09:53,320 --> 00:09:56,120
along after and did deep reinforcement learning.

135
00:09:56,120 --> 00:09:58,320
You are using more traditional RL methods.

136
00:09:58,320 --> 00:10:01,080
Right, we were using imitation learning.

137
00:10:01,080 --> 00:10:02,080
Yeah.

138
00:10:02,080 --> 00:10:03,080
Okay.

139
00:10:03,080 --> 00:10:08,400
And so can you walk us through kind of how you formulated the problem there and how you

140
00:10:08,400 --> 00:10:12,400
applied RL and imitation learning to that particular problem?

141
00:10:12,400 --> 00:10:13,400
Sure.

142
00:10:13,400 --> 00:10:14,400
Yeah.

143
00:10:14,400 --> 00:10:17,440
So you can think of sentence.

144
00:10:17,440 --> 00:10:19,840
So we don't speak sentences all at once.

145
00:10:19,840 --> 00:10:25,280
We speak them, you could say a word at a time, although defining word in some languages

146
00:10:25,280 --> 00:10:29,440
can be difficult, but we speak them incrementally.

147
00:10:29,440 --> 00:10:33,040
And you can think of that as like a time series of steps.

148
00:10:33,040 --> 00:10:40,160
So if you kind of discretize the sentence by words, say, then every time, let's all imagine

149
00:10:40,160 --> 00:10:42,280
you are a translator interpreter.

150
00:10:42,280 --> 00:10:46,520
So you can imagine that every time you hear a word, you have a decision to make.

151
00:10:46,520 --> 00:10:47,760
Okay.

152
00:10:47,760 --> 00:10:49,400
And so what should that decision be?

153
00:10:49,400 --> 00:10:55,200
So you might want to do nothing because you might not be certain and not you might not

154
00:10:55,200 --> 00:11:00,520
know enough like to make a good to say anything or to make a translation and incremental translation.

155
00:11:00,520 --> 00:11:06,080
So you might just wait or you might make the best translation you can with what you have.

156
00:11:06,080 --> 00:11:11,320
You might make the best translation you can with what you have while making some kind of

157
00:11:11,320 --> 00:11:12,440
prediction.

158
00:11:12,440 --> 00:11:15,960
So maybe you have a good idea of what the next word is or what the verb is or something

159
00:11:15,960 --> 00:11:17,720
like that.

160
00:11:17,720 --> 00:11:23,880
And so in a reinforcement learning framework, you typically have some kind of time series

161
00:11:23,880 --> 00:11:29,320
of steps and every time you take a step, you choose an action.

162
00:11:29,320 --> 00:11:33,760
So when in this in our case, the actions were things like wait, which means don't try

163
00:11:33,760 --> 00:11:39,320
to translate anything at that point, commit, which just means that you make the best translation

164
00:11:39,320 --> 00:11:46,040
of that fragment you can at that point, verb, which does the same with a verb prediction.

165
00:11:46,040 --> 00:11:49,160
And we had another one for the next word that does the same thing.

166
00:11:49,160 --> 00:11:54,360
And so that in that formulation, every time you get a word, you just choose an action.

167
00:11:54,360 --> 00:11:58,840
And if you have some examples of good actions, this is where the imitation learning comes

168
00:11:58,840 --> 00:12:07,400
in, then you just try to imitate the good examples and try to generalize from those good

169
00:12:07,400 --> 00:12:12,920
examples when you can take what action.

170
00:12:12,920 --> 00:12:19,560
How are you training the model, were you training it on kind of existing translation, you

171
00:12:19,560 --> 00:12:24,200
know, some corpus of translated phrases or documents or something like that?

172
00:12:24,200 --> 00:12:25,200
Right.

173
00:12:25,200 --> 00:12:31,600
We were using a standard web corpus of parallel data.

174
00:12:31,600 --> 00:12:38,400
So it's not ideal because it's not actual simultaneously translated data, which there's

175
00:12:38,400 --> 00:12:45,920
another paper that shows that these look very different.

176
00:12:45,920 --> 00:12:50,000
So that's a weakness in it, but it's also just because that data doesn't really exist

177
00:12:50,000 --> 00:12:51,000
in large measure.

178
00:12:51,000 --> 00:12:54,640
There's some, but it's not enough to really train a model in the traditional way.

179
00:12:54,640 --> 00:12:57,520
So yeah, we were using basic parallel data.

180
00:12:57,520 --> 00:12:58,520
Okay.

181
00:12:58,520 --> 00:13:06,280
It strikes me that actual simultaneously translated data would be, you know, there's a noise

182
00:13:06,280 --> 00:13:12,120
aspect to it because the translators are making these mistakes that, you know, they might

183
00:13:12,120 --> 00:13:15,720
not make if they have full access to full information.

184
00:13:15,720 --> 00:13:16,720
Is that exact?

185
00:13:16,720 --> 00:13:17,720
Yeah.

186
00:13:17,720 --> 00:13:18,720
That's a problem.

187
00:13:18,720 --> 00:13:23,280
I don't know if it's a problem or not, but it's definitely an aspect of the data.

188
00:13:23,280 --> 00:13:28,760
So there's actually some interesting data where they have three translators.

189
00:13:28,760 --> 00:13:33,960
I think it's Japanese, English, three translators of different, I guess you could say experience

190
00:13:33,960 --> 00:13:41,720
levels or ranks, and so, and they have them all translating the same stuff, it's simultaneously

191
00:13:41,720 --> 00:13:46,000
and you can actually see like the different kinds of things that they do and strategies

192
00:13:46,000 --> 00:13:47,920
they use and mistakes they make and stuff like that.

193
00:13:47,920 --> 00:13:48,920
It's pretty interesting.

194
00:13:48,920 --> 00:13:49,920
Huh.

195
00:13:49,920 --> 00:13:50,920
Wow.

196
00:13:50,920 --> 00:13:51,920
Interesting.

197
00:13:51,920 --> 00:13:57,760
You were starting to introduce the paper that I saw you present the pathologies paper.

198
00:13:57,760 --> 00:14:02,000
Can you tell us a little bit about the background and motivation of that paper?

199
00:14:02,000 --> 00:14:07,880
Yeah, so there's been an uptick in interest, well obviously in deep learning in general.

200
00:14:07,880 --> 00:14:13,040
I mean, everybody knows that and who's going to be listening to this podcast, but one

201
00:14:13,040 --> 00:14:18,800
of the criticisms of deep models is that they're black boxes.

202
00:14:18,800 --> 00:14:22,760
So if you have like a logistic regression or something of some kind of linear model,

203
00:14:22,760 --> 00:14:27,720
you can look at the weights and even though that's not a perfect method of analyzing a model,

204
00:14:27,720 --> 00:14:33,040
you can get a sense of why the model is making the decisions it's making, right?

205
00:14:33,040 --> 00:14:37,320
But with a deep model, it's very difficult, right?

206
00:14:37,320 --> 00:14:43,040
And the deeper and more complex the architecture, the harder it gets to interpret what these

207
00:14:43,040 --> 00:14:45,840
models are doing and why.

208
00:14:45,840 --> 00:14:50,520
And so because we want to interpret these models, we thought, well, let's look at what

209
00:14:50,520 --> 00:14:51,520
they're doing.

210
00:14:51,520 --> 00:14:56,920
And so what we found is that they exhibit what we call these pathological behaviors.

211
00:14:56,920 --> 00:15:02,840
So they don't do what any reasonable person would expect.

212
00:15:02,840 --> 00:15:07,520
They would do when they're given something, but they're not used to seeing.

213
00:15:07,520 --> 00:15:13,320
So I think the example I used in the presentation was you can reduce, so for question answering,

214
00:15:13,320 --> 00:15:18,360
like you can reduce the question to just the word did, right?

215
00:15:18,360 --> 00:15:23,480
And not only, so it still gives the right answer, but the confidence in the right answer

216
00:15:23,480 --> 00:15:31,080
goes up to like 91% if you just ask did, right, which is a ridiculous, yeah.

217
00:15:31,080 --> 00:15:34,520
So, so I found that the example really interesting.

218
00:15:34,520 --> 00:15:37,480
Can you, let's walk through that in a little bit more detail.

219
00:15:37,480 --> 00:15:40,560
It was, well, let's actually, let's go back to the data set.

220
00:15:40,560 --> 00:15:42,520
The data set is question answering.

221
00:15:42,520 --> 00:15:43,520
Was it squad?

222
00:15:43,520 --> 00:15:44,520
It was squad.

223
00:15:44,520 --> 00:15:45,520
Yeah, for that one.

224
00:15:45,520 --> 00:15:46,520
Okay.

225
00:15:46,520 --> 00:15:52,000
And so tell us about what you were trying to do with a given example from the squad data

226
00:15:52,000 --> 00:15:53,000
set.

227
00:15:53,000 --> 00:16:01,640
Okay, so you're given some context, like some kind of short sequence of sentences with

228
00:16:01,640 --> 00:16:10,440
some information and you give it a question and you want the model to give you an answer

229
00:16:10,440 --> 00:16:14,360
to the question based on the context.

230
00:16:14,360 --> 00:16:22,120
And intuitively, you want the confidence, the final confidence of the model to reflect

231
00:16:22,120 --> 00:16:27,280
the value of the information and the question, right?

232
00:16:27,280 --> 00:16:37,440
So like if I ask you, so if I give you a paragraph to read and I ask you, you know, what

233
00:16:37,440 --> 00:16:39,240
or something, right?

234
00:16:39,240 --> 00:16:42,240
So you have no idea what I'm asking.

235
00:16:42,240 --> 00:16:49,640
You wouldn't say, yeah, I'm 91% confident that, you know, the answer is x, but we found out

236
00:16:49,640 --> 00:16:54,000
that these models actually do this a lot.

237
00:16:54,000 --> 00:17:00,000
And so we wanted to, you know, systematically show that this is, this is like a systemic

238
00:17:00,000 --> 00:17:03,480
problem and some of these models.

239
00:17:03,480 --> 00:17:04,480
Okay.

240
00:17:04,480 --> 00:17:08,120
And so how did you go about showing that?

241
00:17:08,120 --> 00:17:16,480
So we would incrementally remove words from, so for squad, for example, we would incrementally

242
00:17:16,480 --> 00:17:25,520
remove words and just observe, so without changing the answer, actually, and observe how

243
00:17:25,520 --> 00:17:32,240
the confidence shifted and how, and what we found is that, you know, it doesn't really

244
00:17:32,240 --> 00:17:37,760
comport with what humans, so we also did some human experiments to verify this, but it

245
00:17:37,760 --> 00:17:41,080
doesn't really comport with what a human would expect.

246
00:17:41,080 --> 00:17:51,360
And nor does the human performance reflect necessarily what the models do one of these circumstances.

247
00:17:51,360 --> 00:18:00,520
And so the, the words that you're removing are from the, the context passage or from

248
00:18:00,520 --> 00:18:06,880
the answer from the, from the question, rather, yes, right, right, right.

249
00:18:06,880 --> 00:18:10,680
So you've got, you've got these three pieces, then you've got the kind of the sequence of

250
00:18:10,680 --> 00:18:18,240
sentences that's the context, you've got the, the question and the answer.

251
00:18:18,240 --> 00:18:24,540
And so you're removing words from the question and you're trying to see the way that that

252
00:18:24,540 --> 00:18:32,040
changes the prediction of the, or the confidence level in predicting the answer, is that the

253
00:18:32,040 --> 00:18:34,040
right way to think about it?

254
00:18:34,040 --> 00:18:35,040
That's right.

255
00:18:35,040 --> 00:18:36,040
Yes.

256
00:18:36,040 --> 00:18:42,960
If I remember in this correctly, you, you would kind of sequence the words that you removed

257
00:18:42,960 --> 00:18:49,920
by, were you sequencing on the one that was most likely or had like the least information

258
00:18:49,920 --> 00:18:52,360
or the most information or something like that?

259
00:18:52,360 --> 00:18:53,360
Right.

260
00:18:53,360 --> 00:18:58,520
So we, we remove what we call the unimportant words.

261
00:18:58,520 --> 00:18:59,520
So right.

262
00:18:59,520 --> 00:19:03,400
So we want to remove the words without changing the answer.

263
00:19:03,400 --> 00:19:13,040
You were removing the words that minimize the changing confidence of the, the answer.

264
00:19:13,040 --> 00:19:18,480
And then I'm trying to remember the, like you had, you showed a sequence of the words

265
00:19:18,480 --> 00:19:20,560
that you removed.

266
00:19:20,560 --> 00:19:29,040
And I guess what was strange about it is that you, oftentimes you were removing words

267
00:19:29,040 --> 00:19:34,520
that, as a human, you would think had the most meaning in the question and you ended

268
00:19:34,520 --> 00:19:37,880
up with this, you know, just like a question word.

269
00:19:37,880 --> 00:19:38,880
Right.

270
00:19:38,880 --> 00:19:43,400
I'm trying to capture like the visual on the podcast, which is sometimes hard to do.

271
00:19:43,400 --> 00:19:48,840
But it was really interesting to see the way that that evolved on that slide.

272
00:19:48,840 --> 00:19:49,840
Yeah.

273
00:19:49,840 --> 00:19:50,840
So, yeah.

274
00:19:50,840 --> 00:19:53,280
So that, that was one of the sort of, sort of, have a lot of behaviors, right?

275
00:19:53,280 --> 00:19:58,000
So what, it doesn't, so nothing that we, I don't want to say nothing we did, but a lot

276
00:19:58,000 --> 00:20:03,360
of what we did just doesn't, you know, doesn't go with what we would expect, right?

277
00:20:03,360 --> 00:20:08,920
So what the model thinks is an important term might not necessarily be what a human would

278
00:20:08,920 --> 00:20:10,920
consider an important term.

279
00:20:10,920 --> 00:20:16,840
Did you end up in this paper taxonomizing these pathological behaviors?

280
00:20:16,840 --> 00:20:20,320
Are there, you know, do they, are there, how many of them are there?

281
00:20:20,320 --> 00:20:21,320
Are there names?

282
00:20:21,320 --> 00:20:26,320
Or is it more exploring kind of this one particular one that we've been discussing?

283
00:20:26,320 --> 00:20:33,000
It was a more exploratory, I mean, so we did have a section, so we talked about model

284
00:20:33,000 --> 00:20:37,640
over confidence, which is one problem, right?

285
00:20:37,640 --> 00:20:40,640
And we, I'm trying to think, do we give these names?

286
00:20:40,640 --> 00:20:43,440
I don't think, I don't think we named the pathologies.

287
00:20:43,440 --> 00:20:46,200
I think we just sort of described them.

288
00:20:46,200 --> 00:20:49,680
And so one of them is this overconfidence?

289
00:20:49,680 --> 00:20:52,760
What are some of the other ones that you tended to see?

290
00:20:52,760 --> 00:20:53,760
Right.

291
00:20:53,760 --> 00:20:59,720
So I think rather than, I think a different way of thinking about it is that, so we tried

292
00:20:59,720 --> 00:21:01,960
this on a number of different tasks.

293
00:21:01,960 --> 00:21:09,320
So the way we, so the method that we used was sort of this word removal, right?

294
00:21:09,320 --> 00:21:15,240
And we did this for squad, we did this for visual question answering, and we did this

295
00:21:15,240 --> 00:21:22,720
for SNLI, which looks for things like contradictions and things like that.

296
00:21:22,720 --> 00:21:29,640
And we noticed, so when we showed the same, sort of the same kinds of pathologies on all

297
00:21:29,640 --> 00:21:34,960
of these different tasks, when, by, by using NPV reduction.

298
00:21:34,960 --> 00:21:35,960
Okay.

299
00:21:35,960 --> 00:21:42,040
So you, kind of the same idea of successfully removing words and you ended up in the same

300
00:21:42,040 --> 00:21:46,680
kind of weird part where you're left with words that wouldn't be the most useful ones

301
00:21:46,680 --> 00:21:48,400
that you would expect.

302
00:21:48,400 --> 00:21:49,400
Right.

303
00:21:49,400 --> 00:21:50,400
Right.

304
00:21:50,400 --> 00:22:01,120
And so what, what did all this tell you about neural network models and trying to interpret

305
00:22:01,120 --> 00:22:02,120
them?

306
00:22:02,120 --> 00:22:05,360
Uh, that, it's, it's difficult.

307
00:22:05,360 --> 00:22:13,120
So I mean, you know, I mean, it's not necessarily surprising.

308
00:22:13,120 --> 00:22:17,720
So there's been some previous work that has shown similar things for, especially sort

309
00:22:17,720 --> 00:22:24,000
of images that there, there's this notion of like rubbish examples where you show what

310
00:22:24,000 --> 00:22:29,200
looks like random noise and, you know, the, the classifier, what gives you 90% confidence

311
00:22:29,200 --> 00:22:32,800
that it's, you know, a flower or something.

312
00:22:32,800 --> 00:22:37,920
And so this is, kind of, this is definitely related to that.

313
00:22:37,920 --> 00:22:44,880
And so we also, you know, used a, proposed method of trying to mitigate this using a kind

314
00:22:44,880 --> 00:22:53,560
of regularization, uh, on the, uh, by training on the reduced inputs, uh, but, um, yeah, I

315
00:22:53,560 --> 00:22:58,400
mean, I think we're not just trying to say that, you know, deep learning models are garbage

316
00:22:58,400 --> 00:22:59,400
or whatever.

317
00:22:59,400 --> 00:23:04,440
We're just, uh, trying to identify sort of the shortcomings and understanding them so

318
00:23:04,440 --> 00:23:06,560
that we can try to address that.

319
00:23:06,560 --> 00:23:07,560
Right.

320
00:23:07,560 --> 00:23:13,400
And did, uh, did the regularization approach that you tried, uh, I'm presuming that that

321
00:23:13,400 --> 00:23:14,880
worked to some extent?

322
00:23:14,880 --> 00:23:15,880
It did.

323
00:23:15,880 --> 00:23:22,760
Uh, so we were, so the nice thing about it is that we were able to, uh, use this to, uh,

324
00:23:22,760 --> 00:23:24,600
with almost no change in accuracy.

325
00:23:24,600 --> 00:23:30,840
So we are able to get more, are more reasonable distribution of confidences, uh, that looks

326
00:23:30,840 --> 00:23:35,360
more like, you know, what a human would, what, what expect, uh, without really hurting

327
00:23:35,360 --> 00:23:36,360
accuracy.

328
00:23:36,360 --> 00:23:37,360
Okay.

329
00:23:37,360 --> 00:23:43,120
Um, so, so even, um, you know, even though the model itself is still difficult to interpret,

330
00:23:43,120 --> 00:23:50,800
there is a way of, of training it, uh, that gives you, I guess you could say more interpretable

331
00:23:50,800 --> 00:23:53,800
confidences that, at the end of the prediction.

332
00:23:53,800 --> 00:23:56,960
Is it this specific approach to regularization?

333
00:23:56,960 --> 00:24:02,760
The one that you tried that you found to have the advantage of, you know, let's say correcting

334
00:24:02,760 --> 00:24:10,160
these models in this way, or is it regularization in general, you know, in trying to, to force

335
00:24:10,160 --> 00:24:14,720
the networks to generalize more that, you know, had some kind of general improvement.

336
00:24:14,720 --> 00:24:15,720
Right.

337
00:24:15,720 --> 00:24:19,960
So I don't, uh, know that we try and drop out or something like that.

338
00:24:19,960 --> 00:24:26,480
Uh, I guess I could, uh, ask the other people who are working on this, uh, but, uh, we just

339
00:24:26,480 --> 00:24:30,840
showed that this particular, uh, form of regularization, uh, helps.

340
00:24:30,840 --> 00:24:33,760
Um, so, um, it's possible.

341
00:24:33,760 --> 00:24:35,800
I mean, I, I doubt it would hurt.

342
00:24:35,800 --> 00:24:40,120
I'm, I don't know, um, but, uh, you know, to use, uh, some other kind of regularization,

343
00:24:40,120 --> 00:24:47,160
uh, if you want to call it, drive-out regularization, um, yeah, but, uh, yeah, in our paper, we just

344
00:24:47,160 --> 00:24:49,800
showed this particular method works.

345
00:24:49,800 --> 00:24:56,840
What have you done kind of in this direction since then, or, or where do you, you know,

346
00:24:56,840 --> 00:25:00,600
how would you see extending this, this kind of work?

347
00:25:00,600 --> 00:25:01,920
Um, yeah.

348
00:25:01,920 --> 00:25:08,280
So I personally haven't, uh, extended this, uh, since last month or whenever we post

349
00:25:08,280 --> 00:25:09,280
this.

350
00:25:09,280 --> 00:25:15,360
Maybe, maybe it was two months ago, um, but, uh, there is another, uh, paper, uh, uh, a workshop

351
00:25:15,360 --> 00:25:21,800
paper by an undergraduate student at the University of Maryland, uh, named Eric Wallace, um, that

352
00:25:21,800 --> 00:25:25,040
is, uh, attempting to do something similar.

353
00:25:25,040 --> 00:25:30,040
I think he's using K nearest neighbors, um, to, so he was also the second author on this

354
00:25:30,040 --> 00:25:36,880
paper, uh, to try to, uh, so I haven't read his paper, I have to confess, but I think he's

355
00:25:36,880 --> 00:25:41,720
trying to do that to, uh, do something similar to try to mitigate, uh, make these models

356
00:25:41,720 --> 00:25:42,800
more interpretable.

357
00:25:42,800 --> 00:25:49,120
Maybe tell me a little bit about how this particular paper fits into kind of your broader

358
00:25:49,120 --> 00:25:51,120
research agenda nowadays.

359
00:25:51,120 --> 00:25:52,120
Sure.

360
00:25:52,120 --> 00:25:55,360
Um, so going back to Verr projection a little bit.

361
00:25:55,360 --> 00:26:01,040
So in 2016, I published this paper on, uh, Verr prediction in Japanese and German, mostly

362
00:26:01,040 --> 00:26:08,640
Japanese, uh, and, uh, so, and that was using just the near models and, uh, so I also interested

363
00:26:08,640 --> 00:26:09,640
in psycho linguistics.

364
00:26:09,640 --> 00:26:16,520
So I was interested in sort of comparing the features that, uh, that, uh, that, uh, model

365
00:26:16,520 --> 00:26:21,280
was using to make its Verr predictions to what humans seem to be using to make the

366
00:26:21,280 --> 00:26:23,160
same predictions, right?

367
00:26:23,160 --> 00:26:29,120
So we always try to, I use the word we loosely, but, uh, some people try to make, you know,

368
00:26:29,120 --> 00:26:34,080
these kinds of analogies between machine learning models and humans, but, uh, without actually

369
00:26:34,080 --> 00:26:38,160
looking at what the model is doing, it's not clear that that is justified.

370
00:26:38,160 --> 00:26:43,560
So as we're moving to these, as we have moved to, you know, more deep learning methods,

371
00:26:43,560 --> 00:26:51,480
uh, I think if you want to make claims about, you know, a model, for example, informing,

372
00:26:51,480 --> 00:26:55,440
you know, something about what a human is doing, I think it's helpful to have some sense

373
00:26:55,440 --> 00:27:01,920
of what the model is actually doing, uh, internally, um, so on that Verr prediction products,

374
00:27:01,920 --> 00:27:08,320
for example, um, uh, we will, I looked, I just looked at the weights of the model and, uh,

375
00:27:08,320 --> 00:27:15,640
you know, and, and tried to, uh, look at, uh, how, you know, how adding certain, uh, features

376
00:27:15,640 --> 00:27:20,200
that were linguistically interesting affected the predictions, uh, and you can also do that

377
00:27:20,200 --> 00:27:24,720
with a deep model, um, you can also look at something like attention weights, those kinds

378
00:27:24,720 --> 00:27:31,640
of things. Um, so I, I guess what I'm pushing here is, you know, uh, accuracy on a data

379
00:27:31,640 --> 00:27:36,120
set is an important, uh, metric to look at, but it's not the only metric. It's not all

380
00:27:36,120 --> 00:27:42,160
we care about, uh, you know, for, and for doing scientific inquiry in parade, and we want

381
00:27:42,160 --> 00:27:47,880
to look at the model itself, we want to understand why the model is doing what it's doing to the

382
00:27:47,880 --> 00:27:52,480
best that we can, uh, which is not, again, not to say that accuracy is an important,

383
00:27:52,480 --> 00:27:57,680
but, uh, as someone who's interested also in linguistic questions, um, it's, uh, much

384
00:27:57,680 --> 00:28:02,560
more satisfying scientifically if we can peek inside the models and see what they're

385
00:28:02,560 --> 00:28:09,720
doing. And if we can't, to at least be able to have some sense of why we can't, if that

386
00:28:09,720 --> 00:28:10,720
makes sense.

387
00:28:10,720 --> 00:28:18,400
What's your sense of the kind of relationship between cycle linguistics and the kind

388
00:28:18,400 --> 00:28:23,480
of work that's happening on the machine learning and deep learning side? Are there, have

389
00:28:23,480 --> 00:28:28,280
there been any interesting kind of cross pollinations or cross results there?

390
00:28:28,280 --> 00:28:33,320
Uh, there have. And for quite some time, I think, uh, so I think there's so much potential

391
00:28:33,320 --> 00:28:39,360
that's only beginning to be, uh, the surface of which is only beginning to be scratched.

392
00:28:39,360 --> 00:28:45,400
Um, so for example, there's a whole, you know, area of, uh, neuroscience of language

393
00:28:45,400 --> 00:28:49,840
where we have, uh, these really interesting experiments where, you know, you give a person

394
00:28:49,840 --> 00:28:55,280
a stimulus of some sentences or words or something and you, you see, you put you, uh, you

395
00:28:55,280 --> 00:29:02,120
give them an EEG helmet or you take an FMRI and you see how their brain is, is, uh, responding

396
00:29:02,120 --> 00:29:07,880
to these stimuli. Um, so I think that's an, it's an incredible treasure trove of potential

397
00:29:07,880 --> 00:29:15,560
machine learning research. Um, and it's starting to be, uh, done. Uh, it's been done for a few

398
00:29:15,560 --> 00:29:23,160
years by some people. Um, so these, so for example, uh, if you wear an EEG helmet, uh, you

399
00:29:23,160 --> 00:29:29,000
know, your brain produces, uh, certain, I'll just, for simplicity, I'll call them brain

400
00:29:29,000 --> 00:29:36,040
waves, but it produces a graph that's called an ERP, uh, event-related potential. And, uh,

401
00:29:36,040 --> 00:29:39,880
you know, I think it's really a really interesting question. So what can you predict by looking

402
00:29:39,880 --> 00:29:47,800
at an ERP? Uh, can you, uh, predict, uh, other aspects of, uh, that person's, uh, linguistic

403
00:29:47,800 --> 00:29:53,080
performance based on looking at these, uh, these kinds of things? But even outside of, sort

404
00:29:53,080 --> 00:29:58,840
of neuroscience, which, um, requires a more specialized background, arguably, um, you

405
00:29:58,840 --> 00:30:03,480
know, so, uh, for my verbal prediction work, I looked at, uh, what are called case markers

406
00:30:03,480 --> 00:30:10,120
in Japanese? And it turns out that both, uh, classifier, the, uh, uh, the model that we built,

407
00:30:10,120 --> 00:30:15,800
and, uh, the humans, there's previous resources. It shows that humans also do this. Uh, we're

408
00:30:15,800 --> 00:30:21,560
using these case markers in Japanese tube, uh, predict verbs. And so that was in, in instance,

409
00:30:21,560 --> 00:30:26,760
where it looked like the model and the humans were using the same information in an interesting

410
00:30:26,760 --> 00:30:33,080
way. Um, and so I think there's a lot of potential there for, for, so for example, you might look

411
00:30:33,080 --> 00:30:37,640
at a model and, and think to test whether, uh, what the humans are doing is related to what the

412
00:30:37,640 --> 00:30:43,640
model is doing or vice versa. Um, so I think there's a lot of potential there. Awesome. Uh,

413
00:30:43,640 --> 00:30:49,800
besides the, the paper that we've been discussing here or that we discussed previously here,

414
00:30:49,800 --> 00:30:55,800
are there any interesting references that you would point someone to who's interested in

415
00:30:55,800 --> 00:31:02,520
exploring this area more deeply? Uh, sure. Uh, so the one that, I mean, there, there's a lot, um,

416
00:31:02,520 --> 00:31:06,440
I, I can't, I'm not going to be able to name them off the top of my head. But one that comes to

417
00:31:06,440 --> 00:31:15,800
mind is, uh, the, the paper on, uh, rubbish examples. Um, so if you just Google rubbish examples,

418
00:31:15,800 --> 00:31:21,400
I think it'll, it'll, uh, deep learning what rubbish examples I think it'll come up. Um, so I

419
00:31:21,400 --> 00:31:27,640
think that's, uh, probably, at least that's coming to mind my, my mind right now, the most

420
00:31:27,640 --> 00:31:32,680
related paper. And that was published before quite a bit before hours. And that's more related to

421
00:31:32,680 --> 00:31:39,880
visual, uh, task, I think than, uh, then linguistic task, but it's a similar idea and was, uh,

422
00:31:39,880 --> 00:31:45,000
certainly, uh, part of the inspiration for our work. Okay. Awesome. Awesome. Yeah. Well,

423
00:31:45,000 --> 00:31:49,480
Evan, thank you so much for taking the time to chat with us. Sure. Thank you very much for having me.

424
00:31:49,480 --> 00:31:59,240
All right, everyone, that's our show for today. For more information on Alvin or any of the topics

425
00:31:59,240 --> 00:32:06,520
covered in this show, visit twimmelai.com slash talk slash 229. For more information on a black

426
00:32:06,520 --> 00:32:14,920
and AI series, visit twimmelai.com slash black and AI 19. As always, thanks so much for listening

427
00:32:14,920 --> 00:32:21,880
and catch you next time.

