WEBVTT

00:00.000 --> 00:16.320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

00:16.320 --> 00:21.480
people, doing interesting things in machine learning and artificial intelligence.

00:21.480 --> 00:31.360
I'm your host Sam Charrington.

00:31.360 --> 00:37.920
In this episode I'm joined by Greg Mulholland, founder and CEO of Citrine Informatics, which

00:37.920 --> 00:42.960
is applying AI to the discovery and development of new materials.

00:42.960 --> 00:46.880
Greg and I start out with an exploration of some of the challenges of the status quo in

00:46.880 --> 00:52.680
material science and what's to be gained by introducing machine learning into this process.

00:52.680 --> 00:57.840
We discuss how limitations and materials manifest themselves and Greg shares a few examples

00:57.840 --> 01:02.800
from the company's work optimizing battery components and solar cells.

01:02.800 --> 01:07.560
We dig into the role and sources of data used in applying machine learning and materials

01:07.560 --> 01:13.200
and some of the unique challenges to collecting it, as well as discuss the pipeline and algorithms

01:13.200 --> 01:16.120
Citrine uses to deliver it service.

01:16.120 --> 01:21.360
This was a fun conversation that spends physics, chemistry and of course machine learning,

01:21.360 --> 01:22.920
and I hope you enjoy it.

01:22.920 --> 01:24.960
All right, let's do it.

01:24.960 --> 01:28.160
All right everyone, I am on the line with Greg Mulholland.

01:28.160 --> 01:32.840
Greg is founder and CEO of a company called Citrine Informatics.

01:32.840 --> 01:38.080
Greg and Citrine are focused on the application of AI to materials and chemistry and I'm really

01:38.080 --> 01:40.600
looking forward to digging into that in this show.

01:40.600 --> 01:43.000
Greg, welcome to this weekend machine learning and AI.

01:43.000 --> 01:45.280
Thanks Sam, really glad to be here.

01:45.280 --> 01:51.360
So tell us a little bit about your background, how do you get started working in this intersection

01:51.360 --> 01:54.800
of materials and machine learning and AI?

01:54.800 --> 02:00.880
Well, when I started my college career, I was really focused actually, I was focused

02:00.880 --> 02:02.280
on electrical engineering.

02:02.280 --> 02:07.000
So I was a computer engineer, I was doing processor design, but I actually became fascinated

02:07.000 --> 02:12.120
by the fact that everything I kept running into in the electronics world and frankly everything

02:12.120 --> 02:17.240
I kept running into among my mechanical engineering friends and product design friends was that

02:17.240 --> 02:23.040
the materials that they had available to them were really the limiting factor in a lot

02:23.040 --> 02:24.920
of the products that they that they loved.

02:24.920 --> 02:29.680
And so I actually, because I had this programming background, because I had this computer engineering

02:29.680 --> 02:36.080
background, I started thinking a lot about how can we use these machine learning techniques

02:36.080 --> 02:41.000
and use these advanced analytics techniques to accelerate this really important field.

02:41.000 --> 02:46.760
And I actually ended up going into the materials fields purely, I worked in a small semi conductor

02:46.760 --> 02:51.360
company where we made materials for LEDs and power electronics.

02:51.360 --> 02:56.680
And I saw exactly what I expected to, which was that we were really good at generating

02:56.680 --> 03:01.280
huge volumes of data and frankly, pretty poor at using them effectively to make better

03:01.280 --> 03:02.280
decisions.

03:02.280 --> 03:08.440
And so when I met my, the person who became my co-founder, Bryce, he had a background

03:08.440 --> 03:14.640
in machine learning for materials and we kind of had this magical overlap of his expertise

03:14.640 --> 03:17.640
in analyzing data, my expertise in generating it.

03:17.640 --> 03:22.600
And it turns out there's a lot of cool things you can do in the materials industry by combining

03:22.600 --> 03:24.080
those two things.

03:24.080 --> 03:30.120
You mentioned running into some limitations of materials early in your career.

03:30.120 --> 03:37.640
What are some of those limitations and what are some of the ways that issues around materials

03:37.640 --> 03:41.720
manifest both for us as consumers and for businesses?

03:41.720 --> 03:46.600
Yeah, so for me, the first place I started was transistors.

03:46.600 --> 03:53.880
I started looking at what is limiting our processor speed, what is limiting our ability to kind

03:53.880 --> 03:58.840
of have these more advanced processors and technologies around us.

03:58.840 --> 04:04.360
But as I looked into it more and more, I realized the fuel efficiency of every vehicle, the

04:04.360 --> 04:09.960
fuel efficiency of a plane, the batteries that power our electronics, the medical devices

04:09.960 --> 04:15.560
that we put inside our bodies, they're all limited by our ability to make them out of

04:15.560 --> 04:17.880
things that do the job better than anything else.

04:17.880 --> 04:24.880
And so for me, I look around the world and see a world filled with materials being used

04:24.880 --> 04:29.600
for things in the shape of products, not a world of products that stand on their own.

04:29.600 --> 04:35.280
Those really are the enabling technology of most things that we love today.

04:35.280 --> 04:42.240
So maybe give us some examples of materials that you've had an opportunity to work with

04:42.240 --> 04:44.920
and with Citrine.

04:44.920 --> 04:49.600
Yeah, so we actually started in a bunch of different energy materials.

04:49.600 --> 04:53.640
And so one of my passions has always been energy.

04:53.640 --> 04:59.400
And using things, when you take something like a battery, for example, a battery is simply

04:59.400 --> 05:05.360
several layers of material stacked in an appropriate way to create a highly engineered structure

05:05.360 --> 05:06.360
of store energy.

05:06.360 --> 05:10.520
And so one thing that Citrine has worked on quite a lot actually is figuring out how

05:10.520 --> 05:16.000
to optimize the anode, the cathode, the electrolyte, and other internal components of batteries

05:16.000 --> 05:20.080
to make them last longer to have higher voltage to be safer.

05:20.080 --> 05:25.240
And that's been a real area for us where we've had some pretty exciting results because

05:25.240 --> 05:31.280
the reality is none of us are happy with how our batteries perform.

05:31.280 --> 05:36.520
And so being able to push those forward is an exciting thing to be able to do.

05:36.520 --> 05:42.640
But we also have had an opportunity to discover new lightweight aerospace alloys.

05:42.640 --> 05:50.000
So 3D print alloys that make a plane lighter or a gen engine perform more efficiently.

05:50.000 --> 05:55.320
Because at the end of the day, the efficiency of these aircraft and the cost of running

05:55.320 --> 06:00.320
them and the pollution they put out is limited very much by the weight and the performance

06:00.320 --> 06:02.480
of the materials in them.

06:02.480 --> 06:05.320
So we really worked very, very broadly.

06:05.320 --> 06:13.800
And we've seen the power of AI across everything from new polymers and plastics, to new glasses,

06:13.800 --> 06:20.000
to new textile materials, to new paints and coatings all the way to batteries and processors.

06:20.000 --> 06:23.400
So it's really a very broad industry with huge impact.

06:23.400 --> 06:29.720
I remember reading not too long ago, maybe six months ago or so, some articles about

06:29.720 --> 06:39.520
some work that I think was Autodesk was doing in printing, 3D printing, it was like basically

06:39.520 --> 06:43.760
the walls that separate the different cabins in an aircraft.

06:43.760 --> 06:45.240
I forget the word for it.

06:45.240 --> 06:46.240
Oh, the bulkhead.

06:46.240 --> 06:47.240
The bulkhead.

06:47.240 --> 06:48.640
Yeah, exactly.

06:48.640 --> 06:49.640
You mentioned 3D printing.

06:49.640 --> 06:54.080
Now, are you talking about the actual material that is used and the 3D printing is what

06:54.080 --> 06:57.760
you worked on or something like they did with the generative design?

06:57.760 --> 06:58.760
Yeah.

06:58.760 --> 07:01.240
So I'm talking about the actual materials.

07:01.240 --> 07:07.960
Because in the 3D printing world, the tradition has been and this makes perfect sense that you

07:07.960 --> 07:13.880
have this list of aluminum alloys or steels or various metals or plastics that you have

07:13.880 --> 07:19.240
used in a non-3D printing way, we have made things without using 3D printing for a long

07:19.240 --> 07:20.240
time.

07:20.240 --> 07:24.240
And the idea is, hey, why don't we just make a powder out of them, put them in a 3D printer,

07:24.240 --> 07:28.280
and then we 3D print the part out of that material.

07:28.280 --> 07:33.080
And it turns out that the way a 3D printer works is actually pretty different from a normal

07:33.080 --> 07:34.320
manufacturing technique.

07:34.320 --> 07:39.560
And so when you put those materials in, you get something very different out than you

07:39.560 --> 07:41.960
would using a traditional manufacturing technique.

07:41.960 --> 07:47.840
So we actually work with companies to help identify how do we modify these materials to actually

07:47.840 --> 07:53.640
create a more performant, better material that's optimized for 3D printing rather than

07:53.640 --> 07:58.560
just trying to take what we've used for 50 or 100 years and repurpose it into this totally

07:58.560 --> 08:00.280
new manufacturing mode.

08:00.280 --> 08:06.160
So you're applying AI machine learning to these types of problems?

08:06.160 --> 08:12.840
So materials presents a pretty unique set of challenges for an AI system.

08:12.840 --> 08:20.000
And the reason for that is the demands on materials are different than most other sort

08:20.000 --> 08:22.920
of machine learning applications.

08:22.920 --> 08:29.640
For example, if you take aluminum alloys, which is a very, almost every plane has a huge

08:29.640 --> 08:34.120
number of different aluminum alloys in it because it's nice and light and it's very strong,

08:34.120 --> 08:40.240
there's a big problem because when you talk most of the folks on your show have access

08:40.240 --> 08:45.640
to many millions or tens of millions or even billions of data points about the things

08:45.640 --> 08:46.920
they're trying to target.

08:46.920 --> 08:52.840
When we talk about aluminum alloys, we're talking about maybe 5 to 10,000 data points

08:52.840 --> 08:55.200
that have ever been rigorously measured.

08:55.200 --> 08:59.280
And if you're talking about what any one person has, it can be just a few hundred or a few

08:59.280 --> 09:00.280
thousand.

09:00.280 --> 09:05.160
And so we're not in this world in materials and chemistry where we can sort of blindly

09:05.160 --> 09:11.520
throw deep learning at a huge data set and then refine it over time and get some really

09:11.520 --> 09:13.360
interesting good results.

09:13.360 --> 09:18.320
In materials and chemistry, we suffer from a small data problem.

09:18.320 --> 09:23.400
And by the way, every additional data point can cost millions of dollars.

09:23.400 --> 09:26.640
And so it's not even cheap to generate new data.

09:26.640 --> 09:28.680
It's actually a really big challenge.

09:28.680 --> 09:33.760
And so as we develop AI systems that are tailored at the materials industry, we actually

09:33.760 --> 09:38.760
have to focus on how do we use the limited data that we have available to us today to

09:38.760 --> 09:40.920
solve a really hard problem.

09:40.920 --> 09:43.680
And then the other piece, and there are a lot of differences.

09:43.680 --> 09:47.520
Actually, before you move on from there, can you elaborate on when you say data point

09:47.520 --> 09:50.760
in this context, what you're specifically referring to?

09:50.760 --> 09:51.760
Sure.

09:51.760 --> 09:57.520
And when I talk about a data point, there are some canonical measurements that people

09:57.520 --> 09:58.680
do on materials.

09:58.680 --> 10:04.920
So take, for example, an aluminum alloy, you would test the strength of that material,

10:04.920 --> 10:08.280
how easy is it to break, how easy is it to scratch?

10:08.280 --> 10:13.120
And you get single numbers out or curves out of those tests.

10:13.120 --> 10:17.880
And there are typically a six to ten of them in any area that are kind of the critical

10:17.880 --> 10:23.120
questions we ask before we use a material, how well does it fatigue, does it corrode?

10:23.120 --> 10:29.000
Is it a material that will break if you smash something against it?

10:29.000 --> 10:35.280
These are both for safety reasons and performance reasons, the critical questions we ask.

10:35.280 --> 10:40.320
But in metals in particular, but in a lot of areas of high performance materials, you

10:40.320 --> 10:45.960
can do a test to know how something ages that lasts for 10 or 100,000 hours.

10:45.960 --> 10:49.480
And that is, understandably, a very expensive test to do.

10:49.480 --> 11:00.320
To what extent do the data points associated with or that can be aggregated from the manufacturing

11:00.320 --> 11:01.800
process plan of this?

11:01.800 --> 11:08.640
So I don't know how long it takes to manufacture a given piece of aluminum, but assuming

11:08.640 --> 11:13.800
that there are some number of steps and there are a whole variety of process parameters

11:13.800 --> 11:21.960
and temperatures and humidities and things like this that go into determining how strong

11:21.960 --> 11:29.920
and light the material will ultimately be, are those also part of what you consider to

11:29.920 --> 11:35.320
be the data set or are you only looking at the properties of the final product?

11:35.320 --> 11:40.640
So that is absolutely part of the data set, but in a very interesting way.

11:40.640 --> 11:45.800
The way most materials companies work today is they have a factory or a plant that produces

11:45.800 --> 11:49.360
a handful of different materials in incredibly high volumes.

11:49.360 --> 11:56.680
And so if you're in Alcoa or you're a Corning or you're a 3M, when you have manufacturing

11:56.680 --> 12:02.320
data, that manufacturing data is just terabytes and terabytes of data, petabytes of data over

12:02.320 --> 12:09.640
days and weeks, about one material, it's just a tremendous amount of depth on one material.

12:09.640 --> 12:13.880
So the big change that's come to the materials industry that's really interesting is that,

12:13.880 --> 12:19.000
well, for some materials that's really important, the demands on these materials companies

12:19.000 --> 12:21.920
is to come up with new materials faster.

12:21.920 --> 12:27.480
And the example I always love to use is guerrilla glass.

12:27.480 --> 12:32.960
So I think a lot of people are familiar with the Steve Jobs story about how that came

12:32.960 --> 12:38.360
to be the front of the iPhone, but what people don't think about is that Apple wants

12:38.360 --> 12:40.440
a new glass for their phone every year.

12:40.440 --> 12:44.000
They're an incredibly demanding company and they are a company that wants to have the

12:44.000 --> 12:46.400
highest performing product on the market.

12:46.400 --> 12:52.080
And most materials companies take three to five years or more to develop a new product.

12:52.080 --> 12:57.880
And so Apple is now saying to Corning, hey, I want a new glass every single year.

12:57.880 --> 13:00.960
And Corning has to figure out a way to do that or else somebody else is going to come

13:00.960 --> 13:03.400
in and provide that to Apple.

13:03.400 --> 13:08.520
And so I think the big change that's come is that companies are very good at producing

13:08.520 --> 13:12.240
what they produce very efficiently and where that used to be enough.

13:12.240 --> 13:16.720
Now there's a real push to be able to innovate faster and to come up with that next generation

13:16.720 --> 13:24.160
battery or next generation vehicle or next generation phone so much faster that really the

13:24.160 --> 13:26.440
manufacturing information is helpful.

13:26.440 --> 13:30.520
But you really need a diversity of data around many different materials to help optimize

13:30.520 --> 13:32.680
for a particular use case.

13:32.680 --> 13:38.040
Is the implication of what you're saying that you're the kinds of customers you're targeting

13:38.040 --> 13:46.000
are not necessarily these, it's not kind of the production, the ongoing production of

13:46.000 --> 13:53.600
existing material as much as this R&D process to get to newer types of materials.

13:53.600 --> 13:54.600
That's exactly right.

13:54.600 --> 14:00.840
So we're really focused on enabling these, the world's finest materials companies to move

14:00.840 --> 14:05.440
much, much faster in the development and deployment of new materials.

14:05.440 --> 14:09.880
And then of course we assist in the scale process and we assist in manufacturing optimization

14:09.880 --> 14:10.960
and those sorts of things.

14:10.960 --> 14:16.480
But where we're really differentiated is that ability to to take these relatively small

14:16.480 --> 14:21.840
diverse development data sets and help companies come up with those new high performance signature

14:21.840 --> 14:22.840
materials.

14:22.840 --> 14:26.680
In some cases 50 to 70% faster than they would have been able to otherwise.

14:26.680 --> 14:32.960
All right, and so given the way you've described this as kind of a small data problem, what

14:32.960 --> 14:34.800
does the approach look like?

14:34.800 --> 14:40.600
So the beautiful thing about materials is that we have a trick up our sleeve that most

14:40.600 --> 14:44.640
other industries don't and that's that we have the laws of physics and chemistry in our

14:44.640 --> 14:45.640
pocket.

14:45.640 --> 14:51.840
You know, as scientists, we learned about how materials behave without ever seeing data.

14:51.840 --> 14:55.000
And then at the end of our careers, as we got graduate degrees or went into industry,

14:55.000 --> 15:00.600
we saw the data around materials help us refine what we believed about those materials

15:00.600 --> 15:01.600
performance.

15:01.600 --> 15:04.400
And we use machine learning in exactly the same way.

15:04.400 --> 15:11.560
When we have a physical understanding of how atoms bond, for example, or how two elements

15:11.560 --> 15:15.920
in a particular material interact, we use those formulae.

15:15.920 --> 15:22.960
We use those relationships built into a machine learning system, built into a larger infrastructure.

15:22.960 --> 15:26.960
But it turns out there are a lot of materials, mysteries in materials still.

15:26.960 --> 15:31.800
There are a lot of areas of materials where we don't exactly know the fundamental phenomena

15:31.800 --> 15:32.880
that are dominating.

15:32.880 --> 15:36.160
And so what we say is for those, we have data.

15:36.160 --> 15:41.360
We were able to use the small amount of data we have to connect the theory we understand

15:41.360 --> 15:45.560
with the things we don't and more accurately fill in the gaps.

15:45.560 --> 15:50.120
And then the beautiful part about it is the companies we work with are highly experimentally

15:50.120 --> 15:51.120
oriented.

15:51.120 --> 15:53.360
So they're constantly generating new data.

15:53.360 --> 15:57.560
And so just like a human, you know, a machine learning system can come in and actually learn

15:57.560 --> 16:00.680
alongside and be the perfect lab assistant.

16:00.680 --> 16:01.960
It never forgets anything.

16:01.960 --> 16:03.840
It's relatively unbiased.

16:03.840 --> 16:08.920
It is able to surface new ideas and it's able to help humans come up with better hypotheses

16:08.920 --> 16:11.520
which lead to better results faster.

16:11.520 --> 16:17.640
Can you maybe help us make this a little bit more concrete by talking about in the context

16:17.640 --> 16:23.680
of one of these examples that you've given, you know, what the process might look like,

16:23.680 --> 16:28.120
what the pipeline might look like, what some of the systems and algorithms look like that

16:28.120 --> 16:33.880
help you get to, you know, developing new materials more quickly?

16:33.880 --> 16:35.200
Sure, absolutely.

16:35.200 --> 16:39.680
There are, like I said, there are many, many places we've worked, but one in particular

16:39.680 --> 16:43.880
that I always like to think about is with a solar cell company.

16:43.880 --> 16:50.520
And so we worked on a specific class of solar cell and basically the goal was they wanted

16:50.520 --> 16:52.640
to absorb light at a certain wavelength.

16:52.640 --> 16:55.880
You know, they want to make sure they're absorbing light that's coming in from the sun.

16:55.880 --> 17:00.360
They wanted to do it highly efficiently and they wanted it to have the specific stability

17:00.360 --> 17:01.360
requirements.

17:01.360 --> 17:04.840
You obviously don't want your solar cell to degrade over time.

17:04.840 --> 17:11.200
And so what we did is we said, look, you have a lot of fundamental data about the solar

17:11.200 --> 17:12.920
absorbance of a material.

17:12.920 --> 17:16.280
So it's, you know, there's sort of all these electronic calculations that people have

17:16.280 --> 17:17.280
done for years and years.

17:17.280 --> 17:22.200
And actually, a Nobel Prize was awarded for coming up with these relationships.

17:22.200 --> 17:24.560
And so we said, well, you have that knowledge to begin with.

17:24.560 --> 17:26.280
So let's not reinvent that.

17:26.280 --> 17:31.000
We don't need to learn the relationships that want to Nobel Prize from, you know, several

17:31.000 --> 17:32.160
hundred data points.

17:32.160 --> 17:34.200
We just can put that into the system.

17:34.200 --> 17:39.960
And then what we do is we say the goal of what you have of your final product is the

17:39.960 --> 17:42.760
solar cell with these six or eight or 10 particular properties.

17:42.760 --> 17:44.880
In this case, it was about 10.

17:44.880 --> 17:51.200
And what we did is we said the data that we have can help us bridge from the fundamental

17:51.200 --> 17:52.480
relationship that we have.

17:52.480 --> 17:54.160
So we'll put those equations in.

17:54.160 --> 18:00.280
We're going to use that alongside data in a framework that I'm happy to describe in

18:00.280 --> 18:01.280
a moment.

18:01.280 --> 18:07.480
And then we're going to generate a set of hypotheses for what new materials might achieve

18:07.480 --> 18:09.880
the goals you've set out to achieve.

18:09.880 --> 18:15.600
And this is the real key because what we don't want to do and what AI is not likely to

18:15.600 --> 18:20.520
do any time soon is to say, aha, here's your one right answer.

18:20.520 --> 18:21.520
Go make it.

18:21.520 --> 18:27.040
What AI is very good at doing in materials and chemistry is to say, here are some highly

18:27.040 --> 18:28.760
probable candidates.

18:28.760 --> 18:29.760
You should go try those.

18:29.760 --> 18:32.160
A human will select a few to try them.

18:32.160 --> 18:36.480
And then as new data is generated, that gets folded back into the algorithm and it's allowed

18:36.480 --> 18:42.880
to obviously relearn and retrain and then make a set of new suggestions.

18:42.880 --> 18:45.480
And that's very in line with the scientific method.

18:45.480 --> 18:51.760
You hypothesize, you test, you conclude something and you come back with a new better hypothesis.

18:51.760 --> 18:56.480
And the AI system simply helps you do that much more efficiently.

18:56.480 --> 19:02.480
One of the themes that surfaces on the podcast pretty frequently is this idea that you're

19:02.480 --> 19:09.200
poking out a little bit, some of the approaches like deep learning take the approach that you

19:09.200 --> 19:14.560
just alluded to, take a whole bunch of data independent of any laws of physics or chemistry

19:14.560 --> 19:19.160
and throw it at this deep learning model and let it try to figure everything out.

19:19.160 --> 19:24.600
Whereas you've incorporated those laws of physics and chemistry, can you talk a little bit

19:24.600 --> 19:32.360
about how you have made those to coexist in a machine learning framework?

19:32.360 --> 19:33.680
Yeah, absolutely.

19:33.680 --> 19:38.800
So we have a couple things that we truly believe in as sort of a philosophical basis here.

19:38.800 --> 19:44.280
And I honestly think we've seen a lot more of this emerge in the AI community over

19:44.280 --> 19:48.400
the last year or two because I think we as a community realize how important they are.

19:48.400 --> 19:51.600
The first is we believe in interpretability.

19:51.600 --> 19:55.840
I think one of the big challenges in deep learning actually is that you can throw a lot of

19:55.840 --> 19:56.840
data at things.

19:56.840 --> 20:00.320
You can sort of see what some of the neurons are doing, but a lot of times there's a big

20:00.320 --> 20:06.600
black box component to what's going on because we sort of manually insert these relationships

20:06.600 --> 20:11.520
in or automatically insert them, but in certain known relationships into these systems, understanding

20:11.520 --> 20:15.000
how they construct themselves is really important.

20:15.000 --> 20:20.880
The second thing that we really believe in which drives the answer to your question is uncertainty.

20:20.880 --> 20:27.880
So one of the big problems in AI for scientific applications is that when you talk about things

20:27.880 --> 20:32.640
like recommendation engines, if Netflix tells you you're going to like something with four

20:32.640 --> 20:36.840
stars and you end up hating it, your plane doesn't fall out of the sky.

20:36.840 --> 20:40.720
It's okay if Netflix is to be wrong every once in a while and it can sort of narrow itself

20:40.720 --> 20:42.880
in on your preferences over time.

20:42.880 --> 20:48.400
For us, we work with scientists who believe that there's a, who understand uncertainty

20:48.400 --> 20:51.800
and who understand when a model is confident or not.

20:51.800 --> 20:58.160
And so what we actually do is we build these very large graph networks where rather than

20:58.160 --> 21:04.040
having a black box situation like deep learning, we're building a graph that's not so different

21:04.040 --> 21:10.360
from a deep net, but has a, has nodes that we can control in it.

21:10.360 --> 21:12.560
Some of those nodes are new data.

21:12.560 --> 21:18.160
Some of those nodes are machine learning systems, whether they're deep learning, we'll use

21:18.160 --> 21:23.760
deep learning for things like images or, or more sort of standard machine learning systems,

21:23.760 --> 21:28.400
you know, forests or, or neural nets or what have you, we can really put anything into

21:28.400 --> 21:29.800
these networks.

21:29.800 --> 21:34.880
And then we can also conveniently put in, you know, arginious equations or a reaction

21:34.880 --> 21:36.280
equation, reaction rates.

21:36.280 --> 21:40.800
I mean, in some ways we sort of program high school chemistry and college chemistry into

21:40.800 --> 21:47.000
the base levels of these nodes so that when the system is constructing these networks

21:47.000 --> 21:52.480
and is testing nodes and is sort of moving through the possible models it can build, it's

21:52.480 --> 21:57.320
able to access things that are not just, you know, the, the billion images of cats and

21:57.320 --> 22:02.880
not cats that we have labeled, it's, it's actually looking at what is the, what is the underlying

22:02.880 --> 22:07.000
chemistry and how does that connect between data and chemistry to minimize the uncertainty

22:07.000 --> 22:08.000
of the model.

22:08.000 --> 22:11.080
So we actually use uncertainties as a first class member of our system.

22:11.080 --> 22:16.280
And that really helps us drive kind of model quality, hypothesis generation in this larger

22:16.280 --> 22:20.760
framework of kind of graphically connected machine learning models.

22:20.760 --> 22:27.600
Is there a simple example or way to explain the way the system uses and navigates these

22:27.600 --> 22:35.080
nodes that, you know, can help us really generate some intuition for how it works?

22:35.080 --> 22:36.080
Yeah, absolutely.

22:36.080 --> 22:39.760
So, so, you know, I'll take a battery as an example.

22:39.760 --> 22:47.480
So a battery is basically three layers of material, an anode, a cathode and an, and an electrolyte.

22:47.480 --> 22:53.800
And those three materials are stacked cathode, electrolyte, anode, and the, the charge

22:53.800 --> 22:58.320
moves between the two ends and that's how you, how you have a battery work.

22:58.320 --> 23:02.080
The three materials you choose are critical because, you know, if they're not compatible

23:02.080 --> 23:05.960
with each other or if they don't take advantage of each other's strengths, you're not going

23:05.960 --> 23:08.200
to have a perform, perform in battery.

23:08.200 --> 23:13.160
And so, that at a very high level is, is very simple, but if you, if you think as a material

23:13.160 --> 23:18.920
scientist what about this problem, you actually realize there's a stacked set of, of problems

23:18.920 --> 23:20.920
that all build up together.

23:20.920 --> 23:26.080
And what I mean by that is each of those materials is composed of atoms.

23:26.080 --> 23:30.280
And so when you talk about lithium ion, obviously lithium is the, is the primary ion that's

23:30.280 --> 23:32.560
moving around in, in that battery.

23:32.560 --> 23:34.400
Now the atom you choose is important.

23:34.400 --> 23:37.360
There are sodium batteries, there are aluminum batteries, there are batteries based on

23:37.360 --> 23:43.520
many elements and we have, and so, so the atoms matter, but atoms aren't all that matter.

23:43.520 --> 23:46.800
So actually how the atom is bond together matters a lot.

23:46.800 --> 23:51.680
And so that's sort of the next layer up is how these things interconnect with one another.

23:51.680 --> 23:56.160
And then you can start thinking about, well, if they bond together one to one, what happens

23:56.160 --> 24:00.160
when you zoom out a little further and say what happens over, you know, a micrometer or

24:00.160 --> 24:04.800
what happens over a centimeter, you know, obviously you don't have perfect bonding, nothing

24:04.800 --> 24:05.800
is perfect.

24:05.800 --> 24:09.040
So you have things that are happening over much larger length scales.

24:09.040 --> 24:13.040
And then you finally talk about what happens when you layer these three materials together

24:13.040 --> 24:15.600
and how do you optimize them all at once.

24:15.600 --> 24:22.960
And so what we do is we say we have good understanding about what a lithium atom is.

24:22.960 --> 24:28.120
We have a very good understanding about how a lithium ion bonds to an oxygen ion.

24:28.120 --> 24:33.720
We have a very good understanding of how things happen over large distances.

24:33.720 --> 24:38.640
But what we don't have a good understanding of as a materials community is how to explicitly

24:38.640 --> 24:43.360
connect all of that knowledge at the different length scales in such a way that allows us

24:43.360 --> 24:47.560
to draw really accurate conclusions and builds really accurate models of how these materials

24:47.560 --> 24:48.720
will behave.

24:48.720 --> 24:53.600
And so what that means is we're really good at understanding one little piece of the

24:53.600 --> 24:57.920
problem, but it's very hard to zoom out and see the whole forest and understand what

24:57.920 --> 24:59.040
the whole structure is.

24:59.040 --> 25:03.960
And so we actually use, when I say we build these graphical models, the graphical models

25:03.960 --> 25:11.040
are actually combinations of data and these underlying sort of single length scale models

25:11.040 --> 25:17.120
to allow us to connect the data and ML allows us to connect to the different length scales

25:17.120 --> 25:23.120
and actually create better models that incorporate everything from high school chemistry.

25:23.120 --> 25:28.800
How many electrons does this particular element have all the way up to what happens

25:28.800 --> 25:33.000
if I leave it in the oven for an extra hour, all the way up to what happens when I seal

25:33.000 --> 25:37.480
it in there with two other materials and how well does the battery perform over time?

25:37.480 --> 25:42.960
In the case of an example like that, what would be you've got some of these nodes in this

25:42.960 --> 25:49.160
graph represent data points about the materials and the atoms and the things that you've described.

25:49.160 --> 25:52.760
And then some others represent machine learning models.

25:52.760 --> 25:57.320
What would those be and what's the relationship that they play in this graph?

25:57.320 --> 26:03.480
Yeah, so the machine learning models are what will say things like, well, we understand

26:03.480 --> 26:08.200
how the lithium atoms all behave one to one.

26:08.200 --> 26:12.240
And we understand how bonding happens at a very large scale, but we don't understand

26:12.240 --> 26:18.280
how swapping 10% of the lithium out for silicon might actually affect the overall performance

26:18.280 --> 26:19.280
of the battery.

26:19.280 --> 26:23.080
We can't know how that propagates through the system.

26:23.080 --> 26:31.080
And so that's where we use data and machine learning to identify trends across these

26:31.080 --> 26:39.080
data sets that contain diverse materials data that allows us to zero in on some specific

26:39.080 --> 26:40.960
higher performance.

26:40.960 --> 26:45.280
And the other thing that it allows us to do is to actually answer a question that most

26:45.280 --> 26:48.400
AI is actually quite bad at answering.

26:48.400 --> 26:54.120
And in most AI systems, the goal is to identify things that are pretty similar to other things,

26:54.120 --> 26:56.480
facial recognition.

26:56.480 --> 27:00.000
It's looking for faces that are similar to faces it's seen before.

27:00.000 --> 27:07.960
For us, R.A.I. systems are actually trying to reorient things and look for new materials

27:07.960 --> 27:09.480
outside of the current space.

27:09.480 --> 27:13.360
And so it might be that we're looking at lithium and silicon together and then all of

27:13.360 --> 27:17.760
a sudden, somebody has a great idea that putting some carbon in there could be a real help.

27:17.760 --> 27:22.320
Well, if no one's ever used carbon before, you have to have a really crafty way of being

27:22.320 --> 27:25.840
able to represent what that looks like so that the machine learning system can lock

27:25.840 --> 27:27.600
onto any signal at all.

27:27.600 --> 27:34.760
And so the representations among these hierarchical units allows us to say to insert changes

27:34.760 --> 27:39.320
like that and actually begin to push the envelope of what's possible rather than just saying,

27:39.320 --> 27:42.120
oh yes, this is 50% between two things I've seen.

27:42.120 --> 27:47.000
I'm just going to take a sort of do a logistic regression between the two and take the midpoint

27:47.000 --> 27:51.880
and have that be the result of my prediction.

27:51.880 --> 27:58.480
So is it fair to say that the nodes in this graph ultimately represent some kind of material

27:58.480 --> 28:05.000
or chemical characteristics, some of which you have direct data about and so you can

28:05.000 --> 28:12.320
insert those data points directly and others you're using machine learning to infer based

28:12.320 --> 28:14.800
on some other data that you have.

28:14.800 --> 28:22.000
So it's kind of like a hierarchical graph of these characteristics, is that fair?

28:22.000 --> 28:23.520
That's precisely correct.

28:23.520 --> 28:24.520
Okay.

28:24.520 --> 28:31.160
And so ultimately then you are applying addition on another level of machine learning

28:31.160 --> 28:39.560
on top of this graph to try to infer or learn about the relationships between these

28:39.560 --> 28:47.120
different characteristics and then predict where you might find new materials that produce

28:47.120 --> 28:49.880
some desired result.

28:49.880 --> 28:50.880
That's exactly right.

28:50.880 --> 28:56.120
So you can imagine in the guerrilla glass example because we all know what the demands

28:56.120 --> 28:57.120
are on that, right?

28:57.120 --> 29:00.520
We don't want it to break, we don't want it to scratch, we want it to be nice and transparent

29:00.520 --> 29:05.680
and pretty light and that's more or less the properties that you're looking for.

29:05.680 --> 29:12.760
And so what we would say is let's put those targets into the system and then use an optimization

29:12.760 --> 29:16.200
system, whether AI based or not and it kind of depends on the use case, whether that

29:16.200 --> 29:24.160
makes sense, to actually optimize the materials, the processing, the input elements, sort of

29:24.160 --> 29:30.960
everything about that glass to achieve the thinnest, lightest, strongest, most scratch-resistant

29:30.960 --> 29:35.520
guerrilla glass possible and we do that hand in hand with a scientist because it turns

29:35.520 --> 29:38.720
out that scientific expertise is absolutely critical.

29:38.720 --> 29:45.240
And so as that optimization step, are you, is the system spinning out other materials

29:45.240 --> 29:55.040
or compounds or is it rather saying, you know, you're looking for something that has,

29:55.040 --> 30:00.440
you know, this degree of distance to this other material or, you know, is it relative

30:00.440 --> 30:03.840
or is it giving you absolute things to investigate?

30:03.840 --> 30:07.080
Oh, it actually gives you absolute things to investigate.

30:07.080 --> 30:11.760
So in the case of a glass, we would give you a specific chemical formula and we would

30:11.760 --> 30:17.280
potentially give you even processing steps that we believe to be the things that will

30:17.280 --> 30:19.760
achieve the goal you're trying to achieve.

30:19.760 --> 30:24.760
And we return those, I mean, this is all part of the software system, right?

30:24.760 --> 30:26.600
This isn't human driven.

30:26.600 --> 30:30.520
But the scientist on the other end, the person who's developing that next generation

30:30.520 --> 30:34.800
material will look at that list and say, well, you know, it turns out you've suggested

30:34.800 --> 30:37.360
a glass that has lead in it.

30:37.360 --> 30:38.680
Well, that might be a great glass.

30:38.680 --> 30:39.920
I don't really want to make it.

30:39.920 --> 30:41.640
So we're going to avoid that one.

30:41.640 --> 30:45.280
And we are going to move on and say, actually, these other three are very interesting.

30:45.280 --> 30:49.760
So the scientists will then exert their intuition to say, I'm going to go make those now.

30:49.760 --> 30:54.440
And when I make them, I will return the data into the system, which obviously helps us

30:54.440 --> 30:57.840
map the data space better and helps us provide even better recommendations.

30:57.840 --> 31:03.880
So it's a sequential learning system that allows us to work hand in hand with the scientists

31:03.880 --> 31:07.320
to develop those next generation products.

31:07.320 --> 31:12.920
What is that process for working with scientists look like from a kind of timeline perspective?

31:12.920 --> 31:16.040
You know, it really varies based on the domain.

31:16.040 --> 31:20.920
There are materials in chemistry is just an incredibly diverse industry.

31:20.920 --> 31:25.720
I think it's easy from the outside to say, well, you know, it's kind of one, one model

31:25.720 --> 31:26.720
of the industry.

31:26.720 --> 31:29.680
Especially more than a dozen subgroups.

31:29.680 --> 31:37.200
In most cases though, we first work with a scientist to identify the data that underpins

31:37.200 --> 31:42.920
what they believe to be the most important signals and trends in their industry.

31:42.920 --> 31:47.000
These scientists, in some cases, spent decades working in a specific area and know their

31:47.000 --> 31:52.000
industry incredibly well or know their area incredibly well.

31:52.000 --> 31:54.240
And so we always start there.

31:54.240 --> 31:59.600
And then what we do is we say, you know, look, we can build these, we just called them

31:59.600 --> 32:00.600
first pass models.

32:00.600 --> 32:05.600
We kind of help them construct these, the first version of the graph.

32:05.600 --> 32:08.800
And then we allow them to begin to edit the graph.

32:08.800 --> 32:10.440
We allow them to insert nodes.

32:10.440 --> 32:16.440
We allow them to create latent variables, add additional data and really strike up a scientific

32:16.440 --> 32:20.320
conversation with the system that refines over time.

32:20.320 --> 32:25.600
The first little bit of somebody using our system is not directly going to experiment.

32:25.600 --> 32:29.400
It's learning how to operate this powerful new tool.

32:29.400 --> 32:32.840
And then over time, they begin to add data.

32:32.840 --> 32:38.600
They begin to gain confidence in why the system is recommending what it's recommending.

32:38.600 --> 32:42.200
And once they have that confidence, it's very easy for us to say, hey, you know, why

32:42.200 --> 32:45.880
don't you go try to make this, make this material, give it a try.

32:45.880 --> 32:49.520
And I'll tell you what, it almost never works perfectly the first time.

32:49.520 --> 32:52.720
There's, you know, the model is never right on target the first time.

32:52.720 --> 32:57.800
But what it does is it creates this back and forth conversation between the AI system

32:57.800 --> 33:03.160
and the expert scientists to be able to say, you as an expert scientist are not perfect

33:03.160 --> 33:04.160
every time.

33:04.160 --> 33:07.920
Humans are deeply flawed in many ways with respect to data.

33:07.920 --> 33:14.160
And an AI system is, frankly, deeply flawed, no matter how hard you try in a completely

33:14.160 --> 33:16.800
understanding what humans have learned about chemistry.

33:16.800 --> 33:22.280
But as a team, there's a really powerful back and forth effect where the AI system can

33:22.280 --> 33:25.320
learn and have additional information injected into it.

33:25.320 --> 33:30.400
And the human can release some of that data analysis burden and start to think creatively

33:30.400 --> 33:33.480
about how to solve these really difficult problems.

33:33.480 --> 33:39.160
Have you run into any use cases that the system just isn't able to handle yet?

33:39.160 --> 33:44.560
And if so, what are, you know, what are the sources of those limitations?

33:44.560 --> 33:50.080
You know, we haven't run into anything that is a complete and abject failure.

33:50.080 --> 33:56.120
I think one of the reasons for that is that there's a, that there's a sort of assumption

33:56.120 --> 33:59.160
that it is an iterative process.

33:59.160 --> 34:03.880
And so that's a, you know, if you sort of understand it to always be a hill climb in

34:03.880 --> 34:10.440
some ways, you know, you kind of don't, taking failure is, you know, saying it failed

34:10.440 --> 34:13.320
completely is sort of impossible.

34:13.320 --> 34:17.440
Because I will say, you know, one of the things that, that we found to be very interesting

34:17.440 --> 34:24.400
is that there are some areas of materials where there is so little knowledge or there is

34:24.400 --> 34:31.360
such a breadth of, of theory about why something happens that it can actually be really challenging

34:31.360 --> 34:36.760
to, to use an AI system to do, to have a great effect.

34:36.760 --> 34:39.080
And an example there I'd use is superconductors.

34:39.080 --> 34:45.280
So this is a type of material that when you cool it down to below liquid nitrogen temperature

34:45.280 --> 34:49.920
or even colder, they are wires that have no resistance.

34:49.920 --> 34:54.800
So you know, you could use them to very efficiently transmit electricity, for example, and this

34:54.800 --> 34:56.960
is something they're used for.

34:56.960 --> 35:00.000
But there are a few different classes of superconductors.

35:00.000 --> 35:04.720
Pretty much any time a new superconductor is discovered, a Nobel Prize is awarded.

35:04.720 --> 35:10.200
Which the theory that underpins it, there are some very strong theories, but the way that

35:10.200 --> 35:17.000
we capture data and the way that we understand these things to work is wholly incomplete.

35:17.000 --> 35:21.360
And so I think there are certainly areas of materials where, you know, we just don't

35:21.360 --> 35:28.480
have the, the grounding to, to build an AI system that is going to really kind of break

35:28.480 --> 35:33.560
into that new space because the data landscape isn't dense enough and, and the, the underlying

35:33.560 --> 35:38.400
theory that we would normally inject is frankly not so well understood that we can do that

35:38.400 --> 35:40.120
with great confidence.

35:40.120 --> 35:46.000
And so I would actually say that that's one of the, one of the biggest challenge areas

35:46.000 --> 35:51.440
and there are a few, a few areas like that where, you know, they're new enough that, you

35:51.440 --> 35:56.400
know, we just don't have established knowledge theory or data that allows us to, to really

35:56.400 --> 35:58.360
model these things well.

35:58.360 --> 36:06.360
How has the, the product evolved like, what was your MVP and how have you iterated over

36:06.360 --> 36:10.800
time to get to the platform that you have now?

36:10.800 --> 36:16.400
So when we, when we first started, the MVP was my, my co-founder Bryce sitting behind

36:16.400 --> 36:22.720
a keyboard, kind of manually tuning up machine learning models and in whatever appropriate

36:22.720 --> 36:28.880
tool set worked at the time and, and really was working with primarily academic groups

36:28.880 --> 36:35.040
to, to show that, that there might even be some value here, you know, most materials

36:35.040 --> 36:39.680
companies see their, their development data as their crown jewels and, and I literally

36:39.680 --> 36:43.440
every company they've talked to has referred to it that way and this is at this point hundreds

36:43.440 --> 36:44.440
of companies.

36:44.440 --> 36:49.440
And, and they're, the, the interesting thing about it is, you know, working with these

36:49.440 --> 36:54.080
academic groups allowed us to really get an understanding of the development workflow

36:54.080 --> 36:58.200
in a very open, open way.

36:58.200 --> 37:03.360
What we realize though is that as we went into companies, there, there's this, uh, skits

37:03.360 --> 37:05.640
of frenia to what they want.

37:05.640 --> 37:09.520
On one hand, everyone is hungry to use AI in a new way.

37:09.520 --> 37:13.960
Everyone is hungry to have AI be their savior in one way or another and, you know, I'm,

37:13.960 --> 37:15.920
I don't need to tell you about the buzz in the industry, right?

37:15.920 --> 37:19.880
You know, it's, it's on everybody's, uh, everybody's lips right now.

37:19.880 --> 37:25.880
But the, the other side of it is in the materials and chemicals industry, the presence and,

37:25.880 --> 37:31.160
and organization of data is actually in general quite poor, um, you know, there are only a few

37:31.160 --> 37:37.240
different, uh, well, that there basically no company has a very large database, you

37:37.240 --> 37:41.480
know, a lot of Google and search results or, you know, any Silicon Valley company and

37:41.480 --> 37:45.880
their digital data, most materials companies have been collecting data on, you know, and

37:45.880 --> 37:50.760
in manual logbooks for the better part of 50 or 100 years.

37:50.760 --> 37:55.320
And so there's data out there, but in general, it's pretty tough to get at.

37:55.320 --> 38:00.120
And so actually what we've, what we've learned is that the AI tools are really nice and incredibly

38:00.120 --> 38:04.800
powerful and, and drive a lot of value, but actually the first step as we work with people

38:04.800 --> 38:09.560
is to go into these organizations and say, hey, you need to start organizing your data

38:09.560 --> 38:15.080
in a way that makes it available to advanced analysis, whether it's AI or even just, you

38:15.080 --> 38:21.960
know, very simple sorts of visualizations, um, and to actually go and, and build the platform

38:21.960 --> 38:25.520
that will allow people to organize materials and data, uh, materials and chemistry data

38:25.520 --> 38:30.880
in a rational way, uh, has been a layer of the platform we've had to, to build and continue

38:30.880 --> 38:35.760
to refine because it turns out AI without any underlying data is not super useful.

38:35.760 --> 38:41.520
Uh, and most of these companies don't have the, the sort of presence in house yet to, to

38:41.520 --> 38:46.360
do that themselves. And so we had, we had to add this extra layer, which has been great

38:46.360 --> 38:52.280
because what it means is as a company, we are able to both create sort of the instantaneous

38:52.280 --> 38:56.640
value of we're helping you organize this stuff for the long term, we're helping you create

38:56.640 --> 39:01.520
long term value. And then we also get to come in with the really cool AI results and say,

39:01.520 --> 39:05.480
oh, by the way, we're helping you generate your next, you know, your next generation products

39:05.480 --> 39:10.200
in, in five or seven business units and, and look at how cool these results are, look

39:10.200 --> 39:14.400
at how excited your scientists are. So in some ways, you know, I think we've stumbled

39:14.400 --> 39:18.720
into a really cool business here because we get to be an ally to everyone in the organization,

39:18.720 --> 39:25.000
which is frankly usually rare in B2B businesses. Great. Well, uh, this has been super interesting.

39:25.000 --> 39:31.400
Are there any additional thoughts that you'd like to share? You know, I think the, the,

39:31.400 --> 39:35.880
the last thought I'd like to share is, is just that there's, you know, when we started

39:35.880 --> 39:40.560
the company, people, we had, you know, we, we obviously have some great investors. We're

39:40.560 --> 39:44.840
super excited about them. Um, but when we were originally pitching a great number of investors

39:44.840 --> 39:49.920
said generalized AI is going to eat your lunch. That is the future and, and the specialty

39:49.920 --> 39:55.920
domain specific stuff really is never going to have any value. And, and, and I think, you

39:55.920 --> 40:00.320
know, I think you've seen, you know, just based on, on the, the guests in your show and,

40:00.320 --> 40:07.120
you know, I know we've seen that the resurgence of domain specific AI has showed that AI is

40:07.120 --> 40:13.080
an incredibly powerful tool. It's an incredible way to do a lot of, of fast tasks, faster

40:13.080 --> 40:19.120
and it's an incredible way to kind of optimize how we behave in a lot of ways. But in materials

40:19.120 --> 40:26.200
and chemistry, the, the domain specificity is incredibly important. And, and so as we've

40:26.200 --> 40:30.280
kind of grown as a company, one thing I've realized is that, you know, kind of sharing

40:30.280 --> 40:33.760
that message with people and understanding that, yeah, you know, what you can do with

40:33.760 --> 40:36.920
scikit-learn is interesting. What you can do with these open source tools is interesting

40:36.920 --> 40:43.320
and powerful. But if you really want to go deep, merging domain expertise and, and you

40:43.320 --> 40:49.360
kind of AI expertise, ML expertise, if you can merge those effectively, you can have

40:49.360 --> 40:54.640
a super powerful tool that's really differentiated from what anyone else has. And so I guess if I,

40:54.640 --> 40:58.080
you know, kind of were to send a message to your listeners, it's, you know, you don't

40:58.080 --> 41:05.760
have to be Andrew Ng to go and dominate the materials or to dominate a segment of AI.

41:05.760 --> 41:10.120
You have to bring your world view and your expertise and combine it with a really effective

41:10.120 --> 41:14.280
AI strategy. And that is the way to actually have real world impact. Or at least that's

41:14.280 --> 41:17.680
what we've seen. And, and it's led to some really exciting results. And I couldn't be more

41:17.680 --> 41:23.000
proud to have been part of this team, which is really a broken new ground in materials

41:23.000 --> 41:27.160
and chemistry. Oh, that's awesome. Well, Greg, I really appreciate you taking the

41:27.160 --> 41:32.320
time to share what you're up to. I will, I'm looking forward to kind of keeping track

41:32.320 --> 41:37.840
of, of Citrine and the work that you're doing. Well, thanks a lot, Sam. It's been, it's

41:37.840 --> 41:42.320
been awesome to be on. I love your show. And I've really enjoyed our chat this morning.

41:42.320 --> 41:50.080
So thank you very much. Thank you. All right, everyone. That's our show for today.

41:50.080 --> 41:55.400
For more information on Greg or any of the topics covered in this episode, head on over

41:55.400 --> 42:23.920
to twimlai.com slash talk slash 148. Thanks so much for listening and catch you next time.

