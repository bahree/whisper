The conversation you're about to hear was recorded live at Twomokon AI
platforms. For more coverage of Twomokon, visit Twomokon.com-news or follow us
on Twitter at Twomokon AI. But first a word from our sponsor. Big thanks to our
friends at IBM for being a founding sponsor of Twomokon AI platforms. IBM
Watson is the company's comprehensive suite of AI tools for the enterprise
which includes Watson Studio, Watson Machine Learning, and Watson Open Scale. The
IBM Watson Suite allows enterprises to build, deploy, and manage AI models in
any environment including on-premises and in private and public clouds. We
encourage you to check out the IBM Data Science and AI community by visiting
Twomokon AI.com slash IBM. And if you join, you'll get a complimentary month of
select IBM programs on Coursera.
All right, so super excited to invite up our next guests. Fran Bell, Fran, runs a
Data Science platform's team at Uber. She's got over 100 data scientists
working on building tools that are a platform that's at a higher level of
abstraction than Uber's already famous Michelangelo platform. Fran. Welcome to
Twomokon. So I kind of paraphrased your role a little bit. Why don't you tell
us a little bit about your team and your charter? Yeah, absolutely. Thank you so
much for having me. It's really a pleasure to be here at the inaugural Twomokon.
About the charter of the team, the vision really is to provide cutting-edge
data science at the push of a button to anyone within the company. So that
basically means that we're aiming to transform anyone within Uber into a
data scientist. An example of this is forecasting. So forecasting obviously
underpins a large number of use cases within Uber. And so the vision here is to
provide the latest and greatest cutting-edge forecasts to folks at a push of a
button via UI, for example, as integrated into a BI stack or programmatically
accessible through our API. And so the only thing that our end users within Uber
need to provide is historic data, whether it's for example in the form of a
CSV file or query, and the forecast horizon. So how far you want to forecast
out. And we do everything else automatically in the background. We scan over
whole suite of forecasting algorithms, either those that we have integrated
off the shelf into the platform, or also those that we have developed in-house
proprietary. And we've gone way beyond forecasting in terms of data science
areas, other areas that we're investing heavily in in platformization or
anomaly detection, experimentation, more recently also conversational AI
and natural language. And then personally I'm very excited about our proof of
concepts of also platformizing and semi-automating intelligent insights
generation and data exploration. So of those conversational AI seems like the
I'd man out so to speak. How did you end up working on that one? Yeah so we see a
lot of really great benefits. We have a lot of conversational AI data at Uber.
One example is our customer obsession ticket assistant, which was one of our
first use cases in this space. So here for example we wanted to aid customer
service representatives in solving customer support tickets that are coming
in. And as you can imagine given the size of our platform we get quite a few
of those. And so using natural language and deep learning approaches we were
able to build recommendations for our customer service representatives of what
is the topic that folks are writing in about. What are potential actions that
the customer service representative might want to take. And then also providing
guard lines or guard rails basically around what could be the best starting
point to actually address the response to the end consumer. Of course the
customer service representative always has the final call and say on this. But we
saw really great improvements in our customer care experience as a result of
having this AI basically assisting our customer service representatives.
Okay cool. So you've got this portfolio of platforms essentially that you're
building to support these different use cases. How do you know when it's time
to platform something? Yeah that's a really great question. And we're looking at
multiple different dimensions here and very deliberately see which of the
data science areas we want to platformize. It's obviously a heavily
investment. And so we look at three items. The first one is can the
platformization of this area really create step function improvements to our
user experience and the business. And so sticking with the example of
forecasting if we can forecast tightly accurately demand in a particular space
and time we can create more magical user experiences. The second thing is
really the wealth of use cases that exist across the company. Of course building
a platform you want to be able to tackle many different use cases. And so with
forecasting for example it does spend the entire enterprise ranging from
marketing to obviously marketplace supply and demand financial aspects
operations as well as our hardware. We still have a lot of hardware on premise
and so accurately forecasting the hardware needs especially on high
demand days and special days such as Halloween or New Year's Eve is really
important. So that's the second aspect. And then thirdly. All we need is a
special day at Uber. Yes that's it is. See lots of demand on that front. And then
the third dimension really is the reusability of the models and
methodologies that we apply. And again with forecasting you know a common
framework that is needed to build forecasting algorithms is a backtesting
framework. So understanding the accuracy of your forecasts and that really is
needed for any step along the forecasting journey. And so having a common
central paralyzed language extensible backtesting framework is something
that's really important. So as your team out evangelizing the
opportunity to platformize and looking for customers that are already working
on things that meet these criteria or our folks coming to you saying hey we've
got these problems help us all them. How does the relationship with your
ultimate customer evolve? Yeah absolutely. So we have a lot of the product teams
coming to us with use cases at the same time because we are this horizontal
team that spans across the entire company across all lines of business we have
a very unique vantage point as well. And so we can also gently nudge some of
the product teams to come and join us in this journey as well. Okay and so when
you identify a problem space that it makes sense to platformize how do you
approach that you just jump in and start building, start coding or what is
the methodology look like? Yeah that's a great question. So the way we build
platforms is in a use case driven manner. So that basically means that with
every use case that is strategically chosen we augment the platform and we
reuse as much capabilities as possible from the platform. And that really
allows us to have wins very early on. And learning from this we actually now
have a three-faced approach to platformization. So step one is really
consulting. So we have these deep domain experts in particular areas of data
science on the team. And so we embed them with particular areas of the
business where we see opportunities of having use cases in these areas. And so
that has a couple of advantages. Firstly the domain experts learn more about the
business, about the opportunities, the pain points, and really can bring back
these learnings to then drive the best design for these platforms. It also
allows us to tackle these use cases early on and really show wins and gain the
trust of our partners and leadership on that front. That of course is not a
scalable approach. This is why we set out to do platforms in the first place. But
it's a very good starting point. And so the second thing that we usually do is
templatization. So what I mean by this is we build recipes, whether it's form of
documentation, example ipython notebooks, providing talks and educational
aspects. And this really allows us now to have a one-to-many multiplicative
effect throughout the organization, mostly to other data scientists that are
dedicated to these business areas. And then over time as we're taking on more and
more of these use cases, we really expand our platform to become more and
more self-service and work towards that vision of really providing it at the
push of a button without domain expertise required. Of course, including
best practices and guardrails in the process. So in introducing you I mentioned
Michelangelo, Uber's low-level machine learning infrastructure platform. Uber
was one of the first companies to publish about what they were doing to
automate machine learning. If I interpret your LinkedIn profile correctly, you
were at Uber doing applied machine learning platforms before at least before
that article hit possibly before the Michelangelo effort even started. What's
the relationship between these two teams? Yeah, we have a fantastic working
relationship with Michelangelo as well as the AI organization engineering branch
that we also work with very closely to platformize. And we have three modes
of interaction here. The first one is as the head of platform data science, I get
pulled in into the strategic and vision setting when it comes to Michelangelo
working closely with their engineering and product lead. So right from the
start, there's a really great collaborative relationship that we can build on.
And then we have two other modes that have evolved over time. The first one is
Michelangelo was more decent. We deeply embedded folks from our teams into the
Michelangelo group. So for example, with the customer obsession ticket assistant
example that I mentioned earlier, this was actually the first deep learning
algorithm that ran on Michelangelo. And so as you can imagine, a lot of the
features required for doing deep learning were in a very nascent state at the
time. And so having data scientists who were working on this particular
problem deeply embedded in the Michelangelo group and working together with the
engineers and product managers there to build all capabilities not only to solve
the customer obsession ticket assistant case but also more generic aspects that
then really benefited the community more at large to build deep learning, you
know algorithms and frameworks was really important here. Of course, as
Michelangelo has evolved over time and became more mature, we are becoming more
of an end consumer of the platform and it becomes more of a self-service
component especially with the onset of PyML. PyML has really provided step
function improvements to what's PyML. So PyML basically allows us to write
Python code and bring our own models that then basically via Michelangelo get
deployed in a sandbox environment at scale. And so that really reduces the
barrier to entry for data scientists to be less reliant on, you know, the
native approaches that are already integrated in Michelangelo or software
engineers that would help with productionization for example. And so this
approach has become really prominent across Uber and has really opened up new
avenues for self-service on Michelangelo. And so with your team pre-existing some
of that effort, you've got platforms and use cases that you've stood up before
they were mature and ready now that they're more mature and ready. Is it a
dynamic relationship in the sense that, you know, there's a heavy migrated in any
of those legacy models over to Michelangelo or, you know, if it's there and is
working, you're going to leave it alone. Yeah, that's a really great question. So
we have a couple of aspects here. One is platforms that are more recent. So for
example, a conversational AI platform here from the get go we build on top of
the Michelangelo capabilities and are actively utilizing this. But as you
correctly pointed out, you know, some of the platforms were built well beyond
before Michelangelo existed or was very recent. And so we have our own
independent stacks on this front. But here it's really important to see the
opportunities for integration and to see a timeline where some of these
platforms are already or may be merging in the future. And then the third part
is platforms that are currently standalone, but likely will never merge with
Michelangelo. So for example, our experimentation platform, which has very
different types of methodologies and workflows, I wouldn't imagine would be
combined with Michelangelo. Can you elaborate on that? What about the methodologies
and workflows makes them? You're not good fits. Yeah, absolutely. So here we use
more statistical approaches. So hypothesis testing, multi-arm bandits, etc.
versus the traditional machine learning approaches. And so for that reason we
keep them separate. What are some of the key technical challenges that you
face for this portfolio of use cases? Yeah, that's a great question. So each of
the platforms is different. We have different users, different use cases, and so
therefore also different requirements on the technology side. But I can go into
two concrete examples here. The first one is for real-time anomaly detection.
This was actually the first platform that I've built at Uber. And so here the
idea is that we wanted to detect system outages as quickly as possible. So
people not being able to sign in or sign up or perhaps trips being degraded,
etc. And here we basically saw that this was still an open research problem.
And we set out to build a new platform around it and also advance the space.
We have a couple of patents now in that space as well. But here the key kind of
requirement beyond the innovation component was that we needed to have
extremely low latencies. So as you can imagine, because there's a real-time
problem, we had basically those considerations to take care of on and
extremely high QPS because we have hundreds of millions of signals
basically back end as well as aggregate mobile signals that we're tracking in
order to understand whether there's a system outage going on. Another one
example is forecasting. As I mentioned earlier, we integrated our forecasting
algorithms also into our BI stack for easy access through UIs. And so there's
now a really great path where you can query a metric, you can visualize this
metric using DashBuilder, an internal tool that we've built. And then you'll
also have this little button that says, I want to forecast this metric. And so
obviously we want to make sure that we have a good user experience here and that
people don't have to wait, you know, minutes or hours.
You're hearing theme here. Exactly, right. And so having low latency for some of
these forecasting algorithms is really important. And so are some of these
technical challenges ever a reason why you might build something from the
ground up yourself, so to speak, as opposed to rely on what the Michelangelo
team offers or is that not a consideration typically? Yes, it does flow into kind
of our decision making process here in terms of what is already available, how
easily it is extensible. And often the timing kind of component comes in as
we discussed earlier in terms of, you know, where was Michelangelo when we
started to build? And how can we evolve that in the future? Can you talk a little
bit about the technology stack that your platforms tend to rely on? Do you have
your own kind of not Michelangelo, but you know, you've got these higher level
platforms that are very application or use case focused. You have your own
kind of intermediate level of abstraction, or are you building kind of use
cases more independently? Yeah, so when we don't build on top of Michelangelo, which
is quite a few of our platforms, we build microservice architectures, we build
them and go in Java actually for performance reasons. Databases are typically
in my SQL. Then we run our instances typically on prem for efficiency
reasons. And then several of our use cases are batch and offline, especially on
the training side. But for those that we discussed earlier, where latency is
something that we want to focus on, we use caching for optimization. And do
you rely heavily on open source in this area or publish open source in this
area? Yeah, both. So we're definitely building on the shoulder of giants and
using open source wherever possible. I think we wouldn't have evolved as
quickly if it wasn't for open source. And so utilizing the methodologies that
have been developed by the communities is very essential. But we also are
heavily invested in open sourcing ourselves as well. And we have quite a few
open source projects. If we look at the data science domain, there are quite a
few examples here as well. We have a pyro that was developed by the AI
organization, which is a probabilistic programming language. We have horror
vod that is distributed deep learning framework on TensorFlow that has gained a
lot of popularity in the community. There is Ludwig that was also built by the
AI organization, which allows for a deep learning framework where you actually
don't have to write code anymore to deploy and train models. And then more
recently, our org has worked together with the AI organization to develop
Plato. This is a very flexible and use case rich platform that allows for
conversational AI, especially in the research and prototyping phase. And then
also our causal ML package is a Python package that we recently launched in
collaboration with our marketing team that basically covers uplift modeling
use cases as well as causal inference in combination with machine learning.
So yeah, quite a lot of efforts in that direction. And we're considering to do
more. With so much out and available in open source that you're incorporating
into these systems that you're building, reminds me a little bit of podcasts
we published not too long ago. Quote that the guest mentioned that became the
title of the podcast was machine learning or AI was is a systems engineering
problem. I wonder how much of what you're doing is systems engineering
connecting pieces that you know in many cases exist already versus kind of
pure innovation and building new stuff. Yeah, it's definitely a combination of
both, especially because we want to be fast to market with a lot of these
things. So we leverage whatever is already there. But often more often than not
actually, we need to not only deploy the cutting edge but actually be and
define the cutting edge as well. And as I was hinting a little bit earlier, one
example is the real-time and non-intection platform that I built when I joined
Uber about five years ago. And so here it became very quickly clear that with
the scale that we're operating at, the real-time nature, the signal to noise
ratio because we actually would be sending pager to the alerts that would wake
people up potentially in the middle of the night. If our algorithm thought there
was a system outage going on. And so we were able to break new ground very
quickly in the space. And one other thing that really helped develop these
algorithms to the precision recall that we needed was I put myself on call. I
was very customer obsessed. We're in the pager. Sorry. Wearing the pager. Yes,
wearing the pager for six teams, multiple consecutive weeks. I can tell you I
did not sleep much. I was working up a lot during this time. But that also helped
to improve the algorithms really quickly. May she want to get it right? Yes,
exactly. Nice, nice. Your team is kind of building very use-case specific
things, you know, very close to the end user, doing low-level kind of
infrastructure as well to support all of this. How do you organize? How do you
build an organization to support all of this? Yeah, so we are organized by
data science expertise. So we have a forecasting team, a non-detection team,
experimentation team, convi team, computer vision team, etc. basically. And then
as many other companies we are cross-functionally organized. So we have
data scientists and engineers, product managers, designing, working
together, basically building all of these platforms. When we zoom into the
data scientists, we tend to hire full stack data scientists for these roles
exactly for the reason that you described. We see a lot of advantages to
having folks who can write production-level code in addition to having this
deep domain expertise in a particular data science area. And in some of the
advantages we see here already start in the design phase. So having somebody
has deep understanding about the constraints of the infrastructure stack. As
we're developing a lot of these things at scale with latency constraints can
be highly advantageous. Because things that might look good on paper might
actually not work in the real world once we deploy it in these ecosystems.
Do you have examples of that? Things that look good on paper that didn't
actually work out? Yeah. So coming back again to the real-time
anomaly detection framework. So at the beginning we were developing algorithms
that would have extremely fine granular data points. So we obviously want to
have near real-time signal. And so we wanted to have an understanding minute by
minute or even in finer granularity what was going on. And so originally we
designed an algorithm that would require multiple weeks of data to train one
way of how we did this is to frame it as a forecasting problem. And then half
that minute granularity. And that obviously would be an extremely large overhead
onto our database systems on that front. And so we basically designed the
algorithms such a way that we have lower course of granularity in more
historic kind of aspects. And then very fine granularity is overlaid as a
secondary step in order to still capture the variance on for example a minute
by minute level. So just having that kind of constraint in mind made sure that
we didn't require unnecessarily high kind of overheads on our databases and
unnecessary infrastructure cost as a result. So that's one example on that
front. Example another example of where we see a lot of advantages for having
full stack data scientists is in the productionization step. So here we're
trying to avoid handoffs in terms of a data scientist writing a script or a
white paper and then you know providing it to a software engineer. We see a
lot of opportunities for errors on that front logical errors in particular. And
so having data scientists to also write the production level code without such a
hand of step is really important to us. And then finally also developer
velocity. So as we all know there is this prototyping step involved and we're
trying to exceed some threshold criterion that we've set out in the
beginning of the design phase. And it's of course not known when are we gonna
exceed basically this threshold criterion. And so that can lead them to
lag times once you have actually found an algorithm you want to productionize
that software engineer might be busy with other things during that time. And so
again having data scientists to can write production level code can really
also help speed up that innovation cycle as well. You mentioned developer
velocity and that makes me think of kind of velocity in a sense of agile
methodologies. Do you mean it that concretely? And is there a methodology that
you've kind of evolved to or developed that works well in the context of
these types of problems? So the way we work is very closely in software
engineering kind of principles. In both in terms of best practices in terms of
our working structure we work on a daily basis hand-in-hand with software
engineers. So we have developed a lot of this. But I think you bring up a
really good point in terms of you know developer velocity and productivity.
And so a big goal of the platform teams more holistically is to really speed
up that innovation cycle while increasing accuracy of the various
different methodologies employed right. And so the way we see it is we have
these four major steps within the development cycle of a machine learning or
broadly speaking data science problem. You have exploratory data analytics
than this iterative prototyping phase productionization and then roll out
and monitoring and that closes the loop for kind of a new cycle to start off.
If there's you know a V2 that we want to progress in. And so building abstraction
layers higher level abstraction layers and this is where you know a lot of the
work that my team and collaborators are coming in to play really helps to
facilitate not us only for us ourselves kind of this cycle but really for the
entirety of the company to really go faster and faster around that loop. So you're
primarily doing this by hiring full-stack engineer full-stack data scientists
they're not easy to find. And we throw that time around like it defines
concretely a specific set of skills but not you not every full-stack data
scientist is going to have the same strengths. How do you grow your data
scientists or if you find folks that need support in one or more areas or how do
you manage the kind of learning cycle for your team? Yeah absolutely so when
we hire for these roles we also hire complimentary of course right so there
would be some folks on the team who lean more towards the research side and
others who lean more towards the engineering and you know software development
side and folks that sit in between. So that's one aspect and then there's a very
strong learning and teaching culture at Uber really continuously striving to
improve oneself and so we have a lot of programs even within Uber educational
programs everything from introductory courses to machine learning all the way
to domain experts basically who then give workshops and training sessions hands
on basically courses. I've invested a lot in mentorship programs at Uber as
well building out a community across all of the data science and analytics where
we then partner folks and in sometimes we also do 20% projects similar to
what Google for example does where we have people then immersed into various
different teams to get hand on a Hans experience basically in some of these
domains so yeah I think continuous learning and teaching is something that's
really really important. Okay and what are you excited for going forward what's
the future of data science platforms that Uber look like? Yeah absolutely so I
see both at Uber as well as in the industry a big push towards these higher and
higher level abstractions for platforms to really kind of commoditize it to
make it available to a broader audience beyond data science machine learning
engineers and engineers more broadly speaking and so coming back to that
four-step data science workflow or machine learning workflow that it
described earlier you know one of the gaps that we saw is that we didn't have
any semi-automation or automation around a data exploration or insights
generation as a whole and that's something that actually goes well beyond
data science or machine learning workflows right? It's that first step of the
four-step cycle exactly the first step right where there is still a lot of human
hours that need to go into that to dig into the data to understand and explore the
data and also for business analysts right a lot of the questions that they
would be getting is I have an important business KPI and it moved up or down
right to investigate you know all the various different slices and
dices of the data on what might have happened right? And so what we have
been starting to work on is a proof of concept to actually have an algorithm
automatically scan through our data and to surface a potentially interesting
insights to folks whether it's machine learning experts or business
analysts for example and they obviously would go and dig into some of these
suggestions and understand them more deeply but here we see a really large
opportunity not only to save people a lot of time but also to really open up
new insights that might not have been discovered previously and this is some of
the feedback that we're getting from our early adopters in this field that the
machine was able to come up with interesting suggestions that they say
wouldn't have come up themselves and so I think that really will if successful
will revolutionize how we do data analytics at Uber and I think more
broadling the industry awesome awesome well friend thanks so much for joining us
here at Twomalcon thanks for having me great speaking with you all right
everyone I hope you enjoyed our show straight from the main stage at Twomalcon
AI platforms for more information about today's show visit Twomalai.com and
for more Twomalcon coverage visit Twomalcon.com slash news thanks so much
for listening and catch you next time
