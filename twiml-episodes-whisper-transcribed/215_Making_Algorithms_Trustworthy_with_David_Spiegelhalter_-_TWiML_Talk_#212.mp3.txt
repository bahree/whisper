Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting
people, doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
In this, the second episode of our NURRIP series were joined by David Spiegelhalter, chair
of the Winton Center for Risk and Evidence Communication at Cambridge University and president
of the Royal Statistical Society.
David, who was an invited speaker at NURRIP's, presented on making algorithms trustworthy,
what can statistical science contribute to transparency, explanation and validation.
In our conversation, David and I explore the nuance difference between being trusted
and being trustworthy and its implications for those building AI systems.
We also dig into how we can evaluate trustworthiness, which David breaks into four phases, the inspiration
for which he drew from British philosopher Onora O'Neill's ideas around intelligent transparency.
Enjoy.
Alright everyone, I am here in Montreal for the NURRIP's conference and I've got the pleasure
of being seated with David Spiegelhalter.
David is chair of the Winton Center for Risk and Evidence Communication at Cambridge, as
well as president of the Royal Statistical Society and he was one of the invited speakers
here at NURRIP's, talking on making algorithms trustworthy.
David, welcome to this week in machine learning and AI.
Thank you for having me, it's our pleasure.
Before we jump into the topic of your talk, please share a little bit of your background
and how you get involved in statistics, machine learning and kind of the confluence of
the two.
Exactly, well I'm a statistician as you can tell and I was around in one of the last summers
of AI in the 1980s and I was very interested in computer-aided diagnosis such as it was
then an interest in statistical approaches to that, using simple Bayesian methods or
logistic regressions, the standard stuff and that was an exciting time and I got very
interested in this new idea of Bayesian networks and graphical models and so in the 1980s
I really worked and developed this thing called the Louds and Spiegelhalter algorithm that
was for exact propagation in Bayesian networks and we did a lot of work in that and then
I went into Bayesian graphical modeling, developing a bug software for Bayesian Markov-J
Monte Carlo analysis and so on and worked all the time in this intersection of machine learning
and AI and statistics.
For the last 10 years I've been much more to do with communication and I've got a job
that involves communicating statistics and risk and evidence and now we've got a center,
a strange center in the maths department at Cambridge with a great gang of psychologists
and communication specialist XBBC people, web designers, I'm very interested in producing
trustworthy material that communicates numbers and statistics and risks and predictions
and so on.
Okay, that's really interesting.
I was wondering what the meaning of risk and evidence communication was, almost anything
to do with numbers.
It's better than public communication and statistics.
Right.
Okay.
Fantastic.
And so you're here at NURBS talking about making algorithms trustworthy.
What does that mean?
Yeah, the issue of trust is very important.
I've been very influenced by this wonderful philosopher in the UK and Nora O'Neill who studied
Kant and has come up with this very important idea which has been very influential that
organizations and developers of systems shouldn't be trying to be trusted.
There's the wrong objective to try to be trusted.
Well, they should be doing what we all should be doing is trying to be trustworthy in other
ways to earn that trust because that is within our control to be trustworthy.
And this idea of being trustworthy has a big impact in the UK, the National Statistics
Code now puts trustworthiness as its number one objective.
Why is that nuance important between being trusted and being trustworthy because being trusted
is something you want, but other people can only offer it up to you.
You trust worthy of something within your control, and that means really analyzing what
it means to be trustworthy.
Okay.
And so what does that mean from a statistical perspective or how can statistics inform trustworthiness?
Well, I think that in the talk I break trustworthiness of an algorithm or any sort of system into
two components that the system itself should be trustworthy, the claims it makes should
be trustworthy.
You should be able to rely on them or if you can't rely on them, it can tell you how
confident it is.
The other thing is that what is very important is that the claims made about the system are
trustworthy by the developers, by the commercial entity or whatever.
So you've got not only believed the system, but you've got to believe what's said about
the system.
Now, what that leads you into very quickly is the importance of evaluation.
And in my talk I draw an analogy with the highly developed evaluation phases that are
used in drug development and pharmaceuticals, which statisticians I've worked in that area
for decades.
And they're just very briefly four phases.
Phase one is safety on a few healthy people.
Phase two is proof of concept done on some selected people to try to optimize the dosage.
Phase three are the big control trials in which you actually compare it with a comparator
and that allows you to market the drug and phase four is post-marketing surveillance.
And what I did was draw an analogy with developing algorithms that are going to go into practice
that phase one is just the digital testing that people do on a set of test cases.
Phase two is laboratory tests where you actually compare it, say with doctors if you've got
a medical system and do the user center design for the interface.
And phase three is where the field tests where it actually goes out there and you actually
evaluate what is impact is, which might be beneficial, but it could be harmful.
You never know what side effects you might have.
And phase four then is the post, once the thing is out there, monitoring to make sure it's
not degrading and that it's not making mistakes.
And so I suppose what I'm saying is that on the whole when I read about evaluations,
they rarely get past phase one, they just sort of accuracy on test cases.
Some of them moving into phase two comparison of diagnostic systems with medical, with experts
and things like that.
Almost nothing about phase three, what actually is the benefit impact when these things are
put into practice in society and properly evaluated.
And I think that the, in order for claims about a system to be trustworthy, then you need
a much more rigorous evaluation.
In order for claims about a system to be trustworthy, you need to have a much more rigorous evaluation.
My sense is that we're very far from that today.
Exactly.
And I suppose what I'm saying, this field is developed so wonderfully.
It's so, the stuff at the conference is so amazing, but it's still, for all that fantastic
technical capacity at a very early stage, because when these things start moving into society,
you find, you know, people are saying, hey, come on, you know, I want to divide it.
It's not, you know, it's not immediately obvious that this is going to be a good thing
in all areas.
And so I think, you know, this area is due to mature into something which does rigorous
evaluations.
It's interesting.
So one of the controversies that last year's Neurup's, then Nips was kind of a call for
increased theoretical rigor around deep learning in particular, but, you know, our current
approaches to AI in general.
This is a call for rigor also, but a very different one, one from more of a statistical
perspective.
And it's about a rigorous test of what does it mean to actually implement this?
Do you need both?
Because you need the rigorous sort of internal analysis in order to demonstrate what it says
is trustworthy.
So because the part of the trustworthiness, of course, this is where we get to explanation
is to be able to say why it's come up with its conclusion, to be able to justify that
conclusion.
And from the other statistical perspective, I take very strongly because statisticians
are obsessed with uncertainty, getting the error bars right, you know, with as much
concerned with the uncertainty as we are about the point estimate.
And so that's what we bring.
And I think again, if a claim is going to be made, and especially when it's made with
some uncertainty or lack of confidence, you've got to understand what that means.
You've got to rely not on the claimed confidence of what is, what is say, what an algorithm
comes up with.
And you've talked to you provide examples of this, the kinds of claims that you envision
this kind of model being applied to and, you know, what you'd expect to see or what you've
seen in kind of passing a claim through these filters.
Well, in the talk, I just give various examples of different phases of how some statistical
ideas can come in, just at the early phase when, you know, you're comparing algorithms
on your database to see decide which is the best one.
You know, I talk about ranking algorithms and how you use some bootstrap methods on the
test set.
You can get a probability that any algorithm is actually the best rather than just producing
a simple league table.
Again, there's been a lot of statistical work on league tables and essentially taking them
apart because just because something happens to rank best on one particular set of data,
it does not mean it's the best algorithm.
Incidentally, for the football team, just because the football team is top of the league,
it doesn't mean it's the best team because there's always luck involved and we're rather
good at trying to put numbers on luck.
So there's that aspect of the phase two.
And again, the, you know, recent critique of systems that have made comparisons with doctors
saying diagnostic systems, which are actually being slightly, you know, quite pulled apart
because of their lack of statistical rigor.
And, you know, it's very good they got to that stage, but actually they're not doing
them very well.
You're not doing to the standard of rigor that one would expect.
And for phase three, I talk about an ultra-alars involved in the diagnostic system.
It's a terrible system, but it actually helped when it was put into practice.
And it's because it wasn't because what the system, the computer was saying, it's because
it just, it changed the culture of data collection and encouraging people to make early diagnoses
and being more confident about their work.
So there's all sorts of unintended ways that systems might benefit, but also unintended
ways in which they might harm.
And so, you know, I went through those applications, but then I went on to this idea of transparency,
which is, you know, is an element of trustworthiness.
And this philosopher, Anora Neal, has got some great things to say about transparency.
She thinks transparency can be really dangerous.
It's not an end in itself, especially in the sense of disclosure in that, you know, you
can be very transparent, and yet nobody can understand what's going on, if you, you
could release the code or something like that, that's very transparent, but people
is hopeless.
Right, hopeless.
So she's really pulled apart transparency and so she's making this appeal for intelligent
openness, which means that any information you give, and this is a really good checklist,
any information you give should be accessible, so people are going to be able to get to it.
It's going to be intelligible.
They're going to understand it.
It's going to be usable.
It's going to meet their needs.
And it's going to be accessible, which means somebody needs to be able to check the working.
Not everybody, but somebody out there, you know, needs to be able to check the working
if necessary.
Now, when you're using deep learning methods, you know, that's really quite a hard challenge
to counter.
I did mention what I thought was some very nice work being done by Google DeepMind,
with in London, with more fields hospital and analysing eye scans, in which they've,
you know, delivered trained this network, so to provide intermediate steps, segmentation
map, so that it doesn't just go straight to a diagnosis, got a probabilistic diagnosis,
but it's actually putting in the intermediate steps, which seems a really cool thing.
Obviously, and that's because that project seems to be very strongly influenced by the
clinicians themselves.
You want that.
That's the way in which they used to thinking about it, and they wanted to map their way
of thinking.
And because many people are claiming now that you don't necessarily have to make a trade
off in performance in order to get a much more interpretable model, that it actually,
you know, there's vast numbers of models, vast numbers of options, it gives very similar
performance, especially, and that, especially, there's actually a lot of the differences
in performance and largely illusory, that was what I talked about earlier.
Okay.
And so, you know, actually, you know, the struggle to, you know, among the great space of
models you can use to choose one that actually enables a much more transparent, much better
explanation, makes it more trustworthy, because people can see the reasoning.
You ran off several of the qualities that's noranil outlined, but they're, they're all
very subjective and, and seem to be in some ways that are with this statistical rigor
that we're talking about.
No, yeah, but no, no, exactly, but that's, I think I, I spent all my time with this
psychologist.
Yes.
So, I've been very influenced by this, and I'm not even going to try to define exactly
what explainable or interpretable or transparent means.
But you can be quite rigorous about your evaluation of some of these aspects.
For example, in the interfaces for the systems we build, we evaluate three things in which
they have an impact on people, they're cognitive, do they understand it, the behavioural, what
does it do to their, their behaviour and their intentions, and the affective, how does
it affect their emotions, and we want to measure all of those, and they can be completely
different.
So, it's very important to get that feeling of what, you know, what do people get from
it?
And for example, through surveys, yeah, yeah, so we, the psychologists would do, you
know, actual direct-to-face-to-face interviews on people, this is the phase two evaluations
within a laboratory, we get patients in, get them to talk through a system, actually
do some eye tracking as well, see how they're using it, using it, and then evaluating
on these things.
The metrics are quite difficult to do, you know, the satisfaction with which a decision
has been made is quite a tricky thing to evaluate, but you need to try to be able
to do it.
You know, these rather loose things, it is worth the effort of trying to measure them
as accurately as possible.
I used as an advocate, an idea of a system we put a front end on could predict, which
is for women merely diagnosed with breast cancer, who are trying to decide what other therapies
to have apart from surgery, which is based on a fairly, you know, basic statistical analysis
of the database of 4,000 cases, and it produces survival curves up to 15 years for women,
and then looks at what would be the effect.
These are personalized to various attributes of the tumor and the woman, and then, you
know, it says how that survival would change if you take particular therapies, and those
the effect of the therapies, all that data is based on clinical trial, causal data from
randomized clinical trials.
And that's fine.
Our idea is that the system, which is currently used by doctors, will be used by doctors
talking to patients, it already, and even by patients themselves and support groups,
but we're using exactly the same system for all these different groups, and that means
having very good explanation facilities, both for the terms, but also ways of portraying
the risks to patients, and this is serious stuff.
This is the chance people are going to be alive in 10 years' time, but with very careful
use of wording and imagery and even the colour and accepting that we can do it.
And the point about this is that for explanation like that, is that two things, one size does
not fit all.
There are people who've got different needs, they've got different levels of understanding
about numbers and graphics.
And so what you need is both multi-layered explanation, with a very simple level at the
top, through to much deeper level, we put the maths in, if you want to, you can see a PDF
with all the maths in, you can get the code if you really want it.
So you've got all those layers of explanation vertically, but also horizontally.
So when we're explaining 15-year survival, we can provide bar charts and survival curves
and icon arrays and tables and text, etc., all of those options, depending on what people
prefer to see.
So you've got both vertical and horizontal explanation choices.
There's no correct way to do it, but you can try to evaluate all of it.
And there are only some people who want to see the stuff at the bottom, but they should
be able to see it, because that's part of the accessible openness.
That example is a compelling one, I find that, you know, often time we're dealing with
physicians, there's a presumption of trust or trustworthiness that, you know, may work
for a lot of people, but sometimes you want a little bit more data, and they're not
always prepared to...
Things are changing.
People are making much, not everybody.
You know, I don't know.
I just completely have them off top of my head.
I'd say half people who are quite prepared still go along with a very paternalist point
of view.
You know, thank you very much.
Tell me what to do.
Right.
Just tell me what to do.
I don't want to know anything else.
You know, are asking questions and actually wanting to exercise some of their own rights.
I don't know.
I've got friends who have used the system that we've been working on in order to challenge
their doctors and saying, okay, I'm not...
It's only a tiny benefit.
I know I'm going to get terrible side effects.
I'm not going to have it.
Yeah.
And they've using that to challenge us and power them and power them.
I think this is very valuable.
Not only that, but in the UK now, there's a much stronger legal structure on what must
be explained to people in order to get informed consent for treatment, and in others, all
this should be explained to people.
So we're providing it as actually some of the tools to allow doctors to carry out their
work better.
Okay.
There's a thread in the AI community around taking ideas from adjacent fields like electrical
engineering, the idea of data sheets or model cards, some folks have called them and
basically different ways of documenting the characteristics or biases of different
AI data sets or systems.
And it sounds like a part of what you're doing is a similar idea, but applying ideas from
clinical trials and the statistical methods associated with clinical trials and medical
and pharmaceutical field to the way we talk about and communicate around AI systems and
machine learning models.
And yes, and not just the pharmaceutical area, but there's been involved for years in
building prognostic systems for people and then both evaluating them and putting them
into practice.
And one of the crucial things about the evaluation that people would get really obsessed about
in a sort of pedantic way that statisticians tend to operate in is that the probabilities
must be meaningful.
If you say 70% probability for something or 70 out of 100 chance, then it's got a meaning
that 70 out of the number of times you say that, it should happen in 70% at the time.
The undercalibrated probabilities, in other words, the uncertainty, the accuracy of the
uncertainty, it is important as the accuracy of the main number, it is a very statistical
idea and yet it's very important because otherwise you get these grossly over-confident things
like, oh, I'm 99% sure that this is the diagnosis, that is grossly misleading, that really
is terrible.
So that's, I think, another very important way, thing that can be brought from statistics,
which has worked a lot on how to evaluate the calibration of probabilities or test statistics
to use and so on in order to check that element of trustworthiness of the claim.
Right, right.
This all calls to mind, at least in the US, I don't know if it's somewhere in the UK,
when you're advertising pharmaceuticals, there's like you have your 30 minute ad and then
you're long, that's red.
Exactly.
Well, you shouldn't have to do that.
That's just some, you know, regulation.
What you want should, we know.
But at the same time, you know, that's kind of a summary of a data sheet.
Yeah, that's not trustworthy communication.
That's like having to sign, you know, we're getting some software and those terms and conditions,
16 pages of terms and conditions.
Right.
Yes, that is not intelligent openness.
No way is that accessible, usable, incomprehensible, accessible, is break breaks every rule.
And it's terrible at sort of communication.
Right.
It's there to obey a law, but it is a complete sham in terms of good communication.
I agree.
I agree.
At the same time, it is one step better than, you know, yes, or no, the computers, yes,
yes, the computers exactly.
Exactly.
And of course, the worst systems of all are proprietary systems that are used in courts
to decide about recidivism, risk, or bail, right, like that, are shocking because they're
proprietary.
They're totally done transparent.
You've got no idea what information is being used in them.
I mean, that's absolutely shocking again, that breaks every rule, you know, everything
I'm trying to, I'm talking about is broken by that kind of system.
So key takeaways from your talk.
Oh.
Yeah.
Well, that's basically basic statistical ideas, you know, and their experience and other
words.
I've got a lot to offer.
A lot to offer.
But also, I can't, you know, I'm not just taking ideas from, you know, from statistics.
I'm taking ideas from philosophy and psychology and empirical testing and things that really
in this maturing disciplines, unbelievably important discipline, I think could take
a lot more account off.
Great.
Very much in line with some of the key themes that I'm hearing at this year's Neurobs,
you know, it's, in fact, two of them.
One is the importance of, you know, fairness, transparency, et cetera.
And the other is kind of the importance of interdisciplinary approaches and you're kind
of bringing those right to you with that.
And there's some wonderful work going on, you know, this morning I really featured the
FATML, you know, social impact statements, you know, lists that they've got partly because
they do not identify transparency as an objective.
They've, they've learned themselves that transparency just means to an end, you know, it's,
it's no good just being transparent, unless you obey a Neel's ideas of what transparency
means.
Well, we'll definitely provide a pointer to Nora Neel and your work as well, of course.
Yeah.
The talk's up on Facebook as well, it just looks to.
Oh, fantastic.
Fantastic.
Well, David, thank you so much for taking the time.
No, the real pleasure, real pleasure, thank you very much for asking me.
All right, everyone, that's our show for today.
For more information on David or any of the topics covered in this episode, visit twimmelai.com
slash talk slash 212.
You can also follow along with our NURP series at twimmelai.com slash NURP's 2018.
As always, thanks so much for listening and catch you next time.
