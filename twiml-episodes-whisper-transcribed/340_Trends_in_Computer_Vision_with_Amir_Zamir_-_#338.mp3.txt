Hey everyone, hope you all had a wonderful holiday.
For the next few weeks we'll be running back the clock with our second annual AI Rewind
series.
Join by a few friends of the show, we'll be reviewing the papers, tools, use cases,
and other developments that made us splash in 2019 in key fields like machine learning,
deep learning, NLP, computer vision, reinforcement learning, and ethical AI.
Be sure to follow along with the series at twomolai.com slash rewind 19.
As always, we'd love to hear your thoughts on this series, including anything we might
have missed.
Send me your feedback or favorite papers via Twitter, where I'm at Sam Charrington, or via
a comment on the show notes page you can find at twomolai.com.
Happy New Year, let's get into the show.
Alright everyone, we are here for AI Rewind, our second annual walk through the top trends
and developments in machine learning and AI, and this time I am with Amir Zameer.
Amir is an assistant professor of computer science at the Swiss Federal Institute of Technology
or EPFL.
In fact, Amir at this current moment for another three days is a postdoc at Affiliated
with Stanford and UC Berkeley, where he was when we first spoke with him back in July
of 2018, Amir, welcome back to the twomolai podcast.
Thanks.
Great to be here.
Yeah, I'm really excited to dig into this conversation about what's new, your take
on 2019 from a computer vision perspective.
And so to kind of get us started with that, why don't we just take broad brushstrokes?
What's your take on 2019?
Thanks for having me.
It was another exciting year, in my opinion, for fields with an AI and including computer
vision, you know, complex conferences are expanding.
So there's more talent coming in, more energy.
I think CPR 2018 was 6,000 roughly, the number of attendees in 2019 was about 10,000.
So we will see, and that was six months ago roughly.
So we will see how it's going to be in 2020, but that expansion in size.
How does that growth rate compare to NERPs, which is the one that gets a lot of headlines?
Right.
NERPs was, I came back from NERPs like roughly two weeks ago.
I believe it was 13,000, but again, they are like six months apart.
So, you know, we will see how CPR will be.
But I think roughly the same size, maybe NERPs, somewhat bigger because it generally includes
many different areas, not just vision, not just NLPMs, so on.
Many vision people are there, myself included, but yeah, but they are a huge conference.
It's big enough that you won't see your friends anymore.
And are the paper submissions as CVPR growing as quickly as NERPs?
Yes.
Yes.
Yeah.
I don't know the exact same year.
I'm an area chair, so I should know this number, but all in, all in what I'm sure about
and is that there is a really sharp expansion.
And, you know, to some extent that presents a problem for us academics because we have
to find reviewers for this load of like papers and so on.
But that's fine.
That's a good problem to have.
The overall outcome is positive.
I'll be at some variances.
And, you know, the growth was in a way, this was proportional because now, for the past
like four or five years, there was a huge interest.
And so we have a lot more like young talent in the field.
For the same reason, they're young and new, so we don't have as many seasoned reviewers
to value the papers for us.
So to some extent, the review quality has a little bit of variance in it compared to
few years ago.
But again, like I said, that's a good problem to have because the field is just generating
more results, so a little bit of variances, in my opinion, acceptable.
So the field is growing dramatically as our other areas in ML and AI.
What else is happening in vision?
Right.
In terms, more technically, I think, you know, we see a few trends that they are not too
exclusive to 2019, but I think some of them are like maturing up in 2019.
One metal trend that I see for sure is that we see a lot of mixes of areas like vision
plus something else, like vision pros, graphics.
I think a nominal example of that are all these like image synthesis pipelines, either
GAN-based and whatnot.
But the pipelines that essentially generate an image, that's, in a way, it's a graphics
problem because graphic is about like generating something, good looking, that you put under
your screen rendering things.
And vision was the inverse of the problem.
You already have an image and you want to understand it.
But these two areas got blended together, and I think the first time in vision that I
saw a reasonably working example was the paper picks the picks a few years ago.
And after that, it became really popular and cycle GAN and so on.
Those other papers actually came primarily from the vision community and of course the graphics
community worked a lot on it too.
So mixes of areas, vision pros, graphics, I guess we'll discuss it partially when we
get into more details.
Vision plus robotics is expanding.
I think it's one of the areas to watch for sure, vision plus like adversarial robustness
literature.
I think that's something that we, not many of us, saw it coming, but in a way it actually
makes sense that how the algorithms, basically there was a line of research going forward
on making machine learning systems more robust and that there are examples where like
poking concerns to people and turns out that if you have more robust algorithms for processing
visual data, they are more useful sort, just processing non adversarial literature and
non adversarial content as well.
Like if you have an image synthesis pipeline, it works better if it was robustified even
though they, even if you don't mess with the input anymore.
We can discuss it in more details and go forward.
But I think generally the trend of mixing different areas with vision is increasingly popular
and I think there's actually a healthy reason to this.
I think it's a realization of the fact that vision is a service to some downstream goal.
It's a very powerful skill, but we don't usually observe the world for the purpose of just
absorbing, like just understanding what's going on.
We usually have an intent in mind, like we understand the world with, you know, when I,
when I get up in the morning, when I open my eyes, I intend to get out of bed safely
and navigate myself out of the bedroom.
So the vision is a very practical skill and so we cannot really make that independent,
the research that we do on vision of these downstream skills.
So vision plus X is, to me, I see that as realization of that fact, especially in
the context of robotics.
The reason we mix vision and robotics together is that our robots need to have a complex
understanding of the world acquired through the cameras.
So whatever the vision pipeline outputs, it should be in a way curated to best support
the downstream goal of a robot.
I particularly don't care, for instance, if I have a robot in my home and it can detect
all the objects and do all sort of like complex things.
I don't really care how the vision pipeline works if the downstream goal of a robot's
whatever it is, make the bed or do laundry or whatever it that is.
If that works just fine, the vision can be as simple as it wants to be.
And so it is really an end-to-end intertwined pipeline and I'm actually happy to see that
these areas are mixing together because we can now do a more meditated design in our
research and do vision in a way that it's more useful to our downstream goals.
There are some caveats in this story, for instance, art.
When I observe a painting, I'm watching it, I'm looking at it, but I'm appreciating it.
I don't necessarily intend to do something with it, but generally speaking vision is a
very practical skill and makes of areas in my opinion is a realization of that.
Is there also an implication that vision has reached a level of maturity or meaning
the core vision tasks have reached a level of maturity or performance that we can now
even consider moving onto a real world, types of things and incorporating in these other
areas, like we've solved enough of core vision to then mix it with these other fields?
Yes and no, I would be really hesitant to say that we have solved enough core vision
problems or we have solved them like fine.
I actually think, let's say, the simplest example, probably the longest running problem
in vision is, let's say, object detection. We are not a point that we can say that we
can, with a high confidence, we can detect an object under varying lighting conditions
and different contexts and so on and so forth.
But at the same time, a huge amount of progress has been made, whether there's a way to make
them useful, I think the answer is yes, and that's why many people that are not vision
experts are actually using vision pipelines and we see APIs as well, and Microsoft or Google
their APIs where you can do somewhat niche problems, but they're sufficiently reliable, something
like face detection.
So the fact that we have APIs that non-experts can use at the command line level, that basically
means, yes, a certain level of maturity has been reached, but that does not mean that
we can solve the problems now that seemingly are simple, because we have huge problems
on that, like specifically 3D, like 3D perception, we have a lot of sensors for it and these
sensors are expensive, such as lighter and so on and so forth.
But understanding the content of an image in terms of 3D is far from like perfection,
so there's a lot more work to do.
We've got the field growing in size, we've got this mix of areas being explored, any other
general trends that you're seeing in vision?
I think in general, we see less and less fixed pattern recognition problems, like I said,
object detection or segmentation and so on, that's going forward too and that's going
forward to strong.
We see progress, but the attention and energy from what I see is shifting towards these
new horizons that are opening up right now.
And I think there's generally also interest in unsupervised or self-supervised learning
has been growing and it continued to grow and I think there were some good progress in
the past year, but that's not something that I anticipate being solved anywhere in the
near future.
And we'll continue to see that as a significant area of research.
Let's transition to some of the specific areas that you've identified to dig in deeper.
We asked you to identify just a few papers that you thought represented the kind of progress
that we've made in vision as a broad field in 2019.
You found that particularly difficult to do.
Can you talk a little bit about why and the areas that you've identified to discuss with
us?
There are a number of areas that I think it's worth discussion.
Like I said, when you have a conference with 10,000 people and that's just CVPR.
We have ICCV and ECCV as well, at least as top-tier vision conferences.
I mean, you can imagine that there's a lot of research going on, thousands of papers
coming out.
It's hard to just pick out one or two or five papers because there's just more good work
than those numbers and a handful of papers.
But I think in terms of we can summarize the trends.
One trend that I specifically see and I expect to grow is vision for robotics.
So generally interesting robotics is growing, especially in what usually to go refer to
as like robot learning these days, like reinforcement learning platforms or generally mix of learning
based robotics with the classic robotics.
And vision is part of this story.
Like robot is a large intertwined framework and it involves multiple aspects like sensory
hardware, perception, planning, control, and so on.
And one common question is like what makes vision vision like robotics robotics then?
These days, especially with the rise of robot learning where like one of the frequent claims
is learning directly from raw pixels.
So one question is like why we need like a specific vision algorithm at all?
All we can do is like learning directly from pixels.
So first like I draw the line between vision and let's say control, acknowledging that
this sort of intertwined pipeline and clearly entwent, sensory observation comes in, goes through
vision, goes through planning, goes through control, goes through hardware, and the robot
in the end does something.
So it's an end to end pipeline clearly.
But to me as a vision researcher, what makes vision research is a robot has some sensory
hardware like a camera and these sensors observe something and have some output.
And the algorithm that just processes this output and extract some useful abstractions
out of those high dimensional sensory data, that's the perception problem.
So if the processing pipeline that black box sits closer to the sensors, it's more about
understanding the world because those sensors are sensing the world.
So the prior, the type of prior is that those black boxes user about like let's say the
world is 3D or there is some dynamics in the world, there is like motion smoothness
and so on.
So perception uses these priors to to tame these high dimensional signal and then hands
that over to the rest of the pipeline, let's say planning and so on.
So the closer to the end we get, then comes the problem of control, like having these abstractions
and having planned something, you want to issue an action that the robotic hardware executes
for you.
So to me, vision researchers are in the beginning of this end to end pipeline.
They are more concerned about the world, they use the priors about the world and turn
that into processing pipelines.
And robotic researchers get these more abstractions and turn are closer to the agent, they understand
the hardware, the robot better, so they control the robot so that they execute something
that in a way that after doing multiple of these iterations, the outcome is achieved.
So like I said, this is immediately brings these multiple fields together, vision plus
robotics and it calls for us vision researchers to develop this vision box in a way that it
can support the rest of the pipeline the best.
It's not an open loop system, it's not like we receive a sensory observation like images
and produce something, an object, frames and object detection and then we just say we don't
care about what's going on after that, it is really important what comes after that.
For instance, uncertainty estimation, it's really important for control theory people.
So it makes sense that as vision researchers, we become aware of that and provide some form
of uncertainty estimation that is tied with our detection and so on and so forth.
So I think there are a number of clear developments in the field acknowledging this vision for
robotics type of research.
You drew a distinction early on in this between, and I want to get this distinction, I don't
think I'm going to get this distinction correct, so please correct me, but it was something
along the lines of vision versus learning from pixels.
Right, well vision is learning from pixels, I see, so I understand, so yeah, so there's
actually tabular also learning, it's a common word, it's not actually new, it's called
like, I think it's Latin, it means like clean slate, so it's a system that let's say a robot
that does tabular also learning in terms of vision, what it does is that, okay, I have the
sensory output just raw pixels, and I'm going to learn directly from raw pixels, let's
say, for instance, using a model free enforcement learning policy, and then you define your goal
in terms of some reward function that rewards you when you do something right and penalizes
you when you don't do that right, and then you hope that by direct interaction with the
world, many, many, many data points, the system will learn how to do the job right, and that's
generally what we see in the enforcement learning literature, especially like in model free
enforcement learning, but there, the distinction is that learning directly from raw pixels,
basically means the state of the world are these raw pixels, and by the way, I'm generalizing
a lot here to get the general concept right, what vision is about when we are processing
these raw pixels, instead of just using them raw, viewing them as like a 2D matrix that
is coming from the camera, so that's what we know, vision is about like having some
priors about the world, and instead of using this signal raw, we use those priors to extract
some abstractions that are easier to understand, they're more interpretable, they're more efficient
and so on, so an example, like I said, is the fact that the world is 3D, so when you look
at an image, it's a projection 2D projection of a 3D world onto 2D plane, so it basically
loses the 3D information, so if a robot is trying to make use of this 2D projection,
it has to somehow recover that information, so either we hope that by these like millions
and millions and millions of interaction, this robot actually understands 3D reconstruction
to some extent at least to be able to solve the problem, because we know that some of
these problems, especially in navigation, they need 3D perception, like you need to know
how far the obstacle is to be able to avoid it, so there's no escape from that problem,
at least at the course level, so either the hope is that by not providing these priors
directly, and learning directly from pixels, the system by many millions of interactions
it learns that, or some people like vision researchers, they use these priors about the
world, they extract these abstractions out of their off pixels, and that's what the
rest of the system brings them to their enforcement and learn policy uses, so you can learn
them, you can view basic vision as identifying these like facts of the world, the fact that
the world is 3D, the fact that there's like motion and smoothness, turn them into processing
pipelines, so when the raw image comes in, it first goes through these abstractions, and
then those abstractions are what the rest of the system, like the robotic system sees,
so they're more interpretable, they're easier to tame, and you don't have to like redo
this process every time that you turn a robot on during the learning phase, because these
are just facts about the world that are generally true, it doesn't matter whether you're navigating
or you're manipulating or you're finding an object or you're just going to GPS coordinate,
you need some understanding of about 3D from the world, so it makes sense to turn that
into more like the standalone problem where vision people solve and then plug that into
the bigger pipeline.
Now, when I talk to folks that are coming from the opposite direction or the other direction,
the pixel-based learning and reinforcement learning, one of the things that they say
is hot this year is model-based reinforcement learning, for example, where they're trying
to learn a higher level model in the process of learning from the pixels to do some of
the things that you're describing, and I guess the question that I'm asking is, does this
mean that vision and the pixel-based approaches are converging or is it saying something else?
So those models in the model-based RL, are they the same kind of abstractions that you're
describing that are core to vision?
Right, I mean, model-based RL or generally model-based everything, it's too general to the
point that it can include anything in everything, so for instance, model-based, yeah, you can't
have a model-based on the 3D of the world and that would become model-based something,
so this doesn't necessarily mean there is a completely orthogonal approach to solving
this problem that is going to replace vision or anything, generally when you have a model
you're encoding some priors, either you're learning these priors or you're encoding them
into the system, and like I said vision is more or less about that too, we define some abstractions
that we believe are generally true about the world and we focus on solving those, up until
a few years ago we were doing it completely independently of who uses the output like a robot,
but now, like I said, with these areas being mixed, we are developing them in a more end-to-end
manner, with the awareness that we need to develop them in a way that they're best useful to
the rest of the pipeline, but yeah, in the end it's all the model, and there are people
call different things in the model, there's like dynamics model, which is about prediction
of the outcome of a certain action, so that helps with efficiency and so on, but for generally
being able to solve this problem, you need to have a model and we are all in the business
of developing that. So what are some of the specific papers that kind of exemplify this trend for
you? Yeah, so for vision for robotics, like I said, the vision community primarily focused on
the navigation, there are good reasons for that again, I think it's one of the most important
things that are probably enlightening here is to look at the data, like where we get the data
for for being able to solve, develop a vision model for an agent that is active in the world,
like a robot that does navigation. So it's a big question, it's a very different from say
an offline data set that sits in the hard drive of a computer, like I mentioned that, you take a
picture once and that's it, you're annotated, you have no control over this pixel any longer,
you cannot say like let me move a little bit in this image, like let me look at the same object
from left or right and so on. So this pixel is like pre-recorded and is offline basically.
Now by definition, active agent has some degree of freedom, so it can do something,
it can move around for instance if it's an navigation agent. So there's a gap between
like static and offline data sets and the type of data that's developing vision for robotics
and balls. We basically need to have some online pipeline for generating data and that's usually
why people use simulators. Again, till a couple of years ago, the simulators were primarily based on
synthetic data. These are synthetic data meaning that a designer would sit down and model,
let's say an apartment for you by putting together some cat models of chairs and tables and so on
and they would be a computer graphics pipeline that renders that into pixels for you and we would
use that as a source of data and that would solve the problem of being active because of course,
you can just shift a little bit in this scene and re-render it. So it would give the possibility
of a degree of freedom to the agent that is using the data but the main problem was that the data
was not done from the real world. So there would be no guarantee that this would generalize and
generally speaking since computer graphics is not completely solved yet. So the type of pixels
that we get as a result of this are not fully photorealistic and even if they were photorealistic,
the underlying semantics are coming from a designer and the designers have biases too.
So if you look at the, let's say some of these models that existed in such data sets like Sun
CG and so on, it's the moment you look at an image it's immediately clear that they are coming from
a simulator they're not from the real world and what gives it out is not just the fact that the
pixels are not like photorealistic is the fact that these models are usually too clean. No designer
would sit down and design say a very messy bedroom and so on and so forth. So there was a
there was a photorealism gap but there was a bigger actually semantic gap. So since a couple of
years ago what changed and I think in my opinion was actually a big change in the community was
that data sets came out that they're based on scans of real world buildings and then see if
here are 16 we have one paper and called building parser does that I think I believe the first time
that multiple large buildings were scanned using like commercial scanning pipelines in full in 3D.
So you would have a mesh of one building I remember at that time you had I think six buildings
of a Stanford scan so you could load the mesh in your computer and look at it and it's the entire
building is at your disposal. So that later became a underlying model for many simulators and now
the simulators instead of using the the designer to develop things or design things for you now
there's actually a real building that is serving as the underlying data and so multiple datasets
came out of that one of them is called like Stanford 2D 3DS Matterport 3D and Facebook replica
and so on there was actually the last one in this time wise in Gibson in 2019 there was a
2018 and so if you are we have the paper and Gibson that had brought that to really larger scale
there was about 600 buildings that they were scanned. So your robot can virtually visit 600
buildings interact with it in as far as navigation is involved of course and and learn from it and
sings 100 is a lot if you go to a new building every week and visit all corners of it because
these are scanners like scan everything if you go to building a new building every week it takes 10
years to get to 600 buildings so there's a lot of visual data certainly more than what like humans
probably observe by the end of age two that's where like vision is sufficiently developed so
that wasn't excused anymore essentially lack of data. Given the the constraint that you mentioned
on you know how long it takes to scan these buildings was the the data crowdsourced?
Yes to to some extent we we spoke with we had like hammer man some of them we would send them
to actually scans buildings for us some of them would have a scans already so it would acquire it
from them but yeah we didn't actually scan them it was like people's that would scan them for
different reasons then we would acquire them and there's actually since the scanning pipeline
series scanning pipelines are now sufficiently mature there is actually a support chain for them
primarily coming from real estate market you know most of the I'm not actually sure if it's
most of the houses but a good percentage of the houses if you want to sell them well you have
to scan them in 3D so you can put up a website for them so the buyer can navigate in it before
they actually come see it and so on so that's actually a good resource of data for us and that
actually created a supply chain of camera man that you can probably any city at least in the
United States and in Canada you can hire a camera man that already has the camera and they can
go with scan and within a few hours they can send them all this habitat competition this is a
benchmark that Facebook is proposing for agents that are navigating spaces can we talk about what
the kind of how do they quantify performance in this environment right yeah it's built
Facebook has a team of software engineers and researchers that developed a habitat platform
like you said to a large extent it was developed on top of Gibson and a few other works that
exist in the community like it said in the past like Minos and so on so the way it works in general
is that the tasks are specific like point math or point navigation the agent is dropped
in a new building completely unseen and it's provided at a random location in a new building
and it's provided with a coordinate to go to so the agent could be randomly spawn in a bedroom
and the coordinate that it's provided as the goal location is somewhere in the living room
and all it sees is a stream of RGB or RGB data and it has to now plan its way around the obstacles
and safely navigate itself to that particular location which could be like tens of meters away
so it's a hard problem to solve especially that this is a new building it's not like you could
spend time scan this building something like a slam pipeline and then run ASDAR and something
like that to plan like post plan trajectory is just like it like imagine as a human when you go to
your friends house when they just bought a house they have never been there and once you enter
the living room I want to use the bathroom you can probably think about okay the bathroom would be
probably I need to be looking for some hallways or doors and maybe a little bit of search
you would find your way to the bathroom but you would not be just randomly wandering around
this building for hours and hours till maybe by chance you would find a bathroom
right so so these agents actually the task is something similar to that completely unseen
building a spawn in a random location and provided with a target coordinate just travel to it so
that was the first competition of habitat in 2019 and there actually a lot of entries I don't know
the exact number but they were enough to actually make a good competition there were two tracks
and they were not about the task but they were about the type of data so if the agent all it sees
is RGB that would be akin to having just an RGB camera on a real robot that would be RGB only
track and there was another track with this RGBD that would be like having a 3D sensor to like connect
now we are at a point that we have a challenge with like tens of teams enter and there is a good
amount of energy in the community that is spent in this area so it's to me it's actually a very
healthy progress towards vision plus robotics in this context and this like I said the robots are
limited to navigation now and the reason for it is that our data platforms right now can support
only navigation not manipulation technically speaking it's hard to scan a building and now go make
a change in it and be able to render it these buildings are scanned aesthetically so we don't have
a good like support pipeline yet as of now for capturing both dynamic content and interactive
content there's work going in this direction the community too but I think it's I would summarize
them to be still in a scouting stage and they're in fancy so we'll see how that plays out in a few
years but navigation is something that you know it's a reasonably stable we have good data platform
um for it and we can look at the output and and to be honest like I was personally impressed
by the the performance of the the winning teams in both tracks it's it's it's actually a hard problem
to drop an agent in an unseen building and give it a goal that is like tens of meters away
navigating the path is hard because you need to identify a lot of things you need to know for
instance where the doorway is to get yourself out of the room and the woman you're out of the room
you don't want to hit like the walls or many obstacles in the way and then maybe the target is behind
let's say a chair and so on so there's a lot of like a fine-grained planning that goes into successfully
doing that but the success rates are actually higher than what I expected so hopefully in the next
year we'll see this becoming more mature and the task of force need to be more realistic
um point navigation is is probably the simplest one uh we need to be able to navigate towards
uh like semantic network towards and that semantic navigation let's say if you task an agent with
finding a key the key doesn't have a specific coordinate it can be anywhere but it's not in arbitrary
locations too are there any other uh kind of highlights uh in the vision for robotics
yeah I think I think I think this was habit that was uh and and Gibson and so on
were actually a good uh representative of the progress uh in terms of the discussion that
we had earlier is uh like if you can learn everything from raw pixels what we didn't need for like
vision uh uh they were actually multiple papers that came out in this area that they had
focused the studies on why actually it is critical to have vision pipelines when you're trying to
do robotic tasks uh one of them was uh a paper that we did um um we published in um last year I believe
in December 2018 on archive it's called mid-level vision representations improve generalization
sample efficiency of vision water policies and an update that we published in in coral uh in
November that's basically uh mid-level vision for uh for navigation and there's also another paper
that had a very similar flavor concurrently came out that uh was actually published in um um
I don't remember actually living in an an aerobotic strontal and that how it actually does
combat a vision matter for action which is a very direct statement of of the question and the
the conclusion to both of these is that yes it's really important and critical to have vision
pipelines and the and the reason is again those priors if you don't supply your um your robotic
pipeline with these priors about the world like the world is treaty there are objects and there's
permanence and so on and so forth yes there's a way that they can learn it but it will take
tremendous amount of data like millions and millions of interactions to recover those facts
so either we we don't provide those priors and we directly use raw pixels and but the
consequence of that is being inefficiency as we know let's say tabularasa or l is very inefficient
that's why most of the time it's focused on simulators because you cannot do as much interaction
with the real world or it will take years or we make ourselves prone to not generalization
you could solve the problem for limited space for where you did learning that's when the algorithms
find the shortcuts but then the moment you go to a new building because the same shortcuts that
they used for learning there was based on bicep data so they wouldn't generalize so both of these
papers and they strongly showed that supplying these priors which is the job of computer vision
significantly improves efficiency so you can learn faster with less data and also it improves
generalization so what you learn in one building it won't be specific that one building anymore
so I think this was actually a very strong conclusion and in a way response to to to demand for a
study that it is really important for robotic pipelines to to use like vision algorithms and priors
so I think those that should basically summarize are at least in my opinion the important
progress in vision for robotics in past year the next area you had in mind was 3d vision which
we've actually talked quite a bit about in the robotics context yes exactly so I think that the
motivation is clear the world is especially 3d but when we look at an image we are looking at
actually a 2d projection of it so basically that calls for the problem of recovering the 3d
structure as a human you understand the world in 3d just because the retina in our eyes receives
a 2d projection that doesn't mean we are unable to recover that underlying 3d but we do that through
multiple mechanisms such as a stereo we have eyes so a stereo is a solution to recovering 3d but
even when you cover one of your eyes you can still see the world in 3d basically it means some
brick ignition or learning based is in process so we need so recovering this 3d
structure from a 2d projection such as an image is a strong problem in vision community especially
in the past year I think I see a rise I think that was a demonstrated by the fact that the
paper award nominations and both to see if you are an ICCV include actually papers and 3d
and also I think the problem has actually another angle to and 3d vision now we have 3d sensors
because 3d is important we need processing pipelines for this 3d data so this is different from
the first problem they mentioned images 2d you want to recover 3d and then you will do something
with that recovered 3d sometimes we have the 3d from a sensor like a lighter or connect and so on
but the processing pipeline that views for images do not directly work on 3d data
they either work or they are very inefficient or they just don't produce as much good results so
that's actually the second category in 3d vision having pipelines for processing 3d data
both of them I believe they expand it notably in 2019 and I see a rising trend over there so
I expect to see more and more of it in the years to come I think they're like in the first
like we can actually maybe talk about a specific example in each of these categories does that sound
to okay yeah absolutely yeah so I think in terms of like generally when we talk about the first
category of problems recovering 3d from a 2d image first question is that where we get the data
you have to have an image and have the underlying 3d data if you want to do it in a
so police supervise learning manner to all to learn let's say your network that receives an image
and a spit out the 3d so where we get the data so 3d sensors are are there but they're less common
than RGB so we have a lot more RGB content than RGB plus 3d out there it work around that is common
and now is that people get RGB data let's say a YouTube video or multi-view cameras and then
recover 3d using like classic methods like the structure from ocean slam and so on and then
once that is recovered using classic methods they use the recovered 3d as a supervise supervision
for doing RGB to 3d from like individual frames like monocular that has been working for a few
years and that has been shown like effective to a reasonable extent one thing that makes that
hard is dynamic content because like those classic 3d reconstruction methods usually work based
on point correspondences and then you have moving objects into scene it becomes basically an
ill-posed problem is that is it a treaty of the scene that is governing this motion or is
actually there's like let's say a human that is moving in the scene and so on so and humans are
actually specifically an important part because humans are a very important object in the world in
general so it was one paper that I personally found like pretty cute and past year it was learning
the depth of moving people by watching frozen people so it was it had a very interesting like
workaround there was the mannequin challenge that became popular if you remember it was about like
people just standing stationary for a period of time and somebody would film them so that is
actually interesting because they did a lot of data collection for us implicitly so people
just stood the stationary and somebody walked between them so we actually had they collected a
snapshot of the world where it was frozen and people were specifically there because it was
mannequin challenge so there was a moving camera but no dynamic content but whereas the rest of
the times it's really hard if you think about it to find people when they are stationary and then
they don't move unless they are sleeping when somebody is awake there's usually some at least
micro motion so mannequin challenge was great that turned into a source of data that this paper used
so now they they apply the methods that would not probably normally work for dynamic content
in people and the mannequin challenge data and then use that as supervision for now they can do
depth of moving people whereas they actually learned it on frozen people so they kind of tricked
the system by learning from frozen people but when you when you do it framed by frame then it
basically doesn't know that the person is moving so that that was an interesting so an interesting
paper and I think it was a representation of the fact that still when we are doing 3D vision
data is a big problem there and this paper actually addressed the data by but it's nice
interesting challenge to happen past year yeah it's a fairly it's quite creative but it also
breaks the question you know for you know where will we get the data sets required to generalize
this kind of approach right yeah so like I said there are aspects of the world that are static
like I said for instance like in Gibson data these are human like primarily these are like
residential places and and people just go and scan them and by default there's little motion
unless there's a human in the same so humans very often are one of the main sources of like
motion and there's some other forms of like motion too like if there's a fan on the ceiling that
is like rotating there's no human there but it's there's motion there so but they're less common
so and also like I said humans are around the most important objects in the world so it makes
sense to actually have processing pipelines that that solve them and then we'll take it from there
yes I guess we need to be either more creative or we can hope for algorithms that come out and
handle the dynamic create a content better to be able to generalize it to to the more general setting
yeah and and specific to this model is the idea that the model that's created with this technique
could be kind of pulled out and used in a transfer learning kind of fashion and other pipelines
or that you would incorporate this process into other pipelines and or none of the above
is it just more of a proof of concept well I think the transfer in a way that happened was
transferred from frozen people to moving people so the data was purely from mannequin challenge
in which nobody's moving right by by the definition of mannequin challenge but once you learn from
there that doesn't mean that when you're using the learn the learn model it has to be applied on
on frozen people too so think about it this way what they let's say if you learn a frame by frame
processing pipeline out of the mannequin challenge so it receives one image of a human
and it can recover the 3D now during the actual data in the in the data set the people are frozen
so five five frames in a row the person is frozen so it's not moving but it's a frame by frame
process processing the frame first frame is independent of the second frame and third frame
so at the test time you can actually apply it on a video where people are moving and it can just
perfectly find recovered the 3D because it basically bridges no connection between the first
frame and second frame so you can you can now apply that on moving people actually they did
show it in the paper that they learn on frozen people and they applied and moving people
and that's the consequence of the fact that you know whenever you do say single frame processes
you can the frames are independent that's actually an old trick you have we have seen it before
in many papers and it goes from there so so that's the type of transfer I'm not aware of any sort of
like other transfer learning being in play here but if we have a pipeline that it can recover
the 3D of people reliably well I think that itself is a pretty important problem and worth
worth attention so that was the on an example from recovering 3D from 2D images like I said the
other batch of problems and 3D is processing the data that is 3D let's say an output of a light
or sensor or connect let's say to extract semantics out of it you can do object detection given
an image or you can do object detection given a lighter scan and you'd hope that if you have
something like lighter you would have more information than what an image gives you so you
should be able to do a better job so that has been progress in general on this front to in in
terms of efficiency and accuracy of extracting semantics given 3D data whether it's lighter
or or connect and whatnot there are multiple papers actually there's I believe a Stanford
there's a pipeline it's called Minkowski engine that is focused on an efficient 3D processing
there's point that too that came a couple of years ago but it has been like matured since then
a few months ago in an ICCV and so there was this paper on deep half voting for 3D object
detection in point cloud that's actually another representative I believe it was one of the
award nominations too so 3D sensors typically give you something like a point cloud and then you
want to do various type of semantic extractions on top of that an object detection is probably
most nominal example you want to put a bounding box around the objects that you see and so this
paper actually had some pretty good results on that and the idea in general was and voting so
when you look at let's say the point cloud is as a name says the cloud of 3D points and each
point can vote for what kind of object that it belongs to like all the points that belong to a
chair that can vote for for being a chair and for like some parametrized like our president
of points let's say a center of the chair and um this paper actually used this voting pipeline
and a voting voting concept and report actually good results in terms of object detection something
similar to that actually happened in 2D sector 2 I believe it was called center net
on something or something similar to that that they're like pixels and an image vote for the center
of the object that they belong to so that's in a way it's different from the the the preceding
angle that was basically labeling each pixels and its own like such as in segmentation and so on
or finding finding exact like it's it wouldn't be a voting pipeline that's that's a distinction
between them so um I think that was also just just to have a nominal example of papers that do 3D
processing for extracting semantics and I and I see that um to be again a rising movement
to develop more efficient pipelines with an vision community for recovering 3D or processing 3D
and you know doing more work rounds um for lack of data there let's move on to the next category
now they're area that I I believe in 2019 continued to observe progress with self supervised learning
in general we can I think summarize that things are working sufficiently where for fixed mapping
problems if you have enough data meaning that if you have an image you want to like extract something
out of it let's say objects and so on if you have enough data it's a matter of probably designing
your architecture and struggling with with hybrid parameters a little bit and training it for long
enough you'll get some sufficient results the question that have been concerning a lot of vision
researchers several since like several years ago after deep learning probably immediately after
deep learning way it came was that whatever you're going to do if you don't have enough data
and that gave rise to multiple research directions um self-suppose learning being one of them
and I think the general uh the general question and the the goal there is that you want to
rely on less and less on human labeled data you have a lot of data raw data in hand
but just rely less and less on human labeled data and use raw data to identify trends in there
and then kick a start your um learning pipeline to the point that the the the reliance on
and human labels is just only for the last mile and perhaps exceptions so self-suppose learning
has been going forward for a number of years um that you can actually look at the workshops
within vision and machine learning community um to and you'll see that every year there's actually
a popular workshop in that um in ICML last year in 2019 the we also had this workshop on on
self-suppose learning hopefully we'll have it again in ICML 2020 in CBPR 18 uh we had the beyond
supervised so there's a lot of basically focus on this problem in the community and a lot of
good researchers have focused on it um so it's not as specific to 2019 as a trend but in 2019 I think
it was for the first time that we saw that uh the image net problem essentially being solved
with less labels or uh it was to be more specific it was the first time that without using labels a
self-supervised pipeline was able to achieve as uh good results that uh uh data as a
fully supervised pipeline would receive and that was actually a good point um I believe there were
like two or three papers that reported that uh success the one that I particularly liked was the
there's a paper based on contrastive predicting coding um so this paper the contrastive
predicting coding that the title of the paper is data-efficient image recognition with contrastive
predictive coding um so the contrastive predicting coding is not a new idea it has existed
existed before but I think they really rendered it into a mature pipeline to the point that
it's stable now and it can at as far as this image net data set is concerned you can actually
get good image net classification results without having to use the image net labels which is
uh which is actually interesting and a good proxy for for progress here the contrastive
predicting coding is one of the ideas that have worked towards this purpose is actually not a complex
concept in general you can let's say you you you learn the regularities in the visual world to be
able to learn good features out of it let's say if you start if I show you the like top half of an
image you will have an idea of what the bottom half would be like if in the top half if you see
like a head of a dog probably you would say in the bottom half there would be the body of a dog
so now when would you be able to make that prediction he would be able to make that prediction when
you know how the world looks like you know that dogs are not just these like heads that are like
floating in the air right there's usually a body to support it so so the raw image if you have a
lot of them is enough for for learning that so so that's basically uh like in a very simplified
way the idea behind like predictive coding you show part of the data and you learn your system
to predict the rest of the data and this requires no labeling because the data is like raw and
available to you you just mask it during learning and you get the on your network to predict the
mask part so that this turns out to be actually a good way of learning features and the paper that
I mentioned actually uses this idea in a more advanced way to not rely on human defined labels
that's in contrast to let's say something like AlexNet that basically just started from images
from the iteration zero of learning it would map pixels into human defined labels human
human defined labels of course those thousand classes but actually in like every a single
instance of an image was labeled by playing annotator so um so that was actually there I think that
what you need to be aware of for the year to come is that this was shown for image net and this
does not mean that we can do this for all problems and vision and under all settings so the caveat
essentially is that this is a specific setup there's a certain architecture there's a linear
layer and so on this doesn't mean that this will continue to happen regardless of these design choices
and uh but still we didn't have this even even this prior to this year so I think it's
that we can there's something to celebrate here and the more importantly uh even if it was
architecture agnostic this is about image net and image net is not the world it's very niche part
of the world it's a very dog biased part of the world because there are like many dog classes
in there and then for instance from other species and animals is like a less of them
so of course this is about the bias that the design of any dataset involves um the fact that we
showed this on as a community on image net that doesn't mean that we can show it for everything
so we can we can call it a success when for any image recognition problem we would have a
self-supervised learning pipeline in which we would not rely have to rely on human labels too much
and be able to solve that problem let's say if somebody does that for a depth estimation
or object segmentation and so on then it would be a lot more convincing but that was definitely
a progress for 2019 and so's with us learning too now we've seen uh uh
2019 and and 2018 as well has brought um self-supervised learning to the fore on the NLP
uh within the realm of NLP in a big way with models like Bert and Elmo and others you know
is there a relationship between the you know this being a focus area vision in the vision space
do you think you think that we're kind of pulling from you know inspiration from you know one to
the other no no it's actually a great point it's both ways um like predictive coding in a way
actually has similarities to the models that that work well in NLP too so um like a language model
in general is something similar to that like you give the beginning of of a sentence where
there is character words but you give it the beginning and it turns to send to predict what comes
next and it can push it really it's a not just the next word that comes but many words that
that come after that or you can turn it into like fill in the blank problem so it's all about like
masking part of the data and filling it in whether it's the next part of the data or some like
proceeding part of data um but the the common just here is that uh we use raw data to be able to
we define a problem that is primarily based on masking part of the data and get the system to learn
to to fill that gap and that is just a good feature and then what comes out of this like
pipeline is a feature whether it's learned on text yeah it becomes something like
birthed if it's learned on images it becomes something like the um predictive coding pipeline
that I mentioned earlier and they become good features that are just sufficiently at least
as for us let's say for images uh image that was involved sufficiently universal so could you
use those features to to read different things out of it such as like image not classification
so yeah there are similarities actually and I see that going both ways there are many people
that that that work on both um the self-survised like models for both image and and text
another category I think is worth mentioning um and it it's uh it's interesting to me the connection
between either cellular postmas um research and and vision so um it's basically an interesting
finding so in general the robustness issue is that you know you would have an image recognition
pipeline let's say object detection and turns out that if you can make very small changes in the
input uh something that humans even won't see like adding a few pixels or making a change like
very tiny tiny changes in the pixels there's a way to actually mess with the uh object detection
pipeline uh whereas the humans wouldn't see that change at all so that kind of made uh um
it raised a big question and it made a lot of people concerned also it's it's
besides like the concerns and safety and so on uh it was actually an interesting question
intellectually that why is it that uh image recognition pipeline that seemed to be working well
there's a there's such a big loophole that we can mess with the pixels in a very imperceptible
way um that that uh changes the output such drastically um so there has been progress in in the
in the community of adversarial robustness community um towards like methods that just
robustifies the neural networks we respect to these changes um a lot of work came out of Alexander
Matthews group at MIT and they showed actually good progress towards like high frequency adversarial
patterns and they basically train the augment the the the training of a system in a way that
the the outcome is in vain and respect to that kind of knowns is and then therefore you would
expect more robustness and indeed the system does become more robust at least with respect to that
particular adversarial pattern now I think what was interesting uh in retrospect it's common sense
but it it had to be shown that if you robustify every image recognition pipeline you would expect
it to have a better understanding of the world and therefore it should actually work even better
for applications that are not within the context of adversarial robustness so let's say if you
um if you have a image recognition pipeline that with a little bit of changes in the pixels around
the dog it would just misclassify a dog for an ostrich and it just basically means it didn't
quite understand what a dog is um so it was prone to these kind of mistakes now if you have a robust
system that it doesn't make that system anymore it means it better understood what a dog is so
it better understood the manifold of real world images and what's possible in the real world and not
so those this paper actually in Europe's just uh two weeks ago um from Alexander Matri's group that
basically showed that if you have a robust image classifier image classifier that is like trained
with this robustness mechanism you can use it to synthesize better looking images compared to
the same exact classifier just without being trained with uh with adversarial robustness the title
paper I believe was image synthesis with a single robust classifier and they basically
show it that uh side by side you can look at uh synthesized image coming out of a neural network
you know in image synthesis pipeline we always use some sort of pre-trained neural network
an image network and what not uh there's a requirement um um in the in the system so for that
uh pre-trained neural network if you use a robust classifier versus a non very same exact data same
exact architecture the robust classifier just renders much better looking uh images for various
kind of synthesis problems like in painting or transferring sketches to images or super
resolution and so on so this was an interesting uh finding it like I said it makes sense because of
force and more robustness I mean to understood you would hope that you'd understood the uh uh the
manifold of real world images better so therefore you should be able to do better synthesis too
but you finally have a paper that that actually showed that too which I personally find interesting
and it's another example of the uh mix of different areas with the envision and this time
adversarial about its next research the the generalization result makes me think a little bit of
the parallel to multi-task learning where we've seen over the past you know relatively recently
I think past year or two a lot of work's been going into multi-task learning that showed that
just the addition of another task and the training process helps the networks generalize and
perform better in a sense often in this uh in the uh adversarial uh robustness research
the robustification of the network is another task and so these are kind of similar results
or or related potentially results right to to some extent that's true like essentially the
robustness is as a result of augmenting the loss with the additional terms that you wouldn't
normally have and you can view as multi-task learning uh versus single-task learning as augmenting
the loss of a single task framework with more losses like more tasks that it would normally have
so there's a there's that like regularization effect that uh though I have to say that in multi-task
learning that observation has been challenged quite few in quite few different settings it
really depends for instance what task that you learn together to be able to uh to be able to
actually see the benefits of multi-task learning um uh ironically actually the publish one paper
on this area which is exactly called which tasks should be learned together in multi-task learning
and that that the the title says at all that there's a question there to ask like it's just being
multi-task is not necessarily better uh there's like multiple other parameters in play such as what
tasks should be learned together and there's also like the uh an equal amount of research going into
let's say even if you know what tasks that you want to learn together what the architecture should be
how the sharing exactly should happen should it be like self-parameter sharing hard parameter sharing
some sort of progressive sharing where you decide what parts of the parameters and layers should
be shared and which parts should be dedicated to to networks to to different tasks so um to be able
to get the benefits of multi-task learning um I personally believe that a lot more research has to
be done and the examples that we see are probably when we got lucky and they worked out doesn't
mean we completely understood the problem yet so you know there you have we've kind of gone through
these four key areas you see us making some advances uh in the vision field in 2019 and those are
primarily from kind of the academic perspective how you seeing these trends play out in the real world
yeah I mean uh I think if by the real world you mean like who has used them or in terms of like
commercials um I think probably I see two areas um image synthesis like I said it became uh
it entered the new maturity level like we see apps that come out to actually use these kind of
learning-based synthesis that was a thing a child of the marriage between vision community and graphics
community and these apps that like transfer you from like young to old or like smiley to not
smiling and so on so forth and then there's uh companies like Nvidia I know that they're active
on that and it had the consequences of uh you know the photo fakery concerns like deep fakes and so on
that if things become so well then how do you know when something is real and when it is not
but it's safe to say that that technology has been proven effective and has been adopted
so that's one area that I see um it it entered a maturity level that was uh uh commercialized
and also used by ordinary people and their phones and so on um another interesting area
continues to be autonomous driving um um again a car an autonomous car is actually a robot it's a
it's a big robot it has sensors it has perception that's planning is control though it's a specific
one it's one for navigation designed to move on the roads that you know marked up and so on so
forth so in a way um um it's an example of like robotics vision plus robotics and and again I'm
as commenting and the perception aspect because there's a lot of things that going to getting
something like autonomous driving um car to work um uh we sell progress um now tesla continues
to release new features like a smart summon that you can call the car to to come to you
um in a parking lot and the waymo I heard um during nerfs that they just released
a an app that works in Arizona then it can actually order caps that are completely autonomous
there's no no person in the cap from what I know I haven't used it I haven't been to Arizona
since I heard this but I'm looking forward to doing it um so waymo did that and it's actually
find it impressive you'll see how it it plays out but it can actually have these autonomous caps now
um and different companies have different approaches like you know we know the tesla is um
you know Elon Musk and and he has been vocal against like using heavy 3d sensors like LiDAR
waymo has the safety first approach even at the expense of like coming having more sensors on it
so different companies have different approaches and like I said we have seen progress
such as the waymo release or tesla features but one summary I have is that again the last mile
has been proven to be more difficult than the optimistic anticipations like a few years ago and
many people had like predicted this like rod brooks and so on um if you look at the predictions about
like when autonomous driving would be here let's say five years ago or three years ago let's say
if you would hear by 2020 it would be uh available but it was then then 2018 and 19 they would
push it like yeah a little bit more into the future like five years and you know in 2019 we heard
this question uh multiple time answered but we don't know exactly when it will be here which is
representation of the fact that again the last mile has been like proven to be harder than the
optimistic anticipations but um but I'm watching it actually carefully because it's an interesting
area there's a lot of investment in it there's a lot of talents going in it um so if something
wants to work I believe autonomous driving will be one of the first examples of as the application
of say vision plus AI machine learning and so on and uh so we'll see how it goes do you think the
last mile problem applies specifically or only to autonomous driving whereas uh do we see this
generally across uh bringing research-based uh innovations in the vision domain into real world
applicants? No I absolutely see it as uh as a latter it's not about autonomous driving anymore
any any real world system has a lot of unanticipated issues that you probably won't even think
of before you reach there and um many of these pipelines that we have right now they have caveats
let's say if you have a sensor that it it works fine it might continue to work fine during summer
but during winter when it is snowed how do you know um how it's going to react or like let's say
something like autonomous driving or even like say indoors navigation if you have a robot home
that like navigates safely from one point to another let's say when that happens for household
robotics? I'm pretty sure that you would be surprised by um by the way the system works when
you take it to a different country when the houses look different or things that are that are okay
here they're not okay there and and so on so forth so now I definitely believe that any kind of
real world like adoption of of this research that we're doing especially in terms of machine
learning where there's a factor of uninterpretability as of now um the last mile will be proven
proven harder so let's shift gears from looking backwards into 2019 to looking forwards to
the future maybe give us your top predictions for computer vision for uh 2020 or or beyond I've
been challenging the folks that I've been doing are AR rewind series this year since we're at the
end of a decade to project uh a decade to the future and no one takes me up on it. The problem
with making predictions is that there's a pretty good chance that um for turning out to be wrong
like if if a decade ago you asked anybody about you turning I'm pretty sure the chance of that being
the prediction um would be even though it just like it's in 2010 it just happened two years after
that in in in in computer vision but yeah I don't think I'm not sure if anybody saw that coming
but maybe some people say they saw it coming but they were probably very very small percentage
but um I think you know I can extrapolate a little bit at least if I don't even make you know
two controversial like predictions one thing that I um in terms of since we just managed
the commercial discussion I think at least I can say that from the commercial development perspective
I'll continue to watch autonomous driving progress I really view that as a good proxy for
progress in a well-focused and well-invested area so lack of talent lack of data lack of money
is not really an excuse so it we are really down to solving the problem the physical problem over there
and and and one of the other reasons that I particularly watch autonomous driving uh
progress is that perception wise ironically autonomous driving to some extent is is a simplified
case compared to the general perception case there are there are complications but there's
the reason it's simplified compared to general perception is that like you know autonomous driving
they're like lane markers and there's signs and there's some code of conduct and there's a lot of
data um so the lane markers are you know designed to tell you where you are there as like when you're
just walking in in the woods there's no lane marker so it's the same navigation problem but you
have to it's the lost kind of a lot less information there's a lot less sort of design ahead of time
that's gone into making the problem perceptually simpler so if if we can solve let's say a lane
detector for autonomous driving a big problem is solved like for a general say navigation problem
in unstructured setting there's no lane to detect so you have to really do something more than that
so in some senses in some senses autonomous driving's perception is simplified so I'm hoping to first
see the simplified problem solved before we even like dream of solving the more uh complex and
unstructured problems but of course there's like like other big issues over there like
there's safety risk the speed that's autonomous driving is high that basically means you have very
limited amount of time for making decisions you have to make make decisions like way ahead of time
compared to the normal setting when the speed is like uh low and then sensitivity is high because
there's like a risk of fatal accidents and varying conditions cars go everywhere and like during
the night during the day during the snow foggy and so on so forth and sensors get constantly
interfered by sun and whatnot then you're capped by the price um so there is like that's not to
basically play down to complications but at least let's say purely focusing in detection aspect
there are some things in autonomous driving that are simplified so I continue as a vision researcher
to watch autonomous driving progress and to see where that goes and when it is that you're going
to finally seal it and say that you safely have autonomous driving system and uh so that's a good
proxie proxie proxie for me that I watch another aspect that I think it's outside this niche
like more specific applications like autonomous driving if we like they're yeah they're like other
applications that we could watch there's actually a lot of focus on like a startups and robotics
that of course they need to solve vision to some of them are focused on warehouses some of them
focus on indoor spaces but if you want to think about like more like general settings
about vision commercially speaking I look forward to more like democratization of vision pipeline
so there are APIs right now so it's a very democratic way of of using vision like if you're
somebody that has no expertise in vision you can actually use Microsoft Azure or Google Cloud
to do certain tasks like face detection or some object detection but compared to general
what vision can do that's a very small part of the potential so another good proxy for progress
is that how much these democratic tools uh vision how far they go how much you go beyond like face
detection and and so on and like I said there's a good reason that these APIs have these now because
they are the things that work good then many other things that we have they just don't work
good enough to be at the API level and that's why I'm saying like the more I see
brought to the API level that works reliably and sufficiently well that's a good progress
proxy for our progress that we can see and examples are like 3D again
there's no good API at least from what I know that would recover the underlying 3D structure
from from given images for you there's like are you predicting that for 2020?
Well maybe let's like up mystically yeah I don't know whether we are going to get to the
API level probably not but I think we're the progress in the field points to the direction that
people would have started like thinking about bringing that to the API level hopefully based
on the progress they'll make in in 2020 so yeah we'll I'm also observing these and looking forward to
more like democratization of the tools that we have envisioned because that's really when
as vision researchers we will let the product of a work to be used by people that are not vision
researchers yeah yeah so that's something from the from the commercial perspective from the
technical perspective I think actually the trends that I anticipate would be more or less
in extrapolations of the trends that we be discussed I do believe that vision plus robotics
will continue to go forward as strong the way it will change is that we will we will actually
start working on more specific sorry more general problems like right now let's say in this
example of like habitat challenge and Gibson to discuss things very primarily about navigation
even with the navigation it was about one case of navigation I go to this particular
coordinate but looking at the more general setting the find objects for me or solve the tasks
that or like a sequential game like find object a and then go to an exit like an example it says
the rescue robot we send the robot to a building that is on fire this robot has not been in this
particular building before but you task it with find the victim and go to the closest fire exit
so it's a really useful application just that but we are not there yet and so in terms of vision
plus robotics I expect the proportion of researchers interested in this topic to to expand
and the technical part the problems to become more challenging more realistic and hopefully work
on the more like more general setting not limited settings that we have been focusing on so far
I multitask learning came up in our discussions and I think that's an area that I hope to see
progress in it generally you know there's very little things about vision that is single task
like your job with an image doesn't end the moment to detect objects in it or the moment to detect
the depth of it the moment to detect the vanishing points for any practical use for most practical
uses you usually need multiple of things such task at the same time so the problem is like
essentially multitask to be useful so being able to bring multitask learning to
a more reasonable state where we can solve multiple problems efficiently and reliably and
consistently is an important problem we did see some progress in 2019 but we are far from
reaching that level to the point that we can supply a multitask vision pipeline with one image
and in the uploaded produces a breadth of different abstractions extracted in a way that
that is done independently and sorry that is done efficiently of course you can't have like under
the hood you can have like end different independent pipelines for end different tasks that's inefficient
because there's a lot of redundancy between them so you don't want to do that being able to do
that efficiently do not sacrifice the accuracy over there all these end outputs should have a
reasonable quality and something that I'm personally excited about these days and hopefully
soon we'll release this stuff on it is on consistency so the output of a multitask system should
be consistent your objects and your computer cannot be inconsistent with each other so how we
actually learn do the learning in a way that the outcome is has some guarantees for being consistent
and one thing that also I think we have overlooked for too long especially again in the
conflicts of multitask learning is uncertainty for each of these outputs sure we are providing
some predictions but you can't do much with the prediction especially as a practitioner
if it's not associated with some confidence metric this is a dog in an image but with what
confidence if it's if you're 99% confident and versus you're 50% confident your decision
probably changes if there's a tiger within like two meters of you with 99% confidence
I'm gonna run if it's 1% confidence probably I'm not gonna run I'm gonna do something else
so there's there's a real real value in an extra acting uncertainty from the visual signal
surprisingly that's not actually a big part of the computer vision research right now
and I think that's part of the part of the reason that's happening is that division plus x
movement is recent so the moment you want to provide vision outcome towards some downstream
goal whether that's a roboticist or some and so on so forth you'll realize that they need a
little bit more than whether you're providing them and one of the things that they they definitely
need is uncertainty estimation so you bring this up in the context of multitask is the idea that
the the consistency desire and the uncertainty desire can be formulated as additional tasks or
objectives as part of your training right the reason I brought up in the context of multitask learning
is that multitask learning itself provides an opportunity for a new way of quantifying uncertainty
but uncertainty is still useful and it can be done in a single task setting too and in the machine
learning community there's actually work on it more than vision community that systems that
supply a confidence score besides the prediction that they're making even that's a single task
there's like research being done and that and so on but it has been again proven harder than
anticipated because neural networks are found to be making confident mistakes so because of various
different things that's most of them are actually theory they're papers in just past icml there was
probably more like a track that why this is happening that's basically a question rather than
something that you already know the answer to it but we have made this observation that neural
networks make confident mistakes so that's a problem so basically means that even if you have a
measure of uncertainty chances are that might not be reliable because if you extract that uncertainty
out of a single task system there's no redundancy over there or there's no unzombling
that uncertainty estimation might not be very may accurate too so multitasking there's a way
to solve that because essentially under the hood there are multiple processing pipelines for
different abstractions and the consistency across them is actually a good proxy for how accurate
each of them are and there has been research on it and hopefully I think soon maybe within a few
weeks we will also release new material on that but from what I see the method point is that we
definitely need to to output uncertainty estimates out of the visual processing pipelines that we do
and and more specifically on multitask learning and so on what I see is that there is a there's
a good opportunity to solve that problem at least within the multitask learning framework and
we'll see how 2020 turns out awesome awesome any additional predictions for us I think that's
that that's enough to embarrass myself in the year from now you heard it here first autonomous
driving in 2020 that's what you said right well I didn't for directors but I hope
nice nice at the very least Amir and an autonomous vehicle cab in Arizona yes that's
at least a thing there and I'm looking forward to trying it awesome well Amir thanks so much
for taking the time to share with us your again your take on 2019 and predictions or
anticipations for the the year ahead thanks for having me all right everyone that's our show
for today for more information on today's guest or for links to any of the materials mentioned
check out twimmelai.com slash rewind 19 be sure to leave us a five star rating and a glowing review
after you hit that subscribe button on your favorite podcast catcher thanks so much for listening
and catch you next time
