Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting
people, doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
If you missed our last show and if you did you definitely want to go check it out because
it was a great conversation but if you missed that show you missed the first of the many
exciting updates we have for you this summer.
Last time we announced Twimble's third birthday and our 5 millionth download which happened
right around the same time.
To help us celebrate this occasion and to request your commemorative Twimble birthday sticker
visit twimbleai.com slash birthday 3.
This week we're continuing the action by kicking off volume 2 of our AI platform series.
You recall that last fall we brought you AI platform's volume 1 featuring conversations
with platform builders from Facebook, Airbnb, LinkedIn, OpenAI, Shell and Comcast.
This series turned out to be one of our most popular series of shows ever and over 1000
of you downloaded our first ebook on machine learning platforms, Kubernetes for machine
learning, deep learning and AI.
Well we'll be back at it over the next few weeks sharing more experiences from teams working
to scale and industrialize data science and machine learning at their companies.
And we've got even more in store on this topic so if it's an area you're interested
in be sure to stay tuned.
You can follow along with the series at twimbleai.com slash AI platforms 2 and by following us on
Twitter at at Sam Charrington and at Twimbleai.
Before we dive in, I'd like to send a giant thanks to our friends over at Sigopt.
They've been huge supporters of my work in this area and I'm really excited to have them
as a sponsor of this series of shows on machine learning and AI platforms.
If you don't know Sigopt, I spoke with their CEO Scott Clark back on show number 50.
Their software is used by enterprise teams to standardize and scale machine learning
experimentation and optimization across any combination of modeling frameworks, libraries,
computing infrastructure and environment.
Teams like 2 Sigma who will hear from later in this series rely on Sigopt software to
realize better modeling results much faster than previously possible.
Of course to fully grasp the potential of a tool like Sigopt is best to try it yourself.
That's why Sigopt is offering you the twimble community an exclusive opportunity to try
their product on some of your toughest modeling problems for free.
Take advantage of this offer, visit twimbleai.com slash Sigopt.
All right, everyone.
I am here in Montreal for the NERB's conference and I am with Alex Ratner, Alex as a PhD student
at Stanford.
Alex, welcome to this week in machine learning and AI.
Thank you so much for having me.
Awesome.
We're going to talk a bit about one of the projects you're working on at Stanford, a project
called Snorkel.
Before we do, I'd love to hear a bit about your background and how you got started working
in ML.
Great.
So like many at the NERB's conference, I was a reformed physicist for my undergrad days.
Went out, financed for a bit and then crawled my way back in academia where I've been for
the past four plus wonderful years at Stanford where there's a ton of exciting stuff going
on and that kind of led to the current work which I guess we'll talk about today around
a system called Snorkel.
It's also about the system.
What is Snorkel trying to do?
So the idea that Snorkel's predicated on is that one of the biggest bottlenecks that
people face right now in the current era of these highly automated deep learning algorithms
is getting sufficient amounts of labeled training data.
So if you want to train one of these, especially here at NERB's, if you want to train one of
these fancy architectures, a lot of them require very, very complex.
They do a lot of stuff like picking out all the features to look at and taking in the
raw data and transforming it properly.
They do this automatically, but this of course comes at a cost which is that they generally
need lots of training data to learn from.
And this training data often needs to be labeled by people who have some kind of domain
expertise.
So if you want to train a model to outperform a radiologist at some mammography task for
example where there are some narrow results on things like these, you need months and
months and months of or longer of radiologists sitting there labeling data.
So this has become a significant capital expenditure for big companies.
It's become a significant bottleneck for people like scientists or clinicians from actually
applying these awesome new ML techniques.
And so the question with Snorkel that we tried to address was, can we enable subject matter
expert users to give kind of higher level inputs, things like rules or patterns or noisy
signals from other models that they have lying around and use this to train up and leverage
these awesome new machine learning models that are out there.
Okay.
I'm intrigued by this idea of allowing subject matter experts to work kind of in the domain
of rules that their expertise, I often characterize one of the things I see in this space is kind
of this swing from model based approaches to machine learning or to systems more broadly
to kind of these purely statistical based approaches that have no, they don't really
attempt to capture subject matter expertise and it sounds like Snorkel is part of a bit
of the pendulum returning to the center which is trying to incorporate in some of this domain
expertise.
Yeah, I think that's a great way of putting it.
I mean, this is, this is really an age old question in, you know, the field of AI more
broadly is how do you inject domain expert knowledge into a system and, you know, ever
since statistical learning techniques have become so powerful and useful in some areas,
how do you inject this domain knowledge into them, right?
Right.
And, you know, more broadly than Snorkel, you can see this pendulum beginning to swing
back a little bit when you go look around the poster session here at Nareps, you know,
there's definitely more work out there around explicitly defined, say, generative models
where, you know, someone has set some explicit structure rather than just learning all that
structure from data.
And, you know, Snorkel is definitely fitting into that narrative in that we're trying to
bridge the gap between the two, we're trying to use domain expert knowledge that is,
you know, expressed in a really simple way.
We have experts or, you know, we have, you know, whether they're clinicians or journalists
or, you know, whoever these subject matter experts are, they write what we call labeling functions.
So just simple little functions that, you know, could express a pattern or a rule or they
could call some other classifier and they just take a data point and say, you know, say
it's a binary classification problem, yes, no, or I don't know, that's all.
And they just, they can be Python functions, whatever they are, they just dump them into
the system.
Okay.
And then we try to use that to take advantage of these statistical models.
So we're trying to kind of bridge the two worlds via this very simple conduit of training
data.
Well, before we get too deep into Snorkel and how it works, it's a successor to an earlier
project called Deep Dive.
There's a clear theme that I hear somebody likes to swim or dive or snorkel or something.
But maybe tell us a little bit about Deep Dive and its origins and trajectory.
Deep Dive, like many, you know, great initiatives and computer science these days, it was kind
of, in large part, based on or based around this big DARPA project, one that's still ongoing
that actually snorkels being used in right now called Memic. So it's this big project
that is run by DARPA around anti-human trafficking.
Okay.
And it's had actually, it's actually in, you know, the hands of law enforcement and making
a really big impact.
And the problem.
So I'm familiar with like the DARPA, like the autonomous vehicles challenge and some
of the others, I'm not familiar with Memic.
So.
Yeah, yeah.
So the Memic challenge, brother has been focused on kind of targeted search trying to extract
information from the dark web and use it to actually aid law enforcement and identifying
individuals that are potentially being trafficked and interceding.
So it's really an incredible project and there's a lot of teams, you know, across both
academia and industry that have contributed.
I know, you know, I wasn't so directly involved, but in our lab, Deep Dive had contributed
around trying to map from what we sometimes call dark data or unstructured data.
So things like websites and just this kind of messy data that's really tough for computers
to deal with and actually pull out structured stuff like imagine pulling out an Excel spreadsheet
with well-defined columns or a graph of entities and their relations from this messy unstructured
data.
And that's a really tough problem that machine learning often does really well at.
But you know, when we tried to do this, it wasn't the, you know, lacking a fancy model
architecture to do this.
So it was really needing training data and, you know, asking, you know, people to look
at these terrible websites for, you know, weeks or months on end to label a training set
and then find out that actually one week later, we don't need that training set anymore.
We need a different training set.
So throw it out and start over again.
You know, that, and this was somewhat surprising because, you know, we had been working on
all these fancy modeling and inference improvements.
That ends up being in that and many other projects that the biggest pain point, the biggest
blocker, right, like when we started moving into snorkel, we would sit down with clinicians
from the Stanford hospital system and, you know, we would be really eager to tell them
about our new fancy model and they'd say, wait, hold up, you know, you said something
about I have to have a large label training set.
I don't have months to sit down and do that.
What gives?
Like, you know, what do I do?
So we realized that all these incredible advances in the models and how they, how easy to
use they were, were coming about in this wave over the last couple of years, but everyone
was blocked on this first step of getting training data for them, right?
You know, and people ignored this for a long time because you had things like ImageNet,
which was a really, you know, incredible resource that jump started.
That and other data sets jump started this, this current, you know, wave of deep learning
progress took several years to create, you know, I know my advisor, Chris was involved
a little bit, you know, downstairs from us was where a lot of this happened.
If you're working on that, that's great, but then you want to take the models that are
doing, you know, you want to take that incredible progress and you want to translate it into
a real world problem where there's no labeled benchmark training set.
It's a huge problem.
So we sort of shifted gears a little bit, you know, deep dive went out into the world
and then, you know, we decided with this new project snorkel to really tackle this problem
of creating training data and rather than viewing training data as this thing that just sort
of existed before you came to the problem, we decided we wanted to make it kind of the
first class citizen of our new framework.
And so in snorkel, really the main activity, you know, snorkel supports arbitrary models
you can plug in your favorite model downloaded from online, plug it into PyTorch or TensorFlow
or whatnot, but really the thing that users do in snorkels, they write these labeling functions.
And the goal is to get subject matter experts to as quickly as possible, dump in all of
the signal and the knowledge that they have in a much kind of higher density way than
if they were just sitting saying yes, no, yes, no, on radiology images for months.
And so you have subject matter experts producing these labeling functions, you know, so you
see any, and so that provides, you know, some degree of abstraction over the labels themselves.
Exactly.
Right.
I can imagine a bunch of different directions you might want to go with that.
Like, are you using these probabilistically, are you like active learning, incorporating
some kind of active, like what's next, what's next, what's next, what's next.
Yeah, that's a great question.
And actually, I mean, there are ongoing projects on kind of like all those and too many other
fronts.
Okay.
This is such a thing is too much fun.
You know, my advisor has a story called the, you know, we call it parable the burritos
that he had a burrito eating contest and then he, you know, finally asks his friend,
why are we eating all these burritos?
What's their reward?
His friend was like, well, it's more burritos.
Kind of how research feels, you know, you work, you work and you work and you work and
well, what's their reward?
You get to do more work.
So it's, you know, you hope that you like the work you're doing, right?
But we have a ton of stuff we're trying to do on, on top of it, you know, I'll go back
to the first thing you said, which is that you have this abstraction away from labels.
And that's one of the things we're most excited about is that the big thing that we're trying
to do with snorkel is turn training data labeling from a hand labeling, you know, hand
annotation activity to a coding one.
And this code is supervision paradigm, this shift, then, you know, we hope and we've seen
with some of our prototypes allows you to use all the benefits of code, different abstraction
layers, modularity, you know, increased interpretability.
So when your training set is 20 labeling functions applied over unlabeled data rather than
a million label data points, if you find out that your modeling goals change, you can
try to change your labeling functions in half an hour rather than having to throw out
your training data and start over again, which is a massive problem in real world deployments
of ML as far as we see.
And so does this apply most directly to a certain type of problem or use case I'm thinking
of, for example, you know, the reason why deep learning has been so effective in the computer
vision realm is because creating, you know, rules for these types of problems like, you
know, where's the cat in this picture?
It's like super hard.
How do you do that?
Well, it's a great question.
It's a great question.
So this, and it really ties in with that notion of like building higher level abstractions
on top.
So we started with text.
We started with these, you know, this like, you know, Memex problem of, okay, we need
to quickly classify our to pull out names of entities or relations, you know, we've
done projects over electronic health records at the VA at Stanford where we want to pull
out mentions of, you know, there was pain in this joint or this area of the body, right?
This is actually kind of messy because language is messy, but you can imagine how you'd write
rules over text, right?
You, you know, you, you give some rules and the model then, you know, learns to generalize
beyond it.
Now images, we've done some interesting stuff.
One thing that we've done, this was a Neurip's paper last year, actually, an extension
called Coral built on top of snorkel where we basically applied a bunch of unsupervised
algorithms to kind of get macro level features of images.
So imagine that, um, and then we had people write labeling functions over these.
So there's a tutorial we have online and all of this, there's a lot of material at Stanford,
uh, snorkel.stanford.edu, um, so this exact demo is there as little toy demo, but imagine
you want to classify, you want to use a, you know, a deep neural network to classify
when people are writing bikes, like you want to do activity detection.
So we just use an off the shelf, uh, bounding box thing to put, you know, bounding boxes
around a person in a bike and then people would write labeling functions that say, okay,
if person is, you know, above bike vertically and centered, then label true else label
false.
So you can write these labeling, and again, this is the abstraction level thing.
You can write labeling functions over these kind of building boxes or if it's mammography,
you know, we could pre tag all the blobs in the image and then the, you know, domain
act, the clinician writes labeling functions about properties of those blobs.
Mm-hmm.
And then of course, this is a pipeline.
So we're just trying to generate training data for some, some things, right?
And then the goal, and this is what we see empirically, is that the deep neural network
will learn to generalize beyond that training data.
So you write these rules, capture some small portion of your data set, and then you use
it to train a model that learns to cover the whole data set.
And so you just, you're just trying to generate training data, are you actually generating
the training data or have you integrated these labeling functions into the training process?
That's a great question.
There's definitely stuff where we want to close the loop.
Yeah.
Um, the basic, you know, the basic setup is that these labeling functions and generating
is maybe a load of term, we're, we're trying to label training data sets.
So what these methods rely on is, or the way that we mostly do is unlabeled data.
To describe another project that I'm quite excited about at the moment, we're collaborating
with the radiology department at Stanford and actually a bunch of teams around Stanford
hospital.
Um, but I'll focus on the radiology applications where we have, they have troves of unlabeled
data.
And what the sum label data looks like is generally comes in pairs of an image and then
a text report that the doctor wrote, right?
And so you can think of that text report as like kind of a messy, jumbled version of
the label that you actually want.
And so there were efforts, which are really cool, where, you know, I think there was one
where they took, you know, a couple of years and we actually have a paper out in radiology
from someone in our lab on using this labeling effort that took years of, you know, radiologist
sitting and labeling data.
With snorkelway access question, could we have a radiology fellow spend a week or two,
write these labeling functions over those text reports and generate labels that were almost
as good.
And then if we piped in 10x the amount of unlabeled data, could we even do better?
Mm-hmm.
You know, this is this intuition, which is an old one in ML of like, well, maybe a larger
amount of noisier data can be a smaller amount of ground truth data, especially if we use
a technique like snorkel that kind of, you know, learns to reweight and combine the labeling
functions, kind of denoises it automatically.
And so the answer was yes, it's actually, you know, we just dump in as much unlabeled data
we can get our hands on, we apply the labeling functions and dump it into snorkel, which
kind of reweights and recombines them.
And then you get a better training set that can be used almost as effectively or sometimes
even more effectively than that hand labeled training set that would have taken months
or years.
Okay.
So we've talked about defining these labeling functions and it sounds like the most
basic uses to use these labeling functions to label data, but it's not a straight application
of this labeling function.
And there's this reweating that you've alluded to a couple of times, what exactly is happening
there?
Yeah, great question.
And that's a part that we really like to get deep into the math on that front.
So this was, I mean, I guess the first paper was at NURPS in 2016, it was called data programming.
So we have a method which, you know, it's based on old intuition.
So if you, you know, the intuition is this, is that if you have a bunch of sources, you
have a bunch of, say, a bunch of crowd lablers and you don't have any ground truth, but
you want to know who to trust.
You can start under some assumptions and just say, okay, say all that they're all independent.
None of them are colluding with each other.
Just say, okay, well, I'm going to trust the ones that are in the majority more often.
Sure.
And if one of the crowd lablers is almost always in the minority, I'm going to discount
them.
Maybe think that they're adversarial.
Whereas if someone is always in the majority on every data point, I'm going to trust
them more.
Okay.
Yeah.
Yeah.
This is, you know, a big area of work in the past and crowd sourcing.
So in our NURPS 2016 paper, what we were doing was kind of extending this to this more,
this kind of more tangled setting of having functions rather than people because these
functions can have all kinds of weird correlations, right?
You know, two people we've had this happen before, two people can write two labeling functions
that are near duplicates of each other or exact duplicates.
And you don't want to double count their votes.
You could have whole clumps of labeling functions that are using the same kinds of patterns
or the same data resources.
It's code, right?
Code can have arbitrary weird correlations and overlaps.
So is it fundamental to the model or the way you envision the model being used that for
a given problem, you'd have multiple subject matter expertise, SMEs kind of creating labeling
functions.
It's almost like instead of crowd sourcing labels, you're crowd sourcing labeling functions.
Yeah.
We're actually doing a study on that idea like meta crowd sourcing that we actually
had a workshop a couple of weeks ago at Stanford where we had 15 teams come and they were
doing stuff around text extraction from in the bio domain.
And we actually have now post hoc collected all 15, like the labeling functions from all
15 teams and actually does better if you just dump them in all and all and together.
But you know, practically, it's usually, you know, with a lot of our engagements, it's
just one subject matter expert.
But what is essential is that they write more than one labeling function, right?
So they usually write for the same problem.
For the same problem.
Yeah.
So, you know, we'll have the, in that example, I gave about, you know, classifying chest
x-rays, for example, the radiology fellow wrote 20 labeling functions.
And they were all, you know, they were a mix of things like looking for certain words
or matching against certain ontologies of diseases or checking how many times the word
normal is said, all these kinds of messy heuristics.
And so, in a sense, you're, you know, it's that you're trying to apply, you know, what
you consider to be like, it's just a good programming, like a single labeling function has
like conceptual boundaries, it's trying to generate a label based on this as opposed
to some, you know, single mango labeling function that takes in everything you know about
the data set and tries to label it.
And that is, that is such a great example because that speaks exactly the kind of our motivation
in that, you know, we, you know, I was kind of glossing over a little bit like we didn't
just go from, okay, people only hand labeled to now we're doing what we didn't snorkel.
We saw that people were trying these, what's often called weaker ways of supervising models
where they were writing code.
But the big pain point there, including our lab with, with the previous system D type
and the problem there was that exactly if you try to make one big mango program to produce
the labels, this becomes the same kind of spaghetti code that you were trying to avoid in the
first place by using a machine learning model.
So the idea of these labeling functions is yes, you're, you're just writing little snippets
and you're leaving it up to snorkel to figure out how to wait them, how to combine them.
You don't have to sit there, again, you know, it's not magic, it doesn't work always
right there, you know, rather than if I have two labeling functions and I want, you know,
I rather than sitting there and being like, well, which one do I trust more?
Or one example is say you have one labeling function that, you know, labels 10,000 points
and you trust it way more and one that labels a million points and you trust it less.
You know, how do you combine, what's the right way to combine them?
Right.
You don't need to worry about that.
You just dump it into snorkel and snorkel waits them accordingly.
Or you have 10 labeling functions and you don't, you don't want to like sit there thinking,
okay, which one should override which one and which combination?
You just dump it into snorkel.
It's interesting.
So I did an interview last week, I think, with Rich Zemel at the University of Toronto
and he's doing some work on fairness and one of his papers is about a system that is trying
to create an unbiased system by having the system refer decisions to kind of human participants
in the system but start to learn their biases and refer things to them based on those biases.
It almost sounds like there's some application of that here where the system can learn which
of the functions work best given a certain type of input data and then dynamically use
these.
Yes.
I mean, no, that's, your questions are uncannily good in this, you're also, you're also asking
all about projects that were currently underway, so it's almost like my advisor paid you.
This is one thing that we're very excited about.
So the base thing, so I'm sure also, I haven't read that particular paper, but it sounds
like I'm quite sure there are lots of parallels in the ideas and stuff.
And in our setting, we start by learning an accuracy for every labeling function, right?
And again, it's that we use some matrix completion style techniques in 2016, we're using
Sephiron Gibbs sampling, we have our various fancy ways of doing it, but the intuition
is that you try to learn the accuracy based on trusting the majority and having some
assumption that they're kind of independent or learning which ones aren't, which ones
are correlated with each other.
But we're increasingly moving into this area where we are learning actually like biases
that are conditioned on data.
So you can learn that, you know, this labeling function is much better for daytime conditions
than nighttime conditions, right?
That's where it's kind of expertise is.
So that's exactly something that we're, you know, playing around with right now.
Okay.
And I would imagine that some of these biases could be explicit, like the SME could
consciously say, oh, if I was looking at this, you know, this radiological image, I guess
that's not a great example, but if I was looking at this picture of a person on a bicycle
and it was dark, you know, I'd look for reflectors, but then there are these implicit biases
that you just kind of, it's better to learn.
Anything that's simple for an SME to write down in code or even, and I should mention one
of the other things we're doing going back to that point of abstraction layers, I guess
I'll briefly go on that tangent, which is that, you know, one of the things we're really
excited about is kind of building up this programming stack.
So you know, if you think of a traditional programming stack, you go from like, you know,
machine language all the way to increasingly higher and higher and more declarative levels
until you get to like, I know, voice commands or GUIs, right?
Yeah.
We're trying to think about the same thing with training data.
The training data is the sort of machine code that's like the common thing you compile
to.
Labeling functions are like the lowest level thing you can program in.
How can you go higher and higher, right?
So that's interesting.
Yeah.
And it's, I mean, a lot of these are still, you know, because they're kind of prototype
level, but like, my lab made a presented a paper called, it's a great name, better than
snorkeling and babble level at ACL, which is one of the big NLP conferences this year,
where the idea is that you give explanations for why you're labeling data points.
And these are compiled.
They're parsed automatically into labeling functions and then dumped into snorkel.
So we want to make the SME provides explanations as to why they're labeling manuals.
Why they would label something, they don't actually have to do the manual, they're just
looking at examples and they're giving explanations and those are then parsed using
something called a semantic parser into labeling functions, which are, of course, are super
noisy, but that's what snorkel is meant to deal with, right?
So whatever the level that the SME can provide this stuff, the goal of snorkel is just to,
you know, whatever they can provide, take it and whatever is too laborious to provide,
like those exact conditions or those exact sort of if-then clauses, just kind of learn
that, right?
Kind of fill that in.
Yeah.
And another practical tool there is that these labeling functions kind of stain.
So, you know, if you know the labeling function is really good for daytime, but not nighttime,
and you have a way to express that, then you can just write a labeling function that says,
if daytime, you know, vote on my, you know, output-alabel else, just abstain.
Right, right.
Yeah, that was another feature of this rich, simple paper that I was referring to, the,
one of the things that they did with their models that it could say yes, nor pass, and kind
of defer to another model or in their case of human.
And we know in our setting, we find that this, you know, this abstention is actually like
a very, very critical tool in practice, this ability to say, you know, I pass, or I don't
know.
And in their case, I imagine in your case, it's, you know, when I first heard that, I was
like, okay, well, okay, if it's, you know, below a threshold of 50% confidence or something
like that, you abstain.
But it was way more nuanced than that.
I imagine that it's similarly nuanced in your case.
Yeah, it's, it's so much because the, the, the place where you pass on or the place
where you abstain on does have some bias to it.
It's where, you know, where the expert thinks this heuristic doesn't really apply, I guess.
Yeah.
And it's a, you know, it's a rough version of that.
Right.
Right.
And again, you know, our goal here is to speed up in a human-in-the-loop process.
And to fundamentally bring it to the level of coding rather than labeling, not to obviate
it.
Right.
So, you know, people have to, you know, people iterate on these labeling functions.
But this ends up taking, you know, days or a week or two, not months or years.
And then it can be repurposed or reapplied when your next problem comes along.
So this conversation about programming levels of abstraction and, and functions in particular
is kind of pulling me towards like an infrastructure path and thinking of like serverless.
Is that mean anything to you?
Like lambda functions, AWS lambda and that kind of thing.
Yeah.
I wonder if there's an intersection here somewhere out of it.
It's not obvious to me what it would be, rather than that functions are the primary currency
of both systems.
Yeah.
There's a lot of interesting stuff we want to do, I don't have any directions we've explored
around that.
I mean, it's certainly an interesting area.
We haven't tried that out with Snorkel yet.
I will say this is, you know, a bit of a right-hand turn.
But something I think is important is that, or that we think is important, is that Snorkel
is very much what we think of as a system's ML type contribution, right?
Like, you know, one of the initial papers or the initial paper was at NUREPS in 2016.
It was about the algorithm.
But the real contribution that we're excited about here is really a merger of sort of,
you know, machine learning algorithms and theory, but with systems and framework constraints.
And that's something that I, you know, a lot of us feel pretty excited about.
We're actually, you know, there's a PC meeting for this new CISML conference here, you
know, dovetailed on with NUREPS.
And this is a conference.
It's, I mean, unfortunately, the call for submission already happens.
I can't advertise it.
I can't say it's here, but, you know, at NUREPS, at the workshops, there are actually two
different systems ML conferences, systems ML and ML for systems.
And then there's this CISML conference happening at Stanford in the spring.
And, you know, those of us involved things that-
Separate from the scaled ML?
Yes, separate, yeah.
Oh, really?
Okay.
And I think a little bit of a broader umbrella.
Okay.
We're going to be releasing a white paper soon on sort of what we see the scope as.
But I would kind of pitch it this way.
There's so many fundamental core problems in core ML that still need to be solved.
They're almost more so because, you know, deep learning came in and made such a splash
and such an empirical impact on certain areas, but we still understand it so, so poorly.
And, you know, we need to work on so many problems around that that are really fascinating.
But it's kind of had an impact enough that a lot of us are saying, look, it's time to
build the engineering infrastructure around it.
Right?
Right?
It's valuable enough that people want to use it and they can't- and it's not because
they don't understand- they don't have a fancine of algorithms.
It reaches the core.
Yeah.
Yeah.
So, you know, I- I think this was, you know, shot down by some of my core organizers.
But I like the metaphor of like a sandwich almost.
Like, you know, the core is the ML stuff.
But, you know, on the bottom you have the, you know, you have the lower level concerns
of how do you- how do you build the hardware for modern ML, you know, algorithms?
How do you- how do you serve it?
How do you set up the distributed systems?
On the top level you have- how do you, you know, put together- put it together in end-to-end
workflows?
Things like snorkel or things for data preprocessing or interpretability or serving or monitoring,
right?
Right.
So, you have these sort of types of considerations that that's not kind of what NERF specializes
in, usually.
And we think there's an exciting area to really fill out these engineering or systems aspects
of ML.
So, that's definitely kind of where we see a lot of the interesting questions around snorkel.
I mean, yeah.
This is the ML algorithm side, which we love too, but also these like, you know, we haven't
dealt with a serverless question yet, but those kinds of questions.
So, we started talking about the way that it kind of chooses between these labeling functions.
And you mentioned that it's evolved.
You started with Gibbs sampling and some other things.
And- but it's an area that, you know, folks enjoy- folks on a team enjoy kind of geeking
out about, like, can you go into some more detail on what exactly is happening there?
Yeah, yeah.
So, you know, I'd say that, again, you know, if you're into this area, one of the things
that we're quite excited about is this handling correlations between the labeling functions.
So, if you think about this as a modeling problem, you know, you could kind of- we call it
weekly supervised.
And this is one of the, you know, areas or little sub-communities that we're quite excited
about.
Ransom workshop to actually at last NERFs and at IClear coming up this year on week supervision.
This idea of noisier or cheaper or higher level supervision.
But the core model in Snarkle that learns the accuracy of the labeling functions, you
could think almost of unsupervised in that we don't have any ground truth labels.
But either way, what it looks like is you have- what you observe is you observe the votes
of these labeling functions.
What you don't observe is this latent variable of the ground truth.
So it fits into this tradition of latent variable models, but in this new kind of way where
these labeling functions can also be correlated with each other.
And so we've done work on, you know, if you know the correlations, if your SME user says,
look, these two labeling functions are basically the same.
I just tweaked the number or I tweaked the threshold.
And then our system can take that into account and learn the model knowing to expect that
these two labeling functions are very correlated that we shouldn't double count their votes basically.
We've also done work.
There is an ICML 2017 paper and all this is up at snarkle.sanford.edu.
Just before you get to the next paper, when you're talking about kind of these correlations,
you've got these functions that are originally at least provided in the Python domain, the programming domain.
Are you kind of projecting them into like a linear algebra domain?
Yeah, so it makes sense to you.
I think, yeah, I mean, that's a definitely a great way of putting it.
I mean, but concretely, just to keep it simple, what we're doing is we're just taking their labels
and then we basically just leave them as black boxes.
Right, so you've got some transformation from some input to the label.
We have a bunch of functions.
We have a bunch of unlabeled data.
That's another critical ingredient.
Apply the functions to the data.
And then we have a basically a matrix of noisy labels.
Yeah, and that's what we work with.
Although we have done one of the past nerfs papers last year was looking at what if we actually
use information in these functions.
So we can actually do static analysis on them.
They're not black boxes.
This is another interesting facet of our kind of unique facet of our setting.
They're not black boxes, right?
I can have a really pretty simple static analysis program that looks at two labeling functions
and says, hey, they're nearly identical except that one number was tweaked.
Therefore, I should model them as correlated.
So we can open that black box.
But at the end of the day, we just have this matrix of noisy labels.
And we just need to know how to re-weight them according to the different sources they
came, the different labeling functions they came from.
Do you apply traditional PCA or things that dimensionality reduction or things that are
trying to find these correlations or some of the things that you've mentioned, I guess
are their unique aspects of this noisy matrix that make other techniques better than
the more generic ways of finding these correlations?
Yeah, yeah.
So, I mean, that's a great intuition as well, and some of the techniques we're looking
into are connections to a sign known as robust PCA for learning the structure.
The ICML paper I mentioned from last year on learning the structure was building on classic
techniques and probabilistic graphical models for learning the structure of the models,
except our setting, we don't have ground truth labels.
So that kind of fundament, that's like the fundamental change here is that we're missing
the ground truth.
And so then we show that we can still learn the structure from data.
And so, you know, again, you either learn the structure or you're given it, and then
you need to take it into account in the modeling.
You don't want to effectively, if we go back to that majority vote intuition, you don't
want to double count two labeling functions that are basically the same in Roots, and
are always going to give the same answer, that'll throw your model off.
And our motivation, once again, is really grounded in developer process.
We've seen failure modes happen according to this.
If you want to have three developers, or if you want to, we just post online about a
report of some of our collaborations with Teams at Google.
So you have a huge scale where you have Teams of multiple people working on these labeling
functions, and they're all different types.
You want to be able to just dump them in and not worry about if some of them are too correlated
or not.
You want to have the system take care of that for you, not because it's going to quadruple
performance necessarily, but because it's going to avoid catastrophic failure modes.
And so that kind of like increasing robustness in a human and the loop driven process is really
our kind of motivation.
You mentioned you just posted this paper or summary of some work done with Google.
What can you share about that?
Yeah, I mean, so at a high level, the interesting kind of academic thread there, I would say,
is that we refer to this sort of how do you leverage organization scale resources?
So I can talk high level about some of the ideas I think are interesting there, and the
details are on our archive.
But one aspect is this notion that if you go to any organization, or many organizations
these days are dealing with this ML question.
We have all these data resources, a lot of companies or labs or whatever, a lot of organizations
they have troves of rules that their chunks of code that their experts wrote from before.
They may have messy labels that are kind of outdated or noisy, but are sitting around.
They may have even other classifiers that are too brittle to really be the solution,
but have some signal.
And then on the other end, you have these shiny new deep learning models, or they could
be complex models of any sort, random, you know, forest, XG boost, whatever it is.
And the question is, how do you bridge that gap there?
And a lot of organizations are just told, okay, we'll throw out the old stuff, that's
useless.
It's legacy.
And an enormous amount of resources, hand labelling training data, and then jump onto the new
train.
And I think what we showed, at least in some cases in this collaboration, was that you
really can bridge the two using techniques like Snarkle, where you take that and use it
as a weak supervision for training these modern models.
And one of the other cool aspects I'll briefly mention there is this notion of going
from non-servable to servable models.
So you can write this weak supervision of these labeling functions over data that you
don't want to serve in production.
It could be like aggregate statistics.
It might be models that are expensive to run, knowledge graphs, stuff that you can't
serve efficiently in production.
You could use that to train a model that can then run over, you know, cheap, public,
servable features.
And so this seemed to be another aspect of this kind of pipeline that is very useful.
That's the one thing we learned.
Interesting.
Interesting.
Awesome.
What's next?
Well, it sounds like you've got a ton of threads that are already kind of spun up about,
you know, pushing this reach our research in different directions.
Yeah.
It's Snarkle.
Is it open source?
I can people play with it.
Yeah.
For sure.
And, you know, we love feedback.
Proversely, like many academics, we like negative feedback even more than positive at
this point.
We've been fortunate to have people have, you know, wins with Snarkle.
We're very interested in, you know, interesting failure modes or requests for features and
anything.
So it's all in line at snarkle.stanford.edu.
We have tutorials.
We have blog posts, links to all papers and stuff.
Okay.
And again, you know, it's great when people, you know, find things that they have questions
or they say, oh, could be used, could it be used in this setting or it didn't work in
this setting?
I think I have an interesting reason why we love that stuff.
So, and then in terms of what's next, I mean, so there's a whole bunch of stuff.
One thing that we're working on, it's actually in a separate repot just while it's in beta
for now, which is this version of snarkle called snarkle metal.
And it's supposed to be the multitask version of snarkle.
So this is an idea, basically, people are getting excited again about this idea, they
called multitask learning.
This is an idea from back in the 90s.
It's just, you know, if you have multiple, and it's something that's an old theme in
the area, right?
If you have multiple things you're trying to do, just like how humans learn, right?
What if you, you know, kind of do it all jointly?
You share the learned representation between these tasks.
And so there's kind of been a resurgence of interest in these techniques, you know, kind
of in the realm of these new architectures, right?
And often what these look like is actually kind of conceptually simple.
You are the base of the vanilla version.
Now, you have some deep neural network.
And you have the bottom layers are all shared across K different tasks.
And then you kind of split off at the end and learn these like little top task specific
bits.
But you benefit from kind of sharing a representation of the world or of the data between all these
different tasks.
So what really we're really interested in exploring this from the perspective of weak supervision.
And you know, one problem that we kind of detailed in a paper that's going to be presented
this year at AAAI was, okay, how do we deal with the weak supervision from multiple
tasks?
People are writing labeling functions for K different tasks that, you know, have different
kinds of relationships to each other.
Like imagine a hierarchical thing.
Like I have some labeling functions that say, you know, this is a lawyer versus a doctor.
This is a bank versus a hospital and then some that say person or organization.
So you have different levels of granularity.
That's one example, right?
How can we have people dump them all in just like as in snorkel, but now in this multi-task
setting and handle it?
So that was one extension we worked on.
But moving forwards, we really think that, you know, most of the multi-task learning
work, which has been really exciting to date, has been around a couple of hand labeled
datasets.
They're kind of static.
We think that as people begin to move towards these weak supervision methods, the number
of datasets is going to explode and you're going to have now these sort of massively multi-task
models where, you know, rather than saying, oh, look, we can do better on these five different
datasets, you know, that are there.
You're going to have tens or hundreds of, you know, training sets for tens or hundreds
of tasks that are all weakly supervised, they're all changing.
Okay.
And the question of how you manage that all in one big model, I think is fascinating.
So there's the kind of the modeling aspect of that multi-task, but also what is that
training data come from and these labeling functions could help contribute to that as
well?
Exactly.
Yeah.
Yeah.
And I think that that's also, again, on the system side, what's really interesting is,
how do you, you know, if you, if you have people, again, it's just a big shift, right?
Right now, you know, training data is, a new training data set takes, you know, months
minimum to put online for a real world problem, right?
And a training set can be created in hours or days, and you're going to have tens or
hundreds of them.
How do you manage that?
Right.
And so that's, again, we think there are all these interesting questions around organizational
level management of new training sets that are weakly supervised.
How do you put them all together?
How do you amortize costs across them?
We think this, you know, MTL line of thinking is one answer to that, along with our existing
work on snorkel.
Well, Alex, thanks so much for taking the time to chat with me about this stuff.
Thank you so much for having me on the show.
Great stuff.
Very fun.
Thank you.
Awesome.
Thanks so much.
All right, everyone.
That's our show for today.
For more information about today's guests, or to follow along with our AI Platform's Volume
2 series, visit twimmelai.com slash AI Platforms2.
Thanks once again to SIGUP for their sponsorship of this series and support of the show.
To check out what they're up to and take advantage of their exclusive offer for Twimmel listeners,
visit twimmelai.com slash SIGUP.
As always, thanks so much for listening and catch you next time.
