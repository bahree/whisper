Welcome to the Twomo AI Podcast.
I'm your host, Sam Charrington.
Hey, what's up everyone?
It's been nearly three years since we launched the Twomo Online Meetup, initially as a way
for listeners to connect with one another and study academic research papers together.
The group really took off in mid-2018 though when I mentioned off-handedly in my interview
with fast.ai's Rachel Thomas that I'd be taking their course and I invited the Twomo
listener community to join me.
That led to the very first Twomo study group and the folks who came together for that group
have become the backbone of a community that's now over 3,000 members strong.
Fast forward 18 months and we've supported each other on our personal learning journeys
and a wide variety of courses from stanfordfast.ai, deeplearning.ai, on Kaggle projects and much
more.
I say all that to say that I am humbled by the role that we've been able to play in helping
folks learn machine learning and I am excited about the new educational programs that we're
bringing to the Twomo community this year.
We recently launched a collaboration to bring you the causal modeling and machine learning
course and study group which I've mentioned here before and today I'd like to share some
new details on the AI enterprise workflow study group that we're launching in just a
few weeks.
If you've been listening for a while you know that I am very excited about the work going
on in ML and AI communities to make developing and deploying machine learning and deep learning
models in the enterprise more accessible and efficient.
In fact we hosted an entire conference TwomoCon AI platforms on this topic just last fall.
Until now there have been very few formal resources that folks could turn to to learn
real world machine learning workflows and deployment strategies.
Our folks at IBM are working to change this though with the AI enterprise workflow specialization
that they recently made available on Coursera.
And I am super excited to share that we've partnered with them to host a study group for
this six course specialization program which I will personally be hosting.
The courses in the sequence teach real world machine learning and an enterprise environment
including applying the scientific process to understanding business use cases and structuring
data collection visualizing and analyzing data and hypothesis testing feature engineering
and identifying and addressing data biases selecting the best models for machine learning
vision and NLP use cases and using ensembles deploying models using microservices and containers
Kubernetes and Apache spark and applying unit testing to ML models and monitoring model
performance in production.
I am really looking forward to working through these courses and I would love for anyone
interested in these topics to join me.
If this sounds interesting and you'd like to learn more, I invite you to join a webinar
that I'm hosting with Ray Lopez, the courses instructor on Saturday, February 15th at 9.30
a.m. Pacific time.
To register, visit twimmelai.com slash AI workflow.
And now on to the show, all right, everyone, I am on the line with Nemo, Nemo is the CTO
at Grow Intelligence.
Nemo, welcome to the twimmelai podcast.
Thanks.
Happy to be here.
Awesome.
Let's jump right in and have you share with us a bit of your background and how you came
to work and the intersection of machine learning, AI and global food security.
Great.
So I've been at Grow Intelligence for about four years now, four and a half years.
And before that, for many years, I was an engineer and tech lead at Google.
And obviously working on a lot of things that are technically similar in terms of machine
learning, using data, mathematical algorithms, what brought me to Grow Intelligence, well,
first I met, it was the founder, Sarah Manker had a really compelling vision for why this
is an important area to be working on today.
And what brought me here was that AI were solving some really important problems, some
of the biggest problems affecting the world.
And secondly, I think that the timing of the problems that we're trying to tackle is
really appropriate, meaning maybe 10 years ago, what we're trying to do today would
not have been realistic or would not have made sense.
And 10 years of the future would probably be not so interesting anymore because it hopefully
will have been done.
So I think we'll kind of have a sweet spot for this industry.
And then thirdly, I think what makes it technically interesting is that from various perspectives,
whether it's computer science, satellite hardware, data storage and infrastructure, a lot
of different enabling technologies are available to us today.
And so make this problem not just important, but also feasible, I mean, addressable today.
You mentioned Sarah, I had the opportunity to hear her speak at the Black and AI workshop
at last year's Neurip's conference.
And so had a chance to learn a little bit about what Grow is up to.
And so all she seems like an amazing person to work with, but also she does a very good
job at articulating the motivation behind what you're doing and the importance of the
work you're doing.
Can you share a bit of that with us?
Yeah, I guess the one way of putting it is what we do for getting the technical aspects
is basically we help people make better decisions in the world of food and agriculture.
So motivations, I think, are that the data-driven decision making hasn't quite taken hold in
this industry.
And obviously it's one of the largest industries in the world.
Think about two billion people in the world are involved in the whole industry of food
and agriculture from production all the way through its redistribution and so on.
And this world is a number one, changing pretty rapidly as economies develop and culture
has changed, people's diets and what type of food they eat change.
So people need to make better decisions about what's going on in the world of food.
Besides this, social and cultural changes and economic changes, another aspect is obviously
global warming has thrown a lot of uncertainty in many different food related scenarios.
And then maybe even more short-term, you know, you have trade wars and tariffs and deforestation
and things like that.
So a lot of locusts, is that something you're involved in?
There's apparently a massive locust swarm currently in Eastern Africa and it's expected
to make its way all the way to India.
Yeah, absolutely.
This is a huge topic in the last couple of weeks and that's exactly the type of problem
where we would help people make better decisions on.
So if suddenly this infestation shows up, you know, if you are trying to decide, well,
what does it really mean in terms of food supply or not just in total but like how it affects?
This region, that region, this type of crop, other types of crops, what will they do to
prices, imports, exports.
So yeah, connecting all those dots together and trying to make more informed decisions
regardless of whether you're a supplier or a consumer or a processor is basically the
type of problem we help to solve.
But yeah, I think that's a great example.
You know, a few months ago maybe you would have chosen to mention the US China trade wars
as an example or the swine flu that's affecting pigs in Asia or the forest fires in Brazil
and the Amazon.
All of those are really great examples of the type of really huge events that affect our
users and that we try to help them deal with.
And so who are the typical users and maybe a bit more concretely, what are the specific
types of problems that they're trying to solve?
Okay, great.
So yeah, I think one thing that's important, there's quite a lot of buzz in this world
of agriculture these days, ag tech.
So one distinction that I want to make is sort of this ecosystem is can roughly be split
into two categories.
There's one that you hear a lot about which is precision ag.
So this is the type of, this is what we don't do, I'll start with that.
And so there's a lot of really cool tech related to helping farmers make very, very local
decisions, even every square foot of your field, you could say, well, should I put more water
here?
Should I put more fertilizer there?
And so you'd have instruments and measurements on the field level and down to subfield level
decision making.
Exactly.
Here might be like Blue River, which was acquired by John Deere.
And they had a technology that you put on the back of a vehicle that would like, they
would identify weeds and spray fertilizer on the weeds to make them burn out or something
like that.
Exactly.
So it's a different, but then so that's what we call precision ag or what the world in
general calls precision eggs is really advising things maybe on a few feet at a time.
We're dealing with a different class of problems, which we, which we don't think there are any
great solutions for right now, but it's more macro.
So maybe you're not trying to decide what to do on every square foot, but if you have
a chunk of land and you're trying to decide what is the best use of this land, or then
in terms of what types of crops are suitable in this place, what other parts of the world
have had success with this type of crop.
What are the environmental conditions that make one type of crop versus another?
So basically, one example is, what should I do with this land?
Six months, one year.
Another example is, well, if you've been producing a certain type of crop and you're concerned
about the trends in production of that crop in other parts of the world, let's say you're
a coffee producer from Vietnam, which is a very large coffee producer, and you're worried
about oversupply or shortages, well, you should be able to quickly analyze what's going
on in Brazil, even though you're in Vietnam, because you're essentially working on the
same commodity.
So that type of decision, you know, you say, well, will the production this year be greater
than expected, less than expected?
What's the weather like in the different producing countries?
What are the expected yields?
So those types of decisions in terms of what crops to plant, how much to expect in terms
of supply and demand are another category, and then going on down the line, you know,
you could be, let's say, a food processor and you're buying a wholesale wheat, let's say
from corn, and you want to know where to buy it from, and maybe you're importing it from
different places, so you want to predict which countries or which parts of the world
will have shortages or excess supply, that's another example, or, you know, in continuing
in other domains, you could have a financial application where you're, let's say, lending
money to farmers, so if you want to know the probability that these loans will become
delinquent, and that means you need to understand, you know, the environment and what the climate
is doing to the farmers, and the probability that their crops will fail, for example,
or that they will be a drought, or things like that, or that they're on the contrary,
that there will be a huge production, and then the prices will be low.
So all these things obviously affect the livelihood of the farmer, and therefore, as a lender,
you would want to know the probability of those things, whether you're, or even if you're
providing insurance, crop insurance, and you could be a bank, you could be a government,
you could be all kinds of different entities that deal with that.
So, yeah, there's a long list and another macro type of decision that we enable is, you
know, impacts of diseases, you know, that I mentioned swine flu, this was a big topic
in the world of agriculture last year, there is aflatoxins, there's like all kinds of
different pests and diseases and things that affect crops that are affected by weather
and climate, so, or the locust that you mentioned is also, you know, it's a typical phenomenon,
but the intensity depends on weather, so if you wanted to know that it was going to be
more severe or less severe than usual, those are the types of decisions that we would
help you make.
And now you started with talking about kind of macro level land use and how that's different
from the precision egg.
Are you, does that, is implication in part that you're less worried about the actual
land itself, like it's clear that a big part of what you do is like market analytics,
if you will, and like thing entire like supply chain of agriculture and different forces
that play there, but are you also looking at the nutrients in a kind of macro plot of
land, or is that all left to like the precision egg piece?
No, no, that's really important actually, yeah, even on, so for example, let's say you're
trying to understand if there's a shortage of one crop, how much will people substitute,
how much substitution would there be in another crop, right?
And a big part of that equation is understanding the content of it.
So for example, we don't just try to predict the how much wheat will be produced, but
we try to predict how much certain types of wheat would be produced because they have
different protein content, and that affects a lot of things, everything from how much
will be consumed for different uses, and as well as what the prices will be for different
categories of things.
So yeah, protein content or calorie content of different crops are, is a huge part of it.
Also things like, you know, if what's the trade off between, I'll give you an example,
I mentioned the swine flu, so that's a disease that's been affecting the pigs in China,
and it's a huge deal because that's the main type of meat that's consumed there.
Well you might want to know, well, okay, if there's suddenly a shortage of pork, would
that translate into an increase in chicken demand, right?
Because people, if pork becomes very expensive, maybe people will start eating more chicken.
Now there's obviously many variables there, culture, and so on, but one of the key components
is, well, how much nutrition do you get?
Like what's the equivalent amount in terms of grams of protein between these two things?
So the biology of plants and understanding the content is critical, even at the macro level.
Now when I think about all of the problems that you just described, and then think about
data that I might want to have access to in order to begin to try to solve these problems
or, you know, provide some insights about them, you know, that seems huge, almost, you
know, I don't want to be hyperbolic, but like infinite, like there's so much data that
you could plug into or might want to plug into, you know, a system that solves these
kind of problems.
Like, what are your typical data sources?
How do you approach data acquisition?
That's a great question.
So I think that, you know, just to cover the types of data.
So the first thing people think of and, you know, that jumps to mind because it's visually
important and quantity wise, it's important is satellite data.
We have, you know, in terms of just sheer volume of data, that's the majority of our data,
obviously.
There's incredible amounts of information about every pixel on Earth, and it's not just
images in terms of, you know, visual photography that as you, most people would imagine when
you think of a satellite picture, some of it is infrared and ultraviolet and, you know,
so different, all the entire frequency range of the electromagnetic spectrum has information
about what's going on.
So what happens is that you have different satellite hardware, which is, which is able to
capture different bands of this electromagnetic signal is bouncing off the Earth.
And from that, some of it is visible, so, you know, so, and some of it is not visible,
but from that you can derive really complicated and very useful things.
For example, you know, obviously you can derive temperature, which, well, maybe it's not
so obvious, like how can a satellite that's way up in orbit know what's the temperature
on the ground, but it looks at various signals bouncing off the Earth, and, you know, there's
a well-established science that says, okay, but, you know, we fit a mathematical model
around that, and we can determine based on the electromagnetic signature, what is the
temperature at this, on each point on Earth.
Similar to how, you know, you can look at a planet or a star and figure out its chemical
composition just by looking at the electromagnetic signals that come out of it, we can do the
same thing with the Earth.
So it's temperature, rainfall, you know, including microwave signals, so you can get an idea
of what kind of clouds there, how much rain is there, and how much humidity is coming out
of the ground and so on.
So you can estimate temperature, rainfall are more complicated things like evapotranspiration,
which is an extremely important signal.
It tells us how much, as the name implies, how much water is being released into the atmosphere
from the transpiration or like sweating of plants.
That's a really important indicator of plant health, and similarly, there's another signal
called vegetation index, so again, looking at just the colors and the infrared signals,
the red and green and infrared, you can determine how much biomass there is, like how much
living matter there is in the flesh of these plants.
That's also an extremely important signal, and you can get this information from satellites
at an individual pixel level and covering almost the entire world.
So there's enormous amounts of data that's from satellites, and that's, yeah.
That's rapidly changing, and probably the biggest part in terms of volume, not probably.
It's way more than half of our data, but in terms of importance, and this is a key point
for us, is that, well, the economic data and social data is all equally important.
So prices, for example, or commodity prices, or retail prices, or production, quantity,
or yield, historical yields, or how much fertilizer has been used in one place or another.
What is the cost of transportation from one place to another?
All of these things fit together to enable people to answer these complex questions, typically
involve climate, weather, economics, and that society.
Even within economic and social, you've got wildly disparate data types.
You've got an imagining time series data that you need to deal with, as well as documents
that are best considered from an NLP type of perspective.
One of the challenges is just as a baseline, many countries, governments, and non-governmental
organizations and companies have been collecting enormous amounts of just basic agricultural
data, in some cases for a really long time, like the US Department of Agriculture, for example,
has over 150 years of data on everything from the yields of every single crop in every
single county in the United States, to the production, and how much area was planted,
and the crop conditions, where they're healthy, unhealthy, what percentage of the corn in
this particular county was considered good as of July 3rd of this year, for every year
going back.
So there's incredibly granular data that we're tapping into.
And same thing, from the United Nations, from the World Bank, from all kinds of different
organizations, academia.
So besides the obvious satellite data, there's enormous, enormously amounts of ground-based
data that have been collected.
And as you mentioned, some of it is really organized nicely and has modern APIs, let's
say for exchange rates, we have excellent APIs, and we're not reinventing the wheel, you
can figure out, you can get currency exchange rates in a really good way.
But in some cases, it's obscure data sets that were typed up 75 years ago, and now exist
as some text that's been scanned into an image, and then the image has been put into a PDF,
and maybe it's in a different language, so you would have to do a combination of OCR, optical
character recognition to extract the text from the images, and then do MLT to interpret that
text and then extract structured data at the end of the day from that, which could be
yields, for example.
So the data itself is often a big part of the challenge.
We probably could spend a whole hour talking about the data, but let's maybe take a step
back and contextualize this by looking at the types of problems you solve.
So we've talked about this from an end user perspective, but are there, do you approach
each of the problems that your customers are looking to solve as kind of one-offs,
like do you do consulting, or do you have specific categories of products that you offer,
and most of the work you do kind of falls into different types?
Yeah, and on a great question.
So the first point, I think, is that we're offering a platform, so everything we do,
as much as possible, except for some rare exceptions, everything we do, we attempt to really
productize it and make it scalable and make it available to all our users, and we're
going to talk a lot about machine learning and AI, but I think one important point is
that we offer a platform where people can access all of this data in a highly organized
way, and then they can access it visually through a web application or through an API,
and we expect our customers to also build their models.
So in a way, we're providing sort of this platform that gives you access to a normal wealth
of data, enormous wealth of data, creating value by deriving data on top of these existing
data sets, but also making you able to access it in a really organized way.
So one of the applications that we have for AI is we've structured this information into
a knowledge graph, so we have what some people call an ontology.
So we have the grow ontology, which organizes data from hundreds and hundreds of different
sources into a common language, so if we're talking about wheat, and there's many dozens
of varieties of wheat, you know that a bit of data from one source that tells you maybe
the protein content of wheat, and then another source that tells you the price of wheat,
well, we have to make sure that the specific type of wheat that these two things are talking
about is the same item.
So also organizing all of that information into a structured knowledge base or in the form
of a knowledge graph that relates all of these entities is a key part, and then our users
can use that through an API or a web app to find insights in the data, and in some cases
we will do the extra work for them in terms of like building forecasting models, predicting
things that the world is interested in and making that part of the platform as well.
And so for the different types of modeling tasks and the machine learning applications,
what are the types of problems that you're typically attacking?
So roughly, I think there's four classes of problems.
One that the first one is yields, agricultural yields, which is a hugely important.
We've been now doing that for about a live in production for a little over three years.
And so this is things like for the first example was, can you predict the yield of corn
in the US in three years?
So in February 2020, we already have a prediction of what the final yield of the US corn production
will be, meaning how many bushels or how many tons of corn will be produced per acre
on average across the United States, as well as down to every single county.
Now we're in February, so nothing has been planted yet, so we have a model that's based
on historical trends, but as the season progresses and we get into March and April and things
are getting planted and they're starting to grow and we see the weather, our prediction
gets constantly refined.
So when we say yield models, basically telling you what everybody will, the conclusion
that we will reach a year from now, meaning how good was the corn yield in the US in 2020,
we're constantly estimating that through a yield model.
So if you log into our product every day, whatever little bit of information comes from the
world that would help us better forecast the US corn yield will be incorporated and this
is fully productized so that there is an answer updating itself every day.
So while this is interesting, obviously people have been concerned with US corn yields for
a really long time, but traditionally it's been done through very slow processes where
there's a lot of human and subjective elements and maybe you'll have official updates on
this year's forecast will be updated maybe once and once throughout the course of the year.
So we're saying that we'll make that completely objective, automate it and do it every day.
So we did that for corn in the US and started out putting our predictions in real time
and it proved to be enormously successful in 2017, 18, 19, then those seasons are we correctly
anticipated the final result or in several instances before the market or any other forecast
or the traditional forecast had detected them and when the truth finally came out in terms
of when at the end of the season when the full crop was realized in each time we'd sort
of been out on a limb and predicting something unusual, it turns out we were right ahead of
the curve.
So that was an enormous success so that's yields and then we've done that now 10 times.
We did it for corn in the US, soybeans in the US, in Argentina and in Brazil, wheat in
India, in Russia, in Ukraine and corn in China as well.
So again, a number of different cases where we've managed to build really good yield models
that are sort of the state of the art and have been able to predict real world events before
anybody else was.
So that's been one area of huge success yields and in each case, like I said, our customers
would also look at maybe they'll look at, you know, bananas and Ecuador tomorrow and
we may not have already built a model for that particular case but we've shown a general
framework that applies to many crops across the whole world and more importantly we're
providing you all of the data so that you can use our models and use our outputs or you
can also use the data and produce predictions of your own if it happens to be a series
that we are not explicitly forecasting.
So that's yield and well, before I move on to the second area, one thing I want to say
is we were very strategic about what we choose to model.
So as in, for example, in the yield case, you know, I gave you a bunch of examples.
Each one sort of pushes the envelope in a new interesting way.
So for example, compared to the US, let's say one of the key pieces when we started modeling
wheat in India as well, a big difference is in the US, you know, most of the heartland,
the corn belt or the wheat belt and so on, especially corn is concentrated in the Midwest.
So even though it's a huge area, the climate in the major producing areas doesn't change
that much.
So, but then you go to India and you say, okay, India produces a lot of wheat, but it's
planted all over the country from the north to the south.
So it's actually a key technical challenge that this new example introduced is that, well,
how do you do accurate crop masks, you know, how do you model the effect, you know, when
it's a wide range of latitudes from north to south and the climate is very different.
And then the second piece there is, unlike the US, you know, the farms tend to be really
small.
So figuring out, you know, which points have wheat versus rice versus corn and so on or
just forest and, you know, other things is much more difficult in India.
So just all that to say that each of these examples is sort of pushes the technical envelope
in a different way.
And so that was, and so yield is that that's one big area where we've done a lot of work
and if you go to our website, we've published a bunch of papers on that and in our products
where, like I said, you know, these forecasts are there on a daily basis fully automated.
The second area, fairly closely related is crop masking, so that is a key piece of yields,
but it's yield forecasting, but it's its own problems.
It's basically looking at satellite imagery as I described before, not just visible, but
all the whole spectrum across time.
We try to predict what is or ought to figure out what is going on in each pixel in terms
of is there wheat being planted, is there sugar, is it bananas, is it coffee, is it what
the crop is.
And this is extremely important and we do that by looking and that's called crop masking.
So it's an important point because by itself, it's a really useful bit of information
to have.
You're essentially trying to label each pixel with a crop?
Yeah.
And what makes it complicated is that, well, first of all, a lot of crops look the same
in terms of, so you use things of how they evolve over time, you know, if it peaks in,
you know, if a certain signal peaks in July versus it peaks in August, then it tells you
whether it's this crop or that plant, this other crop.
But yeah, so we're trying to map the figure and then secondly, is that if you could assume
it's all constant, then it's a fairly well solved problem, you know, you could, but the problem
is, you know, the world is dynamic.
So even in the same form, there's such a, there's a thing called crop rotation where
people, for various reasons, will plant different things every year and sometimes it will be
a predictable cycle or sometimes it's driven by the weather and sometimes it's driven by
market conditions.
So that happens a lot in the US with corn and soy, for example, the same farmer could
choose to use this land for corn or use it for soybeans.
So if you're trying to predict the corn yield of this year, 2020, knowing which pixels are
corn and which pixels are soybeans, obviously has a huge impact because when you then apply
the other signals, let's say the temperature, you know, you need to know, am I looking at
a temperature in a spot where there's corn or am I looking at a temperature in a spot
where there is soy beans because we will have a different effect.
So crop masking is its own problem, even though it's an important input to yield, it's its
own huge problem and so that's a big area where we're doing a lot of work in and that's
a different type of machine learning.
So yields that I mentioned before is you often using regression methods, whereas crop masking
is heavy on image processing and in season crop masking is a combination of, you know,
these image cubes of the same place over time, you apply with neural network techniques
applied to it, help you make a much better guess than otherwise in terms of what each pixel
is.
So that's so crop masking is another example and that one is uses a lot of image processing.
A third example of a class of problems we work on is droughts.
That's very similar to yields, but for different applications that predicting droughts, there
are standard international classifications of, and these are discrete classifications.
This is a drought of level one or level two, level three, you know, these are the correspond
to severities kind of like you have hurricanes, severities and so on.
So unfortunately, the world doesn't have a single, automated, fully objective way to
classify the entire world and say, okay, is there a drought, yes or no, how intense is
it?
Is it a one, two, three, four, five, or zero, or, you know, meaning it's totally normal.
So what we're trying to do, but there is some manual processes that have been developed
over time, historically, for example, governments like the US government will pay or many governments
will pay farmers if there is a drought, it's a, you know, drought insurance.
So that type of stuff, you know, is very inefficient because there isn't a single benchmark.
So we're trying to create a completely objective drought index that the entire world can agree
on and it applies for every place in the world, takes into account all the weather and environmental
signals and produces a consistent labeling, you know, drought severity.
And so that's an interesting class of problems that we're working on.
And the fourth category that I haven't mentioned yet is a completely different type of problem,
which is the knowledge graph automation.
So as I mentioned, you know, we bring in data from, you know, dozens and dozens of sources
all the time and we want to organize that into a common ontology so that if we have some
data, like I mentioned about corn, we know it's about corn.
If it's yellow corn, we know it's about yellow corn, et cetera, or we know if it's temperature
on the ground versus temperature, you know, on the land surface versus temperature, you
know, in a weather station, a couple of meters above ground, it's a totally different concept
like the temperature of the ground and the temperature of the air.
So we, you know, our system has to know what exactly you mean by temperature.
So this means it's highly, highly structured.
That means all these data that come from outside sources have to be mapped and transformed
into this common structure so that we're referring to the same entities.
And that, as, you know, we're growing exponentially, we currently have about 55 million data series
a little over that in our platform and it's doubling every six to nine months.
So one of the key pieces is that, you know, we want to be really accurate but also efficient
about mapping, you know, outside knowledge into our internal knowledge graph.
So that means we have a whole class of problems related to knowledge graph automation.
So that involves graph algorithms and NLP and, you know, slightly less structured neural
network approaches and so on.
That will help us, you say, when we have a source that says, you know, if some of it is just
plain language translation as well, but it's really understanding that, hey, if we have
a Vietnamese data series that talks about, you know, the yields of rice and there's three
varieties of rice, it's really important that, you know, we map that correctly into the
items that we call these three or 10 varieties of rice and making sure that, you know, the
Vietnamese word and the English word, you know, are referring to the same thing.
Right.
So in addition to any models that you're using to kind of extract the data, for example,
you talked about pulling tab your data from PDFs, you're also using models to kind of
dynamically update the way data is kind of ingested into this knowledge graph and how it's
labeled and things like that.
Exactly.
Exactly.
So the knowledge graph, you can think of it as the knowledge graph is sort of the foundation
and then using that knowledge graph, we map stuff.
So and we automatically try to learn what the description of things that come from outside
and map that into our into our ontology so that, but this part, by the way, I think is
an important thing to note is we, you know, AI will help us get 80, 90% of the way there,
but we still have human beings really review and understand these things so that if our
knowledge graph automation says that, oh, a maze is the same thing as corn, well, there's
still going to be, you know, unless it's like 99.99% confidence, there will still be a
human who says, is really is a maze really the same thing as corn, yes, it makes sense
and proves that.
Well, it's a lot easier to fix those problems before you pollute the pool, so to speak,
when you have, was it say, 55 million data series that you're mapping?
Yeah, exactly.
Exactly.
So I think the, sometimes, and those, the way the data comes in from outside, sometimes
it's not clear if, or, you know, the, the, the, not every source has a cleanly organized
way of representing how, what something is versus how it's measured and things like that.
So yeah, so yeah, that's, that's, we're creating the, the transformation instruction set,
if you will, through AI.
Speaking about kind of that quality of that data set that you're working with, how do
you try to account for noise or missing values in the data that you collect?
Do you try?
Do you kind of have a principle that says, we're just going to record what we're given?
Or do you try to correct it on ingest?
What's your approach there?
Yeah, no, great question.
So I think that, for us, reproducibility and attribution is really, really important.
So at the bottom layer, you know, or I should say that the first layer in our platform
after the data has been transformed, the values that we, we have are can be directly traced
to, to where it came from.
So if we say, you know, the MDVI of, you know, the meaning of the amount of vegetation in
this particular pixel is, you know, 0.5 or whatever, like then, then, then, we'll have
that value where it came from.
If the source revised it, we'll also annotate that and say, okay, this was the yesterday
they said it was this much and then today they revised it to this much and this applies
to, you know, economic data as well as satellite data, you know, hardware or changes or algorithm
changes.
So, and so even after transformation, we always have the original value and we annotate it
with things like when it was reported, when it was revised, etc.
So all our data series are actually three-dimensional in the sense that there's the value, there's
the time and then there's also the potential revisions that could have occurred in the past
on that particular point.
And that, that's really, really important when, especially when you're dealing with things
like estimates or forecasts and so on.
And then, like, and of course, it's very important because we don't know in advance every single
use case that people will be using it. So if you've, if you've done some analysis and
then the data changes because the source made some corrections, it's really important
that you can, as a user, you can totally see why that happened and where it came from.
That said, on top of that, we add, so we do a bunch of things.
One is that, as I mentioned, you know, we build models, so we create brand new data that
are forecasts and predictions and so on from the existing data and those are available in
the data in the platform, just like other things.
So as I mentioned, the yield and the drought index and all these things that I mentioned.
We also do a lot of grow derived, which is grow derived data series, which are like basic
operations that combine data from multiple sources.
So for example, you know, you may have a trivial example would be, you know, we have population
data from, let's say, the World Bank about every country in the world and how it's growing
and including projecting it into the future.
And then you have consumption data, let's say, from some other organization or then, you
know, and well, if you join those two, you can get per capita consumption, which, you
know, so you get some data from one source, some data from another source.
You combine them and you create a new data series that's of independent interest.
So that's, so we do a lot of those types of things as well.
And then finally, I think the most basic answer to your question, though, is that, you
know, we try, if something is fundamentally difficult data, we often try to get multiple
sources.
So if you go into our product and you say, how much sugar was produced in Brazil last
year or even, you know, last month, you use probably five different sources that will
tell you that we have that, give you that number and, you know, they might differ by a few
percent because we use different methodologies and so on.
And you can, you can take them each individually or we can give you the average of them or we
can give you what we've selected as sort of the best combination of things.
So yeah, the answer is, you know, often you triangulate the truth from multiple sources.
And that's often the big part of the value, right?
You know, if you look at, especially these hugely important economic, hugely economically important
commodities, you know, there's a lot of speculation and hidden information and so on.
So a lot of the use cases are about getting the same information from multiple sources
to get a better version of the truth and then building decisions on top of that.
And with that, you know, the speculation and competition, even on a kind of nation state
stage, do you worry about adversarial use cases or data poisoning or anything like that?
Yeah, I mean, I think, I mean, we don't have any specific concerns, but it is the type
of thing that each customer would have their own concerns. So we try to make sure that
if there's, if there are multiple versions of the truth, we try to get all of them or
all the relevant ones. So, but at the end, so I think at the end of the day, also satellites
are, it's hard to lie to a satellite. Although it's possible, it's possible, you'd be amazed
that are the adversarial things that can be done. But yeah, I think the, there is, there
is no magic answer. It's trying to get the best possible information, the most objective
possible sources and then make sure that it's interpreted and mapped and organized correctly.
And then obviously, then when we do modeling, we spend an enormous amount of time backtesting,
looking at historical data, looking at different algorithms. And I think this touches on another
point, which is really important that we haven't discussed yet, is that we try to avoid black
box models. If it's possible to predict something or forecast something with a model that
has, you know, more explanatory power, slightly transparent model, we will, versus a black box
model, we will prefer the former, all else being equal. So, for example, you know, we talked
a lot about yield. If we say, well, the yield, we think the yield is going to be higher than
expected this year, you know, all of a sudden, then we want to be able to say, well, the reason
why our forecast is optimistic or whatever, or just change yesterday relative to today
is because this signal moved, right? And that's a hugely important thing in terms of building
confidence in the models, and also letting people use them in the real world. So, if you
have a model that says, for example, the yield of soybeans in the US was dramatically
affected by the temperature in November, and that doesn't make sense, like physically,
because by November, all the soybeans have finished growing, and they should be, you
know, harvested by that point, so that why would the temperature affect it physically
biologically? So, our model is kind of, are not just black boxes where you just throw
tons of data and see what comes out. It's really saying, like, well, we want the model
to model reality and understand the signals that go in or signals that make sense from
a process, physical point of view, and then we can also open the box and see what it's
doing and understand it, and it makes sense from a biological and economic point of view.
It certainly makes sense that all things being equal, you'd prefer the more explainable
things, but often, all things aren't equal to find that you have specific use cases where
the advantages, performance, or otherwise, lead you to use black box models.
Yeah, so I think, yes. And, you know, then it's a trade-off, but, you know, we'll do whatever
is the best trade-off. So, there are cases where it's a little bit black boxy and neural
networky, but, like I mentioned, for example, the image processing and crop masking over
time, you know, it could be, if it's sometimes you would be able to explain it, because let's
say, you know, like I mentioned, there's rotation between crops, sometimes it's a very
regular thing, you know, let's say, one year, you plan, sorry, one year, you plan corn
and you add her. And so, if it's a very predictable pattern and that comes out of it, then you're
like, oh, okay, that pattern was picked up. But, if it's much more, but sometimes in this
case, this is an example of, we've tried both approaches, and there are cases where,
I shouldn't say both, both classes of approaches, and there are some cases where, so we reach
the limit. So, we went through a classical approach where it's not black box and we're
explicitly modeling every single signal and looking at its temporal signature and saying,
you know, when did it turn on, when did it turn off and try and to fit it into very specific
models of behavior. So, we've done a really, we've gotten, we've gotten really great results,
but sort of we got to the point where we want to go even further and get higher accuracy.
And in that case, you have to take models with, you know, many thousands of, there are
tens of thousands of signals, so you're not necessarily going to be able to interpret
it every time. And do your models tend to be, you know, wide or monolithic types of models,
or do you rely heavily on hierarchy and assembling and things like that?
More of the latter. So, for example, when we work, so hierarchy, time scale hierarchy is
really important because the nature of the, for example, let's go back to the yield example.
If you look at the, and yield, by the way, is super important because that's the hard variable.
If you ultimately care about how much food will be produced, but the amount produced is yield
multiplied by area, right? So, the area is relatively easy to estimate. Again, there's parts of
the world where that's hard too, but if the, but so the yield is the sort of the magic variable
that everybody is paying attention to a lot because that's the one that changes year to year if it's
too cold, too hot, et cetera. And so yields that are driven by two very different time scales.
One is if you look at the yield over a hundred years, you'll see just dramatic improvements,
you know, and depending on the crop, that curve will look like an exponential or a straight line
or a sigmoid shape. It's kind of like how you have more's law in electronics. There's like,
there's similar things happening in crops because people get better at breeding the right seeds and
figuring out what seeds work where and things like irrigation comes in or pesticides or fertilizers.
So, there's just long-term trends that are very important. So, that's the long-term trend model,
and that's one time scale, but that will only give you the, that won't tell you what changed
unexpectedly in 2020 or in 2019. So, so we do construct yield models explicitly as two levels.
One is a long-term trend model that's saying even even on, you know, the first day of the year
before a single seed has been planted, we already have an idea of where long-term
progress should be, and it's different for every country and every crop. And then off of that
baseline, so that, you know, that that by itself is valuable, right? Like you could just have long-term
forecasts, I can say, okay, this is what it's going to be like, you know, in Germany in 2021,
and in India or Pakistan in 2022, etc., you can have forecasts. But then the short, the intra-year
part is saying, okay, what's going on this year? How much has been planted? What's what was the
temperature, the time, you know, at different point, reason and so on? So the in-season model is
taking a whole bunch of other daily and weekly and so on signals, whereas the long-term model is
taking, you know, annual and historical trends. So, so yeah, for yield is an example where
decoupling those two, and you could try to model just the, just model it as a time series without
doing that. But then you're just, the problem becomes way, way noisier and harder and you just
don't get good results. And so, so it's very much, yeah, so that's that's one example. Another
example is demand, on the demand side, which we haven't talked too much about, but, you know,
if you are trying to forecast consumption of different things, similarly again, there is some
long-term trends that are driven by like economics, you know, and and culture. So, for example,
you know, the many countries throughout the world, you know, as the GDP, as the income per capita
increases, and people rise out of poverty, the amount of meat that they eat is going to increase
because when very poor people generally can't afford to eat meat or any kind. So, as countries develop,
the first thing, one of the things you see is the, the diets start to change. So, those types of
things are macro long-term trends, so you definitely need a long-term model for that. But then on
top of that, you know, on any given year, prices could be high, and so the demand for one thing
could be high or lower, or things, you know, trends can change. So, similarly on the demand side,
there is also these short and long-term timescales, and modeling them explicitly makes the
the problem much more solvable. And then I think more generally, you know, if we're trying to,
we feature engineering and hierarchical modeling is really important. Again, it goes back to the
point about black boxes, you know, we could, we have, as I mentioned, 50s or 56 million data
series, and if you translate that into different values, it's in the hundreds of trillions
of data points. So, one approach would be, hey, for every problem we have, just throw every data
point that exists in our country to it and say, yeah, sure, let the machine learn it, right?
But the problem is you can't give a machine like 600 trillion data points and say, okay,
predict one value, it needs a lot of help. So, we have enormous amounts of expertise in the
company in terms of, you know, remote sensing experts, agronomists, hydrologists, some people
like that who are saying, okay, well, don't just throw a million features, these are the 100
features that really should matter, right? And then when we see impact of different features,
we can say, oh, yes, that makes sense. But this feature, this phenomenon, this cause, you know,
this cause has this effect because we understand how these, whether it's biology or economics
or transportation or what have you. There is some domain knowledge that that says, yes,
this makes sense. Well, I know we're running a bit long here. If you've got time for one more
question, I'd be curious to have you, you know, just give us a sense for your overall modeling
process. And in particular, if there are any unique aspects to the way you approach models
beyond the things that we've talked about as far systematically. Yeah, I think, I guess,
the one of the key things is what we choose to model. As I mentioned, you know, we have a platform
where you could really be modeling a million different things every day because we're giving
this highly organized platform that allows you to answer all kinds of questions.
There's a few things we do before we decide to model something. One is, is it an important problem
to model, right? Like all the examples that I gave, they're not just, they're economically
really interesting for our user base. So, you know, I'm like, for example, I mentioned a couple of
times wheat in India. Well, you know, there, of course, there's really cool scientific and technical
challenges, but the first question is, does anybody care? And in that case, the answer is, yes,
it's a huge question because historically, you know, India obviously is one of, you know,
one of the largest countries in the world in terms of population and the second largest.
Historically, and wheat is a big staple there, you know, for the people of India. And historically,
though, for the last 50 years, India has been self-sufficient mostly. So, they produce a lot of wheat,
and they consume a lot of wheat, but it doesn't quite interact too much with the rest of the
world's wheat. But now, you know, because India's continues to develop economically,
and as well as, you know, there are some theories that their green revolution of the last 50 years
is sort of leveling off, and they're kind of, some people are concerned that their production
will not continue to rise as it has, but the demand will continue to rise. So, if that happens,
and suddenly a small percentage change in the supply and demand of wheat in India
could translate into enormous amounts of imports, right? Because it's sort of like, you know,
you have this huge number of production and a huge number of consumption that are kind of
well balanced, but if there's a small change, even a 5% or 10% difference shortage, and then they
need to import that much, then that means, you know, a huge new source of demand in the world,
and maybe Australians will start producing a lot more wheat to export to India or Canada and other
countries who are importing will have to import from other places. So, it has, it's sort of like
this big iceberg that's hiding under the water that could have a big impact. So, all that to say
is that the first part of modeling process is, is it an interesting thing to model from a business
point of view, and is it fundamentally? And so, you know, and that, and there's a lot of them,
but it's important that we work on the right ones. And secondly, is then we, like I said,
we don't come at a problem with a solution, which I think distinguishes us from many companies
where we didn't start out as an AI company saying, well, you know, let's just find the
problem and say to work on and stumble into agriculture, it was more a joint thing of figuring out
that, you know, this particular domain needs these answers and needs better decisions, but we don't
care if we end up using neural networks or gradient descent algorithm xg boosts our random forest
algorithms and components, but we're agnostic to technology, but we want to solve the problem.
So, I think that the second part of our approach is where every single new problem were prepared
to use different approaches. But third, third piece is that when we build a particular model,
we do try to build a framework that helps us experiment and reuse things in different situations.
And so, for example, even though each country and crop is different for a yield modeling,
we do have a basic framework that applies across the board and some, you know, the input signals
will change, but there's two or three key algorithms that we will always use and then the specific
inputs will be different, but the back testing and how we evaluate our results and so on
will be following that process. And yeah, we try to look at, finally, when we're getting close to
having something that we like and that we're ready to launch, we look at performance in very
unique ways. So, for example, you know, we don't look at just the average value of our error or
just the signal. But, you know, even when we're predicting a single number, let's say the wheat
yield of Russia or the Black Sea region, it might be a single number at the end, but underneath it,
we're actually making that same prediction in a much more granular way. So, we look at,
oh, how is the error distributed spatially? Does that make sense? How's the error distributed in
the back testing? Like, when we run it, when we back test historically, does it, you know, what are
the years where our model would have performed better or worse? And why is that? And does that,
does it just random noise? Or is it like, does it give us features that we should model?
So, we have a very iterative process where we look at spatial temporal distribution
performance and bring in a lot of domain expertise to sort of figure out the feature engineering
and tweaks that we need to do to have a good model. Is it hard in your case to know when to stop?
When it's good enough? No, I think we're lucky because, again, from the first point I started with,
we usually have a very clear idea of why this is interesting. And together with that comes an idea
of what's reasonable to move the needle, right? So, in terms of to add value to the world.
So, you know, like, let's say if you're trying to model something in the United States, then,
you know, there's typically going to be a lot of really good inputs, a lot of historical data,
and better than most other places in the world. So, we will, our expectation will be like,
well, okay, to make a difference in this case, we really make, we've got to make sure that our
errors less than 0.5 percent, right? Because anybody can get to within 2 percent or whatever, right? Like,
the data is so good. Yeah, exactly. And conversely, you go to a country where, or part of the world,
where there's very little data and there's no, or maybe, maybe no ground truth at all,
so on. And then you're like, well, okay, everybody's the best anybody in the world can do is 10 percent.
So, or 20 percent, or they have, or maybe 50 percent, because they have absolutely no idea what's
going on. And then in that case, we're like, well, okay, can we consistently robustly make a
forecast that has an error of 10 percent? And that's, maybe that's awesome. That would be terrible
for for something that, you know, that everybody else has a 1 percent error on, but if it's
everywhere else has a 20 percent error, then we'll put it and we'll be confident that we're adding
value. So, so we always know when when we're adding value and when we're not adding value. And so
that gives us an easy way to say, okay, let's launch this. Even though there's a million things
that we could do to improve, we're already ahead of the, you know, we're adding value.
Well, Nemo, thanks so much for being so generous with your time and sharing with us what you're
up to there at Grow. Very much appreciated. Yeah, thanks, Sam. And yeah, I enjoyed it. I hope
it was informative and inspiring. Absolutely. Thanks so much. Thank you.
All right, everyone. That's our show for today. For more information about today's guest or
any of the topics mentioned in the interview, visit twomelai.com slash shows.
To learn more about the IBM AI Enterprise workflow study group, I'll be leading.
Visit twomelai.com slash AI workflow. Of course, if you like what you hear on the podcast,
please subscribe, rate, and review the show on your favorite pod catcher.
Thanks so much for listening and catch you next time.
