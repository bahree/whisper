WEBVTT

00:00.000 --> 00:16.320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

00:16.320 --> 00:21.480
people, doing interesting things in machine learning and artificial intelligence.

00:21.480 --> 00:46.960
I'm your host Sam Charrington.

00:46.960 --> 00:50.400
Thank you very much for having me, this is a real pleasure to be here.

00:50.400 --> 00:51.400
Awesome.

00:51.400 --> 00:52.400
Thanks so much.

00:52.400 --> 00:56.200
I am really excited to jump into this conversation.

00:56.200 --> 01:00.480
You are someone that I follow on Twitter and we've had these kind of back and occasional

01:00.480 --> 01:06.240
back and forths over time and it's great to finally meet you in person.

01:06.240 --> 01:09.280
You've got some pretty varied interests.

01:09.280 --> 01:14.120
You spend a lot of time, your research focus on reinforcement learning.

01:14.120 --> 01:17.480
You also tweet a lot about music and the arts.

01:17.480 --> 01:25.280
Looking at your background, you've done applied ML stuff at Google, add from and other things.

01:25.280 --> 01:29.600
Tell us the story, how do all these threads come together.

01:29.600 --> 01:35.240
Well originally I'm from Ecuador and I moved to Canada after high school to come study

01:35.240 --> 01:36.880
at McGill.

01:36.880 --> 01:42.600
So I did my undergrad and then I eventually did my master's in PhD at McGill with

01:42.600 --> 01:48.480
Dwayna Prikup and Prakash Panagaden and so part of the reason why I stayed in Montreal

01:48.480 --> 01:55.000
and McGill was for personal reasons, I was dating someone who's now my wife and I also

01:55.000 --> 02:01.680
had a band and I also had a band so I've always been heavily involved with music.

02:01.680 --> 02:05.840
I grew up with music, learning music, playing music so that was very important to me and

02:05.840 --> 02:09.080
I didn't want to leave that so I decided to make that choice.

02:09.080 --> 02:14.840
I know it's not the typical thing that suggested to do all your degrees in the same university

02:14.840 --> 02:18.680
but for me it was more important to keep playing music.

02:18.680 --> 02:24.760
So I graduated, I finished my PhD at around 2011 and then I moved to Paris for a post-doc

02:24.760 --> 02:29.040
and this was at a time where AI isn't what we see here with 12,000 people in this conference

02:29.040 --> 02:34.120
I mean NURPS didn't have, back then it was called NIPS, maybe 4,000 people.

02:34.120 --> 02:39.640
So I wanted to stay in academia and I was working at this intersection, that was very theoretical

02:39.640 --> 02:44.000
sort of between Markov decision processes and formal verification.

02:44.000 --> 02:48.320
So I was finding it really hard to find a job because I wasn't formal verification enough

02:48.320 --> 02:52.200
for the formal verification community and I wasn't reinforcement learning enough for

02:52.200 --> 02:54.520
the reinforcement learning community.

02:54.520 --> 02:58.880
And so after my post-doc I just feared, I already had two young kids and I feared that I would

02:58.880 --> 03:02.040
just be going post-doc to post-doc for too long.

03:02.040 --> 03:06.600
So I luckily got a job from Google doing applied machine learning and ads and I actually

03:06.600 --> 03:11.720
said goodbye to academia at that point, I stopped reading papers and then I did a little

03:11.720 --> 03:17.440
quick stint in Chrome doing building machine learning infrastructure, so back end infrastructure

03:17.440 --> 03:22.240
and brain opened up in Montreal and Mark Belmar who I had done my masters with, he kept

03:22.240 --> 03:26.480
in research, he was in deep mind for a while and he was one of the first people to join

03:26.480 --> 03:32.440
brain in Montreal and he put in a good word for me and so then they offered me to join

03:32.440 --> 03:38.240
them and I jumped at that possibility and I hadn't been following the research that also

03:38.240 --> 03:41.960
was a huge shock to come back, I mean when I was doing my research we were all working

03:41.960 --> 03:45.360
on grid worlds and very simple environments because a lot of it was theoretical, we didn't

03:45.360 --> 03:49.120
really use deep networks at all for reinforcement learning.

03:49.120 --> 03:54.600
So there was a lot of catch up trying to familiarize myself with the literature and how the

03:54.600 --> 03:57.320
whole landscape had changed.

03:57.320 --> 04:01.680
So throughout all this time I always kept with music, I had a few different bands, I've

04:01.680 --> 04:06.360
always been performing live and writing music and the other thing is when I started my

04:06.360 --> 04:12.080
PhD I was actually considering doing a PhD with Douglas Eck as well as with Doiner Precup

04:12.080 --> 04:17.720
in something with machine learning and music but at the time what was available for music

04:17.720 --> 04:22.840
generation didn't really excite me very much because it was still in the early days and

04:22.840 --> 04:29.440
I feared that it would taint my love of music and I just wanted to keep my music aside

04:29.440 --> 04:30.440
separate.

04:30.440 --> 04:34.720
But when I rejoined the research world and I saw what the magenta team was doing I was

04:34.720 --> 04:40.600
kind of blown away by the quality of things so then I decided to also start going along

04:40.600 --> 04:47.760
that pathway and I think the day after I joined Brain this artist from Canada, he's called

04:47.760 --> 04:50.400
David Usher, he's pretty well known in Canada.

04:50.400 --> 04:56.400
He approached us wanting to, he approached us, he was actually first had a band in the

04:56.400 --> 05:02.400
90s called Moist and it was really popular and he approached us, he wanted to do an album

05:02.400 --> 05:07.280
using like AI techniques and so we just met and kind of brainstormed and the thing he

05:07.280 --> 05:13.120
gravitated towards the most was lyrics and so Hugo La Hoshel who was my manager at the

05:13.120 --> 05:16.600
time was very generous because I had just joined Brain he's like do you want to take this

05:16.600 --> 05:20.240
project because I like music I said sure it sounds fun.

05:20.240 --> 05:24.720
I had never trained a language model, I was still trying to figure out all this deep

05:24.720 --> 05:30.040
network stuff because I hadn't looked at that but yeah, Hugo gave me that opportunity

05:30.040 --> 05:34.640
and I learned a ton in that project and so still it's still an ongoing project so relative

05:34.640 --> 05:39.720
to the first model that I trained with David which we actually made a video out of that

05:39.720 --> 05:44.160
like he rewrote one of his songs with this first prototype and it worked okay but the

05:44.160 --> 05:47.760
model we have now is so much better and I understand all of this language modeling so

05:47.760 --> 05:52.240
much better than when I did before and that's just that experience kind of showed me to

05:52.240 --> 05:56.080
not be afraid of sort of stepping out of because I'm very familiar in reinforcement

05:56.080 --> 06:01.400
learning which is background to step out of that comfort zone and go into other areas

06:01.400 --> 06:05.400
that I'm not as familiar with because they're all interesting problems and sort of really

06:05.400 --> 06:10.520
trying to dig into the details and for me the way I learned the most is actually trying

06:10.520 --> 06:14.640
to implement some of these models and architectures and play around with them because you read

06:14.640 --> 06:18.720
about them in papers and you kind of get it that's fine but until you're actually trying

06:18.720 --> 06:22.480
to get it to work for yourself it's that's a whole different experience and I've learned

06:22.480 --> 06:27.480
so much just from doing this like jumping from one one problem to to the next and in

06:27.480 --> 06:32.400
a separate kind of field and learning about those architectures but while still maintaining

06:32.400 --> 06:36.160
my research and reinforcement learning well it sounds like you've landed in an incredible

06:36.160 --> 06:43.840
place to do that not just kind of the resources of Google and the people that you're surrounded

06:43.840 --> 06:50.200
with and have an opportunity to interact with but your role seems to be defined as like

06:50.200 --> 06:56.720
advancing research you know via implementation absolutely yeah so I'm a software developer

06:56.720 --> 07:03.200
like that's my official title there's also research scientists at Google and until recently

07:03.200 --> 07:07.200
there were still like most people that are in research want to be research scientists

07:07.200 --> 07:14.600
because that's like then you're officially doing science right right so I my like if

07:14.600 --> 07:19.840
I had graduated say four years after when I graduated yeah likely I would have been applying

07:19.840 --> 07:24.440
for research scientist role back when I applied at Google that wasn't really a maybe

07:24.440 --> 07:30.040
Sammy Benjo was a research scientist but probably that about it and so I entered Google

07:30.040 --> 07:34.560
as a software engineer and sort of advance my career and that and that track and when

07:34.560 --> 07:38.600
I joined Google it was as a software engineer or developed we call it developer in Quebec

07:38.600 --> 07:44.640
because engineer you get an iron ring and I don't have that okay initially I was a little

07:44.640 --> 07:48.800
skeptical because the official description is you're there more supporting research scientists

07:48.800 --> 07:52.760
and so I was worried that I wouldn't have a flexibility to sort of pursue my own research

07:52.760 --> 07:57.600
interest but it's been not at all like that so I leave my own research projects and I

07:57.600 --> 08:02.720
still support a lot of people with the engineering aspects of it because I'm I've been working

08:02.720 --> 08:08.440
on this a lot so I'm more familiar with like Google infrastructure and just coding in general

08:08.440 --> 08:12.920
and it's been I mean a lot of the major advances that we see in machine learning and AI nowadays

08:12.920 --> 08:16.560
is a lot of it is engineering right so there is of course there's still math and there's

08:16.560 --> 08:22.280
still a lot of theory behind it but a lot of it is engineering and I don't think it I think

08:22.280 --> 08:28.240
more and more it is but a few years ago I don't feel like got the credit it really deserved

08:28.240 --> 08:33.680
and so living in this sort of intersection of pure engineering and pure research is for

08:33.680 --> 08:37.680
me super exciting because I kind of get to play around in both worlds and learn from both

08:37.680 --> 08:38.680
world.

08:38.680 --> 08:44.920
I've got a long list of things that I want to talk to you about but you mentioned something

08:44.920 --> 08:50.320
that's got me really curious the you know what it means to evolve a language model you

08:50.320 --> 08:55.680
started this prop project with David and came out with this you know early crappy language

08:55.680 --> 09:01.160
model and have evolved it over some number of years it's been like a yeah no it's been

09:01.160 --> 09:05.200
like a year and a half it's been actually it's been almost like two years I think since

09:05.200 --> 09:11.760
we started it but two years calendar calendar wise but it's not it's not one of my main

09:11.760 --> 09:16.760
projects so yeah exactly so it's when I get a chance that I work on that yeah so as I

09:16.760 --> 09:20.280
said when I started this project I had never trained a language model I like I

09:20.280 --> 09:24.560
knew what LSTM's were because I studied it in school but so the first thing I did was

09:24.560 --> 09:34.240
I actually Andre Carpati has the yeah this famous blog post the surprising reliability

09:34.240 --> 09:39.160
of of recurring neural networks something like that anyway so I brought that blog post

09:39.160 --> 09:44.160
and I got his code and sort of played around with it and that was the the V0 model just

09:44.160 --> 09:50.080
over characters and then I started tweaking that a bit and finding new data sets for lyrics

09:50.080 --> 09:57.880
and that initial model that was basically a variant of Andre Carpati's model was the initial

09:57.880 --> 10:02.120
model that I had and so that was okay like just a milestone like okay I was able to train

10:02.120 --> 10:06.320
this and actually get it to do what I wanted it to do but it obviously was it has all the

10:06.320 --> 10:12.240
shortcomings that these types of models do at this round I mean the attention is all you need

10:12.240 --> 10:17.760
paper had come out not not too much before then and so then I started looking into these

10:17.760 --> 10:23.480
attention models and and so it seemed like the right thing to do so I switched over to

10:23.480 --> 10:30.800
to the transformer model and started playing around with that and so the V2 model was using

10:30.800 --> 10:37.480
an attention model and so I had various versions of a V2 part of the difficulty that I had

10:37.480 --> 10:42.000
with the language with training these language models on lyrics data set is that the lyrics

10:42.000 --> 10:47.440
data set is is not the best what sense so the tricky thing about these language models

10:47.440 --> 10:52.600
is that and and for lyrics in particular is that you're trying to get this model to learn

10:52.600 --> 10:58.080
English kind of so how how to structure English phrases together but in a quote unquote poetic

10:58.080 --> 11:03.520
way and to not be boring right because you're trying to use it for creative purposes and

11:03.520 --> 11:06.880
so you don't want it to be boring so we trained this model and if you look at like perplexity

11:06.880 --> 11:10.400
scores and things like that it was doing pretty well on this lyrics data set but then when

11:10.400 --> 11:16.480
you actually look at the output it was extremely boring so because in pop songs you have lines

11:16.480 --> 11:21.840
that repeat often I mean that's just how songs are written so the model would tend to just

11:21.840 --> 11:27.200
repeat the same thing over and over and over again it also had certain phrases that it would

11:27.200 --> 11:31.760
keep on coming back to like they just had very high likelihood so one of you talked about this

11:31.760 --> 11:37.520
it I say like it's hand wavy but the the average pop line over the last six decades is you know

11:37.520 --> 11:43.200
that I'm the one and so that one came up a lot and you can also add sometimes you get you know

11:43.200 --> 11:49.120
that I'm the one come a baby so that's the average pop line it was boring and so the interesting

11:49.120 --> 11:53.520
thing about working with with David is that I build like variants of these models and I

11:53.520 --> 11:59.920
show him and one of the things he remarked on is that it was very non-specific in the sense

11:59.920 --> 12:05.600
that the nouns that it was using it wouldn't use proper nouns so it would use like me you he she

12:05.600 --> 12:10.960
they so it's very I'm kind of ambiguous if you think of like the Beatles I mean there's me

12:10.960 --> 12:15.760
Mr. Mustard Paul Avine Pam Jude you know there's all these I mean the fictional characters but

12:15.760 --> 12:20.960
they're characters yeah and so then you can sort of the ground the the song and something kind

12:20.960 --> 12:26.720
of real where's if you're just talking about him like hey you so let me down like that's not

12:26.720 --> 12:34.000
even though Pink Floyd has a hey you somewhere so anyway then from that feedback I started looking

12:34.000 --> 12:40.560
at other datasets so for instance the fiction books dataset these are available online so we

12:40.560 --> 12:45.440
traded with that and we got much more diverse vocabulary much more diverse themes than we would

12:45.440 --> 12:50.960
get with with lyrics but the structure of the what was coming out no longer looked like a song

12:50.960 --> 12:56.080
line like a song lyric it looked more just like a sentence that you'd sort of cut in half and

12:56.960 --> 13:03.760
it just looked like a sentence with new line characters thrown in random places so then I thought

13:05.040 --> 13:10.480
why not rather than trying to tackle this problem with a single language model where I'm trying

13:10.480 --> 13:16.320
to get all these constraints of like making being coherent from the English perspective being

13:16.320 --> 13:22.480
kind of creative and also following the structure of lyrics why try to do this all with one model

13:22.480 --> 13:29.680
like there for me there's no where you go to multiple models with the you know the language models

13:29.680 --> 13:36.480
you're working at working with at the time is there a way are you trying to express those constraints

13:36.480 --> 13:41.120
explicitly or just based on the data that you're feeding into it was I was more playing with the

13:41.120 --> 13:46.320
data so with the with the training data that I was using to try to enforce these these constraints

13:47.920 --> 13:53.200
so the the lyrics I mean the idea was well if we want to write lyrics then we should train

13:53.200 --> 13:58.000
on the lyrics dataset to try to replicate that distribution but there's as I'm saying it a lot

13:58.000 --> 14:02.800
of the interesting lyrics are in the long tail of the distribution and the long tail is terrible

14:02.800 --> 14:07.360
for a model because it's going to have low likelihood by definition but that's actually what

14:07.360 --> 14:14.000
we're interested in as like it so it has to be coherent but unlikely and so so that's part of

14:14.000 --> 14:21.360
the reason why I I was considering just going to why why try to do it with one single model

14:22.800 --> 14:30.000
as my ultimate my ultimate goal was to have something that artists could actually use and David

14:30.000 --> 14:36.640
could use for for writing a whole new song from scratch and so for him it doesn't like he

14:36.640 --> 14:40.720
won't care and most artists won't care whether it's a single model or multiple models as long as

14:40.720 --> 14:48.880
it is interesting and makes sense and just to understand the the goal is it start with some prompt

14:48.880 --> 14:54.880
and the thing spits out an entire song or kind of is the artists or is the model one in which the

14:54.880 --> 15:00.160
artist is kind of incrementally promptly in the model and kind of refining the output yeah it's

15:00.160 --> 15:04.160
more of the latter so the way we're starting to build out a website that I'm hoping to release

15:04.800 --> 15:10.720
early next year so it's it's almost ready but it's just as I said it's one of one project that I

15:10.720 --> 15:15.760
work on when I have time and with nirips and all that it's been kind of tight but the ideas yeah you

15:15.760 --> 15:20.640
have kind of like an almost like a notepad where you where you're writing your song and this is

15:20.640 --> 15:27.040
coming a lot from feedback from david with how about how he writes songs and so you have you write

15:27.040 --> 15:31.760
your part of your line or you can even start without a prompt and then just ask the the model for

15:31.760 --> 15:36.880
suggestions and so then the model will give you suggestions for the next line you can specify

15:36.880 --> 15:42.240
rhyming schemes so whether like if you want a bba or if you just want it to rhyme with your

15:42.240 --> 15:46.400
current line and then the model will give you some suggestions and then you can like drag those

15:46.400 --> 15:53.680
suggestions over to your your worksheet or inspire yourself from those the suggestions that the

15:53.680 --> 15:57.680
model gave and and then continue working with with your song this way so it's more

15:59.520 --> 16:03.200
what I what I'm not as interested in is having something where you press a button and it produces

16:03.200 --> 16:09.600
your next top 40 hit it's really like a tool the way of view it is as a tool so one analogy I like

16:09.600 --> 16:15.600
to use is if you think of recording music in the 60s or 70s if you wanted to record a good album

16:15.600 --> 16:20.560
you had to go to a professional recording studio and hire a professional recording engineer

16:20.560 --> 16:25.200
because otherwise it wouldn't sound very good now we have things like Pro Tools and Ableton

16:25.200 --> 16:29.280
that you can do it in your basement and like a lot of people have become very famous from

16:29.280 --> 16:33.840
recording albums completely by themselves in their basement so it's not like we got rid of

16:34.400 --> 16:40.000
recording engineers or recording studios they're they're still around and they're doing the

16:40.000 --> 16:46.560
same thing and you still require them for for many for many purposes but these tools like Pro Tools

16:46.560 --> 16:51.920
and Ableton that kind of democratize the recording industry and so a lot of people can start playing

16:51.920 --> 16:57.600
around with recording and maybe pursue music as a career by leveraging what they can do on their

16:57.600 --> 17:04.240
own and so I view these tools as sort of in the same vein where the idea is not to sort of replace

17:04.240 --> 17:10.880
musicians or songwriters but to enhance them so basically give them something that they can play

17:10.880 --> 17:15.360
with the other thing that I find interesting that that could be quite useful for this is if

17:15.360 --> 17:19.520
your first language is in English and but you want to write a song in English because you're

17:19.520 --> 17:24.160
living in English, the language in country this tool might be helpful for because it does provide

17:25.200 --> 17:29.440
well well structured sentences it might help you write your song because you can provide the

17:29.440 --> 17:35.040
context and sort of the themes and the model would help you write in a way that that's more natural

17:35.840 --> 17:39.440
sounding English than than than what you would be able to come up with on your own.

17:39.440 --> 17:44.960
Okay and you mentioned that you're the one of the things the artist is able to do is put in kind

17:44.960 --> 17:50.880
of the rhyming scheme that you're interested in is this kind of a filter after the model is generating

17:52.560 --> 17:56.480
possibilities or is it some kind of constraint that's introduced into model itself?

17:56.480 --> 18:01.600
It's not even no constraint or filter the way it's happening is by nature of how we're training

18:01.600 --> 18:07.280
the model so I have as I mentioned we move to this this system where we have two models so one is

18:07.280 --> 18:10.960
what I call the structure model so they're both transformer models but the first one is trained

18:11.920 --> 18:17.840
on the lyrics data sets but rather than using the English words I convert them to other parts

18:17.840 --> 18:23.200
of speech so like now an advert that type of thing and it also includes like the number of syllables

18:23.200 --> 18:29.200
in the line and then the final phoneme syllable so like basically this is for rhyming

18:30.240 --> 18:34.560
and so this model if you give it the structure of your current line it will the output of it

18:35.040 --> 18:39.680
what it generates will be the structure of the next line and so then you can feed that

18:39.680 --> 18:44.240
next line structure with the actual English words of your first line into the second model

18:44.880 --> 18:49.120
and this is another transformer model which I call the vocabulary transformer and so this

18:49.120 --> 18:53.440
one is trained on on books on the books data set because we want the diverse vocabulary from that

18:54.000 --> 18:57.440
and so what this model is trained to do is to fill in these blanks so these

18:57.440 --> 19:02.560
cat parts of speech and the condition on the condition on the so yeah so the first half gives it

19:02.560 --> 19:06.400
the context right because it's using English words the second half gives it the structure so

19:06.400 --> 19:10.800
the number of syllables what parts of speech and the last phoneme so we trained it with the books

19:10.800 --> 19:18.080
data set by essentially because we know the ground truth we can specify when you have this context

19:18.080 --> 19:23.280
and this structure this is the last phoneme and so the the the target that we're training to emulate

19:24.080 --> 19:30.000
matches that so it rhymes really well the one thing that we're working on right now is that

19:30.640 --> 19:37.120
the easiest way to rhyme is to just use the same word right so red always rhymes with red right

19:37.120 --> 19:42.960
so that's something that that we're exploring a few different approaches on how to overcome that

19:42.960 --> 19:48.640
so that we've tried a few things playing like the first things we tried just again playing

19:48.640 --> 19:54.000
with the data to see if we could discourage it from from rhyming with the same word so far none of

19:54.000 --> 19:59.520
those things have worked very well so like an easy thing you can do is basically when you have the

20:00.800 --> 20:06.080
the weights for the the possible tokens that the model kind of met just set the weight to zero

20:06.080 --> 20:10.560
for the words that you don't want to rhyme and then the model is forced to pick a different word

20:10.560 --> 20:15.760
when when decoding so we're trying a few other things that are a bit more exciting for us

20:16.800 --> 20:21.200
but yeah that's essentially how we get the rhyming for the model is this project is this has

20:21.200 --> 20:27.120
this been published and are you working on the website but like yeah we had a paper in the workshop

20:27.120 --> 20:32.960
the creativity workshop two years ago okay or last no it was last year sorry in the creativity

20:32.960 --> 20:38.160
workshop nerfs in Montreal last year but it's a two-page viewer I think it's on our archive but it's

20:38.160 --> 20:44.240
very like summarized to remember somebody asked me the one to collaborate on the project I sent

20:44.240 --> 20:49.680
them the link and he's like this looks like a proposal it's not a paper so yeah that paper is

20:49.680 --> 20:55.200
still very preliminary we have a more extended version but we are thinking of submitting it to

20:55.200 --> 20:59.920
somewhere we're just trying to refine the model so the paper itself is not published but we're

20:59.920 --> 21:04.640
working on on basically getting more human feedback because we can we have some quantitative

21:04.640 --> 21:09.600
measures but because it's the purpose is creative it's kind of hard to measure quantitatively so

21:09.600 --> 21:14.800
we want to get more humans to evaluate and measure it compared to other models or even true lyrics

21:16.080 --> 21:21.040
so we're working on that and also we're working on making the code open source so you can take it

21:21.040 --> 21:26.560
and train with whatever dataset you want so training with Wikipedia or something and you get

21:26.560 --> 21:31.600
what's the training time or kind of resource for the training well so we train we've been

21:31.600 --> 21:38.480
training on TPUs and that's really fast so the longest distributed like lots of TPUs

21:38.480 --> 21:45.120
no just yes okay and it's been it's been it's pretty fast on the order of like we were training

21:45.120 --> 21:50.800
less than in less than an hour oh wow so yeah it's great fast the the longest part is the pre-processing

21:50.800 --> 21:54.960
of the of the dataset if your dataset is very large because we do this decomposition into parts

21:54.960 --> 22:00.560
of speech and that type of thing that's typically what what takes the longest but once you do it once

22:00.560 --> 22:06.720
and then you can reuse it for whenever interesting I can I ask the question earlier about

22:07.520 --> 22:13.280
applying external constraints into the modeling process is there does that work with

22:13.280 --> 22:17.760
transformer models or their folks that are doing that kind of thing like almost like a model-based

22:17.760 --> 22:24.560
kind of there are yeah there's some work with that I've seen so there's there's some people

22:24.560 --> 22:29.520
in the Toronto office from brain that have been doing some there's one paper called the insertion

22:29.520 --> 22:34.800
transformer so rather than be coding left to right as you typically do with with language models

22:34.800 --> 22:39.360
I don't know all the details but essentially the the way it works is now you can emit tokens that

22:39.360 --> 22:44.080
say where to insert so not to go from left to right but actually inserting into different parts so

22:44.080 --> 22:49.600
this allows it to potentially start from like a higher level of the structure of the phrase that

22:49.600 --> 22:55.200
it's going to generate and then sort of start filling in more of the details once the structure

22:55.200 --> 22:59.520
is is there and so it's actually something I've been meaning to look at because this might be

22:59.520 --> 23:05.520
kind of useful for lyrics where you're trying to to satisfy some structure so maybe we can

23:05.520 --> 23:10.640
generate a full verse rather than line by line if we have if we start doing in this kind of

23:10.640 --> 23:16.160
incremental incremental way there's also like from the magenta team the the the music transformer

23:16.160 --> 23:22.800
that they've been doing and that I don't know I don't think they explicitly encode constraints

23:22.800 --> 23:28.720
but for instance the way they they encode time is is a bit different than than what we use for

23:28.720 --> 23:35.680
for language modeling and so that just the result of that is is for music at least is a more coherent

23:35.680 --> 23:42.000
and more pleasing we have started looking a little bit in reinforcement learning and seeing how

23:42.000 --> 23:46.080
we could potentially use that I'm just going to ask is there an overlap or interplay with our

23:46.080 --> 23:53.520
there is so we are looking into it a little bit it's tricky it's tricky to get right and uh yeah my

23:53.520 --> 23:59.040
thinking is always I don't want to over complexify it if I don't have to so that's why I always start

23:59.040 --> 24:03.120
with the simplest idea and if that isn't working then okay we start considering a bit more

24:03.120 --> 24:08.560
sophisticated things for many reasons I mean when something breaks it's easier to fix if it's

24:08.560 --> 24:13.920
yeah if it's simpler than if it's if you have a lot a lot of moving parts um and I also feel like

24:13.920 --> 24:18.640
with the reinforcement learning aspect of it um the little I've been playing with it with for

24:18.640 --> 24:24.160
this particular project it's a little harder to make it stable in the sense that you can kind of

24:24.160 --> 24:30.160
reproduce the same type of quality that from run to run so it might be interesting to an instructive

24:30.160 --> 24:35.520
to kind of explore like what's your first step how do you think about okay you've got this one tool

24:35.520 --> 24:41.280
that you know well or al you've got this use case area that you want to apply it to like how do

24:41.280 --> 24:49.120
you start to even formulate the problem um it's uh this is the way I work it's more trying to solve

24:49.120 --> 24:53.040
very specific problems that I have with so this is what I'm saying where I like to start with

24:53.040 --> 24:58.320
just a simple thing um because it probably won't work the way you want it and and there will be

24:58.320 --> 25:04.160
very concrete aspects that or problems that you have with it at the at the moment and so then

25:04.160 --> 25:10.400
you I like to focus on those particular problems and see what techniques I can use so um first

25:10.400 --> 25:15.760
is for for this rhyming constraint uh there is some prior work that that has done similar type

25:15.760 --> 25:21.680
things not with transformers but with with R&N so um Natasha Jacques when when she did a uh uh

25:21.680 --> 25:25.760
an internship with the magenta team she had a paper where she was using RL to

25:26.560 --> 25:32.800
for like an R&N that produces melodies um she was trying to use RL to make it encourage it

25:32.800 --> 25:37.600
to to respect the rules of counterpoint so counterpoint is this set of rules from music theory that

25:37.600 --> 25:43.520
um basically specifies how to write music for polyphonic voices so that it sounds better

25:44.080 --> 25:49.520
and so she was trying to use RL for this because the the the R&N for for melody generation on

25:49.520 --> 25:54.240
its own wasn't respecting this necessarily at all and sometimes it was producing like just repeating

25:54.240 --> 26:00.160
the same note over and over which is a violation of one of the rules of counterpoint so that uh she

26:00.160 --> 26:04.400
got some interesting results but I think part of the the limitation was also just the R&N

26:04.400 --> 26:09.440
and encoding it into the training process of the transformers so we're how are you training

26:09.440 --> 26:15.440
these attention models I think that might be a little challenging and it would likely slow down

26:15.440 --> 26:21.600
our training because uh the technical aspects of it of getting these RL things working on first

26:21.600 --> 26:28.400
is TPUs um is not trivial um just because TPUs work well on on very large batches but not as well

26:28.400 --> 26:33.760
with with smaller batches and with RL when you're dealing with this kind of online yeah so then it

26:33.760 --> 26:38.080
becomes tricky to to make it work well in a way that you because you still want to be able to train

26:38.080 --> 26:43.600
these things quickly um so it sounds like it's much less of a hey I've got you know chocolate

26:43.600 --> 26:49.360
peanut butter let's get them together and see what happens oh yeah no no the specific thing uh

26:49.360 --> 26:54.800
you know what you know start at the simplest possible way to solve it yeah and kind of work your

26:54.800 --> 27:01.280
way yeah up in complexity exactly exactly so I don't I don't want to throw RL at just because I know

27:01.280 --> 27:06.560
well I don't necessarily want to use it for everything because sometimes you don't need RL like

27:06.560 --> 27:13.280
sometimes just I don't know an SVM would work just fine and then you should just use that

27:13.280 --> 27:18.960
and it might be pointed and now you've got a couple of papers I think a poster in a paper maybe

27:18.960 --> 27:26.240
here at NERP so that are using RL yes yeah so we had a poster this morning from our team um so

27:26.240 --> 27:31.520
Mark Belmara was the first author and he was the one manning the poster and and he he so I'm

27:31.520 --> 27:38.080
afterwards he his voice was very tired uh so that one is called geometric perspective on optimal

27:38.080 --> 27:42.320
representations and reinforcement learning and so we've been thinking a lot uh our

27:42.320 --> 27:47.600
nerd team in Montreal about representations for reinforcement learning and myself in particular

27:47.600 --> 27:52.880
I'm quite interested in this topic and I'm doing a lot of work right now in this space um but

27:52.880 --> 27:59.200
this this uh paper is sort of a partner paper to this other work that came out in ICML

27:59.200 --> 28:04.880
um I wasn't on the other paper but um I was along for the voyage um the value value function

28:04.880 --> 28:09.600
polytope so this is a paper that basically shows that the the value functions that you get

28:09.600 --> 28:14.000
in mark of decision processes form this this polytope in this high dimensional space

28:14.560 --> 28:19.520
and so it has this these interesting characteristics so for instance the the vertices of this

28:19.520 --> 28:25.040
polytope correspond to deterministic policies and so the path you can look at the path that's

28:25.040 --> 28:30.240
are the different algorithms will take along this polytope as they try to get to the optimal

28:30.240 --> 28:35.120
policy and there's some really interesting visualizations of when you compare like valley-based

28:35.120 --> 28:40.960
methods versus policy-grading methods and some of them actually leave the polytope in their

28:40.960 --> 28:46.880
trajectory so they're essentially the policies that are in a space where they're not valid policies

28:46.880 --> 28:51.600
in the sense that um they're not consistent with with with the with the system but they eventually

28:51.600 --> 28:57.440
end up coming back and and reaching some near optimal policy so the work we have here is basically

28:57.440 --> 29:04.160
leveraging this polytope and trying to see how we can um use it for for optimal representations and

29:04.160 --> 29:10.720
what I mean by optimal representations is that it's not just useful for the optimal policy but you

29:10.720 --> 29:18.320
can actually use this representation theoretically for for many different policies so there's a whole

29:18.320 --> 29:22.800
theory behind it but essentially you're trying to find a representation that will minimize

29:23.760 --> 29:29.760
the error that you get when you use that representation to express a particular value function

29:29.760 --> 29:37.520
for a policy that doesn't need to be optimal so it ends up being akin to having auxiliary tasks in

29:37.520 --> 29:43.680
a sense where uh we there's a lot of work in in the reinforcement learning field where adding auxiliary

29:43.680 --> 29:49.360
tasks to your to your learning process is helpful and that it serves almost like a regularizer for

29:49.360 --> 29:56.240
for your representations a multi-task yeah exactly and so these these different um policies that

29:56.240 --> 30:01.360
you're optimizing for so you're not just trying to get this optimal policy but sort of build your

30:01.360 --> 30:06.320
representation in a way that you can express just express these other policies quite well

30:06.320 --> 30:12.000
they end up serving sort of as auxiliary tasks and um they can help make the representation a bit

30:12.000 --> 30:17.120
more interpretable but also more expressive so there's some visualizations in grid worlds where you

30:17.120 --> 30:22.080
can see if you do the regular learning process if you look at the representations you you get

30:22.720 --> 30:27.040
certain dimensions of the representation are almost useless in the sense that they're not

30:27.040 --> 30:30.880
very expressive in terms of the state space of what they can represent in the state space but using

30:30.880 --> 30:36.800
these uh these they're we call them adversarial value functions um so these are the value functions

30:36.800 --> 30:42.000
for the other policies as these auxiliary tasks you get representations that are much more expressive

30:42.000 --> 30:48.640
and that they cover the state space a lot more um and so they're able to have a richer expressive

30:48.640 --> 30:55.280
power for representing multiple value functions you mentioned expressiveness are you trying to

30:55.280 --> 31:02.320
have minimal representations in a sense that they're not not necessarily with this but so part of

31:02.320 --> 31:07.200
what we're trying to achieve is that whatever representation you end up learning isn't own

31:07.200 --> 31:12.880
overfit to the optimal policy so let's say you tweak your your reward function a little bit and

31:12.880 --> 31:18.400
the policy you have is no longer um optimal under this new reward scheme um but let's say for

31:18.400 --> 31:22.800
whatever reason you maintain your your representation fixed because you just want to do linear

31:22.800 --> 31:27.600
approximation um if you don't have a good representation then you're not going to be able to express

31:28.160 --> 31:33.920
the new the new policy um properly and so this is what these representations are trying to do

31:34.560 --> 31:38.960
not necessarily reduce the mentionality but just increase the expressive power so that they're

31:38.960 --> 31:44.560
able to pretty well express the your current optimal value function but if you were to want to express

31:44.560 --> 31:49.440
the the value function under a different policy or an under a different reward function perhaps

31:49.440 --> 31:53.520
that it would still have a pretty good expressive power to be able to do that with low

31:53.520 --> 31:59.920
approximation error now the concept of generalizability is applicable to both the policy itself and

31:59.920 --> 32:09.680
the representation and so is there a relationship between the two whereas more generalizable or better

32:09.680 --> 32:16.640
generalized policies have better generalized representations or not necessarily unless you apply

32:16.640 --> 32:22.800
this approach that you've described so by generalizable policies do you mean like policies that are

32:23.840 --> 32:32.400
like if you've learned a suite of policies or um I guess I'm trying uh I don't know if policy

32:32.400 --> 32:37.360
is the right place to apply this is the question I'm asking like if I'm thinking of uh thinking

32:37.360 --> 32:45.040
of it from the perspective of I've trained an agent that uh can maximize some uh reward in some

32:45.040 --> 32:50.240
environment you know and once in generalization is I want to be able to put it in a slightly

32:50.240 --> 32:57.440
different environment and have it be able to perform well right and so that being the case where

32:57.440 --> 33:01.760
where does generalization live in that world is it in the policy that it you know or as well you

33:01.760 --> 33:07.520
you probably end up learning a new policy um but this tells you very closely to representation

33:07.520 --> 33:12.240
because if you're representation it's almost like you're doing fine tuning at that point so you've

33:12.240 --> 33:16.880
trained your agent under a particular reward function and now you have your trained agent and now

33:16.880 --> 33:21.440
you say okay well now I want this new task or this new reward function um but I don't want to

33:21.440 --> 33:26.160
retrain from scratch I want to start from where I started so it's kind of like fine tuning uh and

33:26.160 --> 33:32.720
so there if if your representation is highly overfit to the the first policy that you ended up

33:32.720 --> 33:39.040
learning um it might be very difficult to to sort of switch over to to this new policy if you're

33:39.040 --> 33:42.800
doing something even more drastic where you're saying I I already trained this agent so I'm going

33:42.800 --> 33:47.680
to fix the representation no longer backprop um through through the rest of the network and I'm

33:47.680 --> 33:52.240
only going to be learning learning the the last linear layer if your representation is poor you're

33:52.240 --> 33:57.120
not going to be able to learn the new task and so there uh is where generalization comes comes

33:57.120 --> 34:01.520
into play if you have a representation that that is expressive then when you switch the reward

34:01.520 --> 34:06.960
function or try to learn a new policy um if your representation is expressive you you should be

34:06.960 --> 34:12.080
able to do that reasonably well at least better than than um with a lot of the existing methods

34:12.080 --> 34:17.280
where there is evidence that they do tend to overfit to the current policy okay and so what was the

34:17.280 --> 34:22.160
inspiration for this paper you know coming from the original polytope paper was it driven by a

34:22.160 --> 34:30.720
particular use case or uh so uh Mark uh he's uh introduced the distributional approach to

34:30.720 --> 34:35.520
reinforcement learning so rather than backing up values you're back backing up distributions when

34:35.520 --> 34:41.200
when you're when you're doing learning and so this seems to give um a lot of advantages for

34:41.200 --> 34:46.240
for learning uh in terms of performance and there's a lot of follow-up work I've seen a bunch of

34:46.240 --> 34:51.760
papers today particularly there were a lot of distributional papers um but it's still not well

34:51.760 --> 34:57.520
understood how why they're they're giving that advantage so we had a paper uh triple AI this

34:57.520 --> 35:04.480
year um with Claire Lyle uh where we were investigating where this difference comes from comes from

35:04.480 --> 35:08.960
so the traditional way of of doing the rl backup is we call it expectational because you're taking

35:08.960 --> 35:12.720
expectations and then you get a single number versus the distributional approach where you're

35:12.720 --> 35:18.400
backing up uh distributions and so Claire did a lot of work and and she essentially proved that

35:18.400 --> 35:22.480
the there is no difference under certain mild conditions there is no difference between

35:22.480 --> 35:27.760
expectational distribution neither intitributional neither in the tabular setting nor the linear

35:27.760 --> 35:31.280
function approximator setting so they're essentially identical distributional doesn't give you an

35:31.280 --> 35:36.160
advantage then it's really when you go into deep networks that the the advantage of distributional

35:36.160 --> 35:41.360
comes in and it's not always guaranteed to give you an advantage so it's just going to be different

35:41.360 --> 35:46.320
and so sometimes it might actually hurt you um and um there's a like a counter example in the paper

35:46.320 --> 35:52.080
where it shows that uh distributional can actually provide worse performance than expectational

35:52.800 --> 35:57.920
so we in our team we're also very interested in in trying to understand distributional methods

35:57.920 --> 36:02.800
more and so Mark was looking into this quite a bit and uh he came up with this idea of

36:03.840 --> 36:08.480
basically looking at distributional at the distributional perspective almost as a auxiliary

36:08.480 --> 36:16.080
task and through that um he he had a remember a good lunch meeting with Dale Schoenman's and I

36:16.080 --> 36:21.120
think that's where they came up with this idea of the adversarial value functions um but it came

36:21.120 --> 36:25.760
from this initial idea of let's try to understand distributional methods a bit okay and you've got

36:25.760 --> 36:34.480
another uh paper that is being presented in the financial yeah yeah so that's so this is a

36:34.480 --> 36:39.200
collaboration with the Bank of Canada I mean they they've done most of the work I'm more on an

36:39.200 --> 36:47.120
advisory well on an advisory role so um they a lot of them their team their economists so they're

36:47.120 --> 36:50.880
not as familiar with reinforcement learning so that's where I come in and so I just basically

36:50.880 --> 36:55.440
they show me what they're thinking and and we we had a lot of brainstorms at the beginning to

36:55.440 --> 37:02.160
try to frame the the the problem properly so that it sort of satisfies their does it route up for

37:02.160 --> 37:06.880
for their economic theory but also that it makes sense from a reinforcement learning perspective

37:07.520 --> 37:11.760
and so what they're looking at they're part of the Bank of Canada which is part of the government

37:11.760 --> 37:16.480
and uh one of the tasks the main tasks of of the Bank of Canada is to make sure that the economy

37:16.480 --> 37:22.480
is stable and so one of the things they they look at is is the interbank payments that happen

37:22.480 --> 37:29.280
on a daily basis between different banks in Canada so um Bank A owes Bank B some money and so it

37:29.280 --> 37:34.240
may send it throughout the day and so if Bank B has has that extra equity then Bank B can pay

37:34.240 --> 37:40.400
other banks and so how they how they manage these payments affects how much how many of the payments

37:40.400 --> 37:46.080
they can make and how many payments they receive and so not making payments in time can give them

37:46.080 --> 37:51.920
interest penalties and these types of things and so the Bank of Canada plays a role as an

37:51.920 --> 37:57.680
intermediary to try to regulate these things so that you don't have um problems where where there's

37:57.680 --> 38:01.920
a bank that's not making any of the payments and all of the other banks are stuck and not making

38:01.920 --> 38:06.720
able to make any other payments as you're stuck in the stalemate so it's a very complex problem

38:06.720 --> 38:12.400
it's a very dynamic problem and they're interested in in looking at this obviously from the

38:12.400 --> 38:17.120
economics theory perspective but also they're interested in seeing if you can simulate some of these

38:17.120 --> 38:23.520
uh dynamics via reinforcement learning so we've been looking at um framing it as a multi-agent

38:23.520 --> 38:27.120
reinforcement learning problem where each of the agents are the different banks in question

38:27.760 --> 38:33.520
and uh seeing trying to train them to to learn optimal policies and their co-learning so all

38:33.520 --> 38:38.080
of the agents are sort of learning independently and you get some interesting dynamics as as this

38:38.080 --> 38:43.520
is happening so the workshop the paper that we have here is still very preliminary work um where

38:43.520 --> 38:49.040
we're essentially trying to demonstrate that this is even feasible um so as I said I like to take

38:49.040 --> 38:54.800
things from the simple uh angle and then grow from there so we're we're decomposing the the big

38:54.800 --> 39:01.600
problem of solving this interbank payment system um with decomposing it into smaller sub-problems

39:01.600 --> 39:06.800
that we can analytically find the solution for um that's what the economists know how to do well

39:06.800 --> 39:12.720
um and we can sort of validate that uh reinforcement learning is able to to simulate that faithfully

39:12.720 --> 39:18.000
and so far we've we've had a pretty good success with that and so now we're starting to combine some

39:18.000 --> 39:22.240
of these sub-problems and go into the more challenging tasks what are some examples of the kind of

39:22.240 --> 39:27.600
granularity of the sub-problems so the two sub-problems we're considering in this paper the

39:27.600 --> 39:33.520
the basically the simplest that you can consider one is um each bank at the uh start of the day

39:34.080 --> 39:39.600
can choose how much liquidity it's going to bring into start making payments so it's a prediction

39:39.600 --> 39:43.360
problem because you have to this one is almost like a bandit problem because you choose initial

39:43.360 --> 39:48.000
liquidity so like basically pulling one of the the bandit arms and then based on that initial

39:48.000 --> 39:53.680
liquidity that um determines how much you'll be able to pay so obviously if there were no cost

39:53.680 --> 39:58.400
to that you would just pull all of your liquidity and then you'd have as much money as you need to

39:58.400 --> 40:03.520
to make all the payments but there's a cost to to pulling um initial liquidity because this is

40:03.520 --> 40:07.360
coming from the bank of Canada it's almost like you're borrowing money from the bank of Canada

40:07.360 --> 40:12.080
to be able to make payments and then at the end of the day you return it um to the bank so this is

40:12.080 --> 40:16.480
one of the sub-problems that this was more like a bandit problem so they've run some simulations

40:16.480 --> 40:22.480
with with uh log data historical log data that they have um and then the other problem is the

40:22.480 --> 40:28.960
intraday payment so they the if you think of the day divided into hours at each hour you can

40:29.520 --> 40:35.600
make a decision of paying a particular bank that you owe money to um or not um so you may owe

40:35.600 --> 40:40.080
money to to a bunch of different banks if you don't pay them back then that bank might not have

40:40.080 --> 40:45.680
enough liquidity to pay you back um and so there's is when you start adding adding more agents then

40:45.680 --> 40:50.000
this becomes more and more complex and so this one is is less of a bandit problem it's more of a

40:50.000 --> 40:57.040
sequential decision-making problem and for in order to decompose them for the first bandit problem

40:57.040 --> 41:01.680
we're essentially keeping the intraday payment fixed in the sense that the the the choice that

41:01.680 --> 41:05.840
you made at the beginning doesn't really affect the dynamics of what happens later it's a fixed

41:05.840 --> 41:12.160
policy and for the second problem we keep essentially assuming giving the agents as much liquidity as

41:12.160 --> 41:16.160
they want so the the problem is very simple there all they have to do is make all the payments

41:16.160 --> 41:20.720
that that they need to make but because there's this multi-agent interaction um they don't

41:21.520 --> 41:27.200
necessarily always will find that optimal policy when you first started describing the problem

41:27.200 --> 41:32.080
one of the thoughts that came to mind was kind of a graph of the individual banks I don't know

41:32.080 --> 41:37.840
that I've heard much conversation about kind of the intersection of graph and reinforcement learning

41:37.840 --> 41:42.560
is there a war happening there um there's a little bit there's a little bit especially in the

41:42.560 --> 41:47.280
deterministic I believe I saw a paper come out recently where they're decomposed they're

41:47.280 --> 41:54.640
basically using graph algorithms for solving uh um certain reinforcement learning tasks so you

41:54.640 --> 42:01.440
can do um value function approximation by by using different types of of graph learning algorithms

42:03.200 --> 42:10.080
in this particular case I mean they are basically representing a graph the the connections between

42:10.080 --> 42:16.960
these these uh kind of the actual transactions are not necessarily yeah an actual thing between

42:16.960 --> 42:21.840
the right and and so most of the work I've seen with at the intersection of of graph whether

42:21.840 --> 42:26.400
be with their own networks or not and reinforcement learning is in the single agent setting where the

42:26.400 --> 42:30.880
the graph is more representing the environment where's in this case the graph is is representing

42:30.880 --> 42:36.000
the connection between the agents so you have multiple agents that they're not sharing parameters

42:36.000 --> 42:41.760
so they're kind of independent agents and the multi-agent setting um it's not something I have

42:42.320 --> 42:46.880
done a lot of work on so it's also been kind of interesting for me to learn more about the literature

42:46.880 --> 42:54.560
it's a really challenging problem um you have a lot of game theoretic aspects uh to it um and it's

42:54.560 --> 43:00.400
not a clear for for many problems there's no clear solution you have like Nash equilibria but

43:00.400 --> 43:07.280
it's it's that's as good as you can get for for many problems and uh so I haven't in that space

43:07.280 --> 43:13.520
I haven't seen much with with graphs okay with graph theory cool well uh Pablo thanks for taking

43:13.520 --> 43:18.560
a time to chat with us and share a bit about what you're up to thanks so much for chatting with me

43:18.560 --> 43:28.160
yeah absolutely thank you all right everyone that's our show for today for more information on

43:28.160 --> 43:44.800
today's guests visit twomla.com slash shows thanks so much for listening and catch you next time

