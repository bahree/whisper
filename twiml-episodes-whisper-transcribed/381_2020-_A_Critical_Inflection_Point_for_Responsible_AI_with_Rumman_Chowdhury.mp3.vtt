WEBVTT

00:00.000 --> 00:13.840
Welcome to the Twimal AI Podcast. I'm your host, Sam Charrington.

00:13.840 --> 00:23.600
All right, everyone. I am here with Ruman Childry. Ruman is a managing director and global

00:23.600 --> 00:30.560
lead of responsible AI at Accenture. Ruman, welcome to the Twimal AI Podcast. Thank you for having me,

00:30.560 --> 00:35.920
Sam. This has been a few years in the making and I'm glad we were able to do this. You know what?

00:35.920 --> 00:42.800
It only takes a global pandemic to make this conversation happen. These are the best conversations,

00:42.800 --> 00:49.360
I think, when I'm finally able to connect with friends and folks I know from the industry and

00:49.360 --> 00:54.400
you and I in particular, I think I've been trying to make this conversation happen for as you said

00:54.400 --> 00:58.560
a few years and it's always, oh, well, I'm going to be in Asia. I'm not in the Bay Area.

00:59.760 --> 01:03.920
We haven't quite been able to make it happen. So I'm super, super, super excited.

01:05.840 --> 01:13.440
We get this one going. Let's start out as we usually do here on the show and have you share

01:13.440 --> 01:20.080
a little bit about your background? You work in ethical and responsible AI, how did you come into

01:20.080 --> 01:27.920
that field? I do. And the answer to that is a lot of meandering. So by background, I'm a data

01:27.920 --> 01:32.480
scientist and a social scientist. I would officially say I make quantitative social scientists.

01:33.520 --> 01:40.080
I have degrees in political science, management, economics, masters and quantitative methods.

01:40.080 --> 01:49.440
Is that one degree or like five or six? Oh, I know folks like you.

01:54.080 --> 02:00.800
But I moved to Silicon Valley in 2013 to pursue a job in this like weird little field called

02:00.800 --> 02:06.800
data science, which I had heard about anecdotally while at my PhD program at UCSD.

02:06.800 --> 02:12.160
Everyone thought it was crazy. Nobody understood why I was leaving a political science PhD program

02:12.160 --> 02:19.360
to do some weird tech job. But here we are seven years later with data science and anything related

02:19.360 --> 02:26.320
to data science and AI being the only thing people talk about. So after my stint as a data scientist,

02:26.320 --> 02:31.200
I was actually teaching data science at a bootcamp called Metis. And that's one of such a family.

02:31.200 --> 02:39.120
I was doing talks on polling and the elections and in the sense of how numbers and statistics

02:39.120 --> 02:44.000
can be misleading because I have a background in things like survey design, polling and

02:44.000 --> 02:49.600
quantity of human behavior analysis. And you know, Accenture was this is about three years ago

02:49.600 --> 02:58.640
looking for someone to lead this weird thing called responsibility. And that's how I got this job.

02:58.640 --> 03:02.720
Oh, wow. And is the Metis the thing you're doing with Laura?

03:04.400 --> 03:07.200
No, that was something else. Okay. Okay. Cool.

03:08.720 --> 03:12.800
Interesting. Interesting. And so you've been at Accenture how long now?

03:13.520 --> 03:16.320
Three years actually hit the three year mark in early February.

03:16.800 --> 03:21.040
Wow. Nice. Nice. Which in the responsible air world makes me ancient?

03:21.040 --> 03:29.840
Absolutely ancient. And you're based in San Francisco. How have things been going for you

03:29.840 --> 03:34.160
with shelter in place and COVID and all that kind of stuff?

03:34.160 --> 03:39.360
Yeah. I mean, fortunately San Francisco had a really good response and people, you know,

03:39.360 --> 03:43.520
stayed at home and they more or less have been listening. I think everyone's just getting

03:43.520 --> 03:47.360
a little bit antsy. So I see more and more people out. Although people are still being careful.

03:47.360 --> 03:51.440
Fortunately, it's been pretty quiet. I live in Mission Bay, which is near the UCSF

03:51.440 --> 03:55.520
hospitals. And, you know, it hasn't been that bad.

03:56.320 --> 04:02.000
Unfortunately, I live in a really walkable neighborhood. There's parks nearby, etc. So it hasn't

04:02.000 --> 04:06.080
been overly unpleasant. I just think this is also the shortest, as far as the longest

04:06.080 --> 04:12.720
amount of time I've ever spent not flying somewhere in the last few years. So it's kind of been nice.

04:12.720 --> 04:19.520
Yeah, I've commented to, in fact, just earlier today, like by this time on a normal year, I'd

04:19.520 --> 04:26.160
have been probably to half a dozen at least conferences, you know, if not, if not a dozen.

04:26.880 --> 04:31.280
And you probably would have been around the world a couple of times, but I mean, it's funny

04:31.280 --> 04:35.600
because I have all these placeholders on my calendar and one by one, they all got dropped. But,

04:35.600 --> 04:45.200
you know, by now, I would have been in London twice, India and the Nordic. Actually, this week,

04:45.200 --> 04:49.680
I was supposed to be doing it toward different Nordic countries to visit different

04:49.680 --> 04:54.880
Accenture offices and client partners. And then in a month, I was supposed to be in Atlanta.

04:56.400 --> 05:01.280
I still have this thing in Singapore that apparently is still on the calendar for August,

05:01.280 --> 05:07.760
but I think they're being ambitious at this point. But, you know, it's nice to be home.

05:07.760 --> 05:13.760
You know, it's nice to be around my pets and organize my apartment and do all those things

05:13.760 --> 05:14.960
that help me talk it to do.

05:17.040 --> 05:27.360
Nice. So what are you up to at their Accenture, you know, in Responsible AI? What does that mean?

05:27.360 --> 05:32.560
What does Accenture's role in Responsible AI? And how do you help fulfill that?

05:33.200 --> 05:38.320
Yeah, so I have a really interesting and still unique job, although I hope that there will be

05:38.320 --> 05:44.640
more jobs that are like mine. My job is to actually provide and create practical client solutions

05:44.640 --> 05:49.360
around responsible and ethical use of artificial intelligence. And this can mean anything from

05:50.000 --> 05:55.520
data ethics to the unpacking of the black box. There's a lot of people call it,

05:55.520 --> 06:02.320
to just understandability in models, even to like the strategic organizational structure companies.

06:02.320 --> 06:07.840
And how do you create a government's infrastructure? And it's been really fascinating,

06:07.840 --> 06:13.600
because as I said, like I got this job through meandering, but what's fascinating is in this job,

06:13.600 --> 06:19.680
I use every degree I've ever had, every part of my brain. So, you know, while I do have to tap

06:19.680 --> 06:24.320
into my data science skills and think about model explainability, interpretability,

06:24.320 --> 06:29.360
different ways of doing data assessments, etc. I tap into my social science brain all the time.

06:29.360 --> 06:34.480
And I think about human behavior and human responses and how to construct something so that,

06:34.480 --> 06:41.120
you know, we're getting accurate data or we're creating policies that are inclusive or understandable.

06:41.120 --> 06:46.480
But then also I go to companies and we help them redesign their organizational infrastructure

06:46.480 --> 06:50.560
to create the right kind of scaffolding to enable responsible AI. And actually,

06:50.560 --> 06:56.480
I've been working with some folks, and we have a paper coming up quite soon. We did a workshop

06:56.480 --> 07:01.360
at FACT, online in this FACT store. I've been working with a researcher of mine, Bob Donna

07:01.360 --> 07:06.320
Marcova, Penny Wayne at Paymark, who's Spotify Labs and Jeanine Gang.

07:06.320 --> 07:08.880
Probably a huge fan of her yet. I haven't seen her forever.

07:09.520 --> 07:12.320
Because she's super busy and even pandemic.

07:13.520 --> 07:17.440
She's another conversation just like this. So, maybe I should reach out to her and say,

07:17.440 --> 07:20.960
you, you know, you're not pandemic-busier or something.

07:23.600 --> 07:28.720
We're sort of not pandemic-busier, but for sure, similar to me, she's on planes less. So,

07:28.720 --> 07:35.840
that's good. But yeah, so part of my job is the technical deployment. And there's a part of it

07:35.840 --> 07:40.880
that's also about the organizational structure and the strategic deployment. Because a big part

07:40.880 --> 07:48.480
of the public simply understanding what you're doing with AI is to improve how we communicate

07:48.480 --> 07:52.720
what this technology can and cannot do. Because there's a lot of hype, there's a lot of noise,

07:52.720 --> 07:56.400
and frankly, there's a lot of backlash to the hype. There's even ethics hype now, frankly.

07:59.440 --> 08:03.120
And so, I apologize. I got excited when you said, Henry, what was the paper about?

08:03.120 --> 08:11.120
You're like, who paper? What? Henry yet? No, it's fine. So, the paper was really interesting,

08:11.120 --> 08:16.400
because we're at this phase in responsible AI or ethics where there are principles that

08:16.400 --> 08:20.400
there are so many principles were drowning in principles, right? Algorithm Watch keeps

08:20.400 --> 08:27.040
this database. I think right now they have 150 plus principles of AI and ethics and organizations

08:27.040 --> 08:34.880
at the OUCD down to companies like Telefonica or even Accenture. And when you say 150 principles,

08:34.880 --> 08:42.400
are these 150 kind of published frameworks by some group or company or 150? If you want to be

08:42.400 --> 08:49.440
ethical, you need to do things one through 100. I got it. No, so 150 different sets of principles,

08:49.440 --> 08:54.400
so different organizations that have come up. But to point, interestingly, there have been a few

08:54.400 --> 09:00.320
papers trying to understand what are common themes across principles. So, like a meta analysis.

09:00.320 --> 09:05.760
So, Anna Jobin has won the channel Floridian Josh Calls of another. And there are some themes

09:05.760 --> 09:10.400
that are pretty common. I can't remember them all. I'll talk my head with stuff like Malmalfezan,

09:10.400 --> 09:18.560
Non-Malfezans, and you know, there's like five or six common themes that Floridian Calls find.

09:18.560 --> 09:23.200
So, it's interesting. So, to your point, everyone's talking about it, but there are common themes

09:23.200 --> 09:28.960
to it in general. And the big thing now is how do we drive principles into action? How do we do

09:28.960 --> 09:35.360
stuff with this? And our paper was about how do you enable the right sort of organizational

09:35.360 --> 09:39.680
structure? So, drawing from, like I said, I use every word of my brain, drawing from like

09:39.680 --> 09:45.200
management literature, thinking through organizational analysis, organizational structure,

09:46.800 --> 09:52.080
how do we draw from those principles to understand the kinds of shifts companies need to make,

09:52.080 --> 09:57.760
to enable the people they've hired to institute responsible AI. So, we did

09:58.800 --> 10:04.240
our long surveys with 24 different people across 18 different companies. And we specifically

10:04.240 --> 10:10.240
focused on people like myself who are there for the application, not just the research. And,

10:10.240 --> 10:13.200
you know, I found a lot of really, really interesting things.

10:14.080 --> 10:19.120
Interesting. Yeah, in fact, the last time I reached out to Henriette was an advance of

10:19.120 --> 10:29.680
our conference, two moles conference in the fall. And we did a panel on operationalizing AI ethics.

10:29.680 --> 10:34.880
So, you know, for organizations that are, you know, trying to implement AI and do it responsibly,

10:34.880 --> 10:41.200
you know, what are some of the things that they could do. And, you know, we got into this long

10:41.200 --> 10:46.880
back and forth email conversation about how it's all evolving. Yeah, it's, and that's actually what

10:46.880 --> 10:55.040
the paper is about. It's about how we're in this critical inflection point where there's this,

10:55.040 --> 10:59.760
so there's this whole organizational organizational literature about external and internal pressures

10:59.760 --> 11:04.240
and organizations that enable ethical change. And right now we have a significant amount of

11:04.240 --> 11:08.800
external pressure. And it's all just like, so it just takes time, right, and just have to do the

11:08.800 --> 11:14.320
things. And it took a while for the community to evolve for people to make principles, people

11:14.320 --> 11:19.280
to start talking about them, you know, the media engagement, public awareness. And now it's

11:19.280 --> 11:23.760
reached this point where companies are getting this pressure. And we also have internal champions

11:23.760 --> 11:28.800
and companies trying to drive this change. So, you know, how can we utilize this internal and

11:28.800 --> 11:34.000
external, this external pressure to enable the internal champions to do their job and what can

11:34.000 --> 11:39.280
companies do to help them? But yeah, absolutely, it's all, you know, so we look at the current state

11:39.280 --> 11:44.480
which is kind of where things are today. The prevalent state, which is, you know,

11:44.480 --> 11:49.840
an immediate future where things like need to go. And then this ideal future state, like where

11:49.840 --> 11:54.800
would people love things to be in the future? What is this ideal state of a responsible company?

11:55.440 --> 12:03.680
Interesting, interesting. For a while, I would hear about organizations that we're taking this

12:03.680 --> 12:10.320
position that, you know, we're just not going to pursue AI because it's too ethically fraught

12:10.320 --> 12:15.840
and we don't know what to do. Do you see that at all? Or had you seen that? Is it still something

12:15.840 --> 12:22.480
that you come across? Well, I have definitely seen that sentiment when it comes to individual projects.

12:22.480 --> 12:27.520
I think every company is really excited about the promise of AI. And I don't think anybody is

12:27.520 --> 12:34.480
turning down, you know, the whole concept in general. I mean, frankly, you just will be a market leader.

12:34.480 --> 12:39.920
Yeah, you have a chance. However, yes, I have seen hesitation when it came to implementing,

12:39.920 --> 12:44.480
when it comes to implementing certain projects. And it's definitely a blocker in companies

12:44.480 --> 12:52.080
scaling AI. One thing we find in general as a company, as Accenture, you know, there are many

12:52.080 --> 12:56.240
blockers to scaling artificial intelligence. Companies are drowning in proofing concepts.

12:56.240 --> 13:00.960
Anybody can spin up a proofing concept based on a nice database. They find some sort of online

13:00.960 --> 13:05.840
data, do some fancy neural net on it. You can have data science spin something up in three weeks.

13:05.840 --> 13:09.840
But productionizing it, scaling it is a whole other endeavor.

13:11.440 --> 13:17.600
Yeah. And so, part of, you've kind of talked about all these different parts of your brain that

13:17.600 --> 13:28.320
come into play when trying to help organizations think through this. The most organizations

13:28.320 --> 13:38.800
already have the kind of pieces in place to build, you know, to build ethics into the way they

13:38.800 --> 13:45.600
operationalize machine learning. Like, if you're in, you know, financial services, for example,

13:45.600 --> 13:52.400
you know, you've had to deal with, you know, making loan decisions ethically and, you know,

13:52.400 --> 13:57.200
things like that. So, you probably have some kind of organizational, some governance structure

13:57.200 --> 14:04.000
in place. To what extent did most, you know, or how prevalent is that? And what are the things,

14:04.000 --> 14:09.200
even if you have that, that you need to do to make it make sense in the context of ML and AI?

14:09.760 --> 14:14.080
Yeah, that's a really great question. And the answer is it just varies a lot. And some of it is

14:14.080 --> 14:18.320
the culture of the organization or the company and some of it is just the nature of the industry

14:18.320 --> 14:23.920
that it's in. I would say by my rule of thumb for, you know, thinking through which companies are,

14:24.560 --> 14:28.400
you know, would be the most successful at enabling responsibility, at least at this point.

14:29.680 --> 14:35.440
One would be understanding AI output as probabilistic and not deterministic. So,

14:35.440 --> 14:40.640
understanding the math and statistics behind it, rather than thinking of it as a magical

14:40.640 --> 14:47.120
computational outcome, just helps someone in my position a lot because a lot of the bias and

14:47.120 --> 14:51.680
unfairness from artificial intelligence or biases from the fact that, you know, if you understand

14:51.680 --> 14:57.920
that the that the output is a likelihood and on a certainty, then you can appreciate bias in,

14:57.920 --> 15:02.160
you know, in a almost like a technical sense, the way data scientists think about bias.

15:02.960 --> 15:08.080
And then you can appreciate how bias can enter in a system and think of, you know, how to remove

15:08.080 --> 15:16.720
that bias. Second would be some sort of adoption of, you know, use of AI in your organization

15:16.720 --> 15:21.520
already. It's difficult if you're, again, like you think this is like some sort of magical

15:21.520 --> 15:27.680
technology. The third, interestingly, is either develop legal functions or regulated

15:27.680 --> 15:35.200
industry and not just because of external pressures like regulators, but companies that are in

15:35.200 --> 15:39.360
industry that are highly regulated have legal functions that are already data delivery, sort of

15:39.360 --> 15:45.920
building that infrastructure. So, if I were to talk to a company, you know, in a less regulated

15:45.920 --> 15:50.000
industry, their lawyers are usually more like contract lawyers or, you know, certain types of

15:50.000 --> 15:54.880
maybe liquidigation lawyers, but they haven't had to actually work with ethics and compliance

15:54.880 --> 16:00.480
necessarily and not necessarily in that developed with the sense. Although, I really do think

16:00.480 --> 16:06.000
this notion of risk functions and the value of something like a chief risk officer is going to

16:06.640 --> 16:11.920
change quite a bit and that role will be increasingly valuable, you know, as we adopt more

16:11.920 --> 16:16.400
official intelligence. So, to your point about financial services, they're my favorite customers.

16:16.400 --> 16:22.960
In the sense that like, I mean, they're ready to, they get it also. So, marginal amount of bias

16:22.960 --> 16:28.800
if because I am a statistician also by background, like, you know, I think a lot of anecdotally,

16:28.800 --> 16:33.360
I'll say a lot of statisticians who work at these financial services organizations got a little

16:33.360 --> 16:37.920
bit marginalized by the rise of data science because people did not, because again, it was sort of

16:37.920 --> 16:43.120
sold as this magical computer thing, not a bunch of math. And the statisticians are like,

16:43.120 --> 16:47.440
no, we've been doing this for a long time and I wholeheartedly agree they had been doing this for a

16:47.440 --> 16:51.600
long time. So, they understand models, they understand how to assess them, they understand that

16:51.600 --> 16:56.800
accuracy isn't the only thing to look at when you look at models, you know, in this notion of

16:56.800 --> 17:03.040
testing, it's all like built, they get all of it. But also, the industry itself has this culture

17:03.040 --> 17:09.360
of ethics that comes about close to 2008 financial crisis and this really interesting document

17:09.360 --> 17:14.880
called SR-117 and it was written by the Federal Reserve and on that birthday in 2011.

17:14.880 --> 17:20.080
And if you were to read that document, you would be like, it reads as if like someone like

17:20.080 --> 17:27.200
you or me wrote this today about issues with models and need for ethical use, irresponsible use

17:27.200 --> 17:32.160
of data and appropriate use of models. It's fascinating because they were thinking about this 10

17:32.160 --> 17:36.720
years ago and there's a lot you can learn and certainly artificial intelligence introduces

17:36.720 --> 17:42.160
new challenges, but the building blocks are there. And it's probably the most robust in financial

17:42.160 --> 17:50.000
services and any roles. Yeah, I think when I think about ethics and one of

17:50.000 --> 17:58.560
the challenges of, you know, just trying to address it, you know, organizationally, it's,

17:59.760 --> 18:04.480
I guess I can characterize it as like you've got this one side, it's like idealistic and

18:04.480 --> 18:09.520
it's another side that's kind of very practical and I'm wondering, you know, do we lose something

18:09.520 --> 18:15.680
when we kind of hand over this concept of ethics to lawyers and risk management people?

18:15.680 --> 18:21.280
Um, I think that there is an evolution of the risk function that's probably going to happen.

18:21.280 --> 18:28.400
And you're right. When I think about this work, I try to broaden the phrase to be about risk and

18:28.400 --> 18:35.360
impact. And I do agree there's a cynical take on risk, which is more about, you know, how do we,

18:35.360 --> 18:40.800
and it gets close to the line as possible. Yeah, or also like yeah, risk is not liability.

18:40.800 --> 18:48.240
Yeah, but I will say working with a lot of folks who are in this field, I think there is this

18:48.240 --> 18:52.880
desire. So, you know, going back to financial services, a lot of this, a lot of what exists

18:52.880 --> 18:58.880
today as model risk management started from this conference, this gathering called Basil on

18:58.880 --> 19:04.240
his Basil 1, 2, 3, like these different documents. And it's not just about like here's how you

19:04.240 --> 19:08.640
audit a model. It's actually about how do you make an ethical company? They thought about things like

19:08.640 --> 19:13.840
compensation for employees being linked to performance, which would then create incentives for

19:13.840 --> 19:18.960
them to either misrepresented lie about what's going on, right? And not to say it's been perfect.

19:20.080 --> 19:26.160
There have been some notable disasters since then. But the intent is there. Maybe this is,

19:26.720 --> 19:33.120
you know, a good motivator for the people who are truly dedicated. But you're right. I like to

19:33.120 --> 19:38.720
expand the language to move beyond risk to impact. I think impact is more proactive, I think,

19:38.720 --> 19:45.120
impact things more broadly. And impact also isn't focused on like how do we shift the liability

19:45.120 --> 19:49.520
for me to someone else? Because theoretically in a pure risk function, I can just say like well,

19:50.240 --> 19:56.560
my organization's risk is X. And if that person's absorbing the risk, then my risk is lowered.

19:56.560 --> 20:00.480
It doesn't mean that people didn't get harmed, it just means I'm not being sued, right? That's

20:00.480 --> 20:06.640
a very, that's a more cynical thing. But then bringing the language of impact broadens it to me

20:06.640 --> 20:12.320
and like, no, you are actually socially responsible to, you know, the environment at large, whether

20:12.320 --> 20:19.840
it's the market, whether it's society or your customers. So if you're an organization that is

20:20.560 --> 20:25.600
kind of down the path of exploring machine learning, maybe you have, you know, one or

20:25.600 --> 20:34.000
any of these prototypes that you've mentioned and you're starting to operationalize ML technically.

20:34.720 --> 20:40.400
And you're listening to this interview and you're like, oh, ethics, I need to, I need to get

20:40.400 --> 20:50.240
me some of that. You know, maybe, you know, first of all, kind of what do you do there? But also,

20:50.240 --> 20:56.960
you know, I think you mentioned in like thinking about these problems, you kind of, you're using

20:56.960 --> 21:01.040
all these different kind of parts of your brain, your management part of your brain, your social

21:01.040 --> 21:06.560
sciences part of your brain, your technical part of your brain. You know, when I think about,

21:06.560 --> 21:10.560
you know, kind of me, I've got a little bit of management part of my brain from kind of, you know,

21:10.560 --> 21:14.880
industry, I've got a technical part of my brain. But like when I was in school, I went to an

21:14.880 --> 21:21.520
engineering school, RPI, go engineers. I've took like maybe two social science classes in the

21:21.520 --> 21:30.240
whole time. So like, I don't have that, you know, that kind of formal training to draw on. And

21:30.240 --> 21:38.000
there are a lot of people in industry that don't. So how do you, you know, what is the path

21:38.000 --> 21:46.320
look like to start to understand, you know, maybe kind of encapsulate the room on social sciences,

21:46.320 --> 21:53.120
you know, brain or what have you? You can really kind of, you know, if not rigorously,

21:53.120 --> 21:56.640
thoughtfully, think through these kinds of issues and put a structure in place so that you can

21:56.640 --> 22:03.760
do it repeatedly and start to scale it. So two thoughts. One is the first talk there for

22:03.760 --> 22:07.440
at Accenture. I actually still give it today and it's called, what do we talk about when we talk

22:07.440 --> 22:13.040
about bias? And what I realized is that when data scientists were communicating with non-data

22:13.040 --> 22:17.200
scientists, there was this like lost and translation moment about some of the most basic terms,

22:17.200 --> 22:23.360
like bias is one. And to your point about, you know, going to a purely technical school where you

22:23.360 --> 22:28.480
didn't really take social science classes, when I was teaching at Metas, I realized that my

22:28.480 --> 22:34.160
students that came from pure STEM backgrounds had no concept that data could be biased because,

22:34.160 --> 22:42.720
you know, for them, data represented an objective truth. And I had to, and it's something that

22:42.720 --> 22:47.920
you sort of rationalized. You never see that argument on Twitter today. No, never happens.

22:47.920 --> 22:52.720
No, never. But it's interesting because it's sort of anecdotally people get it, but then,

22:52.720 --> 22:56.640
you know, and I understand like if you're a computer scientist or a mathematician, you've always

22:56.640 --> 23:02.720
optimized two A dataset, right? And explaining to them that the collection of data can be flawed

23:02.720 --> 23:08.240
was just like this interesting aha moment. And that that's where some of my early work on this

23:08.240 --> 23:12.080
stuff comes in. And it's still like, you know, I told you, both at top three, over three years

23:12.080 --> 23:16.960
ago at this point, I still give it today. And still people are like, oh, oh yeah, actually,

23:16.960 --> 23:20.480
that's true. So how, also, hat size, my dog.

23:20.480 --> 23:27.520
Okay. Pets are unavoidable in the pen. Pets and pets and children are unavoidable in the pandemic.

23:28.640 --> 23:33.120
I've had too many calls with some of this child that really ran it. It's very cute. It's always very

23:33.120 --> 23:39.840
cute. Just earlier today, I was watching the Microsoft Bill technical keynote and Scott Hanselman,

23:39.840 --> 23:49.680
do you know, Scott? He was giving a, you know, long time kind of blogger inspiration, kind of champion

23:49.680 --> 23:57.120
of diversity and tech, all this stuff, Scott out to shout out to Scott. But he was doing his keynote

23:57.120 --> 24:02.800
in front of who knows how many, you know, virtual tens of thousands of people. And his kid sneaks

24:02.800 --> 24:06.000
into the back of his office. I love it. I love all of it.

24:06.000 --> 24:15.120
He's still some toy. I love it. It's so humanizing. It's like, look, life happens. It's fine.

24:15.120 --> 24:19.920
We all work. We're not, like, we're not automatons. Like, you know, we have pets. We have children.

24:19.920 --> 24:29.840
We have lives for humans. I think it's great. But back to your question, you know, I think that there,

24:29.840 --> 24:36.400
so if I were, if I could reconstruct the world, I think data science, much like quantitative

24:36.400 --> 24:40.960
finance, frankly, should actually have an arm called critical data science that there should be

24:40.960 --> 24:47.840
people trained in data science fields in the art of critiquing data science models. I think it

24:47.840 --> 24:55.040
is way too complex, frankly, for an individual to just do as an add-on to their everyday project.

24:55.040 --> 24:59.440
And also, it's hard to have that level of objectivity when you're auditing your own work, right?

24:59.440 --> 25:04.320
Because everybody likes to think they did a good job and maybe didn't, maybe didn't. And, you know,

25:04.320 --> 25:08.720
there are things you would miss. So a large part of like what people are saying when they say,

25:08.720 --> 25:12.400
how do we institute this? A lot of folks call for things like red teams,

25:12.400 --> 25:16.640
drawing from the way security works. Well, in order to do that, you kind of need these,

25:16.640 --> 25:21.120
this is a third party of people, whether it's another organization or whether it's people with

25:21.120 --> 25:25.920
any organization, but they need to have this like specialized skill set of being able to like

25:25.920 --> 25:32.880
assess models for real business and all sorts of ways, right? Whether it's how the data was collected

25:32.880 --> 25:37.200
to like literally the parameters of the model and even to like where you will be implementing

25:37.200 --> 25:42.800
and who it will be used on, whether it is well suited for those individuals, right? So that

25:42.800 --> 25:47.120
would be my like if I could change the world, I would add something called critical data science

25:47.120 --> 25:54.400
as an actual field of study to then do the science. But for today, you know, I think that people

25:54.400 --> 26:00.000
sometimes forget that quantitative social sciences, scientists exist and that's literally what we've

26:00.000 --> 26:07.200
done, like our whole lives. And it wasn't just saying when I first moved here, like I got a lot of

26:07.200 --> 26:12.880
slack from not being a programmer. And I just did not, like I always like to say I'm not born of tech,

26:12.880 --> 26:21.040
like I was not built and created here. I was 33 when I moved here, so I wasn't like this young,

26:21.040 --> 26:25.280
green kid like learning about how you know places should be. And I'm like, I haven't been so

26:25.280 --> 26:31.680
obsessed with programming. And it's obviously a great skill to have, but for me, it was one of

26:31.680 --> 26:37.280
many skills. And I so I think it was more valuable than other skills and the sort of weird

26:37.280 --> 26:44.320
tearing of what's more or less valuable is very odd to me. And then, you know, like it was interesting

26:44.320 --> 26:48.400
because it's, you know, you certainly get a lot of flak with being a social science scientist

26:48.400 --> 26:55.040
in this world. But you know, now it's interesting because all of my quantum social science

26:55.040 --> 27:00.240
skills come into play. So, you know, I think one is to bring in more social scientists on your

27:00.240 --> 27:05.280
team, you would be amazed and surprised that our level of statistical and quantitative skill

27:05.280 --> 27:10.640
and abilities and our programming skills, we can do all of it. But you know, to be fair,

27:10.640 --> 27:15.040
there's plenty of things engineers do that I can't do, right? And, but the whole goal is to make

27:15.040 --> 27:21.040
this an interdisciplinary group. I actually don't think anyone here for body can do this. I certainly

27:21.040 --> 27:25.600
personally cannot do all of it, it's supposed to be my job kind of at a high level, right?

27:25.600 --> 27:30.080
The more you dig into it, the more you realize how there's a role for everyone to play.

27:30.080 --> 27:35.120
So, thinking through, you know, the study that I did with like a Henrietta and Dreaming and Bobby,

27:35.120 --> 27:39.520
that's actually the answer. It's like the entire organization is responsible and everybody has

27:39.520 --> 27:45.440
their job to do. And I can appreciate how it's extremely daunting task if you are a data scientist

27:45.440 --> 27:49.520
on a project, you know, there's always this literature with data scientists as if they're gods,

27:49.520 --> 27:54.000
as if like, you know, and I remember my first job as a data scientist, like, you're not a god,

27:54.560 --> 27:59.360
you're responding to someone else's demands. You got to meet deadlines, right? I mean,

28:00.400 --> 28:05.040
it actually takes a pretty brave person to say, you know, we need to put this project on hold

28:05.040 --> 28:11.120
because this data is wrong or incomplete or to go to your project manager and say, like, you know,

28:11.120 --> 28:16.480
this product is not built ethically. And we need to create the right sort of incentives and

28:16.480 --> 28:21.840
community structures to do so. It's very, you know, like, I suppose high-level answer to your question,

28:21.840 --> 28:26.080
sorry, I don't have an easy answer to it. I think it's fine that, you know, people are, I think

28:26.080 --> 28:31.200
it's totally fine that people are offering ethics, curricula, etc. I think it's certainly needed,

28:31.200 --> 28:37.440
but is it in software problem? No, it's not. And I think that's a great point. And, you know,

28:37.440 --> 28:42.880
one of the biggest things that's changed in data science over the past, you know, three, five or so

28:42.880 --> 28:49.840
years is kind of this move away from thinking of the data scientists as this kind of, you know,

28:49.840 --> 28:55.600
lone ninja that kind of roms the night. It was never that short. It was never that. It's

28:55.600 --> 29:02.880
all like a weird 10% engineer thing. Like, who is that? I don't know, like, honestly, I don't know

29:02.880 --> 29:06.480
data scientists who are like that. I don't know who these people are. I've certainly never worked

29:06.480 --> 29:10.880
with one and, you know, and if they existed, nobody ever liked them and they were actually never

29:10.880 --> 29:16.640
very good. I remember these pictures that you that we used to see with like the data scientists

29:16.640 --> 29:21.680
and like all of the skills in their backpacks. We remember what I'm talking about.

29:22.880 --> 29:27.840
My favorite used to be these job descriptions. And you can still find them. And it'll be like,

29:27.840 --> 29:37.360
you know, computer science degree PhD preferred 10 years plus engineering knowledge of like

29:37.360 --> 29:44.480
like all of it. And I'm like, this person doesn't exist. And like up on them, they'll also say

29:44.480 --> 29:50.160
something like, you know, back in 2015, I would say like 10, like six to 10 years of experience

29:50.160 --> 29:55.920
in data science. I'm like, that term didn't exist 10 years ago. Okay. Go back to like Yahoo. And

29:55.920 --> 30:01.840
go find the like OG data scientists. Maybe one of them is qualified. Yeah. Yeah. But yeah,

30:01.840 --> 30:07.680
but to your point, actually, at my first job, I one of the their first data science hire was

30:07.680 --> 30:13.040
actually somebody from Yahoo, who was one of their original data scientists. And like he certainly

30:13.040 --> 30:17.360
wasn't that way. He was a really, he was a great teacher, a wonderful person to work with.

30:17.360 --> 30:21.040
He taught me a lot about how to production wise code. That was one of the things I didn't know how

30:21.040 --> 30:26.240
to do. I mean, I knew how to like assess models and build it in like this closed environment,

30:26.240 --> 30:30.160
but how do you take it and, you know, make it ready for an engineering team, though these

30:30.160 --> 30:34.640
were skills I didn't have. And and such it was really helpful and like a wonderful person to work

30:34.640 --> 30:41.440
with. I have no idea what that weird comes from. Interesting, but I think

30:43.280 --> 30:49.680
by and large organizations with few exceptions have kind of moved away from that towards

30:50.480 --> 30:56.960
more of a team approach. StitchFix comes to mind as a counter example where they are very much,

30:56.960 --> 31:02.240
you know, they still look for a full stack data scientist they call them. But in general,

31:02.240 --> 31:08.240
you know, I tend to see more of kind of a team approach that has specialists. And I think to

31:08.240 --> 31:14.560
your point, you know, ethics or, you know, computational social science thinking or everyone

31:14.560 --> 31:19.440
I call it is a, you know, it's a complimentary skill that belongs on that team as opposed to,

31:20.560 --> 31:26.000
you know, we have to, you know, make everyone social science ninjas in addition to being

31:26.000 --> 31:30.960
technical ninjas and deployment. It's actually, it's like, so I guess the, is it one of those

31:30.960 --> 31:36.000
things where the field just had to mature, right? And the analogy that I always think of is like

31:36.000 --> 31:42.880
remember the title live master from the 90s. Like, okay, like try to tell anybody born after the

31:42.880 --> 31:50.800
year 2000 that once upon a time, companies would hire one person to manage their website. The whole

31:50.800 --> 32:00.160
thing. This included graphics design. And now it's not only is it a team, it is a team of people

32:00.160 --> 32:06.480
who are like graphics designers, like, you know, and pure creative folks, two engineers and programmers,

32:06.480 --> 32:11.600
two somebody who's just developing content and really good at writing copy and messaging,

32:11.600 --> 32:16.400
you know, and it's a, and I think that over time, I mean, it took a while to get there and that,

32:16.400 --> 32:21.360
that's where it went because it was just logical. You know, our web presence became just as important,

32:21.360 --> 32:25.840
if not sometimes more so than a physical presence for any given company. If you wanted to do it

32:25.840 --> 32:29.760
right, you had to invest in people. Guess what? With interdisciplinary skill sets and you couldn't

32:29.760 --> 32:35.520
just write, you know, this, this job description of someone who knew HTML, CSS, and, you know,

32:35.520 --> 32:41.440
you know, do you know a color palette? You know, just don't make this website and windings.

32:41.440 --> 32:46.320
I'm perfect. Comic Sans and Winding. Oh, man.

32:48.400 --> 33:01.920
Nice, nice. So there's kind of beyond kind of the ethics, you know, should we shouldn't we,

33:01.920 --> 33:07.760
you know, not to oversimplify ethics. I guess I'm, I want to transition to kind of explainability

33:07.760 --> 33:12.320
and the extent to which that comes up in your work and as a concern for the people that you

33:13.360 --> 33:18.480
are working with and assuming so, you know, what are you seeing as kind of the ways that folks

33:18.480 --> 33:25.120
are addressing it? Yeah, it's interesting. So the notions of both explainability and transparency

33:25.120 --> 33:30.000
come about in part because of a lot of the legislation from the EU, so General Data Protection

33:30.000 --> 33:34.640
Regulation. And it's been interesting to see how people interpret it. So like,

33:34.640 --> 33:40.320
other talk that I give is about both a notion of explainability and transparency and explain

33:40.320 --> 33:44.560
the idea of explaining the concept of what it means to respond is really interesting, right?

33:44.560 --> 33:50.480
So again, like, talking into this other part of my brain. This is where I draw on like political

33:50.480 --> 33:55.680
philosophy and, you know, governance and democracy, right? So, you know, we want to create all

33:55.680 --> 34:03.760
this governments around AI. And we want to like explain things, right? But often the explainability

34:03.760 --> 34:08.560
is this notion that I'm just going to tell you things and somehow you're supposed to understand

34:08.560 --> 34:13.360
what I'm saying. So the idea would be like this panoptic, like, teacher and electoral kind of thing.

34:13.360 --> 34:16.960
Like, I'm standing in front of a room. I'm putting up this lecture. If you don't get it, it's your fault

34:16.960 --> 34:21.840
and you got to figure it out. And the, so sometimes like our notion of explainability because we

34:21.840 --> 34:28.320
translate from data science to, you know, other fields, whether it's a customer service representative

34:28.320 --> 34:34.880
or a loan officer or a judge, right? These are other people who have no interest or skills or

34:34.880 --> 34:39.040
abilities in our field, same way we don't have any interest colorability in their fields, right?

34:39.040 --> 34:43.440
So what are we at? How are we explaining things? And, you know, as it was a more concrete

34:43.440 --> 34:48.640
technical example, I use is that the end user license agreement. So, you know, we've all gone

34:48.640 --> 34:54.480
through the update our OS and then we get this like long legalized document, which none of us read.

34:54.480 --> 34:59.200
And even if we sat down and read it, we would not understand it because it's written by lawyers

34:59.200 --> 35:05.200
or lawyers. Right. So fully explained, completely explained. That is all understood. And often

35:05.200 --> 35:09.920
I think about explainability, I think about the notion of understanding and how, you know, if you're

35:09.920 --> 35:15.440
a good teacher, you're really focused on whether or not you students have understood what you've

35:15.440 --> 35:20.240
said, right? And even if it means explaining it again or spending it differently or, you know,

35:21.120 --> 35:25.680
taking more effort to understand your audience rather than kind of just saying it the way you would

35:25.680 --> 35:31.680
say it. So that would be sort of the government's component of it. And even this notion of transparency,

35:31.680 --> 35:36.880
right? So similarly, transparency assumes that if I have a pro-transparent process or a

35:36.880 --> 35:43.520
transparent model, that it's great. That's it. I'm done. But then there's this assumption there

35:43.520 --> 35:49.360
that I can do something about it. And we're not, we're, you know, all systems are inherently

35:49.360 --> 35:55.040
have a power dynamic. So if a massive tech corporation just says, by the way, we've changed our

35:55.040 --> 36:00.880
models to be like X, what are you or I or the average person's vehicle? What would you be able to do

36:00.880 --> 36:05.760
nothing? Right? Absolutely nothing. And this came up quite a bit thinking about like image, facial

36:05.760 --> 36:11.920
recognition, image detection. One pushback I would get from people, you know, on the use of facial

36:11.920 --> 36:17.120
recognition in stores, et cetera, to maybe that identify a shop lifter. Then that weird behavioral,

36:17.840 --> 36:24.080
you know, behave, what does it quite be? A effective computing stuff. I just mean like straight up,

36:24.080 --> 36:29.760
like identifying someone from a video. And they would say, well, what's the difference between

36:29.760 --> 36:34.880
that and having a security guard? And I'm like, well, the difference is if there's a security guard

36:34.880 --> 36:39.120
and they unfairly targeted me, I can ask who's your, like, I want to talk to your manager,

36:39.120 --> 36:44.880
I can call the company, I can actually take action. But if it is a image recognition system and

36:44.880 --> 36:50.320
it incorrectly maps my face to somebody else who is dealing, I actually have no agency,

36:50.320 --> 36:54.880
I have no form of redress. So I can have full transparency. I know there was facial recognition,

36:54.880 --> 37:00.080
I know it was used, but I actually don't have any agency. So this critical missing part of

37:00.080 --> 37:05.440
transparency as an agency. But from like a technical perspective, I think that's not a really

37:05.440 --> 37:12.080
cool stuff going on. Like I really love, you know, a lot of the, the way adversarial models are

37:12.080 --> 37:17.040
being used to understand model explainability. There are a lot of the, the mimic models, right,

37:17.040 --> 37:21.840
where you have this like student teacher model. I think there's a lot of really good stuff going

37:21.840 --> 37:27.120
on there. And I'm super excited to see more of it being used in production. And like again,

37:27.120 --> 37:31.520
going back to this notion of cooking concept versus production, it's actually really hard to move

37:31.520 --> 37:37.280
some of the more advanced models into a way what big corporation or even like a medium size company

37:37.280 --> 37:42.480
could use it. Like they're conceptually really interesting, but you know, just like how long it

37:42.480 --> 37:50.160
takes to run one of those assessments is sometimes a deal breaker. To run the model or an assessment

37:50.160 --> 37:58.320
of the model because of its like of transparency. The complexity of an explainability model.

37:58.320 --> 38:05.120
So some, so for example, like some of the models on kind of factual fairness would have to

38:05.120 --> 38:10.480
iterate across every single potential scenario that could happen with the data in order to compute

38:11.120 --> 38:16.160
what would happen if someone's gender was which are male to female, right? Those things take

38:16.160 --> 38:23.520
a while. And maybe it works if you have like 10 variables, 15 variables, it's really hard if you

38:23.520 --> 38:34.640
have like 300 variables. What, you know, what other interesting things are you seeing happening in

38:35.280 --> 38:45.040
the field that will kind of, you know, are most likely to impact your customers, your clients.

38:45.040 --> 38:50.560
You know, I think there's a lot of conversations happening in the field that kind of range in their

38:50.560 --> 38:56.720
practicality and pragmatism. And I'm curious about the more pragmatic side of that. Yeah, me too.

38:59.680 --> 39:04.800
What, like, so like I said, I started the shop three years ago. I used to have a slide on

39:04.800 --> 39:08.400
every single one of my decks, like every single one I swear because I just got so tired of it.

39:09.040 --> 39:13.840
I would start every talk by saying there are three things I don't talk about. And my three

39:13.840 --> 39:19.280
things were Terminator Hal and Silicon Valley entrepreneurs saving the world. Because at that time,

39:19.280 --> 39:24.800
we had a closer view of, oh, you know, but I said every, because I swear to God, if someone sucked me

39:24.800 --> 39:28.800
into the Hal Conversation, it was quite into lose my mind. All right.

39:32.480 --> 39:38.480
And so is that, is that to say that, you know, just kind of putting, putting pause on the previous

39:38.480 --> 39:43.520
question, is that to say that you don't get involved in like, you know, do your customers even

39:43.520 --> 39:50.160
care about like AI safety kinds of questions and like paperclip maximizing kinds of questions?

39:50.160 --> 39:56.560
Yeah, I mean, like maybe for like intellectual curiosity, sure. But, you know,

39:57.600 --> 40:04.560
and paperclip reference for those that are not following is a reference to Peter Bostrom. And I'll

40:04.560 --> 40:14.000
we'll drop an analogy that he gives and we'll drop a link to my podcast with Peter in the show notes.

40:14.960 --> 40:19.600
Oh, cool. Is it a podcast? And that's really awesome. Yeah. I'm pretty sure we talked about the paperclip

40:19.600 --> 40:24.560
thing too. I would be surprised if you didn't. But yeah, I mean, I mean, I think yes,

40:24.560 --> 40:30.800
our intellectual curiosity sure. But I don't think, I think we may actually slowly be moving

40:30.800 --> 40:36.800
into a scarier world than we were before, particularly with some of the uses of AI and human resources,

40:36.800 --> 40:40.800
which is interesting because it's not just like super sexy fields and oh, yeah, you know,

40:40.800 --> 40:44.640
autonomous vehicles, sort of that the first field where we're actually adjusting some of these

40:44.640 --> 40:50.240
like more existential ethical concerns has been in HR because of the rise of effective computing

40:50.240 --> 40:56.480
and this concept that you can somehow measure someone's potential by their face or by their

40:56.480 --> 41:03.760
expressions or by their mannerisms. And also, you know, and it's you know, in retrospect,

41:03.760 --> 41:08.240
it's actually not surprising because in when we hire somebody, it is such a

41:09.200 --> 41:16.400
nebulous and difficult to quantify factor that we hire people on, right? Like, sometimes it's

41:16.400 --> 41:22.880
like ability, frankly, right? We like to think it's based on and even if we're being really

41:22.880 --> 41:27.360
vigorous about it, it's often based on potential and you may have a different interpretation

41:27.360 --> 41:33.120
of someone's potential than I do and then and it's it's like almost an inherently a biased process.

41:33.120 --> 41:39.280
So once we, but yet it is a world in which there's clearly a need for some sort of automation,

41:39.280 --> 41:44.240
whether or just in sheer amount of volume that companies have to deal with or, you know,

41:44.240 --> 41:49.920
mitigating the bias that already exists. So it's interesting and kind of a difficult problem.

41:49.920 --> 41:56.000
And some of the answers have brought in new, weird ethical existential problems. But in general,

42:00.400 --> 42:04.960
thank goodness. All right, well, and I say that because not that I'm not interested in the philosophical

42:04.960 --> 42:10.800
conversations, but sometimes it tracks from the it is a way of avoiding the actual problems that

42:10.800 --> 42:14.480
exist, right? Which are, by the way, none of these are new problems. These are problems that people

42:14.480 --> 42:19.440
have, you know, the first thing you realize and any of these ethical conversations about AI is that

42:19.440 --> 42:23.920
these are the same problems I have existed in society. They're just maybe more shumped in our

42:23.920 --> 42:28.880
face because they can happen faster in its scale. Right, right. I think there are,

42:31.120 --> 42:42.320
you know, there are, I think a range of reactions to ideas like AI and machine learning

42:42.320 --> 42:53.680
assisted hiring and, you know, computer vision as a kind of broad class of applications. And

42:55.440 --> 43:03.760
you see reactions ranging from, you know, the, you know, the technology is just a hammer. The hammer

43:03.760 --> 43:08.160
didn't kill anyone. It was the person that used the hammer that killed someone to, you know,

43:08.160 --> 43:15.920
the technology is, you know, the root of the evil and we should not use, you know, technology

43:15.920 --> 43:25.120
X for problem Y. Right. And I'm curious how you, you know, both how you personally kind of parse

43:25.120 --> 43:31.360
those kinds of arguments and also how you lead folks through a process to figure out, you know,

43:31.360 --> 43:36.560
what makes sense for them. This is why I really like talking to my lawyer friends. Seriously,

43:36.560 --> 43:41.600
I know, I've actually learned a lot from legal people, whether it's like, I thought you were about

43:41.600 --> 43:47.600
to run into a disclosure. No, no, no, as in like, you know, these are, these are actually questions

43:47.600 --> 43:52.960
that, you know, they've thought of. So the big question is always who has the liability. And in

43:52.960 --> 43:58.640
some sense, when someone says, oh, technology is just the hammer. They're kind of staying like

43:58.640 --> 44:06.480
technology is liable. Right. And I'm not liable. Right. Or yes, or the, you know, or I'm just the engineer,

44:06.480 --> 44:15.200
like I'm not liable, et cetera. And it is an interesting conundrum, even from like a legal

44:15.200 --> 44:21.280
liability perspective. Like, I think we can more or less agree that we can't hold an AI or model

44:21.280 --> 44:26.880
liable. So fine. Is it the data scientist? Is it the company? And if so, who at the company?

44:26.880 --> 44:32.320
And that's not really a solved problem. I think there was a lot of discussion around the Uber

44:32.320 --> 44:37.200
self driving car incident, right? Whether you're a self driving car, hit this woman on the road.

44:37.200 --> 44:43.440
And there was a cop potato of who's liable. So it turned out that the instruments had been tuned

44:43.440 --> 44:47.760
in a particular way that it didn't actually see her or didn't pick her up as like a, like a,

44:47.760 --> 44:51.760
like an object moving that it should avoid. If it were tuned a different way, it could have,

44:51.760 --> 44:56.960
by the way, so I think that's a really interesting point. I know at one point, they were talking about

44:56.960 --> 45:01.680
how well the woman driving, who was like the test driver should have been paying better attention.

45:02.640 --> 45:06.240
But you know, one can make a pretty easy argument that if you're in an autonomous vehicle,

45:06.240 --> 45:11.920
it's really hard to be constantly paying attention. Yeah. And just to have the level of like quick

45:11.920 --> 45:16.880
response and reaction that you need to, if you're, you know, you can't be on alert when you're not

45:16.880 --> 45:21.760
doing anything for hours at a time. So, you know, and I actually don't know where that, where that

45:21.760 --> 45:28.640
netted. So I'd be curious. But on, you know, on, on the other end, like just, just thinking about,

45:30.320 --> 45:35.600
like, you know, like this, the notion of technology being neutral, like, I, it's just, it's such a,

45:35.600 --> 45:41.760
again, the social science, such a silly concept. Because everything we build is in view

45:41.760 --> 45:46.000
with our values, just literally just by creating something, because we built it in a way just to

45:46.000 --> 45:51.520
solve a problem. And sometimes I like to use, you know, maybe non-value slate and examples,

45:51.520 --> 45:56.640
I think people get very emotionally polarized one way or the other. I'll give you a really

45:56.640 --> 46:02.720
good example and like how I, I thought about this myself. So I was in the Nordics in January,

46:02.720 --> 46:08.000
and I had to go to this event and I was using like whatever maps on my phone, right? And I go

46:08.000 --> 46:14.400
outside and it's like negative 30 or some insane temperature, right? And like in two minutes,

46:14.400 --> 46:18.720
my phone completely breaks like it dies. Like I'm watching the battery go to zero and it dies,

46:18.720 --> 46:22.640
no, it's going on. And then I go to the event and I'm going to and I'm like, hey, it's my

46:22.640 --> 46:26.800
job with my phone, I'm really sorry, I'm late, I don't know, like, oh yeah, yeah, that happens.

46:26.800 --> 46:32.880
Apparently, at certain, at a particular temperature and below that temperature, a smart phone,

46:32.880 --> 46:37.600
batteries die. And all you have to do, so they all, everybody has their own solutions for it,

46:37.600 --> 46:42.480
like they have little, like, they stick them in their mittens or whatever. Or, and then also,

46:42.480 --> 46:46.800
you, you just need to recharge it for a few minutes and it brings with battery back to full power.

46:46.800 --> 46:49.760
And I thought that was really interesting because I'm like, you know, that, that to me doesn't sound

46:49.760 --> 46:56.960
like an intractable problem. But if I were developing this technology in Cupertino, California,

46:56.960 --> 47:01.280
where it is never below 30 degrees, I probably wouldn't see that. I probably would never have

47:01.280 --> 47:05.760
had that problem as, you know, something I would adjust. And I thought it was really interesting

47:05.760 --> 47:10.160
that like I have a watch that knows what kind of slim stroke I'm doing, whether it's a backstroke

47:10.160 --> 47:17.760
or butterfly or, you know, freestyle or whatever. But my phone, the phone for a significant

47:17.760 --> 47:23.360
population of the world completely freezes in the temperature that's actually not very unusual

47:23.360 --> 47:27.600
for what they are. And, you know, all that is to say, like, everything we build has values,

47:27.600 --> 47:32.400
we choose to prioritize certain things over others. So it's on to say that, like, it's,

47:32.400 --> 47:40.000
it's fundamentally, like, not values driven. And, you know, it's, and, and yes, there are panels

47:40.000 --> 47:46.320
to be made with like, you know, the, the creation of the nuclear energy, et cetera. A lot of people

47:46.320 --> 47:52.000
use the Manhattan Project when thinking about things like liability or responsibility.

47:52.720 --> 47:58.160
But ultimately, you know, it is a responsibility because we do make these things, right? And the

47:58.160 --> 48:03.280
things that we make are taken and used by people. And yes, we can't control how everyone's

48:03.280 --> 48:08.880
going to use what we've built. But I do think creating a culture of responsibility is absolutely

48:08.880 --> 48:15.920
critical and necessary. I'm trying to think through whether that answered my question at all.

48:18.800 --> 48:26.720
It did not. Right. Well, you know, and I think there's an argument that, you know, you live in a

48:26.720 --> 48:32.400
domain to which there aren't answers to, you know, answer the way, you know, that, you know,

48:32.400 --> 48:39.680
there may be answers to questions that, uh, you know, engineers might. Yeah. And I think sometimes

48:39.680 --> 48:46.400
it's fine. Like, I think the, the act of interrogation and the act of understanding is, like,

48:46.400 --> 48:51.360
is actually sometimes which the thing that brings you to where you want to be. Because a lot of

48:51.360 --> 48:56.960
these things are like personal choices, right? Personal values. And sometimes it's hard to have

48:56.960 --> 49:01.680
these conversations because they do end up being values-laden. So we did this survey about a

49:01.680 --> 49:06.560
year ago at Accenture called the gray area survey. And rather than ask these, like, really obvious

49:06.560 --> 49:12.640
questions, like, should the AI kill person A or B? They were kind of something that was a lot

49:12.640 --> 49:18.080
more difficult to answer. So one, you know, one that I think was interesting was, you know,

49:18.080 --> 49:23.120
our company is responsible for not having surge pricing if there is a potential threat of a disaster.

49:23.120 --> 49:27.680
And that actually happened to me because I was in London and uh, it was crazy. I was like walking

49:27.680 --> 49:31.760
on bomb street and then all of a sudden this flood of people come running at me and it turns out

49:31.760 --> 49:36.000
that there, like, they, there was a scare where they thought there was somebody with a gun.

49:36.000 --> 49:40.640
It turned out to not be. But then none of us could leave because everyone was trying to get a car

49:40.640 --> 49:46.400
and uh, prices for, like, Uber's et cetera, like, absolutely insane. Or 300 pounds, right?

49:46.400 --> 49:51.120
Um, but then that leads to the question, like, is, is it a company's responsibility to, you know,

49:51.120 --> 49:55.120
make their models such that if there's a threat of a, an attack that, you know, they don't have

49:55.120 --> 49:59.200
search, is that unsafe? Is it unfair? I don't know if the answer is to that, right?

50:00.000 --> 50:04.480
Another one would be, you know, uh, an assert engine, if you search for CEO, you're largely going

50:04.480 --> 50:10.000
to get point men, uh, who are over the age of 40, uh, is that fair or unfair? Is it ethical or

50:10.000 --> 50:14.800
unethical? You could say, well, it's the ground truth. That's what it is. You know, most CEOs in

50:14.800 --> 50:21.440
the world are old white men named John, literally. Um, but, or, and then there's certainly an argument

50:21.440 --> 50:25.360
to be made that, you know, just because you're searching CEO doesn't necessarily mean you have to

50:25.360 --> 50:29.920
be told of bias truth. And maybe it can be more as, maybe we can show images of people who are

50:29.920 --> 50:35.360
CEOs who don't fit that singular mold. And again, that's a values solution that that's not,

50:35.360 --> 50:39.360
I think there is no right answer to some of these things. And I think part of it's actually being

50:39.360 --> 50:42.960
comfortable with the fact that there are no single answers to a lot of these questions.

50:42.960 --> 50:48.560
Yeah. Yeah. Yeah. Yeah. I think the example that was maybe floating around in the back of my

50:48.560 --> 50:54.480
mind was the, I forget the name of the individual, but one of the developers of yellow, which is

50:54.480 --> 51:01.600
a object detection library announced, uh, on Twitter. Hey, this computer is being a scary. I'm out of here.

51:02.400 --> 51:09.040
And it was, uh, right. It was, it was really, it was that's powerful. Yeah. I mean, the, the thing

51:09.040 --> 51:14.080
isn't that's the thing about like somebody creating a technology absolving themselves in

51:14.080 --> 51:19.280
responsibility. It sends a message of indifference going back to this notion of like power structure.

51:19.280 --> 51:23.520
You know, it doesn't matter if I personally mean Ramon say, I don't want to use computer vision

51:23.520 --> 51:27.600
technologies like, okay, you know, whatever. There's the person literally created it says,

51:27.600 --> 51:32.640
you know what? Like, this is not something I can, like, be behind anymore and then horrified by

51:32.640 --> 51:39.040
how this has been used. But it's such a powerful message to send, you know, and it's very clear

51:39.040 --> 51:46.320
that something needs to be done. And so, you know, with something like that as kind of background or

51:46.320 --> 51:54.000
context, say, how do you walk through, you know, personally, where you draw the line or, you know,

51:54.000 --> 52:00.960
say you're an engineer at place X, you know, is there an answer for, you know, where I'm going? You've

52:00.960 --> 52:08.000
had this conversation. It's just like, you know, go off on a mountain and like, you know, sit in the

52:08.000 --> 52:14.400
position. It's tough. But then like, actually, I had like a benefit like an existential crisis

52:14.400 --> 52:19.280
last year and thinking through exactly these decisions, right? You know, one can arrive at the

52:19.280 --> 52:24.720
conclusion that inherently all capitalism is evil, right? This notion that there's no such thing

52:24.720 --> 52:30.560
as an ethical company. And then I had this really great book recommended to me and it's called

52:30.560 --> 52:35.840
against purity and has nothing to do with ethics and AI. It actually has to do with, you know,

52:35.840 --> 52:45.120
movements that are about, you know, sort of like a moral good and how this sort of this idea of

52:45.120 --> 52:50.880
the most pure or the most good is actually detrimental to the cause. And the author actually uses

52:52.320 --> 52:57.520
climate change and the environmental movements to talk about how it can be harmful if we're just

52:57.520 --> 53:01.920
trying to like our ethics each other. And then you do see it sometimes. And I think, you know,

53:01.920 --> 53:07.200
this idea of like who's better than someone else because person acts works at evil company. Why,

53:07.200 --> 53:11.920
you know, I mean, you know, I can get fucked because I work at Accenture, right? Does that mean

53:11.920 --> 53:17.200
that everything I do is, you know, nullified or painted? And, you know, I don't, I would like to

53:17.200 --> 53:22.480
think that's not the best way to approach it because frankly, you do end up in a race to the

53:22.480 --> 53:27.360
bottom, right? Where everybody has to be more ethical or more good to the other person or more pure

53:27.360 --> 53:33.200
than the other person. And it's just not a helpful, it's not not a helpful, especially how ethical is

53:33.200 --> 53:38.160
it to be the ethics police that, you know, thinks they're superior at everybody? Exactly, which

53:38.160 --> 53:42.480
actually interestingly, like when I think about AI governance, I worry about this a lot. So again,

53:42.480 --> 53:48.880
like, thank you about everything going about states markets and democracy. Uh, often the way we do

53:48.880 --> 53:54.960
AI governance, like, and everyone's doing the same thing. You get a group of quote experts together.

53:54.960 --> 53:59.360
These are all like folks like me, right? And we all sit in the room and we just decide

53:59.360 --> 54:03.760
what ethics is. It's very strange and problematic. And if someone would have created a problem

54:03.760 --> 54:08.800
in that way, we would call it an authoritarian regime. We would actually not call it a democracy.

54:09.760 --> 54:17.120
So it's interesting that I have seen very few democratic processes being built around

54:17.120 --> 54:22.560
governments. And interestingly, I think corporations would be the first ones to actually do that

54:22.560 --> 54:29.360
if they do it right. Interesting, interesting. Well, I appreciate the subtitle of this book,

54:29.360 --> 54:35.280
you're recommending against purity, living ethically and compromised times. I think we can all

54:35.280 --> 54:43.760
relate to, yeah, yeah, or another. Um, cool. What else, what else is going on? Anything else that we

54:43.760 --> 54:50.080
should, uh, that we should be sure to cover or any pearls of wisdom for us, other books that we

54:50.080 --> 54:57.920
should be. Oh, man. I was looking at my book. Like, what's going on over here? Um, I don't know.

54:57.920 --> 55:03.200
What am I eating? Actually, my laptop is like propped up on this book called The Essentials of

55:03.200 --> 55:11.040
Risk Management, 600 pages on a financial risk management. Our pass. I thought it was a really

55:11.040 --> 55:17.520
interesting book like making those parallels. I tried to read and go to Esher Bach, right? I just,

55:17.520 --> 55:25.600
it made me go to sleep. I supposed to be one of those classics of, you know, thinking through

55:26.400 --> 55:31.840
computing, etc. And like, I guess I'm not smart enough for it. So, I mean, I thought it was fine.

55:31.840 --> 55:35.760
Like the anecdotes are kind of cool, but like, I can't read like a thousand pages of like

55:35.760 --> 55:43.600
disjointed anecdotes. I was kind of rough. Um, I suppose like for the end time STEM people are,

55:43.600 --> 55:50.320
like, we are, are collapsing social sciences. I don't know if you, uh, uh, I was like, I found

55:50.320 --> 55:55.360
something today where it just sort of boggles my mind where, you know, entire feet like

55:55.920 --> 56:02.320
the soul field can just decide like, oh, wow. Behavioral sciences, we are the ones who discovered it.

56:02.320 --> 56:08.000
And I'm like, yeah, social sciences. Well, as an example, there was something, I don't know,

56:08.000 --> 56:13.200
maybe six, six months ago or something where someone wrote this article that got a lot of

56:13.200 --> 56:17.120
publicity. It was like, oh, we should have a field of study that does X, Y, Z, and

56:17.120 --> 56:21.600
and like, oh, yeah, that exists. We've been STS. We've been doing that for.

56:22.880 --> 56:27.920
Yeah. I mean, without naming names, because it, you know, this person is a very nice individual,

56:28.560 --> 56:34.240
but I was like, was talking to somebody pretty high up at a major tech company,

56:34.240 --> 56:41.680
leading AI. And he had never heard of the field of HCI or STS. Never heard of it, but he was

56:41.680 --> 56:45.600
incredibly proud of the fact that he had just hired an ethicist to advise it. And I'm like,

56:45.600 --> 56:51.760
so you hired a philosopher to advise you, which you've never heard of STS or HCI.

56:53.360 --> 56:58.240
Cool. STS being science and technology studies in HCI human computer interaction.

56:58.800 --> 57:02.080
Yes. And I mean, I was like, good luck with your platform.

57:04.640 --> 57:10.560
And the thing is like, it's not to belittle the people who are trying, right? I think that

57:10.560 --> 57:15.520
there is this inherent ego about tech that I just, I found like, actually like mind boggling

57:15.520 --> 57:20.640
when I moved here. I just never been in a field where I don't just thought they were gods.

57:20.640 --> 57:25.680
I thought it was very strange. But hey, I come from the lowly social sciences. I worked in

57:25.680 --> 57:32.080
public policy and nonprofits before, you know, I was an economist for a minute, you know, like,

57:32.080 --> 57:38.240
I just never worked in a field where literally people thought that they just did everything better

57:38.240 --> 57:45.280
than everybody. It's amazing. As a data scientist, you know, I just, like I said, I didn't understand

57:45.280 --> 57:50.480
and I still don't. This idea that programming is better than everything or that technology is,

57:50.480 --> 57:55.920
you know, this notion of technological solutionism. And I think, you know, with the,

57:56.640 --> 58:00.800
it's kind of being brought to the forefront, the more we build artificial intelligence

58:00.800 --> 58:06.800
technologies, right? That we can't automate away human behavior and human preferences. And

58:06.800 --> 58:13.840
actually, I'd mention Bobby. Bobby and I have a paper on something I was working on called

58:13.840 --> 58:19.760
technological determinism and specifically thinking through recommendation systems and how they

58:19.760 --> 58:25.520
might nudge you to kind of be the same person forever. And they don't really encourage you to explore

58:25.520 --> 58:30.240
and expand your horizons simply because of the way they're constructed, right? A recommendation

58:30.240 --> 58:37.120
system takes your prior behavior, maps that, you know, who you are to an assessment of people who

58:37.120 --> 58:42.160
are quote, like you and gives you recommendations based on things you might like given other people

58:42.160 --> 58:49.280
who are profiled the same as you. That's actually kind of scary. A lot of conversation around this

58:49.280 --> 58:58.240
in the context of, you know, it actually, this conversation gets more prevalent kind of every

58:58.240 --> 59:03.440
kind of political, you know, election cycle, right? As we start talking about filter bubbles and

59:03.440 --> 59:09.200
things like that and monocultures and... Yeah, I mean, filter bubbles are kind of a more extreme

59:09.200 --> 59:16.080
example. But I'm even thinking of like, I don't know, I think a lot. I fortunately went to college

59:16.080 --> 59:22.480
and did all my stupid things before these ways existed of tracking and chasing God forbid,

59:22.480 --> 59:30.320
they're really a Facebook when I was in college for a year. But I wonder about these, you know,

59:30.320 --> 59:36.560
but in college, you know, back in the olden times, was an interesting way to sort of reinvent yourself,

59:36.560 --> 59:41.280
right? To think of yourself as just get like a fresh start. And I feel like sometimes for younger

59:41.280 --> 59:46.400
people, like you can't ever escape who you are and where you're from. And you can't ever be someone

59:46.400 --> 59:50.720
else or think of the world in a different way, right? Because like you have this weird baggage,

59:50.720 --> 59:56.960
just technological baggage with you forever. And I find that kind of sad. And I think about like,

59:56.960 --> 01:00:01.840
how much I have changed and how much it's part of, you know, human nature to change over time.

01:00:01.840 --> 01:00:06.320
Like we're supposed to evolve. We're supposed to be weird and different, you know? I worry a lot

01:00:06.320 --> 01:00:13.360
about the homogeneity that comes with tech. And I wonder whether, you know, social scientists

01:00:13.360 --> 01:00:18.960
or political scientists, middle of a PhD program today could even possibly enter the field of

01:00:18.960 --> 01:00:23.760
data science, right? Because, you know, whether or not it's because these job ad platforms,

01:00:24.800 --> 01:00:31.200
or these, you know, these NLP based engines that match, uh, resumes with jobs, which is not

01:00:31.200 --> 01:00:35.200
sometimes qualified or didn't, you know, didn't know what I was doing. I just didn't fit someone

01:00:35.200 --> 01:00:41.360
paradigm. And I felt my life and not fitting other people's paradigms. And I worry very much about

01:00:41.360 --> 01:00:47.680
like how people can still be individuals and be curious and just learn other things. Even though

01:00:47.680 --> 01:00:53.600
the internet has given us basically unlimited access to things, I wonder if, you know, in a sense,

01:00:53.600 --> 01:00:59.200
we may actually stifle all that possibility by thinking that like everything is this repeat cycle

01:00:59.200 --> 01:01:04.720
of behavior that we can predict patterns and these patterns are deterministic. And I would say that

01:01:04.720 --> 01:01:10.160
like, you know, rule number one of quantitative social science is patterns exist in human behavior,

01:01:10.160 --> 01:01:14.640
right? Rule number two is just because a pattern exists doesn't mean individual follows it.

01:01:14.640 --> 01:01:18.400
And I think rule number two hasn't really been arrived yet in some of the fields that we're talking

01:01:18.400 --> 01:01:28.160
about. That seems like a good place to leave things. Holy you with the big questions.

01:01:32.960 --> 01:01:39.200
I mean, it is interesting stuff to think about right on a, what day is today? I'm like on a Wednesday

01:01:39.200 --> 01:01:45.120
on a Friday. Clearly a Friday. Clearly it's a Friday. Friday called Tuesday.

01:01:46.800 --> 01:01:55.120
I have no idea what day it is anymore. It's really hard. It's, I don't recall if I've talked

01:01:55.120 --> 01:02:03.680
about this in an interview, but I've independently kind of validated this experience for several

01:02:03.680 --> 01:02:09.600
people like that March was this super, super, super long month and then April kind of flew by

01:02:09.600 --> 01:02:17.120
really quickly. But time is just really weird right now. I think for many of us. Yeah, it's like

01:02:17.120 --> 01:02:21.520
an absolute reminder that time is absolutely relative. Although it must be great, kind of great

01:02:21.520 --> 01:02:27.040
to be a kid right now because you're like forever summer vacation. You know, like you're just like

01:02:27.040 --> 01:02:33.920
the longest summer ever. I don't know. You've got to be old enough to know how what you didn't have before, though, I think, right?

01:02:34.880 --> 01:02:39.280
Otherwise, that's true. That's true. It must be kind of cool to be like young enough that you don't

01:02:39.280 --> 01:02:45.680
really get how scary the situation is. Yeah. But like you're like, cool enough that you're like, oh wow,

01:02:45.680 --> 01:02:50.960
I just get to be home for months on end and like playing with my toys. Yeah. And all of that must be,

01:02:50.960 --> 01:02:55.520
it must be really interesting time. I do feel bad for all the kids graduating. And he's sort of

01:02:55.520 --> 01:03:06.000
for a band right now. This is worse than two thousand. Yeah. Well, Ramon, it was wonderful catching

01:03:06.000 --> 01:03:13.440
up with you as always. We'll have to make sure to do it more frequently. Yes, yes, absolutely.

01:03:15.200 --> 01:03:18.160
Thanks so much. Thank you very much. Happy Mianzen.

01:03:18.160 --> 01:03:28.480
All right, everyone. That's our show for today. To learn more about today's guest or the topics mentioned in this interview,

01:03:28.480 --> 01:03:35.040
visit twimmelai.com. Of course, if you like what you hear on the podcast, please subscribe,

01:03:35.040 --> 01:03:50.960
rate and review the show on your favorite pod catcher. Thanks so much for listening and catch you next time.

