1
00:00:00,000 --> 00:00:27,080
Hey everyone, just a quick announcement before we get you over to today's show.

2
00:00:27,080 --> 00:00:33,640
I'd like to invite you to join me on Thursday, August 27th for my conversation with Dylan

3
00:00:33,640 --> 00:00:39,720
Urb, co-founder and CEO of PaperSpace, as we discuss machine learning as a software

4
00:00:39,720 --> 00:00:41,720
engineering discipline.

5
00:00:41,720 --> 00:00:45,520
We've been having a ton of fun with these watch parties and of course Dylan and I will

6
00:00:45,520 --> 00:00:51,240
be in the chat answering all of your questions about the relationship and differences between

7
00:00:51,240 --> 00:00:57,040
traditional software development practices and MLOPS and how to scale up and manage machine

8
00:00:57,040 --> 00:00:59,560
learning pipelines.

9
00:00:59,560 --> 00:01:07,720
We'll begin at 1pm pacific time, register at twimmelai.com slash watch 404 for reminders

10
00:01:07,720 --> 00:01:09,400
and updates.

11
00:01:09,400 --> 00:01:11,520
And now on to the show.

12
00:01:11,520 --> 00:01:15,800
Alright everyone, I am on the line with Don Song.

13
00:01:15,800 --> 00:01:23,600
Don is a professor of computer science at UC Berkeley and CEO and founder at OASIS Labs.

14
00:01:23,600 --> 00:01:26,040
Don, welcome to the Twimmelai AI podcast.

15
00:01:26,040 --> 00:01:27,720
Thanks a lot for having me.

16
00:01:27,720 --> 00:01:31,480
Yeah, I'm really looking forward to digging into our conversation.

17
00:01:31,480 --> 00:01:36,960
We will be talking quite a bit about responsible data and what that means.

18
00:01:36,960 --> 00:01:41,760
But before we do, let's start off with a little bit of background.

19
00:01:41,760 --> 00:01:44,600
What got you started in working in AI?

20
00:01:44,600 --> 00:01:53,680
So my undergraduate was in physics and I switched to computer science in my graduate school.

21
00:01:53,680 --> 00:02:00,600
And in my graduate school and also later on as a professor I spent a lot of time actually

22
00:02:00,600 --> 00:02:06,080
focusing in security and privacy, thinking about how to build security systems and so

23
00:02:06,080 --> 00:02:07,080
on.

24
00:02:07,080 --> 00:02:15,320
But at the same time, I've always been really interested in building intelligent machines.

25
00:02:15,320 --> 00:02:22,080
So yes, I'm really glad that I have had the opportunity and the experience to really

26
00:02:22,080 --> 00:02:25,440
try to see how we can make progress in that space.

27
00:02:25,440 --> 00:02:30,480
AI and trying to figure out how to build intelligent machines.

28
00:02:30,480 --> 00:02:34,240
I think it's really probably the ultimate goal.

29
00:02:34,240 --> 00:02:39,200
If we can build intelligent machines, there are so many problems that we can solve and

30
00:02:39,200 --> 00:02:40,200
so on.

31
00:02:40,200 --> 00:02:50,000
So it's just really, really exciting and I would say probably one of the most important

32
00:02:50,000 --> 00:02:51,000
pursuits.

33
00:02:51,000 --> 00:02:52,000
Awesome.

34
00:02:52,000 --> 00:02:53,000
Awesome.

35
00:02:53,000 --> 00:02:55,400
And you're currently working on a startup now, Oasis Labs.

36
00:02:55,400 --> 00:02:57,400
How long have you been working on that?

37
00:02:57,400 --> 00:02:59,600
Oasis Labs is about two years now.

38
00:02:59,600 --> 00:03:06,320
Tell us a little bit about the genesis of Oasis Labs and the problem that you're looking

39
00:03:06,320 --> 00:03:07,320
to solve there.

40
00:03:07,320 --> 00:03:13,800
So at Oasis Labs, we focus on building what we call a platform for a responsible data

41
00:03:13,800 --> 00:03:14,960
economy.

42
00:03:14,960 --> 00:03:21,280
So as we know, Internet has really changed everybody's lives and mostly for the better,

43
00:03:21,280 --> 00:03:27,240
but at the same time, we do see many challenges in particular.

44
00:03:27,240 --> 00:03:30,160
As we know, data is critical.

45
00:03:30,160 --> 00:03:35,840
It's a key driver for a modern economy, but a lot of this data is also really sensitive

46
00:03:35,840 --> 00:03:41,960
and handling the sensitive data poses many challenges for the, on the user side as well

47
00:03:41,960 --> 00:03:44,200
as on the business side.

48
00:03:44,200 --> 00:03:49,800
So for the user side users are losing control of their data, they don't really know what

49
00:03:49,800 --> 00:03:54,000
their data has been used for, how their data has been used and so on.

50
00:03:54,000 --> 00:04:00,120
And also, they are now getting direct sufficient direct benefits from their data.

51
00:04:00,120 --> 00:04:06,160
And on the business side, businesses continue to suffer from large-scale data breaches

52
00:04:06,160 --> 00:04:11,720
and also it's becoming more and more expensive and cumbersome for them to comply with, for

53
00:04:11,720 --> 00:04:16,440
example, privacy regulations like GDPR and CCPA.

54
00:04:16,440 --> 00:04:22,280
And some more importantly, it's still really difficult for business to utilize data due

55
00:04:22,280 --> 00:04:25,760
to data silos and privacy concerns and so on.

56
00:04:25,760 --> 00:04:33,760
So the hope is that we can build a new platform for a responsible data economy that helps

57
00:04:33,760 --> 00:04:40,240
address many of these challenges that helps users to better maintain control of their data

58
00:04:40,240 --> 00:04:46,120
and rise to data and also help businesses to better utilize data, but in a privacy

59
00:04:46,120 --> 00:04:51,840
preserving and responsible way, essentially to enable a new paradigm to address many of

60
00:04:51,840 --> 00:04:54,320
the challenges that I mentioned.

61
00:04:54,320 --> 00:04:59,320
We've talked quite a bit on the podcast over the past few years about differential privacy

62
00:04:59,320 --> 00:05:01,960
and related techniques.

63
00:05:01,960 --> 00:05:07,160
Is that a core piece of this vision of a responsible data platform?

64
00:05:07,160 --> 00:05:09,960
Yeah, that's a great question.

65
00:05:09,960 --> 00:05:16,080
So in order to enable something like what I just mentioned, this responsible data and

66
00:05:16,080 --> 00:05:21,800
responsible data economy, essentially, it needs to address a number of different

67
00:05:21,800 --> 00:05:25,480
questions, different types of questions.

68
00:05:25,480 --> 00:05:33,640
So first of all, we need to ensure that users' rights to data is properly maintained as

69
00:05:33,640 --> 00:05:37,360
well logs and so on.

70
00:05:37,360 --> 00:05:43,840
So for that, we actually utilize blockchain to maintain an immutable ledger for users'

71
00:05:43,840 --> 00:05:49,920
rights to data and also the log of how the data has been utilized.

72
00:05:49,920 --> 00:05:58,320
And then also, we need to ensure that when the data is used, it's used in a way that we

73
00:05:58,320 --> 00:06:06,120
call it a controlled use that actually satisfies users' privacy requirements and their policies

74
00:06:06,120 --> 00:06:09,440
of how their data should be used.

75
00:06:09,440 --> 00:06:14,840
So for that, essentially, we need to address at least two separate types of questions.

76
00:06:14,840 --> 00:06:21,320
One is typically today, for example, when you talk about data markets, the buyer usually

77
00:06:21,320 --> 00:06:23,320
buys a copy of the data.

78
00:06:23,320 --> 00:06:26,360
And they essentially get a raw copy of the data.

79
00:06:26,360 --> 00:06:32,720
And once they buy or get raw access to the data, they essentially, they can do anything

80
00:06:32,720 --> 00:06:33,920
they want with the data.

81
00:06:33,920 --> 00:06:39,880
They can go ahead and resell the data, they can use the data for other purposes that

82
00:06:39,880 --> 00:06:44,720
may not be for the best interests of the data owner and so on.

83
00:06:44,720 --> 00:06:50,320
So that's also one of the big challenges today for data use.

84
00:06:50,320 --> 00:06:57,440
So in contrast, ideally, for responsible data use, what we need to enable is what we

85
00:06:57,440 --> 00:06:58,760
call controlled use.

86
00:06:58,760 --> 00:07:05,240
So in this case, for example, the buyer of the data, they don't just buy the data itself,

87
00:07:05,240 --> 00:07:09,320
what the buyer is the use of the data.

88
00:07:09,320 --> 00:07:17,360
To enable this, essentially, they shouldn't get a copy or a raw access to the data itself.

89
00:07:17,360 --> 00:07:27,160
So what we enable in this platform in this controlled use is enable the buyer or the user

90
00:07:27,160 --> 00:07:33,120
of the data to actually use the data in a confined environment.

91
00:07:33,120 --> 00:07:36,440
You can also think of it as a black box.

92
00:07:36,440 --> 00:07:44,280
So the data will only be computed over in this black box so that the data itself doesn't

93
00:07:44,280 --> 00:07:52,240
leak out and then the buyer or the user of the data doesn't actually ever get raw access

94
00:07:52,240 --> 00:07:55,120
to the data, doesn't get a direct copy of the data.

95
00:07:55,120 --> 00:07:59,920
They can only use the data in this black box confined to environments.

96
00:07:59,920 --> 00:08:00,920
So that's number one.

97
00:08:00,920 --> 00:08:02,840
We call that secure computing.

98
00:08:02,840 --> 00:08:12,720
There are a number of techniques that you can use to enable this secure computing to essentially

99
00:08:12,720 --> 00:08:16,480
you can use it as a way of computing over encrypted data.

100
00:08:16,480 --> 00:08:24,320
So basically, right, so you can use cryptography-based approaches such as homomorphic encryption or

101
00:08:24,320 --> 00:08:29,160
multi-party computation and so on.

102
00:08:29,160 --> 00:08:36,320
So you can use secure hardware which also essentially provides this type of black box-based

103
00:08:36,320 --> 00:08:43,720
black box like confined execution environments so that the data can only be used inside this

104
00:08:43,720 --> 00:08:45,240
black box environment.

105
00:08:45,240 --> 00:08:46,240
Okay.

106
00:08:46,240 --> 00:08:54,760
And a lot of ways it sounds like there's a much lower tech analogy here in like old school

107
00:08:54,760 --> 00:08:55,760
direct marketing.

108
00:08:55,760 --> 00:09:00,400
You have these list aggregators that would collect people's mailing addresses and they

109
00:09:00,400 --> 00:09:08,680
don't want to give the catalog vendors access to their mailing addresses because then

110
00:09:08,680 --> 00:09:11,120
why would they need to license them again?

111
00:09:11,120 --> 00:09:16,840
So instead, the catalog vendors would send them the things that they want to send out.

112
00:09:16,840 --> 00:09:20,920
I think the same thing happens in email as well, right?

113
00:09:20,920 --> 00:09:26,320
So the list company won't give someone the list, they'll say, we'll send your email for

114
00:09:26,320 --> 00:09:27,320
you.

115
00:09:27,320 --> 00:09:30,520
And a lot of ways you're saying, we're not going to give you the data but we'll do your

116
00:09:30,520 --> 00:09:32,240
compute for you.

117
00:09:32,240 --> 00:09:38,040
And now you need to come up with interesting ways to allow people to actually do the compute

118
00:09:38,040 --> 00:09:43,720
on these personal data without giving them access to it.

119
00:09:43,720 --> 00:09:45,960
That's an interesting analogy.

120
00:09:45,960 --> 00:09:52,520
So of course, computing on a data is much more complicated than sending out emails to

121
00:09:52,520 --> 00:09:54,360
your email address and so on, right?

122
00:09:54,360 --> 00:10:01,520
So that requires an entirely new type of technology to enable the secure computing so that you

123
00:10:01,520 --> 00:10:06,400
can only compute on the data you can now actually get access to the data.

124
00:10:06,400 --> 00:10:11,040
So there's one part and the other part is then you want to ensure that the computation

125
00:10:11,040 --> 00:10:19,480
itself on the data also complies with the user's policies of how their data should be utilized.

126
00:10:19,480 --> 00:10:25,280
For example, you want to ensure that when someone uses the data to train a machine learning

127
00:10:25,280 --> 00:10:33,840
model, then in the end, the machine learning model is being used for queries, for inference,

128
00:10:33,840 --> 00:10:34,840
and so on.

129
00:10:34,840 --> 00:10:38,800
You want to ensure that the machine learning model itself doesn't leak, individual usage

130
00:10:38,800 --> 00:10:41,000
information.

131
00:10:41,000 --> 00:10:46,480
And so you mentioned about differential privacy earlier, differential privacy is one example

132
00:10:46,480 --> 00:10:50,640
technology that can help address this problem.

133
00:10:50,640 --> 00:10:58,040
So speaking of which, like the privacy challenges for training machine learning models, I can

134
00:10:58,040 --> 00:11:00,040
give you one example.

135
00:11:00,040 --> 00:11:06,960
This is a recent work that we did in collaboration with the researchers from Google and so on.

136
00:11:06,960 --> 00:11:13,200
And the question here where trying to explore is the following.

137
00:11:13,200 --> 00:11:19,800
As we know, that neural networks has very high capacity and they can, so the question

138
00:11:19,800 --> 00:11:25,600
is whether when you train a machine learning model, it actually remembers a lot about the

139
00:11:25,600 --> 00:11:34,880
original training data and if it does, it can actually exploit this issue and try to actually

140
00:11:34,880 --> 00:11:40,920
learn sensitive information about the original training data in this case, even just through

141
00:11:40,920 --> 00:11:46,560
requiring the machine learning model without even getting a copy of, for example, the parameters

142
00:11:46,560 --> 00:11:47,800
and so on of the model.

143
00:11:47,800 --> 00:11:48,800
Right.

144
00:11:48,800 --> 00:11:52,920
If I remember correctly, there have been papers where they were able to demonstrate that

145
00:11:52,920 --> 00:11:59,720
you can reconstruct images that were part of a training set of the machine learning model.

146
00:11:59,720 --> 00:12:07,480
So that's one example and so the example work that we did is in the language model setting.

147
00:12:07,480 --> 00:12:14,560
So we showed that if you train a language model, for example, using an email data sets,

148
00:12:14,560 --> 00:12:17,400
in our case, it's called the Unreal email data sets.

149
00:12:17,400 --> 00:12:24,680
The Unreal email data sets naturally contains uses social security numbers and critical

150
00:12:24,680 --> 00:12:25,680
numbers.

151
00:12:25,680 --> 00:12:32,560
And we showed that when you train a language model on this email data sets, an attacker

152
00:12:32,560 --> 00:12:40,320
by devising new attacks and just by querying this language model without even knowing the

153
00:12:40,320 --> 00:12:45,120
details of the model, such as the parameters of the model, the attacker actually is able

154
00:12:45,120 --> 00:12:51,760
to recover the original uses credit card and social security numbers that were embedded

155
00:12:51,760 --> 00:12:55,600
in the original data sets.

156
00:12:55,600 --> 00:13:01,160
So this is another example showing that as we train machine learning models, it's really

157
00:13:01,160 --> 00:13:08,360
important to pay attention to privacy protection to use this data.

158
00:13:08,360 --> 00:13:14,120
And in this case, actually, we showed that for this particular case, we actually can have

159
00:13:14,120 --> 00:13:17,160
a very good solution to the problem.

160
00:13:17,160 --> 00:13:23,840
The solution is that instead of a training vanilla language model, if we train a different

161
00:13:23,840 --> 00:13:31,120
language model, then in this case, we can still have pretty good utility, but at the same

162
00:13:31,120 --> 00:13:41,080
time, we can significantly enhance the privacy protection of the user's data in this language

163
00:13:41,080 --> 00:13:42,080
model.

164
00:13:42,080 --> 00:13:43,080
Right.

165
00:13:43,080 --> 00:13:49,520
So essentially, so that's why, as I mentioned, in order to build a platform for a responsible

166
00:13:49,520 --> 00:13:57,200
data economy, we also need to ensure that the computation on data itself doesn't leak

167
00:13:57,200 --> 00:14:00,120
users sensitive information.

168
00:14:00,120 --> 00:14:06,560
And in this case, utilizing technologies like differential privacy can help us to ensure

169
00:14:06,560 --> 00:14:12,600
that the computation results itself doesn't leak sensitive information about individual

170
00:14:12,600 --> 00:14:13,600
users.

171
00:14:13,600 --> 00:14:22,120
OK, in the example that you are describing to what degree are you being fine-grained about

172
00:14:22,120 --> 00:14:27,360
what you consider sensitive information versus what isn't sensitive information?

173
00:14:27,360 --> 00:14:30,400
Everyone's talking about GPT-3.

174
00:14:30,400 --> 00:14:36,280
And there's a very coarse-grained argument that says that what GPT-3 is doing is kind

175
00:14:36,280 --> 00:14:41,760
of remembering all of the text on the internet and kind of regurgitating it in creative ways

176
00:14:41,760 --> 00:14:44,080
based on the prompt.

177
00:14:44,080 --> 00:14:49,080
And so from that perspective, all it's doing is leaking information, but in a constructive

178
00:14:49,080 --> 00:14:50,080
way.

179
00:14:50,080 --> 00:14:54,320
And trying to draw a parallel between that and the data leaking that we're talking about

180
00:14:54,320 --> 00:14:55,320
in this case.

181
00:14:55,320 --> 00:14:56,320
Right.

182
00:14:56,320 --> 00:14:58,320
Yeah, that's a very good question.

183
00:14:58,320 --> 00:15:05,000
And exactly, even in a work that we did, studying the privacy challenges of language

184
00:15:05,000 --> 00:15:11,640
models, one of the main issues here, when the vanilla language model leaks sensitive

185
00:15:11,640 --> 00:15:18,120
information about user's data, it is memorization, it is remembering those credit card numbers

186
00:15:18,120 --> 00:15:23,280
and social security numbers that were infected in the original training data set.

187
00:15:23,280 --> 00:15:27,560
So in this case, what we want is when you train a language model, you want a language

188
00:15:27,560 --> 00:15:34,640
model to really learn essentially how to predict, for example, the next words, the next

189
00:15:34,640 --> 00:15:43,480
character and so on, for the things that's essentially in this probabilistic way, but now

190
00:15:43,480 --> 00:15:48,440
actually remembering all these individual social security numbers and credit card numbers

191
00:15:48,440 --> 00:15:49,440
and so on.

192
00:15:49,440 --> 00:15:53,200
So essentially, it is how to address this memorization issue.

193
00:15:53,200 --> 00:15:59,760
And in our work, we actually showed studies and that's a, this language model, they

194
00:15:59,760 --> 00:16:06,760
do remember these occurrences for social security numbers and credit card numbers in this particular

195
00:16:06,760 --> 00:16:07,760
case.

196
00:16:07,760 --> 00:16:14,600
And also, we proposed the measures of code exposure to actually measure the, like the degree

197
00:16:14,600 --> 00:16:19,760
of the memorization that the language model has essentially has occurred in the language

198
00:16:19,760 --> 00:16:20,760
model.

199
00:16:20,760 --> 00:16:27,280
And for GPT-3, so we actually, we have further extensions of our work that is studying

200
00:16:27,280 --> 00:16:34,760
these type of models and like you said, these really, really large models, they can actually

201
00:16:34,760 --> 00:16:36,400
remember a lot.

202
00:16:36,400 --> 00:16:42,400
And, and oftentimes, these really large models, they are changed in the public data, but

203
00:16:42,400 --> 00:16:48,240
when you actually have private data, it can really potentially, could remember a lot

204
00:16:48,240 --> 00:16:53,320
about sensitive information and that's, these are the kind of issues that we really need

205
00:16:53,320 --> 00:16:59,120
to pay attention to because otherwise, these private data, they can contain both individual

206
00:16:59,120 --> 00:17:03,120
users' sensitive information and also when you're trained to over business data also

207
00:17:03,120 --> 00:17:05,680
can be a lot of proprietary information as well.

208
00:17:05,680 --> 00:17:13,720
Yeah, I think the, the issue that I was trying to form a question around is, it seems to

209
00:17:13,720 --> 00:17:20,520
me a lot easier to figure out how to get a model to not remember or, or not leak or

210
00:17:20,520 --> 00:17:27,480
not even learn from Social Security numbers because those are, you know, very fixed in

211
00:17:27,480 --> 00:17:28,480
nature.

212
00:17:28,480 --> 00:17:29,480
They've got a fixed format.

213
00:17:29,480 --> 00:17:33,960
It's easy to identify them in the training data, it's potentially easy to teach the

214
00:17:33,960 --> 00:17:37,760
model not to, to remember them in some way.

215
00:17:37,760 --> 00:17:43,720
But if you've got a language model and you're trying to maybe fine tune it on a, on a private

216
00:17:43,720 --> 00:17:50,080
data set, there's potentially a ton of sensitive information, you know, say about the inner

217
00:17:50,080 --> 00:17:55,240
workings of a business or, you know, past, you know, deals or contracts or things like

218
00:17:55,240 --> 00:18:02,160
this that it, I'm envisioning a lot of scenarios where it's hard to separate the information

219
00:18:02,160 --> 00:18:07,000
that you want the model to learn from and the information that you want the, the model

220
00:18:07,000 --> 00:18:08,000
to not leak.

221
00:18:08,000 --> 00:18:10,040
Right, that's a very good question.

222
00:18:10,040 --> 00:18:15,760
So when I talked about the solution of linear, differential private language model, so

223
00:18:15,760 --> 00:18:21,440
in this case, the solution is not just particular for, for example, credit current numbers or

224
00:18:21,440 --> 00:18:25,400
Social Security numbers, it's in, right, it's general.

225
00:18:25,400 --> 00:18:32,640
You don't pre-specify the type of sensitive information that you need to protect.

226
00:18:32,640 --> 00:18:37,560
Differential privacy is a very general notion of privacy.

227
00:18:37,560 --> 00:18:42,520
It's not specific to a particular type of sensitive information.

228
00:18:42,520 --> 00:18:48,640
So the idea there is essentially, so when we say, for example, an algorithm is differential

229
00:18:48,640 --> 00:18:55,600
private, it follows the following, a high level definition is essentially, if you consider

230
00:18:55,600 --> 00:19:03,200
you have two, we call it neighboring data sets, where one data set has one more data points,

231
00:19:03,200 --> 00:19:10,600
then the other, for example, one data point about them, that's not in the other data

232
00:19:10,600 --> 00:19:11,800
sets.

233
00:19:11,800 --> 00:19:17,800
And then we compute an algorithm over these two neighboring data sets.

234
00:19:17,800 --> 00:19:20,600
The algorithm is randomized.

235
00:19:20,600 --> 00:19:24,880
And in this case, we say that the algorithm is differential private.

236
00:19:24,880 --> 00:19:32,320
If the computation results of this algorithm over these two neighboring data sets is very

237
00:19:32,320 --> 00:19:33,320
close.

238
00:19:33,320 --> 00:19:39,480
Essentially, the probability distribution of the computation results from this algorithm

239
00:19:39,480 --> 00:19:48,240
over these two neighboring data sets is communistically very close, then what this says is essentially

240
00:19:48,240 --> 00:19:54,200
from the computation results, the attacker wouldn't be able to tell what their sense data

241
00:19:54,200 --> 00:19:57,440
point has been used in the computation or not.

242
00:19:57,440 --> 00:20:02,960
And this is a way to essentially talk about showing that the computation result has not

243
00:20:02,960 --> 00:20:08,120
really leaked much information about, for example, sense data.

244
00:20:08,120 --> 00:20:11,840
So similarly, when you carry over to the machine model, it's similar.

245
00:20:11,840 --> 00:20:17,160
So essentially, from the machine model, the attacker wouldn't be able to tell whether

246
00:20:17,160 --> 00:20:22,320
a particular, for example, social security number has been used in the data sets and

247
00:20:22,320 --> 00:20:25,640
hence, then in this case, you won't.

248
00:20:25,640 --> 00:20:31,760
So in this way, the training machine model can help enhance the privacy protection for

249
00:20:31,760 --> 00:20:34,200
the original training data set.

250
00:20:34,200 --> 00:20:41,280
Okay, do you draw a distinction between techniques like differential privacy that are preventing

251
00:20:41,280 --> 00:20:47,520
information being leaked and techniques that are preventing the network from memorizing

252
00:20:47,520 --> 00:20:49,520
the information in the first place?

253
00:20:49,520 --> 00:20:50,520
I see.

254
00:20:50,520 --> 00:20:56,440
So in this particular case, what the differential privacy, that's, for example, on your training,

255
00:20:56,440 --> 00:21:02,240
the differential privacy machine model, in this case, what you are doing is actually trying

256
00:21:02,240 --> 00:21:08,200
to, it is actually reducing the memorization that the network is doing.

257
00:21:08,200 --> 00:21:16,560
And in our work, we actually showed, as I mentioned, that with our measure of this exposure,

258
00:21:16,560 --> 00:21:20,960
which measures the degree of memorization, we actually showed that when you train a differential

259
00:21:20,960 --> 00:21:29,600
a private language model, in this case, actually, you are reducing this type of memorization.

260
00:21:29,600 --> 00:21:36,160
In the case of differential privacy, typically in an application that involves differential

261
00:21:36,160 --> 00:21:43,160
privacy, you're limited to doing computation or analysis on an aggregate level.

262
00:21:43,160 --> 00:21:47,960
Does that mean that the kinds of applications you'll be able to support in a, this kind

263
00:21:47,960 --> 00:21:54,480
of, in a responsible data platform or scenario or only these kinds of aggregate types of

264
00:21:54,480 --> 00:21:55,480
computations?

265
00:21:55,480 --> 00:21:57,880
That's a very good question.

266
00:21:57,880 --> 00:22:02,760
So I think it can support different types of computations, for example, if usage data

267
00:22:02,760 --> 00:22:11,640
is only used to, like, the computing results is only used for users' own consumption,

268
00:22:11,640 --> 00:22:17,240
then you can essentially do arbitrary computation on users' data and then just review the computation

269
00:22:17,240 --> 00:22:20,000
results to the user's self.

270
00:22:20,000 --> 00:22:26,160
But when you want to compute over multiple usage data and then release the results, the

271
00:22:26,160 --> 00:22:31,360
computation results, then in this case, oftentimes you are already computing some kind of aggregate

272
00:22:31,360 --> 00:22:37,800
sense, or you're actually machine models of a different user's data and then in this

273
00:22:37,800 --> 00:22:38,800
case, yes.

274
00:22:38,800 --> 00:22:44,320
So you actually, you really do need techniques like differential privacy and so on to ensure

275
00:22:44,320 --> 00:22:45,320
that.

276
00:22:45,320 --> 00:22:49,120
So when you are computing over different users' data, then the computing results doesn't

277
00:22:49,120 --> 00:22:53,080
leak sensitive information about individual users.

278
00:22:53,080 --> 00:22:54,880
You mentioned homomorphic encryption.

279
00:22:54,880 --> 00:22:59,200
When you talk a little bit about where that comes into play, I've not spent a lot of

280
00:22:59,200 --> 00:23:04,280
time looking at it, but I understand that in that case, the set of operations that you

281
00:23:04,280 --> 00:23:09,560
can apply that can retain this homomorphic property is somewhat limited.

282
00:23:09,560 --> 00:23:12,040
Does that is that a big barrier?

283
00:23:12,040 --> 00:23:17,720
So homomorphic encryption is one type of cryptographic techniques to enable your two essentially

284
00:23:17,720 --> 00:23:25,400
to computing over encrypted data, and it's one type of solutions to the problem that

285
00:23:25,400 --> 00:23:27,560
I mentioned, the secure computing.

286
00:23:27,560 --> 00:23:30,160
So the goal of the secure computing is following.

287
00:23:30,160 --> 00:23:38,080
We oftentimes talk about this simulation of ideal worlds with trusted third party.

288
00:23:38,080 --> 00:23:45,720
So with the secure computing, the goal is that, let's say we have trusted third party

289
00:23:45,720 --> 00:23:51,280
in this ideal world, and what you do is that you gave data to this trusted third party,

290
00:23:51,280 --> 00:23:57,280
and also you gave an algorithm in this case to the trusted third party.

291
00:23:57,280 --> 00:24:01,760
And the trusted third party will compute this algorithm function, let's say a function

292
00:24:01,760 --> 00:24:07,760
f over your input x, then there's the computing result f of x.

293
00:24:07,760 --> 00:24:12,520
So in this case, only f of x will be revealed, nothing else.

294
00:24:12,520 --> 00:24:16,320
So this is what happens in the ideal world with this trusted third party.

295
00:24:16,320 --> 00:24:23,400
So you trust that this trusted third party will not leak any sensitive information about

296
00:24:23,400 --> 00:24:29,120
your data only, the computing, the computation result f of x is revealed.

297
00:24:29,120 --> 00:24:33,800
But of course, then how to find this trusted third party?

298
00:24:33,800 --> 00:24:38,800
In the past, essentially, essentially people rely on trust of a particular party based

299
00:24:38,800 --> 00:24:45,920
on business contracts, and so on. But of course, that's suboptimal.

300
00:24:45,920 --> 00:24:52,920
So even in the best case, the trusted third party tries to comply to the contract,

301
00:24:52,920 --> 00:24:56,200
their own system may be compromised, and so on.

302
00:24:56,200 --> 00:25:00,760
So essentially, the question is how we can have technical solutions,

303
00:25:00,760 --> 00:25:07,520
ideally even with the provable guarantees to ensure this ideal world,

304
00:25:07,520 --> 00:25:10,560
essentially, be able to simulate this trusted third party.

305
00:25:10,560 --> 00:25:15,840
And in order to do that, essentially, the community has been developing

306
00:25:15,840 --> 00:25:17,920
different types of solutions.

307
00:25:17,920 --> 00:25:21,400
This is a homomorphic encryption, it's one type of solutions,

308
00:25:21,400 --> 00:25:26,680
utilizing cryptography, where you essentially, you encrypt the inputs,

309
00:25:26,680 --> 00:25:32,640
and then you compute over the inputs, and then you generate this encrypted

310
00:25:32,640 --> 00:25:37,480
computing results, so that only the original user is able to decrypt.

311
00:25:37,480 --> 00:25:42,680
And so then they only learn the computing results.

312
00:25:42,680 --> 00:25:46,560
And another way, as I mentioned, is that you can use secure hardware,

313
00:25:46,560 --> 00:25:52,840
where based on certain hardware and software combined solutions,

314
00:25:52,840 --> 00:25:55,680
you simulate this black box environment.

315
00:25:55,680 --> 00:25:58,440
We call it a secure execution environment,

316
00:25:58,440 --> 00:26:04,840
and also called a secure enclave, where, again, you put data into a black box,

317
00:26:04,840 --> 00:26:11,120
and also you put the function of the program into this black box.

318
00:26:11,120 --> 00:26:14,600
And the hardware and software combined solution

319
00:26:14,600 --> 00:26:19,400
ensures that when the program is executing inside this black box,

320
00:26:19,400 --> 00:26:22,880
nothing else, nothing outside this black box,

321
00:26:22,880 --> 00:26:27,000
including the operating system or other applications,

322
00:26:27,000 --> 00:26:29,760
we'll be able to see what's running inside,

323
00:26:29,760 --> 00:26:32,720
or we'll be able to modify what's running inside.

324
00:26:32,720 --> 00:26:38,920
And hence this black box ensures the confidentiality and integrity

325
00:26:38,920 --> 00:26:40,400
of the computation.

326
00:26:40,400 --> 00:26:45,240
And the secure enclave also provides a capability

327
00:26:45,240 --> 00:26:50,360
called remote at a station, so that a remote verifier is able to

328
00:26:50,360 --> 00:26:52,240
remove the verifier.

329
00:26:52,240 --> 00:26:57,520
The right computation has happens on a particular piece of data,

330
00:26:57,520 --> 00:27:02,320
so essentially, it can verify the initial state of the secure enclave

331
00:27:02,320 --> 00:27:05,200
and the program that will be run in the secure enclave.

332
00:27:05,200 --> 00:27:08,160
So with this method, essentially, it's another way,

333
00:27:08,160 --> 00:27:12,280
another practical way to simulate this trusted third party,

334
00:27:12,280 --> 00:27:16,720
to ensure that when the program computes over the data,

335
00:27:16,720 --> 00:27:20,120
nothing else gets leaked.

336
00:27:20,120 --> 00:27:23,000
And there has been various commercial solutions,

337
00:27:23,000 --> 00:27:26,960
including Intel Asjacks and AMD,

338
00:27:26,960 --> 00:27:30,360
different hardware manufacturers have built

339
00:27:30,360 --> 00:27:34,520
their different types of solutions for us.

340
00:27:34,520 --> 00:27:37,280
But however, all these solutions are closed sourced,

341
00:27:37,280 --> 00:27:42,880
and there has been some security issues discovered,

342
00:27:42,880 --> 00:27:46,320
even though they have been patched and so on.

343
00:27:46,320 --> 00:27:48,760
But being a closed source, it's difficult

344
00:27:48,760 --> 00:27:53,520
for the community to really know what kind of security guarantees

345
00:27:53,520 --> 00:27:56,360
that this type of solution can provide.

346
00:27:56,360 --> 00:28:03,520
So as a research project, a Berkeley in my research group,

347
00:28:03,520 --> 00:28:05,560
with other colleagues and so on, we have

348
00:28:05,560 --> 00:28:10,400
been building what's called a keystone secure enclave.

349
00:28:10,400 --> 00:28:13,400
It's an open source secure enclave, essentially

350
00:28:13,400 --> 00:28:17,320
provides an open source version of this black box

351
00:28:17,320 --> 00:28:21,760
that helps you to do this type of secure computing.

352
00:28:21,760 --> 00:28:27,080
And it's built on top of risk 5 open source risk architecture.

353
00:28:27,080 --> 00:28:31,280
And we have demonstrated that you can actually

354
00:28:31,280 --> 00:28:36,920
build machine learning models inside the secure enclave

355
00:28:36,920 --> 00:28:43,160
that you can do inference and other types of computation.

356
00:28:43,160 --> 00:28:45,200
It can be really efficient.

357
00:28:45,200 --> 00:28:49,160
And so on. So in the future, also, I think

358
00:28:49,160 --> 00:28:53,560
there has been discussions, even like GPUs and CPUs

359
00:28:53,560 --> 00:28:58,880
in the future, there can be this notion of a secure enclave.

360
00:28:58,880 --> 00:29:02,960
So that the data is encrypted going into the chip

361
00:29:02,960 --> 00:29:06,720
and only is decrypted inside the chip

362
00:29:06,720 --> 00:29:10,200
and then you compute over the data essentially

363
00:29:10,200 --> 00:29:12,600
in this black box manner.

364
00:29:12,600 --> 00:29:15,720
When you talk about the risk architecture,

365
00:29:15,720 --> 00:29:18,360
is this something that you're able to build

366
00:29:18,360 --> 00:29:20,480
with off-the-shelf components?

367
00:29:20,480 --> 00:29:24,520
Or does it require custom hardware to implement?

368
00:29:24,520 --> 00:29:26,440
So right, so they came with risk 5.

369
00:29:26,440 --> 00:29:29,640
There are chips already off-the-shelf to enable you

370
00:29:29,640 --> 00:29:30,720
to do that.

371
00:29:30,720 --> 00:29:34,240
And right, I was just actually going to sound that.

372
00:29:34,240 --> 00:29:36,640
OK, cool, cool.

373
00:29:36,640 --> 00:29:42,080
You also are active in exploring different adversarial

374
00:29:42,080 --> 00:29:45,040
attacks on machine learning and that whole space

375
00:29:45,040 --> 00:29:47,360
of adversarial machine learning in general.

376
00:29:47,360 --> 00:29:50,760
Can you talk a little bit about some of your work in that area?

377
00:29:50,760 --> 00:29:52,520
Right, yes.

378
00:29:52,520 --> 00:29:56,800
Well, as we tried to deploy machine learning

379
00:29:56,800 --> 00:29:59,640
AI systems in the real world, one issue we've already

380
00:29:59,640 --> 00:30:03,040
discussed is in the privacy and responsible data

381
00:30:03,040 --> 00:30:05,680
used to ensure that the model actually does

382
00:30:05,680 --> 00:30:07,560
a leak-sensely-faint information about users

383
00:30:07,560 --> 00:30:12,120
and also is used in a way that's for users best interests.

384
00:30:12,120 --> 00:30:14,320
And then another important issue is

385
00:30:14,320 --> 00:30:18,720
to ensure that these models are actually not easily

386
00:30:18,720 --> 00:30:21,200
hacked by attackers.

387
00:30:21,200 --> 00:30:25,120
So that relates to the problem of adversarial machine learning

388
00:30:25,120 --> 00:30:27,640
where in adversarial machine learning,

389
00:30:27,640 --> 00:30:30,680
so one example we have said is essentially

390
00:30:30,680 --> 00:30:33,920
looking at how the attackers can actually

391
00:30:33,920 --> 00:30:37,760
feed, for example, modified inputs,

392
00:30:37,760 --> 00:30:39,520
like with adversarial perturbations that

393
00:30:39,520 --> 00:30:41,520
can fool the machine learning system

394
00:30:41,520 --> 00:30:45,160
to give the wrong answers, for example, the wrong predictions.

395
00:30:45,160 --> 00:30:47,640
And one example that we have stated

396
00:30:47,640 --> 00:30:52,240
is in the self-driving car setting,

397
00:30:52,240 --> 00:30:56,240
we're looking at also, well, there's some of these adversarial

398
00:30:56,240 --> 00:30:59,520
example attacks can even happen in the real world.

399
00:30:59,520 --> 00:31:02,160
It's like putting a picker on a stop sign and...

400
00:31:02,160 --> 00:31:04,240
Right, right, right, exactly.

401
00:31:04,240 --> 00:31:07,960
Right, demonstrating that this type of attacks

402
00:31:07,960 --> 00:31:10,600
can even happen in the real world where the attackers

403
00:31:10,600 --> 00:31:13,640
are putting just stickers on stop signs.

404
00:31:13,640 --> 00:31:16,600
It can, for humans, we can still recognize

405
00:31:16,600 --> 00:31:19,200
these stop signs with no problems.

406
00:31:19,200 --> 00:31:22,120
But for the image classification systems,

407
00:31:22,120 --> 00:31:23,920
or these computer vision systems,

408
00:31:23,920 --> 00:31:27,080
they can be very easily thought to give wrong answers

409
00:31:27,080 --> 00:31:30,560
and also to give the target answers that the attacker wants.

410
00:31:30,560 --> 00:31:34,360
And the important part is that this type of attack

411
00:31:34,360 --> 00:31:39,040
can even remain effective as, even from different viewing

412
00:31:39,040 --> 00:31:42,800
distances, different viewing angles, and so on.

413
00:31:42,800 --> 00:31:44,680
Right, so with my collaborators,

414
00:31:44,680 --> 00:31:49,560
we have demonstrated that this is feasible,

415
00:31:49,560 --> 00:31:52,120
and we develop these real world stop signs,

416
00:31:52,120 --> 00:31:53,840
archefic signs, and so on.

417
00:31:53,840 --> 00:31:58,120
And some of these artifacts actually have been

418
00:31:58,120 --> 00:32:03,120
exhibits at the Science Museum in London.

419
00:32:03,120 --> 00:32:06,720
So it's actually quite fun.

420
00:32:06,720 --> 00:32:10,880
Oh, well, to what degree are you seeing or hearing about

421
00:32:10,880 --> 00:32:15,960
kind of real world examples of these types of adversarial attacks?

422
00:32:15,960 --> 00:32:17,320
Are we there yet?

423
00:32:17,320 --> 00:32:22,240
Is it something that people are practically faced with

424
00:32:22,240 --> 00:32:24,000
and worried about today?

425
00:32:24,000 --> 00:32:25,200
That's a very good question.

426
00:32:25,200 --> 00:32:29,200
I think, definitely, we have seen

427
00:32:29,200 --> 00:32:33,760
the attackers try to fool machine learning systems.

428
00:32:33,760 --> 00:32:38,840
So in particular, for example, there are a lot of these clouds

429
00:32:38,840 --> 00:32:42,960
APIs for different machine learning services.

430
00:32:42,960 --> 00:32:47,560
For example, this cloud APIs may try to identify

431
00:32:47,560 --> 00:32:51,680
whether a certain content is deemed safe.

432
00:32:51,680 --> 00:32:56,640
For example, and it's actually very easy for attackers

433
00:32:56,640 --> 00:32:59,200
to write through these type of attacks

434
00:32:59,200 --> 00:33:02,080
to try to fool this type of cloud APIs.

435
00:33:02,080 --> 00:33:04,960
In our own work, we have demonstrated this as well.

436
00:33:04,960 --> 00:33:07,120
And we call this actually Black Box attacks.

437
00:33:07,120 --> 00:33:10,880
So in this case, we don't even need to know the details

438
00:33:10,880 --> 00:33:14,520
of the machine learning model of the cloud API,

439
00:33:14,520 --> 00:33:17,720
including the architecture or the parameters

440
00:33:17,720 --> 00:33:23,240
of these machine learning models, but through Black Box attacks.

441
00:33:23,240 --> 00:33:27,760
And they're from what's called transferability attacks

442
00:33:27,760 --> 00:33:32,520
where we can build a local model and then

443
00:33:32,520 --> 00:33:37,440
try to generate these adversarial examples by just attacking

444
00:33:37,440 --> 00:33:38,160
the local model.

445
00:33:38,160 --> 00:33:42,160
And then due to this transferability phenomena,

446
00:33:42,160 --> 00:33:44,840
they generated the adversary examples

447
00:33:44,840 --> 00:33:47,640
from the local model actually has high likelihood

448
00:33:47,640 --> 00:33:51,760
to actually be successful against the remote victim model

449
00:33:51,760 --> 00:33:52,480
as well.

450
00:33:52,480 --> 00:33:54,760
And we demonstrated that this type of attacks

451
00:33:54,760 --> 00:33:57,680
can be effective for this cloud APIs.

452
00:33:57,680 --> 00:33:59,560
On the work that you were just mentioning

453
00:33:59,560 --> 00:34:04,880
is the ability to generate these effective local models

454
00:34:04,880 --> 00:34:09,480
incorporating some of the parameter and architecture

455
00:34:09,480 --> 00:34:12,880
leakage that we've talked about previously?

456
00:34:12,880 --> 00:34:15,400
So in this case, we assume that we actually

457
00:34:15,400 --> 00:34:19,040
don't really know anything about the remote model,

458
00:34:19,040 --> 00:34:21,080
like what actually, what parameters

459
00:34:21,080 --> 00:34:24,880
it actually uses.

460
00:34:24,880 --> 00:34:27,240
So we just built a separate local model.

461
00:34:27,240 --> 00:34:28,920
And because of this transferability,

462
00:34:28,920 --> 00:34:31,640
the attacks that we found, this local model

463
00:34:31,640 --> 00:34:37,680
has high likelihood to actually succeed on the remote model.

464
00:34:37,680 --> 00:34:40,120
And also, so in the past, we've done work

465
00:34:40,120 --> 00:34:43,600
and also the community has done work studying

466
00:34:43,600 --> 00:34:47,560
in the computer vision fields for this type of attack.

467
00:34:47,560 --> 00:34:53,200
And recently, we've also studied in the natural language

468
00:34:53,200 --> 00:34:54,160
space.

469
00:34:54,160 --> 00:34:57,800
So in this particular case, for machine translation.

470
00:34:57,800 --> 00:35:01,560
So we actually looked at, for example, Google Translate

471
00:35:01,560 --> 00:35:06,440
and a few other these cloud APIs for machine translation.

472
00:35:06,440 --> 00:35:08,600
So first, we showed that by just creating

473
00:35:08,600 --> 00:35:11,520
this cloud APIs for machine translation,

474
00:35:11,520 --> 00:35:15,920
you can call it an imitation attack or model steaming attack.

475
00:35:15,920 --> 00:35:19,200
We can actually build a local model that

476
00:35:19,200 --> 00:35:24,080
has very high essentially performance

477
00:35:24,080 --> 00:35:27,240
that close to this cloud APIs when you evaluate it

478
00:35:27,240 --> 00:35:29,440
over standard benchmark.

479
00:35:29,440 --> 00:35:31,920
So by creating these cloud APIs, we

480
00:35:31,920 --> 00:35:35,680
are able to build these imitation models locally.

481
00:35:35,680 --> 00:35:41,120
And then by developing attacks, these adversarial attacks

482
00:35:41,120 --> 00:35:44,800
on this local imitation model, we

483
00:35:44,800 --> 00:35:48,000
are able to generate these adversarial examples.

484
00:35:48,000 --> 00:35:51,800
As a simple example, we show that if we try to translate,

485
00:35:51,800 --> 00:35:54,320
for example, from English to German,

486
00:35:54,320 --> 00:35:56,520
the English sentence says, as I said,

487
00:35:56,520 --> 00:36:00,040
today is the temperature is very high.

488
00:36:00,040 --> 00:36:05,480
In this case, the language model generates the correct translation.

489
00:36:05,480 --> 00:36:10,000
But we are able to show that by finding essentially,

490
00:36:10,000 --> 00:36:12,080
we are able to find the attack.

491
00:36:12,080 --> 00:36:15,200
In this case, if we just change six Fahrenheit

492
00:36:15,200 --> 00:36:18,560
to seven Fahrenheit, just changing the number.

493
00:36:18,560 --> 00:36:20,280
And otherwise, it's the same sentence.

494
00:36:20,280 --> 00:36:23,240
And then one way to give the sentence to the language model

495
00:36:23,240 --> 00:36:26,480
actually translates into the temperature

496
00:36:26,480 --> 00:36:29,560
is 21 Celsius, for example.

497
00:36:29,560 --> 00:36:34,360
So it gives the wrong translation.

498
00:36:34,360 --> 00:36:37,760
And in this case, we only change the one

499
00:36:37,760 --> 00:36:39,840
character in the original sentence

500
00:36:39,840 --> 00:36:44,200
and show that the result in translation is very different.

501
00:36:44,200 --> 00:36:46,160
And this is just one of many examples

502
00:36:46,160 --> 00:36:50,560
which shows how using different essentially

503
00:36:50,560 --> 00:36:53,840
last functions using different methods,

504
00:36:53,840 --> 00:36:56,720
we can generate different types of attacks

505
00:36:56,720 --> 00:36:59,480
to try to fool the translation model.

506
00:36:59,480 --> 00:37:03,080
And also, when we transfer this attack

507
00:37:03,080 --> 00:37:06,920
to the remote model, for example, to Google Translate

508
00:37:06,920 --> 00:37:12,920
and to other types of APIs, we show

509
00:37:12,920 --> 00:37:17,240
that these attacks can still work.

510
00:37:17,240 --> 00:37:22,000
So these are other examples demonstrating

511
00:37:22,000 --> 00:37:25,760
that when we deploy machine learning systems

512
00:37:25,760 --> 00:37:27,840
in the real world, it's really important

513
00:37:27,840 --> 00:37:30,040
to think about these types of issues as well.

514
00:37:30,040 --> 00:37:31,960
And we call them the integrity attacks,

515
00:37:31,960 --> 00:37:35,800
essentially trying to see how attackers may

516
00:37:35,800 --> 00:37:39,520
fool the machine learning systems to try

517
00:37:39,520 --> 00:37:42,920
to have the machine learning systems to give the wrong answers.

518
00:37:42,920 --> 00:37:47,360
And as we know in the future, more and more critical decisions

519
00:37:47,360 --> 00:37:50,880
will be made by these learning systems

520
00:37:50,880 --> 00:37:56,120
autonomously, almost potentially every minute,

521
00:37:56,120 --> 00:38:00,320
we really need to be very, very careful

522
00:38:00,320 --> 00:38:03,320
about ensuring that these machine learning systems

523
00:38:03,320 --> 00:38:08,520
cannot be easily attacked, they do give the rights,

524
00:38:08,520 --> 00:38:14,040
and so they have high security assurance and so on.

525
00:38:14,040 --> 00:38:17,120
You also have done some interesting work

526
00:38:17,120 --> 00:38:20,600
on the topic of program synthesis,

527
00:38:20,600 --> 00:38:24,320
so trying to use machine learning models

528
00:38:24,320 --> 00:38:26,320
to generate computer programs.

529
00:38:26,320 --> 00:38:29,560
Can you talk a little bit about your work in that area?

530
00:38:29,560 --> 00:38:32,520
Yes, I think the reason I'm working in program synthesis

531
00:38:32,520 --> 00:38:34,480
is because I think program synthesis

532
00:38:34,480 --> 00:38:38,640
is the ideal playgrounds for trying

533
00:38:38,640 --> 00:38:45,160
to build what we call agatic artificial general intelligence.

534
00:38:45,160 --> 00:38:47,960
I think it's really the ultimate task.

535
00:38:47,960 --> 00:38:49,520
Talking about building intelligent machines,

536
00:38:49,520 --> 00:38:51,920
ideally you want this intelligent machine

537
00:38:51,920 --> 00:38:54,920
to actually be able to do the program,

538
00:38:54,920 --> 00:38:57,600
and hence it's actually able to, in the future,

539
00:38:57,600 --> 00:39:00,440
maybe even build itself and so on

540
00:39:00,440 --> 00:39:06,440
to help building programs to solve many real world problems.

541
00:39:06,440 --> 00:39:11,040
Also, I joke with my colleagues in robotics

542
00:39:11,040 --> 00:39:14,720
is that program synthesis is like doing robotics,

543
00:39:14,720 --> 00:39:19,720
but without the physics constraints of nature

544
00:39:20,760 --> 00:39:25,200
without having to obey laws of physics.

545
00:39:25,200 --> 00:39:28,320
So in program synthesis, essentially, you need to solve

546
00:39:28,320 --> 00:39:30,280
many of the similar problems.

547
00:39:30,280 --> 00:39:33,040
You need to understand goals.

548
00:39:33,040 --> 00:39:36,280
You need to be able to decompose problems.

549
00:39:36,280 --> 00:39:38,240
Into some problems, you need to be able

550
00:39:38,240 --> 00:39:40,240
to do very effective search.

551
00:39:40,240 --> 00:39:45,040
The search space of programs is huge.

552
00:39:45,040 --> 00:39:47,040
As we all know, you know,

553
00:39:47,040 --> 00:39:50,280
comparing to playing Go, playing Starcraft

554
00:39:50,280 --> 00:39:53,280
and so on, the search space of programs

555
00:39:53,280 --> 00:39:58,280
is even larger, even for small, simple programs and so on.

556
00:39:59,920 --> 00:40:04,160
So you need to do planning and also,

557
00:40:04,160 --> 00:40:06,360
you need to better understand semantics

558
00:40:06,360 --> 00:40:09,320
to know what you want the programs to do and so on.

559
00:40:09,320 --> 00:40:12,960
So essentially, all the problems that you need to solve

560
00:40:12,960 --> 00:40:15,560
in robotics and also just in general building

561
00:40:15,560 --> 00:40:20,560
education machines, they all fight in program synthesis.

562
00:40:20,560 --> 00:40:22,960
You need to really solve program synthesis.

563
00:40:22,960 --> 00:40:25,680
You really need to solve all these problems.

564
00:40:25,680 --> 00:40:28,240
So it's a really exciting domain,

565
00:40:28,240 --> 00:40:30,280
but also at the same time, it's a domain

566
00:40:30,280 --> 00:40:32,560
that you can experiment much more easily.

567
00:40:32,560 --> 00:40:36,400
You don't need to build robots, you don't, right?

568
00:40:36,400 --> 00:40:38,280
And what you need to do, like what we did,

569
00:40:38,280 --> 00:40:41,560
is oftentimes you just build these synthetic,

570
00:40:41,560 --> 00:40:43,160
you can build synthetic environments.

571
00:40:43,160 --> 00:40:48,160
You can easily also build this program, you know,

572
00:40:48,160 --> 00:40:50,840
analytics or even just execute the program

573
00:40:50,840 --> 00:40:52,760
to know exactly what it does.

574
00:40:52,760 --> 00:40:55,440
And so also from in terms of evaluation,

575
00:40:55,440 --> 00:40:58,720
the experiments, it's much, much easier.

576
00:40:58,720 --> 00:41:01,960
So in the program synthesis space,

577
00:41:01,960 --> 00:41:04,560
in particular program synthesis by Lenny,

578
00:41:04,560 --> 00:41:07,520
it's still a nascent field.

579
00:41:07,520 --> 00:41:10,760
I remember a few years ago,

580
00:41:10,760 --> 00:41:12,600
when we started working in it,

581
00:41:12,600 --> 00:41:14,800
there were very few people actually working in a space.

582
00:41:14,800 --> 00:41:18,680
And then when you look at new ribs,

583
00:41:18,680 --> 00:41:21,480
I clear, I say now, these conferences,

584
00:41:21,480 --> 00:41:23,880
there were very few papers actually

585
00:41:23,880 --> 00:41:26,360
doing program synthesis by Lenny.

586
00:41:26,360 --> 00:41:28,160
But the way you look at the most recent,

587
00:41:28,160 --> 00:41:30,960
for example, new ribs conference and so on.

588
00:41:30,960 --> 00:41:34,720
And I clear conference, you actually see,

589
00:41:34,720 --> 00:41:36,280
now there are specific sessions,

590
00:41:36,280 --> 00:41:40,560
even dedicated to a program synthesis by Lenny.

591
00:41:40,560 --> 00:41:45,040
I think it is a great progress for the community.

592
00:41:45,040 --> 00:41:49,640
But still, we are at a very early age.

593
00:41:49,640 --> 00:41:53,440
The program that in general, the community can synthesize

594
00:41:53,440 --> 00:41:55,240
is still very small.

595
00:41:55,240 --> 00:42:00,240
And in general, typically we focus on the program synthesis

596
00:42:00,720 --> 00:42:03,240
for certain vertical domains,

597
00:42:03,240 --> 00:42:06,040
is there to make progress that way.

598
00:42:06,040 --> 00:42:08,960
And I think also program synthesis can have,

599
00:42:08,960 --> 00:42:12,600
already can have a huge impact in the real world.

600
00:42:12,600 --> 00:42:16,360
So for example, some of the tasks that we have done,

601
00:42:16,360 --> 00:42:18,720
and translating natural language

602
00:42:18,720 --> 00:42:20,720
into simple programs.

603
00:42:20,720 --> 00:42:23,080
And so this includes translating natural language

604
00:42:23,080 --> 00:42:26,640
into if this and that's type of programs.

605
00:42:26,640 --> 00:42:29,200
For example, if it rains tomorrow,

606
00:42:29,200 --> 00:42:31,960
so any text message,

607
00:42:31,960 --> 00:42:34,480
or translating natural language description

608
00:42:34,480 --> 00:42:36,200
into SQL queries.

609
00:42:36,200 --> 00:42:41,200
So this can enable more people who cannot code

610
00:42:41,440 --> 00:42:42,960
to be able to utilize data.

611
00:42:42,960 --> 00:42:45,840
So for example, with a huge database,

612
00:42:45,840 --> 00:42:47,560
people who cannot code,

613
00:42:47,560 --> 00:42:49,440
who cannot write SQL queries,

614
00:42:49,440 --> 00:42:50,640
they may have a lot of questions

615
00:42:50,640 --> 00:42:54,760
that they want to get answers from these large database.

616
00:42:54,760 --> 00:42:59,240
And when we enable this natural language description,

617
00:42:59,240 --> 00:43:02,120
get translated into SQL queries directly,

618
00:43:02,120 --> 00:43:05,800
we can really enable more people to benefit from data,

619
00:43:05,800 --> 00:43:08,280
you know, talking about responsible data,

620
00:43:08,280 --> 00:43:10,960
responsible data economy, and so on.

621
00:43:10,960 --> 00:43:13,720
So I think it's, right,

622
00:43:13,720 --> 00:43:16,520
it's a really exciting domain.

623
00:43:16,520 --> 00:43:21,760
And deep learning has really been hugely helpful in the space.

624
00:43:21,760 --> 00:43:25,360
It's, well, we first started working in the space.

625
00:43:25,360 --> 00:43:29,360
Who was your first paper in program synthesis?

626
00:43:29,360 --> 00:43:31,760
So our first paper in program synthesis

627
00:43:31,760 --> 00:43:35,240
by learning is actually using deep learning

628
00:43:35,240 --> 00:43:38,360
to enable natural language description

629
00:43:38,360 --> 00:43:40,640
to be translated in this,

630
00:43:40,640 --> 00:43:43,040
this then that's called ifTT programs.

631
00:43:43,040 --> 00:43:43,880
Okay.

632
00:43:43,880 --> 00:43:48,880
And at the time, also there has been some other approaches

633
00:43:48,880 --> 00:43:53,080
using more traditional natural language approaches,

634
00:43:53,080 --> 00:43:54,480
like semantic pricing, and so on,

635
00:43:54,480 --> 00:43:56,760
to try to address a problem.

636
00:43:56,760 --> 00:43:58,960
And we were the first one to actually demonstrate

637
00:43:58,960 --> 00:44:00,000
that using deep learning,

638
00:44:00,000 --> 00:44:02,320
you can actually get much better results.

639
00:44:02,320 --> 00:44:05,160
And we were able to get state-of-the-art results

640
00:44:05,160 --> 00:44:07,320
using deep learning, and so on.

641
00:44:07,320 --> 00:44:10,800
And then since then, we have explores, as I mentioned,

642
00:44:10,800 --> 00:44:13,760
translating natural language description into SQL queries

643
00:44:13,760 --> 00:44:18,200
and also building even like language translated,

644
00:44:18,200 --> 00:44:21,240
translating programs written in one program in language

645
00:44:21,240 --> 00:44:26,240
into another, and many other application domains and so on.

646
00:44:26,680 --> 00:44:31,360
And also trying to develop a method that's essentially

647
00:44:31,360 --> 00:44:35,800
make it the learning process more sophisticated

648
00:44:35,800 --> 00:44:39,760
to be able to leverage more about execution semantics

649
00:44:39,760 --> 00:44:44,760
and also better learning from its past mistakes.

650
00:44:44,840 --> 00:44:47,760
So many interesting techniques in the space.

651
00:44:47,760 --> 00:44:51,600
I think the field overall has really flourished

652
00:44:51,600 --> 00:44:54,080
over the last few years.

653
00:44:54,080 --> 00:44:56,800
I imagine you've seen this demo, again,

654
00:44:56,800 --> 00:44:59,200
referencing back to GPT-3.

655
00:44:59,200 --> 00:45:02,680
There's one demo that's kind of making the rounds

656
00:45:02,680 --> 00:45:05,720
of someone who built a web layout generator

657
00:45:05,720 --> 00:45:08,960
that spits out JSX code, and you can tell it,

658
00:45:08,960 --> 00:45:11,280
make me a web page with a red button, a green button,

659
00:45:11,280 --> 00:45:15,360
and a blue button, and it creates the code to do that.

660
00:45:15,360 --> 00:45:17,040
Have you seen that one?

661
00:45:17,040 --> 00:45:21,320
I mean, now the particular, but I see similar applications.

662
00:45:21,320 --> 00:45:26,040
And in the past, actually, we also tried to do

663
00:45:26,040 --> 00:45:28,800
some of this type of applications as well.

664
00:45:28,800 --> 00:45:32,360
I think, yes, you can do that.

665
00:45:32,360 --> 00:45:35,440
Gigantic language models will play a huge role

666
00:45:35,440 --> 00:45:37,720
in the field of program synthesis,

667
00:45:37,720 --> 00:45:42,280
so do you think it's going to be maybe more constrained

668
00:45:42,280 --> 00:45:46,400
techniques that allow us to make big progress there?

669
00:45:46,400 --> 00:45:47,400
That's a very good question.

670
00:45:47,400 --> 00:45:49,960
I think in general for program synthesis to really work,

671
00:45:49,960 --> 00:45:54,000
we need different components, different types of techniques

672
00:45:54,000 --> 00:45:54,840
and so on.

673
00:45:54,840 --> 00:45:56,680
So certainly, for this type of language models,

674
00:45:56,680 --> 00:46:01,200
we know certain network architectures, like a transformer

675
00:46:01,200 --> 00:46:03,440
and so on, they just have been so powerful

676
00:46:03,440 --> 00:46:07,080
they can solve so many different types of problems

677
00:46:07,080 --> 00:46:09,680
in so many different types of applications and so on.

678
00:46:09,680 --> 00:46:12,680
So I think it's different types of network architectures

679
00:46:12,680 --> 00:46:17,680
and certain components are definitely very, very helpful.

680
00:46:18,720 --> 00:46:21,280
But on the other hand, I think with program synthesis,

681
00:46:21,280 --> 00:46:25,760
there's also another reason that I really like the domain

682
00:46:25,760 --> 00:46:29,760
is that for solving problems in this area,

683
00:46:29,760 --> 00:46:31,560
you also really need to understand

684
00:46:31,560 --> 00:46:35,280
semantics in a much deeper level to write,

685
00:46:35,280 --> 00:46:37,640
to know what you want the program to do

686
00:46:37,640 --> 00:46:40,320
and hence what type of programs you should generate.

687
00:46:40,320 --> 00:46:43,480
I don't think just having a transformer itself

688
00:46:43,480 --> 00:46:44,920
can help us solve program synthesis,

689
00:46:44,920 --> 00:46:47,560
but definitely, I think it's an important component.

690
00:46:47,560 --> 00:46:50,600
And that's also why it's a really exciting field

691
00:46:50,600 --> 00:46:53,720
we need to, for the explore and develop

692
00:46:53,720 --> 00:46:57,080
other types of techniques and approaches.

693
00:46:57,080 --> 00:47:01,680
As I mentioned, trying to learn more about the semantics

694
00:47:01,680 --> 00:47:05,320
and also trying to learn from past mistakes,

695
00:47:05,320 --> 00:47:08,840
also trying to understand if the program

696
00:47:08,840 --> 00:47:10,840
that you've generated is now the right one,

697
00:47:10,840 --> 00:47:13,400
you try to identify the type of issues

698
00:47:13,400 --> 00:47:15,480
that it's why it's not working

699
00:47:15,480 --> 00:47:17,480
and try to learn how to fix it,

700
00:47:17,480 --> 00:47:19,720
very much like a how humans program,

701
00:47:19,720 --> 00:47:23,840
how we can crack sense, right, like the programs and so on.

702
00:47:23,840 --> 00:47:27,520
So we hope that the tools for program synthesis

703
00:47:27,520 --> 00:47:32,520
can leverage many of these types of capabilities.

704
00:47:33,720 --> 00:47:36,960
You've also done a bit of work on contact tracing

705
00:47:36,960 --> 00:47:40,960
as it applies to coronavirus and COVID-19.

706
00:47:40,960 --> 00:47:43,960
Can you share a little bit about what you've done in that area?

707
00:47:43,960 --> 00:47:47,920
That's another concrete example

708
00:47:47,920 --> 00:47:50,880
of what we call responsible data use.

709
00:47:50,880 --> 00:47:51,960
So in particular,

710
00:47:51,960 --> 00:47:53,840
you focused on the privacy side there?

711
00:47:53,840 --> 00:47:55,240
Right, right, right, exactly.

712
00:47:55,240 --> 00:47:58,400
As we know, with this pandemic,

713
00:47:58,400 --> 00:48:01,120
it's changed the world.

714
00:48:03,120 --> 00:48:06,880
Nobody has, I think, really, really expected

715
00:48:06,880 --> 00:48:09,400
that it can change the world so fast

716
00:48:09,400 --> 00:48:12,840
and as such a global scale and so on.

717
00:48:12,840 --> 00:48:15,520
And hence, of course, finding solutions

718
00:48:15,520 --> 00:48:18,080
to fight the pandemic is really important.

719
00:48:18,080 --> 00:48:22,120
And of course, data is the key driver

720
00:48:22,120 --> 00:48:23,840
in fighting the pandemic.

721
00:48:23,840 --> 00:48:26,080
Then the question is, as we use data

722
00:48:26,080 --> 00:48:27,960
to fight the pandemic,

723
00:48:27,960 --> 00:48:31,120
how we also at the same time need to ensure

724
00:48:31,120 --> 00:48:34,280
that the data is being used in a privacy-preserving way

725
00:48:34,280 --> 00:48:37,560
and used in a responsible way.

726
00:48:37,560 --> 00:48:42,560
Because otherwise, you can really have huge compromise

727
00:48:43,360 --> 00:48:44,760
of individuals privacy,

728
00:48:44,760 --> 00:48:49,760
which is really sensitive for individuals and so on.

729
00:48:50,120 --> 00:48:52,280
And also, if users are concerned

730
00:48:52,280 --> 00:48:54,080
about their privacy,

731
00:48:54,080 --> 00:48:56,760
this will also limit their participation

732
00:48:56,760 --> 00:48:59,880
in whatever apps that you try to have users to use

733
00:48:59,880 --> 00:49:03,080
or in whatever systems that you are trying to deploy.

734
00:49:03,080 --> 00:49:07,440
And hence, as a concrete example and application

735
00:49:07,440 --> 00:49:10,760
of this approach of building a platform

736
00:49:10,760 --> 00:49:13,440
for a responsible data economy,

737
00:49:13,440 --> 00:49:18,280
we are investigating how we can utilize data

738
00:49:18,280 --> 00:49:20,560
in a more responsible way.

739
00:49:20,560 --> 00:49:24,400
And at the same time to enable, for example,

740
00:49:24,400 --> 00:49:27,880
fasts and effective contact tracing.

741
00:49:27,880 --> 00:49:32,680
So we have been exploring different approaches

742
00:49:32,680 --> 00:49:34,040
and developing different techniques,

743
00:49:34,040 --> 00:49:39,040
including improvements on cryptographic protocols

744
00:49:39,080 --> 00:49:43,160
for more privacy-preserving contact tracing.

745
00:49:43,160 --> 00:49:46,120
That's extension to the Google Apple Exposure Notification

746
00:49:46,120 --> 00:49:49,720
Protocol to provide more privacy protection

747
00:49:49,720 --> 00:49:54,720
as well as enabling what we call a secure distributed

748
00:49:55,480 --> 00:50:00,160
computing fabric to enable this type of contact tracing

749
00:50:00,160 --> 00:50:03,360
work to be done over different data sources,

750
00:50:03,360 --> 00:50:05,320
but not in a centralized manner,

751
00:50:05,320 --> 00:50:08,680
in a more distributed and decentralized manner.

752
00:50:08,680 --> 00:50:13,040
And also, I wanted to mention that, of course,

753
00:50:13,040 --> 00:50:15,680
this is really important and time-tapping.

754
00:50:15,680 --> 00:50:20,440
We are actually launching summits called

755
00:50:20,440 --> 00:50:22,400
responsible data summits.

756
00:50:22,400 --> 00:50:27,240
And we dedicate one day just on responsible data

757
00:50:27,240 --> 00:50:32,240
in the time of pandemic to discuss the various issues

758
00:50:32,440 --> 00:50:36,680
about how we can use data in a more responsible way

759
00:50:36,680 --> 00:50:41,400
while providing effective solutions for fighting pandemic.

760
00:50:41,400 --> 00:50:46,400
And also another day talking about the cutting edge technologies

761
00:50:47,160 --> 00:50:52,000
and also the latest thinking in legal frameworks

762
00:50:52,000 --> 00:50:56,680
for responsible data technology and policy in the real world.

763
00:50:56,680 --> 00:51:01,360
And I think that that can provide more exciting details

764
00:51:01,360 --> 00:51:03,720
for people who are interested.

765
00:51:03,720 --> 00:51:06,920
And responsible data.AI.

766
00:51:06,920 --> 00:51:07,760
Right, great.

767
00:51:07,760 --> 00:51:10,360
And will the, I believe this will be published

768
00:51:10,360 --> 00:51:12,320
after that conference,

769
00:51:12,320 --> 00:51:15,520
but will the videos be available for folks to check out?

770
00:51:15,520 --> 00:51:17,720
Yes, the videos will be online.

771
00:51:17,720 --> 00:51:20,840
And we have a great line of speakers,

772
00:51:20,840 --> 00:51:24,240
including Yushua Benjou, actually we'll talk about how

773
00:51:24,240 --> 00:51:29,240
he uses data to fight pandemic in contact tracing

774
00:51:31,560 --> 00:51:35,160
and in the privacy preserving a responsible way as well,

775
00:51:35,160 --> 00:51:37,800
as well as many other great speakers.

776
00:51:37,800 --> 00:51:39,080
Awesome, awesome.

777
00:51:39,080 --> 00:51:41,560
Well done, thanks so much for taking the time

778
00:51:41,560 --> 00:51:44,800
to share with us a bit about what you're up to.

779
00:51:44,800 --> 00:51:46,440
Great, thanks a lot for having me.

780
00:51:46,440 --> 00:51:47,280
Thank you.

781
00:51:47,280 --> 00:51:48,280
Thanks, thanks a lot.

782
00:51:51,280 --> 00:51:54,400
All right, everyone, that's our show for today.

783
00:51:54,400 --> 00:51:55,960
To learn more about today's guest

784
00:51:55,960 --> 00:51:58,040
or the topics mentioned in this interview,

785
00:51:58,040 --> 00:52:00,360
visit twimmelai.com.

786
00:52:00,360 --> 00:52:03,120
Of course, if you like what you hear on the podcast,

787
00:52:03,120 --> 00:52:06,000
please subscribe, rate, and review the show

788
00:52:06,000 --> 00:52:08,160
on your favorite pod catcher.

789
00:52:08,160 --> 00:52:11,200
Thanks so much for listening and catch you next time.

