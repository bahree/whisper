1
00:00:00,000 --> 00:00:16,320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

2
00:00:16,320 --> 00:00:21,480
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:21,480 --> 00:00:31,720
I'm your host Sam Charrington.

4
00:00:31,720 --> 00:00:36,520
If you're a Twimble listener, you probably have an opinion about AI.

5
00:00:36,520 --> 00:00:39,760
Are you looking forward to the role AI plays in your life?

6
00:00:39,760 --> 00:00:43,040
Are you fearful of the changes AI will bring?

7
00:00:43,040 --> 00:00:47,400
Or maybe you're just skeptical and don't think AI will make a real difference in the near

8
00:00:47,400 --> 00:00:48,400
future.

9
00:00:48,400 --> 00:00:51,040
Whatever the case, we want to hear your take.

10
00:00:51,040 --> 00:01:00,200
So hit pause right now and jump on over to twimblei.com slash my AI to let us know what you think.

11
00:01:00,200 --> 00:01:05,400
Sharing your thoughts takes two minutes and qualifies you to win some great prizes.

12
00:01:05,400 --> 00:01:09,840
Before we proceed, I want to give a quick shout out to everyone who has submitted a video

13
00:01:09,840 --> 00:01:20,880
so far, so here's to you, Krishnan, Amara, Keith, Shriram, Sharon, Rob, Julian and Chandana.

14
00:01:20,880 --> 00:01:26,680
In today's episode, I'm joined by Roland Mimi Shavitch, co-founder, CEO and chief scientist

15
00:01:26,680 --> 00:01:29,480
at 20 billion neurons.

16
00:01:29,480 --> 00:01:33,960
Roland joined me at the rework deep learning summit in Montreal to discuss the work his

17
00:01:33,960 --> 00:01:40,680
company is doing to train deep neural networks to understand physical actions.

18
00:01:40,680 --> 00:01:46,200
In our conversation, we dig into video analysis and understanding, including how data-rich

19
00:01:46,200 --> 00:01:53,400
video can help us develop what Roland calls comparative understanding or AI common sense.

20
00:01:53,400 --> 00:01:58,480
We touch on the implications of AI systems having comparative understanding and how Roland

21
00:01:58,480 --> 00:02:02,840
and his team are addressing problems like getting properly labeled training data.

22
00:02:02,840 --> 00:02:03,840
All right.

23
00:02:03,840 --> 00:02:04,840
Let's go.

24
00:02:04,840 --> 00:02:08,680
All right, everyone.

25
00:02:08,680 --> 00:02:15,640
I am here at the rework deep learning conference in Montreal and I am with Roland Mimi Shavitch.

26
00:02:15,640 --> 00:02:21,680
Roland is a founder and chief scientist at 20 billion neurons, a company that's based

27
00:02:21,680 --> 00:02:25,680
in Toronto and Berlin and is doing some pretty interesting things.

28
00:02:25,680 --> 00:02:27,840
Roland, welcome to this week in machine learning and AI.

29
00:02:27,840 --> 00:02:29,680
Well, thanks very much for having me.

30
00:02:29,680 --> 00:02:31,960
It's great to meet you.

31
00:02:31,960 --> 00:02:34,960
Why don't we get started by having you tell us a little bit about your background and

32
00:02:34,960 --> 00:02:38,720
how you got interested in the field?

33
00:02:38,720 --> 00:02:44,120
I was interested, I have been interested in AI throughout my career over the last, I would

34
00:02:44,120 --> 00:02:46,160
say almost 20 years.

35
00:02:46,160 --> 00:02:52,200
I was a student in Germany, I'm based from Germany and the neural network research there as

36
00:02:52,200 --> 00:02:54,280
a master student.

37
00:02:54,280 --> 00:03:01,040
And then since the space was fairly sparse in terms of supervision and so on at that

38
00:03:01,040 --> 00:03:08,200
time I decided to go to Toronto to work with Jeff Hinton as an advisor to do a PhD in

39
00:03:08,200 --> 00:03:09,200
2003.

40
00:03:09,200 --> 00:03:10,680
Not a bad choice.

41
00:03:10,680 --> 00:03:11,680
That's true.

42
00:03:11,680 --> 00:03:12,680
Yes.

43
00:03:12,680 --> 00:03:20,040
And it has been a fantastic time and, unsurprisingly, I learned a lot and enjoyed my time very much

44
00:03:20,040 --> 00:03:21,280
in Toronto.

45
00:03:21,280 --> 00:03:26,120
I also like the city incidentally very much and I'm back now in Toronto, partly for that

46
00:03:26,120 --> 00:03:27,120
reason.

47
00:03:27,120 --> 00:03:33,560
And so I spent a few years there doing research on neural networks with a very strong interest

48
00:03:33,560 --> 00:03:36,920
in video understanding specifically.

49
00:03:36,920 --> 00:03:45,400
And like in all the sub areas in deep learning and AI, in video understanding we also experienced

50
00:03:45,400 --> 00:03:54,080
some nice advances and you know a cute, interesting results here and there but not the amazing

51
00:03:54,080 --> 00:03:59,640
breakthrough that we all felt had to be imminent somehow but just never really happened.

52
00:03:59,640 --> 00:04:04,880
And so then I moved around a little bit, I was an assistant professor in Germany for

53
00:04:04,880 --> 00:04:09,960
some time and then at the University of Montreal for the last couple of years.

54
00:04:09,960 --> 00:04:17,240
And then image that happened along the way in 2012 and we realized that the same thing

55
00:04:17,240 --> 00:04:23,840
has to happen for video because there was just no fundamental reason why it wouldn't.

56
00:04:23,840 --> 00:04:31,000
And so we started myself and some from our friends and colleagues from Germany started

57
00:04:31,000 --> 00:04:37,560
to kind of think a little bit about the possibility of just launching a company that would combine

58
00:04:37,560 --> 00:04:44,480
research, perspective and leadership from my side primarily and the ability to build

59
00:04:44,480 --> 00:04:48,800
a company with a strong culture and so on from their side.

60
00:04:48,800 --> 00:04:51,840
So all of these have been colleagues who had been in machine learning for many years

61
00:04:51,840 --> 00:04:55,120
but had successfully built companies and so on.

62
00:04:55,120 --> 00:05:04,520
And we felt like it was the right moment to come together, address the problems head on

63
00:05:04,520 --> 00:05:08,720
by you know building the right kind of engineering environment and operation that would solve

64
00:05:08,720 --> 00:05:14,440
data problems specifically and big engineering problems and see how far we can get.

65
00:05:14,440 --> 00:05:23,720
And so we got together about a year and a half ago and established a lab in Toronto where

66
00:05:23,720 --> 00:05:29,680
there's still a lot of talent in Canada specifically as well as a lab in Berlin where my friends

67
00:05:29,680 --> 00:05:32,200
from there are based.

68
00:05:32,200 --> 00:05:37,400
The company is called 20 billion neurons and we have completely focused on video analysis

69
00:05:37,400 --> 00:05:42,640
and video understanding and have met quite some progress there that I'd be happy to chat

70
00:05:42,640 --> 00:05:45,040
about more in a few moments.

71
00:05:45,040 --> 00:05:50,240
So as part of this I stepped away from my role as an assistant professor at University

72
00:05:50,240 --> 00:05:58,960
of Montreal to dedicate my work full time to this effort because I feel like we are really

73
00:05:58,960 --> 00:06:05,560
really onto something and it takes basically all of your energy to push that through obviously.

74
00:06:05,560 --> 00:06:06,560
Right.

75
00:06:06,560 --> 00:06:07,560
Right.

76
00:06:07,560 --> 00:06:15,160
So you started out with this kind of idea for a grand challenge accumulating a video

77
00:06:15,160 --> 00:06:18,320
data set analogous to ImageNet.

78
00:06:18,320 --> 00:06:19,320
Right.

79
00:06:19,320 --> 00:06:25,360
ImageNet is a huge undertaking you know just for images I can hardly imagine something

80
00:06:25,360 --> 00:06:30,720
of the same scale and impact on the video side what must be involved in doing that like

81
00:06:30,720 --> 00:06:31,720
where are we.

82
00:06:31,720 --> 00:06:36,000
Yeah that's a very good point and it's actually the analogy is not perfect because videos

83
00:06:36,000 --> 00:06:39,320
are a different piece than images.

84
00:06:39,320 --> 00:06:43,720
On various levels obviously the data amount is just ridiculously large back on various

85
00:06:43,720 --> 00:06:48,600
images because there's time and you have like a factor 100 just like there that's going

86
00:06:48,600 --> 00:06:53,840
to increase the amount of pixels that you have to look at basically but then also what

87
00:06:53,840 --> 00:07:00,720
video is really representing and what it's what it's about videos are not about showing

88
00:07:00,720 --> 00:07:07,080
an object you know and the task usually associated with video is not figuring out that there

89
00:07:07,080 --> 00:07:13,360
is an orange in the field of view or a dog or a human or something but it's much more

90
00:07:13,360 --> 00:07:20,640
about verbs, subtle interactions the way people behave in it, relationships between objects

91
00:07:20,640 --> 00:07:24,360
over time how they change and so on and so forth.

92
00:07:24,360 --> 00:07:30,400
So video is really quite a different thing in its own it's own that is though at the

93
00:07:30,400 --> 00:07:36,680
same time a challenge but also an amazing opportunity.

94
00:07:36,680 --> 00:07:41,560
The one of the reasons I would say that actually the reason why I've been drawn to video throughout

95
00:07:41,560 --> 00:07:50,400
my career has been that video can teach you a lot more than images can and then text can

96
00:07:50,400 --> 00:07:51,760
incidentally.

97
00:07:51,760 --> 00:07:57,080
If you have a system that's able to understand some subtle aspects of video and can make

98
00:07:57,080 --> 00:08:02,040
some subtle distinctions like for example understand the difference between putting an object

99
00:08:02,040 --> 00:08:06,800
on top of another object versus next to another object versus behind another object just

100
00:08:06,800 --> 00:08:08,880
simple interactions like that.

101
00:08:08,880 --> 00:08:13,720
If you have a system that can do that then you essentially have a system that has comments

102
00:08:13,720 --> 00:08:19,760
what you would call common sense which humans have you know you have a very natural intuitive

103
00:08:19,760 --> 00:08:25,360
understanding of the world you know unlike incidentally an image net train network that

104
00:08:25,360 --> 00:08:32,600
an object is not just a texture certain distribution of colors in your field of view but it is actually

105
00:08:32,600 --> 00:08:39,760
a thing that has a special extent that is surrounded by air that could be moved from one

106
00:08:39,760 --> 00:08:44,800
place to the other that has a weight associated with it that is subject to gravity that can

107
00:08:44,800 --> 00:08:48,720
behave in certain ways but not others that can teleport for example from one place to

108
00:08:48,720 --> 00:08:49,720
the other.

109
00:08:49,720 --> 00:08:53,440
And all of these things are totally natural to humans humans acquire a lot of that knowledge

110
00:08:53,440 --> 00:09:01,200
throughout the first years in their life and AI systems so far had none of that.

111
00:09:01,200 --> 00:09:06,400
And the interesting part about video is that to the degree that we are able to get systems

112
00:09:06,400 --> 00:09:13,240
to understand video concepts we enable them to get some of those concepts and that means

113
00:09:13,240 --> 00:09:20,160
really equipping AI systems with a certain degree of common sense and that's what's really

114
00:09:20,160 --> 00:09:25,280
exciting in what video and that is the reason why we have been kind of pushing along that

115
00:09:25,280 --> 00:09:29,040
agenda and I have been interested in that for 15 years or so.

116
00:09:29,040 --> 00:09:36,840
Has anyone tried to scope common sense like how much of I don't even know what the unit

117
00:09:36,840 --> 00:09:43,880
would be you know it's not rules it's not necessarily layers or neurons or memory or

118
00:09:43,880 --> 00:09:52,160
storage has anyone in any kind of way tried to get a handle on what the magnitude of what

119
00:09:52,160 --> 00:09:59,520
a computer would need to learn to have common sense or we you know that kind of the you

120
00:09:59,520 --> 00:10:07,240
know the hundred million dollar question kind of the touring test kind of scenario.

121
00:10:07,240 --> 00:10:11,240
There are a few discussions obviously and there are many probably very technical discussions

122
00:10:11,240 --> 00:10:19,920
in the psychology literature. There's one effort school of thought that surrounds common

123
00:10:19,920 --> 00:10:24,440
sense and drives it much further that I completely subscribe to and that has been one of the

124
00:10:24,440 --> 00:10:29,560
main reasons why I've been interested in getting at common sense via video.

125
00:10:29,560 --> 00:10:35,680
That is a school of thought around people like Douglas Hofstadter and George Lekov who

126
00:10:35,680 --> 00:10:43,000
say that our cognitive capabilities are structured around metaphors basically so whenever

127
00:10:43,000 --> 00:10:49,240
you recognize an object or whenever you have some concept in your head of of something

128
00:10:49,240 --> 00:10:55,040
like concept of an argument you do not understand that concept per se but you understand it

129
00:10:55,040 --> 00:11:02,240
by comparison to other things and so one example is that that came from from Lekov back

130
00:11:02,240 --> 00:11:07,480
then is that an argument is like a war basically when people have an argument you think of

131
00:11:07,480 --> 00:11:13,240
it as like a fight between those people or something like that and so specifically people

132
00:11:13,240 --> 00:11:19,080
like Douglas Hofstadter who I have always been a big fan of throughout the years has been

133
00:11:19,080 --> 00:11:25,920
driving this completely to the ultimate end point and he said basically that anything

134
00:11:25,920 --> 00:11:31,160
you do at any point in time is metaphors drawing analogies between different things like

135
00:11:31,160 --> 00:11:37,480
I mean whenever you recognize an elevator as an elevator that you have to walk to you

136
00:11:37,480 --> 00:11:42,400
basically making an analogy at that point and when you understand very subtle complex

137
00:11:42,400 --> 00:11:49,680
concepts like a CEO saying this company will weather the storm you again make an analogy

138
00:11:49,680 --> 00:11:55,560
and all of the cognitive processes that drive human understanding and thinking are just

139
00:11:55,560 --> 00:12:02,640
made of the same stuff which is analogy making and so common sense is kind of on the lowest

140
00:12:02,640 --> 00:12:09,160
level of that but there's a school of thought that says that everything that humans can

141
00:12:09,160 --> 00:12:13,760
do and that makes human thinking amazing is based on the same stuff which is just expan

142
00:12:13,760 --> 00:12:20,680
an expanjan away from from common sense which is focused on very very low level aspects

143
00:12:20,680 --> 00:12:25,560
of the world like what objects are how they move around and so on and so forth.

144
00:12:25,560 --> 00:12:32,280
So what are the implications of that school of thought on machine learning and AI?

145
00:12:32,280 --> 00:12:36,760
I think the implications could be immense and there will be immense to the degree that

146
00:12:36,760 --> 00:12:42,720
we will be able to make them tangible make them actual technologies that actually work

147
00:12:42,720 --> 00:12:48,240
and with what we've been doing on you know understanding how objects relate to each

148
00:12:48,240 --> 00:12:52,080
other what happens to them in a video it's just stretching the surface it's like basically

149
00:12:52,080 --> 00:12:58,920
go getting at the lowest level but there has been a lot of drive towards concepts ideas

150
00:12:58,920 --> 00:13:05,240
like grounding for example so enabling a system that does translation from one language

151
00:13:05,240 --> 00:13:11,440
to another to not just associate semantic patterns with with the word embedding that it

152
00:13:11,440 --> 00:13:17,160
uses but to associate actually something that's connected to a sensor and it's I think

153
00:13:17,160 --> 00:13:21,920
it's quite likely that we will see more and more of that in the future such that a system

154
00:13:21,920 --> 00:13:30,840
that outputs a sentence like there's a cup on the table not only has in its mind if

155
00:13:30,840 --> 00:13:37,400
you want the syntactic structure in the cup the words relate to another like as it learned

156
00:13:37,400 --> 00:13:43,720
from like large large text copper but also has an association of a cup you know some

157
00:13:43,720 --> 00:13:47,680
wake description which is just represented in the feature activation at that moment where

158
00:13:47,680 --> 00:13:53,640
the word is issued of a cup on a on a table and it can distinguish that from a cup under

159
00:13:53,640 --> 00:13:57,800
the table or something just by means of like visual associations I think this is going

160
00:13:57,800 --> 00:14:04,920
to happen more and more of a time are there any particular places or labs that are working

161
00:14:04,920 --> 00:14:13,280
in this area that are worth taking a look at or papers I haven't seen much and I think

162
00:14:13,280 --> 00:14:21,680
one of the reasons is that it is so fundamentally linked to video understanding that that

163
00:14:21,680 --> 00:14:25,680
we haven't seen much of that video understanding positive this own kind of challenges because

164
00:14:25,680 --> 00:14:31,240
or as I just mentioned it because of the really involved technological developments you need

165
00:14:31,240 --> 00:14:35,000
like the lots of engineering you need and the data operation to get all the data and

166
00:14:35,000 --> 00:14:40,840
so on because of that there is a barrier obviously and people have been hesitant to go down

167
00:14:40,840 --> 00:14:49,160
that route there is some interesting work in like common sense reasoning and so on coming

168
00:14:49,160 --> 00:14:56,680
from Josh Tannenbaum from MIT and there have been some efforts happening in the deep learning

169
00:14:56,680 --> 00:15:03,360
community over the last couple of years which didn't fly though like a lot of us have tried

170
00:15:03,360 --> 00:15:08,920
to build systems that can predict the future of a video in order to understand something

171
00:15:08,920 --> 00:15:14,080
about the world that's called unsupervised learning taken and frames and you try to predict

172
00:15:14,080 --> 00:15:20,320
what's going to happen in frames n plus 1 to n plus exactly and exactly and obviously

173
00:15:20,320 --> 00:15:24,320
it is sufficient if you have a system that can do that you could argue that that system

174
00:15:24,320 --> 00:15:28,000
really gets the world you know it really knows what options is otherwise how would it possibly

175
00:15:28,000 --> 00:15:35,040
be able to render the future of a video unfortunately even though it's it's a proof it would

176
00:15:35,040 --> 00:15:41,560
be a proof that a system is basically intelligent it's not feasible so far nobody has been

177
00:15:41,560 --> 00:15:48,360
able to predict video sensibly for up to more than maybe a second into the future and even

178
00:15:48,360 --> 00:15:54,280
then it's not really great and this is a fundamental problem that we all have been facing over

179
00:15:54,280 --> 00:15:59,520
many years when I referred back to the struggles we had a few years ago before image net

180
00:15:59,520 --> 00:16:03,360
and so on we thought that unsupervised learning is going to fly.

181
00:16:03,360 --> 00:16:09,400
It was learning basically means that you take data and you try a system to just represent

182
00:16:09,400 --> 00:16:16,240
that data and then get trained by reconstructing the data by basically inventing images and

183
00:16:16,240 --> 00:16:17,760
videos and so on.

184
00:16:17,760 --> 00:16:21,720
That never flew until now it's not flying.

185
00:16:21,720 --> 00:16:25,800
I have lost phase in that agenda quite to some degree.

186
00:16:25,800 --> 00:16:30,960
Now a lot of that work with the trying to predict the video was using like things like

187
00:16:30,960 --> 00:16:33,960
variational auto encoders and all that.

188
00:16:33,960 --> 00:16:35,880
Well there are many many different techniques.

189
00:16:35,880 --> 00:16:41,640
We had used RNNs back in the day to try to predict synthetic neural videos and that all

190
00:16:41,640 --> 00:16:45,800
worked fairly well and so on and it was nice for the first second.

191
00:16:45,800 --> 00:16:49,040
Yeah for a second or actually for synthetic videos you can go further than that you can

192
00:16:49,040 --> 00:16:54,480
predict like very very very long stretches of video because the structure and the synthetic

193
00:16:54,480 --> 00:16:58,480
data naturally is not that complicated not the real world.

194
00:16:58,480 --> 00:17:05,400
For example there was like a toy data set that showed balls bouncing around in a box.

195
00:17:05,400 --> 00:17:13,040
So very very simple physical simulation without much physics and at some point we became

196
00:17:13,040 --> 00:17:19,360
able to predict that very well and in the grander videos of balls bouncing around in a box

197
00:17:19,360 --> 00:17:23,720
very well but it hasn't gotten us anywhere.

198
00:17:23,720 --> 00:17:29,200
We haven't as a result of that build systems that really understand something about the world.

199
00:17:29,200 --> 00:17:36,840
And I think ImageNet has been a great example of how far supervised learning can actually

200
00:17:36,840 --> 00:17:39,080
push you because the main.

201
00:17:39,080 --> 00:17:44,800
The most interesting part about everything around ImageNet is that it's able to generate

202
00:17:44,800 --> 00:17:52,040
features that people then can use to solve other tasks which are not ImageNet by just

203
00:17:52,040 --> 00:17:56,480
freezing the features that you learned and using like what they call penultimate layer

204
00:17:56,480 --> 00:17:59,840
feature transfer learning or something like that.

205
00:17:59,840 --> 00:18:06,520
And that the fact that this works so amazingly well I think gives us such strong evidence that

206
00:18:06,520 --> 00:18:11,600
there is something to supervised learning and actual tasks that humans have to solve that

207
00:18:11,600 --> 00:18:17,160
it was going down that route and you know pushing the unsupervised agenda to the site

208
00:18:17,160 --> 00:18:21,320
for at least some time.

209
00:18:21,320 --> 00:18:27,480
And so how are you taking on this problem at 20 billion rounds?

210
00:18:27,480 --> 00:18:34,120
So the challenge then is obviously how to get your hands on data that is labeled data

211
00:18:34,120 --> 00:18:38,040
that is video accompanied by a label.

212
00:18:38,040 --> 00:18:43,560
And that is a big challenge in itself because as I just mentioned images can be labeled

213
00:18:43,560 --> 00:18:50,680
by attaching nouns to what's the central object in the image and so on.

214
00:18:50,680 --> 00:18:56,840
For videos it's not quite possible specifically if you want videos as a way to generate some

215
00:18:56,840 --> 00:19:01,960
kind of common sense it's just not going to well you're not going to be able to do that

216
00:19:01,960 --> 00:19:04,480
by just labeling nouns in a video.

217
00:19:04,480 --> 00:19:08,880
What you need is videos that show very subtle concepts people doing things with objects

218
00:19:08,880 --> 00:19:12,440
and interacting and so on.

219
00:19:12,440 --> 00:19:19,720
And the standard way of gathering data just doesn't work in that case either because if

220
00:19:19,720 --> 00:19:24,720
you want to have a video where a person puts an object on top of another object and then

221
00:19:24,720 --> 00:19:28,960
on next to another object so that you can have these two classes.

222
00:19:28,960 --> 00:19:32,560
Good luck if you want to try to find those videos in YouTube.

223
00:19:32,560 --> 00:19:35,840
You're going to have to watch through hundreds of hours of YouTube video until you find

224
00:19:35,840 --> 00:19:39,440
this exact actual happening that you can attach a label to.

225
00:19:39,440 --> 00:19:47,880
So what we do at 20 billion is to have crowd workers not label videos for us but film videos

226
00:19:47,880 --> 00:19:54,880
for us and we build a platform where crowd workers connect and they use their webcams

227
00:19:54,880 --> 00:20:02,320
to do all kinds of stuff in their homes usually that we tell them to.

228
00:20:02,320 --> 00:20:11,320
And what we tell them to do are examples that I just mentioned and many many others behaving

229
00:20:11,320 --> 00:20:15,680
in a certain way do certain things like hide an object behind another cover an object

230
00:20:15,680 --> 00:20:20,360
up and uncover it or push it around or push it so that it falls off the table and so

231
00:20:20,360 --> 00:20:21,880
on and so forth.

232
00:20:21,880 --> 00:20:28,760
And that way we have been able to gather a very, very large amount of data that we now

233
00:20:28,760 --> 00:20:32,800
use to train our networks supervised.

234
00:20:32,800 --> 00:20:37,280
And the name of the game of course is to use the supervised learning not just as a goal

235
00:20:37,280 --> 00:20:42,520
in itself but to cash in on transfer learning there as well.

236
00:20:42,520 --> 00:20:46,720
The idea being that once you have a system that understands what behind and in front

237
00:20:46,720 --> 00:20:52,400
and all of these things are that it can generalize much better to new use cases for which you

238
00:20:52,400 --> 00:20:55,000
have much less data.

239
00:20:55,000 --> 00:21:01,040
And so you've kind of inverted the problem you're having them create the data from the

240
00:21:01,040 --> 00:21:02,040
label.

241
00:21:02,040 --> 00:21:03,040
Exactly.

242
00:21:03,040 --> 00:21:05,840
As opposed to the opposite.

243
00:21:05,840 --> 00:21:10,440
Just going back to the premise though is the idea that with I'm assuming the idea

244
00:21:10,440 --> 00:21:15,120
with is that with you know just going to YouTube videos and like grabbing a video and starting

245
00:21:15,120 --> 00:21:22,280
to label it the your label spaces just too large is just you have to control for the kinds

246
00:21:22,280 --> 00:21:25,280
of things you want to label for any given experiment.

247
00:21:25,280 --> 00:21:26,280
Exactly.

248
00:21:26,280 --> 00:21:27,280
Exactly.

249
00:21:27,280 --> 00:21:32,720
And as opposed to images there are just so many labels that you want to have that are

250
00:21:32,720 --> 00:21:38,120
not just many even but also a compositional.

251
00:21:38,120 --> 00:21:42,600
So it makes much more sense in a video context to have labels that are descriptions like

252
00:21:42,600 --> 00:21:48,720
whole sentences that tell you what's happening in a video clip rather than a single noun.

253
00:21:48,720 --> 00:21:55,400
So videos contain verbs not just nouns and they contain other word classes.

254
00:21:55,400 --> 00:22:00,000
And the labels in our operation take the form of captions typically.

255
00:22:00,000 --> 00:22:04,000
So we tell them because I act out the following concept.

256
00:22:04,000 --> 00:22:09,200
Something like pushing an object over the edge of the table.

257
00:22:09,200 --> 00:22:12,160
So it falls down or something like that.

258
00:22:12,160 --> 00:22:19,560
And so that means the label there right there is structured and it has some similarity

259
00:22:19,560 --> 00:22:26,560
to other labels such as push an object or pushing an object towards the edge of the table

260
00:22:26,560 --> 00:22:30,160
but not so far that it falls down or something like that.

261
00:22:30,160 --> 00:22:36,680
So labels are complex objects in themselves and we don't see any other way of getting

262
00:22:36,680 --> 00:22:40,640
at label data of the type other than inverting this process.

263
00:22:40,640 --> 00:22:45,680
And starting with the label generating them on our site and then just filming what they

264
00:22:45,680 --> 00:22:46,680
represent.

265
00:22:46,680 --> 00:22:54,520
Have there been any efforts to start with a movie for example and just label every scene

266
00:22:54,520 --> 00:23:02,840
or at kind of arbitrary granularities of time label what's happening in a scene?

267
00:23:02,840 --> 00:23:08,520
Yes, there have been a lot of efforts on generating data for video over many years.

268
00:23:08,520 --> 00:23:13,320
As video understanding has been there, people have been trying my included.

269
00:23:13,320 --> 00:23:16,920
But they were usually of that type as you just mentioned.

270
00:23:16,920 --> 00:23:23,200
They're usually involved going through video and labeling video, existing video.

271
00:23:23,200 --> 00:23:27,320
One of the outcomes of that has been that a lot of the video data sets that are available

272
00:23:27,320 --> 00:23:32,520
are a bit funky as training material for networks.

273
00:23:32,520 --> 00:23:37,360
There are data sets out there that are very common and commonly used in the community that

274
00:23:37,360 --> 00:23:44,200
for example contain different sports activities from broadcast TV basically.

275
00:23:44,200 --> 00:23:49,480
And then the task of the networks is to distinguish soccer from basketball or baseball

276
00:23:49,480 --> 00:23:50,960
or something like that.

277
00:23:50,960 --> 00:23:55,840
And so what's problematic with these kind of efforts and data sets is that in many cases

278
00:23:55,840 --> 00:24:00,080
just a single image from the video just reveals what you're looking at.

279
00:24:00,080 --> 00:24:04,480
I can tell you that it's soccer just because I see there's like a green...

280
00:24:04,480 --> 00:24:08,680
Sunderbar, something like that, exactly.

281
00:24:08,680 --> 00:24:14,760
And specifically they don't involve really getting into the nitty-gritty details of the

282
00:24:14,760 --> 00:24:16,400
physics of the scene.

283
00:24:16,400 --> 00:24:22,480
They don't ask a network to really attend to certain objects in the scene and look whether

284
00:24:22,480 --> 00:24:27,800
they are behind another or in front or if they're falling or not falling.

285
00:24:27,800 --> 00:24:34,600
If they are maybe made of liquids or solid or if they're made from cloth or all of these

286
00:24:34,600 --> 00:24:40,960
really, really subtle physical aspects, that video is so great at revealing they are not

287
00:24:40,960 --> 00:24:44,120
basically asked for in these kinds of tasks.

288
00:24:44,120 --> 00:24:48,960
That has been a problem, my opinion, with the way that the video of understanding community

289
00:24:48,960 --> 00:24:53,280
has been rolling over the last couple of years.

290
00:24:53,280 --> 00:25:01,760
The other thing that kind of jumps out at me is if I don't know if like the stage directions

291
00:25:01,760 --> 00:25:10,120
and things like that in a screenplay are rich enough to be used as training data as labels,

292
00:25:10,120 --> 00:25:17,400
but it would be... I wonder if it would be interesting to take a movie and then use the stage

293
00:25:17,400 --> 00:25:21,120
directions as kind of the labels.

294
00:25:21,120 --> 00:25:26,800
But even then, those stage directions, there's so much that's kind of at the discretion

295
00:25:26,800 --> 00:25:33,200
of the director and kind of evolves that's not explicitly specified in the stage direction.

296
00:25:33,200 --> 00:25:34,200
Yes, very true.

297
00:25:34,200 --> 00:25:38,800
And there have been efforts also to generate data sets exactly using that kind of information.

298
00:25:38,800 --> 00:25:43,880
Yeah, using scripts and accompanying movies and making that the source for the labeling

299
00:25:43,880 --> 00:25:44,880
basically.

300
00:25:44,880 --> 00:25:49,520
Still not good enough to convince you that going the other way isn't better.

301
00:25:49,520 --> 00:25:54,520
It's absolutely not because the stage directions, as you just mentioned, are really, really

302
00:25:54,520 --> 00:25:56,440
high level and corporal.

303
00:25:56,440 --> 00:26:04,560
The stage direction is not going to be take a cup, pour slightly next to the glass so

304
00:26:04,560 --> 00:26:07,720
that you spill a little bit or something like that, which is what we want the network

305
00:26:07,720 --> 00:26:08,720
to be understand.

306
00:26:08,720 --> 00:26:13,360
It goes back to this issue that there's so much context that's understood by us as humans

307
00:26:13,360 --> 00:26:14,360
that we don't...

308
00:26:14,360 --> 00:26:15,360
Right, exactly.

309
00:26:15,360 --> 00:26:16,360
Exactly.

310
00:26:16,360 --> 00:26:22,040
We humans already have all that common sense understanding, so the script writer, or whoever,

311
00:26:22,040 --> 00:26:26,040
is going to assume that information is already in the head of the director or something.

312
00:26:26,040 --> 00:26:27,960
They don't need to spell everything out in detail.

313
00:26:27,960 --> 00:26:28,960
Right.

314
00:26:28,960 --> 00:26:34,000
I think to the degree that we advanced on this agenda, and networks get better at getting

315
00:26:34,000 --> 00:26:38,520
this floor right now that we're looking at of basic understanding of physics and so on,

316
00:26:38,520 --> 00:26:43,680
does it agree that we make direction there and it starts to get to work better?

317
00:26:43,680 --> 00:26:54,160
I could see how scripted data from scripts is going to be useful, but right now the foundation

318
00:26:54,160 --> 00:26:55,160
is missing.

319
00:26:55,160 --> 00:27:00,800
We're asking the system to understand something really complex, highly cultural and social

320
00:27:00,800 --> 00:27:05,120
and so on, without even understanding the world in which we're living, and that seems

321
00:27:05,120 --> 00:27:07,440
a bit bizarre to me.

322
00:27:07,440 --> 00:27:11,760
So what have you used this method to accomplish so far?

323
00:27:11,760 --> 00:27:17,000
So we had a breakthrough a few weeks ago, like around three weeks ago, where we trained

324
00:27:17,000 --> 00:27:22,440
a network on a very large amount of data that we've gathered so far, just for the fun

325
00:27:22,440 --> 00:27:23,440
of it.

326
00:27:23,440 --> 00:27:28,640
We have data across many use cases, obviously, but we just thought, okay, let's see how far

327
00:27:28,640 --> 00:27:35,480
we are right now, and much sooner than expected, we saw that a lot of this subtle distinctions

328
00:27:35,480 --> 00:27:38,800
that we were asking the networks to do actually already happened.

329
00:27:38,800 --> 00:27:41,680
And what kinds of distinctions?

330
00:27:41,680 --> 00:27:45,200
Lots of the physical things that I just mentioned, you know, a person throwing an object

331
00:27:45,200 --> 00:27:48,760
in the air and catching it versus throwing an object in the air and it falls on the floor

332
00:27:48,760 --> 00:27:50,760
and it's like that.

333
00:27:50,760 --> 00:27:55,960
And it's still early times, we basically just have the first push through, we basically

334
00:27:55,960 --> 00:28:03,160
saw, okay, there is like a non-rendered performance, the networks make like clearly non-rendered

335
00:28:03,160 --> 00:28:07,880
performance, correct decisions there, which far has been very, very exciting, because

336
00:28:07,880 --> 00:28:10,960
these are incredibly hard tasks.

337
00:28:10,960 --> 00:28:18,160
And so now we have live demos and so on that show how, you know, you can just take the

338
00:28:18,160 --> 00:28:23,400
camera, put it up, and then just do things in front of it, and it'll understand, constantly

339
00:28:23,400 --> 00:28:29,000
describe what you're doing in real time, and get a lot of things that are very subtle.

340
00:28:29,000 --> 00:28:33,600
And it has been very exciting, it has happened just now, basically.

341
00:28:33,600 --> 00:28:34,600
Nice.

342
00:28:34,600 --> 00:28:35,600
Yeah, yeah.

343
00:28:35,600 --> 00:28:37,840
How do you plan to apply it?

344
00:28:37,840 --> 00:28:43,400
So as I mentioned, we're gathering data driven by two considerations.

345
00:28:43,400 --> 00:28:49,200
One being we want to sample the space of things that can happen in video as densely as possible

346
00:28:49,200 --> 00:28:53,720
to catching on transfer learning and so on, but then we also have interactions with prospective

347
00:28:53,720 --> 00:28:58,920
customers and existing customers on a very specific use cases that they have.

348
00:28:58,920 --> 00:29:06,280
And so the data set fills up over time driven by these two considerations, basically.

349
00:29:06,280 --> 00:29:11,880
And so the specific use cases that for which we have gathered data and for which we've

350
00:29:11,880 --> 00:29:16,920
built models and so on are things like human computer interaction.

351
00:29:16,920 --> 00:29:25,320
So for example, we've built one of the first accurate RGB gesture recognition systems that

352
00:29:25,320 --> 00:29:26,320
have ever been around.

353
00:29:26,320 --> 00:29:32,160
So people are sitting in front of the screen doing certain movements with their hands signalling

354
00:29:32,160 --> 00:29:36,800
to the system that they want to turn down the volume or whatever.

355
00:29:36,800 --> 00:29:41,280
You need to talk to this company, I forget the name of the company, but I was at the

356
00:29:41,280 --> 00:29:47,400
Gartner conference last week, it's all a blur at this point, but one of the companies

357
00:29:47,400 --> 00:29:57,040
that I talked to is the company that basically it was founded by the guy that did all of

358
00:29:57,040 --> 00:30:00,360
the gesture stuff for the movie minority report.

359
00:30:00,360 --> 00:30:01,360
Okay.

360
00:30:01,360 --> 00:30:06,200
They commercialized that into a company and so you can have all these screens and you're

361
00:30:06,200 --> 00:30:11,200
kind of dragging and flinging content around on these screens and you're doing it with

362
00:30:11,200 --> 00:30:14,160
this remote and I'm like, yeah.

363
00:30:14,160 --> 00:30:18,240
And they have a conference, conference room set up and there's a camera right there and

364
00:30:18,240 --> 00:30:22,040
I'm like, there's a camera right there, why am I using this clumsy remote to move these

365
00:30:22,040 --> 00:30:23,040
windows around?

366
00:30:23,040 --> 00:30:24,040
Yes.

367
00:30:24,040 --> 00:30:25,360
I can do that with a mouse on a desktop.

368
00:30:25,360 --> 00:30:26,360
This makes no sense.

369
00:30:26,360 --> 00:30:29,800
He's like, well, you know, there'll be a lot of training that would have to happen in

370
00:30:29,800 --> 00:30:33,680
order to get users to make these gestures and like, yeah, I don't buy that.

371
00:30:33,680 --> 00:30:37,280
You need to figure out how to do the gestures and it sounds like you.

372
00:30:37,280 --> 00:30:39,640
Incidentally, we just figured out how to do the gesture.

373
00:30:39,640 --> 00:30:44,880
We have live demos and they just work and we have agreements with customers who want

374
00:30:44,880 --> 00:30:47,080
to use that and so on.

375
00:30:47,080 --> 00:30:49,960
It basically just works.

376
00:30:49,960 --> 00:30:53,920
Just showing once more that there is something to the image and that findings, you know,

377
00:30:53,920 --> 00:30:58,680
if you gather enough data, you can make things work very nicely.

378
00:30:58,680 --> 00:31:03,960
And it's true, there has been a lot of efforts around extra devices that you hold in your

379
00:31:03,960 --> 00:31:10,880
hand or even, even if you get rid of those, like the connect and so on, like the devices

380
00:31:10,880 --> 00:31:15,240
where you have a camera sensor, which is not an RGB sensor, but a depth sensor.

381
00:31:15,240 --> 00:31:20,640
And so it sees something about the depth profile in the scene in order to do its job.

382
00:31:20,640 --> 00:31:27,520
And there has been, you know, almost religious belief in the gesture recognition community

383
00:31:27,520 --> 00:31:29,040
that that's what you need.

384
00:31:29,040 --> 00:31:32,760
You need a funky sensor, otherwise you won't do gestures.

385
00:31:32,760 --> 00:31:37,240
But, you know, if I close one, if I close one of my eyes and you do a gesture in front

386
00:31:37,240 --> 00:31:39,280
of me, I can perfectly recognize it.

387
00:31:39,280 --> 00:31:41,160
There's no problem whatsoever.

388
00:31:41,160 --> 00:31:46,880
So why wouldn't it train neural net that just sees gestures, tons of gestures, be able

389
00:31:46,880 --> 00:31:47,880
to do that as well?

390
00:31:47,880 --> 00:31:50,640
We just showed that it's possible and it is perfectly possible.

391
00:31:50,640 --> 00:31:53,280
So the problem is basically solved.

392
00:31:53,280 --> 00:31:54,280
Yeah.

393
00:31:54,280 --> 00:32:00,080
There's something definitely sticky to this idea of the camera as being kind of the universal

394
00:32:00,080 --> 00:32:06,040
sensor that replaces huge swaths of other types of sensors.

395
00:32:06,040 --> 00:32:11,280
We just need to be able to figure out the intelligence on the back end to allow the camera

396
00:32:11,280 --> 00:32:13,840
sensor to do what we can very easily do.

397
00:32:13,840 --> 00:32:14,840
Indeed.

398
00:32:14,840 --> 00:32:15,840
Indeed.

399
00:32:15,840 --> 00:32:16,840
That's true.

400
00:32:16,840 --> 00:32:23,640
And whenever you can do something with one eye, you have a proof of concept.

401
00:32:23,640 --> 00:32:24,880
It is possible.

402
00:32:24,880 --> 00:32:26,880
And then you know you just have to figure out how.

403
00:32:26,880 --> 00:32:27,880
Yes.

404
00:32:27,880 --> 00:32:28,880
Absolutely.

405
00:32:28,880 --> 00:32:29,880
Awesome.

406
00:32:29,880 --> 00:32:30,880
Great.

407
00:32:30,880 --> 00:32:33,200
Well, I know you did a talk yesterday.

408
00:32:33,200 --> 00:32:37,720
Is there anything that you covered in your talk that we haven't managed to tease out

409
00:32:37,720 --> 00:32:40,680
in our conversation so far?

410
00:32:40,680 --> 00:32:49,720
I showed some demos that I can show to you via audio demos and hand gestures and white

411
00:32:49,720 --> 00:32:52,360
boards are difficult for a podcast.

412
00:32:52,360 --> 00:32:54,160
That is true.

413
00:32:54,160 --> 00:32:57,360
Besides that, we've pretty much covered what I was talking about yesterday.

414
00:32:57,360 --> 00:32:58,360
Awesome.

415
00:32:58,360 --> 00:32:59,360
Great.

416
00:32:59,360 --> 00:33:02,880
Well, Roland, it was great to meet you and great to chat with you.

417
00:33:02,880 --> 00:33:03,880
Thank you.

418
00:33:03,880 --> 00:33:04,880
Great.

419
00:33:04,880 --> 00:33:05,880
Thanks so much.

420
00:33:05,880 --> 00:33:09,200
All right, everyone.

421
00:33:09,200 --> 00:33:11,240
That's our show for today.

422
00:33:11,240 --> 00:33:16,320
For more information on Roland or any of the topics covered in this episode, head on over

423
00:33:16,320 --> 00:33:20,680
to twimlai.com slash talk slash 111.

424
00:33:20,680 --> 00:33:28,760
And remember to submit your thoughts for our My AI contest at twimlai.com slash My AI.

425
00:33:28,760 --> 00:33:31,680
Thanks so much for listening and catch you next time.

