WEBVTT

00:00.000 --> 00:15.920
Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting

00:15.920 --> 00:20.880
people doing interesting things in machine learning and artificial intelligence.

00:20.880 --> 00:23.560
I'm your host Sam Charrington.

00:23.560 --> 00:28.360
A big thanks to everyone who participated in last week's Twimble Online Meetup and

00:28.360 --> 00:31.160
to Kevin T from SIGUP for presenting.

00:31.160 --> 00:35.400
You can find the slides for his presentation in the Meetup Slack channel as well as in

00:35.400 --> 00:37.320
this week's show notes.

00:37.320 --> 00:41.680
Our final Meetup of the Year will be held on Wednesday, December 13th.

00:41.680 --> 00:46.880
Make sure to bring your thoughts on the top machine learning and AI stories for 2017

00:46.880 --> 00:49.040
for our discussion segment.

00:49.040 --> 00:54.680
For the main presentation, prior Twimble Talk guest Bruno Gonzalez will be discussing

00:54.680 --> 01:01.160
the paper understanding deep learning requires rethinking generalization by Shi Huang Zhang

01:01.160 --> 01:04.560
from MIT and Google Brain and others.

01:04.560 --> 01:09.680
You can find more details and register at twimbleia.com slash Meetup.

01:09.680 --> 01:14.000
If you receive my newsletter, you already know this, but Twimble is growing and we're

01:14.000 --> 01:19.720
looking for an energetic and passionate community manager to help expand our programs.

01:19.720 --> 01:24.120
This position can be remote, but if you happen to be in St. Louis, all the better.

01:24.120 --> 01:27.840
If you're interested, please reach out to me for additional details.

01:27.840 --> 01:31.800
I should mention that if you don't already get my newsletter, you are really missing

01:31.800 --> 01:37.040
out and should visit twimbleia.com slash newsletter to sign up.

01:37.040 --> 01:43.120
Now the show you are about to hear is part of our Strange Loop 2017 series brought to you

01:43.120 --> 01:46.120
by our friends at Nexusos.

01:46.120 --> 01:50.520
Nexusos is a company focused on making machine learning more easily accessible to enterprise

01:50.520 --> 01:51.520
developers.

01:51.520 --> 01:56.040
Their machine learning API meets developers where they're at, regardless of their mastery

01:56.040 --> 02:01.280
of data science, so they can start cutting up predictive applications immediately and

02:01.280 --> 02:04.120
in their preferred programming language.

02:04.120 --> 02:08.480
It's as simple as loading your data and selecting the type of problem you want to solve.

02:08.480 --> 02:13.360
Their automated platform trains and selects the best model fit for your data and then outputs

02:13.360 --> 02:14.920
predictions.

02:14.920 --> 02:19.280
To learn more about Nexusos, be sure to check out the first episode in this series at

02:19.280 --> 02:27.520
twimbleia.com slash talk slash 69 where I speak with co-founders Ryan Sevy and Jason Montgomery.

02:27.520 --> 02:32.960
Be sure to also get your free Nexusos API key and discover how to start leveraging machine

02:32.960 --> 02:38.200
learning in your next project at nexosos.com slash twimble.

02:38.200 --> 02:43.600
In this episode, I speak with Matthew Taylor, open source manager at Numenta.

02:43.600 --> 02:47.840
You might remember hearing a bit about Numenta from an interview I did with Francisco Weber

02:47.840 --> 02:54.440
of cortical.io on twimble talk number 10, a show which remains the most popular show of

02:54.440 --> 02:56.720
the podcast to date.

02:56.720 --> 03:02.000
Numenta is basically trying to reverse engineer the neocortex and use what they learn to

03:02.000 --> 03:08.480
develop a neocortical theory for biological and machine intelligence that they call hierarchical

03:08.480 --> 03:10.120
temporal memory.

03:10.120 --> 03:14.640
Met joined me at the conference to discuss his talk, the biological path towards strong

03:14.640 --> 03:16.160
AI.

03:16.160 --> 03:21.960
In our conversation, we discussed the basics of htm, its biological inspiration and how

03:21.960 --> 03:26.160
it differs from traditional neural networks, including deep learning.

03:26.160 --> 03:30.760
This is a nerd alert show and after you listen, I would also encourage you to check out the

03:30.760 --> 03:35.320
conversation with Francisco, which we'll link to in the show notes.

03:35.320 --> 03:44.800
And now on to the show.

03:44.800 --> 03:49.600
Hi everyone, I am here at the strange loop conference in St. Louis.

03:49.600 --> 03:55.320
And I am joined by Matt Taylor, who is an open source community manager at Numenta.

03:55.320 --> 03:58.480
And I am super excited to have you here with me, Matt.

03:58.480 --> 04:04.560
You just delivered a talk here at the conference and I'm looking forward to us diving into that.

04:04.560 --> 04:06.560
But before we go anywhere else, welcome.

04:06.560 --> 04:07.560
Thank you.

04:07.560 --> 04:08.560
It's a pleasure to be here.

04:08.560 --> 04:10.000
Pleasure to have you on the show.

04:10.000 --> 04:13.720
So why don't we get started by having you tell us a little bit about your background and

04:13.720 --> 04:16.720
how you got into machine learning and AI?

04:16.720 --> 04:19.720
Yeah, I don't know how far back to go.

04:19.720 --> 04:23.720
But I mean, in computers, I got interested in computers when I was enlisted in Air Force.

04:23.720 --> 04:26.960
I was an intelligence analyst in the Air Force.

04:26.960 --> 04:31.320
And then that turned into a department of defense job in the same place and I was doing a lot

04:31.320 --> 04:38.840
of simulation, like air defense simulations in like Fortran and Shell scripts.

04:38.840 --> 04:41.760
It was kind of our cake, but it was a very powerful simulation.

04:41.760 --> 04:43.680
So that's sort of what got me into programming.

04:43.680 --> 04:49.000
I didn't really think much about artificial intelligence until I read on intelligence,

04:49.000 --> 04:54.000
which is a book that our founder Jeff Hawkins wrote, I think in 2005.

04:54.000 --> 04:56.600
And I was working in the software industry at that point.

04:56.600 --> 05:03.040
I got away from, you know, old defense industry and moved here to St. Louis to work in software

05:03.040 --> 05:05.840
after I got my software degree.

05:05.840 --> 05:06.840
Oh, wow.

05:06.840 --> 05:10.960
And so I would just consult it around St. Louis, worked at a bunch of different places and

05:10.960 --> 05:12.560
did a bunch of different jobs.

05:12.560 --> 05:13.560
And I read that book.

05:13.560 --> 05:17.880
I remember reading it on intelligence and another book called The Singularity is Near by

05:17.880 --> 05:18.880
Ray Kurzweil.

05:18.880 --> 05:19.880
Sure.

05:19.880 --> 05:23.440
And reading those two books at the same time really like flipped the script for me.

05:23.440 --> 05:29.320
And I was like, it made me start wondering all these big questions like, what is consciousness?

05:29.320 --> 05:30.320
What is intelligence?

05:30.320 --> 05:31.920
How do we even define these things?

05:31.920 --> 05:37.520
Is it really possible that we could build intelligence systems out of non-biological materials?

05:37.520 --> 05:42.640
But at the time, you know, I was just working here doing mundane software programming and

05:42.640 --> 05:43.640
stuff.

05:43.640 --> 05:50.640
But I don't have a math degree, I didn't have an experience in deep learning or artificial

05:50.640 --> 05:51.640
neural networks.

05:51.640 --> 05:55.560
Even at that point deep learning wasn't even a big deal.

05:55.560 --> 05:58.240
So I just gave it up as a pipe dream.

05:58.240 --> 06:04.040
But at some point, I got a job at Yahoo as a front-end engineer, which is odd because

06:04.040 --> 06:08.840
I'd never done front-end engineers before, but I got a job at Yahoo and moved out to the

06:08.840 --> 06:09.840
Bay Area.

06:09.840 --> 06:13.760
Worked there for a couple years and out of the blue, I got a call from a recruiter for

06:13.760 --> 06:16.400
a front-end physician at Nementa.

06:16.400 --> 06:18.600
And I was like, wow, what?

06:18.600 --> 06:19.600
OK, sure.

06:19.600 --> 06:20.600
Wow.

06:20.600 --> 06:24.720
So I jumped on board at that point and just started doing web stuff.

06:24.720 --> 06:27.120
Eventually moved up to do web services.

06:27.120 --> 06:29.160
It was the manager web services.

06:29.160 --> 06:34.680
And when my boss, Jeff, decided he wanted to take all of their algorithms, open source,

06:34.680 --> 06:37.400
I was like, I want to help with that.

06:37.400 --> 06:38.800
Because I like open source.

06:38.800 --> 06:43.000
I've always been an advocate of open source and been a part of different communities.

06:43.000 --> 06:46.760
And I was like, sign me up, so I did.

06:46.760 --> 06:48.120
That's fantastic.

06:48.120 --> 06:54.200
So maybe for folks that aren't familiar with Nementa, you can kind of walk us through

06:54.200 --> 06:58.480
the company and its position in the machine learning space.

06:58.480 --> 07:03.640
I think the company has kind of a unique approach to machine learning.

07:03.640 --> 07:09.000
And folks that have been around with the podcast for a while and listened to Francisco

07:09.000 --> 07:14.840
Weber's podcast might recall Nementa and Jeff Hawkins' work coming up in that context

07:14.840 --> 07:21.160
because the work that cortical is doing is related to what Nementa is doing.

07:21.160 --> 07:25.320
Yeah, that podcast, I think, was a great primer for us for Nementa.

07:25.320 --> 07:30.680
It was a partner of ours, and Francisco is a brilliant guy, you know, is exactly what

07:30.680 --> 07:32.320
he's talking about.

07:32.320 --> 07:37.280
So our mission at Nementa is different than I think most companies, and it's always been

07:37.280 --> 07:39.200
this ever since that I've been at the company.

07:39.200 --> 07:41.000
It's two things.

07:41.000 --> 07:45.320
Understand how intelligence works in the Neo Cortex, and the second thing is implement

07:45.320 --> 07:49.720
those things outside of biological systems, like try and build it, because basically

07:49.720 --> 07:53.720
reverse engineer the Neo Cortex as our mission.

07:53.720 --> 07:56.320
And hopefully, you know, we'll make money out of that at some point.

07:56.320 --> 08:00.120
But honestly, we're like really kind of R&D focused right now.

08:00.120 --> 08:01.120
Yeah.

08:01.120 --> 08:05.240
Very small company, very focused on the research.

08:05.240 --> 08:08.720
And is it primarily just funded by Jeff?

08:08.720 --> 08:11.720
Yeah, it's privately funded by Jeff Donah.

08:11.720 --> 08:12.720
Yeah, yeah.

08:12.720 --> 08:17.200
A group of contributors that have been long time associates of Jeff and Donah, you know,

08:17.200 --> 08:22.640
they built Palm and Handspring, and so there's a crew of board members that I think help

08:22.640 --> 08:25.080
with the funding, but I don't know the details of all that.

08:25.080 --> 08:30.760
And is the implication of that mission, though, that the company is not under your traditional

08:30.760 --> 08:33.560
kind of venture commercialization pressures?

08:33.560 --> 08:39.200
Is it better to think of Nementa as like an open AI than like a, you know, machine learning

08:39.200 --> 08:40.200
company X?

08:40.200 --> 08:41.200
I guess.

08:41.200 --> 08:44.480
I've never thought of it in comparison to open AI, but I guess it would be similar in

08:44.480 --> 08:46.200
that we're not building products.

08:46.200 --> 08:48.840
You know, we're not selling services.

08:48.840 --> 08:51.880
What we're doing is we're trying to make discoveries.

08:51.880 --> 08:58.680
So all of our discoveries are based on neuroscience research, you know, our research engineers are

08:58.680 --> 09:03.640
always reading the most recent neuroscience papers that come out there, they're interacting

09:03.640 --> 09:08.440
with different neuroscientists in the community, trying to answer questions that are relevant

09:08.440 --> 09:11.680
to how we understand intelligence in the context.

09:11.680 --> 09:16.160
And what we do is as we make these discoveries and we test them out, you know, we'll prototype

09:16.160 --> 09:18.320
them and software and think, oh, this is how it works.

09:18.320 --> 09:23.000
It actually does, our theory seems to work in software the way we thought.

09:23.000 --> 09:27.440
Then we will create patents around those discoveries.

09:27.440 --> 09:31.240
So there, you know, specific ones about things that we've discovered about how the brain

09:31.240 --> 09:36.600
is working and how we've implemented it and currently in software, but it could be implemented

09:36.600 --> 09:37.600
in hardware too.

09:37.600 --> 09:38.600
Okay.

09:38.600 --> 09:43.000
The idea being, you know, the monetization strategy is in the value of the IP itself.

09:43.000 --> 09:49.040
So we don't want to be distracted by consulting, by providing services or by creating applications

09:49.040 --> 09:50.040
at this point.

09:50.040 --> 09:54.600
We really want to focus on the discovery, on the brain, trying to figure out how it works

09:54.600 --> 09:57.560
and we think that good things will come of that.

09:57.560 --> 09:58.560
Okay.

09:58.560 --> 10:06.680
So for your talk, one of the big things that I think you talked about at least from the

10:06.680 --> 10:11.480
perspective, from what I got out of the abstract was you kind of premised it on, you know,

10:11.480 --> 10:15.720
hey, there's a lot of excitement out there about neural nets and deep learning and things

10:15.720 --> 10:20.760
like that, but these are all based on a model of a neuron that is, you know, rather

10:20.760 --> 10:21.760
dated.

10:21.760 --> 10:22.760
Right.

10:22.760 --> 10:27.000
And I presume you then walk through some of the new things that we've learned since then.

10:27.000 --> 10:28.000
Yeah.

10:28.000 --> 10:32.560
You kind of walk us through your talk and the ideas that you wanted to share with folks.

10:32.560 --> 10:33.560
Sure.

10:33.560 --> 10:37.720
So I'd like to say, first off, that I don't have anything bad to say about artificial

10:37.720 --> 10:39.400
neural networks or deep learning.

10:39.400 --> 10:44.080
I sure that's necessary technology that we needed to build in.

10:44.080 --> 10:49.840
But one of my main points is that it's not going to naturally evolve into what people

10:49.840 --> 10:51.400
call strong AI.

10:51.400 --> 10:56.960
And the first thing I say in my talk is weak AI is not intelligent and won't become intelligent.

10:56.960 --> 11:03.560
There's not going to be some, this like exponential growth and suddenly, you know, sentience.

11:03.560 --> 11:09.960
There's some core things about the an end point neuron models specifically that don't have

11:09.960 --> 11:14.960
the capacity for intelligence as we understand those are what are some of those core things.

11:14.960 --> 11:18.800
Let's just dive right in there's there's there's two main things.

11:18.800 --> 11:23.760
One is that the neuron needs to have three states and current neurons have two states active

11:23.760 --> 11:25.360
or not.

11:25.360 --> 11:28.240
And we add the idea of a predictive state.

11:28.240 --> 11:34.240
So the neuron goes into a predictive state to indicate that it thinks based upon the

11:34.240 --> 11:38.800
context of its input that it's going to be active soon.

11:38.800 --> 11:44.160
And that prediction is core to everything about our theory that and we, you know, we take

11:44.160 --> 11:46.400
that from understanding how the brain works.

11:46.400 --> 11:50.760
Your brain is constantly making predictions about what it's going to see next when it's

11:50.760 --> 11:52.760
going to feel next all the time.

11:52.760 --> 11:58.280
And you can see that by investigating these depolarized pyramidal neurons in the neuroscience

11:58.280 --> 12:03.160
they call these cells a depolarized, which means that they're primed to fire.

12:03.160 --> 12:05.440
And we're missing that in the an end neuron model.

12:05.440 --> 12:06.880
There's no, there's no concept of that.

12:06.880 --> 12:08.200
So there's that.

12:08.200 --> 12:12.680
And the other thing is pyramidal neurons have different integration zones.

12:12.680 --> 12:17.040
It's not, they don't just have one group of connections to other neurons.

12:17.040 --> 12:22.440
They've got apical dendrites that kind of provide feedback from layers that are either

12:22.440 --> 12:25.000
above it or different parts of the cortex.

12:25.000 --> 12:29.280
There is distal, a distal zone that's kind of, that's lateral.

12:29.280 --> 12:33.800
So that's getting connections from, it could be from another layer, it could be from within

12:33.800 --> 12:36.320
a layer itself, but that provides context.

12:36.320 --> 12:39.360
And these both provide context for the proximal input.

12:39.360 --> 12:43.280
The proximal input is really the driver input.

12:43.280 --> 12:46.880
That's typically coming from the direction of the senses.

12:46.880 --> 12:51.240
And so that's, that's like the sensory input that we need to understand, we need to process.

12:51.240 --> 12:55.800
And the pyramidal neurons do that in the context of these other zones and the context of distal

12:55.800 --> 12:57.880
input and apical input.

12:57.880 --> 13:01.680
So those are the two things I think we're really missing from that point neuron model.

13:01.680 --> 13:07.760
So I get that the neuroscience research has identified these things in human biology,

13:07.760 --> 13:13.040
but it's not clear to me how we've demonstrated that those are required for intelligence, or

13:13.040 --> 13:19.240
even that those things can't be approximated with artificial neural networks as we currently

13:19.240 --> 13:23.760
know that like the last thing, the different zones, you know, maybe think of well, you

13:23.760 --> 13:26.560
know, we just have different inputs and different weights, right?

13:26.560 --> 13:33.840
And then as far as predictions are concerned, if we're able to predict at a network level,

13:33.840 --> 13:38.480
you know, who's to say that the neuron itself has to have that predictive state in order

13:38.480 --> 13:40.240
to create intelligence?

13:40.240 --> 13:47.200
Well, it's, it's true that current artificial neural networks in deep learning could potentially

13:47.200 --> 13:51.560
put together models that replicate the parts of the things about the neuron that we're

13:51.560 --> 13:53.080
saying are required for intelligence.

13:53.080 --> 13:54.080
I think we use it all.

13:54.080 --> 13:55.040
It's for prediction all the time.

13:55.040 --> 13:56.040
Yeah.

13:56.040 --> 14:00.480
But I don't know that that's, it doesn't feel natural to me.

14:00.480 --> 14:05.840
And think about this recently, there's been this big discussion in the deep learning community

14:05.840 --> 14:11.920
about back propagation because Jeff Hinton has recently said, let's give up on back propagation,

14:11.920 --> 14:15.520
go back to the drawing board and try and figure out what's really going on.

14:15.520 --> 14:17.520
We did that 12 years ago.

14:17.520 --> 14:19.600
So we never tried back propagation.

14:19.600 --> 14:23.320
We've always tried to do this because we don't see back propagation happening in the brain.

14:23.320 --> 14:28.520
And for the, for the longest time, you know, Hinton and, and Ben Gio were insisting that back

14:28.520 --> 14:31.280
propagation is happening in the brain, we just don't see it.

14:31.280 --> 14:34.320
So I think, so that's kind of a move in, in our direction.

14:34.320 --> 14:39.160
And even from like the deep mind crew, they recently had this blog post about how important

14:39.160 --> 14:43.560
neuroscience is to contributing to artificial intelligence.

14:43.560 --> 14:46.520
It feels to me like the community started to move in our direction.

14:46.520 --> 14:52.120
And maybe they will be able to hack these properties that we're saying we need in, in

14:52.120 --> 14:55.480
the neuron model into deep learning systems that, that could happen.

14:55.480 --> 14:59.160
But I don't think that it will happen without them doing something to incorporate those

14:59.160 --> 15:00.160
ideas.

15:00.160 --> 15:06.000
And Ben Gio, just this week published a paper that talked about, I forget the exact

15:06.000 --> 15:07.000
title.

15:07.000 --> 15:08.000
Something about consciousness.

15:08.000 --> 15:09.000
I don't know if you saw that.

15:09.000 --> 15:10.920
I did not see that.

15:10.920 --> 15:17.120
And it was controversial might be strong, but it raised a lot of questions because he,

15:17.120 --> 15:22.840
you know, proposed that somehow we need to take into account some notion of consciousness

15:22.840 --> 15:23.840
in our models.

15:23.840 --> 15:28.000
But the paper didn't present any experimental results or whatever it was just like a prod

15:28.000 --> 15:29.000
to the community.

15:29.000 --> 15:33.440
Anything about consciousness is going to be controversial because what is consciousness

15:33.440 --> 15:34.440
Sam?

15:34.440 --> 15:35.440
Right.

15:35.440 --> 15:36.440
What is intelligence?

15:36.440 --> 15:37.440
Exactly.

15:37.440 --> 15:42.920
I was asking people in the audience who believes humans are intelligent and they all raise their

15:42.920 --> 15:43.920
hands.

15:43.920 --> 15:44.920
Who believes chimpanzees are intelligent?

15:44.920 --> 15:49.200
And they just go down the evolutionary ladder and see hands going down.

15:49.200 --> 15:53.320
By the end of it, I'm asking who thinks plants are intelligent and there's still one or

15:53.320 --> 15:56.040
two people that think plants are intelligent and they may be right.

15:56.040 --> 15:57.640
You know, I don't, we don't know.

15:57.640 --> 15:58.640
Yeah.

15:58.640 --> 16:00.160
Permissium, whatever.

16:00.160 --> 16:02.200
So there's a lot of disagreement.

16:02.200 --> 16:05.480
The thing is everybody believes humans are intelligent.

16:05.480 --> 16:06.960
So at least we can start with that.

16:06.960 --> 16:11.560
So we have been, and I think by association, we can include most primates and not two because

16:11.560 --> 16:14.400
they have the same neocortical structure that we have.

16:14.400 --> 16:18.800
So we've focused that on that neuroscience and what we think, what we all know is intelligent

16:18.800 --> 16:22.600
and that's the neocortex of the mammalian brain.

16:22.600 --> 16:26.880
So you started off talking about like level setting on intelligence and just how open

16:26.880 --> 16:31.160
and that is and then talked about kind of the evolution of the neuron.

16:31.160 --> 16:34.240
Like how do you get from there to systems?

16:34.240 --> 16:35.240
Oh, okay.

16:35.240 --> 16:40.480
So you think about the pyramidal neuron, like I said, and it has these integration zones.

16:40.480 --> 16:44.440
It's hard to visualize without a picture, but Francisco said the same thing.

16:44.440 --> 16:45.440
I know.

16:45.440 --> 16:46.440
I know.

16:46.440 --> 16:49.720
But my talk will be online at some point.

16:49.720 --> 16:54.320
So if you could find Matt Taylor talking strangely, but I've got a bunch of drawings and

16:54.320 --> 16:55.320
stuff.

16:55.320 --> 16:57.720
But if you look at a pyramidal neuron and it's got, we'll link to it if you should

16:57.720 --> 16:58.720
us a link.

16:58.720 --> 16:59.720
Okay.

16:59.720 --> 17:02.800
It has these integration zones, distal, which is lateral to the side, proximal, which comes

17:02.800 --> 17:05.760
from below apocode, which comes from on top.

17:05.760 --> 17:09.280
The cortex has this homogenous structure.

17:09.280 --> 17:14.520
If you took your neocortex and you unwrinkled it and unfolded it and flattened it all out,

17:14.520 --> 17:16.040
it's a sheet of cells.

17:16.040 --> 17:19.640
It's about the size of a dinner napkin, it's about the thickness of a dinner napkin.

17:19.640 --> 17:21.040
And it's homogenous throughout.

17:21.040 --> 17:26.280
It has the same structure and what that, there's sort of like this computational unit in

17:26.280 --> 17:28.640
the cortex called a cortical column.

17:28.640 --> 17:32.840
This is something that is more recent of a neuroscience discovery.

17:32.840 --> 17:36.840
We've known for like a hundred years that the cortex had layers, like there is these

17:36.840 --> 17:41.840
distinct little layers in the sheet and that their structure was different enough that

17:41.840 --> 17:45.200
we thought, well, they're doing different things, but we're not exactly sure what they're

17:45.200 --> 17:46.200
doing.

17:46.200 --> 17:50.120
Now that we know they're not just layers, there's also columns.

17:50.120 --> 17:54.120
And we can take that, each column and say, okay, each one of these is some individual

17:54.120 --> 17:56.560
computational unit, right?

17:56.560 --> 18:00.840
And maybe they can share their computation or the output of their computations with their

18:00.840 --> 18:02.560
neighbors and stuff.

18:02.560 --> 18:08.640
So this idea that a column can have layers within it and every layer is full of these

18:08.640 --> 18:10.560
primal neurons, okay?

18:10.560 --> 18:16.080
So imagine a column that's cut up into layers and this is sort of a cylindrical column,

18:16.080 --> 18:17.520
cut up in layers.

18:17.520 --> 18:21.600
Each one of those layers is full of primal neurons that have these integration zones,

18:21.600 --> 18:26.480
apical up and down to the north sort of and proximal to the south and distal to the side.

18:26.480 --> 18:32.200
So each layer itself has the same integration zone properties as an individual neuron

18:32.200 --> 18:35.080
because they're all oriented in exactly the same way.

18:35.080 --> 18:39.040
So you can treat that layer as a computational unit.

18:39.040 --> 18:43.800
So a layer gets proximal input, a bunch of proximal input that all gets piped into its

18:43.800 --> 18:45.720
neurons in different ways.

18:45.720 --> 18:51.280
From some space that's representing generally some spatial sensory features changing over

18:51.280 --> 18:53.120
time or something like that.

18:53.120 --> 18:57.960
So you can think of the layer itself as a computational unit.

18:57.960 --> 19:01.560
Depending on where it gets its proximal input, where it gets its distal input and its

19:01.560 --> 19:04.200
able to input, it does different things.

19:04.200 --> 19:09.560
And also there's a bunch of different layers in the cortex, somewhere between six and

19:09.560 --> 19:11.880
ten, depending on which neuroscience you talk to.

19:11.880 --> 19:15.160
But each one of those layers is structured a little bit differently too.

19:15.160 --> 19:20.080
So there's some minor deviation in the organization of those primal neurons within layers that

19:20.080 --> 19:25.360
also give them a little bit of different computational aspects, organization in what sense.

19:25.360 --> 19:30.120
For example, we have these algorithms that we're saying are happening in these layers.

19:30.120 --> 19:34.920
One is called a spatial pooling algorithm that takes some input and kind of spreads it,

19:34.920 --> 19:38.120
normalizes it while retaining the semantics of the input.

19:38.120 --> 19:42.200
And these create these mini column structures of neurons.

19:42.200 --> 19:44.120
And some layers have this.

19:44.120 --> 19:49.140
And typically the distal connections from each one of those neurons as it's perceiving

19:49.140 --> 19:53.160
proximal input, they start connecting to each other over time.

19:53.160 --> 19:57.480
When you take that distal input to a layer and you say, okay, we're not going to get that

19:57.480 --> 20:02.240
distal input from somewhere else, we're going to have all of the primal neurons within

20:02.240 --> 20:05.560
the layer give each other distal input.

20:05.560 --> 20:09.560
What you're doing is just naturally creating a temporal context.

20:09.560 --> 20:15.800
Because when your only context is some input is what state you've been in in the past,

20:15.800 --> 20:18.160
then that's the temporal context.

20:18.160 --> 20:22.120
If you're getting that input from somewhere else, who knows, that context could mean any

20:22.120 --> 20:23.320
number of things.

20:23.320 --> 20:27.600
But if you're just giving yourself context, that's you're looking at your own past.

20:27.600 --> 20:31.820
That layer has context of its own history when you loop them back to itself.

20:31.820 --> 20:33.760
So that's one of the core things that we discovered.

20:33.760 --> 20:36.120
We call this a temporal memory algorithm.

20:36.120 --> 20:43.440
And it relies on these little mini column structures that takes the input, the bits of input

20:43.440 --> 20:48.640
that are coming in from some sensory organ or perhaps from another part of the cortex,

20:48.640 --> 20:54.080
normalizes it into these column activations and then activates cells within each column

20:54.080 --> 20:57.120
based upon the distal context that it's getting.

20:57.120 --> 21:00.920
So what you get is it's starting to tie sequences together.

21:00.920 --> 21:07.240
When you see a pattern repeating over and over, over and over, you get these distal connections

21:07.240 --> 21:09.240
that are being reinforced.

21:09.240 --> 21:13.040
Because they see the pattern and the distal connection will create a connection to the active

21:13.040 --> 21:17.560
cells that it just saw that represented the previous spatial input.

21:17.560 --> 21:20.040
And then we get another input and there may be a prediction.

21:20.040 --> 21:22.840
So I saw that last time I'm going to be next.

21:22.840 --> 21:27.160
So it makes a prediction and if it's right and the next input activates a column that

21:27.160 --> 21:30.600
that that cells in, then if it comes active, it was a correct prediction.

21:30.600 --> 21:35.020
Well, the context you're creating for me is how I felt when Francisco was explaining

21:35.020 --> 21:36.020
something.

21:36.020 --> 21:37.020
It's like, whoa.

21:37.020 --> 21:39.200
It's a lot easier with visuals.

21:39.200 --> 21:43.720
And hence, that's why I created this bunch of videos on our YouTube channel to try and

21:43.720 --> 21:46.480
explain it all visually.

21:46.480 --> 21:52.760
So you explain kind of the microstructure than the macrostructure and then like what's

21:52.760 --> 21:53.760
next?

21:53.760 --> 21:57.240
So Strange Loop is a developer conference, like how do you get from there to, okay, how do

21:57.240 --> 21:58.240
I build something?

21:58.240 --> 22:00.920
Well, there's two questions there, I guess.

22:00.920 --> 22:05.320
Strange Loop is a developer conference, however, it's also like a weird conference, you

22:05.320 --> 22:06.920
know, it's all granted.

22:06.920 --> 22:07.920
It is.

22:07.920 --> 22:11.080
And so you can like, yes, it's very eclectic.

22:11.080 --> 22:15.360
So you can get in, if you have something that's like on the fringe, but very interesting,

22:15.360 --> 22:16.360
you can get in and talk to them.

22:16.360 --> 22:18.360
So I think that's why I got this talk.

22:18.360 --> 22:25.640
But there's, as far as from a, but still, I mean, in addition to developing IP and all

22:25.640 --> 22:31.520
of that, as I understand it, Numenta's company offers tools that allow people to actually

22:31.520 --> 22:32.520
use this stuff.

22:32.520 --> 22:33.720
Is that correct or no?

22:33.720 --> 22:35.240
All open source tools.

22:35.240 --> 22:40.440
So all of our code is open source and anybody can try and use it if they want to.

22:40.440 --> 22:46.280
I've created a lot of tutorials and code samples and I try and make it as approachable as possible

22:46.280 --> 22:47.280
for our community.

22:47.280 --> 22:52.360
We've got a very active forum, lots of discussions about the theory and about code and stuff.

22:52.360 --> 22:59.480
So as a user of these open source tools and things like, am I, do I need to think about

22:59.480 --> 23:05.160
columns and dendrites and all of that stuff or am I thinking about other representations?

23:05.160 --> 23:06.920
So it can go either way.

23:06.920 --> 23:08.360
It depends on what you're trying to do.

23:08.360 --> 23:12.880
So we have a pretty diverse and eclectic community that are interested in this, typically

23:12.880 --> 23:16.520
people who are really interested in how the brain works or, you know, let's say they

23:16.520 --> 23:17.880
can be a little off.

23:17.880 --> 23:22.960
But I mean, they're always very smart and inquisitive and curious and it, it amazes me the types

23:22.960 --> 23:27.400
of things that people try and do with, with our stuff and I always encourage it and I

23:27.400 --> 23:28.400
was like, yeah, try it.

23:28.400 --> 23:29.400
Give it a try.

23:29.400 --> 23:30.400
Who knows?

23:30.400 --> 23:31.400
We don't know what it's going to have.

23:31.400 --> 23:36.280
It's a software that we open source is called the new pick, the Numenta platform for intelligent

23:36.280 --> 23:37.680
computing.

23:37.680 --> 23:43.720
And we just released 1.0 of that a few months ago and that includes up to what I just talked

23:43.720 --> 23:48.800
about, the temporal memory part of it, and a few years back after we, you know, went

23:48.800 --> 23:52.640
through this research cycle and made the temporal memory discovery, that was a big discovery

23:52.640 --> 23:57.920
for us to see how sequences were memorized in the brain or in the cortex.

23:57.920 --> 24:02.200
We kind of just dumped it all open source and we started like building these potential

24:02.200 --> 24:05.400
sample, you know, which is brainstormed about what could we make with this that people

24:05.400 --> 24:06.800
might want to use.

24:06.800 --> 24:10.640
And we made all these sample applications, there's one that was like rogue human behavior

24:10.640 --> 24:15.200
detection, which is something you can install on a computer and it monitors the different

24:15.200 --> 24:20.320
metrics that are coming out of the computer over days and weeks and given indication

24:20.320 --> 24:21.800
about a user's behavior.

24:21.800 --> 24:26.400
Are they behaving oddly or differently based on the time of day and the thing that they're

24:26.400 --> 24:29.200
doing and the metrics that are coming out of the computer?

24:29.200 --> 24:31.280
So that's a sort of thing that you can do.

24:31.280 --> 24:40.240
We also had a IT analytics program that hooked up to AWS and we actually licensed that to

24:40.240 --> 24:42.040
another company called crock.

24:42.040 --> 24:48.760
And so they are actively selling that to IT companies that have a bunch of servers on Amazon

24:48.760 --> 24:53.760
and it will automatically like through CloudWatch connect to all the different metrics coming

24:53.760 --> 24:57.600
out of your servers and it will create models for all of them and it'll just start streaming

24:57.600 --> 25:01.520
the data into them and you don't really have to do anything, they're all sort of pre-configured.

25:01.520 --> 25:03.680
And then it'll give you anomaly indications over time.

25:03.680 --> 25:08.640
So after it's seen that server data for a while, it gets an idea of what's normal and

25:08.640 --> 25:13.600
what's not normal, then it notifies you that something's wrong with the server.

25:13.600 --> 25:17.440
It doesn't know what's wrong with the server, but it can tell you that something abnormal

25:17.440 --> 25:23.680
is happening and even with this server and this server and the combination of those.

25:23.680 --> 25:27.920
So there's anywhere that there's streaming analytics that you need anomaly detection,

25:27.920 --> 25:33.120
I think that there's a potential application for what we have right now with new pick 1.0.

25:33.120 --> 25:38.800
There's also this really interesting thing that I think is still a big opportunity for

25:38.800 --> 25:40.960
people who want to try and build something novel with this.

25:41.600 --> 25:47.440
We figured out a way to encode geospatial location into a format.

25:47.440 --> 25:52.800
Remember when Francisco and you talked a lot about SDRs about sparse distributed representations.

25:52.800 --> 25:55.680
So corticals, I was all about that, they call them semantic fingerprints.

25:57.040 --> 26:00.400
We found a way to encode location information like latitude,

26:00.400 --> 26:03.040
longitude, altitude, into an SDR.

26:03.040 --> 26:09.200
So we can take something that moves through time and space and give the algorithms,

26:09.200 --> 26:14.960
the intelligence algorithms a way to understand the patterns in the movement of that object.

26:15.600 --> 26:21.840
So for an example that I always do is I go walk my dogs on the same dog walking around every day.

26:21.840 --> 26:26.800
And if I take a tracker with me and then I go put all my points back through the algorithm,

26:26.800 --> 26:29.840
the first time it sees the walk, it's like all anonymous.

26:29.840 --> 26:33.280
It doesn't think, none of it's familiar because it's brand new.

26:33.280 --> 26:35.440
The second time I do it, it's a little bit less familiar.

26:35.440 --> 26:37.680
The third time I do it, it's like no big deal.

26:37.680 --> 26:38.880
This is normal, right?

26:39.520 --> 26:42.560
As soon as I deviate from the path that I've taken,

26:42.560 --> 26:45.040
and even if I just go walk on the other side of the street,

26:45.040 --> 26:47.360
or if my dogs decide they don't want to stop at that tree,

26:47.360 --> 26:49.200
they want to stop at some other tree,

26:49.200 --> 26:52.080
I get anomaly indications coming from my path.

26:52.080 --> 26:55.680
So I think this has big applications in fields like logistics,

26:55.680 --> 26:58.640
air traffic control, human tracking, pet tracking,

26:58.640 --> 27:03.120
stuff like that, where you've got normal routes of things that normally happen.

27:03.120 --> 27:06.240
And you don't necessarily, to the T, want to say,

27:06.240 --> 27:09.280
oh, if they deviate right now, or if they're not at this point at this time,

27:09.280 --> 27:13.040
there's something wrong, you just want to get an idea of their general movement,

27:13.040 --> 27:14.960
whether it's strange or not, or whether it's,

27:14.960 --> 27:19.280
have been seen or not, then it can do that sort of thing. That's really interesting.

27:19.280 --> 27:23.920
Certainly for the network and server anomaly detection,

27:23.920 --> 27:26.480
and the example you gave before that,

27:26.480 --> 27:31.280
there are things that you can do with a variety of different techniques.

27:31.920 --> 27:40.240
Are there things that you've found that either the approach you take,

27:40.240 --> 27:44.800
because of the approach you take, it's just kind of besting class,

27:44.800 --> 27:49.120
or if you need to do X, Y, Z, this is the best way to do it,

27:49.120 --> 27:54.960
or either from a complexity of creating the solution,

27:54.960 --> 27:59.360
or computational costs, or some other metric.

27:59.360 --> 28:01.200
Well, we wondered the same thing,

28:01.200 --> 28:07.440
but the problem we had several years ago is that there are no standard benchmarks

28:07.440 --> 28:11.040
for streaming temporal anomaly detection,

28:11.040 --> 28:13.760
or just for temporal anomaly detection.

28:13.760 --> 28:15.920
Most of the benchmarks are on spatial data,

28:15.920 --> 28:20.080
and most of the machine learning techniques work on spatial data online.

28:20.080 --> 28:23.360
We didn't find anything so we could compare what we did with what,

28:23.360 --> 28:28.320
like LSTM, for example, has some abilities to do temporal analysis on things,

28:28.320 --> 28:31.680
it'd be sort of in batches that move along.

28:31.680 --> 28:34.240
So we created a benchmark we called the anomaly,

28:34.240 --> 28:40.320
the numente anomaly benchmark, and we've set up R as one of them in the running,

28:40.320 --> 28:43.600
and we set up an LSTM one, and we set up, there's one from Twitter,

28:43.600 --> 28:46.400
there's one from Etsy, that does streaming anomaly detection,

28:46.400 --> 28:49.360
like they've got open source projects that do that sort of thing.

28:49.360 --> 28:54.000
And we created these input data sets, things like how many taxi calls

28:54.000 --> 28:58.240
where they're in New York City over an entire period of time or something.

28:58.240 --> 29:01.280
And you can look at that data, and you can say something weird happened there,

29:01.280 --> 29:05.120
for sure, and you go look it up and you're like, oh, there was a big game in town,

29:05.120 --> 29:07.920
or there's stuff like that, you can find that data.

29:07.920 --> 29:10.800
So we'd find data sets like that that had a good amount of data,

29:10.800 --> 29:13.760
and had obvious anomalies that were labeled and marked,

29:13.760 --> 29:16.000
and we've run all of these algorithms against them,

29:16.000 --> 29:19.600
and score them based on how well they detected the anomaly.

29:19.600 --> 29:23.680
And waiting it, I think we waited it pretty heavily on,

29:23.680 --> 29:27.040
not providing false positives, I can't remember exactly,

29:27.040 --> 29:33.440
but it's open source, it's on GitHub, it's a nimenta-slash-nab for nimenta-anomaly-finchmark.

29:33.440 --> 29:36.640
Okay, we have that at least, and of course we're the winner,

29:36.640 --> 29:40.160
because we always mean, but we had like this contest,

29:40.160 --> 29:42.960
we're like, if anybody beat us at this and somebody came and beat us at it,

29:42.960 --> 29:45.520
we're like, okay, we're gonna fix it, so we fixed it,

29:45.520 --> 29:50.320
we're like, we're beating again, but there's always some tweaking that you can do,

29:50.320 --> 29:53.040
you know, to try and get that last few percent.

29:53.040 --> 29:58.880
Ah, okay, it kind of leads me with an impression that, like, this is a tool that,

29:58.880 --> 30:03.040
you know, we've, or a set of tools, that you have a strong feeling that,

30:03.920 --> 30:09.040
you know, closely models the inner workings of the brain as we understand it,

30:09.040 --> 30:14.000
and that over time, that will lead to, you know, I'm assuming you're banking on,

30:14.000 --> 30:18.640
like, order of magnitude, you know, capabilities over current approaches.

30:18.640 --> 30:21.520
Like, the things you can do using nimenta and the things you can do,

30:21.520 --> 30:24.160
using other things will diverge over time, but today,

30:24.880 --> 30:28.560
it doesn't sound like there's a, you know, a bang on the table, like,

30:28.560 --> 30:32.880
if you need to do x, y, z, these tools will get you there,

30:32.880 --> 30:35.440
you know, a hundred times faster and a hundred times cheaper,

30:35.440 --> 30:38.960
or even 10, or, you know, it sounds like, you know,

30:38.960 --> 30:42.560
it's an interesting approach and something that's worthwhile for people to learn,

30:42.560 --> 30:45.440
and take a look at and understand and thinking around.

30:45.440 --> 30:47.440
But it's, we don't have a killer.

30:47.440 --> 30:48.320
There's no killer.

30:48.320 --> 30:50.000
I guess that's what I'm, what I'm getting.

30:50.000 --> 30:53.280
Yeah, there's no killer app, but we're patient, too.

30:53.280 --> 30:56.960
There's a lot of things about the brain that we don't, we still don't understand.

30:56.960 --> 30:57.280
Right.

30:57.280 --> 31:02.880
And what we have currently in nimenta 1.0 is just, you know, temporal memory stuff.

31:02.880 --> 31:05.600
All of our other work that we're doing is in research repository,

31:05.600 --> 31:08.320
is that kind of attach on top of that.

31:08.320 --> 31:11.280
So we're taking those core algorithms, which aren't going to change,

31:11.280 --> 31:13.440
and we're building new and different things with them,

31:13.440 --> 31:16.160
because the core algorithms in your brain don't change,

31:16.160 --> 31:19.840
but we discover that it can do lots of different things with those core algorithms.

31:19.840 --> 31:20.240
Right.

31:20.240 --> 31:24.720
So we're building structures now, because we think we understand how

31:24.720 --> 31:28.400
sensory motor integration happens with sensory input and movement.

31:28.960 --> 31:32.720
But it is the integration of two layers in one of those columns.

31:32.720 --> 31:34.800
Remember I told you about the layers having these integrations.

31:34.800 --> 31:35.600
Yeah.

31:35.600 --> 31:40.960
So we could have one layer that is running the same temporal memory algorithm that I described

31:40.960 --> 31:43.520
earlier, with the mini columns and everything.

31:43.520 --> 31:45.680
But we don't send it its own distal input.

31:45.680 --> 31:47.600
We don't give it a temporal context.

31:47.600 --> 31:48.160
Okay.

31:48.160 --> 31:51.040
We can pipe in the context, the distal connection,

31:51.040 --> 31:52.320
comes from somewhere else in the brain.

31:52.320 --> 31:54.320
It comes from a different layer down.

31:54.320 --> 32:00.720
And if we assume that the output of that layer is providing us with

32:00.720 --> 32:04.480
location information associated with a sensory input

32:05.200 --> 32:07.920
that's proximal coming up to the layer from the bottom.

32:07.920 --> 32:10.640
So that's the driver signal as this sensory input.

32:10.640 --> 32:14.720
The distal signal is going to represent the object being touched

32:14.720 --> 32:19.040
and what location on the object that sensory feature was sensed.

32:19.760 --> 32:25.600
Then we can have a layer that can represent every object we've ever

32:25.600 --> 32:29.360
touched and what sensory input we've felt where on it.

32:29.920 --> 32:34.400
And so that layer now provides that information to another layer,

32:34.400 --> 32:35.600
which we call an output layer.

32:36.320 --> 32:38.960
This output layer has a little bit of a different structure,

32:38.960 --> 32:41.680
because it doesn't have the mini columns like the one underneath it.

32:41.680 --> 32:46.960
But it represents, over time, a library of every object we've ever learned.

32:46.960 --> 32:50.560
So we can train this thing and say, okay, this is a coffee cup.

32:50.560 --> 32:51.760
Touch it all over the place.

32:51.760 --> 32:52.480
Right, right?

32:52.480 --> 32:52.800
Okay.

32:52.800 --> 32:53.840
Here's a banana.

32:53.840 --> 32:55.200
Touch it all over the place.

32:55.200 --> 32:59.920
And we can build a library of objects that that top layer represents.

32:59.920 --> 33:03.280
So the bottom layer is basically just going to represent all the sensory input

33:03.280 --> 33:06.400
you've felt on every location on every object that you've touched.

33:06.400 --> 33:08.720
And this is the temporal memory concept.

33:08.720 --> 33:11.840
It's the temporal memory concept, but it's not doing temporal memory anymore.

33:11.840 --> 33:16.000
It's doing sensory feature and location association,

33:16.000 --> 33:18.080
just because we've changed the distal input.

33:18.080 --> 33:20.080
So it's no longer giving itself distal input.

33:20.080 --> 33:21.440
It's getting it from somewhere else.

33:21.440 --> 33:22.960
And it does something entirely different.

33:22.960 --> 33:25.120
And so it sounds like the idea there is,

33:25.120 --> 33:28.400
like if you think about using deep learning,

33:28.400 --> 33:33.520
object recognition, like our best guess at the way the different layers work now.

33:33.520 --> 33:37.360
So you've got layers that kind of figure out edges and layers that figure out colors.

33:37.360 --> 33:41.840
And so, you know, when the inputs, the banana,

33:41.840 --> 33:45.760
you know, we'll get kind of the curvy layer firing and the yellow layer.

33:45.760 --> 33:48.080
So that's kind of the deep learnings couldn't do.

33:48.080 --> 33:51.280
No, but I'm saying what you're describing sounds like,

33:52.080 --> 33:54.720
you know, maybe in the, kind of in the internals,

33:54.720 --> 33:57.840
is capturing a richer representation of these various things.

33:57.840 --> 33:59.280
That's clear what the big difference is.

33:59.280 --> 34:01.760
Is our model incorporates movement?

34:02.400 --> 34:04.080
And that's the big difference.

34:04.080 --> 34:08.240
So can you name anything that is intelligent that cannot move?

34:08.240 --> 34:09.040
Nothing comes.

34:09.040 --> 34:10.880
I've never, nobody ever does.

34:10.880 --> 34:13.680
Because there's nothing intelligent that can't move.

34:13.680 --> 34:14.320
Yeah.

34:14.320 --> 34:17.680
So we believe that's a core feature of intelligence.

34:17.680 --> 34:21.600
The ability to interact with your environment has to be baked in

34:21.600 --> 34:23.520
to the architecture of the intelligent system.

34:23.520 --> 34:24.960
It's not something that you can just add.

34:24.960 --> 34:28.160
You can't just add behavior to a system that you're building.

34:28.160 --> 34:30.720
It has to be baked into the flow of information.

34:30.720 --> 34:34.560
So like I said, when, when you move your finger to touch an object,

34:34.560 --> 34:36.560
you know where your finger is going to move.

34:36.560 --> 34:38.320
Because you just commanded it to move there.

34:38.320 --> 34:40.560
So that information is available to your brain.

34:40.560 --> 34:42.160
That loop has to be baked in.

34:42.160 --> 34:43.760
So that every time you touch something,

34:43.760 --> 34:46.720
you know where it's going and you know what you expect to feel.

34:46.720 --> 34:48.960
If you don't feel that, something's wrong.

34:48.960 --> 34:49.280
Okay.

34:50.160 --> 34:53.360
And so Matt is demonstrating all this with a glass of water.

34:53.360 --> 34:56.400
And we've been experimenting with a video camera set up here.

34:56.400 --> 35:00.480
So we may be able to show the visual aids

35:00.480 --> 35:02.000
of with motion.

35:02.000 --> 35:04.000
It helps with the visual aids.

35:04.000 --> 35:06.720
But like I said, if you want visuals, go to nemento.org.

35:06.720 --> 35:07.200
Yeah.

35:07.200 --> 35:08.240
I got lots of stuff.

35:08.240 --> 35:08.720
Nice.

35:08.720 --> 35:09.040
Nice.

35:09.040 --> 35:09.840
Awesome.

35:09.840 --> 35:12.320
Well, anything else that you covered in your talk

35:12.320 --> 35:14.400
or last kind of final thoughts that you want to leave us with?

35:15.200 --> 35:19.680
I guess I just want to emphasize that we have a really nice

35:19.680 --> 35:20.400
community.

35:20.400 --> 35:21.920
And like I'm the community manager.

35:21.920 --> 35:23.280
So of course, we're going to say that.

35:23.280 --> 35:26.080
But honestly, there's some really bright people that have even shown up

35:26.080 --> 35:29.760
just in the past year that are doing some really interesting things with HTML.

35:29.760 --> 35:31.600
All of our papers are open access.

35:31.600 --> 35:34.880
So all this theory is everything that we theorize about.

35:34.880 --> 35:37.600
We write papers about and we put it out there.

35:37.600 --> 35:39.040
And we do it with code.

35:39.040 --> 35:40.640
So we're like, here's a paper.

35:40.640 --> 35:41.840
Here's a simulation.

35:41.840 --> 35:42.560
Here's the code.

35:42.560 --> 35:45.120
You can run it yourself if you want to try to run it yourself.

35:45.120 --> 35:47.360
So if you don't believe us, you can try it yourself.

35:47.360 --> 35:50.960
And there's lots of people in our community that have decided they're going to

35:50.960 --> 35:55.040
write their own HTML system and their own favorite language of their own environment.

35:55.040 --> 35:57.760
So there's a lot of people doing new and interesting things.

35:57.760 --> 35:59.680
They're creating their own visualizations.

35:59.680 --> 36:01.840
Last one was thesis from this guy in Turkey.

36:01.840 --> 36:07.520
He did this amazing sensory motor sort of simulation in a 3D game environment where he's

36:07.520 --> 36:12.560
got a player trying to find a point and he wrote his whole thesis on his brain.

36:12.560 --> 36:16.800
But he used our theory and then attached some stuff on top.

36:16.800 --> 36:20.800
He theorized further and he's like, well, what have I got this and this and this?

36:20.800 --> 36:26.400
And trying to create a more complete idea of the brain, not just the cortex.

36:26.400 --> 36:28.960
Because we're really just working on cortex right now.

36:28.960 --> 36:32.960
And he's trying to incorporate some other things like some like real behaviors or real

36:32.960 --> 36:38.560
drivers of what is the motivation for that agent that is running the intelligence.

36:38.560 --> 36:44.080
And we're not quite there, but we're focusing our research right now on location,

36:44.080 --> 36:45.760
like that location signal I'm telling you about.

36:45.760 --> 36:49.040
We've got a really good idea of how that location signal is generated.

36:49.040 --> 36:50.640
And it's super interesting.

36:50.640 --> 36:55.520
Like the way that your brain rocks location of things is amazing.

36:55.520 --> 36:58.640
You probably don't, I don't have knowledge to explain it.

36:58.640 --> 37:02.320
But it's about grid cells, location cells and place cells and stuff like that.

37:02.320 --> 37:05.600
If anybody wants to go research that, there's some really interesting neuroscience papers

37:05.600 --> 37:07.200
coming out about grid cells.

37:07.200 --> 37:09.040
Okay, for example, I'll give you a little example.

37:09.040 --> 37:15.520
If you put a mouse in a box and you let it run around the box and you're monitoring its neurons,

37:15.520 --> 37:19.920
you'll see as it runs around the box and you trace where it goes,

37:19.920 --> 37:23.040
certain neurons will fire when it's in certain places.

37:23.040 --> 37:28.240
And those fire, and you can identify those cells that are greening whenever it's in place.

37:28.240 --> 37:29.360
Yeah, X, Y.

37:29.360 --> 37:31.200
Yeah, the specific neurons going to fire?

37:31.200 --> 37:31.680
Yes.

37:31.680 --> 37:32.400
Wow.

37:32.400 --> 37:36.240
And if you look at it, it forms this hexagonal grid.

37:36.240 --> 37:41.680
So there's this hexagonal pattern of neurons that are firing as you move through space,

37:41.680 --> 37:45.360
representing where you're at in the space that you're occupying.

37:46.000 --> 37:52.080
And we think that that interplay of neurons and that idea of neurons representing locations

37:52.080 --> 37:58.000
in space plays out at a bigger level to even represent objects in space too.

37:58.000 --> 37:58.240
Okay.

37:58.240 --> 38:03.120
Like you have an allocentric representation of any object that you can imagine.

38:03.120 --> 38:07.520
Allocentric meaning not related to where you are, not egocentric,

38:07.520 --> 38:09.760
but just like imagine a cup.

38:09.760 --> 38:11.840
I mean, that's an object that you have.

38:11.840 --> 38:15.600
And if you like used its center of gravity for whatever, as its center,

38:15.600 --> 38:20.080
you could define it entirely based upon all the sensory input that you've ever received

38:20.080 --> 38:23.520
about those objects that you've felt or seen or whatever.

38:23.520 --> 38:26.800
And we think that that has something to do with grid cells.

38:26.800 --> 38:32.720
That how those objects are stored, like the plate that how in 3D space they're defined

38:32.720 --> 38:36.080
is linked to the sensory input that we receive about them.

38:36.080 --> 38:40.560
And what cells are firing in space as we're imagining where we're touching on the object.

38:40.560 --> 38:40.800
Okay.

38:41.520 --> 38:42.080
Wow.

38:42.080 --> 38:43.760
Super, super interesting stuff.

38:44.400 --> 38:49.360
I will definitely make a note for folks to listen to the conversation with

38:49.360 --> 38:51.760
Francisco a couple of times before this one.

38:52.800 --> 38:55.760
Or maybe this one should be the prerequisite for that one, I don't know.

38:55.760 --> 38:58.560
Well, it's hopefully it's standalone, hopefully it's standalone.

39:00.080 --> 39:00.400
Awesome.

39:00.400 --> 39:01.360
Well, thanks so much, Matt.

39:01.360 --> 39:01.840
You're welcome.

39:01.840 --> 39:03.120
I appreciate the opportunity.

39:03.120 --> 39:03.760
Absolutely.

39:07.520 --> 39:08.560
All right, everyone.

39:08.560 --> 39:10.720
That's our show for today.

39:10.720 --> 39:15.360
Thanks so much for listening and for your continued feedback and support.

39:15.360 --> 39:19.920
For more information on Matt or any of the topics covered in this episode,

39:19.920 --> 39:24.080
head on over to twimlai.com slash talk slash 71.

39:25.040 --> 39:28.800
To follow along with our Strange Loop 2017 series,

39:28.800 --> 39:31.840
visit twimlai.com slash ST loop.

39:32.480 --> 39:36.800
Of course, you can send along your feedback or question via Twitter

39:36.800 --> 39:42.560
to at Twimlai or at Sam Charrington or leave a comment right on the show notes page.

39:42.560 --> 39:46.000
Thanks again to Nexosis for their sponsorship of the show.

39:46.000 --> 39:53.120
Check out twimlai.com slash talk slash 69 to hear my interview with the company founders.

39:53.120 --> 40:00.240
And visit nexosis.com slash twimble for more information and to try their API for free.

40:00.240 --> 40:12.720
Thanks again for listening and catch you next time.

