WEBVTT

00:00.000 --> 00:16.320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

00:16.320 --> 00:21.480
people, doing interesting things in machine learning and artificial intelligence.

00:21.480 --> 00:34.960
I'm your host, Sam Charrington, alright everyone, I am on the line with Ahmed El Gamal.

00:34.960 --> 00:40.360
Ahmed is a professor in the Department of Computer Science at Rutgers University, as well

00:40.360 --> 00:43.960
as director of the Art and Artificial Intelligence Lab there.

00:43.960 --> 00:47.440
Ahmed, welcome to this week in machine learning and AI.

00:47.440 --> 00:49.160
Thank you for inviting me.

00:49.160 --> 00:54.280
Absolutely, I'd like to get started by having you share a little bit about your background

00:54.280 --> 01:00.560
and what brought you to an interest in kind of the space of digital humanities and this

01:00.560 --> 01:07.560
confluence of art and AI and more broadly your interests in computer vision.

01:07.560 --> 01:14.720
Yes, so my career has been around artificial intelligence for a long time in particular

01:14.720 --> 01:21.360
computer vision, I'm a computer vision person, but also I have a long version for art, for

01:21.360 --> 01:22.840
long time.

01:22.840 --> 01:32.640
And as a computer vision person, I always find it interesting that we, the vision look

01:32.640 --> 01:41.720
at images and can you recognize cars and dogs and cats and men and women, he's like that.

01:41.720 --> 01:49.520
But obviously, if you look at an artwork, that's not what we're looking for, it's much

01:49.520 --> 01:50.520
deeper.

01:50.520 --> 01:56.720
There are layers and layer of understanding of artworks that we do as human when you look

01:56.720 --> 02:04.240
at artworks and also not only the understanding, but artwork effect us at the emotional level.

02:04.240 --> 02:10.800
So it's much deeper than just what we do with the science and the vision.

02:10.800 --> 02:19.400
That always intrigues me and that as an AI person, I also find it very, very interesting

02:19.400 --> 02:26.000
and very fascinating because I believe that AI is not only about driving a car or

02:26.000 --> 02:33.720
they just, in order to have a machine that is intelligent at the level of human intelligence,

02:33.720 --> 02:39.320
the machine has to be able to understand these creative products like art and music and

02:39.320 --> 02:45.480
literature and jokes and not only understand them, but be able to create them as well.

02:45.480 --> 02:52.000
And this is not something new actually since the beginning of AI and the tone of AI scientists

02:52.000 --> 02:59.200
has been trying to do that, I've been trying to create art and create music and understand

02:59.200 --> 03:03.960
art and music, but it has been very challenging.

03:03.960 --> 03:10.120
And I believe this is one of the important aspects to prove that AI is actually intelligent.

03:10.120 --> 03:19.920
One of your recent projects is a tool called ICANN which is focused on generating art.

03:19.920 --> 03:25.800
Can you talk a little bit about the genesis of that project and ultimately the capabilities

03:25.800 --> 03:26.800
of that tool?

03:26.800 --> 03:27.800
Yes.

03:27.800 --> 03:38.400
So in the last few years, GANN came around, which is an amazing tool to generate images

03:38.400 --> 03:42.640
in particular, given some training sets.

03:42.640 --> 03:50.640
So GANN by design is supposed to generate new samples of some distribution, if you give

03:50.640 --> 03:57.640
it images of pets, it will try to generate more and more images of pets, and some people

03:57.640 --> 04:04.680
have tried to use GANNs before to make art books like you give it portraits, Renaissance

04:04.680 --> 04:08.120
portraits, for example, and try to generate these portraits.

04:08.120 --> 04:12.520
However, our understanding of art is very different.

04:12.520 --> 04:16.360
I believe that art is about innovation and making something novel.

04:16.360 --> 04:21.000
So if you just generate samples from the same distribution of art that you give it, it's

04:21.000 --> 04:25.760
not going to be art, it's not going to be called art, that's imitation of art.

04:25.760 --> 04:29.160
So you have to be creative, you have to boost the machine to be creative.

04:29.160 --> 04:35.440
There's a whole field of stuff in the AI called computational creativity, where scientists

04:35.440 --> 04:42.400
in that GANN has been for a long time studying how to make the machine creative, how to make

04:42.400 --> 04:50.040
a creative process, whether you are creating music or art or visual art or literature,

04:50.040 --> 04:52.760
or drugs or anything like that.

04:52.760 --> 04:59.880
So my goal is really was how can you change GANN to make it creative, not just simulating

04:59.880 --> 05:08.720
the solution, but pushing the envelope of art to make something new, and then our long

05:08.720 --> 05:15.760
study of these issues in the lab helped us into completing this problem.

05:15.760 --> 05:21.800
So first we have to come up with a theory about how art evolved to start with, how artists

05:21.800 --> 05:26.800
make new art, obviously these are theories or several theories about that.

05:26.800 --> 05:32.960
The theory we use is coming from psychology by a scientist, his name is Colin Martin

05:32.960 --> 05:33.960
Dale.

05:33.960 --> 05:41.080
So here's the question, look at history of art, what happened, why artists keep evolving

05:41.080 --> 05:47.920
the styles as they go, why Baroque happened after Renaissance, and why impression is

05:47.920 --> 05:54.120
not happening in 19th century, and why Cubism came after that, and it is just random or

05:54.120 --> 06:00.880
there's something fundamental in how these styles illusion and evolve.

06:00.880 --> 06:09.800
So basically, artists always has to generate new ideas in their art to push against what

06:09.800 --> 06:15.200
is called habituation, because if we keep looking at the same art for a long time, it's

06:15.200 --> 06:22.640
not interesting anymore as an artist, as a viewer, so artists has to always innovate.

06:22.640 --> 06:29.520
However, this innovation has to come with what's called list effort, because if you innovate

06:29.520 --> 06:37.840
too much in your art, unlikely that people will accept that, will be shocking for people.

06:37.840 --> 06:44.400
So artists has to innovate to a certain degree, but with a limited amount of innovation, somehow

06:44.400 --> 06:48.760
push against habituation, according to the list effort principle.

06:48.760 --> 06:53.320
So that's exactly what we're trying to implement to make GANs creative.

06:53.320 --> 06:58.080
So we develop what's called creative adversarial network, it's a variant of GAN that's really

06:58.080 --> 07:02.280
designed to be implementing this concept.

07:02.280 --> 07:07.360
So how can we make it innovative, or push the novelty, there are different ways to do

07:07.360 --> 07:08.360
that.

07:08.360 --> 07:15.760
In particular, we look at what's called style ambiguity, which is a process that relates

07:15.760 --> 07:20.560
how artists break out style, so suppose, for example, you are an impressionist artist

07:20.560 --> 07:25.560
living in the 19th century, making a lot of impressionist work.

07:25.560 --> 07:30.440
After a while, you have exerted all possible things you can do within that style.

07:30.440 --> 07:36.440
You have painted all possible facades, all these possible length games, all possible scenes

07:36.440 --> 07:45.600
in that style, and at some time you already exerted all these possibilities, if you are a

07:45.600 --> 07:50.040
new artist, even if you like that style, and you innovate, there is no room for innovation

07:50.040 --> 07:51.640
anymore within that style.

07:51.640 --> 07:57.720
So that's basically the reason why most artists at certain points break out style and create

07:57.720 --> 07:58.720
something new.

07:58.720 --> 08:03.240
Coming up was something new, like, for example, Tzan at the time, or Van Gogh at the time

08:03.240 --> 08:09.600
trying to break out of ambitionism and do what's called after that was the ambitionism.

08:09.600 --> 08:12.360
So that's exactly what we're trying to do here.

08:12.360 --> 08:17.480
How can we digest what having artistry to certain points, to some certain points, and break

08:17.480 --> 08:19.560
out of style to create something new?

08:19.560 --> 08:24.200
Can I jump into it with a question, and this may lead you into what you actually did.

08:24.200 --> 08:31.600
But when I think about what you're proposing that we develop a model of the way artists

08:31.600 --> 08:38.680
generated, and in this case, the model kind of supposes that the artist lives within some

08:38.680 --> 08:49.240
style, but there are maybe kind of random pushes of the envelope, if you will, or maybe

08:49.240 --> 08:56.000
they're not random, but an individual artist innovates, but not too much.

08:56.000 --> 09:02.040
I kind of get that as a simplistic model for the evolution of art.

09:02.040 --> 09:07.720
But it does seem like coming up with some kind of representation for that and how that happens

09:07.720 --> 09:12.280
would be very complex computationally.

09:12.280 --> 09:14.280
Yes, definitely.

09:14.280 --> 09:17.440
You have to simplify it.

09:17.440 --> 09:20.720
And that's what we managed to do at least to certain degree.

09:20.720 --> 09:26.240
We suppose we give the machine lots and lots of images of art.

09:26.240 --> 09:33.120
What we did was we give it art from the last 500 years of Western canon of art history.

09:33.120 --> 09:39.120
And we only give the machine the artworks and the style of each artwork according to

09:39.120 --> 09:47.720
styles from art and history and like Renaissance Baroque, imperialism, all these art movements

09:47.720 --> 09:49.520
that happened.

09:49.520 --> 09:54.080
So this is the only thing that the machine gives the machine, the images of the art and

09:54.080 --> 09:56.920
these labels.

09:56.920 --> 10:05.760
So the formulation here is that we want the generator, we want to put the generator under two opposing

10:05.760 --> 10:11.480
courses in one hand, like a chemical gun, if we need to learn the distribution of art,

10:11.480 --> 10:12.480
right?

10:12.480 --> 10:13.960
We need something from that distribution.

10:13.960 --> 10:15.880
But that would be only imitation, right?

10:15.880 --> 10:18.120
So how can you push it to be innovative?

10:18.120 --> 10:22.960
So we added what we call style ambiguity.

10:22.960 --> 10:27.040
We want to basically, the generator generates something that follows the distribution but

10:27.040 --> 10:30.320
doesn't follow the existing styles at all.

10:30.320 --> 10:36.520
So in order to do that, the discriminator here has to be equipped with a stylist's fire.

10:36.520 --> 10:43.320
So the estimator has to be able to tell for any new artwork what style is it, is it

10:43.320 --> 10:47.800
Renaissance, is Baroque, is it Ubism, is it abstract.

10:47.800 --> 10:53.800
So the generator job here is to find out a solution that generate images, follow the

10:53.800 --> 10:58.160
general aesthetics of art, follow the same distribution of art.

10:58.160 --> 11:02.640
But in the same time, doesn't follow any existing styles because if it generated another

11:02.640 --> 11:07.560
ambitionist art or another, a cubism art, you're going to get been allowed.

11:07.560 --> 11:12.920
So you see the dilemma here, so it has to generate something that fits the aesthetics in

11:12.920 --> 11:17.160
general, but doesn't fit existing style.

11:17.160 --> 11:24.440
And that what needed to innovate in style, but follow the aesthetics doesn't really make

11:24.440 --> 11:28.520
something shockingly random that people won't accept.

11:28.520 --> 11:32.920
And we see that right away in the generation, what it generates is always following the

11:32.920 --> 11:38.160
style, following the aesthetics so you can see a good choice of colors, a good choice

11:38.160 --> 11:44.280
of composition rules, variety of texture, but in the same time, you can not easily put

11:44.280 --> 11:47.480
what it generates into any existing style.

11:47.480 --> 11:51.440
It doesn't repeat what happened before, we don't see anything that looked like Renaissance

11:51.440 --> 11:58.000
or a cubism art or a cubism art, we can always see that it innovates and try to make something

11:58.000 --> 11:59.520
new out of that.

11:59.520 --> 12:06.160
So these two forces actually kind of boost the innovation, but limit the innovation at

12:06.160 --> 12:11.160
the same time because we bullet back to the distribution of art, so it doesn't go straight

12:11.160 --> 12:17.080
and start generating random things that's going to be too innovative.

12:17.080 --> 12:27.160
How do you kind of enforce this rule to maintain the aesthetic, but not the style within the

12:27.160 --> 12:28.160
GAN?

12:28.160 --> 12:33.960
Basically, the loss function of the GAN, we change the loss function of the GAN such that

12:33.960 --> 12:36.880
it has these two opposing losses.

12:36.880 --> 12:45.280
In one hand, it has a typical GAN loss, which is try to generate images from the distribution

12:45.280 --> 12:52.280
of the real fake loss as usually in a GAN setting, so it is, that's incentive to be within

12:52.280 --> 12:58.040
the aesthetics, but in the same time, we have the style ambiguity loss, which is basically

12:58.040 --> 13:05.720
based on the style classifier of that denominator, we compute some sort of entropy score out

13:05.720 --> 13:13.720
of that style classifier, and we basically try to maximize this style ambiguity.

13:13.720 --> 13:22.240
So in one hand, when I minimize the divergent from the distribution of art, but in the same

13:22.240 --> 13:27.560
time when I maximize the ambiguity of the style, so this implemented the two opposing forces.

13:27.560 --> 13:34.280
Okay, and now is the loss function, when it's evaluated, is that a scalar, and I guess

13:34.280 --> 13:39.440
what I'm asking you is, do you have this issue where you have these two opposing components

13:39.440 --> 13:45.160
of your loss function that just average out and kind of you lose all of the information

13:45.160 --> 13:50.680
and the nuance in the two different components, or is that not an issue here?

13:50.680 --> 13:51.680
That is exactly what it's trying to do.

13:51.680 --> 14:00.080
I mean, it tries to find out both in the surface of the optimization that you are doing that

14:00.080 --> 14:01.080
in between the two.

14:01.080 --> 14:06.000
You want to minimize one, maximize the other, so it has to find some sweet spot between

14:06.000 --> 14:07.000
the two.

14:07.000 --> 14:13.160
I'm curious about the training process you've got.

14:13.160 --> 14:16.360
How many images did you train the Gannon?

14:16.360 --> 14:25.800
Yeah, the first version we did, it was about 80,000 images from last 500 years of Western

14:25.800 --> 14:32.720
art, about 20 different styles that we used at starting from my sons to all the way

14:32.720 --> 14:40.160
to contemporary art, almost tiny form among all the styles, make sure that we don't have

14:40.160 --> 14:47.560
any bias in the data, so roughly all the styles are in uniform number.

14:47.560 --> 14:54.760
So the interesting thing was that under distance trains, we find that the machine tends to generate

14:54.760 --> 15:02.680
more and more abstract works, less figurative artworks, and more abstract works for

15:02.680 --> 15:05.400
that, which is very interesting.

15:05.400 --> 15:06.400
Why is that?

15:06.400 --> 15:15.280
I mean, why they would generate abstract artworks and not figurative artwork?

15:15.280 --> 15:18.200
We kept thinking about that for a long time, so why is that?

15:18.200 --> 15:25.840
But when we look at the progression of history of art, one thing we find interesting in the

15:25.840 --> 15:32.480
results is that the machine under distance trains that we put in generate more and more

15:32.480 --> 15:37.360
abstract artworks and less and less figurative, we hardly see any portraits or any landscape

15:37.360 --> 15:38.360
generated.

15:38.360 --> 15:46.760
It's more about more abstract and more, it's a little bit surrealist nature.

15:46.760 --> 15:51.680
So we were wondering why is that, why the machine generates more abstract?

15:51.680 --> 15:55.320
However, if you look at art history and look at the progression art history, at least

15:55.320 --> 16:03.000
some other research we have done also, we actually find that it's clear that art history moves

16:03.000 --> 16:10.200
into a trajectory from a figurative art through the history in the last five centuries until

16:10.200 --> 16:17.640
we reached the 20th century, where artists move into abstraction and very beautiful abstraction,

16:17.640 --> 16:25.480
then later into a color field painting and abstract exhibitionism and contemporary art.

16:25.480 --> 16:31.680
So there is a clear trajectory of art moving away from figurative art into abstraction,

16:31.680 --> 16:37.080
which basically seems that the machine here figured out that there is this trajectory

16:37.080 --> 16:44.760
and in order to create novel art, it has to be more into the abstract realm of art.

16:44.760 --> 16:50.680
So under this constraint that we put into the formulation, it seemed that finding a good

16:50.680 --> 16:57.480
solution, abstract art provides more good solution that fits these two constraints.

16:57.480 --> 17:03.960
Because if it's a figurative art, it probably gonna generate things that looks like Renaissance

17:03.960 --> 17:12.000
or Baroque or other classical styles, so it didn't analyze why for abstract art, I think

17:12.000 --> 17:16.800
there's a lot of room for experiment for the machine or results that machine can generate

17:16.800 --> 17:21.720
that doesn't really fit into existing styles that easy.

17:21.720 --> 17:26.160
So it's seen that's why the machine is getting more and more of these abstract works.

17:26.160 --> 17:32.760
The website is aicanicand.io and we'll put a link in the show notes.

17:32.760 --> 17:38.160
I definitely, you know, the abstract nature of the results are very apparent.

17:38.160 --> 17:43.040
But I think about kind of the, you know, art that might be generated by again, what comes

17:43.040 --> 17:49.280
to mind is more like the, if you remember back in 2012, this woman tried to do a hand

17:49.280 --> 17:57.920
restoration of this Spanish fresco echihomo and basically made it look really, really

17:57.920 --> 18:00.280
bad.

18:00.280 --> 18:04.200
And that's kind of what I think of when you kind of think of ganger-erated images, at

18:04.200 --> 18:11.760
least, you know, the gans are getting a lot better now, but these are decidedly abstract.

18:11.760 --> 18:19.400
And in fact, you have a gallery on the site that's kind of the 2017 version of the, of

18:19.400 --> 18:30.640
Ican and then a 27, 8, 2018 version of Ican and just kind of comparing the two visually.

18:30.640 --> 18:35.020
And I'd like you to comment on, you know, the, whether, you know, this could be kind of

18:35.020 --> 18:40.040
selection bias, you handpicked, you know, the images kind of differently or something.

18:40.040 --> 18:45.600
But the, the art is much better in the gallery that you're displaying for 2018.

18:45.600 --> 18:50.840
Is that characteristic of the, the second version of the model's results or did you just

18:50.840 --> 18:53.240
pick better art for the website?

18:53.240 --> 18:59.200
And no, just physically we change more and more data for longer time and it gets, it

18:59.200 --> 19:03.040
is affecting the, the process, so that's why it's getting better.

19:03.040 --> 19:08.320
And that's, that's maybe not, you know, not intuitive that, you know, I would almost expect

19:08.320 --> 19:13.880
maybe that a counterintuitive result or the opposite result, you train it, you know, with

19:13.880 --> 19:23.720
more, if you're training it with more historical art that it would tend more towards kind of

19:23.720 --> 19:29.120
reproducing what it sees then to get further out into abstraction, if that makes any sense.

19:29.120 --> 19:33.640
That's absolutely not, that's a whole idea that, that, again, we'll do that.

19:33.640 --> 19:38.920
Again, we'll definitely try, if you get more data and more training it will emulate the

19:38.920 --> 19:39.920
data better.

19:39.920 --> 19:45.240
But, exactly, but what we doing here is the variant that we drive, which is creative at

19:45.240 --> 19:50.600
the cellular network, which is, by definition, it doesn't try to generate anything that existed

19:50.600 --> 19:56.160
before, because if you generate things that stimulate what existed before that get, get,

19:56.160 --> 20:02.440
get been alive because of the style ambiguity, but of the loss, so it has to generate something

20:02.440 --> 20:07.040
that doesn't fit existing style at all, but follows the aesthetics, that's why it's always

20:07.040 --> 20:14.040
has tried to generate things that are not holding into existing styles, and that's why

20:14.040 --> 20:19.320
bush, bush, bush the creativity of the, of the result.

20:19.320 --> 20:26.240
So having more data will, will, will, will not make it emulate the results by definition,

20:26.240 --> 20:30.600
because we, it still have to innovate according to the loss.

20:30.600 --> 20:36.360
And actually, let me discuss this, according to your question, why this is different from

20:36.360 --> 20:39.880
GANs, GINID images, of course.

20:39.880 --> 20:47.040
Exactly, if you, if you look at most of our GINID by using GANs, especially maybe a year

20:47.040 --> 20:52.440
or two years old, now GANs getting better, if you give it a board trace, for example,

20:52.440 --> 20:58.720
as training sets, it will generate what's, what's looks like the form of board trace, right?

20:58.720 --> 21:06.920
So you can see kind of, Francis Bacon, the form of style of board trace in the generation,

21:06.920 --> 21:15.400
specifically, GAN images that you see in several artists, the genetic art using GANs

21:15.400 --> 21:18.000
are having this characteristic.

21:18.000 --> 21:24.960
So what happens in that case is actually, the GANs are not successful in generating good

21:24.960 --> 21:26.480
board trace.

21:26.480 --> 21:30.840
So the GAN actually fails in generating the board trace correctly, and, and because of

21:30.840 --> 21:35.000
the deformation, that's why you will find it interesting.

21:35.000 --> 21:40.240
So it surprised you because of the deformation, and that surprise is why you might like it,

21:40.240 --> 21:43.000
or might not like it as a viewer.

21:43.000 --> 21:51.040
That's why, that's, I mean aesthetics in GANs, genetic images, the element of deformation

21:51.040 --> 21:54.960
or failure to imitate the training data.

21:54.960 --> 21:59.280
So, and that's exactly what we've pointed out in our academic paper, when we derived

21:59.280 --> 22:07.720
the PICAN, is that we, we thought, we think of this failure to imitate the solution as

22:07.720 --> 22:11.120
a failure case, not a creative process.

22:11.120 --> 22:14.120
The machine here is not intentionally deforming the image.

22:14.120 --> 22:17.640
The machine is just failing to imitate the poetry.

22:17.640 --> 22:20.960
For us as a viewer, it's interesting thing to look at.

22:20.960 --> 22:23.720
But the process here is not creative.

22:23.720 --> 22:28.760
The process here is a, a, a emulative process that just failed, right?

22:28.760 --> 22:33.320
And that's totally what, not what we are trying to do, we're trying to really make it

22:33.320 --> 22:38.280
be constructive in being creative, meaning that the machine in our case in, in, when we

22:38.280 --> 22:43.200
We do a hand, but the machine from the beginning

22:43.200 --> 22:48.320
trying to generate something that have these two constraints

22:48.320 --> 22:50.280
both into place follows the aesthetic,

22:50.280 --> 22:52.760
but doesn't follow existing style.

22:52.760 --> 22:54.560
So with these two constraints,

22:54.560 --> 22:56.200
the machine has to find a solution

22:56.200 --> 22:58.200
that satisfies these two constraints.

22:58.200 --> 23:00.840
Art history, if you look at the way art history progress

23:00.840 --> 23:03.960
over time, there is a quote from an art historian,

23:03.960 --> 23:07.880
his name is Henry Walbren, who say that style

23:07.880 --> 23:13.080
has to evolve in a smooth way, like a rock rolling down a hill.

23:13.080 --> 23:16.400
Meaning that first, a style doesn't jump.

23:16.400 --> 23:18.640
We cannot have renaissance moving into ambitionism

23:18.640 --> 23:21.640
and moving that to Baroque and then moving to realism.

23:21.640 --> 23:23.240
No, it doesn't happen this way.

23:23.240 --> 23:27.920
Style moves in a very slowly over time

23:27.920 --> 23:32.640
from renaissance to Baroque to in a very ordered way.

23:32.640 --> 23:35.480
So if you boost the machine to generate something that

23:35.480 --> 23:37.840
doesn't fit existing styles, but keep the aesthetic

23:37.840 --> 23:41.600
it cannot find a solution by mixing out, for example,

23:41.600 --> 23:44.880
renaissance and Baroque or mixing out Baroque and

23:44.880 --> 23:46.840
ambitionism or try to find something in between.

23:46.840 --> 23:50.680
There is no way to find something in between a solution.

23:50.680 --> 23:53.800
So the only way for the machine to solve this problem

23:53.800 --> 23:56.640
is really to extrapolate on that trajectory,

23:56.640 --> 23:59.960
figure out that is a trajectory of how art progressed

23:59.960 --> 24:03.000
and then generate something on top of that.

24:03.000 --> 24:06.760
And that's why Jeanette Moore and more abstract works.

24:06.760 --> 24:10.440
So by construction here, as the machine has a creative

24:10.440 --> 24:13.840
process in terms of generating the art,

24:13.840 --> 24:16.920
it doesn't emulate, it has to always innovate.

24:16.920 --> 24:20.560
And the source of aesthetics of what you see

24:20.560 --> 24:22.840
is this creative process is not the failure

24:22.840 --> 24:26.440
through which you need something from the data set.

24:26.440 --> 24:28.160
It's not the failure to generate a board tree.

24:28.160 --> 24:30.360
It's not a failure to generate a landscape.

24:30.360 --> 24:32.440
The real aesthetics here coming from the fact

24:32.440 --> 24:36.960
it's followed the aesthetics rules, learning the ethics rules,

24:36.960 --> 24:39.640
but try to constructively be constructively

24:39.640 --> 24:42.240
generating something new.

24:42.240 --> 24:44.440
Yeah, I'm not sure I fully understand that.

24:44.440 --> 24:47.840
So what I hear you saying is you've got this,

24:47.840 --> 24:52.480
the history of art is this progression or sequence of styles.

24:52.480 --> 24:58.160
And what your can kind of creative adversarial network

24:58.160 --> 25:02.680
is creating is something that is art

25:02.680 --> 25:05.720
that doesn't exist kind of between two styles,

25:05.720 --> 25:10.320
but kind of by definition projects out

25:10.320 --> 25:14.480
from kind of the last element of that sequence.

25:14.480 --> 25:17.080
And maybe you're just speaking figuratively,

25:17.080 --> 25:20.440
and I'm trying to be too literal or rigorous about this.

25:20.440 --> 25:22.480
But why is that necessarily the case?

25:22.480 --> 25:25.640
Why is it that the things that we're seeing

25:25.640 --> 25:29.840
are not kind of orthogonal to the sequence of styles

25:29.840 --> 25:31.240
that we've seen?

25:31.240 --> 25:32.160
Yeah, it's a good question.

25:32.160 --> 25:32.840
Yes, it can.

25:32.840 --> 25:34.040
It can be.

25:34.040 --> 25:38.720
Because at the end, I mean, we, the machine-strike solution

25:38.720 --> 25:40.520
giving the constraint we give it.

25:40.520 --> 25:43.200
But let me bring you out to another piece of research

25:43.200 --> 25:44.920
that we have done.

25:44.920 --> 25:50.040
In another research, we looked at what representation

25:50.040 --> 25:55.720
the machine learns if we teach it to classify styles.

25:55.720 --> 25:58.840
So we give the machine lots of images and style labels

25:58.840 --> 26:02.520
and train lots of new networks to classify styles,

26:02.520 --> 26:05.200
such as to try and classify whether the art is very

26:05.200 --> 26:07.800
sounds borrowed, ambition is man, and so on.

26:07.800 --> 26:09.360
Just like classification.

26:09.360 --> 26:12.640
But then what we looked at, we looked at what representation

26:12.640 --> 26:13.520
the machine learned.

26:13.520 --> 26:17.640
So we look at the activation of the fully committed layers

26:17.640 --> 26:21.360
and in this network and look at things

26:21.360 --> 26:24.760
like the principal component analysis of these activitions

26:24.760 --> 26:27.640
and the manifold of activation and things like that.

26:27.640 --> 26:30.600
And to our surprise, when you look at this,

26:30.600 --> 26:35.600
we find that the machine arranged the data

26:37.200 --> 26:39.520
in a chronological order by itself.

26:39.520 --> 26:40.640
Interesting.

26:40.640 --> 26:42.400
Yeah, we never give the machine anything

26:42.400 --> 26:46.240
about the date of the artwork or that any sounds

26:46.240 --> 26:50.720
happened before Baroque or Baroque happened before Cubism

26:50.720 --> 26:51.840
or Embritionism.

26:51.840 --> 26:54.520
We found that when you look at two-dimension plots

26:54.520 --> 26:59.520
of the data, Renaissance comes first, followed by Baroque,

27:00.480 --> 27:04.000
followed by 17th century art.

27:04.000 --> 27:08.680
And then comes 19th century art realism and Embritionism

27:08.680 --> 27:13.400
and then move to Cezanne and move to Cubism and abstract.

27:13.400 --> 27:14.400
By itself.

27:14.400 --> 27:16.800
Amazing. I mean, you can find that the machine actually,

27:16.800 --> 27:19.480
the art has evolved into a full circle

27:19.480 --> 27:21.720
going from Renaissance all the way to 20th century

27:21.720 --> 27:22.720
in a very smooth way.

27:22.720 --> 27:24.680
And the machine discovered that by itself.

27:24.680 --> 27:25.680
Interesting.

27:25.680 --> 27:31.000
I was going to ask what happens if you tell the network

27:31.000 --> 27:32.720
kind of the sequence by feeding in dates,

27:32.720 --> 27:35.240
but it sounds like you don't have to it, figures it out.

27:35.240 --> 27:36.760
Exactly, exactly.

27:36.760 --> 27:41.760
And that exactly explains why the machine actually generated

27:41.760 --> 27:46.760
art that is more figurative and more abstract at the end.

27:49.520 --> 27:52.600
Can you elaborate a little bit on how you got

27:52.600 --> 27:57.600
to this two-dimensional or kind of time representation

27:58.640 --> 28:01.560
or sequential representation from the activations?

28:02.640 --> 28:04.840
Yeah, so basically we look at the activation space

28:04.840 --> 28:08.960
and look at the modes of variation,

28:08.960 --> 28:13.840
the principal component analysis of this activation space.

28:13.840 --> 28:17.200
And what we find that first, you can explain

28:18.800 --> 28:22.840
almost 95% of the variance of the data

28:24.120 --> 28:27.280
using very few number of components

28:27.280 --> 28:29.960
in the activation space, usually five, six components

28:29.960 --> 28:32.040
in all the networks that you have seen.

28:32.040 --> 28:34.840
And actually the first two dimensions,

28:34.840 --> 28:36.400
the first two modes of variation captured

28:36.400 --> 28:38.360
about 70% of the variance.

28:38.360 --> 28:41.720
And if you plot the data against these two modes of variation,

28:41.720 --> 28:46.720
you can always clearly see this chronological progression

28:46.720 --> 28:49.440
from 14th century till now,

28:49.440 --> 28:54.440
where if you go radially across clockwise, across that

28:55.680 --> 29:00.680
to the emission plot, it has about 70% correlation

29:01.240 --> 29:02.880
with time.

29:02.880 --> 29:07.240
So the machine figured out this smooth transition by itself,

29:07.240 --> 29:11.360
which basically tells us that one thing we already know

29:11.360 --> 29:15.360
from art history is that style progress in a very smooth way,

29:15.360 --> 29:17.760
it doesn't progress in random ways,

29:17.760 --> 29:21.520
it progressed in a very smooth way.

29:21.520 --> 29:24.480
And that's what the machine was figured out by itself.

29:24.480 --> 29:29.480
If I were to create a model for the evolution of art,

29:31.320 --> 29:33.320
being someone who's probably as far as you can get

29:33.320 --> 29:36.040
from someone who knows anything about this,

29:36.040 --> 29:40.240
but I would kind of evolve the model that you've described

29:40.240 --> 29:43.960
where you've got these kind of styles

29:43.960 --> 29:48.400
and then these perturbations in aesthetic

29:48.400 --> 29:52.520
or perturbations in, how did you describe that earlier?

29:52.520 --> 29:56.000
That kind of the simple model for the way art evolves?

29:57.880 --> 29:59.480
You wanna boost innovation

30:00.960 --> 30:03.520
and there are multiple ways to boost innovation,

30:03.520 --> 30:07.360
but you wanna boost innovation in a least amount

30:07.360 --> 30:09.480
or least with least effort.

30:09.480 --> 30:10.640
Right, right, right.

30:10.640 --> 30:15.640
So you've got this style that has evolved

30:16.680 --> 30:20.480
and you kind of innovate in these small incremental ways.

30:20.480 --> 30:25.280
And I guess what's missing for that for me is,

30:25.280 --> 30:28.480
it strikes me that you have a lot of that

30:28.480 --> 30:31.360
and then you have these kind of big step function innovations

30:31.360 --> 30:34.720
that are much more dramatic than the small random innovations.

30:34.720 --> 30:38.560
Is that just not true from the kind of research

30:38.560 --> 30:42.880
into the evolution of art or is that possible

30:42.880 --> 30:45.520
further extension of work like this

30:45.520 --> 30:48.000
to incorporate that type of model?

30:49.360 --> 30:51.320
Actually, what you're saying is true.

30:51.320 --> 30:54.320
There are different ways artists innovate in their work.

30:54.320 --> 30:56.000
They can innovate within the style

30:56.000 --> 30:57.880
and they can break out of style.

30:57.880 --> 31:01.280
And this is something that Martin Delecchi pointed out.

31:01.280 --> 31:05.360
Most artists work within a style,

31:05.360 --> 31:08.280
meaning that there is a predominant style

31:08.280 --> 31:10.880
at any time in history.

31:10.880 --> 31:13.200
And most artists just work within that style.

31:13.200 --> 31:16.400
Like if you are in 17th century,

31:16.400 --> 31:18.360
probably you're gonna do Baroque.

31:18.360 --> 31:20.720
That's what's happening and everybody is doing

31:20.720 --> 31:22.880
within the rules of Baroque style.

31:22.880 --> 31:25.000
But what happened is at certain points,

31:25.000 --> 31:30.000
artists really are a good board of these schools

31:30.800 --> 31:32.080
that they are working with

31:32.080 --> 31:36.520
because they tried all possible subject matter

31:36.520 --> 31:40.240
within that style and there's no room for innovation anymore.

31:40.240 --> 31:41.480
When you're working within the style,

31:41.480 --> 31:43.720
the only way you innovate is to look

31:43.720 --> 31:45.760
at different subject matter within the style.

31:45.760 --> 31:47.080
You can try the different story,

31:47.080 --> 31:50.440
different scene, different things,

31:50.440 --> 31:52.320
but everything within the style rules.

31:52.320 --> 31:56.480
But at certain points, you exerted all these possibilities.

31:56.480 --> 32:01.560
And then that's where really important artists

32:01.560 --> 32:05.240
come up with new break out of the style.

32:05.240 --> 32:06.200
That's what happened, for example,

32:06.200 --> 32:10.000
when Picasso broke out of what's happened

32:10.000 --> 32:15.000
and painted ladies of Avignon, for example, in 1907.

32:15.360 --> 32:19.800
And that was too innovative at that time.

32:19.800 --> 32:23.080
And it led to the beginning of Cubism

32:23.080 --> 32:27.320
or earlier when Monk bended the scream

32:27.320 --> 32:31.400
and early works of exhibitionism.

32:31.400 --> 32:34.200
So there are always certain artists who break out of style

32:34.200 --> 32:38.200
and other artists are to copy these new styles

32:38.200 --> 32:40.800
and follow their direction.

32:41.960 --> 32:46.040
And that's the kind of innovation we plugged in

32:46.040 --> 32:47.840
into the formulation of Can,

32:47.840 --> 32:50.360
which is the style ambiguity.

32:50.360 --> 32:52.320
We don't want an machine to generate something

32:52.320 --> 32:54.040
that fits existing style.

32:54.040 --> 32:57.080
Because if it generates something that fits existing style,

32:57.080 --> 33:01.520
it's the entropy, there would be no surprise.

33:01.520 --> 33:03.760
The entropy would be very low for that bottle of the loss

33:03.760 --> 33:06.800
and we're trying to maximize that more.

33:06.800 --> 33:08.360
But the machine has to always generate something

33:08.360 --> 33:10.960
that the classifier, the discriminator,

33:10.960 --> 33:15.960
cannot classify, be almost uniform in terms of classifying

33:15.960 --> 33:20.960
that style according to the styles that you give the machine.

33:21.840 --> 33:25.240
So that's where the style ambiguity gets maximized.

33:26.760 --> 33:28.040
Okay, interesting.

33:28.040 --> 33:30.960
So I'm curious, this working creativity

33:30.960 --> 33:34.880
is kind of one area of interest of yours.

33:34.880 --> 33:37.680
Were there any principles or observations

33:37.680 --> 33:40.880
that you made in working on this project

33:40.880 --> 33:45.880
that kind of informed your broader work in computer vision?

33:47.320 --> 33:49.640
I have been focused more and more into that area

33:49.640 --> 33:51.760
for the last maybe six, seven years

33:51.760 --> 33:56.360
to really understand more how to make the machine understand art.

33:56.360 --> 33:59.760
And so the generation of art is something that came

33:59.760 --> 34:04.760
actually out of long progress into understanding this concept.

34:05.560 --> 34:08.160
So the earlier works that we have done in that area

34:08.160 --> 34:11.040
those things like how to classify styles,

34:11.040 --> 34:14.920
how to classify the genre of art, how to classify artists,

34:14.920 --> 34:17.200
how to look at artistic influences

34:18.200 --> 34:20.400
and even you have worked earlier

34:20.400 --> 34:22.840
on how to quantify creativity in artwork.

34:22.840 --> 34:25.960
Like can you tell, can you look at art history

34:25.960 --> 34:30.680
and look at images only and their time stamps

34:30.680 --> 34:35.680
and find out artworks that were creative in their time.

34:35.680 --> 34:40.680
And actually we find that the machine by some formulation

34:40.680 --> 34:43.080
really what's called network centrality

34:43.080 --> 34:45.520
could figure out important piece of art

34:45.520 --> 34:49.120
to art history just by looking at images of art

34:49.120 --> 34:52.960
and their dates without knowing anything else about styles

34:52.960 --> 34:56.440
or about artists or anything like that.

34:56.440 --> 34:58.360
So the machine figure out for example things like

34:58.360 --> 35:00.160
Picasso's ladies have been known

35:00.160 --> 35:03.440
or a monk's theme or Monet's tag

35:03.440 --> 35:08.440
has fundamental important pieces of artwork in history.

35:09.560 --> 35:12.360
And that lead us, that lead us actually

35:12.360 --> 35:16.320
to formulating the creative adversarial network

35:16.320 --> 35:20.840
as a variant of GAN that has this built in process

35:20.840 --> 35:25.840
of style ambiguity as a constraint

35:26.160 --> 35:30.560
that help pushing the machine to be creative

35:30.560 --> 35:32.640
and innovative.

35:32.640 --> 35:33.480
Interesting.

35:33.480 --> 35:34.720
So really, really interesting work.

35:34.720 --> 35:37.720
How do you see this work evolving?

35:37.720 --> 35:38.560
What's next?

35:40.120 --> 35:42.600
What's next is you are focused now on our focus now

35:42.600 --> 35:46.760
and how to, what we've proved is basically that the machine

35:46.760 --> 35:49.480
can, we can give the machine all this art history

35:49.480 --> 35:52.960
without any curation just to give it all art history

35:52.960 --> 35:57.360
and it can generate almost autonomously new artworks

35:57.360 --> 35:58.360
that what we achieved.

35:59.520 --> 36:01.000
So that's a proof concept that machine

36:01.000 --> 36:05.400
can reduce things autonomously to some degree.

36:05.400 --> 36:09.600
But what we really want to do is to give artists

36:09.600 --> 36:11.680
a control over the process.

36:11.680 --> 36:15.320
How can artists control this generation

36:16.920 --> 36:18.840
to achieve something that they want to do,

36:20.080 --> 36:22.360
not just make it autonomous?

36:22.360 --> 36:23.920
And that's really what we try to do now,

36:23.920 --> 36:28.880
how to make it more collaborative and more controllable.

36:28.880 --> 36:30.000
Fantastic, fantastic.

36:30.000 --> 36:32.600
Well, Ahmed, thank you so much for taking the time

36:32.600 --> 36:35.080
to chat with us about your work.

36:35.080 --> 36:36.080
Thank you very much.

36:40.080 --> 36:43.160
All right, everyone, that's our show for today.

36:43.160 --> 36:44.760
If you like what you've heard here,

36:44.760 --> 36:48.960
please do us a huge favor and tell your friends about the show.

36:48.960 --> 36:51.920
And if you haven't already hit that subscribe button yourself,

36:51.920 --> 36:54.840
make sure you do so you don't miss any of the great episodes

36:54.840 --> 36:56.080
we've got in store for you.

36:57.040 --> 36:59.600
As always, thanks so much for listening

36:59.600 --> 37:00.720
and catch you next time.

