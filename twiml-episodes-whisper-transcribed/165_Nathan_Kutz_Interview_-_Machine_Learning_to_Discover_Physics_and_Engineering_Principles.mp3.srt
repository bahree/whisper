1
00:00:00,000 --> 00:00:16,320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

2
00:00:16,320 --> 00:00:21,480
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:21,480 --> 00:00:36,040
I'm your host Sam Charrington, a quick update about the upcoming Twimble Online Meetup.

4
00:00:36,040 --> 00:00:41,620
Next Tuesday July 17th at 5pm Pacific time, Nick Teague will lead a discussion on the paper

5
00:00:41,620 --> 00:00:47,440
Quantum Machine Learning by Jacob Biamante at all, which explores how to devise and implement

6
00:00:47,440 --> 00:00:50,920
concrete software for machine learning tasks.

7
00:00:50,920 --> 00:00:59,120
Visit twimbleai.com slash meetup for more details or to sign up.

8
00:00:59,120 --> 00:01:04,200
In this episode, I'm joined by Nathan Kuts, professor of Applied Mathematics, Electrical

9
00:01:04,200 --> 00:01:08,400
Engineering and Physics at the University of Washington.

10
00:01:08,400 --> 00:01:12,680
Nathan and I met a few months ago at the Prepare AI Conference here in St. Louis, where

11
00:01:12,680 --> 00:01:18,360
he gave a great talk on machine learning to discover physics and engineering principles.

12
00:01:18,360 --> 00:01:23,160
Our conversation is laser focused on his research and to the use of ML to help discover the

13
00:01:23,160 --> 00:01:30,440
fundamental governing equations for physical and engineering systems from time series measurements.

14
00:01:30,440 --> 00:01:35,320
We explore in fact the application of his work to self-tuning fiber optic lasers, as well

15
00:01:35,320 --> 00:01:40,080
as to biology and other complex multi-scale systems.

16
00:01:40,080 --> 00:01:42,840
And now on to the show.

17
00:01:42,840 --> 00:01:49,160
All right, everyone. I am on the line with Nathan Kuts. Nathan is professor of Applied

18
00:01:49,160 --> 00:01:54,200
Mathematics, Electrical Engineering and Physics at the University of Washington.

19
00:01:54,200 --> 00:01:57,040
Nathan, welcome to this weekend machine learning and AI.

20
00:01:57,040 --> 00:01:59,600
Thank you very much. Glad to be here.

21
00:01:59,600 --> 00:02:04,880
I'm really looking forward to our conversation. I had a chance to see a talk you did at

22
00:02:04,880 --> 00:02:11,080
the Prepare AI Conference here in St. Louis a few weeks ago and I thought it was a great

23
00:02:11,080 --> 00:02:18,480
presentation. It was on machine learning to discover physics and engineering principles

24
00:02:18,480 --> 00:02:22,760
and we are going to dive pretty deeply into that.

25
00:02:22,760 --> 00:02:29,520
But before we do, Nathan, take a few minutes to introduce us to your background and how

26
00:02:29,520 --> 00:02:34,000
you got involved in machine learning and artificial intelligence.

27
00:02:34,000 --> 00:02:36,160
Sure. So we'll be happy to do that.

28
00:02:36,160 --> 00:02:42,560
So I come from a pretty standard, let's say, physics, Applied Math training. I did

29
00:02:42,560 --> 00:02:51,480
my PhD back in the early the 90s and then went off to do a postdoc at Princeton University

30
00:02:51,480 --> 00:02:56,760
jointly with Bell Labs. And at the time, I was doing a lot of just standard modeling

31
00:02:56,760 --> 00:03:03,320
of physical systems. I was particularly interested in optical systems. At that time at Bell Labs,

32
00:03:03,320 --> 00:03:08,000
we were doing long haul communications. So the idea there was to, of course, write down

33
00:03:08,000 --> 00:03:13,640
some governing equations. And typically this was some version of Maxwell's equations for

34
00:03:13,640 --> 00:03:19,360
understanding how light propagates down fiber optic cables. And so I would call this coming

35
00:03:19,360 --> 00:03:26,720
from the forward modeling perspective, which is you write down some set of equations.

36
00:03:26,720 --> 00:03:33,120
You drive them and then you do lots of simulations. And then you build your understanding in that

37
00:03:33,120 --> 00:03:37,640
architecture. And of course, there's some data that's collected. But back in the 90s,

38
00:03:37,640 --> 00:03:42,640
the data wasn't the same as what we think about today. And so I spent a long time there

39
00:03:42,640 --> 00:03:48,560
really building out sort of that perspective and applying it to various other types of

40
00:03:48,560 --> 00:03:55,080
problems once they came to the University of Washington. And about 10 years ago, I got

41
00:03:55,080 --> 00:04:00,600
very interested in using some of these dimensionality reduction techniques that were being used

42
00:04:00,600 --> 00:04:06,680
in the fluid, fluid dynamics community. And there it's called proper orthogonality composition,

43
00:04:06,680 --> 00:04:12,560
which really was the same as PCA. And everybody has these different terms for the same thing.

44
00:04:12,560 --> 00:04:16,960
So then I started realizing that you said it was called proper orthogonality. Proper

45
00:04:16,960 --> 00:04:22,000
orthogonality composition. Proper orthogonality composition. Okay. And then interestingly enough,

46
00:04:22,000 --> 00:04:28,880
the atmospheric scientists called it empirical orthogonal functions. So I started realizing

47
00:04:28,880 --> 00:04:33,480
that everybody's doing the same thing, which is taking your data and maybe doing a little

48
00:04:33,480 --> 00:04:38,640
bit of mile pre-processing, but running it through the singularvality composition. And I

49
00:04:38,640 --> 00:04:44,160
started applying it initially to optics problems and started realizing that, oh my God, this

50
00:04:44,160 --> 00:04:50,400
is an amazing idea. And in fact, it was kind of interesting that I had no idea that there's

51
00:04:50,400 --> 00:04:56,640
whole other community existed. And that's how I started getting into the idea of using

52
00:04:56,640 --> 00:05:04,640
data methods over towards more physics engineering-type problems. And since that time, then I brought

53
00:05:04,640 --> 00:05:13,680
in all of this architecture borrowed from the MLAI type communities around model discovery,

54
00:05:13,680 --> 00:05:20,520
regression, dimensionality reduction. And that has completely changed the way I think

55
00:05:20,520 --> 00:05:27,640
about these problems. And it's also allowing me to start thinking about using these ideas

56
00:05:27,640 --> 00:05:32,160
towards control architectures. And that was ultimately the goal we had in the engineering

57
00:05:32,160 --> 00:05:38,160
sciences. It's always going to come down to some kind of control theory. But this allowed

58
00:05:38,160 --> 00:05:43,400
my group and my collaborators to start thinking about framing things in terms of control.

59
00:05:43,400 --> 00:05:50,440
So since that time, we just are really integrating these engineering systems with machine learning

60
00:05:50,440 --> 00:05:57,840
architectures. And that's how I've continued to develop in this direction is to use both

61
00:05:57,840 --> 00:05:59,240
perspectives.

62
00:05:59,240 --> 00:06:08,040
The context that you presented on, if I remember correctly, was in controlling, was it lasers

63
00:06:08,040 --> 00:06:14,440
or controlling the laser coherence or something like that?

64
00:06:14,440 --> 00:06:23,520
Yeah, I can speak to it. So we were interested. A lot of my expertise comes in this laser

65
00:06:23,520 --> 00:06:30,720
physics, which I had started back all the way back in the late 90s working on laser physics.

66
00:06:30,720 --> 00:06:35,360
And one of the interesting things was that over this time period, this is a big commercial

67
00:06:35,360 --> 00:06:43,440
market. Laser technologies are, I mean, the amount of money in that field is enormous.

68
00:06:43,440 --> 00:06:47,240
I mean, hundreds of millions of dollars a year in sales for lasers because they're pretty

69
00:06:47,240 --> 00:06:53,040
much ubiquitous, right? They're from your supermarket scanner all the way to what's emerging

70
00:06:53,040 --> 00:07:00,400
light art technologies. So lasers have become this really important technological piece in

71
00:07:00,400 --> 00:07:06,000
regards to censored technologies. And so the data coming from this is quite large. And

72
00:07:06,000 --> 00:07:10,760
we'd like to tune them. So one of the things that I noticed was that people were like trying

73
00:07:10,760 --> 00:07:15,800
to tune them sort of hand-tune these lasers for performance and then environmentally sealing

74
00:07:15,800 --> 00:07:20,440
it. So whatever you did to tune it, you'd lock it in and you'd sell it. And if anything

75
00:07:20,440 --> 00:07:24,320
happened, then you'd have to have your rep come out and fix the laser. And then I was

76
00:07:24,320 --> 00:07:29,560
thinking, well, this is a lot easier than a self-driving car. In fact, nobody gets hurt.

77
00:07:29,560 --> 00:07:35,200
And it's only a few knobs. And that was the remarkable thing. This community has been

78
00:07:35,200 --> 00:07:41,120
very slow on the uptake, I feel, in terms of adopting this technology. So we've been

79
00:07:41,120 --> 00:07:45,680
really pushing it out there. So one of the things I highlighted at the talk was, here's

80
00:07:45,680 --> 00:07:51,040
an architecture, a software architecture that you could implement today towards essentially

81
00:07:51,040 --> 00:07:55,880
a laser system that would tune itself. And not only would tune itself, it would learn all

82
00:07:55,880 --> 00:08:01,280
the possible behaviors that has available to itself. And even if something happens,

83
00:08:01,280 --> 00:08:06,320
someone hit the laser and so you've changed the parameter space a little bit, the same

84
00:08:06,320 --> 00:08:12,440
could learn its new parameters and learn how to tune itself under this new conditions.

85
00:08:12,440 --> 00:08:17,920
If I can interrupt, let's maybe take a step back and talk through what it means to tune

86
00:08:17,920 --> 00:08:23,240
a laser. What are those knobs that are used in tuning them?

87
00:08:23,240 --> 00:08:30,000
Sure, sure. So the laser that I've mostly worked on is a fiber optic laser. So it's nice.

88
00:08:30,000 --> 00:08:36,160
It has its own cavity. It's just the laser, the fiber itself contains the light. And the

89
00:08:36,160 --> 00:08:40,520
way that the mode, what's called mode locking happens, in other words, you want to create

90
00:08:40,520 --> 00:08:46,520
a nice, very high intensity pulse of light. And in order to shape the light, there are

91
00:08:46,520 --> 00:08:53,120
a set of polarizers and some waveplates that you can turn, you know, you can rotate them

92
00:08:53,120 --> 00:08:58,520
through 360 degrees. And it turns out, as you turn these set of waveplates and polarizers,

93
00:08:58,520 --> 00:09:04,320
you can get very different behavior out of the laser cavity. And so it's a pretty, you

94
00:09:04,320 --> 00:09:08,900
know, there's about five knobs. There's three waveplates, a polarizer, and also the

95
00:09:08,900 --> 00:09:13,960
amount of pumping or energy put into the cavity. And with these five parameters, you can

96
00:09:13,960 --> 00:09:20,240
get either very exceptionally high performing lasers or you can get just garbage. It's completely

97
00:09:20,240 --> 00:09:28,240
in coherent light that's worthless. So the idea is to use these five parameters to

98
00:09:28,240 --> 00:09:36,560
tune this thing into not only a very coherent ultra short pulse with very high peak intensities,

99
00:09:36,560 --> 00:09:42,280
but among all the set of, there's these islands of stable behaviors, which one should you

100
00:09:42,280 --> 00:09:50,960
be on to give you the very highest performance possible for the fiber and for the gain you

101
00:09:50,960 --> 00:09:58,640
have available to you. The approach you took was an interesting one. It was ultimately,

102
00:09:58,640 --> 00:10:04,520
you know, in terms of the spectrum of machine learning applications, it was fairly simple,

103
00:10:04,520 --> 00:10:10,560
but the way you constructed the problem, you know, is what lent it to that simple approach.

104
00:10:10,560 --> 00:10:15,640
Can you kind of talk through the problem construction and ultimately the solution?

105
00:10:15,640 --> 00:10:20,560
Yeah, sure. So, of course, the first thing I, I guess, one way I think about machine learning

106
00:10:20,560 --> 00:10:24,880
in general is the first thing you have to pause it is some kind of loss function, right?

107
00:10:24,880 --> 00:10:29,280
So this is, this is the, I mean, I think of all of machine learning AI is just really

108
00:10:29,280 --> 00:10:34,360
just some giant optimization problem. And so the main thing with an optimization is just

109
00:10:34,360 --> 00:10:38,800
figuring out what your objective function. The nice thing for us in the laser system

110
00:10:38,800 --> 00:10:45,640
is it's very clear what the objective function was. We wanted a very coherent high intensity

111
00:10:45,640 --> 00:10:52,160
pulse. Now, interestingly enough, in optics, you, you don't actually measure the electric

112
00:10:52,160 --> 00:10:57,720
field. You, you measure the intensity of electric field. So you lose phase information. So right

113
00:10:57,720 --> 00:11:02,200
away, you're limited in terms of what your measurement space is. And there's also one

114
00:11:02,200 --> 00:11:05,960
of the interesting thing. You might say I want the maximum energy in the cavity, but that

115
00:11:05,960 --> 00:11:11,600
has actually not the best objective function available to you. It turns out if you just

116
00:11:11,600 --> 00:11:18,000
make the objective function the energy, then what it will do is the best performance of

117
00:11:18,000 --> 00:11:22,760
the laser cavity will put you at the edge of instability. So it's basically a walk you

118
00:11:22,760 --> 00:11:27,440
up this, it'll walk you up to the edge of cliff. And the cliff, of course, has the best

119
00:11:27,440 --> 00:11:34,280
performance, but you move slightly one way and you lose everything. And so we've formulated

120
00:11:34,280 --> 00:11:39,160
a new objective function, which was based upon not only do I want a lot of energy in the

121
00:11:39,160 --> 00:11:46,160
cavity, I also want to constrain the laser bandwidth. So we were able to construct an objective

122
00:11:46,160 --> 00:11:51,480
function that had both the bandwidth and the energy. And that actually allowed us to keep

123
00:11:51,480 --> 00:11:57,480
a boundary away from the instability. So once we have that objective function, by bandwidth

124
00:11:57,480 --> 00:12:02,880
are you ultimately talking about coherence or, but you said you didn't, you weren't able

125
00:12:02,880 --> 00:12:06,640
to directly measure coherence if I remember correctly. Well, so we can't measure the

126
00:12:06,640 --> 00:12:10,840
coherence directly, but we can measure the energy. We can also measure through the

127
00:12:10,840 --> 00:12:17,480
spectrogram, just what the total frequency content is. So when you look on a oscilloscope,

128
00:12:17,480 --> 00:12:21,560
you'll be able to see immediately what the band, the total bandwidth is. And so you're

129
00:12:21,560 --> 00:12:28,080
trying, you know, for a very short pulse, you have a nice broad bandwidth pulse. So typically,

130
00:12:28,080 --> 00:12:33,600
for these laser systems, they can produce somewhere around 30 nanometers of bandwidth. So

131
00:12:33,600 --> 00:12:39,880
it's pretty broad band. It's around telecom wavelengths. And so it's a great light source

132
00:12:39,880 --> 00:12:46,600
this way. So we were able, so we know then that what we were trying to do roughly speaking

133
00:12:46,600 --> 00:12:50,280
is, you know, produce this. And so we were able to construct the objective function.

134
00:12:50,280 --> 00:12:55,120
That was the number one step initially was just to simply figure out, given for what

135
00:12:55,120 --> 00:13:01,040
we think we want, how do we actually mathematically construct this for an algorithm to go after

136
00:13:01,040 --> 00:13:06,440
the search, right, go after the optimization. So we were able to do and also make sure

137
00:13:06,440 --> 00:13:10,960
that we stay away from the edge of instability for physical systems, right? It really matters

138
00:13:10,960 --> 00:13:15,440
that you've got to have a cushion away from the boundary. You can't just walk up a gradient

139
00:13:15,440 --> 00:13:19,040
and off a cliff. Otherwise, it's it's worth this technology.

140
00:13:19,040 --> 00:13:25,280
And did you consider, as opposed to the objective function with two components, energy and

141
00:13:25,280 --> 00:13:33,480
bandwidth, some formulation that tries to maximize energy, but then back it off or maybe even

142
00:13:33,480 --> 00:13:38,360
out, even outside of the machine learning, maximize energy, but then in the implementation,

143
00:13:38,360 --> 00:13:44,720
back it off a little bit. Yeah, we had various perspectives on this. We

144
00:13:44,720 --> 00:13:48,720
automatically could do something like this, which is once we have the learning, we could

145
00:13:48,720 --> 00:13:53,600
back off of these things. But it turned out it was so easy to use this scope tray. So

146
00:13:53,600 --> 00:13:58,520
in others, that was that was probably the easiest measurement of all that we use that directly

147
00:13:58,520 --> 00:14:03,320
as, okay, this is a perfect measurement because that's something that is is actually probably

148
00:14:03,320 --> 00:14:08,720
the easiest thing for to actually make a measurement of you. It's actually very difficult to

149
00:14:08,720 --> 00:14:13,680
even measure the light pulse itself in the cavity because typically these pulses are

150
00:14:13,680 --> 00:14:20,080
in the 100 attempt to second regime. So this is ultra fast optics. And so measurement

151
00:14:20,080 --> 00:14:26,000
of the light pulse itself requires you either to put on some kind of very expensive measurement

152
00:14:26,000 --> 00:14:29,680
device. And of course, you don't want that in a practical setting because you'd like

153
00:14:29,680 --> 00:14:37,160
a cheap device. And so we just use the bandwidth itself as a proxy. Okay. And then once

154
00:14:37,160 --> 00:14:42,320
you have that, you can now you can set the thing free. It's got five input knobs, these

155
00:14:42,320 --> 00:14:48,800
four or three wave plates of polarizer, the gain, and you just say, okay, go look around.

156
00:14:48,800 --> 00:14:53,360
And the nice thing is these parameters, so this is another nice feature. These wave plates

157
00:14:53,360 --> 00:14:59,160
and polarizers, right, they, they, they, there's two pi periodic, right? You rotate a polarizer

158
00:14:59,160 --> 00:15:04,600
around, it goes around two pi, you're back where you started. So it allowed us to do what

159
00:15:04,600 --> 00:15:09,720
we call the toroidal search, right? So you could start spinning these wave plates and polarizers

160
00:15:09,720 --> 00:15:15,480
at irrationally related frequencies. And what this will do, it was will cover some kind

161
00:15:15,480 --> 00:15:21,560
of hyper torus. And so it's a very effective search strategy because if you have parameters

162
00:15:21,560 --> 00:15:28,080
that don't wrap around themselves, right? So the slowest, the slowest search, when it

163
00:15:28,080 --> 00:15:34,200
goes around two pi, all the other ones have gone around even multiples times, it allows

164
00:15:34,200 --> 00:15:39,000
you to map out this, this torus structure and look in the torus where the good regions

165
00:15:39,000 --> 00:15:42,320
are for, for these, for these poll solutions.

166
00:15:42,320 --> 00:15:46,800
There was an element that I recall from the presentation that was not just that you

167
00:15:46,800 --> 00:15:54,000
were able to optimize the subjective function, but that you were also able to do it in such

168
00:15:54,000 --> 00:16:02,480
a way that gave you insight into the underlying physical principles. Can you, are we at that

169
00:16:02,480 --> 00:16:08,360
point in kind of the story yet? Or yeah, we can certainly talk about that because I think

170
00:16:08,360 --> 00:16:13,840
that's actually one of the more exciting places. So we started with this idea of machine

171
00:16:13,840 --> 00:16:18,400
learning on the laser for control, which was, you know what, if this thing can just tune

172
00:16:18,400 --> 00:16:26,240
it, we're fine. It can be a sort of an AI agent sitting on top of sort of these machines

173
00:16:26,240 --> 00:16:32,120
and you just give it the knobs input output and get it going. But the, where this led to

174
00:16:32,120 --> 00:16:37,600
was this idea of data driven discovery of governing equations. So we started thinking

175
00:16:37,600 --> 00:16:44,080
about if I measure system and I don't know it's underlying physics. So a lot of the models

176
00:16:44,080 --> 00:16:50,480
I had spent time with in the past, I actually knew the governing physics. Optical fields

177
00:16:50,480 --> 00:16:54,080
are governed by Maxwell's equations and of course they have these quantum interactions.

178
00:16:54,080 --> 00:16:58,160
So we can write down these governing equations and, you know, we've had a lot of experience

179
00:16:58,160 --> 00:17:04,560
with them. We know that they are true in, in these regimes we're operating in and so

180
00:17:04,560 --> 00:17:09,920
we can do in some sense first principles modeling. But another part of my life that developed

181
00:17:09,920 --> 00:17:14,000
over the last five to ten years was looking at neuroscience. So now you'd measure from

182
00:17:14,000 --> 00:17:20,280
the brain and we don't know the governing equations of the brain. We don't have an F equals

183
00:17:20,280 --> 00:17:26,240
M A formulation. In other words, here are the governing equations. We have posited various

184
00:17:26,240 --> 00:17:34,000
models for what neurons do. But on some level you're interrogating a system where you fundamentally

185
00:17:34,000 --> 00:17:41,040
don't know the governing equations. And so what we started to, what we set out to do then

186
00:17:41,040 --> 00:17:46,720
is to start figuring out, is there a way from measurements alone, time series measurements,

187
00:17:47,360 --> 00:17:53,520
can we infer the governing equations? And the beauty of this is it all comes down to just

188
00:17:53,520 --> 00:17:58,560
Ax equal to B. So we're excited because that's, you know, I like to tell everybody that's

189
00:17:58,560 --> 00:18:03,680
the height of applied math. If you actually understand Ax equal to B, that's something because

190
00:18:03,680 --> 00:18:08,640
Ax equal to B is a lot harder than it looks. You know, when I think of a Maxwell's equation,

191
00:18:08,640 --> 00:18:14,560
it's a lot more complicated than a linear, a linear equation. How, talk a little bit about that

192
00:18:14,560 --> 00:18:22,000
background of how you kind of equate these ultimately complex and and higher order equations

193
00:18:22,000 --> 00:18:28,640
to a linear relationship? Sure, you bet. So the way we've formulated the problem is we take time

194
00:18:28,640 --> 00:18:34,560
series measurements of some dynamical systems. So we think of these things as systems were

195
00:18:34,560 --> 00:18:39,760
measuring, they're changing in time, they're evolving. And we would like to discover the equations

196
00:18:39,760 --> 00:18:48,400
of motion. So one of the my backgrounds mathematically that really framed everything I did application

197
00:18:48,400 --> 00:18:55,840
wise was in dynamical systems. And the basic structure for that is to look at DXDT. The change of

198
00:18:55,840 --> 00:19:02,800
some vector X is equal to F of X. So it could be a nonlinear function of X. It could be

199
00:19:02,800 --> 00:19:08,400
very complicated. It could even be have parametric dependencies. So that's the simplest thing.

200
00:19:08,400 --> 00:19:16,480
DXDT equals F of X. And this is already framing what we want to do. And in a sense that the

201
00:19:16,480 --> 00:19:22,400
standard way we think about when we pick up a textbook on E and M or quantum, we're given F.

202
00:19:22,400 --> 00:19:28,160
We know what it is. Now what we're trying to do is say you give me time series measurement X.

203
00:19:28,160 --> 00:19:34,320
Can I tell you what F was? What was the F that's consistent with the data I read? So

204
00:19:34,320 --> 00:19:40,240
DXDT is the first derivative of with respect to time. And what you're doing is you're taking

205
00:19:41,360 --> 00:19:46,880
you've got some time series measurements that you're taking and you're using them to ultimately

206
00:19:46,880 --> 00:19:53,200
work your way back to the underlying function from these time series measurements. Exactly.

207
00:19:53,200 --> 00:19:58,960
And so the way we do this is we simply say the following. We say, okay, DXDT. So let's call

208
00:19:58,960 --> 00:20:05,440
that X dot. So it's the rate of change of time. So if you give me time series data, I can differentiate

209
00:20:05,440 --> 00:20:13,360
it. But you gave me X. And that means I can compute X dot. Okay, that's for an X dot is going to be

210
00:20:13,360 --> 00:20:19,920
the B vector in AX equal to B. Now let's talk about what the A is going to be. So now what

211
00:20:19,920 --> 00:20:24,000
we're going to do is say, well, what could the right hand side be? Well, the right hand side could

212
00:20:24,000 --> 00:20:28,400
be a lot. There's lots of possibilities. And I don't know what they are. So what I'm going to do

213
00:20:28,400 --> 00:20:33,440
is just make up a library of all kinds of potential right hand sides. Well, maybe the right hand side

214
00:20:33,440 --> 00:20:42,480
depends on X or X squared or or X cubed or sine X or E to the X. It's only limited by my imagination.

215
00:20:42,480 --> 00:20:49,440
So the idea is typically to put in a bunch of potential functions, which could be right hand side

216
00:20:49,440 --> 00:20:57,920
terms. So the matrix A we're going to build is the following. Each column will be a candidate

217
00:20:57,920 --> 00:21:05,360
right hand side functions, a candidate that for describing what F is. Okay, so I could put in

218
00:21:05,360 --> 00:21:12,640
thousands of candidates. And that's what creates the columns of the matrix A. And each row is the

219
00:21:12,640 --> 00:21:19,280
time measurement. So if you give me X, I can compute X squared. I can compute sine X over time.

220
00:21:19,280 --> 00:21:25,120
So the rows will be time columns will be potential functions that might be in the right hand side.

221
00:21:25,120 --> 00:21:30,160
That's the matrix A. So what we're going to do when we do this regression, this AX equal to B is

222
00:21:30,160 --> 00:21:34,640
just say, well, here's a matrix A. The right hand side B I have, which is the time derivative,

223
00:21:34,640 --> 00:21:39,680
solve it. And generally, you're going to have a lot of time measurements. So this is going to be

224
00:21:39,680 --> 00:21:45,760
a system, which is highly over determined. And I want a solution. And here's the key. I want a

225
00:21:45,760 --> 00:21:52,240
sparse solution. So what's going to happen is when you do this regression, it's going to select

226
00:21:52,240 --> 00:22:00,240
out the smallest number of columns that they have to use so that X dot is equal to F of X.

227
00:22:00,240 --> 00:22:08,000
So, and when it does this, it selects out the physical terms. So for instance, what this will do

228
00:22:08,000 --> 00:22:14,000
for you is if you measure a fluid flow, just take measurements of the time series of a

229
00:22:14,000 --> 00:22:20,000
vorticity of a fluid flow, what comes out in this regression process is the navier stokes equations.

230
00:22:20,000 --> 00:22:25,600
If you were to look at trajectories of a flying object, thrown object, what would come out would

231
00:22:25,600 --> 00:22:31,600
be F equals M A? It recovers all of these systems, all the canonical models out of mathematical

232
00:22:31,600 --> 00:22:36,800
physics. You just give me the time series measurements. It recovers back the governing equations,

233
00:22:36,800 --> 00:22:43,760
all by this simple regression of a linear system where the matrix contains all these potential

234
00:22:43,760 --> 00:22:48,400
library terms. And by promoting a sparse solution, it just gives you back your solution.

235
00:22:48,400 --> 00:22:55,680
Yeah, so I thought this part was really clever. Yeah, again, as you just said it, you are solving

236
00:22:55,680 --> 00:23:03,520
just a simple linear regression, but your terms are, you know, they can be anything. They can be

237
00:23:03,520 --> 00:23:10,480
quadratics and qubits and exponentials and all these different things. What is this? You know,

238
00:23:10,480 --> 00:23:16,240
in a lot of ways, it's like a decomposition. Do you consider this part of what you did to be

239
00:23:16,240 --> 00:23:24,640
kind of a novel contribution of your researcher is that, you know, is it really more, you know,

240
00:23:24,640 --> 00:23:31,680
standard or well explored and you're applying it to these types of physical problems?

241
00:23:31,680 --> 00:23:38,400
As far as we've seen, this is the first application of this towards really discovering dynamics

242
00:23:38,400 --> 00:23:46,240
itself and getting out dynamical systems. What's kind of remarkable about it is, in some sense,

243
00:23:46,240 --> 00:23:52,320
how to say it in a good way, we had no business discovering this. While this wasn't discovered

244
00:23:52,320 --> 00:23:57,920
30 years ago, it's so simple, right, that like, you know, it's like, why didn't someone do this 30

245
00:23:57,920 --> 00:24:01,840
years ago? They should have. They should have. They really should have. And it's really interesting.

246
00:24:01,840 --> 00:24:06,000
I think from the physics community and the engineering community, and I'm going to talk

247
00:24:06,000 --> 00:24:11,760
personally for a minute here, if I look back at my educational experience, whenever we solved

248
00:24:11,760 --> 00:24:17,280
under or over determined systems that were AX equal to B, you just did it with Lee Square fitting.

249
00:24:18,000 --> 00:24:23,120
There was no concept of sparse solutions there. You know, it's interesting because

250
00:24:23,120 --> 00:24:28,160
Tip Shirani has a paper on the last so method from 1996, which have gotten a ton of traction,

251
00:24:28,160 --> 00:24:34,160
this basically variable selection technique, but that never made it over to like this physics

252
00:24:34,160 --> 00:24:39,120
engineering community. And in some sense, the technology has been just sit there, and we just

253
00:24:39,120 --> 00:24:44,320
finally made the observation. Well, wait a minute. This actually can work here and look at this.

254
00:24:44,320 --> 00:24:49,920
It actually works. And you can just run it on your laptop. It's not like this. It's not one of these

255
00:24:49,920 --> 00:24:57,040
neural, you know, neural networks or even another competing technology developed by Hod Lipson at

256
00:24:57,040 --> 00:25:02,320
he's now Columbia. They had done something similar, but they had done a genetic search. So,

257
00:25:02,320 --> 00:25:08,080
you know, they basically would put a genetic algorithm that's positing some right hand side terms,

258
00:25:08,080 --> 00:25:12,560
which ones are best. Let's keep them, let's modify them and have them evolve until we find this.

259
00:25:12,560 --> 00:25:17,840
So it was fairly expensive computationally, because it was really going through tons of

260
00:25:17,840 --> 00:25:25,760
possibility versus this regression is extremely simple. It's fast, it's effective, it's very robust.

261
00:25:25,760 --> 00:25:31,920
And since that time, we've been starting to use this on more complicated problems where we

262
00:25:31,920 --> 00:25:36,320
don't know the answer. So for all the problems that we know the answer, which are these canonical

263
00:25:36,320 --> 00:25:40,880
models out of the mathematical physics engineering community, we've been able to discover them all.

264
00:25:40,880 --> 00:25:46,080
So we've been able to sort in some sense validate or cross validate the method. And now we're

265
00:25:46,080 --> 00:25:51,120
moving it on to biological applications. And one of the exciting areas we've been trying

266
00:25:51,120 --> 00:25:57,120
to find this on is the sea elegance worm, which is a one millimeter worm. It's got 302 neurons.

267
00:25:58,480 --> 00:26:03,600
And when you watch these neurons, they can now do whole brain imaging of this little worm.

268
00:26:03,600 --> 00:26:09,920
It's amazing what people can do with this worm. We are now starting to understand that when the

269
00:26:09,920 --> 00:26:14,800
worm is forward crawling or backward crawling, it's very low dimensional structure that's

270
00:26:14,800 --> 00:26:18,960
being executed there. And what we really want to understand is the control architecture

271
00:26:18,960 --> 00:26:25,040
and the governing equations of how 302 neurons coordinate their dynamics to produce

272
00:26:25,920 --> 00:26:32,160
very simple dynamics, which are functional across the entire connectome of that worm.

273
00:26:32,160 --> 00:26:37,840
So this is an exciting way forward because there is no truth in this case. Nobody has the right

274
00:26:37,840 --> 00:26:43,200
answer, right? So this is going to give us the ability to get towards understanding biological

275
00:26:43,200 --> 00:26:49,360
neural systems with a very new architecture, which seems to be incredibly robust and easy to use.

276
00:26:51,440 --> 00:26:58,240
So I want to dig a little bit further into the biological applications. But before we do that,

277
00:26:58,240 --> 00:27:07,120
I wanted to connect a couple of points that you mentioned. You mentioned the idea of

278
00:27:07,120 --> 00:27:16,400
sparsity as being key to what you've done here. And the idea there is that if you collect all

279
00:27:16,400 --> 00:27:24,320
these data points, even if the equations are perfectly expressed in simple underlying

280
00:27:24,320 --> 00:27:31,680
relationships between these terms, you've got noise, you've got other things, and the result of

281
00:27:31,680 --> 00:27:40,560
your analysis, your optimization is going to likely include some very small fractional

282
00:27:40,560 --> 00:27:46,800
terms of things that don't necessarily need to be there. And so what you mentioned,

283
00:27:47,600 --> 00:27:52,240
the lasso work in passing, if I remember correctly, what you did was you implied lasso to

284
00:27:54,000 --> 00:27:57,600
bring out that sparsity to reduce those terms. Is that correct?

285
00:27:57,600 --> 00:28:03,360
Yeah, so we use something lasso-like. So what we actually did, and this is

286
00:28:04,880 --> 00:28:09,920
maybe taken as a criticism alasso, it's interesting if you really look at what people do,

287
00:28:09,920 --> 00:28:17,200
people tend not to use lasso directly, but something like elastic net. So the lasso itself is

288
00:28:17,200 --> 00:28:22,640
a little bit unstable in terms of actually finding for you the right sparsity pattern that you're

289
00:28:22,640 --> 00:28:30,560
looking for. And so elastic net helps regularize this process. But we actually avoid those

290
00:28:30,560 --> 00:28:38,240
altogether. What we do is we do a sequential least square thresholding. So essentially least

291
00:28:38,240 --> 00:28:44,480
squares is extremely fast, but what least squares will do is says everybody participates. So there

292
00:28:44,480 --> 00:28:50,480
is nobody that's zero. However, there's a bunch of terms that, well, you know, it depends on how

293
00:28:50,480 --> 00:28:58,000
you define zero. It's 10-6-0. So we start doing this in the sense that we say, it works incredibly

294
00:28:58,000 --> 00:29:03,120
well where you can just do these squares, which is very fast. All the terms below some threshold,

295
00:29:03,120 --> 00:29:08,000
you throw away. And then you do it again with the terms that are left over, and this thing converges

296
00:29:08,000 --> 00:29:15,680
right back right to the solution you want. So it's in some sense a more stable way. We found

297
00:29:15,680 --> 00:29:21,680
it to be stable. We're working actually right now on convergence proofs around this, but we found

298
00:29:21,680 --> 00:29:30,480
it to be a very stable robust way to do sparse regression. And so it's lasso-like, but it doesn't

299
00:29:30,480 --> 00:29:37,840
inherit some of the known problems of lasso itself. How do you end up setting the threshold? If you're

300
00:29:37,840 --> 00:29:45,120
working on modeling something that's got a constant like a Planck's constant that's ultimately

301
00:29:45,120 --> 00:29:50,960
very, very small, how do you make sure you're not getting rid of important things? Yeah, so that's

302
00:29:50,960 --> 00:29:56,240
always a great question. I mean, so what we've been doing so far for most of the models that we have

303
00:29:56,240 --> 00:30:04,640
is we will do some kind of cross validation for this threshold level. And actually it's interesting

304
00:30:04,640 --> 00:30:10,320
the threshold level, we haven't done this yet, but probably the most effective way to do it is to

305
00:30:10,320 --> 00:30:16,880
have an adaptive threshold level. So if you're on threshold, if you're going to apply the threshold

306
00:30:16,880 --> 00:30:20,880
and your iteration number three, it should probably be different than on iteration number two.

307
00:30:20,880 --> 00:30:26,640
But even if you don't do something like that, you can typically learn it through cross validation

308
00:30:26,640 --> 00:30:32,480
what the sparsity, what the threshold level should be. But that is your one tuning knob. And by the

309
00:30:32,480 --> 00:30:41,920
way, it's very, very similar to last so where in the last so you have to tune what this L1 penalty

310
00:30:41,920 --> 00:30:48,800
strength is. So typically there's this lambda parameter where you can make it bigger or smaller.

311
00:30:48,800 --> 00:30:55,200
And the bigger you make it, the more it enforces sparsity. And that's very similar to the threshold

312
00:30:55,200 --> 00:31:02,640
here, which is it's it's a tuning knob that all of these sparsity promoting techniques seem to

313
00:31:02,640 --> 00:31:09,280
carry around. And again, there are ways that people suggest or argue you can kind of cross validate

314
00:31:09,280 --> 00:31:14,800
to get the right values for these things. But it's still a little bit of a still a little bit of

315
00:31:14,800 --> 00:31:20,560
alchemy on that one. Okay. And so you were starting to introduce us into to some of your future

316
00:31:20,560 --> 00:31:27,200
application of this moving away from these underlying physical control systems where we we know a

317
00:31:27,200 --> 00:31:35,600
lot of the equations to something that is more complex and unknown. And in this case, the

318
00:31:36,640 --> 00:31:42,960
the relationship between neurons and the C elegans. Is that the right way to say it? Yeah, yeah,

319
00:31:42,960 --> 00:31:48,640
exactly. And I would say more broadly, the thing that we're really trying to go after

320
00:31:48,640 --> 00:31:56,400
sort of in sort of the grand challenge problem of this, I still remain very rooted in thinking

321
00:31:56,400 --> 00:32:02,720
about engineering and physical applications. But one of the things that's become increasingly

322
00:32:02,720 --> 00:32:11,440
clear to me is what we've developed over the last centuries is the ability to exceptionally well

323
00:32:11,440 --> 00:32:17,120
with what I call uniscale physics problems. In other words, I can write down one set of equations

324
00:32:17,120 --> 00:32:22,800
and models in the whole system. But it's increasingly clear that a lot of these complex systems we

325
00:32:22,800 --> 00:32:29,040
look at that we want to understand today, they're multi-scale systems. There's a fast scale that

326
00:32:29,040 --> 00:32:35,360
is doing some kind of physical process that's connected to some mezzo scale, which is connected

327
00:32:35,360 --> 00:32:40,480
to some macro scale. And by the way, they're all talking to each other. And they're all very

328
00:32:40,480 --> 00:32:47,920
different physics at these levels. And yet that is the dynamics that really produces the manifestation

329
00:32:47,920 --> 00:32:52,240
of the dynamics on the measurement space you may be looking at. So for instance, you may not care

330
00:32:52,240 --> 00:32:56,400
about the micro scale because you're watching it from the macro scale. But whatever happened

331
00:32:56,400 --> 00:33:01,840
in the micro scale is in fact feeding that information up. And you need to, if you're going to model

332
00:33:01,840 --> 00:33:06,720
the system well, you have to understand how these different scales are talking to each other.

333
00:33:06,720 --> 00:33:12,400
So it really requires us to start moving away from uniscale physics models, where I just write

334
00:33:12,400 --> 00:33:17,600
everything down and do simulations, too. Actually, there's different physics at each level.

335
00:33:17,600 --> 00:33:23,280
I've got to figure out how they're connected together and discover different physics models at

336
00:33:23,280 --> 00:33:29,440
each level. And that's going to be the way we make progress forward. At least that's those,

337
00:33:29,440 --> 00:33:34,240
and by the way, biology is a perfect representation of this. Even in the sea elegans, there's

338
00:33:34,240 --> 00:33:40,240
quite a different number of different time scales that ultimately produce time and spatial

339
00:33:40,240 --> 00:33:44,720
scales that ultimately produce the thing that I'm most interested in, which is this macroscopic

340
00:33:44,720 --> 00:33:50,960
behavior of the worm crawling forward or crawling backward or doing what are called omega turns,

341
00:33:50,960 --> 00:33:56,320
where it turns its body around. And the only way to understand that completely is to understand

342
00:33:56,320 --> 00:34:02,160
this architecture of multi-scale physics. And so to make that a little bit more concrete,

343
00:34:02,160 --> 00:34:09,200
we're talking about one scale might represent the chemical, the underlying chemical reactions,

344
00:34:09,200 --> 00:34:15,760
and then there's a scale above that that's representing neurological activity and connections

345
00:34:15,760 --> 00:34:22,240
and things like that. And then finally, at the top scale, how the entire set of neurons are

346
00:34:22,240 --> 00:34:28,640
sort of evolving together in a functional form, all being driven by the chemistry, which drives

347
00:34:28,640 --> 00:34:33,760
the voltage activity of neurons, which then drives this bigger functionality of muscle activation.

348
00:34:34,960 --> 00:34:40,080
Yeah, that's exactly the cascade. And you know that the muscle activation is a very different physics

349
00:34:40,080 --> 00:34:46,000
than the chemical activations, right? These are different physical processes. And you can't just

350
00:34:46,000 --> 00:34:52,160
write one model down that says, you know, and again, I think the discovery we've made in physics,

351
00:34:52,160 --> 00:34:56,240
right? We have Maxwell's equation, we have quantum mechanics, we have the fluid dynamics,

352
00:34:56,240 --> 00:35:01,680
navier strokes of fluids, we've done impressive things, right? We can build airplanes,

353
00:35:01,680 --> 00:35:07,440
we can build iPhones, you know, quantum based devices. The kind of stuff we can do is quite

354
00:35:07,440 --> 00:35:12,400
remarkable, but we've kind of hit a wall there in terms of our modeling capabilities.

355
00:35:13,440 --> 00:35:21,920
It strikes me that in the physical case, you know, we've got these multi-scale problems do exist.

356
00:35:21,920 --> 00:35:26,720
Maybe we just don't understand them because of their complexity or the number of skills or

357
00:35:26,720 --> 00:35:32,480
things like that, meaning, you know, at some level, there's quantum mechanics and there are

358
00:35:34,320 --> 00:35:41,280
I'm thinking even for simple kind of mechanics problems, you know, there's, you know, maybe even an

359
00:35:41,280 --> 00:35:47,920
infinite number of levels, you know, ranging to, you know, atomic cohesion and all kinds of stuff,

360
00:35:47,920 --> 00:35:55,120
I don't know. Yeah. Are you getting a question in there somewhere? Yeah. No, but it's actually

361
00:35:55,120 --> 00:35:58,880
interesting you asked this, right? Because I think the way I think of multiple scale physics,

362
00:35:58,880 --> 00:36:04,960
like, you know, if I was in a push a table and see how it moved, I push it with a certain force,

363
00:36:06,240 --> 00:36:11,040
you know, typically we do that, we draw like a vector diagram, okay? Your force is this,

364
00:36:11,040 --> 00:36:15,200
you know, this much at this angle and you could compute through ethical, so may what's going to

365
00:36:15,200 --> 00:36:22,000
happen to the table movement. An alternative to this would be I'd model the atomic interaction

366
00:36:22,000 --> 00:36:26,800
between my hand pushing the table and the atoms of the table. Of course, nobody does this, but

367
00:36:26,800 --> 00:36:31,520
there are systems where you're required to do that. You really need to do that kind of, so

368
00:36:31,520 --> 00:36:36,880
especially molecular dynamics simulations, this is what they're trying to do, right? Really

369
00:36:36,880 --> 00:36:45,040
model the molecular interactions that produce some macro scale structure that's important to see.

370
00:36:45,040 --> 00:36:51,120
Okay. Okay. So that's where we're headed, I think. Interesting. And so how far have you gotten

371
00:36:51,120 --> 00:37:00,880
in applying this technique to these kinds of multi-scale problems? So far, we have a first paper that

372
00:37:00,880 --> 00:37:07,520
just came out, it's on the archive, where we did essentially some toy problems where we've had mixed

373
00:37:07,520 --> 00:37:13,840
dynamics, a fast scale and a slow scale dynamical system and we asked the following question. If I

374
00:37:13,840 --> 00:37:19,760
just gave you the data and I'm just measuring this joint dynamics together, what would I get?

375
00:37:21,040 --> 00:37:25,680
Do I actually, can I extract them from each other? Can I, is there a way to disambiguate what's

376
00:37:25,680 --> 00:37:30,800
going on in the fast scale from the slow scale and could I really discover these two dynamics? And

377
00:37:30,800 --> 00:37:36,640
the initial evidence is suggest yes, you can. In fact, you can discover a slow and a fast dynamics,

378
00:37:36,640 --> 00:37:41,280
you can even discover how they're connected together. So we've done this on some, let's call it the

379
00:37:41,280 --> 00:37:45,840
baby toy problem. I mean, that's where you always have to start. I always like to start with something

380
00:37:45,840 --> 00:37:51,360
where we can test the truth against it, right? One of the things that makes me uncomfortable in some

381
00:37:51,360 --> 00:37:57,200
of the application areas of some of the, these methods that I've seen is people will do hard problems

382
00:37:57,200 --> 00:38:01,200
and sometimes they'll neglect to do the very simple problem to see if it even just works on the

383
00:38:01,200 --> 00:38:05,760
simple problem. And I never go forward unless I can get it to work on a simple problem because

384
00:38:05,760 --> 00:38:09,600
if you can't get that to work, it's certainly not going to say anything about a hard problem.

385
00:38:09,600 --> 00:38:17,520
Right. And so I think the ability to test bet it and see if it works is important. So we've got

386
00:38:17,520 --> 00:38:22,080
that first result along that direction and then our hope is to continue building this way.

387
00:38:22,080 --> 00:38:28,480
And then also acquiring good data sets is a really important thing. You know, getting a good

388
00:38:28,480 --> 00:38:33,360
data set from a, you know, physical biological system where there's good time resolution, good

389
00:38:33,360 --> 00:38:38,160
spatial resolution where you can really bring these methods on. We're getting better and better

390
00:38:38,160 --> 00:38:42,000
about how to handle bad data. But you know, at the end of the day, there's a great deal of pressure.

391
00:38:42,000 --> 00:38:47,680
If you want to build a good model, you need good data, right? I think that's one of the really

392
00:38:47,680 --> 00:38:53,760
big advantages of computer vision, right? You can, you can bring in a bunch of really good

393
00:38:53,760 --> 00:39:01,200
data sets, right? Here's pictures. They're high quality HD cameras. And that helps a lot. They're

394
00:39:01,200 --> 00:39:07,280
not, you imagine if you were trying to train image net where all the images were corrupt and blurred.

395
00:39:07,280 --> 00:39:11,760
You know, how well would it do now on classification tasks? I think it would be much

396
00:39:11,760 --> 00:39:16,800
challenging. And the same thing for us is the case is we want to be able to have really high quality

397
00:39:16,800 --> 00:39:21,600
of data initially. And you know, the biologists are, are getting their job done. It's amazing

398
00:39:21,600 --> 00:39:26,240
the revolutions you're seeing in biology in terms of the kind of recordings they can do.

399
00:39:28,000 --> 00:39:32,880
The, just the vast sheer quantity of what they're able to extract now from even in the

400
00:39:32,880 --> 00:39:37,360
C. elegans worm. They'll be able to image this thing in real time where you're seeing the whole

401
00:39:37,920 --> 00:39:45,840
brain region 150 neurons just lighting up as it's doing its, its motion is, is just unbelievable.

402
00:39:45,840 --> 00:39:50,160
This is something that 10 years ago, I don't think anybody thought was possible. And here we are.

403
00:39:50,160 --> 00:39:56,560
We've got it. So it's really exciting. So you started to answer one of the questions I had, which

404
00:39:56,560 --> 00:40:04,160
was for these different scales for C. elegans, where the data was coming from. It sounds like

405
00:40:04,640 --> 00:40:12,240
at some level, at one of the levels you described is coming from imagery. But is there also a level

406
00:40:12,240 --> 00:40:20,320
beneath that that is looking at actual chemical activity? And I imagine the level above where we're

407
00:40:20,320 --> 00:40:26,720
talking about physical motion is coming from imagery as well. Yeah. So the motion comes from imagery.

408
00:40:26,720 --> 00:40:31,520
A lot of times you can easily just see it moving around. There's the actual imaging of the neurons.

409
00:40:32,480 --> 00:40:38,080
One level down from that would be electrophysiology recordings where you actually stick in a

410
00:40:38,080 --> 00:40:43,600
voltage probe into neurons and you monitor those directly. And they do have this available in

411
00:40:44,320 --> 00:40:49,920
monkeys and rats and other creatures like this where you can actually get some direct recordings

412
00:40:49,920 --> 00:40:55,280
from neurons to see how they're spiking. And so there, of course, is a little harder because,

413
00:40:56,080 --> 00:41:02,240
even if even in your best days you might be able to get a tetrode fork in the brain with

414
00:41:02,240 --> 00:41:09,920
the four prongs or the fork have four recordings each. So maybe get up to 16. And so we can do this

415
00:41:09,920 --> 00:41:15,520
for instance with insects and tenelopes of moths. We work with a gentleman over here in biology,

416
00:41:15,520 --> 00:41:20,880
Jeff Riffle, who has this. So there is more and more data becoming available, but it's still

417
00:41:20,880 --> 00:41:25,280
hard to acquire. And in humans, of course, it's very difficult, right? Because you can't just

418
00:41:25,840 --> 00:41:30,880
go in and put probes in people. They have done this, of course, with certain people like, for

419
00:41:30,880 --> 00:41:36,320
instance, we are working with people with where there are e-cog recordings from the brain,

420
00:41:36,320 --> 00:41:42,000
often a big array. But these were from patients who needed this for because they had such severe

421
00:41:42,000 --> 00:41:48,560
epilepsy that they put this into monitor that. And as a byproduct, we actually have great data

422
00:41:48,560 --> 00:41:54,720
to collect on the brain itself. So we'll see what the future holds in terms of what it's, I think

423
00:41:54,720 --> 00:41:59,360
it's just remarkable how they keep coming up with new and novel ways. I think what we want to do

424
00:41:59,360 --> 00:42:04,560
is be prepared on our end to have the mathematical architecture in place so that when they do produce

425
00:42:04,560 --> 00:42:10,320
these data sets, we can essentially squeeze out as much as possible, as much learning and science as

426
00:42:10,320 --> 00:42:15,840
possible. I think that's what we're really after is that is the mathematical engine so that as

427
00:42:15,840 --> 00:42:21,680
these data sets come in, we can we have a new way to start interrogating that data and discovering

428
00:42:21,680 --> 00:42:27,360
underlying biophysical principles or physical principles. Well, that's fantastic. Well, Nathan,

429
00:42:27,360 --> 00:42:33,520
I really appreciate you taking the time to walk us through this. It's really, really interesting work

430
00:42:33,520 --> 00:42:38,000
and I'm looking forward to kind of keeping tabs on it and seeing what else you guys you all come

431
00:42:38,000 --> 00:42:42,800
up with. Great. Thank you so much for having me. It's been a pleasure and hopefully, hopefully

432
00:42:42,800 --> 00:42:47,040
everybody who listened got something out of this. Absolutely. I'm sure they will. Thanks Nathan.

433
00:42:47,040 --> 00:42:47,920
Okay, take care.

434
00:42:47,920 --> 00:42:57,920
All right, everyone. That's our show for today. For more information on Nathan or any of the topics

435
00:42:57,920 --> 00:43:07,760
covered in this episode, head on over to twomolei.com slash talk slash 162. Don't forget to visit twomolei.com

436
00:43:07,760 --> 00:43:15,680
slash nominate to cast your vote for us and the people's choice podcast awards. And as always,

437
00:43:15,680 --> 00:43:24,400
thanks so much for listening and catch you next time.

