1
00:00:00,000 --> 00:00:26,080
Hey everyone, hope you all had a wonderful holiday.

2
00:00:26,080 --> 00:00:30,840
For the next few weeks we'll be running back the clock with our second annual AI Rewind

3
00:00:30,840 --> 00:00:32,160
series.

4
00:00:32,160 --> 00:00:33,960
Join by a few friends of the show.

5
00:00:33,960 --> 00:00:39,240
We'll be reviewing the papers, tools, use cases, and other developments that made a splash

6
00:00:39,240 --> 00:00:46,720
in 2019 in key fields like machine learning, deep learning, NLP, computer vision, reinforcement

7
00:00:46,720 --> 00:00:49,480
learning, and ethical AI.

8
00:00:49,480 --> 00:00:55,600
Be sure to follow along with the series at twomolai.com slash rewind 19.

9
00:00:55,600 --> 00:00:59,960
As always, we'd love to hear your thoughts on this series, including anything we might

10
00:00:59,960 --> 00:01:01,040
have missed.

11
00:01:01,040 --> 00:01:06,480
Send me your feedback or favorite papers via Twitter, where I'm at Sam Charrington, or via

12
00:01:06,480 --> 00:01:11,400
a comment on the show notes page you can find at twomolai.com.

13
00:01:11,400 --> 00:01:12,400
Happy New Year.

14
00:01:12,400 --> 00:01:14,440
Let's get into the show.

15
00:01:14,440 --> 00:01:17,960
All right, everyone.

16
00:01:17,960 --> 00:01:23,480
Welcome to Twomolai's AI Rewind 2019, where I check in with friends of the show to hear

17
00:01:23,480 --> 00:01:31,920
from them on their favorite papers, perspective, thoughts, reflections on the year 2019 in their

18
00:01:31,920 --> 00:01:33,840
area of work and research.

19
00:01:33,840 --> 00:01:37,320
I've got the pleasure of speaking with Zach Lipton.

20
00:01:37,320 --> 00:01:41,080
Zach is a jointly appointed professor in a Tepper School of Business and the machine

21
00:01:41,080 --> 00:01:46,520
learning department at CMU, where he's also affiliated with the Heinz School of Public

22
00:01:46,520 --> 00:01:47,520
Policy.

23
00:01:47,520 --> 00:01:53,160
Zach was my guest in July, where we talked about fairwashing and the folly of ML solution

24
00:01:53,160 --> 00:01:54,160
mechanism.

25
00:01:54,160 --> 00:01:55,640
I encourage you to check out that show.

26
00:01:55,640 --> 00:02:00,000
If you're not already familiar with Zach in his background, Zach, welcome back to the

27
00:02:00,000 --> 00:02:01,000
Twomolai podcast.

28
00:02:01,000 --> 00:02:02,840
Thanks for having me, Sam.

29
00:02:02,840 --> 00:02:07,640
So I'm really looking forward to digging into some of the papers that you identified for

30
00:02:07,640 --> 00:02:09,840
us to walk through.

31
00:02:09,840 --> 00:02:17,080
But before we do that, I'd love to just start with your general sense of 2019 and what

32
00:02:17,080 --> 00:02:22,720
it meant for you or what you think it means for us kind of for machine learning in general.

33
00:02:22,720 --> 00:02:29,640
I think that we've been for the last few years kind of in this just mad dash, just sort

34
00:02:29,640 --> 00:02:36,200
of just catch up with what's changed following, I think 2010 to 2012, disruption in terms

35
00:02:36,200 --> 00:02:41,200
of the kind of like new set of tools that were made available by kind of line of successful

36
00:02:41,200 --> 00:02:44,160
works that got deep learning actually working.

37
00:02:44,160 --> 00:02:49,760
And so the metaphor I settled on at NURBS is it's sort of like the task of doing research

38
00:02:49,760 --> 00:02:55,160
which is kind of like wandering around in the dark like swinging around for like a pinata.

39
00:02:55,160 --> 00:03:01,200
And somebody smashed it open in 2012 and just suddenly became really easy to do research.

40
00:03:01,200 --> 00:03:04,640
Like it was really easy to collect candy because there was a bunch of it sitting on the

41
00:03:04,640 --> 00:03:05,640
ground.

42
00:03:05,640 --> 00:03:10,680
And I think people have just been trying to pick all the low hanging for 2013.

43
00:03:10,680 --> 00:03:15,920
You could get really smashing successful papers just by saying we played with a number

44
00:03:15,920 --> 00:03:21,400
of layers in the neural network and the results to be gotten by doing that were sufficiently

45
00:03:21,400 --> 00:03:24,920
compelling that you know it didn't matter how much sort of intellectual content there

46
00:03:24,920 --> 00:03:25,920
was.

47
00:03:25,920 --> 00:03:29,640
I think you know over the over the last few years it's gotten a little bit more refined

48
00:03:29,640 --> 00:03:33,320
and it's taking a bit more work to make an interesting paper.

49
00:03:33,320 --> 00:03:37,360
But I think right now more than any other point I think what you're starting to find is

50
00:03:37,360 --> 00:03:42,440
people just kind of coming up against the limitations of the general paradigm of like we collected

51
00:03:42,440 --> 00:03:46,000
the big data said you know we trained it on one partition we evaluated on the whole

52
00:03:46,000 --> 00:03:51,200
doubts that we kind of saw how we did and you know you can come up with a whole bunch

53
00:03:51,200 --> 00:03:57,840
of sophisticated tools to sort of either analyze that process or to try to sort of improve

54
00:03:57,840 --> 00:03:59,440
its efficiency a little bit.

55
00:03:59,440 --> 00:04:04,200
But there's a fundamental gap I think between sort of where people sort of got near dreams

56
00:04:04,200 --> 00:04:08,280
about what they thought was technology would could go and sort of like what we actually

57
00:04:08,280 --> 00:04:09,280
kind of do with it.

58
00:04:09,280 --> 00:04:14,000
I think what you're really seeing now in 2019 is people starting to think stock of the

59
00:04:14,000 --> 00:04:18,800
limitations of the current setup and the current paradigm and I think a lot of the creative

60
00:04:18,800 --> 00:04:22,960
people starting to think more seriously about going beyond just doing supervised learning

61
00:04:22,960 --> 00:04:27,640
and thinking really seriously whether it's about causal inference or robustness under

62
00:04:27,640 --> 00:04:32,360
domain adaptation or starting to think more seriously about the economic aspects of if

63
00:04:32,360 --> 00:04:37,120
you're deploying systems to make decisions then you're you're interacting in an environment

64
00:04:37,120 --> 00:04:41,320
with other agents who are also going to update their behavior and response to whatever

65
00:04:41,320 --> 00:04:46,760
it is that you changed and I think that this is kind of what's going on right now is

66
00:04:46,760 --> 00:04:51,840
that the sort of like rocket fuel from deep learning and starting to in my eyes I think

67
00:04:51,840 --> 00:04:57,280
it's starting to run out but I think the hope is that people are starting to get ambitious

68
00:04:57,280 --> 00:05:02,800
again and starting to get ambitious not just about scale but sort of about the kinds of

69
00:05:02,800 --> 00:05:04,520
problems that they pick on.

70
00:05:04,520 --> 00:05:13,040
Yeah, when I think back on this time of year a couple of years ago post-Nerrips a lot

71
00:05:13,040 --> 00:05:18,920
of people were talking about Ali Rahimi's kind of call for increased rigor in deep learning

72
00:05:18,920 --> 00:05:23,440
strikes me that that's kind of somewhat related to what you're describing and what we're

73
00:05:23,440 --> 00:05:28,880
seeing now but what I'm hearing you saying is less about kind of this shift to really

74
00:05:28,880 --> 00:05:35,320
digging into trying to find rigor but more looking elsewhere incorporating other ideas

75
00:05:35,320 --> 00:05:40,320
I'm hearing a ton about causality as well coming up in conversations do you know are those

76
00:05:40,320 --> 00:05:43,200
related kind of themes do you think?

77
00:05:43,200 --> 00:05:48,320
I think it's an important distinction to be made between doing science in a rigorous

78
00:05:48,320 --> 00:05:53,760
fashion and what are the set of scientific questions that you're working on and I think

79
00:05:53,760 --> 00:06:00,560
that this sort of big discussion that started after Ali's talk about sort of doing this

80
00:06:00,560 --> 00:06:08,560
work more carefully is I think very important and I've participated in that discussion

81
00:06:08,560 --> 00:06:14,920
a bit I think also like many people inspired by Ali's call to attention in 2017 but I

82
00:06:14,920 --> 00:06:19,760
think that's a sort of separate question from this other question about the set of problems

83
00:06:19,760 --> 00:06:24,800
that we start looking to like you could be very rigorous but still just focus entirely

84
00:06:24,800 --> 00:06:29,120
on this sort of trained test mode of machine learning and that's a little bit different

85
00:06:29,120 --> 00:06:35,160
from like you can become rigorous without leaving that world right you could just do just

86
00:06:35,160 --> 00:06:39,320
there's enough there's enough questions to probably still keep a large number of researchers

87
00:06:39,320 --> 00:06:45,920
working for a lifetime just about generalization of from one night samples to some some distribution

88
00:06:45,920 --> 00:06:50,400
but there's a different question about doing work that addresses qualitatively sort of

89
00:06:50,400 --> 00:06:55,440
broader sets of questions I think causality offers that right which is it's not just about

90
00:06:55,440 --> 00:07:00,800
doing work rigorously it's sort of an all-concentration which is what what are the categories

91
00:07:00,800 --> 00:07:06,400
of questions that you can answer and so you've identified some papers that were particularly

92
00:07:07,200 --> 00:07:13,920
meaningful for you in 2019 let's jump into let's jump into those that where would you like

93
00:07:13,920 --> 00:07:19,920
to start yeah so I try to include papers that or groups of papers that that sort of capture

94
00:07:20,480 --> 00:07:24,560
multiple aspects of this paper's papers sort of doing the old thing and interesting why

95
00:07:24,560 --> 00:07:31,680
paper is doing something new I guess to start off I thought two really interesting papers one

96
00:07:31,680 --> 00:07:35,920
of them was 2018 I guess but we could bundle it with a more recent one so don't hold that

97
00:07:35,920 --> 00:07:42,400
against me are these papers from Ben Wreck and Ludwig Schmidt and Becca Roelhoff's call do

98
00:07:42,400 --> 00:07:49,040
see far 10 classifiers generalized see far 10 and do image net classifier generalized to image net

99
00:07:49,520 --> 00:07:55,200
and in both cases these were basically like taking a hard look at this we we've had a whole

100
00:07:55,200 --> 00:08:00,480
community that part of how we put so many people to use is that we've really embraced this leaderboard

101
00:08:00,480 --> 00:08:05,280
in benchmark way of evaluating research or at least empirical research only been able to organize

102
00:08:05,280 --> 00:08:10,160
a large community around doing this work because we had these objective benchmarks and they're

103
00:08:10,160 --> 00:08:15,680
kind of asking this critical question of well after 10 million different researchers have trained

104
00:08:15,680 --> 00:08:21,120
their models on the see far turning set and evaluated on the the same exactly far holdouts that

105
00:08:21,120 --> 00:08:27,120
are we actually getting sort of like faithful sense of how these models generalize or

106
00:08:27,120 --> 00:08:33,280
or are we just overfitting that particular holdouts that basically we tapped out the the

107
00:08:33,280 --> 00:08:37,040
capacity of that holdouts that to tell us anything interesting about which bed models are better

108
00:08:37,040 --> 00:08:42,160
than what's other ones and so they did in both cases they undertook this really incredibly

109
00:08:42,160 --> 00:08:48,080
laborious effort to follow to the tee to the extent possible and you know in a good faith effort

110
00:08:48,080 --> 00:08:54,400
to create new holdouts sets for both both data sets and to basically say okay it's uh

111
00:08:54,400 --> 00:09:00,160
if instead of having 50,000 trained images and 10,000 holdout images if now suddenly in 2019

112
00:09:00,160 --> 00:09:05,600
an additional independent sample you know 10,000 holdout images fell out of the sky how well

113
00:09:05,600 --> 00:09:11,200
with all these models that have been basically all fit in the same kind of rat race of fitting

114
00:09:11,200 --> 00:09:15,680
it gets single as a how well would they generalize to this sort of new fresh data

115
00:09:15,680 --> 00:09:21,280
ostensibly from the same distribution and so a big part of the effort to went into these papers

116
00:09:21,280 --> 00:09:28,560
it sounds like was curating this new data set that to the best of their ability they demonstrated

117
00:09:28,560 --> 00:09:34,160
met you know felt were in line with the the distributions of the the popular data sets

118
00:09:34,160 --> 00:09:40,000
right so so in both cases there isn't actually an additional 10,000 seafarer images sitting

119
00:09:40,000 --> 00:09:44,880
around other aren't also an additional however many image net images sitting around so they

120
00:09:44,880 --> 00:09:50,080
actually had to go and sort of read the papers and say exactly what procedures that they followed

121
00:09:50,080 --> 00:09:55,680
to create this data set right so in the case of image net they first created some kind of ontology

122
00:09:55,680 --> 00:10:00,320
of of nouns based on the word net hierarchy and once they had these categories they used them

123
00:10:00,320 --> 00:10:05,520
to create Google image search these created set of candidate images which then were subjected

124
00:10:05,520 --> 00:10:10,320
to a crowdsourcing protocol that they used to basically you know to do something like identify

125
00:10:10,960 --> 00:10:15,440
several thousand images that were likely to be Persian cats but then you needed to show them

126
00:10:15,440 --> 00:10:20,880
to a crowd worker and say is this a Persian cat and then like this was their way that we're

127
00:10:20,880 --> 00:10:25,040
able to create the data sets so they tried to go through and this included you know creating

128
00:10:25,040 --> 00:10:32,080
sets of candidate images going through the crowdsourcing pipelines it was really you know interesting

129
00:10:32,080 --> 00:10:37,440
work I think sort of on a sort of qualitative sense it they're sort of taking a fresh look at

130
00:10:37,440 --> 00:10:43,440
this data set construction process as they're going through trying to follow in the footsteps of

131
00:10:43,440 --> 00:10:48,880
the original data set creators and they come up with a bunch of interesting observations like what

132
00:10:48,880 --> 00:10:53,920
precisely is you know you get the weird questions that you wouldn't think would be so

133
00:10:53,920 --> 00:10:58,960
vexing like what really is a basketball you know where you'll find some of these images that they

134
00:10:59,600 --> 00:11:04,400
annotators disagree on it's because maybe it's not a real basketball maybe it's a toy basketball

135
00:11:04,400 --> 00:11:10,560
like is it maybe it's a picture of basketball what precisely is going on there and and how

136
00:11:10,560 --> 00:11:16,960
off the thing could be categorized but then the sort of key result right is that besides this

137
00:11:16,960 --> 00:11:21,200
sort of really interesting aside about how the data set were collected some of what they believe

138
00:11:21,200 --> 00:11:25,840
were in the paper is in the number in some really nice talk that the others have given a long

139
00:11:25,840 --> 00:11:30,000
away an event right given I just talked about this work at the Simon's Institute last summer

140
00:11:30,560 --> 00:11:35,520
is that then what they do is they evaluate basically our current leaderboard right to take all

141
00:11:35,520 --> 00:11:40,160
these great models that are sort of the state of the art classifiers for these different data sets

142
00:11:40,160 --> 00:11:45,760
that you know have some kind of ranking among them and they evaluate them all on the new the

143
00:11:45,760 --> 00:11:51,280
new C bar test that right on the new image net test that the funny thing about it is that basically

144
00:11:51,280 --> 00:11:56,800
the model is all performed so here's here's like the like don't get too excited or too angry or

145
00:11:56,800 --> 00:12:03,920
whatever the first step is that the model is all performed of course right and because of that

146
00:12:03,920 --> 00:12:08,320
there's some you know this is sort of like usual crowd on Twitter the guy could be critical on

147
00:12:08,320 --> 00:12:15,920
Twitter but like like like the evidence say you know what it says and not you know the usual like

148
00:12:15,920 --> 00:12:19,760
whether it was like see like I told you deep learning doesn't work and sort of interpreted it

149
00:12:19,760 --> 00:12:24,400
that way like this is like a damning result about deep learning but actually this was a really

150
00:12:24,400 --> 00:12:30,080
positive result and that's because the other side of the result was that it's not yes all the models

151
00:12:30,080 --> 00:12:37,360
did worse but they were still arranged sort of in the same order so basically like the best model

152
00:12:37,360 --> 00:12:43,200
was a certain amount better than you know the fifth best model and that was also true on the new

153
00:12:43,200 --> 00:12:48,160
data right yeah so what it sort of showed was that like if you think of the leaderboard as being

154
00:12:48,160 --> 00:12:54,240
that's like step of like incremental progress towards like ever better models that the models that

155
00:12:54,240 --> 00:12:58,960
we thought were better because they did better on that C bar like the actual C bar holdout set

156
00:12:59,600 --> 00:13:06,480
actually were better on the new C bar holdout on the fresh C bar data or on the fresh

157
00:13:06,480 --> 00:13:12,400
image net data it's just that they were the entire benchmark like all the models in the benchmark

158
00:13:12,400 --> 00:13:17,040
sort of across the board were a bit worse but in the same order so the better models really were

159
00:13:17,040 --> 00:13:24,320
better and we kind of talk about this informally as the industry overfitting on image net the

160
00:13:24,320 --> 00:13:31,280
industry overfitting on these kind of standardized data sets and that this paper is is demonstrating

161
00:13:31,280 --> 00:13:37,200
is that perhaps that's true but at least there's still some order here right this stuff is really

162
00:13:37,200 --> 00:13:42,720
complicated and I think the general problem is that basically we have one way of talking about

163
00:13:42,720 --> 00:13:49,200
generalization normally and supervising which is from a finite sample to the the underlying

164
00:13:49,200 --> 00:13:53,760
distribution and then when people started saying like well what about on some different distribution

165
00:13:53,760 --> 00:13:58,000
does the generalization we start overloading the word generalize in different ways yeah and so

166
00:13:58,000 --> 00:14:01,920
that actually that part of the story also comes in here which is why it's interesting the question

167
00:14:01,920 --> 00:14:07,280
is then okay all the models are appearing the same order which means basically if nothing else

168
00:14:07,280 --> 00:14:12,880
it means we didn't scorch the test set to the point that we got no value out of continuing to

169
00:14:12,880 --> 00:14:18,320
do this leaderboard chasing right right but then the question is why did we do worse on the fresh

170
00:14:18,320 --> 00:14:24,000
data and there's two possible explanations right one possible explanation is that well we sort of

171
00:14:24,000 --> 00:14:31,200
are overfitting the image net cold outside but this is having sort of it somehow like an equal

172
00:14:31,200 --> 00:14:35,280
effect on all the models that they all deteriorate by the same amount or something like that

173
00:14:36,160 --> 00:14:42,080
but the other one which I believe the authors seem to believe is the most likely explanation

174
00:14:42,080 --> 00:14:46,400
and we've done some experiments it suggests that it's plausible the other explanation is that

175
00:14:46,400 --> 00:14:50,960
the reason why they do worse is because they're not actually see far data and they're not actually

176
00:14:50,960 --> 00:14:57,440
image net data so basically like despite the authors very best effort and very best like

177
00:14:57,440 --> 00:15:03,920
intention to in every way like you couldn't expect you couldn't hope for a better better behave

178
00:15:03,920 --> 00:15:09,520
deployment scenario right like imagine that you trained on some data and then you go like this

179
00:15:09,520 --> 00:15:14,720
is from like whatever your business is and you go to deployment time and basically at deployment time

180
00:15:14,720 --> 00:15:20,000
what you do is you face a team of academic researchers an absolute best faith effort to in every

181
00:15:20,000 --> 00:15:25,040
single way create data that statistically is identical to the data you saw during training

182
00:15:25,040 --> 00:15:29,520
like you can never hope for this in a real world scenario right exactly of all the ways you can

183
00:15:29,520 --> 00:15:35,200
go out of domain this is the most friendly right someone really trying hard and extremely

184
00:15:35,200 --> 00:15:40,240
confident president even PhDs who want nothing more than to like make this a valid scientific

185
00:15:40,240 --> 00:15:46,320
experiment do everything possible to make it so that this is i.e. like true holdout data and still

186
00:15:46,320 --> 00:15:50,800
despite their best efforts the distribution of data is slightly different because it's

187
00:15:50,800 --> 00:15:56,080
it's a few years later so maybe photographs look slightly different maybe compression algorithms

188
00:15:56,080 --> 00:16:00,400
using cameras that then get logged in image search or whatever is slightly different somehow

189
00:16:00,400 --> 00:16:05,120
you're getting you're getting new images in that that just are slightly different in distribution

190
00:16:05,120 --> 00:16:09,520
and that's enough to make your classifiers say 7% worse 8% worse something like that if we get

191
00:16:09,520 --> 00:16:13,280
the exact number yeah so that's having the interesting thing is that like there's all these

192
00:16:13,280 --> 00:16:18,560
different notions of generalization here one is that it seems that they actually like the benchmarks

193
00:16:18,560 --> 00:16:22,000
for whatever reason like this process with the only way that we're squeezing juice out of the

194
00:16:22,000 --> 00:16:26,400
holding set is changing our neural network architectures and hyper parameters doesn't leak as

195
00:16:26,400 --> 00:16:32,880
much information as in my fear the models do generalize as well the benchmark like leaderboard

196
00:16:32,880 --> 00:16:39,200
chasing kind of works better than we have any right to hope it would and at the same time also we

197
00:16:39,200 --> 00:16:44,160
sort of see how in the context of the experiments how brittle our classifiers are through distribution

198
00:16:44,160 --> 00:16:49,680
jet so I think the fact that all this is going on in these papers and and I have a fondness for

199
00:16:49,680 --> 00:16:54,080
this kind of paper that they're not introducing a new model they're not claiming a new leaderboard

200
00:16:54,080 --> 00:16:59,520
result but they're asking is a scientific question and I think doing that precise kind of hard work

201
00:16:59,520 --> 00:17:05,200
experiment to kind of produce that knowledge and that whole story kind of comes out of here

202
00:17:05,200 --> 00:17:09,680
are there other papers that come to mind that took similar approaches this year there were

203
00:17:09,680 --> 00:17:14,640
feet of different papers that have various different aspects I think sometimes this this is unique

204
00:17:14,640 --> 00:17:19,200
and then it's sort of saying do these models generalize do the very same distributions that they

205
00:17:19,200 --> 00:17:24,640
were trained on there are also a lot of papers that kind of look at various ways of saying basically

206
00:17:24,640 --> 00:17:30,240
these models were trained on some data set that was meant to sort of capture some kind of real world

207
00:17:30,240 --> 00:17:34,640
task you know like you had some kind of idea of the confidence we thought you were evaluating and

208
00:17:34,640 --> 00:17:39,600
a lot of papers are more like these performance on this data set really indicating that confidence

209
00:17:39,600 --> 00:17:44,720
like as as you might hope it would and there were a number of these over the last year and a half

210
00:17:44,720 --> 00:17:51,120
or so so I had a student did young's project we had a paper at EMNLP last year that basically was

211
00:17:51,760 --> 00:17:56,640
said something like how much reading does reading comprehension require and so we looked at these

212
00:17:56,640 --> 00:18:01,120
reading comprehension data sets and we found that essentially in reading comprehension you're given

213
00:18:01,120 --> 00:18:06,720
a question and a passage you have to produce an answer and so presumably producing the answer

214
00:18:06,720 --> 00:18:11,520
correctly if it's really testing reading comprehension it should require that you've actually read

215
00:18:11,520 --> 00:18:15,520
the question and should require that you've actually read the passage otherwise it's hard to

216
00:18:15,520 --> 00:18:20,240
claim that the what the model is doing is question answer this passive based question answer

217
00:18:20,240 --> 00:18:23,840
and right if it doesn't actually look at the passage or if you can't be sure that it actually

218
00:18:23,840 --> 00:18:27,760
looked at the passage or if you can't actually be sure that it read the question we had a paper

219
00:18:27,760 --> 00:18:32,640
basically just said hey how come nobody's run this baseline on all these data sets just training

220
00:18:32,640 --> 00:18:36,480
the exact same models but looking only at the passage and not the question just get it based

221
00:18:36,480 --> 00:18:41,200
on performance or just looking only at the question and looking at a randomized passage and so

222
00:18:41,200 --> 00:18:46,240
when we did that it turned out that you can match a lot of the best results reported in the literature

223
00:18:46,240 --> 00:18:50,080
either not looking at the question or not looking at the passage so it says something about

224
00:18:50,080 --> 00:18:54,320
does that exactly the same but it's a similar spirit and it's sort of saying what you know asking

225
00:18:54,320 --> 00:18:59,840
out a question about the trying to ask the sort of fundamental question about this data or another

226
00:19:00,640 --> 00:19:06,400
example was there was a English polia at a paper that did a similar thing with natural language

227
00:19:06,400 --> 00:19:11,840
inference this is a task where basically you have two sentences the similar kind of said you have

228
00:19:11,840 --> 00:19:17,040
two sentences one is called a trimis and the other is called a hypothesis and these could be in

229
00:19:17,040 --> 00:19:23,040
a relationship with each other which is either entailment contradiction or like a neutral posture

230
00:19:23,040 --> 00:19:27,920
so it's a three-way classification problem and so the trick is to sort of read the two sentences

231
00:19:27,920 --> 00:19:33,120
and to deduce like which one of the three best describes the relationship of these sentences

232
00:19:33,120 --> 00:19:37,840
to each other and they basically found that if you just the way these data sets had been created

233
00:19:37,840 --> 00:19:43,520
for that task you could often get the same performance as safety art models just by only looking

234
00:19:43,520 --> 00:19:47,440
at the hypothesis and ignoring the premise so you know someone might have thought that what they

235
00:19:47,440 --> 00:19:51,920
saw was this task of entailment but what they really did was made a sentence classifier

236
00:19:51,920 --> 00:19:54,960
yeah they made a sentence classifier and it turned out there were some clues like

237
00:19:54,960 --> 00:19:59,120
the hypothesis tended to have like negation words and it when it was a contradiction or something

238
00:19:59,120 --> 00:20:03,120
like that something so this kind of gets that high-level thing that we're talking about of like

239
00:20:03,120 --> 00:20:08,160
beyond supervised learning of hey from from a pure supervised learning standpoint what's wrong

240
00:20:08,160 --> 00:20:12,560
with what the classifier is doing there's nothing wrong with it right it's getting good predictive

241
00:20:12,560 --> 00:20:17,200
performance the problem is that what we want is something a little bit more than that we want

242
00:20:17,200 --> 00:20:21,200
something that's going to perform well in other environments but we know we don't know how to

243
00:20:21,200 --> 00:20:27,040
you know we're still very immature about how to incorporate that into our kind of learning setup

244
00:20:27,040 --> 00:20:32,160
right so most of what people know how to do is to say here's represented a data fit a model and

245
00:20:32,160 --> 00:20:35,760
and everything you know everything that we have a right to expect about the model is that it'll do

246
00:20:35,760 --> 00:20:40,960
well on new data from the same distribution and now we reveal that actually what we really wanted

247
00:20:40,960 --> 00:20:45,680
or what it really takes to do interesting things in the real world is is something that that other

248
00:20:45,680 --> 00:20:51,440
notion of generalization to different data sets to different environments yeah speaking of

249
00:20:52,080 --> 00:20:59,040
beyond supervised learning the next paper that you identified is the the birth paper technically

250
00:20:59,040 --> 00:21:06,720
I think we first saw that one late 2018 but we certainly came to understand it a lot more in 2019

251
00:21:06,720 --> 00:21:14,320
right so I think you know if we have our different pools of things going on people trying to get

252
00:21:14,320 --> 00:21:19,120
beyond the current paradigm and sort of counterargument that there's a lot of juice yet to be squeezed

253
00:21:19,120 --> 00:21:26,720
maybe births actually that latter category but it is basically the idea more you know the

254
00:21:26,720 --> 00:21:33,600
the highest level idea is just basically semi-supervised learning right and and I want to semi-supervised

255
00:21:33,600 --> 00:21:37,520
learning and machine learning right is the approach we say I have lots of unlabeled data a small

256
00:21:37,520 --> 00:21:42,240
amount of labeled data how can I make magic out of that right how can I how can I basically the

257
00:21:42,240 --> 00:21:46,800
the baseline would be I only have labeled data like ignore the unlabeled data it's just

258
00:21:46,800 --> 00:21:52,000
trying to classify or using the labeled data right or the unlabeled data so what can I do with

259
00:21:52,000 --> 00:21:56,880
that unlabeled data and deep learning gives you a nice kind of answer to it like with a lot of

260
00:21:56,880 --> 00:22:01,920
things which is well use the unlabeled data to learn the representation the earliest forms of this

261
00:22:01,920 --> 00:22:09,280
that I saw where I think around 2015 or so I think quickly in Andrew die had a paper that was doing

262
00:22:09,280 --> 00:22:14,240
as I'm sure I bet your papers that predated that's one that I remember were the they were doing

263
00:22:14,240 --> 00:22:19,440
stuff like just train a language model on a bunch data and then fine tune the language model to

264
00:22:20,240 --> 00:22:26,080
make predictions on your downstream classification task now the models they were using weren't that big

265
00:22:26,080 --> 00:22:31,280
they weren't using tens of TPUs they were using you know probably they were still probably I don't

266
00:22:31,280 --> 00:22:34,640
even know if tenser flows out they might have written their code into the auto like I was at time

267
00:22:34,640 --> 00:22:39,120
and and I don't think they were using a specially enormous data set or whatever but like you know

268
00:22:39,120 --> 00:22:43,040
that key idea has been there for a while we've talked about you know training auto encoder is

269
00:22:43,040 --> 00:22:49,200
fine tuning to you know supervised task here that the idea basically put if the next word

270
00:22:49,200 --> 00:22:55,040
and then Elmo came out basically 2018 it's basically the same exact idea that there's a

271
00:22:55,040 --> 00:22:59,040
slight wrinkles the wrinkles are on the details right like they say we train a forward

272
00:22:59,040 --> 00:23:04,400
language model from left to right backwards language models from right to left you can catenate them

273
00:23:04,400 --> 00:23:08,160
and then you take some mixture of their representations but the qualitative idea is

274
00:23:08,160 --> 00:23:14,000
train a giant language model on more data play around with some some variants but that are really

275
00:23:14,000 --> 00:23:18,880
not conceptually different they're just kind of you know different levers different offsiturn

276
00:23:19,440 --> 00:23:23,920
and basically see what could do the best performance on the number of downstream tasks and

277
00:23:23,920 --> 00:23:28,160
Elmo was this break your moment in that whether or not you found a conceptually interesting

278
00:23:28,160 --> 00:23:34,640
every single like virtually every single NLB task experience some significant bump in accuracy

279
00:23:34,640 --> 00:23:38,640
in those moments are not so common right where you just say oh there's a new state of the art

280
00:23:38,640 --> 00:23:44,560
for every single task effective tomorrow and so Bert it was Elmo didn't have that much time

281
00:23:44,560 --> 00:23:52,240
in the sun before the the mubbid meme took off and Bert which I think has sort of stayed now it's

282
00:23:52,240 --> 00:23:57,440
been there've been enough variations on Bert but not enough that anybody's willing to you know

283
00:23:57,440 --> 00:24:01,520
this is the same thing happens image of rights like there's been slight changes on resonance

284
00:24:01,520 --> 00:24:06,560
or whatever but for the most part everyone still uses resonance now four years later yeah Bert I

285
00:24:06,560 --> 00:24:10,960
think was this moment where you know they made a few more modifications one key modification was

286
00:24:10,960 --> 00:24:17,360
that they use transformers instead of LSDMs like in Elmo another big change is that for their

287
00:24:17,360 --> 00:24:23,520
modeling objective instead of saying that what they're going to do is like auto regress from left

288
00:24:23,520 --> 00:24:27,440
to right and just try to predict the next word given all the previous ones they do that sort of fill

289
00:24:27,440 --> 00:24:32,160
in the blank type of objective where they mask out certain words and then try to predict which words

290
00:24:32,160 --> 00:24:39,040
were masked out but Bert gave us absolutely massive boots to sort of after after that had already

291
00:24:39,040 --> 00:24:44,400
happened shortly before sort of did it again across the board and at this point you basically cannot

292
00:24:44,400 --> 00:24:48,560
publish a paper in natural language processing without building on top of it you know the question

293
00:24:48,560 --> 00:24:52,960
that then arises is sort of what is the sort of two perspectives one is to say if you want to do

294
00:24:52,960 --> 00:24:58,720
interesting work in this field you have to go to Google and get a TPU farm and this is what you

295
00:24:58,720 --> 00:25:03,600
have to do to to move the state of the art the other way to think about it is to say that that's no

296
00:25:03,600 --> 00:25:09,600
longer the interesting part that the architecture is sort of something that someone will come out with

297
00:25:10,160 --> 00:25:14,320
there's Bert on those Roberta someone will come out with yeah I think they've already come out with

298
00:25:14,320 --> 00:25:20,240
every other method but you know right so someone will go out with something did you create a research

299
00:25:20,240 --> 00:25:24,640
do you say that that is the creative research do you say hey that is the tool upon which anything

300
00:25:24,640 --> 00:25:30,400
that I want to do that's just one part of what I do is this function setting and if I'm doing that

301
00:25:30,400 --> 00:25:34,000
for natural language that this is the base model that I use and someone will come up with another

302
00:25:34,000 --> 00:25:38,240
base model but that you know on one hand it takes a massive amount of resources to train Bert on

303
00:25:38,240 --> 00:25:42,880
the other hand it takes very few resources comparatively to you to fine tune Bert to sundown

304
00:25:42,880 --> 00:25:47,920
stream pass it's kind of opening this interesting wedge of life we're sort of now in this position

305
00:25:47,920 --> 00:25:53,600
where sort of the only way you can you can do at least like leaderboard competitive and alp

306
00:25:53,600 --> 00:25:58,320
work is the build on top of one of these models what's next up in your list of tapers

307
00:25:59,040 --> 00:26:05,520
yeah so kind of continuing this theme of going beyond the standard just like I have some offline

308
00:26:05,520 --> 00:26:11,840
data I say the model I evaluate how predictive it is on some holdout that right there's sort of

309
00:26:11,840 --> 00:26:16,000
a number of things that we're interested in going beyond there one is under what settings can you

310
00:26:16,000 --> 00:26:22,560
detect or adapt to distribution shift the other side is I think the social component which is

311
00:26:23,200 --> 00:26:26,560
you know we're always talking about using machine learning and it came up when we were done

312
00:26:26,560 --> 00:26:30,320
of fairness before right we're always when we talk about machine learning technically we just

313
00:26:30,320 --> 00:26:34,480
use the language of prediction I have some data and make a prediction how accurate it isn't like

314
00:26:34,480 --> 00:26:39,440
the notion of accuracy assumes like a fixed reference distribution right accuracy is a self is

315
00:26:40,240 --> 00:26:45,600
is a is a probable statement what fraction of the times are we right assuming that there is some

316
00:26:45,600 --> 00:26:50,400
objective background distribution that's generating our data but the reality is that you've

317
00:26:50,400 --> 00:26:53,520
been look at what people say they're doing with machine learning like we want to build self-driving

318
00:26:53,520 --> 00:26:59,280
cars we want to make your doctor in AI we want to do whatever all of these tasks involve not just

319
00:26:59,280 --> 00:27:04,800
making predictions but actually driving some kind of decisions in some real world process in a lot

320
00:27:04,800 --> 00:27:09,280
of those cases you know you can think of it like like Google search right what happens the moment you

321
00:27:09,280 --> 00:27:13,600
change to Google search algorithm right the very instant you change to Google search algorithm like

322
00:27:13,600 --> 00:27:18,000
the reddit message boards light up and people are going to start adapting they're going to start

323
00:27:18,000 --> 00:27:22,320
chatting about strategies to take advantage of the new algorithm in some way or another right

324
00:27:22,320 --> 00:27:27,520
and so basically you're making decisions and so our predictions are informing decisions we usually

325
00:27:27,520 --> 00:27:32,960
kind of alive that step but once you turn a prediction into a policy and you deploy it and like a

326
00:27:32,960 --> 00:27:37,840
real social setting where you're not the only agent the next thing that happens is everybody else

327
00:27:37,840 --> 00:27:42,400
is going to start updating their behavior which fundamentally shifts the distribution

328
00:27:42,400 --> 00:27:46,800
against what you're trying to make predictions right exactly like the same individual

329
00:27:46,800 --> 00:27:50,880
who may or may not have the same but it's going to start changing their behavior right people

330
00:27:50,880 --> 00:27:54,480
in aggregate are going to change their behavior which is this is going to somehow manifest in the

331
00:27:54,480 --> 00:27:59,120
data that you see in the next round right I don't think there are necessarily cleanly separated

332
00:27:59,120 --> 00:28:04,400
rounds but we could idealize it that way for you know like analytic purposes so there are

333
00:28:04,400 --> 00:28:07,600
couple papers I think I think I think sort of two groups of people have been thinking really

334
00:28:07,600 --> 00:28:12,560
seriously about these kinds of problems I just wanted to highlight their work is one is Lily

335
00:28:12,560 --> 00:28:18,560
who who's a PhD student at Harvard I believe in like something crazy like I think she's like

336
00:28:18,560 --> 00:28:24,240
joined philosophy and applied math but works with computer scientists something like this but anyway

337
00:28:24,240 --> 00:28:28,880
I think she's done really fantastic work for a long time but specifically I've been looking

338
00:28:28,880 --> 00:28:32,640
at problems with this flavor which is sort of like oh especially in a context of fairness which is

339
00:28:32,640 --> 00:28:36,480
okay you're proposing that we make predictions in this particular way or the design classification

340
00:28:36,480 --> 00:28:41,200
this way but then what happens in the next step right well we're always like missing the next part

341
00:28:41,200 --> 00:28:45,920
of the picture is like okay so then then we make those decisions then how do people think their

342
00:28:45,920 --> 00:28:50,800
behavior or how does this influence censorship or like the data that we see in the next round and

343
00:28:50,800 --> 00:28:55,200
then you know basically what are we going to see next and another group of people that have worked

344
00:28:55,200 --> 00:28:59,840
on this is more it's hard with and he's working on it for a long time I think even before he

345
00:28:59,840 --> 00:29:05,520
joined faculty and now is a faculty member with his students Smith and Millie among others

346
00:29:05,520 --> 00:29:10,560
and so they work you know with the student Lydia Lo that Lydia Leo they won the best paper

347
00:29:10,560 --> 00:29:14,960
at ICML a year ago and this was about the delayed impacts of fair machine learning

348
00:29:14,960 --> 00:29:18,640
and so the two papers that I thought you're just seeing which were both from this year at the

349
00:29:18,640 --> 00:29:24,000
FastStar conference so this is the fairness accountability and transparency conference these

350
00:29:24,000 --> 00:29:28,800
were sort of two contemporary works both called one is called by Lily who called the disparate

351
00:29:28,800 --> 00:29:34,240
effects of strategic manipulation one by Smitha called Smith Millie called the social cost of

352
00:29:34,240 --> 00:29:38,720
strategic classification but they're both addressing the setting more basically someone all the

353
00:29:38,720 --> 00:29:44,320
individuals are characterized by some covariates then there's a firm that drives some kind of decisions

354
00:29:44,320 --> 00:29:49,520
by fitting some kind of classifier which assigns people to predictions they've positive or negative

355
00:29:49,520 --> 00:29:56,640
based on their covariates but then in the very next round basically individuals are going to they

356
00:29:56,640 --> 00:30:01,360
have some power to sort of manipulate the value of their covariates so I can motivate an example

357
00:30:01,360 --> 00:30:06,720
in Lily's paper that I think is a very good one is think about like school admissions

358
00:30:06,720 --> 00:30:13,280
and people notice that maybe people who get better SAT scores tend to do you know in somehow

359
00:30:13,280 --> 00:30:18,560
this tends to be predictive of success in college or whatever but if you then lean more heavily

360
00:30:18,560 --> 00:30:22,720
on the SAT scores or you incorporate this as a more prominent category and how you make your decisions

361
00:30:22,720 --> 00:30:27,600
if you do admit the very next thing that happens is that people who have some resources like

362
00:30:27,600 --> 00:30:31,360
as much as the college board wants you to think that these things are not manipulable they are

363
00:30:31,360 --> 00:30:37,040
and what people are going to do is go out and find a way to spend money to prepare for the test

364
00:30:37,040 --> 00:30:41,840
and when they start preparing for the test the same individuals who you know

365
00:30:41,840 --> 00:30:45,360
they give an individual by preparing really hard for the SATs doesn't necessarily become

366
00:30:45,360 --> 00:30:49,600
fundamentally a better student but they do increase their test score right and so this is like the

367
00:30:49,600 --> 00:30:54,800
the key idea is that you have someone making decisions but you have other people who in response

368
00:30:54,800 --> 00:30:59,840
are able to sort of invest in manipulating their their features or their covariance to influence

369
00:30:59,840 --> 00:31:05,440
their prediction so someone right the decision maker has to publish their model then other people

370
00:31:05,440 --> 00:31:10,320
get to sort of respond to it strategically and then both of these papers they they look both

371
00:31:10,320 --> 00:31:15,520
as dynamic but also in the context of fairness so so one way that they look at things is this

372
00:31:15,520 --> 00:31:20,240
context of well what if not everyone has access to the same resources to manipulate their features

373
00:31:20,240 --> 00:31:25,440
and I think that the SAT example is a really good example of that right where you have people

374
00:31:25,440 --> 00:31:29,840
have spent really exorbitant sums on test prep I remember I had friends when I was in

375
00:31:29,840 --> 00:31:33,760
ecolumbia there was some service I don't want to say what they are maybe they'll help my friend

376
00:31:33,760 --> 00:31:37,680
who works with them you know there was some service they go to his fault so I'm like kind of like

377
00:31:37,680 --> 00:31:44,000
posh like Manhattan test prep service the chart I think the tutors made I'm sure the tutors were

378
00:31:44,000 --> 00:31:48,480
getting a small slice of the cut and they were getting like 100 200 bucks an hour something

379
00:31:48,480 --> 00:31:52,480
absolutely ridiculous I didn't think the requirement was you had to have a perfect SAT score

380
00:31:52,480 --> 00:31:56,400
like the tutors so that was like part of their limit but you know you have these these

381
00:31:56,400 --> 00:32:00,880
services and people will really spend enormous amounts of money and then you rate you create

382
00:32:00,880 --> 00:32:05,280
this dynamic database but people who have lots of resources to influence their decisions will do

383
00:32:05,280 --> 00:32:11,520
so people who have less will you know can't really compete and kind of analyzing this dynamic of

384
00:32:11,520 --> 00:32:16,560
sort of what happens depending on how you make your predictions and how people adjust strategically

385
00:32:16,560 --> 00:32:22,880
and they also both came up with interesting scenarios where sort of often everyone could be made

386
00:32:22,880 --> 00:32:27,920
worse off and like the institution making decisions becomes better off like in the setting of

387
00:32:27,920 --> 00:32:32,560
strategic manipulation like a somehow like works against the this might have something to do

388
00:32:32,560 --> 00:32:36,480
the fact that like you're sort of assuming like a monopolistic party on one end and like competitive

389
00:32:36,480 --> 00:32:41,040
parties on the other but you can have these settings where basically lots of people are competing

390
00:32:41,040 --> 00:32:45,440
with each other to manipulate their predictions and net overall like they're incurring some cost

391
00:32:45,440 --> 00:32:50,320
for doing so and are made worse off and the institution on the other side sort of

392
00:32:50,320 --> 00:32:54,720
is made strictly better off. I don't know that either and I think both papers are addressing a

393
00:32:54,720 --> 00:33:00,000
fairly idealized setting and it's not clear that they're giving you a a tool in the algorithmic

394
00:33:00,000 --> 00:33:05,280
sense that you could go out and sort of analyze it and you know directly say a lending decision

395
00:33:05,280 --> 00:33:09,440
or school admissions or not that I would endorse you know making those decisions based on

396
00:33:09,440 --> 00:33:14,240
machine learning classifier in the first place but I think they are giving and I think this is

397
00:33:14,240 --> 00:33:19,920
true of a lot of this stuff as we start going sort of towards causality and for thinking about

398
00:33:19,920 --> 00:33:25,200
economic mechanisms is there at least giving us I think a framework for thinking coherently

399
00:33:25,200 --> 00:33:29,840
about the problem and I think the problem is you know when we don't do that work you wind up with

400
00:33:29,840 --> 00:33:34,480
people pretending that these are just that the more or less we can just continue to think of these

401
00:33:34,480 --> 00:33:41,200
things as prediction and that you know just like there's some kind of really simple trivial fix

402
00:33:41,200 --> 00:33:46,240
that I think it's around us and I think these are you know nice steps towards thinking coherently

403
00:33:46,240 --> 00:33:51,760
about these problems. Yeah it strikes me that part of the issue in the examples you described

404
00:33:51,760 --> 00:33:59,280
is kind of the gap between the real world thing that you're trying to optimize and how you formulate

405
00:33:59,280 --> 00:34:06,720
that as a machine learning problem does the the framework that they provide in their discussion

406
00:34:06,720 --> 00:34:12,880
help provide additional tools or ways to think about that core problem formulation question.

407
00:34:12,880 --> 00:34:20,400
I think what it gives us at least is a way of stepping back and appreciating at least one component

408
00:34:20,400 --> 00:34:24,560
of yeah there's this thing that is always omitted which is like what is our data the what is the

409
00:34:24,560 --> 00:34:29,680
data where does it come from and to some extent you know when the data represents individuals the

410
00:34:29,680 --> 00:34:34,720
data is the data is subject to choices that they make and some of those choices are directed

411
00:34:34,720 --> 00:34:40,480
specifically to to have them classified and treated it in a certain way yeah and I think just like

412
00:34:40,480 --> 00:34:46,560
that very sort of realization starting to model it into starting to create sort of mathematical models

413
00:34:46,560 --> 00:34:51,360
of this interaction as itself you know that the it's a conceptual tool you know again I don't

414
00:34:51,360 --> 00:34:54,480
think it's a practical tool like I don't need to give you an algorithm is it oh just run this

415
00:34:54,480 --> 00:34:59,440
on the bank and then it'll it'll tell you what is you know and in fact you need to know things

416
00:34:59,440 --> 00:35:02,880
going into it right like you need to know what are the cost of manipulating different features

417
00:35:02,880 --> 00:35:08,000
some things that might not be specified in the data but it's at least sort of starting to cast

418
00:35:08,000 --> 00:35:14,160
these problems in a way that is sort of richer in captors these things that you know we're sort of

419
00:35:14,160 --> 00:35:18,400
nave like ignoring like if nothing else is about like what is this conceptual tool to do we see

420
00:35:18,400 --> 00:35:22,560
all these people talking about like just using big data to do all kinds of whatever I have access

421
00:35:22,560 --> 00:35:28,400
to someone's Facebook likes and this and that and I can say whatever I can make whatever kinds

422
00:35:28,400 --> 00:35:32,480
of decisions based on that data this is giving you a reason not to right you know if you start

423
00:35:32,480 --> 00:35:37,680
thinking it if only in just like the very consideration of these interaction dynamics because

424
00:35:37,680 --> 00:35:42,960
well why should you use Facebook likes well because if somebody knows that their career and all

425
00:35:42,960 --> 00:35:48,320
these other sort of choices that are really consequential to them are being driven based on

426
00:35:48,320 --> 00:35:53,600
these like really easily manipulable features they're going to start changing that behavior right

427
00:35:53,600 --> 00:35:59,600
and so there's multiple sort of aspects of this right one is just the very consideration of strategic

428
00:35:59,600 --> 00:36:05,120
behavior and then the other side is a sort of fairness implications when you're making decisions

429
00:36:05,120 --> 00:36:09,680
about people and then you'll have different groups of people with different abilities sort of

430
00:36:09,680 --> 00:36:14,480
disparate ability can manipulate yeah right you know it's like a new craze of joy somewhere you

431
00:36:14,480 --> 00:36:18,480
know maybe just rich people get whatever classification they want or something like that

432
00:36:19,360 --> 00:36:26,080
okay cool the next paper on your list kind of returns to this theme of generalization

433
00:36:26,080 --> 00:36:32,080
and machine learning it's the invariant risk minimization paper tell us why that paper made your

434
00:36:32,080 --> 00:36:37,920
list yeah so this is a paper that I don't know if they've hit the exact solution to these kind

435
00:36:37,920 --> 00:36:43,280
of family problems but I think was a really solid and interesting attempt that I think got a lot

436
00:36:43,280 --> 00:36:48,320
of people thinking and basically you know they're they're just sort of a connection between prediction

437
00:36:48,320 --> 00:36:56,400
out of domain between causality and some of this is sort of well known but one aspect that sort of

438
00:36:56,400 --> 00:37:02,000
hasn't been folded into it is is representation learning so so the way that causality is like

439
00:37:02,000 --> 00:37:06,480
the prediction out of domain is that you say well my source data came from some distribution my

440
00:37:06,480 --> 00:37:11,840
target data came from a different distribution what changed presumably in some ways that these two

441
00:37:11,840 --> 00:37:16,000
different distributions are related to each other right otherwise like why should I expect that I

442
00:37:16,000 --> 00:37:20,320
could apply a class by returning on one to the other causality gives you one way of thinking about it

443
00:37:20,320 --> 00:37:26,000
like there's you know there was an intervention of some sort something you know one one variable

444
00:37:26,000 --> 00:37:30,160
that used to take its data in some organic way has now been intervened upon and so but otherwise

445
00:37:30,160 --> 00:37:36,000
the process is the same but this paper starts getting at is this idea of well thinking about

446
00:37:36,000 --> 00:37:40,720
predicting well on a range of environments in terms of the representation that you learn which is

447
00:37:40,720 --> 00:37:46,320
the idea that came up in some earlier more a classical domain adaptation work like from Shibon David

448
00:37:46,320 --> 00:37:51,600
but it's developed here and some some novel series developed in the context of classical models

449
00:37:51,600 --> 00:37:57,040
but then also some interesting experiments in the context of deep models but the core nugget

450
00:37:57,040 --> 00:38:04,640
here is this idea that what is the ideal representation that you should learn and the intuition the

451
00:38:04,640 --> 00:38:11,760
thing that they roll with is this idea that a good representation is one such that the optimal predictor

452
00:38:11,760 --> 00:38:16,320
built on top of that representation should be the same for all environments so if you've got a

453
00:38:16,320 --> 00:38:20,880
bunch of different environments or a family of different environments you want to learn a representation

454
00:38:20,880 --> 00:38:27,040
such that the predictor that you would then fit on top of that representation with the sort of

455
00:38:27,040 --> 00:38:34,640
environment diagnostic and what is environment in this context is it the the the world that generates

456
00:38:34,640 --> 00:38:43,120
the distribution of your input and targets that's right so this is for example like I might have

457
00:38:43,120 --> 00:38:47,840
to see you had this example of yeah classical example of like if I see fish they're always

458
00:38:47,840 --> 00:38:51,360
against the water in the background and I see something else that's always against certain

459
00:38:51,360 --> 00:38:57,200
back on these things are correlated but they're also you know exist I could I could also create a

460
00:38:57,200 --> 00:39:02,880
data set where I'd have like the fish is not against is you know that you can try for example

461
00:39:02,880 --> 00:39:06,960
but you need to get the idea where that correlation would be broken yeah right and the reason

462
00:39:06,960 --> 00:39:12,240
why is because it is not you know like the the key fundamental difference you know there there

463
00:39:12,240 --> 00:39:16,080
exists some environment where it's maybe even like anti-correlated or something like that so

464
00:39:16,080 --> 00:39:20,640
that even though like if you if you're just being supervised learning you can be very well by just

465
00:39:20,640 --> 00:39:26,000
using the the water background to produce the fish like in the worst case in the environments where

466
00:39:26,000 --> 00:39:30,400
all the fish are on land and all the you know the zebras are in the ocean or something then you're

467
00:39:30,400 --> 00:39:35,680
going to do really badly right right and so this paper kind of simply is saying we want a classifier

468
00:39:35,680 --> 00:39:41,280
that can perform equally well on fish whether there's water in the background or they're in an aquarium

469
00:39:41,280 --> 00:39:46,480
or they're mounted on a wall or whatever yeah I think if this is you know we rounded off a lot of

470
00:39:46,480 --> 00:39:52,800
the mathematical detail but I think we are you're getting at the ideas and in the key ideas like the

471
00:39:52,800 --> 00:39:58,080
deal representation learning that you should produce this representation such that the predictor

472
00:39:58,080 --> 00:40:02,160
on top of that is the same across all environments then there's these questions of well how many

473
00:40:02,160 --> 00:40:07,840
environments do I need to see and what kind of like under what like conditions have I actually

474
00:40:07,840 --> 00:40:15,040
covered adequately you know all the environments that I would like reasonably anticipate and in this case

475
00:40:15,040 --> 00:40:21,440
they sort of have a in the case of linear models have like a very sort of well developed and

476
00:40:21,440 --> 00:40:28,320
kind of involved theory but sort of the questions wide open about when this makes sense and how

477
00:40:28,320 --> 00:40:33,520
many environments we would need to have represented in our sort of training system in their set like

478
00:40:33,520 --> 00:40:37,680
your training set is you have multiple distinct environments and from each environment you have a

479
00:40:37,680 --> 00:40:42,960
data set they could say something that if if each of these environments is related to each other and

480
00:40:42,960 --> 00:40:48,240
some kind of complicated way then you could say that you know for fitting linear models that that

481
00:40:48,240 --> 00:40:54,400
we you know we need however so many environments in order to find this in varying predictor

482
00:40:54,400 --> 00:40:59,360
but things get a little bit more complicated when we start stepping towards you know I think the

483
00:40:59,360 --> 00:41:03,760
kinds of data and kinds of models that we want to work with which are in general not linear

484
00:41:03,760 --> 00:41:07,600
which is why we care about the representation learning at least in context of deep learning

485
00:41:07,600 --> 00:41:14,880
and so is the the contribution of this paper kind of a theoretical framework for these

486
00:41:14,880 --> 00:41:22,720
invariant risk minimization models and you know how to know if you have one or is it something

487
00:41:22,720 --> 00:41:28,080
more concrete where I can go create an invariant risk minimization model given the tools of the

488
00:41:28,080 --> 00:41:35,120
paper the the contribution right is I think there's this principle for how do you create a predictor

489
00:41:35,120 --> 00:41:41,280
there is a set of simple environments where you could argue that in a more like theoretical

490
00:41:41,280 --> 00:41:44,400
way that you're doing the right thing and then a set of empirical experiments and I believe

491
00:41:44,400 --> 00:41:48,960
doing empirical experiments are still very toy it's something like I think they look at something

492
00:41:48,960 --> 00:41:55,200
like like image and ad s phn images things like this us or not in a genesis image I'm like

493
00:41:55,200 --> 00:42:02,000
amnest where where you have some kind of distractor like there's colors is is associated with

494
00:42:02,000 --> 00:42:07,120
the category but this is not necessarily going to you know the degree to which color is associated

495
00:42:07,120 --> 00:42:11,920
but the category is changing and obviously you know the correlation the hope is that you know

496
00:42:11,920 --> 00:42:15,680
you're going to produce a model that depends less on color and then it's going to do better you know

497
00:42:15,680 --> 00:42:20,320
in the test environments where you know those the the the correspondence between color and

498
00:42:20,320 --> 00:42:27,600
category are that you've come to sort of rely on don't actually stand up unhauled out data okay

499
00:42:27,600 --> 00:42:32,480
so maybe you maybe case on price on the data sets where that is a predictive signal for not

500
00:42:32,480 --> 00:42:36,720
relying on it but you produce backup by producing a model that doesn't rely on color then

501
00:42:37,280 --> 00:42:40,880
if you go find a world where you know the correspondence between color and images is

502
00:42:40,880 --> 00:42:44,720
completely flat then you know your model will be robust something like that yeah there's

503
00:42:44,720 --> 00:42:49,840
another interesting feature of this paper that you pointed out that you don't see a lot of

504
00:42:49,840 --> 00:42:54,960
in paper there's a bit of a secratic dialogue in the back of the paper yeah you know I think I'm

505
00:42:54,960 --> 00:43:01,280
just going to have to lead that to the to the listener to read through this there's form their own

506
00:43:01,280 --> 00:43:07,440
opinions it's certainly different you're not it's not the typical section six in a technical

507
00:43:07,440 --> 00:43:13,360
machine learning paper uh-huh uh-huh I think they maybe wanted to keep with the theme of uh

508
00:43:13,360 --> 00:43:18,160
Greek since there's a lot of Greek symbols earlier in the paper yeah you know the dialogue does

509
00:43:18,160 --> 00:43:22,640
jump the shark at that but it's interesting that throughout the paper you know the authors and I

510
00:43:22,640 --> 00:43:27,920
think this is just they they are they are thinking seriously about these problems about causality

511
00:43:27,920 --> 00:43:34,240
and invariance and are in dialogue a little bit more than maybe you usually encounter initially

512
00:43:34,240 --> 00:43:39,520
in learning with the um philosophy of science which is not to say that you would expect to find a

513
00:43:40,160 --> 00:43:45,760
you know a forpage long secratic dialogue in a modern work on philosophy of science either

514
00:43:45,760 --> 00:43:52,400
but you know that there is uh the treatment of of the kind of like core philosophical questions is

515
00:43:52,400 --> 00:43:58,720
I think sincere and serious and and that the authors are unusually broad in their reading which I think

516
00:43:58,720 --> 00:44:04,640
you know in general is really nice to see mm-hmm so the next paper on your list is one that I was

517
00:44:04,640 --> 00:44:11,760
excited that you included and it's one that I've seen pop up quite a bit of late it's the your

518
00:44:11,760 --> 00:44:16,720
classifier is secretly an energy-based model and you should treat it like one paper uh what's that

519
00:44:16,720 --> 00:44:24,320
one about yeah so this is a paper um that I just came across because uh someone I know um left

520
00:44:24,320 --> 00:44:30,560
to comment and pointed me to it but this this paper was I just accepted with an oral presentation

521
00:44:30,560 --> 00:44:40,560
at ICLR and the key idea is basically so an energy-based model basically it is a model that

522
00:44:40,560 --> 00:44:47,280
assigns scores associated with each input and these scores correspond like unnormalized probability

523
00:44:47,280 --> 00:44:52,720
densities and you know there there's a literature on discriminative models where people try to

524
00:44:52,720 --> 00:44:57,600
produce classifiers that do really good at assigning conditional probabilities of predicting labels

525
00:44:57,600 --> 00:45:02,880
given some input uh there's a corresponding literature of people taking on you have to be very

526
00:45:02,880 --> 00:45:07,120
careful with the word generative modeling because there's a set of tasks associated with generative

527
00:45:07,120 --> 00:45:10,480
modeling that I think most of what we're doing almost what we're calling generative modeling in

528
00:45:10,480 --> 00:45:15,040
the context of GANS not really addressing but using it lightly in that sense of like trying to

529
00:45:15,040 --> 00:45:20,640
learn explicitly or implicitly uh you know uh a probability density over over your inputs over

530
00:45:20,640 --> 00:45:25,040
over your x over your images there's this other literature that does that right including

531
00:45:25,040 --> 00:45:31,360
variational auto encoders including GANS and there have been plenty of approaches on that side

532
00:45:31,360 --> 00:45:37,600
that have adopted this sort of energy-based approach to incorporating it either into the GANS

533
00:45:37,600 --> 00:45:43,200
framework or whatever but sort of there's always this kind of tension that sort of it seems that

534
00:45:43,200 --> 00:45:48,000
like when you train a generative model you know a generative models are capable of performing

535
00:45:48,000 --> 00:45:53,520
prediction but it seems like you never do as well as when you just train a proper discriminative

536
00:45:53,520 --> 00:45:57,680
model itself right if you want to do prediction you know sort of like the theme of like if you

537
00:45:57,680 --> 00:46:02,560
want to do prediction the takeaway so far is like you're better off straight up training a

538
00:46:02,560 --> 00:46:07,120
discriminative model not training a generative model and then trying to trying to run it in some

539
00:46:07,120 --> 00:46:12,240
kind of manner to to generate predictions and with this this model this we says that it sort of

540
00:46:12,240 --> 00:46:17,440
just says hey let's just take a discriminative model like a standard image classifier and you can

541
00:46:17,440 --> 00:46:21,040
sort of interpret it you're keeping the architecture and everything as you would for just uh

542
00:46:21,040 --> 00:46:25,600
sort of off the shelf the discriminative model you are changing uh the learning objective and how

543
00:46:25,600 --> 00:46:31,200
you sample from it but the key idea here is to sort of say that you feed an input to a discriminative

544
00:46:31,200 --> 00:46:34,960
model you can get out some logits then you run the softmax function it turns you logits into

545
00:46:34,960 --> 00:46:39,680
predictive probabilities and you also have like an extra degree of freedom there which is that

546
00:46:39,680 --> 00:46:43,760
if you were to like move all your logits up by some fixed amount and move it down by some fixed

547
00:46:43,760 --> 00:46:48,400
amount you would get out the same softmax probabilities and that's just because sort of um

548
00:46:49,280 --> 00:46:54,080
all of your probabilities need to sum up the one so uh there's actually a degree of freedom there

549
00:46:54,080 --> 00:46:57,760
so it says hey we'll just keep the entire architecture we'll keep all the parameters we keep

550
00:46:57,760 --> 00:47:00,720
everything the same degree of discriminative model but what we're going to do is we're going to

551
00:47:00,720 --> 00:47:07,440
basically interpret sort of pre softmax logits as sort of unnormalized probability densities

552
00:47:07,440 --> 00:47:12,560
like probability of x and y and then they're going to train in uh you know the manner of an energy

553
00:47:12,560 --> 00:47:17,840
based model sort of doing a train with like two objectives one is to just maximize the classification

554
00:47:17,840 --> 00:47:22,320
performance using like the standard cross entropy loss and they're also going to train using

555
00:47:22,320 --> 00:47:29,440
this like energy type objective to make it so that basically the sum of the pre softmax logits

556
00:47:29,440 --> 00:47:34,240
has like a sort of high energy for in distribution data and hopefully you know low energy for

557
00:47:34,240 --> 00:47:38,720
out of distribution data and what what it ends up being really interesting about this is that

558
00:47:38,720 --> 00:47:43,520
in this tension between discriminative and generative models you find basically like usually

559
00:47:43,520 --> 00:47:47,360
if you train a generative model you do good at the things that you want the generative model do

560
00:47:47,360 --> 00:47:52,880
you do bad at things uh you do bad at prediction even when you try to you know leverage your generative

561
00:47:52,880 --> 00:47:57,440
model in the predictions and vice versa when there's ways of using it the discriminative models

562
00:47:57,440 --> 00:48:02,320
generative model they're usually not as good as if you just did uh you know use them more familiar

563
00:48:02,320 --> 00:48:07,920
I mean I'm using the word generative model here but like gans or something yeah yeah and so the end

564
00:48:07,920 --> 00:48:12,720
result here which you know if this stands up I think it's super exciting is that they basically

565
00:48:12,720 --> 00:48:19,760
have a single model that they have trained and this model ends up being good at classification

566
00:48:19,760 --> 00:48:25,040
it also um basically you can now sample from it by basically doing something uh calls the

567
00:48:25,040 --> 00:48:29,840
caustic gradient the launch of end dynamics and so you basically you have some initialization

568
00:48:29,840 --> 00:48:34,640
at the input and you take uh gradient steps over the energy function with some amount of noise

569
00:48:34,640 --> 00:48:39,600
injected and this is sort of equivalent to like sort of sampling if you run it for a long time

570
00:48:39,600 --> 00:48:43,040
and so there's other people out there that are much better experts than I am about uh

571
00:48:43,040 --> 00:48:48,720
London dynamics but the the interesting thing is that if you sample from this thing the samples

572
00:48:48,720 --> 00:48:54,320
that come out end up doing better or like as well are better on the generative model type

573
00:48:54,320 --> 00:48:59,200
objectives as a lot of generative models out there the model does you know still does really good

574
00:48:59,200 --> 00:49:05,360
as a discriminative model and then it turns out that the uh probabilities that you get out of this

575
00:49:05,360 --> 00:49:11,760
model tend to be uh pretty well calibrated whereas in general a neural network probabilities

576
00:49:11,760 --> 00:49:15,280
that you know conditional probabilities assigned to class is tend to be way overconfident

577
00:49:15,280 --> 00:49:19,040
the predicted probability of like you know whichever class you know is the art max of the

578
00:49:19,040 --> 00:49:23,840
salt max tends to be overconfident and then this model also turns out that these these scores

579
00:49:23,840 --> 00:49:28,480
is unnormalized densities that basically give higher probability to in some you know things that

580
00:49:28,480 --> 00:49:34,080
you know you think are high density data and lower lower unnormalized probabilities things that are

581
00:49:34,080 --> 00:49:40,560
you know low density data turn out to be good on a range of empirical benchmarks for detecting

582
00:49:40,560 --> 00:49:46,400
shifts like out of distribution and then finally it seems that the models that they get out of it

583
00:49:46,400 --> 00:49:53,840
also do a strangely good job as evaluated against they do slightly worse than the state of the art

584
00:49:53,840 --> 00:49:58,400
for adversarial robustness and so it's a kind of interesting and exciting here is hey this is

585
00:49:58,400 --> 00:50:02,640
the model that's doing well for discriminative modeling it's getting generative performance that

586
00:50:02,640 --> 00:50:08,880
is similar to uh GANS at least by determined by I think like Frichet inception distance or one

587
00:50:08,880 --> 00:50:13,440
of those or inception similarity one of those you know metrics that they use it's doing well

588
00:50:13,440 --> 00:50:18,160
in calibration which I'm a little bit fuzzier on why that should be on in whether that

589
00:50:18,160 --> 00:50:22,640
that there's any principle that this classifier should be calibrated but perhaps interesting that

590
00:50:22,640 --> 00:50:27,840
it tends to be at least empirically on the data sets that they've looked at it's getting adversarial

591
00:50:27,840 --> 00:50:32,480
robustness and it's useful for out of distribution shift detection so there is something kind

592
00:50:32,480 --> 00:50:36,880
of interesting here and it'll be really interesting to see over the next few months do

593
00:50:36,880 --> 00:50:40,720
does it turn out that basically you know that there's a long history of someone sort of

594
00:50:40,720 --> 00:50:44,800
coming up with a new kind of model that appears to be adversarial robust but that's just because

595
00:50:44,800 --> 00:50:50,960
there's some you know as of yet there's some smart way to attack it that they hadn't considered

596
00:50:50,960 --> 00:50:54,880
because they're proposing the new model we hadn't set the adversaries against it yet

597
00:50:54,880 --> 00:51:00,480
right the adversary is being the graduates right exactly so yeah so I think that you know there

598
00:51:00,480 --> 00:51:04,240
is something interesting here and I think that it also captains the maybe a broader current

599
00:51:04,240 --> 00:51:10,240
in the literature so there's these other papers by Alex Majrey's group at MIT where they've shown

600
00:51:10,240 --> 00:51:15,920
things that adversarial robust models have all kinds of properties that you would normally associate

601
00:51:15,920 --> 00:51:21,280
with GANS or like you know basically models performing this if we don't want to use the

602
00:51:21,280 --> 00:51:26,400
abuse or generative any more because that synthesis is high pass right and so you could do in

603
00:51:26,400 --> 00:51:31,200
sailing with an adversarial robust model or you could generate images or when you do a targeted

604
00:51:31,200 --> 00:51:36,240
adversarial attack that turn an image from one class to another so basically if you take a picture

605
00:51:36,240 --> 00:51:41,440
of a kitten and you try to turn it into a picture of a banana a picture that's classified as a

606
00:51:41,440 --> 00:51:46,880
banana right exactly if you do this with a vanilla classifier what you get is a picture that looks

607
00:51:46,880 --> 00:51:51,760
like a slightly noisy kitten that's misclassified as a banana would you do with the adversarial robust

608
00:51:51,760 --> 00:51:57,360
model you update the pixels of a kitten to look like banana so instead it looks to the classifier

609
00:51:57,360 --> 00:52:03,520
like a banana the weird thing is that it actually makes it look like a banana so like the weird

610
00:52:03,520 --> 00:52:08,000
thing is it's sort of of all the things that could have made it look like it could have made it

611
00:52:08,000 --> 00:52:12,720
just look like something so bizarre that it was not an image in which case like we don't have any

612
00:52:12,720 --> 00:52:17,440
right to expect the model classified as a banana or as a not banana right it could have just made

613
00:52:17,440 --> 00:52:22,880
it look like total garbage and that would sort of it's not clear why the adversarial objective

614
00:52:22,880 --> 00:52:26,800
would have a problem with that as long as it's sufficiently different from the input image right

615
00:52:26,800 --> 00:52:30,880
along it doesn't look like a cat the weird thing is that it ends up making it actually look visually

616
00:52:30,880 --> 00:52:35,200
like a kitten and so there's already this evidence that like some of these things that we maybe

617
00:52:35,200 --> 00:52:40,960
didn't have any good reason to believe a related like robustness and this sort of like image synthesis

618
00:52:40,960 --> 00:52:46,240
capability are actually related to each other and in a way that still right now I think it's

619
00:52:46,240 --> 00:52:50,960
floating around at the level of intuition but it'll be interesting to see if we can kind of like

620
00:52:50,960 --> 00:52:55,360
unify why these tasks are related to each other vis-a-vis you know maybe the dynamics of neural

621
00:52:55,360 --> 00:53:04,000
network training all right cool so you've also got a couple of papers or things that your group

622
00:53:04,000 --> 00:53:10,400
has been working on since we last spoke what do you have going there so one paper that I'm really

623
00:53:10,400 --> 00:53:18,160
excited about just got accepted at ICLR 2020 congrats thanks um so this is work with my student

624
00:53:18,160 --> 00:53:23,680
Vivian Kaushik and basically the idea we talked a little bit earlier about this concern and natural

625
00:53:23,680 --> 00:53:30,320
language processing about the models you know it's okay made it's accurate but is it is it really

626
00:53:30,320 --> 00:53:35,840
like learning the right correlations right and the problem is that you it's not so well defined

627
00:53:35,840 --> 00:53:40,640
what makes a correlation like like you know these people starting to say like is it picking is

628
00:53:40,640 --> 00:53:45,040
a really learning the past or is it learning superficial correlations or bad correlations or

629
00:53:45,040 --> 00:53:49,120
spurious ones the problem is those words don't have like a clear formal meeting within statistical

630
00:53:49,120 --> 00:53:53,520
learning like if you've got two sets of features and they're both associated with the output what

631
00:53:53,520 --> 00:53:58,080
is the general principle according to which you should your model should be relying on one first

632
00:53:58,080 --> 00:54:05,280
the other right and so we kind of cast this problem of within sort of a causal framework you know

633
00:54:05,280 --> 00:54:10,480
we don't use the mathematical machinery of causality but we use this sort of thinking that the

634
00:54:10,480 --> 00:54:15,840
relationship here is vis-a-vis intervention so the reason we give you a clear example to make

635
00:54:15,840 --> 00:54:21,040
this like real so we take a movie reviews and if you're trying to classify our movie reviews

636
00:54:21,040 --> 00:54:25,360
you find that the classifier puts a lot of weight you know just a linear classifier on bad words

637
00:54:25,360 --> 00:54:30,320
it puts a lot of weight positive weight on words like fantastic excellent but it also puts positive

638
00:54:30,320 --> 00:54:35,520
weight on words like romance which is a genre not really a sentiment and then if you it puts

639
00:54:35,520 --> 00:54:40,000
negative weight on words like terrible but also on the word horror which is again the genre like

640
00:54:40,000 --> 00:54:43,280
why can't you have a great horror movie or why why should the fact that it's a horror movie

641
00:54:43,280 --> 00:54:48,160
matter and the reason why is because for whatever other reason perhaps like the confounder is

642
00:54:48,160 --> 00:54:52,560
something like low budget that like Hollywood romance movies tend to be better received tend to

643
00:54:52,560 --> 00:54:58,160
have higher reviews and that horror movies tend to be lower rate right yep so you think like why

644
00:54:58,160 --> 00:55:02,880
shouldn't why shouldn't that be the case why shouldn't the model put weight on those and one

645
00:55:02,880 --> 00:55:08,400
reason why is to say it's the formalizing these like through the the concept of an intervention

646
00:55:08,400 --> 00:55:14,240
that if I took a movie and I changed the genre keeping everything else the same it shouldn't

647
00:55:14,240 --> 00:55:18,400
change the sort of applicable label to that movie review whether it's positive or not right even

648
00:55:18,400 --> 00:55:24,720
if it's correlated vis-a-vis this like sort of confounding that actually intervening upon it

649
00:55:24,720 --> 00:55:30,880
should make a difference so the key idea we had here was to use is that what is real the you know

650
00:55:30,880 --> 00:55:36,880
the the actual like substantial the causal connection verse what is not might not even be identifiable

651
00:55:36,880 --> 00:55:41,360
from the observational data alone so the key idea is to use humans in the loop to supply that

652
00:55:41,360 --> 00:55:46,720
information and so what we do is we kind of flip the crowdsourcing paradigm so instead of saying

653
00:55:46,720 --> 00:55:52,000
here's a review and here's a label or so here's a review give me the label instead what we do is we

654
00:55:52,000 --> 00:55:56,640
say here's a review and here's the actual label associated with it like positive or negative

655
00:55:57,200 --> 00:56:02,720
then below that review we have the same exact review like pre-populating an editable text box

656
00:56:02,720 --> 00:56:06,960
and then to the right of it we have a counterfactual label which is like the opposite you know like

657
00:56:06,960 --> 00:56:11,360
this was a positive review now make it negative it was a negative review now make it positive

658
00:56:11,360 --> 00:56:16,880
and so their goal is to edit the review such that it accords with a counterfactual label right

659
00:56:17,680 --> 00:56:21,760
so then what we're basically told to say editor review said it accords with a kind of actual label

660
00:56:21,760 --> 00:56:26,640
basically leave it in a in a internally consistent state so don't just like edit one sentence you

661
00:56:26,640 --> 00:56:32,400
basically have to edit it so that it's fully coherent yeah however three don't make

662
00:56:32,400 --> 00:56:36,640
gratuitous changes so basically don't change any facts that don't need to change to flip the

663
00:56:36,640 --> 00:56:40,960
applicability of the label and so that's what's happening is that we now for every single original

664
00:56:40,960 --> 00:56:45,440
image every single original review that was negative we have a mirror image review that's positive

665
00:56:45,440 --> 00:56:49,520
for every original review positive we have a mirror image review that's negative it's like you

666
00:56:49,520 --> 00:56:54,320
know evil twin yeah and in these ones what's happened is that you know in that original

667
00:56:54,320 --> 00:56:58,240
data set horror occurred in all these negative reviews but in the new data set those are all

668
00:56:58,240 --> 00:57:02,640
positive reviews but they all still have the genre horror and that's because the human knows that

669
00:57:02,640 --> 00:57:06,400
horror is not the thing that needs to be flitted to change the applicability of labels so they

670
00:57:06,400 --> 00:57:11,280
don't intervene on it so the humans are actually like communicating some information to us that

671
00:57:11,280 --> 00:57:16,240
wasn't even necessarily in that sort of an argument we make it's not clear that information is

672
00:57:16,240 --> 00:57:21,280
even identifiable from the original data set alone but the humans by virtue of not intervening

673
00:57:21,280 --> 00:57:26,080
on that data has sort of revealed to us that structure and so we see interesting things like if you

674
00:57:26,080 --> 00:57:31,440
basically if you train on the original data and evaluate on the holdouts that from the counter

675
00:57:31,440 --> 00:57:37,200
factually revised data you go from 90% accuracy to like 55% accuracy and vice versa train on

676
00:57:37,200 --> 00:57:41,440
the revised data you get like 90% accuracy but evaluate it on the original data and you go down

677
00:57:41,440 --> 00:57:46,640
like 55% accuracy if you train on both data sets together so we call the counter factually

678
00:57:46,640 --> 00:57:52,480
augmented data you end up with a model that gets like 88% accuracy on both which is pretty good

679
00:57:52,480 --> 00:57:57,840
it basically means you know you pay a small price for not relying on these sort of like spurious

680
00:57:57,840 --> 00:58:02,720
correlates but surprisingly small you know you're getting like just you're just paying a couple

681
00:58:02,720 --> 00:58:06,800
percentage points in accuracy and you're basically getting good performance on both the original

682
00:58:06,800 --> 00:58:11,200
data and the counter factually augmented data so the name of the paper is learning the difference

683
00:58:11,200 --> 00:58:15,680
that makes a difference with counter factually augmented data and I think that this sort of you

684
00:58:15,680 --> 00:58:20,400
know leads us towards what I'm excited about is that I think the conversation about these data set

685
00:58:20,400 --> 00:58:25,360
artifacts, spuriousness, whatever has been a little bit derailed by a sort of failure to recognize

686
00:58:25,360 --> 00:58:30,880
that one this this this is not just that sort of IIDML problem it's asking us about something

687
00:58:31,680 --> 00:58:37,920
beyond predicted accuracy and perhaps even beyond what's identified in the in the observational data

688
00:58:37,920 --> 00:58:43,200
but by soliciting this this intervened upon data we're we're actually able to tease apart

689
00:58:43,200 --> 00:58:47,280
and so we actually see some interesting things so we can go empirically places that we can't go

690
00:58:47,280 --> 00:58:51,760
theoretically and one thing that we see is that if you train the model on the counter factually

691
00:58:51,760 --> 00:58:55,840
augmented data not only do you do well on both but all those words that you thought should not be

692
00:58:55,840 --> 00:59:01,840
related like horror romance both live some like a lot of these words that just like why is that

693
00:59:01,840 --> 00:59:06,560
associated with sentiment yeah they actually fall out of the model so they they've ceased to become

694
00:59:06,560 --> 00:59:12,160
like high high co-efficient features and like the linear model but then we see for both linear models

695
00:59:12,160 --> 00:59:17,600
LSTMs birth models if we train on the counter factually augmented data we also do better out of

696
00:59:17,600 --> 00:59:22,240
domain so we go outside movie reviews and we train those models and we run the value of those

697
00:59:22,240 --> 00:59:27,120
models on on a different sentiment task like Yelp restaurant reviews or something like that

698
00:59:27,120 --> 00:59:31,120
that are not movies and what is romance or horror mean in the context of a restaurant review

699
00:59:31,120 --> 00:59:35,920
it's not clear right but those the models that were trained on the counter factually augmented data

700
00:59:35,920 --> 00:59:41,840
empirically generalize significantly better not you know in the iid notion of generalization

701
00:59:41,840 --> 00:59:46,000
but in that across domain sense that we're talking about earlier right so it gives us some

702
00:59:46,000 --> 00:59:51,120
suggestion that there you know there's something substantial here and you know I think the next big

703
00:59:51,120 --> 00:59:55,920
leap is to sort of figure out how can we just like on one hand I think we've learned something

704
00:59:55,920 --> 01:00:00,080
really interesting conceptually and we've created a data set and resource that we've released

705
01:00:00,080 --> 01:00:04,080
in the public that hopefully other people can do interesting things with on the other hand is

706
01:00:04,080 --> 01:00:09,360
an extremely laborious process so so the huge part of the work that makes possible was my student

707
01:00:09,360 --> 01:00:15,600
had to basically single handedly wrangle a workforce of like 800 crowd workers in order to

708
01:00:16,160 --> 01:00:21,120
and then actually manually inspect them to make sure that all of these revisions were coherent and

709
01:00:21,120 --> 01:00:24,880
made sense and that people were actually doing the past we were asking a lot more from them and

710
01:00:24,880 --> 01:00:30,000
just like check the right radio button we're saying revise a paragraph of prose which is usually

711
01:00:30,000 --> 01:00:35,840
hard with crowd crowdsource crowd workers right you know but I think we learned a lot of things one

712
01:00:35,840 --> 01:00:43,520
is that I think you get a lot of benefit from really actively monitoring the process you get a

713
01:00:43,520 --> 01:00:50,240
lot of benefit from paying them well and that there's a huge like really significant factor that's

714
01:00:50,240 --> 01:00:55,600
determined by the quality of the instructions that you give them so we we kind of recognize over

715
01:00:55,600 --> 01:01:00,720
that and I think getting the instructions right and reaching them crystal clear was a big factor that

716
01:01:00,720 --> 01:01:07,120
made this the results better than if we had given them some hard work of ours instructions all right

717
01:01:07,120 --> 01:01:13,200
we're getting short on time so before we move on to your predictions you also mentioned another

718
01:01:13,200 --> 01:01:17,680
paper that you're excited about can you give us a quick summary of the fair ML from a non ideal

719
01:01:17,680 --> 01:01:24,400
perspective paper sure so this is and I'm going to ruin his name in a horrible way such that

720
01:01:24,400 --> 01:01:29,200
you know I'll never talk to you again but this is work with a postdoc student is got a Persian

721
01:01:29,200 --> 01:01:36,800
surname Sina Fuzzleport so sorry Sina for your name but this is work that we've done together

722
01:01:36,800 --> 01:01:42,160
that basically we've been looking at a lot of the work in fair machine learning both from the

723
01:01:42,160 --> 01:01:46,720
sort of physical approach and the causal approach and coming up against a certain set of frustrations

724
01:01:46,720 --> 01:01:50,640
and the main set of frustrations is sort of that and we this is sort of what we talked about last

725
01:01:50,640 --> 01:01:55,040
time is that what what they're sort of missing is the right set of ingredients to even make a

726
01:01:55,040 --> 01:01:59,360
determination about what is the just thing to do that you could very easily for a lot of these

727
01:01:59,360 --> 01:02:04,000
things construct two different scenarios where you could describe them both in terms of you know

728
01:02:04,000 --> 01:02:08,160
choose your covariance here's your label here's your prediction and here's a sensitive feature

729
01:02:08,640 --> 01:02:13,280
and you know what seems like the Justin to do is very different you know if we're say on

730
01:02:14,160 --> 01:02:20,400
hiring and like one case is why do we think that there's something different about the way you

731
01:02:20,400 --> 01:02:24,720
should approach incorporating demographic in the hiring when you're considering say whites and

732
01:02:24,720 --> 01:02:30,640
Asian applicants versus white and black applicants in the United States and the reason why is because

733
01:02:30,640 --> 01:02:36,400
that there's a very different question of justice is because in one of them the current situation

734
01:02:36,400 --> 01:02:42,400
is a product of very well documented very well understood history of discrimination in justice

735
01:02:42,400 --> 01:02:47,520
and the other case you have a sort of over representation that sort of occurs not because of

736
01:02:47,520 --> 01:02:52,960
discrimination but maybe in spite of and so that absent this coherent causal story of the data

737
01:02:52,960 --> 01:02:59,440
that you're addressing and also maybe a coherent causal story of what is the impact of a proposed

738
01:02:59,440 --> 01:03:04,240
policy intervention that you know you don't really have the right ingredients to offer someone

739
01:03:04,240 --> 01:03:11,200
a off-the-shelf solution to say oh this is the way to be fair to be just so basically in this

740
01:03:11,200 --> 01:03:16,880
context of formulating these this sort of argument about so we can't just sort of hand people

741
01:03:16,880 --> 01:03:21,360
tools and tell them that they are these like oh like plug your classifier into this and it'll

742
01:03:21,360 --> 01:03:25,680
magically become fair that it doesn't have the right set of inputs scene I made a really nice

743
01:03:25,680 --> 01:03:30,160
connection to a lot of the literature that arose in the context of political philosophy

744
01:03:30,160 --> 01:03:34,960
in the context of segregation and integration and there it was actually were especially influenced

745
01:03:34,960 --> 01:03:39,520
by a work called the imperative of integration by a political philosopher named Elizabeth Anderson

746
01:03:39,520 --> 01:03:43,920
and you see throughout this book she makes this distinction really clear between the ideal and

747
01:03:43,920 --> 01:03:49,760
the non-ideal approach in philosophy and basically the ideal approach sort of asserts what does the

748
01:03:49,760 --> 01:03:54,160
like in the most naive form in the ideal approach you sort like what does the perfect world look like

749
01:03:54,160 --> 01:03:58,640
and you sort of identify some discrepancies between the world you live in and the ideal approach

750
01:03:58,640 --> 01:04:03,840
and this is your justification and so historically you know a sort of naive of

751
01:04:03,840 --> 01:04:08,160
application of ideal approach might be something like saying that we must have a colorblind policy

752
01:04:08,160 --> 01:04:13,040
for everything because in the perfect world everyone would be colorblind and the non-ideal approach

753
01:04:13,040 --> 01:04:17,520
is more concerned with well what are we don't live in a perfect world so what is the landscape

754
01:04:17,520 --> 01:04:22,880
of injustices and and basically what you know why are we in a situation where in the who are the

755
01:04:22,880 --> 01:04:28,000
agents what the moral responsibility to do something about it and then what are the strategies

756
01:04:28,000 --> 01:04:32,400
that are actually going to be effective given the dynamics of the kind of messed up world that we

757
01:04:32,400 --> 01:04:40,240
live in and so you know in the context of like say the naive ideal approach for a raceblind policy

758
01:04:40,240 --> 01:04:46,240
the non-ideal approach would say something like well given we don't live in that ideal world so

759
01:04:46,240 --> 01:04:50,400
given that people have whether or not we even believe that race is real given that on the count

760
01:04:50,400 --> 01:04:54,880
of perceived race people have been discriminated against in their these mechanisms in place

761
01:04:54,880 --> 01:05:01,120
what then is sort of your responsibility to act and so the I think that you know sort of maybe

762
01:05:01,120 --> 01:05:04,880
argument against this casting of like ideal approach is that it's almost like so

763
01:05:04,880 --> 01:05:10,640
uh it's also naive that like you'd say well maybe no one actually believes it so literally and

764
01:05:10,640 --> 01:05:16,400
that actually you know in some ways it's you know besides maybe like someone reaching for like

765
01:05:16,400 --> 01:05:22,080
us like a like a naive argument against affirmative action or something like that but actually in a

766
01:05:22,080 --> 01:05:27,520
lot of ways it's actually precisely in uh the fair machine learning approaches that we're seeing

767
01:05:28,160 --> 01:05:33,600
precisely like this kind of you know like like each of these parodies is something that would hold

768
01:05:33,600 --> 01:05:39,600
in our ideal world where um whatever is the demographic didn't matter and the proposed approach

769
01:05:39,600 --> 01:05:45,440
you just latch on to one of them and then minimize the disparity. So is the idea is is this

770
01:05:45,440 --> 01:05:53,520
principally a critique of the fair ML kind of the state of the the world in fair ML or

771
01:05:53,520 --> 01:06:00,880
is are you more trying to point to tools and that have been discovered or presented in the

772
01:06:00,880 --> 01:06:07,520
political philosophy world and kind of getting to non-ideal approaches and drawing analogies for

773
01:06:07,520 --> 01:06:14,160
fair ML. Yeah so I wouldn't say so much that the purpose of the paper is to offer a critique. I

774
01:06:14,160 --> 01:06:21,200
think there are some sort of well-known problems with any of these individual parity metrics but

775
01:06:21,200 --> 01:06:28,640
I think that's really more to one maybe present something of a more unifying view and maybe

776
01:06:28,640 --> 01:06:34,560
most importantly just to make a connection to a large existing body of work that I think a lot of

777
01:06:34,560 --> 01:06:39,600
people there are people in the space are aware of but I think a lot of people in the space are

778
01:06:39,600 --> 01:06:45,280
not and I think this is something that maybe you know by by by stepping back from the like

779
01:06:45,280 --> 01:06:50,960
fair ML kind of I've utilized classification context and looking at actually cases that have been

780
01:06:50,960 --> 01:06:56,160
probed you know really much more critically and I think that like cases like segregation

781
01:06:56,160 --> 01:07:01,280
integration offer a sort of policy context where people have really thought deeply about the

782
01:07:01,280 --> 01:07:07,600
problem and by reading a book like I think Anderson's imperative of integration you see sort of what

783
01:07:07,600 --> 01:07:14,720
what is the actual work that goes into making a coherent policy argument the fact that you know

784
01:07:14,720 --> 01:07:19,920
that this includes among other things a sort of an account for the disparity a sort of like

785
01:07:20,480 --> 01:07:25,840
normative framework that tells you who's responsible for intervening which is not necessarily

786
01:07:25,840 --> 01:07:29,440
congruent with a set of people that are responsible for the problem in the first place

787
01:07:29,440 --> 01:07:34,880
that actually is a pragmatic one that takes into account the best evidence for for what the

788
01:07:34,880 --> 01:07:40,960
actual impact of proposed interventions would be and is it you know are they actually not just

789
01:07:40,960 --> 01:07:46,160
beneficial in a sense of like the entries to your confusion matrix then you're like idealized

790
01:07:46,160 --> 01:07:50,800
view of the world but actually do they sort of lead to the kind of social change that we sort of

791
01:07:50,800 --> 01:07:57,760
are are seeking I think that hopefully by making this connection between this sort of previous

792
01:07:57,760 --> 01:08:03,600
body of work and philosophy and what's happening right now in Fair ML I think if anything maybe we

793
01:08:03,600 --> 01:08:08,800
allow the hope is made to give people an opportunity that sort of learn from sort of people's

794
01:08:08,800 --> 01:08:15,360
past mistakes rather than having to make them all ourselves got it got it so maybe let's switch gears

795
01:08:15,360 --> 01:08:23,600
and talk a little bit about your predictions for 2020 or if you dare the next decade since we're

796
01:08:23,600 --> 01:08:31,760
entering a new one number one prediction and I think this is already well underway so maybe

797
01:08:31,760 --> 01:08:36,960
it's a conservative prediction but I think that what we're seeing and we'll continue to see and

798
01:08:36,960 --> 01:08:45,520
become more obvious is is a kind of great commodification that the technology probably talk about like

799
01:08:45,520 --> 01:08:50,800
exponential progress and technology so I think actually in a lot of ways certain aspects of ML

800
01:08:50,800 --> 01:08:55,600
progress have started to stagnate a little bit because that's how things go we don't we don't just

801
01:08:55,600 --> 01:08:59,680
blow up exponentially we we make a little progress we have a little bit of a rupture and then things

802
01:08:59,680 --> 01:09:05,280
slow down again and tell the next big idea where things are growing it sort of horizontally is that

803
01:09:05,280 --> 01:09:10,400
I think we used to be you went to nerfs or ICML and you saw the same four companies that were

804
01:09:10,400 --> 01:09:17,040
basically massively represented it which is you know Google Microsoft Facebook you know some Amazon

805
01:09:17,760 --> 01:09:22,800
some Apple maybe and now what you're starting to see is a lot of like boring companies

806
01:09:22,800 --> 01:09:27,760
showing up the nerfs and sending you know and I say it's not as like a put down but more like this

807
01:09:27,760 --> 01:09:36,640
isn't like just like sole province of the extremely like tech elite but that this technology I think

808
01:09:36,640 --> 01:09:41,520
is going to start to become boring technology in the way that like any successful technology does

809
01:09:41,520 --> 01:09:46,800
right so I think there was a time when you built the first object oriented program you were doing

810
01:09:46,800 --> 01:09:52,080
something riveting that was you know academic kind of happening and there's a time now where

811
01:09:52,080 --> 01:09:57,280
basically every single person building a website is building an object oriented program and I think

812
01:09:57,280 --> 01:10:04,480
that we're seeing a lot more of this of sort of banks government just sort of all throughout the

813
01:10:04,480 --> 01:10:10,320
economy that there's sort of a larger sort of base of people that are sort of participating in

814
01:10:10,320 --> 01:10:15,360
these technologies and these jobs and it's becoming much more widely diffused at the same time I think

815
01:10:15,360 --> 01:10:19,440
you know a lot of this there was this moment right where if you could train a neural network you

816
01:10:19,440 --> 01:10:25,120
had a route to possibly like a 400k year job or something absolutely ludicrous you know that

817
01:10:25,120 --> 01:10:31,440
a number of companies and I think that now at the very top you know I think there's not as many

818
01:10:31,440 --> 01:10:36,880
people that could could run a research lab that could you know I think they're still there's

819
01:10:36,880 --> 01:10:42,480
still parts of the economy that are as hot or hotter than ever but I think that a lot of this

820
01:10:42,480 --> 01:10:49,520
just kind of can train neural network to do X is getting commodified and it's just going to you

821
01:10:49,520 --> 01:10:55,760
know the there's a lot more people could do it and a lot more companies will be doing it and I

822
01:10:55,760 --> 01:11:01,680
think it's going to start becoming just more kind of commonplace and obvious I think at the same

823
01:11:01,680 --> 01:11:06,480
time hand in hand with that and this ties into the maybe like theme of the conversation is that I

824
01:11:06,480 --> 01:11:11,920
think that that's sort of like that's the direction I think for this established sort of just

825
01:11:11,920 --> 01:11:20,320
predictive models as on one hand yeah get boring become widely diffused at the same time maybe

826
01:11:20,320 --> 01:11:25,120
a little bit of a stagnation on progress on the other hand I think a lot of the creative people

827
01:11:25,120 --> 01:11:30,320
are going to be pushing more and more sort of beyond the limits of just this sort of train test

828
01:11:30,320 --> 01:11:35,360
prediction and I think that'll you know one direction is people thinking more seriously about

829
01:11:35,360 --> 01:11:40,480
generative models I think one direction is people thinking much more seriously about causality

830
01:11:40,480 --> 01:11:45,920
people trying to come up with more expansive or ambitious ideas about robustness so we've been

831
01:11:45,920 --> 01:11:51,040
sort of my authentically fixated on this idea of perturbations within the L2 ball or within the

832
01:11:51,040 --> 01:11:56,240
L infinity ball like for adversarial examples but starting to get a bit more ambitious with

833
01:11:56,240 --> 01:12:02,160
kinds of invariances and kinds of robustness that we want to build into models and I think

834
01:12:02,160 --> 01:12:06,240
hand in hand with causality and the papers we talked about earlier I think is also starting to

835
01:12:06,240 --> 01:12:13,760
ask research questions that situate these models in the context of the like wider decision-making

836
01:12:13,760 --> 01:12:18,640
process that they're actually part of so I think this kind of integration of machine learning

837
01:12:18,640 --> 01:12:24,400
and economics is sort of going to be an exciting area and I think that's really if I have to look

838
01:12:24,400 --> 01:12:30,000
you know over the next five ten years what I think is going to blossom I think that is that sort of

839
01:12:30,000 --> 01:12:34,880
I can't say that I see completely the technical router that I have all the right tools to make it

840
01:12:34,880 --> 01:12:39,360
blossom but I think in terms of like what needs to in order for this technology actually to be

841
01:12:39,360 --> 01:12:44,640
deployed in the kinds of ways that we imagine it should be that's the kind of research that needs

842
01:12:44,640 --> 01:12:49,360
to start happening and so I think we're going to start seeing the field kind of setting it

843
01:12:49,360 --> 01:12:54,560
sides a little bit a little bit wider yeah do you have a sense for you you mentioned not being

844
01:12:54,560 --> 01:12:59,440
able to see clearly what the technical pieces of that but do you have any kind of sense for what

845
01:12:59,440 --> 01:13:06,000
that needs to look like certainly some of the things we've talked about in the context of fairness

846
01:13:06,000 --> 01:13:12,560
some of these fat star papers feedback loop papers are in this vein is there kind of a broader way

847
01:13:12,560 --> 01:13:18,160
to characterize what happens when these two fields start to you know collide more frequently

848
01:13:18,160 --> 01:13:23,280
I think what really needs to happen is that where I think where we're sort of in trouble right now

849
01:13:23,280 --> 01:13:28,320
is that we have the pure predictive modeling world which is sort of conceptually involved

850
01:13:28,320 --> 01:13:35,040
rich but is able to deal with really rich real world better so if we pretend all we care about

851
01:13:35,040 --> 01:13:42,000
is prediction then we're like limited to a very kind of you know really flat set of conceptual

852
01:13:42,000 --> 01:13:48,080
questions we can ask but we're able to address them concerning really rich spaces of interesting

853
01:13:48,080 --> 01:13:54,160
data then on the other hand I think we have a much richer conceptual worlds that are offered by

854
01:13:54,160 --> 01:14:00,560
the language of causality the language of the economic modeling that get in towards a much more

855
01:14:00,560 --> 01:14:06,080
you know deeper and critical consideration of these multi-agent environments or or even just

856
01:14:06,080 --> 01:14:11,840
you know the causal structure of the world that are really allow us to frame like philosophically

857
01:14:11,840 --> 01:14:17,440
coherent questions that are much more expansive than what we could say and to sort of supervise

858
01:14:17,440 --> 01:14:22,320
learning business as usual but the downside there is that we don't have tools that we can take

859
01:14:22,320 --> 01:14:27,440
to real data so it's like do we want the sort of impoverished tools that we could really take the

860
01:14:27,440 --> 01:14:32,800
data or do we want the really rich tools that we can just use to run thought experiments or

861
01:14:33,440 --> 01:14:40,240
you know even toy data experiments and and I think bridging this gap and if I you know if I

862
01:14:40,240 --> 01:14:43,280
had all the answers I certainly wouldn't be telling you you know I'd be

863
01:14:43,280 --> 01:14:52,000
be writing this family the archive of everyone else yeah no but I don't I don't know I don't

864
01:14:52,000 --> 01:14:58,400
want to pretend to to know what that looks like I don't think it's sufficiently like respectful

865
01:14:58,400 --> 01:15:03,680
to the difficulty of the task but that's that's that's what I try to look for right now

866
01:15:03,680 --> 01:15:10,720
it one of the things that I try to do coming back from noreps and this time of year is try to

867
01:15:10,720 --> 01:15:21,920
identify a few kind of key ideas or thoughts that were notable out of that event but also kind of

868
01:15:21,920 --> 01:15:27,680
broadly you know gaining traction over the year and a couple that come to mind for me this year

869
01:15:28,400 --> 01:15:35,760
were causality and generative models and they certainly came up quite a bit in our conversation

870
01:15:35,760 --> 01:15:41,600
today you know do you think similarly in terms of those those particular do you have others

871
01:15:41,600 --> 01:15:47,680
you know if if those are at the top of your list like why do you think that is the case now

872
01:15:47,680 --> 01:15:52,640
well I don't know about you know I think those those two are a bit different like I mean there's

873
01:15:52,640 --> 01:15:57,200
other contexts amongst people talking about generative models that that aren't you know the

874
01:15:57,200 --> 01:16:01,360
the sort of graphical models we talked about in the context of causality of those those are

875
01:16:01,360 --> 01:16:07,600
our generative models but I think the reason why they're pressing now is just because we're

876
01:16:07,600 --> 01:16:13,360
actually using this technology right so it's like we sort of have a technology that addresses

877
01:16:13,360 --> 01:16:18,560
a narrow set of concerns that it produces sort of like artifacts that are enough to get us excited

878
01:16:18,560 --> 01:16:24,480
enough to get us excited about deploying the technology but not enough to actually really

879
01:16:24,480 --> 01:16:30,320
address the needs of like the deployment environment if that makes sense we we basically like we

880
01:16:30,320 --> 01:16:36,080
were running this stuff in the lab forever we have these tools that that do well at this sort of like

881
01:16:36,080 --> 01:16:42,480
guess the answers on the test set and now we're deploying tools based on it we're not actually like

882
01:16:43,200 --> 01:16:47,920
ready for prime time in terms of being able to address address the needs of those real world

883
01:16:48,480 --> 01:16:52,400
deployment environments I think what's happening is people are starting to go to those stakeholders

884
01:16:52,400 --> 01:16:58,320
this starting to come up against the limits of like what's wrong like why why it's insufficient to

885
01:16:58,320 --> 01:17:05,680
use neural network like like why hold out test performance isn't enough to make decisions in a

886
01:17:05,680 --> 01:17:10,000
medical decision-making scenario like why it's not enough to make clinical decisions just to have

887
01:17:10,000 --> 01:17:13,920
good prediction accuracy and so I think the more people start using this technology like those

888
01:17:13,920 --> 01:17:17,680
issues have to come up because they were there in the first place we just weren't thinking

889
01:17:17,680 --> 01:17:21,840
seriously about it but we sort of like put ourselves in this bind like I think like that that's I

890
01:17:21,840 --> 01:17:27,120
think the drivers that we suddenly are I think it's some amounts of sobriety as we start coming up

891
01:17:27,120 --> 01:17:32,160
again sort of like failure after failure I think same for just like in the problem robustness

892
01:17:32,160 --> 01:17:35,920
out of distribution and causality are quite quite related to each other and I think a lot of the

893
01:17:35,920 --> 01:17:40,720
conditions under which we try to ensure robustness correspond to causal stories like we talked about

894
01:17:40,720 --> 01:17:45,120
before but I think you know when we start many people you look at what are people spending billions

895
01:17:45,120 --> 01:17:52,080
of dollars on in a machine learning space one is you know medical decision-making one is self-driving

896
01:17:52,080 --> 01:17:58,160
cars right so if you're building self-driving cars and you have no assurance that given training

897
01:17:58,160 --> 01:18:03,600
on the enormous statistic like through 2019 or something like that you have no assurance that

898
01:18:03,600 --> 01:18:08,960
they're gonna not crash in 2020 and that any you know small things like Mazda comes out with a new

899
01:18:08,960 --> 01:18:14,800
paint job that your cars are gonna start killing people that's you know obviously a problem so I

900
01:18:14,800 --> 01:18:18,160
think we've already sort of you know I think I think the reason why they're coming out is to be

901
01:18:18,160 --> 01:18:22,000
already like sort of sign the contracts like we've already like hitched our reputations towards

902
01:18:22,000 --> 01:18:27,200
delivering these products and and we're discovering a little bit too like that that these other

903
01:18:28,000 --> 01:18:33,120
sort of things that are that you know these other competencies are that that our machinery

904
01:18:33,120 --> 01:18:37,520
doesn't provide or actually necessary to do the things we told people we would deliver for them

905
01:18:37,520 --> 01:18:44,080
I think like the real driver is just coming up against nature also awesome well Zach all good

906
01:18:44,080 --> 01:18:50,800
things must come to an end so it goes for 2019 as well as this wonderful conversation reflecting

907
01:18:50,800 --> 01:18:59,360
on 2019 thanks so much for taking the time to chat with us your perspective on these papers and

908
01:18:59,360 --> 01:19:07,280
the field in general really appreciate it thanks for having me Sam awesome thank you all right

909
01:19:07,280 --> 01:19:12,800
everyone that's our show for today for more information on today's guest or for links to

910
01:19:12,800 --> 01:19:20,160
any of the materials mentioned check out twimmol ai.com slash rewind 19 be sure to leave us a

911
01:19:20,160 --> 01:19:24,720
five star rating and a glowing review after you hit that subscribe button on your favorite

912
01:19:24,720 --> 01:19:40,400
podcast catcher thanks so much for listening and catch you next time

