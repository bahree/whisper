WEBVTT

00:00.000 --> 00:16.280
Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting

00:16.280 --> 00:21.440
people, doing interesting things in machine learning and artificial intelligence.

00:21.440 --> 00:24.280
I'm your host Sam Charrington.

00:24.280 --> 00:27.920
Once again the recording you're about to hear is part of a series of interviews I've recorded

00:27.920 --> 00:32.960
live from the O'Reilly AI and Stratocomferences in New York City.

00:32.960 --> 00:38.200
My guess this time is Francisco Weber, who is the founder and general manager of Artificial

00:38.200 --> 00:41.920
Intelligence Startup, cortical.io.

00:41.920 --> 00:47.560
Francisco's presentation at O'Reilly AI was called AI is not a matter of strength but

00:47.560 --> 00:49.360
of intelligence.

00:49.360 --> 00:54.540
To set the stage for my conversation with Francisco, recall that in the last interview Pascal

00:54.540 --> 01:00.460
Feng noted how recent advances in natural language understanding have been based largely

01:00.460 --> 01:04.580
on ignoring language structure and focusing on statistics.

01:04.580 --> 01:09.620
While in this interview you'll hear Francisco argue that the next advance in NLU will come

01:09.620 --> 01:14.500
from shifting our attention from statistical models to models based on a more sophisticated

01:14.500 --> 01:15.980
model of the brain.

01:15.980 --> 01:22.620
A warning in advance, this conversation is very technical and moreover rather abstract.

01:22.620 --> 01:26.060
Don't be afraid to listen to it a couple of times to allow the ideas and opportunity

01:26.060 --> 01:27.820
to sink in.

01:27.820 --> 01:33.580
You'll find this week's show notes at twimlai.com slash talk slash 10.

01:33.580 --> 01:38.660
You might be particularly interested in a link to Francisco's presentation slides which

01:38.660 --> 01:41.860
are helpful to review alongside the podcast.

01:41.860 --> 01:44.340
And now on to the show.

01:44.340 --> 01:51.380
Hello everyone here at the O'Reilly AI conference and I am with Francisco Weber, who gave a great

01:51.380 --> 01:56.660
talk earlier on AI is not a matter of strength but of intelligence.

01:56.660 --> 01:58.660
So welcome Francisco.

01:58.660 --> 02:03.740
Hello, great to be here and to talk about my talk actually.

02:03.740 --> 02:08.300
Nice, why don't we start out by learning a little bit more about you and hearing a bit

02:08.300 --> 02:09.620
about your background?

02:09.620 --> 02:18.500
Yeah, so I'm coming from the natural sciences, I'm trained in medicine in Vienna but I have

02:18.500 --> 02:27.660
since ever sort of a built-in affinity to technology and ended up sort of going into

02:27.660 --> 02:34.700
the natural language processing information retrieval domain where I'm in for like 20

02:34.700 --> 02:42.800
years now but I've been the so cortical I always actually my third company, I previously

02:42.800 --> 02:48.940
worked in the field of patent information which is also a sort of complex natural language

02:48.940 --> 02:56.980
issue and that was basically where I learned of the limitations of the current systems

02:56.980 --> 03:03.300
and that motivated me to actually try and find something substantially different.

03:03.300 --> 03:06.740
Okay, so tell us a little bit about cortical.

03:06.740 --> 03:14.580
Yeah, so cortical is all about two things in fact, so one thing is a theoretical framework

03:14.580 --> 03:23.820
that we have discovered I would say and that we explore that is about how the human brain

03:23.820 --> 03:32.980
to be more specific actually the new cortex supposedly handles language information and

03:32.980 --> 03:40.860
the other is that we basically use this theoretical framework to also create a real technology

03:40.860 --> 03:51.780
that we basically offer to the markets currently this is mainly for enterprise customers.

03:51.780 --> 03:59.700
They have a lot of problems out there and so say the effort to revenue ratio makes us

03:59.700 --> 04:06.780
work there for a while but we do also have a public API where basically everybody can play

04:06.780 --> 04:11.020
around with our technology for free.

04:11.020 --> 04:21.220
Yeah, so that's what the company sort of does and we try to find the really alternative

04:21.220 --> 04:28.700
way to deep learning and to the more traditional ways of statistical modeling and machine learning

04:28.700 --> 04:39.420
for the moment our approach doesn't actually use any statistics so which might not be

04:39.420 --> 04:45.860
the case in the future so there are some motivations to maybe team this up with deep neural

04:45.860 --> 04:51.500
networks or so so that's but in fact it's not our speciality so I leave this to others

04:51.500 --> 04:58.340
to try out what we basically do is that we have solved I would say or we have found a

04:58.340 --> 05:05.700
solution to the famous representational problem that exists in natural language understanding

05:05.700 --> 05:15.380
since decades basically a very fundamental issue is that basically says if you find out

05:15.380 --> 05:22.740
how to represent language which means text at some point in a way that you can actually

05:22.740 --> 05:32.780
compute with it then many of the big problems like ambiguity like vocabulary mismatch all

05:32.780 --> 05:42.460
the traditional problems we have in NLU basically are solved in one approach and that's what

05:42.460 --> 05:51.540
I actually presented today is that by this little shift in how to generate the features

05:51.540 --> 06:00.940
in falling place afterwards in a very convenient and most importantly efficient manner so tell

06:00.940 --> 06:11.740
us about this shift yeah so our approach basically is founded on the work of Jeff Hawkins

06:11.740 --> 06:19.820
who is a researcher in in the area of cortical processing so he works on finding out how

06:19.820 --> 06:27.660
the human new cortex actually processes data as a data in general because one of his findings

06:27.660 --> 06:36.780
was that regardless what kind of data so might it be hearing or seeing or touching all

06:36.780 --> 06:44.100
of that data when it comes to the new cortex looks the same it has the same format which

06:44.100 --> 06:49.220
is a what is called a sparse distributed representation so it's like a large vector

06:49.220 --> 06:56.220
of binary features where you have like two of two to five percent of those features are

06:56.220 --> 07:02.940
actually set to one and all the rest is zero and everything is encoded into such a such

07:02.940 --> 07:12.220
an SDR and that was basically our first goal is to find a systematic unsupervised because

07:12.220 --> 07:20.060
otherwise it's not doable in practice a systematic unsupervised way of converting text into

07:20.060 --> 07:27.540
such an SDR okay now I've heard a couple of times even at this event there were a couple

07:27.540 --> 07:33.220
of comments that were made that one of them was even I think in the key notes this morning

07:33.220 --> 07:37.820
there was a comment about how you know what we've got with neural nets are don't have

07:37.820 --> 07:44.900
the complexity and the nuance available to express what's actually happening in the brain

07:44.900 --> 07:51.380
and and in another talk the kind of follow on statement was so therefore we shouldn't

07:51.380 --> 07:55.180
try we should just use these as tools and now it sounds like you you have a totally different

07:55.180 --> 08:03.300
belief system around this well I mean fundamentally what we use as neurons nowadays has in fact

08:03.300 --> 08:11.780
very little to do with real neurons yeah so it was an abstraction that was made like 30 40

08:11.780 --> 08:19.780
years ago on a compared to today on a very rudimentary understanding of what neurons actually

08:19.780 --> 08:27.940
do nowadays we know more we know that for example the actual learning happens through the building

08:27.940 --> 08:37.940
and unbuilding of synopsis between them and if you actually model a neuron not not chemically

08:37.940 --> 08:43.540
so it's not about sort of creating all the molecules that are there because that's something

08:43.540 --> 08:50.260
that nature uses yeah so nature you know in evolution you always have the components from

08:50.260 --> 08:55.780
the previous evolution evolutionary state and you have to play with this kind of Lego bricks

08:55.780 --> 09:03.940
and do something which sometimes looks a bit inefficient but what is key on the other side is what

09:03.940 --> 09:13.220
is the mechanism that those real neurons create and that is what Jeff Hawkins actually has figured

09:13.220 --> 09:20.340
out and is about to even figure out in more detail okay and so certain aspects like the the

09:20.340 --> 09:28.740
sparse binary representation are actually key for this to work properly and by working on text so

09:28.740 --> 09:34.420
our approach is basically okay if Jeff is right with his theory everything he says about the

09:34.420 --> 09:40.500
general way how the cortex processes has also to be truthful language as the language is generated

09:40.500 --> 09:48.340
by the cortex too and so we basically took his theoretical framework as a set of constraints and

09:48.340 --> 09:54.180
we tried to say okay if that is the limitation how can I put everything I know about language

09:54.180 --> 10:03.780
within this limitation and it took a while actually 25 years or so in general I mean not I know

10:03.780 --> 10:09.540
Jeff's work since since a little bit over 10 years but everything that was sort of needed to

10:09.540 --> 10:18.580
me at least to sort of understand and to operate at this abstraction level took a while but then I

10:18.580 --> 10:24.260
had at some point while I was listening to his talks reading his book on intelligence and so

10:24.260 --> 10:34.100
and it was literally sort of taking a shower and in a second I had this visual idea so to say

10:34.100 --> 10:44.020
how this could happen okay because it boils down to a sort of visual aspect in the sense that

10:44.740 --> 10:52.820
as a necessity we have to find a representation where two words that mean similar things have to

10:52.820 --> 10:59.540
actually look similar and when I say look there as the hours have to be similar and literally

10:59.540 --> 11:06.580
similar so in fact and that's also what the brain is doing to put one word representation on top

11:06.580 --> 11:12.340
of the other word representation and by measuring the overlap how many of the bits actually stay

11:12.340 --> 11:20.100
at the same position you get two things one is how related are those words and the second is by

11:20.100 --> 11:26.180
looking where the overlap happens within this representation because this is a two-dimensional

11:26.180 --> 11:37.300
it's like a bitmap with 128 times 128 pixels and like 2% of those 16,000 bits are set to one

11:37.300 --> 11:44.980
therefore are like pixels dark pixels if you want and so it's actually a visual thing and

11:45.940 --> 11:52.660
you can try this out on our website when you take two words that are sort of have a common context

11:52.660 --> 12:00.020
or so you can actually literally see that they look similar yeah and interestingly I mean it's

12:00.020 --> 12:07.780
hard to or maybe even impossible to absolutely decode what it means but if you as I have done

12:07.780 --> 12:13.140
stare a lot into these representations you end up seeing the differences like in the blink of an

12:13.140 --> 12:20.260
eye you might not know the details but to identify that two words are similar in this representation

12:20.260 --> 12:31.300
takes a millisecond yeah and that is already a hint so to say that shows what the major gain is

12:31.300 --> 12:38.900
of this approach which is efficiency and that's what our brain is famous for so I know that

12:39.860 --> 12:45.620
on the deep learning in the deep learning community things like precision and so are the key

12:45.620 --> 12:53.220
metrics and they are of course important it's like having glasses that are blurry nevertheless

12:53.940 --> 13:01.620
at the very end the choice of algorithm is not so much on the precision but it relates to

13:03.540 --> 13:08.260
down to earth energy efficiency yeah I mean the brain works with something like 10 watts or something

13:08.260 --> 13:15.060
so I don't even want to know how much power the GPU servers

13:16.180 --> 13:22.260
each up and that is already a very good hint of how well a certain approaches yeah if

13:22.820 --> 13:30.820
and that's why I chose the title also of brute force because statistics especially if you do

13:30.820 --> 13:37.940
statistics of large combinatorial spaces like like language I mean you basically can create

13:37.940 --> 13:47.380
an indefinite number of combinations of words to make meaningful sentences so to do a statistics

13:47.380 --> 13:55.860
on such an open system it's a real hard work because you have to provide endless examples

13:56.660 --> 14:06.100
to have like a microbit of semantic payload in your representation yeah and it works up to a certain

14:06.100 --> 14:14.100
point no question I mean the statistical systems work but what you see is that in order to make

14:14.100 --> 14:21.860
the model a little bit smaller or to gain a tens of a percent in precision you have to put a lot

14:21.860 --> 14:29.060
of effort in yeah so from to get from 60 to 61 percent precision you might even double the effort

14:29.060 --> 14:36.580
of like going from one to 60 is the same as from 60 to 61 yeah if I could drag you drag you back

14:36.580 --> 14:43.780
a little bit just it sounds like understanding the Jeff Hawkins stuff is important to understanding

14:43.780 --> 14:52.820
what you guys are doing to some degree yes so they so he's defined the sparse data representation

14:52.820 --> 15:02.180
this SDR and is there also a different concept of a neuron that underlies that absolutely so he's

15:02.180 --> 15:09.220
modeling a real neuron but on the functional level so he's also modeling a neuron if you want

15:09.220 --> 15:15.140
but he's modeling everything that is relevant within the neuron for processing data is part of

15:15.140 --> 15:21.940
his model and everything that might be housekeeping building up proteins and stuff like that is not

15:21.940 --> 15:27.860
part of the actual data processing layer and therefore not represented there so he's basically

15:27.860 --> 15:37.700
tried to expand the simplistic concept of a neural net neuron to become a real neuron and sometimes

15:37.700 --> 15:46.180
if you face the problem the way it is the solution is much easier to understand yeah because it's

15:46.180 --> 15:53.940
basically a model-based approach versus a model-free approach so sort of bringing it into one

15:53.940 --> 16:00.820
sentence yeah and so on this base of this more robust model of a neuron there's this notion of

16:00.820 --> 16:07.220
the SDR which is capturing you know and I think of a neuron I think of the you know there's

16:07.220 --> 16:12.260
state plus action right and so this is capturing state even more it's state action and time

16:12.260 --> 16:20.500
that is key to what Jeff is doing okay because his networks have a time built in okay so it's not

16:20.500 --> 16:31.220
only of deciphering a pattern of input bits but it's rather memorizing a sequence of patterns

16:32.020 --> 16:39.460
because in reality things are interconnected so to say they have a semantics built into the system

16:39.460 --> 16:49.860
everything and therefore it is highly improbable if not impossible that by having an initial state A

16:51.060 --> 16:58.100
you can predict which are the let's say physical possible next steps and that's what the processing

16:58.100 --> 17:05.140
relays are yeah the fact that not like in statistics after state A any state could happen

17:05.140 --> 17:11.460
because I need to do the statistics for it but the reality is that after a step A there is a

17:11.460 --> 17:20.500
certain set of steps which have all to be possible in reality and what we learn as walking brains if

17:20.500 --> 17:28.260
you want is what are those potentials what are the potential outcomes and how many hints from the

17:28.260 --> 17:35.060
initial state could point me to the right next state and that's in the end what the brain is doing

17:35.060 --> 17:42.340
yeah the brain is nothing more than a sequence learning engine that does prediction based on what

17:42.340 --> 17:50.660
it has seen so far and if you think through that on a let's say philosophical level you will find

17:50.660 --> 17:59.380
out that you basically can solve or explain everything we do that basically follows this basic

17:59.380 --> 18:05.940
computation yeah so there are two interesting aspects so this one is there is no processor

18:05.940 --> 18:11.460
so the brain does this by being a memory system which is interesting I mean in computers it's

18:11.460 --> 18:17.380
exactly the other way around yeah the processing happens in the processor and the RAM is just a

18:17.380 --> 18:29.300
dormant store yeah and the brain obviously does this differently and the other aspect is that the prediction

18:29.300 --> 18:38.740
is in fact the condensed intelligence because the more I'm right in predicting the more I'm intelligent

18:38.740 --> 18:45.220
and by the way I mean there you know there have been very behavioral ways of looking at intelligence

18:45.220 --> 18:53.700
that's the reason why the dog looks intelligent to the dog owner because the dog owner knows the

18:53.700 --> 19:01.220
dog and knows what predictions the dog is making about things and is right in doing so and therefore

19:01.220 --> 19:06.820
the dog indirectly so to say looks more intelligent to the owner than to everybody else yeah

19:06.820 --> 19:18.180
yeah so does the SDR capture all of that are just the state so the SDR is all about getting a

19:19.620 --> 19:26.180
an explicit representation of the state so that's the other difference in the world of brains

19:26.180 --> 19:32.260
and the SDRs you only work with what is called semantically grounded information so every bit

19:32.260 --> 19:41.300
in representation of the SDR actually corresponds to something real and concrete so for the visual

19:41.300 --> 19:47.220
system it's pretty easy because in the end every bit of the image that is produced on the retina

19:48.340 --> 19:53.700
if you have two dots that are close to each other and have the same color or nearly the same color

19:54.660 --> 19:59.940
you are probably right in guessing that they are part of the same item in the physical world yeah

19:59.940 --> 20:05.380
if you have now a representation that gives you the same phenomenon namely that two

20:06.420 --> 20:12.740
bits that are set to one stay close to each other it's easy to guess that they are related and they

20:12.740 --> 20:18.980
are part of the same maybe subunit of the system the only thing you have to be sure is that

20:19.780 --> 20:26.980
the data that is provided is actually inherently semantic so it has to be part of a system in

20:26.980 --> 20:34.420
on a very abstract level yeah so the world is a system therefore any data that I can hear or see

20:34.420 --> 20:42.340
or so about the world is semantic because there are rules of physics rules of biology and so on

20:43.060 --> 20:50.260
and the same thing is truthful language language is data that is inherently tied together by a

20:50.260 --> 20:57.700
framework of grammar of syntax and all these aspects we know since a long time but we have

20:58.660 --> 21:06.660
we had the problem on how to actually store these mechanisms and the realities don't store the

21:06.660 --> 21:12.980
mechanism but just store the examples just store the detailed information the explicit information

21:12.980 --> 21:22.580
and are those words or those yeah so in our approach we declare the semantic atoms in language

21:22.580 --> 21:30.180
to be words I mean there are like smaller units like phonemes for example but they have no

21:30.180 --> 21:34.180
meaning but themselves so the first time you actually have a meaning is when you have a word

21:35.380 --> 21:41.700
and all the subsequent meaning of a sentence of a paragraph a document an utterance even

21:41.700 --> 21:49.940
comes out of the sequence of those words yeah and so what we do is basically we convert every single

21:49.940 --> 21:56.980
word into such a sparse representation we call this because it's so hard to say we call this

21:56.980 --> 22:04.260
a semantic fingerprint okay and the interesting thing is that through the way how we convert

22:04.260 --> 22:11.060
the semantic fingerprint you take advantage of some of the properties that are inherent of sparse

22:11.060 --> 22:17.940
binary vectors for example you can make a union of as many sparse vectors as you want and you

22:17.940 --> 22:24.740
don't lose information yeah you can always say from an unseen vector if it was part of the union

22:24.740 --> 22:31.460
or not if you try to do the same thing with a dense representation let's say the the ASCII encoding

22:31.460 --> 22:38.260
you have eight bits and every possible combination corresponds to another character if you make a

22:38.260 --> 22:46.100
union of a couple of them no way to say what was the initial part yeah and as I said the the

22:46.100 --> 22:54.260
generation of this pattern is done in a way that every single pixel of our fingerprints correspond

22:54.260 --> 23:01.620
to an explicit learned context and you can infect not a word a context it's a context which is

23:01.620 --> 23:08.580
basically technically it's a bag of words if you want it's a bag of words of utterances in which

23:08.580 --> 23:17.700
the word occurred and so is a what's the scope of an SDR in this model is it at the level of a

23:17.700 --> 23:21.860
corpus at the level of a language at the level of an utterance well not an utterance because

23:21.860 --> 23:28.500
an utterance in fact all of that so the the what we do is in principle we generate the atoms which

23:28.500 --> 23:38.100
is a fingerprint for a word but if I want to create a fingerprint for a sentence I just convert every

23:38.100 --> 23:44.340
single word into its fingerprint I make a stack and aggregate them together I make an ore of all of

23:44.340 --> 23:51.220
them and then I have depending on the location on the fingerprint you can do that's because of

23:51.220 --> 23:56.980
this union property exactly exactly yeah that's the reason why we have to stay on the sparse side

23:56.980 --> 24:02.980
yeah and if you make for example a union of let's say ten words that are in a sentence you of

24:02.980 --> 24:10.500
course fill up the representation therefore after making the union we we do what we call resparsify

24:11.380 --> 24:17.940
we introduce a threshold to cut away everything that fills the fingerprint on more than the two

24:17.940 --> 24:24.580
percent and so we end up with a fingerprint for a sentence that has basically the same topology that

24:24.580 --> 24:31.300
is directly comparable to a fingerprint of a word and we can do this with a sentence with a paragraph

24:31.300 --> 24:43.460
with a book yeah of course the SDR for book or for whatever let's say a book is it end dimensionality

24:43.460 --> 24:49.620
where n is the number of unique words in the book no so there is the topology and there is

24:49.620 --> 24:56.900
that's the name why why we call it semantic folding there is this semantic space folded into the

24:56.900 --> 25:05.140
representation so the way how we do this how we generate the word SDRs is that we take a collection

25:05.140 --> 25:11.300
of documents which are the reference documents that's for a human that would be everything you ever

25:11.300 --> 25:20.340
read and heard yeah all language elements that you got exposed to and we digest them and we do

25:20.340 --> 25:25.060
this of course using machine learning because we are not like humans we have not the time to wait

25:25.780 --> 25:32.180
20 years or so so that's in fact where we apply machine learning and what we do is that we

25:32.980 --> 25:40.580
first of all cut the training material in little pieces and then we define the size of our

25:40.580 --> 25:46.820
fingerprint which is a metric space so there is no dimensionality if you want it's a two-dimensional

25:46.820 --> 25:54.340
metric space and we position all of our training snippets on this space in a very simple rule

25:55.380 --> 26:02.180
two snippets that are similar stay close together and two snippets that are different stay far apart

26:02.180 --> 26:10.100
from each other and then it's you know one of these classical iterative algorithms similar to

26:11.140 --> 26:18.020
heavier learning a bit like this local inhibition mechanism and what you end up with is that you have

26:18.980 --> 26:26.420
all snippets about animals in one region all snippets about family in another region and so on

26:26.420 --> 26:33.780
and you get a semantic map and this semantic map is basically used to encode every word because

26:33.780 --> 26:40.260
I can take all the words that are in my training material and for each of the words I can say light

26:40.260 --> 26:45.300
up the positions of the snippets where this word occurs in and then you get this distributed

26:45.300 --> 26:54.180
representation and because you have to fold it in semantics so to say two similar words like cat

26:54.180 --> 27:01.380
and dog look similar if you look them on the on the semantic map representation of the fingerprint

27:02.740 --> 27:09.940
so you mentioned earlier kind of a you give an example of 128 by 128 matrix at that size matrix

27:09.940 --> 27:15.460
like what are you able to represent like is that a book all of the books I've ever read or

27:15.460 --> 27:24.980
so what it actually represents is a semantic space because it's the fundamental of the representation

27:24.980 --> 27:34.260
and if you just do the math of selecting 300 bits which is about close to 5% of 16,000 bits

27:35.540 --> 27:41.540
the number of combinations you can do is like the number of stars in the Milky Way so it's a huge

27:41.540 --> 27:49.540
combinatorial space and as you know we have not the same assumption as in statistics that in principle

27:50.180 --> 27:55.300
every word could be combined to every other word yeah so that's one of the central

27:57.140 --> 28:03.860
simplification methods is to say in the language statistics that every word is independent which

28:03.860 --> 28:10.020
is absolutely not true yeah if you have on the semantic level a certain set of adjectives that

28:10.020 --> 28:18.340
you associate to a certain noun yeah so there is semantic sort of glue between everything

28:18.340 --> 28:25.780
and in reality that shrinks the combinatorial space and that's precisely what we need to learn

28:25.780 --> 28:33.860
the semantics of it yeah okay okay so this there are elements of this that remind me of war

28:33.860 --> 28:44.500
Tevac I guess it's a natural development that we have started NLP and information retrieval

28:44.500 --> 28:51.300
with so-called document vectors everything was sort of derived from a document and we found out

28:52.340 --> 28:58.100
over the years that word vectors the representation of individual words seems to be more appropriate

28:58.100 --> 29:06.420
nevertheless there is a fundamental but crucial difference so word-to-veg like other word embedding

29:07.220 --> 29:14.820
mechanisms use they try to do dimensionality reduction and they end up with a dance vector

29:15.380 --> 29:21.940
and to put even more on it a dance vector of double of float numbers so sort of computationally

29:21.940 --> 29:31.300
expensive representations we don't do a dimensionality reduction we might even to an increase the

29:31.300 --> 29:37.780
dimensionality at some point if you want but we make it a sparse representation so we have

29:38.500 --> 29:45.620
sparse binary vectors versus dance floating point value the double vectors yeah which already

29:45.620 --> 29:55.060
sort of gives you a hint on where the efficiency will be right right so have we talked

29:55.060 --> 29:58.260
through have we got to what you talked about in your talker is this all been back

29:58.260 --> 30:05.700
I mean I was talking because my head already I understand I mean especially because you are

30:05.700 --> 30:10.660
listening to this without any visual support and this is a very visual thing yeah so

30:10.660 --> 30:15.860
typically when I when I show this and people see the the fingerprints on the screen and how they

30:15.860 --> 30:21.220
interact and how they overlap you can see in their faces ah I understand this yeah you don't

30:21.220 --> 30:27.780
need to know anything about machine learning or so it's so intuitive but if I imagine to sort of

30:27.780 --> 30:37.460
follow a description that is purely verbal then you're doing a great job yeah so the rest basically

30:37.460 --> 30:46.420
was that I gave a number of practical examples where we apply this okay and I can cite a few

30:47.220 --> 30:55.300
for example we do pro let's say we we have certain prototype ways of solving typical problems

30:55.860 --> 31:02.980
and what is the case is that we solve all of them with one unique operator which is similarity

31:02.980 --> 31:09.940
yeah so we only the only sort of verb we have in our universe is is similar or is not similar

31:10.660 --> 31:16.420
and so one thing you can do of course is search yeah so you can and since you're operating on

31:17.460 --> 31:23.540
essentially the sparse vector representations is when you hear similar like is it fair to think

31:23.540 --> 31:30.100
geometrically similar geographic yeah literally so so we actually measure this by calculating the

31:30.100 --> 31:36.420
overlap between two fingerprints which is the most generic way I mean we we do offer a number of

31:36.420 --> 31:41.620
distance metrics as I said this is a metric space so we have different ways of calculating a

31:41.620 --> 31:49.380
distance metrics like a Euclidean distance and others but I have to say that in fact the pure

31:49.380 --> 31:56.020
overlap count is fully sufficient to get the result out of it and it's very computationally efficient

31:56.020 --> 32:03.460
yeah so one of the prototypes as I said is search imagine you have a collection of documents you

32:03.460 --> 32:10.980
convert each of the documents into a fingerprint you have a user who types in a language-based query

32:10.980 --> 32:17.940
now I'm looking for information about red spot cars you create a fingerprint of that query

32:18.500 --> 32:23.620
and you just match how much overlap you have between all the documents and the query

32:23.620 --> 32:30.340
and you rank all your documents according to the size of the overlap yeah very generic it's

32:31.540 --> 32:38.180
it's a real search mechanism so what you get is really all the balanced aspects that you have

32:38.180 --> 32:44.980
in a document so it's not just does a document contain the word sports car but it's about

32:44.980 --> 32:50.340
the aspects that you might have developed in a document that make it match or less

32:50.340 --> 32:55.860
and then theory the document need not even say sports car and exactly are doing theory

32:55.860 --> 33:00.900
the similarity to the this conceptual yeah yeah yeah so it could be the race car it could be a

33:00.900 --> 33:05.860
text about the race car and my query could be about sport cars and it would still sort of give a

33:05.860 --> 33:11.940
good match yeah and how does it apply to non-English languages I didn't hear anything English

33:11.940 --> 33:20.020
completely independent of languages so as I used to say it gives me enough dictionaries and

33:20.020 --> 33:28.340
encyclopedias in klingon and I put you up a klingon system no problem the point is that we have

33:28.340 --> 33:35.620
even brought this to a step further because we were able to not only train in different languages

33:35.620 --> 33:43.620
the semantic spaces but to also topologically align them and as a result and I gave the example

33:43.620 --> 33:49.780
in my talk we take the word philosophy in English has a certain representation and the

33:49.780 --> 33:56.580
word philosophy in French has the same representation so the patterns are the same and what this means

33:56.580 --> 34:04.180
is you could have a system that contains English documents and you can post French queries and

34:04.180 --> 34:10.980
it will still work without any translation or or anything in between only for those words that

34:10.980 --> 34:18.420
have a fair degree of overlap or well that the word the words with the same meaning regardless

34:18.420 --> 34:25.620
of the language have the same fingerprint right yeah so a second prototype where I could give you

34:25.620 --> 34:32.500
an example is classification so our classifiers are actually just fingerprints I don't need to train

34:32.500 --> 34:41.460
my classifier if I say I want to get let's say all the tweets about mobile phones I can take the

34:41.460 --> 34:47.140
word mobile phone create a fingerprint and then compare the fingerprint of every incoming tweet

34:47.140 --> 34:54.740
to my fingerprint of the word mobile phone and even if it talks about iPhone it will have sufficient

34:54.740 --> 35:01.860
overlap for me to detect it and even if the tweet is in Chinese it will be converted into something

35:01.860 --> 35:07.300
that I can filter with my English mobile phone fingerprint even simultaneously in Chinese

35:08.260 --> 35:14.820
because you're fingerprinting that based on its language representation and there's the

35:14.820 --> 35:23.940
and the similarity exactly transferable from one to the next the Chinese description of the new

35:23.940 --> 35:29.300
iPhone generates the same fingerprint as the English description of the new iPhone

35:30.100 --> 35:37.140
why is that because the should we have surprised at that definitely you should be surprised

35:37.140 --> 35:42.180
yes and no I mean people who know two languages are able to do this in the same way yeah so there

35:42.180 --> 35:50.260
has to be a let's say mathematical way of doing this and the point is that we aligned the two

35:50.260 --> 35:56.900
semantic spaces so we have one topology that we generate in one language and we can then with a

35:56.900 --> 36:03.540
with a pure dictionary lookup mechanism which is the dumbest way of doing a translation

36:04.020 --> 36:09.860
we can convert all the distributed snippets in the vocabulary of the other language

36:09.860 --> 36:14.740
and use the same distribution that we have trained with for example the English method

36:15.860 --> 36:20.820
and therefore you have now the convenience to listen let's say to the Twitter

36:20.820 --> 36:27.460
firehose and regardless of what language message comes along you can filter it with an English

36:27.460 --> 36:35.380
example and I've done that just to give you a feeling on efficiency I've done that in real time

36:35.380 --> 36:43.780
on the firehose with my notebook yeah so it was sort of running locally I'm trying to run through

36:43.780 --> 36:50.260
the I'm trying to run through the physical analogy that were the biological analogy of this like

36:50.740 --> 36:56.500
and you know if the if the notion here is that you've kind of extracted this model that you know

36:56.500 --> 37:05.140
more closely represents what's happening in the the brain then and you can you have this kind

37:05.140 --> 37:12.660
of transferability across languages um is there some you know again we're kind of way beyond

37:12.660 --> 37:16.900
you know the pale of what's actually going to happen but like is there some like you take the

37:16.900 --> 37:21.460
you know some part of the brain from someone who learned Chinese and you transplanted into

37:22.660 --> 37:28.340
a person you know and then you know who has some other part of the brain that is kind of

37:28.340 --> 37:34.420
symbolically linked to English and they could then you know translate on the fly like

37:34.420 --> 37:41.060
yeah in theory in theory that would be possible the truth is that there has been research for

37:41.060 --> 37:48.340
example comparing the brain patterns of people who have who have been grown up with two languages

37:49.140 --> 37:55.540
they have they have a sort of speech area in the brain that is actually

37:57.060 --> 38:04.180
intricately mixed yeah so the two languages are represented in a mixed fashion whereas people

38:04.180 --> 38:09.700
who have been grown up in one language and who have then learned the other one they have added the

38:09.700 --> 38:17.060
second area so to say and that's the reason why a native speaker of two languages can actually

38:17.060 --> 38:24.420
easily do translation on the fly and can listen or read text in two languages without even noticing

38:24.420 --> 38:30.580
that there might be two languages right and someone who has just learned another language has

38:30.580 --> 38:37.860
always in his head to map from the one region into the other region now interestingly there is

38:37.860 --> 38:47.300
and I showed this also in my presentation there is sort of new research in brain science that

38:47.860 --> 38:56.100
supports our representations strongly so they were able to do an fMRI study to be precise so

38:56.100 --> 39:02.420
there has been an earlier version of this experiment where people were exposed to words and then they

39:02.420 --> 39:09.220
made like snapshots from their fMRI activity and what they found out that was in counting a

39:09.220 --> 39:18.900
melon if I remember well what they found out is that you can actually calculate starting with the

39:18.900 --> 39:23.940
picture an fMRI picture you can say what word this person was hearing when this picture was

39:23.940 --> 39:31.860
taken yeah so this is as you say wow but it comes better and they have sort of trained a

39:32.740 --> 39:38.260
machine learning algorithm to make this transfer to correlate the picture with the word that the

39:38.260 --> 39:45.860
persons have been hearing and the absolutely unbelievable thing is that let's say you have

39:45.860 --> 39:51.540
been in the fMRI first the model has been trained on your images to map to certain terms

39:51.540 --> 39:59.140
if now you present this very same model the images taken from my brain it will still recognize

39:59.140 --> 40:05.380
the terms properly and that is independent of whether we speak the same language no I'm talking

40:05.380 --> 40:12.980
about this has been done in English yeah I'm I'm pretty sure that even if I would do this with

40:12.980 --> 40:18.980
with the portuguese meaning of your English terms it still might work out but maybe not Chinese but

40:18.980 --> 40:28.660
but why not the the the the fact is that obviously if two individuals have been grown up

40:30.100 --> 40:34.900
sufficiently similar from a cultural point of view yeah so we both went to school for

40:34.900 --> 40:39.380
more or less the same time we more or less read the same stuff we've heard about the world

40:39.380 --> 40:48.740
in the same way the representation ends up being similar across individuals and in the end it's

40:48.740 --> 40:54.580
it makes a lot of sense I mean just imagine if we would really be wired completely different

40:54.580 --> 41:01.780
from one to each other it's it would be very hard to have a simple conversation yeah and in fact

41:01.780 --> 41:07.300
if you if you do the the investigation for example I'm pretty sure again this is just guessing

41:07.300 --> 41:13.860
but the fmri pictures from I don't know some distant tribe living somewhere in the Amazon and

41:13.860 --> 41:19.940
jungle they're the overlap between the two representations is probably less because they have

41:19.940 --> 41:28.820
just not been exposed to a very similar kind of environment yeah and and there is a newer

41:28.820 --> 41:35.220
publication which is I think it's from this year I think it's it's it's from a lab in the MIT

41:35.220 --> 41:44.820
and there they were actually able to create a map of about thousand words on the basically

41:44.820 --> 41:51.940
nearly the entire cortex and and what it shows is that every it's not like every word has a

41:51.940 --> 41:58.900
specific position but every word has a pattern of all sorts of positions all over the cortex that

41:58.900 --> 42:07.060
lights up which is in fact exactly what we are doing with our fingerprints yeah so I claim that

42:07.060 --> 42:14.580
we are the first NLP algorithm that gets support by fmri so this is fascinating stuff

42:15.540 --> 42:22.180
how do you help people make it practical like what if I'm you know if what what are the problems

42:22.180 --> 42:27.780
that hey if I have this problem I should be looking at this as a possible approach so so as I say

42:27.780 --> 42:34.180
earlier we are very strong with this approach in doing similarity calculation and therefore

42:34.180 --> 42:41.780
classification and as you might know in business natural language processing nearly all problems

42:41.780 --> 42:48.260
can be reduced to one or several classification problems okay so we do all sorts of things yeah

42:48.260 --> 42:59.380
I mean companies who say we want to classify our inbound emails in product requests complaints

42:59.380 --> 43:05.940
and I don't know looking for a person an individual in the company and believe it or not

43:07.620 --> 43:14.100
I haven't seen any working machine learning solution for that problem out there I mean I've

43:14.100 --> 43:21.540
been visiting like 150 companies over the last two years of course trying to sell our stuff

43:21.540 --> 43:29.220
right but I haven't seen a working solution for simple I mean this is really one of the most

43:29.220 --> 43:35.780
basic issues you could have and nearly nobody is actually using technology for that

43:35.780 --> 43:44.820
because the statistical approach has a lot of noise that comes in has false positives which is

43:44.820 --> 43:52.020
by the way the the biggest problem in business and we solve we solve this in a couple of weeks

43:52.660 --> 43:59.380
so we we make use of the efficiency of the approach in solving this kind of problems within very

43:59.380 --> 44:05.620
short time for people and so that's that's a specific use case are there is there like a higher level

44:05.620 --> 44:14.500
characterization like you know in terms of problem yeah so we have customers in the domain in a lot of

44:14.500 --> 44:21.300
customers for example are in the banking domain there we solve problems like compliance monitoring

44:21.300 --> 44:29.620
nor your customer activities or automation of business processes that depend on some text input

44:29.620 --> 44:39.380
at some point we have consumer good companies who want to know how to segment their customers for

44:39.380 --> 44:47.220
example we have a manufacturing industry where for example in technical products the documentation

44:47.220 --> 44:54.340
the manual of the product is so complicated good example is car industry for example a modern car

44:54.340 --> 45:00.500
is so complicated that if something breaks you need to visit the manual or to find out what is

45:00.500 --> 45:07.540
this funny light meaning there is this dangerous or can I just continue and people can't find

45:07.540 --> 45:15.060
anything because they have the problem that the person in the car doesn't speak the technological

45:15.060 --> 45:23.060
language so an example that I've that I've learned is the query where do I find the donut

45:24.020 --> 45:29.700
in in the US yeah so I didn't know that before but obviously the donut is a spare wheel that

45:29.700 --> 45:35.620
is sometimes pretty good hidden so if you look for donut in the in the manual you probably don't

45:35.620 --> 45:42.340
find it yeah and there is a lot of these issues yeah I mean to be even a more extreme case a person

45:42.340 --> 45:50.020
speaking only Spanish driving a US car and being unable to actually find the the right answer

45:50.020 --> 45:57.380
could use our system to sort of pose a Spanish query and be pointed to an English page for

45:57.380 --> 46:08.180
example so as I said I mean in principle we have solutions all across the domain we can do

46:08.180 --> 46:16.500
things like for example you have a LinkedIn profile you describe yourself in your LinkedIn profile

46:16.500 --> 46:22.580
I can make a fingerprint of your profile and if I do a fingerprint of my profile we probably have

46:22.580 --> 46:28.580
a lot of overlap as we are interested in the same kind of topics traditionally to make matching

46:28.580 --> 46:36.580
of people in HR for example you needed to actually if one person says I'm expert in G2E

46:36.580 --> 46:41.540
and the other person or the other job description contains Java Enterprise there was no way of

46:41.540 --> 46:49.300
matching it yeah in our case we matched this easily right wow so the very fascinating stuff how can

46:49.300 --> 46:58.020
folks learn more find out more about it contact you so basically on our website cortical.io

46:58.020 --> 47:09.380
what you find you can go there you find a white paper where you get basically a more in-depth

47:09.380 --> 47:17.140
introduction to the whole approach you find access to a public REST API that you can play around

47:17.140 --> 47:24.900
it's trained on Wikipedia and English Wikipedia data you can then even spin up an instance containing

47:24.900 --> 47:31.620
the software on Amazon or Azure to play around if you have more proprietary data so that you that

47:31.620 --> 47:39.620
you want to use and of course you can contact us if you need help to sort of get started I mean

47:39.620 --> 47:47.620
the problem is that many of us who have been struggling using conventional tooling sometimes it

47:47.620 --> 47:53.940
needs a little bit of help to sort of get the right angle on how to solve something yeah so we do

47:53.940 --> 48:01.140
for example offer a keyword extraction functionality you can throw in a text and you get like the 10

48:01.140 --> 48:08.260
most important keywords out of it and I've observed that many people try to systematically extract

48:08.260 --> 48:14.260
keywords and then try to do some magic with that and I just told them okay that keywords you

48:14.260 --> 48:20.020
need them if you want to show keywords at some point but you don't need them to make any computation

48:20.020 --> 48:26.500
because you can compare to fingerprints directly so yeah it's a bit of a mind shift change of

48:26.500 --> 48:32.580
mindset exactly yeah exactly right well thanks so much Francisco as it was great talking to you

48:32.580 --> 48:37.220
and amazing learning a little bit about what you guys are up to thanks a lot thanks

48:41.700 --> 48:46.500
all right everyone that's it for today's show please leave a comment on the show notes page at

48:46.500 --> 48:55.700
twimmaleye.com slash talk slash 10 or tweet to me at at Sam Charrington or at twimmaleye to discuss

48:55.700 --> 49:01.780
this show or just reach out let me know how you liked it thanks so much for listening and catch you

49:01.780 --> 49:20.020
next time

