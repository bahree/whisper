1
00:00:00,000 --> 00:00:15,440
Welcome to the Tumel AI Podcast. I'm your host, Sam Charrington.

2
00:00:15,440 --> 00:00:27,800
Alright everyone, I am on the line with Sharred Goyle. Sharred is an assistant professor in

3
00:00:27,800 --> 00:00:33,400
the Management Science and Engineering Department at Stanford University. Sharred, welcome to the

4
00:00:33,400 --> 00:00:40,040
Tumel AI Podcast. Hi, thanks for having me. Absolutely, absolutely. So we are going to dive into a

5
00:00:40,040 --> 00:00:46,280
conversation about your paper, the measure and mismeasure of fairness, a critical review of fair

6
00:00:46,280 --> 00:00:51,960
machine learning. But before we do that, you've got an interesting background. You have joint

7
00:00:51,960 --> 00:00:59,880
appointments or courtesy appointments actually to be more precise in the computer science, sociology,

8
00:00:59,880 --> 00:01:05,560
and the law school at Stanford in addition to your primary appointment at Management Science and

9
00:01:05,560 --> 00:01:12,360
Engineering. How did you come to work at the confluence of these many disparate areas?

10
00:01:13,240 --> 00:01:18,760
Yeah, I mean, it's definitely an unusual intersection that I'm working in. I hope more people

11
00:01:18,760 --> 00:01:24,040
start working in this area. I guess I had a slightly non-traditional path to university. I did my

12
00:01:24,040 --> 00:01:32,280
PhD in math. I did a postdoc in math and then I realized maybe it wasn't exactly for me,

13
00:01:32,280 --> 00:01:39,000
that kind of traditional academic math path. And I went off to industry first at Yahoo and

14
00:01:39,000 --> 00:01:44,760
then Microsoft and I spent seven years doing, I guess we were doing data science, but it wasn't

15
00:01:44,760 --> 00:01:50,920
really called data science at that point. And I did that for a while and I started really becoming

16
00:01:50,920 --> 00:01:58,760
interested and applied statistical, applied machine learning problems, this so-called big data,

17
00:01:58,760 --> 00:02:04,840
distributed computing, and especially how they were applied to social science questions. And so

18
00:02:04,840 --> 00:02:09,160
again, at that point, I was, you know, this was kind of the people were still trying to figure out

19
00:02:09,160 --> 00:02:13,400
what's going on with Facebook, Twitter, and these were kind of the early days for network analysis.

20
00:02:13,400 --> 00:02:19,560
And there was a sense that there was something that computer scientists could bring to these

21
00:02:19,560 --> 00:02:25,000
questions, these social scientific questions, but it wasn't exactly sure what that was. And so I was

22
00:02:25,000 --> 00:02:31,880
fortunate enough to be in some of these places that were driving these conversations. So that was

23
00:02:31,880 --> 00:02:38,680
my first kind of foray into this intersection of computer science and social science. And then

24
00:02:38,680 --> 00:02:43,800
at the end of my time in industry, I started becoming even more interested in policy questions. I was

25
00:02:43,800 --> 00:02:49,560
living in New York at the time and I became quite interested in policing practices. Stop and frisk

26
00:02:49,560 --> 00:02:56,280
was in the news as it is actually reoccurring now in the news. There are a lot of high profile court

27
00:02:56,280 --> 00:03:03,880
cases. There was a high profile mayor's race and the police chief ultimately was replaced.

28
00:03:03,880 --> 00:03:11,480
And so I, you know, was coming at it from the perspective of what, you know, what can we say that

29
00:03:11,480 --> 00:03:18,440
goes beyond the traditional legal and policy analysis to these questions that I thought were,

30
00:03:18,440 --> 00:03:23,400
you know, super important I wanted to get involved, but I wasn't quite sure what that involvement

31
00:03:23,400 --> 00:03:28,600
would look like. And what we ended up doing is, you know, there's this one line from a court case

32
00:03:28,600 --> 00:03:35,640
that struck me saying that lots of people were being stopped unjustifiably, but we would never

33
00:03:35,640 --> 00:03:41,560
know how many people were, that was affecting. And this was in reference to what are called

34
00:03:41,560 --> 00:03:46,760
fourth amendment constitutional violations where someone has stopped without what's called a

35
00:03:46,760 --> 00:03:51,800
reasonable suspicion of criminal activity. And the traditional way that this is looked at by the

36
00:03:51,800 --> 00:03:55,880
courts is that you interview the people involved, the individuals who are stopped, the police

37
00:03:55,880 --> 00:03:59,880
officers involved, any witnesses. And then you have to come to, you know, here I've been

38
00:03:59,880 --> 00:04:05,000
storing, then you have to come to a decision. And, you know, in New York, at the height of stop and

39
00:04:05,000 --> 00:04:10,040
frisk, you're talking about half a million people stopped a year. And so there is no way that you

40
00:04:10,040 --> 00:04:16,920
could go and interview people in all of these cases. And so kind of the first project that I worked

41
00:04:16,920 --> 00:04:21,320
on that was at the intersection of machine learning and policy was trying to develop a method

42
00:04:21,320 --> 00:04:27,960
to estimate how many people were being stopped in violation of this constitutional protection

43
00:04:28,840 --> 00:04:34,600
against unreasonable stops and seizures. And so we ended up, and ended up being a pretty

44
00:04:34,600 --> 00:04:40,840
straightforward project, but the result I think was striking, at least to me, we estimated that

45
00:04:40,840 --> 00:04:48,600
something like 40% of people were being stopped with less than 1% chance of having a weapon on them.

46
00:04:48,600 --> 00:04:54,920
You know, it's not like totally clear what the bar is for reasonable suspicion. I mean, the courts

47
00:04:54,920 --> 00:05:02,280
have, I would say, adamantly refused to put a probabilistic threshold on that kind of phrase,

48
00:05:02,280 --> 00:05:07,560
reasonable suspicion. But I think a lot of people, including myself, would say 1% is pretty low.

49
00:05:08,200 --> 00:05:14,920
And if you have 40% of people who are being stopped without even a 1% chance of having

50
00:05:14,920 --> 00:05:21,800
contraband of having a weapon on them, it really raises the question of whether or not it meets

51
00:05:21,800 --> 00:05:28,760
that constitutional threshold. That was, I think, an interesting and revelatory to me that these

52
00:05:28,760 --> 00:05:33,480
machine learning tools could be used in these contexts that were very different from what I was

53
00:05:33,480 --> 00:05:38,760
traditionally doing, like, you know, maximizing click-through rates, you know, just to give an example.

54
00:05:38,760 --> 00:05:46,360
And so it was very different domain and something that I became quite interested in pursuing.

55
00:05:46,360 --> 00:05:51,480
And at that point, I more or less at the same time moved to Stanford and then started really

56
00:05:52,040 --> 00:05:57,320
tried to understand what these computational statistical and specific machine learning methods

57
00:05:57,320 --> 00:06:01,560
could add to our broader conversations about policy and social issues.

58
00:06:01,560 --> 00:06:11,000
So that experience awakened you to an interest in the application of machine learning and data science

59
00:06:11,000 --> 00:06:17,800
to these social challenges. How did, you know, what were some of the first things that you did to

60
00:06:18,680 --> 00:06:22,440
really dig into that to understand how to apply them?

61
00:06:22,920 --> 00:06:30,280
Yeah, so literally the first project that I started when I came to Stanford was a collaboration

62
00:06:30,280 --> 00:06:37,560
with a journalist here who was interested in understanding police practices across the country.

63
00:06:37,560 --> 00:06:41,880
So again, this was very close to the work that we were doing in New York or revolving stop and

64
00:06:41,880 --> 00:06:51,800
frisk. And the basic question was to see if we could quantify some of the potential

65
00:06:51,800 --> 00:06:57,960
discrimination that was happening in police encounters. And so again, it's like if you kind of

66
00:06:57,960 --> 00:07:03,240
backtrack to, you know, five, seven years ago, there was, there were a lot of conversations

67
00:07:03,240 --> 00:07:09,080
happening around police practices. But to the most, for the most part, these were kind of compelling

68
00:07:09,080 --> 00:07:16,440
stories that people would tell about their own experiences. But there was a lack of data that we

69
00:07:16,440 --> 00:07:24,120
could bring to this problem. And I say this not it all to minimize the value of these stories

70
00:07:24,120 --> 00:07:28,680
that people were telling, but to highlight some of the challenges that we were facing at the time,

71
00:07:29,240 --> 00:07:35,720
where, where we just didn't have any, we didn't have any broad systematic way of looking at

72
00:07:35,720 --> 00:07:41,640
policing across the country. We had these disparate stories, these incidents, these news reports,

73
00:07:41,640 --> 00:07:46,920
but we didn't have a way of bringing this all together. And so we started on this project.

74
00:07:46,920 --> 00:07:51,080
And, you know, I was like admittedly pretty naive. I was like, okay, this is great. This is

75
00:07:51,080 --> 00:07:55,720
super important. And you know, we're going to spend like a year, we're going to try to collect

76
00:07:55,720 --> 00:07:59,720
all this data and try to make, make some sense of it. And then hopefully that'll add to this conversation.

77
00:08:00,520 --> 00:08:08,360
Now I think this is six years into the project. We're still going. It's been, it's been

78
00:08:08,360 --> 00:08:15,560
extremely difficult. We filed hundreds of requests, public records requests, FOIA requests,

79
00:08:15,560 --> 00:08:21,880
with state agencies, with municipal police departments, we have a team of maybe, you know,

80
00:08:21,880 --> 00:08:28,680
something like 10, 11 people who have worked on this project, probably over 10,000 hours of work,

81
00:08:28,680 --> 00:08:32,360
just compiling the data. And so this is what we call the Stanford Open Policing Project.

82
00:08:32,360 --> 00:08:37,960
Yeah. There are really kind of two big challenges in that project. The first is the data collection

83
00:08:37,960 --> 00:08:43,800
and the standardization, which was extraordinarily difficult. So you can imagine you get

84
00:08:43,800 --> 00:08:49,640
information from, or even even, even getting information from a department or any form is

85
00:08:49,640 --> 00:08:54,760
extremely difficult. But when you get that information, it comes to you in paper records,

86
00:08:54,760 --> 00:08:59,560
it comes to you in CDs, it comes to you in all sorts of, you know, different formats. And then

87
00:09:00,360 --> 00:09:06,280
coercing that, coalescing that into something coherent is extraordinarily difficult. So that was

88
00:09:06,280 --> 00:09:11,720
the first challenge. I mean, formats aside, how do you even capture the salient aspects of a

89
00:09:11,720 --> 00:09:16,680
police stop? It's, it's, we didn't know how to do it. And so we took a very narrow

90
00:09:17,880 --> 00:09:22,440
view of this. And we said, we're going to clock as much information that we can, we can,

91
00:09:22,440 --> 00:09:28,840
but really we're going to focus on searches, police searches. Because that's something where we

92
00:09:28,840 --> 00:09:36,600
felt that we could develop a machine learning toolkit for understanding how these, how these

93
00:09:36,600 --> 00:09:41,640
searches are carried out in the extent to which there might be discrimination in that particular

94
00:09:41,640 --> 00:09:47,480
police action. And so we spent several years both collecting, cleaning the data and then also

95
00:09:47,480 --> 00:09:54,760
developing statistical methods to audit police search behavior. And what we found, again, perhaps

96
00:09:54,760 --> 00:10:01,000
unsurprisingly, but I do think it's sort of, it was, I do hope that we're bringing something

97
00:10:01,000 --> 00:10:06,920
new to that conversation is saying that we, that we did find, I would say pre compelling statistical

98
00:10:06,920 --> 00:10:11,880
evidence that the threshold for searching individuals across the country, black and Hispanic

99
00:10:11,880 --> 00:10:16,760
individuals across the country, was lower than the threshold for searching white individuals.

100
00:10:16,760 --> 00:10:20,680
And so these are conditions on being stopped. So people will be stopped for all sorts of reasons,

101
00:10:20,680 --> 00:10:27,640
maybe speeding, maybe reasons that are, that are less directly related to a traffic violation.

102
00:10:27,640 --> 00:10:31,960
For example, a broken tail light, but there are, you know, various reasons that people are stopped.

103
00:10:31,960 --> 00:10:37,640
And then after someone has stopped, we can ask what, what is it that props a search? And so a

104
00:10:37,640 --> 00:10:43,080
search might happen somewhere between one and 10% of the time, depending on where you are. And

105
00:10:43,080 --> 00:10:49,400
what we're finding pretty consistently is that that threshold for searching, black and Hispanic

106
00:10:49,400 --> 00:10:55,480
drivers tended to be substantially lower than the threshold for searching white drivers. And

107
00:10:55,480 --> 00:11:02,520
that's what we took as evidence of bias in that particular interaction. And you asked, what is it

108
00:11:02,520 --> 00:11:10,040
that prompts a search? Did you have characteristics in your data that was able to answer that question?

109
00:11:10,040 --> 00:11:15,400
What specifically prompts a search like are the officers required to report, you know, what their

110
00:11:15,400 --> 00:11:22,600
suspicion was? Yeah. So it, again, it depends given the plurality of diversity of jurisdictions

111
00:11:22,600 --> 00:11:26,280
that we're looking at. You know, it varies a little bit from jurisdiction to jurisdiction.

112
00:11:26,840 --> 00:11:31,080
But broadly, I would say there are kind of two reasons that people are conducting searches.

113
00:11:31,640 --> 00:11:38,360
Buying the kind of predominant reason is suspicion of drugs. So relatively,

114
00:11:38,360 --> 00:11:43,960
relatively low-level drugs, you know, think marijuana, small amounts of marijuana.

115
00:11:44,680 --> 00:11:50,680
And then the second is officer safety. So think of a weapon. And so these are the two big

116
00:11:50,680 --> 00:11:56,040
reasons for carrying out a search. What was it ultimately looking at kind of the relative success

117
00:11:56,040 --> 00:11:59,880
rates of finding the thing that they were looking for once they started the search? That's exactly

118
00:11:59,880 --> 00:12:04,200
right. And so done. I mean, the simplest thing, and there's actually this kind of, this was called

119
00:12:04,200 --> 00:12:11,560
the hit rate test or the outcome test. It goes back to Gary Bakker, the Nobel Laureate in economics.

120
00:12:11,560 --> 00:12:17,640
And it's very clever idea. It just says, let's look at the rate at which an officer recovers

121
00:12:17,640 --> 00:12:21,960
contraband. So let's say you're looking for drugs. How often do you actually recover drugs?

122
00:12:21,960 --> 00:12:25,560
So let's take an extreme example. Let's say that when an officer is searching for drugs,

123
00:12:25,560 --> 00:12:32,440
they end up in they search a black motorist. They find drugs 1% of the time. But on the other hand,

124
00:12:32,440 --> 00:12:39,080
let's say when they search a white driver for drugs, they find drugs 99% of the time. So intuitively,

125
00:12:39,080 --> 00:12:45,160
that suggests that they're only carrying out that search of a white driver when they're really

126
00:12:45,160 --> 00:12:50,200
sure they're going to find something. But for a black driver, they're willing to carry out that

127
00:12:50,200 --> 00:12:56,120
search on much less suspicion. And so that's evidence of a double standard. And again, we would

128
00:12:56,120 --> 00:13:03,000
think of that as a discriminatory search threshold. And so it's a great starting point.

129
00:13:03,000 --> 00:13:09,240
This kind of hit rate analysis or outcome analysis. What we discovered is that the test itself,

130
00:13:09,240 --> 00:13:14,120
while informative, has all these kind of funny statistical anomalies. And so it could be,

131
00:13:14,120 --> 00:13:20,440
and this is what we found in some jurisdictions, that the hit rate for finding a contraband on

132
00:13:21,000 --> 00:13:27,160
black drivers was sometimes higher than the hit rate for finding contraband and white drivers.

133
00:13:27,160 --> 00:13:32,360
And so the typical, background style analysis would say that the officers are discriminating

134
00:13:32,360 --> 00:13:38,760
against white drivers in that situation. And we thought that was pretty unusual. And so it's like,

135
00:13:38,760 --> 00:13:44,120
maybe there were, maybe there's situations where there wouldn't be any discrimination. But we thought

136
00:13:44,120 --> 00:13:48,360
it was, you know, a little bit unusual that there's actually active discrimination against white

137
00:13:48,360 --> 00:13:53,960
drivers in these types of police interactions given everything we know. And so we dug into this test

138
00:13:53,960 --> 00:13:59,720
and we found out that they're, even though it's quite intuitive, there are all sorts of things

139
00:13:59,720 --> 00:14:06,360
that can throw it off. And so we designed a new test, what we call the threshold test.

140
00:14:06,360 --> 00:14:10,440
A word of some examples of the things that would throw it off. Yeah, so it's really good for

141
00:14:10,440 --> 00:14:15,320
just not stopping any white drivers. Yeah, so it's really good question. And so this is what

142
00:14:15,320 --> 00:14:20,760
economists call the problem of informationality. So let me give you an example of this.

143
00:14:21,320 --> 00:14:27,640
If I could hit pause on that, because I don't want to forget this, you described the test.

144
00:14:27,640 --> 00:14:37,720
And I forget the name of the Nobel laureate, but the suggestion was that this was a nuance test

145
00:14:37,720 --> 00:14:42,280
or maybe even it contributed to him winning the Nobel Prize. But it seems obvious. Like, what am I

146
00:14:42,280 --> 00:14:48,840
missing? What is baked into this that is beyond what you might expect? So I think what's great

147
00:14:48,840 --> 00:14:56,040
about this test is that in hindsight, it's, you're right, it is obvious. You know, it feels obvious.

148
00:14:56,040 --> 00:15:01,400
I would say it wasn't the way that people were thinking about it at the time. So the way that most

149
00:15:01,400 --> 00:15:06,680
people, it would say still the way that most people think about detecting discrimination is looking

150
00:15:06,680 --> 00:15:12,840
at rates of action. So looking at search rates. And so you might say, let's look to see, is it the

151
00:15:12,840 --> 00:15:19,080
case that black drivers are being searched more often than white drivers? And in fact, we see

152
00:15:19,080 --> 00:15:23,320
that that's true. We see that black driver that searched twice as often on average across the

153
00:15:23,320 --> 00:15:27,000
country than white drivers. You don't have a baseline or a ground truth or anything to compare.

154
00:15:27,000 --> 00:15:33,000
Exactly. And so the standard way that people address this issue is starting to adjust for

155
00:15:33,000 --> 00:15:37,800
various factors. So they're saying, okay, let's adjust for where you are. Let's adjust for the

156
00:15:37,800 --> 00:15:42,840
time of day. Let's adjust for, you know, maybe the type of car you're driving. Let's adjust for

157
00:15:42,840 --> 00:15:48,760
all these things. But ultimately, you can't really adjust for the fact that maybe the officer

158
00:15:48,760 --> 00:15:54,680
sees something that's not in the data. And that's what is prompting the search. And so back

159
00:15:54,680 --> 00:15:59,640
around this very, very clever insight in your right that in retrospect, it's totally obvious.

160
00:16:00,280 --> 00:16:05,000
But the time it was quite clever of saying, let's turn this problem on its head. Let's not look at

161
00:16:05,000 --> 00:16:11,240
the rate at which we're searching people. Let's look at the rate at which those actions are successful.

162
00:16:11,240 --> 00:16:17,480
And that gets around what statisticians call omitted variable bias. Now we don't have to know

163
00:16:17,480 --> 00:16:25,480
exactly why did the officer or the bank manager or the hiring manager decide to take action.

164
00:16:25,480 --> 00:16:28,840
Let's just look at whether or not their decision is ultimately successful.

165
00:16:28,840 --> 00:16:34,520
Great. And then you were about to introduce another statistical term statistical anomaly.

166
00:16:34,520 --> 00:16:38,680
Yes. Yes. Yes. And so this is a funny term. It's called the problem of informationality.

167
00:16:38,680 --> 00:16:45,800
And for marginality. It's not, it's not that common. It's not even that common in economics.

168
00:16:45,800 --> 00:16:49,560
It's, you know, we sort of stumbled upon it where we're going through this literature.

169
00:16:50,200 --> 00:16:54,360
And here's the idea that our notion of discrimination

170
00:16:55,480 --> 00:17:01,560
intuitively is based on a double standard. Whether or not officers are applying a double standard

171
00:17:01,560 --> 00:17:05,800
when they decide to search individuals from different race groups.

172
00:17:06,840 --> 00:17:13,880
Now, hit rates, the search success rate doesn't directly tell you the standard that was being

173
00:17:13,880 --> 00:17:17,480
applied. It just tells you how often those decisions were successful.

174
00:17:18,040 --> 00:17:23,640
And so the funny thing is, let's say, and we actually see that this is happening some jurisdictions,

175
00:17:24,200 --> 00:17:29,400
let's say that officers are discriminating by applying a lower standard

176
00:17:29,960 --> 00:17:35,960
when searching black drivers relative to white drivers. But let's say that there's a group of

177
00:17:35,960 --> 00:17:41,640
black drivers who, if you search them, you're very likely to be successful. So,

178
00:17:41,640 --> 00:17:46,840
so in some cases, you might actually see the contraband sitting on the passenger seat,

179
00:17:46,840 --> 00:17:52,360
or you might see some strong evidence that suggests, if I search this person, there's like a 90%

180
00:17:52,360 --> 00:17:59,640
chance I'm going to find something. Now, because there is this group of really high likely

181
00:18:00,200 --> 00:18:05,400
to find contraband drivers, and if they're not equally distributed across race groups,

182
00:18:05,400 --> 00:18:12,840
that can boost your hit rates even though you're applying a lower standard on average

183
00:18:12,840 --> 00:18:19,320
when searching black drivers in this hypothetical. And so it's a very kind of subtle phenomenon.

184
00:18:19,880 --> 00:18:26,680
And it's not only kind of subtle to see that it could be happening. In theory,

185
00:18:26,680 --> 00:18:32,680
it's subtle to see if it's happening in practice. And so I would say we made two contributions

186
00:18:32,680 --> 00:18:38,120
to that literature. And the first was to find evidence in the data that this is something you

187
00:18:38,120 --> 00:18:42,840
really do have to worry about. And it was triggered by the fact that we were seeing all these cases

188
00:18:42,840 --> 00:18:47,800
where Becker's test would have concluded that you're discriminating against white drivers.

189
00:18:47,800 --> 00:18:54,280
And just kind of naive analysis, even though super intuitive, would suggest something that we

190
00:18:54,280 --> 00:18:58,680
really had strong reason to believe wasn't really happening in the world. That we didn't think

191
00:18:58,680 --> 00:19:03,480
that officers were discriminating against white drivers. It seemed odd, not impossible,

192
00:19:03,480 --> 00:19:09,000
but it definitely got us scratching our heads when we saw that pattern. And then the second

193
00:19:09,000 --> 00:19:14,360
contribution was developing this, what we call the threshold test, that tries to get around

194
00:19:14,360 --> 00:19:20,200
this problem of informationality and directly infers that standard of evidence itself,

195
00:19:20,200 --> 00:19:28,520
not just the hit rate, which is a proxy for the standard. It's a lot. I know. So it's definitely.

196
00:19:30,440 --> 00:19:38,520
There's part of this description suggests to me, you know, some kind of statistical approach

197
00:19:38,520 --> 00:19:45,400
where you, instead of assuming a single distribution, you assume two distributions or multiple

198
00:19:45,400 --> 00:19:51,640
distributions, depending on how many of these classes that you assume and then trying to,

199
00:19:52,440 --> 00:19:56,200
you know, I don't know, fit a stop to a distribution or something like that.

200
00:19:56,840 --> 00:20:02,120
That's exactly right. And so the idea, this was Becker's insight, is that it's possible

201
00:20:02,120 --> 00:20:07,240
in theory that drivers are carrying contraband at different rates. Like we don't know,

202
00:20:07,240 --> 00:20:11,880
but if I see elevated search rates for one group, I at least have to be open at the possibility

203
00:20:11,880 --> 00:20:17,240
that one group is carrying contraband more often than another group. And that might be a

204
00:20:17,240 --> 00:20:23,720
non-discriminatory explanation for why they're being searched at higher rates. And so the idea for

205
00:20:23,720 --> 00:20:31,720
the threshold test was let's try to find a way to explain the results that are consistent with

206
00:20:31,720 --> 00:20:40,120
elevated search rates, but also consistent with the hit rates that we're seeing that would explain

207
00:20:40,120 --> 00:20:45,400
all of these different things at the same time. And statistically what we're trying to do is find

208
00:20:45,400 --> 00:20:52,040
distributions of risk that match all of these observable features in our data.

209
00:20:52,040 --> 00:20:59,480
Drillin to that, the observable feature, does the data that you receive report on

210
00:21:00,200 --> 00:21:08,280
beyond that the individual was stopped and a search cause? Does it report some kind of feature

211
00:21:08,280 --> 00:21:14,920
that allows you to separate the stop into multiple classes? Yeah, it does. I mean, again,

212
00:21:14,920 --> 00:21:19,960
when we're lucky, when we're not lucky, it's all lumped together. But when we're lucky, we can

213
00:21:21,080 --> 00:21:26,200
get rid of, I would say, what are sometimes called non-discretionary searches. And so this might

214
00:21:26,200 --> 00:21:31,720
be an impound. Like once a vehicle's been impounded, then an officer will carry out a search

215
00:21:31,720 --> 00:21:35,960
just kind of part of the process. And so we want to throw those out because those aren't really

216
00:21:35,960 --> 00:21:40,600
directly predicated on suspicion of anything in particular. It's just part of the procedure.

217
00:21:41,480 --> 00:21:46,520
When someone is arrested, then often a search is conducted afterwards. It's called a search

218
00:21:46,520 --> 00:21:52,760
instant to an arrest. Again, not really predicated on specific suspicion. It's just what the

219
00:21:52,760 --> 00:21:58,440
procedure is. And so we try to throw out all of those. And when we leave what we think of as

220
00:21:58,440 --> 00:22:07,160
discretionary searches that are really directly tied to suspicion of, of contraband. Are you trying

221
00:22:07,160 --> 00:22:15,560
to classify those searches that are left based on some feature that says, oh, well, in this search,

222
00:22:15,560 --> 00:22:21,480
there was a gun sitting on the passenger seat. And so this was one of these high-risk situations

223
00:22:21,480 --> 00:22:26,920
versus the other or is the idea that you don't actually have that information in the data. And so

224
00:22:26,920 --> 00:22:35,160
you're, this threshold is being used to try to associate the different stuff. So that's it. So

225
00:22:35,160 --> 00:22:40,680
so in theory, the test is supposed to be able to distinguish between those without the information

226
00:22:40,680 --> 00:22:46,920
directly. Now in practice, of course, it's trickier. And what we're finding that gave us a little

227
00:22:46,920 --> 00:22:53,880
bit of confidence in some places like North Carolina, we had particularly good data. And we were able

228
00:22:53,880 --> 00:22:59,560
to, you know, we were we asked the test to try to infer what was going on without explicitly giving

229
00:22:59,560 --> 00:23:04,840
it that information. And it turned out though that some of this information was in the data also.

230
00:23:05,480 --> 00:23:10,440
So we can look at these stops that were being flagged as potentially high risk. And we say, oh,

231
00:23:10,440 --> 00:23:15,640
in fact, the officer marked that this was, quote, unquote, a plain view search, meaning that they

232
00:23:15,640 --> 00:23:21,960
saw evidence of something in plain view or contraband and plain view. And in fact, the test

233
00:23:21,960 --> 00:23:27,000
was able to infer that without us directly providing that information. And so this gave us some

234
00:23:27,000 --> 00:23:31,960
confidence that even in places where we don't have that information, we could apply it and it would

235
00:23:31,960 --> 00:23:38,440
try to automatically infer what was going on. And I saw note that the data set that you've collected

236
00:23:39,160 --> 00:23:45,240
of these stops is now over 100 million traffic stops. Yeah, we are I think over what period of

237
00:23:45,240 --> 00:23:49,720
time is it? That's an insane number. Yeah, we've actually released over 200 million

238
00:23:49,720 --> 00:23:55,640
traffic stops from across the country. You know, it varies by jurisdiction, but going back about 10

239
00:23:55,640 --> 00:24:03,000
years. Everything is publicly available now. And so if you go to openpolicing.stanford.edu,

240
00:24:03,000 --> 00:24:09,320
you can just go and download everything. We've made it super easy or when we think it's super easy

241
00:24:09,320 --> 00:24:16,840
to use. We've cleaned it all up. We've standardized it. A lot of researchers, policymakers,

242
00:24:16,840 --> 00:24:23,240
law enforcement agencies, reporters, lots of different groups, community activists, a lot of

243
00:24:23,240 --> 00:24:30,200
different groups are using the data to kind of audit their own jurisdictions to help improve

244
00:24:30,200 --> 00:24:35,800
local practices. And so we're super supportive of all of those uses of the information. We

245
00:24:35,800 --> 00:24:40,840
analyze in about of those 200 million stops, about 100 million were in good enough shape that

246
00:24:40,840 --> 00:24:45,800
we could statistically analyze, but we have released everything. And so we hope that other people

247
00:24:45,800 --> 00:24:51,800
continue working on it. We've already seen some positive outcomes from releasing the data.

248
00:24:52,600 --> 00:24:58,920
One thing that I was particularly happy to see is in collaboration with Los Angeles Times,

249
00:24:58,920 --> 00:25:06,440
they did their own analysis using the open policing data and using the threshold test and using

250
00:25:07,640 --> 00:25:14,280
other statistical tests, which they complemented with on the ground reporting. And about a week

251
00:25:14,280 --> 00:25:21,000
after they released their reporting, the LAPD dramatically reduced their stop practices.

252
00:25:21,640 --> 00:25:28,920
Are there questions that you wish people would try to answer based on this data? Do you have

253
00:25:29,720 --> 00:25:33,800
or things that you wish people would do to help support the project in terms of,

254
00:25:34,440 --> 00:25:38,680
you know, we've got a pecking order of, you know, these are the 10 things that are wrong that,

255
00:25:38,680 --> 00:25:43,960
hey, if someone just magically did this, yeah. I mean, yeah, I mean, I think they're like kind of, they're

256
00:25:44,680 --> 00:25:51,240
sort of maybe three big things that I would like people to do. I mean, one is making everything

257
00:25:51,240 --> 00:25:57,480
super local. And so our analysis was broad by necessity across the country. We have, I think,

258
00:25:57,480 --> 00:26:04,520
something like 100 jurisdictions represented. And we can't go in and look at the idiosyncrasies

259
00:26:04,520 --> 00:26:10,840
of any given jurisdiction. And so we are counting on local journalist, policy makers,

260
00:26:10,840 --> 00:26:17,800
community activists to look at the data for their communities and try to understand what is going

261
00:26:17,800 --> 00:26:23,000
on and try to, you know, push law enforcement agencies to improve practices. And so that's

262
00:26:23,000 --> 00:26:27,880
something that we can do. And we've tried to set a template for doing this nationally. But

263
00:26:28,440 --> 00:26:32,360
we really, we know if there are anomalies in any specific jurisdiction, we just can't,

264
00:26:32,360 --> 00:26:37,400
we don't have the bandwidth to deal with that. And so that's where we've seen kind of the most

265
00:26:37,400 --> 00:26:42,840
direct consequence is when people like in Los Angeles, when they go in and they really make it

266
00:26:42,840 --> 00:26:50,360
their own. And especially if they're kind of complementing the data analysis with reporting,

267
00:26:51,240 --> 00:26:58,520
we think that that can make a big difference. The second big thing is we need to figure out

268
00:26:58,520 --> 00:27:04,440
how to make sure this project moves forward. We've spent something like six years doing this.

269
00:27:04,440 --> 00:27:11,240
But I'm not sure how much we can do going forward. This was kind of an enormous kind of commitment

270
00:27:11,240 --> 00:27:17,320
of resources. And I would love to see this project continue and even expand. But I don't know

271
00:27:17,320 --> 00:27:23,080
what the best way is to make that happen. And so I hope that other people will take up the cause

272
00:27:23,080 --> 00:27:29,400
and again, make this project their own and help, you know, filing these records, cleaning the data.

273
00:27:29,400 --> 00:27:37,080
We released all of our code. We've tried to make this easier for other people to do themselves,

274
00:27:37,080 --> 00:27:41,720
but it's hard. It's very difficult to do. But I am hoping that there'll be someone who can

275
00:27:41,720 --> 00:27:50,200
can help take this up. And then the last thing is really on the ML sat side is that there is a lot

276
00:27:50,200 --> 00:27:58,360
of work that needs to go in to still understanding what doesn't mean to have an equitable police interaction.

277
00:27:58,360 --> 00:28:05,800
And part of this is statistical. Part of this is policy. But even when we understand some of the

278
00:28:05,800 --> 00:28:10,200
policy issues, it's still not clear how to measure them statistically. And so we, you know,

279
00:28:10,200 --> 00:28:16,280
we did the threshold task, which I think is a good first step or maybe a good second step into

280
00:28:16,280 --> 00:28:21,640
this problem of understanding potential bias and search decisions, but it's really an early

281
00:28:22,840 --> 00:28:29,400
foray into this area. And I would love to see more people create more nuanced ways of measuring

282
00:28:30,680 --> 00:28:36,520
potential discrimination in that action and that search action, and then more broadly in every

283
00:28:36,520 --> 00:28:42,920
type of policing action that's happening. So that's maybe a good opportunity to segue to

284
00:28:42,920 --> 00:28:51,480
the paper, the measure and mismeasure of fairness, a critical review of fair machine learning,

285
00:28:52,360 --> 00:28:59,400
is there a connection between these two works beyond the kind of application of statistics

286
00:28:59,400 --> 00:29:06,680
and computational approaches to policy questions and problems? Yeah, it's very directly related.

287
00:29:06,680 --> 00:29:12,600
So all of the insights pretty much that we got in the five, six years that we were working on

288
00:29:12,600 --> 00:29:18,920
understanding bias and human decisions, we translated quite directly to understanding bias and

289
00:29:18,920 --> 00:29:24,760
allergic decisions. And even at the technical level of this problem of informationality,

290
00:29:24,760 --> 00:29:31,640
the exact same issue comes up when we're talking about bias and allergic decisions. And so it was

291
00:29:31,640 --> 00:29:36,680
really just a direct continuation of that work. We didn't know it at the time. And so we were like,

292
00:29:36,680 --> 00:29:43,560
oh, this is interesting. And kind of over the course of thinking about this problem,

293
00:29:43,560 --> 00:29:49,800
we're like, actually, these are like deeply connected. And then we kind of put all the pieces

294
00:29:49,800 --> 00:29:54,600
together. We're like, oh, you know, really it's the same problem. It's just two sides of the same

295
00:29:54,600 --> 00:30:01,880
coin. All right. Well, can you walk us through the kind of the goals of this particular paper?

296
00:30:02,520 --> 00:30:09,560
Yeah. So the background of our paper is that there are many in the computer science community,

297
00:30:09,560 --> 00:30:16,840
in particular, who were interested in, for lack of a better word, methodizing fairs. And so what

298
00:30:16,840 --> 00:30:20,840
does this mean? That means that you have an algorithm. And we kind of all, or at least now,

299
00:30:20,840 --> 00:30:26,760
I would say many of us have this idea that algorithms are not just through a neutral objective

300
00:30:28,440 --> 00:30:34,280
objects. But they, you know, they have implications. And we want to ensure that these algorithms

301
00:30:34,280 --> 00:30:39,000
are reusing in these high stakes context. For example, judicial decision making,

302
00:30:39,000 --> 00:30:45,080
healthcare decisions, policing, all of these high stakes context. We want to make sure that they're

303
00:30:45,080 --> 00:30:52,120
fair, whatever that means. And I think one way that I, you know, I've been trained to think,

304
00:30:52,120 --> 00:30:55,880
and I think a lot of other computer scientists have been trained to think, is let's formalize this

305
00:30:55,880 --> 00:31:07,000
idea in terms of math. And I understand that kind of that impulse to do it. But what we saw is

306
00:31:07,000 --> 00:31:14,200
that the leading mathematical definitions of fairness that we proposed were missing. I think a lot

307
00:31:14,200 --> 00:31:19,480
of what was happening on the ground and how we interpret what these algorithms are ultimately

308
00:31:19,480 --> 00:31:26,440
doing. And it wasn't, at least in my view, it wasn't moving us in a direction where we could

309
00:31:26,440 --> 00:31:32,920
really start evaluating the equity of these types of algorithms. And in some cases, definition,

310
00:31:32,920 --> 00:31:37,000
in fact, I would say the most popular mathematical definition of fairness, what's what's called

311
00:31:37,000 --> 00:31:44,680
equal false positive rates, equal error rates. In many cases, this can lead us astray and cause one

312
00:31:44,680 --> 00:31:49,320
to create algorithms that many people would consider to be patently unfair.

313
00:31:49,320 --> 00:31:56,200
Well, let's maybe go through the different approaches that folks take and then we'll talk about

314
00:31:56,200 --> 00:32:00,920
their failures and how folks should be thinking about this. So you started, you were just mentioning

315
00:32:00,920 --> 00:32:05,160
classification parity. Yeah, so classification parity. And so again, I should point out these are

316
00:32:05,160 --> 00:32:12,600
kind of words that we have made up in the field is still evolving. And so there isn't necessarily

317
00:32:12,600 --> 00:32:19,000
a common vocabulary around this yet. But this idea of classification parity or equal error rates

318
00:32:19,000 --> 00:32:24,520
means that let's say that we have an algorithm and I'll give you a concrete example. So an algorithm

319
00:32:24,520 --> 00:32:30,200
that might try to assess the risk of some adverse outcome. For example, someone failing to appear at

320
00:32:30,200 --> 00:32:35,800
a scheduled quartet and we might say, well, how often does the algorithm get this wrong? So how

321
00:32:35,800 --> 00:32:42,760
often does the algorithm flag someone as high risk who actually would ultimately show up to their

322
00:32:42,760 --> 00:32:48,280
quartet? And so this is called a false positive. And so you can say we, you can say that algorithm is

323
00:32:48,280 --> 00:32:56,040
fair if it's a case that false positive rates are equal across race groups or across gender groups

324
00:32:56,040 --> 00:33:05,400
or across whatever your favorite group classification is. So that's one popular definition. And another

325
00:33:05,400 --> 00:33:11,000
popular definition is saying that an algorithm is fair if it doesn't use protected characteristic

326
00:33:11,000 --> 00:33:18,120
once making its decision. So it doesn't use, so the algorithm doesn't have access to, for example,

327
00:33:18,120 --> 00:33:23,400
someone's race or gender. In some ways, this is just blinding the algorithm. And so it's again

328
00:33:23,400 --> 00:33:27,800
an intuitively quite appealing thing that I can say, okay, well, if I didn't tell the algorithm

329
00:33:27,800 --> 00:33:33,000
about someone's race or gender, then how can it be discriminating? So that's another, you know,

330
00:33:33,000 --> 00:33:40,520
quite popular definition that people use. They challenge with that one being correlations and

331
00:33:41,400 --> 00:33:47,080
zip code and race, for example. Yeah, so it's super challenging. So one kind of common

332
00:33:47,080 --> 00:33:55,480
objection to that blindness notion of fairness is that there's lots of stuff that is correlated

333
00:33:55,480 --> 00:34:01,160
with race. That's not race itself, like where you were, you know, where you grew up, where you live,

334
00:34:01,720 --> 00:34:07,960
all of these aspects that really feel that they might be a proxy for race. Now the problem is

335
00:34:07,960 --> 00:34:13,320
everything is correlated with everything else. And so it's not even clear where to draw the lie.

336
00:34:13,320 --> 00:34:18,920
Like what constitutes a proxy and what, you know, what is kind of an innocuous correlation.

337
00:34:19,880 --> 00:34:24,200
So now I think it's been pretty well understood for a long time. One thing that's a little bit

338
00:34:24,200 --> 00:34:32,120
less well understood is that in some cases, you might even want to have, you might want to include

339
00:34:32,120 --> 00:34:38,520
the protected characteristic in your algorithmic decision. And so this strikes people as unusual.

340
00:34:38,520 --> 00:34:45,560
Let me give you an example in the decision or in the in the algorithm. In both. You know,

341
00:34:45,560 --> 00:34:52,520
there are approaches that people are proposing that include the protected attribute in the algorithm

342
00:34:52,520 --> 00:34:57,960
so that the algorithm or the model can ensure anticorrelation with that attribute. But that's

343
00:34:57,960 --> 00:35:04,760
slightly different than including it in the decision. So I guess I think these are a little

344
00:35:04,760 --> 00:35:09,320
bit related. So it's like a hard to totally distinguish the algorithm from the decision. So let

345
00:35:09,320 --> 00:35:15,480
me give you an example where you might use this directly affirmative action. So here's a case

346
00:35:15,480 --> 00:35:23,560
where a lot of people including myself would say that it's completely ethical, equitable to use

347
00:35:23,560 --> 00:35:30,600
a protected characteristic namely race when you're making that decision about who to admit to a

348
00:35:30,600 --> 00:35:37,880
college. You know, I mean, hopefully this example points out that there are cases where we very

349
00:35:37,880 --> 00:35:42,280
actively want to consider someone's group membership in order to make better decisions.

350
00:35:43,400 --> 00:35:48,520
Now let me give you another example which is even more subtle and it's more controversial. I

351
00:35:48,520 --> 00:35:52,600
actually don't know what the answer is, but I think it's an instructive example. So what we found

352
00:35:52,600 --> 00:35:58,760
and we've looked across the country is that if you take a man and a woman who have similar criminal

353
00:35:58,760 --> 00:36:03,480
history or similar age, and so these are the traditional things that people use when they're

354
00:36:03,480 --> 00:36:11,080
trying to assess risk, women are less likely to re-event than men. And so again, men and women

355
00:36:11,080 --> 00:36:17,160
have similar backgrounds, similar criminal history, similar age, women are less likely to re-event

356
00:36:17,160 --> 00:36:22,520
and re-event in the future than men. And so if you have a blind algorithm that doesn't consider

357
00:36:22,520 --> 00:36:27,560
gender, you're going to systematically overestimate the risk of women and you're going to systematically

358
00:36:27,560 --> 00:36:35,800
underestimate the risk of men. What the implication of this is is that if I now just decide to

359
00:36:35,800 --> 00:36:40,680
take some action, for example, detain an individual who's deemed high risk, I'm going to end up

360
00:36:40,680 --> 00:36:45,800
detaining women who are deemed high risk who actually statistically know are not high risk.

361
00:36:46,440 --> 00:36:53,880
They just happen to look like the men who in fact are high risk. And so this is a tricky situation

362
00:36:53,880 --> 00:37:00,600
because statistically it's an easy fix. Statistically, I just say that I know all our

363
00:37:00,600 --> 00:37:05,800
sequel women are less likely to re-event than men. And so that should go into my decision-making

364
00:37:05,800 --> 00:37:10,440
process. That should go into my statistical algorithm. And some jurisdictions actually do this.

365
00:37:10,440 --> 00:37:17,960
Wisconsin does this, for example. But the flip side is that there might still be, and I think

366
00:37:17,960 --> 00:37:23,880
this is true, that there's kind of a social norm that's being violated. And we don't like to

367
00:37:25,160 --> 00:37:32,120
focus on these differences across groups. And there's a real cost to sending the signal that

368
00:37:32,840 --> 00:37:39,320
there might be average differences across groups, even when we just for certain things. And that's

369
00:37:39,320 --> 00:37:43,000
not necessarily saying that this is inherent. It's not saying that it's biological or that it's

370
00:37:43,000 --> 00:37:48,120
going to last forever. You know, maybe it's related to all sorts of things, whether or not you're

371
00:37:48,120 --> 00:37:55,480
a primary caregiver or who knows what it's related to. But it might be a reasonable proxy for

372
00:37:55,480 --> 00:38:01,560
figuring out who is higher risk and who is not. But do we want to use that attribute? Do we

373
00:38:01,560 --> 00:38:05,720
fundamentally want to use gender when we're making these types of assessments? And I don't know what

374
00:38:05,720 --> 00:38:10,120
the answer is. And different people, I would say, have recently come to different conclusions.

375
00:38:10,120 --> 00:38:16,360
But the reason I say there's no easy answer is that if you decide to use gender, well, you're

376
00:38:16,360 --> 00:38:21,800
using gender, and now we're advertising to everybody, now we think this is an important factor to

377
00:38:21,800 --> 00:38:29,080
consider. And if you don't use gender, then you're ultimately going to be detaining women at a higher

378
00:38:29,080 --> 00:38:38,760
rate than is really necessary to ensure public safety. And so it's very difficult. Most jurisdictions

379
00:38:38,760 --> 00:38:43,080
that I know have decided not to use gender. But I don't know. I don't know if that's the right answer.

380
00:38:43,960 --> 00:38:51,160
And again, I think it's just a challenging question that to me points out why these kind of

381
00:38:51,160 --> 00:38:56,440
hard and fast rules like saying we can't use protected characteristics are really

382
00:38:57,400 --> 00:39:00,760
trying to sweep some of the hard policy questions under the rock.

383
00:39:01,560 --> 00:39:07,240
And so is there a third traditional definition that you look at calibration?

384
00:39:07,240 --> 00:39:11,000
So another definition that people use, they're really three, the one, the first is these

385
00:39:11,000 --> 00:39:17,560
error equal error rates, the second is anti-classification, and then the third is what's called

386
00:39:17,560 --> 00:39:23,960
class, is called calibration, where it says that if I give individuals, two individuals,

387
00:39:23,960 --> 00:39:30,760
or two groups of individuals the same risk score, the outcomes should happen similar rates.

388
00:39:30,760 --> 00:39:37,000
And so if I say that here's a group of white individuals that are classifying as medium risk

389
00:39:37,000 --> 00:39:41,640
and a group of black individuals that I'm classifying as medium risk, then they both should ultimately

390
00:39:41,640 --> 00:39:48,120
re-offend its similar rates. And if that's not true, then it suggests that something is missing

391
00:39:48,120 --> 00:40:00,120
from my algorithm that maybe that I somehow created it in a way that isn't appropriately

392
00:40:00,120 --> 00:40:07,400
making use of the data that's available to me. And in my mind, this is an important property

393
00:40:07,400 --> 00:40:12,760
to have, but it's a pretty low bar. And so there are many algorithms out there that would have

394
00:40:12,760 --> 00:40:19,080
this property that they are calibrated, but they're not actually algorithms that we would really

395
00:40:19,080 --> 00:40:25,320
want to use in practice. And so even something like redlining, kind of this historical

396
00:40:25,320 --> 00:40:33,400
example of what we now recognize to be a highly discriminatory way of making lending decisions,

397
00:40:33,400 --> 00:40:39,480
that if I classify a neighborhood as quote unquote high risk, that that might be true on average,

398
00:40:40,360 --> 00:40:46,200
but still the fact that I'm classifying neighborhoods at high risk as opposed to

399
00:40:47,160 --> 00:40:52,360
looking at the specifics of an individual and determining, oh, you happen to live in a neighborhood

400
00:40:52,360 --> 00:40:57,960
that on average is defaulting at higher rates in another neighborhood, but you in particular

401
00:40:57,960 --> 00:41:03,480
for creditworthy, that strikes people, I think correctly, is being inappropriate.

402
00:41:04,600 --> 00:41:10,600
And so sending an example of an algorithm, which is calibrated, but is not really getting

403
00:41:10,600 --> 00:41:18,120
at the heart of what we think of as equity. And so what the paper is doing is it's looking at these

404
00:41:18,120 --> 00:41:26,360
these three definitions. And yeah, I think we've talked through and introducing these definitions,

405
00:41:26,360 --> 00:41:33,640
some of the unique challenges of each of them and kind of why they're difficult,

406
00:41:34,760 --> 00:41:40,840
but the paper is taking it a step further if I'm understanding it correctly and saying,

407
00:41:41,560 --> 00:41:47,240
is it saying that it's not that one of these is bad and the other is good, but that they're all bad

408
00:41:47,240 --> 00:41:53,080
and that maybe help me understand what is the paper go from here. So we definitely take in,

409
00:41:53,080 --> 00:41:59,960
in some ways, like burn the house down approach to this. I don't want to be the last minute,

410
00:41:59,960 --> 00:42:07,880
I don't want to be too pessimistic. I advocate against taking a formal mathematical view

411
00:42:08,520 --> 00:42:13,160
of fairness. I say that even though my training is in math, all my degrees are in math.

412
00:42:13,160 --> 00:42:18,200
So that's what is coming down to us. It's like we're going to talk about and introduce these

413
00:42:19,000 --> 00:42:27,560
approaches, but they all are problematic. And the answer is not another mathematical approach.

414
00:42:27,560 --> 00:42:31,160
It is taking a step back from the math and why it's that.

415
00:42:31,160 --> 00:42:37,640
Yeah, so I think that's exactly right. And I think you said it well, but in my mind, it's not

416
00:42:37,640 --> 00:42:43,960
that we need more math. It's that we need to understand what is it that we're trying to accomplish.

417
00:42:43,960 --> 00:42:48,440
And so if I were to give you a piece of legislation, you know, just traditional legislation,

418
00:42:48,440 --> 00:42:53,400
I wouldn't say, you know, give me a mathematical definition that I could apply to the text of the

419
00:42:53,400 --> 00:42:59,160
legislation and then tell you whether or not it's fair. We sort of recognize that that's,

420
00:42:59,160 --> 00:43:06,120
and nothing is technically difficult to do, but that's missing the point of what we mean by equity

421
00:43:06,120 --> 00:43:13,320
in these broader policy contexts. And I think when we use order to algorithm, it encourages us

422
00:43:13,960 --> 00:43:20,680
to think in those mathematical terms, which while it has some value, I think it still misses

423
00:43:21,240 --> 00:43:27,800
the goal of any policy intervention, which is trying to improve outcomes down the road. And so

424
00:43:27,800 --> 00:43:34,200
that's where I would like the work to go of trying to understand how can we determine

425
00:43:34,200 --> 00:43:41,800
what the consequences of any particular algorithmic decision-making system are. And to the

426
00:43:41,800 --> 00:43:46,840
extent that we can formalize that great, but in many cases, I guess I'm personally pessimistic

427
00:43:46,840 --> 00:43:52,920
that there's going to be some universal or a small set of definitions that one could apply

428
00:43:52,920 --> 00:44:00,600
that lets us audit an algorithm and comes out with the green chap mark or the red ax that says,

429
00:44:00,600 --> 00:44:05,960
this is fair and this is not fair. Do you have a kind of a keystone example where

430
00:44:07,080 --> 00:44:14,120
all of the traditional approaches to fairness fail for various reasons? And we probably won't be

431
00:44:14,120 --> 00:44:19,880
able to come up with another measure, but if we take a simple approach, ABC, that will

432
00:44:20,840 --> 00:44:26,200
get us closer to what we want as a society. So I think criminal risk assessment system in

433
00:44:26,200 --> 00:44:33,000
an area that has received a lot of attention. And so almost all criminal risk assessments are

434
00:44:33,000 --> 00:44:42,040
going to violate the equal air rate principle. If you don't use gender, so adopting this anti-classification

435
00:44:42,040 --> 00:44:50,520
point of view, it will violate calibration. And if you do use gender, it will violate anti-classification,

436
00:44:50,520 --> 00:44:56,760
but it will be calibrated. And so it's hard to know what the right answer is, but you definitely can't

437
00:44:56,760 --> 00:45:04,120
satisfy, you'll probably likely violate several of these formal fairness criteria at the same time.

438
00:45:04,760 --> 00:45:11,000
Now what you do here, I think this is super tough. And the way that I think about the problem here

439
00:45:11,000 --> 00:45:17,160
is again, the risk assessment algorithm in some sense is just trying to give you predictions about

440
00:45:17,160 --> 00:45:22,680
what's going to happen to this individual. Is this person likely to miss court or not miss court?

441
00:45:22,680 --> 00:45:26,760
Now when I think about fairness, I think about what you do with that information. So one thing

442
00:45:26,760 --> 00:45:33,000
that we're trying to do is identify people who are high risk of missing court and then arranging

443
00:45:33,000 --> 00:45:37,960
for them to have door to door or free door to door ride chair service from their home to court.

444
00:45:37,960 --> 00:45:42,120
Because we recognize a lot of the time that people miss their court date is because they don't have

445
00:45:42,120 --> 00:45:47,880
easy access to public transportation. They don't have another way of getting to court. And so taking

446
00:45:47,880 --> 00:45:52,680
the exact same information, this like risk, this assessment that you're likely to miss court,

447
00:45:53,720 --> 00:45:58,520
and saying we're going to provide a supportive service to you. And my mind makes it much more

448
00:45:58,520 --> 00:46:03,640
equitable than if we were to take a punitive approach and say we're going to lock you up,

449
00:46:03,640 --> 00:46:10,280
which I don't think addresses the court. To your point, even if your punitive system is based on

450
00:46:10,280 --> 00:46:14,360
a much better message. Exactly. Even if you even if you use the exact same information,

451
00:46:15,560 --> 00:46:22,920
you know, one scenario, I would say your action is not justifiable or I would not think of it as

452
00:46:22,920 --> 00:46:28,680
ethical. And the other situation where you're providing supportive services, I think it becomes

453
00:46:28,680 --> 00:46:37,240
much more tolerable. And so it's really in this sense, whether or not a decision is equitable,

454
00:46:37,240 --> 00:46:43,800
can be justified is not so much how accurate your algorithm is, but what you use that information

455
00:46:43,800 --> 00:46:50,520
to do. Well, Sarah, thank you so much for taking the time to kind of talk through your research

456
00:46:50,520 --> 00:46:54,680
really interesting stuff. And I appreciated the opportunity to learn about what you're up to.

457
00:46:54,680 --> 00:46:56,280
Thanks. It's been a great conversation.

458
00:47:00,280 --> 00:47:05,240
All right, everyone. That's our show for today. For more information on today's show,

459
00:47:05,240 --> 00:47:13,400
visit twomolai.com slash shows. As always, thanks so much for listening and catch you next time.

