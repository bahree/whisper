Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting
people doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
Well team, I guess I'll just jump straight to the bad news.
This week we shared with excitement the news about a special Halloween event that we were
planning for October 30th in New York City.
Well, due to unforeseen events beyond our control, the event is now canceled.
We were really really looking forward to it and are incredibly disappointed about its cancellation.
If you purchase tickets via either the event bright or splash that pages, you should
have been automatically refunded.
The good news though, is that you can still connect with us on Monday evening because
those will be headed to the NYU Future Labs AI Summit Happy Hour.
If you're in New York City, we hope you'll join us at the happy hour and more importantly
the AI Summit itself.
As you may remember, we attended the inaugural Summit back in April and had a great time
and delivered some great interviews, all of which will link to in the show notes for
your listening pleasure.
This year's event features more great speakers, including Karina Cortez, head of research
at Google New York, David Venturelli, science operations manager at NASA Ames Quantum
AI Lab, and Dennis Mortensen, CEO and founder of StartupX.ai.
For the event homepage, visit aiSummit2017.futurelabs.nyc, and for 25% off tickets, use the
code Twimmel25.
You can find links to this and more great events on our new events page at twimmelai.events,
and of course, this shows notes page at twimmelai.com slash talk slash 57.
The episode you're about to hear is the first of a new series of shows on autonomous vehicles.
Now we all know that self-driving cars is one of the hottest topics in machine learning
in AI, so of course, we had to dig a little deeper into the space.
And to get us started on this journey, I'm excited to present this interview with Darren
Nakuda, CEO and co-founder of Mighty AI.
Darren and I discussed the many challenges of collecting training data for autonomous
vehicles, along with some thoughts on human-powered insights and annotation, semantic segmentation,
and a ton more great stuff.
You may not realize it, but if you're a long-time listener, you already know Mighty AI from
my interview with their lead data scientist Angie Hugeback for twimmeltalk number six.
It is so hard to believe that that was over 50 shows ago.
Mighty AI was one of the first sponsors of this podcast, and it's great to have them
back as a sponsor for this series.
As you'll hear, the company delivers training and validation data to firms building computer
vision models for autonomous vehicles.
Their platform combines guaranteed accuracy with scale and expertise.
Based to their full stack of annotation software, consulting and managed services, proprietary
machine learning, and global community of pre-qualified annotators.
We thank Mighty AI for being a valued sponsor, so please be sure to visit them at www.mty.ai
to learn more and follow them on Twitter at www.mty.ai.
Alright everyone, I am on the line with Darren Nikuda, the CEO of Mighty AI.
Mighty AI is a company that you've heard from on the podcast before, but in fact they
were formerly called spare five, and we interviewed one of their lead data scientists, Angie Hugeback
back on twimmeltalk number six, just about a year ago.
Darren, welcome to the show.
Thanks Sam.
It's great to have you on.
Why don't we start by having you introduce yourself and talk a little bit about your
role at Mighty AI?
Sure, so my name is Darren Nikuda, I'm the CEO and one of the founders of Mighty AI.
So we've been working at Mighty AI, as you said formerly known as spare five for about
three years, and really trying to harness human insights and human power into building
better training data sets for artificial intelligence.
And tell us a little bit about your background.
Sure, so my background has been in software engineering for about 20 years, mainly in
internet technology, so everything from e-commerce and communications platforms through marketplaces
and yeah, okay.
And is Mighty AI the your kind of first for and to the AI space or have you been doing
that for a while?
So Mighty AI is really a first for in the AI, but really using human insights has been
something I've been doing for a while.
So at both startups as well as when I worked in Amazon, I leveraged mechanical Turk and
other platforms to use humans to augment what we could do with our systems.
Okay, awesome.
So since the conversation with Angie, again, just under a year ago, it sounds like you
guys have gotten a lot more focused and in particular, you're spending a lot of time
in the autonomous vehicle space.
Can you tell us a little bit about what you're up to there?
Sure.
So when we started a few years ago, we were really focused on human powered insights for
almost anything.
And what we realized was what really set us apart was our focus on quality.
So like you talked about with Angie, you know, a year ago, really building our own models
for user reputation and data, data quality predictions was key to our success.
And really that was resonating with customers who were focusing on building training data,
for building, you know, models where they need a really highly accurate data.
And that boiled down to natural language and computer vision and really where we saw
a lot of focus was on the computer vision side, specifically in autonomous driving, which
is, you know, a huge field as you've, as you've seen.
And we had a lot of a lot of demand there for really specialized, really highly accurate
data.
So we decided to focus purely on that area.
And I've mentioned the conversation with Angie and you've mentioned human powered insights
a couple of times.
And I think I may be taking for granted that folks will have heard that podcast, but I
probably shouldn't do that.
So why don't you take a second to kind of step back and really walk through what you
guys do so that we can, you know, make sure everyone's on the same page on that?
Sure.
So what we have a platform called Spare 5, which is basically a community of people around
the world who we give a small microtask to and they are able to perform those.
We have a quality control system in which that we can review and manage that both automatically
and with other people.
And what we deliver to our customers is a high quality result.
So they'll come to us with a requirement.
For example, a photograph and some requirements as far as what types of things in the photo
need to be labeled.
In the case of autonomous driving, that might be drawing bounding boxes around pedestrians
and vehicles on the road, or it could be something like segmenting every pixel of the
image into semantic class.
And we'll build a workflow and we'll go through that and have humans do that and then using
the combination of the humans and our AI deliver back a result to them that they can then
use to build their own models.
Okay.
And you were previously doing this for folks that operated in a variety of market segments,
but you've again focused more tightly on autonomous vehicles for some time now.
Can you talk a little bit about some of what makes that market unique for what you're doing?
Sure. I think autonomous vehicles, especially on the computer vision side, is really a great
example of what needs to happen in order to build highly accurate models.
In a lot of the other use cases that we dealt with in the past, there was a lot of flexibility
or more subjective insights as to taste, like in retail or something like that, where you're
much more focused on things that are not life and death and safety related.
And with the vehicles, it really is about getting as much data as possible, with a lot
of diversity as possible in getting it labeled in an accurate way in which we can feel comfortable
that we can take this model, train a system, integrate all the sensors in the controls
and put a car on the road and have it drive with humans and other cars right next to it.
There are a number of different perspectives on the right way to do autonomous vehicles
in terms of the different types of sensors.
There seems to be one world view that's very heavily computer vision focused and looks
at the camera as the ultimate end all be all sensor and there seems to be another point
of view that's a little bit more integrative and includes LIDAR and other types of sensors.
Do you guys have any perspective on that?
Yeah, so most of the work we're doing is on the image side, camera side, but really our
perspective is that in order to have a car drive like a human it needs to have the sensors
of a human, right?
So there's more than just your eyes, so that's where other sensors come in and maybe humans
don't have a built-in LIDAR system, but we do have a sensor surrounding, right?
So it's not just our eyes, but it's you sound so when you think about radar or ultrasonic
other contexts that's more 360 than just a front camera or a back camera or yeah.
Yeah, I rely pretty heavily on my spidey sense, which is about as close to LIDAR as I'm
going to get.
Sure, I mean, there's things that you pick up as you're driving, you know, like you
see a person way down the road on the sidewalk and you're going to be thinking about will
they cross or not?
Maybe that is, you know, a camera seeing that, but also the intent of which way are they
moving?
What are they doing?
Like in just what is your experience?
Like in downtown Seattle, people usually stop at the crosswalk.
Not really the case, the rest of the world.
Right, right.
Can you talk a little bit about where the service that you are providing fits into kind
of the broader pipeline that your customers are deploying?
Sure.
And maybe as a prelude to that, you can talk a little bit about the customers that you
target and any customers that you can name and kind of what they're working on.
Yeah, so we work with a variety of customers, really everybody you could picture in the
automotive space.
So that could be the OEMs, which are the car manufacturers, the tier one suppliers who
are the people who traditionally have provided parts, but are also now providing integrated
systems.
And then, you know, what we'll call disruptors.
So companies like Uber that, you know, are using autonomous driving, maybe not as their
core business, but as part of their broader offering and then startups who are purely focused
on, we've never been in the automotive space before, maybe we have some individual experience
but now we're going to go straight after kind of full autonomy.
And so that's, you know, a wide range of customers.
Most of them are starting out with a car on the road.
So I cooked with whatever sensors they have.
And then they're taking that data and the requirements coming from their research team,
which might be object detection or might be, you know, semantic segmentation, it could
be a combination of them, and then they give us their raw data, which is video or still
extracted still frames in the requirements.
And then we have to develop a workflow in order to give them back label data.
Okay.
And so object detection, that sounds pretty obvious in terms of what that means on face
value, but are there nuances that are part of the process there that folks don't generally
think of when they hear the phrase object detection?
Oh, absolutely.
I think more so in autonomous vehicles than in other spaces where it's not just about,
you know, what shape is this thing so I can decide whether it's a car or a truck, but
even when you see a truck, you might have, you know, four jeeps.
One is a male delivery van, one's an ice cream delivery truck, one's a passenger vehicle.
And those nuances actually become really important when you think about driving patterns.
A ice cream truck may have kids running out to it, you know, as it drives down the street,
a male man might be driving on the wrong side of the road and, you know, stopping very
often at mailboxes.
And, you know, who knows what a passenger vehicle might do, right, right, right.
And then you also mentioned scene segmentation, tell us about that.
Sure.
So, you know, another part of thinking about computer vision is not just, you know,
what are the objects in front of you, but really what is your context.
So in the segmentation, what we're doing is labeling really every pixel that's in the
field of view, whether that's a road or marking on the road or a vehicle or pedestrian with
different sorts or vegetation and buildings and curbs.
So really making sure that we have enough information about the different types of things
that you're looking at that you can make better decisions.
And so every individual pixel gets a label and the pixels are labeled essentially as objects
or is there a like a fixed vocabulary that you're labeling the pixels with or is it across
a, you know, a broad spectrum of objects.
So the taxonomy of labels changes based on our customer requirements, but you can think
of them in broader terms kind of as classes or as types of things.
So it's not necessarily an object, but something like the sky or vegetation or an individual
car.
And usually from that, what we're doing is labeling everything we can see and then there's
additional labeling steps afterwards.
So we might break down vehicles into like I was describing earlier, various specific types
of vehicles.
And a lot of this also changes based on, you know, the location of the footage because
terminology can change, the types of road markings can change based on what part of the world
you're in.
Hmm.
Can you take us a little deeper into how you do all this?
And in particular, I'm really interested in hearing how that's evolved from when you
were tackling the problem more broadly to doing this specifically for the autonomous vehicle
market.
Sure.
So probably the best example I can walk you through is on that semantic segmentation
that we just spoke about because it's really hard.
I mean, just thinking about the amount of time it would take to figure out how to label
every pixel in an image of regards to tools.
And so when we first started out, you know, especially with the spare five app, we were
a mobile mobile only app.
And what we've done is we we solve that platform, but we've also developed a desktop client
or a web web based desktop view.
And really that was about preference.
So some people really like working on the tablet with their fingers or with the stylist,
other people really like using the large screen.
And either way, you need to be able to zoom in and really get within a couple of pixels
of an edge when you're doing this type of drawing.
And you know, work flow wise, we learned a lot.
When we first started doing this, we said, okay, we've got a list of 75 classes of things
that are in an image.
And we built a couple tools to help you draw polygons.
So you could go click around a shape and make a closed polygon and say this belongs to
the sky or a car.
And what we found was one, it was really hard to instruct humans on 75 different things
at once.
So even if we gave them a long instructional set and quiz them, it's a lot to keep in
your mind.
Two, it takes a long time.
So drawing and labeling any one of these images from start to finish might take an hour
of your time.
And three, if you made mistakes, it was really hard to figure out where the mistake was,
how to pick up on it, how to have somebody else come in and fix it or how to have you,
you fix it.
So we're not just throwing out an hour of effort.
So we iterated on that process several times as far as an overall workflow.
And our first pass of that was, well, instead of doing 75 classes at once, we'll do one.
So let's have people focus purely on pedestrians or purely on vehicles.
And that helped a lot because it made them really focus on the instructions as far as
where to draw the lines, what accounts like do you go around the tires, do you go around
the bumpers, how tight do you have to be.
But it still is very time consuming, especially if you think about something like highway,
highway scene in the middle of rush hour, where there's 50 cars within the field of
view.
And some of them really fire out on the horizon.
And so when you switched to that, that first iterative step, did you go from a model
where you would have like one worker work on all of the various things in an image to
one where the image would kind of pass through steps and be routed to like the pedestrian
team and the tree team and the vehicle team, that kind of thing.
Yeah, that's right.
And I think it's a little foreshadowing of our next step.
But yeah, so what we had was for one person, one image, one person doing one class, so 75
people, roughly for a full image.
Right.
And then the next, what we realized was the time between these different tasks was hard
to predict.
And it was still pretty exhausting to do all every single car in a scene.
So the next iteration was really what we call recursion in our world, but it's basically
we present the image with all the previous activity that's been done to it.
So if there's 30 cars and 25 of them have already been boxed, we'll show you an image with
25 boxed cars and say, are there more cars in the picture that haven't been labeled?
And if they say yes, then we give them the drawing tool and say, draw the shape around
one of the cars.
So just do one at a time.
And so they will outline a car, they'll label it, you know, according to which kind of
class it belongs to and they'll hit next next.
To that point, you know, we've taken something that took an hour or more to do the entire
image to a short, you know, minute or two task to get it right, which allows us both to
let, you know, have that individual unit of work be reviewed, both by our automated systems
as well as by, you know, our reviewers.
And then taking that and aggregating all of those, all the individual cars and all the
individual, you know, different classes into that final composite image, just gives us
a lot more flexibility into actually how quickly things can run because things can run
in parallel as well as, you know, the quality because we have a lot finer green control
as far as what we keep and what we don't keep and even how we, how we edit things.
Hmm.
Now, a couple of questions jump out at me.
The first is have you thought about making this into like a capture?
It seems like the, if you can simplify the UI enough, it seems like the perfect task
to turn into a capture and just let people who are trying to sign into their bank or whatever
do all this work for you.
Yeah, I mean, you know, one thing is there are some really specific instructions as far
as the different types of classes and labels we want.
So in some cases like, you know, once something's been drawn and we just need to have you categorize
it, that would make sense.
But typically there's, you know, enough enough kind of context and we need to train and
instruct people on just for them to really do a good job.
Yeah, and I realized that part of the value proposition that you are bringing to the
table is that you, as opposed to what someone might be able to find with the mechanical
Turk you've got a pool of workers that you've, that you've taught how to do these kind of
classification tasks and so the accuracy is higher and things like that.
And so the, the suggestion is a little bit, you know, tongue in cheek, but it, you know,
I've been getting a lot of these captures recently that are, you know, it'll show you
a scene and it'll say, pick all the squares that have street signs in them and it makes
me wonder if it's, you know, something someone like you folks doing, you know, basically
farming out their object detection to, you know, folks that are trying to sign into websites.
Yeah, certainly.
I'm sure that, you know, recapture, which is owned by Google, that would be very obviously
a use case for them to leverage.
Right.
And, you know, what we found is, like you said, there's a lot of instruction, we have annotators
around the world.
And so we have a community, a really large community, either in 155 countries around the
world.
And really it's us communicating with them and really getting alignment so that when we
have somebody doing these tasks, the same person is going through, you know, the same workflow
multiple times and they really aren't, aren't just a stranger being shown an image and
saying, which one has a street sign, but really given really specific instructions and also
giving them a way to engage with us because inevitably as you have this large data set,
you'll run into, you know, places where the instructions need to be clarified or where
they're confused as to, well, what do I do if there's a car, like partially, including
another car, like how to, which way do I want to draw the boxes, that kind of thing that
really requires us to interact with them a lot more closely.
Okay, so my next question is, you have clearly, or will have clearly accumulated a ton
of, you know, label data sets here is the, is a future step, automate, you know, building
some AI models that automate the, some of this or, you know, for example, predict which
of the pixels are sky or put a bounding box around the sky and ask the humans to correct
as opposed to draw a new.
Yeah, that's absolutely somewhere where we're going to be focusing is how to make the process
more efficient.
So that's both some automation up front as well as assistive tools so that we can make
the community just perform better.
So right now our drawing tools are very manual where you're clicking every point in a
pixel in a polygon so that you can get a really sharp edge.
But there's no reason why with edge detection and other techniques, we can't make that
an easier process for the, for the community members as well as, as you said, if we can
take that process, I was describing that workflow of recursion where we have, you know, for
40 cars, we have 40 people going through doing each car.
If we can skip the first 25 because we can do that in an automated way, that, that'll
be great.
That's not to say we ever want to replace the human because there's a huge value to
kind of having that human eye, that human judgment because ultimately they are only going
to be as smart as the people who are training it.
But over time, if we can kind of up level them into doing more, you know, less of the
road task and more of the task that really involve a human's particular judgments, that's
where we want to get to.
Right.
I think the, you know, the safety of the autonomous vehicle is going to be, will be proportional
to the amount of data that has been, you know, properly labeled and used in training.
And so I think it just strikes me that, you know, there is tons more, you know, seen data
to be processed.
And even if you automated that easy, you know, 40 to 80 percent, there's still going
to be plenty of work for the humans to do to do that, the more difficult task.
Absolutely.
And, you know, this is an industry where 95 percent accuracy isn't going to cut it,
right?
There's lives in the line.
It's about safety.
So you're really trying to get, you know, as perfect as you can get, and that's really
going to take iteration.
That's going to take always having a human in the loop to make sure that, you know,
there isn't a misjudgment at this point where we're talking about training and validation
before we're even talking about putting this onto the road and it's the wild.
Mm-hmm.
Do you think at all about the, any of the research that's happening around adversarial examples
and there are some that are particularly focused on, I guess it's a little bit of a different
context from where you're focused since your focus is on human annotation, but there's
some research that looks at, you know, things that ways that you can manipulate images
so that a neuron that will look at a stop sign and see a giraffe or whatever, is that,
is that on your radar at all?
Yeah, you know, it's definitely an area that we've considered both from that adversarial
side and the generative kind of synthetic data side.
And really, I think the more that we're tied to what's in the wild, whether it is, you
know, running into stop signs that have been vandalized in a way that are intentionally
trying to, you know, confuse models and vision systems or whether it's about getting a greater
diversity.
You know, one of the biggest challenges that auto manufacturers or people who are focusing
on autonomous driving have is just how different, you know, scenarios are around the world.
And also rare cases.
So it's not just about our stop signs being in a different language or a different shape
or the road markings being different, but even the types of vehicles we see on the road,
right, like a pickup truck in the U.S. is pretty different from a pickup truck in parts
of Asia, right?
And, you know, there's a lot of rare cases a few months ago here in the Northwest, there
was like a tractor trailer that turned over in the middle of the highway with a bunch
of like, slime eels in the back.
Right, I remember that.
And, you know, it's like, how would, you know, what would you do if you were the car behind,
you know, the autonomous vehicle behind that truck as that happened?
And currently, even with hundreds of thousands of hours of footage, the odds of getting something
like that on tape is going to be difficult, right?
So, or low.
So really, I think there is a balance of how do we augment what we have with other scenarios
and other things to get that bigger picture of what could possibly happen?
Hmm.
Along those lines, you know, granted that for a lot of the companies in this space, their
data is a core element of their IP and ability to differentiate.
But are you aware of any movements to create like data consortia, for example, where, you
know, OEMs would contribute their data with, you know, all in the agreement that they
would get data back so that, you know, they may have cars operating in North America
and, you know, they can contribute their data and get access to data from, you know, that's
based in other geographies, and is that something that, A, is that something that is, you
know, happening that you're aware of, or, and B, is that something that you might be
able to help facilitate?
Absolutely.
So I think right now, it's, you know, there's a lot of secrecy in this industry, so everybody
keeps their images, their data, even their requirements, as far as what they're labeling
pretty close.
But there are, there are starting to form more partnerships, companies working together.
I think both for the reasons you described, as well as just, you know, everybody's working
on slightly different angles.
So if they can leverage each other's to build a solution and come to market sooner or
be the first, I think they're going to, you know, embrace that.
And where we can fit in is we, there's certainly a place in which we can leverage the data
that we've already labeled and help people distribute that and manage that.
So we're not duplicating as much effort, but we are really thinking about how to build,
you know, a really useful kind of full data set.
Right, right.
So let's maybe dive back into the, you know, the process and, and the lessons learned
and how that's expressed itself in technology that you've developed, anything else in terms
of specifics, you know, things that you've observed specific to the autonomous vehicle
market?
Sure.
You know, a lot of things I think could fit a broader market, but really by focusing
here, it's allowed us to dive deep and not be, you know, distracted by what's going
on in linguistics and natural language processing versus, you know, different parts of robotics
and vision.
But, you know, all of these, all of these approaches that require humans require a lot
of, you know, management of the humans, right.
So as far as really working to make sure that we can translate requirements to something
can be understood, making sure that we understand when people are making mistakes,
what is the, what is the reason behind it?
There's actually, you know, a lot of psychology to, you know, why, why do we get bad data?
Is it because people are being fraudulent?
Is it because we didn't explain it right?
Is it because we didn't even think about the scenario or is it because we explained it
in a way that they're actually being consistent with what we told them to do, but we were, you
know, we were wrong or we misunderstood something.
So it's a really iterative process.
It's, it's not something where you can just say there's a one-size-fits-all tool dropping
your data, use a generic community and get, get good data out.
And we've definitely, I think, learned that more than anything over the past few years
as far as how much we need to understand really specific requirements, as well as how
those fit with data that changes over time.
And how do you, how does your platform express those requirements?
Are they kind of hard-coded in for each project that you take on or do you have element
of the platform that's like a rules engine or something like that?
I'm trying to wrap my head around how I might implement something like that.
So, you know, it's a combination of many things.
So as I said, it's been an iterative process over the past few years as far as us developing
it.
On the instructional side, you know, we spent a lot of time on the instructional design
as far as just making sure that once we internally have understood all the requirements and translated
them into something that our community can understand, we're both giving them enough information
in small enough pieces that they can understand how to use the tool, how to follow instructions
for very specific tasks for a particular customer.
So, you know, even the definitions of how to box or how to, you know, draw a shape around
a vehicle might change from project to project.
And so, making sure that within the context of what they're doing, we're constantly reminding
them of the exact rules and then, you know, and testing them.
So we do have ways to, you know, inject known task and make sure that they are meeting
the right accuracy level as well as getting feedback constantly so we can tell them, you know,
you're doing a great job at, you're drawing, but your labels are consistently or sometimes
off in some way.
Like you keep categorizing a, you know, a box van as a pickup and really they're two different
types of things.
So, you know, we try to have as much feedback as we can as well as the upfront instructions.
And the upfront instructions, it's really, it's written, it's showing photographs and
showing them examples of good and bad and then it's even sometimes going in and producing
videos that really talk about a nuanced detail that is easier to express with words and
in motion than it is with just a, you know, a paragraph and an image.
And part of that too is, you know, we have a international community so making sure that
we're conveying these in the language they understand, you know, there's no reason why
some of these tasks can be done better by one language or one community than another.
And it's really up to us to make sure that we're, we're opening it to the right people.
If we have a community that as it speaks, you know, is natively Spanish speaking, if you
give them very nuanced technical instructions in English, it's going to be harder to understand
than if we give it to them in Spanish, for example.
So that's the type of thing that we have to think about whenever we're doing our targeting
as far as who's going to have access to this task as well as making sure that, you know,
between us and our customers that there's alignment for a given task and to be more specific
for a given scene and an object within a scene, how much redundancy is there in the process
meaning, you know, for a given frame of a video, how many times you're asking someone
to label a given object before you have that confidence level that it's done correctly.
Is there a ton of redundancy in the process or have you managed to kind of filter that
out?
There's not a ton of redundancy.
What early on in the process, we may have multiple people doing tasks in order to get
a better understanding for the types of differences we'll see as people do the task.
But ultimately, that's part of what makes our system work really well is that we get
more efficient over time.
We have less people doing it over time.
Mm-hmm.
Okay.
Yeah, so unlike a traditional crowdsourcing model where your only quality control mechanism
is looking for consensus or asking, you know, 10 people and saying, six of them agreed
so that must be the right answer.
We've tried to be a little bit more intentional and intelligent about how we make these decisions
using our reputation engines and some of our other internal models.
Okay.
Now, this is maybe something that I should have asked earlier, but do you have any, can
you share any data points that can help us contextualize the scope of the challenge
within the autonomous vehicle space or, you know, the volume of data in that space or
that you're focused, that you're working with in particular?
Sure.
So, you know, right now there's a few cars on the road.
I think it's the easiest way to think about it, you know, all these, each company has
a handful of cars at best collecting data.
And even any one of those cars might be collecting, you know, terabyte of video a day.
And most of that doesn't need to be human labelled.
But there is, you know, significant volume, especially when you think about the diversity
problems we were talking about earlier as far as that's one car on one road or one set
of roads in one area of the world, you know, in the valley or in Germany or in Michigan.
And so really, as these fleets develop, that's just going to scale exponentially, right?
We're going to have both the test fleets, which will be hopefully located around the world
in collecting different types of data, so not just images, but light art and other sensors.
And then when we get into production, where we're going to start looking really for validation
and feedback loops, especially when, you know, a system gets triggered.
So for talking about an event-based system and we have emergency braking triggered, you're
going to want to have a human validate was that the right, you know, right thing to do
or not.
And so I think over time, we end up with more and more use cases that are going to require
human insight, even beyond just the raw data that's being captured.
And part of the art will be figuring out what to annotate or what things need better labeling
or what things don't, because obviously we can't take petabytes of data a day and process
that in a meaningful way that's going to really improve things.
Alright, do you often get tasks that are incremental in nature, meaning you've got, as opposed
to processing all of the scenes or objects within, or all of the objects within the scene,
the particular use case calls for only, you know, only the road dividers or signs or
things like, it sounds like that's a typical thing for you to do, is that true?
Yeah, definitely.
I mean, it's actually, might be telling about what parts of the problem any one customer
is focusing on a given time, right?
So lane markings are obviously, you know, a very discrete task as far as, you know, looking
at exits in dashed line, solid lines, road boundaries, and then pedestrians would be
another really good example, as far as trying to understand what, you know, urban scene,
where the pedestrians are located and how they're moving over time.
So are they, you know, likely hit somebody to cross the roadway or cross in front of
the vehicle or just stand around to, you know, to bus stop, we kind of have to understand
that that's a bus stop where people just stand, they're not going to cross the street,
they're not going to move anyway, they'll disappear magically, you know, in a couple of
frames after the bus passes by.
You know, there's things like that that I think really are individual areas of focus.
So beyond the general kind of computer vision, like building a better eye for the camera
is, is building, you know, context and building semantic understanding that I think are involved
involved more of these discrete tasks.
So do you do any labeling, do you do any, for lack of a better term on thinking of this
like first derivative labeling, like as opposed to saying, you know, that's a pedestrian labeling,
the pedestrian is walking in direction, you know, X or at a speed that you can calculate
based on the, you know, the timestamps on different images and things like that.
Yeah, so we definitely do tracking across video.
So in that, that's actually, there's two ways to do that, right?
You can either derive it from two still frames or you can play a video, which can sometimes
be helpful as far as understanding what else is going on in the frame and just getting all
that data at once.
So having one person view five seconds can, you might give you more information as far as
like if the rate of movement changes, like person's walking, then the, you know, the cross-signal
turns to a blinking hand, they start walking faster or something like that.
It's, you know, it's kind of helpful to see that happen or, you know, cars turning into
their lane and so they stop in the middle of the road.
Like, it's a little harder to see that when you're talking about an individual frame one
at a time, even if you're trying to piece that data back together.
And there are definitely nuances where it's not just a box around a person, but like I said,
it's what kind of person, you know, are they, do they have a stroller?
You know, are they, are they walking?
Are they, are they distracted in some way?
And then their orientation, so what direction are they moving?
All of that metadata kind of feeds into it where you end up with an annotation that's
not just an image with a box in coordinates, but it's an image with a box with a coordinate
so it has a lot of metadata that might be related to this point in time versus the same
image, you know, later point in time that has a lot of shared metadata, but also certain
things change.
And it sounds like you're also able to uniquely identify and track not just a person in
a box, but, you know, person X in a box, you know, in frame one across, you know, all of
the frames in a segment in which they're visible.
Yeah.
I mean, that's hugely important, right?
To have that instance level kind of tracking.
So you can say our car is changing lanes or somebody crossing the street or is it just
that there's different people throughout the scene, right?
So it's really important to know that kind of tracking.
Do you have a sense for who is kind of leading the field in terms of data collection?
You mentioned that, you know, most of the folks that are doing this have, you know, one
or two cars out there, but you know, certainly Google's got more cars, at least they've got,
you know, they've got a lot of cars that they've instrumented for maps that are capturing
some of the same types of data and, you know, Tesla's got a lot of cars out there with
cameras mounted and who's your, what's your sense for who's, you know, got the most
data, visual data on vehicles, real life, you know, in the wild vehicles.
Well, you know, I don't know that I would name a particular company, but really think
about companies that have vehicles in the wild, which might be, you know, manufactured
in production vehicles or could be fleets.
So certainly there are companies that are trying to, say, distribute dash cams across, you
know, consumer market so that they can capture video and use it for building, you know, autonomous
systems while also providing value to the end customer.
There's also things like taxis or ubers where there's an inherent value of that data to
the driver.
So there's a reason for them to put this device in their car, but there's also the value
of the data collection.
So I think ultimately there's going to be a couple of different strategies.
It's not necessarily going to be, you have to produce cars and get them out there.
Right.
You have to produce a way to collect this data that's meaningful for people so that they're
willing to do it.
I've not seen the, you know, free dash cam if you give us the ability to, you know, use
the data is who, do you have a, you know, specifically someone who's doing that?
There are a couple of companies I can find the names for, for you later a little bit later.
Okay.
There's one company called Nexar that has a dash cam app.
There's another company that's doing it specifically around ride sharing.
So that's inside outside camera, right?
So the idea of being, you're going to have, you know, you're going to see your customers
in the back in case there's any situation where you have an abusive customer or an accident,
you need to have that liability coverage as well as you've got your, you know, your front
camera for, for actions and that kind of thing.
In formulating that question, I hadn't even really thought about all of the dash cams
and the various cameras that are mounted on public safety vehicles and utility fleets
and things like that.
There's just a ton of image data out there.
Yeah.
Well, you know, if you think about companies that have been working on mapping for a long
time, like you mentioned the Google Street View and the Google Maps cameras, but even
beyond that, you know, any, any fleet or any, you know, all of us who can carry a smartphone
in our, in our pocket, so have some apps running in the background with location awareness.
You know, that's all valid data as far as understanding kind of movement patterns in
that, you know, that alone might not be enough with that in tandem with, with the camera
becomes hugely valuable in that in tandem with, with high, high def maps can tell you
when there's patterns that are changing.
Is anyone doing anything as far as integrating and visual data collected via drones in
the space?
You know, I think that's a whole separate field as far as on the mapping side, for sure,
on the actual vehicle driving systems, not that I know of.
And is that because you really need to kind of be, you know, for the visual data to be,
to have the unique perspective of the vehicle to be useful or because, you know, just hasn't
happened yet?
I think it's probably a little of both.
So certainly companies think that one of their unique advantages is not just the footage
they're collecting, but the way they're collecting it, whether it's using multiple sensors
in a certain way, in certain positions.
So you know, where do they locate their cameras?
Are they using a stereo camera?
Are they using, you know, side cameras as well?
Wide field of view camera in tandem.
So I think there is value to that, you know, uniqueness, but also, you know, I think we're
just going to figure out what the right combination of data is.
Obviously, you know, the more we can get the better, and for certain things like figuring
out, you know, a big picture view of your current area, it would be great if you had something
flying above you the whole time that could see, you know, a wider, further distance and
a wider range than your eyes or your front cameras might see.
Interesting.
Interesting.
What questions should I be asking that I might not have asked yet?
Are there other thing areas that we might want to dig into before we start to wrap things
up?
Well, you know, I think we've covered a lot of a lot of good topics.
I think when we talk about how people approach this problem, so it might be interesting.
Before, you know, before my DAI, when you talk about other crowdsourcing or you talk about
like doing it yourself, one of the biggest challenges is about the quality control, like
I said, the instructions and all of that.
But even when you're trying to do it in-house with your own team who knows all the requirements
and all the instructions, it's really about that scale and diversity.
And so I think, you know, really iterating that or kind of thinking about the fact that
this data in Seattle is different from the data in Detroit, which is different than the
data in Stuttgart or Singapore or any of these parts of the world.
I think that's pretty key.
So I mean, like you asked earlier about how to distribute, you know, how to get collect
more data, it's not just like drive the same car in the same route over and over and over
and over again.
Like you do need to do that for a little bit.
Right, right.
But really it's about the diversity and then the understanding because, you know, your
labelers in the U.S. might not even recognize what does it mean when I see a zigzag line
on the road, on the side of the road in Europe, where, you know, we might go, oh, that's
a no parking zone or that's, you know, that's a merge area, right?
Like there's, so there's so much like context and so much localization that I think
is easy to overlook.
Even if you're somebody who's traveled and you know that you need to go learn the rules
of the road somewhere else, it's like a lot of things that a human can adapt to that.
If you're thinking about a system that really is just looking at what it sees and not having
that higher level understanding, it's a really hard problem, right?
Like I've never seen the zigzag road is that I mean, I have to like, you know, slalom
my way down the road, right, as a car or does it just mean, like, stay away from that
line, right?
But the car figure out wouldn't be surprised to see some autonomous vehicle see one of those
markings and just go go crazy as far as, you know, what it's supposed to do.
Yeah, it's interesting.
There's so much of this problem space that is, you know, the benefits from, from having
that intelligence and the human or the computer intelligence and the human intelligence
kind of melded together, right?
So it's not just this training data labeling problem that we've talked about.
You've made a very strong case for the power of combining human insight with automated,
you know, automated tools, but even within the vehicle itself, there's a, there are folks
doing research on, you know, how the car can benefit from the input of just looking at
the driver and, you know, understanding what their, you know, state is what they're looking
at.
Things like that.
Absolutely.
Yeah, I think ultimately when we get, when we get to a point where we have full autonomy,
we're going to be in a safer world, right, where we don't have distraction or distraction
doesn't matter, you know, it's okay to sit there and start your phone if you're not the
one driving and you don't need to be the one who's ready to grab the wheel.
So we're talking about early stages where you need to be attentive and have your hands
on the wheel or close to it.
But as we get further down the road, you know, you take the best of the humans, which
is the judgment and the vision and the decision-making processes, and you take away, you know,
the fatigue and the distraction and the things that, you know, are going on in our lives
and make it hard for us to stay focused, and I think you're going to end up in a better
place.
Awesome.
Well, that is the hope and, you know, the vision behind autonomous vehicles for is, you
know, as much as people talk about, you know, for example, the economic issues associated
with deploying a bunch of autonomous vehicles in terms of labor and things like that, the,
you know, the promise to stave off, you know, the, you know, huge numbers of vehicular related
deaths, you know, that occur around the world.
That's just huge.
Yeah, absolutely.
Well, awesome.
Well, I really enjoy this conversation.
Thank you so much for taking the time, and I really appreciate, you know, getting a
chance to catch up with, with my DAI and hear about what you're doing in this space.
Great.
Thank you, Sam.
Have a great day.
Thanks, Darren.
All right, everyone.
That's our show for today.
Thanks so much for listening, and of course, for your ongoing feedback and support.
For more information on Darren, or any of the other topics covered in this episode, head
on over to twimlai.com slash talk slash 57.
To keep track of this autonomous vehicle series, visit twimlai.com slash AV2017.
Please, please, please remember to send us any comments or questions you may have for us
or our guests via Twitter, at Twimlai, or at Sam Charington, or leave a comment on the
show notes page.
Thanks again for listening, and catch you next time.
