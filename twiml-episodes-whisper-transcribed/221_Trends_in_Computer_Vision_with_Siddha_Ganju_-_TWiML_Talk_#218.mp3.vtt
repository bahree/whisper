WEBVTT

00:00.000 --> 00:16.320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

00:16.320 --> 00:21.480
people, doing interesting things in machine learning and artificial intelligence.

00:21.480 --> 00:35.920
I'm your host Sam Charrington to close out 2018 and open the new year we're excited

00:35.920 --> 00:40.640
to present to you our first ever AI rewind series.

00:40.640 --> 00:45.040
In this series I interview friends of the show for their perspectives on the key developments

00:45.040 --> 00:49.720
of 2018 as well as a look ahead at the year to come.

00:49.720 --> 00:54.320
We'll cover a few key categories this year, namely computer vision, natural language

00:54.320 --> 00:59.200
processing, deep learning, machine learning and reinforcement learning.

00:59.200 --> 01:03.840
Of course we realize that there are many more possible categories than these, that there's

01:03.840 --> 01:08.800
a ton of overlap between these topics and that no single interview could hope to cover

01:08.800 --> 01:12.240
everything important in any of these areas.

01:12.240 --> 01:17.120
Nonetheless we're pleased to present these talks and invite you to share your own perspectives

01:17.120 --> 01:26.000
by commenting on the series page at twimbleai.com slash rewind 18.

01:26.000 --> 01:31.240
In this episode of our AI rewind series, we're excited to have Sida Gondu back on the show.

01:31.240 --> 01:36.480
Sida, who is now an autonomous vehicle solution architect at Nvidia, shares her thoughts on

01:36.480 --> 01:40.040
trends in computer vision in 2018 and beyond.

01:40.040 --> 01:45.320
We cover her favorite CV papers of the year in areas such as neural architecture search,

01:45.320 --> 01:50.600
training from simulation, application of computer vision to augmented reality and more,

01:50.600 --> 01:54.160
as well as a bevy of tools and open source projects.

01:54.160 --> 01:55.160
Enjoy.

01:55.160 --> 02:00.640
Alright everyone, I've got Sida Gondu on the line.

02:00.640 --> 02:07.160
Sida is a solution architect working on autonomous vehicles at Nvidia.

02:07.160 --> 02:11.120
Sida, welcome back to this week in machine learning and AI.

02:11.120 --> 02:12.120
Thank you.

02:12.120 --> 02:15.280
It's been a great year so far and thank you for having me once again.

02:15.280 --> 02:16.280
Absolutely.

02:16.280 --> 02:17.280
Absolutely.

02:17.280 --> 02:24.200
So for folks that want to go back to your original podcast on the show, it was twimble talk

02:24.200 --> 02:25.520
number 95.

02:25.520 --> 02:31.720
At the time you are, you were at deep vision, but you've since moved over to Nvidia.

02:31.720 --> 02:32.720
Yeah.

02:32.720 --> 02:35.280
What will you be working on at Nvidia?

02:35.280 --> 02:42.520
So at Nvidia, I'm focusing on a particular part of the entire self-driving stack, which

02:42.520 --> 02:48.520
is called simulation and resimulation, which is the part that does the testing and the

02:48.520 --> 02:51.640
verification of the entire perception stack.

02:51.640 --> 02:57.520
So how do we know that the computer vision, the lidar, the radar networks that we have

02:57.520 --> 03:02.760
trained, how do we know that they actually work well in real life because real life testing

03:02.760 --> 03:07.800
for autonomous vehicles is very hard to do because you can't test out the millions and

03:07.800 --> 03:15.320
millions of possibilities on actual roads with real test drivers inside the vehicles.

03:15.320 --> 03:20.920
So simulation is a develop a friendly way to do that.

03:20.920 --> 03:24.880
And it's also, you know, you can squeeze millions and millions of miles.

03:24.880 --> 03:29.200
You can add tens and thousands of scenarios to simulate and test.

03:29.200 --> 03:33.480
So it's really a nice solution to testing.

03:33.480 --> 03:40.000
And so you're joining us to represent computer vision in our Year in Review and Prediction

03:40.000 --> 03:41.720
series.

03:41.720 --> 03:43.320
So let's jump right in.

03:43.320 --> 03:51.000
So you took the time to kind of prepare your thoughts on some recent, some papers this

03:51.000 --> 03:57.480
year that struck you as particularly noteworthy in the computer vision space.

03:57.480 --> 03:58.760
What do you want to start?

03:58.760 --> 04:03.440
So before I start, I think I want to say that, you know, I'm only human and I'm probably

04:03.440 --> 04:09.000
missing tons of game-changing papers that came out this year and, you know, through this

04:09.000 --> 04:14.360
discussion, we're probably just scratching the surface of this year's CV research.

04:14.360 --> 04:19.680
Now that being said, I'm going to start with a couple of papers that are providing different

04:19.680 --> 04:25.040
solutions to problems rather than improving on existing tasks.

04:25.040 --> 04:31.880
So the very first paper that I really, really liked was called learning transferable architectures

04:31.880 --> 04:34.360
for scalable image recognition.

04:34.360 --> 04:36.720
And this is by Google Brain.

04:36.720 --> 04:43.240
Now this is being projected as the future of deep learning and the main reason behind

04:43.240 --> 04:49.880
that is because it introduces something called neural architecture search or NAS.

04:49.880 --> 04:57.120
So you know, Sam, you've been doing these podcasts definitely more than 95 podcasts by

04:57.120 --> 04:58.120
now.

04:58.120 --> 05:05.760
And I'm sure during your discussions, a lot of people have talked about how designing

05:05.760 --> 05:09.440
a network architecture is a big pain point.

05:09.440 --> 05:13.600
And especially for those people who are just entering the field.

05:13.600 --> 05:17.080
So neural architecture search is really the beacon of light for them.

05:17.080 --> 05:22.600
It's a network that searches for the best model structure instead of you manually designing

05:22.600 --> 05:24.560
the network architecture.

05:24.560 --> 05:30.600
Now internally, the search for this ultimate model is based on a reward function that rewards

05:30.600 --> 05:33.360
the model for performing well on the data set.

05:33.360 --> 05:39.400
It has been validated on and at the same time, it's using the same metrics to validate

05:39.400 --> 05:43.080
and test the ultimate network.

05:43.080 --> 05:48.600
So I really like this paper because the details that such architectures achieve better accuracy

05:48.600 --> 05:51.200
than manually designed models.

05:51.200 --> 05:57.320
And searching also allows the network itself, you know, a more brute force type coverage

05:57.320 --> 06:00.480
over the entire service space.

06:00.480 --> 06:06.160
Now we can only imagine the huge benefits that a good NAS algorithm can give, rather than

06:06.160 --> 06:11.520
hand designing a specific network for the millions and millions of specific applications

06:11.520 --> 06:13.920
that we might want to develop.

06:13.920 --> 06:19.000
So this is actually one of its biggest advantage that it is generalizable.

06:19.000 --> 06:24.760
So a well-designed NAS algorithm is supposed to be flexible enough to find a good network

06:24.760 --> 06:27.840
for any specific task.

06:27.840 --> 06:33.400
And within the paper, they've outlined both detection and classification.

06:33.400 --> 06:39.760
So on ImageNet, they reported a 1.2% improvement in top 1% accuracy.

06:39.760 --> 06:46.640
And this is compared to what I guess we can now call the best human invented architecture.

06:46.640 --> 06:53.360
And the coolest thing is that this architecture has 9 billion fewer flops.

06:53.360 --> 06:58.760
Can you imagine the improvement in speed in real-life production systems that we can get

06:58.760 --> 06:59.760
from this?

06:59.760 --> 07:05.800
I mean, NAS is bringing a whole new definition of real time.

07:05.800 --> 07:12.440
And on C for 10, I think NAS net achieves a 2.4% error rate, which is the new state of

07:12.440 --> 07:14.160
the art.

07:14.160 --> 07:20.720
And on the detection side of things, the features learned by NAS net from ImageNet classification

07:20.720 --> 07:27.040
combined with foster R C and N gives a new state of the art, which has a 4% improvement

07:27.040 --> 07:32.080
on the previous state of the art paper.

07:32.080 --> 07:39.600
So NAS is something really exciting, and I expect a lot of people entering the field to really

07:39.600 --> 07:45.760
go and test it out because it gives them a huge advantage of not wasting hours and hours

07:45.760 --> 07:51.200
trying to think what is the ideal architecture for their specific application.

07:51.200 --> 07:56.920
The paper talk at all about the training time to achieve that kind of performance relative

07:56.920 --> 08:03.360
to the training time for a single static architecture.

08:03.360 --> 08:11.360
So when we talk about a single static architecture, it's not like we fix a single architecture and

08:11.360 --> 08:16.360
we stick to it throughout the experimentation pipeline.

08:16.360 --> 08:24.280
As a deep learning engineer, I often start with one architecture and go back and forth

08:24.280 --> 08:29.200
on every single aspect of that architecture to get to the final architecture that is put

08:29.200 --> 08:30.520
into production.

08:30.520 --> 08:37.400
So I think comparing the times between what NAS will do versus what a single architecture

08:37.400 --> 08:41.240
does isn't really an apples to apples comparison.

08:41.240 --> 08:46.440
And that's kind of what I was getting at, the training time to train a single resident

08:46.440 --> 08:52.320
or something like that on ImageNet while it's decreasing can still be significant for

08:52.320 --> 08:58.480
some types of problems and so I'm imagining that kind of searching through solutions like

08:58.480 --> 09:02.440
that is going to be even more complex.

09:02.440 --> 09:06.880
But you're saying that the comparison isn't really, you know, against, you know, just

09:06.880 --> 09:13.760
training, but also the time that the, you know, that a human would typically spend in coming

09:13.760 --> 09:15.760
up with the architecture.

09:15.760 --> 09:16.760
Absolutely.

09:16.760 --> 09:17.760
Yeah.

09:17.760 --> 09:23.120
So that was the learning transferable architectures paper, what else do you have for us?

09:23.120 --> 09:24.120
Mm-hmm.

09:24.120 --> 09:25.120
Yeah.

09:25.120 --> 09:32.000
So the next one is from Nvidia and it's called training deep networks with synthetic

09:32.000 --> 09:37.080
data bridging the reality gap by domain randomization.

09:37.080 --> 09:44.720
So through this paper, Nvidia tries to solve an age old problem, which is can we pre-train

09:44.720 --> 09:52.400
with synthetic data for tasks that demand either expert knowledge or labels that were difficult

09:52.400 --> 09:59.120
to specify manually or images that were difficult to capture in large quantities with, you

09:59.120 --> 10:03.680
know, a huge variety in many such unanswered questions.

10:03.680 --> 10:12.840
So Nvidia built a plugin tool for Unreal Engine 4 UE4 that simulates images for training

10:12.840 --> 10:18.680
data and uses that to train convolutional neural networks.

10:18.680 --> 10:27.440
Now the biggest plus point is that all the factors spanning the main object to structures,

10:27.440 --> 10:35.560
viewpoints, all these factors that the training data depends on are randomized and automated,

10:35.560 --> 10:40.800
making it so much more easier and useful to develop a data set.

10:40.800 --> 10:47.800
So when we talk about the number, variety, texture of objects, the background, then for

10:47.800 --> 10:55.080
destructors, a huge variation in their number, types, colors, scales.

10:55.080 --> 10:59.960
For the camera, you can change the camera location view point, be it the virtual camera with

10:59.960 --> 11:06.040
respect to the scene or the angle of the camera with respect to the scene or the number

11:06.040 --> 11:09.120
and the location of point lights.

11:09.120 --> 11:15.520
An interesting thing that they reported was that with additional fine tuning and real data,

11:15.520 --> 11:20.680
this network yields better performance than using real data alone.

11:20.680 --> 11:27.720
So this result really opens up the possibility of using inexpensive synthetic data for training

11:27.720 --> 11:29.720
neural networks.

11:29.720 --> 11:36.760
I mean, imagine there are millions and millions of applications that don't have concentrated

11:36.760 --> 11:43.280
data collection efforts or maybe collecting data is just too difficult for certain types

11:43.280 --> 11:45.160
of applications.

11:45.160 --> 11:51.600
So such an example of using synthetic data for training would be a game changer in these

11:51.600 --> 11:52.600
cases.

11:52.600 --> 12:00.240
Now, I remember hearing about papers that showed that synthetic data wasn't all that

12:00.240 --> 12:07.840
effective at training agents that can perform in the real world, but results like this are

12:07.840 --> 12:10.360
changing that is that.

12:10.360 --> 12:20.280
It has 2018 been kind of a key year in our ability to incorporate synthetic data into training

12:20.280 --> 12:27.880
or is this paper building on successes that we've kind of seen in recent years?

12:27.880 --> 12:35.720
I think in 2018, we had a lot of papers coming out on simulation or simulating data or synthetic

12:35.720 --> 12:42.560
data and different papers write out different techniques to how do you develop the data?

12:42.560 --> 12:49.920
How do you, for example, change the surface or the material of an object?

12:49.920 --> 12:52.080
How do you make sure it looks different from the background?

12:52.080 --> 12:57.840
So there are a lot of papers in this synthetic data field coming up and a lot of them

12:57.840 --> 12:59.240
achieve good results.

12:59.240 --> 13:04.280
And I was talking about this one because they've made it so much more easier and useful

13:04.280 --> 13:06.360
because they have a plug-in tool.

13:06.360 --> 13:11.640
You know, they've got so many options to change from, for example, the object, the background,

13:11.640 --> 13:14.400
the distractors, the viewpoints.

13:14.400 --> 13:22.320
It's, you know, if you can imagine a Photoshop kind of a generator that, you know, that can

13:22.320 --> 13:28.360
spit out image frames based on how you're coding it up.

13:28.360 --> 13:30.000
It's really similar to that.

13:30.000 --> 13:31.920
So what's next on your list?

13:31.920 --> 13:38.560
So this is a really fun but extremely amazing piece of research and it comes from University

13:38.560 --> 13:44.640
of Washington that makes for more commonplace AI applications and it's called soccer on

13:44.640 --> 13:46.600
your tabletop.

13:46.600 --> 13:52.120
So they've developed a system that takes, as input, a YouTube video of a soccer game and

13:52.120 --> 13:58.040
the system outputs a dynamic 3D reconstruction of the game that can be viewed interactively

13:58.040 --> 14:02.800
on your tabletop with an augmented reality device.

14:02.800 --> 14:09.320
So you know how people right now watch matches on their mobile devices, tablets, laptops,

14:09.320 --> 14:12.080
television sets and so on.

14:12.080 --> 14:16.280
Imagine watching it on your dining table, your work table, your kitchen counter literally

14:16.280 --> 14:20.000
everywhere or anywhere for that motto.

14:20.000 --> 14:22.280
So this system is really multimodal.

14:22.280 --> 14:29.040
They are combining different types of information, for example, bounding boxes, poses, trajectories,

14:29.040 --> 14:35.200
all extracted from the player to segment them and these 3D segments are projected onto

14:35.200 --> 14:40.400
any plane which becomes the virtual soccer field.

14:40.400 --> 14:48.640
And they've released an example video of this and on YouTube and it's really cool and

14:48.640 --> 14:51.760
I encourage all of you to check it out.

14:51.760 --> 14:53.400
It's called soccer on your tabletop.

14:53.400 --> 14:58.360
Oh, that sounds incredible, I haven't made that I haven't come across that video.

14:58.360 --> 15:00.800
Yeah, you said definitely check it out now.

15:00.800 --> 15:07.920
So the next one, let's talk a little bit about vision and language coupled together.

15:07.920 --> 15:13.600
So before we talk about the papers, I think it's worth mentioning the second edition of

15:13.600 --> 15:21.280
the VQA Challenge dataset, VQA 2.0, which actually was released in 2017 but it's worth talking

15:21.280 --> 15:28.040
about it because it was a much more balanced dataset and it reduces the language prices

15:28.040 --> 15:33.560
over VQA 1.0 and it's double the size of VQA 1.0.

15:33.560 --> 15:41.200
So the very first paper in VQA is learning to count objects in natural images for visual

15:41.200 --> 15:42.880
question answering.

15:42.880 --> 15:46.880
This is by Jan Zang and others.

15:46.880 --> 15:56.320
So this paper focuses on developing a counting solution for VQA and they use mostly attention

15:56.320 --> 15:57.640
for that.

15:57.640 --> 16:02.200
They've proposed a differentiable counting component which explicitly counts the number

16:02.200 --> 16:10.520
of objects based on a hand design architecture using a graphical object proposals and non-maximum

16:10.520 --> 16:12.360
suppression.

16:12.360 --> 16:17.520
So it's basically just applying non-maximum suppression on object proposals and counting

16:17.520 --> 16:21.720
the number of objects that are within the image.

16:21.720 --> 16:24.560
What is non-maximum suppression?

16:24.560 --> 16:29.880
So it's an algorithm for mostly use for detection.

16:29.880 --> 16:38.040
So this method improves the baseline by about 6.6% on the counting questions.

16:38.040 --> 16:45.440
And I don't remember the statistics in VQA 2.0, but in the VQA 1.0, the counting was

16:45.440 --> 16:51.560
a very big problem and it was extremely biased because most questions, for most counting

16:51.560 --> 16:54.840
questions, the answers was usually two.

16:54.840 --> 16:58.560
So boosting a baseline by 5% is huge.

16:58.560 --> 17:01.080
So I really like this paper for that.

17:01.080 --> 17:08.040
And so with this paper, is the idea that this model that they've developed for counting

17:08.040 --> 17:15.760
would be used as kind of like a submodel for a broader VQA system?

17:15.760 --> 17:16.760
Yes.

17:16.760 --> 17:18.360
So I was actually coming on to that.

17:18.360 --> 17:24.800
The next paper, which is bilinear attention networks for visual question answering, they

17:24.800 --> 17:32.680
have integrated the counting module that we just talked about from Zang, and along with

17:32.680 --> 17:41.360
that, they've introduced other techniques to improve their VQA accuracy.

17:41.360 --> 17:46.240
And this paper is called bilinear attention networks for visual question answering.

17:46.240 --> 17:50.640
It's by Jean Joachim from SEAL National University.

17:50.640 --> 17:53.360
They are using the counting module.

17:53.360 --> 18:00.720
They are also using bilinear attention, which is the interaction between the word and the

18:00.720 --> 18:02.680
visual concepts.

18:02.680 --> 18:10.000
They also have a low rank bilinear pooling and they have residual learning with attention

18:10.000 --> 18:13.880
mechanism for incremental inference.

18:13.880 --> 18:16.160
That sounds a lot.

18:16.160 --> 18:23.320
And I think I'm not sure, but I think this was among the top VQA 2.0 winners.

18:23.320 --> 18:31.240
And then one paper that is one model that is right for the right reasons is women also

18:31.240 --> 18:35.320
snowboard overcoming bias in captioning models.

18:35.320 --> 18:41.120
So they've introduced a new equalizer model that encourages equal gender probability when

18:41.120 --> 18:47.480
gender evidence is occluded in a scene and confident predictions when gender evidence

18:47.480 --> 18:48.880
is present.

18:48.880 --> 18:54.400
So they're basically forcing the resulting model to look at the person rather than use

18:54.400 --> 18:59.080
contextual cues to make a gender specific prediction.

18:59.080 --> 19:05.320
And they've also introduced two kinds of losses, which is the appearance confusion loss and

19:05.320 --> 19:07.480
the confident loss.

19:07.480 --> 19:12.040
Both of them are generalizable and they can be added to any description model in order

19:12.040 --> 19:16.960
to mitigate the impacts of any unwanted bias in the description dataset.

19:16.960 --> 19:21.880
And when we talk about their results, this research gives two things.

19:21.880 --> 19:30.640
They get a lower error than existing work when describing images with people that mentions

19:30.640 --> 19:37.280
the gender and it much more closely matches the ground truth ratio of sentences that include

19:37.280 --> 19:41.000
women to sentences, including men.

19:41.000 --> 19:48.200
And the second achievement is when we visualize the results, we see that the model is actually

19:48.200 --> 19:53.640
looking at the people when predicting their gender, which is really important because a lot

19:53.640 --> 20:02.000
of visualization results, you know, they focus on, they don't necessarily focus on the

20:02.000 --> 20:04.600
people when they are predicting the gender.

20:04.600 --> 20:10.400
So this was a really interesting find by this paper, women also snowboard overcoming

20:10.400 --> 20:12.080
bias and captioning models.

20:12.080 --> 20:14.680
That one sounds really interesting.

20:14.680 --> 20:21.560
The next one is weekly supervised photo-enhanced for digital cameras.

20:21.560 --> 20:24.960
So I think Sam, you enjoy photography rate.

20:24.960 --> 20:30.360
I do enjoy photography, especially digital photography.

20:30.360 --> 20:37.720
Yeah, so this one, I hope you find interesting because they've trained a generator adversarial

20:37.720 --> 20:43.680
network to improve the asphetic quality of standard or normal looking photos.

20:43.680 --> 20:49.680
So let's say I take really bad photos and I put it in this generator adversarial networks

20:49.680 --> 20:55.040
and it comes out looking completely professional looking, you know, applying through the thirds,

20:55.040 --> 21:01.160
improving lightning, having enhancements that you get from image editing software.

21:01.160 --> 21:03.920
And there are two cool parts about this.

21:03.920 --> 21:12.000
The first one is that they're using GANs, so you don't need a pair of good looking and

21:12.000 --> 21:13.880
bad looking images.

21:13.880 --> 21:19.640
You only need a set of good looking images and a set of bad looking images.

21:19.640 --> 21:25.640
And the second part is that because it's weekly supervised, the pair of input and visual

21:25.640 --> 21:28.520
enhanced images isn't necessary.

21:28.520 --> 21:32.480
So there are these two really cool things about this paper on GANs.

21:32.480 --> 21:38.760
And if you look at the results, they are, you know, GANs have become so photorealistic

21:38.760 --> 21:45.200
that it's just, you know, it's crazy to imagine that GANs are once putting out what that

21:45.200 --> 21:51.200
was, you know, not as aesthetically pleasing, but now when you look at these images, it's

21:51.200 --> 21:52.200
just amazing.

21:52.200 --> 22:00.480
Yeah, the original images that you see with GANs are like these kind of grotesque approximations

22:00.480 --> 22:06.480
and now the celebrity work that Nvidia did.

22:06.480 --> 22:11.840
I think that was earlier this year or maybe late last year.

22:11.840 --> 22:19.280
And there's one that's been going around even more recently, the photorealistic images.

22:19.280 --> 22:24.920
I mean, it is getting quite incredible what they're able to do.

22:24.920 --> 22:26.880
Absolutely.

22:26.880 --> 22:33.560
So another people that I really like is called, this is a really long name.

22:33.560 --> 22:34.640
So bear with me.

22:34.640 --> 22:43.200
So it's called efficient interactive annotation of segmentation data sets with polygon RNN.

22:43.200 --> 22:47.080
So this is an interactive annotation tool.

22:47.080 --> 22:52.960
Now if you think about labeling of data sets and we talk about segmentation data, we know

22:52.960 --> 22:58.920
that class labeling needs labeling of each and every pixel in the image.

22:58.920 --> 23:04.480
Now this is quite literally forever if you're talking about millions and millions of

23:04.480 --> 23:05.880
images.

23:05.880 --> 23:11.680
We know that deep neural networks work well when they can feast on a large and fully annotated

23:11.680 --> 23:13.360
data set.

23:13.360 --> 23:18.160
So this paper is really the economical bridge between these two worlds because with polygon

23:18.160 --> 23:26.360
RNN, you can set rough polygon points around each object in the image that you want to annotate.

23:26.360 --> 23:31.640
And then the network will automatically generate the segmentation annotation.

23:31.640 --> 23:36.480
And a big advantage is that a method generalizes well.

23:36.480 --> 23:41.640
So it can be used to create quick and easy annotations for segmentation tasks.

23:41.640 --> 23:48.520
So it's the idea here that typically for image segmentation, we want this to be on a pixel

23:48.520 --> 23:53.760
by pixel basis so that we can kind of train the network very accurately.

23:53.760 --> 23:59.760
But what this is doing is allowing someone who's doing segmentation to just do it on a polygon

23:59.760 --> 24:00.760
basis.

24:00.760 --> 24:09.360
And then the network will map that to what will find more the more accurate pixel based segmentation.

24:09.360 --> 24:13.080
Yes, that is the idea.

24:13.080 --> 24:20.960
But it makes the process of annotations much faster because you don't have to go pixel by

24:20.960 --> 24:21.960
pixel, right?

24:21.960 --> 24:23.480
You only have to give a rough polygon.

24:23.480 --> 24:31.680
And then the system will adapt the polygon to the actual, the edges of the object.

24:31.680 --> 24:32.680
Mm-hmm.

24:32.680 --> 24:33.680
Yeah.

24:33.680 --> 24:36.280
And do you have a sense for this how close do you need to get?

24:36.280 --> 24:39.360
We still need to get within a few pixels.

24:39.360 --> 24:45.360
Is it meant to be kind of a fine tuning mechanism or can you kind of very roughly put a polygon

24:45.360 --> 24:50.720
around, say, a person, a picture of a street scene?

24:50.720 --> 24:52.960
You can make it pretty rough.

24:52.960 --> 24:58.320
I mean, in their examples, they've shown a real imagery and those examples are pretty

24:58.320 --> 24:59.320
neat.

24:59.320 --> 25:00.320
Oh, interesting.

25:00.320 --> 25:01.320
Cool.

25:01.320 --> 25:02.320
Yeah.

25:02.320 --> 25:05.840
I've got one more that I really, actually two more that I really like.

25:05.840 --> 25:06.840
Okay.

25:06.840 --> 25:07.840
Okay.

25:07.840 --> 25:13.640
So this one is super slow mo, high quality estimation of multiple intermediate frames for

25:13.640 --> 25:16.080
video interpolation.

25:16.080 --> 25:21.480
And this comes from University of Massachusetts at Amherst.

25:21.480 --> 25:25.960
So you're given two consecutive frames.

25:25.960 --> 25:32.560
And they apply a method of video interpolation that aims at generating intermediate frames

25:32.560 --> 25:38.080
that are both spatially and temporally coherent sequences.

25:38.080 --> 25:42.560
So internally, it's utilizing optical flow and convolutional neural networks between

25:42.560 --> 25:48.240
frames to interpolate video frames input at 30 frames per second.

25:48.240 --> 25:52.720
And it produces crisp looking results to 40 frames per second.

25:52.720 --> 25:59.120
And they've shown examples of like a bullet going through an egg.

25:59.120 --> 26:04.760
And you know, to a human eye, you don't really notice much.

26:04.760 --> 26:09.440
But when you look at it in super slow modes, it's really breathtaking.

26:09.440 --> 26:16.400
I mean, you can see how the cracks on the egg shell propagate.

26:16.400 --> 26:17.400
And that's really cool.

26:17.400 --> 26:18.400
Oh, wow.

26:18.400 --> 26:19.400
Yeah.

26:19.400 --> 26:20.400
Wow.

26:20.400 --> 26:23.040
And they're doing this from optical flow and CNN's.

26:23.040 --> 26:24.040
Right.

26:24.040 --> 26:25.040
Right.

26:25.040 --> 26:31.640
Against slow-mo data, or is this interpolation happening without specific training in that

26:31.640 --> 26:32.640
way?

26:32.640 --> 26:34.360
It's got training.

26:34.360 --> 26:35.360
OK.

26:35.360 --> 26:43.000
So the network kind of learns what the effect of slow-mo is from slow-mo training data and

26:43.000 --> 26:48.960
then can apply that effect to non-slomo video.

26:48.960 --> 26:49.960
Yeah.

26:49.960 --> 26:50.960
OK.

26:50.960 --> 26:51.960
Very interesting.

26:51.960 --> 26:57.680
And then the GAN-based photo-enhanced, you can start to see how some of these techniques

26:57.680 --> 27:01.280
can work their way into our everyday devices.

27:01.280 --> 27:02.280
Absolutely.

27:02.280 --> 27:11.360
And that's actually a really good point because handheld devices now have GPUs in them.

27:11.360 --> 27:17.920
You know, they have separate accelerators that accelerate CNNs.

27:17.920 --> 27:24.520
So you know, I guess even the mobile industry is expecting developers to use these techniques

27:24.520 --> 27:29.400
because they're putting these heavy GPUs inside these devices.

27:29.400 --> 27:32.160
And the salt started, I think, in 2015.

27:32.160 --> 27:37.240
So if you go around benchmarking all these devices, you notice that in 2015, there is

27:37.240 --> 27:42.840
a sudden jump in hardware acceleration.

27:42.840 --> 27:48.120
And that is when these GPUs were introduced into mobile phones.

27:48.120 --> 27:49.120
OK.

27:49.120 --> 27:51.680
So I'm really excited for all the new applications that are about to come.

27:51.680 --> 27:52.680
Yeah.

27:52.680 --> 27:53.680
Same here.

27:53.680 --> 27:54.680
Yeah.

27:54.680 --> 27:55.680
So you have one more paper?

27:55.680 --> 27:56.680
Yes.

27:56.680 --> 28:02.880
Now, this is called, who let the dogs out, modeling dog behavior from visual data, and

28:02.880 --> 28:05.440
this comes again from the University of Washington.

28:05.440 --> 28:06.440
OK.

28:06.440 --> 28:12.800
So I remember when this paper came out, I found it through a YouTube recommendation.

28:12.800 --> 28:20.200
And a dog was walking around with all these sensors, GoPro and Arduino attached and a number

28:20.200 --> 28:26.200
of sensors on the dog's limbs, and they'd linked an archive paper with it.

28:26.200 --> 28:34.440
And I was really fascinated by what is this dog doing on archive paper.

28:34.440 --> 28:40.080
And then I read it and it's very interesting because when we talk about visually intelligent

28:40.080 --> 28:41.480
agents.

28:41.480 --> 28:48.240
We tend to break that into smaller, more approachable subproblems, like classification, detection

28:48.240 --> 28:49.560
and planning.

28:49.560 --> 28:54.240
But this paper conquers it as a one big problem.

28:54.240 --> 28:59.400
So it takes input images and it produces planning actions.

28:59.400 --> 29:05.360
And again, the data collection is from a dog using a GoPro, Arduino, and a number of

29:05.360 --> 29:08.080
sensors on the dog's limbs.

29:08.080 --> 29:14.280
So they've got feature extractors from CNNs, which they used to get image features from

29:14.280 --> 29:16.240
the video frames, of course.

29:16.240 --> 29:20.960
And then all of this is passed to a set of LSTMs along with a sensor data.

29:20.960 --> 29:25.200
And the system learns and predicts dog's actions.

29:25.200 --> 29:29.440
And in the paper, they've exemplified three particular tasks.

29:29.440 --> 29:31.840
One is acting like a dog.

29:31.840 --> 29:40.320
So you have a previously seen sequence of images and you want to predict what will be the

29:40.320 --> 29:42.720
future movement of the dog.

29:42.720 --> 29:49.840
The second task is planning like a dog where you have a sequence of a source and a destination

29:49.840 --> 29:51.560
locations.

29:51.560 --> 29:58.200
And the goal is to find a sequence of actions that take the dog from the initial, the source

29:58.200 --> 30:01.440
location and the destination location.

30:01.440 --> 30:05.400
The third and the final task is learning from a dog.

30:05.400 --> 30:10.040
So can we learn, learn representations for a third task?

30:10.040 --> 30:17.520
For example, if I'm a dog, can I try to understand if the surface in front of me is walkable?

30:17.520 --> 30:18.840
So stuff like that.

30:18.840 --> 30:26.640
And I think this paper is really interesting because it's like an end-to-end solution,

30:26.640 --> 30:35.040
not it's not breaking up different tasks like planning detection or classification.

30:35.040 --> 30:37.960
It's really treating it as one big intelligent agent.

30:37.960 --> 30:42.160
And I haven't seen those examples in a really long time.

30:42.160 --> 30:46.280
So I think that's why this paper stands out so much.

30:46.280 --> 30:51.880
Thinking of applications of this kind of paper and a thing that jumps out at me is when

30:51.880 --> 30:58.400
you think about these Boston Dynamics robots, how they might, this kind of training might

30:58.400 --> 31:02.160
help them get to something that's more intelligent.

31:02.160 --> 31:08.200
So the Boston Dynamics robots are really, you know, they've got four limbs and like an upper

31:08.200 --> 31:12.360
body part and then a sense of network of cameras and so on.

31:12.360 --> 31:17.640
But when I think about this paper, I think that it's not only applicable to like the Boston

31:17.640 --> 31:28.600
Dynamics robots, but generally all robots because planning, action, learning, you know, coupling

31:28.600 --> 31:35.600
with understanding of the outside world, all these are, you know, very essential parts

31:35.600 --> 31:39.880
of the visually intelligent agent.

31:39.880 --> 31:44.720
And you know, it's not, it's not just a single robot that can benefit from it.

31:44.720 --> 31:51.440
I mean, you know, this system could be easily applicable in, in phones, for example, you

31:51.440 --> 31:58.000
know, for helping blind people or for the deaf community in certain cases.

31:58.000 --> 32:03.600
So it's, it's really exciting to see how end-to-end applications are developing and coming

32:03.600 --> 32:05.880
into real life.

32:05.880 --> 32:12.200
Definitely some interesting papers from, from 2018 and, you know, particularly with

32:12.200 --> 32:18.520
your caveat, this is just the few that came to mind as, particularly meaningful contributions

32:18.520 --> 32:25.200
and, you know, related to the, the kind of scope of, of the elements of the field that

32:25.200 --> 32:27.520
are of most interest to you.

32:27.520 --> 32:28.520
Yeah.

32:28.520 --> 32:30.400
And I think I've already said this.

32:30.400 --> 32:37.400
I don't know how many times, but the speed of publications is just immense.

32:37.400 --> 32:40.960
And the good thing is that it's all quality work that's coming out.

32:40.960 --> 32:46.160
So keeping up with all of this is so much more challenging than it used to be before.

32:46.160 --> 32:54.280
One of the things that we wanted to talk about was kind of your perspective on the different

32:54.280 --> 32:57.880
kind of research accomplishments of 2018.

32:57.880 --> 32:59.280
But that's been difficult for you.

32:59.280 --> 33:03.640
You said there's so many interesting papers in 2018.

33:03.640 --> 33:08.600
So here's an idea, you know, we know that there are different stages in the AI pipeline.

33:08.600 --> 33:12.720
Like get a collection, labeling, training, and so on.

33:12.720 --> 33:17.360
And then there are some new entries, like modern reproducibility, phoenix, and biasing.

33:17.360 --> 33:23.200
And so why don't we sort of use these stages as a demarcation?

33:23.200 --> 33:27.920
And in each talk about papers and tools and new research that have come up.

33:27.920 --> 33:29.440
Oh, that sounds great.

33:29.440 --> 33:33.800
And again, I think like you've been doing through all the Tremelai AI talks, we will try

33:33.800 --> 33:38.800
to link all the tools and use cases so people can read about them a little more.

33:38.800 --> 33:44.080
You know, if you consider data as the new oil, then labeled data really becomes the new

33:44.080 --> 33:45.240
gold.

33:45.240 --> 33:49.040
And that brings us to a first category, which is data collection and labeling.

33:49.040 --> 33:59.440
So AWS released something called AWS Deep Racer, which is a small scale race car that

33:59.440 --> 34:04.920
gives you an interesting way to get started with reinforcement learning.

34:04.920 --> 34:10.200
So as we all know, reinforcement learning takes a very different approach to training models

34:10.200 --> 34:18.040
than other machine learning techniques, because it learns complex behaviors without requiring

34:18.040 --> 34:20.520
any labeled training data.

34:20.520 --> 34:25.680
And at the same time, you can make short-term decisions while optimizing for a longer-term

34:25.680 --> 34:32.720
goal. And especially with AWS Deep Racer, you can get hands-on reinforcement learning.

34:32.720 --> 34:38.720
You can do a lot of experimentation with cloud-based 3D-racing simulators.

34:38.720 --> 34:43.840
You can raise your friends all while tapping your toes into autonomous driving.

34:43.840 --> 34:51.080
I think AWS Deep Racer really comes out as a top-new tool that Amazon has released this

34:51.080 --> 34:52.080
year.

34:52.080 --> 34:57.840
Yeah, I was at re-invent last week when they announced it.

34:57.840 --> 35:06.960
And they also announced a new extension to their SageMaker tool that's focused on reinforcement

35:06.960 --> 35:07.960
learning.

35:07.960 --> 35:12.640
And I'm really glad you brought up RL in this context.

35:12.640 --> 35:19.480
I think when I think about the work that OpenAI and Deep Mind are doing around reinforcement

35:19.480 --> 35:28.120
learning, for those organizations, I think, a lot of their motivation is that we kind

35:28.120 --> 35:33.520
of, as humans kind of learn in a reinforcement learning way, we don't have labeled training

35:33.520 --> 35:34.520
data.

35:34.520 --> 35:38.720
We kind of explore our worlds and learn.

35:38.720 --> 35:47.040
And so they are aggressively pursuing reinforcement learning as a stepping stone to AGI.

35:47.040 --> 35:56.120
And I think for most of us, the practical implications of RL or that kind of gets us,

35:56.120 --> 36:01.520
it sidesteps this need that we usually have for label training data.

36:01.520 --> 36:06.600
And in the near future, that's going to be its big contribution.

36:06.600 --> 36:14.400
So I'm particularly excited that AWS, which has so much weight in the space, is starting

36:14.400 --> 36:19.880
to kind of shine the light on reinforcement learning and get people, you know, starting

36:19.880 --> 36:22.560
down the path to experimenting with it.

36:22.560 --> 36:23.560
Yeah.

36:23.560 --> 36:30.400
And now that you've mentioned, you know, going into the future, how will we have labels

36:30.400 --> 36:31.400
available?

36:31.400 --> 36:38.520
There's work done by the Microsoft machine learning team and the Apache Spark community.

36:38.520 --> 36:45.960
And they actually worked on creating a deep distributed object detector that works without

36:45.960 --> 36:50.440
any human labels or human generated labels.

36:50.440 --> 36:56.040
Now, you know, if anyone who has ever trained their own detector knows how incredibly

36:56.040 --> 37:01.440
painful it is to get the bounding boxes and the labels right, but now thanks to a technology

37:01.440 --> 37:05.560
called Lime, we can work without any data.

37:05.560 --> 37:10.760
And Lime is local interpretable model agnostic explanations.

37:10.760 --> 37:17.240
And this was built by Marco Ribeiro and a team from University of Washington.

37:17.240 --> 37:22.640
And it helps in understanding the classification of any image classifier.

37:22.640 --> 37:27.320
So it's really telling us where the classifier is looking.

37:27.320 --> 37:28.800
And then here's the coolest thing ever.

37:28.800 --> 37:32.640
It makes no assumptions about the kind of model.

37:32.640 --> 37:37.720
So you can use it for your own secret model, random published models that you downloaded

37:37.720 --> 37:44.320
from the internet or even a very patient human classifier, which means that it has extremely

37:44.320 --> 37:50.960
wide applicability, not just across models, but also across domains.

37:50.960 --> 37:58.480
So Lime was originally published a few years ago, I think, but you're saying that folks

37:58.480 --> 38:08.440
are now using it to beyond its initial intent of model explainability, but to allow you

38:08.440 --> 38:12.320
to create classifiers without label training data.

38:12.320 --> 38:13.320
Yeah.

38:13.320 --> 38:14.920
So I was actually coming to that.

38:14.920 --> 38:21.400
So Lime has a huge drawback that it is extremely computationally intensive.

38:21.400 --> 38:28.200
So what the Microsoft machine learning and the Apache Spark community did was they

38:28.200 --> 38:33.520
made this available in a distributed implementation and SparkML.

38:33.520 --> 38:37.320
And that's how you can make it more real time.

38:37.320 --> 38:45.800
And because of that, you can create image classifiers for classification or detection from

38:45.800 --> 38:47.880
bounding boxes.

38:47.880 --> 38:53.760
And they experimented with this on a real life use case for the conservation of snow leopards

38:53.760 --> 38:55.000
in Krigaston.

38:55.000 --> 38:56.960
So that was pretty interesting.

38:56.960 --> 39:03.880
And so can you give me a sense for how the use case works or what the flow is in working

39:03.880 --> 39:05.280
with this tool?

39:05.280 --> 39:06.280
Yeah.

39:06.280 --> 39:12.280
So it's basically you have all this data, which is not labeled.

39:12.280 --> 39:20.080
So you have plain basic images, which can be, you know, without the actual object, which

39:20.080 --> 39:25.640
is a snow leopard in this case, or it can include the snow leopard.

39:25.640 --> 39:28.880
And you run Lime over all these images.

39:28.880 --> 39:33.760
And then you ask simple questions like, what is the most common object that you're seeing

39:33.760 --> 39:35.960
over all the data?

39:35.960 --> 39:40.480
Where is the object located in most of these images?

39:40.480 --> 39:47.080
And that's how the system really learns what is most common, what it should be looking

39:47.080 --> 39:48.240
for.

39:48.240 --> 39:55.160
And once you have these questions answered, you can use that sort of like a label.

39:55.160 --> 39:58.920
And without any human actually working on labeling.

39:58.920 --> 40:02.920
And then you can generate the entire system of object detection with that.

40:02.920 --> 40:04.080
Oh, interesting.

40:04.080 --> 40:05.080
Yeah.

40:05.080 --> 40:07.320
And then again, all of this is real time.

40:07.320 --> 40:08.320
It's distributed.

40:08.320 --> 40:10.440
So it's pretty quick.

40:10.440 --> 40:17.880
And if I remember correctly, they had a statistic in their published work, which was that

40:17.880 --> 40:24.560
it takes, if it takes one hour for you to evaluate your model on a particular data set,

40:24.560 --> 40:32.040
then it would take 50 days worth of computation to convert these predictions to interpretations.

40:32.040 --> 40:33.840
And now that doesn't sound attractive at all.

40:33.840 --> 40:39.360
And that is where the improvement is coming by making it a more distributed implementation

40:39.360 --> 40:41.520
and packaging it in SparkML.

40:41.520 --> 40:42.520
Okay.

40:42.520 --> 40:43.520
Very cool.

40:43.520 --> 40:44.520
Yeah.

40:44.520 --> 40:54.360
And then a Christmas miracle is, when we talk about labeling, is two companies, digital

40:54.360 --> 40:59.040
divide data by Jeremy Hawkinsstein and Samir Reyna.

40:59.040 --> 41:00.440
And I met it.

41:00.440 --> 41:05.760
Now both these are nonprofit companies that provide labeling solutions.

41:05.760 --> 41:10.600
But you know, when I'm talking about it right now, it probably sounds like the million

41:10.600 --> 41:13.400
other labeling companies out there.

41:13.400 --> 41:16.160
But here is really where the heart is.

41:16.160 --> 41:20.880
They train and employ people who have no other sources of employment.

41:20.880 --> 41:24.840
And so the most part don't have access to higher education.

41:24.840 --> 41:28.880
These people learn by doing and they earn enough money to support themselves and eventually

41:28.880 --> 41:37.360
hone enough skills to go for certifications or undergraduate cases.

41:37.360 --> 41:43.320
And I think digital data divide mentioned that they have uplifted more than 3,000 individuals,

41:43.320 --> 41:46.200
their families and communities all around the world.

41:46.200 --> 41:47.520
That's pretty interesting.

41:47.520 --> 41:48.520
Oh yeah.

41:48.520 --> 41:54.520
And we talk about labeling in terms of how it's really impacting the world.

41:54.520 --> 41:55.520
It's interesting.

41:55.520 --> 42:00.280
We talk a lot about AI for social good.

42:00.280 --> 42:04.360
And it's usually the, you know, we're thinking about the AI itself.

42:04.360 --> 42:10.360
But now this is an example of how even the process of creating AI can be beneficial to

42:10.360 --> 42:11.360
communities.

42:11.360 --> 42:12.360
Yeah.

42:12.360 --> 42:14.680
I mean, imagine what these people can achieve in the future.

42:14.680 --> 42:16.400
It's just limitless.

42:16.400 --> 42:24.360
And then there is another tool called Prodigy, which is helping solve a really big issue.

42:24.360 --> 42:31.800
Now we all know that the AI workflow doesn't really follow what fault development strategy.

42:31.800 --> 42:37.000
It's more like a chicken and egg problem because you can't really start experimentation

42:37.000 --> 42:41.640
until you have at least the first batch of annotations.

42:41.640 --> 42:47.160
And the annotation team can start until they receive the annotation manuals and to produce

42:47.160 --> 42:52.720
the annotation manuals, you need to know what statistical models you need based on the

42:52.720 --> 42:54.600
features you're trying to build.

42:54.600 --> 43:00.360
So all in all machine learning is an inherently uncertain technology, whereas the waterfall

43:00.360 --> 43:05.200
annotation process really relies on accurate and upfront planning.

43:05.200 --> 43:11.440
So what Prodigy does, honestly, living up to its name, it solves this problem by letting

43:11.440 --> 43:16.640
data scientists conduct their own annotations for rapid prototyping.

43:16.640 --> 43:22.480
It puts the model in the loop so it can actively participate in the training process and learns

43:22.480 --> 43:25.640
as you go in an active learning kind of a setup.

43:25.640 --> 43:31.600
It has AB evaluations and a whole suite of tools to help you prototype much faster, bringing

43:31.600 --> 43:36.880
in the much needed increased agility.

43:36.880 --> 43:42.240
And what's really awesome about Prodigy is that it's written by the authors of Spacey,

43:42.240 --> 43:45.160
which is a really cool NLP library.

43:45.160 --> 43:47.760
I've heard quite a bit about Spacey.

43:47.760 --> 43:54.360
And so what's the user experience of working with Prodigy?

43:54.360 --> 43:57.160
Prodigy is extremely efficient.

43:57.160 --> 43:59.760
It's got, you know, you can use it via command line.

43:59.760 --> 44:01.760
It's also got a beautiful UI support.

44:01.760 --> 44:09.200
It helps in both annotation and training, and I've tested it out for detection problems.

44:09.200 --> 44:16.160
And it's really easy to, you know, get like 10, 20 images to find what object you want

44:16.160 --> 44:23.640
to detect, give a few bounding boxes and see how the model predicts the bounding boxes

44:23.640 --> 44:29.920
on the next couple of set of images and then use those as a bigger data set for training.

44:29.920 --> 44:32.600
Huh, interesting.

44:32.600 --> 44:40.160
That sounds like it would take a lot of the early effort out of kind of experimentation.

44:40.160 --> 44:47.680
I remember going through the fast.ai course and, you know, this whole process of like searching

44:47.680 --> 44:55.160
for images on, you know, like Google images and, you know, labeling them and putting them

44:55.160 --> 45:01.800
in the right, you know, the right folders and all this stuff can be pretty time consuming.

45:01.800 --> 45:03.200
Yeah, that's true.

45:03.200 --> 45:10.000
And what's even better is because it's got an active learning setup, it can figure out

45:10.000 --> 45:15.960
which of the following images is something that, you know, if it's included in the training

45:15.960 --> 45:21.600
data set, it will perform better rather than, you know, adding images that don't really

45:21.600 --> 45:23.600
improve the accuracy.

45:23.600 --> 45:30.560
On that note, AWS announced something in the same vein at re-invent SageMaker Ground

45:30.560 --> 45:35.840
Truth, which uses active learning in the same way.

45:35.840 --> 45:45.160
It basically is a labeling pipeline that incorporates human and loop laborers, which can

45:45.160 --> 45:52.480
be mechanical Turk or some of their partners, maybe some of the companies that you previously

45:52.480 --> 45:54.560
mentioned are included in that.

45:54.560 --> 46:02.440
They also use active learning to try to make the labeling process more efficient by only

46:02.440 --> 46:11.920
labeling the images that will, actually, I don't even recall if it's specific to vision

46:11.920 --> 46:19.840
or if it's broader, but only labeling the kind of the data that will add the most value.

46:19.840 --> 46:26.000
That's true, it's definitely available in AWS now.

46:26.000 --> 46:34.000
It's also been, if I'm not wrong, available in Google Cloud from their AutoML team.

46:34.000 --> 46:38.640
And internally, they use Crowdflower or what is now known as Figure 8.

46:38.640 --> 46:43.480
And that also employs a method of active learning, if I remember correctly, and hopefully

46:43.480 --> 46:44.480
I am.

46:44.480 --> 46:51.600
But yeah, so active learning for labeling has been a top priority, I think, for companies

46:51.600 --> 46:52.600
this year.

46:52.600 --> 46:55.080
At least that's what it seems like to an onlooker.

46:55.080 --> 46:56.080
I agree.

46:56.080 --> 46:57.080
Yeah.

46:57.080 --> 47:04.560
Yeah, and then a really interesting trend that's up in coming is gamification of labeling,

47:04.560 --> 47:09.080
which keeps the interest going in human and the loop labeling.

47:09.080 --> 47:15.880
So what you do is, you know, you put these certain objects that you're looking to detect

47:15.880 --> 47:23.280
or classify at really weird places that you wouldn't expect.

47:23.280 --> 47:30.720
And we know that convolutional neural networks, they depend not only on the object, but also

47:30.720 --> 47:35.000
on the usual surroundings of that object.

47:35.000 --> 47:42.080
So it kind of, you know, focuses the network to try to concentrate on the object itself

47:42.080 --> 47:50.160
rather than its surroundings, because you are getting so much variation in the surroundings.

47:50.160 --> 47:55.800
Michael Arthur, I know that actually talks about this in his TEDx talk, and it's really

47:55.800 --> 48:02.560
interesting to see how companies are incorporating gamification in their annotation cycles.

48:02.560 --> 48:12.360
And so is this gamification for human laborers or gamification somehow trying to gain the

48:12.360 --> 48:13.800
neural networks themselves?

48:13.800 --> 48:23.160
So it's kind of both, because you are looking for these certain images that, you know,

48:23.160 --> 48:31.880
you know, will not work as well, because the object isn't in their natural environment.

48:31.880 --> 48:32.880
So that's one.

48:32.880 --> 48:39.560
And then second, it also keeps the interest of the human laborers really ongoing in the

48:39.560 --> 48:45.480
system, because, you know, they are so familiar with the system, they know how to break

48:45.480 --> 48:49.960
it, what are the kind of images that will really break the system, because they are the

48:49.960 --> 48:56.760
ones who've gone through the entire data distribution that the model really depends on.

48:56.760 --> 48:58.360
So it's a bit of both.

48:58.360 --> 49:07.720
It's interesting, it makes me think of the humans and the network in some kind of adversarial

49:07.720 --> 49:11.640
relationship, you know, separated by distance and time.

49:11.640 --> 49:15.960
Yeah, I mean, I actually think it did the same way.

49:15.960 --> 49:16.960
Interesting.

49:16.960 --> 49:22.160
I think we should move on to your next category to make sure we can get everything in.

49:22.160 --> 49:23.160
Okay.

49:23.160 --> 49:24.160
Awesome.

49:24.160 --> 49:32.160
And the category would be training and training, I think there have been a lot of frameworks

49:32.160 --> 49:42.760
which have come out, which help to do hyperparameterization, which help in, you know, maintaining

49:42.760 --> 49:48.840
like a GitHub equivalent of the entire training cycle.

49:48.840 --> 49:49.840
Okay.

49:49.840 --> 49:59.280
And then moving on to an open source experimentation, optimization, library, which is called hyper

49:59.280 --> 50:00.280
OPT.

50:00.280 --> 50:08.800
It's a distributed asynchronous hyperparameterization, optimization, library and Python.

50:08.800 --> 50:15.960
And it does both serial and parallel optimizations over certain spaces, which can be real value,

50:15.960 --> 50:18.760
discrete and conditional dimensions.

50:18.760 --> 50:27.640
It hyperass is actually a very simple, convenient wrapper around hyper OPT, foster prototyping with

50:27.640 --> 50:28.640
Keras models.

50:28.640 --> 50:34.400
And I think hyperass is something that's, that I feel, you know, it can really be a game

50:34.400 --> 50:40.800
changer because a lot of people when they start in deep learning, they don't, you know,

50:40.800 --> 50:45.400
they don't really learn about how do you actually tune the hyperparameters?

50:45.400 --> 50:49.080
What is the initial value at which you want to start?

50:49.080 --> 50:54.640
But with hyperass, you know, you can define an entire hyperparameter range and then internally

50:54.640 --> 51:00.480
it will learn which is the best value for each particular parameter.

51:00.480 --> 51:06.400
And this has really the only difference that, or the only additional thing that you would

51:06.400 --> 51:12.000
have to do apart from just defining your model as you already do in Keras.

51:12.000 --> 51:17.960
And then you can also run multiple models, training, testing in parallel while you're

51:17.960 --> 51:19.680
using MongoDB as a backend.

51:19.680 --> 51:26.440
So hyperass has become really cool, I think, for companies, you know, that are really

51:26.440 --> 51:29.520
looking for a small-scale solution.

51:29.520 --> 51:30.520
Interesting.

51:30.520 --> 51:39.720
Yet one of the other things that I've repeatedly seen as a best practice in this experiment

51:39.720 --> 51:45.800
management category is automating hyperparameter optimization.

51:45.800 --> 51:52.400
I think a lot of people think of that, or historically, a thought of hyperparameter optimization

51:52.400 --> 51:57.840
as this, you know, the cherry on the cake or the icing on the cake or something like the

51:57.840 --> 52:04.440
thing that you do last to kind of eke, you know, a few more percentage points, or, you

52:04.440 --> 52:08.800
know, a few more points of accuracy or performance out of your models.

52:08.800 --> 52:19.760
But all of the folks that I'm seeing doing this bake it very deeply into their experimentation

52:19.760 --> 52:28.080
pipelines so that you're really automating this optimization throughout the, you know,

52:28.080 --> 52:33.040
the whole experimentation cycle as opposed to kind of just at the very end.

52:33.040 --> 52:45.160
And if you parameterize your models correctly, the hyperparameter optimization can help you

52:45.160 --> 52:50.680
kind of define and evolve your architectures as well.

52:50.680 --> 52:59.080
So another product out in this base is Sig Opt, who is a sponsor of this Agile Machine

52:59.080 --> 53:05.160
Learning Platforms eBook that I referred to earlier.

53:05.160 --> 53:13.280
And they've also been, they've presented at the, the Twoma Online Meetup, I think that

53:13.280 --> 53:18.880
might have been last year, about some of the things you're doing, but I definitely expect

53:18.880 --> 53:25.880
to see a lot more folks working with the, with hyperparameter optimization and kind of

53:25.880 --> 53:30.800
building it deeply into their, their pipelines.

53:30.800 --> 53:31.800
Yeah.

53:31.800 --> 53:38.200
And another thing, interesting to note here is that, you know, 2018 has been a year of

53:38.200 --> 53:46.680
neural architecture search and combining that with Hyperass is just, you know, taking

53:46.680 --> 53:51.880
training and optimization to a whole new level, because you're not only optimizing the

53:51.880 --> 53:57.160
parameters, you're also optimizing the entire architecture, which is, I mean, architecture

53:57.160 --> 54:02.600
definition is always something that, you know, it's, it's really difficult to get right.

54:02.600 --> 54:06.680
And you never know, is this exactly right?

54:06.680 --> 54:11.480
Or, you know, if I remove some layers, will it still be the same?

54:11.480 --> 54:15.600
So that's, you know, that's a problem that I don't know if there's any solution for

54:15.600 --> 54:20.840
it, but neural architecture search is definitely amazing.

54:20.840 --> 54:25.440
I mean, it's absolutely stunning how they've been able to achieve it.

54:25.440 --> 54:27.680
And it's results are amazing.

54:27.680 --> 54:28.680
Yeah.

54:28.680 --> 54:33.680
There was a period of time on the podcast where I would, I would ask every guest in this

54:33.680 --> 54:36.160
space, like, how do you come up with architectures?

54:36.160 --> 54:37.160
Where do they come from?

54:37.160 --> 54:40.360
I probably asked you this.

54:40.360 --> 54:47.600
And the conclusion that I came to, you know, a year ago, a couple of years ago, is that,

54:47.600 --> 54:51.680
you know, it's a little bit of black magic, like, you know, people just try a bunch of

54:51.680 --> 54:54.120
things and see what works.

54:54.120 --> 54:57.840
And I think what's interesting about what we're seeing now with the neural architecture

54:57.840 --> 55:05.120
search is that, you know, we're bringing a little bit more of science and methodology

55:05.120 --> 55:06.120
back to it.

55:06.120 --> 55:07.120
Yeah.

55:07.120 --> 55:08.120
Yeah.

55:08.120 --> 55:11.440
It's, it's a little more of, you know, the software engineering techniques that we've

55:11.440 --> 55:17.480
all been learning and incorporating those into the AI development cycle.

55:17.480 --> 55:18.480
Right.

55:18.480 --> 55:19.480
Right.

55:19.480 --> 55:20.480
Yeah.

55:20.480 --> 55:25.760
And I think another tool that's worth mentioning is Rapids from Nvidia.

55:25.760 --> 55:33.160
So, you know, we talk about how training has been accelerated by GPUs, how inference,

55:33.160 --> 55:38.400
for inference, you have a different kind of GPUs or different kind of hardware on which

55:38.400 --> 55:40.360
you want to do inference.

55:40.360 --> 55:49.600
But Nvidia's Rapids is really a suite of software libraries that allow the execution of the

55:49.600 --> 55:54.520
entire end-to-end data science pipeline entirely on GPUs.

55:54.520 --> 56:01.400
So it can be data processing, training, testing, loading, anything.

56:01.400 --> 56:07.240
Everything is inbuilt onto GPUs and of course, internally it's using CUDA primitives for

56:07.240 --> 56:13.760
low-level compute optimization and it exposes the GPU parallelism that we all love.

56:13.760 --> 56:17.240
And it's got really high bandwidth memory speed.

56:17.240 --> 56:23.080
It exposes very easily usable Python interfaces.

56:23.080 --> 56:28.080
And because it's been focusing on a lot on data preparation, which I know is a really,

56:28.080 --> 56:30.760
you know, I mean, I've been struggling with data preparation.

56:30.760 --> 56:37.200
How do you load such heavy data when you know that this data said won't fit into memory?

56:37.200 --> 56:42.640
So you know, these kind of problems that people face every day, Nvidia Rapids has actually

56:42.640 --> 56:48.640
a solution to it because it's introduced the data frame API that integrates with existing

56:48.640 --> 56:54.880
machine learning algorithms for the complete end-to-end pipeline acceleration.

56:54.880 --> 57:04.160
And you know, you forego the serialization costs which you would have to endure otherwise.

57:04.160 --> 57:10.040
I came across the Rapids announcement, which was relatively recently, but never quite

57:10.040 --> 57:14.360
had a chance to dig into it and see what they're doing, I think.

57:14.360 --> 57:21.960
I don't know if it was Rapids or something else that one of the big points about it was

57:21.960 --> 57:30.880
that, you know, we kind of think about GPUs as, you know, having primary, you know, being

57:30.880 --> 57:37.600
primarily useful for neural networks, but they went ahead and created GPU-accelerated versions

57:37.600 --> 57:44.640
of a bunch of traditional machine learning operations like, yeah, okay, yeah, yeah.

57:44.640 --> 57:49.120
And then, you know, it's also available for multi-node, multi-GPU deployment.

57:49.120 --> 57:55.240
So, you know, if your work is scaling up, Rapids is the best, you know, sort of software

57:55.240 --> 58:02.000
libraries that you can use to scale up down as your requirement changes.

58:02.000 --> 58:07.120
You know, when I think about when I started off learning about machine learning and day

58:07.120 --> 58:13.000
planning, we, I think we didn't have anything like this.

58:13.000 --> 58:19.600
And, you know, today I feel like, you know, people have so many sources to learn from.

58:19.600 --> 58:28.200
And, you know, be it fast.ai, Kaggle or deep learning.ai or even Siraj Ravel School

58:28.200 --> 58:29.200
of AI.

58:29.200 --> 58:35.680
There's just so much to learn and so much to do that I feel 2019 is going to be a year

58:35.680 --> 58:41.280
of, you know, every single day, there's going to be something new because it seems like

58:41.280 --> 58:43.280
that sometimes doesn't it?

58:43.280 --> 58:49.200
Yeah, I mean, sometimes it feels like, you know, we're on an episode of, um, um,

58:49.200 --> 58:55.680
Silicon Valley because every single day something is coming up and, you know, one day it's the

58:55.680 --> 58:59.560
state of the art in the next day, there's another state of the art.

58:59.560 --> 59:03.640
And then we have testing and deployment.

59:03.640 --> 59:08.720
And I honestly feel that, you know, as an AI, as a member of the AI community, testing

59:08.720 --> 59:16.120
and deployment have become really, um, like even though there's a lot of focus on training,

59:16.120 --> 59:21.640
and deployment have become even more important these days because, you know, that's where

59:21.640 --> 59:24.160
things are actually in production.

59:24.160 --> 59:30.960
So for example, if you talk about Onyx, Onyx came a couple years ago when Facebook and,

59:30.960 --> 59:36.760
um, Microsoft said, you know, why do you want to limit yourself to only one framework?

59:36.760 --> 59:40.240
And we want to promote model interoperability.

59:40.240 --> 59:52.080
So Onyx came through and now if you look at the benchmarks for Onyx.js, you know, TensorFlow.js

59:52.080 --> 59:58.440
and Keras.js are not even understanding when it comes to how ridiculously fast Onyx.js

59:58.440 --> 01:00:00.440
is.

01:00:00.440 --> 01:00:06.640
And, you know, all this comes through years of research and experimentation when we realize

01:00:06.640 --> 01:00:11.160
that, you know, we want to build smaller models when the model size is smaller.

01:00:11.160 --> 01:00:13.960
You can have smaller image sizes.

01:00:13.960 --> 01:00:21.280
You can use the inherent hardware acceleration in JavaScript and, you know, literally within

01:00:21.280 --> 01:00:29.280
the snap of a finger, you can have an entire batch of images, tested and predictions made

01:00:29.280 --> 01:00:31.280
on.

01:00:31.280 --> 01:00:35.800
And now the way to actually talking about browsers, I have to mention the amazing work

01:00:35.800 --> 01:00:43.960
by Osramos, who is using OpenPose to track eyes to help navigate through a browser.

01:00:43.960 --> 01:00:48.400
And of course, there's, there is a limit to how much is possible.

01:00:48.400 --> 01:00:54.720
But imagine the benefits that people with impairments can read from this.

01:00:54.720 --> 01:01:04.680
And, um, you know, the inspiration behind Osramos working on this is also really, um, encouraging.

01:01:04.680 --> 01:01:13.760
And, you know, he was a veteran who sadly lost his home and, um, you know, he was seeing

01:01:13.760 --> 01:01:23.080
people around him who weren't really capable, um, physically to, you know, use their systems

01:01:23.080 --> 01:01:26.000
or laptops or, um, desktops.

01:01:26.000 --> 01:01:31.840
So he decided to use an eye tracking software to actually help people navigate browsers

01:01:31.840 --> 01:01:32.840
and so on.

01:01:32.840 --> 01:01:37.840
I believe he's on patron and, you know, he's doing a lot of cool and interesting work.

01:01:37.840 --> 01:01:40.840
Uh, so maybe switching gears a little bit.

01:01:40.840 --> 01:01:48.840
Uh, I'm curious what you foresee for 2019 in computer vision.

01:01:48.840 --> 01:01:58.320
Um, a lot of people are expecting GANs, you know, to really be in, um, videos, for example.

01:01:58.320 --> 01:02:02.960
I mean, this is more evolutionary rather than revolutionary because when you look at the

01:02:02.960 --> 01:02:07.640
current state of the art, GANs have achieved almost photorealistic results and it's just

01:02:07.640 --> 01:02:08.960
amazing.

01:02:08.960 --> 01:02:12.800
And I can't wait to see what's next with GANs and hopefully it's going to be something

01:02:12.800 --> 01:02:13.800
in videos.

01:02:13.800 --> 01:02:14.800
Mm hmm.

01:02:14.800 --> 01:02:21.720
And then, um, in robotic perception, because we know robots are really good at doing only

01:02:21.720 --> 01:02:27.120
one thing right now, but when you talk about general understanding, they don't really

01:02:27.120 --> 01:02:28.680
work as well.

01:02:28.680 --> 01:02:34.160
So, you know, this is their visual understanding, scene understanding, sort of like a multimodal

01:02:34.160 --> 01:02:41.400
input comes in and it would be really interesting to see how, um, perception systems or, you

01:02:41.400 --> 01:02:48.040
know, how the, uh, human AI interactivity, um, takes off.

01:02:48.040 --> 01:02:53.600
And then I think utilizing AI in other fields, for example, now so it's fronted development

01:02:53.600 --> 01:02:54.600
lab.

01:02:54.600 --> 01:03:00.360
It's amazing here this year, um, I'm looking forward to see what happens next year in

01:03:00.360 --> 01:03:06.240
their work in astrobiology, detecting meteors, asteroids and so on.

01:03:06.240 --> 01:03:12.080
And then in healthcare, AI is really useful and in particular, computer vision is really

01:03:12.080 --> 01:03:19.560
useful for both, you know, learning about healthcare and for patients as well.

01:03:19.560 --> 01:03:25.160
So when you're learning about healthcare, you know, for example, you can learn or you can

01:03:25.160 --> 01:03:31.600
visualize how the heart is pumping the blood, you know, you can visualize all the different

01:03:31.600 --> 01:03:39.360
valves in the heart and see how the pulmonary, um, veins in the pulmonary arteries are sending

01:03:39.360 --> 01:03:43.240
blood to the lungs and getting it back to the heart.

01:03:43.240 --> 01:03:48.920
You know, that's, that's really something that is absolutely revolutionary and, you know,

01:03:48.920 --> 01:03:54.320
imagine how the entire education system would change with just, um, you know, these simple

01:03:54.320 --> 01:03:55.720
applications.

01:03:55.720 --> 01:03:56.720
Right.

01:03:56.720 --> 01:04:02.360
And then, oh, I have to mention this, um, most of my friends have actually stopped buying

01:04:02.360 --> 01:04:07.680
cars in our instead of leasing and they're waiting for 2021 to come by to experience self-driving

01:04:07.680 --> 01:04:08.680
cars.

01:04:08.680 --> 01:04:13.560
I imagine, you know, you're calling a lift or an Uber or taxi and it's self-driving.

01:04:13.560 --> 01:04:15.360
I mean, that would be so cool.

01:04:15.360 --> 01:04:19.640
Yeah, it's, uh, it's, it's funny.

01:04:19.640 --> 01:04:24.920
I was having a conversation with a friend about kind of this idea that, you know, we may

01:04:24.920 --> 01:04:32.320
be the last generation for whom, you know, buying a car is a real thing and increasingly,

01:04:32.320 --> 01:04:37.280
you know, we're kind of entering this domain of, you know, more transportation as a service,

01:04:37.280 --> 01:04:43.760
whether it's, you know, summoning a vehicle from someone else's fleet or, um, you know,

01:04:43.760 --> 01:04:50.920
leasing a self-driving vehicle, you know, you know, maybe one per family and you've, you've,

01:04:50.920 --> 01:04:56.240
the vehicles kind of always doing something for someone, uh, as opposed to, you know, most

01:04:56.240 --> 01:05:02.240
cars sitting around, you know, waiting for you to finish work or finish whatever you're

01:05:02.240 --> 01:05:03.240
doing.

01:05:03.240 --> 01:05:04.240
Yeah.

01:05:04.240 --> 01:05:08.160
I mean, it's really interesting, you know, I mean, no one really knows what the future is

01:05:08.160 --> 01:05:13.400
going to be like, but it's really interesting to, you know, um, think about it and I think

01:05:13.400 --> 01:05:17.320
it becomes even more exciting when you know you're actually working on it and, you know,

01:05:17.320 --> 01:05:24.200
you see the perception that different people have regarding it and you realize, um, oh,

01:05:24.200 --> 01:05:28.560
you know, we can do this to improve the system or, you know, this feature would be a really

01:05:28.560 --> 01:05:31.040
cool thing that people might even love.

01:05:31.040 --> 01:05:32.040
Uh-huh.

01:05:32.040 --> 01:05:33.040
Yeah.

01:05:33.040 --> 01:05:39.800
So, um, another thing I think that would be really exciting is development around zero

01:05:39.800 --> 01:05:47.520
short learning because we all realize that academic data sets are really good, but real

01:05:47.520 --> 01:05:53.240
life deployment is very, very difficult and some sort of practical application would be

01:05:53.240 --> 01:05:56.840
really interesting around zero short learning.

01:05:56.840 --> 01:06:03.280
And, um, a couple more things around reproducibility and finis and biasing.

01:06:03.280 --> 01:06:09.000
Um, so the number one thing I think is going to take off and it's really the need of

01:06:09.000 --> 01:06:15.680
the R is model cards, which is a revolutionary piece of research that comes from Margaret

01:06:15.680 --> 01:06:17.640
Mitchell and a team at Google.

01:06:17.640 --> 01:06:24.000
So model cards are like short book reports for training machine learning models that

01:06:24.000 --> 01:06:31.520
provide benchmarked evaluations across different kind of conditions like cultural demographic

01:06:31.520 --> 01:06:37.760
or different kind of intersectional groups, which are all relevant to the intended application

01:06:37.760 --> 01:06:38.760
domain.

01:06:38.760 --> 01:06:43.800
So, they also disclose the context in which the models are intended to be used, details

01:06:43.800 --> 01:06:50.040
of how the performance of the model is evaluated and other relevant information.

01:06:50.040 --> 01:06:55.960
So you know, in two years when, you know, people will be freely communicating through models,

01:06:55.960 --> 01:07:01.240
it will seem insane that it's not standard to report how well machine learning models

01:07:01.240 --> 01:07:06.000
work when they're actually made available for general public use.

01:07:06.000 --> 01:07:09.920
And I can see model cards really taking off in this aspect.

01:07:09.920 --> 01:07:18.880
It's interesting, over the past year or so, uh, many of the large AI, many of the research

01:07:18.880 --> 01:07:24.280
arms of the large, uh, the companies that are, you know, biggest in AI I'm thinking specifically

01:07:24.280 --> 01:07:31.840
about, you know, Microsoft research with the data, data sheets for data sets work that

01:07:31.840 --> 01:07:42.160
Timnick, Timnick Gebruh worked on, IBM research did an extension of that concept and it's,

01:07:42.160 --> 01:07:46.680
which sounds very similar to the model cards work you're describing, but none of these

01:07:46.680 --> 01:07:52.240
companies are publishing any of these, you know, data sheets or model cards for their

01:07:52.240 --> 01:07:54.440
commercial projects yet.

01:07:54.440 --> 01:07:59.880
That's definitely something I hope to see them do kind of putting their money where their

01:07:59.880 --> 01:08:04.520
mouth is so to speak in, you know, 2019 or 2020.

01:08:04.520 --> 01:08:13.120
Yeah, and especially given the fact that, you know, we have these, like, a model exchange

01:08:13.120 --> 01:08:19.000
service that has really come up this year, for example, TensorFlow has TensorFlow Hub.

01:08:19.000 --> 01:08:26.000
So you have, you know, some piece of reproducible code, sorry, reusable code that, you know,

01:08:26.000 --> 01:08:33.040
you use in most applications, for example, if I want to write code to load a large piece

01:08:33.040 --> 01:08:37.200
of data and I don't know how to do it, I would look it up, find something on GitHub or

01:08:37.200 --> 01:08:42.280
Stack Overflow and reuse that code with the appropriate references.

01:08:42.280 --> 01:08:48.080
And TensorFlow Hub really makes this much more easier because they've set up a library

01:08:48.080 --> 01:08:54.400
that has these small snippets of reusable parts with machine learning code.

01:08:54.400 --> 01:09:00.440
It's published, you know, it's easy to discover things that, you know, are efficient and,

01:09:00.440 --> 01:09:05.640
you know, just really use the best out that's available out there rather than, you know,

01:09:05.640 --> 01:09:09.920
spending hours of your time actually trying to figure it out yourself.

01:09:09.920 --> 01:09:14.760
And then similar to this is Model Depot, which is a place where you can find and share

01:09:14.760 --> 01:09:19.520
optimized, pre-trained machine learning models that are perfect for your development needs,

01:09:19.520 --> 01:09:22.400
similar to what Model Zoo was before.

01:09:22.400 --> 01:09:33.520
Okay. Yeah. Interesting. So we're running low on time, any final thoughts or predictions

01:09:33.520 --> 01:09:40.920
before we close out? I guess, you know, I'm looking forward to the day

01:09:40.920 --> 01:09:47.360
when we will thank AI for taking away our work of holism and then let us get on with the

01:09:47.360 --> 01:09:52.800
job of being human. I mean, that would be pretty cool. Yeah. Yes. You know, we talked a lot

01:09:52.800 --> 01:10:00.400
about different tools, different research projects that have come up in different parts

01:10:00.400 --> 01:10:06.960
of the AI pipeline, how these are applicable in real life examples.

01:10:06.960 --> 01:10:13.600
So I mean, it's been a pretty eventful year and I can't wait for 2019 and, you know,

01:10:13.600 --> 01:10:18.400
be the downpour of publications. Come on one by one.

01:10:18.400 --> 01:10:25.040
All right, Cedar. Well, thanks so much for once again, kind of spending the time to kind

01:10:25.040 --> 01:10:27.520
of go through this stuff with us. Thank you.

01:10:30.480 --> 01:10:36.480
All right, everyone. That's our show for today for more information on CIDA or any of the topics

01:10:36.480 --> 01:10:43.920
covered in this episode, visit twimlai.com slash talk slash 218. You can also follow along with

01:10:43.920 --> 01:10:53.840
our AI rewind 2018 series at twimlai.com slash rewind 18. As always, thanks so much for listening

01:10:53.840 --> 01:11:08.720
and catch you next time. Happy holidays.

