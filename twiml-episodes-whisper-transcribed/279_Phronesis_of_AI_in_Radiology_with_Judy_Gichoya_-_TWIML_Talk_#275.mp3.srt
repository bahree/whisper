1
00:00:00,000 --> 00:00:16,320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

2
00:00:16,320 --> 00:00:21,480
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:21,480 --> 00:00:31,940
I'm your host Sam Charrington.

4
00:00:31,940 --> 00:00:36,660
You are invited to join us for the very first Twimblecon conference which will focus on

5
00:00:36,660 --> 00:00:41,200
the tools, technologies and practices necessary to scale the delivery of machine learning

6
00:00:41,200 --> 00:00:43,720
and AI in the enterprise.

7
00:00:43,720 --> 00:00:48,600
The event will be held October 1st and 2nd in San Francisco and early bird registration

8
00:00:48,600 --> 00:00:58,840
is open today at twimblecon.com, again that's twimblecon.com, I can't wait to see you there.

9
00:00:58,840 --> 00:01:04,680
Alright everyone, I am on the line with Judy Gesoya, Judy is an interventional radiology

10
00:01:04,680 --> 00:01:10,960
fellow at the Daughter Institute at Oregon Health and Science University as well as a co-organizer

11
00:01:10,960 --> 00:01:15,680
of Black and AI, Judy, welcome to this week in machine learning and AI.

12
00:01:15,680 --> 00:01:18,280
Thank you Sam, I'm very excited to be here.

13
00:01:18,280 --> 00:01:24,720
It is great to have you on the show and I'm really looking forward to diving into this

14
00:01:24,720 --> 00:01:34,040
conversation about the intersection of radiology and AI but before we do that, how did

15
00:01:34,040 --> 00:01:37,840
you come to start working in this space?

16
00:01:37,840 --> 00:01:46,200
So I had some interest and experience already working in digital health in medicine.

17
00:01:46,200 --> 00:01:52,200
I just a little bit of background about me, I went to medical school in Kenya and during

18
00:01:52,200 --> 00:01:57,800
that time I was taking care of HIV patients and it was really difficult to sort of just

19
00:01:57,800 --> 00:02:04,200
use paper and Excel spreadsheets and so I gravitated to sort of walking on an open source

20
00:02:04,200 --> 00:02:11,840
medical record system and so I started walking, you know, deeply deploying, programming and

21
00:02:11,840 --> 00:02:14,280
understanding how these systems are deployed.

22
00:02:14,280 --> 00:02:22,800
And so at that point I did move to the US to do masters in clinical informatics and during

23
00:02:22,800 --> 00:02:29,200
that point I decided that I wanted to get a clinical specialization just because the

24
00:02:29,200 --> 00:02:33,840
truth is you get a little more respect if you have a clinical background, especially

25
00:02:33,840 --> 00:02:38,840
when talking to clinicians, but also that I realized that what I was passionate about

26
00:02:38,840 --> 00:02:44,400
which was building health systems that I use by doctors would be very difficult if you

27
00:02:44,400 --> 00:02:46,880
are not using them yourself.

28
00:02:46,880 --> 00:02:54,760
So I subsequently went to Indiana University for my radiology residency and the program

29
00:02:54,760 --> 00:02:59,840
that allowed me to grow a lot, I had lots of opportunities to work on informatics on

30
00:02:59,840 --> 00:03:06,680
a national level and so when I was attending the American College of Radiology Anum meeting

31
00:03:06,680 --> 00:03:11,920
and that's around also when Joff Hinton, you know, publicly say that we should stop

32
00:03:11,920 --> 00:03:18,120
training radiologists, you know, there was this sort of force and just uprising of physicians

33
00:03:18,120 --> 00:03:24,440
who could do radiology and computer science to figure out why, you know, what was this

34
00:03:24,440 --> 00:03:25,440
deep planning?

35
00:03:25,440 --> 00:03:26,440
Why was it a threat?

36
00:03:26,440 --> 00:03:32,480
And so I would say it was just right place, right time that I got involved into this sort

37
00:03:32,480 --> 00:03:38,000
of this fantastic intersection of tech and medicine.

38
00:03:38,000 --> 00:03:46,720
And that uprising as you call it resulted in a paper that you're the lead author on called

39
00:03:46,720 --> 00:03:52,840
Frenesis of AI and Radiology, superhuman meets natural stupidity.

40
00:03:52,840 --> 00:03:59,200
I think poking fun at this idea that AI will put radiologists out of business.

41
00:03:59,200 --> 00:04:05,640
Yes, you know, some, it was very, very, first of all, you know, in medicine we don't publish

42
00:04:05,640 --> 00:04:13,600
an archive, you know, and I have, so it's really such a big shift to be honest and one

43
00:04:13,600 --> 00:04:19,720
of my really good friends who's, Satoshi, who's on the paper and we've known each other

44
00:04:19,720 --> 00:04:26,520
on global health and work as good collaborators right now and, you know, it took me a lot

45
00:04:26,520 --> 00:04:31,920
of evenings to try and explain to him what radiology does or drag him to the reading room

46
00:04:31,920 --> 00:04:36,720
and just, you know, explain to him and just show him the workflow.

47
00:04:36,720 --> 00:04:42,400
And, you know, at this, when we came to sort of this advert and, I mean, Andrew M. was

48
00:04:42,400 --> 00:04:46,960
saying, you know, we're doing better than radiologists every, I mean, every article every

49
00:04:46,960 --> 00:04:53,640
week just talked about how they had better performance, you know, and it was just sort

50
00:04:53,640 --> 00:05:01,080
of like almost like, okay, maybe you should focus your efforts on maybe these areas that

51
00:05:01,080 --> 00:05:04,680
would actually make you make us all better.

52
00:05:04,680 --> 00:05:11,880
And to be honest, I think someone who's also an engineer, you get a little bit cocky when

53
00:05:11,880 --> 00:05:13,320
you start to solve problems.

54
00:05:13,320 --> 00:05:19,800
I think you always, it's very easy and I've suffered from this to dismiss domain expertise

55
00:05:19,800 --> 00:05:24,840
and decide, you know, I can do this or all, that's just a few lines of code.

56
00:05:24,840 --> 00:05:29,540
And so these people was sort of like, to be honest, we were hoping that it would get

57
00:05:29,540 --> 00:05:35,600
debated at one of the debates in machine learning conferences and maybe we get to debate

58
00:05:35,600 --> 00:05:39,280
it today at this weekend machine learning.

59
00:05:39,280 --> 00:05:46,560
That's funny because we just recently hosted our first debate, if you want to call it,

60
00:05:46,560 --> 00:05:55,560
not a formal debate, but a discussion around some recent announcements by open AI on

61
00:05:55,560 --> 00:05:56,560
there.

62
00:05:56,560 --> 00:06:02,080
I'm not going to go into the details there, but it was really interesting and we'd like

63
00:06:02,080 --> 00:06:03,080
to do more of it.

64
00:06:03,080 --> 00:06:09,080
So we'll talk offline about who the right people at the table should be for this debate

65
00:06:09,080 --> 00:06:11,480
and maybe pull something together.

66
00:06:11,480 --> 00:06:17,920
But it sounds like you're saying that part of the problem is that the people that are

67
00:06:17,920 --> 00:06:24,320
out there proclaiming that we've solved radiology don't really understand radiology.

68
00:06:24,320 --> 00:06:25,320
Absolutely.

69
00:06:25,320 --> 00:06:27,760
Can you elaborate on that?

70
00:06:27,760 --> 00:06:31,360
Have you figured out what it is that they don't understand or just that they don't understand

71
00:06:31,360 --> 00:06:32,360
it?

72
00:06:32,360 --> 00:06:36,520
Well, they just, they don't understand it.

73
00:06:36,520 --> 00:06:43,120
And this is not also, to be honest, both sides can be very quirky.

74
00:06:43,120 --> 00:06:48,960
The physician's medicine is very hierarchical, you wait for your turn.

75
00:06:48,960 --> 00:06:53,560
In fact, I remember my teachers would say medicine is a club, we decide when and who

76
00:06:53,560 --> 00:06:54,560
joins it.

77
00:06:54,560 --> 00:07:01,680
So there's also the sort of like the nonchalant comfort, but I'm a doctor, what do you

78
00:07:01,680 --> 00:07:03,240
mean you can do my work?

79
00:07:03,240 --> 00:07:05,240
But there's also the other side of engineers.

80
00:07:05,240 --> 00:07:10,480
And so I think what's missing is people who sit at the intersection of both, you know,

81
00:07:10,480 --> 00:07:16,480
when you can easily describe things to the engineers and also sort of bring a little

82
00:07:16,480 --> 00:07:19,840
bit of reason to the physicians.

83
00:07:19,840 --> 00:07:26,520
And you know, I have sort of, back times with a little bit of stigma actually, because

84
00:07:26,520 --> 00:07:31,080
in medicine, if you do anything but clinical work, now it's being embraced, but it was

85
00:07:31,080 --> 00:07:34,560
always assumed that you're not good enough to be a doctor.

86
00:07:34,560 --> 00:07:36,720
You know, you're not doing well.

87
00:07:36,720 --> 00:07:39,720
If you do research, they're like, oh, yeah, he's good at research, but he's a terrible

88
00:07:39,720 --> 00:07:40,720
doctor.

89
00:07:40,720 --> 00:07:45,440
If you do IT or deep learning, you know, people expect you to not do a good job.

90
00:07:45,440 --> 00:07:50,200
And so you have to fast fight that bias and stigma and do a very good clinical job as

91
00:07:50,200 --> 00:07:53,640
a doctor so that people get that respect from you.

92
00:07:53,640 --> 00:07:59,920
So there's all these sort of soft skills that you have to stimulate, even before we can

93
00:07:59,920 --> 00:08:04,840
have a candid conversation where we have, you know, sort of engineers embedded in the healthcare

94
00:08:04,840 --> 00:08:12,120
system to innovate, you know, and also to address problems that actually make an impact.

95
00:08:12,120 --> 00:08:22,640
And so, for example, you know, we, we notice a little bit, so I think on online some

96
00:08:22,640 --> 00:08:31,600
Altman recently wrote on CNBC that, you know, he'd rather have computer radiologists, more

97
00:08:31,600 --> 00:08:38,080
than human radiologists, and you have these sort of people posting every, every week.

98
00:08:38,080 --> 00:08:43,480
And it takes a long time to be a radiologist, you know, and it's, I think a little bit,

99
00:08:43,480 --> 00:08:48,200
I was having a conversation with a friend of mine, you become emotionally attached.

100
00:08:48,200 --> 00:08:52,240
And it's not just an attack on a profession, it becomes an attack on you.

101
00:08:52,240 --> 00:08:59,520
You know, you're not good enough, you know, or you're indispensable, you know, dispensable.

102
00:08:59,520 --> 00:09:05,760
And that can be a really big challenge also just to reconcile that personal conflict.

103
00:09:05,760 --> 00:09:09,760
And, you know, I'm glad that radiology community has completely moved from this.

104
00:09:09,760 --> 00:09:14,600
I mean, if you look at the last year, RSNN, you money challenged the top groups, the top

105
00:09:14,600 --> 00:09:21,440
10 groups, what are radiologists, you know, and if you, we have a new radiology AI journal

106
00:09:21,440 --> 00:09:29,720
that's online, that's, you know, good quality, pure reviewed, and it's focused on AI.

107
00:09:29,720 --> 00:09:35,120
I mean, so it's amazing what in three years, you know, the rapid, that uprising,

108
00:09:35,120 --> 00:09:39,520
the rapid transformation that came to sort of a little bit guard the profession,

109
00:09:39,520 --> 00:09:45,520
but also to bring a little bit of common sense in discussing what's the best way forward,

110
00:09:45,520 --> 00:09:50,200
because the truth is AI is coming, you know, we can't just control and make sure

111
00:09:50,200 --> 00:09:53,880
that you're not wasteful and we get the benefit of it.

112
00:09:53,880 --> 00:10:01,320
It sounds like you're saying on one hand, kind of the, this constant, you know,

113
00:10:01,320 --> 00:10:09,520
derision of radiologists relative to AI, you know, isn't empathetic and it's not productive,

114
00:10:09,520 --> 00:10:12,280
but also that is wrong.

115
00:10:12,280 --> 00:10:19,920
And the examples that people try to out to demonstrate that AI has exceeded

116
00:10:19,920 --> 00:10:25,360
the capability of radiologists are, well, you tell me how you characterize them,

117
00:10:25,360 --> 00:10:32,720
you know, cherry-picked or incomplete or, you know, not representative of what's actually

118
00:10:32,720 --> 00:10:35,200
happening in radiology.

119
00:10:35,200 --> 00:10:40,560
And maybe as a, you know, taking a step back from that question, maybe a good way to get

120
00:10:40,560 --> 00:10:46,480
there is to have you explain, you know, when you think of, you know, radiology and kind

121
00:10:46,480 --> 00:10:53,200
of, in its broadest sense in what radiologists are doing, that image classifiers aren't

122
00:10:53,200 --> 00:10:57,400
doing, you know, maybe we need to understand that more broadly.

123
00:10:57,400 --> 00:10:58,400
Yeah.

124
00:10:58,400 --> 00:11:05,000
So, I think you have to think about the sort of like the pixel level, pixel walk, which

125
00:11:05,000 --> 00:11:09,800
is like the computer vision, can you diagnose pneumonia, can you diagnose a fracture, can

126
00:11:09,800 --> 00:11:12,280
you diagnose a brain bleed?

127
00:11:12,280 --> 00:11:19,880
And, you know, that's, I think a lot of people have focused on that, you know, because one

128
00:11:19,880 --> 00:11:26,600
radiologist, especially if you've always digitized our, you know, our studies and their report.

129
00:11:26,600 --> 00:11:30,840
So as long as you have access to the data, to be honest, you don't need to be a radiologist

130
00:11:30,840 --> 00:11:34,920
to start to, you know, do some of this work.

131
00:11:34,920 --> 00:11:38,720
But, you know, some, I think there'll be two things.

132
00:11:38,720 --> 00:11:43,960
I think impactful AI will be embedded in healthcare institutions.

133
00:11:43,960 --> 00:11:49,520
In fact, today, if I was, my advice to sort of healthcare managers, instead of buying

134
00:11:49,520 --> 00:11:55,880
purchasing AI algorithms, I would say rent them or give them a place within your institution

135
00:11:55,880 --> 00:11:59,040
to innovate and do the work and experiment.

136
00:11:59,040 --> 00:12:04,960
And the reason is, I mean, your witness to this, the technology changes so much every

137
00:12:04,960 --> 00:12:11,880
week, you know, and for you to sort of, if you understand the purchasing process of medicine,

138
00:12:11,880 --> 00:12:16,440
it takes a long time by the time you, I mean, implementations of electronic medical records,

139
00:12:16,440 --> 00:12:18,440
system ticks, months and years.

140
00:12:18,440 --> 00:12:23,680
So if my implementation of your algorithm ticks, you know, nine months, by the time I'm

141
00:12:23,680 --> 00:12:29,200
using it, it's updated, unless you have sort of new ways, and especially if you have to

142
00:12:29,200 --> 00:12:33,360
get FDA approval for every algorithm, you're not going to go back in nine months before

143
00:12:33,360 --> 00:12:36,400
you've made your money to get a new approval.

144
00:12:36,400 --> 00:12:44,080
And so I see a big role, honestly, a big, if I was to say, what do I think are the big

145
00:12:44,080 --> 00:12:47,400
grand challenges for AI in medicine?

146
00:12:47,400 --> 00:12:49,800
It's really the workflow processes.

147
00:12:49,800 --> 00:12:52,120
And you've, I mean, I'm a big fan of the podcast.

148
00:12:52,120 --> 00:12:59,080
You had a few guests recently, like Microsoft talking about the notes, you know, enabling

149
00:12:59,080 --> 00:13:06,000
the voice transcription on the notes for doctors, because physicians spend a lot of time at home

150
00:13:06,000 --> 00:13:08,720
trying to catch up with documentation.

151
00:13:08,720 --> 00:13:12,800
And we also have things around population health management.

152
00:13:12,800 --> 00:13:20,360
So for example, I had a patient in May last year that I took care of and unfortunately

153
00:13:20,360 --> 00:13:26,360
we had a congenital blood disease ended up getting institutional disease.

154
00:13:26,360 --> 00:13:32,360
And this patient of mine would come to the hospital three times a week, because that's all

155
00:13:32,360 --> 00:13:36,400
all dialysis patients come for dialysis three times a week.

156
00:13:36,400 --> 00:13:41,560
So literally someone in the healthcare system, so this patient, unfortunately, 10 years

157
00:13:41,560 --> 00:13:44,720
ago, he had had an IVC filter.

158
00:13:44,720 --> 00:13:49,080
This is, you can think about this like an inverted umbrella, which you put in your vein to

159
00:13:49,080 --> 00:13:52,200
sort of catch the cloth so that they don't go to your lungs.

160
00:13:52,200 --> 00:13:57,880
And he had displaced, you know, after an accident, and it was never removed.

161
00:13:57,880 --> 00:14:02,880
If you said IVC filters, there are lots of lawyers who like to sue about this.

162
00:14:02,880 --> 00:14:09,800
And but the challenging part was, you know, these filters are placed in multiple institutions,

163
00:14:09,800 --> 00:14:11,720
some of them are not placed under imaging.

164
00:14:11,720 --> 00:14:16,040
So they just, it's just like, you know, it's like the wild west, you know, about where

165
00:14:16,040 --> 00:14:17,920
these filters drop in.

166
00:14:17,920 --> 00:14:24,480
And at the same time, why couldn't we, we must have had a, you know, an abdomen radiograph

167
00:14:24,480 --> 00:14:26,400
that was done for something different.

168
00:14:26,400 --> 00:14:31,320
And, you know, the person reading it, interpreting it may not really realize, okay, this is an

169
00:14:31,320 --> 00:14:35,600
institutional disease patient who probably should not be having this old type of filter

170
00:14:35,600 --> 00:14:37,800
that should affect their veins.

171
00:14:37,800 --> 00:14:40,400
But you know, sort of like that addition of value.

172
00:14:40,400 --> 00:14:43,920
So maybe the X-ray was done for constipation or something else.

173
00:14:43,920 --> 00:14:47,920
So they focused on the constipation and they answered the question, the clinical question

174
00:14:47,920 --> 00:14:50,200
that was asked before performing the study.

175
00:14:50,200 --> 00:14:55,480
But with an AI running on the background, and this is a project that we are working on,

176
00:14:55,480 --> 00:15:03,520
with Adam, my colleague at Emory, is to create an automatic, it's a computer detection problem.

177
00:15:03,520 --> 00:15:06,080
And we create an automatic filter detector.

178
00:15:06,080 --> 00:15:11,160
We run this every night, you know, at the end of the day with all our radiographs.

179
00:15:11,160 --> 00:15:16,600
And if a patient has a filter, is anticoagulated, we send a message to their doctor and say,

180
00:15:16,600 --> 00:15:20,200
hey, the next time they're coming in, you know, you may want to talk to them about these

181
00:15:20,200 --> 00:15:25,520
IVC filter, refer them to a clinic to get this evaluated for removal.

182
00:15:25,520 --> 00:15:31,880
So literally a decision that it's not really anyone's fault, but just sort of, we're so

183
00:15:31,880 --> 00:15:37,640
narrow-minded and focused on the clinical problem at hand or for the day, we completely

184
00:15:37,640 --> 00:15:41,960
miss the bigger picture of this patient, and that means he can never get a renal transplant

185
00:15:41,960 --> 00:15:47,560
because he doesn't have the right veins, the old scar done after the IVC filter placement.

186
00:15:47,560 --> 00:15:53,640
So this type of clinical questions, they actually come out naturally when you're embedded

187
00:15:53,640 --> 00:15:55,840
in the healthcare system.

188
00:15:55,840 --> 00:16:02,800
You know, I can go on and on with some really low-hanging fruits, and you know, it's very

189
00:16:02,800 --> 00:16:09,840
difficult if you're not sort of bringing this, you know, cross-pollination of teams to

190
00:16:09,840 --> 00:16:11,960
sort of answer these questions.

191
00:16:11,960 --> 00:16:16,880
And that was actually, I would say, if you read this paper, the biggest thing is actually

192
00:16:16,880 --> 00:16:23,720
the table one, landing passes in radiology, which is something that has been well-studied

193
00:16:23,720 --> 00:16:27,200
by informaticians even before this era of deep planning.

194
00:16:27,200 --> 00:16:31,720
And even give you some examples of things that you could do as an application that would

195
00:16:31,720 --> 00:16:36,360
definitely augment and value to the radiologists.

196
00:16:36,360 --> 00:16:38,200
Let's walk through that table.

197
00:16:38,200 --> 00:16:42,080
What are some of the big biases that come up?

198
00:16:42,080 --> 00:16:48,160
So for example, you could have a confirmation bias.

199
00:16:48,160 --> 00:16:53,520
This is, for example, I may say, you know, man, this is pneumonia, and you keep searching

200
00:16:53,520 --> 00:16:54,520
for evidence.

201
00:16:54,520 --> 00:16:58,360
You know, you go back to the electronic medical record system, you call the doctor, you're

202
00:16:58,360 --> 00:17:00,120
like, oh, you think this is pneumonia, right?

203
00:17:00,120 --> 00:17:08,720
So you have sort of this preformed bias, and an AI tool could probably scout all the information

204
00:17:08,720 --> 00:17:13,960
that's relevant already about that patient and present it in a consumable way for the radiologists

205
00:17:13,960 --> 00:17:14,960
to interpret.

206
00:17:14,960 --> 00:17:21,280
It could also give, you know, best if the AI tool was sort of not blind to the previous

207
00:17:21,280 --> 00:17:22,280
studies.

208
00:17:22,280 --> 00:17:28,840
It could say, you know, Judy, some of these studies that have been done in the past, which

209
00:17:28,840 --> 00:17:35,080
have a similar sort of pixel appearance like this, were interpreted by these five radiologists

210
00:17:35,080 --> 00:17:39,000
who, and we definitely know this high variability, but we said most of the radiologists thought

211
00:17:39,000 --> 00:17:43,680
that this was something else like tuberculosis, not pneumonia.

212
00:17:43,680 --> 00:17:50,320
And so it just gives you, you can get differential alternative diagnosis and, you know, then

213
00:17:50,320 --> 00:17:55,800
contextually be like, okay, maybe it was a pneumonia or just raised the possibility

214
00:17:55,800 --> 00:18:00,600
of both diagnosis, which I think is a big value.

215
00:18:00,600 --> 00:18:06,880
Something else that happens a lot is this satisfaction of report or satisfaction of such,

216
00:18:06,880 --> 00:18:10,000
and actually satisfaction of report which is different.

217
00:18:10,000 --> 00:18:17,520
So for example, if you saw the previous radiologists who read a study said, oh, you know, I have,

218
00:18:17,520 --> 00:18:19,040
this is cancer.

219
00:18:19,040 --> 00:18:23,040
And if you read the report before looking at the images to form, first form your mental

220
00:18:23,040 --> 00:18:28,560
representation and understanding of what the problem is, you could say, you could perpetuate

221
00:18:28,560 --> 00:18:31,560
that diagnosis sometimes even when it's wrong.

222
00:18:31,560 --> 00:18:39,760
And so one, some of the deep learning techniques that can be helped here is, for example, volumetric

223
00:18:39,760 --> 00:18:45,120
just measurement, you know, you could measure the size of the otter, the size of a tumor

224
00:18:45,120 --> 00:18:46,120
of multiple studies.

225
00:18:46,120 --> 00:18:52,040
I mean, that, that task, specifically, is actually kind of repetitive and tedious for

226
00:18:52,040 --> 00:18:53,040
our radiologists.

227
00:18:53,040 --> 00:18:58,560
It's easier for me to have these representations, you know, on images and presented easily,

228
00:18:58,560 --> 00:19:01,680
I'm like, okay, that's, that looks like accurate measurements.

229
00:19:01,680 --> 00:19:03,720
And the computer is, we know this.

230
00:19:03,720 --> 00:19:08,240
It's very accurate at determining volume and maybe even more consistent that radiologists.

231
00:19:08,240 --> 00:19:16,400
And so it can sort of help instead of just me glancing and being like what we call eyeballing

232
00:19:16,400 --> 00:19:18,640
and being like, oh, that tumor looks the same size.

233
00:19:18,640 --> 00:19:24,960
It can help sort of even give more additional findings or additional data points to be like,

234
00:19:24,960 --> 00:19:30,600
yeah, I can tell you that this tumor grew by 20%, which, you know, even just understanding

235
00:19:30,600 --> 00:19:34,840
maybe an event, a timeline of the patient, maybe they got changed the chemotherapy three

236
00:19:34,840 --> 00:19:40,640
months ago, can have an impact more than, because 20% change in a tumor in a patient

237
00:19:40,640 --> 00:19:45,400
on chemotherapy versus one who's not on chemotherapy, completely means different things.

238
00:19:45,400 --> 00:19:48,800
So there's always this sort of things that are going on.

239
00:19:48,800 --> 00:19:52,960
It's not just sitting at a computer, looking at the image and be like, okay, this is pneumonia,

240
00:19:52,960 --> 00:19:55,280
there's always this clinical context.

241
00:19:55,280 --> 00:20:03,920
And I feel if you had an AI assistant or whoever an assistant at the back who was consistently,

242
00:20:03,920 --> 00:20:11,280
you know, knows the correct information to send to you because the EMR, that is the electronic

243
00:20:11,280 --> 00:20:17,000
medical record system is another monstrous dump of information would just present you

244
00:20:17,000 --> 00:20:19,680
just in time intervention that made sense.

245
00:20:19,680 --> 00:20:27,200
I think those are some of the true augmentation and true actually benefits we can rip from AI.

246
00:20:27,200 --> 00:20:36,960
And some other examples could be, you know, the satisfaction of such a mention and this

247
00:20:36,960 --> 00:20:42,160
because sometimes you just see the tumor. So for example, maybe we usually get some studies

248
00:20:42,160 --> 00:20:50,000
around cities of the neck and, you know, you go in there and you see a big fracture,

249
00:20:50,000 --> 00:20:54,560
maybe, you know, compressing the cord. I mean, that's very dramatic and drastic and you

250
00:20:54,560 --> 00:20:57,960
call the ordering doctor and you tell them, you know, I see a big fracture.

251
00:20:57,960 --> 00:21:04,360
And you know, this is probably a patient who just has, you know, came in because of trauma.

252
00:21:04,360 --> 00:21:10,520
But you completely miss the big cancer that's just at the edge of the film, you know, on

253
00:21:10,520 --> 00:21:14,960
the upper lungs because you, you know, you're happy. You're like, okay, I found the findings.

254
00:21:14,960 --> 00:21:18,600
This patient is going to get treated this way. And those incidentals are really the places

255
00:21:18,600 --> 00:21:25,920
where radiologists shine. But the AI could learn our blind spots and just ask, you know,

256
00:21:25,920 --> 00:21:31,480
have just like another system that helps me be more cognizant of my blind spots. And

257
00:21:31,480 --> 00:21:35,960
this is maybe not something documented, but this is how we learn, you know, every time,

258
00:21:35,960 --> 00:21:40,840
you know, what you've missed and what you've caught and you start getting better and, you

259
00:21:40,840 --> 00:21:45,600
know, like my such pattern every time I look at the city abdomen is to repetitively look

260
00:21:45,600 --> 00:21:51,000
through the pancreas because those pancreatic tumors are very subtle and, you know, are

261
00:21:51,000 --> 00:21:56,080
places where you can make a huge difference if you have early diagnosis. So I think we

262
00:21:56,080 --> 00:22:01,680
have real good areas. And the reason why I actually just focused on buyers is because

263
00:22:01,680 --> 00:22:09,920
this, we know this exists. We know that there's a big cost of medical errors and that there's

264
00:22:09,920 --> 00:22:18,040
a pretty, pretty big opportunity to save costs and also to reduce death because medical

265
00:22:18,040 --> 00:22:22,440
errors is the fat living cost of death in the United States.

266
00:22:22,440 --> 00:22:34,000
It strikes me that this whole area of bias applied to radiology and AI is paradoxical in

267
00:22:34,000 --> 00:22:42,200
the sense that you could easily argue that the existence of bias and human and radiologists

268
00:22:42,200 --> 00:22:50,960
is the reason why we should be kind of all rooting for AI's to take over and for all these

269
00:22:50,960 --> 00:22:58,560
stories about, you know, AI's up-performing radiologists to be true. At the same time,

270
00:22:58,560 --> 00:23:04,760
these AI's are all trained on, you know, data sets that were labeled by humans and the

271
00:23:04,760 --> 00:23:11,240
biases that you've outlined, you mentioned, you know, three or four here in the paper

272
00:23:11,240 --> 00:23:17,000
you've got, you know, eight to ten, you know, these are all kind of embedded in our training

273
00:23:17,000 --> 00:23:22,880
data sets. So, you know, the example you mentioned with an image that's got, you know, the

274
00:23:22,880 --> 00:23:27,920
big compression fracture, but, you know, is labeled as, you know, perhaps in a training

275
00:23:27,920 --> 00:23:33,880
data set is labeled as such, but not labeled for the, you know, the cancer that's at the

276
00:23:33,880 --> 00:23:39,920
fringe. You know, we're kind of baking that into the AI's we're building today.

277
00:23:39,920 --> 00:23:44,480
How do you kind of pick apart that paradox and is that something that you covered in the

278
00:23:44,480 --> 00:23:47,960
paper or in your research or thinking about this generally?

279
00:23:47,960 --> 00:23:56,800
So actually, we haven't covered this and I think, you know, this fairness and transparency

280
00:23:56,800 --> 00:24:02,880
for ideology. First of all, this is a huge, huge, huge, huge area. Actually, this is

281
00:24:02,880 --> 00:24:09,360
one of the areas I'm going to be focusing on as I begin my faculty position later this

282
00:24:09,360 --> 00:24:16,760
year in July. And one is, even just the explainability, you know, like, why did it get here? You

283
00:24:16,760 --> 00:24:21,760
know, you know, this idea of the black box AI, and this is not a problem, you know, we

284
00:24:21,760 --> 00:24:28,960
always know that image net is the sort of the parent that all these AI tools at this

285
00:24:28,960 --> 00:24:34,680
and computer vision are trained in for ideology. And the moment you say, yeah, we started

286
00:24:34,680 --> 00:24:41,360
from dogs, cats and aeroplanes and cars. I mean, the radiologist and it goes back to the,

287
00:24:41,360 --> 00:24:45,360
you know, the other discussion we had during this session where I said this a little bit

288
00:24:45,360 --> 00:24:50,600
of soft power because when you say, oh, man, we started from cats and dogs, then you're

289
00:24:50,600 --> 00:24:57,240
very far from replacing me or this, I mean, how do you come back to grayscale imaging,

290
00:24:57,240 --> 00:25:04,480
which is radiology? But, you know, so there's one area about a gap in having more explainability.

291
00:25:04,480 --> 00:25:11,120
But the issue around even bias, even North, the application and the downstream consequences

292
00:25:11,120 --> 00:25:18,720
on policy and patient outcomes, the bias, I see radiology and I have to tell you that the

293
00:25:18,720 --> 00:25:23,280
engineers who are working on this are not looking at these sort of questions, as far as

294
00:25:23,280 --> 00:25:30,800
I know of, why maybe the AI's that will start to build help us open our eyes to the biases

295
00:25:30,800 --> 00:25:36,480
that exist in medicine. We know this, but maybe it's that maybe the AI revolution will

296
00:25:36,480 --> 00:25:42,720
help us sort of, you know, open this, open up these biases and come up with systemic

297
00:25:42,720 --> 00:25:51,400
changes that actually end up improving our patients. And so, for example, there is, you

298
00:25:51,400 --> 00:25:56,640
know, with their hospital scoring and the surgical scoring, people get very dinged for

299
00:25:56,640 --> 00:26:03,600
re-admissions within 30 days or performing surgeries with poor outcomes. So, what the

300
00:26:03,600 --> 00:26:10,000
doctors do, this is not documented, but you know it, is you self-select, you know. So,

301
00:26:10,000 --> 00:26:15,520
if and also they sell selection for specialties. So, there's a reason why most of my calls,

302
00:26:15,520 --> 00:26:19,920
I get them in the middle of the night and at that point, the excuse is that the patient

303
00:26:19,920 --> 00:26:25,440
is not a good surgical candidate for a procedure because, you know, it's the middle of the night.

304
00:26:25,440 --> 00:26:29,840
And so, we have all these subtle things that you as a doctor in the healthcare system,

305
00:26:29,840 --> 00:26:37,840
you kind of know, but the people building these AI tools are a little blinded too. And we don't

306
00:26:37,840 --> 00:26:44,560
talk, to be honest, we don't talk about them as biases in, you know, in sort of like the

307
00:26:44,560 --> 00:26:50,720
stricter sense and sort of like this new discipline around bias and fairness and accountability

308
00:26:50,720 --> 00:26:58,880
in AI, there just things that happen, you know. And so, maybe we will reap benefits, but only if we

309
00:26:58,880 --> 00:27:06,800
start to look at the correct sort of information and again, work with multidisciplinary teams

310
00:27:06,800 --> 00:27:13,760
that we can be able to address this issue. And specifically, if you look at, for example,

311
00:27:13,760 --> 00:27:18,160
the engineer who gets, let's say you go, you buy a bunch of data and you come in and say,

312
00:27:18,160 --> 00:27:22,880
oh, I trained on pneumonia, how will they know that they completely use the wrong data set

313
00:27:22,880 --> 00:27:26,960
if they're not working with radiologists? So, I think it's going to be sort of this

314
00:27:28,400 --> 00:27:35,600
union of both teams realizing that no, I'm knows it all. And we will discover things that

315
00:27:35,600 --> 00:27:42,800
are embarrassing and things that forces to rethink how we provide healthcare. But it's only going

316
00:27:42,800 --> 00:27:47,520
to come if the hype is not, I'm going to replace our radiologists because you will just replace

317
00:27:47,520 --> 00:27:52,720
our radiologists with another radiologist. Maybe one who doesn't get tired because they can keep going

318
00:27:52,720 --> 00:27:59,920
as long as they have electricity. But the subtle biases will be perpetuated and maybe amplified

319
00:27:59,920 --> 00:28:05,200
because I mean, if there's no right now, we don't have a framework for continuous monitoring

320
00:28:05,200 --> 00:28:11,520
or mandates for continuous monitoring in production. And we will continue to experience those

321
00:28:11,520 --> 00:28:19,680
things. So, I know I was in a presentation where the Harvard group had trained an AI to detect

322
00:28:19,680 --> 00:28:26,640
breast cancers. And you know, they got collaborators. I think this would be a big deal to validate

323
00:28:26,640 --> 00:28:34,160
the algorithm around Detroit. And it really caught a lot of cancers in Detroit. And they

324
00:28:34,160 --> 00:28:42,000
paused the question, why do we think it caught a lot of cancers in Detroit? And the reason was the

325
00:28:42,000 --> 00:28:48,240
patients who come to the Harvard healthcare system come for regular mammograms for screening

326
00:28:48,240 --> 00:28:54,560
Ali. And the Detroit patients came in late when they had the diagnosis, you know, head alarm

327
00:28:54,560 --> 00:28:59,920
and they already had cancer. So, if you didn't understand who was coming to the healthcare system,

328
00:28:59,920 --> 00:29:06,560
you'd be bragging how your AI is performing better, yet it just represents how healthcare is

329
00:29:06,560 --> 00:29:11,840
delivered and the health-seeking behaviors of the two different populations in both areas.

330
00:29:12,400 --> 00:29:20,320
So, the paper kind of invokes this idea of natural stupidity in the title. And you outline

331
00:29:21,120 --> 00:29:26,560
kind of three behaviors that you label as kind of representative of this natural stupidity,

332
00:29:26,560 --> 00:29:34,400
kind of the things that you see AI researchers talking about when they're not in close

333
00:29:34,400 --> 00:29:38,480
collaboration with subject matter experts. Can you outline those for us?

334
00:29:39,120 --> 00:29:46,000
Yeah, so we sort of picked up some areas and one was wishful, you know,

335
00:29:46,000 --> 00:29:53,840
in the Monics. And I think this has been amplified honestly by the media. And also like when leaders

336
00:29:53,840 --> 00:30:00,800
of group street, you know, that, oh, we are much better or we are the same as, you know, any

337
00:30:00,800 --> 00:30:07,760
specialist. And so, for example, one of the things that came up actually around the check

338
00:30:07,760 --> 00:30:15,520
net pneumonia paper was that there are many types of pneumonia. And actually, as someone who's

339
00:30:15,520 --> 00:30:22,640
walked in global health and developing countries, that when you take care of HIV patients,

340
00:30:22,640 --> 00:30:27,040
it's very different type of pneumonia. There's clinic on pneumonia and radiologic pneumonia.

341
00:30:27,040 --> 00:30:32,080
So by just saying pneumonia, you know, everyone will be like, oh man, that's really amazing. But

342
00:30:33,680 --> 00:30:39,280
when it comes to sort of like rational, just reading a negative chest x-ray, for example,

343
00:30:39,280 --> 00:30:45,440
on a HIV patient, does not mean that it's negative for disease. You could say they're not finding,

344
00:30:45,440 --> 00:30:51,440
not radiologic findings, but that does not always translate to negative for disease.

345
00:30:51,440 --> 00:30:56,640
Can you elaborate on this distinction between clinical pneumonia and radiological pneumonia?

346
00:30:57,200 --> 00:31:05,440
So, for example, radiologic would have a finding. So you may have like a consolidation that

347
00:31:05,440 --> 00:31:11,280
would be pretty obvious. So for example, a low-burning pneumonia or multifocus pneumonia. But the patient

348
00:31:11,280 --> 00:31:16,960
may, may, depending on the time or also when, where they are in terms of antibiotic treatment,

349
00:31:16,960 --> 00:31:22,480
may not have radiologic findings because of the sort of like the spectrum of disease. Or if they

350
00:31:22,480 --> 00:31:28,320
immunosuppressed to not monitor response that presents as opacities that you can see on a

351
00:31:28,320 --> 00:31:33,520
radiograph, they still are sick and you can have lab tests and just the clinical appearance of

352
00:31:33,520 --> 00:31:39,120
the picture and the history to help you diagnose, you know, clinical pneumonia. But at the same time,

353
00:31:39,120 --> 00:31:44,800
their chest x-ray will be negative. So if your algorithm is only looking at one facet,

354
00:31:44,800 --> 00:31:49,440
just the x-ray, which is maybe probably the only data available for you to look at,

355
00:31:49,440 --> 00:31:55,600
then you would completely mislabel these patients as, you know, pneumonia,

356
00:31:56,720 --> 00:32:01,840
negative for pneumonia, yet they have pneumonia. So not all pneumonia patients will have positive

357
00:32:01,840 --> 00:32:08,240
findings on a radiograph. Got it, got it. And so, advertising your marketing, your

358
00:32:08,240 --> 00:32:16,000
accomplishment as detecting, you know, some high percent of pneumonia without stating that

359
00:32:16,000 --> 00:32:22,560
it's radiologic pneumonia, which is a subset of kind of the broader things that a radiologist needs

360
00:32:22,560 --> 00:32:28,400
to be able to identify or that the medical community rather needs to be able to identify

361
00:32:29,200 --> 00:32:33,440
is misleading. That's what you're getting out there. Yes. And so, what was the next one?

362
00:32:33,440 --> 00:32:40,880
So, the next one is this assumption that human performance is decotomous. So, in this example,

363
00:32:40,880 --> 00:32:48,000
using this sort of statement, we can say that you either diagnose pneumonia as positive or negative.

364
00:32:48,000 --> 00:32:55,920
Okay. And a little bit, this is a bias of how the AI is allowed to give probabilities. You know,

365
00:32:55,920 --> 00:33:01,680
I may think that maybe 80 percent probability that this is pneumonia. But when you look at the

366
00:33:01,680 --> 00:33:08,000
performance, it's been compared to you, you just say, well, radiology, did you diagnose pneumonia

367
00:33:08,000 --> 00:33:14,800
or non pneumonia? We do, we completely, or me, that we also depend on probabilities. And those

368
00:33:14,800 --> 00:33:20,880
probabilities don't come out as a percentage. But for example, they varied a study and say,

369
00:33:20,880 --> 00:33:25,840
you know, finding some most consistent or suggestive of malignancy and less likely infectious

370
00:33:25,840 --> 00:33:31,840
etiology. That statement to a radiologist has a probability. And even for the referring doctor,

371
00:33:31,840 --> 00:33:37,280
we'll know, okay, I should really make sure that there's no cancer here before, you know, going

372
00:33:37,280 --> 00:33:42,480
the rabbit hole of infectious disease. And this one is says, okay, let's compare human performance

373
00:33:42,480 --> 00:33:50,000
and AI. But you have to also factor in the probabilities of of of certainty of diagnosis

374
00:33:50,000 --> 00:33:55,680
that the human performance look at. And so you can't just treat it as a zero, one

375
00:33:56,560 --> 00:34:02,240
phenomenon for human performance, but have a little more variability when you think about the

376
00:34:02,240 --> 00:34:08,480
machine performance. The other one we talked about was this idea of training with secondary data.

377
00:34:08,480 --> 00:34:15,440
And, you know, I have to give full disclosure here that I really love to study how humans

378
00:34:15,440 --> 00:34:22,560
work with technology, not just AI. And we know that data is a problem. The big institutions

379
00:34:22,560 --> 00:34:28,080
have the data. But even when you have the data, it's not well labeled. And like I think you

380
00:34:28,080 --> 00:34:34,080
exactly said, you may have a city for compression fracture, which was dictated as such. But if it

381
00:34:34,080 --> 00:34:41,840
or completely omitted the tumor incidental tumor, then you perpetuate sort of those biases.

382
00:34:41,840 --> 00:34:51,360
But this idea of secondary data, I think is we say that it's not enough for you to come up and

383
00:34:51,360 --> 00:34:56,400
just use secondary data without a prospective validation trial where your head to head with

384
00:34:56,400 --> 00:35:03,200
the radiologist in clinical practice and say I'm better than pneumonia. You know, and the examples

385
00:35:03,200 --> 00:35:09,520
that give here are, for example, you could say it's not unusual when you're reading at 7 a.m.

386
00:35:09,520 --> 00:35:15,120
you're reading the ICU films. You just say stable and stable. It doesn't mean normal. You know,

387
00:35:15,120 --> 00:35:21,680
and if your algorithm doesn't your NLP tool doesn't recognize this, then you could this,

388
00:35:21,680 --> 00:35:26,800
especially these groups, tend to be classified as negative. But you could have a patient who's

389
00:35:26,800 --> 00:35:31,920
very sick and tubated with bilateral gestives and pneumothorax. But what I'm trying to communicate

390
00:35:31,920 --> 00:35:38,160
to the ICU doctor is that nothing has changed. The tubes and lines are in stable position.

391
00:35:38,160 --> 00:35:44,160
The appearance, the opacity on the chest structure are stable. So it helps them determine,

392
00:35:44,160 --> 00:35:50,320
okay, is the patient, you know, getting better? But that's not the only metric that they use.

393
00:35:50,320 --> 00:35:55,280
But you know, when I'm looking at tubes and lines, I'm checking how they moved in position so that

394
00:35:55,280 --> 00:36:04,000
they need to be in position because that's sort of the purpose of an ICU team. And so to say that

395
00:36:04,000 --> 00:36:12,400
you're looking at a one time, you know, look where you see some radiologists three or four. Have

396
00:36:12,400 --> 00:36:17,440
them look at studies and then you come up and say, man, I can do better than radiologists. I think

397
00:36:17,440 --> 00:36:23,280
is inaccurate, which, you know, which radiologists are you doing better than the chest experts? Are you

398
00:36:23,280 --> 00:36:30,160
doing better than the resident who's just touching out to be a radiologist? I think relying on secondary

399
00:36:30,160 --> 00:36:37,520
data is not bad, but I don't think it gives us enough mandate and enough platform to say that

400
00:36:37,520 --> 00:36:44,880
we are better than a certain profession. And we need and calls for a calm and discipline to

401
00:36:44,880 --> 00:36:51,040
identify that we do need prospective trials to figure out how the human machine assemblage walks

402
00:36:51,520 --> 00:36:58,800
to to augment or be better than radiologists. Yeah, I think what I'm hearing here is that

403
00:36:58,800 --> 00:37:06,560
it's well, several things, but one is that, you know, when you think about kind of the capabilities

404
00:37:06,560 --> 00:37:14,400
of these algorithms, what really happens on the ground, the multiple parties involve the actual

405
00:37:14,400 --> 00:37:25,600
nuance of these diagnoses, they don't necessarily translate to very well to kind of the way some of

406
00:37:25,600 --> 00:37:36,880
these academic studies have been presented, but also that, you know, as a community of radiologists,

407
00:37:36,880 --> 00:37:41,840
you're not, you know, it's not like kind of sticking your head in the sand and, you know, waiting

408
00:37:41,840 --> 00:37:47,200
for this AI thing to go away. You know, what I'm seeing at least from you, you know, maybe you're

409
00:37:47,200 --> 00:37:54,320
way more sophisticated than your colleagues, but it sounds like the field as a whole is recognizing

410
00:37:54,320 --> 00:38:01,520
that AI can be a valuable tool. You know, let's try to work together to apply it to the problem

411
00:38:01,520 --> 00:38:09,120
in a more sophisticated way, the better reflects the way, you know, what that community really needs.

412
00:38:09,760 --> 00:38:17,120
Absolutely. Yeah, and I mean, so one of the things that are surprising was that the American

413
00:38:17,120 --> 00:38:23,360
College of Radiology, which represents, you know, a lot of radiology has over 20,000 members

414
00:38:23,360 --> 00:38:28,640
came together and set up a data science institute. And one of the things that's there, you know,

415
00:38:28,640 --> 00:38:34,160
they have been walking on is setting up these writing use cases. And honestly, those can bring

416
00:38:34,160 --> 00:38:39,600
a little bit of calm to say, okay, if I want to walk on this, maybe these, these are done by

417
00:38:40,640 --> 00:38:47,440
several people, several radiologists, and they come together and say, hey, this would make

418
00:38:47,440 --> 00:38:54,000
much more sense if I was walking on this. And, and you know, I think that's been a really good

419
00:38:54,000 --> 00:38:59,840
way to sort of bring some calm. And you actually, there can be a win-win, you know, you can

420
00:38:59,840 --> 00:39:05,200
build things that we need and we'll use them. And sort of aligning yourself with sort of these

421
00:39:06,160 --> 00:39:12,640
initiatives and not, you know, going around the world and doing your own innovations,

422
00:39:12,640 --> 00:39:18,800
I think can be a win for an entrepreneur. The other thing that, you know, for radiology,

423
00:39:18,800 --> 00:39:24,480
what we've started to do, I've been doing this from December 2017, is organized a monthly

424
00:39:24,480 --> 00:39:30,800
journal club on AI for the residents and the fellows. And we also have some faculty radiologists

425
00:39:30,800 --> 00:39:37,040
who join in. And this has been fantastic. And just bringing those dialogues, bringing

426
00:39:37,040 --> 00:39:43,920
engineers and bringing radiologists together and looking at papers and digesting this material.

427
00:39:43,920 --> 00:39:49,760
And I think it's been, personal, it's been a fantastic experience to do this. But also to see,

428
00:39:49,760 --> 00:39:55,280
also sort of like an understanding, right now, it's, you know, I know this hype and there'll be

429
00:39:55,280 --> 00:40:01,280
a new person who posts something. But it's more of like, okay, I think I can't start to see the role

430
00:40:01,280 --> 00:40:07,520
that I can play and where this can be beneficial. And I think that's, that's great. And I also see

431
00:40:07,520 --> 00:40:14,240
a change in tone, even in terms of the new, the newer leaders when they speak, the recently

432
00:40:14,240 --> 00:40:21,520
launched Coursera course by Andrew and I was listening to it on my flight yesterday. And you know,

433
00:40:21,520 --> 00:40:27,360
he literally shows what machine learning can do and what he cannot do with an example from

434
00:40:27,360 --> 00:40:33,840
radiology and states. Machine learning can learn from 10,000 just extra images or, you know,

435
00:40:33,840 --> 00:40:40,000
just trying to say that from a large number. But machine learning cannot learn like a radiologist

436
00:40:40,000 --> 00:40:45,200
who can just have three or four images of pneumonia and a short blob of text. And it's just this,

437
00:40:45,920 --> 00:40:51,680
sort of learning that occurs. And some it's a little difficult to describe how actually that

438
00:40:51,680 --> 00:41:00,240
learning occurs because the way we do it is you go in, you start to walk on an apprenticeship model

439
00:41:01,040 --> 00:41:08,240
and you all of a sudden are left and call at night and you just don't realize how much you've

440
00:41:08,240 --> 00:41:11,840
learned when the other doctors come in to ask you a question and you're like, oh, I think,

441
00:41:12,480 --> 00:41:17,680
I think this is, you know, this is this or this is this and just brings your confidence.

442
00:41:17,680 --> 00:41:26,000
I would, in fact, we are going to do this actually. We're going to start to maybe use some of the

443
00:41:26,000 --> 00:41:31,840
techniques that we are seeing, for example, when people use YouTube to teach games and try and

444
00:41:31,840 --> 00:41:40,160
get representations of the learning process, some of the tasks that can be of how especially

445
00:41:40,160 --> 00:41:48,720
radiology residents learn and bring this human machine assemblage that is based within the system

446
00:41:48,720 --> 00:41:55,200
that keeps learning and knows when to augment. And I think this will start will be probably one of

447
00:41:55,200 --> 00:42:00,880
the earliest efforts to understand the future of work where radiologists work with AI.

448
00:42:01,680 --> 00:42:08,480
Sounds like at least in what you're seeing being published by Andrew, there's maybe a broader

449
00:42:08,480 --> 00:42:13,280
perspective or some recognition that there are some things that radiologists are good at.

450
00:42:13,840 --> 00:42:22,000
Yeah. Well, Judy, thanks so much for taking the time to chat with me. It's been great discussing

451
00:42:22,000 --> 00:42:31,920
your perspective on the way AI is impacting radiology and the kinds of things we need to move forward.

452
00:42:31,920 --> 00:42:38,160
Absolutely. Thank you so much for inviting me. I'm a big fan.

453
00:42:40,720 --> 00:42:45,600
All right, everyone. That's our show for today. For more information about today's show,

454
00:42:45,600 --> 00:42:53,040
visit twimmolay.com. Be sure to visit twimmolcon.com for information or to register for Twimmolcon AI

455
00:42:53,040 --> 00:43:05,120
platforms. As always, thanks so much for listening and catch you next time.

