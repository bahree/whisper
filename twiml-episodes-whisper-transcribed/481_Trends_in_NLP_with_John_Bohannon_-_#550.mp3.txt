All right, everyone. I am here with long time friend of the show, John Bohannon, who is director of science at primer AI.
If you recognize John's name, it may be from his May 2018 interview or his appearance in our Twimal Fest office hours, which we're focused on NLP back in October of last year.
John, it is so great to have you back on the show. Welcome.
Great to be back. I'm really looking forward to digging into our conversation. This is part of our AI rewind 2021 series and you are going to help us review all of the amazing things that happen in the NLP sphere this past year.
Yeah, I, first of all, this has been so fun over the past week. I've been preparing for this. I don't think anyone's keeping up with everything in NLP. It's so much happening.
So I really had to dig in, talk to my team, really review, and so I've learned a ton.
The big picture that emerged for me at least was two things. One is that what we used to call NLP, you know, just text text in stuff comes out, but nothing else.
That seems to be chilling out that like for the past few years, it didn't seem like a month would go by without some huge revolution, some brand new architectures, some totally new way of, you know, dealing with the data.
And now that's kind of like flattening out and we're in what I think you can call the incremental phase of the science. So the big explosion. And now it's just like heads down trying to make it more efficient, you know, just better.
And then the second big thing that emerged for me was that NLP is kind of like eating up the rest of ML. You know, they used to, they used to say like software is eating the world. And then it was ML eating software. Now I think NLP is eating ML because computer vision and language are just coming together.
So I think that was the most like freakishly new really cool stuff. The rest is just like getting to business, making it work.
Yeah, that's awesome. And in fact, that theme was the court that ladder of the two themes that you mentioned was kind of the core of the conversation in our computer vision.
But in the AI rewind series, I think we'll talk about it from well, you know, obviously a slightly different take in this conversation, but so far the consensus is that that has absolutely been the case.
Let's maybe dive into that particular point. What were some of the specific things that you saw that kind of led to this feeling of NLP eating the world.
So, I mean, it was right in January of this year that Dolly came out, open eyes just crushing it, open eye, open AI is just owning this weird new hybrid space. And they're clearly having a blast. I mean, you almost giggle while you read their papers, you can just tell how fun it is to do this new work.
I mean, it's, you know, we were talking earlier about how hard it is to keep up with the space, but it's hard to keep up with just open AI.
I know, I know it's, God, it's just a really exciting time. So, you know, Dolly was followed by Clip and most recently we have this new thing called glide.
That just came out like in the recent like past week or two.
And I actually played with it this morning and I emailed you some of the output. I was just playing with it this morning.
It's, it's just getting so dang good. You, you just literally use language to ask a model to generate images that you want.
And one way that we've been using it, you can actually make it do useful work too. You can do image search.
So, if you've got a whole bunch of images, let's say that you harvested from Twitter or something.
And you want to say, Hey, find me blah. And you just describe it like write the caption for an image you think exists.
So, you just take all those images and you put them into this embedding space that lives in the same world as the text.
And so, it takes your text. It finds that high dimensional address. And then it just goes finds images that are kind of in that neighborhood and shows it to you.
So, like semantic search on images is suddenly really, really working. So, that's just cool.
Maybe contextualize glide relative to dolly and clip. What are the differences between the.
Yeah, so dolly generates images clip is this classifier that that's the thing that knows how to take captions and images.
That's the training data originally and put them into a common high dimensional space so that you can go from text to image or image to text.
And so, glide is a new innovation where rather than guiding the generator of the images to try and satisfy the caption you gave it.
With a classifier, it uses a noising technique so that you actually don't even need the classifier.
The result, which I'm pretty convinced by even sending you those low res images is that it's much better at making photo realistic images.
You know, like up till now, most of the fun that people have been having with VQGAN and clip systems has been like dreamy stuff.
You know, something crazy. Like I did a whole holiday themed one at primer where I was just doing variations on Christmas trees.
And one of them was a Christmas tree growing in a bathroom. And sure enough, it created this surreal image of a Christmas tree growing out of a toilet with a kind of a hand towel looking like a Santa hat.
You know, just like.
You're not going to you're not going to like make anything useful for the world, but it's fun.
But with glide, they're getting so good that I think it actually will actually get commercial use and the really cool innovation is in painting.
So now you can use this system to make an image. Let's say you're a designer and you want to make like a mock up of a living room or landscape or whatever it is you're trying to like visualize.
You describe it and then you want to change something about it. What you can do is literally like finger paint. You can kind of circle a little zone of the image and then you can add an element to be like and a red barn.
And then you could go in on that red barn. You'd be like with yellow windows. So it's this is the future of Photoshop.
There will be a language interface to Photoshop. Either Adobe is going to do that or someone's going to eat their lunch.
That's that's what we're going to be using in the future.
And so is glide using clip to.
No, you don't need it. That's the whole point. You don't need it anymore.
Okay.
Yeah.
So it's pretty cool. And they haven't released the full weights model, but they released a small version, which is what I use this morning to send you the.
Why are you talking about the, they're probably in a robot costume and robot for giant friendly robot visiting St. Louis.
Nice. And those images are you were just mentioning this, but their generated images, based on the text as opposed to the way you originally describe it.
It was almost like a image search where it's finding the images. What's the relationship?
that knows how to take text and try and generate an image.
But the way that you do it in the back end,
I think from now on, we're not going to be using clip
to guide that generation.
So it starts off with just a random bunch of pixels.
And then it tries to move in the direction of an image
that matches your caption, because there's that common space.
So clip was doing all that guidance before.
Now with glide, you don't need clip to do that.
So that's that's the idea.
And it's pretty, it's pretty fast too.
I mean, even though they're not very high res,
it was like a minute per image this morning to make those.
So I could just play around.
That's awesome. That's awesome.
And along the same line, do you mentioned a few other models
and papers that were in similar vein?
What were some of those?
Yeah. So on the theme of NLP eating the world,
papers and usable systems have started to emerge
in chemistry, in medicine,
and I think it's just going to keep on going.
The general utility of the transformer architecture
and the approach of feeding in data in a similar way
that we teach these models in an unsupervised fashion.
Language is just starting to pay off.
So we've seen protein structure prediction.
We've seen chemical formula,
you know, like manipulation.
I think it'll just keep on going.
We're going to just keep, it's like a little asset
that's eating through problem space.
And I, you know, I don't know where it'll stop.
But I, I also don't think it's going to stop with these
things that are like clip and glide.
There's no, there's nothing special about images
other than we just happen to have a whole bunch of data.
We have a whole bunch of images with captions.
That's super convenient.
But I think like what's coming next, of course, is video.
And, you know, with robotics,
you've got other senses.
What about tactile?
What about movement?
You know, I think that the trend is going to be towards
multi-modality with a single system.
So you'll have, you'll be able to just like
describe things in text or show like a video example
of what you're trying to get at or a sketch.
You can come in from any angle and there will be this
massive, common embedding space for all modalities.
I think that's, that's probably where we're headed.
And a lot of ways this is, you know,
we're approaching this world that NLP folks have been
evangelizing for a while in the sense of, you know,
we create language around all these concepts that we care
about.
And so NLP, you know, is a fundamental currency of thought,
right?
And so the NLP community has been arguing that, you know,
all the work in that field is going to pay off because
that's the way we, you know, we think we think using
language, we communicate using language.
And so, whereas before computer vision was an isolation,
you know, that wasn't grounded by language.
And so now we, you know, we're starting to bring it all
together.
And what you're saying is we're just getting started.
Oh, yeah.
Oh, yeah.
This is just like the very beginning.
A lot of people are saying that videos coming this year.
I don't think so.
I think that's really hard.
I think it's going to be a little while before we have a
clip or glide equivalent for video.
But what I'm waiting for is that first moment,
kind of like a, you know, like the very first moments of
audio recording when you get that first hello world moment,
a hello world moment for computer generated video,
you know, like,
GAN style generated video.
Will probably be like a little cartoon animation,
like an animated GIF.
I don't know why has anyone hasn't done it yet,
but, you know, just come.
Giffy.
I haven't seen a thing.
The closest I've seen is you take a real video and then you
basically apply, you know,
something like transfer kind of thing.
Yeah.
You basically take frame by frame and you just transform it.
So you can find Reddit just flooded with these cute short
clips of real video that have been transformed into weird
things.
But I mean, we're sitting on a gold mine of data called Giffy.
It's just someone's got to be doing that right now.
Just take that data set and make a version of clip for Giffy
so that we can have animated GIFs on demand.
Yeah.
So that's going to be the hello world.
And from there, you know, it's onwards to Hollywood.
You mentioned the work that's happening proteins and chemistry
and medicine.
What's the.
What's kind of the common theme behind these papers?
What are they trying to do?
It's, it really all boils down to the original trick of the
transformer.
It's, it's, it's amazing how valuable that idea was, you know,
a paper in 2017 has just completely taken over the world.
You know, the idea is if you have a sequence of some kind,
language could be pixels, it could be sound, could be DNA or
protein sequences.
You just, you make the, what we call a language model and NLP.
It'll probably, maybe it'll always be called language model,
even when it's not language.
I see that in papers like I link two papers and what I sent you.
One's called a protein language model.
So maybe that'll be the trend is you just call that a language
model, even if it's not human language.
But the idea is you just.
You teach the system to learn the patterns and the data by
having itself supervised at a massive scale is just essentially
doing, you know, we call it tokens by tokens.
You just take this attention when you stick it in there and the
usual trick is you do a mass token task where you hide some
of those tokens and you teach it to fill them in or autoregressive.
You cut it and you say, what would you, you know, continue
writing from here?
It's all amounting to the same thing.
You're forcing this very, very massive function.
That's all a neural network is.
It's just a huge function with a like ridiculous number of variables
that we call parameters to, you know, essentially satisfy this
objective of being able to predict.
And the weird thing is all of these cool skills just seem to
emerge from that, I think dumb trick.
I think of it as a, and that's not a pejorative.
I think it's a beautiful dumb trick.
And that's another theme, by the way.
We still don't know understand.
We don't understand how it works, which blows my mind.
ML used to be a branch of math.
Now it feels more like biology.
It feels like we're studying things that we don't fully understand
and we're just taking an empirical approach, probing and prodding,
trying to figure out how they work.
Do you follow the more theoretical side of NLP and the efforts
that are taking place to try to better understand how the model
will work?
The extent of my engagement with that, you're full honesty.
Full honesty is here at Primer, we have a Slack channel called
Algorithms, just for his throw of reasons.
And people drop papers in there and start discussions.
And every once in a while, I will see something that's a theoretical
paper that just seems to be making some bold claim.
Usually it's like, it sounds like physics to me.
And I'll just tag tag in people, usually people with a physics
background who are on my team and say, please explain this.
Is this a big deal?
And so we like dig into it.
And usually the consensus is maybe, even people on my team who
can really come to grips with these theoretical papers are always
like, well, let's see.
It feels very nascent.
There hasn't been any big reduction on reductive understanding
that you would get in physics with these models.
It's still very empirical.
People take a theoretical take.
It's like intriguing.
But I'm just, you know, like, I'm just waiting for someone to say,
oh, yeah, this is the breakthrough we've been waiting for.
This explains a lot.
And I just haven't heard that.
Kind of along those lines.
One comment that was recently made to me relating to NERPs was this
feeling that the NERPs, and let me be clear.
And individuals experience with NERPs was that, you know, they're,
they were able to take less practical, practical.
They got less out of it, I guess, is the only way to say it.
And I was reflecting on that.
And this idea that, you know, over the past few years,
what, one of the things that's been really interesting about this field
is that you, to, to stay on top of the field and to implement,
you know, useful things that were beyond something that was already packaged
in a, in a library, like you, you had to read papers to understand what
folks were doing, because the field was evolving so quickly.
And I think you kind of alluded to this in your opening and that the
field is in some sense slowing down.
And I wonder if there's a corollary to that that the kind of the hard core
research is, you know, is diverging from practice more so than in the past.
Yeah, I agree.
And it does resonate and it manifests in a bunch of different ways.
And one of them, here's a really practical way that this manifests.
Early on, and by early, I mean, like, circa 2018.
NLP, you know, modern NLP is so new.
These benchmarks emerged.
You know, it was like, okay, let's get down to business people.
We need, you know, natural language processing needs to take the,
you know, measurement of these models seriously, because we started to
create these giant language models that had these amazing capabilities and
everyone wanted to play the game of is my model better than yours.
And so, you know, you had glue and then that got beaten.
We had super glue.
Now we have something called gem and like a whole zoo of benchmarks out there.
And something I've noticed is that they are just less and less useful.
I think of them as academic benchmarks, academic data sets.
So what you do is you, you know, you take.
If you want to know how good name density recognition.
It's one of the like the old school core tasks of NLP.
Find me the people places things in this text.
There's this old data set floating around called con LL.
It's really old.
It's just a little sample of news.
You know, just from I think like one source.
From one period of time.
And we've been using it ever since.
To measure how good you are at, you know, any are.
And.
It wasn't that long before, you know, when we built our own any our model,
we started to notice that performance on that benchmark that academic con LL
benchmark started to drift away from the performance on real customer data
that we had gold labeled that we cared about.
And that was the trend that we just saw happening over and over again is that.
You know, if you're chasing state of the art on these on these big benchmarks.
You're actually often driving down the performance on something you care about.
And so you have to make your own internal benchmarks.
And I think we're not alone.
I think everyone across the industry is quietly maintaining their own internal benchmarks.
I could keep track of stuff.
And that's bad, right?
That's the thing that came up in the computer vision conversation.
Also, we've we've we're focused on the flip side of that, which was the academic tendency
at least in CV to, you know, quote unquote overfit on image net.
Yeah.
And you're describing the, you know, you know, you know, the flip side of yours will be overfit on blue or blue or whatever it is, right?
Yeah.
So, I mean, that's one of the manifestations though of this problem is that, you know, early on when NLP just started to explode.
You know, a big conference like Neurips.
You know, and these kind of shared resources like these benchmarks made sense.
Like we didn't we didn't even have a map. We didn't have a compass.
We were just, okay, let's get together for some now.
Yeah.
Now we're past the explosion.
And we're in the like settling like stable phase of the science where it's like, okay, let's make it good.
Let's make it efficient.
And now it's like very applied.
Now like the performance of some base language model on some, you know, completely kooky task like natural language inference.
It's kind of beside the point that's like not even helpful anymore.
So, yeah, no one solved this, but that's a, that's a big story of this year is like,
turmoil over benchmarks, how to measure the performance, what's relevant, how to improve things.
And all the big companies came out with their own new big approach to this.
The only thing that I've seen this year that's a truly new take on this is something from Facebook.
Meta called Dynabench. And that's an experiment that's running now.
So there, rather than just have a frozen gold label set and, you know, like a test.
And everyone takes the same test.
It's an ever evolving test that's adversarial.
The whole point is to have humans in the loop try and trick models.
And so you're collecting all these adversarial examples.
That models find really difficult.
And so it's just constantly evolving.
And as a side virtuous side benefit, you're generating all this useful, truly instructive adversarial data.
That's, that's a really fresh take.
I'm going to be really interested this year to see what results come out of that.
That is really interesting.
Do you kind of on this benchmark that this idea that in industry,
folks are collecting their own benchmark data sets.
Do you, do you see that changing in the sense that of, you know,
industry consortia or groups of organizations kind of sharing.
This information of credit tried to create better data sets.
Or is it more, you know, either some combination of, you know,
this is our proprietary advantage.
And so, you know, we're going to keep it or it's, you know,
it's really only relevant to us anyway, because it's, you know,
our specific problem, our specific customers.
That kind of thing.
No, I don't, I don't think there's a lot of goodwill around in this face anymore.
The trend I'm seeing is use benchmarks to be able to brag about achieving state of the art,
even if it's not really relevant.
And quietly curating your own high value data that actually gets you models that solve the problems that people pay money for.
The big, the big like exception to this that's going against that tide is Ellie Uther.
That's how you pronounce it. I've never actually never heard that name.
The creators of the GPT three alternative, the GPT neo folks.
So I love that they're out there open sourcing nonstop.
They put out a huge data set called the pile.
They put out GPT neo.
And I think, you know, they're, they're a force for, for good going against the tide of,
basically closed, closed ML development.
Because the problem is like that's going to slow down the science.
And the majority of shared resources and standards is like we can all march to the same drum beat and make progress more quickly.
But there's just a lot of money to be made right now.
I mean, NLP is where all the investments going.
So you're going to see a lot of, a lot of anti scientific trends.
Along those lines, are you seeing a decrease in.
Publishing by, you know, corporate research teams.
No, I haven't seen any trends like that.
They're still publishing, you know, solder chasing papers.
We got to have the art in this.
Even when it's just crazy.
I mean, in, in, you know, a lot of NLP.
They're still using this, this standard called Rouge.
So it was invented for, you know, machine translation.
And now it's used to evaluate summary quality.
And then everyone just openly knows that it's completely flawed.
But people just keep on reporting state of the art results, you know, with the Rouge metric, even though we have alternatives now.
But it just reveals that a lot of the publishing that's happening, whether it's in universities or big companies is really just bragging rights.
Yeah.
Other than true scientific sharing, I mean, there's a mixture, there's a lot of, there's a lot of goodwill still, but yeah, it's a troubling trend.
So maybe going back to this idea of incremental progress within NLP.
You know, it sounds bad, but you know, when you think about kind of the hype cycle and, you know, the money markets.
I think it's beautiful.
It sounds less exciting, less sexy than NLP eating the world.
But, you know, I think we're saying the same thing here.
It's a, it's a necessary phase.
And it's where, you know, we talk, we throw around this idea of democratization and stuff like that is hard.
And this is related to the point I was making about, you know, everyone needed to be a researcher to use NLP.
Like it's not the case anymore because things are settling down, the tools are standardizing, like you can pull state of the art stuff off of hugging phase and, you know, just use it.
You don't have to implement the paper.
Yeah, exactly.
I think it's great.
I mean, that's the business I'm in.
You know, like we're trying to build stuff that people put in money for us.
So you got to make that stuff efficient and actually high performance.
So yeah, it's just settling down into normal science, normal engineering.
But the revolution is still ongoing, you know, it's just that this first big, you know, explosion that was made possible by transformer based language models.
It's kind of cooling down.
It's like a lot of alternatives to transformers have been proposed.
It was a paper, you know, you can forget transformers.
You can use Fourier transform.
Forget this.
You can use this, you know, and none of them seem to have taken off.
Transformers seems to be here this day.
And so what are you seeing in the dimension of improving efficiency?
Well, the really exciting thing is it started with something called the switch transformer that Google implemented successfully.
Apparently, the idea is a lot older.
I didn't realize that.
But in a nutshell, rather than having one gigando neural net where when you feed in something at the bottom,
you have to do Pachinko through the whole network to get your answer out the other side.
You instead fragment it into what's called a mixture of experts.
So bunches of little neural networks.
And then you have something at the bottom that does the routing.
So when little piece of information comes in like language token or whatever,
it gets routed to the right expert.
Sometimes these systems routed to a couple experts and let them kind of do it out.
But the point is you can have a gigantic system that's very sparsely activated.
So it's way cheaper to train way cheaper to do inference on.
And you can do it with distributed architecture very naturally.
Whereas if you have your whole neural network that needs to be in memory, maybe even on a chip, you know,
there's a hard limit to how big you can make these things.
The size, the parameter size of these models has been outstripping Moore's law by a healthy, healthy helping over the past few years.
And that's got people worried, you know, money, power consumption, climate change impact.
Only a few giant companies being able to even operate with these things.
So the switch transformer has been a really exciting development this year.
And two models, one from DeepMind, one from Google, came out just recently.
Gofer and Glam.
Bamsman around the longer that implemented and also there's a really amazing result from Facebook AI.
They go by Facebook AI or meta AI now.
I guess it's meta AI then.
Where they had a multi-lingual translation model that beat all of the bilingual models for the first time on the big WMT annual machine translation contest.
So it used this mixture of experts architecture.
So ran a lot more efficiently.
So that idea seems to be taken off.
And I can expect that this year, it'll just keep going.
And maybe eventually we'll say goodbye to single neural network, you know, systems.
Is there anything special required at the infrastructure hardware?
For sure.
For sure.
And that will come.
We'll get, you know, the pie torch for distributed, you know, mixture of expert transformers.
Indie guy.
I'm sure.
Yeah.
But yeah, for now it's pretty bespoke.
I think you need a real team of experts to implement this.
Got it.
But can't be long before you know, tools get good.
Well, it's, I think it's the same process that we've been talking about.
We've, you know, there's an innovation and the switch transformers kind of standardized and demonstrated to be effective across or at least from several different groups.
And, you know, now the pie towards community and others can, you know, take it and kind of push it down into their level and take advantage of it.
What are you seeing around kind of building.
Operational pipelines around transformers or transformers and production.
That kind of thing.
You know, this relates somewhat to the.
The broader problem or your challenges around like fairness and.
And predictability of language models and, and ethics that kind of thing.
And thinking about the, the guardrails that an organization needs to put in place around large language models.
If they're productizing them.
But also the.
Kind of ML ops processes that are required to operation wise them.
Are they, are they diverging from any other kind of model.
Or are they, you know, you're seeing the same practices applied to transformers.
This, this touches on a pretty rich vein.
We could take this from a lot of different angles.
There's the whole issue of.
ML ethics and responsible.
ML.
And that, that's been a sea change over this past year.
And that really is starting to have really practical effects on, as you say, ML ops.
Like how do you actually operationalize this stuff.
And protecting against harm, increasing the, you know, the quality of data by accounting for things like bias and toxic language.
So all of that is happening.
There's also a kind of more boring side of this, which is just how do you make these systems cheaper to, you know, cost assert drive down cost to serve.
Make it more data efficient to train these models.
So there's a couple of bunch of new tricks that are emerging on that side, too.
So there's like the side of spicy spicy side and the mild side, which, which, which do you want to.
Yeah, let's start with the, the mild side.
And we'll get to the spicy side.
Sounds good.
All right.
So for the mild appetizer.
One thing that's emerging is.
So there's a lot of different kinds of clever tricks for increasing data efficiency.
So when I say data efficiency, I mean like how many training examples did you need to get your model to some target performance.
So, you know, you have some target in mind, how long did it take you to get there.
And the whole trend has been that that number is getting lower and lower and lower.
And the most exciting thing over the past year has been a few shot learning.
When GPT two and GPT three came out, people started using the phrase few shot learning and zero shot learning.
Aroniously.
So it's like really confusing because you know, when you when you do prompt engineering and you and you write.
There's no learning happening here.
You've just changed the format of the input.
But true few shot learning has started to actually emerge.
So all kinds of really clever tricks for driving down that cost in terms of data to get some performance measurement.
Here's one of the like really cute tricks.
So when you want to make a class fire.
So the document comes in and you tell me whether it belongs in bucket a B or C.
Yeah.
You know, this is like NLP task 101.
Usually what we do old school is you give it a whole bunch of label data.
And the first thing the model has to do during training is figure out what the task even is.
You don't give it instructions.
You just punish it every time it gives a wrong answer.
And so the whole beginning of the of the training is just figuring out what the task is.
And that's not efficient.
You know, that's like administering a test to students where you don't even tell them what's going on.
You know, and they just have to kind of figure out what it is first.
And so one of the cute tricks I've seen emerge is just make it multiple choice.
Just literally spell it out right there in the input data.
What the possible choices are.
It's just like such a nice simple trick.
And that's what I mean when it comes to incremental science.
You know, it's like bunches and bunches of little artisanal advanced, you know,
improvements to how we do things that squeezing out efficiency and performance.
And that is that an example, and as your broader point related to you've already got a trained large language model.
And you're trying to figure out the, you know, basically how to get it to do what you want.
Yep.
It's just old fashioned now old fashioned fine tuning of a language model, the same old same old.
Another neat trick that's emerged is the pipelines themselves.
So this is something we've been grappling with it primer.
You know, if you're just a some person at home wanting to do a little batch of data using some hugging face model.
You don't worry too much about the cost.
It's just sort of a quick one off.
It's no big deal.
But if you need to do millions of documents with let's say dozens of different machine learning models for a customer.
And you're on the hook for the cost to serve.
It really matters.
You look at your monthly AWS bill and you're shocked.
So, you know, there's a lot of motivation out there to try and figure out how do we drive down the cost to serve of these systems once we scale it.
And so one of the things that you can do.
And this is borrowed so true of of NLP these days.
Take an old concept from like computer vision and just figure out how to reapply it.
So it's the idea of model cascades.
That's what they call it over in computer vision.
You have them just have your big, great model doing all the work. You have a cascade of models.
Maybe sort of like not that good, better, better, best.
And you basically do what I call inference triage.
So what we found and we're about to publish a paper on this is you can with some really expensive classification tasks.
You can have a old school scikit learn CPU driven no GPU's needed model to 99% of the work and all it has to learn how to do is take the ambiguous tricky cases and pass those on to the big model.
Yeah, it's stuff like that just like really simple tricks. That's that's what engineering evolution looks like just an accumulation of tricks.
Yeah, I mean, it, you know, I guess all the best tricks sound obvious once you say them, right?
Yeah, although I got to say transformers. That's clever.
It's still such a cool idea. It's to me, it's never going to feel obvious. That's just such a cool idea.
Language models, even like cool idea. So that's the mild side.
Let's talk about the evolution of the way we're thinking about ethics and responsibility in the context of NLP. What are you seeing there?
Well, I'll start by making a prediction for 2022 since we're supposed to be doing some of those, right? We're going to do some of those. Yeah, absolutely.
Alright, I'm going to throw one in. I don't think that Amazon Alexa or Apple Siri or Google Assistant are going to get dramatically better this year, even though they could.
And I think it's because they are, these companies are rightly cautious about the language models that would power that.
It just doesn't feel safe enough yet. It just takes one really, really egregious, you know, public relations disaster to nuke the whole effort.
And I think they're going to hold back until that stuff is all ironed out.
So I don't think you're going to, even though like you're going to have hugging face models and maybe little startup, you know, efforts that feel like they're miles ahead.
And I think that in some regards with human interaction, whether it's speech or text or just making sense of things.
I think that the big companies are going to lay low a lot longer. So that's my prediction. And rightly so.
So on the spicy side, in retrospect, this no one should have been surprised by any of this. Like, what are these models? They're just big mathematical objects trained on data or that data come from us.
It's our written text on the internet mostly. Where did that come from? Well, a very biased point of view.
Very, very biased point of view. English speaking almost entirely Western industrialized male dominated.
And, you know, it comes from a human history that is only now starting to, you know, on the time scale of human history, kind of yesterday woke up to equality.
And so it shouldn't have surprised anyone that if you put a language model behind a free form interface where you can just like talk to it or make it say stuff that you can very easily make it say things that are just like politically unacceptable and sometimes ethically shocking.
And yet the past year, I mean, really it started the year before, but it continued this year. People just keep on stepping in that landmine.
The one that I noticed this year was the Allen Institute for artificial intelligence made this cute little cute little experiment called Delphi.
And man, that blew up in their face. And, you know, I don't think it was a mistake to do the science. It was a cool question.
Could you, if you made a big data set of ethical statements, you know, things that are right and wrong. Could you generalize at all?
Could you get a model to like take statements as input and say whether it's like sounds ethically right or wrong?
Interesting idea. It's like basically a toy experiment. And the one mistake they made was they put it online in a way that allowed anyone to say anything to this model.
And of course, if you're motivated, you want to make the model say something racist, it'll take you like 15 seconds.
And then they, you know, this is the part I don't like, then they absolutely mob bullied the, you know, these poor scientists who had good intentions. Come on, like we weren't out to harm anyone.
And so that's the trend we've been seeing is just sort of people making these self-inflicted mistakes using language models and, you know, like Twitter beating up on them.
And I mean, there was a time when we talked, when it came to machine learning and ethics, we talked about systems that were deployed in the world, like things that determined parole, things that determined whether you got hired or not, that we're having impacts today.
And people were truly being harmed and like those, you know, facial recognition systems being used, you know, to, you know, like hassle people that clearly had racial bias.
Suddenly that whole conversation moved to, can you make a language model say something racist. And like, so what's come of that some very good things.
Complete shift, I think there are still people that are there are there are, but like it has made a lot of heat noise. And I think some good things are finally coming out of this.
One is that you, you can't not talk about the ethics of your work anymore, like papers, you know, routinely will have an ethics section. It's just expected now.
When you dream up some project, you're not going to be able to avoid the conversation. Hopefully before you even get started of like, well, what what are the possible harms.
That's a sea change that, you know, just a couple of years ago that that just wasn't the case. And I think that's the biggest positive impact of all this is that we're talking about it.
And then the second one is people are starting to get practical about it. Deep mind I saw put out a blog post describing practical ways that they try and reduce these kind of biases in data and also ameliorate like the impact on the other side.
So we're just starting to get really practical one of the really cool results from the recent glam and go for papers. I can remember which one, but one of the two of them.
They claimed that they got a lot of bang for buck by actually practically addressing the problem of data quality.
So they had all kinds of clever filters on huge gobs of internet data to try and get rid of some of the worst stuff like, you know, internet chat and toxic language, you know, they pre filter that out.
And they got better performance on the task that they cared about practical. So it's not just it's not just a bunch of heat noise. It's it's actually productive.
I don't think I saw the deep mind paper that you referred to have to get that link from you.
I think you're you're touching on the medium spicy that that I was referring to earlier and the example that comes to mind for me was this.
It was a tweet from a while back if this was an October and a October.
Someone did a search on Google about, you know, what should you do when someone's having a seizure and you know how it has the feature snippets.
The feature snippets basically gave us some bullet points, which if you get went to the text of the page that it was referring to were the things not to do.
You know, we're starting to see more kind of model monitoring and governance tools. But I wanted to get your take, you know, for an organization that's putting these models out into the wild, like what are the tools and processes that you're using to try to constrain their behavior.
Let's just don't put them, don't put them up.
Another is, you know, we see, you know, we've seen with Google like they'll put like, they'll filter like you can't, you know, search this, you can't use some set of search turns because it knows the model breaks under those searches.
All, you know, create lists of filter terms, you know, that's one thing. I'm curious what, you know, what are the other things that you're seeing folks doing in the kind of the full spectrum of, you know, sophistication, you know, or is it just kind of a, you know, an exclusion list of front kind of.
I can tell you as an outsider to this because, you know, where I am, primer, primer is working with big, a small number of very big customers. And so the relationship we have when we deploy models, we can talk directly to the very small, manageable number of people who actually need it.
So there's a trusting relationship there that's very dynamic, we can, you know, fix problems as we occur them. So in a certain sense, we can be a little less, I guess you could say just less cautious, but you don't need to be as cautious.
When you unleash something onto the internet and anyone at scale is using it, you got to be really cautious.
So it's a different game.
I've observed looking at, looking at how people deal with that.
I think we're still stepping in the do-do phase, honestly.
I mean, I just see it happening over and over again.
You know, things going horribly wrong.
And I guess that's probably because it still pays off.
The game out there is to be the first to do something, and there's a huge payoff for doing that.
And so people are still in this era of just cowboy it, just like make something and put it out there and see, you know, what sticks.
And then when it blows up too bad for you, but it was still worth the risk.
And I think that's going to change.
I think that's going to change.
For one thing, like the number of new things seems to be decreasing over time.
Like, I don't think people are that impressed by a chatbot on Twitter anymore.
You know, like, okay, yeah, we've seen that.
So I think we're going to be moving into like practical stuff.
Like, can you solve problems?
And the nature of solving practical narrow problems.
I've noticed is that it forces you to be more cautious just for good engineering.
You know, so we're going to see less and less of interfaces where there's just a raw language model willing to say anything.
Because that it's just there aren't that many problems that require that it's going to get more and more narrow.
And I think that, you know, the things that are left where it's still dangerous.
Yeah, we're going to have to figure out how to do it.
And to your point, like, I think we are still essentially filtering.
I think that is the best tool we have right now is to just make big exclude lists of terms and keep on adding to it.
I've done that myself.
I, you know, not not for toxicity, but we were building a system once and it wasn't dealing very well with certain famous people that, you know, just pollute your data.
And so we would just exclude them because they weren't relevant.
And so we're just like clean up clean up results. That way it's an old school trick.
And I think we're still doing that.
And I think that'll continue.
Ultimately, though, well, a lot of people think that the solution is to train better language models, make them more polite and, you know, helpful and cautious from the get go.
Other people feel like that's too hard.
Just train them on the raw world as it is.
And then do something downstream to clean up their output.
I don't think that's been settled.
You mentioned chat bots.
Have you seen anything?
Nope, no matter what you're going to say, the answer is nope.
I don't think chat thoughts have improved at all.
I mean, you would think they would inherit some some improvement due to transformers and that whole thing.
But they are more fluent in terms of the, you know, they're not, it's not word salad.
But they still can't keep track of what you're talking about.
They're still not, they still haven't achieved utility.
I don't think so.
I've one thing I loved this year was AI dungeon.
Have you played with this game or at least heard about it?
I don't remember.
Do you remember?
Do you remember Zork?
Yep.
So it's just a text adventure game like Zork, but it's powered by like GPT two and three.
I think I remember this.
So it's just, you know, it's a text adventure that you get to basically be the player and dungeon master simultaneously.
And that what I think that's a glimpse at what the really cool hello world of tomorrow's chat box is going to be.
I think that's that that kind of like you're not trying to solve a problem that, you know, you're stressed about and monies on the line.
It's going to start with games, something fun.
And where it's a little more freeform and can be cooking.
It's okay.
And I think that's where the first really sophisticated chat bots are probably going to get used.
I don't think it's going to be customer service. Everyone thinks it's going to be chat bots.
You know, helping you with questions, you know, that you have of a company or your phone company or something like that.
You can solve those kind of problems with much simpler systems because you don't need just to have a conversation with those bots is really just a fancy menu.
But with games, you really do want to have a conversation. And so something that really bugs me is like they haven't figured out co reference resolution, which is if I'm talking about something.
And then I say, what do you think about that?
That is a reference to something that we've talked about earlier, you have it in memory, you know what it is.
These these models really struggle with co reference resolution.
And so someone's got to crack that code. Someone's got to figure out how to make a chat bot that can keep track of things.
One of the really cool papers that came out this year that was a really truly innovative idea is something called scratch pads.
It was a Google paper.
And the idea is you let the model literally take notes as it processes stuff.
So you can imagine, you know, they did it for mathematics. Their paper was all about having this model work on a mathematical problem and it could take notes.
But the sparks flying in my head were like, oh, it's not going to stop there. Imagine you have a model that has to deal with a book length document or an hour long conversation.
We can't fit all that stuff into a standard transformer attention window. It's just got this little keyhole that it looks at the world and it forgets everything as soon as you move that window.
And so, you know, this might be a way forward to sort of give it a space where I can keep notes of the most important things and keep referring to it as it goes.
And thinking about trying some version of this for longer documents. That's the something I care about in the chatbot world. Maybe it'll be something like that where you literally have a model that's keeping track of the most important information, erasing it and changing it as it goes so that it can remember what that is when you say that.
I don't know. Had some interesting conversations on the general topic of fusing various types of memory with, you know, modern deep learning models.
And that seems to be one of the, well, hey, there's a lot of activity and B seems to be high potential. And, you know, if we get that right, we open up a whole other dimension of performance.
Yeah, absolutely. Because right now we live in the world of the attention window. It's, it's only, it's, it's only about 500 words.
It's really small. You know, imagine you had amnesia where you forgot, you know, everything you read before 500 words ago.
Pretty tough.
I think I've seen that movie and the solution of writing things on your hands.
That's scratch that we reinvent a memento.
There's going to be a paper called memento. Actually, there are a strong prediction.
That's a strong prediction.
I feel like there is one though.
I'm going to have to look that up. It can be rebranded. I think I can be appropriate for sure.
You briefly mentioned multi-lingual results this year.
Yeah, that's something that the community has been kind of advocating for or elements in the community have been kind of advocating for for a long time kind of pushing NLP beyond English or English dominant.
What were the highlights in that in that domain?
Well, so this year, Chinese has just completely blown up. Chinese NLP is just amazingly amazingly rich now.
Multiple models, language models, bigger than GPT-3, whole new dialogue type models that is like Chinese based.
So there's just no doubt like the language that's going to be on par with English if it isn't already is Chinese.
It's huge investment.
And then a bunch of other languages are starting to play catch up.
I noticed that a Korean version of glue came out and a GPT-3 Korean model called hyperclova.
A whole bunch of other languages are also playing catch up mostly by utilizing English models and then kind of doing tricks to make the multi-lingual.
The most exciting thing in multi-lingual NLP was this result from Facebook meta where a multi-lingual model beat all the bilinguals. That's a paradigm change.
But I think actually the thing that's going to have the bigger impact long term potentially is this other thing also from meta called XLSR.
So this is really new and intriguing.
The idea here is forget about the written text.
Let's just do language and audio the way humans learn it and use it.
And besides opening up an unbelievably vast potential data source, you know, most human speech is actually audio.
We NLP has just been grounded in text because we can easily deal with it with the computers of today.
But meaning we have this is it's this is an interesting problem to kind of formulate. So if you, you know, certainly the volume in terms of bits of audio data is going to be greater than written data.
But that's not relevant.
Are you saying that we have more recorded words and audio formats than written formats?
It's slightly weaker claim, which is that every human alive today who utters language in any form is mostly doing it with their with their throat, not not their typing, you know, fingers.
And I'm not sure that's that's an interesting and different question is if we had to do this today, how much data do we actually have counted by words, let's say, you know, written versus recorded.
Probably written.
If I had to guess, just the libraries, you know, accumulated.
But with all the listening devices we now have in our pocket, in our home, in our meetings, that could be eclipsed very, very quickly.
Yeah, but do we even know how to build models that are kind of channel characteristic, agnostic.
But that's that's the exciting promise here is like maybe, but I think that the well, the thing that excites me about it is this unlocks low resource languages in a way that we never could with written text.
You just don't have enough written text from all of the African languages and all of the Aboriginal languages of South America.
And even a bunch of European languages, I hear Icelandic has this problem that just isn't enough written text to train a language model the way we do today.
But you could easily get a whole bunch of recording.
And so that's pretty exciting that that might be longer term, the bigger deal.
We'll see very cool.
The multilingual model that you referred to.
I think it was meta.
Yeah, they did English to many and many to English was the paradigm.
So yeah, and by the way, they used this nice, efficient, new distributed mixture of experts architecture to pull it off.
But yeah, it was English to many and many to English.
So they still English at the heart of this effort.
You know, it's like the, it's the node connecting all the other languages data wise.
Got it, got it.
So we've covered NLP eating the world, we've covered kind of this transition to incremental progress and driving efficiency multilingual benchmarks ethics.
And you've got this section here and some bad things that did not happen.
People are always talking about, you know, the things that did happen.
I think it's worth talking about the things that didn't happen, even though I expected them to.
So one of those was, man, there was so much concern about GPT three generated text flooding the internet.
I'd let a whole team working on this problem for a whole year.
Yeah, that concern about GPT two also.
The whole thing is too powerful to share with anybody.
And I've been looking real hard and I just don't see it happening yet.
So we don't, we don't Twitter.
Could that be because it's so good that you can't detect it?
Of course, of course.
I don't think so, though, because I can tell you as someone who's done this before, the first thing that'll happen is someone will announce a prank.
And I haven't even seen a large scale prank yet.
I haven't seen anyone maintaining a well, you know, regarded or widely read blog that turns out to be all GPT text.
I haven't seen a Twitter account that's had a big account, you know, a big impact on the world that turns out to be all generated.
And often, it's just like a big whimper.
And I think having used these models myself to solve problems with text generation.
The reason I think is that they're just not that easy yet.
It's not the case that you can command an army to generate, you know, things that are actually going to achieve some goal.
It's still too weird, too much hallucination, too much fuss.
But I do think that troll farms, state sponsor troll farms, and they continue to toil away, trust me.
If they aren't already, they will soon be using, you know, GPT style models to accelerate the human effort.
And so, you know, there are going to be efforts. I'm part of, you know, I'm part of one of surely many such efforts to try and make detection systems to combat these.
But that will be a problem, but it didn't seem to happen this year.
I just, I didn't see any evidence that any bad actors were using these text generation models at scale.
So that's nice.
The other thing that didn't happen was reinforcement learning didn't take over NLP.
A lot of people thought it was inevitable because RL is just so amazingly impactful in other fields.
Just didn't make any inroads.
There was one exception that I noticed, which was this open AI paper, which was using reinforcement learning to shape a summarization model to human preferences.
I mean, the results weren't like mind blowing, but worked.
But yeah, hasn't has an RL is not eating ML.
That's not happening yet.
The other big surprise, of course, is that we still don't know how language models work.
Still big mystery. We're still in this era of, you know, treating these things like cells that you have to do experiments on to figure out how they what they can do and how they do it.
You know, one of the things that we've, we've talked about language models, of course, throughout this conversation.
And we've mentioned open AI specifically.
We haven't specifically talked about GPT three and all the things that have happened just in that world.
Do you think maybe a place to start is do you think it's worthy of talking about do you think GPT three?
We'll have an outsized impact relative to the innovation that is large language models or is it just, you know, a hosted example of which, you know, there are many.
That is the million dollar question.
It is crazy how, how big the gap is between the excitement about these large other aggressive language models like GPT three and the posity of applications.
Unbelievable divide there.
Do you say the posity of applications, do you mean the, you know, in fact, actual, you know, in the wild applications or the posity of use cases for which they could be kind of reliably applied.
Well, I think they're related.
You put your finger right on the problem.
I think of GPT three and it's many cousins as wild horses, it's clearly an awesome horse, really powerful.
But good luck writing that thing to get like to town.
They're just very hard at this point to control to do the things you need.
We have some tasks and trust me, we were trying like others to use them to try and make progress on the problems you know we care about.
You usually better off with a smaller, simpler model, just training data is more important than parameter size for the narrow practical applied ML problems of today.
But one way we tried to use, we haven't been using GPT-3, but GPT Neo, the 16 billion
per amber one, or a 6 billion, it's called GPT-J6B, so that's the biggest that was open
source from Eleuther.
We've been trying to use it for data augmentation, so we were like, okay, well, if you can't put
this model literally into production to do a task, maybe you can use it as a tool to train
a simpler model more efficiently.
So we've been trying to find all kinds of ways to get it to, you know, teach another model
to do cool things.
You can't even get it to do that yet, we have to admit, we just have not cracked the code
on that.
What you can do with it is all kinds of fun stunts.
So I mean, it's just irresistibly fun to engineer a prompt to do some brand new trick
all in the space of 10, 20 minutes, you know, where else in ML can you, can you do that?
I've used it to generate new neologisms, urban dictionary style neologisms where you give
it a description of a word you wish existed and it just spits out a weird word, fun.
I've used it to generate movie plots or movie names given a plot.
I just made a prompt from stuff I copy pasted from Amazon, prime, super easy.
And I've recently used it to generate what I call deep talk questions.
So this is a real practical use case.
So COVID came and we were all like a bunch of people who used to work in an office and
breathe the same air and feel connected and, you know, it's, it's very hard to stay connected
when you just, you know, everyone's a little tiny face on a grid.
And so I have this Monday ritual with my team now that we start with deep questions.
We used to use a, a list that was floating around the internet of a bunch of deep questions,
you know, stuff like what is something you regret from your childhood?
What is, what is, you know, what, who's the most influential person in your life?
If you could go back in time and change one thing, what would it be, you know, these kind
of questions, they elicit real conversation.
And we blew through that list because we were doing it every week and we would randomly
choose three and then vote on one.
So we got exposed to all the questions pretty fast.
So what I did was I just made a prompt of actual deep questions and used GPT Neo to just
generate tons of these things and I put it on the internet.
It's like taking a life of its own.
It's on the internet now.
Like, yeah, you can use them for certain things, but no one's figured out how to tame
that horse.
Do you follow and have you played with the various kind of incremental things that OpenAI has
done with the API, like the instruct engine?
I think technically it's not 2021.
It was actually a year ago from the recording of this conversation that I haven't, but
only because I'm so busy using GPT Neo and trying to figure out how to make it useful.
So I have access to GPT 3 have for a long time, but I didn't find myself using it because
every trick that GPT 3 could do, I could do myself with an open source model.
But for all I know that they've improved it by leaps and bounds, I just wouldn't know.
Yeah, the instruct model allows you to kind of bypass a lot of the prompt creation and
just say in natural language what you want the model to create.
That's kind of interesting and they've got fine tuning and specific ways to use it for
classification and question answering and other things.
The idea of being to make it easier and more accessible and to try to reduce the need
to be a prompt engineering ninja that's something done.
I'll give it another try.
Yeah, I mean, OpenAI is doing such wonderful things.
I wouldn't be surprised if it made some really cool advances on that front too.
One thing that we have been using that I think should go in the list of most successful
applied ML, NLP, most successful examples of actual day-to-day useful NLP is co-pilot.
I know it's gotten a lot of complaints, especially that copyright, but people on my team actually
use it.
If you know what you're doing and you understand its limitations, it's pretty wonderful.
It takes a lot of dreary boilerplate code writing out of your life.
You switch to a mode of taking a template that's generated by the model and making sure
it's correct in a lot of cases.
That is like 10 times faster than writing that boilerplate yourself.
How are they using it?
They're in VS code.
In VS code?
Yep.
I haven't done it myself, but it's top of my to-do list now to try it.
But I only discovered this past week that they were using it.
I was like, oh, wow, so it's gotten to the point where it's actually useful.
I'm going to check it out.
Yeah, I haven't checked in a while, but the thing that I was most looking for was like
some kind of Jupyter Notebook plug-in.
I think it's through VS code because you can imagine you have to make these API calls.
Yeah.
There's a lot of guts that have to be set up for that.
Yeah.
Interesting.
Interesting.
And that's one that I can relate to quite a bit, just that I'm not a professional engineer,
data scientist, whatever day to day, I forget.
What's the specific format for doing such and such in Python or pandas or whatever?
And you do what anyone does, which is you go to Stack Overflow, you find some version of
the problem, and then you have to copy paste and then change it because that's actually
not quite the problem I was solving.
No pilot is just doing exactly that, but as a neural network, it's already got all of
GitHub.
Exactly.
And so it just knows how to spit out boilerplate.
Yeah.
So I'm definitely going to have to go look at that plug-in.
I had not played with it yet.
If anyone knows about a way to do that inside of Jupyter, let me know.
That would be a really cool little gadget.
If someone in the open source Jupyter community would make that, that would be really cool.
Yeah.
Yeah.
Absolutely.
Absolutely.
So also on your list of fun things that did happen in 2021, my little pony, G-P-T.
What is that?
I think I missed that one.
Yeah.
So I think, like, of the things that happened this year, that I noticed and remembered were
the delightful things.
And so one of the delightful things was someone, of course, someone did this.
They basically trained a language model on my little pony text so that, you know, it speaks
in the language of this kid's toy universe.
And I think we're just going to see a lot more of that playfulness before people start
caching in and figuring out how to make industrial scale text generation, which is coming.
But I think it's going to be the artists and pranksters who are going to lead the way.
And you see that already with Clip and these things, it's play, play is leading the way,
which is cool.
That's great.
That's great.
I think we've talked about a lot of your predictions already.
Are there ones that we've not mentioned yet?
Let me see.
I mean, myself a little list.
One thing we didn't mention is that I think that we're going to have the emergence of AI
first gaming companies.
So AI dungeon, I think, can claim the mantle of being the first that I know of.
But there's going to be many AI first gaming companies and they're going to be taking
advantage of this concept of an infinite game, you know, where NPCs are actual agents that
evolve and you can talk to and the world itself is evolving in response to your decisions,
not just through procedural generation, but more sophisticated things.
So that's going to be really fun.
While you're describing this, I'm looking for this interesting company that a friend
just told me about transforms.ai, have you heard of that?
They're kind of, you know, 30,000 foot bolting language models onto ARVR as kind of a
metaverse gaming play.
Interesting stuff.
Cool.
Well, it looks like a lot of this stuff is already arriving then.
Great.
Yeah, that's going to be fun.
The other thing is there are a bunch of very narrow application AI startups in the NLP
space that are only going to get better like Grammarly.
I think if it's expense, I think expense is kind of an NLP startup, even though it's
a mixture of computer vision and NLP AR kind of, yeah, but they're just so good.
I use them all the time and it's like a beautiful example of narrow, but really nailed it applications.
And I think all of those very narrow startups are going to get better at what they're doing,
but they're going to start to expand inevitably, like you're only just a walk away from this
neighborhood and you can kind of support that workflow.
And I think they're going to get acquired real fast and those that remain are going to
be contenders.
It'll take more than a year, but the ones that don't get acquired and stick around and
keep expanding are going to be new fan companies eventually, and that'll be cool to see.
Let's find out.
Let's see, the fan companies are going to be very, very cautious about anything involving
a language model because of PR and rightly so.
But I do think they're going to be putting more and more AI muscle behind their existing
features.
It'll be invisible progress.
You won't realize it, but more and more of the stuff you take for granted will be taking
advantage of AI.
And then I guess the other thing that I haven't mentioned is we're not going to stop at language
to image, you know, clip and glide and dolly.
That's just the first shot across the bow.
I think all the rich multimodality will start to get eaten up.
I just can't wait for movement to be part of it.
So, you know, like, first of all, of course, you'll have videos where you can say what
you want, you know, a boy walking down the street, you know, right now you get an image
of a boy walking down the street.
Next step, of course, is to get a little video clip, little animation of a boy walking
down the street.
But where it gets interesting is where it gets really rich, where you could just write
a little scene, a little movie plot, and it'll just make that coherently.
I can't wait for that.
And then also some really weird stuff, like a mechanical hand, you know, the data coming
in and out of this, you know, electronic gadget is just data.
And so you can imagine, you know, like essentially doing the trick that computer vision has been
using with, you know, text as a kind of data, a brand new data paradigm with robotics.
I don't see why NLP won't start to eat robotics, where you, you know, rather than just having
kinematics where you have to like do all the vector math to figure out how to pick up
something, you know, or use reinforcement learning to somehow train it to do this.
I think language will somehow come into the mix sooner or later.
I don't know if that'll be this year, but it feels inevitable.
Language is just so good for encapsulating information at the highest level about the
goals that we care about.
The thought that that prompted me was, do you see or anticipate a role for artificial
language?
Well, like, you know, well, is what's happening in NLP going to enable like lay people DSLs
that allow us to accomplish tasks better?
It's kind of a half-form thought, but do you mean like you could say, do my taxes and
you don't have to be more technical than that?
Yeah, I'm not sure what I mean.
I thought you were going someplace even weirder.
You know, maybe one thing in the back of my head is like really, really early on in the,
it's probably like five or six years ago, like, there was some crazy article about, hey,
Facebook had these two chatbots talking about, and they invented their own language.
Yeah, that's what I thought you were talking about.
For transacting.
Yeah.
You know, I've always kind of remembered that plus, you know, what a cool concept in engineering
and computer science, you know, to me has always been like the DSL like, you know, you can
write code to do something, you can configure an engine to do something, or you can create
a language that allows someone to do the task at a higher level of abstraction than code
and a more kind of meaningful and fluid and visualizable and understandable way than configuration.
And so, and I find myself like, you know, just everyday productivity, trying to kind of
put together, you know, using tools like Alfred on the Mac, like, you know, how can I put
together a bunch of like three letter things that will do some task for me.
You're talking about compression.
But I think there's, you're hinting at something even cooler, which is making thoughts possible
that aren't yet possible.
So, you know, this is all theory and linguistics, yeah, that, you know, like thought is truly
composed of language.
And if you don't have the language for it, you truly can't think it.
I don't know if that's true or not, but I do know that practically, it's very difficult
to do the mental gymnastics for certain things without the vocabulary and the kind of,
you know, linguistic structures that support that.
And I think what it comes down to, if you're talking about machine learning is, do you
need a human in the loop?
So, you know, the example of two machines talking to each other, evolving their own language.
I mean, imagine you took the data set of a whole bunch of audio and a whole bunch of good
transcription of that audio.
So, you've got speech and you've got text.
I've been thinking about this quirky, pointless experiment where you could then take that
system that's been trained on that.
And then, in fact, let's just do English.
You've got a ton of English recording and the transcription.
You've got a model that's good at that.
And now you give it a language that's never heard.
It'll try and write it in English with English phonemes, you know, and it'll make a sort
of, like, an anglicized version, I imagine, of that language, right?
So, you know, why stop there?
Imagine, like, going from, let's say, mathematical descriptions to formulas, you know, like English
to math, or maybe from between any two domains, and you just, you basically have a grounding
vocabulary, and you have a system discover a brand new language for some new domain.
What's the utility of that?
I couldn't tell you, but I think it would be hilarious.
The thing that it made me think of, you may have seen this video a few years ago.
It's like, what English sounds like to non-English speakers on YouTube?
Yes.
You remember that?
Yes.
Exactly.
So, you know, do you need a human in a loop?
Not necessarily, but I think we should care about that more than, you know, anything else.
So, you know, how can we, I had a really simple, simple idea I implemented once.
This is not ML.
This is old-school NLP.
I made this algorithm that detected jargon, and specifically abbreviated jargon, which
the scientific papers I was reading was just so full of, and I was trying to learn new
fields.
You get all these big acronyms, and you have no idea.
So I just wrote an algorithm that would find all those things and find their expanded forms.
Usually, regular expressions, it was nothing fancy.
And it occurred to me that you could use the opposite of this to invent new jargon that
text seemed to need, because phrases seemed to be just used over and over again, so you
could just invent an algorithm, invent an acronym, you know, an abbreviation that you didn't
realize you needed, and you could, you know, even invent, if you want to get machine learning
involved, you could define it.
You could have something that will generate definitions of these things and just make
a glossary of invented jargon.
Yeah.
Yeah.
Well, I think the last few minutes of this conversation kind of illustrates the delight
aspect of NLP, and then to some degree machine learning.
Absolutely.
NLP is finally fun.
I'd say it was not fun in 2017, and before, it was hard.
Yeah.
Like, now it's just fun.
That's awesome.
Well, that sounds like a great way to finish up this year's AI Rewind.
John is so great reconnecting and talking through what you've seen this year and what
you expect to see looking for.
Happy Holidays, and looking forward to 2022.
Absolutely.
Happy Holidays.
Thank you.
