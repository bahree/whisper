1
00:00:00,000 --> 00:00:13,400
Welcome to the Tumel AI Podcast.

2
00:00:13,400 --> 00:00:18,840
I'm your host Sam Charrington.

3
00:00:18,840 --> 00:00:23,680
Hey, what's up everyone?

4
00:00:23,680 --> 00:00:28,560
I want to send a huge thanks to Emily Bender and everyone who joined us for the viewing

5
00:00:28,560 --> 00:00:32,960
party and AMA, we held with her earlier this week.

6
00:00:32,960 --> 00:00:37,560
These have been a really, really fun way for us to connect with you and to connect you directly

7
00:00:37,560 --> 00:00:39,200
with our guests.

8
00:00:39,200 --> 00:00:45,400
If you miss any of them, you can check them out at twomelai.com slash viewing party.

9
00:00:45,400 --> 00:00:50,680
I'm also really happy to share more details on our next panel, which will focus on advancing

10
00:00:50,680 --> 00:00:54,240
your data science career during the pandemic.

11
00:00:54,240 --> 00:00:59,960
The panel will be held next Tuesday, May 26th at 12 noon Pacific time.

12
00:00:59,960 --> 00:01:03,960
I'll be joined by Hilary Mason, who you know from her time at Cloudera Fast Forward

13
00:01:03,960 --> 00:01:10,360
Labs and this podcast, Caroline Chavier, Data Science Recruiting Maven and co-founder

14
00:01:10,360 --> 00:01:15,760
of Paris Women in Machine Learning and Data Science, Jacqueline Nolis, co-author of Build

15
00:01:15,760 --> 00:01:21,800
a Career in Data Science, which is Hot Off the Presses, and Anna Maria Eshevery of IBM

16
00:01:21,800 --> 00:01:25,120
and the Open Data Science for All Project.

17
00:01:25,120 --> 00:01:29,880
With the help of your questions, this amazing panel and I will explore best practices, tips,

18
00:01:29,880 --> 00:01:35,360
advice and direction for those of you affected by layoffs, new to the job market, or ready

19
00:01:35,360 --> 00:01:37,600
to accelerate your careers.

20
00:01:37,600 --> 00:01:44,000
To register for this panel, visit twomelai.com slash DS Careers.

21
00:01:44,000 --> 00:01:47,040
And now on to the show.

22
00:01:47,040 --> 00:01:52,040
All right, everyone, I'm here with Alpha Lee. Alpha is a group leader in the Department

23
00:01:52,040 --> 00:01:58,680
of Physics at the University of Cambridge, as well as co-founder of the Startup Post-Error.

24
00:01:58,680 --> 00:02:01,840
Alpha, welcome to the Twomelai podcast.

25
00:02:01,840 --> 00:02:03,520
Thank you very much for having me.

26
00:02:03,520 --> 00:02:07,160
It's great to get a chance to meet and speak with you and I'm looking forward to learning

27
00:02:07,160 --> 00:02:13,000
a bit about what you're up to, both from a research perspective as well as with your

28
00:02:13,000 --> 00:02:14,000
start-up.

29
00:02:14,000 --> 00:02:17,160
Why don't we get started by having you share a little bit about your background and how

30
00:02:17,160 --> 00:02:21,960
you came to work at this intersection of chemistry and materials and machine learning?

31
00:02:21,960 --> 00:02:22,960
Sure.

32
00:02:22,960 --> 00:02:29,320
So, I started out as a chemist, and by undergraduate training, I was always fascinated

33
00:02:29,320 --> 00:02:35,360
about making molecules and how can you make very complicated natural products from simple

34
00:02:35,360 --> 00:02:36,360
selling materials.

35
00:02:36,360 --> 00:02:42,280
So, I started off my academic training trying to become a chemist as an undergrad and

36
00:02:42,280 --> 00:02:47,120
then I soon realized that being a lab is interesting and fun, but I also wanted to think

37
00:02:47,120 --> 00:02:53,000
a step back and understand why and how chemical reactions work and understand from a theoretical

38
00:02:53,000 --> 00:02:54,000
perspective.

39
00:02:54,000 --> 00:03:01,680
That's why I did my PhD in mathematics at Oxford, where I looked into the physics of particles

40
00:03:01,680 --> 00:03:03,840
in solution.

41
00:03:03,840 --> 00:03:09,640
And then through my postdoc at Harvard, I thought, well, mathematical theories and physical

42
00:03:09,640 --> 00:03:14,240
theories are very, very powerful, but there seems to be a gap between these theories and

43
00:03:14,240 --> 00:03:18,880
actual materials and chemistry, and that's where the data part comes in.

44
00:03:18,880 --> 00:03:25,600
Can we leave rich experimental data that has been done before to build better models, but

45
00:03:25,600 --> 00:03:28,000
obviously adding the physics in as well?

46
00:03:28,000 --> 00:03:33,640
A nice part, a nice bit is that actually the physics of particles and the physics of data

47
00:03:33,640 --> 00:03:34,880
are actually very much related.

48
00:03:34,880 --> 00:03:38,640
They are brought family of physics or statistical physics.

49
00:03:38,640 --> 00:03:43,040
And that's how I got into some machine learning through this physics chemistry angle.

50
00:03:43,040 --> 00:03:47,720
So since two and a half years ago, I returned back to the UK and started my own research group

51
00:03:47,720 --> 00:03:53,320
in Cambridge, where we now work at the intersection between a chemistry of drug discovery, physics

52
00:03:53,320 --> 00:03:59,640
of materials and machine learning, and so six months ago, I co-founded postera, which

53
00:03:59,640 --> 00:04:05,800
offers medicinal chemistry as a surface powered by machine learning, which retakes the machine

54
00:04:05,800 --> 00:04:10,440
learning approach that we developed once that further and deployed in the wild, so to

55
00:04:10,440 --> 00:04:12,640
speak, in drug discovery projects.

56
00:04:12,640 --> 00:04:13,640
Awesome.

57
00:04:13,640 --> 00:04:14,640
Awesome.

58
00:04:14,640 --> 00:04:16,400
You mentioned the physics of data.

59
00:04:16,400 --> 00:04:17,400
What does that mean to you?

60
00:04:17,400 --> 00:04:21,120
Is that just statistics or is there something more to it than that?

61
00:04:21,120 --> 00:04:27,680
I think it's both about how do we understand noise in the data sets, for example, some

62
00:04:27,680 --> 00:04:35,680
of my research pertains to estimating uncertainty in model predictions, both in terms of how

63
00:04:35,680 --> 00:04:40,680
do we estimate measurement noise and how to measure noise on certain, due to not having

64
00:04:40,680 --> 00:04:44,280
enough data in a certain area of chemical or material space.

65
00:04:44,280 --> 00:04:49,920
And that motivates a line of research on patient deep learning or patient approaches to machine

66
00:04:49,920 --> 00:04:52,120
learning in general.

67
00:04:52,120 --> 00:04:57,040
And that's actually quite important in we link it back to real life experiments, because

68
00:04:57,040 --> 00:05:02,640
experiments are often extremely expensive and you really want to be able to probe regions

69
00:05:02,640 --> 00:05:09,440
of chemical or material space, which are the most fruitful and that you really gain the

70
00:05:09,440 --> 00:05:12,280
most information from doing the least amount of experiments.

71
00:05:12,280 --> 00:05:17,200
So the physics of machine learning and the physical process of doing experiments have become

72
00:05:17,200 --> 00:05:21,160
pretty joined up together when you think about the whole process.

73
00:05:21,160 --> 00:05:26,600
Several of the conversations I've had with folks working in areas like this rely heavily

74
00:05:26,600 --> 00:05:30,600
on simulation for their work, do you as well?

75
00:05:30,600 --> 00:05:34,200
We take simulations obviously as one sort of data, and then there you're often extremely

76
00:05:34,200 --> 00:05:42,160
powerful in the middle of pure ML and pure experiments, which usually experiments are

77
00:05:42,160 --> 00:05:45,880
usually slightly slower and often more costly.

78
00:05:45,880 --> 00:05:52,640
But I think we are now trying to put the physics behind those simulations into the construction

79
00:05:52,640 --> 00:05:58,920
machine learning models or conversely interpret the architecture of the simulations as a machine

80
00:05:58,920 --> 00:06:00,560
learning model by itself.

81
00:06:00,560 --> 00:06:07,720
So for example, a lot of physics-based simulations often contains parameters, often contains equations

82
00:06:07,720 --> 00:06:13,320
that are empirically parameterized and a strand of my research is indeed trying to interpret

83
00:06:13,320 --> 00:06:17,520
these very powerful simulation engines as machine learning models themselves.

84
00:06:17,520 --> 00:06:23,200
And then you can think about, well, can I judiciously tune and decide these parameters based

85
00:06:23,200 --> 00:06:24,200
on data?

86
00:06:24,200 --> 00:06:28,880
Because I think the physics is so much to offer in terms of the frameworks, modeling frameworks

87
00:06:28,880 --> 00:06:35,960
and data is so much to offer in terms of fine tuning, the gap between predictions and observables.

88
00:06:35,960 --> 00:06:37,480
Can you elaborate on that a bit more?

89
00:06:37,480 --> 00:06:42,640
What does that mean to turn a physics simulation into a machine learning model?

90
00:06:42,640 --> 00:06:43,640
Right.

91
00:06:43,640 --> 00:06:51,280
So for example, we recently tried to use machine learning to predict the structure of complex

92
00:06:51,280 --> 00:06:52,280
liquids.

93
00:06:52,280 --> 00:06:59,760
So if you have a bunch of particles moving around in the liquid state, a question in physics

94
00:06:59,760 --> 00:07:05,400
as well, what will be the structure of these particles, what will be the structure of the

95
00:07:05,400 --> 00:07:06,400
liquid?

96
00:07:06,400 --> 00:07:12,040
That's one of these very basic questions in soft condense metal or condense metal physics.

97
00:07:12,040 --> 00:07:18,200
And a branch of physics known as liquid state theory parameterized, discussed a very elegant

98
00:07:18,200 --> 00:07:20,040
way to solve this question.

99
00:07:20,040 --> 00:07:26,080
And there's one equation called the constitutive equation or the closure relation that is unknown.

100
00:07:26,080 --> 00:07:31,760
And although the physical framework is there, that equation is what sort of hinders a lot

101
00:07:31,760 --> 00:07:32,760
of progress.

102
00:07:32,760 --> 00:07:38,760
And we say, hey, why not just take the framework and parameterize the equation using data.

103
00:07:38,760 --> 00:07:41,800
Just do a lot of simulation data and parameterize the equation using ML.

104
00:07:41,800 --> 00:07:44,360
And that's a lot better than throwing away the whole physical framework.

105
00:07:44,360 --> 00:07:47,720
Just use ML to learn everything from scratch because it's so much progress there.

106
00:07:47,720 --> 00:07:52,080
So that's one example where we can take a classical theory, look at the weakest link,

107
00:07:52,080 --> 00:07:57,080
which is usually an empirical equation and say, okay, let's tackle the empirical equation

108
00:07:57,080 --> 00:08:00,680
using ML, but leaving the classical theory intact.

109
00:08:00,680 --> 00:08:01,680
Awesome.

110
00:08:01,680 --> 00:08:10,480
And so ultimately you're trying to apply this to medicinal applications, drug discovery.

111
00:08:10,480 --> 00:08:16,960
But also, I guess I'm curious about the relationship between drug discovery and materials and material

112
00:08:16,960 --> 00:08:18,320
science.

113
00:08:18,320 --> 00:08:25,240
I often think of drugs more from the perspective of their chemical properties and materials

114
00:08:25,240 --> 00:08:31,520
in terms of wanting to create new kind of macro materials.

115
00:08:31,520 --> 00:08:37,080
I'm not sure what the question is there, but I'm trying to get at the techniques the same

116
00:08:37,080 --> 00:08:43,440
across drug and materials discovery, or are they very different?

117
00:08:43,440 --> 00:08:49,640
I think the questions that we are trying to ask, obviously, are somewhat different.

118
00:08:49,640 --> 00:08:55,120
Because as you alluded to from materials, you're usually interested in self-bulk properties

119
00:08:55,120 --> 00:08:58,560
or drugs you're interested in properties at the molecular scale.

120
00:08:58,560 --> 00:09:04,600
But I think in terms of the models that we construct, and in particular the philosophy

121
00:09:04,600 --> 00:09:08,000
that we take and construct the models, that's very similar.

122
00:09:08,000 --> 00:09:15,240
So for example, we use a lot of graph neural networks for chemistry because we can look

123
00:09:15,240 --> 00:09:21,440
at a chemical compound or chemical molecule as a graph, and you can perform operations

124
00:09:21,440 --> 00:09:22,440
on the graph.

125
00:09:22,440 --> 00:09:27,160
And we have extended this to think about Bayesian graph neural networks because uncertainty

126
00:09:27,160 --> 00:09:30,160
is extremely important for drug discovery.

127
00:09:30,160 --> 00:09:37,000
But conversely you can also think about a formula of an inorganic material, let's say a

128
00:09:37,000 --> 00:09:39,960
battery cathol material as a graph as well.

129
00:09:39,960 --> 00:09:44,000
So you can think of a formula of a material as a graph, and recently showed that we can

130
00:09:44,000 --> 00:09:50,600
basically featureize, let's say, a cathol material of battery as a graph, and then using

131
00:09:50,600 --> 00:09:57,240
this material graph to predict materials property and also estimate uncertainty and drive

132
00:09:57,240 --> 00:09:58,240
experiments.

133
00:09:58,240 --> 00:10:01,480
So in those are the methodology, I think there's a lot of synergies and similarity.

134
00:10:01,480 --> 00:10:06,720
And I think in terms of thinking about the whole design cycle in chemistry and drug discovery

135
00:10:06,720 --> 00:10:12,400
and everything about the design, make test cycles or how to design compounds, how to synthesize

136
00:10:12,400 --> 00:10:16,800
compounds in the lab, which usually is the rate determining step actually, and how to design

137
00:10:16,800 --> 00:10:19,160
experiments to test compounds.

138
00:10:19,160 --> 00:10:22,360
And if you think about that framework, then we can see a lot of parallels between why

139
00:10:22,360 --> 00:10:24,920
it's done in chemistry and why it's done in material science.

140
00:10:24,920 --> 00:10:28,640
Obviously, I think the key difference in material science is that experiments actually

141
00:10:28,640 --> 00:10:31,240
even costlier than chemistry.

142
00:10:31,240 --> 00:10:38,800
So in drug discovery, a lot of these measurements can be done relatively, there are protocols

143
00:10:38,800 --> 00:10:43,480
to power chemical assays that can do these experiments to test where the compound is

144
00:10:43,480 --> 00:10:47,440
actually potent against the protein relatively quickly.

145
00:10:47,440 --> 00:10:53,960
From materials trying to make a new superconducting material and test it can be some a PhD project

146
00:10:53,960 --> 00:10:54,960
in of itself.

147
00:10:54,960 --> 00:10:58,600
So I think the throughput is very different, which means that it's a lot more interesting

148
00:10:58,600 --> 00:11:03,800
ML, specifically in the low data limit that we can think about in material science.

149
00:11:03,800 --> 00:11:10,680
You've mentioned a few times estimating uncertainty is being one of the goals or at least a useful

150
00:11:10,680 --> 00:11:13,600
property to have in this type of work.

151
00:11:13,600 --> 00:11:18,720
What are the types of uncertainties that you're trying to estimate and how do they play

152
00:11:18,720 --> 00:11:20,760
out in the chemical and material realms?

153
00:11:20,760 --> 00:11:21,760
Right.

154
00:11:21,760 --> 00:11:29,040
I think the two types of uncertainty is first uncertainty due to having insufficient

155
00:11:29,040 --> 00:11:35,280
data in a particular chemical space or material space, and that's usually known as the epistemic

156
00:11:35,280 --> 00:11:36,280
uncertainty.

157
00:11:36,280 --> 00:11:42,960
The second type of uncertainty is the uncertainty due to the measurements themselves because

158
00:11:42,960 --> 00:11:49,200
a measurement inherently is not noise free and usually the noise can be an inhomogeneous

159
00:11:49,200 --> 00:11:53,360
function of where you are in chemical space and that is usually overlooked in a lot of

160
00:11:53,360 --> 00:11:54,360
approaches.

161
00:11:54,360 --> 00:11:57,580
If you actually do an experiment, you know that some molecules are, some materials are

162
00:11:57,580 --> 00:12:03,000
inherently more difficult to deal with than others.

163
00:12:03,000 --> 00:12:05,600
And that's called, it's about allotoric uncertainty.

164
00:12:05,600 --> 00:12:10,160
And we capture both within the framework of probabilistic patient deep learning and actually

165
00:12:10,160 --> 00:12:16,320
capturing the variation of uncertainty as a function of where you are in chemical space

166
00:12:16,320 --> 00:12:17,600
due to measurements.

167
00:12:17,600 --> 00:12:22,280
I think it's something that is super important because in a lot of material discovery, typically

168
00:12:22,280 --> 00:12:27,580
want to discover material that is robust rather than a material that will sometimes give

169
00:12:27,580 --> 00:12:30,800
you good results, but other times less so.

170
00:12:30,800 --> 00:12:34,200
For these different types of problems, how are you formulating your problem?

171
00:12:34,200 --> 00:12:35,720
What are you modeling?

172
00:12:35,720 --> 00:12:40,000
What are you trying to predict ultimately that you want to incorporate these different measures

173
00:12:40,000 --> 00:12:41,680
of uncertainty into?

174
00:12:41,680 --> 00:12:46,600
So we are to be trying to predict figures, figures of merit, for example, in drug discovery

175
00:12:46,600 --> 00:12:54,320
that would be a balance between how strongly would a molecule bind towards a protein and

176
00:12:54,320 --> 00:12:58,680
also how strongly would the molecule bind to other proteins which you would want to

177
00:12:58,680 --> 00:13:04,520
avoid because that causes toxicity effects and as well as other properties like solubility

178
00:13:04,520 --> 00:13:07,880
or other properties that require to make a molecule drop.

179
00:13:07,880 --> 00:13:13,480
In materials, it would be the property trying to optimize, for example, so the band gap of

180
00:13:13,480 --> 00:13:19,000
materials should try to think about photovoltaics or the strength of material, think about functional

181
00:13:19,000 --> 00:13:24,160
materials like alloys, but nicely about our framework is that by thinking about materials

182
00:13:24,160 --> 00:13:30,560
and molecules in a more abstract way, we're able to create frameworks that are generalizable

183
00:13:30,560 --> 00:13:33,040
across chemical material space.

184
00:13:33,040 --> 00:13:38,840
In the case of materials, the characteristics that you've mentioned, strength, for example,

185
00:13:38,840 --> 00:13:42,560
is often the target you're looking to build stronger materials.

186
00:13:42,560 --> 00:13:47,040
In the case of drug discovery, it's not like the things that you're trying to predict

187
00:13:47,040 --> 00:13:48,360
aren't so much.

188
00:13:48,360 --> 00:13:52,960
I think that the primary focus is efficacy against something.

189
00:13:52,960 --> 00:13:57,920
Is that a prior or something that you know operating and you have a laundry list of things

190
00:13:57,920 --> 00:14:04,000
that you want to test for these secondary characteristics or is efficacy also a part

191
00:14:04,000 --> 00:14:07,840
of what you're trying to use machine learning to identify?

192
00:14:07,840 --> 00:14:10,360
That's a great question, actually.

193
00:14:10,360 --> 00:14:16,240
So in drug discovery, typically, we think about in terms of three stages, the biology,

194
00:14:16,240 --> 00:14:19,480
the chemistry, and the medicine, I don't know why that way.

195
00:14:19,480 --> 00:14:24,080
So the biology part is indeed about asking what is the question.

196
00:14:24,080 --> 00:14:27,800
So if you want to cure a disease, which proteins you should target?

197
00:14:27,800 --> 00:14:28,800
Yeah.

198
00:14:28,800 --> 00:14:33,200
And then the chemistry part is conditioned on these proteins from the target, give me the

199
00:14:33,200 --> 00:14:36,800
best molecule that can hit those proteins from the others and the medicine part is

200
00:14:36,800 --> 00:14:42,640
so carrying forward the molecule to how do you think of drugs and impact patients.

201
00:14:42,640 --> 00:14:46,320
So right now, we are not really working that much on the biology part.

202
00:14:46,320 --> 00:14:49,720
I think it's a fascinating question, it's a very difficult question, we're not redoing

203
00:14:49,720 --> 00:14:50,720
that much there yet.

204
00:14:50,720 --> 00:14:56,320
I think a lot of very talented folks are dedicating a lot of effort in the more biological

205
00:14:56,320 --> 00:14:57,320
parts.

206
00:14:57,320 --> 00:15:00,120
So I will mostly put things to the chemistry part.

207
00:15:00,120 --> 00:15:03,080
So we've talked a lot about the chemistry part thus far.

208
00:15:03,080 --> 00:15:12,640
We also focus in your research on more theoretical questions around machine learning and algorithms

209
00:15:12,640 --> 00:15:17,800
and tell us a little bit more about that aspect of your research.

210
00:15:17,800 --> 00:15:18,800
Right.

211
00:15:18,800 --> 00:15:23,000
So I think in the process of trying to come up with algorithms or chemistry materials,

212
00:15:23,000 --> 00:15:29,920
we realize there are some interesting questions that are so general across the different algorithms

213
00:15:29,920 --> 00:15:32,560
that we are dealing with.

214
00:15:32,560 --> 00:15:39,040
For example, the question of well, we all use gradient optimizers to optimize algorithms

215
00:15:39,040 --> 00:15:45,120
but from an optimist parameters rather but from a physics perspective, do we know why

216
00:15:45,120 --> 00:15:51,240
these algorithms work and why do they not get stuck in bad solutions, high energy solutions

217
00:15:51,240 --> 00:15:57,720
or high loss function solutions, do we know how the loss function landscape looks like.

218
00:15:57,720 --> 00:16:03,120
I mean, those are questions that obviously has been considered by a lot of machine learning

219
00:16:03,120 --> 00:16:05,440
pioneers in the past.

220
00:16:05,440 --> 00:16:11,560
So we build on those works and so use techniques in statistical physics and pure mathematics

221
00:16:11,560 --> 00:16:16,400
to sort of analyze and sort of toy models of these neural networks and trying to get

222
00:16:16,400 --> 00:16:21,120
to the heart of why certain optimizers work and we in fact show that there's a very

223
00:16:21,120 --> 00:16:28,200
nice synergy and mapping between machine learning and a statistical physics problem of

224
00:16:28,200 --> 00:16:33,600
sort of free energy optimization that allows us to explain why these optimizers work and

225
00:16:33,600 --> 00:16:39,920
we also then move on to think about what's the landscape of the loss function in machine

226
00:16:39,920 --> 00:16:40,920
learning algorithm.

227
00:16:40,920 --> 00:16:46,080
And so to derive a set of very interesting analytical results showing why deep networks

228
00:16:46,080 --> 00:16:49,040
are easier to optimize than shallow networks.

229
00:16:49,040 --> 00:16:52,680
And obviously, the end goal of these research is to try to think about whether they are

230
00:16:52,680 --> 00:16:57,960
better inference algorithms and they we can obtain by taking a more principled view to

231
00:16:57,960 --> 00:16:59,680
why and how algorithms work.

232
00:16:59,680 --> 00:17:00,680
Okay.

233
00:17:00,680 --> 00:17:05,520
Is there a short answer to why deep networks are easier to optimize than shallow networks?

234
00:17:05,520 --> 00:17:12,000
So we find is that deep networks, minimized deep networks are actually closer together

235
00:17:12,000 --> 00:17:18,160
than shallow networks that deep networks actually of good solutions are easier to find than

236
00:17:18,160 --> 00:17:19,160
that solutions.

237
00:17:19,160 --> 00:17:20,960
I think that's something in a nutshell.

238
00:17:20,960 --> 00:17:26,200
So the summary of the analytical results, if you just minimize, it's just easier to find

239
00:17:26,200 --> 00:17:28,280
good solutions than bad ones.

240
00:17:28,280 --> 00:17:36,680
In our conversation thus far and in some of your work, you mentioned energy, energy landscape

241
00:17:36,680 --> 00:17:38,680
and free energy and things like that.

242
00:17:38,680 --> 00:17:44,680
Can you elaborate on how you use that concept and how it comes into play?

243
00:17:44,680 --> 00:17:45,680
Right.

244
00:17:45,680 --> 00:17:53,440
So energy landscape in physics and chemistry relates to the concept of how the potential

245
00:17:53,440 --> 00:17:58,400
energy of a system changes as you change the system.

246
00:17:58,400 --> 00:18:04,920
So for example, if I have two atoms connected by a spring, pull the atoms apart, the energy

247
00:18:04,920 --> 00:18:07,040
increases because atoms really want to be together.

248
00:18:07,040 --> 00:18:11,440
If I squash atoms too much, the energy also increases but atoms want to be sort of not overlapping

249
00:18:11,440 --> 00:18:12,440
each other.

250
00:18:12,440 --> 00:18:14,160
Therefore, you get a chemical bond.

251
00:18:14,160 --> 00:18:17,840
You think of this, generalizing this concept to many, many degrees of freedom, so many,

252
00:18:17,840 --> 00:18:22,080
many atoms and you have a whole landscape, depending on the coordinates of each atom,

253
00:18:22,080 --> 00:18:23,080
you get a different energy.

254
00:18:23,080 --> 00:18:24,080
Okay.

255
00:18:24,080 --> 00:18:28,200
If you think of each atom as a parameter in the machine learning model, then aha, this

256
00:18:28,200 --> 00:18:34,480
and you think of the energy as the loss function, then you can basically map a lot of what

257
00:18:34,480 --> 00:18:41,680
we have, people have fallen about in physics and chemistry into ideas in machine learning.

258
00:18:41,680 --> 00:18:48,520
So for example, glasses in physics correspond to the first certain interesting time of physics.

259
00:18:48,520 --> 00:18:49,520
Sorry.

260
00:18:49,520 --> 00:18:50,520
Sorry.

261
00:18:50,520 --> 00:18:51,520
What in physics?

262
00:18:51,520 --> 00:18:52,520
Glasses.

263
00:18:52,520 --> 00:18:54,040
The idea of glass of a glass system.

264
00:18:54,040 --> 00:19:03,280
So if you think about a sort of a glass, then that system is disordered, it doesn't really

265
00:19:03,280 --> 00:19:06,040
have an order or crystalline structure in it.

266
00:19:06,040 --> 00:19:11,640
It flows extremely slowly, in fact, a glass is technically not a solid because it flows

267
00:19:11,640 --> 00:19:13,960
extremely, extremely slowly.

268
00:19:13,960 --> 00:19:19,560
And the energy landscape of glasses actually shares some similarity with the energy landscape

269
00:19:19,560 --> 00:19:23,400
or the loss function landscape machine learning models and it says that it's highly

270
00:19:23,400 --> 00:19:24,400
disordered.

271
00:19:24,400 --> 00:19:28,800
The parameters are definitely not regular at all, like you see very bumpy energy landscapes.

272
00:19:28,800 --> 00:19:29,800
So that's one extreme.

273
00:19:29,800 --> 00:19:34,080
The other interesting example to think about is proteins, for example, the energy landscape

274
00:19:34,080 --> 00:19:35,800
of proteins are very fun.

275
00:19:35,800 --> 00:19:40,640
So go funnel like because the protein needs to reach a state very quickly as it won't

276
00:19:40,640 --> 00:19:41,640
vote.

277
00:19:41,640 --> 00:19:46,120
And if we can engineer a machine learning landscape that is funnel like, rather than glass

278
00:19:46,120 --> 00:19:50,200
like, and that's great because that means the machine learning model quickly finds the

279
00:19:50,200 --> 00:19:52,640
best parameters with minimal effort.

280
00:19:52,640 --> 00:19:58,000
So you can think about these physical objects and this analogies of particular models to

281
00:19:58,000 --> 00:20:00,920
think about how we can optimize the models.

282
00:20:00,920 --> 00:20:07,400
So you draw this parallel between physics and the kind of loss function optimization space

283
00:20:07,400 --> 00:20:08,400
of models.

284
00:20:08,400 --> 00:20:13,400
I guess I'm, here's what are some concrete results that you have, you know, either, you

285
00:20:13,400 --> 00:20:16,640
know, from physics to machine learning or the other way around.

286
00:20:16,640 --> 00:20:18,480
You mentioned a few things.

287
00:20:18,480 --> 00:20:23,200
Are those, you know, how concrete are those as opposed to ideas if we could think about

288
00:20:23,200 --> 00:20:28,960
the machine learning, the protein example that you use, like, have you actually implement

289
00:20:28,960 --> 00:20:34,280
that implemented that and use that to find concrete deep learning architectures that converge

290
00:20:34,280 --> 00:20:39,280
faster because they use some properties of physical protein?

291
00:20:39,280 --> 00:20:40,400
That's a great question.

292
00:20:40,400 --> 00:20:43,920
So we have had, we are still working towards that.

293
00:20:43,920 --> 00:20:47,880
There will be a dream of this direction that research released from my perspective is

294
00:20:47,880 --> 00:20:52,120
to find some math maps between the two and actually accelerate ML.

295
00:20:52,120 --> 00:20:53,120
Yeah.

296
00:20:53,120 --> 00:20:58,520
Right now we have characterized ML algorithms both numerically and analytically and

297
00:20:58,520 --> 00:21:05,640
created, I guess, an idea of how how ML algorithms can be mapped to physical objects, physical

298
00:21:05,640 --> 00:21:07,400
heuristics, right?

299
00:21:07,400 --> 00:21:11,200
And then an active area of research is indeed trying to go once that further and think

300
00:21:11,200 --> 00:21:16,480
about how to create some more optimizable ML algorithms.

301
00:21:16,480 --> 00:21:24,800
Do you envision that there are things that we can learn about physics by observing machine

302
00:21:24,800 --> 00:21:30,560
learning or is physics so much further ahead that, you know, there's probably nothing interesting

303
00:21:30,560 --> 00:21:31,560
there?

304
00:21:31,560 --> 00:21:35,920
Oh, I think physics is a lot to gain from machine learning.

305
00:21:35,920 --> 00:21:44,600
I think a lot of physical theories are derived based on be looking at data and say, and

306
00:21:44,600 --> 00:21:49,360
the physics is saying, oh, like, because I can only keep track of, well, two degrees of

307
00:21:49,360 --> 00:21:56,800
freedom, if I'm plotting in 2D, that I want to get a line and I only focus on two things.

308
00:21:56,800 --> 00:22:03,040
And that biases you towards sort of neglecting a lot of data, which could actually be interesting

309
00:22:03,040 --> 00:22:04,040
and useful.

310
00:22:04,040 --> 00:22:11,600
So in this theme, for example, we published a recent paper on a battery degradation where

311
00:22:11,600 --> 00:22:18,000
a lot of practitioners in the field has sort of looked at spectroscopic ideas to predict

312
00:22:18,000 --> 00:22:24,040
how the lifetime of battery, given non-invasive spectroscopic measurements, if you can do

313
00:22:24,040 --> 00:22:28,600
that, then you can tell them, so how whether battery is still alive or not, or how many

314
00:22:28,600 --> 00:22:33,560
cycles is left before it would die, and that's very useful for some like electric vehicles

315
00:22:33,560 --> 00:22:35,640
or personal consumer electronics.

316
00:22:35,640 --> 00:22:42,440
But the upshot is that a lot of folks look at, so very clear, observable features of

317
00:22:42,440 --> 00:22:46,960
these spectrum, whereas if you just train the machine learning model on the spectrum to

318
00:22:46,960 --> 00:22:50,640
use it to predict degradation, the machine learning model actually identifies very subtle

319
00:22:50,640 --> 00:22:51,640
features.

320
00:22:51,640 --> 00:22:57,200
So very high order correlations of features that you will never even expect to be important.

321
00:22:57,200 --> 00:23:02,200
And then you can go back and go to the physics and ask, well, what does this feature correspond

322
00:23:02,200 --> 00:23:03,200
to?

323
00:23:03,200 --> 00:23:07,720
And in fact, we are now thinking about are there new explanations of battery degradation

324
00:23:07,720 --> 00:23:12,440
based on just assigning, or this feature is actually important for degradation.

325
00:23:12,440 --> 00:23:16,520
That's not something you can just do looking at the data or conversion of physical theories.

326
00:23:16,520 --> 00:23:17,520
Interesting.

327
00:23:17,520 --> 00:23:27,440
Do you think it's possible that some of the well-known analytical, close analytical results

328
00:23:27,440 --> 00:23:33,880
that we take for granted in physics that are low-order polynomial actually have a lot

329
00:23:33,880 --> 00:23:40,080
of these subtle additional features that will come to discover based on this line of thinking?

330
00:23:40,080 --> 00:23:45,320
Or am I oversimplifying, I'm thinking like maybe it's not equals MC squared, but there's

331
00:23:45,320 --> 00:23:50,920
all these other, you know, there are all these other kind of higher order features that,

332
00:23:50,920 --> 00:23:56,640
you know, because of our bias to clean polynomial solutions, we've, you know, ended up, you

333
00:23:56,640 --> 00:24:01,160
know, holding these relationships up that aren't as, you know, nuanced enough.

334
00:24:01,160 --> 00:24:02,160
Yeah.

335
00:24:02,160 --> 00:24:08,280
I think I would have a distinction between, um, solve fundamental physics frameworks and

336
00:24:08,280 --> 00:24:10,880
how physics is being implemented and executed.

337
00:24:10,880 --> 00:24:15,520
I think if you think about fundamental physics frameworks, like special relativity, general

338
00:24:15,520 --> 00:24:18,640
relativity, quantum mechanics, gravity, I think.

339
00:24:18,640 --> 00:24:21,480
And I was just in a group of low-order polynomial relationships.

340
00:24:21,480 --> 00:24:24,280
I wasn't necessarily talking about relativity.

341
00:24:24,280 --> 00:24:29,840
So I'm not an expert in those areas, and I think that the frameworks they are relatively

342
00:24:29,840 --> 00:24:30,840
robust.

343
00:24:30,840 --> 00:24:37,200
Um, but I think the implementation of these frameworks into material discovery or not usually

344
00:24:37,200 --> 00:24:41,080
require lost simplifications, like quantum mechanics, for example, um, can we solve quantum

345
00:24:41,080 --> 00:24:42,080
mechanics?

346
00:24:42,080 --> 00:24:45,640
Um, numerically even, like super computers, like exactly numerically.

347
00:24:45,640 --> 00:24:50,080
So you make a lot of simplifications on the way, the same with other areas of physics.

348
00:24:50,080 --> 00:24:55,760
And the quality of these approximations can be massively improved with machine learning.

349
00:24:55,760 --> 00:24:56,760
Got it.

350
00:24:56,760 --> 00:25:02,160
So it's not that you're, you know, fundamental laws of thermodynamics aren't robust.

351
00:25:02,160 --> 00:25:03,160
They probably are.

352
00:25:03,160 --> 00:25:04,160
Yeah.

353
00:25:04,160 --> 00:25:05,160
Pretty good.

354
00:25:05,160 --> 00:25:10,160
But when in the real world, there are a lot of, there are a lot of other factors that aren't

355
00:25:10,160 --> 00:25:15,320
captured by kind of idealized models that we tend to overlook and machine learning, you

356
00:25:15,320 --> 00:25:19,880
know, what we learn from machine learning might give us a path to incorporating those

357
00:25:19,880 --> 00:25:20,880
as features.

358
00:25:20,880 --> 00:25:21,880
Yeah.

359
00:25:21,880 --> 00:25:26,360
And I think also it's one of the lot of these physical frameworks are so very easy to express

360
00:25:26,360 --> 00:25:27,360
mathematically.

361
00:25:27,360 --> 00:25:28,360
Yeah.

362
00:25:28,360 --> 00:25:33,840
But actually, I'm solving it for real world, for real molecule or real world problem is actually

363
00:25:33,840 --> 00:25:37,520
exponentially complex, the quantum mechanics, for example, is very, it's very straightforward

364
00:25:37,520 --> 00:25:43,080
to write down an ordinary or partial differential equation, but then to solve it for an, an

365
00:25:43,080 --> 00:25:47,160
electron problem is really non trivial.

366
00:25:47,160 --> 00:25:52,640
And so then that's why people start making a lot of these very good approximations, but

367
00:25:52,640 --> 00:25:57,880
so these approximations could be improved when you take a step back and say, well, why

368
00:25:57,880 --> 00:26:02,880
not just use existing measurements as a way to fine tune the approximations?

369
00:26:02,880 --> 00:26:10,000
Do you in your work look at the activity that's happening around neural ODE research and

370
00:26:10,000 --> 00:26:11,000
does that work?

371
00:26:11,000 --> 00:26:15,280
I've heard of, I've certainly read briefly the paper and I think it's a very interesting

372
00:26:15,280 --> 00:26:18,800
approach to how to integrate physics.

373
00:26:18,800 --> 00:26:24,920
So frameworks like ODE's into machine learning, I think so much could be done to physics if

374
00:26:24,920 --> 00:26:28,960
we start thinking about some semi empirical and empirical physical frameworks as machine

375
00:26:28,960 --> 00:26:29,960
learning.

376
00:26:29,960 --> 00:26:34,280
So people in automatic differentiation field, for example, think about differentiating

377
00:26:34,280 --> 00:26:36,280
through machine learning models.

378
00:26:36,280 --> 00:26:41,280
I've not done research in this area per se, but I am aware of a lot of amazing work coming

379
00:26:41,280 --> 00:26:47,040
out of this community of thinking about maybe three simulations as machine learning and

380
00:26:47,040 --> 00:26:51,080
do automatic differentiation through simulations, et cetera.

381
00:26:51,080 --> 00:26:58,000
You are currently in Santa Clara in California, a long way from either of the Cambridges.

382
00:26:58,000 --> 00:27:04,080
And what brought you there is the startup that you founded, Posterra, and the opportunity

383
00:27:04,080 --> 00:27:08,240
that you had to go through the Y-combinator accelerator.

384
00:27:08,240 --> 00:27:13,440
Can you talk a little bit about Posterra and what it's specifically looking to do?

385
00:27:13,440 --> 00:27:14,440
Yep.

386
00:27:14,440 --> 00:27:20,480
So Posterra is a company that tries to offer some medicinal chemistry as a surface powered

387
00:27:20,480 --> 00:27:22,960
by machine learning.

388
00:27:22,960 --> 00:27:30,440
And we realized that although a lot of excitement has gone into AIML for drug discovery, we take

389
00:27:30,440 --> 00:27:35,960
a step back, we realized there are two key pain points amongst the whole life cycle.

390
00:27:35,960 --> 00:27:42,080
So the first pain point is how do we make molecules?

391
00:27:42,080 --> 00:27:45,920
So the chemical synthesis of molecules, and that usually is the rate determining step

392
00:27:45,920 --> 00:27:49,320
when you try to make thousands of molecules.

393
00:27:49,320 --> 00:27:55,800
Making a molecule has been the focus for a lot of academic and industrial groups, but

394
00:27:55,800 --> 00:27:58,880
how to make molecules relatively less so.

395
00:27:58,880 --> 00:28:01,280
And so we developed a state of the art algorithm.

396
00:28:01,280 --> 00:28:05,400
Part of it is actually published a molecular transformer for reaction prediction and

397
00:28:05,400 --> 00:28:10,480
retro synthesis, which is now the state of the art for reaction prediction, predicting

398
00:28:10,480 --> 00:28:15,720
how to make molecules react and inverse from how to make molecules.

399
00:28:15,720 --> 00:28:19,880
And we know that, and we think that this can really accelerate medicinal chemistry.

400
00:28:19,880 --> 00:28:23,160
And the second pillar that we focus on is uncertainty.

401
00:28:23,160 --> 00:28:29,200
How do we use uncertainty to design experiments such that we explore the most fruitful chemical

402
00:28:29,200 --> 00:28:30,200
space?

403
00:28:30,200 --> 00:28:34,360
And I think we are the, the first really thing about how to integrate the whole design

404
00:28:34,360 --> 00:28:41,240
mech test cycle together in one machine learning platform and offering it to accelerate

405
00:28:41,240 --> 00:28:44,200
drug discovery in both farm and biotech.

406
00:28:44,200 --> 00:28:48,800
Okay, you mentioned molecular transformers that related to the concept of transformers

407
00:28:48,800 --> 00:28:51,440
that we see in like natural language processing?

408
00:28:51,440 --> 00:28:52,440
Yep.

409
00:28:52,440 --> 00:28:59,040
It is inspired by that where we take the basically reaction prediction as a machine translation

410
00:28:59,040 --> 00:29:00,040
problem.

411
00:29:00,040 --> 00:29:01,040
Okay.

412
00:29:01,040 --> 00:29:02,040
We're reacting reagents.

413
00:29:02,040 --> 00:29:06,160
So what goes into the flask is treat as a language or comes out of the flask.

414
00:29:06,160 --> 00:29:10,920
The product is in another language and you think about whether you can use translation

415
00:29:10,920 --> 00:29:17,480
to as a as a as a as a heuristic concept to think about processing these inputs and outputs.

416
00:29:17,480 --> 00:29:22,280
And what we have shown is that actually this perhaps very simple, simple, but direct way

417
00:29:22,280 --> 00:29:26,600
of thinking about chemical reaction actually outperforms a lot of the more sort of heuristic

418
00:29:26,600 --> 00:29:35,480
way of sort of hand crafting reaction rules or hand crafting chemistry templates from textbook.

419
00:29:35,480 --> 00:29:40,360
This very simple approach based on translation achieves over 90% accuracy in predicting

420
00:29:40,360 --> 00:29:46,160
correctly what comes out of the flask given what goes into the flask and that accuracy

421
00:29:46,160 --> 00:29:48,800
outperforms even trained human chemists.

422
00:29:48,800 --> 00:29:51,520
And so how do you go about training a model like that?

423
00:29:51,520 --> 00:29:58,120
Are you doing similar types of techniques where you're doing close kinds of, you know,

424
00:29:58,120 --> 00:30:02,960
you're trying to train in the model to predict things that are left out of your training

425
00:30:02,960 --> 00:30:06,320
data or this is something different.

426
00:30:06,320 --> 00:30:12,480
So we have over 9 million reactions reported in patents, which we have cleaned, aggressively

427
00:30:12,480 --> 00:30:19,120
cleaned and augmented using sort of chemistry knowledge and we train the model on those

428
00:30:19,120 --> 00:30:23,400
data and obviously we invalidated the model using standard training test splits.

429
00:30:23,400 --> 00:30:24,400
Okay.

430
00:30:24,400 --> 00:30:28,560
So you've got the existing relationships that you're training on, right?

431
00:30:28,560 --> 00:30:29,560
Okay.

432
00:30:29,560 --> 00:30:34,640
We use data has been published and we use patent data, which which is a data source of

433
00:30:34,640 --> 00:30:40,160
robust chemical reactions and that means a lot of these chemical transformations can be

434
00:30:40,160 --> 00:30:43,760
readily translated into industrial context.

435
00:30:43,760 --> 00:30:48,600
What's the granularity of the or the format of the input data?

436
00:30:48,600 --> 00:30:53,960
Is it kind of the chemical equations that we're used to seeing from basic, you know, chemistry

437
00:30:53,960 --> 00:30:55,600
class and some representation?

438
00:30:55,600 --> 00:31:02,680
Are you providing lower level physical information about the various molecules or atoms?

439
00:31:02,680 --> 00:31:10,600
Yeah, so the data format is the molecular structure of the reactants, the molecular structure

440
00:31:10,600 --> 00:31:18,240
of the reagents, these reactants plus reagents are what goes into the flask and the product

441
00:31:18,240 --> 00:31:22,960
is what you isolate from this reaction again, the molecular structure is given and then

442
00:31:22,960 --> 00:31:29,080
transformer takes these two inputs and outputs and performs machine translation to predict

443
00:31:29,080 --> 00:31:31,200
the output given inputs.

444
00:31:31,200 --> 00:31:37,160
And that reaction prediction step or reactions in general is actually, I think, very important

445
00:31:37,160 --> 00:31:40,320
part in the whole chemistry stage in drug discovery.

446
00:31:40,320 --> 00:31:45,160
For example, we're actually leading a COVID project, a life COVID project trying to develop

447
00:31:45,160 --> 00:31:50,760
new ND virus against COVID is open source and non-profit initiative that we are having

448
00:31:50,760 --> 00:31:52,400
to lead with academic groups.

449
00:31:52,400 --> 00:31:58,240
And so we launch a crowdsourcing platform, sourcing compounds based on a fragment

450
00:31:58,240 --> 00:32:03,840
merge, so a high throughput screen that our colleagues at Oxford ran, we got like

451
00:32:03,840 --> 00:32:05,800
four, a few thousand structures.

452
00:32:05,800 --> 00:32:09,120
If you were to just look at it one by one and decide which ones can be made easily, it

453
00:32:09,120 --> 00:32:13,960
would take you weeks, whereas an algorithm like molecular transformer would take a weekend

454
00:32:13,960 --> 00:32:18,200
to triage, like which compounds are easily make up both from purchasable starting material

455
00:32:18,200 --> 00:32:23,920
and is that kind of speed, which allows rapid iterations and allows you to sort of make

456
00:32:23,920 --> 00:32:28,520
more molecules, test more molecules and ultimately, the discovered drugs faster.

457
00:32:28,520 --> 00:32:29,520
Okay.

458
00:32:29,520 --> 00:32:33,240
And have any interesting candidates come out of that process yet for COVID?

459
00:32:33,240 --> 00:32:38,840
Yeah, we have got several pretty promising hits against the target, which we are now

460
00:32:38,840 --> 00:32:39,840
rapidly developing.

461
00:32:39,840 --> 00:32:42,440
Oh, that's awesome.

462
00:32:42,440 --> 00:32:48,840
So you started why a commonator in January, are you close to your demo day?

463
00:32:48,840 --> 00:32:49,840
What's your January?

464
00:32:49,840 --> 00:32:50,840
You just did our demo day.

465
00:32:50,840 --> 00:32:51,840
Oh, you just did it.

466
00:32:51,840 --> 00:32:52,840
I go, yep.

467
00:32:52,840 --> 00:32:53,840
Oh, how did it go?

468
00:32:53,840 --> 00:32:55,600
It went really well.

469
00:32:55,600 --> 00:32:58,920
We were able to close our seat ground within a week.

470
00:32:58,920 --> 00:32:59,920
Okay.

471
00:32:59,920 --> 00:33:04,760
This is really interesting stuff, and I appreciate you taking the time to chat with me.

472
00:33:04,760 --> 00:33:10,520
Are there, do you have any parting words or thoughts or resources that folks can follow

473
00:33:10,520 --> 00:33:13,360
up on if they're interested in learning more about this area?

474
00:33:13,360 --> 00:33:21,560
Yeah, I'm happy to chat with any folks interested in our work and so our COVID platform,

475
00:33:21,560 --> 00:33:26,040
the plug on that is COVID.posterror.ai.

476
00:33:26,040 --> 00:33:32,440
Our whole project is non-profit and completely open, so if folks are interested in tripping

477
00:33:32,440 --> 00:33:39,880
in ideas or commenting on the ML or the chemistry, if we do so, we have a forum, a live forum

478
00:33:39,880 --> 00:33:46,240
where a lot of the chemist discuss ideas, welcome people, so we're chiming in.

479
00:33:46,240 --> 00:33:47,240
Awesome.

480
00:33:47,240 --> 00:33:48,240
Awesome.

481
00:33:48,240 --> 00:33:49,720
Well, Alpha, thanks so much for joining us.

482
00:33:49,720 --> 00:33:50,720
Yeah.

483
00:33:50,720 --> 00:33:58,000
All right, everyone, that's our show for today.

484
00:33:58,000 --> 00:34:03,800
For more information on today's show, visit twomolai.com slash shows.

485
00:34:03,800 --> 00:34:23,920
As always, thanks so much for listening and catch you next time.

