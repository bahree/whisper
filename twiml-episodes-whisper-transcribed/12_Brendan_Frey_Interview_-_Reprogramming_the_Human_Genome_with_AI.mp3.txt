Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting
people doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
In earlier episodes of the podcast, we've talked about some of the many implications of machine
learning and AI in health care, but we've not yet had an opportunity to dive deeply into
this application area.
Well, that changes now. I'm excited to share with you a really interesting conversation I had
with Brendan Frye, professor of engineering and medicine at the University of Toronto,
and co-founder and CEO of the startup Deep Genomics.
You're going to love this show and learn a ton.
I met Brendan at the rework Deep Learning Summit in San Francisco a few weeks ago,
and I expect to share with you a few other conversations from that conference over the next few weeks.
One of the questions I'm often asked is which AI events are worth going to?
Well, there are a ton of them nowadays, and it can be difficult to separate the good from the
bad. So what I'll be doing regularly here on the podcast is sharing some of the events on my radar.
I'll be sharing them on the Twimble website as well.
For March, I'm planning to be at a bunch of events in the Bay area.
The week of the 6th, I'll be at AI by the Bay, which looks to be a really interesting technical
conference, and next, the Google Cloud Developer Conference. The following week, I'll be at
Strata Hadoop World, which is a great event for machine learning and analytics discussions,
with a particular emphasis on data engineering and infrastructure.
And then, the week of the 20th, I'm attending another couple of events by rework,
this time their machine intelligence and autonomous vehicle summits.
What I'm most excited about, however, and I'll only share this brief teaser now,
is an event I'm organizing called the Future of Data Summit, which will take place in Las Vegas in
May. Stay tuned. I'll share all the details on a future podcast.
Going back to the Strata Hadoop Conference for a sec, you may remember that we partnered with
O'Reilly last year to offer a free ticket to their AI and Strata conferences to lucky Twimble
and AI listeners. Well, we're at it again. We've partnered with the good folks at O'Reilly
to bring you another opportunity to win a free ticket to Strata Hadoop World in San Jose, California.
Join Twimble and thousands of innovators, leaders, and practitioners at Strata Hadoop World to
develop new skills, share best practices, and discover how tools and technologies are evolving
to meet new challenges. One lucky listener will win a pass, but everyone can save 20% on registration
with discount code PC Twimble. That's PC-TW-I-M-L.
To enter, visit our brand new Facebook page at facebook.com slash Twimble AI, where you'll find
full details. Of course, I'll link to the Facebook page in the show notes, which will be posted at
Twimble AI dot com slash talk slash 12. Since the conference is coming up quickly, you'll only have
until March 3rd to enter. Winners will be notified shortly thereafter and announced on the next
podcast. One more quick note before we jump into the interview. Towards the end of my chat with
Brendan, I mentioned a sci-fi series that I like, but I blank on the title. Well, this series is
called the Xenogenesis Trilogy, and it's by Octavia Butler. It's also been re-released as Lilith's
brood, so you may see that as well. In any case, I'll post a link to the books in the show notes,
and now onto the show.
So hello everyone, I've got Brendan Fry on the line with me. Brendan and I met recently at the
Rework Deep Learning Summit in San Francisco, where he delivered a really, really great presentation
called reprogramming the human genome. Why AI is needed? Brendan was kind enough to agree to
discuss his presentation and work in the field here on the podcast. So welcome, Brendan.
Hi Sam, thanks for having me on the show. I'm really excited about this conversation.
You know, we've talked about deep learning and machine learning AI in general,
and healthcare several times on the show. Not in a lot of detail, but just covering the news,
and there's been a lot of advancement in this area. One of the things that we talked about was
Beth Israel Deaconis did the work with applying deep learning to breast cancer detection.
Google Deep Mind is active in applying deep learning to eye disease. And when I think about
examples like the ones that I've tended to see, they are often, they often fit into the pattern of,
hey, we've got a bunch of image data. We know deep learning is great for helping us
kind of find patterns and image data. Let's apply deep learning algorithms to see if we can,
you know, either augment or replace, you know, the medical technicians that are, you know,
finding tumors and things like that. But when I heard and thought about your presentation and the
way you walk through what you guys are doing, it struck me that, and let me know if this is fair,
but it struck me that you guys are applying or trying to apply deep learning at a much
more fundamental level, like looking at the, you know, interactions between proteins and things
like that that create disease. And that strikes me as just super exciting, and that's why I wanted
to really get, have this conversation. So, you know, maybe to get things started, you didn't start
your research career in genomics, how did you find your way into the field?
Yeah, so in the 1990s, I was a machine learning research, right? I did my PhD with Jeff Hinton,
and we were looking at image data and speech and text as well. We published one of the first papers
on deep learning in 1995, which appeared in science. And so really, those are good days,
just discovering new algorithms and trying them out, but we didn't have big data sets,
and we didn't have fast computers, and so, and so that was really a big bottleneck.
Of course, that's completely changed now, but around 2002, by that time I was a professor,
around 2002, my wife and I at the time discovered that the baby shows caring had a genetic problem.
We went and saw a genetic counselor and had to deal with very difficult news. The feedback
was it could be nothing or it could be a really big problem. So that was a really difficult
experience emotionally to go through that and really changed my focus in terms of what I was
doing as a researcher, and it decided to start working on vision and speech recognition and
text analysis and really focus and said on the human genome and figuring out how to connect
what's going on in people's DNA to to their health and also how to how to figure out how to treat
treat disease. And so was my assessment of what you guys are doing relative to some of the examples.
I provided, is that fair? How do you think about the the approach you're taking? Yeah, yeah,
that's that's accurate. So the a lot of other players in this field are essentially leveraging
their previous experience on on image analysis to to then look at medical images. Our approach is
very, very different. We're starting with the genome as the input and the genome is just a long
string of letters, ACG and T three billion of them from your mom and three billion from your dad.
And the challenge there is really to to figure out what the language is. So so first of all the
the language in the genome is not understood and how how words are put together to lead to to life
essentially is not well understood. And so reverse engineering how the genome works is a big
challenge. And then of course, once that's done figuring out how you can manipulate the genome,
what you can do to to fix diseases is the second challenge. So yeah, I've been working on that for
about 13 years and applying machine learning techniques to to crack that problem. And you're talk
you presented some pretty staggering statistics. I think it was that the lifetime risk for
a genetic related disease is something on the order of 65 percent and 8 million births per year
with serious genetic disorders. Yeah, that's right. It's a big problem. And you know, we've
we've been able to sequence the genome and now we can sequence individual genomes for about
a thousand dollars and in a few years it should cost less than a trip to the grocery store to
have your genome sequenced. And so we can read the text of your genome, but the tragic truth is
that we are not currently able to accurately figure out what's wrong with you if you have a
particular mutation, let alone figure out how to fix it. And so there's really a big gap. I call
that the genotype phenotype gap. There's a big gap between our ability to read the text of the
genome and the and make sense of it and then act on it. And so those statistics you gave like 8
million births per year with a serious genetic disorder. That's it's kind of horrendous when
you think we can sequence their genomes. We can find their mutations, but we don't really have
the ability currently to figure out what's going wrong. And that's what we're working on. So both
in my research lab and now with deep genomics, we're figuring out how to how to understand those
mutations and what their implications are. So you know, I'd like to I'd like to understand all
this better. And I'd like the audience to understand all this better. You know, what's the way?
How can you give us kind of a, you know, push us off the deep end perhaps into biology and genomics?
How, you know, how biology works and what are the various issues and implications so that we can
start to have a conversation about this? Yeah, sure. So a common pattern in the field right now is
people get a lot of data and then they kind of say, well, let's just throw it in a big bucket and
give it to machine learning researchers and they'll solve it all. And I think that's a really
bad approach. So our approach is very much a systems approach and that we try to understand biology.
We bring to bear very carefully all biological knowledge that we can ascertain and then we build
our machine learning systems to mimic that biology. And so for example, DNA is replicated when
a cell divides DNA is replicated. So that's an important process. The way DNA is used within
a cell is DNA is transcribed into RNA molecules. RNA molecules are chopped up and put back together
again in a process called splicing. The splice RNA molecules then they are translated into proteins.
And then the proteins go off and do things in the cell. And one of the things the proteins do is they
bind to DNA. So the proteins interact with DNA and they actually interact with DNA in a way that
controls transcription. The proteins also interact with RNA in a way that controls splicing and
similarly processes of translation. And so you can think of biology as these multiple layers
of processing, complex interactions, highly nonlinear, and really the phenotype that we see whether
it's maybe cancer or neurological disorder is something that's gone wrong within one of these
processes. And so that's just a brief summary of what's actually going on in the biology.
So in between the DNA and your phenotype, multiple layers of complex biological processes that
are nonlinear and combinatorial. And so what we do is we build machine learning models for each
of these processes. So right now in the biology community there's just an explosion of data sets
profiling what's going on within cells, essentially allowing scientists to peer right inside of cells
and measure at the single molecule level what's going on. And so there's this rapid growth of data
sets in the last few years and it's growing exponentially. And what we do is we use those massive
data sets to train models to mimic these cellular processes. And like to give you an idea,
the kinds of data sets we're looking at, we have trillions of data points that we use to train
models. Wow. So for any given one of these interactions, you know, before we even start talking
about the computational side of things, just as a community of biologists, how well do we understand
what's really happening in the processes and when we have a data set that we're looking at?
Yeah. Well, that's one of the things that we spent a lot of time on here at Deep Genomics
is basically taking known biology and then figuring out what kinds of data we have that allow us
to better model that known biology and then also try to account for unknown biology as well,
which is one of the nice things that the machine learning offers. People on the past have tried
literally writing down programs, computer programs to try to simulate what's going on in a cell.
And then you might guess those, yeah, you might guess that kind of approach breaks pretty easily.
First of all, we don't know all the rules. Second of all, quantities in the cell are real value.
They're not binary and logical. And so that approach doesn't really work that well.
People have also tried to write down stochastic differential equations describing the
concentrations of molecules in the cell. And that'll work for small sort of very simple,
contained systems where there aren't many molecules that kind of approach can work,
but it won't work for for living cells. There's just too many different molecules
and the processes are too complex. So the approach that we take is the machine learning approach.
We can measure different data sets for these different molecules and then we train machine learning
techniques to mimic the relationships between those data sets that then emerge due to these
biological processes. Is there some characterization for how many relationships there are?
Well, we have a roadmap. We have a technology roadmap, a deep genomics,
which lists all the different modules that we were trying to account for.
And we have a couple dozen described in our roadmap. But the number is much larger than that
and growing every year as well. But having said that, it's just sort of a notion of diminishing
returns. You can get quite a bit out of just modeling one or two processes. For example,
I mentioned splicing where our name molecule is chopped up and then glued back together again.
And that process depends on words essentially in the RNA molecule sequence. So RNA like DNA
is a sequence of letters. And the machinery inside of the cell recognizes little patterns of
letters or words. And those words tell the machinery how to cut up the RNA.
And those letters represent proteins. So the sequence of words is a sequence of proteins in
the RNA or DNA. Is that right? Oh, there's two kinds of words in there. So one kind of
word in the RNA molecule sequence of letters does encode a protein. So it corresponds to amino acids
that make the protein. But there are other words in the RNA that are more like control commands.
So it's kind of like in a computer, you have print statements and then you have control logic.
Right. Right. And the print statements are like proteins. But what's even more important
than the print statements is the control logic itself. That's what creates the system that's
responsive to its environment and that can do different things. Otherwise, we just keep printing
the same thing over and over. Okay. And so the ability of the system to respond to different
circumstances and be dynamic is really crucial. And that's achieved with these control statements,
if you like, that are also embedded within the RNA sequence. So we have a system that was trained
to mimic this process of splicing. And just to give you an example, one of the leading causes
of infant mortality in North America is spinal muscular atrophy. And there's a mutation in the DNA
that leads to this disease. And that mutation is in the RNA molecule as well. And it causes this
process of splicing to go wrong. It leads to so normally a certain chunk of RNA is included in
in the protein that makes up a certain gene called SMN1 or SMN2. And if there's a mutation,
then that chunk is left out and that leads to the disease. Usually those infants die within
the first year of birth. Recently, a therapy was given FDA approval. And what's interesting
about that therapy is that it doesn't target that particular mutation. It actually modifies
another part of the RNA molecule, which goes to show you the importance of commonatorics.
Right? This is not, the biology is not something where there's a correlative effect or there's
a particular mutation. You just need to fix that one mutation quite often. You need to change
something else in order to fix a problem that's occurring in the genome.
And so how I'm just thinking about the scale of this system and the interactions. You mentioned
on the order of a dozen molecules or sorry modules. For a given disease,
is it typically only one direct cause or they're often combinations of things happening
in these different subsystems that is what causes the disease to spring forth?
Yeah, that's a good question. And one thing I've learned about biology is almost anything goes.
And so there's sort of the what's called the central dogmobology, which is very simple. But then
you realize that a lot more complicated things can happen. So yeah, in the context of diseases,
there are diseases called Mendelian disorders, which you can think of them as just a single mutation
if you like, and one gene and a very simple relatively, relatively simple mechanism.
It's still hard to find those, still hard to figure out how to treat those diseases, but
but relatively simple in the sense it's just one mutation or one gene. But at the other end of
the spectrum is the much more complicated situation where you have many different mechanisms or many
different causes that combine together to result in the disorder or the disease. So a couple of
examples there would be diabetes or autism spectrum disorder. And so if you take autism spectrum
disorder as an example, the phenotype isn't even simply described. It's the whole range of phenotypes
really. And also when you look at the heritability of the disorder, you find out that it can't just be
pinned down to a single gene. It's many different genes. And even within those genes,
it's not just a mutation in a particular location. It's a wide range of different genetic variability,
different mutations at different places within that gene. And so the kinds of systems
are building can be used for both situations. So we can pin down the single mutation that's
causing a disease. And we can also use our systems to understand the complex combination of mutations
that that is involved in a disease. Okay. Let me take a maybe a little bit of a tangent.
There were a couple of technologies that you mentioned in your presentation that I wanted to hear
a little bit about. The first is one that I've heard quite a bit about recently, but haven't really
had a chance to dive into it. And that is CRISPR. It sounds like, well, tell us about CRISPR. What is
that and what are the implications of it? Yeah. So CRISPR, Cas9, it's what's commonly referred to
as a gene editing system. And it's a fairly straightforward idea. You basically program if you
like a template into the into the system. So it's a group of proteins effectively and other
molecules. And you essentially program a template into those. And then in living cells,
you insert these molecules of system. And then the template will find its match within the
within the DNA. And then once it's found a match, the system will then edit the DNA according to
your specification. So there's different ways that can happen. It could be as simple as the template
finds the match. And then it sticks there. Or it could be that the template finds a match. And then
and then the DNA is is actually edited. So it's changed. And that's so that's basically an
example of gene editing, which CRISPR Cas9 is one instance. And so how does that play into the
kinds of work that you're doing in your lab and at the company? Yeah. So so there are a couple
problems with with gene editing systems. And one of them is off target effects. And so and so the
template might not be perfectly specific, which means the the system might bind to the DNA in two
different places and edit the DNA in two different places. And so one of the places the correct one,
maybe there's a mutation you're trying to fix to address some sort of a disease. But what happens
is the system will also bind somewhere else to some other region of the DNA and then edit the DNA
there. And that could actually lead to a problem. And so that and that's called an off target effect.
The other one is that suppose the other problem with these kinds of systems as supposed as a
particular mutation that you're trying to correct trying to fix. It may be that the sequence surrounding
that mutation is just not good in terms of designing a template. So you can't actually design a
template in your CRISPR Cas9 system that will that will work properly in terms of finding that
mutation and then correcting the mutation. And so there's a couple different problems with these
gene editing systems. Another problem is delivery. Just has to do with the fact that the the machinery
you need to get inside of the cell involves several different molecules. And each of those molecules
has different properties in terms of in terms of whether or not it can successfully be
be delivered into the cell. And so there's several different issues with gene editing systems.
And there's a few different ways in which our technology that we develop that deep genome is
can be helpful. So one of them is because we we have these machine learning systems that can mimic
biological processes. One biological process is this template matching. So how well the template
matches the DNA that's actually a biochemical process that occurs within the cell. And so our
systems can identify off target effects and predict predict what might happen. The other example
is when you can't actually edit a particular mutation. So there's a mutation that a patient has
and you'd like to fix it. You can't actually edit that mutation because the you can't design
an appropriate template. So then what do you do? Well one idea is maybe you can edit some other
region of the DNA and somehow there will be some sort of compensatory effect. But in order to do
that you actually need a model. You need a system that can mimic how that DNA is going to be
processed because you can't you can't just fix the mutation. You need to introduce a mutation
somewhere else that is going to correct the problem introduced by the first mutation. So for
example the the first mutation the disease mutation may cause a problem with splicing. And what you'd
like to do is introduce a mutation somewhere else that will reverse that problem with splicing.
Right. And again that up and that again that that requires that you have some sort of a model
for how the cell is going to process that piece of DNA to control splicing and that's the kind
of model we build. Okay so you guys the work that you're doing can improve these genome editing
systems and at the same time the the work that you're doing around diseases the genome editing
system is one way that this work would eventually be deployed if you will put it into practice.
That's right at this point in time in deep genomics we have not developed any products to address
that particular therapeutic approach the gene editing system approach but it is a research
endeavor at this point. And so why have I heard so much about CRISPR you know maybe not this year
but towards the tail end of last year is it just that it was you know new or with their new research
results or is it better faster cheaper like what's the what was the big deal with research? Yeah yeah
it's leading to all sorts of breakthroughs in terms of research so the ability of genome biologists
to conduct different experiments different screens fabulous tool for research and in terms of medicine
there's a lot of promise there are some issues that need to be worked out but those are being
looked at and and I think it's very likely that it'll it'll prove to be a useful therapeutic technique
under some circumstances in any case. And so the other question I had in terms of just the
context in which you're you're doing your work is you mentioned and you mentioned this in early
in our conversation now is the increasing drive towards cheaper and cheaper sequencing and at
the deep learning summit you mentioned some new technology that you were expecting to drive the
cost down to as little as $20 within the next year or so what can you give me a kind of a quick
summary of the activity there? Yeah yeah sure there's just as I mentioned before this rapidly
growing diverse array of different biotechnologies that allow us to measure what's going on inside
of cells and genome sequencing is of course an important one it's the one that kind of gives us
the software if you like for the the basic source code of the of the person or the cells
and yeah there's technologies like the Oxford Nanopore technologies and other companies have
similar technologies which will which will allow cheaper sequence much cheaper sequencing genome
sequencing. But that's not the only technology that's helpful there's there's other kinds of
methods that allow you to for example look inside of DNA and see which genes are being transcribed
and measure how quickly they're being transcribed or the transcription rate of the gene techniques
that allow us to measure quantitatively how much protein is being produced and where the protein
is being located within the cell by the by the molecules that that shuttle proteins around.
So there's a lot of different kinds of biotechnologies being developed that allow us to essentially
look inside of cells and measure what's going on. Okay so lots of stuff going on you know maybe
let's yeah I've kind of been just trying to satisfy some curiosity because I've had about
the biological side of this but maybe let's kind of bring the conversation to machine learning
and deep learning and maybe let's start by talking through you know prior to the work that you
and others are doing to apply deep learning it sounds like machine learning a traditional machine
learning linear regression and things like that were applied to these types of problems well how
was talk about the kind of the standard to date approach. Sure yeah you know and I should emphasize
that deep genomics the word deep is is referring not just to deep learning but also these deep layers
of biological processes that get stacked upon one another that relates DNA to the phenotype
and really that's what's most crucial and as you might guess for each one of these modules the
first thing we do is try linear regression and it's the simplest technique quite often the best
technique but when you have a lot of data it does it does help to to look at more sophisticated
methods and so deep learning is a big part of what we do with deep genomics. Okay yeah so really
in in biology two approaches have been taken on supervised learning and supervised learning
and so way way back in the late 1990s people trained hidden markup models on on DNA sequences
and those hidden markup models were able to learn patterns that indicate the starts of genes
and the ends of genes and the locations of axons within the genes the axons are those parts of
the genes that actually they're like the print statements they're the parts of the genes that tell
you what the protein content is. Okay and so yeah that was back in the late 1990s researchers were
training hidden markup models to to model gene structure. And can you give us a 30,000 foot
view into what a hidden markup model is? Oh yeah sure so a hidden markup model is like a machine and
of course simulated inside of the computer that it has several different states and the the model
can switch back and forth between different states. And so for example it might be in the promoter
so so the structure of a gene is there's a promoter there's an axon and then there's an intron and
there's an axon and there's an intron and that just alternates until the end of the gene. There's
a couple other parts of the gene but for simplicity that's just that's just say those are the components
of the gene. So the hidden markup model would start off in the promoter state and then it was
switched to the axon state and then back to the intron state and then back to the back and forth
right. And so that's so that's the hidden markup model has a finite set of states and then there's
a model for how the or probability distribution that describes how the model switches between states.
So for example if you're in the promoter state then there's a high probability that you'll switch
to the axon state. And if you're in the axon state there's a high probability switch to the
intron state. If you're in the intron state there's a high probability switch to the axon state
and so on. Okay. And so it's a probabilistic model that just allows this machine to switch back
and forth between these different states. And then for each of the states part of the hidden
markup model is also a description of what the data will look like in that state. And so for
example when there's a transition from the axon state to the intron state the hidden markup model
also has a component or a probability distribution over what the DNA symbols will look like at that
transition point. And so if you run the hidden markup model just just simulate it which means
let it flip back and forth between these different states and for each state let it generate some
of the DNA sequence. If you run the hidden markup model you'll end up with a synthetic if you like
a synthetic gene sequence. Okay. And the way the hidden markup model is trained is to make the
synthetic gene sequence the output of the hidden markup model match the real data as best as possible.
And so in the late 1990s researchers trained these hidden markup models using actual examples of
DNA sequences and these models were able to automatically learn what the structure of a DNA
sequence looks like. So they were able to learn that there's a promoter, there's an axon and an
intron and that there's alternation between these axons and introns. So that's one example and
that's unsupervised learning and there are lots of other examples of how unsupervised learning has
been used in genome biology ranging from as I said modeling DNA to to actually just visualizations
of dimensionality reduction taking taking for example expression gene expression measurements
which would be say 22,000 gene expression measurements and compressing them down to a three
dimensional or a two dimensional representation for visualization. So the whole wide range of
different uses of unsupervised learning. And then the other the other process has been taken
to supervised learning where you're actually trying to solve a very specific task and probably the
one of the earliest uses of supervised learning in the context of genetic medicine as what was
called the genome wide association study. And in the genome wide association study what you do is
you measure for each patient a what's called the genotype which is just a measure of a variety of
mutations. So those mutations might be measured using a micro array. People might have heard of
the name SNEP array and so SNEP stands for single nucleotide polymorphism and that's just a
location within your DNA which could have a mutation in it. And so these SNEP arrays would measure
say 500,000 different possible mutations in your in your DNA. Another way you might measure
genotype is whole genome sequencing. So you'd literally read out the three billion letters or if you
have if you can do it for both your paternal and maternal DNA you'd have six six billion letters.
And so whatever you however you go measuring this genotype you can essentially think of it as a
vector. So it's going to be a sequence of of of letters ACG and T. So for 500 nucleotide array you
have 500 letters for a 500,000 nucleotide SNEP array you'd have 500,000 letters and then you can
imagine encoding that vector as a binary vector. So the letters ACG and T you can encode it using
one hot encoding. So A is 1, 0, 0, 0, C would be 0, 1, 0, 0. So there's different ways of doing that
or what's often done is what you do is you compare you compare the person's genetics to the reference
genome and so then you represent whether or not they have a mutation there compared to the
the reference genome something like that. So so basically though you represent the person's
genetics as a big long vector of zeros and ones and then what you do is use linear regression to
try to predict the phenotype. So you've got a whole bunch of patients with cancer and a whole
bunch of patients without cancer and and then you just try to predict the whether or not they have
cancer using linear regression. That's what a genome-wide association study is. So it's probably the
simplest and and one of the original uses of machine learning in in genomic medicine. Okay.
Okay. And what are the challenges associated with that approach?
Well if you think about it, what that approach is essentially assuming is that your phenotype is
a linear function of your genetics. Exactly. You mean it's not. And so you know we already talked
about these complex non-linear biological processes that relate your genetics to your phenotype
and we know from experience that this relationship is not linear. And so it's it's a it's an assumption
that has been used successfully to find mutations that are involved in disease but it doesn't
really accurately mimic the biological process. And so there are a few consequences of that
of that limitation and and one of them is that the is that the genome the genome-wide association
study is not guaranteed to find the causal mutation. Okay. And so it may actually find a mutation
that isn't rich, for example, that is common in patients with with cancer but it's not guaranteed
to be the mutation that actually caused the disease. And that's a big problem if you're trying to find
a drug or a therapy to treat the disease because that mutation will be the wrong one. Now they're
their techniques called fine mapping where you you look for nearby mutations and try to try to
couple up those mutations with the one you found in the genome-wide association study
and that that fine mapping approach has been used to to find the the causal mutation the one
that should be treated with the drug but is still limited and doesn't it doesn't solve all of the
problems. And I guess there are a few different other issues and one of them one of them is the
amount of data that's required. And so because of the way the genome-wide association study works
if you think about it there's three billion letters in the genome, three billion possible places
is the commutation. And for each of those locations you're going to try to use that location to
predict whether or not the person has cancer and then compare it against the experimental data.
And so there's really three billion different places you can look. And the problem is if you don't
have much data just by random chance one of those locations is going to match up with the phenotype
that is just even if the DNA is just noise even if you're just generating a whole bunch of noise
patterns you do that for your cases and your controls just by chance one of the locations one of
the positions in the genome you're going to get a good agreement between that mutation and whether
or not the person has a so-called cancer even though it's just noise it's meaningless. And so
and so so many get a lot of false positives. Yeah that's right a large number of false positives.
Okay. And the only way to get rid of that in a genome-wide association study is collect more
and more data and and that's the way you get rid of those false positives. But the problem there is
meaning you're not addressing you're trying to address it through kind of brute force statistics as
opposed to a better technique. That's right that's right you're just trying to get so much data
the overwhelmed the the fact that you have a model of what's really going on well. So the
approach we take which is this deep learning approach allows us to build models that take us from
the DNA to these intermediate molecular phenotypes or cell variables if you like variables representing
things like transcription and splicing. And those intermediate biological processes are really
what's crucial for disease. And so by modeling those explicitly we can take this big sequence of
three billion letters and user machine learning technique to map it down to a much smaller space
that represents what's really going on inside of the cell. And then we can relate that much smaller
and more compact representation with the phenotype whether the person has cancer or not.
Okay so strikes me then that you know basically what you guys are doing is feature engineering
for this particular type of system. Is that a fair way to think about it?
It's a it to some degree yes it's um I would say instead of feature engineering it's biological
engineering in the sense that we're choosing we're choosing because these features that we're
looking at are fairly complex and high level. Right. And and also it certainly doesn't do what you're
doing justice. But if you think about you've got all this raw data that doesn't really express or
kind of model the underlying phenomena and you guys are creating these meta models if you will
that does based on the raw data. It's kind of feature engineering-ish. Yeah well the thing is we
do have data for these intermediate variables. So for example. Got it. Okay. Yeah so one of the
one of the ways you might think of it as feature engineering is we actually model where a protein
will bind to the DNA right. So so protein binding to DNA is a very important biological process
in understanding how a mutation disrupts that is really important for understanding disease.
So you might sort of say well what we've done is we've designed features that describe how the
protein is binding to the DNA. But the way we actually account for that is we obtain training data
for where the protein bound. So we've got a data set of DNA sequences and whether or not the
protein bound to that DNA sequence and then we train a model for that. So if you like each of these
features is actually a machine learning system. And so that's where that's where it's quite
different from traditional feature engineering where you actually hand code the features. So we don't
we don't really hand code the features. We obtain training sets and then we train the models to
extract the features. Okay. Yeah but it is you know it is it does have that you know that sort of
confidential structure to it and we do there is this notion where each of these features is validated.
We do carefully validate each of these we call them a biomodule. So we validate each
biomodule to make sure that it's really counting for that particular biological mechanism.
The other way you can think about this actually is multi-path multi-task training. Okay so deep
neural networks one of the techniques that works really well is it's called multi-task training.
That's where you you train your system to solve multiple tasks at the same time.
So you might have you might have a very simple example might be the input as an image and your
training it to to classify animals and at the same time you're also training it to classify
some other some other kind of object and the ideas that is that the what it learns about those two
for example faces. So maybe you're trying to classify faces and you're also trying to classify
animals and there's some components to those two different problems that are that are of shared
value. For example the detection of body parts or the detection of eyes something like that and
so by training the system to solve these two different tasks at once it can learn sub components
it can learn intermediate variables if you like that are useful for for the two different tasks.
And so you can also think about what we're doing that way we have this these very deep
multi-layer architectures and they're trained to predict phenotype but they're also trained to
predict protein binding they're also trained to predict splicing they're also trained to predict
transcription and these different processes that are going on within the cell. And by training
them jointly to solve these different tasks they get better at solving any one of them and in
particular they can get better at detecting disease and also predicting the effects of therapies.
Are the different modules? How do I ask this question? If you think about this as a if the model
that we're talking about here is a deep neural network are the different modules expressed
explicitly as layers meaning like the the network architecture or does the training process
kind of cause the modules to be expressed in the layers does that question make sense?
Yeah no it does it's a good question you're sort of asking what's the mapping between the
layers of biology and the layers of our machine learning systems.
Yes yeah so so we have two separate maps if you like two separate networks if you like one network
represents the biological processes and for each if you like for each node and each arrow in
that network we train a deep learning system and so and so if the biological network is 10 layers
deep and each layer is modeled by a deep learning system with 10 layers and there's an overall
depth of 100 so that's the one way you can think about it and so because the system is trained
in a modular fashion we can focus in on each component to the biological network and then train
a deep neural network to model that component and sometimes sometimes use a shell
in that work sometimes linear regression is sufficient so the complexity of each of those biological
modules in terms of machine learning is is carefully selected using the traditional machine learning
types of techniques cross validation and perturbation analysis and methods like that and
does it ever make sense to rather than training these models as a stacked neural network to think
of it more as like an ensemble approach where your modules are more separate and you're training
them independently and then you've got some discriminator network you know it's it's sort of the
overall ideas that each of these modules does have a place within the biological system but
having said that we do we do sometimes run into situations where there are fairly different ways
we can conceive of building each module just like in traditional machine learning you might have
different types of you might use a random forest in one case and a neural network and then
you'd like to combine the outputs and see what happens so that sort of thing happens for for
given biological module and might conceive of different ways we can build the machine learning
system to come up with that process and then combine the outputs the the other the other interesting
aspect of this is the sort of end-to-end training that once we put the the system together
then we can fine tune it to make the overall system perform better so even though we
module is initially built using a machine learning system could be a deep neural network
independently of the other modules once we put them together we can adjust all the modules so
they work better together okay in your presentation you had a slide where you were talking about
kind of applying you know what you were doing and applying AI to these types of problems and you
spent quite a bit of time talking about inductive learning versus transductive learning and
how that seems to be you know something that's overlooked in practice can you recap that for us
yeah there's a big focus right now just on collecting data and I think not enough attention is
being paid to analyzing the data when it comes to genomic medicine and so there are a lot of
private and public efforts to just collect data you know the big genome projects the the 100,000
genome project the way the way success is measured is in the number of genomes as opposed to the
information that's being extracted right and and so I think more attention needs to be paid on
analyzing the data now if you look at the genome wide association study where it's this idea of
correlating mutations with the output that's what I would call is more more of a transductive
reasoning approach or basically you're just comparing you're comparing your mutation to the
training data and then trying to make a sort of a winner take all or or taking a voting approach
trying to make a prediction for that mutation now I that's one type of machine learning a different
kind is inductive learning and inductive learning what you do is you you take your training data
and then you build a machine learning model of what's going on and then you apply that model
to the test cases so you apply the model in the future and the advantage of inductive learning
is is generalization so for inductive learning you can learn if in sort of a in one way to view
it is you're learning the rules of what relates the input to the output you're you're learning more
general patterns and this allows you to to take that learning and apply it to completely new
circumstances and so for example if there's a completely new mutation that's never been seen
before if you've used inductive learning you might hope that your machine learning model can still
figure out what's going to happen with that new mutation in contrast to transductive learning
approach if there's a new mutation that's never been seen before that transductive learning
approach can't do anything and so that's true for genome-wide association studies for example if
there's a mutation in a patient that it doesn't exist in the training set then the genome-wide
association study can say nothing about that mutation whereas with the inductive machine learning
approach you might hope that it could take the system could take a look at that mutation and say
ah this mutation is going to cause something to go wrong with splicing and that's going to lead
to the disease and actually that's what we find with our systems so the systems we trained at
deep genomics were able to analyze mutations that have never been seen before that don't exist in
any database okay okay oh that's huge yeah really fighting actually so maybe let's let's dig
into the the data aspect of this a bit in order to do what you're doing I'm imagining you're
benefited you're benefiting pretty significantly by your new data sets coming online all the time like
how is that landscape change and what are some of the types of data that you're looking at
yeah it's one of the most exciting areas right now is biotechnology just the the number of
different kinds of data sets is growing very rapidly in the sizes of those data sets so 10 years
ago we were looking at small data sets consisting of a few thousand examples and now deep genomics
we look at data sets with billions of examples so we're so the amount of data has just grown
very very rapidly and is going to continue to grow there are publicly available data sets so these
are publicly funded research efforts from university labs and so there's plenty of publicly available
data and then there's also different kinds of proprietary data data coming from patient populations
or data that we generate within deep genomics to to study particular aspects of biochemistry and
sort of fine tuner models if you like and then in terms of the in terms of what the data is telling
us as I mentioned before it's these data sets are measuring all sorts of things that are going
on within cells so it's giving us more and more accurate resolution higher and higher resolution
in terms of pinpointing different processes going on within cells and relationships between
those processes so I really do think that in in five to ten years because of this massive growth
in data if we combine machine learning techniques with all these data sets we're going to be able to
produce models of these cellular processes that are quite accurate and reliable.
I took a look at one of your papers the paper that goes into the work that you're doing
about around deep bind and one of the points that you brought up there was the difficulty of
extending results that are seen with in vitro analyses to in vivo analyses and I'm assuming that
or that's that's tied to this issue of the or to what degree is this tied to this issue of the
data sources that you're getting being primarily in vitro and maybe could talk through some of the
issues there yeah yeah so yeah there's different kinds of data and in vitro is data that is
measured under it's in the test tube and it's so this data that's measured in the lab under very
controlled conditions in vivo with the other end of the spectrum is within the living organism
and so it's the idea is that data would reflect more accurately what's actually going to happen
and say a patient and and historically there's been a big disconnect between these data sets but
as as we sort of fill in if you like if we fill in the map of all the different kinds of things
we can measure within the cell and we also fill in the different kinds of conditions under which
we can measure that data and so in vitro in vivo but also different kinds of organisms different
kinds of cell types different tissue types and as we as we measure more and more data for these
different dimensions if you like of different conditions in which we measure the data we get a
more accurate understanding of how all those different kinds of data relate to one another
and so we're also building better and better models that are accounting for confounding factors
or experimental bias or example of a confounding factor might be what oh yeah so there's a different
a wide range of different kinds of confounding factors they'll lump them all together into one group
so experimental bias is a big one and experimental bias just means how your experiment was conducted
you know what what were the very what were some of the technical details that were used to obtain
the data and those things can have a big impact on the on the data itself actually one of the first
projects I worked on in genobiology we we used a particular kind of unsupervised learning method
to analyze the data and we thought we discovered something really interesting and it turned out what
we discovered is who did the experiment on which day and so that's an experimental type of confounding
factor and then there are confounding factors that are biological and so for example if you're
looking at let's take the genome wide association study approach a very very simple machine learning
technique and so if you're looking at a bunch of patients that have a disease and a bunch of
patients that don't have the disease the the problem is that those patients are not really
independent and identically drawn from some simple distribution they're actually related to one
another in some way and so so maybe half of your cases are derived from a single ancestor that
lived 100,000 years ago or something like that so that kind of structure in the population
is going to lead to dependencies of course between these measurements and those dependencies can
lead you astray it's a little bit like you know suppose you're one problem that all machine
learning researchers are familiar with is suppose you have a training data of a hundred examples
and you take one of your examples and just replicate it a million times right you do not have a
training set a bona fide training set of a million a 99 examples because you've biased your
yeah yeah just copied one of the examples a whole bunch of times so that's an example of a
confounding factor that really does arise in human genetics and is really important to to avoid
and is the idea that your approach or deep learning in general is has a higher level of
is more impervious to these types of confounding factors yes yeah that's that's right and so because
we're building a system to model these different biological components we can factor out certain
confounding factors and so as I mentioned for example our system can detect mutations that have
never been seen before which obviously means that it's not sensitive to the to to the structure of
of the human population in terms of the genetics okay but at the same time I you know I should
add that of course our systems are being trained using data that has biases because basically all
the biology is highly biased so and so it's not like the problem is completely gone but yes they're
more impervious yeah awesome awesome well this is this has been fantastic maybe any closing thoughts
on things that you guys are working on are you know what you're excited about yeah I guess the
the the challenges for us are so we have our systems are working well and we're making
good progress in terms of addressing interesting machine learning problems as well as having an
impact in medicine but I think one one area that's interesting to talk about is is the kinds of
problems that we're facing in terms of our machine learning techniques and how those relate to
what generally the field is looking at and and and being challenged with and one of those is
is building systems that can explain themselves and so this has been a people have been talking
about this quite a bit recently is how do you build or train a neural network say or deep learning
system that in such a way that it can actually explain what's going on so it can explain why it makes
a decision right right and you know so that's that's really important for for earning trust and so
if we have a if we have a machine learning system that predicts that you you know woman should
has a has a disease causing mutation in the context of say breast cancer in the system recommends
a double mastectomy then you really do want the system to be reliable and trustworthy and be able
to explain you know why why it made that decision why it made that provided that advice
and so I think that's a really interesting area for machine learning you know I don't have the
answer to to how we how we do that but people are working on that area and a lot more work needs to
be done well so on that point I did do an interview with Carlos Gastrin not too long ago and
our listeners might remember that one if you're interested in this issue of explainability check
out that interview but traditionally if you can use traditionally when talking about deep neural
networks I guess people you know when we're looking at machine learning models people look to
other types of models and not deep neural networks because of this explainability challenge
that is you know particularly acute with neural networks like do you see where do you see
that going do you see light at the end of the tunnel yeah that's a good point and actually think
that that belief is completely wrong-minded so so here's the traditional argument for why you
should look at simple techniques like linear regression or random forests or something like that
so the the argument goes like this to figure out an explanation for why machine learning system
made its prediction what you do is you should look inside of the machine learning system you should
look at the parameters okay so linear regression is really simple because for each input there's
only one parameter connecting it to the output and so you can just look at that parameter and if it's
positive it means that input has a positive impact on the output and if it's negative it has a
negative impact and so that's that's the justification for looking at simple machine learning systems
right okay now this is why I think that's completely wrong-minded if you if you turn to your
friend and ask them why did you make the decision you just made you don't crack open their skull
and look at their synapses to figure out the explanation that's not what you do and yet that's
what the traditional argument is for why you should use simple machine learning systems you're
going to look at the parameters and so therefore you need a simple system no you don't do that
so what you do is you ask your friend to explain themselves well why did you make the decision
you made so I think the future of machine learning is all about using complex deep neural nets
but training them in such a way that they actually produce an explanation at the output
so we don't crack them open and look at the parameters we we actually train the system so that the
output of the neural network is an explanation as well as a decision does that make sense
it does make sense so I'm thinking of the picture I have in my head is yeah we talked
earlier about neural networks that are trained to produce multiple outputs and so in this case
one of the outputs is the explanation and the other output is the you know the thing we're asking
it to make a decision you got it you got it that's exactly right so you can think of this as a
multitask training problem yeah where one output is the decision the other output is the explanation
and so really I think that's that's the future of machine learning in terms of explanations
and there are obviously some really challenging technical issues for for how we get that to work
and it's not really working well yet but I think that is really where things where things will go
in that regard and the other I guess the other observation I can make is is what's what I find really
exciting about this area that deep genomics is working on is the kind of artificial intelligence we
need and so if you look at some recent big successes like deep mines alphago or Google
Google's results or Facebook so if you look at some of the really exciting results that have come
out of those labs therefore things like games which humans invented or image recognition which
humans have evolved to be good at or speech recognition which humans invented or evolved and
or evolved now these are all tests that humans are good at whereas what I think is really
exciting about genomic medicine is that the AI systems we build need to go beyond what humans
are capable of so no human is it ever going to be capable of understanding genome or how to
cure genetic disease or no group of humans will right it's so complex and so common tutorial
and so really what we need is superhuman AI and so now it's making me think of this is making me
think of a sci-fi book that I like by Octavia Butler I forget the name of the book but basically
there are these race of aliens that come down to a north that's been you know kind of ravaged by
you know disease and this gift that these this alien race has is to effectively repair genetic disorder
what that has to do with AI who knows but you're also I mean there's some interesting things kind
of switching the subject here there's also some interesting things happening up in Toronto right
yeah yeah so you're asking what things happening in Toronto so I can't talk a lot about it right
now but in the next couple weeks there's going to be an announcement for a new type of artificial
intelligence institute in Toronto we have over 170 million dollars of funding for it and the
ideas to build or rebuild the AI research capacity in Toronto and and ensure that we can use that
capacity to to foster innovation in the startup community and another another bigger businesses
in the in the area so yeah that should be announced in a few weeks and it should be really big
big news for Toronto and I think big news for the AI community more broadly that's fantastic
we'll definitely keep our eyes open for that and so before we go where can people learn more about
what you're up to and keep tabs on you yeah so you can go to www.deepgenomics.com also just google
me and we have various papers posted online there where you can for example a tutorial paper that
describes the approach and and how you can use machine learning not just deep learning but just
different kinds of machine learning techniques to to approach problems in genomic medicine also
well Brendan thanks so much this was an amazing conversation I really appreciate you taking the
time out you best sound it was a pleasure all right everyone that's our show for today a huge
thanks to all you listeners out there I appreciate all of the notes and comments that you share be
the mailing list sign up form the show notes pages be a Twitter iTunes and all the other channels
that you use to share your love for the podcast and don't forget to visit our brand new Facebook
page at facebook.com slash swimmol ai and give us a like and register for the strata
Hadoop giveaway while you're there the notes for this show will be up at twimmol ai.com slash talk
slash 12 and there you'll find links to all of the resources mentioned in the show thanks so
much for listening and catch you next time
