1
00:00:00,000 --> 00:00:05,780
All right, everyone. Welcome to another episode of the Twilmo AI podcast. I am your host Sam

2
00:00:05,780 --> 00:00:11,480
Charrington. And today I'm joined by Tony Jabara. Tony is a vice president of engineering

3
00:00:11,480 --> 00:00:16,680
and head of machine learning at Spotify. Before we get going, be sure to take a moment

4
00:00:16,680 --> 00:00:22,200
to hit that subscribe button wherever you're listening to today's show. Hopefully Spotify.

5
00:00:22,200 --> 00:00:26,800
Tony, this conversation is a long time in the works. We're super excited to have you

6
00:00:26,800 --> 00:00:32,520
welcome to the podcast. Thank you, Sam. Great to be with you and with all the folks joining

7
00:00:32,520 --> 00:00:38,320
us here today. Very excited. I'm super excited. I mentioned to Tony before we got started

8
00:00:38,320 --> 00:00:44,920
recording that I've got this memory of I think 2017 or something like that at the rework

9
00:00:44,920 --> 00:00:49,840
deep learning conference in San Francisco. I think it's the hiate that's up on the hill

10
00:00:49,840 --> 00:00:56,680
just by Chinatown. And I think you did a presentation while you were in Netflix about maybe machine

11
00:00:56,680 --> 00:01:02,800
learning for for breakfast or something. And I kind of harangued you. You were on your

12
00:01:02,800 --> 00:01:08,600
way to a meeting out of the hotel and been trying to get a conversation going for a while.

13
00:01:08,600 --> 00:01:13,720
So super excited to finally connect. Yeah. Yeah. Great. We're doing it finally. You know,

14
00:01:13,720 --> 00:01:19,560
thanks for for keeping up and you know, hopefully there's more folks in the audience today

15
00:01:19,560 --> 00:01:24,640
than in 2017 as your podcast seems to be taking off. So that's great. Yeah. Absolutely.

16
00:01:24,640 --> 00:01:27,880
Absolutely. Absolutely. There's no better time than the present. This episode is going to be

17
00:01:27,880 --> 00:01:33,920
part of our NURP series and you participated in a couple of workshops and NURPs mostly

18
00:01:33,920 --> 00:01:38,720
around the topic of reinforcement learning. And that'll be one of the main things we focus

19
00:01:38,720 --> 00:01:46,360
on in our conversation. But before we dig into RL in particular and or at least the the

20
00:01:46,360 --> 00:01:51,360
workshop participation in NURPs in particular, I'd love to have you share a little bit about

21
00:01:51,360 --> 00:01:55,880
your background and how you came to the field of machine learning. Sure. So I was, you

22
00:01:55,880 --> 00:02:00,360
know, excited by what computers could do a long time ago before machine learning really

23
00:02:00,360 --> 00:02:04,880
was a thing. I was looking at computer vision, looking at a bunch of computer science in

24
00:02:04,880 --> 00:02:09,640
general and saying how do we automate what people seem to be doing so well and how do they

25
00:02:09,640 --> 00:02:15,600
recognize faces and images. And you can write a bunch of rules, thinking here's how I would

26
00:02:15,600 --> 00:02:20,840
do it as a person and those were quite brittle. And so, you know, when I first started

27
00:02:20,840 --> 00:02:26,800
in the 90s, it was write rules for things to make computers intelligent. And then that

28
00:02:26,800 --> 00:02:31,560
didn't really seem to scale. It turns out it's much better to show the data and have an

29
00:02:31,560 --> 00:02:37,880
algorithm that learns all by itself what to do from examples of real data, real examples

30
00:02:37,880 --> 00:02:42,880
of intelligent behavior. And so that was, I think, how I really started and I was an academic

31
00:02:42,880 --> 00:02:47,680
because there wasn't much going on in industry in the 90s in the early 2000s. But then of

32
00:02:47,680 --> 00:02:52,400
course, industries where a lot of the big machine learning is happening these days. And

33
00:02:52,400 --> 00:02:57,320
so I went to Netflix and then at Spotify now where machine learning really is driving

34
00:02:57,320 --> 00:03:02,280
a lot of our advances and as a huge part of our business and our personalization offer.

35
00:03:02,280 --> 00:03:06,640
Maybe talk a little bit about your role as head of ML at Spotify. What are some of the

36
00:03:06,640 --> 00:03:10,080
aspects of machine learning that kind of fall under your purview there?

37
00:03:10,080 --> 00:03:16,000
So I'm focused on how machine learning can help across all Spotify. But also in particular,

38
00:03:16,000 --> 00:03:22,560
I lead the teams that build a homepage, the search engine, the programming platform, the

39
00:03:22,560 --> 00:03:27,000
user evaluation and understanding models, the content understanding from music understanding

40
00:03:27,000 --> 00:03:33,400
to talk understanding. And as well, some of the technologies that help us with playlists.

41
00:03:33,400 --> 00:03:40,160
And then I also advise all the business units about how they can incorporate best practices

42
00:03:40,160 --> 00:03:47,360
of machine learning. So our ads business unit, our let's say messaging and user membership

43
00:03:47,360 --> 00:03:52,360
growth business units. All of them really have to work with the same machine learning

44
00:03:52,360 --> 00:03:57,280
infrastructure and best practices. And so I'm also tapped into those conversations, but

45
00:03:57,280 --> 00:04:01,840
you should that we're pushing for the right investments for the long term across all

46
00:04:01,840 --> 00:04:03,960
aspects of Spotify's business.

47
00:04:03,960 --> 00:04:09,160
Any thoughts on how the company's use of ML has evolved since you joined?

48
00:04:09,160 --> 00:04:16,320
Yeah, so ML has been an important tool in our tool chest, but now we're increasingly

49
00:04:16,320 --> 00:04:24,040
relying on it as we have it making so many more decisions more often in every part of

50
00:04:24,040 --> 00:04:30,640
our personalization and our, let's see, connections between the user base and the content.

51
00:04:30,640 --> 00:04:35,440
And what we've seen is huge scale growth of both sides. So we've not only added music

52
00:04:35,440 --> 00:04:42,360
to our catalog, but we've also started looking at talk and audio books and podcasts and live

53
00:04:42,360 --> 00:04:47,280
and video as well. And our user base is grown across countries and cohorts and different

54
00:04:47,280 --> 00:04:54,080
plans. And so the complexity on the, on the people that want to listen and the creators

55
00:04:54,080 --> 00:04:57,560
and the content that should be listened to, that's growing on both sides. And so machine

56
00:04:57,560 --> 00:05:03,160
learning's actually increasingly important to figuring out what is the right valuable connections

57
00:05:03,160 --> 00:05:08,360
to make between the listeners and, and the artist's creators and content. And so that's

58
00:05:08,360 --> 00:05:14,640
been growing at a great clip. The other really interesting thing is Spotify kind of started

59
00:05:14,640 --> 00:05:19,920
off as a curation engine. You would come to Spotify and organize your own playlist and

60
00:05:19,920 --> 00:05:25,200
curate what you thought would be great audio experience. But over the years, we've evolved

61
00:05:25,200 --> 00:05:31,520
from a curation first experience to a recommendation and machine learning experience. So we're still

62
00:05:31,520 --> 00:05:35,400
having people curate their playlists and build them and share them with friends and, and

63
00:05:35,400 --> 00:05:41,200
in the world. But we're also creating playlists for you automatically, creating a homepage

64
00:05:41,200 --> 00:05:46,120
for you that's tailored to what you would be interested in driving discovery, promotions

65
00:05:46,120 --> 00:05:50,400
and search that are tailored for you. And so we're moving very much from a curation

66
00:05:50,400 --> 00:05:56,200
first product to a recommendation first product. And then even beyond that, a recommendation

67
00:05:56,200 --> 00:06:00,680
first product that also explains to you why you should care about this recommendation.

68
00:06:00,680 --> 00:06:04,880
So giving meaning to the recommendation is another aspect, understanding why this makes

69
00:06:04,880 --> 00:06:09,640
sense and how to explain to you and convince you to give it a try. So that's that's been

70
00:06:09,640 --> 00:06:14,520
the journey. So more and more machine learning throughout every stage of that journey.

71
00:06:14,520 --> 00:06:20,040
Can you talk a little bit about the kind of the business value that those recommendations

72
00:06:20,040 --> 00:06:28,120
provide for Spotify? I'm thinking back to a paper from Netflix actually may have been

73
00:06:28,120 --> 00:06:34,760
while you were there. I forget the names of the authors, but one was the head of product

74
00:06:34,760 --> 00:06:44,480
and there was this note in the paper that even back, you know, whatever that was 2015-16,

75
00:06:44,480 --> 00:06:49,200
they attributed a billion dollars of value to the recommendation systems that were built

76
00:06:49,200 --> 00:06:56,440
at Netflix. Can you talk about the, you know, how you, how the business thinks about the

77
00:06:56,440 --> 00:07:01,120
value of these kinds of recommendations? And I'll maybe contextualize this a little bit

78
00:07:01,120 --> 00:07:08,320
with my personal experience as a Spotify user. I'm not, you know, I'm not a big music person

79
00:07:08,320 --> 00:07:14,240
necessarily. I've got some things that I listen to and, you know, that I like and I've

80
00:07:14,240 --> 00:07:20,160
got enlist in Spotify. And I usually just go and, you know, play one of those and or play

81
00:07:20,160 --> 00:07:26,040
one of, you know, the same couple of playlists. Like, I don't necessarily, I'm not your best

82
00:07:26,040 --> 00:07:31,480
consumer of recommendations necessarily. And so I'm wondering kind of how broadly how

83
00:07:31,480 --> 00:07:37,920
they play out across the business. Sure. So we've been investing in a series of AB tested

84
00:07:37,920 --> 00:07:43,960
wins. Each, each test improves retention, reduces churn, increases engagement. And we

85
00:07:43,960 --> 00:07:47,800
can take all those tests or layered on more and more machine learning and say, well, what's

86
00:07:47,800 --> 00:07:52,360
the some value of all of those? The problem is that's happening while users are coming

87
00:07:52,360 --> 00:07:56,760
through the platform and being acquired even not just retained but acquired by machine

88
00:07:56,760 --> 00:08:03,000
learning. And it turns out if you go and survey people, users and, you know, premium subscribers,

89
00:08:03,000 --> 00:08:07,000
which is the majority of how we generate our revenue. And you ask them what drives you

90
00:08:07,000 --> 00:08:12,400
to Spotify? What makes you sign up for Spotify versus consume and find your music elsewhere

91
00:08:12,400 --> 00:08:18,200
or through some other channel or medium? 81% say it's because of the personalization.

92
00:08:18,200 --> 00:08:23,000
And that's really heavily driven by machine learning. So you can think of it as the majority

93
00:08:23,000 --> 00:08:30,600
of users are coming to Spotify's thing at Spotify because of the personalization. And

94
00:08:30,600 --> 00:08:34,440
of course, the content is crucial. You can't personalize when you don't have content.

95
00:08:34,440 --> 00:08:39,840
There's nothing to personalize. But the bigger the content catalog becomes, the more

96
00:08:39,840 --> 00:08:44,600
of the personalization matters. So we're now at, you know, over 100 million tracks, over

97
00:08:44,600 --> 00:08:50,360
100 million podcast episodes. We've added hundreds of thousands of books and we keep going.

98
00:08:50,360 --> 00:08:54,760
And so that means personalization constantly gets more and more important. And the machine

99
00:08:54,760 --> 00:08:59,880
learning algorithm is to find the right thing for you, especially when for you means one

100
00:08:59,880 --> 00:09:06,440
out of half a billion people almost. That value becomes much bigger as the user population

101
00:09:06,440 --> 00:09:13,440
grows as the content catalog grows. And as we improve and roll out more intelligent algorithms

102
00:09:13,440 --> 00:09:15,440
with better wins that are av tested.

103
00:09:15,440 --> 00:09:21,440
I guess that brings us to your presentation at the offline RL workshop, which talks about

104
00:09:21,440 --> 00:09:29,120
some of the ways that you apply offline RL for personalization. But before we jump into

105
00:09:29,120 --> 00:09:38,400
that, talk broadly about the techniques you use. Are you kind of mostly using RL for personalization

106
00:09:38,400 --> 00:09:44,160
or is that one of many tools that you use depending on the specific scenario?

107
00:09:44,160 --> 00:09:48,960
I'd say it's one of many tools. So we leverage a lot of machine learning tools. Think of the

108
00:09:48,960 --> 00:09:53,680
machine learning treasure chest as having, you know, many, many buckets. RL is one of those

109
00:09:53,680 --> 00:09:57,760
buckets. You've got deep learning in another bucket. You've got causal techniques, it

110
00:09:57,760 --> 00:10:03,120
causes inference in another one, probabilistic models. For one of the things we've realized

111
00:10:03,120 --> 00:10:09,480
is we kind of iterated our way and realized that we need more and more RL. And that's

112
00:10:09,480 --> 00:10:15,280
because we started doing heavily, let's say multi-arm bandits at the beginning where you

113
00:10:15,280 --> 00:10:20,520
basically do things like try things out, do a little exploration and then start taking

114
00:10:20,520 --> 00:10:25,440
action that seemed to get good responses from the user. And we did that in all sorts of

115
00:10:25,440 --> 00:10:31,280
places in our systems from our, you know, surfaces like our homepage, to our banners and

116
00:10:31,280 --> 00:10:36,880
our promotions. We use these techniques from baby RL, which is multi-arm bandits. This

117
00:10:36,880 --> 00:10:43,160
is like if you have a forgetful RL agent that doesn't know what state it's in, then basically

118
00:10:43,160 --> 00:10:49,520
it's a bandit. But that's maybe chapter one of the RL textbook. There's many more sophistications

119
00:10:49,520 --> 00:10:55,480
after what you start to say. Well, there's a state, you know, people change as they consume

120
00:10:55,480 --> 00:11:00,920
and discover and help new habits. And so you can't just think of it as a, you know,

121
00:11:00,920 --> 00:11:05,760
multi-arm bandit in the casino, which kind of doesn't really remember what happened before.

122
00:11:05,760 --> 00:11:10,520
You have to really understand users are impacted by your recommendations or what they consume,

123
00:11:10,520 --> 00:11:14,960
that changes who they are. And then they come back the next day and maybe the decision

124
00:11:14,960 --> 00:11:20,520
then is actually a little bit different. So then you understand that you're really not transacting

125
00:11:20,520 --> 00:11:24,600
in the moment only with the user. You're building a journey that's going to last many months

126
00:11:24,600 --> 00:11:29,560
with them, especially if you're a subscription service like we are. Users are with us for

127
00:11:29,560 --> 00:11:35,400
many months, many years. We're now thinking of our business more about building a journey

128
00:11:35,400 --> 00:11:40,600
rather than getting you to just click on something with a bandit and building a journey is

129
00:11:40,600 --> 00:11:45,560
much more of an RL style problem. If you look at what you're actually trying to do with

130
00:11:45,560 --> 00:11:50,840
RL, you're trying to play games and get to the end of a maze and so on or get a robot

131
00:11:50,840 --> 00:11:56,800
to do some useful tasks. Well, it turns out RL is also about getting a user to go on a

132
00:11:56,800 --> 00:12:02,880
journey and discover new things and enrich the way they use Spotify and their day-to-day

133
00:12:02,880 --> 00:12:09,960
life. And one of the themes that occurs in your presentation in the workshop is this

134
00:12:09,960 --> 00:12:19,960
idea of kind of transitioning from a single-step reward to optimizing over the lifetime value

135
00:12:19,960 --> 00:12:25,560
of a subscriber. Can you talk a little bit more about that and the complexities that it

136
00:12:25,560 --> 00:12:30,080
presents for you? Yeah, absolutely. So we started by just optimizing

137
00:12:30,080 --> 00:12:36,040
for the next click. And if you looked at our old-style homepage, old-style search pages,

138
00:12:36,040 --> 00:12:41,760
old-style playlists, it really was about just getting that next consumption, that next

139
00:12:41,760 --> 00:12:46,720
track, that next play session, which is really kind of click through a maximization. You

140
00:12:46,720 --> 00:12:52,040
want to get a good click to the rate. Which makes sense if you're serving ads, but I guess

141
00:12:52,040 --> 00:12:56,280
it's better than the customer closing the app. You're right. So you don't want to completely

142
00:12:56,280 --> 00:13:00,520
give up on instantaneous rewards. You need to do something when the customer opens the

143
00:13:00,520 --> 00:13:05,400
app. However, you can't just always think about what's the easy thing that gets the immediate

144
00:13:05,400 --> 00:13:10,080
click. Because for us, especially, the easiest thing to recommend is just listen to what

145
00:13:10,080 --> 00:13:15,800
you listen to yesterday. Here's the exact same playlist. Here's the exact same tracks.

146
00:13:15,800 --> 00:13:19,240
We know it's not going to be a trust buster. It worked yesterday, so it's not a pretty

147
00:13:19,240 --> 00:13:22,920
good click through rate. The problem is if you keep doing that over and over again, and

148
00:13:22,920 --> 00:13:27,560
you don't worry about the user tomorrow and six months from now and how happy are they

149
00:13:27,560 --> 00:13:35,640
this Spotify? You realize you quickly wear out this user because you haven't layered the

150
00:13:35,640 --> 00:13:40,840
familiar recommendations with discoveries and long-term growth. So you got to go for a

151
00:13:40,840 --> 00:13:46,720
short-term instantaneous reward, but also set up for long-term success. So the user keeps

152
00:13:46,720 --> 00:13:55,160
coming back and in the way is enriched and feels more long-term fulfillment at Spotify.

153
00:13:55,160 --> 00:13:59,240
Because they're building new discoveries of content, new habits, maybe now they're

154
00:13:59,240 --> 00:14:04,440
listening to a weekly podcast on Mondays that is a new habit for them. It's not just listening

155
00:14:04,440 --> 00:14:09,560
to dance music on Saturday, which might be why you started your subscription with Spotify

156
00:14:09,560 --> 00:14:15,440
to begin with, but we want to add new habits. Monday afternoon, listen to this podcast,

157
00:14:15,440 --> 00:14:22,640
maybe Thursday evening's meditation podcast on Sundays, maybe an audio book. So those

158
00:14:22,640 --> 00:14:27,680
habits really keep you coming back for the long term, and maybe they have lower clicks

159
00:14:27,680 --> 00:14:32,880
at a rate immediately, but once you do click, you keep coming back afterwards. So we've unlocked

160
00:14:32,880 --> 00:14:38,640
not just the next reward, but the sum of cumulative rewards into the future, which is what really

161
00:14:38,640 --> 00:14:44,720
RL is all about. Yeah, you've got this interesting kind of pictorial illustration of this where

162
00:14:44,720 --> 00:14:50,480
you talk about machine learning, kind of moving you in circles around your current state versus

163
00:14:50,480 --> 00:14:54,800
RL, which does a better job of getting you to that higher value state.

164
00:14:54,800 --> 00:14:59,200
Yeah, so I like that picture because it really captures what we actually did see in our data,

165
00:14:59,200 --> 00:15:03,600
where users would just go around in these circles of, you know, they play yesterday's thing,

166
00:15:03,600 --> 00:15:08,080
and then they play their typical kind of routine stuff, and they just circle around through

167
00:15:08,080 --> 00:15:13,920
their five, six frequent playlists. And you know, that's great, but if that's what you're doing

168
00:15:13,920 --> 00:15:18,320
eventually, you're going to get worn out by that, and you're not really, you know, getting

169
00:15:18,320 --> 00:15:23,280
more value out of Spotify, you kind of stuck in that rabbit hole going on the circles. And that's

170
00:15:23,280 --> 00:15:30,320
happening a lot with a lot of, let's say, recommendation engines. So how do you make you deliberately

171
00:15:30,320 --> 00:15:35,280
break out of that rabbit hole and go to a higher, you know, altitude location where you are

172
00:15:36,480 --> 00:15:42,960
expecting many more future rewards? Because you're now open to content categories, you didn't think

173
00:15:42,960 --> 00:15:48,400
existed before, you, you know, you've discovered jazz, and now you're going to open up that new source

174
00:15:48,400 --> 00:15:53,920
of reward, which is amazing jazz discoveries for many, many months to come. So that's how we think

175
00:15:53,920 --> 00:15:59,120
of altitude. You're at a higher point where, you know, much more about the audio landscape,

176
00:15:59,120 --> 00:16:04,080
and you can consume for the future much more easily. I'm not sure I thought of myself as stuck as

177
00:16:04,080 --> 00:16:09,600
a Spotify user before this conversation, but now I'm going to be super self conscious about it,

178
00:16:09,600 --> 00:16:15,920
and if not the recommendation systems changing my behavior, maybe this conversation.

179
00:16:16,880 --> 00:16:23,520
We'll take it. We'll take anyway we can. When you start thinking about, you know, this broad

180
00:16:23,520 --> 00:16:31,280
or longer term journey with the user and trying to make decisions around that, do you run into

181
00:16:31,280 --> 00:16:38,480
challenges with attribution? Yeah, absolutely. Attribution is a big problem because it's very easy

182
00:16:38,480 --> 00:16:44,720
to attribute an instantaneous reward to an action because the reward shows up, you know, a few

183
00:16:44,720 --> 00:16:49,520
hundred milliseconds later. You presented the thing I clicked. Exactly. So then it's pretty clear

184
00:16:49,520 --> 00:16:55,520
that, you know, our recommendation that got a click, you know, a hundred milliseconds later was a

185
00:16:55,520 --> 00:17:00,480
good one or a bad one. But when we're talking about, you know, the user retaining longer and

186
00:17:00,480 --> 00:17:04,560
building habits, it's harder to say it was one specific action, which triggered this thing that

187
00:17:04,560 --> 00:17:12,080
may actually have needed several nudges, we call them. You know, I nudged you to try out a podcast

188
00:17:12,080 --> 00:17:16,720
once. It got it into your mind. You thought about it, but you never actually followed through.

189
00:17:16,720 --> 00:17:20,240
And then some other time, you typed a search query that was related to that podcast and they

190
00:17:20,240 --> 00:17:27,120
showed up again. And so it takes a few steps before we actually get the user to build a new habit.

191
00:17:27,120 --> 00:17:32,240
And then you have to reinforce that habit by showing it in shortcuts at the top of the page. And so

192
00:17:32,240 --> 00:17:37,760
that's kind of a long term outcome. And it's harder to say, here's the thing that actually

193
00:17:38,560 --> 00:17:44,640
created the reward. It's several things in sequence. And it's pretty easy to lose the attribution

194
00:17:44,640 --> 00:17:50,400
when it's several sequences of actions that led to the reward. And the reward is delayed. So

195
00:17:50,400 --> 00:17:56,160
causality and attribution become much trickier. What are some of the ways that you apply causality

196
00:17:56,160 --> 00:18:04,480
or causal modeling in trying to model attribution? Well, I mean, we do track lots of

197
00:18:06,400 --> 00:18:13,760
actions. So we have the ability to stitch together an entire, let's say, set of actions that

198
00:18:13,760 --> 00:18:21,680
led to an actual stream. So we have linkages through our data sets, which lets us follow the user

199
00:18:21,680 --> 00:18:26,480
throughout the app and the sequence of actions that led to the final stream and not just think it

200
00:18:26,480 --> 00:18:31,920
was the last last page that triggered it, but maybe you searched and went to an artist page,

201
00:18:31,920 --> 00:18:36,080
then came back and then searched for something else, came to a different artist page and then finally

202
00:18:36,080 --> 00:18:42,000
pushed click and play. We connect those little steps along the way. So we have little trajectories.

203
00:18:42,000 --> 00:18:46,400
And those trajectories get stitched into longer trajectories. So we've built trajectories

204
00:18:46,400 --> 00:18:52,240
for all the users, the history of what they've done and what they've consumed. And that trajectory

205
00:18:52,240 --> 00:18:58,160
data is actually a rich data set that's perfect for things like offline RL because we see

206
00:18:58,960 --> 00:19:06,480
not just action reward, action reward, we actually see the whole trajectory of state action reward

207
00:19:07,200 --> 00:19:12,240
triplets in a time series. And then we can say, all right, this time series clearly is leading

208
00:19:12,240 --> 00:19:17,440
to great outcomes and long-term rewards and consistent rewards. This other time series doesn't

209
00:19:17,440 --> 00:19:23,040
lead to, you know, a bigger sum of cumulative rewards. So we actually are now working with sequence

210
00:19:23,040 --> 00:19:28,960
data and are able to do things with our offline sequence data before going into an AB test.

211
00:19:29,600 --> 00:19:34,000
And we also are building simulators, which is another way to capture this kind of long-term

212
00:19:35,040 --> 00:19:40,640
attribution problem. We simulate how our homepage will look and how users will respond to it.

213
00:19:40,640 --> 00:19:45,920
We simulate playlists and how the users will skip and play them. And so simulation is another

214
00:19:45,920 --> 00:19:50,240
key technology. And both of those were topics of the workshops. So how do you work with offline

215
00:19:50,240 --> 00:19:55,360
sequence data, which we have, and we've logged across our user base. And how do you work with

216
00:19:55,360 --> 00:20:00,880
simulation, which we've built for our homepage, at least, and our playlists? Let's jump into the

217
00:20:00,880 --> 00:20:08,160
offline data. How do you do that? So for every user, we don't just keep track of, let's say,

218
00:20:08,160 --> 00:20:13,840
the last session. We actually look at the series of actions they've taken, the recommendations

219
00:20:13,840 --> 00:20:18,400
we've made, the rewards they've generated for us. And so we actually have time series data.

220
00:20:19,200 --> 00:20:26,480
And that's been valuable. We also use this to build lifetime value models. So we look at users

221
00:20:27,200 --> 00:20:31,680
on the service for many months and say, okay, who is retaining after X many months and who isn't?

222
00:20:31,680 --> 00:20:37,680
And those models look at the history of consumption and also some sequential aspects of

223
00:20:38,480 --> 00:20:44,400
consumption and engagement with our app. And that sequence that is used to make a prediction,

224
00:20:44,400 --> 00:20:49,200
saying, how much longer is this user going to stay? How long will they survive on Spotify, for instance?

225
00:20:49,760 --> 00:20:54,800
And those are things we were able to build also because we have long-term historical data

226
00:20:54,800 --> 00:20:59,840
on retention and what led to retention. So we look at sequences of actions. We look at

227
00:20:59,840 --> 00:21:07,920
long-term consumption histories and how they've led to retention and survival. And then of course,

228
00:21:07,920 --> 00:21:15,360
we build the simulators. But one of the things we've converged on is, in a way, lifetime value models

229
00:21:15,360 --> 00:21:20,400
have been around for a long time. They're used in kind of subscription services. We've realized

230
00:21:20,880 --> 00:21:27,600
lifetime value models are really just the reward function in RL because it's the sum of cumulative

231
00:21:27,600 --> 00:21:33,520
rewards with a discount. And that's literally what a lifetime value is in businesses, in subscriptions.

232
00:21:34,560 --> 00:21:38,720
The link goes a little different, but they actually turn out to be the same thing on

233
00:21:40,560 --> 00:21:46,400
an equation line at least. Yeah. How does that translate to implementation land? Is there

234
00:21:47,200 --> 00:21:52,480
are you building some model that at the end of the month, if the users are still around,

235
00:21:52,480 --> 00:21:59,760
there's a 995 reward or whatever that is. Now it is. Yeah. So one aspect is we literally say

236
00:22:01,200 --> 00:22:04,800
each month when you subscribe, that's a big reward for us because you say, hey Spotify,

237
00:22:04,800 --> 00:22:10,320
you did a good job last month. I'm going to keep betting on you. Here's my 995 or 10 bucks.

238
00:22:11,120 --> 00:22:16,320
And so what we're trying to capture is, you know, we've got 10 bucks for this user,

239
00:22:16,320 --> 00:22:21,360
but how many more months are they going to keep giving us that 10 bucks? And if I can change that,

240
00:22:21,360 --> 00:22:27,040
make it go from 15 more months of 10 bucks to 17 more bucks, then I'm really happy. That might

241
00:22:27,040 --> 00:22:31,520
not show up for a while, right? That user is going to be around. We're not going to know it until

242
00:22:31,520 --> 00:22:36,240
15 months go by and oh, wow, they stayed a little longer. But what we're trying to do is calculate

243
00:22:37,200 --> 00:22:43,360
the sum of those monthly rewards. And we actually do it over a rising of 60 months. So we look at

244
00:22:43,360 --> 00:22:49,120
over the next five years, you know, what's the probability each month that you're going to stick

245
00:22:49,120 --> 00:22:56,320
around? And we sum up all those probabilities multiplied by, you know, the dollar value of that

246
00:22:56,320 --> 00:23:01,280
month. And it's, you know, roughly 10 bucks for subscribers a month. But for free users,

247
00:23:01,280 --> 00:23:05,600
it's coming from their ads and their ad load. And actually, it's a little more complicated. So it's

248
00:23:05,600 --> 00:23:13,040
summing all their future months and with various degrees of ad load as the dollars. And we also

249
00:23:13,040 --> 00:23:19,280
applied discount factor because a dollar today is worth more than a dollar tomorrow, let's say. And

250
00:23:19,280 --> 00:23:27,360
that's exactly what a, you know, the RL textbook problem is it's the maximize a sum of discounted

251
00:23:27,360 --> 00:23:33,680
cumulative rewards. And it turns out LTV is exactly the sum of discounted cumulative dollar rewards.

252
00:23:33,680 --> 00:23:44,080
Is that in a lot of areas in machine learning applied to kind of business types of problems,

253
00:23:44,640 --> 00:23:49,360
you've got to create these proxy metrics because it's, you know, either your actual metric is,

254
00:23:50,240 --> 00:23:57,280
you know, too opaque or too difficult to, you know, turn into a metric suitable for a machine

255
00:23:57,280 --> 00:24:03,760
learning model. Is this an instance where you're able to more closely map the business metrics to

256
00:24:03,760 --> 00:24:10,480
machine learning than in other examples in your experience? Or is it just different?

257
00:24:11,040 --> 00:24:17,040
It is a good proxy metric, as you say, because at the end, yes, a business wants to optimize.

258
00:24:17,040 --> 00:24:22,000
So it's still a proxy metric is kind of the first thing you're saying here. It's not a holy

259
00:24:22,000 --> 00:24:27,520
grill of like we've, you know, fully captured the business need in this computational model here.

260
00:24:28,720 --> 00:24:32,640
Yeah, it's not the perfect metric. It doesn't capture every aspect of the business. There's all

261
00:24:32,640 --> 00:24:40,480
sorts of other costs and revenue and, you know, it's not, we're not trying to put the CFO

262
00:24:40,480 --> 00:24:47,280
at its entire organization into one ML model, but it is a very good proxy, let's say, because

263
00:24:47,280 --> 00:24:52,400
it really is capturing, you know, especially for a business like ours where we have subscribers

264
00:24:52,400 --> 00:25:00,240
and actually multiple plans and free and, you know, churns and premium. We're trying to capture all

265
00:25:00,240 --> 00:25:09,440
of that with a model that really summarizes, let's say, a good portion of the revenue and the margins

266
00:25:09,440 --> 00:25:13,360
for the business, but not all of it. It's not a simulation of the entire business, but it's actually

267
00:25:13,360 --> 00:25:19,600
the best we have considering, you know, it's a very complex business at the end.

268
00:25:20,400 --> 00:25:26,560
So we capture, I would say, a good chunk of the business complexity with this proxy, but it's not,

269
00:25:27,280 --> 00:25:32,480
it's not as good as the actual real data that's showing up every day when we actually see the real

270
00:25:32,480 --> 00:25:37,440
dollars and the real payouts to the artists and to the creators and so on. So it's, there's still

271
00:25:37,440 --> 00:25:43,840
some proxy there. It's not perfect. A big portion of your talk is kind of reviewing some of your

272
00:25:43,840 --> 00:25:50,320
teams, papers over the past year. So the first one you talked about is LTV and survival models,

273
00:25:50,320 --> 00:25:56,160
and I think that's what we just kind of talked about this idea that LTV is really the sum of

274
00:25:56,160 --> 00:26:03,520
the probabilities of, or this weighted sum of, here it's expected gross profit, but I think you map

275
00:26:03,520 --> 00:26:11,920
that to the survival model and the probabilities there. What's the right way to say that?

276
00:26:11,920 --> 00:26:20,720
Yeah, so it's basically the sum of survival probabilities scaled by the profit for each month,

277
00:26:21,600 --> 00:26:28,320
and then you also do it at discount factor because, you know, there's a, you know, the capital

278
00:26:28,320 --> 00:26:35,360
has a time discounts. Yeah. So you could think of it as net, net present value, lifetime value,

279
00:26:36,560 --> 00:26:43,600
or you can literally think of it in the RL normaclature as the value function, V of S, where S is

280
00:26:43,600 --> 00:26:49,600
the state. So if the user is in this state, what's the value? And it's basically the sum of expected

281
00:26:49,600 --> 00:26:55,760
rewards. If you play well from the starting state, according to, you know, a good policy. So

282
00:26:55,760 --> 00:27:04,400
it depends on the policy, of course. So if you continue to act, as we've acted, this user in this

283
00:27:04,400 --> 00:27:11,840
state will generate the following future rewards. And that's, and we've been modeling that with these,

284
00:27:12,960 --> 00:27:19,360
we call them beta geometric survival models, because you don't want to just use geometric,

285
00:27:19,360 --> 00:27:24,400
geometric is kind of the, you know, users don't really flip a coin each month and say I'm going to

286
00:27:24,400 --> 00:27:30,960
stay or not, they actually have these more complicated probabilities that actually depend on

287
00:27:30,960 --> 00:27:36,320
something more than just a single coin. And so we look at everything we know about the user,

288
00:27:36,320 --> 00:27:43,440
and we actually describe their survival through basically two numbers that are computed from

289
00:27:43,440 --> 00:27:48,400
everything we know about them. And those describe the shape of their future survival. And that's,

290
00:27:48,400 --> 00:27:57,520
that's been something we've published in a paper in 2021. And then we recently, in this past summer,

291
00:27:57,520 --> 00:28:04,320
published a version which extends beyond survival to multi-state. So it turns out, you know,

292
00:28:04,320 --> 00:28:08,800
users aren't in just one of two states that's modified. They're not just either subscribed,

293
00:28:08,800 --> 00:28:12,080
they're not subscribed. They actually can have many states. They could be

294
00:28:13,040 --> 00:28:17,280
subscribed. They could be in a free state. They could be in a family plan. They can be in a duo

295
00:28:17,280 --> 00:28:22,480
plan. They can be churned out. They can be churned out, but still registered. We still have

296
00:28:22,480 --> 00:28:27,680
information. They still have emails. And we can still, you know, potentially resume their

297
00:28:27,680 --> 00:28:32,240
account where they left off. And then there's users who just have never even interacted with

298
00:28:32,240 --> 00:28:38,160
Spotify whatsoever and have yet to enter any information into our, you know, into our logins,

299
00:28:38,160 --> 00:28:44,160
let's say. And so, and then furthermore, you can slice those states into more granular states of,

300
00:28:44,160 --> 00:28:50,400
is this user in this country or that one? And we can keep going. But then we've extended the

301
00:28:50,400 --> 00:28:56,720
survival modeling to multi-state survival. And then that starts to look like multi-state reinforcement

302
00:28:56,720 --> 00:29:03,680
learning. And also, a lot of the lessons learned from survival map to this kind of multi-state world.

303
00:29:03,680 --> 00:29:10,560
So, you know, LTV was really about a binary survive dot survive. We've extended to multi-state.

304
00:29:10,560 --> 00:29:17,040
And it actually now looks much more like a nicer connection with RL because RL almost from day one

305
00:29:17,040 --> 00:29:23,200
was multi-state to begin with. It never was just a binary state. And that paper you get into talking about

306
00:29:25,120 --> 00:29:31,680
talking about categorical distributions and Dirichlet distributions. What's the, where did those

307
00:29:31,680 --> 00:29:38,640
come into play? So, just like, you know, in, so if you're going to stay subscribed and not

308
00:29:38,640 --> 00:29:43,600
stay subscribed, we said it's kind of like a user flipping a coin. Each month, the user's flipping

309
00:29:43,600 --> 00:29:49,120
a coin and if it lands on heads, they turn off, if it lands on tails, they stick around for another

310
00:29:49,120 --> 00:29:55,360
month. That model is not perfect because it turns out users aren't just flipping a coin each month.

311
00:29:56,560 --> 00:30:03,520
They're, think of it as they're drawing a coin from a coin factory and flipping that coin each

312
00:30:03,520 --> 00:30:11,040
month. And so, that's how you think about it. And that's the coin factory is called a beta

313
00:30:11,040 --> 00:30:17,360
distribution. And then the coin flip itself is like a Bernoulli event. And it's a coin factory

314
00:30:18,640 --> 00:30:25,200
their next state. And the flip is whether they go there or not. So, what we're doing for each

315
00:30:25,200 --> 00:30:30,800
user is trying to predict what kind of coin factory are you writing as a user. And each month,

316
00:30:30,800 --> 00:30:34,800
you grab a random coin from that coin factory, you flip it and that decides what you do.

317
00:30:35,520 --> 00:30:41,040
And so, that was the analogy. And it turns out that fits the data way better. If each user is

318
00:30:41,040 --> 00:30:47,040
described as having their own coin factory, and that fits the data way better than saying each

319
00:30:47,040 --> 00:30:53,680
user has a secret coin that they flip. So, it's analogous to the beta survival model where you had

320
00:30:53,680 --> 00:30:57,520
these two parameters that you're trying to figure out for each user. Now, you're trying to figure

321
00:30:57,520 --> 00:31:02,640
out a coin factory number of parameters. How many parameters characterize a factory?

322
00:31:02,640 --> 00:31:08,480
So, the beta is the coin factory. And then the Dirschley is the dice factory. And so,

323
00:31:08,480 --> 00:31:14,000
okay. And so, when you have multiple states, you don't just flip a coin, you roll the dice.

324
00:31:14,000 --> 00:31:20,000
And so, I'm a state 1, 2, 3, 4, 5, or 6. And it turns out users don't transition

325
00:31:20,000 --> 00:31:27,360
by rolling at, you know, the dice. What they also seem to be doing is they have their own dice factory.

326
00:31:27,360 --> 00:31:33,600
They grab a dice from it every day. The dice are slightly loaded differently, and then they roll

327
00:31:33,600 --> 00:31:38,800
the dice. And that actually fits the data better. So, it turns out, you know, human beings are not

328
00:31:38,800 --> 00:31:46,000
a single dice or a single die. They're not a single coin. They're acting more like a factory

329
00:31:46,000 --> 00:31:51,040
of these things. And there's a distribution of dice or a distribution of coins. And that captures

330
00:31:51,600 --> 00:31:59,360
the dynamics of multi-state transition better than what we saw with just the simple models,

331
00:31:59,360 --> 00:32:03,920
like the Markov models and so on, that, you know, are single dice and single coin models.

332
00:32:04,800 --> 00:32:08,560
And now that's what we're using in our systems. Got it. Got it.

333
00:32:08,560 --> 00:32:15,760
And so, the next paper you talked about is the RL and temporally consistent survival.

334
00:32:17,040 --> 00:32:22,480
So, this sounds like an extension of the idea to temporal consistency. What were the

335
00:32:22,480 --> 00:32:27,040
challenges that you were looking at there? So, this was kind of like the last, you know,

336
00:32:27,040 --> 00:32:32,000
put the bow around the connection between LTV and RL. And so, this was published

337
00:32:32,000 --> 00:32:40,320
last week at NURRIPS. And what we said was, these survival models are great. They look like RL

338
00:32:41,040 --> 00:32:46,240
kind of as well. But there's one aspect of RL, which is missing, which is when you estimate

339
00:32:46,240 --> 00:32:52,480
a survival model, let's estimate my survival model for today. Tomorrow, you're also going to

340
00:32:52,480 --> 00:32:57,920
look at my data and estimate my survival model. Those two survival models should be consistent.

341
00:32:57,920 --> 00:33:03,280
You shouldn't estimate complete different survival from one day to the next. And yes,

342
00:33:03,280 --> 00:33:08,320
maybe they can start to change a little bit because today, maybe I discovered one more great

343
00:33:08,320 --> 00:33:13,200
podcast, the Tumel podcast with Sam. And so, maybe now I want to survive much better.

344
00:33:13,520 --> 00:33:17,440
But there should be some consistency in time. And if you enforce that consistency,

345
00:33:18,160 --> 00:33:23,040
you actually get a much better estimator for these survival models that works better than just

346
00:33:23,040 --> 00:33:27,520
fitting them to the data with maximum likelihood, which has been how we did this before.

347
00:33:27,520 --> 00:33:33,280
Or Bayesian kind of marginal likelihood. So, if you enforce temporal consistency,

348
00:33:33,280 --> 00:33:37,520
everything also seems to work better. And that was an aha moment. And that led to, again,

349
00:33:37,520 --> 00:33:43,600
a performance improvement in our models. So, you add temporal consistency to survival models.

350
00:33:43,600 --> 00:33:49,920
And you go from point flips and dice rolls to factories of coins and factories of dice.

351
00:33:50,960 --> 00:33:56,480
Both of those two ideas really seem to improve how well these models fit our real human data.

352
00:33:56,480 --> 00:34:01,280
And that's those are the lessons learned. And it turns out, those bring survival modeling

353
00:34:01,280 --> 00:34:06,960
very close to RL. And we feel like now, you know, there's almost a kind of a one-to-one

354
00:34:06,960 --> 00:34:11,440
the source between the two communities where you can say, okay, I've got this concept of temporal

355
00:34:11,440 --> 00:34:18,080
consistency. Oh, that's related to, you know, how RL enforces Markovian dynamics and Bell

356
00:34:18,080 --> 00:34:23,280
many equations and temporal difference learning. So, there's kind of a nice the source between

357
00:34:23,280 --> 00:34:28,960
these two technologies that have existed in very different communities, kind of all mapping to one

358
00:34:29,920 --> 00:34:36,640
one real big framework that's consistent. That strikes me a lot of evolution of the around

359
00:34:36,640 --> 00:34:42,400
the sophistication of the way you're applying RL to your problem over the course of just a year or two.

360
00:34:43,040 --> 00:34:47,040
Yeah, I mean, we're we have researchers thinking about this and trying to connect it. What's

361
00:34:47,040 --> 00:34:50,960
great about Spotify is we're not just building, you know, science for science to say we're really

362
00:34:50,960 --> 00:34:54,960
thinking about the business, thinking about our users, how we can give them the most value and

363
00:34:54,960 --> 00:35:02,800
understand their behaviors as opposed to just building, you know, algorithms in, you know,

364
00:35:04,720 --> 00:35:12,320
in a, you know, isolated way. Yeah, yeah. We're also testing some of these things now in production

365
00:35:12,320 --> 00:35:18,800
and seeing the benefits. So, some of the learnings are now that we've understood these LTV models

366
00:35:18,800 --> 00:35:26,000
and we start connecting them to value functions in RL and Q functions from RL. We're now understanding

367
00:35:26,000 --> 00:35:33,440
how they, how they can help us better make recommendations now that we think about our recommendations

368
00:35:33,440 --> 00:35:39,520
as an RL problem. And what are we trying to maximize in RL? You're trying to maximize the sum

369
00:35:39,520 --> 00:35:48,720
of future rewards and or maximize the Q function really. That Q function now we can start to

370
00:35:48,720 --> 00:35:53,680
understand better and realize for our domain that the Q function is a combination of getting

371
00:35:53,680 --> 00:35:58,400
you to click, but also giving you something that's very valuable for the long term when you do click.

372
00:35:59,040 --> 00:36:07,520
And so this was in the last paper that we presented and it's actually encouraging us to view kind

373
00:36:07,520 --> 00:36:14,320
of recommendation as not just maximize click through rate, but maximize the click through rate

374
00:36:14,320 --> 00:36:20,640
of something that's going to continue to generate let's say a long, long term sum of rewards.

375
00:36:22,240 --> 00:36:27,520
So some high value consumption item. So don't just show me something I'm very likely to click on,

376
00:36:28,080 --> 00:36:33,120
but if I do click on it, it's going to increase my lifetime value by a big amount. And so that's how

377
00:36:33,120 --> 00:36:37,920
we're shifting our recommendations now. That's the optimizing audio recommendations for the long term

378
00:36:37,920 --> 00:36:43,280
paper. That's right. So we're realizing, don't just get things that are clicky, but get things that

379
00:36:43,280 --> 00:36:49,600
are clicky and sticky. So once I click on it, I'm going to keep coming back to it. It's going to be

380
00:36:49,600 --> 00:36:56,080
something that becomes a habit for me as opposed to I click consume and forget. And it's going to

381
00:36:56,080 --> 00:37:02,800
there's no real, you know, change in my long term value once I do click on something. So we're trying

382
00:37:02,800 --> 00:37:08,080
to show you something you're likely to click on and try. But and if you do try it, also will increase

383
00:37:08,080 --> 00:37:14,400
your LTV. And that that's kind of how we're shifting our recommendations now. So can you talk about

384
00:37:14,400 --> 00:37:25,040
the the process of going from the the research to actual recommendations? You, you know, there's

385
00:37:25,040 --> 00:37:29,920
this one idea of hey, you know, we've got these research. They identify these methods. We take

386
00:37:29,920 --> 00:37:36,160
these methods. We implement them against our data. They produce these models and poof. The models

387
00:37:36,160 --> 00:37:43,120
will recommend the constant the content. What you're describing here is the models are informing you

388
00:37:43,120 --> 00:37:48,800
about ways to think about how to make recommendations. And then you make different different

389
00:37:48,800 --> 00:37:55,840
recommendations. I'm not hearing you put models in production that make different recommendations.

390
00:37:55,840 --> 00:38:02,720
Like bridge the gap for me around this. Are we just talking time scales or? No, you're right.

391
00:38:02,720 --> 00:38:07,760
This is a longer process. I'm kind of, you know, jumping to the to the conclusion. But the reality

392
00:38:07,760 --> 00:38:12,880
is the way this starts is we have the ideas who write down, you know, some modeling assumptions

393
00:38:12,880 --> 00:38:19,440
and some, you know, aha moments, maybe a paper. We build a prototype. Then we maybe take that

394
00:38:19,440 --> 00:38:24,320
prototype and refine it with the offline data that I talked about. We get some good offline results.

395
00:38:24,320 --> 00:38:30,080
And we may even put the model through the simulator and see how well it does in simulation.

396
00:38:31,120 --> 00:38:36,400
Then we actually get it to be a productionized model that we can run, run live on real users.

397
00:38:36,400 --> 00:38:42,240
And what we then do is we A, B test it. We say, let's run this model in kind of a side-by-side

398
00:38:42,240 --> 00:38:47,760
horse race against what we're already doing from last year, let's say. So we got model A and

399
00:38:47,760 --> 00:38:53,600
model B running on half users get model A, half users get model B. And then that's really how

400
00:38:53,600 --> 00:38:59,120
we evaluate. We don't just stop offline or stop at the prototype. So then we have these two

401
00:38:59,120 --> 00:39:04,960
production models running side-by-side. And then we actually say, do we see after, you know,

402
00:39:05,600 --> 00:39:11,920
X many days of running this model side-by-side better engagement. Our people sticking around longer,

403
00:39:11,920 --> 00:39:17,040
retaining better after a couple of months of horse racing these models side-by-side. And it turns

404
00:39:17,040 --> 00:39:25,040
out we do see better long-term metrics with these kind of RL-inspired models. And it turns out

405
00:39:26,080 --> 00:39:32,560
what the models seem to do is they have lower click-through rate, the new models. But long-term,

406
00:39:32,560 --> 00:39:38,640
the users are streaming and retaining better. So I've given up on showing something that, you know,

407
00:39:38,640 --> 00:39:44,400
you're going to click on as often. But what ends up happening is they get to show you things that

408
00:39:44,400 --> 00:39:48,960
maybe are tiny bit lower click-through rate. But once you do start clicking on them, then you keep

409
00:39:48,960 --> 00:39:54,880
coming back to them and they become habits. And then you're listening more on Spotify and you're

410
00:39:54,880 --> 00:39:59,600
retaining better on Spotify. And so that's kind of what we're going after. We're going after the long-term

411
00:39:59,600 --> 00:40:07,600
outcomes. It's okay to go from a click-through rate of let's say, you know, 47% down to 45% click-through rate.

412
00:40:07,600 --> 00:40:15,280
What I'd much rather say is, okay, but then two months later, I'm actually getting more listening

413
00:40:15,280 --> 00:40:21,440
total even though my click-through is lower. Because I'm actually building longer-term habits and

414
00:40:21,440 --> 00:40:27,280
coming back to that same podcast. It's a habitual podcast. It's a new way of engaging with Spotify.

415
00:40:27,280 --> 00:40:35,360
Maybe now I listen to a, you know, a meditation podcast in the evenings. And I listen to my,

416
00:40:35,360 --> 00:40:42,560
you know, my news podcast in the morning and my way to work. So all these new habits now have been

417
00:40:42,560 --> 00:40:48,560
added. They might have actually had lower click-through rate in the moment, but long-term they generate

418
00:40:48,560 --> 00:40:53,120
more engagement for that user. The user spends more time on Spotify and retains better on Spotify

419
00:40:53,120 --> 00:40:58,560
at the end of the 60 day or 70 day trial. And then we roll it out so that everybody gets that

420
00:40:58,560 --> 00:41:07,520
better experience. And so, is that cycle something that, you know, the models produced by this

421
00:41:07,520 --> 00:41:14,640
RL approach kind of been through that full cycle and, you know, that happens on some frequency,

422
00:41:14,640 --> 00:41:20,800
or, you know, given that we're talking about long-term value here, is this also a long-term

423
00:41:20,800 --> 00:41:27,120
assessment process? And the jury's still out on the models. You're liking what you're seeing,

424
00:41:27,120 --> 00:41:33,520
but it's not a full flage to commitment to this particular approach. So, yeah, that's it.

425
00:41:33,520 --> 00:41:38,960
That's a great question. So some of these things are actually fully rolled out in parts of our

426
00:41:38,960 --> 00:41:45,760
product. So we've got a fleet of machine learning systems. Sure. Some of them now are now completely

427
00:41:45,760 --> 00:41:51,040
on this kind of approach. So we tested it, AB tested it, it was a win, we rolled it out. And so now

428
00:41:51,040 --> 00:41:56,560
this is the default approach in some parts of the app. Other parts of the app were still testing.

429
00:41:57,760 --> 00:42:03,280
Other parts of the app we haven't even tried it out yet. So the approach has legs.

430
00:42:03,280 --> 00:42:08,320
It doesn't mean it works everywhere, but we're past the stage of just prototyping and trying

431
00:42:08,320 --> 00:42:14,800
stuff in simulation or offline. It's getting real users, giving it thumbs up. And actually,

432
00:42:14,800 --> 00:42:18,560
in some parts of the app, it's actually fully deployed as an approach.

433
00:42:18,560 --> 00:42:24,080
Yeah. So this approach that is kind of being pioneered on the research side, are there

434
00:42:25,680 --> 00:42:31,360
ways that your data scientists and machine learning engineers need to think differently about

435
00:42:31,360 --> 00:42:40,000
modeling to use the RL types of approaches or to kind of embrace what you're doing here? Or

436
00:42:42,240 --> 00:42:47,280
is it just another tool in the toolbox for them? So it's a great question. We're obviously trying

437
00:42:47,280 --> 00:42:53,200
to make some of these things, you know, let's say, easy to reuse and try out in different places.

438
00:42:53,200 --> 00:42:57,760
So you're not starting from a blank slate in other parts of the product. What do you want to try

439
00:42:57,760 --> 00:43:03,760
out these techniques? But really, this is a multi-disciplinary endeavor. We worked first off with

440
00:43:03,760 --> 00:43:10,800
researchers and we spoke to users and got user research even to tell us people liked this idea of,

441
00:43:10,800 --> 00:43:15,600
you know, recommendations that aren't just click-vading for the moment, but they're actually great

442
00:43:15,600 --> 00:43:21,200
for the long term. And then we took those intuitions, fleshed out research prototypes,

443
00:43:21,200 --> 00:43:25,920
and those prototypes have to look promising. We brought in engineers who could build the

444
00:43:25,920 --> 00:43:31,360
scalable productionized versions of them. We A, B test them, we get data scientists to look at

445
00:43:31,360 --> 00:43:35,520
those results. The data scientists say, okay, this is what's happening to the metrics. This is how

446
00:43:35,520 --> 00:43:39,520
they move. This is what we recommend use this setting of this algorithm. That's what we would

447
00:43:39,520 --> 00:43:44,640
recommend rolling out. You know, and product managers are also involved. So it really is

448
00:43:44,640 --> 00:43:50,400
all the expertise is coming together. And it's not just researchers doing great research and

449
00:43:50,400 --> 00:43:54,720
then throwing it over the fence. They really sit down with the engineers, the data engineers,

450
00:43:54,720 --> 00:43:59,760
backend engineers, machine learning engineers. So I would say that's kind of the Spotify way.

451
00:43:59,760 --> 00:44:06,000
It's bring all the skills necessary for the problem to the table. So then we go from end to end,

452
00:44:06,000 --> 00:44:13,760
we go from an idea to an actual user productionized value add. Awesome. Awesome. Those are great

453
00:44:13,760 --> 00:44:20,240
case study and real world applicability of reinforcement learning. It's been an interesting topic

454
00:44:20,240 --> 00:44:27,680
for folks for a while. And I also hope is the first of many conversations. I'd love to have

455
00:44:27,680 --> 00:44:33,280
you kind of close us out with, you know, just what you're most excited about in terms of the future

456
00:44:33,280 --> 00:44:37,840
of this particular work where you think that goes. Well, I mean, I think reinforcement learning is

457
00:44:37,840 --> 00:44:44,240
about doing things with, you know, human feedback and what really matters to people for the long term,

458
00:44:44,240 --> 00:44:49,600
not just building algorithms like click optimization algorithms, which is maybe where the internet started,

459
00:44:49,600 --> 00:44:55,760
but where's it going? It's going to, you know, get the long term user feedback and human feedback.

460
00:44:55,760 --> 00:45:00,320
And we're trying to do that now. It's Spotify. We're seeing other companies do that, you know,

461
00:45:00,320 --> 00:45:06,720
for example, people are fine tuning with RL. They're large language models with human feedback. So

462
00:45:06,720 --> 00:45:10,720
they start to do more intelligent things. So we're really viewing reinforcement learning as a way

463
00:45:10,720 --> 00:45:17,360
to incorporate more valuable human feedback in how the algorithms behave. And I think this is

464
00:45:18,400 --> 00:45:23,600
maybe now a nice and fletching points where RL is moving out of the textbooks into the real world

465
00:45:23,600 --> 00:45:28,720
more and more. So I'm very excited that it will help us build algorithms that are actually more

466
00:45:28,720 --> 00:45:37,360
long-term intelligent and not just kind of clickbait, like myopic click chasers. That's awesome.

467
00:45:38,560 --> 00:45:42,640
All right. Well, thanks so much, Tony. I really appreciate you taking the time to chat with us.

468
00:45:42,640 --> 00:45:58,560
Thank you, Sam. What's up? I'm talking to you today.

