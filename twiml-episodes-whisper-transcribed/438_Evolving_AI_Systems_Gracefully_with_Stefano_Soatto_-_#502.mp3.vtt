WEBVTT

00:00.000 --> 00:13.640
All right, everyone. I am here with Stefano Soato.

00:13.640 --> 00:18.760
Stefano is VP of AI Application Science at AWS,

00:18.760 --> 00:22.440
as well as a professor of computer science at UCLA.

00:22.440 --> 00:25.360
Stefano, welcome to the Twomla AI podcast.

00:25.360 --> 00:27.360
Thanks for having me.

00:27.360 --> 00:29.760
I'm really looking forward to digging into this conversation.

00:29.760 --> 00:34.480
We'll be talking about some of your recent work on an area

00:34.480 --> 00:38.400
you called Graceful AI, which is very interesting.

00:38.400 --> 00:41.120
But before we do that, I'd love to have you share a little bit

00:41.120 --> 00:45.640
about your background and how you came to work in machine learning.

00:45.640 --> 00:48.680
Yeah, I appreciate that my background is a bit unusual.

00:48.680 --> 00:51.320
I grew up in Italy as you can tell from my accent.

00:51.320 --> 00:53.280
And I grew up studying class, studying classics,

00:53.280 --> 00:56.880
like history, philosophy, Latin, Greek, and so on and so forth.

00:56.880 --> 01:02.840
And before going to college, I entered a context

01:02.840 --> 01:04.880
for a summer course organized by a tiny school

01:04.880 --> 01:07.920
that has a class of letters and a class of science.

01:07.920 --> 01:10.960
And I was there to study philosophy,

01:10.960 --> 01:13.640
but I realized that there were problems that people were writing

01:13.640 --> 01:17.080
on a board that I had no clue how to solve.

01:17.080 --> 01:18.120
And these were math problems.

01:18.120 --> 01:19.120
And it really, really bugged me.

01:19.120 --> 01:21.920
And so I got more and more interested in that side

01:21.920 --> 01:24.200
and I started attending those seminars.

01:24.200 --> 01:26.880
And then I rolled in engineering and in 1989, I ran across

01:26.880 --> 01:29.520
the work of Ernst Dickman's.

01:29.520 --> 01:32.760
Ernst Dickman's was a pioneer of autonomous driving

01:32.760 --> 01:34.920
back in 1989 in Germany.

01:34.920 --> 01:38.840
He was the one that had autonomous vehicle going on the Autobahn

01:38.840 --> 01:41.960
at speeds up to 180 kilometers per hour.

01:41.960 --> 01:44.680
And when I joined the Caltech for my PhD,

01:44.680 --> 01:48.040
I was in the Department of Control and the Renial Systems.

01:48.040 --> 01:50.640
And that's where I got exposed to computer vision

01:50.640 --> 01:54.000
because the reason we don't have domestic robot helpers

01:54.000 --> 01:55.600
is not because we don't know how to control them,

01:55.600 --> 01:57.720
because we don't know how to endow them

01:57.720 --> 01:59.480
without a presentation of the surrounding world.

01:59.480 --> 02:02.200
And so I got more and more interested in that problem.

02:02.200 --> 02:04.160
And that was a quarter of a century ago

02:04.160 --> 02:06.880
and I'm still very excited and interesting about that area.

02:06.880 --> 02:08.880
And so that's how I got into AI.

02:08.880 --> 02:11.120
Nice.

02:11.120 --> 02:14.400
We could spend a whole different interview

02:14.400 --> 02:15.400
talking about that.

02:15.400 --> 02:17.880
I, one of my perennial favorite topics

02:17.880 --> 02:21.440
is the relationship between kind of control

02:21.440 --> 02:25.120
and machine learning and end-to-end systems

02:25.120 --> 02:29.720
versus more modular systems that use what we've learned

02:29.720 --> 02:32.200
about physics and control and other things.

02:32.200 --> 02:34.600
But I think that's going to be a different conversation.

02:34.600 --> 02:39.680
Tell us a little bit about both your role at AWS

02:39.680 --> 02:44.800
as well as your research interests across AWS and UCLA.

02:44.800 --> 02:45.640
Yeah.

02:45.640 --> 02:48.680
Yeah, so at AWS, I am in charge of the science

02:48.680 --> 02:50.480
for AI applications.

02:50.480 --> 02:54.080
Any applications are grouped by modalities.

02:54.080 --> 02:58.600
So we have vision, so images and video.

02:58.600 --> 03:01.000
And then we have speech, and then we have text,

03:01.000 --> 03:02.480
and then we have vertical.

03:02.480 --> 03:05.240
And verticals are domain-specific applications

03:05.240 --> 03:07.680
in the industrial space as well as operations

03:07.680 --> 03:10.920
and time series for casting and so on support.

03:10.920 --> 03:14.960
And we are organized as research teams,

03:14.960 --> 03:17.800
but we are very closely coupled with engineering teams,

03:17.800 --> 03:21.480
product teams and data teams.

03:21.480 --> 03:25.400
And we practice what we call customer-obsessed science,

03:25.400 --> 03:27.840
which is a different curiosity-driven science,

03:27.840 --> 03:30.120
which is what we do at the university.

03:30.120 --> 03:33.960
And so our work ends up in the hands of customers

03:33.960 --> 03:37.560
in a time frame that is quite fast

03:37.560 --> 03:40.360
for somebody who's used to academic time clock

03:40.360 --> 03:43.560
where you expect your impact in the world to be posthumous

03:43.560 --> 03:45.240
or if you're lucky during your lifetime.

03:45.240 --> 03:47.840
But here, there's a very quick turnaround between ideas

03:47.840 --> 03:50.720
being generated, services being deployed,

03:50.720 --> 03:54.240
and customers using them and getting benefit from it.

03:54.240 --> 03:56.720
So my interest is broadly speaking in AI.

03:56.720 --> 03:58.920
I've always been interested in autonomous system systems

03:58.920 --> 04:01.960
that interact intelligently with the environment,

04:01.960 --> 04:05.080
where intelligent is to be defined, everybody has their own.

04:05.080 --> 04:07.640
But to me, a dog is plenty intelligent.

04:07.640 --> 04:13.120
I wish we could build something that behaves like a dog.

04:13.120 --> 04:17.320
So that's much more difficult than doing a chess-playing program

04:17.320 --> 04:19.080
in a fight for survival.

04:19.080 --> 04:22.080
The dog will win over the chess-playing program any time.

04:22.080 --> 04:25.240
So you just need to unplug the power.

04:25.240 --> 04:30.720
And definitely, there are opportunities that are...

04:30.720 --> 04:34.720
The time is right for research that we've been doing

04:34.720 --> 04:37.360
for 10, 20 years to become useful now.

04:37.360 --> 04:40.400
And for me, the game changer was in 2009,

04:40.400 --> 04:45.280
I read a report by Cisco called Visual Network in Index,

04:45.280 --> 04:51.080
that pointed out that back then, 2009,

04:51.080 --> 04:54.120
peer-to-peer traffic was surpassed by video traffic

04:54.120 --> 04:55.360
on the internet.

04:55.360 --> 04:57.120
And they were forecasting that by 2022,

04:57.120 --> 04:59.240
it would be 90% of wireless traffic

04:59.240 --> 05:01.160
and almost the totality of internet traffic.

05:01.160 --> 05:02.400
And so to me, that was a game changer.

05:02.400 --> 05:07.280
But I thought, now we have data, we store data.

05:07.280 --> 05:09.880
But we don't know how to extract intelligence from data.

05:09.880 --> 05:11.640
And that will take more than my lifetime.

05:11.640 --> 05:15.040
So I was not thinking that it would be so quick.

05:15.040 --> 05:18.280
And then in 2014, I was involved in a project

05:18.280 --> 05:21.160
where there were training systems

05:21.160 --> 05:25.880
to detect anomalies in CT scans in medical imaging.

05:25.880 --> 05:28.440
And then I realized that on a task,

05:28.440 --> 05:30.800
where humans are not naturally evolved,

05:30.800 --> 05:33.160
which is to interpret not natural, but medical images.

05:33.160 --> 05:38.680
So where you have to train these physicians for many years,

05:38.680 --> 05:40.040
you could train a deep learning model

05:40.040 --> 05:44.640
and beat even the most experienced radiologists.

05:44.640 --> 05:46.360
And so that's where I realized, oh my God,

05:46.360 --> 05:48.200
this is happening in my lifetime.

05:48.200 --> 05:50.880
And so what is the right place to be part of this

05:50.880 --> 05:51.880
if you want to be involved?

05:51.880 --> 05:54.400
Well, you need to be in a place that has exposure

05:54.400 --> 05:58.360
to a variety of problem domains and has patience

05:58.360 --> 06:01.400
and has data resources to make it happen.

06:01.400 --> 06:03.160
And slowly, one little step at a time,

06:03.160 --> 06:04.720
but we've been able to make a difference

06:04.720 --> 06:06.720
in several of our products that our customers have been

06:06.720 --> 06:08.600
benefiting from.

06:08.600 --> 06:11.520
Yeah, I like the characterization of areas

06:11.520 --> 06:16.440
that we're evolved to be good at versus those that we aren't.

06:16.440 --> 06:19.080
You know, as a classifier for problems

06:19.080 --> 06:21.840
that we should seek to apply a machine learning

06:21.840 --> 06:23.720
for, it kind of reminds me of Andruing's

06:23.720 --> 06:26.360
anything that takes us more than a second.

06:26.360 --> 06:30.160
But it's got a maybe a more philosophical spin to it.

06:30.160 --> 06:31.640
Well, both the philosophical is very easy

06:31.640 --> 06:34.160
to underestimate how difficult some tasks are.

06:34.160 --> 06:37.040
One anecdote I have is that with my students

06:37.040 --> 06:40.280
are participated to the first two data program challenges.

06:40.280 --> 06:42.400
And it was very interesting to be in the audience

06:42.400 --> 06:44.520
and watching people who are not experts

06:44.520 --> 06:45.880
see this card right by.

06:45.880 --> 06:47.720
And these cards really look like they didn't know

06:47.720 --> 06:48.400
what they were doing.

06:48.400 --> 06:50.640
And so, you know, how can you possibly not

06:50.640 --> 06:52.640
see that bush and drive over it

06:52.640 --> 06:54.680
when anybody can see that bush, it's obvious.

06:54.680 --> 06:56.720
People forget that this is roughly

06:56.720 --> 06:59.320
the size of your brain, the size of your two hands,

06:59.320 --> 07:02.000
and roughly half of it processes visual information.

07:02.000 --> 07:04.760
So most of the real estate in your brain, even when

07:04.760 --> 07:07.560
you're absorbed in the most, you know, in the most

07:07.560 --> 07:10.120
abstracts, true thinking, most of your brain is busy trying

07:10.120 --> 07:12.680
to make sense of the sensory data that comes through.

07:12.680 --> 07:14.200
So it's really a cool and task.

07:14.200 --> 07:17.160
And on the contrary, some things that

07:17.160 --> 07:19.080
look very, very difficult to humans

07:19.080 --> 07:20.440
are trivial to computers.

07:20.440 --> 07:23.400
And so we've been able to make inroads in both.

07:23.400 --> 07:25.400
Some traditionally hard problems,

07:25.400 --> 07:27.840
like learning one from few shots.

07:27.840 --> 07:30.720
We have made progress, thanks to also the evolution

07:30.720 --> 07:32.280
of deep learning of the past five years,

07:32.280 --> 07:34.160
as well as the evolution of hardware, optimization

07:34.160 --> 07:35.800
methods, and so on.

07:35.800 --> 07:38.040
But also, we've been able to better understand

07:38.040 --> 07:40.320
the problem, you know, in a way which

07:40.320 --> 07:43.720
is independent of whether it's implemented

07:43.720 --> 07:47.320
with biological hardware or silicon hardware.

07:47.320 --> 07:48.880
There are some characteristics of learning

07:48.880 --> 07:51.240
problems that are absolutely fascinating

07:51.240 --> 07:53.440
that we've just started to poke into.

07:53.440 --> 07:56.040
And they are from the perspective of somebody

07:56.040 --> 07:58.240
with an academic inclination is very fascinating

07:58.240 --> 07:59.920
to expect to think about.

07:59.920 --> 08:01.040
Yeah, yeah.

08:01.040 --> 08:02.880
And we want to dig into one of those.

08:02.880 --> 08:05.040
And that is the work that you and your teams have been

08:05.040 --> 08:09.520
knowing around this idea of graceful AI.

08:09.520 --> 08:12.280
You know, what I thought was most compelling about that

08:12.280 --> 08:15.600
is I think we've come to appreciate that when we're

08:15.600 --> 08:19.200
using machine learning models and production,

08:19.200 --> 08:22.160
we need to constantly evolve them, constantly train them

08:22.160 --> 08:27.360
to counteract the effects of data drift,

08:27.360 --> 08:30.160
or drift in the distribution of our data.

08:30.160 --> 08:33.480
And your paper in this work, in general,

08:33.480 --> 08:36.520
kind of ask the question or prompts the question

08:36.520 --> 08:39.400
that, you know, are there some negative effects associated

08:39.400 --> 08:42.760
with constantly retraining those models, you know,

08:42.760 --> 08:46.080
what are they, and how do we deal with them?

08:46.080 --> 08:47.800
I'd like to have you tell us a little bit

08:47.800 --> 08:51.960
about the broader motivation that led to this body of work.

08:51.960 --> 08:54.080
Yeah, that's a, it's actually interesting story

08:54.080 --> 08:55.600
because it's certain dipitres, right?

08:55.600 --> 09:01.080
So, you know, one of the luxuries of being at AWS

09:01.080 --> 09:04.520
is that you get exposed to a media of problems

09:04.520 --> 09:06.200
that you don't even exist.

09:06.200 --> 09:08.000
And you would not dream of by just sitting

09:08.000 --> 09:09.880
in your office at the university.

09:09.880 --> 09:13.320
And so this was a case where, you know,

09:13.320 --> 09:17.360
the natural mode of evolution of knowledge is continuous,

09:17.360 --> 09:19.120
like you mentioned, so we don't have this

09:19.120 --> 09:21.080
arbitraris separation between training

09:21.080 --> 09:24.680
and inference phases, which we do have in machine learning.

09:24.680 --> 09:26.160
But in machine learning, we have this phase

09:26.160 --> 09:28.120
where we train something, we make,

09:28.120 --> 09:29.960
we build a model, we offer to customer,

09:29.960 --> 09:33.480
people use it at some point, either a better model comes along

09:33.480 --> 09:35.720
and certainly if you follow the academic literature,

09:35.720 --> 09:38.680
every single conference, there's an incremental improvement

09:38.680 --> 09:40.360
and you would like to harvest all these improvements

09:40.360 --> 09:42.520
so that can benefit customers downstream.

09:43.480 --> 09:45.960
But what we're gonna allow is that customers

09:45.960 --> 09:47.680
were not updating their models

09:47.680 --> 09:52.680
and we were not understanding why wouldn't you change the model

09:52.680 --> 09:55.440
when this one performs better.

09:55.440 --> 09:56.720
Now, the definition of performance

09:56.720 --> 09:58.680
is the one that comes from academic benchmark

09:58.680 --> 10:02.200
where we count the average number of errors

10:02.200 --> 10:04.840
and we train to minimize the proxy of that

10:04.840 --> 10:07.560
whether it's empirical cross-entropy or some other loss.

10:07.560 --> 10:09.680
So, but basically the criterion that we,

10:09.680 --> 10:12.400
as academic, had guessed is most relevant

10:12.400 --> 10:14.440
is the average number of errors.

10:14.440 --> 10:15.640
Turns out, what people don't care

10:15.640 --> 10:16.680
for the average number of errors.

10:16.680 --> 10:18.560
They have very specific requirements

10:18.560 --> 10:21.200
for what type of chords they care about

10:21.200 --> 10:25.960
and even specific subsets of subtypes of the data

10:25.960 --> 10:28.400
that particularly care about.

10:28.400 --> 10:33.400
And so, we realized that the proxies that we were training for

10:34.080 --> 10:36.240
were not the right ones for the customers.

10:36.240 --> 10:39.560
They were just the ones that we, by default,

10:39.560 --> 10:40.760
have been using for years.

10:40.760 --> 10:42.480
But the conversation that triggered that was important,

10:42.480 --> 10:45.120
it's an example of the customer obsessed science

10:45.120 --> 10:47.200
because we need to release the model

10:47.200 --> 10:50.080
that was better than everything we've done before.

10:50.080 --> 10:51.480
So, we were very excited.

10:51.480 --> 10:54.960
And then we heard the customers were not happy.

10:54.960 --> 10:57.200
And so, we were wondering why is that?

10:57.200 --> 10:59.880
Well, because there is regression, what is regression?

10:59.880 --> 11:03.000
Regression is when you have a new model

11:03.000 --> 11:04.960
and that model, even though on average,

11:04.960 --> 11:07.080
it performs better than the old model

11:07.080 --> 11:11.280
but it introduces mistakes that the previous model didn't make.

11:11.280 --> 11:12.120
Okay.

11:12.120 --> 11:14.680
And so, even if these mistakes are fewer,

11:14.680 --> 11:16.520
when you notice the old model makes mistake

11:16.520 --> 11:19.640
that the new model makes mistakes the old one did not make,

11:19.640 --> 11:22.600
your puzzle doesn't use it because how is this any better?

11:22.600 --> 11:27.600
And so, and the conversation between a scientist

11:27.720 --> 11:30.480
and a program manager was around the tone, well,

11:30.480 --> 11:33.880
of course, the new model can make a mistake

11:33.880 --> 11:35.600
that the old model didn't make.

11:35.600 --> 11:37.880
Let me explain something about machine learning to you.

11:38.880 --> 11:40.200
And the program manager would say,

11:40.200 --> 11:43.400
no, let me explain something to you about customers

11:43.400 --> 11:44.360
solving their own problem.

11:44.360 --> 11:45.520
And I was listening and I was thinking,

11:45.520 --> 11:47.560
well, maybe we are solving their own problem.

11:47.560 --> 11:51.160
So, this type of interaction with customer escalations

11:51.160 --> 11:52.800
is really a treasure trove for scientists

11:52.800 --> 11:55.160
because every time there is an obstacle,

11:55.160 --> 11:56.480
for an engineer, it's a frustration

11:56.480 --> 11:58.120
because it's a yet another problem to solve

11:58.120 --> 11:59.800
to get to the finish line for the scientist

11:59.800 --> 12:02.520
is an opportunity is because there's something to be understood.

12:02.520 --> 12:04.600
And here, what was there to be understood

12:04.600 --> 12:07.200
is that we are really not pretty much in the right proxy.

12:07.200 --> 12:09.600
And so, that's where we started thinking about,

12:09.600 --> 12:14.440
why should the average error be the right metric?

12:14.440 --> 12:19.840
Because some people care for having sort of even performance

12:19.840 --> 12:21.640
across different courts of the data set

12:21.640 --> 12:23.880
or some people care about having performance

12:23.880 --> 12:27.560
that behaves similarly in models that are deployed at the edge

12:27.560 --> 12:29.480
or at the cloud and some people care

12:29.480 --> 12:32.000
to maintain compatibility with previous models

12:32.000 --> 12:34.760
because they built pre-processing post-processing

12:34.760 --> 12:35.760
around the train model.

12:35.760 --> 12:38.360
And so, if you change the model suddenly you break it,

12:38.360 --> 12:41.640
it is months of work, it's extremely expensive.

12:41.640 --> 12:46.800
And so, we realized this whole universe of problems

12:46.800 --> 12:48.400
that arise when you're trying to train a machine

12:48.400 --> 12:51.520
but learning model not just to perform well

12:51.520 --> 12:53.760
in terms of the average probability of error,

12:53.760 --> 12:56.600
but that performs well relative to the context

12:56.600 --> 12:59.360
of pre-processing algorithm, post-processing algorithm,

12:59.360 --> 13:02.120
pre-existing models, models that are deployed

13:02.120 --> 13:04.120
on different hardware and so on and so forth.

13:04.120 --> 13:06.000
And so, it's fascinating because this is a problem

13:06.000 --> 13:08.120
that I would have never thought about

13:08.120 --> 13:11.080
seeing a university anywhere have not ever occurred to us

13:11.080 --> 13:13.320
if it wasn't for a customer coming back

13:13.320 --> 13:15.280
and say, I don't like this, fix it.

13:15.280 --> 13:19.840
So, that's kind of the essence of customer-obsessed science.

13:19.840 --> 13:25.160
Yeah, there's so many interesting facets to this one

13:25.160 --> 13:28.160
that jumps out at me is just how it's a reminder

13:28.160 --> 13:31.040
of how immature machine learning is.

13:31.040 --> 13:34.040
On the traditional engineering side,

13:34.040 --> 13:39.040
we've got this whole set of methodologies around testing

13:39.040 --> 13:42.160
and one of those types of tests is regression testing.

13:42.160 --> 13:43.480
We know how to do that.

13:43.480 --> 13:46.440
Every engineering team worth its salt runs a series

13:46.440 --> 13:49.160
of regression tests before they release their product

13:49.160 --> 13:53.960
to make sure that the product isn't taking steps backwards,

13:53.960 --> 13:58.000
but this is new in the context of machine learning.

13:58.000 --> 13:58.800
Exactly.

13:58.800 --> 14:00.640
And there's even some aspects of it

14:00.640 --> 14:03.080
which are very peculiar and specific to deep learning

14:03.080 --> 14:07.080
or more in general, over-complete models

14:07.080 --> 14:11.280
for instance, for classification, where if you take a deep learning

14:11.280 --> 14:13.560
model, let's say you're SNed 152 and you train it

14:13.560 --> 14:16.240
on a large data cell, let's say, ImageNet.

14:16.240 --> 14:18.560
If you repeat the experiment a hundred times,

14:18.560 --> 14:21.840
retrain the same exact model on the same exact data

14:21.840 --> 14:24.720
just from different initial conditions,

14:24.720 --> 14:26.920
you compare it to a hundred different models,

14:26.920 --> 14:29.760
but all of them have exactly the same average error,

14:29.760 --> 14:33.880
let's say 87.3, whatever it is.

14:33.880 --> 14:35.200
They all have the same average error.

14:35.200 --> 14:37.360
What do you go and look?

14:37.360 --> 14:39.960
The mistakes they make, they're completely different.

14:39.960 --> 14:42.040
So it's as if they were trading mistake,

14:42.040 --> 14:44.560
I'll get this one right, but give me this one

14:44.560 --> 14:45.400
and I'll get it wrong.

14:45.400 --> 14:48.000
So it was really eye-opening because it's way the second.

14:48.000 --> 14:51.160
So they all make the same number of mistakes,

14:51.160 --> 14:53.160
but they're different mistakes.

14:53.160 --> 14:55.840
And this is what we realize that very often,

14:55.840 --> 14:58.120
some of these criteria, for instance,

14:58.120 --> 15:00.400
equal error rate across different demographics,

15:00.400 --> 15:02.880
as well as compatibility with our models,

15:02.880 --> 15:06.520
are conflicting with average performance.

15:06.520 --> 15:09.320
But here we have a case we have an ISO error rate surface

15:09.320 --> 15:12.480
where all models are equivalent in terms of error rate,

15:12.480 --> 15:14.360
but you can move along the surface

15:14.360 --> 15:16.600
in high dimensional space to make sure

15:16.600 --> 15:18.640
that your model does not make,

15:18.640 --> 15:20.720
or makes as few mistakes as possible,

15:20.720 --> 15:22.200
that were not made at the previous model.

15:22.200 --> 15:25.080
So you can optimize criteria that are orthogonal

15:25.080 --> 15:26.560
and they're not conflicting with each other.

15:26.560 --> 15:28.320
So this was the first time we realized that,

15:28.320 --> 15:30.960
okay, this is yet another performance criteria

15:30.960 --> 15:33.560
that does not impinge on the existing one that we know

15:33.560 --> 15:36.040
and know how to optimize.

15:36.040 --> 15:37.840
So there are many fascinating phenomena

15:37.840 --> 15:40.440
that arise when you start observing the behavior

15:40.440 --> 15:44.760
of these networks beyond just very standard

15:44.760 --> 15:46.920
and what establish metrics.

15:46.920 --> 15:50.360
A lot of work's been going into trying to understand

15:50.360 --> 15:52.640
how deep learning models work

15:52.640 --> 15:54.960
and understand their internals.

15:54.960 --> 16:01.120
Have you or other previous researchers looked at this idea

16:01.120 --> 16:05.280
of maybe the way the error rate or not error rate,

16:05.280 --> 16:10.080
but the types of errors kind of cluster across this ISO error

16:10.080 --> 16:13.000
rate or accuracy frontier?

16:13.000 --> 16:15.440
Yes, quite a bit of work.

16:15.440 --> 16:18.520
So first of all, it kind of be ambly.

16:18.520 --> 16:20.880
It's fascinating how, you know, 20 years ago

16:20.880 --> 16:23.240
we thought that with math and analysis

16:23.240 --> 16:26.440
we would be informing research neuroscience.

16:26.440 --> 16:29.480
And now we find ourselves doing kind of artificial neuroscience

16:29.480 --> 16:31.680
and probing deep network the way neuroscientists

16:31.680 --> 16:34.880
probe neural network, which is kind of interesting

16:34.880 --> 16:36.400
twist of events.

16:36.400 --> 16:38.240
But yes, we've been looking at that at various stages.

16:38.240 --> 16:41.160
So one is with former postdoc of mine,

16:41.160 --> 16:42.400
his name is Hussein Mobahi.

16:42.400 --> 16:45.320
We were looking at universal adversarial perturbations

16:45.320 --> 16:47.680
where we realized that if you take the data

16:47.680 --> 16:51.800
and perturb them in a way that hits the closest decision

16:51.800 --> 16:54.040
boundaries so that with the smallest possible perturbation

16:54.040 --> 16:57.480
you change the class and you fool the network so to speak.

16:57.480 --> 17:00.200
All of these perturbations are aligned,

17:00.200 --> 17:02.560
which is very mysterious.

17:02.560 --> 17:05.360
And that might be in what sense?

17:05.360 --> 17:07.880
Say, again, aligned in what sense?

17:07.880 --> 17:09.680
They're aligned in the sense that their direction

17:09.680 --> 17:12.400
in the high dimensional space of representations

17:12.400 --> 17:14.400
is parallel, they're parallel to each other.

17:14.400 --> 17:16.960
So that you can find a single perturbations

17:16.960 --> 17:19.920
that apply to all the data with high probability changes

17:19.920 --> 17:21.960
in the class at the output.

17:21.960 --> 17:26.120
So that says something about the structure

17:26.120 --> 17:27.320
of the decision boundaries.

17:27.320 --> 17:28.960
There are regions of high curvature.

17:28.960 --> 17:31.840
And so it's very different from what we had in mind

17:31.840 --> 17:33.640
kind of coming from stand there,

17:33.640 --> 17:37.280
the dimensional classic fires, like SPM and so on.

17:37.280 --> 17:39.520
There was another aspect that was really puzzling to me.

17:39.520 --> 17:40.960
So this was when Alessandro Aquila,

17:40.960 --> 17:43.440
who's a scientist at AWS, now,

17:43.440 --> 17:45.040
the one who would still a student with a friend

17:45.040 --> 17:49.120
from Harvard near science, they had this conjecture

17:49.120 --> 17:52.800
that neural networks exhibit critical learning periods.

17:52.800 --> 17:54.360
Now, what is critical in a period?

17:54.360 --> 17:58.160
So in biological systems, either you learn a skill

17:58.160 --> 18:00.320
when you're young or you don't learn.

18:00.320 --> 18:02.920
This is why you cannot teach all-down new tricks.

18:02.920 --> 18:06.720
And this is why if you're born with a defect like cataract

18:06.720 --> 18:11.080
or with severe myopia, unless you correct it early,

18:11.080 --> 18:13.000
no matter how much time you have to recoup,

18:13.000 --> 18:14.880
you never learn, right?

18:14.880 --> 18:17.720
So the optical defect is fixed.

18:17.720 --> 18:21.280
So it's a result, but your brain has not learned correctly,

18:21.280 --> 18:22.960
and then you never are correct.

18:22.960 --> 18:26.440
So, and this is normally attributed to biology,

18:26.440 --> 18:28.040
to biochemistry of the brain, you know,

18:28.040 --> 18:31.480
you stop generating synapses and so you age.

18:31.480 --> 18:33.080
But neural networks don't age.

18:33.080 --> 18:35.360
Their connectivity is fixed at the outset, it doesn't change.

18:35.360 --> 18:38.200
So I told you, you know, this is, you guys are crazy,

18:38.200 --> 18:42.240
this is why would you ever expect the neural network

18:42.240 --> 18:43.840
would have a behavior like that?

18:43.840 --> 18:46.160
It turns out it does, and which is really puzzling

18:46.160 --> 18:49.120
because now you want, okay, now it cannot be biochemistry

18:49.120 --> 18:52.920
because an artificial neural network doesn't have any.

18:52.920 --> 18:54.360
There's a very resemblance to the brain,

18:54.360 --> 18:57.760
but really, this phenomenon must be an information phenomenon.

18:57.760 --> 18:59.240
And then we start to dig into say, okay,

18:59.240 --> 19:01.960
what is even meaning information in a deep network?

19:01.960 --> 19:02.800
You know, what?

19:02.800 --> 19:05.080
A deep network is a deterministic system.

19:05.080 --> 19:07.560
So as zero entropy, the weights are fixed, right?

19:07.560 --> 19:09.960
The input output map, one strain is a deterministic.

19:09.960 --> 19:12.240
So as infinite mutual information is in the input and output.

19:12.240 --> 19:13.920
So all of the standard concept,

19:13.920 --> 19:16.920
information theory, not useful, and they're not useful

19:16.920 --> 19:21.440
to probe the inside, the guts of the network.

19:21.440 --> 19:23.360
And so we spend a lot of time defining

19:23.360 --> 19:25.480
and measuring information quantities

19:25.480 --> 19:26.680
in these gigantic networks,

19:26.680 --> 19:28.040
in hundreds of millions of parameters

19:28.040 --> 19:30.040
and now even trillions.

19:30.040 --> 19:31.960
And what we discover, for instance,

19:31.960 --> 19:36.160
is that I don't know if you remember the movie,

19:36.160 --> 19:38.480
the eternal sunshine of the spotless mind.

19:38.480 --> 19:43.480
I don't know if you remember it.

19:43.480 --> 19:44.480
For whatever reason,

19:44.480 --> 19:47.080
it wants to forget, experience it to a person or a partner.

19:47.080 --> 19:49.080
And so it goes to a company called La Cuna

19:49.080 --> 19:51.760
that under, you know, there's something,

19:51.760 --> 19:54.240
you know, just showing some pictures of the partner,

19:54.240 --> 19:56.960
zap the brain to erase memory of it.

19:56.960 --> 19:59.240
So we thought, well, maybe we can do that with deep networks,

19:59.240 --> 20:00.080
right?

20:00.080 --> 20:03.560
We can zap the brain to forget or to erase memory

20:03.560 --> 20:06.560
of something that you saw in your training set.

20:06.560 --> 20:08.720
And it turns out that that is possible to do with deep networks

20:08.720 --> 20:11.360
because once you understand how information is

20:11.360 --> 20:15.120
defined and computed and distributed in the representation,

20:15.120 --> 20:18.600
then you can inject noise in very specific direction

20:18.600 --> 20:22.400
that will force you to erase a particular datum

20:22.400 --> 20:25.040
or a class or a cohort of data,

20:25.040 --> 20:26.160
which now is also important

20:26.160 --> 20:28.000
because of privacy issues and so on and so forth.

20:28.000 --> 20:30.640
So there's a lot of fascinating questions

20:30.640 --> 20:33.920
that arise when you try to understand how these

20:33.920 --> 20:36.720
deep networks operate and you have existence proofs

20:36.720 --> 20:39.880
of what is possible to do thanks to biology,

20:39.880 --> 20:42.200
the human visual system, the animal visual system.

20:42.200 --> 20:45.760
So there is definitely a lot more back and forth

20:45.760 --> 20:48.720
between the biological inspiration and the analysis

20:48.720 --> 20:52.080
than there was 20 years ago when we thought, you know,

20:52.080 --> 20:55.600
that maybe we'll solve the brain, you know, with analysis.

20:55.600 --> 20:59.440
And somebody in the 90s told me that, you know,

20:59.440 --> 21:01.240
if you think of science and understanding

21:01.240 --> 21:04.600
is a process of compression, you know, observe the astra

21:04.600 --> 21:07.120
and you could record their positions,

21:07.120 --> 21:09.600
but once you understand the laws of physics,

21:09.600 --> 21:11.560
you can compress them into a law.

21:11.560 --> 21:13.560
And maybe the brain is the most compressed possible

21:13.560 --> 21:14.520
representation of itself.

21:14.520 --> 21:16.960
There is no easier brain of representation of brain

21:16.960 --> 21:18.680
than the brain itself.

21:18.680 --> 21:20.000
And if it's true for deep networks,

21:20.000 --> 21:23.120
then how do we leverage, you know,

21:23.120 --> 21:26.400
our reductionist scientific method

21:26.400 --> 21:29.520
has not been successful in this particular area.

21:29.520 --> 21:31.400
So we need a more holistic approach

21:31.400 --> 21:32.760
to define a measure information.

21:32.760 --> 21:35.480
And then once you do that, you realize that

21:35.480 --> 21:40.480
despite the gigantic dimension of these spaces,

21:41.280 --> 21:44.520
the amount of information that they store is a tiny fraction.

21:44.520 --> 21:46.400
And the way in which they store it is fascinating.

21:46.400 --> 21:49.560
I love it because I can claim that I'm still learning

21:49.560 --> 21:52.280
even if I'm aging because these networks at the beginning

21:52.280 --> 21:55.120
accrue a lot of information, sort of they memorize.

21:55.120 --> 21:57.480
And then they start shedding information,

21:57.480 --> 21:59.000
throwing away information.

21:59.000 --> 22:02.040
And they do this while improving the expected error

22:02.040 --> 22:04.720
or the test error in the data site that you have sequestered.

22:04.720 --> 22:07.080
So in a sense, it seems like forgetting

22:07.080 --> 22:10.760
or throwing away information is a necessary part of learning,

22:10.760 --> 22:12.600
which when I talk to biology, they say,

22:12.600 --> 22:15.760
oh, of course, but I've never seen a written math.

22:15.760 --> 22:20.240
I've never seen a claim that is defensible based on data

22:20.240 --> 22:21.080
arise from that.

22:21.080 --> 22:22.840
So it's really, there's a lot of interplay

22:22.840 --> 22:25.200
between understanding biological networks

22:25.200 --> 22:30.200
and understanding artificial networks.

22:30.200 --> 22:35.160
Yeah, it seems like there would be once you've established

22:35.160 --> 22:40.200
that there are many, many levers that folks are using

22:40.240 --> 22:43.560
during the training process, the batch sizes

22:43.560 --> 22:46.160
and learning rates and cyclical learning rates,

22:46.160 --> 22:49.920
all of these things that if they're ultimately correlated

22:49.920 --> 22:52.880
to the rate at which the network learns

22:52.880 --> 22:57.880
and how things are forgotten would be even more impactful

22:58.040 --> 23:00.880
than was originally believed.

23:00.880 --> 23:02.840
Yes, that's a great point.

23:02.840 --> 23:04.440
One thing that we understood is the following.

23:04.440 --> 23:08.240
So all of these inducted biases that you mentioned,

23:08.240 --> 23:11.080
which could be in the class of functions with polling

23:11.080 --> 23:15.040
or could be in the optimization with SGD,

23:15.040 --> 23:16.600
with the choice of the batch size

23:16.600 --> 23:18.520
and the learning rate and so on and so forth,

23:18.520 --> 23:21.560
or they could be in explicit regularizer.

23:21.560 --> 23:25.240
So all of these processes are some type of regularization,

23:25.240 --> 23:27.520
meaning that these are added terms,

23:27.520 --> 23:29.680
whether explicit or implicit to your training process

23:29.680 --> 23:31.960
so that you don't just minimize the empirical error

23:31.960 --> 23:33.480
otherwise you would overfit,

23:33.480 --> 23:35.200
but you optimize something that hopefully

23:35.200 --> 23:36.200
will allow you to generalize

23:36.200 --> 23:39.040
and they can be computed using, for instance,

23:39.040 --> 23:42.000
a spec based bound and now we know how to compute

23:42.000 --> 23:44.640
the information terms in the spec based balance and so forth.

23:44.640 --> 23:47.480
What we discover, however, is that normally

23:47.480 --> 23:50.320
you think of regularization as a process

23:50.320 --> 23:54.240
that schools or regularizes your loss function.

23:54.240 --> 23:56.880
And this is not what's happening in deep learning.

23:56.880 --> 23:59.680
So there is a paper that we wrote with an internet AWS

23:59.680 --> 24:02.920
at NewRips two years ago, which is titled

24:04.600 --> 24:08.080
Time Matters in regularizing deep networks.

24:08.080 --> 24:09.960
And what it means is a following.

24:09.960 --> 24:14.160
So the intern Aditya Goldakkar did the following experiment.

24:14.160 --> 24:16.680
He took standard regularizers that people use,

24:16.680 --> 24:19.760
for instance, WADK and data augmentation.

24:19.760 --> 24:24.760
And then he had a few epochs of training

24:24.840 --> 24:28.600
without regularization and then turn on regularization.

24:28.600 --> 24:32.080
So asymptotically, the loss function is regularized.

24:32.080 --> 24:34.080
The network would not behave at all.

24:34.080 --> 24:36.400
Vice versa, turn on the regularizer,

24:36.400 --> 24:38.040
let the network converge for a few epochs,

24:38.040 --> 24:40.240
then turn off the regularizer.

24:40.240 --> 24:43.080
So the asymptotic loss is as irregular

24:43.080 --> 24:45.280
as if it never saw regularization.

24:45.280 --> 24:48.080
It only saw regularization during the initial transit.

24:48.080 --> 24:50.400
Yet generalization power is just as good

24:50.400 --> 24:52.520
as the regularized all the way.

24:52.520 --> 24:56.280
So it appears that regularization does not affect

24:56.280 --> 25:00.560
the geology and the geometry of the loss function

25:00.560 --> 25:01.400
at convergence.

25:01.400 --> 25:03.240
So the asymptotic is really not that important.

25:03.240 --> 25:04.960
It's all in the transient.

25:04.960 --> 25:08.080
Regularization affects what bottlenecks in the loss landscape

25:08.080 --> 25:10.200
you can manage to get into.

25:10.200 --> 25:11.480
And then critical and imperial,

25:11.480 --> 25:13.080
which is what we were talking about earlier,

25:13.080 --> 25:15.760
tells you that once you get into one of this bottleneck

25:15.760 --> 25:18.440
and onto a wide valley, and people talk about wide,

25:18.440 --> 25:20.720
minimum, and so on and so forth, flat, minimum,

25:20.720 --> 25:22.960
it's very difficult to come back from that.

25:22.960 --> 25:25.400
So if you enter during the initial transit,

25:25.400 --> 25:27.880
enter the wrong valley, which you could do

25:27.880 --> 25:29.880
is if you're trained with the wrong data, for instance,

25:29.880 --> 25:34.120
because your parents didn't realize that you were myopic

25:34.120 --> 25:35.640
and so they didn't put your glasses on

25:35.640 --> 25:37.760
until you were five, six years old.

25:37.760 --> 25:41.120
At that point, you cannot get back out from that valley

25:41.120 --> 25:44.240
and you will never learn how to see correctly, right?

25:44.240 --> 25:45.920
And this is for a variety of skills,

25:45.920 --> 25:49.920
for a variety of species from songbirds to walking

25:49.920 --> 25:54.440
to deep neural networks, which is puzzling and fascinating.

25:54.440 --> 25:56.320
The analogy that comes to mind for me,

25:56.320 --> 25:59.400
and I may be bluttering my undergraduate material science,

25:59.400 --> 26:02.280
but is one of a kneeling where you can have two materials

26:02.280 --> 26:07.040
that are functionally equivalent by structure,

26:07.040 --> 26:10.280
overall structure, but their internal structure

26:10.280 --> 26:12.800
is different because of the way that they've been created

26:12.800 --> 26:16.480
in a via heating and cooling cycles and things like that.

26:16.480 --> 26:19.520
And that reminds me of a point that you made in the blog post,

26:19.520 --> 26:23.960
which is talking about model compression,

26:23.960 --> 26:28.600
which is I think illustrative of this entire conversation.

26:28.600 --> 26:33.960
The point was that in model compression,

26:33.960 --> 26:36.040
we often think of it as simply trying

26:36.040 --> 26:40.240
to find a model with equivalent error that is smaller

26:40.240 --> 26:42.080
or meet some other set of constraints.

26:42.080 --> 26:45.120
But the architecture, what I'm taking

26:45.120 --> 26:49.440
to be the structure of the models matters a lot

26:49.440 --> 26:50.920
for the reasons that we've talked about.

26:50.920 --> 26:52.640
Can you elaborate a bit on that?

26:52.640 --> 26:55.360
Yeah, so we have this concept that we call information

26:55.360 --> 26:59.480
plasticity as analogous to neuronal plasticity.

26:59.480 --> 27:02.320
Neural plasticity is where neurons forms in absence,

27:02.320 --> 27:03.920
form, you know, form dendrites.

27:03.920 --> 27:07.200
And so they physically change configuration.

27:07.200 --> 27:09.160
In deep neural network, the configuration is fixed.

27:09.160 --> 27:12.160
However, what you notice when the network is staying

27:12.160 --> 27:16.480
is that some parameters don't matter at all,

27:16.480 --> 27:19.360
meaning that if you take that parameter and change it,

27:19.360 --> 27:23.400
liberally, the input-out behavior does not change.

27:23.400 --> 27:26.040
So you could replace that parameter with noise

27:26.040 --> 27:27.800
or you could replace it with zero.

27:27.800 --> 27:30.640
So this is called pruning.

27:30.640 --> 27:34.600
And observe no input-out would changes.

27:34.600 --> 27:37.480
Changes in the input-out would behavior.

27:37.480 --> 27:41.560
And so it appears, however, that during the initial transient,

27:41.560 --> 27:43.600
the amount of, so you could say also that this parameter

27:43.600 --> 27:46.240
doesn't have any information, because you could store it

27:46.240 --> 27:48.520
with zero bits, and you will never know the difference.

27:48.520 --> 27:49.840
Vice versa, because if you have a parameter,

27:49.840 --> 27:51.560
do you change it a little bit?

27:51.560 --> 27:52.880
I know the subtle behavior changes

27:52.880 --> 27:54.280
that has a lot of information.

27:54.280 --> 27:56.880
So you want to store it with a large number of bits.

27:56.880 --> 28:01.760
So, but it turns out that during the initial transient,

28:01.760 --> 28:04.680
the information counted as number of bits per weight

28:04.680 --> 28:07.000
moves around from the different layers, you know,

28:07.000 --> 28:10.360
so the crosses and goes from upper layer to lower layers

28:10.360 --> 28:11.440
until it settles.

28:11.440 --> 28:14.240
Beyond a certain point, it cannot move.

28:14.240 --> 28:17.000
And so beyond certain points, weights that are uninformative,

28:17.000 --> 28:20.480
stay uninformative, even if you change the training set.

28:20.480 --> 28:22.800
And so that's really the structure of the network.

28:22.800 --> 28:25.440
So it appears that the effective connectivity,

28:25.440 --> 28:27.200
which is not the physical connectivity,

28:27.200 --> 28:30.040
because it's, you know, that's the term in the outside.

28:30.040 --> 28:32.080
But the effective connectivity as measure

28:32.080 --> 28:34.520
by the amount of information that that connection carries,

28:34.520 --> 28:36.120
which we now know how to measure,

28:36.120 --> 28:38.360
after several years of work.

28:38.360 --> 28:39.720
So that is very important.

28:39.720 --> 28:41.960
And if you knew that at the beginning,

28:41.960 --> 28:43.480
okay, you could train with a smaller network.

28:43.480 --> 28:45.280
The problem is that to get there,

28:45.280 --> 28:47.400
you need to go through this very high bump

28:47.400 --> 28:48.400
in information in the network,

28:48.400 --> 28:51.280
which needs the high-dimensional space

28:51.280 --> 28:53.880
and needs a large number of parameters.

28:53.880 --> 28:56.720
And so yeah, so and there are,

28:56.720 --> 28:59.000
like I was suggesting, statistical physicists

28:59.000 --> 29:00.640
and physicists in general are very fascinating

29:00.640 --> 29:01.880
with some of these questions,

29:01.880 --> 29:05.240
because they have thought about high-dimensional systems,

29:05.240 --> 29:07.000
including spin glasses and so on.

29:09.000 --> 29:11.000
So we diverged pretty quickly

29:11.000 --> 29:15.080
from the specifics of the graceful AI set of papers.

29:15.080 --> 29:18.720
But I want to return to those and dig in deeper.

29:18.720 --> 29:22.080
You talked about kind of how two models

29:22.080 --> 29:25.200
with the same accuracy can have very different types

29:25.200 --> 29:29.960
of errors and how from a user perspective,

29:29.960 --> 29:31.880
that might be frustrating.

29:31.880 --> 29:33.800
They may get used to a certain type of error.

29:33.800 --> 29:35.960
Now the model changes and all of a sudden,

29:35.960 --> 29:39.080
the behavior that they're used to is different.

29:39.080 --> 29:44.080
And we want to, well, I guess in introducing this,

29:45.040 --> 29:48.680
you focus on the idea of regression

29:48.680 --> 29:52.680
and meaning things that were working previously

29:52.680 --> 29:54.560
don't break.

29:55.760 --> 29:59.440
But are there kind of broader types of continuity

29:59.440 --> 30:02.080
that a customer might come to expect?

30:02.080 --> 30:04.600
And have you looked at that, or to what degree

30:04.600 --> 30:05.920
have you looked at that?

30:05.920 --> 30:06.760
Yes, we have.

30:06.760 --> 30:08.320
In fact, when we started this project,

30:08.320 --> 30:12.440
we thought it was a narrow set of use cases where,

30:12.440 --> 30:14.800
let's say you have a photo collection.

30:14.800 --> 30:18.520
And now you update to the latest software,

30:18.520 --> 30:21.760
and which is, of course, much better than the one before,

30:21.760 --> 30:25.240
except now you're searching for a picture of your cousin

30:25.240 --> 30:27.360
that you were able to find before and now they don't

30:27.360 --> 30:29.160
show up in the search and what's going on.

30:29.160 --> 30:30.200
So that's the regression, right?

30:30.200 --> 30:33.600
So that you are frustrated as an individual user

30:33.600 --> 30:35.760
because something was working before

30:35.760 --> 30:38.160
and it's not working now, something broke.

30:38.160 --> 30:40.200
So as soon as we came up with this,

30:40.200 --> 30:42.720
we discovered that there's a host of other related problems.

30:42.720 --> 30:46.840
For instance, we didn't realize that it is exactly the same problem

30:46.840 --> 30:51.000
as a cross-model compatibility where you have a service

30:51.000 --> 30:52.560
that runs on different platforms, let's say,

30:52.560 --> 30:55.040
the smartphone as well as on the cloud.

30:55.040 --> 30:57.720
And of course, on the cloud, you prioritize performance.

30:57.720 --> 31:01.560
You really have no constraint on how big the model is.

31:01.560 --> 31:03.480
But on your phone, you don't want to keep the battery

31:03.480 --> 31:05.800
in no time, and so you run a smaller model.

31:05.800 --> 31:07.360
So what do you do?

31:07.360 --> 31:11.480
Do you run the best you can do with the hardware

31:11.480 --> 31:14.120
you have on the phone, regardless of what the big model

31:14.120 --> 31:15.480
on the cloud does?

31:15.480 --> 31:17.680
Or do you try to train the model on the phone

31:17.680 --> 31:21.080
in a way that resembles or mimics the model in the cloud

31:21.080 --> 31:23.480
in a way that can be defined?

31:23.480 --> 31:26.120
And if you do that, then your trade space

31:26.120 --> 31:30.840
is not just the parameters for a children architecture.

31:30.840 --> 31:32.800
You also can optimize over the architectures,

31:32.800 --> 31:35.440
and it becomes a hybrid search space,

31:35.440 --> 31:37.480
over a continuous space of weights,

31:37.480 --> 31:39.280
as well as the discrete space of architectures.

31:39.280 --> 31:42.480
And so that, we thought, was a different problem,

31:42.480 --> 31:45.360
but it turns out to be exactly the same problem.

31:45.360 --> 31:49.720
Same thing with languages, we have services that build chat

31:49.720 --> 31:50.640
bots.

31:50.640 --> 31:53.640
And so the train model sits at the center,

31:53.640 --> 31:56.240
but then customers build up on top of this model,

31:56.240 --> 32:00.400
very elaborate post-processing workflows.

32:00.400 --> 32:05.440
And even changes that you would think are innocuous

32:05.440 --> 32:06.960
can break this post-processing.

32:06.960 --> 32:09.920
And you really don't know a priority,

32:09.920 --> 32:11.440
what is and what is not doable.

32:11.440 --> 32:13.920
And so there is a whole space of dimensions

32:13.920 --> 32:16.880
along which you may want to impose either constraints

32:16.880 --> 32:19.880
or optimize together with the average error.

32:19.880 --> 32:23.280
And so for the past years, we've seen dozens of cases

32:23.280 --> 32:26.560
across different applications at AWS,

32:26.560 --> 32:29.120
where we see this problem arises.

32:29.120 --> 32:33.680
It's interesting that when we first published this paper,

32:33.680 --> 32:39.200
it was not reviewed positively, because, you know,

32:39.200 --> 32:41.600
well, there's no comparison with anything,

32:41.600 --> 32:45.640
but yes, because this is not a nobody

32:45.640 --> 32:47.040
has to look at this problem before.

32:47.040 --> 32:49.160
So it's really one of the most exciting times

32:49.160 --> 32:52.240
when you're not just having a new solution to an old problem,

32:52.240 --> 32:55.680
but you open up a new problem that one hand

32:55.680 --> 32:58.520
is exciting for scientists to send their teeth in.

32:58.520 --> 33:00.080
On the other, it can be beneficial

33:00.080 --> 33:03.160
because it can also help democratize the use of AI.

33:03.160 --> 33:06.760
Because right now, many customers are

33:06.760 --> 33:09.480
sitting by the sideline as new and improved models

33:09.480 --> 33:11.840
are coming by just because they don't want to face

33:11.840 --> 33:15.080
the issue of having to redo all the post-processing

33:15.080 --> 33:15.920
and so on and so forth.

33:15.920 --> 33:19.000
Let alone the cost of repressing large galleries,

33:19.000 --> 33:21.000
because if you have a photo collection,

33:21.000 --> 33:23.120
when I come in with a new model,

33:23.120 --> 33:25.400
you have to reprocess every single image in your gallery

33:25.400 --> 33:27.680
through the new model so that you can recreate an index

33:27.680 --> 33:29.160
to search.

33:29.160 --> 33:32.000
And also, if you have hundreds of thousands of images,

33:32.000 --> 33:33.760
that's OK, but if you have billions, OK,

33:33.760 --> 33:38.160
that starts becoming a little bit more complex.

33:38.160 --> 33:42.200
And so it turns out that both the regression problem

33:42.200 --> 33:46.600
and the reprocessing problem are solved in the same way,

33:46.600 --> 33:51.200
and that is by utilizing the existing model.

33:51.200 --> 33:54.200
Can you talk more about the way you approach solving the problem?

33:54.200 --> 33:57.960
Yeah, so the backward compatibility problem

33:57.960 --> 34:01.360
is fairly simple to formalize and the solutions

34:01.360 --> 34:04.440
that we have to pose are very simple to implement.

34:04.440 --> 34:07.160
And now the backward compatibility problem, the regression.

34:07.160 --> 34:08.320
Yes, the backward compatibility problem

34:08.320 --> 34:10.080
is where you have an old model.

34:10.080 --> 34:11.880
You replace it with a new model.

34:11.880 --> 34:13.880
And you would like, as in the case, for instance,

34:13.880 --> 34:16.040
of your photo collection, to be able to use

34:16.040 --> 34:17.760
the old model to search old pictures

34:17.760 --> 34:20.640
without having to reprocess them through the new model.

34:20.640 --> 34:24.560
OK, and so one way to do that is to optimize

34:24.560 --> 34:27.640
over a new backbone, a new model that

34:27.640 --> 34:31.400
has bigger, larger number of parameters,

34:31.400 --> 34:33.200
different architecture, completely different,

34:33.200 --> 34:37.400
except that model is biased to use the classifier

34:37.400 --> 34:38.920
that the old model used.

34:38.920 --> 34:41.600
So it says you force it to live in the same metric space

34:41.600 --> 34:43.760
where you can do clustering, where you can do search,

34:43.760 --> 34:44.840
and so on and so forth.

34:44.840 --> 34:47.880
Yeah, just to interject, is that you

34:47.880 --> 34:50.320
might think that, OK, my typical practice

34:50.320 --> 34:52.440
might be to start with my old model

34:52.440 --> 34:55.960
and freeze the weights of my classifier or something

34:55.960 --> 35:01.520
like that and just retrain and fine tune based on new data.

35:01.520 --> 35:05.520
Is that inadequate to assure the kind of backward compatibility

35:05.520 --> 35:06.760
and the errors?

35:06.760 --> 35:08.560
So you can do backward compatibility

35:08.560 --> 35:11.920
by just taking the same architecture and retraining it

35:11.920 --> 35:14.720
with the same classifier that you're fine.

35:14.720 --> 35:18.520
But if you follow the literature, every few months

35:18.520 --> 35:20.720
a new architecture comes by, and you

35:20.720 --> 35:23.160
want to harvest benefits of that.

35:23.160 --> 35:25.840
And so in that case, these two representations

35:25.840 --> 35:26.960
that are in fit to a classifier

35:26.960 --> 35:29.560
live in different spaces, spaces of different dimensions.

35:29.560 --> 35:32.320
So you cannot even compare them.

35:32.320 --> 35:35.840
And so, but you can, however, force the network

35:35.840 --> 35:38.720
to develop a representation that is

35:38.720 --> 35:41.120
mathematically compatible with the old classifier.

35:41.120 --> 35:45.280
So you can compute distances, compute angles, and so on.

35:45.280 --> 35:51.320
So that becomes a fairly cleanly formalized problem

35:51.320 --> 35:54.480
and that you can attack with standard methods

35:54.480 --> 35:56.000
of machine learning.

35:56.000 --> 35:58.000
Now, the positive congruent training,

35:58.000 --> 36:00.080
which is the training done in a way

36:00.080 --> 36:02.400
that minimizes the regression errors,

36:02.400 --> 36:04.240
that's a more amorphous problem.

36:04.240 --> 36:08.000
Because depending on what type of errors you want to avoid,

36:08.000 --> 36:09.640
it may take different shapes.

36:09.640 --> 36:12.320
And something that is still puzzling us

36:12.320 --> 36:20.520
is that the current best performing model for positive congruent

36:20.520 --> 36:23.600
training is one that does not explicitly

36:23.600 --> 36:27.960
enforce that you make a mistake on a certain course of data.

36:27.960 --> 36:33.880
But that is trained using ensembles, which don't know

36:33.880 --> 36:34.880
anything about each other.

36:34.880 --> 36:37.480
And they don't know anything about the old model.

36:37.480 --> 36:40.760
So we call this future proofing, because training with this

36:40.760 --> 36:45.520
ensures that later there will be fewer of what we call negative flips.

36:45.520 --> 36:51.120
Negative flips are data points for which the decision

36:51.120 --> 36:54.000
before it becomes wrong.

36:54.000 --> 36:56.880
So there are definitely very open scientific questions

36:56.880 --> 36:57.920
that need to be understood.

36:57.920 --> 37:01.600
We just put there the first seed.

37:01.600 --> 37:03.680
But definitely there's a lot to be understood

37:03.680 --> 37:08.280
that we and others are working on acting.

37:08.280 --> 37:10.560
And so what do these ensembles look like?

37:10.560 --> 37:13.280
What are the components of the ensembles?

37:13.280 --> 37:15.520
The ensembles could be the same architecture

37:15.520 --> 37:17.360
training different ways, or with different breakdown

37:17.360 --> 37:19.920
of the data, or complete different architectures.

37:19.920 --> 37:23.920
The only problem is that the ensemble is not viable in practice,

37:23.920 --> 37:26.760
because if at different time you want

37:26.760 --> 37:30.000
to run an ensemble of 10 models, your cost of interest

37:30.000 --> 37:31.320
will get multiplied by 10.

37:31.320 --> 37:34.040
And that's something that the customers will not accept

37:34.040 --> 37:35.400
gladly, right?

37:35.400 --> 37:37.120
So we really need to figure out ways

37:37.120 --> 37:39.680
to perform positive congruent training

37:39.680 --> 37:42.320
at exactly the same cost as running one model,

37:42.320 --> 37:43.920
not an ensemble of models.

37:43.920 --> 37:47.160
So it says it's a paragon of performance,

37:47.160 --> 37:49.400
because right now it's achieved the best performance,

37:49.400 --> 37:53.080
but it's not a viable target for deployment,

37:53.080 --> 37:55.840
because it multiplies the cost by an integer multiple,

37:55.840 --> 37:58.480
and that's not, you know, it's a nice target.

37:58.480 --> 38:01.440
So, you know, this is a problem.

38:01.440 --> 38:04.680
And so when you say that the ensembles can be,

38:06.200 --> 38:07.880
it can be, you didn't say anything,

38:07.880 --> 38:09.720
but that, you know, there's some,

38:09.720 --> 38:13.920
you made it sound like the ensemble was not constructed

38:13.920 --> 38:15.960
in a particular way that had this property,

38:15.960 --> 38:19.040
but rather the act of using ensembles

38:19.040 --> 38:21.720
had kind of a regularization type of effect

38:21.720 --> 38:25.080
that addressed PCT.

38:25.080 --> 38:26.560
Correct, correct.

38:26.560 --> 38:29.800
And one would be induced in thinking

38:29.800 --> 38:31.120
that because ensembles work,

38:31.120 --> 38:33.440
then these negative flips are points

38:33.440 --> 38:35.720
that are very close to the decision boundary.

38:35.720 --> 38:37.400
So when you train different models,

38:37.400 --> 38:38.560
the boundary jitters around,

38:38.560 --> 38:39.680
and so these points flip.

38:39.680 --> 38:43.760
But in fact, we discover that many of the points,

38:43.760 --> 38:46.320
data points that flip are actually very far

38:46.320 --> 38:47.360
from the decision boundary.

38:47.360 --> 38:50.000
They are very high confidence data points.

38:50.000 --> 38:51.880
And so you're making high confidence mistakes,

38:51.880 --> 38:53.280
which are the worst kind, right?

38:53.280 --> 38:56.640
Absolutely, it's not knowing that you don't know.

38:56.640 --> 38:57.760
It's a real problem.

38:57.760 --> 38:59.600
It's that don't include an effect and all that.

39:01.600 --> 39:05.680
And so what is the best practical approach

39:05.680 --> 39:07.760
to addressing this issue?

39:08.680 --> 39:13.680
Yeah, so right now we have a form of what we call

39:13.920 --> 39:15.320
focal distillation.

39:16.360 --> 39:18.080
Distillation is where you train a model

39:18.080 --> 39:19.560
and then you train another model to mimic

39:19.560 --> 39:21.880
the input of behavior of the first,

39:21.880 --> 39:23.560
but that model could be smaller or could have

39:23.560 --> 39:25.160
other characteristics.

39:25.160 --> 39:28.840
And of course, you don't want to imitate the old model

39:28.840 --> 39:31.480
because then you would inherit also the mistakes.

39:31.480 --> 39:34.000
You want to imitate the old model only

39:34.000 --> 39:36.000
where the old model gets it right.

39:36.000 --> 39:38.200
And otherwise, you want to have the freedom

39:38.200 --> 39:41.080
to optimize according to the new architecture,

39:41.080 --> 39:45.040
better data, more balanced data and whatnot.

39:45.040 --> 39:48.840
So is there a difference between distillation,

39:48.840 --> 39:51.760
broadly and student teacher type of an approach?

39:51.760 --> 39:54.200
Yeah, so it's a general umbrella of methods

39:54.200 --> 39:56.040
that go under the name of distillation

39:56.040 --> 39:57.240
or student teacher models.

39:57.240 --> 39:58.240
Yes.

39:58.240 --> 40:00.760
And this is a particular class of them

40:00.760 --> 40:03.920
where you don't just try to mimic the behavior of the model,

40:03.920 --> 40:05.520
but you try to mimic the behavior of the model

40:05.520 --> 40:09.520
restricted to the course of data that interests you.

40:09.520 --> 40:11.760
Now because typically, this course of data,

40:11.760 --> 40:15.000
including the negative flips, is a tiny minority

40:15.000 --> 40:16.200
of the total.

40:16.200 --> 40:18.720
I mean, tiny could be in the order of five, six, seven percent.

40:18.720 --> 40:21.600
So it is significant if you account for the fact

40:21.600 --> 40:25.560
that people fight very hard for a 1% performance improvement.

40:25.560 --> 40:27.320
And all of a sudden, you're squander 7%

40:27.320 --> 40:29.400
because of these negative flips.

40:29.400 --> 40:31.520
And so, but if you just do distillation

40:31.520 --> 40:34.600
on 7% of the data, that would be lost in the general loss

40:34.600 --> 40:35.600
function.

40:35.600 --> 40:37.200
So you have to do distillation in a clever way.

40:37.200 --> 40:40.200
That's what we call a focal distillation.

40:40.200 --> 40:42.440
But you know, it's, again, it's an active area of research

40:42.440 --> 40:44.960
and we both read about methods that are coming up

40:44.960 --> 40:45.960
that improve on that.

40:45.960 --> 40:47.640
And we also have internal efforts

40:47.640 --> 40:50.880
that are aimed at improving that.

40:50.880 --> 40:56.720
It sounds coarsely like a compositive distillation

40:56.720 --> 40:59.200
and like active learning where you're

40:59.200 --> 41:03.760
trying to identify the most important data

41:03.760 --> 41:05.920
to train the model on.

41:05.920 --> 41:07.440
Yeah, that's an important point.

41:07.440 --> 41:09.680
We very strongly believe in active learning.

41:09.680 --> 41:13.160
We haven't quite been able to make it work the way

41:13.160 --> 41:14.800
we would like it to work.

41:14.800 --> 41:17.520
But yeah, there's so many open questions.

41:17.520 --> 41:21.040
We could spend the rest of the afternoon on this.

41:21.040 --> 41:24.960
But these are all very good questions.

41:24.960 --> 41:32.920
And so you started with this was a problem that came up

41:32.920 --> 41:36.600
in the context of customer challenges.

41:36.600 --> 41:42.200
Have you, is the solution in front of customers

41:42.200 --> 41:45.560
or is this still at the research frontier

41:45.560 --> 41:48.360
and working its way towards practice?

41:48.360 --> 41:50.640
It's both so that we are still working at the frontier.

41:50.640 --> 41:52.680
But some of our backup compatible training

41:52.680 --> 41:54.840
are now in the hands of customers.

41:54.840 --> 41:59.040
So typically, this is how it works at AWS.

41:59.040 --> 42:03.400
So once you have a solution, it gets deployed to customers

42:03.400 --> 42:05.040
very quickly.

42:05.040 --> 42:07.720
And there are customers that are also engaged in the process

42:07.720 --> 42:09.640
so we can ensure that once we launch something,

42:09.640 --> 42:12.280
it has customer fighting and feedback.

42:12.280 --> 42:18.680
One of the things that you mentioned early on in the conversation

42:18.680 --> 42:23.000
is as well as in your blog post on the topic

42:23.000 --> 42:30.400
is pointing out this idea of the artificial, separate

42:30.400 --> 42:32.880
relationship between training and inference

42:32.880 --> 42:36.480
and kind of speaking to, you know, how the brain is kind

42:36.480 --> 42:42.080
of online learning, are you working in that area

42:42.080 --> 42:42.920
as well?

42:42.920 --> 42:46.040
And I'd love to kind of get your take on, you know,

42:46.040 --> 42:48.160
where we are as a community with regard

42:48.160 --> 42:54.560
to kind of, you know, breaking the artificial separation

42:54.560 --> 42:56.200
between training and learning.

42:56.200 --> 42:57.400
Yeah, that's a great question.

42:57.400 --> 42:59.000
Yeah, that goes under the general umbrella

42:59.000 --> 43:01.960
of lifelong learning or continual learning.

43:01.960 --> 43:04.680
And typically in the literature,

43:04.680 --> 43:08.080
people focus on the problem of catastrophic forgetting,

43:08.080 --> 43:11.000
meaning that as you train with new tasks and your model,

43:11.000 --> 43:12.960
you forget that are things, so how do you avoid that

43:12.960 --> 43:14.000
and so on and so forth?

43:14.000 --> 43:16.120
There are big issues of scale that goes out

43:16.120 --> 43:18.560
because as you train more and more tasks

43:18.560 --> 43:20.680
and more and more models, the question is,

43:20.680 --> 43:22.240
how do you fit them into an architecture?

43:22.240 --> 43:24.720
How do you grow organic or the architecture and so on and so forth?

43:24.720 --> 43:27.280
So there are a lot of very interesting questions.

43:27.280 --> 43:31.640
There's one piece of work that recently that gave us hope

43:31.640 --> 43:36.640
in this arena, which is a work actually on language.

43:36.640 --> 43:44.320
Language is, you know, there's always a small diatribe

43:44.320 --> 43:47.160
between language science team and vision science team

43:47.160 --> 43:51.120
because language is quote unquote easy.

43:51.120 --> 43:53.120
Of course, it's not easy.

43:53.120 --> 43:57.520
But you know, it's a domain that is very large but finite

43:57.520 --> 44:00.640
and the type of nuisance variability to which

44:00.640 --> 44:03.240
language that are subject is very small compared to images.

44:03.240 --> 44:07.040
So, you know, we have a sentence, you can move the words around,

44:07.040 --> 44:09.200
you can misspell them and so on.

44:09.200 --> 44:10.360
But if you have an image of a cat,

44:10.360 --> 44:11.520
there's image of different cats

44:11.520 --> 44:12.280
and the different positive,

44:12.280 --> 44:14.360
illumination, code color and so on and so forth.

44:14.360 --> 44:19.360
So it's infinitely many different embodiments of that concept.

44:20.400 --> 44:25.560
But in languages, one, both the data and the object of inference

44:25.560 --> 44:27.280
live in the same space.

44:27.280 --> 44:29.160
So, which is not a revision.

44:29.160 --> 44:31.280
In vision, you want the labeled cat,

44:31.280 --> 44:34.600
but in the image, there's no, in the physical scene,

44:34.600 --> 44:37.280
there's no labeled cat, you know, there's a pixel.

44:37.280 --> 44:40.360
So within this work in languages called tan,

44:40.360 --> 44:44.560
which is basically takes all language tasks.

44:44.560 --> 44:48.520
There's a variety we took the most common,

44:48.520 --> 44:50.880
maybe it doesn't or so.

44:50.880 --> 44:52.800
And you know, co-reference of the solution,

44:52.800 --> 44:57.960
a 90-tier recognition, semantic role labeling

44:57.960 --> 44:59.520
and so on and so forth.

44:59.520 --> 45:02.080
And Giovanni Paulini, who's a mathematician

45:02.080 --> 45:05.080
who joined our team a couple of years ago,

45:05.080 --> 45:09.440
was able to translate all of these different tasks

45:09.440 --> 45:12.120
into a single task which is to translate

45:12.120 --> 45:15.320
between different augmented languages

45:15.320 --> 45:17.600
where the format of the language

45:17.600 --> 45:19.960
embodies the task, okay?

45:19.960 --> 45:22.640
So by doing that, you have all these different tasks

45:22.640 --> 45:24.080
which you can keep training,

45:24.080 --> 45:26.880
but you're training one model, okay?

45:26.880 --> 45:29.480
And then the format of the query or the format

45:29.480 --> 45:31.280
of the training data specifies

45:31.280 --> 45:33.200
whether you're looking for name 90-tier recognition

45:33.200 --> 45:34.560
or some other task.

45:34.560 --> 45:36.360
And so in language, we're able to do that

45:36.360 --> 45:39.760
in a way that we haven't yet been able to do in vision.

45:41.040 --> 45:44.040
And still, you know, it's only very early stages

45:44.040 --> 45:48.840
of multi-task learning and continual learning

45:48.840 --> 45:51.200
because we still have the pre-training phase

45:51.200 --> 45:52.960
which is artificially separated

45:52.960 --> 45:54.400
where you train a language model

45:54.400 --> 45:56.640
to predict missing words and so on.

45:56.640 --> 45:58.120
So it is very early.

45:58.120 --> 46:01.760
I think that area of investigation will go on for a while.

46:01.760 --> 46:05.680
And but definitely it's an artificial separation,

46:05.680 --> 46:09.440
as you said, that we need to do away with it some point.

46:09.440 --> 46:11.400
We're just not there yet.

46:11.400 --> 46:14.440
And so I don't think I followed the connection

46:14.440 --> 46:17.680
between the format of the language

46:17.680 --> 46:20.600
and the lifelong learning aspect of

46:21.640 --> 46:23.160
that we were originally speaking to.

46:23.160 --> 46:24.440
Can you elaborate on that a bit?

46:24.440 --> 46:29.440
Yeah, so right now, if you train for one task,

46:29.440 --> 46:31.720
the type of knowledge that you acquire

46:31.720 --> 46:32.920
is specific to that task

46:32.920 --> 46:34.880
and you cannot transfer it to a new task

46:34.880 --> 46:37.400
which you learn because if you now take that model

46:37.400 --> 46:38.720
that you train on task one,

46:38.720 --> 46:40.520
now you find you don't task two,

46:40.520 --> 46:42.960
you've lost something with respect to task one.

46:42.960 --> 46:46.080
Now you need to worry about not forgetting something

46:46.080 --> 46:46.920
about task one.

46:46.920 --> 46:48.280
So if you learn them seriously,

46:48.280 --> 46:49.720
you'll have to face this problem

46:49.720 --> 46:50.920
when you start forgetting.

46:50.920 --> 46:54.160
If you learn them simultaneously,

46:54.160 --> 46:57.080
it's also synergistic because now there is knowledge

46:57.080 --> 46:58.600
which is shared across this task.

46:58.600 --> 47:00.800
So if you have small data set on task one,

47:00.800 --> 47:02.040
small data set on task two,

47:02.040 --> 47:04.000
you can train on the union of the sets

47:04.000 --> 47:05.960
and put it as if you have, you know,

47:05.960 --> 47:09.720
Tolkien is an author and he lives in Moscow.

47:09.720 --> 47:10.720
Somewhere in the model,

47:10.720 --> 47:14.520
the information that Moscow is in Russia is there.

47:14.520 --> 47:17.840
And so if you ask the question is Tolkien Russian,

47:18.880 --> 47:20.680
you may be induced and say yes.

47:20.680 --> 47:24.240
So which you wouldn't, if you had trained

47:24.240 --> 47:25.360
for each task individually

47:25.360 --> 47:26.800
and then you don't need to worry about

47:26.800 --> 47:28.200
to get the stuff you're forgetting,

47:28.200 --> 47:29.160
quite the contrary,

47:29.160 --> 47:32.320
you can harvest synergistically information

47:32.320 --> 47:33.320
from different tasks.

47:34.240 --> 47:36.640
Got it, got it, got it.

47:36.640 --> 47:38.000
Very good.

47:38.000 --> 47:41.440
What else is your team excited about?

47:41.440 --> 47:43.600
What else are you focusing on?

47:43.600 --> 47:46.440
Or, you know, speaking broadly about,

47:46.440 --> 47:49.440
speaking broadly about the field,

47:49.440 --> 47:51.200
you know, what things that we haven't talked about

47:51.200 --> 47:53.280
or kind of on your mind?

47:53.280 --> 47:54.120
Yeah.

47:54.120 --> 47:57.520
So, well, first of all, it's a very exciting time

47:57.520 --> 48:00.760
because I don't think there's ever been a time,

48:01.760 --> 48:03.000
certainly not in my memory,

48:03.000 --> 48:07.320
but when you as a scientist have a chance

48:07.320 --> 48:11.160
to do things that impact people's life, you know,

48:11.160 --> 48:12.800
right here right now.

48:12.800 --> 48:14.480
You know, even if you were a scientist

48:14.480 --> 48:17.160
at the forefront of research through these waves

48:17.160 --> 48:18.920
that went through like the semiconductor wave,

48:18.920 --> 48:22.600
the communication waves, the control waves, the wireless wave,

48:22.600 --> 48:26.400
you know, the time it passed between ideation

48:26.400 --> 48:29.160
and realization was in the decades, right?

48:30.200 --> 48:32.440
But here, you know, we have scientists to join

48:32.440 --> 48:34.280
past of the PhD and six months later

48:34.280 --> 48:36.560
that go this in production, you know, it's just unheard of.

48:36.560 --> 48:38.080
So it's very exciting because of that

48:38.080 --> 48:39.600
and there's a lot of stuff happening

48:40.920 --> 48:43.960
so that the entropy is very high.

48:45.320 --> 48:48.760
I think where we start seeing a lot of excitement

48:48.760 --> 48:53.760
is where there is cross task and cross model learning

48:57.160 --> 49:00.280
and, you know, right now we understand data

49:00.280 --> 49:02.560
and now we finally also understand

49:02.560 --> 49:04.840
what information is, information is in the data,

49:04.840 --> 49:06.520
but it's not the data is more.

49:06.520 --> 49:09.000
And now we don't quite yet what knowledge is,

49:09.000 --> 49:11.320
but knowledge has something to do with information

49:11.320 --> 49:13.720
and we're just beginning to shed light on that.

49:13.720 --> 49:16.400
And hopefully at some point we'll be able to reason

49:16.400 --> 49:19.640
not in the way in which we say we do reasoning

49:19.640 --> 49:21.720
in artificial systems, but really,

49:21.720 --> 49:23.400
he ways that allows us to interact naturally

49:23.400 --> 49:26.280
with the environment and naturally with physical space

49:26.280 --> 49:29.760
and naturally I'm with machines between different machines.

49:31.520 --> 49:34.000
And so being of US again is a little bit of a luxury

49:34.000 --> 49:37.840
in the sense that, you know, when we joined,

49:37.840 --> 49:39.000
we had mechanical turf.

49:39.000 --> 49:43.600
Mechanical turf was one of the culprits for the AI revolution.

49:43.600 --> 49:46.600
And there's A to I, which blends, you know,

49:46.600 --> 49:48.760
artificial systems with humans.

49:48.760 --> 49:51.680
And so all the pieces are there and, you know,

49:51.680 --> 49:53.400
it's up to the leadership of individuals

49:53.400 --> 49:56.160
to go and find the connections and make things happen,

49:56.160 --> 49:57.880
which is quite exciting.

49:57.880 --> 49:59.560
Yeah, yeah.

49:59.560 --> 50:02.200
Well, Stefano, thanks so much for taking the time

50:02.200 --> 50:06.440
to join us and talk through some of what you're working on.

50:06.440 --> 50:10.720
Very, very interesting stuff and enjoy the conversation.

50:10.720 --> 50:12.000
My pleasure, I enjoy this one.

50:12.000 --> 50:13.160
Thank you.

50:13.160 --> 50:14.000
Thank you.

