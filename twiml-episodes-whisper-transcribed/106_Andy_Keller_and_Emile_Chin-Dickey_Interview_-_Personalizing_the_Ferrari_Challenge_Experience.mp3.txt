Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting
people, doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
Contest alert.
This week we have a jam-packed intro, including a new contest we're launching.
So please bear with me, you don't want to miss this one.
First, a bit about this week's shows.
As you may know, I spent a few days at CES earlier this month.
While there, I spoke with a bunch of folks applying AI in the consumer electronics industry,
and I'm including you in those conversations via this series of shows.
Stay tuned as we explore some of the very cool ways that machine learning and AI are being
used to enhance our everyday lives.
This includes work being done at Anki, who built Cosmo, the cutest little computer vision-powered
robot.
Nighthouse, whose smart home security camera combines 3D sensing with deep learning and NLP.
Intel, who's using the single-shot multi-box image detection algorithm to personalize video
fees for the Ferrari Challenge North America.
First beat, a company whose machine learning algorithms analyzed your heartbeat data to
provide personalized insights into stress, exercise, and sleep patterns.
3AI and Koito, who have partnered to bring machine learning-based adaptive driving beams
or automatically adjusting high beams to the U.S.
And last but not least, aerial.ai, who applies sophisticated analytics to Wi-Fi signals to
enable some really interesting home automation and healthcare applications.
Now, as if six amazing interviews wasn't enough, a few of these companies have been so
kind as to provide us with products for you, the Twimmel community.
And keeping with the theme of this series, our contest will be a little different this
time.
To enter, we want to hear from you about the role AI is playing in your home and personal
life, and where you see it going.
Just head on over to twimmelai.com slash myaicontest, fire up your webcam or smartphone camera, and
tell us your story in two minutes or less.
We'll post the videos to YouTube, and the video with the most likes wins their choice
of great prizes, including an Anki Cosmo, a lighthouse smart home camera, and more.
Submissions will be taken until February 11th, and voting will remain open until February
18th.
Good luck.
Before we dive into today's show, I'd like to thank our friends at Intel AI for their
continued support of this podcast.
Intel was extremely active at this year's CES, with a bunch of AI autonomous driving
and VR related announcements.
One of the more interesting partnerships they announced was a collaboration with the Ferrari
Challenge North America race series.
Along with the folks at Ferrari Challenge, Intel AI aspires to make the race viewing experience
more personalized by using deep computer vision to detect and monitor individual race
cars via camera feeds and allow viewers to choose the specific cars feeds that they'd
like to watch.
You'll learn much more about this application in today's show, which features Intel's
Andy Keller and Emil Chinnicki.
Andy is a deep learning data scientist at Intel, and Emil Chinnicki is senior manager of marketing
partnerships at the company.
In this show, Emil gives us a high level overview of the Ferrari Challenge partnership and the
goals of the collaboration.
Andy and I then dive into the various machine learning aspects of this project, including
how the training data was collected, the techniques they used to perform fine-grained object
detection in the video streams, how they built the analytics platform, some of their remaining
challenges, and more.
And now on to the show.
All right, everyone.
We are here at CES, and I have the pleasure of being seated with Andy Keller, who is
a deep learning data scientist at Intel, and Emil Chinnicki, who is a senior manager of
marketing partnerships at Intel, and we've got an opportunity to chat about one of the
cool announcements that was made yesterday, or today it was kind of announced yesterday,
and then kind of more fully announced today.
I'm going to keep you in suspense for just a little bit longer, and actually ask these
guys to introduce themselves, and then we'll get into what that announcement was.
So why don't we start with you, Andy?
Yeah, thanks for having me.
So I'm a deep learning data scientist at Intel.
I started at Nirvana before the acquisition, working as an intern, doing some of the same
kind of object localization stuff that we're using for this soon-to-be-in-house project,
and specifically I was implementing some other models, like FasterRCNN.
Like what?
FasterRCNN?
Okay.
More recently, I've moved to kind of more similar work that I did for my masters at UCSD,
which is natural language processing and dialogue systems and question-answer systems.
Okay.
And so now I'm started full-time after finishing my degree, and I love being here.
Awesome.
Awesome.
The mail?
My name's Emil Tinnicki.
I joined last year the Artificial Intelligence Products Group at Intel, and I'm responsible
for marketing partnerships.
So what that means is that I work with our partner companies to showcase both partner
technologies, as well as intel technologies, showcasing our efforts on an artificial intelligence.
And in fact, there's one of these partnerships that Intel CEO Brian Krozenich announced
at his CES keynote last night, and it's a partnership with Ferrari Racing.
And I think for me, thinking back to Brian's keynote, it was a little surprising for
me, I guess, you know, I kind of exist in this AI bubble, and I expected it to be just
pure AI consumer and devices stuff.
But Intel as a company is way more all-in into this virtual reality and immersive experience
that then I knew.
I didn't know anything about Intel's efforts around true VR and the other virtual reality
plays in the immersive experience that he talked about.
I mean, it's everything from outfitting the Olympics that are coming up.
You guys built a studio, like a volume metric studio, and there was a partnership with Paramount
Theaters announced.
There was a ton of discussion around like how you would use the stuff in sports.
And then one of the partnerships that was announced in that context was this partnership
with Ferrari.
So tell us a little bit about that partnership and what you're hoping to achieve with it.
Yeah, so what was announced was a three-year partnership with Ferrari Challenge.
Ferrari challenges a race series put on by Ferrari, and it takes place in different regions.
We're specifically partnering with the North America team.
So one of the areas that we're applying artificial intelligence is to the race stream.
So that's basically applying AI techniques around a single shot detection and fine green
classification to recognize the cars that are being captured in each of the camera feeds.
And then the intent is to use that as metadata to curate camera feeds for viewers that are
specific to each car or driver.
So basically a fan could pick their favorite driver and we could deliver a curated feed
specific to that driver as they make their way around the entire track.
As I understand it, it's not just fans, but today, as every all of the folks that are
broadcasting a race like the Ferrari Challenge, they're all operating off of the same feed,
like everybody gets the same feed.
Is that how it's working today?
Yeah, so if you think of a traditional race broadcast or more typical race broadcast,
you have kind of a director's cut of the action and normally the feed is focused on the
first three or four cars in the race.
So if your favorite driver is not among those couple cars, then you may not actually see
too much of them during a broadcast.
We notice that people's consumption of media is changing and we expect that there's no
reason why that shouldn't change for motorsports broadcasts as well.
This basically puts more choice in the hands of the fan to kind of get a more tailored
customized viewing experience.
How would I configure my feed?
Is this something that is this being delivered for me like via a live stream and I'm choosing
a car or something like that or what's the experience of tailoring the feed?
Yeah, so the intention is that this would be a stream that would be broadcast live over
an app or a website and exactly what you said, you could pick your favorite driver and
kind of get that tailored stream according to that driver.
And this isn't being done by just putting traditional cameras around the track?
Well, yeah, so there's two components.
We're using drones to capture the race footage.
So that actually adds a rather dramatic element to the race experience, if you will.
So these are not fixed point cameras.
And then the notion is that you can apply these recognition techniques to each of those
camera feeds so that at any given point in time, you know exactly which cars are showing
up in which camera feeds.
Okay.
So with that information, you can kind of tailor that stream to a end user and viewer.
Interesting.
And how many drones are going to be flying around the Ferrari challenge when this thing
is running?
It's somewhat a function of the track configuration and length.
But for starters, we're looking at between five and six drones.
And we'll kind of go from there.
And are the drones mostly kind of fixed in each one covers an area of the track or are
they like following the cars or something?
Yeah.
So there's, they're not fixed in one location, they have kind of more of like a territory
if you will that cover.
So they may, you may see them like zipping around.
Yeah, they'll be moving around.
Territory.
Yeah.
Exactly.
Okay.
So I'm going to someone on your team yesterday about this at the, like, at the keynote,
all right, before the keynote.
And you know, we're going to talk a little bit about the kind of the AI elements and
challenges associated with this.
But, you know, just thinking about even like the, you know, this day and age, you've got
to be capturing like 4K, 8K streams, like off of a drone and like getting those down
to, you know, some place where you're going to process.
I mean, aside, there's a ton of stuff that, you know, keeping the drones in the air is
going to challenge.
But a ton of technical challenges here, really, really interesting.
So maybe this is a good segue to actually talk about some of the AI stuff.
And Andy, this is where some of your work is come in.
Yeah.
Maybe tell us a little bit about kind of the underlying AI technologies and what the primary
goals are for the project that you worked on.
Yeah.
So we thought a good place to start.
We needed a foundation on which to kind of build other analytics about all of the drivers
and the racing that was going on.
So we realized we needed something else to build some sort of analytics platform on.
And to start that, we really needed to be able to know, okay, we're going to be using
the video footage.
We need to know which drivers we're looking at in any given feed and we need to know where
they are.
We need to actually be able to localize them.
So obviously the first thing that makes sense is some sort of object detection or object
localization model, which can draw boxes around all the cars on the track and then potentially
classify them uniquely as who's in what car.
So kind of as a starting point for that, we had tested out using models that were pre-trained
on some other data sets and realized that pretty quickly realized we were going to need to
gather domain specific data set just because like Emule mentioned, the drone footage is
dramatic in some sense and you don't find a lot of that out on YouTube.
Exactly.
What did you start with?
What data sets did you try to train on initially?
Yeah, we were using the kitty data set, which is kind of what in the self-driving
data sets, but so it has bounding boxes around all of the cars and you can build a basic
car detector off of that, but it's all from the viewpoint of kind of the dashboard of
a car.
So some of those viewpoints apply from a drone, but a lot of our shots are really long
distance and kind of unique angles that we'd never seen before in that data set.
Right, that was kind of our main driving factor in generating this new data set, which
was a lot of the portion of the beginning of this project.
So in the typical race, you'll have these five or six drones kind of zipping around
their territories, capturing data of the field as a dozen or two dozen cars typically.
Yeah, somewhere around there.
And for each of these drones is instrumented with a camera and you're pulling down a
feed and you're doing what otherwise might seem like a typical autonomous driving
task, like putting bounding boxes around the cars that are on the track.
So the first step is building a model based on this kitty data set.
And then that wasn't giving you the accuracy that you're looking for.
So you started building your own data set and was this like buys, you know, running
drones on top of racetracks or yeah, basically.
So we had a substantial amount of footage from across the entire 2017 season of recording
from a bunch of drones at every single race.
Okay.
And this was in 4K.
And so we knew that a lot of this footage maybe wasn't as useful.
So we needed to kind of sort through that as a first step to generating a training set.
So we had probably hundreds of hours of 4K footage and it was some of the data scientists
jobs actually look through and find kind of really high variance shots, shots with
differences in lighting, differences in size of the cars.
Since these drones will move anywhere from 10 feet off the ground to 100 feet off the
ground, that really changes the appearance of the car and what the model is going to be
able to learn.
So we really tried to get as much variance as possible.
And what was the approach to doing that was this something that, you know, that was kind
of manually coming through footage and hand annotating or did you, did you automate it
in some way?
We did have some of the cut from the live broadcast that was kind of curated by the drone
team that was broadcast on the stream during the race.
So we knew a little bit of kind of what shots to follow and what was the general.
I mean, obviously there are at least going to have cars in the shot if it's in the broadcast.
So we were able to use those and then some amount of manually coming through unfortunately.
That wasn't the most exciting way that you spent time.
And with that, did you, the manual coming through, like, what, did you use some, like, some
kind of off-to-shelf tools to facilitate that?
Luckily, we were kind of able to just write down timestamps in videos.
And then I had written some scripts and FFM Peg is kind of the default tool for-
To just snip out snippets and dump them in a drive or something like that.
And so once we had kind of parsed through all of those and extracted some percentage of
the frames, we were able to send those off to a labeling company and then provide them
with a series of guidelines and kind of helpful data sheets about each of the different
cars that we wanted labeled since we had something close to 40 or 50 different cars, or some
of them looked pretty similar.
So it was a, that was kind of the, one of the larger challenges was making sure the labeling
team was actually able to do their job correctly such that the model itself was able to do
it.
It's just, it's, it's, so how long do you have a sense for how long you spent on just this
kind of data collection and pre, like pre labeling task?
The data collection itself, the recording of the video was over the entire 2017 race season,
so I guess that maybe shouldn't be included.
But kind of the curation of what we were going to provide to them, I would say happened
over a week or two, we were able to get a couple of different data scientists looking
at it and people who.
And you started with 100 or so hours, I think you said, what was the, how, what was the
size, like if you added up all the snippets that you sent on to get annotated, how many
hours did you have of that?
It was close to I think one hour.
Okay.
So significant.
So dense, set, yeah.
We even, after that, we obviously, because from frame to frame, there's really not that
much difference.
We were pulling something like maybe 10% of the frames, so that we can get more variety
with a lower number of frames, since it really, every single frame is more effort for the
labeling team and you're really trying to reduce that as much as possible.
Right.
Right.
So meaning you used 10% of the frames in the one hour of footage.
And then you had this label by the labeling team, I would have imagined that you needed
a lot more training examples to achieve, you know, acceptable performance on this.
Were you using these examples in conjunction with the previous, like was this a transfer
learning type of task, or did you train from scratch with these new examples?
Yeah.
We ended up actually going straight from scratch.
We kind of tried to approximate the size of some of the other popular object detection
data sets, like Pascal VOC.
Okay.
And so that was kind of our goal to begin with, and basically from scratch was a little
bit challenging, but we realized we probably needed to do it that way just because of kind
of some of the uniqueness of this problem, the fact that these cars are very small and
we have 50 different classes, but they all look very similar.
So it's more of a fine grain classification problem, which a lot of these typical object
detection data sets don't really cover.
And is it generally the case that for these fine grain types of object identification
problems that you need less data than, you know, if you were training up kind of object,
you know, coarse grain object detection scratch?
I don't know if I'd say less data, I think it depends on also how you develop the model.
So there are some kind of future steps and kind of some of the state of the art and fine
grain detection that we're planning to implement where you'll train a deep network and then
chop off the top and add an SVM on top.
And that was able to do a lot better at these tasks where the features between the two
classes are very similar, but there is a boundary versus kind of the traditional something
with something like what we use for SSD where it's an end-to-end neural network.
So I think for the end-to-end case, you probably need about the same amount of data.
We ended up having probably close to 1,000 labels per car, maybe 2,000, which seemed to
be similar to some of these other data sets.
Okay.
Obviously some cars were more frequent than others and there wasn't a ton we could do besides
rebalancing afterwards, but yeah, it turned out to work pretty well.
Oh, nice.
Thank you.
What kind of model approach model architecture did you end up using and how did you arrive
at that?
Yeah.
So we ended up using a single shot multi-box detector, which I think Andres, who was on
your show a few weeks ago talking about the NASA project.
They also used that for the crater detection.
So because of some of the optimizations that we have in Neon for Intel, that made sense
on that front.
We were able to get kind of the live speed that we were looking for.
And then also kind of the general architecture itself.
It's a one shot, not one shot in one shot learning sense, but in a single shot like it, it's
a single architecture.
You don't have kind of a two-step process like faster or CNN has where first it proposes
boxes and then it does the classification, which kind of happens all at once.
So that is partially one of the reasons why it's significantly faster, which we liked.
And it also has kind of feature maps of multiple different scales.
So what does that mean?
We have a convolutional networks when they operate and you go up and hire layers, the kind
of the feature maps continue to shrink down and down further.
So if we provide all of those to the end classifier, it's able to kind of get the same object
at a bunch of different resolutions.
So it's able to, in the end, basically means we're able to classify objects better at a
lot of different scales.
So small objects, big objects, which was really one of the main challenges of this problem
in this data set.
You mentioned that one of the things that you looked at or are looking at or considered
was doing a network where you kind of chopped off the end of the network and replace it
with an SVM.
How far did you go down that path?
We haven't gotten there at all yet.
So there's a lot of next steps that we're looking into.
I'm not sure how many I can discuss, but it's, yeah, that's definitely something we're
considering.
Okay.
What are the kind of outstanding challenges in trying to productionalize this?
I always kind of change whether I say productize, productionize, productionize, but it's a production.
Yeah.
What's remaining for you to kind of take on with this project?
Yeah.
One of the big ones is the fact that the appearance of the cars isn't necessarily static
in between races or even in between days of a given race.
A driver may change the color of their rims on their tires or even completely change
the wrap on their car or crash the car at one day.
So if we're trying to do a purely supervised kind of one of these, like just pound it with
as much data as possible to get it to learn, then that makes it a little more challenging
or to keep up.
Hard to keep up.
Yeah.
So how do you address that?
So there's a couple ways.
The way that we're thinking to address it at this point is we would like to be able
to just during practice laps, maybe take five, three, four, five pictures of a given car.
We can know when a car has changed appearance slightly.
And then if we have a model that's able to learn to classify cars just based on that
small support set, that is kind of that would be ideal.
So there's some architectures out there that are kind of attempting to do this today and
it's kind of in the one shot, few shot learning field.
And so stuff like matching networks or some of the other metal learning techniques, I think
are what we're going to be exploring in the immediate future.
So much of you dug into that stuff so far.
One shot, few shot, metal learning or things that I, you know, are on my list of things
to dig into a bit more on the podcast this year.
So if you've learned about some of this stuff already, I'd love to get a sense for, you
know, what you've seen out there and what you find interesting.
Yeah.
I mean, I saw a ton of interesting metal learning talks this year at NIPs.
Okay.
And a lot of significant portion of them focused around kind of this few shot learning idea.
And it seems like there's a lot of different ways to approach it.
And metal learning as a framework is pretty general.
And so some people were approaching it from the, just like kind of the transfer learning
perspective where you say, okay, we're actually going to design our optimization function
such that our goal is that we learn a set of parameters that, with a single gradient
step, we're able to achieve a variety of different tasks or achieve high performance on a variety
of different tasks, which is different than kind of just optimize for a single specific
task.
I think that work is called mammal out of Berkeley.
And so that, that's something that I think is generally applicable to, I mean, obviously
any model, which is cool.
And then some stuff like the matching network where it's almost like a can yours neighbors
type approximation of classification where you say, okay, I'm going to give it five images
of this type of dog and now show it a new dog.
And it's going to compare with these images that it knows and see which one is the closest
and then or do some sort of majority vote based on the class using that.
So the cool thing is that they're all kind of differentiable and not necessarily related
to a specific data set.
So I think that is an interesting.
So is the idea that you would have someone like at the track who is, or I guess they
could be remotely, but someone who is looking at these live practice run videos and like
coming up with an annotated sample and shooting that like, are you, are you then triggering
an entire retrain of some model, you know, just before the race, that sounds like that.
I hope as we don't have to do that.
Yeah.
That would be dangerous.
Yeah, sounds dangerous.
I think we had discussed that before and we were like, well, what could go wrong?
What if we just doesn't finish training in front of?
Right.
Yeah.
So I think we want to have an interface where you can kind of just draw, either click
on the cars or draw a couple of boxes around the cars.
Our models will work as a general car detector from drone footage, kind of regardless of
how the appearance of the car changes.
So it should be as simple as just clicking on the new car over a series of frames and
the boxes will already be there.
Hopefully if we use something like a matching network, those can just be cropped out and dumped
into a directory and we actually don't have to do any retraining.
So the model uses those images as part of it's inference procedure, which is kind of the
ideal scenario.
So in what way, how is the model incorporating those images as part of the inference?
So you talked about the five images of the dog and getting a new one, is it?
These annotated car images would be presumably the five images of the dog, but that's happening
at inference time, not training time.
Yeah.
So maybe we have five images of every single car associated with that as the labels.
And this is kind of a really rough overview of the matching network.
They did.
They added some fancy stuff on top of this, but basically like a can yours neighbor.
So all of those images, so maybe you have 10 different cars, you have 15 images total.
They're all embedded with some learned embedding function, just a mapping.
And then an embedding function into what space?
One lower dimensional, so you just have like a fully connected neural network layer and
it goes to some smaller space, so you have new, now features for all of those pictures.
You also do the same mapping on your new input.
And then you can just compare through some distance metric or some similarity metric, that
new input and all of your 50 kind of just loaded images.
So these images were potentially never trained on.
You're really just learning that embedding function.
And then once you compare and find, okay, it's closest to these five or these three or
this one, you can take a weighted combination of those classes and then whichever class
kind of has the highest vote is what you end up going with.
And how does the train model interact with and inform the, you know, this matching process
that happens at inference time?
Is it that in this model, like the model that you've trained, the network is only doing object
detection and then this matching process is doing the object identification or does having
the train model somehow inform the identification as well, but you've got this extra layer that
refines it.
Yeah, so I think we could do it either way.
We would like to be able to keep it as a single model, just for speed reasons, but that's
still kind of in the research phase.
An alternative is to kind of have the SSD model just do the object detection and then
have the whatever additional network that we have on top does kind of the few shop learning
and classification of the different cars.
So I think, yeah, a lot of these are things to explore in the year.
Right.
You mentioned SSD a couple of times.
What is, what is SSD I'm sorry, it's the single shot multi box, single shot multi box
detector.
Right.
Okay, as opposed to your hard drive on, and we have an implementation of SSD that's open
source on the neon get a repo, okay, so that's actually what we ended up using for this
project.
Okay.
And it's pretty much just plug and play with a new data set and a little bit of tuning
and we were able to achieve pretty good performance.
Nice.
And now is the, the data set that you've, like are you publishing this data set or anything
like that?
Not at this point, okay.
Or any of the, any of the models or anything like that, are you publishing them or do you
have like technical blog posts or something that folks can take a look at if they want
to get more detailist into, like how you approach this problem?
Yeah, we have a technical blog post that should be out.
Or will we coming out, describing some of the data collection and the modeling procedure
and preprocessing steps that we used, as well as a little bit of the training, so that
should give people a pretty good idea.
And are there, are there other kind of types or classes of problem that you think that
this same kind of approach would lend itself to?
Or is this, like do you think this is very custom to the unique challenges of having six
drones flying around trying to identify, you know, formula racing cars or Ferrari racing
cars?
Yeah, I definitely, even today, here at CES of the booth, we had a lot of people coming
up and saying, oh, this could work for this sport that I participate in, or we haven't
quite explored those areas in terms of partnerships or how that would work yet.
But I think it's definitely applicable to all sorts of different either broadcast races
or sports or when just when there's a fast moving object that it's difficult for viewers
to follow and it helps a lot to have some sort of either AI assistants or overlay or automatic
broadcast control.
So Andy, so this is kind of, you know, this project was just announced here.
What are some of the other things that you're working on?
Yeah, so I'm working on, obviously, continuing this project and some of the optimizations
that I mentioned.
My other work is more related, like I said, to what I had done my master's on, which is
kind of end to end question-answer systems and dialogue systems, using similar network
topologies if you look at it kind of squint and from a distance.
And stuff like memory networks are a large part of my work.
So I'm really interested in kind of memory augmented neural networks and the role that
can play and not just question answering, but kind of a bunch of different challenges.
And our memory networks and memory augmented networks is that of these related to like LSTM's
and attention mechanisms and that kind of thing or yeah, very similar to attention.
It's typically the memory augmented network means it has kind of some sort of fixed readable
memory that you can either load items into.
So for the case of the memory networks for question answering, you'll load in a story,
like a text story of something that happened and then ask it questions about that story.
And so similar to attention, the memory network will kind of compare whatever its input
is with all of the story that's loaded in memory.
So those are very similar.
Awesome.
Well, thanks so much guys for taking a hike over in the rain and the traffic.
I was great chatting with you and I enjoyed learning more about what you're doing with
the Ferrari challenge.
Absolutely.
Thank you.
Thanks for having us.
Alright, everyone.
That's our show for today.
Thanks so much for listening and for your continued feedback and support.
Remember, for your chance to win in our AI at home giveaway, head on over to twimlai.com
slash my AI contest for complete details.
For more information on Andy, Amille, or any of the topics covered in this episode, head
on over to twimlai.com slash talk slash 104.
Thanks once again to Intel AI for their sponsorship of this series.
To learn more about their partnership with Ferrari North America challenge and the other
things they've been up to, visit ai.intel.com.
Of course, we'd be delighted to hear from you, either via a comment on the show notes page
or via Twitter directly to me at at Sam Charrington or to the show at at twimlai.
Thanks once again for listening and catch you next time.
