WEBVTT

00:00.000 --> 00:13.400
Welcome to the Twimmel AI Podcast.

00:13.400 --> 00:21.320
I'm your host Sam Charrington.

00:21.320 --> 00:26.960
This week on the podcast, I'm happy to share just a few of the nearly 20 interviews I recorded

00:26.960 --> 00:31.800
earlier this month at the 33rd annual NURRIPS conference.

00:31.800 --> 00:35.920
If you've been waiting for the Twimmel pendulum to swing from workflow and deployment back

00:35.920 --> 00:39.920
over to AI and ML research, this is your time.

00:39.920 --> 00:45.200
We've got some great interviews in store for you over the upcoming weeks.

00:45.200 --> 00:49.280
Before we move on, I want to send a huge thanks to our friends at Shell for their support

00:49.280 --> 00:53.840
of the podcast and their sponsorship of this NURRIPS series.

00:53.840 --> 00:58.600
Shell has been an early adopter of a wide variety of AI technologies to support use cases

00:58.600 --> 01:04.920
across retail, trading, new energies, refineries, exploration, and many more, and is doing

01:04.920 --> 01:08.680
some really interesting things, but don't take it from me.

01:08.680 --> 01:14.720
Microsoft CEO Satya Nadella recently noted that what's happening at Shell is pretty amazing.

01:14.720 --> 01:19.600
They have a very deliberate strategy of using AI right across their operation from the drilling

01:19.600 --> 01:22.800
operations to safety.

01:22.800 --> 01:27.800
Last year, the company established the Shell.AI Residency Program, a two-year full-time

01:27.800 --> 01:32.680
program, which allows data scientists and AI engineers to gain experience working on

01:32.680 --> 01:37.080
a variety of AI projects across all Shell businesses.

01:37.080 --> 01:40.960
If you're in a position to take advantage of an opportunity like this, I'd encourage

01:40.960 --> 01:46.560
you to hit pause now and head over to Shell.AI to learn more.

01:46.560 --> 01:53.560
Once again, that's Shell.AI, and now on to the show.

01:53.560 --> 01:55.520
All right, everyone.

01:55.520 --> 02:00.040
I am here in Vancouver at NURRIPS, and I've got the pleasure of being seated with Celeste

02:00.040 --> 02:01.040
Kid.

02:01.040 --> 02:04.720
Celeste is an assistant professor of psychology at UC Berkeley.

02:04.720 --> 02:06.640
Celeste, welcome to the Twoma AI podcast.

02:06.640 --> 02:07.640
Thank you so much.

02:07.640 --> 02:09.120
I'm so excited to be here.

02:09.120 --> 02:15.480
I am super excited to dive into this conversation you delivered and invited talk here yesterday

02:15.480 --> 02:20.840
that has been blowing up the Twitter's, and I'm really looking forward to chatting

02:20.840 --> 02:22.200
with you about it.

02:22.200 --> 02:24.680
But before we do that, tell us a little bit about your background.

02:24.680 --> 02:29.760
You operate in the intersection of psychology and machine learning, and we're talking

02:29.760 --> 02:32.360
about you building models and stuff like that.

02:32.360 --> 02:33.360
What are you up to?

02:33.360 --> 02:34.360
I do.

02:34.360 --> 02:38.920
I like pulling a lot of things from a lot of places.

02:38.920 --> 02:44.920
My background was actually in investigative reporting, which sounds not that relevant,

02:44.920 --> 02:48.720
but I was interested in doing big data type analyses.

02:48.720 --> 02:56.160
My first real science was getting together public records and looking for corruption.

02:56.160 --> 03:01.320
I started at UC Santa Cruz, and then I also liked computers, so started in CS.

03:01.320 --> 03:07.920
I ended up transferring and changing my degrees to linguistics, and then I finished the journalism

03:07.920 --> 03:18.000
degree, and while I was doing those, I got very lucky and happened upon science via some

03:18.000 --> 03:24.240
really amazing mentors and professors at USC, and fell in love with it, and liked that unlike

03:24.240 --> 03:28.160
journalism in science, truth is on your side.

03:28.160 --> 03:33.480
I think when you find the truth, you win much more so than I was experiencing at that time

03:33.480 --> 03:34.480
in journalism.

03:34.480 --> 03:36.960
I thought the journalism was going to be like that, but I found...

03:36.960 --> 03:38.520
I think hope journalism is like that.

03:38.520 --> 03:42.120
I think it's becoming more like that, and that's actually an interesting discussion, too,

03:42.120 --> 03:45.640
because that's where it's going is much more machine learning relevant than it used

03:45.640 --> 03:51.960
to be, when I was making the decision about whether to continue in journalism or to transition

03:51.960 --> 03:53.040
to science.

03:53.040 --> 03:57.640
I knew I really loved science, but there were still a part of me that thought that journalism

03:57.640 --> 04:06.240
was a kind of higher calling, and at that time, I had a bunch of stories that I had composed

04:06.240 --> 04:11.400
that I was very proud of, but they contained things that my editors weren't expecting.

04:11.400 --> 04:12.400
They weren't that sophisticated.

04:12.400 --> 04:18.760
I hadn't taken formally a statistics class, but in that era, if I brought something to

04:18.760 --> 04:25.320
my editor and it wasn't a pie chart or a bar graph, they were confused and asked me

04:25.320 --> 04:28.800
to go back and turn it into a pie chart or a bar graph, and I was like regression analyses

04:28.800 --> 04:29.800
or not.

04:29.800 --> 04:35.400
We're not things that editors would even like hear you out on, so I think that's changed

04:35.400 --> 04:36.400
a whole lot.

04:36.400 --> 04:39.120
Well, data journalism is a whole thing now.

04:39.120 --> 04:40.120
It's a thing.

04:40.120 --> 04:44.560
In the era in which I was in college, the thing that was the version of that was obviously

04:44.560 --> 04:47.960
named in the 70s and was called computer assisted reporting.

04:47.960 --> 04:48.960
Oh, wow.

04:48.960 --> 04:53.560
The conventions were really, really disappointing.

04:53.560 --> 04:57.640
Pretty much what everybody was doing is there's all these workshops and there's reporters

04:57.640 --> 05:02.800
that have had high profile stories, and the promise was, we'll tell you how to do something

05:02.800 --> 05:06.480
new and innovative and almost all of them, it was like, what I did is I got some public

05:06.480 --> 05:10.800
records for school bus drivers and I got registered sex offenders, cross reference

05:10.800 --> 05:13.080
them, bam, story.

05:13.080 --> 05:18.000
Now I got some priests and registered sex offenders, cross reference them, story, that was

05:18.000 --> 05:19.840
the only thing that people were really doing.

05:19.840 --> 05:23.000
And that should be like table stakes of, hey, doing some research, right?

05:23.000 --> 05:24.000
Yeah, yeah.

05:24.000 --> 05:31.680
So it wasn't, I was having a hard time finding inspiration in that field and then the

05:31.680 --> 05:40.040
stories I was producing often got, the editor wasn't willing to run them as they were or

05:40.040 --> 05:45.560
they wanted me to simplify them to the point that I thought it wasn't true anymore and

05:45.560 --> 05:48.000
nobody was paying investigative reporters.

05:48.000 --> 05:53.400
I applied to grad school, also had a job that it was very likely that I could have, was

05:53.400 --> 05:58.440
like lined up, that I was considering and I called my journalism mentors expecting them

05:58.440 --> 06:03.320
to talk me out of going to grad school for science and every single one of them said, this

06:03.320 --> 06:08.600
is a terrible time journalism might be dead, can you do something else, go do it.

06:08.600 --> 06:13.360
And in that era, I was like my grad school stipend, those are not known for being generous

06:13.360 --> 06:17.200
but the journalism salaries are really abysmal, wasn't my grad school stipend actually was

06:17.200 --> 06:20.760
higher than my salary would have been if I'd taken the journalism job.

06:20.760 --> 06:28.400
So yeah, I'm happy to be here in science where we can do all sorts of more sophisticated

06:28.400 --> 06:33.120
analyses than, you know, 2000 era journalism.

06:33.120 --> 06:37.040
Yeah, so tell us about the focus of your lab at Berkeley.

06:37.040 --> 06:43.920
We are very interested in belief formation, we're interested in how people form their

06:43.920 --> 06:49.640
beliefs and we apply that to domains that I think people don't think of usually, they

06:49.640 --> 06:54.960
don't use the term belief, it's like they use terms like knowledge acquisition, things

06:54.960 --> 07:00.600
that people think of as you learning and you're done, probably that's not true.

07:00.600 --> 07:07.000
So things like words, if you know the word table, you might think, we talk about a kid who

07:07.000 --> 07:10.560
knows the word table, if I say like, hey, show me a table, they can point to it, they produce

07:10.560 --> 07:12.880
the word table in the right instances.

07:12.880 --> 07:19.320
But what is actually true, but you can't observe directly because the mind is a black box

07:19.320 --> 07:26.720
is you're never done forming your concept of table with every new table you encounter,

07:26.720 --> 07:32.640
your belief about the concept of table changes just a little bit and it updates if you move

07:32.640 --> 07:39.040
to a place where the tables look systematically different, your concept will move in that

07:39.040 --> 07:40.040
direction too.

07:40.040 --> 07:46.440
So we think about what other people might call knowledge in terms of beliefs and we think

07:46.440 --> 07:51.200
of beliefs in terms of being kind of packets of probabilistic expectations.

07:51.200 --> 07:56.120
And thinking about this example of table, like how would how do you experimentally validate

07:56.120 --> 07:57.360
those kinds of ideas?

07:57.360 --> 07:59.360
Oh, that's the fun part.

07:59.360 --> 08:06.240
Like I didn't get to do that much, you didn't get to do as much in reporting.

08:06.240 --> 08:17.400
So we do things like we ask people to, we give them a choice, as we say, here's a concept,

08:17.400 --> 08:24.680
for example, pick a political figure, Donald Trump is Donald Trump more like Richard Nixon

08:24.680 --> 08:27.280
or more like Elizabeth Warren.

08:27.280 --> 08:34.040
And people make a selection, as we collect these comparisons across a whole set of examples

08:34.040 --> 08:42.920
and based on people's responses, we're able to use clustering algorithms in order to infer

08:42.920 --> 08:48.600
the true number of types of categories that exist in the population.

08:48.600 --> 08:51.080
That's a good summary.

08:51.080 --> 08:55.920
And it sounds like from what I've read about some of the work at your lab, you're also,

08:55.920 --> 09:00.920
in addition to kind of an experimental type of approach, you're also building models

09:00.920 --> 09:08.720
and using machine learning to, well, you describe, what's the connection between the model

09:08.720 --> 09:12.480
work that you do and the experimental work?

09:12.480 --> 09:19.120
It depends on the particular type of project, but in general, we're building computational

09:19.120 --> 09:26.160
models that represent formal versions of classic theories from learning science.

09:26.160 --> 09:31.560
So we take a lot of inspiration from people like Jean Piaget and Maria Montessori and Lev

09:31.560 --> 09:33.000
Vagatsky.

09:33.000 --> 09:39.120
All of them had pretty similar ideas about what the relationship should be between what

09:39.120 --> 09:43.080
you currently understand and what you're interested in sampling from next or what you

09:43.080 --> 09:45.720
are most able to learn from next.

09:45.720 --> 09:51.400
All of them said similar things about there being a just right amount of information.

09:51.400 --> 09:54.720
I was like, you want to seek out stuff that's a little bit different from what you currently

09:54.720 --> 09:58.640
understand, but not so different from what you currently understand that you can't gain

09:58.640 --> 10:00.200
any traction.

10:00.200 --> 10:05.320
Those ideas remain largely untested because of the black box problem.

10:05.320 --> 10:09.440
You can't directly, you know, open up a kid's head and see what's in there.

10:09.440 --> 10:19.280
So we use models in order to represent a sort of formal version of those kinds of ideas.

10:19.280 --> 10:23.600
I was like, for that particular work, I was like, we have a set of research about infants

10:23.600 --> 10:25.920
and how they sample from the world.

10:25.920 --> 10:32.200
We were interested in testing whether or not infants are generating probabilistic expectations

10:32.200 --> 10:38.240
in the absence of any specific goal and whether those probabilistic expectations were influencing

10:38.240 --> 10:42.520
their decisions about what to look at, whether or not they should continue looking at something

10:42.520 --> 10:46.440
or they should cut and run and find something else.

10:46.440 --> 10:55.880
So we created a probabilistic model, a Dirichlet multinomial, so pretty standard stuff.

10:55.880 --> 11:04.680
And then I've used that to compute the surprise value for different actions in a sequence.

11:04.680 --> 11:08.520
I'm not sure if this is, I was like, now I'm taking a moment to those, I don't know if

11:08.520 --> 11:09.520
I'm going backwards.

11:09.520 --> 11:10.520
Is this understandable?

11:10.520 --> 11:11.520
No, no.

11:11.520 --> 11:12.520
So the surprise value, it is...

11:12.520 --> 11:15.360
Well, I just realized I forgot to tell you what they're looking at.

11:15.360 --> 11:16.360
I should like...

11:16.360 --> 11:17.360
Oh, yes.

11:17.360 --> 11:21.200
We can fill in the context now, sir.

11:21.200 --> 11:27.960
For these experiments, people have been interested in how infants decide what to look at moment

11:27.960 --> 11:30.320
to moment for a long time for a number of reasons.

11:30.320 --> 11:36.400
Like, one of them is you can't ask infants questions the only way that you can find out.

11:36.400 --> 11:40.160
What they know is to look at what they're looking at, what they're interested in and try

11:40.160 --> 11:45.920
to make inferences. The most common format of infant experiments is you show the most

11:45.920 --> 11:50.240
stimulus A, you show the most stimulus B, and you see if there's a difference in looking

11:50.240 --> 11:51.240
between those two.

11:51.240 --> 11:52.240
Okay.

11:52.240 --> 11:57.760
And from that, you try to draw very rich inferences about what is in that black box.

11:57.760 --> 12:02.440
And as you can imagine, like those are one bit experiments, that's challenging.

12:02.440 --> 12:07.120
You can't really learn anything from a single experiment.

12:07.120 --> 12:13.240
But the people that are great in this area do, like people like Liz Spelke and Renee

12:13.240 --> 12:16.920
Bayajan, is they're not really drawing inferences from just one experiment.

12:16.920 --> 12:20.840
They do a whole series of experiments, and they know other things in the background about

12:20.840 --> 12:21.840
what infants know.

12:21.840 --> 12:25.520
So, I was interested in that method.

12:25.520 --> 12:29.560
That was a method that I used with Toby Mintzi at USC, and it was sort of fascinated

12:29.560 --> 12:36.120
that scientists were inferring things like whether or not infants had object concepts.

12:36.120 --> 12:41.040
It's like that's a pretty rich kind of representation that you're inferring just on the basis

12:41.040 --> 12:45.120
of a kid looking longer over here versus over here.

12:45.120 --> 12:54.320
I was interested in that and wanted to know more about the linking function between expectations

12:54.320 --> 12:56.400
and infant's interests.

12:56.400 --> 13:00.640
And I had that idea and then it took a few years before I could figure out how you might

13:00.640 --> 13:03.080
be able to get at that.

13:03.080 --> 13:08.880
I was watching a kid play whack-a-mole and objects pop out in some order, and if you imagine

13:08.880 --> 13:14.440
whack-a-mole as I imagine there's like three moles and three holes, that's a perfect instance

13:14.440 --> 13:20.160
in which I can imagine a way to model what you think is likely in this very limited toy

13:20.160 --> 13:21.160
space.

13:21.160 --> 13:25.760
So, you walk up to the whack-a-mole machine at the very onset.

13:25.760 --> 13:30.080
You haven't observed any data, but if I ask you like, how often do you think each one

13:30.080 --> 13:31.080
pops up?

13:31.080 --> 13:35.080
People say like, they're all equally likely, okay, great, there's your prior.

13:35.080 --> 13:40.840
Now you put a quarter in the machine, you see one mole pop-up, if I stop you right there

13:40.840 --> 13:45.800
and say like, okay, how likely do you think it is, mole will be, we'll see, you've only

13:45.800 --> 13:49.720
observed one pop-up so it doesn't change your mind very much.

13:49.720 --> 13:59.720
But if mole pops up a hundred times, there's like more data, you come to, that shifts your

13:59.720 --> 14:05.960
beliefs more, so we're using that setup where this is a domain in which if we just care

14:05.960 --> 14:13.800
about the sequence of events, we can actually quantify how predictable or surprising a particular

14:13.800 --> 14:14.800
event is.

14:14.800 --> 14:19.280
That's a way of getting at that linking function question.

14:19.280 --> 14:21.280
So we made a version of-

14:21.280 --> 14:24.360
And sorry, the linking function represents what?

14:24.360 --> 14:31.520
The linking function represents the relationship between surprise value, yeah, surprise value

14:31.520 --> 14:34.640
for an event in a sequence in an infant's interest.

14:34.640 --> 14:41.280
And surprise value is a formal way about trying to start thinking about expectations influencing

14:41.280 --> 14:46.960
infants' beliefs, which is obviously true, but people weren't sure exactly what that

14:46.960 --> 14:50.440
relationship looked like because nobody ever varies it on a continuum for infants.

14:50.440 --> 14:54.760
And when you call this a linking function, are you trying to actually define the function

14:54.760 --> 15:00.560
or get to, okay, yeah, these are probably correlated or these are not correlated or something

15:00.560 --> 15:01.560
like that?

15:01.560 --> 15:09.560
We are trying to understand how infants guide their search for information in the world.

15:09.560 --> 15:16.680
I think it's maybe easy to forget this because we have limited attention, but at every

15:16.680 --> 15:22.720
moment where you or an infant is looking at one thing, you're necessarily not looking

15:22.720 --> 15:24.440
at everything else.

15:24.440 --> 15:28.360
Each decision that you make about where you're going to put your attention or where you're

15:28.360 --> 15:31.760
going to click or what you're going to listen to or who you're going to talk to, because

15:31.760 --> 15:35.280
the world isn't static, comes with a huge opportunity cost.

15:35.280 --> 15:40.880
I'm here in this room talking to you and I'm not at the conference seeing whatever is

15:40.880 --> 15:41.880
happening over there.

15:41.880 --> 15:43.040
I'm not looking at a poster.

15:43.040 --> 15:47.560
So the linking function that we're interested, we're linking interested, the linking function

15:47.560 --> 15:53.400
because we're interested in understanding given how much richness and how vast all of

15:53.400 --> 15:59.360
the information is in the world, how could an infant possibly get started trying to make

15:59.360 --> 16:04.280
the decisions about where they should look and when they should terminate and how could

16:04.280 --> 16:10.600
you design a system that can go from possessing as little information as an infant has to

16:10.600 --> 16:15.440
eventually having a not perfect, but I was like a relatively sophisticated network of

16:15.440 --> 16:19.880
knowledge like an adult that's studying machine learning.

16:19.880 --> 16:26.000
Even calling these decisions suggests a higher level of processing than I might think is

16:26.000 --> 16:29.640
the case in a lot of, particularly for infants and whether they're looking at the banana

16:29.640 --> 16:31.800
or the picture or something like that.

16:31.800 --> 16:36.280
I don't want to, when I use that word, imply that I mean that they're conscious.

16:36.280 --> 16:39.080
I think all of these things are happening automatically.

16:39.080 --> 16:43.440
I use the word decisions, I was like you could use the word choice, me and Ben Hayden have

16:43.440 --> 16:51.040
a paper in Neuron that's about how we don't really care what words people use for.

16:51.040 --> 16:53.400
Use the word curiosity and people.

16:53.400 --> 16:56.920
Yeah, so we've already thought of that question and wrote a paper about it.

16:56.920 --> 17:03.840
Yeah, literally it's part of our research program in the lab to work on, yeah, how people's

17:03.840 --> 17:07.760
concepts vary and how one two people use the same word, they're not activating the same

17:07.760 --> 17:08.760
concept.

17:08.760 --> 17:12.840
So, yeah, I don't worry, I don't get in fights over there.

17:12.840 --> 17:13.840
Right, human languages.

17:13.840 --> 17:16.320
The good point is they're looking at one thing or another thing and that's what you're

17:16.320 --> 17:17.320
calling a decision.

17:17.320 --> 17:18.320
Yeah, well and also.

17:18.320 --> 17:19.320
Whatever the mechanism is.

17:19.320 --> 17:20.320
That's correct.

17:20.320 --> 17:25.280
And I'm using the word decision or choice because I think it's very important that if

17:25.280 --> 17:34.120
you're going to have a smart, intelligent selection, attentional system, if you want that,

17:34.120 --> 17:40.560
you want the same general guiding principles to maybe explain where you put your eyes,

17:40.560 --> 17:45.160
it's like your saccades, your eye movements, where are you going to look, but then that

17:45.160 --> 17:49.320
same system should also guide other ways in which you might sample information from

17:49.320 --> 17:50.320
the world.

17:50.320 --> 17:55.720
So, what you click, what you're willing to pay for if you're buying movies on some

17:55.720 --> 18:01.360
kind of streaming services, like there are decisions that are more or less conscious,

18:01.360 --> 18:06.240
they happen at different time scales, but if you were to design a smart system, what

18:06.240 --> 18:10.880
it should do is seek out information that's valuable.

18:10.880 --> 18:16.320
And what it means to be valuable is that it offers something new, but you can integrate

18:16.320 --> 18:18.320
it with your existing representation.

18:18.320 --> 18:27.240
So, the idea of this infant work is trying to see whether or not probabilistic expectations,

18:27.240 --> 18:31.360
guide infants looking at all, they've been theorized to do that for a long time, and if

18:31.360 --> 18:39.320
they do, what is the relationship between some metric-like surprise and their interest?

18:39.320 --> 18:43.880
And what we found is that, like many people had suggested that it might be, you get a

18:43.880 --> 18:49.680
U-shaped relationship between infants' interest and the surprise value for an event in a sequence,

18:49.680 --> 18:55.320
meaning that infants are most likely to terminate their attention to events that are very low

18:55.320 --> 19:00.160
surprise value, so things that are very expected don't offer you much, you thought that was

19:00.160 --> 19:01.160
going to happen, you did.

19:01.160 --> 19:08.360
You're not learning a lot, but on the opposite end of the spectrum, if it's too surprising,

19:08.360 --> 19:14.880
if it's a high surprise value, you also terminate your attention, infants are most interested

19:14.880 --> 19:19.960
at maintaining their attention when they encounter events in the sequence that are a little

19:19.960 --> 19:23.560
bit surprising, given what they were expecting, but not overly surprising.

19:23.560 --> 19:28.640
And is the overly surprising result, is that counterintuitive to you, or surprising at

19:28.640 --> 19:30.320
all, or is that expected?

19:30.320 --> 19:37.080
I think it is to some people, if you are just coming into these questions and you think,

19:37.080 --> 19:38.080
like, what should I do?

19:38.080 --> 19:41.920
I'm going to design a robot that's going to search for places that it should learn in

19:41.920 --> 19:43.080
the world.

19:43.080 --> 19:49.080
You might think, like, the most new information is the place that I should start.

19:49.080 --> 19:55.040
I like to use the analogy of, like, you're going to pick a book to reader, a movie to watch.

19:55.040 --> 20:00.320
If I go for the most new information that I could find, maybe you pick like a book

20:00.320 --> 20:04.520
in a language you don't understand on a topic you don't know, and you can learn both

20:04.520 --> 20:10.480
theoretically, but really you can't, because you're missing the base levels of representation

20:10.480 --> 20:11.480
to make sense of it.

20:11.480 --> 20:15.720
If you know a lot about the topic, you can probably pick up some of the words from a

20:15.720 --> 20:20.200
language you don't understand, if you know the language and you don't know the topic,

20:20.200 --> 20:22.160
you can make progress on the topic.

20:22.160 --> 20:27.720
But trying to do two things simultaneously, the intuition that Maria Montessori had is

20:27.720 --> 20:32.680
you're not going to make a lot of traction there, you're not going to make a lot of progress,

20:32.680 --> 20:36.840
and yeah, there's something to that.

20:36.840 --> 20:41.800
That's why that French version of Game of Thrones is sitting on my shelf, only 10 pages

20:41.800 --> 20:42.800
having been read.

20:42.800 --> 20:46.240
How could you speak French?

20:46.240 --> 20:47.240
Not a while ago.

20:47.240 --> 20:48.240
Not enough.

20:48.240 --> 20:49.240
I see.

20:49.240 --> 20:50.240
Yeah.

20:50.240 --> 20:53.520
You don't learn about dire wolves in, you know, university, French class.

20:53.520 --> 20:54.520
Yeah.

20:54.520 --> 20:55.520
Yeah.

20:55.520 --> 20:59.680
Most people have had the experience of trying to absorb information.

20:59.680 --> 21:06.040
They like want to go wise at a high level, but it's just like hard to stick on it.

21:06.040 --> 21:07.520
That's the point of all of this.

21:07.520 --> 21:13.280
So what we're theorizing is that you have built in attentional mechanisms that are guiding

21:13.280 --> 21:17.120
you towards material that won't waste your time.

21:17.120 --> 21:22.800
And if you are encountering something that is a little bit below, where you're at, as

21:22.800 --> 21:26.680
like if it's overly redundant, it's really hard to stay on task for those things.

21:26.680 --> 21:30.840
If you encounter stuff that's beyond where you're at, that should also be similarly difficult

21:30.840 --> 21:32.840
to focus on.

21:32.840 --> 21:37.280
From the perspective of you're trying to not waste time moment to moment, you should seek

21:37.280 --> 21:42.360
out stuff where you're making progress, but it's not too overlapping before you know.

21:42.360 --> 21:50.840
And so the models that you're building is the idea or goal to advance or enhance machine

21:50.840 --> 21:57.320
learning by applying these traditional psychological learning models to, you know, create better machine

21:57.320 --> 22:04.320
learning models or more to try to validate concretely the things that you're observing

22:04.320 --> 22:08.240
experimentally, you know, with these models or both or neither.

22:08.240 --> 22:09.240
Yeah.

22:09.240 --> 22:12.920
So it's multiple things.

22:12.920 --> 22:20.960
Our first goal is to just understand how human systems work from a basic science perspective.

22:20.960 --> 22:29.320
I'm very interested in understanding how people come to know the things that they know

22:29.320 --> 22:35.080
and how beliefs that you form early influence the sampling process downstream.

22:35.080 --> 22:39.960
I just talked about an instance in which your previous experiences shape the knowledge

22:39.960 --> 22:44.760
and the beliefs that you have, the beliefs that you have influence what you're interested

22:44.760 --> 22:50.720
in next, which means that little things that happen early could potentially have really

22:50.720 --> 22:53.000
profound downstream effects.

22:53.000 --> 23:00.240
So we're interested in these systems in humans, but we're also interested in understanding

23:00.240 --> 23:08.080
human belief formation because we're interested in making sure that people can design technologies

23:08.080 --> 23:09.960
that interface well with humans.

23:09.960 --> 23:16.000
I was like, if you design a technology without respect to what we know about humans form,

23:16.000 --> 23:22.440
how humans form beliefs, you run the risks of designing something that's pushing people

23:22.440 --> 23:26.000
away from access to reality.

23:26.000 --> 23:30.960
There's ways in which you might push information to form beliefs that are not right that don't

23:30.960 --> 23:31.960
match the ground.

23:31.960 --> 23:37.920
Truth, one of the things that I talked about in the talk was the relationship between your

23:37.920 --> 23:44.920
subjective sense of certainty and your willingness to seek out new information and also encode

23:44.920 --> 23:45.920
it.

23:45.920 --> 23:52.600
Even if you're not choosing it, if you become very certain, a rational agent, shouldn't

23:52.600 --> 23:53.960
waste time there, right?

23:53.960 --> 24:00.680
As I talked about that in infants, the problem is that sometimes people become certain when

24:00.680 --> 24:01.680
they shouldn't be.

24:01.680 --> 24:10.040
John justified certainty is a thing too, and if people become certain, their curiosity

24:10.040 --> 24:14.920
goes way down, I was like, if we present them information after they're certain, they

24:14.920 --> 24:22.240
don't wait it in the same way, they don't attend to it, it counts for a whole lot less.

24:22.240 --> 24:27.040
If you're designing a system that delivers information to people, it's really important

24:27.040 --> 24:28.520
that you're aware of that.

24:28.520 --> 24:35.320
A lot of platforms make decisions to optimize engagement and make sense that you push

24:35.320 --> 24:39.840
content to people that they appear to once as indicated by them reading it or clicking

24:39.840 --> 24:49.320
it or whatever, but it's potentially dangerous if in doing that, you're giving people more

24:49.320 --> 24:54.560
confirmatory evidence than they would encounter if they sampled randomly from the world.

24:54.560 --> 25:00.560
Our systems were not designed to have information presented that optimized our interests.

25:00.560 --> 25:05.120
Our systems were designed to forage for information in the world.

25:05.120 --> 25:09.080
Second reason we're interested in this is kind of from a cautionary perspective.

25:09.080 --> 25:14.680
It's very important that we understand how human belief formation works so that we can

25:14.680 --> 25:20.640
design technologies that don't mess it up in ways that are bad for individuals on society.

25:20.640 --> 25:26.880
It's very bad if somebody logs on to Facebook, not sure of whether or not they should

25:26.880 --> 25:33.360
vaccinate their kids and walks away thinking that they should not vaccinate their kids.

25:33.360 --> 25:39.160
As people, yeah, I'm not going to, I'm not going to, people should definitely vaccinate

25:39.160 --> 25:42.200
their kids that will go on record saying that.

25:42.200 --> 25:49.080
I was saying if they walked away with the impression that no one vaccinate their kids, that

25:49.080 --> 25:50.560
would be the bad thing.

25:50.560 --> 25:57.640
Yes, those are actually, yeah, I didn't, there's something that wasn't the talk and then

25:57.640 --> 26:03.240
I had to cut for time, you're referring to an inference about what is true for other

26:03.240 --> 26:07.280
people in the population and I did have a slide about that, but I didn't present it in

26:07.280 --> 26:08.600
the talk.

26:08.600 --> 26:16.600
We showed that when somebody enters a search pretty neutral, they form beliefs very quickly.

26:16.600 --> 26:21.640
We used the example of searching for activated charcoal.

26:21.640 --> 26:25.960
If you search for activated charcoal, trying to figure out whether or not that's like a

26:25.960 --> 26:32.200
useful thing to use as a start by being equally likely to say like, maybe it's good, maybe

26:32.200 --> 26:38.640
it's bad, but in just three clicks of about two to three videos, people are all the way

26:38.640 --> 26:44.440
up at like 80%, 90% for thinking it's probably a great thing for wellness.

26:44.440 --> 26:50.240
And the slide that I cut, not only are they forming that belief from like, I'm not sure

26:50.240 --> 26:55.320
to like, okay, I think this is probably right pretty quickly, they're also drawing social

26:55.320 --> 27:02.560
inferences about the prevalence of that belief in the world, given the overrepresentation

27:02.560 --> 27:08.200
of, you know, super scientific materials on all of the streaming platforms, this is

27:08.200 --> 27:13.000
potentially dangerous because it runs the risk of people becoming certain before they

27:13.000 --> 27:17.160
have the chance to encounter disconfirming evidence.

27:17.160 --> 27:18.160
That was number two.

27:18.160 --> 27:19.160
Number three.

27:19.160 --> 27:20.800
I think this is the like way future one.

27:20.800 --> 27:27.040
This is like the like we're like not near this yet, the goal downstream, way downstream

27:27.040 --> 27:34.760
in the future, if you want to design truly, truly intelligent, artificial intelligence,

27:34.760 --> 27:41.120
you want to understand how human systems work because we fail sometimes, because sometimes

27:41.120 --> 27:45.920
we form strong beliefs that are not justified given reality as like sometimes we make bad

27:45.920 --> 27:52.240
decisions, you want to understand what the pitfalls are in human belief formation so that

27:52.240 --> 27:56.000
you can design an intelligent system that doesn't, doesn't have those.

27:56.000 --> 28:01.360
Kind of speaking of AI safety types of research or research directions.

28:01.360 --> 28:02.800
What do you mean by AI safety?

28:02.800 --> 28:08.760
The, I mean, in a sense, an aspect of what you described, if we're heading in a direction

28:08.760 --> 28:15.360
we're rebuilding AGI, what are the, how do we build safeguards into the AGI so that

28:15.360 --> 28:17.600
we protect ourselves as humans, I guess?

28:17.600 --> 28:26.360
I'm not so much thinking of that as I'm thinking of situations in which people don't use the

28:26.360 --> 28:29.120
data in the way they really should.

28:29.120 --> 28:36.440
So for example, it's well documented that doctors, although well intentioned, have gender

28:36.440 --> 28:43.400
and racial biases that prevent them from seeing the evidence objectively black women

28:43.400 --> 28:48.720
are much more likely to die giving birth to a child than white women are, the reason

28:48.720 --> 28:53.440
for that is because when they report that they are in pain, when they report the same symptoms

28:53.440 --> 28:58.240
they're not taken as seriously because of racial biases about black women complaining.

28:58.240 --> 29:04.000
We are increasingly looking to AI to make decisions in the medical field.

29:04.000 --> 29:11.240
People do that badly and as we're introducing AI into these processes, we ideally do not

29:11.240 --> 29:15.680
want to replicate those bad parts of the way humans make those decisions.

29:15.680 --> 29:18.960
So those are the three driving goals for your work.

29:18.960 --> 29:25.880
In your talk you listed or reviewed five conclusions of your work, would you call those conclusions

29:25.880 --> 29:31.520
of the, I would call them lessons lessons lessons walk us through those.

29:31.520 --> 29:40.160
Number one lesson is that people are continuously forming probabilistic expectations.

29:40.160 --> 29:44.760
They are constantly monitoring the statistics of their environment and using those statistics

29:44.760 --> 29:49.400
to inform what they're looking at, what they're listening to, what they're integrating

29:49.400 --> 29:50.920
into their new representations.

29:50.920 --> 29:54.000
You don't learn the concept for something and then you're done.

29:54.000 --> 29:55.000
We open with that.

29:55.000 --> 29:57.320
That was the table example.

29:57.320 --> 30:00.240
And so what were you mentioned that you've got graphs supporting all those?

30:00.240 --> 30:02.640
What were the graphs that tell that story?

30:02.640 --> 30:09.240
That one is the infant work showing that you can use, you can compute surprise over sequential

30:09.240 --> 30:15.400
displays and you get a U-shaped trend between their lookaway behavior and the surprise

30:15.400 --> 30:16.400
about you.

30:16.400 --> 30:17.400
Got it.

30:17.400 --> 30:18.400
Got it.

30:18.400 --> 30:19.400
Okay.

30:19.400 --> 30:22.160
Second point, we also covered is that certainty diminishes interest.

30:22.160 --> 30:29.520
So the evidence for that, I just picked one example from a study by Shirley N. Wade where

30:29.520 --> 30:38.040
we look at people's certainty as they're generating answers to trivia questions and that

30:38.040 --> 30:43.680
the take home message is that when you're very certain that you know the right answer,

30:43.680 --> 30:44.680
you're not curious.

30:44.680 --> 30:47.280
You don't want that information a little bit worse than that.

30:47.280 --> 30:51.480
As we present that information, you're less likely to integrate that.

30:51.480 --> 30:55.680
Once you're certain, you cut and run, you move on to something else.

30:55.680 --> 31:04.200
And that is problematic because sometimes people are certain when they should not be.

31:04.200 --> 31:09.200
So this is a potential explanation for why people sometimes get stuck with stubborn beliefs

31:09.200 --> 31:11.480
that aren't justified in the world.

31:11.480 --> 31:16.000
If you're very certain, it's really hard to get people to go back and reconsider.

31:16.000 --> 31:19.960
The third thing was that certainty is driven by feedback.

31:19.960 --> 31:22.040
So this work actually is the project.

31:22.040 --> 31:27.360
I think we've talked about all of the main lines of research except for this one.

31:27.360 --> 31:33.800
This evidence comes from work by Lewis Marti in which we try to figure out when you feel

31:33.800 --> 31:37.480
very certain where that subjective sense of certainty is coming from.

31:37.480 --> 31:39.160
Oh, that's an interesting question.

31:39.160 --> 31:42.400
Yeah, it is an interesting question.

31:42.400 --> 31:45.120
It did not come out the way that we were expecting.

31:45.120 --> 31:49.240
It did not come out the way that we were expecting at all when we first...

31:49.240 --> 31:51.920
What does that even mean?

31:51.920 --> 31:57.120
Coming from within the brain or something else?

31:57.120 --> 32:01.120
When you feel, so just like how certain do you feel?

32:01.120 --> 32:06.280
Does that influence your behavior and is your certainty some reflection of how certain

32:06.280 --> 32:10.400
you should be given the strength of the evidence?

32:10.400 --> 32:15.840
There are these rational models that show that how certain you should be given the evidence

32:15.840 --> 32:19.880
are good at predicting people's accuracy when they're learning new concepts.

32:19.880 --> 32:26.040
These studies were done in which you ask people to learn new concepts by just observing

32:26.040 --> 32:27.040
evidence.

32:27.040 --> 32:30.840
So as I gave them, you say you're going to learn whether or not something is daxi and

32:30.840 --> 32:35.560
then you give them the examples of things that are daxi or not.

32:35.560 --> 32:36.560
Sometimes the concept is...

32:36.560 --> 32:38.080
Or there's something is what?

32:38.080 --> 32:39.080
Daxi.

32:39.080 --> 32:41.080
We're making up a new concept for these experiments.

32:41.080 --> 32:46.160
We're like, not worrying about the like complicities of the world, we're just like, yeah,

32:46.160 --> 32:47.160
experimental psych.

32:47.160 --> 32:49.800
Just like make up something new that doesn't have any of the confounds or problems or

32:49.800 --> 32:51.320
messiness of real world data.

32:51.320 --> 32:52.320
Okay.

32:52.320 --> 32:57.920
So we ask people to figure out what's daxi and then we give them examples.

32:57.920 --> 33:02.800
And how you learn in these tasks is we just ask you a moment to moment to say like, is

33:02.800 --> 33:04.880
this daxi or is this not?

33:04.880 --> 33:08.600
I'm going to show you some shapes and they vary along some numbers of dimensions.

33:08.600 --> 33:14.720
It sounds like there's going to be different colors, different sizes and different shapes.

33:14.720 --> 33:19.880
And the concept you're going to try to infer just by guessing.

33:19.880 --> 33:22.800
So at the very onset, is this daxi yes or no?

33:22.800 --> 33:23.800
You can't possibly.

33:23.800 --> 33:24.800
No.

33:24.800 --> 33:30.320
You take a shot and say like, yes, and then you get a second and you get feedback, you

33:30.320 --> 33:33.440
get a second example and you keep going.

33:33.440 --> 33:40.000
And what the original studies that used these paradigm showed was that the complexity of

33:40.000 --> 33:44.080
the concept made a difference in terms of people's accuracy as you might expect.

33:44.080 --> 33:49.960
So if the concept is simple, if it's something like red, just varies along one dimension,

33:49.960 --> 33:53.120
people are pretty good at learning that pretty quickly.

33:53.120 --> 33:58.800
If the concept is something more complex, like it could be something like triangle and

33:58.800 --> 34:06.240
red and small or it's like big and blue and whatever.

34:06.240 --> 34:15.800
So the more logical operators, the more difficult it was for people to infer the concepts and

34:15.800 --> 34:17.960
the worst their accuracy.

34:17.960 --> 34:23.680
And those cases, these models were pretty good at predicting people's accuracy and the

34:23.680 --> 34:27.920
amount of data that was required before you could figure out what daxi met.

34:27.920 --> 34:35.480
But those same models as we were hoping would be a good predictor of how certain you feel.

34:35.480 --> 34:41.840
We were expecting that people might be more certain than they should be, they may feel

34:41.840 --> 34:44.520
more certain than they should be given the evidence.

34:44.520 --> 34:51.000
But instead, what we found is that how certain you feel about whether or not the concept

34:51.000 --> 34:56.200
you have in mind being right was pretty divorced from the strength of the evidence.

34:56.200 --> 35:01.180
Instead, the best predictor of how certain you are was whether or not you're getting

35:01.180 --> 35:06.800
stuff right, which is a little disturbing because if you're just saying yes or no, it's

35:06.800 --> 35:11.520
pretty easy just by chance to get a string of answers correct.

35:11.520 --> 35:16.560
If you get a string of answers correct, whatever idea you had in mind, when you got that string

35:16.560 --> 35:22.280
of answers correct, you gain high confidence and the reason why that's problematic is you

35:22.280 --> 35:23.280
don't keep sampling.

35:23.280 --> 35:27.720
As if we give you the option of like moving on to something else you do, you leave the

35:27.720 --> 35:28.720
task.

35:28.720 --> 35:32.680
And you don't actually figure out what daxi means.

35:32.680 --> 35:37.560
If we create a circumstance in which you like keep collecting evidence, you don't wait

35:37.560 --> 35:40.680
at the same as you did before you were certain.

35:40.680 --> 35:48.640
So we think that this is a part of the puzzle in understanding how people sometimes had

35:48.640 --> 35:54.160
their e-stubborn beliefs that aren't justified given evidence in the world.

35:54.160 --> 35:55.640
You start with some idea.

35:55.640 --> 36:03.200
If you get a few pieces of feedback that are consistent with that, you may develop a

36:03.200 --> 36:09.080
high degree of certainty and once you've done that, it may be hard to go back and revise.

36:09.080 --> 36:13.960
This links in with like reason number two, why can't we do this kind of research.

36:13.960 --> 36:20.000
This maybe was less common when you were walking around the world, sampling from, I don't

36:20.000 --> 36:25.400
know, I hesitate to use the term natural environment, but an environment that's not optimized

36:25.400 --> 36:30.040
what you want to, like I'll call it that.

36:30.040 --> 36:37.120
Now if you're going online to form your beliefs, if you have some kind of idea and you watch

36:37.120 --> 36:41.160
some YouTube video, maybe you think like maybe the earth is flat, let me search for that.

36:41.160 --> 36:43.960
You get a few videos that are consistent with that.

36:43.960 --> 36:49.160
The risk is that you could develop a high degree of confidence that that's correct and

36:49.160 --> 36:55.840
once you develop a high degree of confidence, you feel very, very certain your curiosity,

36:55.840 --> 37:00.520
your interest in revising plummets and you may get stuck with that wrong belief.

37:00.520 --> 37:08.760
Do you think that these kind of models for belief formation are inherent and therefore

37:08.760 --> 37:15.720
kind of unchangeable or can we possibly as humans adapt to these new environments that

37:15.720 --> 37:21.680
we're in that are kind of optimizing around our attention and priors?

37:21.680 --> 37:26.000
That is an excellent question that I would like to know the answer to.

37:26.000 --> 37:33.160
As my whole lab, let the lab do the answer to, this is a question that one of the lab

37:33.160 --> 37:39.840
members led one of the people that's now in the lab working on this stuff to the lab.

37:39.840 --> 37:44.520
One of the people in the lab that's a research scientist is someone named Adam Conover who

37:44.520 --> 37:52.640
is predominantly his background is in science communication and also comedy and he was

37:52.640 --> 38:02.720
very interested in how people might have a shot at not forming bad beliefs online.

38:02.720 --> 38:10.440
An idea that we plan to test but are still working out right now is whether or not these

38:10.440 --> 38:17.840
tendencies people have may be able to be mitigated if they're aware of human belief formation

38:17.840 --> 38:22.240
processes or maybe alternatively if they're aware that the systems that are giving them

38:22.240 --> 38:27.800
information aren't the same as the natural world outside.

38:27.800 --> 38:35.240
I have extended family members that do not understand that when they go on Facebook,

38:35.240 --> 38:41.280
this is not a true representative sample of opinions in the world.

38:41.280 --> 38:46.120
It's like they, you know, we might think they should know that as like they should know

38:46.120 --> 38:50.400
that they're only seeing content from people that they opted into seeing they don't understand

38:50.400 --> 38:54.320
the algorithms that back the system, they don't understand that what they hang on longer

38:54.320 --> 38:57.480
influences what they're more likely to see next.

38:57.480 --> 39:03.200
So instead, they're drawing inferences under an incorrect assumption which is that what

39:03.200 --> 39:07.040
they see represents what's true in the world, that's how our systems are made.

39:07.040 --> 39:13.960
It's possible that understanding either something about your internal system, you could maybe

39:13.960 --> 39:19.560
make those bad tendencies like to fall beliefs that are not justified, maybe you could

39:19.560 --> 39:23.200
out mitigate some of those, it's also it's also possible that knowing something about

39:23.200 --> 39:27.800
the back end system, you could adjust, but we really don't know.

39:27.800 --> 39:37.360
I'm revisiting like a routine or an experiment that uses the results of surprise all to kind

39:37.360 --> 39:44.800
of train people to shift their internal belief distribution or something like that.

39:44.800 --> 39:55.840
Right, yes, people are sensitive to their capable of comprehending information about distributions

39:55.840 --> 40:01.560
and drawing correct inferences under different assumptions like replace versus not.

40:01.560 --> 40:08.800
So the fact that we as scientists are capable of learning statistics, it's obviously possible

40:08.800 --> 40:14.360
how much of a difference it could make in these day-to-day moment-to-moment decisions

40:14.360 --> 40:16.320
that you're forming beliefs constantly.

40:16.320 --> 40:22.400
So is yet to be seen, but there's some precedent from the implicit bias literature.

40:22.400 --> 40:28.920
If you are trying to make people less racist, the worst thing that they can do is say like

40:28.920 --> 40:32.200
I'm not racist, I don't see color as I'm not considering it.

40:32.200 --> 40:37.600
If you point out to people that they may have implicit biases, they're not conscious,

40:37.600 --> 40:43.640
but they're influencing their decision-making processes, it doesn't undo the biases,

40:43.640 --> 40:46.280
but they're lessened just through that knowledge.

40:46.280 --> 40:51.800
So it's possible that people knowing something about how they work and how they form beliefs

40:51.800 --> 40:53.760
could make a difference, but we don't know.

40:53.760 --> 40:57.720
All right, so we're on number three, number four.

40:57.720 --> 41:03.080
I talked about the influence of feedback and feedback being the primary driver of how

41:03.080 --> 41:09.960
certain you feel and why this is problematic for forming beliefs everywhere, but especially

41:09.960 --> 41:14.560
on the internet, point number four is that in the absence of feedback, maybe you think

41:14.560 --> 41:19.120
like if the feedback's problematic, we'll just remove it, but that's probably not right

41:19.120 --> 41:20.120
either.

41:20.120 --> 41:24.960
In the absence of feedback, people seem to be overconfident.

41:24.960 --> 41:31.200
And our evidence for that is we look at situations in which people don't get very much feedback.

41:31.200 --> 41:35.360
We wanted to do something naturalistic and more kind of real-worldly.

41:35.360 --> 41:40.960
This is work that was led by Louis Marty that was published in OpenMind very recently.

41:40.960 --> 41:47.080
The domain we came up with is when two people use a word, are they activating the same

41:47.080 --> 41:48.080
concept?

41:48.080 --> 41:51.520
So this is relating to the concept stuff that I talked about.

41:51.520 --> 42:01.000
I talked about how the first set of findings is about how when two people use a word,

42:01.000 --> 42:02.520
they have different concepts in mind.

42:02.520 --> 42:09.440
So like two people don't usually have the same concept in mind for abstract political

42:09.440 --> 42:14.280
kinds of concept like Joe Biden, but they also don't have the same kind of concept for

42:14.280 --> 42:15.280
table either.

42:15.280 --> 42:21.640
We were expecting to find more disagreement about abstract things like people in politics

42:21.640 --> 42:28.640
and less disagreement about concrete objects because you can observe them, but even for

42:28.640 --> 42:33.840
things like table and chair, those things also mean different things to different people.

42:33.840 --> 42:38.080
So many questions that I'm not going to ask due to lack of time.

42:38.080 --> 42:39.080
Oh, you can't.

42:39.080 --> 42:41.600
You want to hear about something else.

42:41.600 --> 42:48.680
We were interested in this, so it looks like people have, there's more than one concept

42:48.680 --> 42:49.680
in the population.

42:49.680 --> 42:57.720
I was like we tried to, we used tools from ecology and various clustering techniques for trying

42:57.720 --> 43:01.320
to find in for the true distribution of clusters of beliefs in the population.

43:01.320 --> 43:05.400
And when you do that, you get something like five to ten for most concepts, although

43:05.400 --> 43:06.400
they're not all the same.

43:06.400 --> 43:08.240
I was like, there's more disagreement about it.

43:08.240 --> 43:12.440
What are examples of the five to ten beliefs people have about tables?

43:12.440 --> 43:16.680
Well, so the data from this, you know, the question that I wanted to ask that I said that

43:16.680 --> 43:22.520
I wouldn't ask it, you know, how do we know that it's not just differences in the way

43:22.520 --> 43:27.000
we describe things as opposed to the fundamental inherent belief about this thing?

43:27.000 --> 43:31.400
Because we control the context, actually, I don't know if that, I started saying that

43:31.400 --> 43:34.760
and then I'm not sure if that's actually the answer to your question.

43:34.760 --> 43:41.320
We get this data, we didn't want people, so first of all, that we don't ask about words

43:41.320 --> 43:44.920
that have synonyms because it could be that it looks like two different concepts, but

43:44.920 --> 43:47.480
people, that's like a different weird situation.

43:47.480 --> 43:56.440
We're asking about things like penguins or people in politics or concrete objects or abstract

43:56.440 --> 44:01.520
concepts depending upon the particular experiment.

44:01.520 --> 44:06.960
We're controlling the context to minimize the chance that when people appear to have two

44:06.960 --> 44:10.760
different concepts, it's because they're imagining a different situation.

44:10.760 --> 44:15.920
Like, instead, we're getting this data from like, here's Donald Trump, who is he more

44:15.920 --> 44:21.560
similar to when you just pick one or the other, so we're fitting the data to binary vectors.

44:21.560 --> 44:27.160
And we're thinking about the concrete example, like a table, I might describe it functionally,

44:27.160 --> 44:30.280
it's the thing that you put things on, I might describe it structurally, it's a thing that

44:30.280 --> 44:32.280
has legs.

44:32.280 --> 44:36.480
That variation, I would say, falls under the context in which you're thinking of a table

44:36.480 --> 44:41.720
and we're controlling for that by saying just pick is a table more similar to a glass

44:41.720 --> 44:43.640
or to a tape recorder.

44:43.640 --> 44:48.360
So we're getting around that by just having you make a judgment with respect to the other

44:48.360 --> 44:52.160
objects to control for exactly, well, that problem and then also the like people imagining

44:52.160 --> 44:53.680
a different situation.

44:53.680 --> 44:58.160
We wouldn't want to conclude that people have two different concepts of table because

44:58.160 --> 45:01.960
somebody is picturing a table that's like sitting there and somebody else is picturing

45:01.960 --> 45:05.040
a table that like someone is standing on, so it's more like a stage or something like

45:05.040 --> 45:06.040
that.

45:06.040 --> 45:11.440
So we control the context by just having you judge how similar a table is to like a glass

45:11.440 --> 45:16.080
and then hopefully everybody has, that's like pretty much the same concept in mind.

45:16.080 --> 45:21.000
I should also maybe add that for this work, in the past most people have thought about

45:21.000 --> 45:24.760
concepts like table as being relatively stable.

45:24.760 --> 45:30.320
It's less important exactly how many concepts there are and more important that there's

45:30.320 --> 45:31.320
not one.

45:31.320 --> 45:38.080
So the way people usually think about these things is just one, instead your concept of

45:38.080 --> 45:44.080
table is slightly changed by every table you encounter in your life and what that means

45:44.080 --> 45:52.480
is that what you'd accept as a table as opposed to a stage, maybe table is a weird example

45:52.480 --> 45:55.160
unless you're going to make a distinction between like our coffee table and like a dining

45:55.160 --> 45:56.160
table.

45:56.160 --> 45:57.680
Right or like a high top.

45:57.680 --> 46:01.920
Right, yeah, a cup of cup of bowl is a classic example.

46:01.920 --> 46:06.040
I mean, I'm imagining many conversations I've had with my wife who grew up in a different

46:06.040 --> 46:10.400
area of refers to things slightly differently and you know, we have these circular

46:10.400 --> 46:15.120
commercial, that's not a XYZ, that's a, you know, ABC, you know, it's a XYZ.

46:15.120 --> 46:21.600
Yes, yeah, that's exactly the, there were some classic studies in psychology that thought

46:21.600 --> 46:24.040
of like cups and bowls is happening on a continuum.

46:24.040 --> 46:28.920
So what counts as a cup and what counts as a bowl maybe depends partially on the material

46:28.920 --> 46:34.080
but then also like the width versus the height, but also like the absolute size.

46:34.080 --> 46:39.480
Or if all the cups are dirty, then my daughter was known to have drunk out of a bowl.

46:39.480 --> 46:40.480
Correct.

46:40.480 --> 46:41.480
Correct.

46:41.480 --> 46:42.480
Now it's a bowl.

46:42.480 --> 46:43.480
Yeah, that's great.

46:43.480 --> 46:44.480
All the bowls are dirty.

46:44.480 --> 46:50.560
Now this cup is a bowl and I've also pulled that one with small children.

46:50.560 --> 46:56.440
So yeah, so the where you draw the line, you might expect people had pretty similar

46:56.440 --> 47:02.360
places, but it looks like it's more different than you might expect even for the concrete

47:02.360 --> 47:03.360
objects.

47:03.360 --> 47:07.840
So where I would draw the line at the cup bowl distinction depending on the relationship

47:07.840 --> 47:14.560
of absolute size and height and width is probably different from some substantial portion

47:14.560 --> 47:19.920
of the population and where people draw the line along all of these dimensions that you

47:19.920 --> 47:25.160
can cluster them together and try to come up with an estimate for roughly how many overall

47:25.160 --> 47:26.600
concepts there are.

47:26.600 --> 47:27.600
Okay.

47:27.600 --> 47:29.400
So yeah, big point is there's not one.

47:29.400 --> 47:30.400
Right, right, right.

47:30.400 --> 47:35.760
Like for even things like penguin and cup and bowl and the question we were most interested

47:35.760 --> 47:43.200
in asking is given that there is variation when two people use one word, they're not necessarily

47:43.200 --> 47:46.680
activating the same concept, are people aware of that possibility?

47:46.680 --> 47:50.720
Like do I know when I say something, you may not have the same thing in mind?

47:50.720 --> 47:51.720
And the answer.

47:51.720 --> 47:52.720
Back to points two, three.

47:52.720 --> 47:53.720
Right.

47:53.720 --> 47:55.720
The answer is no.

47:55.720 --> 48:01.560
There's a really fun like sub-finding which is people generally overestimate the degree

48:01.560 --> 48:07.080
to which they believe that their concept will align with somebody else's.

48:07.080 --> 48:11.560
But if you have a weirdo deviant definition of something, if you're activating a concept

48:11.560 --> 48:15.560
that's like very different from other people in the population, you actually have a better

48:15.560 --> 48:20.640
chance at being aware of that, which is not what I would have guessed if you talk to somebody

48:20.640 --> 48:28.080
that's like using words weirdly, like you might expect it's because they are unaware of

48:28.080 --> 48:33.640
that deviant usage as like they're more likely to be aware that that is a weird way of,

48:33.640 --> 48:35.440
that's a weird concept that they have.

48:35.440 --> 48:41.200
Is all this related to the, I forget the name of the law, like people who, you know, experts

48:41.200 --> 48:42.200
perceive there.

48:42.200 --> 48:43.200
Oh, done in Kruger.

48:43.200 --> 48:44.200
Yeah, done in Kruger.

48:44.200 --> 48:45.200
Yeah.

48:45.200 --> 48:46.200
It's possible.

48:46.200 --> 48:49.440
There's definitely connection to this.

48:49.440 --> 48:57.280
So the done Kruger effect is if you are very competent, you're more likely to be aware

48:57.280 --> 49:03.360
of the areas in which you have incompetence, but if you're totally clueless, you are not

49:03.360 --> 49:04.680
aware of all the words.

49:04.680 --> 49:12.640
So if you're estimating your own competence, it doesn't go well given those dynamics.

49:12.640 --> 49:19.520
So I don't know, it's not exactly the same thing, but that is an interesting point.

49:19.520 --> 49:27.040
You might think that it's a bad design if people, their estimates of their own incompetence

49:27.040 --> 49:34.440
are not well matched to how incompetent they actually are, but Charlene Wade's research

49:34.440 --> 49:40.440
from my lab showing that people are most curious when they believe that they're about to

49:40.440 --> 49:45.000
know the answer, when they believe that they're about to know everything.

49:45.000 --> 49:51.640
Those two things taken together might actually indicate that that's a feature, not a bug.

49:51.640 --> 49:59.120
If you at the beginning of learning something new, you were acutely aware of how incompetent

49:59.120 --> 50:00.120
you were.

50:00.120 --> 50:04.360
I was like, what are data from the lab would say is that you would not be motivated to

50:04.360 --> 50:06.160
take that first step.

50:06.160 --> 50:07.640
You may never try to approach it.

50:07.640 --> 50:13.240
It actually may be a good thing from a motivational standpoint that when you first start out

50:13.240 --> 50:20.480
learning something new, you don't know how much you don't know as to have your incompetence

50:20.480 --> 50:26.320
revealed incrementally, we would predict is the gives you the best chance of continuing

50:26.320 --> 50:27.320
support.

50:27.320 --> 50:29.480
It's a very quickly that fifth point.

50:29.480 --> 50:33.320
That fifth point, people form beliefs very quickly.

50:33.320 --> 50:40.240
It's possible for people to go from completely undecided to believing that they should buy

50:40.240 --> 50:45.320
the weird black smoothie bagel, those things that are in the expensive coffee shops that

50:45.320 --> 50:53.240
have charcoal added to them for unclear pseudo-scientific wellness purposes.

50:53.240 --> 51:01.640
It doesn't take very long to form that belief with high confidence and so when we're designing

51:01.640 --> 51:06.600
systems that offer information to people, it's really important that we keep that in

51:06.600 --> 51:12.680
mind when somebody's certain, they stop searching, when they see disconfirming evidence after

51:12.680 --> 51:15.200
that point, it doesn't count for the same.

51:15.200 --> 51:21.840
So I think the takeaway of all of this, the big point that I wanted to make in the talk

51:21.840 --> 51:30.360
was that when you hear people say that this tool or this platform is neutral, that's,

51:30.360 --> 51:38.600
I would say dishonest, we know that there are ways of making decisions behind the scenes

51:38.600 --> 51:40.360
that influence people's behavior.

51:40.360 --> 51:45.560
It's like all of us design things that change behavior as like whether or not you're making

51:45.560 --> 51:50.480
different decisions about how to present different options of suggested items that people might

51:50.480 --> 51:58.200
purchase or whether you are trying to keep people on your site for longer.

51:58.200 --> 52:04.360
If people's behavior is changing as a function of different design decisions you make or

52:04.360 --> 52:09.680
different types of things that you optimize, it's important to remember that the mediating

52:09.680 --> 52:15.280
variable there is human beliefs, you're messing with human beliefs and you're messing with

52:15.280 --> 52:20.760
what people know and they walk away into the real world to make real world decisions with

52:20.760 --> 52:21.880
those changed beliefs.

52:21.880 --> 52:29.600
So there is no neutral platform, I think that's a really ill-conceived way of thinking about

52:29.600 --> 52:35.680
these problems, I think it's irresponsible, it's really important to appreciate that the

52:35.680 --> 52:41.160
way you present information, the order in which you present information, it really matters

52:41.160 --> 52:42.160
and it has profound impacts.

52:42.160 --> 52:47.720
It's a really important result, I think we tend to think about the kind of influences

52:47.720 --> 52:53.560
you're describing as just influencing discrete behaviors or actions as opposed to fundamentally

52:53.560 --> 52:54.560
beliefs.

52:54.560 --> 52:55.560
Right.

52:55.560 --> 52:56.960
And they're very different.

52:56.960 --> 53:05.040
And you can't change behavior without altering people's beliefs and it's important to

53:05.040 --> 53:12.680
keep that in mind and to treat that duty with the respect it deserves.

53:12.680 --> 53:13.680
Right.

53:13.680 --> 53:14.680
Really interesting stuff.

53:14.680 --> 53:16.840
We could go on for another hour, I'm sure.

53:16.840 --> 53:17.840
Yeah.

53:17.840 --> 53:22.920
Well Celeste, thanks so much for taking some time out of your busy nirups to come chat

53:22.920 --> 53:25.480
with us and share a bit about what you're up to.

53:25.480 --> 53:27.680
Of course, thank you so much for having me.

53:27.680 --> 53:28.680
Thank you.

53:28.680 --> 53:33.680
All right everyone, that's our show for today.

53:33.680 --> 53:38.840
For more information on today's guest or our NIRP's podcast series, head over to

53:38.840 --> 53:43.600
twimlai.com slash NIRP's 2019.

53:43.600 --> 53:47.320
Thanks once again to Shell for sponsoring this week's series.

53:47.320 --> 53:53.440
Check out the Shell.ai Residency program by typing Shell.ai into your browser's address

53:53.440 --> 53:54.440
bar.

53:54.440 --> 54:04.440
Thanks so much for listening, happy holidays, and catch you next time.

