1
00:00:00,000 --> 00:00:15,920
Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting

2
00:00:15,920 --> 00:00:20,880
people doing interesting things in machine learning and artificial intelligence.

3
00:00:20,880 --> 00:00:23,600
I'm your host Sam Charrington.

4
00:00:23,600 --> 00:00:28,160
This week on the podcast, we're featuring a series of conversations from the Nips conference

5
00:00:28,160 --> 00:00:30,400
in Long Beach, California.

6
00:00:30,400 --> 00:00:34,360
This was my first time at Nips and I had a great time there.

7
00:00:34,360 --> 00:00:37,600
I attended a bunch of talks and of course learned a ton.

8
00:00:37,600 --> 00:00:43,760
I organized an impromptu roundtable on building AI products and I met a bunch of wonderful

9
00:00:43,760 --> 00:00:47,920
people, including some former Twimble Talk guests.

10
00:00:47,920 --> 00:00:52,720
I'll be sharing a bit more about my experiences at Nips via my newsletter, which you should

11
00:00:52,720 --> 00:00:59,400
take a second right now to subscribe to at twimblei.com slash newsletter.

12
00:00:59,400 --> 00:01:05,080
This week, through the end of the year, we're running a special listener appreciation contest

13
00:01:05,080 --> 00:01:09,800
to celebrate hitting 1 million listens on the podcast and to thank you all for being

14
00:01:09,800 --> 00:01:11,760
so awesome.

15
00:01:11,760 --> 00:01:16,440
Tweet to us using the hashtag Twimble1Mill to enter.

16
00:01:16,440 --> 00:01:21,080
Everyone who enters is a winner and we're giving away a bunch of cool Twimble swag and

17
00:01:21,080 --> 00:01:23,080
other mystery prizes.

18
00:01:23,080 --> 00:01:29,880
If you're not on Twitter or want more ways to enter, visit twimblei.com slash twimble1Mill

19
00:01:29,880 --> 00:01:32,320
for the full rundown.

20
00:01:32,320 --> 00:01:36,680
Before we dive in, I'd like to thank our friends over at Intel Nirvana for their sponsorship

21
00:01:36,680 --> 00:01:39,920
of this podcast and our Nips series.

22
00:01:39,920 --> 00:01:44,920
While Intel was very active at Nips with a bunch of workshops, demonstrations and poster

23
00:01:44,920 --> 00:01:50,640
sessions, their big news this time was the first public viewing of the Intel Nirvana

24
00:01:50,640 --> 00:01:54,440
neural network processor or NNP.

25
00:01:54,440 --> 00:01:59,360
The goal of the NNP architecture is to provide the flexibility needed to support deep learning

26
00:01:59,360 --> 00:02:04,480
primitives while making the core hardware components as efficient as possible, giving

27
00:02:04,480 --> 00:02:09,960
neural network designers powerful tools for solving larger and more difficult problems

28
00:02:09,960 --> 00:02:14,600
while minimizing data movement and maximizing data reuse.

29
00:02:14,600 --> 00:02:20,440
To learn more about Intel's AI products group and the Intel Nirvana NNP, visit Intel

30
00:02:20,440 --> 00:02:22,720
Nirvana.com.

31
00:02:22,720 --> 00:02:28,440
This time around, I'm joined by Matthew Crosby, a researcher at Imperial College London,

32
00:02:28,440 --> 00:02:31,880
working on the kinds of intelligence project.

33
00:02:31,880 --> 00:02:36,120
Matthew joined me after the Nips symposium of the same name, an event that brought researchers

34
00:02:36,120 --> 00:02:41,880
from a variety of disciplines together towards three aims, a broader perspective on the possible

35
00:02:41,880 --> 00:02:47,480
types of intelligence beyond human intelligence, better measurements of intelligence, and

36
00:02:47,480 --> 00:02:54,000
more purposeful analysis of where progress should be made in AI to best benefit society.

37
00:02:54,000 --> 00:02:59,280
Matthew's research explores intelligence from a philosophical perspective, exploring ideas

38
00:02:59,280 --> 00:03:04,760
like predictive processing and controlled hallucination and how these theories of intelligence

39
00:03:04,760 --> 00:03:08,760
impact the way we approach creating artificial intelligence.

40
00:03:08,760 --> 00:03:13,200
This was a very interesting conversation and one that I'm sure you'll get a kick out

41
00:03:13,200 --> 00:03:14,200
of.

42
00:03:14,200 --> 00:03:23,320
And now on to the show.

43
00:03:23,320 --> 00:03:27,400
Alright everyone, I am here in Long Beach, California at the Nips Conference, and I have

44
00:03:27,400 --> 00:03:33,160
the pleasure of being joined by Matthew Crosby, research associate at Imperial College London.

45
00:03:33,160 --> 00:03:37,160
Matthew, welcome to the podcast, thank you very much.

46
00:03:37,160 --> 00:03:39,880
So why don't we get started by having you tell us a little bit about your background

47
00:03:39,880 --> 00:03:42,840
and how you got involved in AI.

48
00:03:42,840 --> 00:03:49,960
So I was always very, very interested in intelligence as a concept, and I had a fairly mathematical

49
00:03:49,960 --> 00:03:55,920
sort of upbringing and start, and so my first thought was, I'm going to explore intelligence

50
00:03:55,920 --> 00:04:00,680
through mathematics and computer science, and so I worked for a while in high level planning

51
00:04:00,680 --> 00:04:08,040
in AI, and over the course of the time working on that stuff, I sort of got a bit disillusioned

52
00:04:08,040 --> 00:04:12,320
with the fact that AI was less about intelligence than I was hoping it would be.

53
00:04:12,320 --> 00:04:17,560
And of course for me, intelligence always spoke in some sense to human-like intelligence,

54
00:04:17,560 --> 00:04:22,120
to this disability to sort of experience the world and plan in the world, that's why

55
00:04:22,120 --> 00:04:27,280
I was working in planning, but to form ideas about what you're going to do and represent

56
00:04:27,280 --> 00:04:31,160
this picture of the world in front of you, and that was a fundamental part of intelligence.

57
00:04:31,160 --> 00:04:32,160
No, not quite there yet.

58
00:04:32,160 --> 00:04:36,360
Well, it wasn't being covered in the part of the field that I was working on, and then

59
00:04:36,360 --> 00:04:39,800
as I was sort of becoming a bit disillusioned with that, I found these theories in philosophy

60
00:04:39,800 --> 00:04:44,520
which I'd been dabbling in, which suddenly spoke to what I felt, you know, my experience

61
00:04:44,520 --> 00:04:48,640
of the world was like, and what intelligence actually was, and it was the first time

62
00:04:48,640 --> 00:04:53,080
when I thought, no, people are actually explaining this at a level where it's making sense to

63
00:04:53,080 --> 00:04:56,640
me, and it's progressing us further to all the better understanding.

64
00:04:56,640 --> 00:04:57,640
Okay.

65
00:04:57,640 --> 00:05:01,840
So I just completely transitioned, went back and re-learned philosophy so that I could

66
00:05:01,840 --> 00:05:04,840
understand these theories and philosophy of mind better.

67
00:05:04,840 --> 00:05:10,320
And while I was doing that, we were seeing all these advances now in neural networks and

68
00:05:10,320 --> 00:05:14,120
in deep learning and machine learning, where we are actually approaching something that

69
00:05:14,120 --> 00:05:18,680
looks like intelligence in a way that we can talk about, especially philosophically,

70
00:05:18,680 --> 00:05:22,640
that's interesting and like makes sense and does speak to what I was interested in in

71
00:05:22,640 --> 00:05:24,040
terms of intelligence.

72
00:05:24,040 --> 00:05:28,120
So now after making this transition to philosophy, I'm back in a sort of partial role in between

73
00:05:28,120 --> 00:05:34,120
the two, looking at intelligence in AI, but from a more philosophical perspective.

74
00:05:34,120 --> 00:05:38,200
And so how does that translate into a research path for you?

75
00:05:38,200 --> 00:05:43,120
Well, one of the problems with that is that's obviously a giant research project.

76
00:05:43,120 --> 00:05:48,640
So the project I'm on right now is the kinds of intelligence project, which is a sub-project

77
00:05:48,640 --> 00:05:51,640
at the Level Hume Center for the Future for Intelligence.

78
00:05:51,640 --> 00:05:56,840
This is a large 10 million pound project in the UK, mainly based at Cambridge, where

79
00:05:56,840 --> 00:06:01,480
the idea is to take people from multiple disciplines and look at the future of intelligence

80
00:06:01,480 --> 00:06:06,200
in the path we're on towards the future of intelligence and take expertise from AI,

81
00:06:06,200 --> 00:06:12,200
but also from philosophy, political science, literature, and all the broad range of subjects

82
00:06:12,200 --> 00:06:14,840
that could have something to say about intelligence.

83
00:06:14,840 --> 00:06:21,240
And use it to shape research in the future, shape policy decisions, and make sure that

84
00:06:21,240 --> 00:06:26,040
we are moving towards a future that's best for everyone in terms of intelligence.

85
00:06:26,040 --> 00:06:30,720
The kinds of intelligence is also the name of a symposium that you could organize here

86
00:06:30,720 --> 00:06:31,720
at NIPS.

87
00:06:31,720 --> 00:06:34,120
Tell us a little bit about that and the goals for it.

88
00:06:34,120 --> 00:06:39,120
So the goals of the kinds of intelligence project are to map this space of intelligence

89
00:06:39,120 --> 00:06:42,520
so that we can better understand the intelligence landscape.

90
00:06:42,520 --> 00:06:46,760
And the kinds of intelligence symposium and NIPS, the idea was to bring a lot of people

91
00:06:46,760 --> 00:06:50,360
who are at the forefront of different types of intelligence together into the same place

92
00:06:50,360 --> 00:06:54,400
and so we can have this conversation and look at the way the intelligence landscape is

93
00:06:54,400 --> 00:06:59,120
shaped, both from plant cognition to animal cognition to child development, which are all

94
00:06:59,120 --> 00:07:04,240
very important parts of understanding intelligence to obviously being at NIPS, AI and machine learning

95
00:07:04,240 --> 00:07:07,680
and the type of intelligence we can now create.

96
00:07:07,680 --> 00:07:12,360
We had people talking like Demis Hassebus from DeepMind, who's talking about alpha zero,

97
00:07:12,360 --> 00:07:17,920
which is now solving, go and chess and show the levels way beyond human intelligence.

98
00:07:17,920 --> 00:07:22,560
And we also had people talking about plant cognition and the way if you drop certain

99
00:07:22,560 --> 00:07:27,880
plants from a height over time, they can learn a response to curl up in protection.

100
00:07:27,880 --> 00:07:31,480
And falling on the floor, you drop them enough times, eventually they learn to anticipate

101
00:07:31,480 --> 00:07:34,080
what is going to happen and curl up ahead of time.

102
00:07:34,080 --> 00:07:35,080
Oh wow.

103
00:07:35,080 --> 00:07:37,800
So this is a form of learning that doesn't have any neurons involved and it's very alien

104
00:07:37,800 --> 00:07:40,680
to the type of learning that we generally think about.

105
00:07:40,680 --> 00:07:42,080
Huh, interesting.

106
00:07:42,080 --> 00:07:47,880
And so your personal kind of slice through this is from a philosophical perspective and

107
00:07:47,880 --> 00:07:54,320
you mentioned some kind of a body of work or research within philosophy that you stumbled

108
00:07:54,320 --> 00:07:55,320
across.

109
00:07:55,320 --> 00:07:56,320
What is that?

110
00:07:56,320 --> 00:08:01,240
So the general term for this is predictive processing and it's the idea that when we take

111
00:08:01,240 --> 00:08:06,120
in sensory data from the world, we have this huge jumble of messy information coming into

112
00:08:06,120 --> 00:08:07,120
the system, right?

113
00:08:07,120 --> 00:08:12,400
We have 130 million or so photo receptors in each eye, all transducing electromagnetic

114
00:08:12,400 --> 00:08:16,600
information and somehow the brain has to make sense of that and understand it.

115
00:08:16,600 --> 00:08:21,440
And at some point, it's, understands it in a way that we sort of experience the world

116
00:08:21,440 --> 00:08:23,640
and that's how that happened.

117
00:08:23,640 --> 00:08:27,760
But the old sort of very old view was that this information comes into the system and

118
00:08:27,760 --> 00:08:31,720
in a very bottom up way, it gets pieced together more and more and more complicated as it goes

119
00:08:31,720 --> 00:08:36,600
up through the system and eventually you get ideas such as tables and chairs and the

120
00:08:36,600 --> 00:08:39,080
kind of objects that we feel like that we see.

121
00:08:39,080 --> 00:08:42,920
Predictive processing idea sort of turns that on its head and says, we're not in the

122
00:08:42,920 --> 00:08:46,920
process of sort of taking these components of information and putting them together.

123
00:08:46,920 --> 00:08:50,040
We're actively trying to work out what we're going to experience.

124
00:08:50,040 --> 00:08:55,640
We're predicting the incoming sensory information and actively doing so, we're always trying

125
00:08:55,640 --> 00:08:57,480
to work out what's going to happen next in the world.

126
00:08:57,480 --> 00:09:02,120
And by turning it that way around and looking at how we could actively predict, we see

127
00:09:02,120 --> 00:09:07,280
that our experience of the world takes the form of what Annalceta is called a controlled

128
00:09:07,280 --> 00:09:08,280
hallucination.

129
00:09:08,280 --> 00:09:12,000
And this phrase is becoming much more, much more popular nowadays.

130
00:09:12,000 --> 00:09:16,160
So the idea is in a hallucination, you're just making stuff up, right?

131
00:09:16,160 --> 00:09:19,920
Maybe your brain is making stuff up that's not really there and that's what you see,

132
00:09:19,920 --> 00:09:20,920
right?

133
00:09:20,920 --> 00:09:25,440
In a controlled hallucination, you're making stuff up just as you were in a real hallucination,

134
00:09:25,440 --> 00:09:27,520
but it's controlled by the actual sensory data.

135
00:09:27,520 --> 00:09:28,520
It's up.

136
00:09:28,520 --> 00:09:33,000
So there's no real difference from me seeing this table in front of me right now to hallucinating

137
00:09:33,000 --> 00:09:37,640
one except for the fact that there's a ground truth from the sensory data that is binding

138
00:09:37,640 --> 00:09:38,640
it together.

139
00:09:38,640 --> 00:09:42,560
So that hopefully when I see a table there, from your perspective, you're also seeing

140
00:09:42,560 --> 00:09:43,880
a similar table.

141
00:09:43,880 --> 00:09:48,920
So what are the implications of seeing, you know, seeing cognition as this controlled

142
00:09:48,920 --> 00:09:49,920
hallucination process?

143
00:09:49,920 --> 00:09:54,560
There's a huge number of implications from this and I think that's one of the beauties

144
00:09:54,560 --> 00:09:58,360
of the theory and probably one of the potential downfalls of the theory too is that it can

145
00:09:58,360 --> 00:10:00,560
apply it to so many different levels across the brain.

146
00:10:00,560 --> 00:10:06,000
And also in relation to machine learning, we're seeing obviously a huge focus on predictive

147
00:10:06,000 --> 00:10:10,360
algorithms and on generative models, which are generating predictions about the world

148
00:10:10,360 --> 00:10:14,200
or the sensory data or the data that they're being input.

149
00:10:14,200 --> 00:10:18,920
So we can think of this as like a very low level, like in the retina at the back of the

150
00:10:18,920 --> 00:10:25,080
eye, we're doing what is called predictive coding, which is whenever I get say a particular

151
00:10:25,080 --> 00:10:31,040
rod cell is hit by electromagnetic radiation, it has the amount of information of the intensity

152
00:10:31,040 --> 00:10:34,640
of the light that it can transfer up further in the brain, right?

153
00:10:34,640 --> 00:10:38,280
That could be a large number of different values that this takes.

154
00:10:38,280 --> 00:10:42,720
But if instead of transferring that value, I look at what I would expect just by looking

155
00:10:42,720 --> 00:10:48,040
locally at all the values of the rod cells or the cone cells around me, I can take the

156
00:10:48,040 --> 00:10:53,760
average of that and see how much that particular cell is different from that average, then you

157
00:10:53,760 --> 00:10:58,840
will get a much smaller number, which increases the bandwidth that you, well, decreases the

158
00:10:58,840 --> 00:11:01,760
bandwidth that you need to use to send the same amount of information.

159
00:11:01,760 --> 00:11:08,040
And so are you describing a theory of what is actually happening physiologically or are

160
00:11:08,040 --> 00:11:13,880
you describing a modeling approach or this is a serious point happening in the eyes

161
00:11:13,880 --> 00:11:14,880
or something?

162
00:11:14,880 --> 00:11:19,000
A bit like that, yes, but so I was starting at the point where this is actually, yes,

163
00:11:19,000 --> 00:11:22,440
we know this is happening at a neurophysiological level.

164
00:11:22,440 --> 00:11:26,320
And I was going to move on to the complete other side where this approach can be applied

165
00:11:26,320 --> 00:11:31,480
to beliefs and desires and where our beliefs and desires are updated in a similar sort of

166
00:11:31,480 --> 00:11:33,120
predictive format.

167
00:11:33,120 --> 00:11:37,360
But before I move on, one thing with this predictive coding approach to the retina is if

168
00:11:37,360 --> 00:11:42,280
you look at it, it is very much like convolutions that we're seeing in machine learning.

169
00:11:42,280 --> 00:11:45,480
And the way they work is a very similar approach.

170
00:11:45,480 --> 00:11:49,320
They're obviously a bit more focused on invariance in the visual field and how we can apply the

171
00:11:49,320 --> 00:11:52,880
same map at different locations, which is another area of this.

172
00:11:52,880 --> 00:11:58,160
But they could also be used to do a similar approach to predicting local variations and

173
00:11:58,160 --> 00:12:02,960
only transmitting information about that variation as opposed to the raw data itself.

174
00:12:02,960 --> 00:12:08,840
So that's one level of it, but at the other level, we have the idea that our beliefs are

175
00:12:08,840 --> 00:12:13,400
updated in a similar way and our high level understanding of the world is based on these

176
00:12:13,400 --> 00:12:15,400
predictions that we're making.

177
00:12:15,400 --> 00:12:19,160
And comparing to the data coming in and when it's wrong, we have two choices.

178
00:12:19,160 --> 00:12:23,560
We either update our prediction and therefore change our model of the world and see the

179
00:12:23,560 --> 00:12:24,560
world differently.

180
00:12:24,560 --> 00:12:28,000
Or we act in the world and move around the world.

181
00:12:28,000 --> 00:12:31,360
And that might make our prediction that was wrong, turn into a prediction that was

182
00:12:31,360 --> 00:12:32,920
correct.

183
00:12:32,920 --> 00:12:38,000
So for example, if I predict that table is off to my right, there's two ways I can make

184
00:12:38,000 --> 00:12:39,000
that true.

185
00:12:39,000 --> 00:12:43,880
Well, I can turn, I can change my prediction, I can be, no, it's wrong, it's to my left

186
00:12:43,880 --> 00:12:45,120
and then update it that way.

187
00:12:45,120 --> 00:12:49,640
Or I could move my head and that would make the say another way of making the same prediction

188
00:12:49,640 --> 00:12:50,640
right.

189
00:12:50,640 --> 00:12:54,480
And I think this is a way that we could bring actions and interacting with the world

190
00:12:54,480 --> 00:12:59,960
into our understanding in, especially in machine learning and robotic, of how we can incorporate

191
00:12:59,960 --> 00:13:05,040
this sort of predictive approach to the whole brain or the whole way of modeling the world

192
00:13:05,040 --> 00:13:09,320
into a system that's not just understanding the world, but also acting in it and performing

193
00:13:09,320 --> 00:13:10,320
task.

194
00:13:10,320 --> 00:13:13,320
And therefore, being intelligent, hopefully.

195
00:13:13,320 --> 00:13:19,160
Is your research kind of approaching this from a theoretical perspective exclusively

196
00:13:19,160 --> 00:13:24,440
or is it there an experimental element or applied element as well?

197
00:13:24,440 --> 00:13:28,680
So I have been running experiments with predictive coding style on your own network.

198
00:13:28,680 --> 00:13:29,680
Okay.

199
00:13:29,680 --> 00:13:35,920
So you coming out recently based on this structure where you have a hierarchical generative network

200
00:13:35,920 --> 00:13:41,080
and each layer of the hierarchy is just trying to predict anything that the lower layers

201
00:13:41,080 --> 00:13:42,920
have so far failed to predict.

202
00:13:42,920 --> 00:13:47,320
So the first layer tries to predict the world, but it doesn't, it's not strong enough

203
00:13:47,320 --> 00:13:50,280
by itself to fully model everything.

204
00:13:50,280 --> 00:13:51,280
So then what it can't predict.

205
00:13:51,280 --> 00:13:52,280
Can you give an example?

206
00:13:52,280 --> 00:13:53,280
Can you give an example?

207
00:13:53,280 --> 00:13:54,280
Kind of the experiment.

208
00:13:54,280 --> 00:13:57,040
Like what it, what specifically is it trying to predict?

209
00:13:57,040 --> 00:14:03,720
So there's been work on this in experiments from video data, from car images for videos

210
00:14:03,720 --> 00:14:08,200
on top of a car moving around, driving around, you give it the first nine frames and ask

211
00:14:08,200 --> 00:14:09,840
it to predict the 10th.

212
00:14:09,840 --> 00:14:13,720
And then you can get fairly good results at predicting, you know, how the road is going

213
00:14:13,720 --> 00:14:15,800
to have moved and the things that will have come into you.

214
00:14:15,800 --> 00:14:16,800
Okay.

215
00:14:16,800 --> 00:14:19,880
I've been experimenting with this in maze like three dimensional domains working on the

216
00:14:19,880 --> 00:14:25,600
raw pixel inputs and predicting how the maze is going to update or how the pixels are

217
00:14:25,600 --> 00:14:27,400
going to update as I move around this maze.

218
00:14:27,400 --> 00:14:28,400
Okay.

219
00:14:28,400 --> 00:14:34,120
I spoke with a researcher working on something very similar like she was looking at it from

220
00:14:34,120 --> 00:14:37,680
the perspective of like embodied computer vision.

221
00:14:37,680 --> 00:14:44,680
So not just fixed frames, but you know, fixed frames plus the ability to move the orientation

222
00:14:44,680 --> 00:14:51,920
of the viewport, if you will, and one of the sets of experiments or use cases was this

223
00:14:51,920 --> 00:14:55,480
kind of sensor mounted on a car that was changing direction and trying to do the

224
00:14:55,480 --> 00:14:56,480
prediction.

225
00:14:56,480 --> 00:14:57,480
Things like that.

226
00:14:57,480 --> 00:14:58,480
Yeah.

227
00:14:58,480 --> 00:14:59,480
Interesting.

228
00:14:59,480 --> 00:15:03,760
And so you've got this scenario with, you know, the sick, the video case, you've got

229
00:15:03,760 --> 00:15:07,640
this scenario where you've got the camera on the vehicle and you're trying to predict

230
00:15:07,640 --> 00:15:08,800
ahead.

231
00:15:08,800 --> 00:15:13,360
How does that then tie into this hierarchical structure that you were describing?

232
00:15:13,360 --> 00:15:18,760
Well, the idea of the hierarchy is that you'll have the first layer of your network would

233
00:15:18,760 --> 00:15:23,800
be not have enough potential space inside it to fully predict everything that's going

234
00:15:23,800 --> 00:15:24,800
on.

235
00:15:24,800 --> 00:15:29,280
It's going to have space to represent the full function of the mapping of the change of

236
00:15:29,280 --> 00:15:30,880
the inputs over time.

237
00:15:30,880 --> 00:15:35,720
So then anything that it can predict, you're not interested in that anymore because that's

238
00:15:35,720 --> 00:15:36,720
sort of, that's done.

239
00:15:36,720 --> 00:15:40,120
That gets, that gets sort of finished at that layer of the system.

240
00:15:40,120 --> 00:15:43,840
But anything that it doesn't get predicted, this is called the prediction error, will get

241
00:15:43,840 --> 00:15:45,880
sent up to the next layer of the system.

242
00:15:45,880 --> 00:15:50,240
And this is the, if everything's working as intended, this is the parts that are a bit

243
00:15:50,240 --> 00:15:54,320
more complex than could be predicted just really easily, like, you know, something this

244
00:15:54,320 --> 00:15:57,800
pixel change always stays the same, so I'm just going to predict it stays the same.

245
00:15:57,800 --> 00:16:03,000
But then something slowly moving across the visual field or the input space in this video

246
00:16:03,000 --> 00:16:06,680
might be impossible to predict at that lower level, but at a higher level, once you've

247
00:16:06,680 --> 00:16:11,520
removed the easy stuff and you've got access to more machinery for sure, layers deep into

248
00:16:11,520 --> 00:16:13,760
the system, you might be able to predict that.

249
00:16:13,760 --> 00:16:18,720
And the idea is that as you move up the hierarchy, you'll get more high level representations

250
00:16:18,720 --> 00:16:21,600
of elements of the world that are changing.

251
00:16:21,600 --> 00:16:25,040
So the low level would be very simple stuff, the high level could potentially be things

252
00:16:25,040 --> 00:16:29,400
changing, you know, very slowly over time or changing, you know, more modeling, like

253
00:16:29,400 --> 00:16:32,920
the intuitive physics going on behind the domain that you might expect.

254
00:16:32,920 --> 00:16:37,280
Oh, this object's hitting something, so now it's going to change direction.

255
00:16:37,280 --> 00:16:41,920
The lower level might think, oh, it's just going to continue going on in the same direction.

256
00:16:41,920 --> 00:16:45,480
And then when it gets that wrong, the higher level, which hopefully has representation

257
00:16:45,480 --> 00:16:49,880
of the physics, will be able to correct for that and say, no, this is how it's going

258
00:16:49,880 --> 00:16:50,880
to change.

259
00:16:50,880 --> 00:16:56,040
So what you're describing sounds like, you sounds like what I think of a, you know, maybe

260
00:16:56,040 --> 00:17:00,120
a very deep convolutional net, right, that's going to figure out different things at different

261
00:17:00,120 --> 00:17:01,120
levels.

262
00:17:01,120 --> 00:17:06,400
How is it different, or are you doing things to kind of force it to learn certain aspects

263
00:17:06,400 --> 00:17:07,400
in certain layers?

264
00:17:07,400 --> 00:17:13,440
Yes, I think the beauty of the research is that at one level, you could think of it as

265
00:17:13,440 --> 00:17:15,920
just as a very deep convolutional net.

266
00:17:15,920 --> 00:17:20,600
But the one thing that is different is your focusing on this particular prediction

267
00:17:20,600 --> 00:17:23,520
paradigm, which has its own nice properties to it.

268
00:17:23,520 --> 00:17:28,840
So for example, if I, if I give you a noisy input, so if the video is full of noise, and

269
00:17:28,840 --> 00:17:33,880
I'm trying to predict the next frame in a noisy video, yeah, if the noise is unbiased,

270
00:17:33,880 --> 00:17:38,960
so that over time the average of the noise is zero, then the prediction automatically

271
00:17:38,960 --> 00:17:42,000
filters that out at the very bottom layer of the structure.

272
00:17:42,000 --> 00:17:46,440
And that doesn't get passed up to the higher levels, so they see less noisy input.

273
00:17:46,440 --> 00:17:50,280
So you get powerful results just from moving to this prediction paradigm.

274
00:17:50,280 --> 00:17:54,920
And also the second thing that is different is the main propagation from layer to layer

275
00:17:54,920 --> 00:18:00,760
is the errors of the stuff that you can't predict, as opposed to just a more complex

276
00:18:00,760 --> 00:18:03,560
combination of everything you've got so far.

277
00:18:03,560 --> 00:18:10,360
Does that result in a network that has fundamental differences in properties than what you

278
00:18:10,360 --> 00:18:18,080
might see in a typical CNN-like in terms of the density of the weights or the way the

279
00:18:18,080 --> 00:18:20,480
layers are interconnected with one another?

280
00:18:20,480 --> 00:18:21,480
Yeah, I think so far.

281
00:18:21,480 --> 00:18:25,080
I mean, the research is fairly early here, and there will be, I don't really know the

282
00:18:25,080 --> 00:18:26,080
answer to that question.

283
00:18:26,080 --> 00:18:31,600
The answer is going to be yes, but I can tell you in good details exactly how this approach

284
00:18:31,600 --> 00:18:32,600
differs from the others.

285
00:18:32,600 --> 00:18:38,400
And I think the more you incorporate from different areas in machine learning techniques,

286
00:18:38,400 --> 00:18:42,320
the more you are going to find that this approach looks like what we've got.

287
00:18:42,320 --> 00:18:43,320
Right, right.

288
00:18:43,320 --> 00:18:48,280
So the layers are trained into N, you're not training individual layer separately.

289
00:18:48,280 --> 00:18:52,600
You can train the individual layer separately because each layer's input is just the error

290
00:18:52,600 --> 00:18:53,600
from the layer.

291
00:18:53,600 --> 00:18:54,600
Right.

292
00:18:54,600 --> 00:18:55,600
Right.

293
00:18:55,600 --> 00:19:02,040
And so when you, you set, you acknowledge this early, have you had any preliminary results

294
00:19:02,040 --> 00:19:03,520
from this line of research?

295
00:19:03,520 --> 00:19:07,840
There are many results that the idea works.

296
00:19:07,840 --> 00:19:12,520
And the suggestion there is that if this is a good philosophical theory of how the brain

297
00:19:12,520 --> 00:19:17,600
might be working, and then when we do implement it, you know, actually implement it in your

298
00:19:17,600 --> 00:19:19,640
networks, we see positive results.

299
00:19:19,640 --> 00:19:24,480
So that's just a sort of a backup like result to say, yeah, maybe the philosophical theory

300
00:19:24,480 --> 00:19:25,800
is onto something, right?

301
00:19:25,800 --> 00:19:27,000
Maybe this is a good idea.

302
00:19:27,000 --> 00:19:32,240
So it can predict future frames with high accuracy, you know, just when moving around

303
00:19:32,240 --> 00:19:36,320
in the main, but it's still early days to say how well that will be when we move it into

304
00:19:36,320 --> 00:19:40,680
reinforcement learning type of experiments where we can compare to say to the art and say

305
00:19:40,680 --> 00:19:43,680
how that learning that kind of representation helped.

306
00:19:43,680 --> 00:19:44,680
Yeah.

307
00:19:44,680 --> 00:19:49,720
The impression that I get from the conversations I've had recently on this and related topics

308
00:19:49,720 --> 00:19:54,600
is that the, you know, our understanding of the brain and the neurophysiology and our

309
00:19:54,600 --> 00:19:59,040
understanding of the machine learning are, you know, we're kind of one leap frogs the

310
00:19:59,040 --> 00:20:02,320
other and then feeds back, you know, learnings to the other.

311
00:20:02,320 --> 00:20:06,880
And it's kind of this, you know, iterative process, is that your sense as well?

312
00:20:06,880 --> 00:20:12,200
Or as one area like, you know, far ahead of the other and, you know, for example, we

313
00:20:12,200 --> 00:20:16,320
understand the brain a lot more than we do the, you know, the machine learning side and

314
00:20:16,320 --> 00:20:21,160
machine learning continues to pull from that or the other way around or I think we understand

315
00:20:21,160 --> 00:20:23,200
them in very, very different ways.

316
00:20:23,200 --> 00:20:28,240
But there are a lot of people that are sort of on the cusp of between the two disciplines

317
00:20:28,240 --> 00:20:32,080
that are grabbing stuff from one and pulling it into the other and grabbing stuff in the

318
00:20:32,080 --> 00:20:33,080
other direction.

319
00:20:33,080 --> 00:20:38,560
We've seen that work like convolutions, as I mentioned before, across these two disciplines

320
00:20:38,560 --> 00:20:43,120
and they work on both side of the spectrum and what side did they come from, do you know?

321
00:20:43,120 --> 00:20:45,320
I think it depends on your view pit.

322
00:20:45,320 --> 00:20:51,000
So I was at the symposium, Gary Marcus was saying that perhaps Janlequin wasn't aware of

323
00:20:51,000 --> 00:20:55,000
all the predictive coding type work in the retina and how convolutions might be applied

324
00:20:55,000 --> 00:20:58,920
in low-level visual systems and he was just trying stuff and found something that works

325
00:20:58,920 --> 00:21:01,120
that happens to be very, very much related.

326
00:21:01,120 --> 00:21:02,120
Okay.

327
00:21:02,120 --> 00:21:03,120
Yeah.

328
00:21:03,120 --> 00:21:04,120
I think that depends who you ask.

329
00:21:04,120 --> 00:21:05,120
Okay.

330
00:21:05,120 --> 00:21:06,120
Interesting.

331
00:21:06,120 --> 00:21:07,120
Interesting.

332
00:21:07,120 --> 00:21:14,080
So you also mentioned earlier in the kind of the conversation of philosophy, theory of mind

333
00:21:14,080 --> 00:21:15,080
more broadly.

334
00:21:15,080 --> 00:21:18,120
Can you describe that and how that fits into all of this?

335
00:21:18,120 --> 00:21:19,120
Yeah.

336
00:21:19,120 --> 00:21:25,880
So at first, my, my intuitions about why intelligence is interesting are that it involves

337
00:21:25,880 --> 00:21:30,680
introspection and thoughts and the ability to reason about the world and in the way we

338
00:21:30,680 --> 00:21:34,280
do, which is in a sense in a sort of symbolic process, right?

339
00:21:34,280 --> 00:21:37,880
We construct sentences, we have language that's obviously a very key component.

340
00:21:37,880 --> 00:21:42,160
We construct sentences in our heads and we understand things through those sentences

341
00:21:42,160 --> 00:21:43,160
sometime.

342
00:21:43,160 --> 00:21:49,160
And it's really interesting to see how like modern work, it's starting to look at representations

343
00:21:49,160 --> 00:21:54,600
of the world that in machine learning where we can answer questions in natural languages

344
00:21:54,600 --> 00:21:59,880
applied to images, for example, we've seen relation nets where you take in an image

345
00:21:59,880 --> 00:22:03,800
containing, you know, a few objects of different sizes and different locations and you answer

346
00:22:03,800 --> 00:22:08,200
a natural language question such as is the red object to the left of the yellow object

347
00:22:08,200 --> 00:22:09,680
to that kind of question.

348
00:22:09,680 --> 00:22:14,840
And the way they work is, is to try and create a representation inside the network that

349
00:22:14,840 --> 00:22:21,040
understands this relational kind of information, which is moving towards, in, in my opinion,

350
00:22:21,040 --> 00:22:26,320
moving towards a more symbolic or at least potential for a symbolic understanding of the

351
00:22:26,320 --> 00:22:30,080
world inside the standard machine learning algorithm.

352
00:22:30,080 --> 00:22:36,320
My reaction to that from very little kind of reading in linguistics is that like part

353
00:22:36,320 --> 00:22:43,240
of that's not the idea of, you know, thinking and sentences is not universally accepted.

354
00:22:43,240 --> 00:22:44,240
Is that right?

355
00:22:44,240 --> 00:22:48,920
Like, it's, you know, a lot of ways thought is more abstract than sentences, their experiments

356
00:22:48,920 --> 00:22:55,320
about trying to remember the, you know, there's this line of thinking around, you know,

357
00:22:55,320 --> 00:23:01,840
whether the degree to which language impacts thought and, you know, I think there's kind

358
00:23:01,840 --> 00:23:06,560
of this popular belief that, you know, people think in their languages, but it's also been

359
00:23:06,560 --> 00:23:08,240
disproven in a lot of ways.

360
00:23:08,240 --> 00:23:09,240
Yeah.

361
00:23:09,240 --> 00:23:16,000
I don't know too much about that area, but I do think that some kind of, not necessarily

362
00:23:16,000 --> 00:23:20,880
language type processing, but symbolic level processing or processing where we understand

363
00:23:20,880 --> 00:23:26,040
objects as entities that persist over time that we can therefore then label is going to

364
00:23:26,040 --> 00:23:30,440
be necessary as we move towards more general types of intelligence, especially if we end

365
00:23:30,440 --> 00:23:34,840
up on this track where it seems, at least there's a lot of key players in the field right

366
00:23:34,840 --> 00:23:40,400
now pulling towards human like general intelligence, that that's going to be a key component.

367
00:23:40,400 --> 00:23:44,320
And I think as we move towards it in AI machine learning, we will be able to answer those

368
00:23:44,320 --> 00:23:45,320
questions better.

369
00:23:45,320 --> 00:23:49,040
But for now, yeah, I'm not really sure.

370
00:23:49,040 --> 00:23:53,240
What's kind of the future of your particular research, both from the philosophical side

371
00:23:53,240 --> 00:23:55,000
as well as the machine learning side?

372
00:23:55,000 --> 00:23:56,000
Yeah.

373
00:23:56,000 --> 00:23:59,640
So we actually ended up talking about a sort of slightly small part of the research I've

374
00:23:59,640 --> 00:24:03,880
been doing, which I think doesn't really necessarily reflect on that.

375
00:24:03,880 --> 00:24:05,880
Let's dive further into your research.

376
00:24:05,880 --> 00:24:06,880
Okay.

377
00:24:06,880 --> 00:24:12,720
So one thing, Subin said at the symposium was that maybe at the moment we're at a pre-compernican

378
00:24:12,720 --> 00:24:15,720
revolution for our understanding of intelligence.

379
00:24:15,720 --> 00:24:20,240
So obviously in the concurrent revolution, we went from having humans at the center of

380
00:24:20,240 --> 00:24:26,400
the universe with everything revolving around it to humans as no longer at the center.

381
00:24:26,400 --> 00:24:31,320
And our understanding of intelligence seems very human-centric at the moment, or at least

382
00:24:31,320 --> 00:24:35,640
the layperson or the everyday understanding of intelligence.

383
00:24:35,640 --> 00:24:40,960
And we're seeing a sort of move away from this with the, you know, ideas of plant cognition

384
00:24:40,960 --> 00:24:46,600
and also these ideas of AI systems where, as Demis was saying at the symposium, Alpha

385
00:24:46,600 --> 00:24:50,400
Zero playing chess played very alien type of chess to him.

386
00:24:50,400 --> 00:24:52,320
It wasn't a human-like way of playing.

387
00:24:52,320 --> 00:24:54,440
It was a new type of playing.

388
00:24:54,440 --> 00:24:59,480
So when we explore this intelligence landscape, humans are going to be a very, very tiny part

389
00:24:59,480 --> 00:25:01,360
of that giant landscape.

390
00:25:01,360 --> 00:25:05,680
And with AI, because everything is artificial, we have the ability to explore way beyond

391
00:25:05,680 --> 00:25:09,960
the scope of this little area that, well, there's a very tiny area that biological life

392
00:25:09,960 --> 00:25:11,560
potentially exists in this landscape.

393
00:25:11,560 --> 00:25:15,200
And there's an even smaller area of the human life exists in this landscape.

394
00:25:15,200 --> 00:25:21,120
But my particular interest, and I think the big question moving forward for AI is, when

395
00:25:21,120 --> 00:25:25,960
will we create intelligences and what type of intelligences have some kind of moral

396
00:25:25,960 --> 00:25:26,960
patience?

397
00:25:26,960 --> 00:25:28,560
You'd have some kind of, some kind of what?

398
00:25:28,560 --> 00:25:29,560
Moral patience.

399
00:25:29,560 --> 00:25:33,400
So they are, patience in our moral understanding of them.

400
00:25:33,400 --> 00:25:38,800
So they can, it can be ethically correct or wrong to put them in certain situations.

401
00:25:38,800 --> 00:25:44,400
So for example, Nick Bostrom has this term, the mind crime where potentially we could create

402
00:25:44,400 --> 00:25:50,040
conscious artificial entities into some kind of slavery because they're just doing task

403
00:25:50,040 --> 00:25:51,040
for us.

404
00:25:51,040 --> 00:25:55,460
Or we could create entities that just live a life of suffering because they're never

405
00:25:55,460 --> 00:25:59,440
achieving their goals and it actually means something to say that they're suffering.

406
00:25:59,440 --> 00:26:04,760
And I think we need to explore their space of intelligence and compare it to the space

407
00:26:04,760 --> 00:26:06,480
of possible mind.

408
00:26:06,480 --> 00:26:11,600
The type of intelligences, not all intelligences are minds.

409
00:26:11,600 --> 00:26:12,840
We know that, that seems obvious.

410
00:26:12,840 --> 00:26:14,000
But some of them are.

411
00:26:14,000 --> 00:26:18,440
And we don't know right now whether it's just this little tiny, and it seems absurd,

412
00:26:18,440 --> 00:26:22,000
you know, it would be very pre-capernica and to say it's just this little human dot that

413
00:26:22,000 --> 00:26:26,580
is the space of possible minds where we need to map that onto the space of possible

414
00:26:26,580 --> 00:26:27,580
intelligences.

415
00:26:27,580 --> 00:26:31,280
So I see that as the broad research goal that's most important.

416
00:26:31,280 --> 00:26:36,880
So beyond the tiny piece of it, we discover what are some of the other kind of concrete

417
00:26:36,880 --> 00:26:40,080
research areas within that broad umbrella.

418
00:26:40,080 --> 00:26:46,020
So I think that the most concrete question, at least for me, is how can we understand

419
00:26:46,020 --> 00:26:52,800
intelligence in a way where we can then say about certain agents that are intelligent,

420
00:26:52,800 --> 00:26:54,360
whether or not they have a mind.

421
00:26:54,360 --> 00:26:58,360
But obviously that itself is a massively huge question and it's going to take results

422
00:26:58,360 --> 00:27:02,840
from philosophy, from neuroscience, to get to the human understanding of, like, we

423
00:27:02,840 --> 00:27:07,360
know humans have minds, so we can learn about minds from them, and then from AI and from

424
00:27:07,360 --> 00:27:12,520
completely other side of the field to think about what different types of alien intelligences

425
00:27:12,520 --> 00:27:14,560
or artificial intelligence could have.

426
00:27:14,560 --> 00:27:17,600
Do we even have a functional definition of mind?

427
00:27:17,600 --> 00:27:20,520
We don't even have a function with our definition of intelligence.

428
00:27:20,520 --> 00:27:25,320
That was one of the great results I thought of their symposium was we've invited all

429
00:27:25,320 --> 00:27:29,040
these people together to talk about intelligence in different ways and they've brought their

430
00:27:29,040 --> 00:27:30,600
own expertise.

431
00:27:30,600 --> 00:27:36,160
But that expertise, each comes with its own assumptions about what intelligence is, and

432
00:27:36,160 --> 00:27:42,280
we even have this debate there with Alpha Zero now playing Go and Chess better than humans.

433
00:27:42,280 --> 00:27:46,320
If I told you, however, Srendan is a really great chess player, you're automatically think

434
00:27:46,320 --> 00:27:47,680
he's intelligent.

435
00:27:47,680 --> 00:27:51,320
But now there's people saying, oh, Alpha Zero is not intelligent, chess is easy.

436
00:27:51,320 --> 00:27:57,400
Which, I mean, there's not even an agreement on that front of what intelligence is, so

437
00:27:57,400 --> 00:28:01,560
yeah, so minds is going to be harder than intelligence, and we haven't sort of intelligence

438
00:28:01,560 --> 00:28:02,560
yet.

439
00:28:02,560 --> 00:28:08,160
From the various folks involved in the symposium, are there patterns in the way they define

440
00:28:08,160 --> 00:28:10,560
intelligence that are easy to characterize?

441
00:28:10,560 --> 00:28:11,560
Like, is there...

442
00:28:11,560 --> 00:28:12,560
I think so, yeah.

443
00:28:12,560 --> 00:28:13,560
So there's a...

444
00:28:13,560 --> 00:28:17,760
Pedro Domingo's tribes of AI, is there an intelligence version of that?

445
00:28:17,760 --> 00:28:21,200
Yeah, because I think there's definitely a count that's...

446
00:28:21,200 --> 00:28:25,920
Very interesting human-like intelligence, and what they tend to do is define intelligence

447
00:28:25,920 --> 00:28:31,520
in terms of human intelligence, and then automatically assume that AGI, as we move to more general

448
00:28:31,520 --> 00:28:35,120
intelligence, is going to be on a path towards human-like intelligence, because that's the

449
00:28:35,120 --> 00:28:36,960
type they're very, very interesting.

450
00:28:36,960 --> 00:28:40,720
And then on the other side of the spectrum, you've got this idea that intelligence is so

451
00:28:40,720 --> 00:28:44,800
much more broad than as humans we could possibly imagine, so I think you have these sort of

452
00:28:44,800 --> 00:28:46,440
two separate camps.

453
00:28:46,440 --> 00:28:47,440
Okay.

454
00:28:47,440 --> 00:28:48,440
Interesting.

455
00:28:48,440 --> 00:28:52,640
So maybe going kind of circling back to, you know, the future and kind of how you push

456
00:28:52,640 --> 00:28:53,960
all this forward.

457
00:28:53,960 --> 00:28:56,000
How are you thinking about that today?

458
00:28:56,000 --> 00:28:59,960
Having just finished your, you know, pulling together the symposium and bringing together

459
00:28:59,960 --> 00:29:05,360
some of the, you know, folks that are kind of pushing this research forward.

460
00:29:05,360 --> 00:29:10,800
So I'm thinking that it's never too early to start these asking these questions.

461
00:29:10,800 --> 00:29:15,720
It might be too early to expect to have concrete answers to these questions, but it's definitely

462
00:29:15,720 --> 00:29:19,560
the time that we can actually bring these different types of people together to have this

463
00:29:19,560 --> 00:29:20,560
conversation.

464
00:29:20,560 --> 00:29:25,320
Because although everyone has different understandings of intelligence, we're getting results

465
00:29:25,320 --> 00:29:29,040
in these different fields that are comparable, and we can start comparing them and talking

466
00:29:29,040 --> 00:29:30,040
about the issue.

467
00:29:30,040 --> 00:29:35,280
So my feeling is very optimistic that this is the important and right area that we should

468
00:29:35,280 --> 00:29:38,760
be working on, and that we are going to get results.

469
00:29:38,760 --> 00:29:44,080
Now that we're at a state where AGI, you know, is as advanced as it is, and our understanding

470
00:29:44,080 --> 00:29:49,200
of intelligence across the animal kingdom is that it is, but we can stop bringing these

471
00:29:49,200 --> 00:29:50,200
things.

472
00:29:50,200 --> 00:29:51,200
Awesome.

473
00:29:51,200 --> 00:29:52,200
Awesome.

474
00:29:52,200 --> 00:29:54,960
Well, Matt, thanks so much for taking the time to chat with me about what you're up

475
00:29:54,960 --> 00:29:55,960
to.

476
00:29:55,960 --> 00:29:58,240
I wish I had an opportunity to attend the symposium.

477
00:29:58,240 --> 00:30:02,640
It sounds excellent, and I know that you had some really excellent speakers and participants.

478
00:30:02,640 --> 00:30:06,280
So I'm looking forward to keeping up with the work of the group.

479
00:30:06,280 --> 00:30:07,280
Thank you.

480
00:30:07,280 --> 00:30:08,280
Thank you.

481
00:30:08,280 --> 00:30:15,560
All right, everyone, that's our show for today.

482
00:30:15,560 --> 00:30:20,800
Thanks so much for listening, and for your continued feedback and support.

483
00:30:20,800 --> 00:30:25,880
For more information on Matthew or any of the topics covered in this episode, head on

484
00:30:25,880 --> 00:30:30,440
over to twimmelaii.com slash talk slash 91.

485
00:30:30,440 --> 00:30:37,360
To follow along with the NIP series, visit twimmelaii.com slash NIPs 2017.

486
00:30:37,360 --> 00:30:44,000
To enter our twimmel 1 mil contest, visit twimmelaii.com slash twimmel 1 mil.

487
00:30:44,000 --> 00:30:49,800
Of course, we'd be delighted to hear from you, either via a comment on the show news page,

488
00:30:49,800 --> 00:30:54,960
or via a tweet to act twimmelaii or at Sam Charrington.

489
00:30:54,960 --> 00:30:59,440
Thanks once again to Intel Nirvana for their sponsorship of this series.

490
00:30:59,440 --> 00:31:04,160
To learn more about the Intel Nirvana NNP and the other things Intel's been up to in

491
00:31:04,160 --> 00:31:08,400
AI Arena, visit intelnervana.com.

492
00:31:08,400 --> 00:31:13,160
As I mentioned a few weeks back, this will be our final series of shows for the year.

493
00:31:13,160 --> 00:31:18,400
So take your time and take it all in and get caught up on any of the old pods you've been

494
00:31:18,400 --> 00:31:20,040
saving up.

495
00:31:20,040 --> 00:31:22,440
Happy holidays and happy new year.

496
00:31:22,440 --> 00:31:24,720
See you in 2018.

497
00:31:24,720 --> 00:31:37,080
And of course, thanks once again for listening and catch you next time.

