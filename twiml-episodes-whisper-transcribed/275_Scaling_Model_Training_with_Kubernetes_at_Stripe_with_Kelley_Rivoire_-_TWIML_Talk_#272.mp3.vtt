WEBVTT

00:00.000 --> 00:16.320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

00:16.320 --> 00:21.480
people, doing interesting things in machine learning and artificial intelligence.

00:21.480 --> 00:32.440
I'm your host Sam Charrington.

00:32.440 --> 00:37.840
Two weeks ago we celebrated the show's third birthday and a major listenership milestone.

00:37.840 --> 00:42.840
And last week we kicked off the second volume of our listener favorite AI platform series,

00:42.840 --> 00:47.200
sharing more stories of teams working to scale and industrialize data science and machine

00:47.200 --> 00:49.800
learning at their companies.

00:49.800 --> 00:54.160
We've been teasing that there's more to come and today I am super excited to announce

00:54.160 --> 00:59.360
the launch of our inaugural conference, Twimblecon AI platforms.

00:59.360 --> 01:04.600
Twimblecon AI platforms will focus on the platforms, tools, technologies and practices

01:04.600 --> 01:09.400
necessary to scale the delivery of machine learning and AI in the enterprise.

01:09.400 --> 01:14.840
Now you know Twimble for bringing you dynamic practical conversations via the podcast and

01:14.840 --> 01:18.640
we're creating our Twimblecon events to build on that tradition.

01:18.640 --> 01:24.240
The event will feature two full days of community oriented discussions, live podcast interviews

01:24.240 --> 01:30.440
and practical presentations by great presenters sharing concrete examples from their own experiences.

01:30.440 --> 01:34.640
By creating a space where data science, machine learning, platform engineering and ML ops

01:34.640 --> 01:39.640
practitioners and leaders can share, learn and connect, the event aspires to help see

01:39.640 --> 01:44.800
the development of an informed and sustainable community of technologists that is well equipped

01:44.800 --> 01:48.560
to meet the current and future needs of their organizations.

01:48.560 --> 01:52.880
Some of the topics that we plan to cover include overcoming the barriers to getting machine

01:52.880 --> 01:58.120
learning and deep learning models into production, how to apply ML ops and DevOps to your machine

01:58.120 --> 02:03.040
learning workflow, experiences and lessons learned in delivering platform and infrastructure

02:03.040 --> 02:07.520
support for data management, experiment management and model deployment.

02:07.520 --> 02:11.520
The latest approaches, platforms and tools for accelerating and scaling the delivery

02:11.520 --> 02:16.760
of ML and DL and the enterprise, platform deployment stories from leading companies like

02:16.760 --> 02:23.480
Google, Facebook, Airbnb, as well as traditional enterprises like Comcast and Shell, an organizational

02:23.480 --> 02:27.000
and cultural best practices for success.

02:27.000 --> 02:31.600
The two day event will be held on October 1st and 2nd in San Francisco and I would really

02:31.600 --> 02:33.600
love to meet you there.

02:33.600 --> 02:39.520
EarlyBurt Registration is open today at twimblecon.com and we're offering the first 10 listeners

02:39.520 --> 02:45.720
who register the amazing opportunity to get their ticket for 75% off using the discount

02:45.720 --> 02:48.160
code TwimbleFirst.

02:48.160 --> 02:54.920
Again, the conference site is twimblecon.com and the code is TwimbleFirst.

02:54.920 --> 03:00.080
I am really grateful to our friends over at Sigopt who stepped up to support this project

03:00.080 --> 03:01.880
in a big way.

03:01.880 --> 03:07.000
In addition to supporting our AI Platforms podcast series and next ebook, they've made

03:07.000 --> 03:12.960
a huge commitment to this community by signing on as the first founding sponsor for the event.

03:12.960 --> 03:17.760
App Software is used by enterprise teams to standardize and scale machine learning experimentation

03:17.760 --> 03:23.200
and optimization across any combination of modeling frameworks, libraries, computing

03:23.200 --> 03:25.760
infrastructure and environment.

03:25.760 --> 03:31.480
Teams like Two Sigma, who will hear from later in this podcast series, rely on Sigopt Software

03:31.480 --> 03:36.520
to realize better modeling results much faster than previously possible.

03:36.520 --> 03:41.440
Of course, to fully grasp its potential, it's best to try it yourself and this is why

03:41.440 --> 03:46.760
Sigopt is offering you an exclusive opportunity to try their product on some of your toughest

03:46.760 --> 03:49.600
modeling problems for free.

03:49.600 --> 03:55.760
To learn about and take advantage of this offer, visit twimblei.com slash Sigopt.

03:55.760 --> 04:00.400
And now on to the show.

04:00.400 --> 04:03.960
All right, everyone.

04:03.960 --> 04:06.040
I'm on the line with Kelly Revoir.

04:06.040 --> 04:11.000
Kelly is an engineering manager at Stripe working on machine learning infrastructure.

04:11.000 --> 04:13.840
Kelly, welcome to this week in machine learning and AI.

04:13.840 --> 04:14.840
Thanks for having me.

04:14.840 --> 04:16.680
I'm really excited to chat.

04:16.680 --> 04:22.280
We got in touch with you kind of occasioned by a talk you're giving at strata, which

04:22.280 --> 04:25.400
is actually happening as we speak.

04:25.400 --> 04:32.160
I'm not physically in SF for it this time, but your talk, which is going to be later today,

04:32.160 --> 04:37.440
is on scaling model training from flexible training APIs to resource management with

04:37.440 --> 04:43.280
Kubernetes, and of course, machine learning infrastructure and AI platforms is a very popular

04:43.280 --> 04:45.560
topic here on the podcast.

04:45.560 --> 04:52.760
And so I'm looking forward to digging into the way Stripe is platforming its machine learning,

04:52.760 --> 04:54.080
processes and operations.

04:54.080 --> 04:58.040
But before we do that, I'd love to hear a little bit about your background and how you

04:58.040 --> 05:00.360
got started working in this space.

05:00.360 --> 05:02.160
Yeah, sounds great.

05:02.160 --> 05:05.960
Maybe I'll say a little bit about what I do now and then kind of work backwards from

05:05.960 --> 05:06.960
that.

05:06.960 --> 05:12.600
So right now, I'm an engineering manager at Stripe, and I work with our data infrastructure

05:12.600 --> 05:19.040
group, which is seven teams kind of at the lowest level things like our production databases

05:19.040 --> 05:23.960
or things like Elasticsearch clusters and then kind of working up through like batch

05:23.960 --> 05:30.840
and streaming platforms, core, like ETL, data pipelines and libraries, and also machine learning

05:30.840 --> 05:31.840
infrastructure.

05:31.840 --> 05:38.120
I've been at Stripe for very close to six years now from when the company was about 50

05:38.120 --> 05:42.840
people and have basically worked on a bunch of different things in sort of like risk,

05:42.840 --> 05:51.560
data and machine learning, both as an engineer and engineering manager and also initially

05:51.560 --> 05:56.360
more on kind of like the application side and then over time moving over to the infrastructure

05:56.360 --> 05:59.360
side.

05:59.360 --> 06:05.320
By training, I'm like a kind of research scientist person, so I studied physics and electrical

06:05.320 --> 06:11.520
engineering in school, did my PhD at Stanford working on nanopotonics and then did a short

06:11.520 --> 06:14.360
postdoc at HP labs on nanopotonics.

06:14.360 --> 06:15.360
Is that a nanopotonics?

06:15.360 --> 06:16.360
Yeah.

06:16.360 --> 06:21.600
I think you had like I get an odds count on recently, which is not too far away, so maybe

06:21.600 --> 06:23.600
that gives you a little bit of an idea.

06:23.600 --> 06:27.240
And then yeah, it was at HP labs for a year, so working on sort of similar things and

06:27.240 --> 06:29.520
also some 3D imaging.

06:29.520 --> 06:34.240
And I guess I like to call what I did, although I don't know that anyone else calls it that sort

06:34.240 --> 06:40.440
of like full stack science where like you have an idea and then you do some theory or modeling

06:40.440 --> 06:44.120
or simulation and then you use that to design a device and then you actually go in the

06:44.120 --> 06:47.720
clean room and like make the device and then you actually go in the optics lab and like

06:47.720 --> 06:51.480
you know shoot a bunch of lasers at your device and measure it and then you sort of like

06:51.480 --> 06:54.800
process the data and compare it to your theory and simulation.

06:54.800 --> 07:00.560
And I was like I found like kind of the two ends the most, like sort of the magical moment

07:00.560 --> 07:05.600
where like you know the data that you collected like matches what you thought was going to

07:05.600 --> 07:08.120
happen from your modeling.

07:08.120 --> 07:11.960
And I kind of decided that I wanted to do more of that and a little less of like fabrication

07:11.960 --> 07:12.960
or material science.

07:12.960 --> 07:16.920
And I was kind of sitting in Silicon Valley and started looking around and like stripe

07:16.920 --> 07:22.280
was super exciting in terms of its mission, like having interesting data and just like having

07:22.280 --> 07:23.280
amazing people.

07:23.280 --> 07:29.120
Awesome, awesome, stripe sounds really interesting but shooting lasers at stuff also sounds really,

07:29.120 --> 07:30.120
really cool.

07:30.120 --> 07:33.880
Yeah, people get really excited when you tell them that.

07:33.880 --> 07:36.040
So that was fun for a while.

07:36.040 --> 07:42.440
Maybe tell us a little bit about stripes, kind of machine learning journey from an infrastructure

07:42.440 --> 07:44.440
perspective.

07:44.440 --> 07:51.120
You know how did it, it sounds like you're doing a bunch of interesting things both from

07:51.120 --> 07:57.440
a training perspective, from a data management perspective, inference, but how did it evolve?

07:57.440 --> 08:02.440
Yeah, I think one thing that's interesting about machine learning at stripe, like I think

08:02.440 --> 08:07.880
a lot of places you talk to machine learning kind of like started out as being for some,

08:07.880 --> 08:13.080
some kind of like offline analytics more like you know internal business questions like

08:13.080 --> 08:16.760
maybe like you're trying to calculate long term value of your users.

08:16.760 --> 08:20.880
And we do stuff like that now, but we actually started like our kind of core uses have always

08:20.880 --> 08:27.000
been very much on kind of the production side, like our kind of most business critical

08:27.000 --> 08:32.440
and first machine learning, machine learning use cases were things like scoring transactions

08:32.440 --> 08:36.840
in the charge flow to evaluate whether they're fraudulent or not.

08:36.840 --> 08:44.240
We're doing kind of like internal risk management of like making sure our users are selling

08:44.240 --> 08:48.520
things that we can support from our terms of service or that they're kind of like good

08:48.520 --> 08:51.200
users that we want to support.

08:51.200 --> 08:55.920
And so we started out from having kind of a lot of these more like production requirements

08:55.920 --> 08:59.120
of it needs to be this fast and it needs to be this reliable and I think our machine

08:59.120 --> 09:04.440
learning platform kind of like evolved from that side, where you know initially we had

09:04.440 --> 09:08.240
kind of like one machine learning team and then even just having a couple of applications

09:08.240 --> 09:13.920
we started seeing like here are some commonalities like everyone needs to be able to score models

09:13.920 --> 09:18.880
or you know even like having some notion of shared features could be really valuable

09:18.880 --> 09:21.240
across just a couple of applications.

09:21.240 --> 09:25.880
And then as we split our machine learning team one piece of that became machine learning

09:25.880 --> 09:30.240
infrastructure, which we've developed since then and you know it's really important

09:30.240 --> 09:33.760
for that team to work both with the teams doing the business applications, which now

09:33.760 --> 09:38.800
include a bunch of other things in our user facing products like radar and billing as

09:38.800 --> 09:43.640
well as internally and also you know it's important for the machine learning infrastructure

09:43.640 --> 09:48.240
to build on the rest of your data infrastructure and really the rest of all of your infrastructure

09:48.240 --> 09:52.760
and we've worked really closely with like our orchestration team on you know as you said

09:52.760 --> 09:56.280
and chatting about my talk like getting training to run on Kubernetes.

09:56.280 --> 10:01.080
Yeah man that's maybe an interesting place to start.

10:01.080 --> 10:07.160
You kind of alluded to the interfaces between machine learning infrastructure as a team

10:07.160 --> 10:15.880
and you know data infrastructure and you know just infrastructure how do they how do they

10:15.880 --> 10:23.400
connect you know maybe even organizationally and how do they tend to work with them up

10:23.400 --> 10:24.400
with one another.

10:24.400 --> 10:30.520
For example you know in you know training on Kubernetes you know where is the line

10:30.520 --> 10:37.240
between what the ML infrastructure team is doing and you know what it's requiring of some

10:37.240 --> 10:40.680
you know broader technology infrastructure group.

10:40.680 --> 10:44.440
Yeah I think the Kubernetes case is really interesting and it's one that's been super

10:44.440 --> 10:50.880
successful for us so I guess maybe like a year or two ago we we didn't really focused

10:50.880 --> 10:54.920
on the kind of scoring like real-time inference part of models because that's the hardest

10:54.920 --> 10:58.280
and we'd sort of left people on their own it's like well you figure out how to train a

10:58.280 --> 11:02.840
model and then you know if you manage to do that we'll help you score it and we realized

11:02.840 --> 11:08.040
that that wasn't like great right so we started thinking you know what can we do and at first

11:08.040 --> 11:12.280
we built some CLI tools to kind of like wrap the Python people were doing but then we wanted

11:12.280 --> 11:16.360
to kind of do more so eventually we built an API and then a big hassle had been the resource

11:16.360 --> 11:21.000
management and we just kind of wanted to like abstract that all the way and as it happened

11:21.000 --> 11:25.800
at that time our orchestration team had gotten like really interested in Kubernetes and I think

11:25.800 --> 11:30.600
they wrote a blog post like maybe a year and a half ago they had kind of just moved our first

11:30.600 --> 11:35.400
application to Kubernetes which was some of our con jobs that we use in our financial infrastructure

11:35.400 --> 11:40.600
and so we ended up collaborating this was kind of like a great next step of a second application

11:40.600 --> 11:45.320
they could work on and you know we had some details we had to work out we had to figure out like

11:45.320 --> 11:50.520
how do we package up all of our Python code and to you know some Docker file we can deploy and

11:50.520 --> 11:55.560
it was really useful to be able to work with them on that but I think we have found really

11:55.560 --> 12:00.040
good interfaces and working with them where you know we wrote a client for the communities API but

12:00.040 --> 12:05.160
it's like anytime we need help or anytime there's management of the Kubernetes cluster they take

12:05.160 --> 12:09.880
care of all of that so it's kind of given us this flexibility where we can define different

12:09.880 --> 12:14.280
instance and resource types and swap them out really easily if we need CPUs or GPUs or we need

12:14.280 --> 12:19.160
to like expand the cluster but we as machine learning infrastructure kind of like don't have to

12:19.160 --> 12:23.480
deal with managing Kubernetes or updating it we have this amazing team of people who are like

12:23.480 --> 12:29.960
totally focused on that restraint so your talk at strata was focused on this area

12:31.320 --> 12:36.520
what was kind of the flow of your talk what were the main points that you are that you're

12:36.520 --> 12:43.080
planning to go through with the audience there yeah great question so we we kind of think about

12:43.080 --> 12:48.920
this in two pieces and you know maybe that's because that's how we actually did it so one piece

12:48.920 --> 12:52.840
was the resource management that I talked about was you know getting getting things to

12:52.840 --> 12:57.640
around on Kubernetes that was actually kind of like the second piece for us the first piece was

12:58.760 --> 13:02.840
figuring out sort of like how should the user interact with things and like where should we give

13:02.840 --> 13:08.600
them flexibility and where should we constrain things and so we ended up building what we call

13:08.600 --> 13:14.040
internally rail yard which is like a model training API and it goes with there's sort of two

13:14.040 --> 13:18.840
pieces there's like what you put in the API request and then there's what we call a workflow

13:18.840 --> 13:23.720
and the API request is a little bit more constrained like you have to say you're meta data for whose

13:23.720 --> 13:29.080
training so we can track it you have to tell us like where your data is like how you're doing things

13:29.080 --> 13:34.360
like hold out just kind of basic things that you'll always need but then we have this workflow piece

13:34.360 --> 13:39.640
that people can write like kind of like whatever Python they want as long as they define a

13:39.640 --> 13:45.720
train method in it that will hand us back like the fitted model and we definitely have found that

13:45.720 --> 13:51.080
like initially we were very focused on binary classifiers for things like fraud but people have done

13:51.080 --> 13:56.120
things like word embeddings we have people doing time series forecasting we're using like

13:57.320 --> 14:02.280
things like psychic learn actually views fast text pytorch profit so this has worked pretty

14:02.280 --> 14:06.680
well in terms of like providing enough flexibility that people can do things that we actually didn't

14:06.680 --> 14:12.520
anticipate originally but it's constrained enough that we can run it and sort of track what's going

14:12.520 --> 14:18.120
on and and give them what they need and be able to automate the things we need to automate do you

14:18.120 --> 14:24.680
think of your users as more kind of the data science type of user or machine learning engineer type

14:24.680 --> 14:31.240
of user or is there a mix of those two types of backgrounds yeah it's a mix which has been really

14:31.240 --> 14:36.040
interesting and I think back to what I said earlier like because we initially focused on

14:36.600 --> 14:41.640
these kind of critical production use cases we started out where the teams users were really

14:41.640 --> 14:45.720
pretty much all machine learning engineers and very highly skilled machine learning engineers

14:45.720 --> 14:50.280
like people who are excellent programmers and you know they know stats in ML and they're

14:50.280 --> 14:56.120
kind of like the unicorns to hire and over time we've been able to broaden that and I think

14:56.120 --> 15:01.880
having things like you know this tooling has made that possible like in our user survey right

15:01.880 --> 15:07.080
after we first shipped even just the kind of like API workflow piece and we were actually just

15:07.080 --> 15:11.640
like running it on some boxes aside car process we hadn't even done Kubernetes yet but a lot of the

15:11.640 --> 15:15.800
feedback we got was like oh this new person started on my team and I just like pointed them to the

15:15.800 --> 15:19.800
directory where the workflows are and I like didn't have to think about how to split all these

15:19.800 --> 15:23.880
things out because like you know you just kind of pointed me in the right direction and I could

15:23.880 --> 15:28.520
point them in the right direction so I think that having having these kind of like common ways of

15:28.520 --> 15:34.120
doing things has been a way to broaden our user set and as our data science team which is more

15:34.120 --> 15:38.840
internally focused has grown they've been able to kind of like start picking up increasingly

15:39.480 --> 15:44.840
large pieces of what we built for the ML engineers as well and we've been like excited to see

15:44.840 --> 15:53.320
that and work with them and so the interface then is kind of Python code and our is the platform

15:54.200 --> 16:00.680
containerizing that code or is the user expected to do it or is it integrated into some kind of

16:00.680 --> 16:06.280
workflow like they check it in and then it becomes available you know to the platform via

16:07.880 --> 16:16.120
check-in or CICD type of process yeah so we still have the experimental flow where people can

16:16.120 --> 16:20.440
like kind of try things out but when you're ready to productionize your workflow basically what you

16:20.440 --> 16:27.080
do is you get your code reviewed you merge it we use we ended up using google subpar library because

16:27.080 --> 16:32.600
it works really well with basil which we use for a lot of our build tooling what are the those two

16:33.480 --> 16:41.080
yeah so subpar is a google library that helps us like package Python code into like a self-contained

16:41.080 --> 16:45.640
executable both the source code and any dependencies like if you're running PyTorch and you need some

16:45.640 --> 16:51.320
CUDA stuff okay and it works kind of out of the box with basil which is the open source version

16:51.320 --> 16:57.560
of Google's build system which we have started to use that stripe a few years ago and have expanded

16:57.560 --> 17:03.160
since it's really nice for like speed reproducibility and working with multiple languages

17:04.360 --> 17:08.840
so this is where our ML info team kind of worked with our orchestration team to figure out the

17:08.840 --> 17:13.560
details here to be able to kind of like package up all this Python code and have it so that

17:13.560 --> 17:18.360
basically almost like a service deploy you can kind of like have it turn into a Docker image that

17:18.360 --> 17:25.080
you can deploy to like Amazon's ECR and then Kubernetes will kind of like know how to pull that down

17:25.080 --> 17:30.280
and be able to run it so the ML engineer or the data scientist doesn't really have to think about

17:30.280 --> 17:34.440
any of that it just kind of works as part of the you know you get your PR emerged and you

17:34.440 --> 17:39.960
deploy something if you need to change the workflow but earlier on in the process when you're

17:39.960 --> 17:49.400
experimenting the currency is a you know some Python code what kind of tooling have you built up

17:49.400 --> 17:56.920
around experiment management and automatically tracking various experiment parameters or hyper

17:56.920 --> 18:02.920
parameters hyper parameter optimization that kind of thing are you doing all that or is that

18:02.920 --> 18:10.760
all on the user to do yeah that's a really good question so one of the things that we added

18:10.760 --> 18:15.320
in our API for training as we found it was really useful to have this like custom prams field

18:16.760 --> 18:20.840
especially because we eventually people ended up and you know we have some shared services

18:20.840 --> 18:25.800
to support this like sort of a retraining service that can automate your training requests

18:26.360 --> 18:30.760
okay and so one of the things that people from the beginning use the custom

18:30.760 --> 18:35.240
programs for was hyper parameter optimization optimization we are kind of working toward building

18:35.240 --> 18:41.160
that out as a first-class thing like we now have like evaluation workflows that can be integrated

18:41.160 --> 18:44.840
with all of this as well and that's kind of like the first step you need for hyper parameter

18:44.840 --> 18:48.920
optimization if you want to do it as a service like what are you optimizing if you don't know what

18:48.920 --> 18:53.480
metrics are looking at right and so that's something we hope to do like over the next you know

18:53.480 --> 18:57.720
three to six months is to make that like a little bit more of first-class support and you mentioned

18:57.720 --> 19:06.840
this this directory of workflows elaborate on that a little bit yeah so one of the nice things is

19:06.840 --> 19:11.480
you know when you're writing your workflow if you put it in the right place then our our

19:11.480 --> 19:16.120
scholars service really are we'll know where to find it but one of the side benefits has also

19:16.120 --> 19:21.400
just been that there is one place where people's workflows are and so that that's been kind of

19:21.400 --> 19:25.480
like a nice place for people to get started and see like you know what models are other people

19:25.480 --> 19:31.960
using or like what preprocessing or kind of what other things are they doing or what what types

19:31.960 --> 19:37.960
of parameters like estimator parameters are they looking at changing to just kind of you know have

19:37.960 --> 19:44.360
that be like a little bit more available to our users are internal users and the workflow

19:45.000 --> 19:54.280
element of this is it is it graph-based is it something like airflow how's that implemented

19:54.280 --> 20:00.440
yeah so in this case by workflow all I mean is just like Python code that you know you give it

20:00.440 --> 20:07.000
like we're actually really hard our API passes to it like what are your features or what are your

20:07.000 --> 20:14.920
labels and then you are Python code returns like here is the fitted pipeline or model and like usually

20:14.920 --> 20:22.920
something like the evaluation data set that we can pass back we have had so we've people have

20:22.920 --> 20:28.360
kind of built us and users like interesting things on top of having a training API

20:28.360 --> 20:33.080
okay so some of our users built out actually the folks working on radar for our product built out

20:33.080 --> 20:38.520
like an auto retraining service that we've since kind of taken over and generalized where they

20:38.520 --> 20:44.200
schedule like nightly retraining of all the tens and hundreds of models and you know that's

20:44.200 --> 20:49.400
integrated to be able to even like if the evaluation looks better like potentially automatically

20:49.400 --> 20:55.960
deploy them we do also have people who have put like training models via our service into like

20:55.960 --> 21:01.160
air flow decks if they have you know some some slightly more complicated set of things that they

21:01.160 --> 21:06.760
want to run so we're definitely seeing that as well and you've mentioned radar a couple of times

21:06.760 --> 21:14.040
is that a product that's right we're an internal project yeah radar is our like user facing fraud

21:14.040 --> 21:20.920
product it runs on all of our machine learning infrastructure and you know every charge that

21:20.920 --> 21:25.800
goes through stripe within usually a hundred milliseconds or so we've kind of like done a bunch

21:25.800 --> 21:31.240
of real-time feature generation and evaluated like kind of all of the models that are appropriate

21:31.800 --> 21:37.240
and in addition to sort of the machine learning piece there's also a product piece for it

21:37.240 --> 21:42.600
where users can get more visibility into what our ml has done they can kind of like write their own

21:42.600 --> 21:48.440
rules and like set block thresholds on them and there's there's sort of like a manual review

21:48.440 --> 21:53.080
functionality so they're kind of some more product pieces that are complimentary to the underlying

21:53.080 --> 21:59.240
machine learning just trying to complete the picture here you've got these workflows which are

21:59.240 --> 22:06.520
essentially Python they expose a train entry point and you you mentioned this directory of

22:06.520 --> 22:11.640
workflows is that like a directory like on a server somewhere with just like dot py files or is

22:11.640 --> 22:18.600
that are they do you require that they be versioned and are you kind of managing those versions

22:19.240 --> 22:24.360
yeah so that that's just like actually like in a code basically so that's like yeah the workflows

22:24.360 --> 22:29.800
live together in code as part of kind of our training API it's like when you submit here's my

22:29.800 --> 22:35.000
training request which has you know here's my data here's my metadata this is the workflow

22:35.000 --> 22:41.000
I want you to run we give you back a job ID which then you can check the status of you can check

22:41.000 --> 22:47.240
the result the result will have things in it like what was the getcha and so that's like something

22:47.240 --> 22:55.240
that we can track as well got it so you're submitting the job with the code itself as opposed to

22:55.240 --> 23:03.000
a getcha so I guess it depends a little bit which workflow you're running through like in the case

23:03.000 --> 23:07.800
where you're running on Kubernetes you've merged your code to master and then we kind of

23:07.800 --> 23:12.520
package up all this code and deploy the Docker image and then from there you can kind of make

23:12.520 --> 23:18.840
requests to our service which will run the job on Kubernetes so at that point your code it's

23:18.840 --> 23:22.520
you know whatever's on master for the workflow plus whatever you've put in the request

23:23.240 --> 23:27.720
let's talk about where the the data comes from for training and what kind of

23:29.320 --> 23:34.280
you know platform support your offering folks yeah that's a really interesting question

23:34.280 --> 23:42.680
kind of within the framework of like what do you need for a like really RDPI request we support

23:42.680 --> 23:50.360
two different types of data sources one is more for experimentation which is like you can kind of

23:50.360 --> 23:55.800
tell us how to make this equal to query the data warehouse and that's kind of nice for experimentation

23:55.800 --> 24:01.400
but not so nice for production what pretty much everyone uses for production is the other data

24:01.400 --> 24:08.520
source we support which is parquet from s3 so it's like you tell us you know where to find that

24:08.520 --> 24:15.240
and what your future names are and usually that's generated by our features framework that we call

24:15.240 --> 24:23.080
semblance which is basically like a DSL that helps you know gives you a lot of ways to write complex

24:23.080 --> 24:28.680
features like think have things like counters be able to do things like joins do a lot of transformations

24:28.680 --> 24:36.120
and then you know the ML infrastructure team figures out like how to run that code in batch if you

24:36.120 --> 24:42.760
are doing training or like there's a way to run it in real time basically and kind of like a

24:42.760 --> 24:47.640
Kafka consumer setup but you only have to write your code feature code like once

24:48.520 --> 24:53.800
is it the user that's only writing a feature code once are you going after kind of sharing

24:53.800 --> 25:02.200
features across the user based to what extent are you seeing shared features yeah that's like a

25:02.200 --> 25:08.040
really excellent question um yeah so the user writes their code once and like also I think having

25:08.040 --> 25:11.800
a framework similar to the training workflows where people can see what other people have done

25:11.800 --> 25:19.080
has been really powerful um so we do have people who are like definitely kind of sharing features

25:19.080 --> 25:22.840
across applications and there's there's a little bit of a tradeoff like it's like a huge amount

25:22.840 --> 25:27.480
of leverage if you don't have to rewrite some complicated business logic um you do have to manage

25:27.480 --> 25:33.000
a little bit of making sure that um you know everything is versioned and that you're paying attention

25:33.000 --> 25:37.800
to like not deprecate something someone else is using and that you're not like just like changing

25:37.800 --> 25:43.000
a definition in place that you are kind of like creating a new version every time you are changing

25:43.000 --> 25:46.840
something right so there's a little bit more management there and hopefully over time we can

25:46.840 --> 25:51.560
improve our tooling around that but I think it's you know even even since before we had a feature

25:51.560 --> 25:56.040
framework like being able to kind of share some of that stuff has been like hugely valuable for us

25:57.320 --> 26:07.400
is the features framework is that a set of APIs or is that uh kind of a runtime uh thing like

26:07.400 --> 26:13.080
what what exactly is it yeah there's kind of two pieces so um which is basically sort of what you

26:13.080 --> 26:18.520
said like you know one is more like the API uh um like what are what are the things we you know

26:18.520 --> 26:23.960
let users express and one thing we tried to do there is actually constrain not a little bit

26:23.960 --> 26:28.680
so we like you have to use events for everything and we don't really let you express notions of time

26:28.680 --> 26:33.720
so you kind of can't mess up that time machine of like what was the state of the features

26:33.720 --> 26:37.560
at some time in the past where you want to be training your model we kind of like take care of that

26:37.560 --> 26:44.520
for you um so that's kind of one piece and then yeah we kind of compile that into like an AST

26:44.520 --> 26:48.760
and then we use that to essentially write like a compiler to be able to run it on different

26:48.760 --> 26:54.280
backends um and then we can kind of like you know write tests and try and check um at the framework

26:54.280 --> 26:58.600
level that that things are going to be as close as possible to the same across those different

26:58.600 --> 27:04.200
backends so back end could be um something for training where you're going to materialize like

27:04.200 --> 27:08.600
what was the value of the features at each point in time in the past that you want as inputs to

27:08.600 --> 27:13.080
training your model um or another back end could be like I mentioned we have kind of this

27:13.080 --> 27:18.840
Kafka consumer based back end that we use like for example um for radar to be able to like evaluate

27:18.840 --> 27:25.480
these features like as a charge is happening uh and so to what extent you find that limitation of

27:25.480 --> 27:31.800
everything being event based gets in the way of what folks want to do yeah that that's a really

27:31.800 --> 27:37.480
good question to um it's definitely was originally a little bit of a paradigm shift for people

27:37.480 --> 27:43.880
they're like oh I just want to use this thing from the database right um but we found that actually

27:43.880 --> 27:48.840
it's worked out pretty well and that especially when you have users who are ML engineers like they

27:48.840 --> 27:54.200
do really understand the value of like why you want to have things be event based and like the

27:54.200 --> 27:59.720
sort of gotchas that that helps prevent um because I think everyone has their story about how you

27:59.720 --> 28:04.840
were just looking something up in the database but then you know the value changed uh and you didn't

28:04.840 --> 28:09.240
realize it so it's kind of like you're leaking future information into your training data and

28:09.240 --> 28:15.800
then your model is not going to do as well as you thought it did so like I think moving to a more

28:15.800 --> 28:21.160
event based world and I mean I think in general shape has also kind of been doing more streaming work

28:21.160 --> 28:27.400
and um more having like good support also as as uh at the infrastructure level with Kafka has been

28:27.400 --> 28:36.200
really helpful with that and so does that mean that the models that they're building need to be

28:36.200 --> 28:43.400
aware of kind of this streaming paradigm during training uh or do they get a static data set

28:43.400 --> 28:48.440
to train? Yeah so basically um you can kind of use our future's framework to just generate like

28:48.440 --> 28:54.360
par k and s3 that has materialized like all of the information you want of what was the value of

28:54.360 --> 28:59.800
each of the features that you want at all the points in time that you want and then yeah your input

28:59.800 --> 29:05.400
to the training API is like please use this par k from s3 um we could make it a little more seamless

29:05.400 --> 29:11.960
than that but that's worked pretty well and par k is just like a serialized like a file format

29:11.960 --> 29:17.480
yeah it's pretty efficient you know I think it's used in a lot of kind of big data uses um you can

29:17.480 --> 29:21.160
also do things like predicate pushdown and we have like a way in the training API to kind of

29:21.160 --> 29:27.640
specify some filters there um to just kind of like save save some effort uh use a predicate pushdown

29:27.640 --> 29:32.120
yeah so if you know you only need certain columns or something like you know you can you can load it

29:32.120 --> 29:36.920
a little bit more efficiently and not have to carry around a lot of extra data got it okay the other

29:36.920 --> 29:43.800
interesting thing that you talked about in the context of the this event base framework is the

29:43.800 --> 29:51.560
whole um you know time machine is the way you said it kind of alluding to the the point in time

29:51.560 --> 29:59.960
correctness of uh you know feature snapshot can you elaborate a little bit on um did you

29:59.960 --> 30:07.240
did you start there or did you evolve to that that seems to be in my conversations kind of uh

30:08.200 --> 30:12.360
I don't know maybe you'd like one of the the cutting edges or bleeding edges that people are

30:12.360 --> 30:17.880
trying to deal with as they scale up these um these data management systems for features

30:19.000 --> 30:24.920
yeah for this particular project um in this version we started there straight previously

30:24.920 --> 30:29.720
had kind of looked at something a little bit related a couple years before um and in a lot of ways

30:29.720 --> 30:34.200
we kind of learned from that so we ended up with something that was more more powerful and sort

30:34.200 --> 30:40.200
of solved some of these issues at the platform level um we did you know at that point we had been

30:40.200 --> 30:44.120
running machine learning applications in production for a few years so I think everyone has

30:44.120 --> 30:50.040
their horror stories right of like all the things that can go wrong um especially kind of

30:50.040 --> 30:53.640
out of correctness level and like everyone has their story about like re-implementing features

30:53.640 --> 30:57.880
in different languages which we we did for a while too and kind of like all the things that can

30:57.880 --> 31:04.040
go wrong there so um yeah I think we we really tried to learn from both like what are all the

31:04.040 --> 31:09.560
things we'd seen go well or go wrong in individual applications and then also from kind of like

31:09.560 --> 31:14.600
our previous attempts um at some of this type of thing like what what was good and you know what

31:14.600 --> 31:20.840
could still be better and out of kerosene what do you use for data warehouse and are there multiple

31:20.840 --> 31:27.560
or is it is there just one um we've used a combination of redshift and presto um over the past

31:27.560 --> 31:33.720
couple of years um yeah they have a little bit of sort of like different abilities and strengths

31:34.280 --> 31:39.320
and those are those are things that people like to use to experiment with machine learning although

31:39.320 --> 31:43.400
like you know we generally don't use them in our production flows because we kind of prefer the

31:43.400 --> 31:51.320
event based model and so is the event based model parallel or orthogonal to to redshift or

31:51.320 --> 31:57.560
presto is there or is it a front end to either of these two systems yeah I guess we have we

31:57.560 --> 32:02.920
actually have a front end that we've built for redshift and presto um you know separately from

32:02.920 --> 32:08.200
from machine learning that's really nice and lets people like um you know to the extent they have

32:08.200 --> 32:15.400
permissions to do so like explore tables or put annotations on tables um we haven't integrated

32:15.400 --> 32:20.760
our in general I would say we could do some work on our UIs for for ML stuff we've definitely

32:20.760 --> 32:24.920
focused more on the back end and in front API side although we do have some things like our

32:24.920 --> 32:30.200
auto retraining service has a UI where you can see like um what's the status of my job like was

32:30.200 --> 32:35.480
it you know did it finish um did it produce a model that was better than the previous model

32:35.480 --> 32:42.920
I think I'm just trying to wrap my head around the event based model here you know as an example

32:42.920 --> 32:50.120
of a question that's coming to mind uh in an event based world are you regenerating the features

32:50.840 --> 32:55.880
you know every time and if you've got you know some complex feature that involves a lot of

32:55.880 --> 33:00.680
transformation or you have the backfill of ton of data like what does that even mean in an event

33:00.680 --> 33:06.680
based world where I think of like you have events and they go away uh is there some kind of store

33:06.680 --> 33:13.000
for all that that isn't redshift or presto um well whenever we say event you know we're publishing

33:13.000 --> 33:18.680
something to Kafka and then we're archiving it to s3 okay then that persists like you know as long

33:18.680 --> 33:26.680
as we want it to um in some cases basically forever um and so that is available we do do end up

33:26.680 --> 33:32.920
doing a decent amount of backfilling of kind of like you know you define the transform features you

33:32.920 --> 33:37.640
want but then you need you know you need to run that back over all the data you'll need for your

33:37.640 --> 33:41.880
training set that's something that we've actually done a lot of from the beginning partly because of

33:41.880 --> 33:47.160
our applications like when you're looking at fraud um you know the way you find out if you were

33:47.160 --> 33:53.400
right or not is that like in some time period usually within 90 days but sometimes longer than that

33:53.400 --> 33:59.480
the cardholder decides um whether they're going to dispute something as fraudulent or not um and

33:59.480 --> 34:03.560
that's compared to like you know if you're doing ads or trying to get clicks like you kind of get

34:03.560 --> 34:10.360
the result right away um and we you know so I think we've always like been interested in kind of

34:10.360 --> 34:14.600
like being able to backfill so that is you know you can log things forward but then it's like you'll

34:14.600 --> 34:18.120
probably have to wait a little bit of time before you have enough of a data set that you can train

34:18.120 --> 34:24.120
on it cool so we talked about the data uh side of things we talked about training and experiments

34:24.840 --> 34:30.760
how about inference yeah that's that's a really great question and that's that's kind of like the

34:30.760 --> 34:37.400
first thing that we built infrastructure support for um at first a decent number of years ago like

34:37.400 --> 34:43.960
I think even before things like TensorFlow were really popular and so we have um like our own

34:43.960 --> 34:51.320
scholar service that we use to do our production real-time inference um and you know we started out

34:51.320 --> 34:55.560
especially because we have like mostly transactional data we don't know a lot of things like images

34:55.560 --> 35:00.600
at least as our most critical applications at this point um a lot of our early models and even

35:00.600 --> 35:04.360
still today like most of our production models are kind of like tree based models like initially

35:04.360 --> 35:10.040
things like random forest and now things more like xg boost and so you know we've kind of like um

35:10.040 --> 35:15.800
um we have the serialization for that built in to our training workflows and um we've optimized

35:15.800 --> 35:20.360
that to run pretty efficiently in our scholar inference service and then we've built some kind of

35:20.360 --> 35:27.000
nice layers on top of that um for things like model composition kind of what we call meta models where

35:27.000 --> 35:32.280
you know you can kind of like take your machine learning model and um kind of like almost like

35:32.280 --> 35:38.840
within the model sort of compose something like add a threshold to it um or like for radar we train

35:38.840 --> 35:44.040
you know some array of like in some cases user specific models along with like maybe more of

35:44.040 --> 35:49.720
some global models and so you can kind of incorporate in the framework of a model um doing that

35:49.720 --> 35:54.520
dispatch for your kind of like if it matches these conditions that score with these models otherwise

35:54.520 --> 35:59.960
score with this model and like here's how you combine it um and then the way that interfaces with

35:59.960 --> 36:06.200
your application is that each application has uh what we call a tag and basically the tag points

36:06.200 --> 36:10.840
to the model identifier which is kind of like immutable and then whenever you have a new model

36:10.840 --> 36:16.040
you're ready to ship you just like update what does that tag point to um and then you know

36:16.040 --> 36:20.840
print in production you're just saying like score the model for this tag I think that's pretty

36:20.840 --> 36:24.360
similar to like you know if you read about uber's Michelangelo and things like that sometimes

36:24.360 --> 36:30.440
we're like oh we all came up with the same thing yeah I think that like a lot of people have kind

36:30.440 --> 36:34.600
of come up with some of these these ways of doing things that just kind of make sense yeah it

36:34.600 --> 36:40.840
also sounds a little bit like uh some of what uh seldom is trying to capture in the kubernetes

36:40.840 --> 36:48.760
environment um uh which I guess brings me to is the inference running in kubernetes or is that

36:48.760 --> 36:53.320
a separate infrastructure it's not right now but I think that's mostly like a matter of time

36:53.320 --> 36:59.160
and prioritization um like the first thing we move to kubernetes was uh the training piece because

36:59.160 --> 37:03.880
the workflow management piece was so powerful or sorry the resource management piece was so powerful

37:03.880 --> 37:09.720
like being able to swap out CPU GPU high memory we've moved some of our the sort of real-time

37:09.720 --> 37:14.600
feature evaluation to kubernetes which has um been really great and made it like a lot less

37:14.600 --> 37:19.480
toil to kind of deploy new feature versions at some point we will probably also move the

37:19.480 --> 37:23.080
inference service to kubernetes we just kind of haven't gotten there yet because it is still some

37:23.080 --> 37:31.640
work to do that and is the uh the inferences happening on AWS as well and are you using kind of

37:31.640 --> 37:41.080
standard CPU instances or are you doing anything fancy there yeah so um we ran on cloud for pretty

37:41.080 --> 37:48.280
much everything and um definitely use a lot of AWS for the real-time inference of the most sensitive

37:48.280 --> 37:55.480
like production use cases um we're definitely mostly using um CPU and we've done a lot of optimization

37:55.480 --> 38:00.440
work um so that has worked pretty well for us um I think we do have some folks who've kind of

38:00.440 --> 38:07.320
experimented a little bit with like hourly or batch scoring um using some other things so

38:07.320 --> 38:11.400
I think that's something that we're definitely thinking about as we have more people productionizing

38:12.360 --> 38:16.280
kind of like more complex types of models where you know we might want something different

38:16.280 --> 38:23.320
you mentioned a lot of optimization that you've done is that uh on a model by model basis or are

38:23.320 --> 38:30.840
there uh platform uh things that you've done that help optimize across the various models that

38:30.840 --> 38:36.360
you're deploying uh for inference yeah definitely a lot of things at the platform level like I think

38:36.360 --> 38:41.880
the first models that we ever ever scored in our inference service were serialized with YAML

38:42.680 --> 38:47.960
and they were like really huge and um they caused a lot of garbage when we tried to load them

38:47.960 --> 38:54.840
and so like we did some work there for kind of tree-based models um to be able to load things

38:54.840 --> 38:59.720
from disk to memory really quickly and like not producing much garbage um so that that kind of

38:59.720 --> 39:04.280
thing are things that we did especially kind of like in the earlier days what are you using for

39:04.280 --> 39:11.800
querying the models are you doing rest or grpc or uh something altogether different

39:11.800 --> 39:16.840
yeah we use rest right now I think grpc is like something that we're interested in um but we haven't

39:16.840 --> 39:25.560
done yet is all of your all of the inference done via um kind of via rest and like kind of

39:25.560 --> 39:34.200
microservice style or do you also do uh more I guess embedded types of uh inference for like where

39:34.200 --> 39:40.040
you need have super low latency requirements does rest kind of meet the need across the application

39:40.040 --> 39:45.640
portfolio yeah um even for our most critical applications like shield things have worked pretty

39:45.640 --> 39:49.880
out one other thing our orchestration team has done that's worked really well for us is um

39:49.880 --> 39:55.560
migrating a lot of things to envoy um so we've seen some some things where like we didn't understand

39:55.560 --> 39:59.720
why there was some delay like in what we measured for how long things trick versus like what it

39:59.720 --> 40:04.680
took to the user there's just kind of one away as we move to envoy and what is envoy

40:05.320 --> 40:10.600
envoy is like a service service networking mesh that was developed by lift um and is kind of like

40:10.600 --> 40:16.120
an open source open source library and so it handles a lot of it can handle a lot of things like

40:16.120 --> 40:23.000
service to service communication the inference environment is it doing absent of Kubernetes all the

40:23.000 --> 40:29.160
things that you'd expect Kubernetes to do in terms of like auto scaling and um you know load

40:29.160 --> 40:39.480
balancing across the different service instances or is that stuff all done um statically um we take

40:39.480 --> 40:45.800
care of the routing um ourselves and we also at this point have kind of like charted our inference

40:45.800 --> 40:51.160
service so not all models are stored on every host so that you know we don't need hosts with like

40:51.160 --> 40:58.760
infinite memory and so that we take care of ourselves um the scaling we is not fully automated

40:58.760 --> 41:03.480
at this point we do we have kind of like quality of service so we have like multiple kind of clusters

41:03.480 --> 41:08.280
of machines and we tear a little bit by like you know how sensitive your application is and what

41:08.280 --> 41:13.080
you need from it um so that we can be a little bit more relaxed with people who are developing and

41:13.080 --> 41:18.840
want to test and not have that like potentially have any impact on more critical applications um but we

41:18.840 --> 41:22.840
haven't done like totally automated scaling that's something we kind of still look at a little bit

41:22.840 --> 41:30.520
ourselves uh so if you were kind of just starting down this journey uh without having done all the

41:30.520 --> 41:35.000
the things that that you've done it's right where do you think you would start if you just

41:35.000 --> 41:40.840
you know you're you're at an organization that's kind of increasingly invested in or investing in

41:40.840 --> 41:47.880
machine learning and you know needs to try to you know gain some efficiencies. Yeah I mean I

41:47.880 --> 41:52.040
think if you're just starting out like it's good to think about like what are your requirements

41:52.040 --> 41:57.080
rate um and you know if you're just trying to iterate quickly it's like do the simplistic

41:57.080 --> 42:03.000
thing possible right so you know if you can do things in batch like great do things in batch um

42:03.000 --> 42:09.880
I think a lot of there are a lot of both open source libraries as well as manage solutions um like

42:09.880 --> 42:14.680
on all the different cloud providers so I think you know I don't know you know if you're only one

42:14.680 --> 42:19.000
person then I think that those could make a lot of sense also for people starting out because I

42:19.000 --> 42:23.240
think one of the interesting things with machine learning applications is that um it takes a little

42:23.240 --> 42:28.200
bit of work like usually there's sort of this threshold of like your modeling has to be good enough

42:28.200 --> 42:32.600
for this to be like a useful thing for you to do like for fried detection that's like if we can't

42:32.600 --> 42:37.080
catch any fraud with our models then like you know we probably shouldn't have like a fraud detection

42:37.080 --> 42:42.680
product um so I think it is useful to kind of have like a quick iteration cycle to find out like

42:42.680 --> 42:46.680
is this a viable thing that you even want to pursue and if you have an infrastructure team they

42:46.680 --> 42:51.480
can kind of like help um lower the bar for that but I think there are other ways to do that

42:51.480 --> 42:55.640
especially as you know there's been like this Cambrian explosion in the ecosystem of different

42:55.640 --> 43:00.760
open source platforms as well as different managed solutions. Yeah how do you how do you think

43:00.760 --> 43:05.320
in an organization knows when they should have an infrastructure team?

43:06.440 --> 43:13.400
ML in particular. Yeah I think that's a really interesting question um I guess uh in our case I

43:13.400 --> 43:18.280
think um you know the person who originally founded the machine learning infrastructure team

43:19.000 --> 43:24.520
um had worked in this area before at Twitter and kind of had a sense of like this is going to be

43:24.520 --> 43:28.360
a thing that we're really going to want to invest in given how important it is for a business

43:28.360 --> 43:32.760
and also that if you don't kind of like dedicate some folks to it it's easy for them to kind of

43:32.760 --> 43:37.080
get sucked up in other things like if you just have data infrastructure that's undifferentiated

43:37.880 --> 43:42.440
so I think it's a really interesting question there probably is this business piece right of like

43:42.440 --> 43:48.280
what are your ML applications like how critical are they to your business and like how difficult

43:48.280 --> 43:52.760
are your infrastructure requirements for them as well I think a lot of companies develop their

43:52.760 --> 43:57.560
ML infrastructure like starting out with things like making the notebook experience really great

43:57.560 --> 44:01.400
because they want to support like a lot of data scientists who are doing a lot of analysis

44:01.400 --> 44:05.320
and so that's like a little bit of a different arc from from the one that we've been on and I think

44:05.320 --> 44:11.560
that's like actually a pretty business dependent thing. Okay awesome awesome well Kelly thanks so

44:11.560 --> 44:17.880
much for taking the time to chat with me about this really interesting uh story and I've enjoyed

44:17.880 --> 44:26.840
learning about it. Cool um thanks so much for chatting really enjoyed it. All right everyone that's our show

44:26.840 --> 44:33.240
for today for more information about today's guest or to follow along with AI platform volume 2

44:33.240 --> 44:41.480
visit twimmelai.com slash AI platforms 2 make sure you visit twimmelcon.com for more information

44:41.480 --> 44:47.960
order register for twimmelcon AI platforms thanks again to sig up for their sponsorship of this

44:47.960 --> 44:52.680
series to check out what they're up to and take advantage of their exclusive offer for twimmel

44:52.680 --> 45:00.200
listeners visit twimmelai.com slash sig opt as always thanks so much for listening and catch you next

45:00.200 --> 45:11.400
time

