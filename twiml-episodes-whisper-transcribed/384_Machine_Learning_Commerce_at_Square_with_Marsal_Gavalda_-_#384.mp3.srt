1
00:00:00,000 --> 00:00:13,360
Welcome to the Twemal AI Podcast.

2
00:00:13,360 --> 00:00:24,360
Hey, what's up everyone?

3
00:00:24,360 --> 00:00:29,000
Before we get to today's show, just a few quick updates.

4
00:00:29,000 --> 00:00:33,440
First off, I'd like to ask for your help with our latest survey, this time around on

5
00:00:33,440 --> 00:00:36,560
the Python data science ecosystem.

6
00:00:36,560 --> 00:00:40,880
This is our second year exploring this space and we'd love to have your input.

7
00:00:40,880 --> 00:00:47,080
If you use Python for data science, please visit twemalai.com slash Python survey to take

8
00:00:47,080 --> 00:00:48,680
the survey.

9
00:00:48,680 --> 00:00:53,080
The survey will only take a few minutes to complete and three lucky submissions will receive

10
00:00:53,080 --> 00:00:58,560
a $25 Amazon gift card.

11
00:00:58,560 --> 00:01:03,600
Speaking of languages, we are super excited to announce the great machine learning language

12
00:01:03,600 --> 00:01:05,400
on debate.

13
00:01:05,400 --> 00:01:10,400
Join us for a thoughtful discussion exploring the strengths, weaknesses, and approaches

14
00:01:10,400 --> 00:01:16,400
of popular and emerging programming languages for machine learning like Python, R, C++,

15
00:01:16,400 --> 00:01:22,680
Swift, Julia, Closure, and more, and an opportunity for you to engage with experts and peers on

16
00:01:22,680 --> 00:01:23,680
the topic.

17
00:01:23,680 --> 00:01:28,280
It's not too late to let us know the languages you'd like to see included and who you'd

18
00:01:28,280 --> 00:01:30,120
like to see representing them.

19
00:01:30,120 --> 00:01:37,120
Send me your suggestions via Twitter at at Sam Charrington or hit me up at Sam at twemalai.com.

20
00:01:37,120 --> 00:01:42,920
Visit twemalai.com slash languages for more information or to register.

21
00:01:42,920 --> 00:01:45,280
And now on to the show.

22
00:01:45,280 --> 00:01:46,760
All right, everyone.

23
00:01:46,760 --> 00:01:49,480
I am on the line with Marsal Gavolda.

24
00:01:49,480 --> 00:01:54,720
Marsal is head of machine learning for the commerce platform at square.

25
00:01:54,720 --> 00:01:57,560
Marsal, welcome to the twemalai podcast.

26
00:01:57,560 --> 00:01:58,560
Thank you so much.

27
00:01:58,560 --> 00:02:01,240
I'm very excited to be in your podcast.

28
00:02:01,240 --> 00:02:04,040
It's great to have an opportunity to speak with you.

29
00:02:04,040 --> 00:02:09,840
And I'm looking forward to digging into how ML is working at square, what you're up to.

30
00:02:09,840 --> 00:02:13,720
But before we do that, share a little bit about your background.

31
00:02:13,720 --> 00:02:20,040
What got you interested in machine learning and what are some of your interest areas?

32
00:02:20,040 --> 00:02:21,040
Yeah, sure.

33
00:02:21,040 --> 00:02:23,200
I'm happy to share my sort of life story here.

34
00:02:23,200 --> 00:02:29,040
I grew up in Barcelona, Catalonia, Spain, which actually is in the news still quite a bit

35
00:02:29,040 --> 00:02:34,200
because there's an unresolved conflict between Catalonia and the rest of Spain.

36
00:02:34,200 --> 00:02:35,720
And a lot of it is based on language.

37
00:02:35,720 --> 00:02:40,840
So growing up there, I got very interested in languages because I wondered how come half

38
00:02:40,840 --> 00:02:44,920
of my classmates speak Catalan and they all have speak Spanish.

39
00:02:44,920 --> 00:02:50,160
When if you look at the history as the Roman Empire extended throughout Europe, they were

40
00:02:50,160 --> 00:02:53,040
all supposed to be speaking Latin, right?

41
00:02:53,040 --> 00:02:58,800
And yet Latin gave rise to Italian in one place, Romanian, French, Portuguese, Spanish,

42
00:02:58,800 --> 00:03:02,640
Catalan, all the languages that we now call Latin or Roman's languages.

43
00:03:02,640 --> 00:03:07,800
So that got me curious about where do languages come from and how do they evolve?

44
00:03:07,800 --> 00:03:09,680
What is common across all languages?

45
00:03:09,680 --> 00:03:11,200
What is unique?

46
00:03:11,200 --> 00:03:12,720
Take the word for window, right?

47
00:03:12,720 --> 00:03:19,360
And Catalan window is finistra, which actually is the exact same word as in Italian finistra.

48
00:03:19,360 --> 00:03:21,920
Very close to the French finetra, right?

49
00:03:21,920 --> 00:03:26,560
Even close to the German finster, even though Germany is not even a Latin language, whereas

50
00:03:26,560 --> 00:03:29,560
in Spanish, I don't know if you know any Spanish.

51
00:03:29,560 --> 00:03:30,560
Spentana, right?

52
00:03:30,560 --> 00:03:31,560
Exactly.

53
00:03:31,560 --> 00:03:32,560
Yes.

54
00:03:32,560 --> 00:03:34,520
And where is the etymology of Entana?

55
00:03:34,520 --> 00:03:36,320
Where does it come from?

56
00:03:36,320 --> 00:03:40,280
Entana comes from Viento, which means wind.

57
00:03:40,280 --> 00:03:45,640
And it's super interesting because the work, the origin of the, in English, of window also

58
00:03:45,640 --> 00:03:51,480
comes from like all Germanic wind out, so like a wind's eyelid, a little hole for the

59
00:03:51,480 --> 00:03:57,640
wind to go through, which if it's, you know, what would you call now something for air

60
00:03:57,640 --> 00:04:00,920
or wind to go through that doesn't have a glass?

61
00:04:00,920 --> 00:04:03,200
You would call that event, right?

62
00:04:03,200 --> 00:04:08,280
So you have window and ventana, all these sort of related to wind, and on the other hand,

63
00:04:08,280 --> 00:04:09,280
there's finistra, right?

64
00:04:09,280 --> 00:04:10,280
Which apparently...

65
00:04:10,280 --> 00:04:12,280
This might be the first episode of my language podcast.

66
00:04:12,280 --> 00:04:13,280
Oh, yeah.

67
00:04:13,280 --> 00:04:16,080
Let's expand our horizons.

68
00:04:16,080 --> 00:04:17,080
Yeah.

69
00:04:17,080 --> 00:04:18,080
You know what?

70
00:04:18,080 --> 00:04:22,160
It's interesting that you're starting off with this conversation about language is something

71
00:04:22,160 --> 00:04:24,240
that I'm super excited about.

72
00:04:24,240 --> 00:04:30,680
I don't spend nearly enough time studying it, but this conversation around the etymology

73
00:04:30,680 --> 00:04:34,960
of words reminds me of a book that I read, the history of the English language that kind

74
00:04:34,960 --> 00:04:36,920
of goes through.

75
00:04:36,920 --> 00:04:41,120
The thing that I remember most vividly from that book is this concept of the Great Language

76
00:04:41,120 --> 00:04:46,440
Shift, I don't know if something similar happened in the room, the, or sorry, the Great

77
00:04:46,440 --> 00:04:47,440
vowel shift.

78
00:04:47,440 --> 00:04:51,320
If something similar happened in the romance languages, but in English, there was this

79
00:04:51,320 --> 00:04:57,840
period of time in which the vowels all shifted in the sounds that they made that made, and

80
00:04:57,840 --> 00:05:03,560
it's apparent when you look at like Beowulf and other writing, and you see these words

81
00:05:03,560 --> 00:05:07,640
that don't really make any sense until you realize that there was this vowel shift.

82
00:05:07,640 --> 00:05:08,640
Anyway, I just...

83
00:05:08,640 --> 00:05:09,640
Yeah.

84
00:05:09,640 --> 00:05:15,880
Yeah, and one very interesting thing about English is that it basically has this duality

85
00:05:15,880 --> 00:05:20,960
of vocabulary, because English is a Germanic language, but then it had a huge influence

86
00:05:20,960 --> 00:05:23,200
of Latin through French, right?

87
00:05:23,200 --> 00:05:28,720
And so you get these pairs, you know, what is the difference between freedom and liberty?

88
00:05:28,720 --> 00:05:33,000
Well, it's really kind of the same, it's just, you know, freedom is a Germanic one and

89
00:05:33,000 --> 00:05:35,600
liberty is the romance.

90
00:05:35,600 --> 00:05:41,800
On the other hand, there's also these super fun pairs like pig and pork, or cow and beef,

91
00:05:41,800 --> 00:05:47,200
where the animal, the word for the animal is the Germanic one, whereas the one for the,

92
00:05:47,200 --> 00:05:51,960
for the, for the, for the dish or the cook thing is the, the Latin one.

93
00:05:51,960 --> 00:05:55,480
Even the difference between kitchen and cuisine, right?

94
00:05:55,480 --> 00:05:56,480
Look at that.

95
00:05:56,480 --> 00:05:58,320
It's cuisine is just kitchen in French, right?

96
00:05:58,320 --> 00:05:59,320
Yeah.

97
00:05:59,320 --> 00:06:00,320
Yeah.

98
00:06:00,320 --> 00:06:03,080
But so word is endlessly fast, languages are endlessly fascinating.

99
00:06:03,080 --> 00:06:09,080
It's also not just the words, but also the constituents like in Japanese, you, you say

100
00:06:09,080 --> 00:06:11,400
the girl, a book reads, right?

101
00:06:11,400 --> 00:06:15,560
You put the verb at the end, which incidentally is also how Yoda talks.

102
00:06:15,560 --> 00:06:21,360
So it just appears to kind of, it lends more gravitas if you at the end of the sentence,

103
00:06:21,360 --> 00:06:24,480
the verb place.

104
00:06:24,480 --> 00:06:30,080
But of course, so, but so the interesting is that all languages have nouns and verbs,

105
00:06:30,080 --> 00:06:33,240
which is not surprising, because this is how we humans perceive the world.

106
00:06:33,240 --> 00:06:36,640
There's sort of static objects, like a cup, and then there's actions.

107
00:06:36,640 --> 00:06:39,440
You pour into the cup, or you drink from the cup, right?

108
00:06:39,440 --> 00:06:43,800
But then the way in which you put these building blocks together is more, it's more arbitrary.

109
00:06:43,800 --> 00:06:48,680
But of course, fundamentally, all languages have the same expressive power, it's just

110
00:06:48,680 --> 00:06:54,120
that certain languages tend to put more emphasis in certain aspects, like another cool example

111
00:06:54,120 --> 00:06:59,720
is in Chinese, you could not just say brother or sister, you have to specify whether it's

112
00:06:59,720 --> 00:07:01,400
older brother or younger sister, right?

113
00:07:01,400 --> 00:07:03,960
There's like four different words for sibling.

114
00:07:03,960 --> 00:07:12,360
So an interest in kind of a political interest in language, drove an interest in languages broadly,

115
00:07:12,360 --> 00:07:14,600
it sounds like, and is that right?

116
00:07:14,600 --> 00:07:15,600
Right.

117
00:07:15,600 --> 00:07:17,600
So you and to machine learning.

118
00:07:17,600 --> 00:07:18,600
Yes.

119
00:07:18,600 --> 00:07:22,280
So that's one aspect that've always been just, and I've tried to like learn a few languages,

120
00:07:22,280 --> 00:07:25,120
et cetera, sort of can mumble my way through a few.

121
00:07:25,120 --> 00:07:29,440
But then the other piece of technology, don't put me on the spot.

122
00:07:29,440 --> 00:07:34,080
But basically, so Catalan and Spanish, German, English.

123
00:07:34,080 --> 00:07:39,120
And then Chinese and Japanese, a little bit, I can know some basic stuff.

124
00:07:39,120 --> 00:07:43,440
And then actually, other languages like French and Italian Portuguese, just through sort of

125
00:07:43,440 --> 00:07:47,720
Romans, Osmosis, right, it's they're so close that you can basically understand.

126
00:07:47,720 --> 00:07:48,720
Cool.

127
00:07:48,720 --> 00:07:49,720
Yeah.

128
00:07:49,720 --> 00:07:52,160
But then the other piece is sort of technology.

129
00:07:52,160 --> 00:07:58,120
One day my dad brought home an HB85, one of the early home computers, a beautifully

130
00:07:58,120 --> 00:07:59,200
design machine, right?

131
00:07:59,200 --> 00:08:05,360
So the CRT monochrome screen, magnetic cartridge for storage, a thermal printer, this built

132
00:08:05,360 --> 00:08:06,360
in keyboard.

133
00:08:06,360 --> 00:08:10,840
And then some manuals that were very well done and taught you how to program at that time

134
00:08:10,840 --> 00:08:12,440
that was in basic.

135
00:08:12,440 --> 00:08:18,520
So that's how I sort of got hooked into coding and I ended up studying computer science as

136
00:08:18,520 --> 00:08:24,280
an undergrad in Barcelona at the UPC, the University of Polytechnic at Catalonia, which

137
00:08:24,280 --> 00:08:26,880
is I think now goes by Barcelona tech.

138
00:08:26,880 --> 00:08:31,560
But when I finished, I realized that there was this sort of nascent field at the time called

139
00:08:31,560 --> 00:08:36,520
computational linguistics, now better known as sort of maybe language technologies, which

140
00:08:36,520 --> 00:08:41,360
is this attempt of getting computer systems to understand aspects of human language.

141
00:08:41,360 --> 00:08:43,720
And then I thought, wow, that's absolutely amazing.

142
00:08:43,720 --> 00:08:49,360
I can sort of merge, combine my two interests in languages and technology.

143
00:08:49,360 --> 00:08:55,040
So after my undergrad in Barcelona, I did one year in Germany at the University of

144
00:08:55,040 --> 00:08:58,840
Cascua and then I came to the U.S. for grad school.

145
00:08:58,840 --> 00:09:03,120
I was at Carnegie Mellon for quite a few years because I first did a master's in computational

146
00:09:03,120 --> 00:09:07,800
linguistics and then I stayed for a PhD in computer science and in language technologies.

147
00:09:07,800 --> 00:09:08,800
Okay.

148
00:09:08,800 --> 00:09:13,480
So I start working on things like speech recognition, machine translation.

149
00:09:13,480 --> 00:09:19,880
Some of the early were calling sort of end to end speech translation, which was doing

150
00:09:19,880 --> 00:09:25,440
the recognition and then a fair amount of minimum amount of understanding so that you could

151
00:09:25,440 --> 00:09:29,120
sort of generate into a target language.

152
00:09:29,120 --> 00:09:35,400
Something that now is more common, but 25 years ago was sort of a pioneering work with

153
00:09:35,400 --> 00:09:39,360
Alex Vival, the Janus project and some other researchers.

154
00:09:39,360 --> 00:09:40,360
Okay.

155
00:09:40,360 --> 00:09:41,360
Nice.

156
00:09:41,360 --> 00:09:42,360
Nice.

157
00:09:42,360 --> 00:09:45,000
And I've got a note here that you went to CMU with Manuel Ovalosa.

158
00:09:45,000 --> 00:09:48,600
We've talked obviously lots of folks on the past, of course.

159
00:09:48,600 --> 00:09:51,720
We went on through CMU, but were you there at the same time?

160
00:09:51,720 --> 00:09:52,720
Do you know one?

161
00:09:52,720 --> 00:09:53,720
Yeah.

162
00:09:53,720 --> 00:09:56,200
I had her as a teacher, as a professor, of course, yes.

163
00:09:56,200 --> 00:09:57,200
Oh.

164
00:09:57,200 --> 00:09:58,200
Yeah, yeah.

165
00:09:58,200 --> 00:10:04,160
I think like some like just like ML or some, I remember an assignment that I did for

166
00:10:04,160 --> 00:10:10,720
Manuel Ovalosa, which was to automatically create crossword puzzles, not to solve them

167
00:10:10,720 --> 00:10:12,200
but to create them.

168
00:10:12,200 --> 00:10:13,200
Okay.

169
00:10:13,200 --> 00:10:15,200
That was one of the assignments I remember.

170
00:10:15,200 --> 00:10:16,200
Yeah.

171
00:10:16,200 --> 00:10:17,200
Awesome.

172
00:10:17,200 --> 00:10:18,200
Cool.

173
00:10:18,200 --> 00:10:20,440
So how long have you been at Square?

174
00:10:20,440 --> 00:10:28,080
So at Square's, but it's three years now, I did some, I was, so I spent some time doing

175
00:10:28,080 --> 00:10:33,160
like speech analytics for like call centers and then I moved more into kind of the startup

176
00:10:33,160 --> 00:10:34,160
world.

177
00:10:34,160 --> 00:10:36,760
It was with mine melt with Yic Yac.

178
00:10:36,760 --> 00:10:42,720
And then three years ago, I took it with some other team from Yic Yac we joined Square.

179
00:10:42,720 --> 00:10:44,040
Okay.

180
00:10:44,040 --> 00:10:51,040
And you're particularly focused on commerce at Square.

181
00:10:51,040 --> 00:10:58,320
I imagine there are lots of different ways that machine learning is used at the company,

182
00:10:58,320 --> 00:11:04,880
but what is the, you know, earlier, a handful of core use cases for ML at commerce.

183
00:11:04,880 --> 00:11:05,880
Yeah.

184
00:11:05,880 --> 00:11:06,880
Yeah.

185
00:11:06,880 --> 00:11:07,880
Yeah.

186
00:11:07,880 --> 00:11:10,880
The cool thing about Square is that sort of machine learning is foundational to the company,

187
00:11:10,880 --> 00:11:11,880
right?

188
00:11:11,880 --> 00:11:17,840
Because we can be so open as a platform and allow for anybody to sign up to be a square

189
00:11:17,840 --> 00:11:24,440
seller or even a square developer through our API is also because we have the mechanisms

190
00:11:24,440 --> 00:11:27,360
in place to watch out for bad actors, right?

191
00:11:27,360 --> 00:11:34,200
Anytime that you deal with money, you have to watch out for all kinds of fraud and risk.

192
00:11:34,200 --> 00:11:39,640
And so to some extent, risk management is sort of the foundational piece of the application

193
00:11:39,640 --> 00:11:45,560
of ML at Square, but then over the course of the years, because we're not just the people

194
00:11:45,560 --> 00:11:52,240
know about the little white reader that converts a smartphone into a credit card or a point

195
00:11:52,240 --> 00:11:58,760
of sale system, but actually we've expanded quite a lot the offerings in terms of products

196
00:11:58,760 --> 00:12:00,000
and services.

197
00:12:00,000 --> 00:12:05,400
You can run a small business just using Square with things like inventory management and

198
00:12:05,400 --> 00:12:10,160
even team management, we can do payroll and taxes for your employees.

199
00:12:10,160 --> 00:12:19,280
So that on the one hand, obviously, it's an excellent way of helping our sellers.

200
00:12:19,280 --> 00:12:23,960
On the other, from a risk perspective, you also have to watch out for sort of new avenues,

201
00:12:23,960 --> 00:12:26,480
new avenues for sort of malicious users.

202
00:12:26,480 --> 00:12:32,520
I'll give you a cool example, which is we have a product called Square Marketing, right?

203
00:12:32,520 --> 00:12:37,120
Which allows our sellers to have a conversation with their buyers through email campaigns.

204
00:12:37,120 --> 00:12:42,760
And we provide some nicely designed templates so that they can show their latest products

205
00:12:42,760 --> 00:12:47,960
or loyalty program, et cetera, except that in a couple of occasions, we saw that that

206
00:12:47,960 --> 00:12:51,360
was being misused as phishing attempt, right?

207
00:12:51,360 --> 00:12:57,600
So someone would craft what looked like was supposed to be an email marketing campaign

208
00:12:57,600 --> 00:13:04,400
that actually contain some link to some nefarious phishing site.

209
00:13:04,400 --> 00:13:07,320
So what happens there, sort of ML to the rescue, right?

210
00:13:07,320 --> 00:13:13,080
So we quickly develop a classifier so that before an email gets sent, it gets inspected.

211
00:13:13,080 --> 00:13:19,560
And then we look at what are some signals here that tells us that something is off.

212
00:13:19,560 --> 00:13:23,760
And of course, you can think of, well, usually there's a button associated with that sort

213
00:13:23,760 --> 00:13:28,760
of with a call to action, with a phishing attempt, so we can inspect the URL and compare

214
00:13:28,760 --> 00:13:33,200
whether the domain of the URL matches the URL of the seller if they have a website.

215
00:13:33,200 --> 00:13:35,880
If it doesn't, that's a bit of a red flag.

216
00:13:35,880 --> 00:13:41,280
But interestingly, and going back to the language, just by looking at a couple of examples,

217
00:13:41,280 --> 00:13:46,040
you can see that the language being used in the phishing attempts is very different from

218
00:13:46,040 --> 00:13:49,240
the normal sort of marketing campaign.

219
00:13:49,240 --> 00:13:52,040
In fact, it tends to be much more negative, right?

220
00:13:52,040 --> 00:13:57,080
It says, problem with your account, go fix it.

221
00:13:57,080 --> 00:14:01,800
And so interestingly, as I'm sure you know, there's this technique in natural language processing

222
00:14:01,800 --> 00:14:06,520
called sentiment analysis, where you can take a bunch of text and basically map it onto

223
00:14:06,520 --> 00:14:10,560
a single number, you know, minus one sad to plus one happy.

224
00:14:10,560 --> 00:14:17,880
So the sentiment polarity is a signal that we now use for this classifier and not by

225
00:14:17,880 --> 00:14:20,960
itself, but in conjunction with other features.

226
00:14:20,960 --> 00:14:27,200
And of course, now, so I tell, I always say, you know, don't tell these to the bad guys,

227
00:14:27,200 --> 00:14:28,200
right?

228
00:14:28,200 --> 00:14:32,720
Otherwise, they're going to adapt and start crafting marketing campaigns, saying wonderful

229
00:14:32,720 --> 00:14:33,720
opportunity.

230
00:14:33,720 --> 00:14:36,520
We can press this button, so we can have that.

231
00:14:36,520 --> 00:14:37,520
That's right.

232
00:14:37,520 --> 00:14:39,920
Wouldn't you want to freshen up your password?

233
00:14:39,920 --> 00:14:40,920
Yeah.

234
00:14:40,920 --> 00:14:46,840
So, but so the overall point here is that anytime that you have a new product, anytime that

235
00:14:46,840 --> 00:14:51,160
you have a new API, you obviously have to also think about, you know, how could these

236
00:14:51,160 --> 00:14:54,840
be misused and how can we sort of protect against that?

237
00:14:54,840 --> 00:15:01,720
So, manage degrees remains one of the sort of largest applications of ML at square.

238
00:15:01,720 --> 00:15:06,080
Another one though, which is also very interesting is just being able to be better at automating

239
00:15:06,080 --> 00:15:13,280
operations and being able to, for example, do more accurate marketing campaigns.

240
00:15:13,280 --> 00:15:20,160
As our number of products grows, it's also important to be able to let a certain subset

241
00:15:20,160 --> 00:15:25,040
of our sellers know about that new product, the ones that are going to be most interested

242
00:15:25,040 --> 00:15:26,040
in in them, right?

243
00:15:26,040 --> 00:15:31,600
So, a good example is a square appointments, square appointments, if you're a hairdresser,

244
00:15:31,600 --> 00:15:34,240
and you don't want to be on the phone all the time, you know, are you open at three, can

245
00:15:34,240 --> 00:15:35,640
I come in tomorrow?

246
00:15:35,640 --> 00:15:41,080
You'd rather have a self-serve external portal where people can just a calendar where people

247
00:15:41,080 --> 00:15:42,960
can book themselves, right?

248
00:15:42,960 --> 00:15:46,120
So that's exactly what square appointments is, and actually you're offering these for

249
00:15:46,120 --> 00:15:49,280
free for just for an initial location.

250
00:15:49,280 --> 00:15:56,040
So we've had a lot of success, but then the next question is, among our millions of sellers,

251
00:15:56,040 --> 00:15:58,400
who would benefit the most from square appointments, right?

252
00:15:58,400 --> 00:16:01,760
Because we don't want to send like a mass email that people are going to consider spam

253
00:16:01,760 --> 00:16:07,240
if they're not, if they're just a little kiosk, and they're not like booking doesn't

254
00:16:07,240 --> 00:16:09,200
make sense for that particular seller.

255
00:16:09,200 --> 00:16:15,920
So obviously we can do that by sort of what we call MCC Merchant Category Code, but also

256
00:16:15,920 --> 00:16:22,040
there's some cool ways of kind of inspecting the items and services that are in our seller's

257
00:16:22,040 --> 00:16:27,400
catalogs to understand whether there's certain things that are being booked by time, right?

258
00:16:27,400 --> 00:16:31,880
Is there any place in the description of your item where you mentioned one hour or a half

259
00:16:31,880 --> 00:16:32,880
an hour?

260
00:16:32,880 --> 00:16:36,080
Well, that's a perfect candidate for square appointments.

261
00:16:36,080 --> 00:16:42,200
So we're able to sort of use also a mail to have a much more targeted marketing campaigns.

262
00:16:42,200 --> 00:16:45,880
And so that's going to put a million operations.

263
00:16:45,880 --> 00:16:53,960
And then there's a third category, which you can think of driving sort of product features

264
00:16:53,960 --> 00:16:57,840
something that is visible to the seller, right?

265
00:16:57,840 --> 00:17:04,120
And so a couple of examples are we now are able to make suggestions in your catalog like

266
00:17:04,120 --> 00:17:09,320
based on what type of business you are, or what type of items you already have in your

267
00:17:09,320 --> 00:17:10,320
catalog.

268
00:17:10,320 --> 00:17:15,760
We're able to sort of suggest something that other products that you may want to carry.

269
00:17:15,760 --> 00:17:20,160
Or a simple thing is when you create a new item, let's say you're a bakery, which by

270
00:17:20,160 --> 00:17:21,440
the way, it's interesting.

271
00:17:21,440 --> 00:17:25,880
We can also know what type of business you are, obviously based on your name, right?

272
00:17:25,880 --> 00:17:30,080
If you're Josephine's bakery, you're a bakery, also this is a true story.

273
00:17:30,080 --> 00:17:36,880
If you're, what was the, oh yeah, rolling scones, you're also a bakery.

274
00:17:36,880 --> 00:17:40,880
And then so we're able to based on your name and certainly based on the type of items

275
00:17:40,880 --> 00:17:41,880
that you sell.

276
00:17:41,880 --> 00:17:42,880
Have a good name.

277
00:17:42,880 --> 00:17:44,200
Where is that business?

278
00:17:44,200 --> 00:17:46,240
It's a real bakery somewhere.

279
00:17:46,240 --> 00:17:47,240
I'm not sure.

280
00:17:47,240 --> 00:17:50,800
I think maybe Seattle will have what can look it up.

281
00:17:50,800 --> 00:17:54,640
So we know what type of business you are based also a little bit of sort of this sort of

282
00:17:54,640 --> 00:18:05,240
behavior, not just the name, but also the items that you sell and the ticket sizes, etc.

283
00:18:05,240 --> 00:18:08,920
But then when you create an item, let's say you're a bakery and you create a new item

284
00:18:08,920 --> 00:18:14,560
called croissant, well, we know that there's going to be certain very common variations,

285
00:18:14,560 --> 00:18:18,280
like plain, almond, chocolate, right?

286
00:18:18,280 --> 00:18:23,320
And we actually, what, you don't like croissants?

287
00:18:23,320 --> 00:18:31,600
No, I was thumbs down, plain, almond, meat, chocolate, yeah, it's a progression, yeah.

288
00:18:31,600 --> 00:18:32,600
Yes.

289
00:18:32,600 --> 00:18:38,080
And so we're able to auto suggest that because those are sort of, you know, we, because

290
00:18:38,080 --> 00:18:43,040
we understand a bit of the domain, but in a way that is sort of fully automated.

291
00:18:43,040 --> 00:18:45,880
So that's one example, suggestions in the catalog.

292
00:18:45,880 --> 00:18:52,440
And then another very sort of new area where we're just starting with is with what we call

293
00:18:52,440 --> 00:18:54,600
the square assistant.

294
00:18:54,600 --> 00:18:59,080
This is the, the result of an acquisition we did last year of eloquent labs.

295
00:18:59,080 --> 00:19:01,080
We now call it the conversations team.

296
00:19:01,080 --> 00:19:06,200
And they recently released square assistant right now for appointments, which means that

297
00:19:06,200 --> 00:19:11,240
when you, when a buyer gets a reminder of one of these square appointments, they can

298
00:19:11,240 --> 00:19:17,760
now reply and say, oh, yes, I'll be there or oh, shoot, can I come in tomorrow instead?

299
00:19:17,760 --> 00:19:24,400
And rain out so the assistant is able to respond on the seller's behalf about the appointments.

300
00:19:24,400 --> 00:19:29,240
And so these are kind of a new area where I think there's a lot of potential.

301
00:19:29,240 --> 00:19:37,440
So when you have a business, you know, for which ML is so fundamental, yeah, I'm thinking

302
00:19:37,440 --> 00:19:41,800
about how you kind of manage the portfolio of projects and how you kind of organize

303
00:19:41,800 --> 00:19:42,800
around that.

304
00:19:42,800 --> 00:19:48,480
Very different from, you know, an enterprise that is, you know, has some other business

305
00:19:48,480 --> 00:19:52,440
and is thinking about how can we, you know, take advantage of machine learning.

306
00:19:52,440 --> 00:19:55,600
Let's spin up a project here, spin it, spin up a project there.

307
00:19:55,600 --> 00:19:56,600
Right.

308
00:19:56,600 --> 00:20:03,200
You know, here you've got, you know, I can imagine, you know, every, you know, every product,

309
00:20:03,200 --> 00:20:07,360
every feature effort you're thinking about, you know, an engineer is thinking in the

310
00:20:07,360 --> 00:20:12,120
back of their head is, you know, is my goal best served by machine learning or something

311
00:20:12,120 --> 00:20:17,440
simpler, you know, and that might have you with a lot of, you know, places where you're

312
00:20:17,440 --> 00:20:22,640
trying to have a lot of impact across the, the product portfolio.

313
00:20:22,640 --> 00:20:27,320
How do you, well, yes, yes, I would say to that is it's certainly the case that it's

314
00:20:27,320 --> 00:20:31,560
where I guess we're sort of enough, you know, sort of a lucky situation where, sort

315
00:20:31,560 --> 00:20:37,440
of technology and ML was understood as a kind of a core component from the get go.

316
00:20:37,440 --> 00:20:42,920
But for any other organization, I think there's some steps that anybody can take, which is

317
00:20:42,920 --> 00:20:45,280
the first one is just kind of more awareness, right?

318
00:20:45,280 --> 00:20:51,160
So provide some broad machine learning training so that people at least sort of understand

319
00:20:51,160 --> 00:20:54,720
a little bit of the concepts of how do you have this.

320
00:20:54,720 --> 00:20:58,080
So the key thing about machine learning is that it's something that learns from the data,

321
00:20:58,080 --> 00:21:00,600
learns from experience, right?

322
00:21:00,600 --> 00:21:08,160
And so what we do even at square is we have sort of a training that we call ML for everyone

323
00:21:08,160 --> 00:21:11,200
where sort of we sort of teach this concept.

324
00:21:11,200 --> 00:21:14,920
And then we follow that up with a brainstorming session, like now that we know a little bit

325
00:21:14,920 --> 00:21:22,080
about ML, let's think about how could ML help your team or your customers and we come

326
00:21:22,080 --> 00:21:24,440
up with specific examples, right?

327
00:21:24,440 --> 00:21:29,160
And actually it's cool to also use the, you know, Andruang's sort of heuristic about

328
00:21:29,160 --> 00:21:34,600
what ML can do, but he says any task that a human can do in about a second of thought,

329
00:21:34,600 --> 00:21:39,560
about about one second of cognitive effort is something that we can probably automate,

330
00:21:39,560 --> 00:21:40,560
right?

331
00:21:40,560 --> 00:21:44,240
And of course, it's not a perfect heuristic, like, you know, chess playing, we would

332
00:21:44,240 --> 00:21:48,400
already take way longer than a second to decide the next move.

333
00:21:48,400 --> 00:21:51,000
But it gives us a little bit of some parameters.

334
00:21:51,000 --> 00:21:54,160
And so that gives us a lot of ideas to our teams.

335
00:21:54,160 --> 00:21:59,080
And then of course, you need to kind of aggregate and prioritize.

336
00:21:59,080 --> 00:22:00,560
But that typically leads.

337
00:22:00,560 --> 00:22:03,320
So a lot of these ideas are very good.

338
00:22:03,320 --> 00:22:07,200
But then you realize that they depend on the availability of data.

339
00:22:07,200 --> 00:22:13,720
So the third point is from an organizational perspective to treat data as a first class

340
00:22:13,720 --> 00:22:20,080
citizen, meaning that it's not that you have some sort of analytics as an afterthought

341
00:22:20,080 --> 00:22:25,600
of some, you know, kind of looking over and trying to, you know, look at some logs.

342
00:22:25,600 --> 00:22:31,200
But rather half the data already have a, when you design a functionality, when you design

343
00:22:31,200 --> 00:22:38,440
a product or an API already having mine, which data is this service going to consume?

344
00:22:38,440 --> 00:22:44,920
And most importantly, which data the service is able to generate so that other services

345
00:22:44,920 --> 00:22:47,080
can easily use that data, right?

346
00:22:47,080 --> 00:22:52,640
So there's quite a bit of work in terms of standardizing how we produce that data.

347
00:22:52,640 --> 00:22:56,960
And then also kind of the data infrastructure to have kind of a common repository of that

348
00:22:56,960 --> 00:23:02,360
data and not just the raw data, but then also have you sort of aggregated into sort of

349
00:23:02,360 --> 00:23:07,920
useful features, meaning, for example, in terms of transactions, you don't have to some,

350
00:23:07,920 --> 00:23:11,680
for in some cases, you don't need to know every single transaction about a seller.

351
00:23:11,680 --> 00:23:17,760
You just want to know what their average weekly, you know, processing volume is, right?

352
00:23:17,760 --> 00:23:21,680
And so you aggregate that into a signal and you can put that into what we call a feature

353
00:23:21,680 --> 00:23:27,840
store, then can be used by other teams and other, even other ML models.

354
00:23:27,840 --> 00:23:33,480
So once you have these sort of data as a first-class citizen, then it's much easier to start

355
00:23:33,480 --> 00:23:39,120
sort of weaving machine learning into your kind of products and processes by making use

356
00:23:39,120 --> 00:23:40,840
of these data.

357
00:23:40,840 --> 00:23:46,440
And then all of these, obviously, also, there's all these talk about, and necessarily talk

358
00:23:46,440 --> 00:23:54,960
about sort of ethical AIML principles and also being aware of regulations like GDPR in

359
00:23:54,960 --> 00:24:00,400
the European Union or CCPA in California, that you want to make sure that these systems

360
00:24:00,400 --> 00:24:05,520
sort of are, the technical term is, you need to show that they don't have a so-called

361
00:24:05,520 --> 00:24:09,680
disparate impact on protected classes, right?

362
00:24:09,680 --> 00:24:15,960
But it basically means that they're sort of neutral and there's a side where, so when

363
00:24:15,960 --> 00:24:20,040
you create a certain model, there's certain features that you don't want to look at,

364
00:24:20,040 --> 00:24:23,360
at the same time, for a compliance perspective, you still need to know what those features

365
00:24:23,360 --> 00:24:26,720
are to show that there's no disparate impact.

366
00:24:26,720 --> 00:24:30,320
But that's done by different teams, okay?

367
00:24:30,320 --> 00:24:37,440
So if you do that, so provide broad ML training, brainstorming about what ML can do for you,

368
00:24:37,440 --> 00:24:43,040
treat data for first-class citizen, then you can start doing some cool ML driven.

369
00:24:43,040 --> 00:24:48,280
Initially, it could just be some smart defaults, personalization, and then over time, you

370
00:24:48,280 --> 00:24:55,200
can have almost entire divisions, like Square Capital, which is sort of the armament within

371
00:24:55,200 --> 00:24:58,080
Square where we facilitate loans to our sellers.

372
00:24:58,080 --> 00:25:06,680
You can basically argue that that's pretty much all driven by ML, because, in fact, Square

373
00:25:06,680 --> 00:25:15,440
Capital is a great example where we can really show that the over-arching purpose at Square

374
00:25:15,440 --> 00:25:23,240
we call it economic empowerment, and that means making it easier for everyone to be able

375
00:25:23,240 --> 00:25:32,160
to participate in the economy and to have very clear understanding of what the processing

376
00:25:32,160 --> 00:25:41,720
fees are, exactly what is expected, and there's no hidden fees or issues like that.

377
00:25:41,720 --> 00:25:48,480
And so one of the good examples of this is how Square Capital just looks at how a seller

378
00:25:48,480 --> 00:25:50,720
is doing within our platform.

379
00:25:50,720 --> 00:25:56,920
We don't necessarily need to go out and check your FICO score or your history outside.

380
00:25:56,920 --> 00:26:04,840
We just look at how you've been doing within Square, and that has allowed us to basically

381
00:26:04,840 --> 00:26:09,160
facilitate these loans to many sellers to expand their business.

382
00:26:09,160 --> 00:26:16,400
On this idea of data as a first-class citizen, does that automatically mean just kind of

383
00:26:16,400 --> 00:26:24,480
save everything, or are you still needing to be very selective as to what you save, how

384
00:26:24,480 --> 00:26:31,560
to obtain it, compliant applications, how do you balance those operations?

385
00:26:31,560 --> 00:26:38,760
There's a good point, I mean, your initial instinct might be, well, let's just save everything,

386
00:26:38,760 --> 00:26:42,080
but that quickly becomes unmanageable.

387
00:26:42,080 --> 00:26:47,760
If you look at the log files generated by every single service, and we have hundreds

388
00:26:47,760 --> 00:26:53,520
if not thousands of services, so even there's some operational logs and stuff that you

389
00:26:53,520 --> 00:26:56,920
just retain for a little bit.

390
00:26:56,920 --> 00:27:04,120
But then there's the important piece, more of the semantics of this user logged in, and

391
00:27:04,120 --> 00:27:05,800
they have these transactions.

392
00:27:05,800 --> 00:27:08,320
That is certainly recorded.

393
00:27:08,320 --> 00:27:17,640
So there is a little bit of certainly sort of filtering of the important actions, and

394
00:27:17,640 --> 00:27:23,920
then there's also a lot of what I was mentioning before of converting these raw data into

395
00:27:23,920 --> 00:27:26,160
more useful aggregates.

396
00:27:26,160 --> 00:27:35,960
I'm so curious about given all of the places that you are trying to make an impact at Square,

397
00:27:35,960 --> 00:27:43,360
how does your particular team, you run an ML organization as opposed to engineering,

398
00:27:43,360 --> 00:27:50,720
a particular feature, or I'm trying to get at the relationship between your organization

399
00:27:50,720 --> 00:27:59,240
and the product teams, and are you kind of embedding people, are you kind of a centralized

400
00:27:59,240 --> 00:28:06,800
organization that takes on ownership of ML capabilities that get plugged into other

401
00:28:06,800 --> 00:28:07,800
features.

402
00:28:07,800 --> 00:28:08,800
How does all that work?

403
00:28:08,800 --> 00:28:13,120
Yeah, it's a good question because it's really a bit of a fluid situation.

404
00:28:13,120 --> 00:28:18,400
So, a couple of things there, first of all, Square is very much a product oriented, and

405
00:28:18,400 --> 00:28:25,200
so ultimately what really drives the adoption of machine learning is how well it can serve

406
00:28:25,200 --> 00:28:32,360
the ultimate function of overall its economic empowerment, make the life of our sellers easier,

407
00:28:32,360 --> 00:28:38,760
and create sort of in the parlance of these remarkable or delightful experiences that

408
00:28:38,760 --> 00:28:43,560
really make it sort of almost fun to use, you know, a Square software.

409
00:28:43,560 --> 00:28:50,200
So that is the ultimate sort of driver, and so we see ML as an enabler to that, and the

410
00:28:50,200 --> 00:28:55,120
question about how exactly do you do that, so they're supposed sort of kind of vertical

411
00:28:55,120 --> 00:29:01,200
and horizontal in the sense that first of all, we do want every engineer at Square to

412
00:29:01,200 --> 00:29:03,960
be at least aware of machine learning, right?

413
00:29:03,960 --> 00:29:10,360
In the same way that you want every engineer to be infosec conscious, right, aware of information

414
00:29:10,360 --> 00:29:15,800
security issues, and when you develop an API, how do you check that you're not being hacked?

415
00:29:15,800 --> 00:29:25,000
We also encourage all the engineers to get more familiar with what ML is capable of.

416
00:29:25,000 --> 00:29:32,440
Now we do also have teams that are sort of highly skilled data scientists and ML engineers,

417
00:29:32,440 --> 00:29:38,640
and they can sort of drive more of that, but typically, so we have some teams that are

418
00:29:38,640 --> 00:29:44,440
almost like all data science ML, but then the other idea that they were increasingly

419
00:29:44,440 --> 00:29:49,240
doing is kind of in the same way that you in order to have a full stack team, you need

420
00:29:49,240 --> 00:29:56,160
backend and you need front end, well, you will also add some like ML sort of expert

421
00:29:56,160 --> 00:30:03,200
or some engineer that has more interest in ML to kind of balance the team.

422
00:30:03,200 --> 00:30:08,520
Is there a typical project or a way that you engage with these product groups to help

423
00:30:08,520 --> 00:30:11,600
deliver new ML capability?

424
00:30:11,600 --> 00:30:17,920
So yes, typically there's an assessment of kind of the backlog, like what all the features

425
00:30:17,920 --> 00:30:21,720
are already sort of planning to do, but then we also do these brainstorming sessions

426
00:30:21,720 --> 00:30:26,880
to kind of bubble up some new ideas, and then of course there's this interesting sort

427
00:30:26,880 --> 00:30:33,000
of assessment of the idea in terms of, well, what is the expected impact, but also how

428
00:30:33,000 --> 00:30:36,400
feasible is it from a ML perspective, right?

429
00:30:36,400 --> 00:30:43,280
And also do we have enough data for the model to be able to make a good decision.

430
00:30:43,280 --> 00:30:49,040
And so if we go through a bit of that, and then we kind of select the sort of the most

431
00:30:49,040 --> 00:30:55,640
promising projects, and then we typically, what we, a good model that we have found is

432
00:30:55,640 --> 00:31:02,960
that the ML team is responsible for kind of the backend, kind of computing these suggestions

433
00:31:02,960 --> 00:31:08,640
or being able to respond to all these different messages from buyers.

434
00:31:08,640 --> 00:31:16,720
And then the product team that owns that sort of the product surface that we're talking

435
00:31:16,720 --> 00:31:22,880
about kind of ends up owning the feature, but basically they make calls to the backend

436
00:31:22,880 --> 00:31:25,760
that is managed by an ML team.

437
00:31:25,760 --> 00:31:34,320
Yeah, one of the issues that I think that that kind of relationship raises is that there's

438
00:31:34,320 --> 00:31:42,880
so much crossover in terms of the way machine learning originated information is surface

439
00:31:42,880 --> 00:31:49,920
to users, and do you express a degree of certainty on a probabilistic determination?

440
00:31:49,920 --> 00:31:56,640
And if so, how, you know, kinds of design issues that come into there, you know, something

441
00:31:56,640 --> 00:32:01,640
that you think a lot about there, or as you approach that.

442
00:32:01,640 --> 00:32:02,640
Right.

443
00:32:02,640 --> 00:32:03,640
Yeah, yeah.

444
00:32:03,640 --> 00:32:07,760
As I mentioned, so fundamental to how we approach the development of an ML driven feature

445
00:32:07,760 --> 00:32:09,720
is how is it going to look like?

446
00:32:09,720 --> 00:32:15,600
What's the experience going to be both when the model is accurate and most importantly,

447
00:32:15,600 --> 00:32:18,040
what happens when the model fails, right?

448
00:32:18,040 --> 00:32:23,040
And failing, there's ways of failing failing because there's no suggestion that is above

449
00:32:23,040 --> 00:32:28,320
a certain confidence threshold, in which case there are like, there's no suggestion.

450
00:32:28,320 --> 00:32:32,000
That's one situation and then the other one, which is obviously worse, is what happens

451
00:32:32,000 --> 00:32:34,240
when the suggestion is incorrect, right?

452
00:32:34,240 --> 00:32:35,960
And there's different degrees of that.

453
00:32:35,960 --> 00:32:41,080
So for example, in the case of item suggestion, you know, what is the worst that could happen?

454
00:32:41,080 --> 00:32:46,120
Well, you know, you're a bakery, and instead of suggesting a croissant, we suggest sock,

455
00:32:46,120 --> 00:32:47,120
right?

456
00:32:47,120 --> 00:32:49,920
Well, a little bit odd, but not the end of the world, right?

457
00:32:49,920 --> 00:32:52,880
So in that case, we can be a bit more lenient.

458
00:32:52,880 --> 00:32:58,440
On the other hand, for something like, you know, the credit, you know, issuing a loan or

459
00:32:58,440 --> 00:33:04,160
not, we have to be quite accurate in terms of modeling precision versus recall and the

460
00:33:04,160 --> 00:33:09,360
cost of a false positive, for example, a seller that would not pay us back versus a false

461
00:33:09,360 --> 00:33:13,720
negative, a seller that would have benefited, but we didn't issue a loan.

462
00:33:13,720 --> 00:33:20,600
And so ultimately, it's kind of a business decision, but it's highly informed by the model

463
00:33:20,600 --> 00:33:24,480
and being able to kind of set the correct operating point.

464
00:33:24,480 --> 00:33:29,800
To your point, it's certainly something that we have sort of front and center is the design

465
00:33:29,800 --> 00:33:35,960
of the feature and being able to make sure that that overall the product and the feature

466
00:33:35,960 --> 00:33:41,840
is still useful even when the model makes a mistake.

467
00:33:41,840 --> 00:33:47,240
And so that could go a little bit about sort of the sort of these four aspects that we

468
00:33:47,240 --> 00:33:54,640
think about when designing an ML driven feature, certainly the design and the actual UI and

469
00:33:54,640 --> 00:33:57,320
the different sort of experience.

470
00:33:57,320 --> 00:34:00,280
So the other one is the modeling, right?

471
00:34:00,280 --> 00:34:02,280
So how can we make the model more accurate?

472
00:34:02,280 --> 00:34:06,440
How do we make sure that it actually improves over time?

473
00:34:06,440 --> 00:34:14,080
And that typically means that we also have kind of a product analytics layer that kind of

474
00:34:14,080 --> 00:34:19,520
looks at how the adoption of that feature and how often those suggestions get accepted

475
00:34:19,520 --> 00:34:22,600
or not, and that feeds back into the model.

476
00:34:22,600 --> 00:34:25,800
And then the other piece is sort of engineering, right?

477
00:34:25,800 --> 00:34:29,960
Sometimes you may have a very complex model, but then when you need to run it, not for

478
00:34:29,960 --> 00:34:35,120
one user, but for millions of users, then you suddenly realize that maybe you need something

479
00:34:35,120 --> 00:34:38,920
a little bit simpler, so that it's actually computationally tractable.

480
00:34:38,920 --> 00:34:50,720
I'm curious when you kind of look across your portfolio of models, what is the technology

481
00:34:50,720 --> 00:34:57,320
mix? The square is obviously working with a lot of kind of traditional tabular data.

482
00:34:57,320 --> 00:35:03,980
Does that mean you're attending a more traditional techniques, or do you have a significant deep

483
00:35:03,980 --> 00:35:05,280
learning footprint?

484
00:35:05,280 --> 00:35:08,280
How do you think about technology landscape?

485
00:35:08,280 --> 00:35:09,280
Yeah.

486
00:35:09,280 --> 00:35:10,280
Yeah.

487
00:35:10,280 --> 00:35:15,440
Well, so it's interesting because on a per-task basis, you always want to start with a simplest

488
00:35:15,440 --> 00:35:17,560
model possible, right?

489
00:35:17,560 --> 00:35:23,520
It's sort of very alluring to say, oh, we're going to move everything to deep learning.

490
00:35:23,520 --> 00:35:29,560
But in fact, if taking a weighted average of what happened in the last two weeks gives

491
00:35:29,560 --> 00:35:34,080
you a good sense, you don't need a super complex model.

492
00:35:34,080 --> 00:35:39,640
And certainly when we started, now the company is 11 years old, there wasn't even deep

493
00:35:39,640 --> 00:35:40,640
learning at the time.

494
00:35:40,640 --> 00:35:49,360
So certainly some of the models are simpler, like the XG boost, et cetera, et cetera.

495
00:35:49,360 --> 00:35:57,200
But over time, now we also have some very sophisticated models that do use things like RNNs and

496
00:35:57,200 --> 00:35:59,000
CNNs.

497
00:35:59,000 --> 00:36:04,120
But we always sort of compare and contrast kind of making sure that the additional complexity

498
00:36:04,120 --> 00:36:08,640
is worth it and that the accuracy actually goes up.

499
00:36:08,640 --> 00:36:13,440
Are there any particular examples of places where you found that the additional complexity

500
00:36:13,440 --> 00:36:19,840
was justified because the problem was that interesting or complex or nuanced?

501
00:36:19,840 --> 00:36:20,840
Yeah.

502
00:36:20,840 --> 00:36:23,440
So I'll give you a relatively simple example.

503
00:36:23,440 --> 00:36:31,680
But going back to these item suggestions, we basically, rather than trying to do some

504
00:36:31,680 --> 00:36:37,920
simple, I don't know, like TFI, DF, like work frequencies and stuff, we just trained

505
00:36:37,920 --> 00:36:39,960
our own embeddings, right?

506
00:36:39,960 --> 00:36:45,960
So we applied something similar to Word2Vec in the description of the item.

507
00:36:45,960 --> 00:36:51,120
So we have, usually an item has a category like pastry, the item is the croissant and

508
00:36:51,120 --> 00:36:57,000
then the variations are in your favorite order, chocolate, almond, plain, although some

509
00:36:57,000 --> 00:37:02,440
people call it butter instead of plain, to make it less plain, they call it butter.

510
00:37:02,440 --> 00:37:09,160
And so you put all of these millions of descriptions together and you train a Word2Vec, which is

511
00:37:09,160 --> 00:37:14,000
basically you learn these vector representations in this hyper-dimensional space that is very

512
00:37:14,000 --> 00:37:17,040
sort of semantically true to the semantics.

513
00:37:17,040 --> 00:37:23,560
So interestingly, when we do that, we realize that even though, say, small and large are

514
00:37:23,560 --> 00:37:29,200
kind of opposites of each other in general English, within the context of these item catalogs,

515
00:37:29,200 --> 00:37:33,360
small and large are actually semantically very close and literally they are geometrically

516
00:37:33,360 --> 00:37:36,360
close in this hyper-dimensional vector space.

517
00:37:36,360 --> 00:37:42,280
And that is because in this case, both small and large encode a value for the attribute

518
00:37:42,280 --> 00:37:47,320
size, right, which can be applied to a coffee or to a t-shirt.

519
00:37:47,320 --> 00:37:52,720
And so this is how we kind of approach some of more sophisticated models, but we kind

520
00:37:52,720 --> 00:37:54,880
of make sure that it works.

521
00:37:54,880 --> 00:38:01,360
And also, one thing to add is we recently, so Square recently acquired Desa from Toronto,

522
00:38:01,360 --> 00:38:04,200
where they have a lot of deep learning expertise.

523
00:38:04,200 --> 00:38:09,200
In fact, they were famous for some cool sort of deep fakes.

524
00:38:09,200 --> 00:38:14,320
And so we have a lot of ideas about how to apply some of more of these sophisticated

525
00:38:14,320 --> 00:38:20,280
techniques to both the Square Cellar, POS, and also to Square Cash, the Cash app.

526
00:38:20,280 --> 00:38:29,280
What kind of tooling and technology platforms have you established to allow your data scientists

527
00:38:29,280 --> 00:38:35,520
to move more quickly, be more agile, get models and a production more repeatedly?

528
00:38:35,520 --> 00:38:36,520
Yeah.

529
00:38:36,520 --> 00:38:38,440
This is also evolving.

530
00:38:38,440 --> 00:38:44,520
As you can imagine in the beginning, it's, and also because Square is fairly decentralized,

531
00:38:44,520 --> 00:38:47,320
we have sort of different teams exploring different solutions.

532
00:38:47,320 --> 00:38:51,240
But it's also a combination of, we have our own data centers, but we also use quite a

533
00:38:51,240 --> 00:38:54,440
bit of cloud, both AWS, GCP.

534
00:38:54,440 --> 00:39:00,600
And so, you could, roughly speaking, we do a lot of the training and model development

535
00:39:00,600 --> 00:39:02,120
in the cloud.

536
00:39:02,120 --> 00:39:07,760
But when it comes time to serve, we still serve from our data centers, not, you know,

537
00:39:07,760 --> 00:39:11,440
not an absolute, but as, you know, kind of overall.

538
00:39:11,440 --> 00:39:15,880
And then in terms of making it easy for internally to write.

539
00:39:15,880 --> 00:39:23,000
That proximity to, to feature data or some other factor that has you as that split.

540
00:39:23,000 --> 00:39:26,240
Yeah, also kind of historical reason.

541
00:39:26,240 --> 00:39:31,280
And like we, we do have for some of the core services, we do want to maintain sort of

542
00:39:31,280 --> 00:39:33,240
full control.

543
00:39:33,240 --> 00:39:38,840
We have a good record of sort of availability and we want to maintain that.

544
00:39:38,840 --> 00:39:45,520
And then we have kind of a platform team that is developing some things like, you know,

545
00:39:45,520 --> 00:39:55,080
hosted Python notebooks and some feature store and some data infrastructure that is sort

546
00:39:55,080 --> 00:39:57,080
of used across all the teams.

547
00:39:57,080 --> 00:40:03,320
And then each team also has a, but so each team also has some sort of the freedom to use

548
00:40:03,320 --> 00:40:07,120
slightly different, you know, text acts, depending on their preferences.

549
00:40:07,120 --> 00:40:13,120
But we do have a very sort of active internal sort of community where we get together every

550
00:40:13,120 --> 00:40:19,920
week and we sort of share the different projects across the different teams and units so that

551
00:40:19,920 --> 00:40:24,360
people are aware of sort of best practices and we can collaborate.

552
00:40:24,360 --> 00:40:30,360
Are the folks in your, or primarily engineers or data scientists or a mix?

553
00:40:30,360 --> 00:40:35,680
Yeah, it's a mix because at the end of the day, we're still shipping features, right?

554
00:40:35,680 --> 00:40:42,200
And so we can partner with product teams or to some extent, we now also have ML heavy

555
00:40:42,200 --> 00:40:51,760
teams that end up being full stack because that's also made the faster path to productizing

556
00:40:51,760 --> 00:40:52,760
a feature.

557
00:40:52,760 --> 00:40:58,320
A good example is this conversations team, right, that is responsible for score assistant.

558
00:40:58,320 --> 00:41:04,280
They are extremely sort of ML AI heavy, but now they also have, you know, front and

559
00:41:04,280 --> 00:41:08,240
engineers to help them get the product out faster.

560
00:41:08,240 --> 00:41:16,880
And so when you think about that spectrum of, you know, ML heaviness or readiness and kind

561
00:41:16,880 --> 00:41:23,040
of a role or an organization that's focused on ML, you know, that's ML in the, in the

562
00:41:23,040 --> 00:41:28,440
name, like head of ML, you know, being your title, like is ultimately like, do you think

563
00:41:28,440 --> 00:41:36,400
that your role is to put yourself out of a job by making the product teams, you know,

564
00:41:36,400 --> 00:41:40,800
self-sufficient that, you know, it's, ML is kind of a corporate capability.

565
00:41:40,800 --> 00:41:46,920
And if so, like, you know, what's the timeline on that or do you, do you think that standalone

566
00:41:46,920 --> 00:41:52,200
ML organizations are kind of long-term, you know, have a long-term sustainable role?

567
00:41:52,200 --> 00:41:54,400
And if so, kind of what do you think that is?

568
00:41:54,400 --> 00:41:55,400
Right.

569
00:41:55,400 --> 00:42:02,480
Yeah, I do like a lot the idea of basically increasing the overall skill set of, of engineers and

570
00:42:02,480 --> 00:42:08,520
product managers and designers about ML. And so ultimately, that is my goal.

571
00:42:08,520 --> 00:42:12,080
That doesn't mean though that obviously there's going to be always a core that is sort of

572
00:42:12,080 --> 00:42:16,600
more following the latest developments and doing some research of their own.

573
00:42:16,600 --> 00:42:22,440
But I think that's certainly to make a product feature successful, you could not have too

574
00:42:22,440 --> 00:42:25,600
much of a separation between ML and product, right?

575
00:42:25,600 --> 00:42:27,480
It has to be more embedded.

576
00:42:27,480 --> 00:42:34,960
I mean, it's certainly been amazing to see, you know, how quickly kind of innovation jumps

577
00:42:34,960 --> 00:42:41,600
from pure research academia into commercial environments.

578
00:42:41,600 --> 00:42:47,840
And, you know, your typical kind of, yeah, I guess just comparing it to, you know, other

579
00:42:47,840 --> 00:42:53,680
kind of technology waves that, you know, we've seen like mobile and cloud and these other

580
00:42:53,680 --> 00:42:58,160
things like they didn't necessarily require people to read papers in order to, you know,

581
00:42:58,160 --> 00:43:04,600
have an impact, whereas, you know, with machine learning, you know, I'm, you know, when

582
00:43:04,600 --> 00:43:09,520
I'm talking to people doing kind of practical things to push a business forward, that often

583
00:43:09,520 --> 00:43:15,080
involves, you know, being under being aware of kind of the latest developments and research

584
00:43:15,080 --> 00:43:20,480
and using those to push the kick the ball forward, push the needle forward, whatever

585
00:43:20,480 --> 00:43:24,160
the right analogy is.

586
00:43:24,160 --> 00:43:26,160
Do you see that well?

587
00:43:26,160 --> 00:43:27,160
Yes.

588
00:43:27,160 --> 00:43:32,800
So, to some extent, I mean, obviously there, it's amazing to have people that are super excited

589
00:43:32,800 --> 00:43:39,400
and following the latest research and we do have, you know, paper reading groups that some

590
00:43:39,400 --> 00:43:45,480
of us participating, having said that, I would actually argue that there is also beginning

591
00:43:45,480 --> 00:43:51,080
to be quite a strong sort of a democratization process of ML.

592
00:43:51,080 --> 00:43:57,560
And if you look at what the, you know, the usual suspects offer in terms of ML capabilities,

593
00:43:57,560 --> 00:44:03,080
they started with some basic, oh, we can do model hosting for you, right?

594
00:44:03,080 --> 00:44:08,480
But now it's not just model hosting, but basically there's some like ready-made models that

595
00:44:08,480 --> 00:44:12,880
you don't even need to like train yourself or, or, or, or, or, you don't, you don't even

596
00:44:12,880 --> 00:44:17,520
need to have almost like a training set, you can use sort of off the shelf things for

597
00:44:17,520 --> 00:44:24,560
say recognizing the, the license plates, right, or, or analyzing driver's license.

598
00:44:24,560 --> 00:44:30,200
So to some extent, it's also going to be much easier in the future to use some of these

599
00:44:30,200 --> 00:44:34,400
models and it's almost just going to put these building blocks together, right?

600
00:44:34,400 --> 00:44:40,800
Have teams that square built products on third party, you know, AI as a service types

601
00:44:40,800 --> 00:44:42,560
of models?

602
00:44:42,560 --> 00:44:48,600
So yeah, typically not, typically, because we are actually very protective of our data,

603
00:44:48,600 --> 00:44:49,600
right?

604
00:44:49,600 --> 00:44:55,880
So we typically like do our own models, although, I mean, for certain, you know, explorations

605
00:44:55,880 --> 00:45:01,240
outside of the core business, certainly, we, you know, we, we, we kind of play with have

606
00:45:01,240 --> 00:45:04,280
some prototypes with some external models.

607
00:45:04,280 --> 00:45:10,640
And I'm interested, I have more questions about kind of the, you know, all the stuff

608
00:45:10,640 --> 00:45:15,480
that we talked about the relationship between your group and others and kind of the tooling

609
00:45:15,480 --> 00:45:16,480
and platform.

610
00:45:16,480 --> 00:45:23,760
I mean, maybe a good place to, to go to start to wind down is, you know, just in terms

611
00:45:23,760 --> 00:45:29,600
of the things you've learned, kind of building and, you know, what was, was the team small

612
00:45:29,600 --> 00:45:33,840
when you arrived or did you inherit a team even or did you build it up from scratch?

613
00:45:33,840 --> 00:45:34,840
Yeah.

614
00:45:34,840 --> 00:45:40,080
Well, this specific team, the Commerce and Mail, I, I, yeah, I, that, that one, I sort

615
00:45:40,080 --> 00:45:44,360
of, I started as a, you know, as a, as a single person and then, and then sort of growing,

616
00:45:44,360 --> 00:45:45,360
growing over time.

617
00:45:45,360 --> 00:45:52,060
I think the most interesting lesson is, um, there's certainly a lot of interest in how

618
00:45:52,060 --> 00:45:58,960
ML can improve the, you know, the, the, the day to day of both internal teams and, and,

619
00:45:58,960 --> 00:46:01,680
um, and our sellers and, and customers.

620
00:46:01,680 --> 00:46:07,760
Um, but of course, the, the fun part is kind of identifying which projects are going to

621
00:46:07,760 --> 00:46:12,840
be the most relevant and be able to do some quick prototyping to kind of either validate

622
00:46:12,840 --> 00:46:16,560
or realize that that may not be the, a good path forward.

623
00:46:16,560 --> 00:46:20,800
So being able to kind of do some quick iterations is, is important.

624
00:46:20,800 --> 00:46:29,560
And the other thing was, as in any organization, there's always ways to improve the data quality,

625
00:46:29,560 --> 00:46:30,560
right?

626
00:46:30,560 --> 00:46:35,320
So, so not just the kind of the, the infrastructure and the pipelines, but also, uh, the type

627
00:46:35,320 --> 00:46:43,000
of data that is, uh, that is locked and, and how it is locked so that, um, other teams basically

628
00:46:43,000 --> 00:46:51,240
increase the, the, the clarity of the semantics about, uh, what that data is, um, meaning these

629
00:46:51,240 --> 00:46:57,480
team call this field, uh, you know, item description, um, by what do you mean by item description

630
00:46:57,480 --> 00:47:02,280
because there's, there's the category, there's the variation, does it include the price?

631
00:47:02,280 --> 00:47:07,280
Um, so different people may have different assumptions about what that field means.

632
00:47:07,280 --> 00:47:13,320
Um, and so also being able to have a, kind of a consistent semantics across services and

633
00:47:13,320 --> 00:47:17,880
teams, um, this is, uh, you know, I'm not saying that we've solved it, but this is a, sort

634
00:47:17,880 --> 00:47:22,160
of an ongoing and something that we've made a huge, uh, advances on.

635
00:47:22,160 --> 00:47:23,160
Mm-hmm.

636
00:47:23,160 --> 00:47:31,200
Yeah, you mentioned, uh, kind of being thoughtful about the types of projects that you go after.

637
00:47:31,200 --> 00:47:37,600
One of the, the often recurring themes that, uh, has come up when I've talked to folks

638
00:47:37,600 --> 00:47:44,640
that are kind of in this, uh, head of ML type of role is kind of balancing the practicality

639
00:47:44,640 --> 00:47:52,200
of your portfolio, but also kind of having some moonshot aspects of it that if slash one

640
00:47:52,200 --> 00:47:57,720
achieve significantly can, you know, make a big dent and, you know, can be kind of game

641
00:47:57,720 --> 00:48:02,560
changes. Is that, do you, do you think about it similarly? And you kind of get your portfolio

642
00:48:02,560 --> 00:48:03,560
in that way?

643
00:48:03,560 --> 00:48:04,560
Yeah.

644
00:48:04,560 --> 00:48:05,560
Yeah.

645
00:48:05,560 --> 00:48:11,240
Um, we do want to have a mix of more sort of short-term, uh, quote unquote realistic,

646
00:48:11,240 --> 00:48:15,200
uh, projects, you know, something that we feel is very doable.

647
00:48:15,200 --> 00:48:18,840
It's just going to take, you know, one quarter or two quarters or three quarters.

648
00:48:18,840 --> 00:48:23,840
But then there's also the kind of a longer horizon of, um, more sort of, yeah, one shot

649
00:48:23,840 --> 00:48:30,880
as, as you called it, um, or just explorations of features that would be extremely cool

650
00:48:30,880 --> 00:48:32,560
if we could do these or that.

651
00:48:32,560 --> 00:48:37,280
So what would typically what we do is, um, almost like the, the, the old famous, you know,

652
00:48:37,280 --> 00:48:38,960
the Google 8020.

653
00:48:38,960 --> 00:48:43,800
So we, we do have some, uh, hack weeks and we have some people devoting a sort of amount

654
00:48:43,800 --> 00:48:48,360
of their time to more of these, uh, long-term, uh, projects.

655
00:48:48,360 --> 00:48:55,160
Um, and it's, it's, they also need a bit of a, of a balance, um, um, and so, but so,

656
00:48:55,160 --> 00:49:00,040
yeah, typically the way I organize it is making sure that in any given quarter, we have

657
00:49:00,040 --> 00:49:07,000
some work done for some of these more long-term, um, projects and some of it is just sort

658
00:49:07,000 --> 00:49:14,040
of researching new technologies, um, doing some, uh, sort of just, just kind of mock ups

659
00:49:14,040 --> 00:49:21,160
of how things could be, if only we had this, so if we, we only, only had that technology.

660
00:49:21,160 --> 00:49:31,080
Are the long-term plays as equally product-driven as the, the short-term or are they, you

661
00:49:31,080 --> 00:49:35,560
know, did they come from the product teams like each, each of the product teams has their

662
00:49:35,560 --> 00:49:36,560
long-term vision?

663
00:49:36,560 --> 00:49:42,560
Or are they, you know, more driven by the opportunity created by the technology and, you

664
00:49:42,560 --> 00:49:43,560
know, yeah.

665
00:49:43,560 --> 00:49:45,520
But almost say it's, it's more like the, the latter.

666
00:49:45,520 --> 00:49:50,040
Like so, because the product teams already have a, a well-defined backlog and, and some

667
00:49:50,040 --> 00:49:52,880
of it is already very sort of forward-looking.

668
00:49:52,880 --> 00:49:58,200
And so the thought experiment is more about even in, you know, like a, the, you know, three

669
00:49:58,200 --> 00:50:02,720
to five years, not even, you know, one to two years, but like three to five, what, where

670
00:50:02,720 --> 00:50:09,640
could we be, um, what are the, the general trends in society and in technology?

671
00:50:09,640 --> 00:50:14,320
I mean, frankly, like, you know, the whole COVID situation and working from home, that seems

672
00:50:14,320 --> 00:50:18,160
to, you know, it will, it will stay here for quite a while.

673
00:50:18,160 --> 00:50:24,440
So how does society change because of that and, and how can we sort of position square

674
00:50:24,440 --> 00:50:28,880
to help our sellers in that environment, um, and also in general, what, you know, what

675
00:50:28,880 --> 00:50:35,920
kind of, um, new forms of almost like human behavior are going to arise from these

676
00:50:35,920 --> 00:50:41,760
new circumstances? Hmm. Very cool, very cool. Well, Marcel, thanks so much for taking

677
00:50:41,760 --> 00:50:48,280
the time to chat. If I do decide to start the Sam, you know, oh, yeah, the linguistics.

678
00:50:48,280 --> 00:50:54,040
Yeah. Yeah. Yeah. Yeah. I can tell you some more anecdotes about, about, uh, funny languages

679
00:50:54,040 --> 00:50:59,640
like, uh, Google Imuthir, where you could not say I'm standing in front of Sam. I would

680
00:50:59,640 --> 00:51:05,440
have to say I'm standing northwest of Sam or something. Anyway, of course, this is all

681
00:51:05,440 --> 00:51:10,960
changed by, by remote technologies too. But, uh, yeah, but it's been a pleasure. Um, and

682
00:51:10,960 --> 00:51:16,120
I thank you for having me on your, on your podcast, Sam. All right. Thanks so much, Marcel.

683
00:51:16,120 --> 00:51:24,960
Take care. All right, everyone. That's our show for today. For more information on today's

684
00:51:24,960 --> 00:51:32,280
show, visit twomolai.com slash shows. As always, thanks so much for listening and catch

685
00:51:32,280 --> 00:51:44,280
you next time.

