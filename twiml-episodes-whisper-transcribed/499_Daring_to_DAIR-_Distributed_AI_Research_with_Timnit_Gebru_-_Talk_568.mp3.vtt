WEBVTT

00:00.000 --> 00:05.120
Like I said, your experience teaches you a lot more than what anybody else writes or says.

00:05.760 --> 00:10.960
Is that if you don't have the right institution and the right structure,

00:11.920 --> 00:15.440
there's just no way that you can do things fairly.

00:21.280 --> 00:25.520
All right, everyone. Welcome to another episode of the Twomo AI podcast.

00:25.520 --> 00:31.280
I am, of course, your host, Sam Charrington. And today, I'm joined by a very special guest,

00:31.280 --> 00:38.080
none other than Timnett Gebru, founder and executive director of Dare, the Distributed Artificial

00:38.080 --> 00:43.680
Intelligence Research Institute, and of course, a great friend of the show. Before we dive into

00:43.680 --> 00:47.840
today's conversation, be sure to take a moment to head over to Apple Podcast or your listening

00:47.840 --> 00:53.760
platform of choice. And if you enjoy the show, please leave us a five star rating and review.

00:53.760 --> 00:58.720
Timnett, it is wonderful to have you back on the show. It has been a bit.

00:59.440 --> 01:06.720
I think this is actually your fourth time ish because you did a meetup that you probably don't

01:06.720 --> 01:15.600
remember back in January 17 about your Google Street View work. And then your first time on

01:15.600 --> 01:23.920
the show is in January of 18 episode number 88. We're probably at five 88 or something like that now.

01:23.920 --> 01:32.080
And of course, you helped us cover trends in fairness and AI ethics in January of 20, kind of

01:32.080 --> 01:44.000
looking back on 19. Wow, it's been a long two and a half years. Why don't we get started by

01:44.000 --> 01:49.040
having you share a little bit about what's been going on for you? Welcome back.

01:50.000 --> 01:56.720
Yeah, I can't even, it's really interesting being back, you know, because I remember our first

01:56.720 --> 02:04.400
black night workshop, you all had, you were at like a hotel room, you had a whole set off. It was just

02:04.400 --> 02:11.120
like, it just feels like such a long time ago. Yeah, that was long beach. Yeah, yeah, yeah.

02:11.120 --> 02:18.480
And it's very interesting. It's kind of like chronicling a journey, you know, every time I come back

02:18.480 --> 02:27.200
here. Well, I have to say that, you know, right now, I'm focused on dare, as you mentioned.

02:28.880 --> 02:40.000
And I'm trying to take the time to calm down a little bit. And also think about, you know,

02:40.000 --> 02:45.440
just take, take a step back. So one of the things I wanted to do was think about, you know,

02:45.440 --> 02:50.960
there are all of these issues that we're talking about, right fairness, ethics, labor issues, etc.

02:50.960 --> 02:56.800
And but what does the right model for doing things look like, right? What does the right

02:56.800 --> 03:01.680
institute look like? What do the right incentive structures look like? How should we approach

03:01.680 --> 03:08.000
the way we do research and what we build, what we don't build? And I am just kind of trying to

03:08.000 --> 03:16.640
take the time to figure those out at this right now with there. Is it fair to ask you to give a 30,000

03:16.640 --> 03:24.400
foot 30-second overview of your recent experiences to get some at Google to help folks get some

03:24.400 --> 03:34.800
context if they've not heard any of the previous, where do we start? Well, so well, I got fired from

03:34.800 --> 03:43.520
Google or as some of my former teammates have called it, actually Sami Bianjo. He coined the term

03:43.520 --> 03:49.360
being resonated. He was like, in French, he said in French, you know, you have this word where like

03:49.360 --> 03:59.360
someone resigns you. And so like they call it being resonated. So I was resonated from Google. And

03:59.360 --> 04:07.600
it was a whole, to be honest with you, I still have not processed it because I don't, you know,

04:08.960 --> 04:16.400
it was in the middle of a pandemic, in the middle of, you know, a war that just started in Ethiopia,

04:16.400 --> 04:24.240
the most horrible war I have ever seen that is not really being talked about, that also gets us,

04:24.240 --> 04:30.960
has gotten me to see all of the issues on social media. And in a way that I've never seen before,

04:30.960 --> 04:37.120
you know, people talk about these issues. And it's like you never learn about it as much as when

04:37.120 --> 04:45.840
you experience it. And so in the middle of that whole thing, and I wrote, you know, this paper on

04:45.840 --> 04:51.760
the dangers of large language models. And the way this actually happened, believe it or not,

04:51.760 --> 05:00.640
was not because I wanted to write a paper. But I saw that people at Google were basically saying,

05:00.640 --> 05:05.200
you know, why are we not the leaders in like large language models? You know, this is, we should

05:05.200 --> 05:11.920
be the ones doing these giant models. And you know, you see this race, just people are so fixated

05:11.920 --> 05:18.000
on having larger and larger models. And I was, I was very worried about that because it seemed to

05:18.000 --> 05:24.800
be this rush to a bigger thing without clarity on like why that bigger thing and also what are the

05:24.800 --> 05:33.680
issues. And so I asked Emily Bender and I said, hey, you know, do you have papers on this that you've

05:33.680 --> 05:40.640
written before that I can cite because right now I'm citing your tweets. And if I could cite a

05:40.640 --> 05:45.120
paper that you've written that I can send to people because people are also internally at Google

05:45.120 --> 05:49.280
asking me what are things we should worry about. And so she said, hey, why don't we write something

05:49.280 --> 05:53.760
together? And I'm like, well, I don't know what I'd contribute, you know. And so then I, and we

05:53.760 --> 05:58.400
each pulled in other people, I pulled in Meg and other people from that play team and we wrote this

05:58.400 --> 06:04.320
paper. And honestly, I never thought it would be controversial. It was it, you know, I, I just

06:04.320 --> 06:09.120
thought it was just going to be this paper. And that's it, right? I didn't think they would love,

06:09.120 --> 06:12.880
I didn't think the Google people were going to be like super happy about it. But I didn't think

06:12.880 --> 06:20.240
they were going to just, you know, do what they did, obviously. And so long story short, I found

06:20.240 --> 06:29.200
myself basically disconnected from my corporate account in the middle of my supposed vacation.

06:29.200 --> 06:36.480
And I found out from my direct report that I had apparently sent in my resignation. And that's

06:36.480 --> 06:43.920
sort of a whole, you know, very, very stressful few months because then, you know, there was all

06:43.920 --> 06:48.320
this harassment online. There was all of this, you know, you have to make sure you're safe.

06:49.360 --> 06:58.240
There are literally like people from the dark web who made it a point, like a point to to

06:58.240 --> 07:03.840
harass me come to all the talks I'm giving. And you know, just kind of harass anybody who is

07:03.840 --> 07:08.400
coming to my defense, you know, a lot of other people found themselves writing documents,

07:08.400 --> 07:12.240
having to talk to lawyers and things like that. People who don't even know me, by the way,

07:12.240 --> 07:17.840
just because people just, you know, were coming to my defense on Twitter or something like that,

07:17.840 --> 07:25.040
just because of that, I found myself being a thirst thrust into the public space. And so then that

07:25.040 --> 07:33.680
also just that fact itself brings in more attention for more people. And then I was like really

07:33.680 --> 07:38.800
worried about my team and what was going to happen to them. But then, you know, my co-lead

07:38.800 --> 07:46.400
Mitchell was also fired. So it was a whole few months. It was a whole thing. And, you know,

07:46.400 --> 07:51.680
that's what I mean, but I didn't have a chance to really process what has happened. In the midst of

07:51.680 --> 07:56.960
that, of course, I was thinking, what is the next, what is the thing I could do next? Because I really,

07:57.600 --> 08:02.400
you know, couldn't get myself to think about being at another large tech company and do that

08:02.400 --> 08:10.240
fight again. I also know that I would not, there would be some companies that would be unwilling

08:10.240 --> 08:15.680
to hire somebody like me after all of that. There's, you know, some members of my former team,

08:15.680 --> 08:22.960
their office were rescinded from some places like after this publicity. And it's real, you know,

08:22.960 --> 08:29.840
it really is real that people can, you know, by speaking up, just destroy their entire careers

08:29.840 --> 08:36.960
and any options. But, you know, I had been thinking about creating a distributed independent

08:36.960 --> 08:41.920
research institute. I didn't even think about like creating a university. Why can't we have a

08:41.920 --> 08:46.560
distributed kind of different kind of, you know, I've been thinking about these things. But

08:47.200 --> 08:53.440
if I hadn't been fired, probably what I would have done is slowly start something, you know,

08:53.440 --> 08:58.640
maybe start something from on the side and grow it there, be very slowly, not like the way,

08:58.640 --> 09:04.640
you know, we just started this. So anyhow, and after that, I decided to start there, the

09:04.640 --> 09:10.720
distributed air research institute. That's awesome. And so what's the, how do you think about the

09:10.720 --> 09:16.720
charter for dare? What's kind of in the, in the zone, in the scope versus out of scope?

09:17.520 --> 09:23.760
Yeah. So, you know, dare is the air research institute, like, you know, like any other research

09:23.760 --> 09:29.520
institute that you can think of. The thing that we are is we're an interdisciplinary research

09:29.520 --> 09:35.520
institute. So, you know, Alexandra recently joined as our director of research. She's a sociologist.

09:36.720 --> 09:43.360
And the distributed aspect was very important for me because I saw it even at Google in the

09:43.360 --> 09:51.120
ethical AI team, you know, Meg was very good at retaining a distributed team. And, you know,

09:51.120 --> 09:58.560
one of the last people we hired was Mahdi, who's a Moroccan. And he was raising the alarm

09:58.560 --> 10:04.080
on social media, like no other person. And he was doing all this research, his friends were in

10:04.080 --> 10:11.120
jail, their journalists. And I could see that nobody, you know, even the people in ethics or

10:11.120 --> 10:18.240
whatever could not really grasp the gravity of the situation. And if you didn't have that person,

10:18.240 --> 10:22.320
with that experience, there was no way you would, you would, you know, find out about that issue

10:22.320 --> 10:28.080
and look into it, right? And that showed me the importance of, of having people, you know,

10:28.080 --> 10:33.680
like that and not forcing them to move to Silicon Valley or whatever. I don't want to, you know,

10:33.680 --> 10:39.200
I'm, what I'm thinking about is how not to consolidate power, right? Not how to, far,

10:39.200 --> 10:46.400
there kind of contribute to the brain drain of different other locations. So, so that's why the

10:46.400 --> 10:51.440
first word that came to my mind was distributed. And I called, you know, I told Eric Sears,

10:51.440 --> 10:56.640
who's a program officer, a director at MacArthur, the MacArthur Foundation, I was like, hey,

10:56.640 --> 11:00.240
look, you know, the first word that came to my mind is distributed. I want to call it dare,

11:00.240 --> 11:07.280
like does it sound weird, you know? It's like, no, it's, it's cool. And so, so that's, that's dare.

11:07.280 --> 11:11.840
And so when you say what's in scope versus out of scope, you know, that's honestly something

11:11.840 --> 11:17.440
that we're still trying to figure out because it, I'd like it to be kind of a combination of,

11:17.440 --> 11:23.760
of course, we have a few top-down directions, but I really feel strongly that it's, it's very

11:23.760 --> 11:29.600
important to have a bottoms-up approach to research because you can't be the all-knowing person

11:29.600 --> 11:34.960
who knows like what the next important thing is, right? So it's important to let other people

11:34.960 --> 11:43.120
drive that too. But the thing we're focused on right now is, you know, what is our research

11:43.120 --> 11:49.760
philosophy? And what, what do we care about, right? And so first of all, we care very much about

11:49.760 --> 11:56.320
not exploiting people in the research process. One of the most, one of the things that is super

11:56.320 --> 12:01.680
clear in research in general, and especially when you look at this field where you, you know,

12:01.680 --> 12:06.560
there's a lot of knowledge that's extracted from people, a lot of data in different forms

12:07.120 --> 12:14.560
that's extracted from people without compensation, without, you know, acknowledgement, etc, right?

12:14.560 --> 12:20.000
Like you have that also in the fairness space. For instance, you have a group of researchers,

12:20.000 --> 12:25.040
you know, they get tenure and they're ascending based on work on fairness or something.

12:25.040 --> 12:29.760
And who are the subjects that they talk about? Oh, they'll talk about formerly incarcerated people

12:29.760 --> 12:34.000
or people in prison currently. They'll talk about like different groups of people who are

12:34.000 --> 12:40.480
harmed by this technology who are not, you know, getting the money, you know, for the research or

12:40.480 --> 12:46.800
the fame or, or, you know, many times their lives are not changing because of this work, but they're

12:46.800 --> 12:52.400
subjects of it, right? And so we're trying to figure out how do we not do that? You know, how do we

12:52.400 --> 12:58.160
do the opposite of that? What does it mean to have research that that that incorporates

12:58.160 --> 13:04.400
these people and actually is led by many times people like that? And how do you funnel resources?

13:05.440 --> 13:12.480
And so one of our research fellows who just joined Meela is actually one of the things she's

13:12.480 --> 13:15.920
doing is helping us figure that out, right? What is our research philosophy and how do we

13:15.920 --> 13:23.840
operationalize it? So in terms of, you know, what's in scope and out of scope, so there's a self-selection

13:23.840 --> 13:30.960
going on there where the people, you know, who do want to do research at their are people who

13:30.960 --> 13:35.760
care about these kinds of things are somehow embedded in community building, not just, you know,

13:37.520 --> 13:45.440
like research that has nothing to do with that. And, you know, like, for instance, if you want to work

13:45.440 --> 13:51.600
on, you know, low, you know, so I'm, I'm, I'm advising on a workshop, which I had

13:51.600 --> 13:57.200
coordinated before on practical machine learning and, you know, for developing countries or

13:57.200 --> 14:02.000
practical machine learning in low resource scenarios. So if you want to, you know, kind of think

14:02.000 --> 14:08.880
about what about, like, small data and small compute, right? Like that, I think you might want to

14:08.880 --> 14:12.640
join, you know, we might want to think about working out there. But if you're interested in, like,

14:12.640 --> 14:17.520
even larger models and even larger or something, then I don't understand what we would, you know,

14:17.520 --> 14:22.560
provide in that sense. So that's kind of how I'm thinking about it right now. What I'm hearing in

14:22.560 --> 14:30.720
part is that the, the areas that you've traditionally been working in and a researcher, ethics,

14:30.720 --> 14:38.880
fairness, and that you're probably best known for, that is not necessarily a research focus for

14:38.880 --> 14:47.840
Dare, but more like a undercurrent or a foundation. And Dare is going to be broader and encompass,

14:47.840 --> 14:53.600
you know, like you said, all the things that another research institute might, like Amila might

14:53.600 --> 14:58.720
be interested in depending on, you know, who it is that comes and starts out research programs

14:58.720 --> 15:04.800
there. Exactly. So like, a lot, some people describe Dare as like an AI ethics research institute,

15:04.800 --> 15:10.880
right? And I'm like, no, it's like, yeah, that's not what we're, we're hoping to do. And by,

15:10.880 --> 15:17.760
by virtue of who we are, we will, so there's two ends of the spectrum that we were looking at,

15:17.760 --> 15:22.560
right? And I think our advisory committee members, when you look at Safiya Noble and Shira,

15:23.440 --> 15:30.960
Meina, they encompass those two ends of the spectrum. So the first end is how do you develop,

15:30.960 --> 15:35.840
how do you do this research in a way that we think is beneficial to the groups of people that we

15:35.840 --> 15:40.320
care about? And actually, when you say what's in scope and out of scope, our focus is, you know,

15:40.320 --> 15:46.160
we're starting with thinking about, you know, people in, in Africa and the African diaspora, right?

15:46.160 --> 15:52.160
Like so, you know, you know, you know, there's no kind of question. Like I don't have, I don't know

15:52.160 --> 15:57.760
if I have to explain why, but like, you know, black people in general around the world who are

15:57.760 --> 16:03.680
very much harmed by this technology and not necessarily benefiting from it. So when you look at

16:03.680 --> 16:10.560
Shira, he's, in the area he's in, he's in Kenya, and a lot of his work is on how to, you know,

16:10.560 --> 16:16.800
work on climate change and data science, right? He analyzes bird migration patterns to, to,

16:16.800 --> 16:21.760
that tells you something about the climate and how it's changing. He, he was at the first black

16:21.760 --> 16:28.560
name workshop. He probably covered his work, food security and conservation. He works on stuff

16:28.560 --> 16:33.280
like the he co-founded data science Africa, right? So it's kind of like, you know, how to work on

16:33.280 --> 16:38.160
data, quote unquote, data science or related fields in a way that is beneficial to start, you know,

16:38.160 --> 16:44.880
to the groups of people that he cares about. On the other end, you have Sophia who's in, you know,

16:44.880 --> 16:51.920
in, in the US, and she is more on the other end of the spectrum, how to, you know, raise the alarm

16:51.920 --> 16:59.840
when we know there are issues that with technology that's already been built, right? So we, and,

16:59.840 --> 17:06.640
you know, she's more from the social sciences side, right? So like, for me, that encompasses sort

17:06.640 --> 17:11.680
of what I want to build with dare, right? Interdisciplinary have different groups of people

17:11.680 --> 17:18.400
to, to be able to work on research that, you know, we think is beneficial to our communities.

17:19.280 --> 17:24.320
And, you know, way that's not exploiting the people who are actually, you know, who might not

17:24.320 --> 17:29.440
have PhDs or whatever, but have a lot of knowledge about the, the systems and how they're impacting

17:29.440 --> 17:35.040
them. So I like what you said. Yeah, it is an undercurrent, right? Of like, how do we do this work?

17:35.040 --> 17:39.040
Is, is, is that's how we're building this foundation? I mean, this institute.

17:39.040 --> 17:46.560
One of the things that we chatted about before we started recording was that a lot of your focus

17:46.560 --> 17:51.200
right now is on institution building. For obvious reasons, you're building an institution. Like,

17:52.240 --> 18:00.800
I'm curious what that means for you. And also, well, afterwards, I want to relate that back to

18:00.800 --> 18:08.960
your experience at Google and, and the, the idea around, you know, how to, how to ethics organizations

18:08.960 --> 18:12.960
inside large companies? Like, how do we build those so that they have teeth, so to speak, so that

18:12.960 --> 18:19.120
they can be effective? Yeah, that's a very good question. And so I've been going on this fairness

18:19.120 --> 18:24.560
rabbit hole, as you know, and, you know, I've been like, I've worked on things related to math

18:24.560 --> 18:31.520
and or documentation or auditing, community building, like Black Day Eye, Power Building, you know,

18:31.520 --> 18:38.400
met all the different kind of ways in which I think you can attack the problem. And I have kind

18:38.400 --> 18:43.600
of just kind of come to the conclusion, like many. And of course, this is not something new that

18:43.600 --> 18:49.120
I'm saying. It's just like I said, your experience teaches you a lot more than what anybody else writes

18:49.120 --> 18:57.920
or says is that if you don't have the right institution and the right structure, there's just no way

18:57.920 --> 19:04.080
that you can do things, quote unquote, fairly, right? So, um, so that's why I'm, I'm kind of working

19:04.080 --> 19:09.200
on institution building, right? I've, I've had experiences in academia. I've had experiences in

19:09.200 --> 19:13.120
industry. And when I, after I got fired from Google, I was thinking, you know, a lot of people

19:13.120 --> 19:18.160
were saying, well, you obviously won't have academic freedom in industry. If you want that, you should

19:18.160 --> 19:24.320
go to academia. And I'll say, that's not true, right? To me, it's a pyramid scheme up here at the

19:24.320 --> 19:30.160
top of the, you know, somebody just tweeted the other day that graduate students make $36,000 a year,

19:30.160 --> 19:36.080
perhaps, right? And, you know, it's like they're in this weird position. Are they students? Are they,

19:36.080 --> 19:41.680
are they workers? Like do they get vacation or not? But they're in this situation for years, right?

19:41.680 --> 19:48.720
Very similar to college athletes. Oh, apps 100%, which also should get paid exactly. So that's

19:48.720 --> 19:55.120
where we are, right? And so, um, yes, and it makes absolutely no sense. It's, I think it's very,

19:55.120 --> 20:00.800
very exploitative. And so imagine you're doing that work as a graduate student at your advisor

20:00.800 --> 20:05.680
controls your life. And then you're going to tell them, you know, whatever research they're doing

20:05.680 --> 20:10.880
is not fair. You should have a different sort of direction. You, you're, you should stop. How are

20:10.880 --> 20:16.000
you going to do that? You, you will lose your, your money. You will lose your, um, career,

20:16.000 --> 20:20.080
like your future prospects, because they won't write you a recommendation. If people are on

20:20.080 --> 20:26.560
visas, you will lose your visa. So, so, so, um, how are we telling people to do the right thing

20:26.560 --> 20:31.520
when we know we're not setting them up, right? With the incentive structure to do the right thing.

20:31.520 --> 20:38.080
And it's the same thing at work too, right? Like, um, again, what did I, I spoke up? I got fired.

20:38.080 --> 20:44.400
So, um, then why, why would anybody do something differently then, right? Like, and so, so, that's

20:44.400 --> 20:50.320
why I really believe we have to think about, um, the, um, incentive structures. And it's not just

20:50.320 --> 20:56.800
about, for instance, labor practices that we're talking about, right? It's about what kind of work

20:56.800 --> 21:02.800
is valued and what kind of work is not valued. Um, you know, so I, I think you have Mariel Gray.

21:02.800 --> 21:08.160
Well, so her and Sudarsasri have this book called Ghost Work, um, how Silicon Valley is creating a

21:08.160 --> 21:13.760
global underclass, and they're talking about data labor, right? So all of this automation that we

21:13.760 --> 21:18.960
talk about is sort of pseudo, it's not, you know, real automation is that there's a lot of people

21:18.960 --> 21:24.160
behind it labeling data, you know, doing all sorts of things, but they're being exploited. They're

21:24.160 --> 21:31.280
not being paid, right? Um, and so in, in, in, in graduate school, if you're telling your PhD

21:31.280 --> 21:36.720
student that they should spend all of this time working on data related work, data labor, that's

21:36.720 --> 21:41.840
the very, the most important thing you should think about how you're gathering and annotating data,

21:41.840 --> 21:47.760
take the time to do this right. But then they can't publish their work or they, it's not valued

21:47.760 --> 21:52.720
or they can't get a job after they graduate. Again, that's an incentive structure and institution

21:52.720 --> 21:58.800
building issue, right? So now, there's some people working on journals, for instance, to be able to,

21:58.800 --> 22:05.680
for people to be able to publish on data. And there was this new rips, this new, new rips,

22:06.320 --> 22:12.160
data sets and benchmarks to act where we actually published a paper too for dare. So that's what I

22:12.160 --> 22:18.480
mean, like, this is exactly why I'm thinking about the, the, the incentive structures, right? Because

22:18.480 --> 22:22.960
there's no way you could, you know, do quote unquote the right thing if you're in the wrong

22:22.960 --> 22:30.000
incentive structure. Yeah, yeah, we, I did an interview with Safe Savage, who researches

22:30.000 --> 22:37.520
that area as well. That was a future of work for the invisible workers in AI. Exactly.

22:37.520 --> 22:43.840
I can episode 447. You know, if you kind of, you know, chart your path as

22:45.360 --> 22:49.600
experimenting with different institutional structures to try to see what works,

22:49.600 --> 23:00.320
um, is your decision to start dare, uh, you know, can we infer from that that support organizations

23:00.320 --> 23:05.040
aren't enough, internal organizations aren't enough. There needs to be just an independent,

23:05.520 --> 23:11.840
uh, alternative to kind of traditional research structures. What I'm exactly, what I'm thinking is,

23:11.840 --> 23:16.640
so let's say if I went to academia, I'm, I told you, I'm trying to spend the time to think about

23:16.640 --> 23:21.440
the meta questions. How do we build something, et cetera? How am I going to survive? Like,

23:21.440 --> 23:26.080
I have to publish tomorrow. My students have to, you know, I'm, oh, no 10 year old, sorry,

23:26.080 --> 23:33.600
what are you working on? Like, it's not even an option, right? So, so my, so my hope is that, yes,

23:33.600 --> 23:39.600
um, we start these smaller independent institutes where we can actually say stuff. Um, Alex was

23:39.600 --> 23:45.360
telling me about a talk that she gave the other day. And that might, might not have made a number

23:45.360 --> 23:49.120
of people happy, but she's like, well, that's fine because I'm not looking to get tenure. And I'm like,

23:49.120 --> 23:53.840
yeah, that's the kind of stuff you can do when you're not looking to get tenure. So I think, you

23:53.840 --> 23:59.440
know, it gives us the opportunity to actually advocate for things that we think are important. And

23:59.440 --> 24:06.960
maybe, um, slowly, those other larger institutions might change or, you know, have pressure to,

24:06.960 --> 24:12.000
to do things differently. If they know that there are different options, like, like our institute,

24:12.000 --> 24:18.160
and if other people create other institutes. And honestly, um, if you look at, you know,

24:18.160 --> 24:24.880
even how I started Black Neye, before Black Neye, I had been involved in a lot of other organizations,

24:24.880 --> 24:30.640
like for diversity or for this or for that. And I was like, you know, there's no way I can convince

24:30.640 --> 24:36.800
you all to do the things that I think we need to do for Black people, you know, I just, I fought,

24:36.800 --> 24:41.360
I tried at this and that. And I was like, let's start something new and do it the way we think it

24:41.360 --> 24:46.880
should be done. And this is kind of similar to that, right? I tried this. I tried that. I tried

24:46.880 --> 24:54.000
inside the organizations. I tried appealing to, you know, higher ups. I feel whatever. But, you know,

24:54.000 --> 25:01.360
I found that that's not, you know, it's not working. And so what I want is to have an alternative.

25:01.360 --> 25:06.960
And even when you look at Black Neye, right? What has, you know, there's now, um, Latinx,

25:06.960 --> 25:12.080
and AI, queer, and AI, indigenous, and AI, disabilities, and AI, you also have a lot of Black

25:12.080 --> 25:17.360
in X, I'm Black in robotics, Black in neuro, Black in physics, Black, I don't even know like there's

25:17.360 --> 25:24.800
so many of them, right? And we can then, you know, build, there's like a network. Now you have,

25:24.800 --> 25:30.400
you build power and you can advocate for things collectively. Um, and so hopefully that's what I'm

25:30.400 --> 25:37.840
hoping with Dare, right? It's kind of an alternative to what we have right now. Um, and hopefully,

25:37.840 --> 25:43.520
you know, other people can kind of replicate it in a way that not exactly replicated, but,

25:43.520 --> 25:50.080
you know, in a way that works for their context. Um, and, you know, so with Dare, I can do things like,

25:50.080 --> 25:54.960
you know, think about funding. Right? Where is our funding coming from? You know, honestly,

25:54.960 --> 26:00.800
sometimes it feels like pick your poison, like there's no really clean money, like, you know, I'm learning

26:00.800 --> 26:06.720
all those things, but, you know, you, I'm thinking about, right, like, again, like I said,

26:06.720 --> 26:11.920
the meta questions, right? If we're thinking about AI and where the money comes from, you think

26:11.920 --> 26:18.000
about technology in general, when the government really invests in technology, right? Um, it's doing

26:18.000 --> 26:22.800
warfare, or when they're interested in something to do with warfare, like so the transistors and

26:22.800 --> 26:28.160
silicon valley, right? Um, in World War II, you think about machine translation, why people

26:28.160 --> 26:33.040
were interested in their research, has to do with Russia, Cold War, you think about DARPA

26:33.040 --> 26:38.160
and self-driving cars. It wasn't because they were like, oh, we need, you know, to make cars more

26:38.160 --> 26:43.680
accessible. You know, we need, we need to make sure that blind people can very freely move around.

26:43.680 --> 26:51.920
So let's build that's not what they said. They said we, we care about, you know, autonomous warfare,

26:51.920 --> 26:58.480
right? And so how do we expect to come to a different conclusion when from the very beginning,

26:58.480 --> 27:02.960
our funding, our incentive structures, every, the paradigm that we're using has something to do

27:02.960 --> 27:08.480
with warfare. And it's the same with industry too, like if, if all you're thinking about is how to

27:08.480 --> 27:14.880
make money for this large, huge humongous company that, that, you know, affects the entire world,

27:14.880 --> 27:19.600
controls the entire world, how do we have the space to think about a different paradigm? Right?

27:19.600 --> 27:24.960
So like we're hoping to think about a different paradigm. I'm sure like, you know,

27:24.960 --> 27:29.600
not that these paradigms don't exist. Other people are doing it too. But like, you know, kind of

27:30.240 --> 27:34.240
take the time for ourselves to think about what paradigm should we follow starting from the funding

27:35.200 --> 27:39.360
to how we do research to, you know, who we are hoping to serve.

27:40.320 --> 27:45.920
You know, we, we both have a lot of kind of colleagues in the industry that are working within

27:45.920 --> 27:53.280
larger organizations trying to help them use AI responsibly. You know, what does it say about

27:53.280 --> 27:59.280
that work? Is it, you know, futile? Is it for not? Is it, you know, a pessimistic view? Or is it,

28:00.560 --> 28:06.720
you know, do you, do you have examples of that process working correctly that you refer to?

28:07.440 --> 28:12.080
And, you know, you're just offering an alternative or do you think that that is,

28:12.080 --> 28:22.240
um, you know, yeah, you know, there's, um, there's this paper called, um, what is it? The Grey Hoodie

28:22.240 --> 28:26.960
project from the university project? Yeah, from people at the University of Toronto. And, um,

28:27.760 --> 28:34.720
and they talk about, they say how big text tactics are close to big tobacco. So they talk

28:34.720 --> 28:40.400
about how, um, they give examples of how like, you know, the tobacco industries would give lots of

28:40.400 --> 28:46.160
money to certain academics who talk at who write about how, well, you know, it's not, it's unclear

28:46.160 --> 28:52.160
if smoking causes cancer or something like that. Or they would then internally retaliate against

28:52.160 --> 28:57.760
people who actually have those kinds of funding, I mean, of conclusions, right? Or fossil

28:57.760 --> 29:02.320
fuel industry who's scientists knew about climate change way back, but they were suppressing it.

29:02.880 --> 29:09.200
And so why, you know, why wouldn't big tech be like that? I mean, what is their incentive not

29:09.200 --> 29:14.880
to be like that? So I, I have seen it myself, how they capture, how they, you know, people

29:14.880 --> 29:21.440
talk about industry capture, how they use, um, research in order to like, fight back against

29:21.440 --> 29:27.520
regulation. So I do believe, honestly, that the number one, um, reason that these large tech

29:27.520 --> 29:34.880
companies want to have these clinical ethics teams is to, um, in order to like, fight back against

29:34.880 --> 29:41.280
regulation. So after I got fired, you know, uh, members of Congress and representatives sent a

29:41.280 --> 29:45.440
letter to Google, and there was a number of letters they sent. First of all, they sent letters

29:45.440 --> 29:51.760
about, you know, um, you know, the, the number of black people they have in these AI divisions,

29:51.760 --> 29:58.640
do they have special like, uh, training, uh, in AI, except, you know, racial equity or in

29:58.640 --> 30:03.280
the training or impacts that or anything. And they write back and they say, oh, we have, you know,

30:03.280 --> 30:08.320
these, um, ERGs or whatever, you know, employee resource groups and we have lots of black people,

30:08.320 --> 30:13.600
we have this event, that event. And similarly, um, they are, uh, wrote a letter to them about

30:13.600 --> 30:17.840
Lara's English models and their impacts, et cetera. And they're like, we have had hundreds of,

30:17.840 --> 30:23.520
uh, papers in ethics and fairness. You know what I mean? But I know for a fact, they are actually

30:23.520 --> 30:30.080
suppressing more papers about the dangers of Lara's language models. You would think that they've

30:30.080 --> 30:35.680
learned from, uh, their lessons. So when they're doing this, they, they are freely allowed to suppress

30:35.680 --> 30:41.520
and persecute people with certain kinds of works, but not others, you have to ask why. And that's,

30:41.520 --> 30:46.960
that becomes more of a propaganda than research, right? So I do think that this is their goal, but

30:46.960 --> 30:53.760
so the people inside can know that and try to fight that, right? And, um, I think the way they can

30:53.760 --> 30:58.960
fight that is through collective organizing. Like, people before them have done, right? Um,

30:58.960 --> 31:04.160
Polaroid or a workers organized against Polaroid's, um, partnership with apartheid South Africa,

31:04.160 --> 31:09.680
right? And that was, that had a, a big impact. So it's not that I don't think that people in the

31:09.680 --> 31:16.880
inside cannot, um, cannot change things, but it's that they have to be vigilant to understand

31:16.880 --> 31:22.880
why they want them there and how their work is being used, right? If your Meg Mitchell used to

31:22.880 --> 31:27.440
call it fig lift, right? She's like, oh, fig leaf. She's like, I don't want to do a fig leaf work,

31:27.440 --> 31:32.880
you know, because like they already do everything else. And you do, you do the fig leaf work,

31:32.880 --> 31:36.560
right? Like you're like, just stamping what they say, oh, we're going to write your ethical

31:36.560 --> 31:40.960
consideration section or we're just not changing the course, the direction that you're doing.

31:41.840 --> 31:46.960
But we'll, we'll sort of do a fig leaf thing. That can do much more harm than good.

31:46.960 --> 31:54.560
Do you, are there examples in the industry that you look to as you're creating there?

31:54.560 --> 32:01.760
Um, well, I mean, there's examples of what I don't want it to be like and what that was a huge

32:01.760 --> 32:08.160
motivation for, you know, like the open AI type stuff is not what I want. Like when open AI was

32:08.160 --> 32:14.320
announced, and I've been so clear about this, I've never had kind of, um, I remember when open AI

32:14.320 --> 32:19.600
was announced, I was at New York's. And I think it was in 2015. And that was before the name change.

32:19.600 --> 32:29.600
And I had just gotten harassed at some, you know, bro, like, at a Google party. I was harassed.

32:29.600 --> 32:34.080
I was having a horrible time. I just like, I don't ever want to come back to this conference.

32:34.080 --> 32:42.400
And, you know, and it was after the whole Google gorillas incident. And they announced this company

32:42.400 --> 32:49.680
that said this, that's supposed to save humanity from AI, like the whole world. It's all about

32:49.680 --> 32:56.720
Elon Musk and Peter teal and all these people. They had 10 deep learning people 100% no interdiscipline

32:56.720 --> 33:04.720
or whatever. Eight of them white men. Uh, one white woman, one Asian woman. Um, and I know for,

33:04.720 --> 33:11.280
I like, I knew for a fact, this was not going to save humanity. And fast forward, what are we talking

33:11.280 --> 33:16.080
about? We're talking about GPT three. We're talking about the dangers of large language models.

33:16.080 --> 33:19.920
We're talking about how we're worried about things. We're talking about how, you know, there's

33:19.920 --> 33:25.840
unsafe products out there, etc. Right. So of course, like, you start at the insect. This is exactly

33:25.840 --> 33:30.160
what I'm thinking about institution building. Right. So you start at the inception. Who was at the

33:30.160 --> 33:35.840
table? Where did the funding come from? Where were, you know, and so unless you think about that,

33:35.840 --> 33:42.720
you can't not arrive at the kind of the state that we're in today. There are a lot of those

33:42.720 --> 33:48.880
kinds of models. I am finding out about institutes left and right right now. One working on AGI,

33:48.880 --> 33:53.840
one working on some other thing that has like 50 million dollar endowment or whatever.

33:54.640 --> 33:58.880
I get irritated. I'm like, where is that 50 million dollars coming from? I wanted to

33:58.880 --> 34:04.560
dominate. So I don't have to think, but you know, but then I would have to compromise on of course,

34:04.560 --> 34:09.600
probably where we get the money or something like that. So that is, you know, on the one hand,

34:09.600 --> 34:15.520
I have models of what I don't want, but I do have models of the kinds of grassroots organizing

34:15.520 --> 34:20.800
that I've seen that I'm really excited about. Right. So for instance, I gave you an example of

34:20.800 --> 34:27.440
Masakani, which is a network. And it was really beautiful to see, right, because it grew up

34:27.440 --> 34:33.920
of a grew out of the deep learning in Daba, which is a convene, right. And so then they create a

34:33.920 --> 34:39.680
people there who met there created Masakani network. And it's really cool. It's a whole bunch of

34:39.680 --> 34:45.840
people focused on working on natural language processing tools for African languages. And

34:46.400 --> 34:51.600
their values are very much kind of in line with the kinds of values I'm thinking about for

34:51.600 --> 34:57.360
dare. And they grew it super slowly, you know. And I think now they have a foundation. I don't think

34:57.360 --> 35:05.280
they have any full-time people. Before the show, I was talking to you about this article, this

35:05.280 --> 35:12.320
wired article I had read about the Maori who created speech-related technology to benefit

35:12.320 --> 35:21.360
their community. So they had this competition using their local radio for people to send in

35:21.360 --> 35:27.120
kind of annotated speech for speech-to-text and other kinds of speech-related, you know,

35:27.120 --> 35:33.520
language-related technology. And they had like hundreds of hours of data, right. And then all of

35:33.520 --> 35:38.160
a sudden, this company, I think, was called Lion Bridge or something in the American company,

35:38.160 --> 35:44.400
wanted to license their data. And they, you know, said no. And they published their reasoning. And

35:44.400 --> 35:49.680
they said, you know, we think this is the last frontier for colonization. They beat the language

35:49.680 --> 35:53.520
out of our grandparents. Literally, we're not allowed to speak this language. And they were

35:53.520 --> 35:58.960
beat up for speaking it. And why is this company interested in like, you know, buying stuff for

35:58.960 --> 36:03.120
now? It's not, it's obviously not because they want to benefit this community. So they're like,

36:03.120 --> 36:07.440
we want to make sure that whatever we do with this data and how, you know, it's something that

36:07.440 --> 36:11.760
benefits us. So those are the kinds of models I'm looking at. I'm like, oh, that's awesome. Like,

36:11.760 --> 36:16.560
I like that, you know, how they're doing data stewardship, you know. And, you know,

36:16.560 --> 36:22.240
unless it can't, I like their approach for grassroots organizing. Mijante, right? It's another

36:22.240 --> 36:27.600
grassroots organizing. They're doing such great work. I just read their report on, for instance,

36:27.600 --> 36:34.560
border technology. And they're talking, they're educating people about what are the different

36:34.560 --> 36:40.240
companies involved in this like digital border, digital border walls? How should we organize?

36:40.240 --> 36:46.720
They drove Palantir out of Palo Alto, right? And they had this no tech for ice campaigns. So, so I'm

36:46.720 --> 36:53.040
looking at that them too. And how they're, they've been able to be so successful with their grassroots

36:53.040 --> 36:59.360
organizing. So I'm looking at different kind of, you know, different kind of models to see what,

36:59.360 --> 37:03.680
what it is that I like about each of these models and what makes sense for dare.

37:03.680 --> 37:12.160
Yeah, yeah. One of the things that you mentioned, as we were chatting before, getting started was

37:12.160 --> 37:18.160
this realization that you had that you can't reduce fairness to a mathematical problem. Like,

37:19.120 --> 37:25.360
I think you saw that experience that. I see a ton of that elaborate on that a bit and kind of

37:25.360 --> 37:32.160
your journey to realizing that and where you see it. How it occurs for you out in the industry?

37:32.160 --> 37:39.520
You know, I mean, like my sisters have been saying, you know, doctors of Vienna was been saying this

37:39.520 --> 37:45.360
forever. Dr. Ruha Benjamin has been saying this forever is Simone Brown. And, you know, like

37:46.160 --> 37:52.000
the people not trained as engineers and computer scientists have been saying this for a long,

37:52.000 --> 37:59.360
long time. And unfortunately, I was reading Philip Agres. It was the most depressing thing. In the

37:59.360 --> 38:08.560
90s, he wrote, so there was even actually a Washington Post article about him. He was in AI and then

38:08.560 --> 38:16.960
kind of became much more of a critic of it. He became a professor and he was like talking about a

38:16.960 --> 38:21.360
number of issues that, for instance, like people are going to share their data much more

38:21.360 --> 38:28.640
freely with, you know, for various applications, right? This is way before social media and stuff.

38:28.640 --> 38:32.400
And he was like, it's not going to be so much of a big brother kind of thing, but people are

38:32.400 --> 38:37.280
just going to like share it without knowing, without thinking carefully, talked about face recognition.

38:37.840 --> 38:42.240
And so I was reading this, like, lessons from trying to reform AI or something like that. I'm like,

38:42.240 --> 38:47.600
you got to be kidding me. What year was this in the 90s?

38:47.600 --> 38:52.880
I don't remember when. I mean, but it was like lessons from trying to reform AI talks about how

38:52.880 --> 38:57.680
the field is not reflective, how it's arrogant. You can't get people to think about disciplinary

38:57.680 --> 39:03.680
norms and whatever. And I'm like, oh my god, like it's true, right? And that's what it is. It's

39:03.680 --> 39:10.640
that when the field feels like it's better than other fields has a lot of power and money

39:10.640 --> 39:15.280
thrown at it. At this point, you have money from the government, money from industry, money from

39:15.280 --> 39:20.400
everywhere. You don't have to think about what anybody else says, even though these people have

39:20.400 --> 39:29.760
been saying this stuff forever. And so what my own experience showed me, of course, we can talk

39:29.760 --> 39:38.640
all we want about, we want to reduce fairness to mathematical equations, because first,

39:38.640 --> 39:44.640
there's again, it takes me back to that incentive structure. When you think about how you ascend

39:44.640 --> 39:51.680
in the academic world in computer science or in engineering in general, there is this hierarchy

39:51.680 --> 39:55.680
of knowledge. I gave a whole talk about the hierarchy of knowledge, right? Certain kinds of knowledge

39:55.680 --> 40:01.440
and contributions are valued. So if you spend five years working on data-related stuff, first of all,

40:01.440 --> 40:06.400
in these conferences, if you have a data track, it's already inferior. Oh, it's just a data set paper

40:06.400 --> 40:12.960
or whatever. Where's the engineer? That's how they talk. And actually, Kiri Wack staff gave

40:12.960 --> 40:19.440
a keynote at ICML 2012 called machine learning that matters, which was basically about this kind

40:19.440 --> 40:26.960
of stuff and how what conferences are valuing versus not. So that, to me, makes it such that you

40:26.960 --> 40:32.560
want to reduce everything to the algorithm, to the math, to whatever. You want to not look at it

40:32.560 --> 40:39.840
as a sociotechnical problem as many people in SDS have said. And when you do that, like the paper,

40:39.840 --> 40:46.000
there's a paper called fairness and abstraction. They do a really good job of giving examples of

40:46.000 --> 40:50.560
what kind of issues might arise when you do that, right? You're just like looking at the system

40:50.560 --> 40:58.720
in isolation, not thinking about it as, you know, as part of a larger system, which is like, how

40:58.720 --> 41:04.000
is it being used? What domain is it being used in? Who is using it against whom, et cetera? Then

41:04.000 --> 41:09.040
your analysis becomes very different and much more complex. But you're not incentivized to do that.

41:09.040 --> 41:14.000
Where you're not going to ascend, the people who want to give you tenure are going to be like,

41:14.000 --> 41:18.800
oh, whatever, that's just data, or that's just fluffy, whatever. You know, that's how I'm telling

41:18.800 --> 41:25.360
you what I talk. And so because of that, you're incentivized to be like, oh, you know, what does

41:25.360 --> 41:31.920
fairness mean? I have no idea what fairness means, right? And, you know, Mimi Anoha, actually,

41:31.920 --> 41:38.640
was Sita Pena, I think, who said she came to fact and she gave a talk about some of her

41:38.640 --> 41:46.400
observations there at a some other workshop. And she said, what, why, you know, what does it

41:46.400 --> 41:51.920
need to make systems fair that are punitive? That their job is to just be punitive, right? So,

41:53.360 --> 41:58.960
for instance, when people talk about risk assessment, they jump to, you know, using the

41:58.960 --> 42:06.080
compass data set and then, you know, Christian Lam and others, like, have written about all the

42:06.080 --> 42:11.440
issues with that data set and why people shouldn't just jump to use it. And then they, they say,

42:11.440 --> 42:17.360
okay, like, you know, we, we looked at that data set and we have this new algorithm and it makes,

42:17.360 --> 42:22.960
you know, this other metric higher by x percent, right? What does exactly, what does that mean in

42:22.960 --> 42:28.960
reality, right? Like what you're doing is you're still locking people up, like, you know, and you're

42:28.960 --> 42:32.800
trying to figure out how, you know, what does fair mean in this case? Like, you're locking this

42:32.800 --> 42:37.360
other person out of the same map. So a lot of people don't, you know, abolitionists don't even think

42:37.360 --> 42:42.240
that whole system should exist, right? So when you're not looking at the entire system and you're

42:42.240 --> 42:50.160
just focusing on this map, it's even, it's unclear like what, you know, if that's even something that

42:50.160 --> 42:55.200
will help or not, and many times it can be very harmful. I've seen this in the face recognition

42:55.200 --> 43:01.920
discussion, you know, after Joy and I wrote the paper, like gender shades, a lot of people were like,

43:01.920 --> 43:06.080
oh, okay, you know, Microsoft came out with an announcement saying now that, like, they've changed

43:06.080 --> 43:10.560
their training data, you know, data, and now it's much better. Now it's all, you know, accurate.

43:10.560 --> 43:17.600
It doesn't have, you know, the, for darker skin women, the air rates are not as high,

43:17.600 --> 43:22.880
but then when you look at all of this scholarship from especially trans and non-binary scholars,

43:22.880 --> 43:27.680
they talk about how automatic gender recognition should not even exist. It shouldn't even be a thing.

43:27.680 --> 43:32.880
So why are you jumping to making it go unfair, right? That's because you're not incentivized to

43:32.880 --> 43:39.200
look at the whole system. So, you know, and then even if you want to do the right thing, even if you

43:39.200 --> 43:44.800
want to do the right thing, like I tried to do at Google, if you're, you're going to get fired,

43:45.440 --> 43:50.400
then what, what does that mean, right? Like, where are we writing papers about what to do if everybody's

43:50.400 --> 43:54.800
going to get fired if they try to do the right thing? So that's what I mean. You just cannot

43:54.800 --> 44:00.720
reduce it to this mathematical thing, but you could, you keep on doing it and our field people

44:00.720 --> 44:04.960
keep on doing it because that's what they're incentivized to do and that's what they're rewarded for.

44:04.960 --> 44:12.320
I think what I love so much about talking to you is that you have this very clear view of all of

44:12.320 --> 44:18.240
the challenges, the systemic systematic challenges that are kind of inherent and baked into,

44:18.240 --> 44:26.640
you know, all of these systems that, you know, we struggle against and, you know, that doesn't,

44:28.480 --> 44:31.920
you don't get jaded. You just, oh, let me try something else. Let me try something else.

44:32.800 --> 44:33.760
I'll try something else.

44:38.560 --> 44:42.480
Honestly, I think sometimes it's because I don't really think about, you know,

44:42.480 --> 44:49.840
the other option, I think, is way too depressing is what I think, right? The other option of, like,

44:51.280 --> 44:56.880
you know, being like, I guess this is too big of a problem. We can't do anything. I just, you know,

44:56.880 --> 45:03.120
it's too depressing, right? And then sometimes, like, when I started there, now I think about,

45:03.120 --> 45:09.360
oh, my God, I was doing, I was doing this visualization of the plots of, like, how much money you have,

45:09.360 --> 45:16.960
your burn rate, when there's a red line, when you run out of money. And I made a mistake,

45:16.960 --> 45:22.640
and that red line was like literally like this October. And I just, my heart was just like,

45:23.440 --> 45:27.920
and I knew I made a mistake, but I'm like, oh, my God, I'm doing something right now where this could

45:27.920 --> 45:33.920
be, this could be the scenario. And it's not just my job. It's like all of these people's jobs

45:33.920 --> 45:38.960
that are on the line, you know? And so when I think about those things, I'm like, oh, my God,

45:38.960 --> 45:47.280
what am I doing? But you know, if I don't really, if I don't really, then, yeah, we have to be,

45:47.280 --> 45:53.360
and I love this quote by Maryam Kaba, and I heard it from Ruha Benjamin saying it,

45:53.360 --> 45:59.920
that hope is a discipline. It's, you know, we have to, yeah, what's the alternative? What do we,

45:59.920 --> 46:05.600
you know, and it's not like a lot of times, I think we think that things are so far away,

46:05.600 --> 46:11.040
they're not going to touch us, but we're seeing that that is not true, right? With the pandemic,

46:11.040 --> 46:16.240
with the wildfires in California, with, you know, all of the ways in which we're all connected

46:16.240 --> 46:23.040
in the world, like, you know, if you want a, just even a better, a better world for ourselves,

46:23.040 --> 46:28.080
I'm not even thinking about the next generation, who should sue the hell out of all of the prior

46:28.080 --> 46:35.120
generations for living them, like a world with, you know, with the climate catastrophe.

46:36.080 --> 46:40.400
Even if we want a different alternative, you know, I think we should work for it, and for me,

46:41.920 --> 46:46.560
I, it makes me feel better to do that, right? To feel like, at least, you know, I'm trying this

46:46.560 --> 46:54.000
other thing, you know, this other alternative, you know, otherwise it's just too depressing.

46:54.000 --> 47:01.520
I don't, I don't know how to not do that, you know? Yeah, yeah, yeah. So the initial funding

47:01.520 --> 47:08.160
for Dare came through MacArthur, have you identified funding sources beyond that, or how that

47:08.160 --> 47:13.200
all is going to work? Yeah, that's the big question that I'm working on right now. So the initial

47:13.200 --> 47:19.680
funding came from MacArthur and Ford, and also the Kapoor Center gave us a gift, and I'm Rocket

47:19.680 --> 47:25.360
Feller Foundation and Open Society Foundation. So it's all these foundations right now. And so

47:25.360 --> 47:30.640
now we're applying for, you know, project-based funding for grants, like based on specific projects

47:30.640 --> 47:35.440
that we're working on. And just like other people, you know, if there's an NSF grant, we're looking

47:35.440 --> 47:42.160
to that and, you know, see if we can apply. But I am extremely worried about having a whole

47:42.160 --> 47:46.480
institute that is only based on grants. So one of the things I'm doing right now is trying to

47:46.480 --> 47:52.320
figure out how do we have our own revenue stream, and what does that look like? And really hoping

47:52.320 --> 47:57.360
to have some things that we can experiment with in the next few months, because, you know,

47:57.360 --> 48:02.320
we have a bunch of people with expertise, and I think we can provide that expertise in different

48:02.320 --> 48:08.720
ways that are valuable to people, and that help us kind of generate revenue for our institute,

48:08.720 --> 48:13.040
in a way that gives us a little bit more freedom of independence and flexibility, right?

48:13.040 --> 48:20.320
Imagine right now, I say something wrong that one of the funders doesn't like, and they're all

48:20.320 --> 48:25.440
know each other, and then everybody can just be like, sorry, bye. Like, you know, that can happen.

48:27.280 --> 48:33.680
And it's really interesting, you know, the nonprofit world. You realize, you know, I mean,

48:33.680 --> 48:38.320
it is because of wealth inequality that this world even exists. It's actually really sad.

48:38.320 --> 48:44.560
And it's all the people I ran away from, like, you know, Eric Schmidt, you know, Chan Zuckerberg

48:44.560 --> 48:49.840
Basils, and these are all the people who have these large foundations that want to fund tech-related

48:49.840 --> 48:55.600
stuff. So it's, you know, so that's kind of what I'm thinking about right now. Like, we're

48:55.600 --> 49:02.320
identifying different funding sources, thinking about how to diversify our funding sources,

49:02.320 --> 49:09.360
what would our own revenue stream look like? And once I figure it, especially the revenue

49:09.360 --> 49:16.640
stream part, and we have a few things to experiment with, I'll be much happier. I'll be, you know,

49:16.640 --> 49:25.120
I'll feel much better about it. Nice, nice. Now, how far along are you? Are there folks that are

49:25.120 --> 49:32.560
dare-affiliated that have research projects that are spun up and things that you can talk to?

49:32.560 --> 49:38.800
So we have, we have Alex Hanna as a director of research, Dylan Baker, who used to be

49:38.800 --> 49:47.200
under me at Google 2. As a, as a research research and such engineer, we have, I think, two research

49:47.200 --> 49:55.920
fellows, Milla and Raseja. Milla just joined, like, this week. And one person who's probably going

49:55.920 --> 50:01.840
to join us full-time in the next month or so. And, yeah, so we have, for instance, one of the

50:01.840 --> 50:09.600
projects that I had been working on with, we've been working on with Raseja is this project to

50:09.600 --> 50:15.520
analyze spatial apartheid, the impacts of spatial apartheid using satellite images and computer

50:15.520 --> 50:20.960
vision techniques. And that's a project where, again, all the issues I talked about appear,

50:20.960 --> 50:26.320
like, it takes you a long, long time to, the innovation is on figuring out the data, right? Like,

50:27.440 --> 50:32.480
how to get the data and how to process it, how to annotate it. That's very hard to-

50:32.480 --> 50:34.720
What does that mean, spatial apartheid?

50:34.720 --> 50:42.240
Oh, spatial apartheid is basically, like, segregation, but it was mandated in 1950 by the

50:42.240 --> 50:51.120
Group Areas Act in South Africa. So it's a big, like, it's a feature of apartheid, you know, and so

50:53.280 --> 50:58.320
people of European descent could live in certain areas and everybody else had to live in,

50:58.320 --> 51:03.680
and, you know, other areas like townships. And the budget application was a lot lower for townships,

51:03.680 --> 51:09.920
of course. And so the question is, you know, supposedly apartheid has ended legally, right?

51:09.920 --> 51:16.160
But when you look at these aerial images, it's so clear. Like, the delineation is so

51:16.160 --> 51:22.080
clear. And so the question is, can we analyze the evolution of these neighborhoods and how things

51:22.080 --> 51:27.840
are changing? Because we know, right? It passes the smell test in that you can look at these things

51:27.840 --> 51:35.520
visually and do an analysis. We're not just trying to do magic, right? So the question is,

51:35.520 --> 51:42.240
you know, how can we use computer vision techniques to do that? So Rassadja, when speaking of,

51:42.240 --> 51:47.200
you know, exploitation versus not, et cetera, Rassadja, someone who grew up in a township,

51:47.200 --> 51:51.440
I mean, so this is a very personal project for her. So it's like she's, you know,

51:51.440 --> 51:56.000
investigating her own, you know, like stuff that's related to her. It's not like this,

51:56.000 --> 52:00.720
what people say, parachute science, right? So that's one of the projects we're working on. We just

52:00.720 --> 52:06.880
had a paper on Neuritz. We're working on releasing the data. That's one of the things I like

52:06.880 --> 52:12.880
about being a dare because I, you know, we didn't just stop, you know, publish the paper and like,

52:12.880 --> 52:17.040
really quickly release the data and we're done. We're like, okay, how do we release the data? How

52:17.040 --> 52:21.440
do we create visualizations? How do we allow people to interact with the data? What art,

52:21.440 --> 52:25.920
what follow up work are, you know, we're writing an article for Africa as a country. I don't know if

52:25.920 --> 52:31.120
you know the outlet. It's it's one of my favorite outlets about about the work and one of the

52:31.120 --> 52:37.600
things we want to say is that actually the South African government has to label townships

52:37.600 --> 52:42.320
if they want to analyze the impacts of spatial apartheid. What they do is they end up in the census,

52:42.320 --> 52:50.800
they lump it with just suburbs as formal residential areas. But, you know, that doesn't allow you to,

52:50.800 --> 52:56.160
to because townships were created as because of apartheid that doesn't allow you to. And this is

52:56.160 --> 53:01.040
interesting. It's, it's part of a larger kind of issue. Mimio Nguho was talking about how

53:01.760 --> 53:06.480
some of her work, I think it's called Data Voids or something like that talks about how,

53:06.480 --> 53:12.080
for instance, Google Maps didn't have Fabela in Brazil, you know, right? That's a huge, huge,

53:12.080 --> 53:19.120
huge community of people. So it's part of this larger thing about, you know, who's data is, is

53:19.120 --> 53:26.080
visible anyhow. But yeah, like, but that's an example of a project that we're working on and

53:26.080 --> 53:30.720
there's a few others too. Well, how, how can the community support what you're doing?

53:31.680 --> 53:38.880
Well, you know, follow us a dare. We're going to, you know, on Twitter, I think we're going to

53:38.880 --> 53:45.840
also have more stuff on our website, just about more stuff we're working on. And you can donate

53:45.840 --> 53:50.960
to dare if you're interested. We're going to have, you know, fellowships for people that we're,

53:50.960 --> 53:57.840
you know, we have to think through how to do these fellowships too. And yeah, I think that's it,

53:57.840 --> 54:03.040
you know, and advocate for more funding for these kinds of independent research institutes.

54:03.040 --> 54:09.760
I don't want to have to cater to like a billionaire to get, you know, funding for our institute.

54:09.760 --> 54:16.000
I'd rather apply for a grant that comes from, you know, public, you know, taxpayers and, you know,

54:16.000 --> 54:21.440
be accountable to that. So that's another way I think in which people can advocate for these things.

54:21.440 --> 54:24.640
And are you still hiring? Are you bringing on additional researchers?

54:24.640 --> 54:29.600
Yeah, I mean, we have a lot of requests for hiring. And so we have to figure out, like I said,

54:29.600 --> 54:36.800
we have to first build the initial foundational team. And so before I, we open it up for like

54:36.800 --> 54:42.480
applications that will fly like that we've had like hundreds of people asking about internships

54:42.480 --> 54:49.360
and volunteer and, you know, full-time jobs. So after we set up the initial team, then we're

54:49.360 --> 54:53.040
going to be thinking, you know, thinking very carefully about what kind of internship fellowship

54:53.040 --> 54:58.000
opportunities will have, what kind of, you know, other full-time opportunities will have.

54:58.000 --> 55:02.160
I mean, that's the thing about having a small research institute and having to think about

55:02.160 --> 55:07.680
funding sources as I can't grow it really fast, right? I can't, like, so that's the sad part. But,

55:08.320 --> 55:12.960
and the thing about volunteer opportunities that I'm thinking about very carefully is,

55:12.960 --> 55:19.120
who does that prioritize? Right? A lot of people can't do volunteer stuff because they have to

55:19.120 --> 55:24.080
work. So I think I feel strongly about people being compensated for their work.

55:24.080 --> 55:31.680
Very cool. Very cool. Well, Timnett, it has been wonderful, as always,

55:32.480 --> 55:36.000
connecting, reconnecting with you and learning a little bit about

55:36.000 --> 55:42.640
dare and what you're building there. Thank you for having me. It's a lot of fun to

55:42.640 --> 55:48.320
come back periodically and kind of reminisce on like how much stuff has changed, you know.

55:48.320 --> 55:54.480
Yeah. Yeah, we'll have to be sure to schedule the next one not quite as far out.

