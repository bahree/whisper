WEBVTT

00:00.000 --> 00:13.000
Welcome to the Tumul AI Podcast.

00:13.000 --> 00:25.040
Alright everyone, I am on the line with Jenshu Chen.

00:25.040 --> 00:31.480
Jenshu is a scientist in the assay development group at the Allen Institute for Cell Science.

00:31.480 --> 00:34.120
Jenshu, welcome to the Tumul AI Podcast.

00:34.120 --> 00:35.320
Hi Sam.

00:35.320 --> 00:42.200
Hey, it's great to speak to you and meet you and I'm looking forward to our conversation

00:42.200 --> 00:43.800
to get started.

00:43.800 --> 00:49.040
Why don't you tell us a little bit about your background and how you came to work at

00:49.040 --> 00:52.680
the intersection of machine learning and biology?

00:52.680 --> 01:01.120
Okay, I got my PhD in computer science in 2017 from University of Notre Dame and during

01:01.120 --> 01:09.960
my PhD, I mainly conducted research in machine learning and image analysis and more importantly

01:09.960 --> 01:13.120
and their applications in biology.

01:13.120 --> 01:18.560
So before I came to Notre Dame, I have no clue what I'm going to do in the future and

01:18.560 --> 01:25.280
later I just, all of a sudden, like we're randomly came into a very fascinating problem

01:25.280 --> 01:30.680
in biology and trying to use computer science algorithm to solve it and that is the moment

01:30.680 --> 01:35.800
I say, okay, oh my god, this is so fun and I'm very impactful.

01:35.800 --> 01:43.480
So that's the place where I decide, okay, this is my career.

01:43.480 --> 01:51.920
And then after I graduate in 2017, I came to Allen Institute for Cell Science where we

01:51.920 --> 02:00.680
have very heavy like multi-team science where we have lots of biologists, computer scientists,

02:00.680 --> 02:06.240
computational biologists and software engineer and everyone worked together to solve very

02:06.240 --> 02:13.080
big fundamental cell biology problems and machine learning is playing a critical

02:13.080 --> 02:16.840
role that attracts me here.

02:16.840 --> 02:22.880
When you first got started, were you more coming from a biology perspective or more from

02:22.880 --> 02:25.760
a computer science perspective?

02:25.760 --> 02:34.120
I will say I came from more computer science perspective and I always use a very interesting

02:34.120 --> 02:35.120
example.

02:35.120 --> 02:39.920
I would say like before 2017, I know nothing about mitochondria.

02:39.920 --> 02:45.600
I don't even know that particular English word and I don't even know what does it mean.

02:45.600 --> 02:48.280
So that's really funny.

02:48.280 --> 02:54.440
Right now I'm doing lots of segmentation analysis interpretation on mitochondria.

02:54.440 --> 02:59.520
So that's really something really funny to me actually.

02:59.520 --> 03:01.760
How did you approach that?

03:01.760 --> 03:08.680
A lot of folks that are doing machine learning don't have deep experience in a domain

03:08.680 --> 03:15.240
and have to get up to speed in the domain and the domain that you're getting up to speed

03:15.240 --> 03:21.200
in is one that is particularly complex which has its own many open questions around the

03:21.200 --> 03:22.280
biology.

03:22.280 --> 03:25.400
How did you approach coming up to speed in it?

03:25.400 --> 03:27.760
Yes, that's a fantastic question.

03:27.760 --> 03:30.480
So actually I will probably cover more.

03:30.480 --> 03:35.520
I cover a lot in that particular question in my GTC talk.

03:35.520 --> 03:43.080
So the basic idea is that in computer science, we try to make design abstract problems.

03:43.080 --> 03:49.560
Say we have a very practical problem like in biology, in medical domain or in whatever,

03:49.560 --> 03:52.880
the problem in our real life is more complicated.

03:52.880 --> 03:58.200
But computer scientists sometimes may want to start with a simplified version or abstract

03:58.200 --> 03:59.200
version.

03:59.200 --> 04:05.040
And starting from there, we develop methods, we develop all the kind of deep learning models

04:05.040 --> 04:08.920
and artificial intelligence techniques put in there.

04:08.920 --> 04:14.680
But later, I realized that how much we want to do this simplification or abstract, abstract

04:14.680 --> 04:22.640
is actually a key when we want to put artificial intelligence into biology.

04:22.640 --> 04:25.720
So sometimes the problem is oversimplified.

04:25.720 --> 04:26.720
That's the problem.

04:26.720 --> 04:33.120
That's the place I find myself really struggling with and having lost, I'm struggling a lot

04:33.120 --> 04:38.480
when I do the transition from a computer scientist to like half and half.

04:38.480 --> 04:45.200
So the strategy I took to do this transition is going back and forth between the knowledge

04:45.200 --> 04:51.360
that the biologists and come back to my problem and come back for many times.

04:51.360 --> 04:56.960
When I show my result to biologists, I try to understand how they see my result.

04:56.960 --> 05:01.480
And sometimes, or most of the time, they see the result or they see the image or they

05:01.480 --> 05:07.120
see the figures, the numbers completely different from what I saw it.

05:07.120 --> 05:10.680
So they are viewing it from a very different aspect.

05:10.680 --> 05:17.160
And by analyzing or by learning all such discrepancy between how I view the problem and

05:17.160 --> 05:23.400
how they view the problem, that's the strategy I took to transition myself from computer

05:23.400 --> 05:27.320
scientists to like half computer scientists, half biologists.

05:27.320 --> 05:28.320
That's awesome.

05:28.320 --> 05:33.520
Does any particular example of that kind of interaction come to mind where, you know,

05:33.520 --> 05:36.800
you presented some results and they really saw them very differently from how you did

05:36.800 --> 05:38.960
and you had to kind of close that gap?

05:38.960 --> 05:39.960
Yes, absolutely.

05:39.960 --> 05:42.160
There are tons of cool examples there.

05:42.160 --> 05:48.480
A very simple example is when we know lots of like segmentation, say the basic idea

05:48.480 --> 05:52.120
of segmentation is the computer raised in an image.

05:52.120 --> 05:57.360
We want to find or we want to outline the structure in this image.

05:57.360 --> 06:03.200
And then we can quantify the size of it or how long it is, something like that, right?

06:03.200 --> 06:05.720
So the problem is pretty straightforward.

06:05.720 --> 06:12.200
So for computer scientists, I will just say, hmm, I will do a like this and that method

06:12.200 --> 06:13.680
and extract object.

06:13.680 --> 06:18.320
I will, how good is my output or how good is my algorithm?

06:18.320 --> 06:25.240
I will do that visually by a video assessment or some kind of comparison between my result

06:25.240 --> 06:27.960
and what I see in the image.

06:27.960 --> 06:30.640
So this is my understanding as a computer scientist.

06:30.640 --> 06:34.760
But when I go to the biologist, they are seeing that in a different way.

06:34.760 --> 06:40.320
So a key message is what you see in the image may not be the truth.

06:40.320 --> 06:48.320
You have take the microscope effect into account when you shoot a light to light up a small

06:48.320 --> 06:55.800
ball under the microscope, we are talking about spinning this confocal microscope, where

06:55.800 --> 07:01.960
some particular cellular structure is labeled by some kind of fluorescent protein and then

07:01.960 --> 07:06.000
it will be light up under the microscope and by the laser power.

07:06.000 --> 07:11.720
But what you see in the image is actually you have to take into account the blurry

07:11.720 --> 07:19.000
effect of the microscope because of all those optical details that I have no idea to understand

07:19.000 --> 07:20.320
at the beginning.

07:20.320 --> 07:27.640
So you see a ball with, for example, like 10 pixel wide, the actual object may be only

07:27.640 --> 07:30.160
5 pixel wide, something like that.

07:30.160 --> 07:32.280
What you see is not the truth.

07:32.280 --> 07:34.480
That's what drives me crazy.

07:34.480 --> 07:42.400
So I have to understand what the optical details of that particular microscope and all those

07:42.400 --> 07:47.280
fundamental biology, say, for example, the things cannot be like having two branches.

07:47.280 --> 07:51.040
It has to be a single filament, things like that.

07:51.040 --> 07:57.040
I have to combine my computer knowledge and computer science, also my, the new things

07:57.040 --> 08:03.720
I'm learning about optics and also the new things I'm learning about biology, I glued

08:03.720 --> 08:08.440
these three piece together and reach a new, more accurate solution.

08:08.440 --> 08:09.440
That's great.

08:09.440 --> 08:14.440
And I think that example is one that you're going to come back to when I look through your

08:14.440 --> 08:22.480
slides from GTC, a lot of the work that you're doing in the, the cell explorer toolkit,

08:22.480 --> 08:27.800
which we'll be talking about, look like trying to kind of close this gap computationally

08:27.800 --> 08:31.880
between what you might see in one image and what's really, you know, there, but you're

08:31.880 --> 08:36.320
not seeing it due to some of these artifacts and effects that you're describing.

08:36.320 --> 08:37.320
Exactly.

08:37.320 --> 08:38.320
Yes.

08:38.320 --> 08:39.320
Is that right?

08:39.320 --> 08:40.320
Yes.

08:40.320 --> 08:41.320
Right.

08:41.320 --> 08:49.000
So maybe let's then use that as an opportunity to transition and talk a little bit about the

08:49.000 --> 08:54.360
cell explorer toolkit and, you know, what is it really trying to do?

08:54.360 --> 08:56.040
Oh, yeah, sure.

08:56.040 --> 09:02.320
So at Alan Institute for Cell Science, we are trying to build this concept called Alan

09:02.320 --> 09:09.960
Cell Explore Toolkit, which is a combination of cell creator, cell image generator, cell

09:09.960 --> 09:15.760
image analyzer, cell image view analyzer and cell image simulator.

09:15.760 --> 09:22.880
And in my GTC talk, I focused mostly on three parts, the generator analyzer and view

09:22.880 --> 09:29.760
analyzer, where the computation or the GPU computing is heavily used there.

09:29.760 --> 09:38.080
So the generator, if I want to describe it with one sentence is like how we get a image

09:38.080 --> 09:39.520
to be analyzed.

09:39.520 --> 09:48.600
So apparently we need to get image from microscope, but with the help of GPU computing, we can

09:48.600 --> 09:52.560
generate more image from one single experiment.

09:52.560 --> 09:55.440
So that's the key idea of generator.

09:55.440 --> 10:03.000
The analyzer is, as the name indicates, it's just given this image how we analyze them.

10:03.000 --> 10:06.840
And finally, realization is a big part of cell biology.

10:06.840 --> 10:15.440
So and also for GPU computing is certainly a big, a play a critical role in the modern

10:15.440 --> 10:23.680
realization tools and taking a step back, what's the goal for the Cell Explore Toolkit?

10:23.680 --> 10:31.040
Is this an internal tool that's used at the Alan Center or is it designed to be used

10:31.040 --> 10:39.560
by external folks and who are the main users, presumably biologists or is this something

10:39.560 --> 10:40.560
different?

10:40.560 --> 10:41.560
Yeah, great.

10:41.560 --> 10:42.880
That's a perfect question.

10:42.880 --> 10:48.440
So actually, at Alan Institute for Cell Science, we are doing open science and we want to

10:48.440 --> 10:55.360
make the tool that we develop here to be usable for everyone outside this institute.

10:55.360 --> 11:00.520
And the reason we believe our tool will be useful is that we are doing academic research

11:00.520 --> 11:02.240
at the industrial scale.

11:02.240 --> 11:08.640
By doing research at this level or at this scale, we may realize something we hope to build

11:08.640 --> 11:16.440
some tools that will be more general or more stable or more user friendly for a broader

11:16.440 --> 11:20.840
audience so that everyone can use it.

11:20.840 --> 11:28.080
So that's the basic idea of why we want to build this toolkit.

11:28.080 --> 11:35.480
And so you mentioned that you focused on these three areas, starting with the cell image

11:35.480 --> 11:36.800
generator.

11:36.800 --> 11:41.600
Tell us a little bit more about that and where machine learning comes into play.

11:41.600 --> 11:42.600
Okay.

11:42.600 --> 11:44.000
Cell image generator.

11:44.000 --> 11:49.680
Of course, again, as I just mentioned, we need the microscope to get some image.

11:49.680 --> 11:58.240
However, with the help of GPU computing, we can break some limit where the traditional

11:58.240 --> 12:00.480
microscope has.

12:00.480 --> 12:09.560
For example, in my talk, I showed an example where I took the image as a very low resolution,

12:09.560 --> 12:17.400
which can help me get overview of the whole colony, which is like maybe 500 of cells in

12:17.400 --> 12:24.280
the same image, but each of them has relatively not that detailed because of the resolution.

12:24.280 --> 12:30.160
However, if we change some settings in the microscope, we switch to a different objective

12:30.160 --> 12:37.640
or using a different modality, the same colony, we can get it in a very, very high resolution.

12:37.640 --> 12:43.080
Say, for example, but we may sacrifice something because for example, we can only see maybe

12:43.080 --> 12:49.520
tens cells in that particular image, but each cell has lots of details, which contains

12:49.520 --> 12:52.840
lots of meaningful biological information in there.

12:52.840 --> 12:55.120
But there's a trade-off.

12:55.120 --> 12:59.120
So you cannot get both things at the same time.

12:59.120 --> 13:06.520
So you want either get more cells, but less details, or get less cells, but more details.

13:06.520 --> 13:07.920
There's always a trade-off.

13:07.920 --> 13:13.720
So we are thinking that whether a computational model can help.

13:13.720 --> 13:20.480
So that's what motivates us to design this, what we call the transfer function.

13:20.480 --> 13:27.240
The basic idea is we build a deep learning model, train on some low-res image and high-res

13:27.240 --> 13:28.640
image pair.

13:28.640 --> 13:35.960
So whenever in the real image acquisition, we got a sequence of low quality or low-res

13:35.960 --> 13:42.920
image, which give us like 500 cells in each time step.

13:42.920 --> 13:45.520
And even though there's not that much detail.

13:45.520 --> 13:53.480
We're after applying this fully trained transfer function model on this low resolution images.

13:53.480 --> 14:00.600
We can, like, make it up and we can improve the quality or improve the resolution of

14:00.600 --> 14:07.760
each image so that every single cell, we can put back all the details of each cell into

14:07.760 --> 14:09.280
that low-res image.

14:09.280 --> 14:18.560
In other words, we are seeing high resolution of each cell at a very large colony, or

14:18.560 --> 14:25.880
it's a combination of larger field of view, or maybe seeing more cells in the image and

14:25.880 --> 14:28.840
also seeing more details in the image.

14:28.840 --> 14:32.880
So traditionally, yeah, in the microscope, we cannot do that.

14:32.880 --> 14:37.720
But combining that with the computational model, we can achieve that to a very, very

14:37.720 --> 14:38.720
high accuracy.

14:38.720 --> 14:46.120
I'm envisioning something along the lines of applying style transfer or deoldify kind

14:46.120 --> 14:52.760
of one of these image coloring techniques to the low-resolution images based on the

14:52.760 --> 14:56.240
data from the high-resolution portion of those images.

14:56.240 --> 14:57.240
Yes.

14:57.240 --> 15:04.760
So the underlying techniques is pretty much the same as style transfer was the common models

15:04.760 --> 15:10.360
people used in deep learning and computer science for style transfer for super resolution

15:10.360 --> 15:11.600
and things like that.

15:11.600 --> 15:15.600
But later, at the beginning, I thought, yeah, I have the exact same feeling.

15:15.600 --> 15:20.600
I think if people can do that on natural scene image, I think it's just a piece of cake

15:20.600 --> 15:23.440
and just put it on the microscope image.

15:23.440 --> 15:25.120
There's no big deal.

15:25.120 --> 15:32.760
But later, after doing more and more validation, I realized one important piece.

15:32.760 --> 15:41.440
In the natural scene image, say you are trying to supervise your image of a cat, right?

15:41.440 --> 15:48.080
So in your output, the supervised image, if the cat, the eyes of the cat, looks a little

15:48.080 --> 15:51.120
bit larger or smaller, it doesn't really matter.

15:51.120 --> 15:55.840
Maybe one pixel larger or two pixels smaller, you may not even notice that, right?

15:55.840 --> 15:58.400
That's still a cat.

15:58.400 --> 16:02.720
But for sale, that's different, that will be a totally different story.

16:02.720 --> 16:05.920
I was going to ask that, I mean, a lot of, you know, when I think about those kind of

16:05.920 --> 16:10.360
techniques, I think of generative models that, you know, in a lot of ways, they're just

16:10.360 --> 16:11.360
making stuff up.

16:11.360 --> 16:16.600
But if you're trying to use these images for scientific purposes, you actually want them

16:16.600 --> 16:21.200
to reflect the reality of what's happening happening in that cell population, not just,

16:21.200 --> 16:24.720
you know, a visual, high resolution thing that looks nice.

16:24.720 --> 16:25.720
Yes.

16:25.720 --> 16:31.840
Let's give us two things, first of all, it gives us some direction about how we can improve

16:31.840 --> 16:35.600
our model on top of existing style transform models.

16:35.600 --> 16:42.320
On the other hand, it gives us, it gives us some hint on how we should validate our result

16:42.320 --> 16:47.360
and how much we should trust our generated image.

16:47.360 --> 16:50.200
So why don't you go into those two in more detail?

16:50.200 --> 16:51.560
Yes, for sure.

16:51.560 --> 16:55.560
So for the first part, so after seeing this particular

16:55.560 --> 17:00.840
thing about whether the prediction is to be larger or smaller and whether how much it

17:00.840 --> 17:04.480
affects, it drives us going back to the model.

17:04.480 --> 17:11.360
See, what can we do to, in the model, to make this prediction more accurate?

17:11.360 --> 17:13.360
What's wrong with the model?

17:13.360 --> 17:19.680
So then we realized that in our training data or the way we collect the pair of low-res

17:19.680 --> 17:25.520
image and high-res image, it's very, very hard to get a fully aligned, low-res image

17:25.520 --> 17:27.920
and low-res and high-res pair.

17:27.920 --> 17:33.360
Think about how we capture the image in the real life.

17:33.360 --> 17:40.240
Say in microscope, when we capture a 3D image, you can think of like, if you have a piece

17:40.240 --> 17:42.840
of meat, you want to slice it, right?

17:42.840 --> 17:48.720
So in the low-res image, the same piece of meat, you may slice it like at three different

17:48.720 --> 17:49.720
positions.

17:49.720 --> 17:56.960
But in a high-res image, you may slice that piece of meat like at ten different positions,

17:56.960 --> 18:00.480
much denser, much smaller gap between each step.

18:00.480 --> 18:09.800
So in that case, and also the position used to the slicing, or you take the slice, they

18:09.800 --> 18:11.800
may not at the same place.

18:11.800 --> 18:18.640
So anyway, anyhow, the low-res image and high-res image, they may miss a line in Z.

18:18.640 --> 18:20.080
That is a fundamental issue.

18:20.080 --> 18:21.080
Right.

18:21.080 --> 18:29.880
So you can't necessarily directly match your target image or label image in the high-res

18:29.880 --> 18:36.600
space to the low-res space because they're misaligned in terms of these slices.

18:36.600 --> 18:37.600
Correct.

18:37.600 --> 18:40.120
So we are trying two different ways.

18:40.120 --> 18:49.360
First of all, we modified a model, which we embedded a spatial transformer network into

18:49.360 --> 18:54.360
the model to learn such misalignment and try to correct it.

18:54.360 --> 18:55.880
That's one thing we tried.

18:55.880 --> 19:03.080
The other thing we tried is we tried to develop registration algorithm, which can make

19:03.080 --> 19:06.360
this alignment to its best.

19:06.360 --> 19:13.840
Maybe they are not perfect, but as much as we can using a traditional image registration

19:13.840 --> 19:16.360
algorithm to the line image.

19:16.360 --> 19:23.400
And we tried both, and each has pros and cons, but the good news is, both methods are

19:23.400 --> 19:28.000
improving the quality of the prediction by a lot.

19:28.000 --> 19:34.480
I'm not sure if the image that I'm thinking of is one that corresponds to the point you're

19:34.480 --> 19:35.480
making.

19:35.480 --> 19:42.480
There's an initial reaction I had to that, that you've got these additional slices, and

19:42.480 --> 19:48.040
you're not able to align your slices, and so the approach is maybe one of interpolation,

19:48.040 --> 19:52.400
but there's this one image in there that's showing that actually these different slices

19:52.400 --> 19:54.320
have totally different things in them.

19:54.320 --> 20:00.080
And so you can't necessarily statistical interpolation of some sort probably isn't going to work

20:00.080 --> 20:01.080
very well.

20:01.080 --> 20:05.280
So in terms of the slice, so I'm not sure whether I...

20:05.280 --> 20:11.800
I explained that clearly, but if I would try to put it in a different way.

20:11.800 --> 20:19.280
So if you think about the low-rise image, have three different slices on the same mid.

20:19.280 --> 20:23.600
On the high-rise image, you have ten different slices on the same piece of mid.

20:23.600 --> 20:29.960
So the first step, we need to absent or do some interpolation on low-rise image so that

20:29.960 --> 20:33.480
it will have ten slices.

20:33.480 --> 20:37.880
So now, on both images, we have ten slices, right?

20:37.880 --> 20:47.200
So now, the number of slices match, but the first slice in the up-sample or the interpolated

20:47.200 --> 20:54.680
version, the first slice of that may correspond to the second slice of the true, like, high-rise

20:54.680 --> 20:55.680
slice.

20:55.680 --> 20:57.480
And that's where you're doing your registration.

20:57.480 --> 21:03.240
Yes, that's what we are doing either we want to shift them a little bit and at the beginning

21:03.240 --> 21:09.760
or we can also, we also develop a model that can handle that inside the model.

21:09.760 --> 21:10.760
Okay.

21:10.760 --> 21:15.320
I think the image that I was thinking of is referring to something else.

21:15.320 --> 21:19.360
It was talking about predicted channels, and it has, like, each of the channels has very

21:19.360 --> 21:20.600
different things on it.

21:20.600 --> 21:23.800
And I was interpreting that as slices, but that's probably something different.

21:23.800 --> 21:28.240
Oh, by slice, I mean, the slice along C direction.

21:28.240 --> 21:36.600
Yeah, so you're doing the registration, which allows you to then use your up-sampled image

21:36.600 --> 21:40.040
as a label, is that essentially right?

21:40.040 --> 21:46.840
So use my up-sampled image as the input, use my tool, yeah, use the true, high-rise

21:46.840 --> 21:48.240
image as my target.

21:48.240 --> 21:49.240
Cool.

21:49.240 --> 21:50.840
So that was the first part, correct?

21:50.840 --> 21:54.800
And then the second piece is what?

21:54.800 --> 21:59.440
The second part of the, actually, there's a second part of the image generator, which

21:59.440 --> 22:01.720
we call the label-free method.

22:01.720 --> 22:08.120
So what we have been talking about is from low-rise to high-rise, but there's another type

22:08.120 --> 22:10.440
of generation.

22:10.440 --> 22:17.000
So think about, now we come to multi-channel, is what you are teaching a moment ago.

22:17.000 --> 22:24.600
So yeah, so usually when we do, we collect cell images, we have multiple channels, say,

22:24.600 --> 22:29.240
we should light at different, the light spectrum, we have different types of light.

22:29.240 --> 22:35.520
And then we can have different channels, as they have different dye in there.

22:35.520 --> 22:44.040
So usually we will have an image with some dye to light up the nucleus and some kind of

22:44.040 --> 22:50.200
protein to light up the one of the intercellular structure and another type of dye to light

22:50.200 --> 22:52.680
up the cell boundary.

22:52.680 --> 22:59.240
And there's another type, another special channel, which is from the transmitted light, which

22:59.240 --> 23:02.600
gives us what we call the bright field image.

23:02.600 --> 23:07.960
The bright field image is different from what we talk about in the other three, which

23:07.960 --> 23:13.520
requires some fluorescent tagging, some fluorescent imaging.

23:13.520 --> 23:17.840
The bright field image is just transmitted light, you just shoot the light there, you see

23:17.840 --> 23:26.760
the image without any harmful or photo, any dye that may damage your cell.

23:26.760 --> 23:30.000
So the light will not damage your cell.

23:30.000 --> 23:38.160
So in that sense, this is a general description of what we have in general.

23:38.160 --> 23:46.360
So I mentioned that there's a specific channel tagging only one intercellular structures,

23:46.360 --> 23:54.200
because that's a special cell line, which we did lots of hope, bunch of gene editing,

23:54.200 --> 23:59.200
so that that particular cell can have that property where that particular structure

23:59.200 --> 24:04.320
can be light up with the type of experiment.

24:04.320 --> 24:09.400
Each experiment can only light up one intercellular structures.

24:09.400 --> 24:15.400
And sometimes with very, very special gene editing techniques, we may light up two or

24:15.400 --> 24:20.320
at most three, going beyond three will be super, super hard.

24:20.320 --> 24:27.360
So basically, we can see three structures at the time, at most, in the real experiment.

24:27.360 --> 24:32.840
However, we always have the bright field as a reference, the bright field image, I mean

24:32.840 --> 24:37.160
the image we collect by shooting the transmitted light.

24:37.160 --> 24:43.120
So that is something harmless and can be cheaply acquired.

24:43.120 --> 24:50.360
So what we are thinking is, whether we can predict the special intercellular structure from

24:50.360 --> 24:52.360
the bright field image, right?

24:52.360 --> 25:00.440
So if we can do that, but given any single bright field image, we can predict 20 different

25:00.440 --> 25:04.400
structures at the same time for the same style.

25:04.400 --> 25:06.200
And an non-destructive way?

25:06.200 --> 25:13.000
Yes, that is the most important part, or it's the brand new way of designing your essay.

25:13.000 --> 25:15.720
Or your experimental essay.

25:15.720 --> 25:19.320
So before, we can only tack one structure at a time.

25:19.320 --> 25:25.400
And you cannot see how these 20 structure leads together in the same style.

25:25.400 --> 25:30.160
You will never see that, because every time you can only see one structure or two, right?

25:30.160 --> 25:36.120
So now with this particular technique, with any single bright field image, we can predict

25:36.120 --> 25:44.440
a lot of different structures for the same style, that is a fundamental improvement in imaging

25:44.440 --> 25:47.280
or how we generate images.

25:47.280 --> 25:54.680
So that will allow us to study the cell as a whole, as like with all the different parts

25:54.680 --> 25:57.880
playing together, not individually.

25:57.880 --> 26:06.240
And so in both of these cases, kind of going back to the earlier comments around the

26:06.240 --> 26:12.640
kind of the generative nature of some of these techniques, how do you measure performance

26:12.640 --> 26:23.600
and compare these cells that you're generating to actual cells in a way that ensures that

26:23.600 --> 26:27.160
they are high fidelity to what's actually there?

26:27.160 --> 26:29.480
Yeah, that's a good question.

26:29.480 --> 26:36.680
So the strategy we are taking is applications for the scientific validation.

26:36.680 --> 26:43.360
Say you are carrying, if you are, for example, when we talk about the label free prediction,

26:43.360 --> 26:49.120
say we are predicting 10 different structures from the same bright field image.

26:49.120 --> 26:52.120
And how good they are, how much can we trust them?

26:52.120 --> 26:59.640
And if we are talking about some application, we require the absolute accuracy.

26:59.640 --> 27:06.160
Say whether the pixel-wise accuracy is exactly the same matching the choose, then probably

27:06.160 --> 27:11.240
in the 10 structure, nine of them is not that trustable in that sense.

27:11.240 --> 27:16.000
Because the pixel-wise accuracy may not be that high for some structures.

27:16.000 --> 27:20.680
So maybe one structure will have very high pixel-wise accuracy.

27:20.680 --> 27:27.680
In other applications, where we care about how different parts correlate it to each other

27:27.680 --> 27:29.520
like in the space.

27:29.520 --> 27:33.920
We don't actually need that much pixel-wise accuracy.

27:33.920 --> 27:38.800
What we care more about is whether they are predicting the overall position of that particular

27:38.800 --> 27:41.600
structure in the correct place.

27:41.600 --> 27:49.320
So with that in place, we can study how different structures functioning together or whether

27:49.320 --> 27:52.280
there's any correlation with the structures.

27:52.280 --> 27:59.200
So anyway, that is what the strategy we call, like we care more is whether this result

27:59.200 --> 28:01.800
is suitable for that application.

28:01.800 --> 28:04.000
So this is our strategy.

28:04.000 --> 28:09.400
So kind of taking an application-by-application approach to evaluating performance, some

28:09.400 --> 28:17.880
of which are concerned about absolute characteristics like size and others more concerned with relative

28:17.880 --> 28:20.040
characteristics like position.

28:20.040 --> 28:21.440
Exactly.

28:21.440 --> 28:25.960
That is the image generator component.

28:25.960 --> 28:30.440
The next one you mentioned is the cell image analyzer.

28:30.440 --> 28:38.720
What is that piece trying to do and where, you know, how have you used ML in that component?

28:38.720 --> 28:39.720
Yeah.

28:39.720 --> 28:46.880
For image analyzer, in lots of cases, when we've got an image, where we want to analyze

28:46.880 --> 28:50.960
it, the very first step is to do a segmentation.

28:50.960 --> 28:53.400
We just like binarize the image.

28:53.400 --> 28:58.280
Generate a binary mask indicates where are the structures?

28:58.280 --> 29:00.880
Where are the interstellar structures?

29:00.880 --> 29:04.120
So that's the very first step we call a segmentation.

29:04.120 --> 29:11.360
So doing segmentation in microscopy image or microscopy image of cells, it's actually

29:11.360 --> 29:18.920
different from what we are doing like the semantic segmentation or instant segmentation

29:18.920 --> 29:25.840
or all sorts of segmentation in computer vision in natural scene image.

29:25.840 --> 29:27.560
That could be very different.

29:27.560 --> 29:31.960
So that difference comes from a couple of different sources.

29:31.960 --> 29:34.360
Let's start with a very simple example.

29:34.360 --> 29:41.360
Most of people may know that in deep learning, people can just manually draw the boundary

29:41.360 --> 29:46.920
of that object, so no matter if it's a people or cat or dog, we just manually draw the

29:46.920 --> 29:54.920
boundary of that and we draw that boundary for, say, 500 images or 5,000 images and there

29:54.920 --> 29:55.920
may not be that hard.

29:55.920 --> 29:58.920
It may be take some time, but it's not that hard.

29:58.920 --> 29:59.920
That's possible.

29:59.920 --> 30:00.920
Right?

30:00.920 --> 30:06.360
So we have to have this 5,000 image or even more, we just throw it into a unit or some

30:06.360 --> 30:13.440
kind of rest net or some deep neural network and it will predict the mask for us.

30:13.440 --> 30:18.720
That's what a lot of existing work are doing.

30:18.720 --> 30:28.440
But in microscopy images, especially for cellular structures, if you think about how complicated

30:28.440 --> 30:36.720
the topology or morphology of the structure could be, then you immediately appreciate that

30:36.720 --> 30:42.640
this is not possible to do this kind of manual annotation.

30:42.640 --> 30:48.800
So if you think about, I think when you think about, you know, bounding boxes and how tightly

30:48.800 --> 30:54.280
packed these cells are in some cases, you know, it's clear that that won't work.

30:54.280 --> 30:59.800
But then you have pixel masks that are manually applied and other techniques that could conceivably

30:59.800 --> 31:02.120
be used.

31:02.120 --> 31:07.960
For the pixel mask, even that, that could be hard if you think about it 3D.

31:07.960 --> 31:14.040
By the way, I forgot to mention that everything we are doing is 3D.

31:14.040 --> 31:15.040
Okay.

31:15.040 --> 31:16.040
Yeah.

31:16.040 --> 31:23.320
If you are thinking about a ball in 3D and if you ask me to draw the mask pixel wise, for

31:23.320 --> 31:28.040
that particular ball, yes, 3D, I think I can do a pretty good job.

31:28.040 --> 31:36.760
But if you give me a tree or a more complicated structure where different parts interacting

31:36.760 --> 31:43.280
with different other parts, something like that, or a ball of yarn, you think about how

31:43.280 --> 31:45.640
they tangle up, right?

31:45.640 --> 31:53.280
And I want to you to draw the outline of each single piece of yarn, go through the ball.

31:53.280 --> 31:54.280
Yeah.

31:54.280 --> 31:55.280
It gets a lot harder in 3D.

31:55.280 --> 31:56.280
Yeah.

31:56.280 --> 31:57.280
It's not possible.

31:57.280 --> 31:58.280
Right?

31:58.280 --> 32:05.240
So the strategy we are approaching that is what we called, which we implemented in what

32:05.240 --> 32:12.520
we call the segmenter is we use some classic method there, like more than 20 years of study

32:12.520 --> 32:18.160
of classic image processing techniques that are still very useful and they can give us

32:18.160 --> 32:21.080
a very, very good result to start.

32:21.080 --> 32:28.360
So we make a collection of a classic algorithm to give us some quick start.

32:28.360 --> 32:34.240
So based on that, for example, if you think about the ball of yarn, we design some classic

32:34.240 --> 32:41.640
algorithm can give us a rough segmentation of each piece of yarn, mostly like not very

32:41.640 --> 32:47.120
accurate, but give us a lot of good segmentation here and there.

32:47.120 --> 32:55.160
So instead of manually draw where each piece of yarn is, we use a different way to do the

32:55.160 --> 32:56.160
annotation.

32:56.160 --> 33:02.400
We draw a bounding box here, this area, what the classic algorithm is doing is good.

33:02.400 --> 33:03.960
Let's confirm that.

33:03.960 --> 33:09.040
And then for that particular area, what the classic algorithm is doing is bad.

33:09.040 --> 33:10.880
So throw that away.

33:10.880 --> 33:15.720
So by doing this kind of curation, we say here are good, here are bad, here are good,

33:15.720 --> 33:23.520
and there are bad, when we may have a small amount of good segmentation and use that as

33:23.520 --> 33:30.040
our initial training data for our, to train our deep learning model.

33:30.040 --> 33:35.720
And then you have this model one, you apply it on your data, and then the result will

33:35.720 --> 33:36.720
be better.

33:36.720 --> 33:42.600
Now let's do again, here is doing good job, here is doing not that good, throw it away,

33:42.600 --> 33:45.840
here we are doing a good job, and there we are doing a good job.

33:45.840 --> 33:54.080
Okay, now we have our second round of iteration, we collect a larger set of good segmentation.

33:54.080 --> 33:56.200
Then we use that to train our second model.

33:56.200 --> 34:03.280
Now we keep this iteration going, going like iteration by iteration.

34:03.280 --> 34:08.840
So every iteration, you are model is improving a little bit, more or less.

34:08.840 --> 34:16.040
So we hope to improve our model throughout these different iterations.

34:16.040 --> 34:23.320
And at the end, finally, we will achieve a model that we can never achieve by collecting

34:23.320 --> 34:25.000
a manual annotation.

34:25.000 --> 34:31.960
What I'm hearing is that I know it's not quite the same, but it's making me think a little

34:31.960 --> 34:37.520
bit of like an active learning type of a scenario where you can't be.

34:37.520 --> 34:44.000
You're trying to feed back to the system, the training data that is struggled with the

34:44.000 --> 34:45.000
most.

34:45.000 --> 34:52.440
And in this case, you're using human annotators, but not to identify the actual features

34:52.440 --> 34:59.040
in your image, but rather to identify where your segmentation algorithm didn't do a good

34:59.040 --> 35:00.040
job.

35:00.040 --> 35:01.040
Correct.

35:01.040 --> 35:06.800
So that is a big part of the segmenter, there's another complementary part where you think

35:06.800 --> 35:13.120
about if you have two types of cells in your image, say, mytotic cell and interface

35:13.120 --> 35:14.120
cell.

35:14.120 --> 35:21.440
They are in different cell cycle and they may show up as different morphology.

35:21.440 --> 35:28.320
So one single classic method will not be able to do a good job there, right?

35:28.320 --> 35:30.760
So then we can use two.

35:30.760 --> 35:35.600
We use two methods, each will give us a different segmentation version.

35:35.600 --> 35:40.640
Now we have the same image, we have segmentation version one, we have segmentation version

35:40.640 --> 35:41.640
two.

35:41.640 --> 35:44.840
Each version has its own advantage and disadvantage advantage.

35:44.840 --> 35:45.840
Okay.

35:45.840 --> 35:53.360
Now we just use our human annotator as the follows, say we circle out in cell one, in

35:53.360 --> 35:55.080
this cell, we make a circle.

35:55.080 --> 35:56.080
Okay.

35:56.080 --> 35:59.320
In this cell, we use segmentation version one.

35:59.320 --> 36:03.680
In that cell, we circle out, we use segmentation version two.

36:03.680 --> 36:06.600
In that cell, we use segmentation version one.

36:06.600 --> 36:09.720
In that cell, we use segmentation version two.

36:09.720 --> 36:17.160
So by doing this kind of circling, we are creating emerged ground shoes that can help

36:17.160 --> 36:18.680
us in the model.

36:18.680 --> 36:24.760
At the end, the model is able to segment both type of cells correctly, you know, very single

36:24.760 --> 36:32.560
model or multiple models, single model to deal with two different type of cells.

36:32.560 --> 36:40.760
And so this is again resting on the segment or the part of the image analyzer.

36:40.760 --> 36:49.320
So a little bit like one minute or two minutes down the segment or direction is sometimes

36:49.320 --> 36:57.880
is, we may think of segment segmentation is most of the analysis is about segmentation.

36:57.880 --> 37:00.360
Sometimes it's not.

37:00.360 --> 37:04.360
Sometimes we can do something on the original image.

37:04.360 --> 37:11.160
So that's another part of the analyzer, this is what we call the integrity cell, where

37:11.160 --> 37:19.360
we build a auto encoder on the original image, where we try to learn the underlying correlation

37:19.360 --> 37:25.480
between each part of the cells and how they function as the whole.

37:25.480 --> 37:29.920
And that's another part of the analyzer.

37:29.920 --> 37:35.600
You mentioned the auto encoder part, elaborate on what exactly that's doing.

37:35.600 --> 37:36.600
Okay, sure.

37:36.600 --> 37:46.240
So I showed an example in my slides where I see cell boundaries like this, I see DNA patterns

37:46.240 --> 37:53.640
like this, and then can we use these two pieces of information, just purely taking an image

37:53.640 --> 37:59.000
to predict where the mitochondria should live.

37:59.000 --> 38:08.600
So there might be a correlation between the position of the mitochondria condition on

38:08.600 --> 38:12.880
the shape and shape of the cell and DNA.

38:12.880 --> 38:18.280
So this is how the auto encoder is trying to learn.

38:18.280 --> 38:24.480
So basically I take this, I'm trying to reconstruct it, and if I'm able to reconstruct it, and

38:24.480 --> 38:31.680
I learn throughout this process, I learn some correlation between the mitochondria and

38:31.680 --> 38:35.800
the shape of the cell and the position of the DNA, things like that.

38:35.800 --> 38:41.760
So by doing this, we are trying to model the relationship between the different structures.

38:41.760 --> 38:47.040
I'm not sure if I make it clear, but once you've done that and you've got this hidden

38:47.040 --> 38:51.280
representation in your auto encoder, how are you using that?

38:51.280 --> 38:57.360
Yeah, that is actually, we'll give us lots of, that's something we want to try to make

38:57.360 --> 39:04.360
some biological interpretation, say, what are the correlations, and how should we interpret

39:04.360 --> 39:07.240
from a biological respective?

39:07.240 --> 39:13.840
So that's something we are doing right now to give it a biological interpolation.

39:13.840 --> 39:16.760
Got it, got it, so that part is ongoing.

39:16.760 --> 39:17.760
Yes.

39:17.760 --> 39:23.280
Okay, cool, and then just quickly the third part of the toolkit that you mentioned is

39:23.280 --> 39:25.000
the visualizer.

39:25.000 --> 39:30.160
How to folks use the visualizer is that, you know, just kind of a GUI that, you know, sits

39:30.160 --> 39:34.440
on some data structures and allows you to look at these images that you've generated

39:34.440 --> 39:37.240
or what else going on in there?

39:37.240 --> 39:45.680
Yes, so we build this, I mean, I already instilled, we build this software called Agave, and

39:45.680 --> 39:53.040
that used the GPU technology to do the photorealistic lightening and shading on our 3D

39:53.040 --> 39:55.200
microscopy and data.

39:55.200 --> 40:03.760
So the reason we care about making the realization more realistic and also including something

40:03.760 --> 40:09.600
like dApps of the image and things like that to make it looking amazing, it's not more

40:09.600 --> 40:15.040
than looking amazing, actually by looking this amazing image, it's give us some feedback

40:15.040 --> 40:17.760
from biological perspective.

40:17.760 --> 40:23.280
So when you look at a particular colony, the traditional rendering method will give you

40:23.280 --> 40:28.240
kind of messy realization in 3D, especially in 3D, right?

40:28.240 --> 40:34.360
So you don't have a good first impression about the biological property of the colony,

40:34.360 --> 40:40.920
but with proper shading and the lightening and all these, the retree tracing techniques

40:40.920 --> 40:48.640
that we are using, we make the image looking very clean and very easy to digest so that

40:48.640 --> 40:53.800
biologists have a better understanding what's going on inside the cell and inside this

40:53.800 --> 40:55.600
whole colony.

40:55.600 --> 41:06.000
And we will include a link to your slides from GTC on the show notes page, and I encourage

41:06.000 --> 41:14.280
folks to check it out for this part in particular, if only because the image of the, you've got

41:14.280 --> 41:22.240
one mode in a gavi that's called cinematographic path tracing and the images do look spectacular.

41:22.240 --> 41:30.920
Yes, this is from our animated cell team, so I love their image, they're very beautiful.

41:30.920 --> 41:37.320
Cool, and is there a ML component to this work as well, or is it just kind of raw number

41:37.320 --> 41:40.520
crunching using the GPU?

41:40.520 --> 41:45.920
So far, there's no ML yet, but we are thinking about further integration.

41:45.920 --> 41:48.120
Well, very interesting stuff.

41:48.120 --> 41:55.600
Jen, she thanks so much for taking the time to chat with us about what you're working

41:55.600 --> 41:56.600
on.

41:56.600 --> 41:57.600
Very cool.

41:57.600 --> 42:02.840
Any parting thoughts or words for folks that might be interested in exploring this area

42:02.840 --> 42:03.840
further?

42:03.840 --> 42:12.000
I would say we find that all these machine learning stuff and the GPU computing solutions

42:12.000 --> 42:19.520
sounds like it's much more than faster computation, it's much more than better realization.

42:19.520 --> 42:26.480
It's actually making us thinking about or doing cell biology research in a completely

42:26.480 --> 42:33.280
new way, and we are also exploring different possibilities down this road, and that's

42:33.280 --> 42:40.000
something we are doing now and keep pushing forward into different directions.

42:40.000 --> 42:42.680
Well, once again, thanks so much.

42:42.680 --> 42:44.680
Thank you.

42:44.680 --> 42:50.320
All right, everyone, that's our show for today.

42:50.320 --> 42:56.160
For more information on today's show, visit twomolai.com slash shows.

42:56.160 --> 42:59.880
As always, thanks so much for listening, and catch you next time.

