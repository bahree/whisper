1
00:00:00,000 --> 00:00:13,840
Welcome to the Twimmel AI Podcast.

2
00:00:13,840 --> 00:00:26,000
Alright everyone, I am on the line with Andreas Madsen.

3
00:00:26,000 --> 00:00:32,000
Andreas is an independent researcher based out of Copenhagen, Denmark.

4
00:00:32,000 --> 00:00:35,000
Andreas, welcome to the Twimmel AI Podcast.

5
00:00:35,000 --> 00:00:37,000
Thank you. It's great to be here.

6
00:00:37,000 --> 00:00:38,000
It's great to chat with you.

7
00:00:38,000 --> 00:00:47,000
I first came across your name as a result of a medium post that you wrote about your experience

8
00:00:47,000 --> 00:00:55,000
becoming an independent researcher in the machine learning in AI field and ultimately getting published in ICLR

9
00:00:55,000 --> 00:00:59,000
with a spotlight paper and we're going to dig into all of that here.

10
00:00:59,000 --> 00:01:07,000
But let's get started with you sharing a little bit about your background and what motivated your interests in ML and AI.

11
00:01:07,000 --> 00:01:13,000
That's a long time ago, probably like nine years ago I think.

12
00:01:13,000 --> 00:01:23,000
I was reading a book called Programming Collective Intelligence, like all the way back in high school you would call it.

13
00:01:23,000 --> 00:01:29,000
That's the O'Reilly book that talks about collaborative filtering and stuff.

14
00:01:29,000 --> 00:01:35,000
Yeah, exactly. And I was reading this and at the time I wasn't really sure about what I would do for university.

15
00:01:35,000 --> 00:01:44,000
But after reading that, I mean I read it again and I was really sure this was the direction I wanted to go in.

16
00:01:44,000 --> 00:01:50,000
And so this was all the way before like machine learning really became like a popular subject.

17
00:01:50,000 --> 00:01:58,000
Uh-huh. And particularly at that point interested in how he did natural language processing.

18
00:01:58,000 --> 00:02:00,000
Okay. Yeah.

19
00:02:00,000 --> 00:02:04,000
They used non-negative matrix factorization.

20
00:02:04,000 --> 00:02:10,000
You did that now that will be a really old fashion.

21
00:02:10,000 --> 00:02:18,000
But nevertheless that was like how I started and then I did like apply mathematics at university and I just took all the machine learning courses.

22
00:02:18,000 --> 00:02:38,000
I could. And so you kind of pursued machine learning independently and in your blog post you you recommend that people avoid pursuing machine learning as independent researchers elaborate a little bit on your experience and why you make that recommendation.

23
00:02:38,000 --> 00:02:48,000
I mean if you just want to develop machine learning like in the industry and you don't necessarily want to do anything novel as we call it.

24
00:02:48,000 --> 00:02:52,000
Right. You can do that independently. I think that's totally fine.

25
00:02:52,000 --> 00:03:07,000
But if you want to do research, if you want to like develop new methods, then this is a really hard field to do it in because you need like huge computational resources that you're not going to have.

26
00:03:07,000 --> 00:03:17,000
And you probably also need to like collaborate with others who are really experts and who can stay up to date and like at least 300 papers a year as opposed.

27
00:03:17,000 --> 00:03:24,000
And you're not going to have like conversation with those kind of people because you really you're just kind of alone right.

28
00:03:24,000 --> 00:03:38,000
And also research in itself is just a very frustrating experience for everybody and also sort of to go through that alone without having like PST colleagues or such.

29
00:03:38,000 --> 00:03:46,000
I think it's not that healthy like mentally just to be kind.

30
00:03:46,000 --> 00:03:56,000
And mentally taxing just because of the isolation. Exactly. Like it's not that I don't have friends that I talk to right but they're not in the research business right.

31
00:03:56,000 --> 00:04:10,000
So maybe they cannot really emphasize with how it feels to get rejected after seven years or seven months of writing a paper on your own funding right.

32
00:04:10,000 --> 00:04:25,000
And so when you started to do this when you set out as an independent researcher was that coming right out of university or graduate school or did you leave a job to do this where were you coming from.

33
00:04:25,000 --> 00:04:40,000
Right. I think it's like two and a half years ago and right after that I started as kind of like a freelancer at this company called near form that basically just do like open source JavaScript.

34
00:04:40,000 --> 00:04:45,000
They're really good at that. And at the time it was kind of like fit up with academia.

35
00:04:45,000 --> 00:04:52,000
And I didn't want to do a PST at that point. I think it's my master just for context.

36
00:04:52,000 --> 00:05:02,000
Yeah. And I had this opportunity that was like very perfect for me in terms of my skills. And in that sense I could get paid quite a lot.

37
00:05:02,000 --> 00:05:09,000
And I didn't quite know what I wanted to do next. So doing this for like three or six months was kind of perfect for me.

38
00:05:09,000 --> 00:05:22,000
And JavaScript JavaScript freelancing. Yeah, exactly. Yeah. Yeah. Yeah. So like you're talking about creating like a profiling tool for programs or JavaScript programs.

39
00:05:22,000 --> 00:05:29,000
So it's not like your typical kind of JavaScript is. And then after that they were like, okay, like this party is great.

40
00:05:29,000 --> 00:05:40,000
If you want to do another six months sort of focusing on machine learning projects that are sort of minor and then I did that for six months and that was then the end of it.

41
00:05:40,000 --> 00:05:49,000
Because it didn't really materialize into them getting a client there and an outsourcing company just to give some context.

42
00:05:49,000 --> 00:05:57,000
And that was sort of also what I expected. So during that time I had already been working on this paper here called

43
00:05:57,000 --> 00:06:05,000
Visualizing Memorization in the colonial networks. There was also one of the projects I did for them.

44
00:06:05,000 --> 00:06:11,000
I just proposed this just by the paper as like marketing effort.

45
00:06:11,000 --> 00:06:19,000
And so it's been like one month from that with them. And then I just finished it on my own time. After that.

46
00:06:19,000 --> 00:06:34,000
Yeah. And so then I thought after that, I got published in the still with this paper here. I thought like I would have a chance at getting like a research software engineer position where you kind of do both engineering and machine research.

47
00:06:34,000 --> 00:06:47,000
Because I thought like I was a good mix of both. But that turned out to be really difficult. Then I look into getting PST positions into PST programs because that's sort of the unwritten requirement.

48
00:06:47,000 --> 00:07:02,000
And this was where I was told like this publication you have right now is not enough. You need to have like at least one top tier publication right or publication in like a top venue conference.

49
00:07:02,000 --> 00:07:08,000
Right. This was someone telling you that you needed the publication and the top tier conference to answer.

50
00:07:08,000 --> 00:07:24,000
They get a top advisor, right? No, not necessarily a top advisor. What was the context for that statement? I talked to like five different professors from different universities and they were like, yeah, it's like really competitive.

51
00:07:24,000 --> 00:07:37,000
It's not that you're going to be more qualified. Like your abilities will be that much greater because you had this publication is just, you know, that is the field that they use because they get like a thousand.

52
00:07:37,000 --> 00:07:42,000
Applications. And so that's really just the host reality.

53
00:07:42,000 --> 00:08:02,000
And so in spite of being a little fed up with with academia, you embarked on, you know, what is in many ways an academic pursuit to pursue research independently, push your paper and your your ideas a little bit further.

54
00:08:02,000 --> 00:08:14,000
How did you get from the mapping or visualizing paper to the neural arithmetic unit idea?

55
00:08:14,000 --> 00:08:35,000
To an assistant researcher at the same community University of Denmark here that I live quite close to. It was like at an opening for a foundation, I think, and he was just talking like if you're interested in doing some kind of research, maybe we can collaborate on that.

56
00:08:35,000 --> 00:08:44,000
And so I worked with him and we just talked about different kind of projects. And I felt this project here was kind of like in my.

57
00:08:44,000 --> 00:08:53,000
Wheelhouse because I had already done quite a lot of optimization work, not research, but done a lot of study in that field.

58
00:08:53,000 --> 00:09:03,000
Well, some of the comments that you got suggest that, you know, it's not necessarily not necessarily easy to find the willing collaborator if you don't have that credential.

59
00:09:03,000 --> 00:09:10,000
So you kind of locked out there, maybe I mean, he's not a professor. He's not even a PC program, right?

60
00:09:10,000 --> 00:09:13,000
Okay.

61
00:09:13,000 --> 00:09:20,000
He's just sort of assisting students with the master thesis and stuff like that, right? Got it.

62
00:09:20,000 --> 00:09:36,000
And during a course they were running, they were trying to reproduce this paper here that we will probably talk about Nellow neural arithmetic, lot of units. And they were really struggling to that and like nobody could reproduce it consistently.

63
00:09:36,000 --> 00:09:40,000
And so he thought that might be an interesting research subject.

64
00:09:40,000 --> 00:09:46,000
So I think like yes, I did log out a bit, but not that much, I think.

65
00:09:46,000 --> 00:10:09,000
Okay. So what, you know, before we kind of jump into the topic of the paper, you know, you kind of suggest that this is a hard way to go the kind of independent researcher life, but you were able to successfully get your paper written and accepted at ICLR.

66
00:10:09,000 --> 00:10:26,000
You know, what works for you? What, you know, someone else wants to, you know, disavow your advice, not to pursue this path, but, you know, is open to your advice on what to do if you do decide to go down this road, you know, what would that advice be?

67
00:10:26,000 --> 00:10:44,000
I mean, you definitely need to, to collaborate because you're not going to go out of university and be an expert at Chris, criticizing your own work. Even the ones that been in the academia for like 10 years, I don't think are experts at that, right?

68
00:10:44,000 --> 00:10:56,000
Just inherently resistant to criticizing your own work. I think it's a fundamental property of being human. And so you need to collaborate with somebody who can do that. This not necessarily has to be a professor, I think.

69
00:10:56,000 --> 00:11:15,000
Like anybody who has experienced reading machine learning papers and criticize them, I think we'll suit just fine. And then I think like also have reasonable expectations, like if you're going in with the expectation that there's like an eight percent change of success, like you're delusional.

70
00:11:15,000 --> 00:11:29,000
Like, sorry, like the success rate in these conferences is like 20 percent, right? And you're probably going to have like less of a chance than that, because you don't have the resources everybody else have.

71
00:11:29,000 --> 00:11:37,000
And so you should expect to get rejected from your first summation, I think. And I was rejected.

72
00:11:37,000 --> 00:11:49,000
First, I proposed this paper here to to no lips, right? I got rejected from that. And then I could use that peer review to improve the paper and then submit for for I clear. And then we then got accepted.

73
00:11:49,000 --> 00:11:59,000
And even that, I think it's not not a guarantee, right? I mean, could also been rejected there could also have been less lucky with the peer reviewers.

74
00:11:59,000 --> 00:12:11,000
We're very fortunate to have actually the same one of the same peer viewers that have from new lips who are very constructive and could see that we had made like really good progress.

75
00:12:11,000 --> 00:12:23,000
And I think also they themselves had also had like a few months to actually think about this work and also be able to think a bit more critical about the competing paper.

76
00:12:23,000 --> 00:12:31,000
This was also an open review, so he was sort of like comment on other people's reviews, those are really helpful.

77
00:12:31,000 --> 00:12:39,000
So there's like almost once in a lifetime, you get that kind of really constructive people, you're like again, like loggy there, I think.

78
00:12:39,000 --> 00:12:45,000
So tell us a little bit about the paper. What's the motivation for the research?

79
00:12:45,000 --> 00:12:57,000
Right. And so it really comes down to very abstract weapon that is neural networks are really good at interpolating. They can create very complex approximations.

80
00:12:57,000 --> 00:13:11,000
But they're not very good at extrapolating, right? It's a classic example with for example image recognition, you put in like a white pixel somewhere and suddenly it goes from being a separate to being a couch.

81
00:13:11,000 --> 00:13:15,000
And that really comes down to neural networks and are very good at extrapolating.

82
00:13:15,000 --> 00:13:27,000
And what specifically do you mean by that? Because in a lot of ways extrapolating is what we expect machine learning to do, like make predictions based on data that it's seen. That's kind of extrapolation.

83
00:13:27,000 --> 00:13:41,000
It's probably interpolation, right? Well, you're seeing pictures of of separas before, but there never been like a dead pixel in any of these pictures here.

84
00:13:41,000 --> 00:13:47,000
So it's not it's filler with the concept of of separas, but it's not familiar with the concept of a dead pixel.

85
00:13:47,000 --> 00:14:01,000
Right. And that's really where you go from interpolation, like this is just another picture of a separate that is never seen before this interpolation. But as soon as you add like a dead pixel, then you're now into a new realm of extrapolation.

86
00:14:01,000 --> 00:14:08,000
And it's not an easy concept, I think, to understand when do you go from interpolation to extrapolation.

87
00:14:08,000 --> 00:14:15,000
But mathematically, at least there's no reason to expect the neural network to perform well or an extrapolation tasks.

88
00:14:15,000 --> 00:14:28,000
And so the research was on what measuring networks ability to do extrapolation or coming up with methods that help them to do extrapolation.

89
00:14:28,000 --> 00:14:45,000
Right. So, so what is the deep mind paper here by Andrew Tresky, the nano paper, and proposes to look at like a subset of problems where the underlying problem has some fundamental arithmetic to it.

90
00:14:45,000 --> 00:14:57,000
So some combination of addition and multiplication, let's say. And that you will have example in physical models in financial models or some question answering models.

91
00:14:57,000 --> 00:15:08,000
Like what is three times five. And given that assumption, can you then develop a null unit similar to, for example, LSM.

92
00:15:08,000 --> 00:15:20,000
But in this case for like a rhythmic extrapolation that given this training data can actually work on a numerical values that is outside of the training distribution.

93
00:15:20,000 --> 00:15:31,000
And so that's the central problem here. And then there's the deep mind paper now that they propose one solution, we then find that this does not converge very consistently.

94
00:15:31,000 --> 00:15:39,000
So it does work sometimes, but if you're training like 100 different seats, then it might only work on 10 or one of those seats.

95
00:15:39,000 --> 00:15:48,000
And what we are doing is then to improve that consistency, such that it works on almost all the seats, like 99 or 100.

96
00:15:48,000 --> 00:15:51,000
How did you go about doing that?

97
00:15:51,000 --> 00:15:54,000
Right.

98
00:15:54,000 --> 00:16:07,000
I think like the, the Nell papers was a very, it is a very empirical paper. It just proposes the small here doesn't really give any kind of explanation behind why it looks exactly like this.

99
00:16:07,000 --> 00:16:23,000
And then it evaluates that on seven different tasks. And so a lot of the typical mathematical analysis you might put into proposing a new unit wasn't really done.

100
00:16:23,000 --> 00:16:31,000
So even simple things like how do you initialize the weights was a question that was not answered and probably not analyzed.

101
00:16:31,000 --> 00:16:37,000
And so what we looked at this example in the station as I mentioned also gradients.

102
00:16:37,000 --> 00:16:41,000
You can also just plot like the law space.

103
00:16:41,000 --> 00:16:50,000
If there's some odd behavior here, in this case, we found like big area of singularities around global minimus.

104
00:16:50,000 --> 00:16:59,000
That's a really give you an indication that something going wrong also redundant parameters can also be a problem when optimizing.

105
00:16:59,000 --> 00:17:04,000
Like if you can somehow simplify this units while still achieving the same things.

106
00:17:04,000 --> 00:17:07,000
Maybe that's an easier to optimize.

107
00:17:07,000 --> 00:17:16,000
And so these are all the properties we looked at just analyze them one by one and then we sort of OK identified that these are problematic areas.

108
00:17:16,000 --> 00:17:25,000
And that's really the easy part. Then the hard part is then to come up with a new unit that doesn't have these issues.

109
00:17:25,000 --> 00:17:33,000
There's there was different components to this Nellow unit here. So there's like a part they can do additions, part they can do modifications.

110
00:17:33,000 --> 00:17:38,000
And then they have like a gaining unit that can switch between these two parts here.

111
00:17:38,000 --> 00:17:47,000
And what we also did was we for example could say OK let's just say it's a multiplication problem which fix the gaining to always choose modification.

112
00:17:47,000 --> 00:17:50,000
How well does that improve anything.

113
00:17:50,000 --> 00:18:00,000
And so this is what you probably call like an emblaziness study where you remove some of the complexity and then you'll see if it still works or if it works better.

114
00:18:00,000 --> 00:18:06,000
And they are there also for example find that the gate mechanism that they're using really doesn't work as it should.

115
00:18:06,000 --> 00:18:14,000
It more or less just chooses other addition or multiplication randomly independent of what the actual problem is.

116
00:18:14,000 --> 00:18:16,000
That doesn't sound like what you wanted to do.

117
00:18:16,000 --> 00:18:24,000
No, no, of course. Things to convert dependent on your data.

118
00:18:24,000 --> 00:18:31,000
And so you develop this unit with kind of a different arrangement of gates.

119
00:18:31,000 --> 00:18:33,000
Is that the general idea?

120
00:18:33,000 --> 00:18:42,000
We actually skipped the gating problem so this is still on salt right to me if you want to solve it.

121
00:18:42,000 --> 00:18:52,000
And then we just focused on getting multiplication to work more consistently because that only converts like 30% of the times.

122
00:18:52,000 --> 00:18:55,000
That I will say our primary contribution.

123
00:18:55,000 --> 00:19:03,000
So does that mean you you punted on arithmetic problem or did you do that independently but the network didn't have to choose which one I was doing.

124
00:19:03,000 --> 00:19:11,000
Like we said OK you have to do multiplication or you have to do addition it didn't like understand that itself.

125
00:19:11,000 --> 00:19:13,000
Yeah OK got it.

126
00:19:13,000 --> 00:19:15,000
It turns out to be hard enough problem itself.

127
00:19:15,000 --> 00:19:27,000
Yeah, like what was happening before was really just it chose like randomly and then you know if you turn a train on enough different seats then one of the times it's going to hit the right thing.

128
00:19:27,000 --> 00:19:40,000
Yeah and and our idea here was essentially to make it more linear linear functions or at least partial linear functions are typically more easy to optimize than very not linear function.

129
00:19:40,000 --> 00:19:50,000
So the narrow function is essentially a exponential function so it has like X to the power of your weight stop you.

130
00:19:50,000 --> 00:19:55,000
And then if W zero then it's not selected and if W is one then you multiply.

131
00:19:55,000 --> 00:20:06,000
And where in our case we we placed that concept with essentially gaining mechanism between one and X.

132
00:20:06,000 --> 00:20:14,000
And so this is easier to optimize because it's a little more linear.

133
00:20:14,000 --> 00:20:31,000
And so did you also apply this to a variety of different benchmark tasks or were you strictly evaluating performance on kind of this mathematical operation.

134
00:20:31,000 --> 00:20:45,000
Right so the the number of people who posted I think six or seven different tasks and I mean I'll just be honest here if most of those tasks are not relevant to what they're actually solving.

135
00:20:45,000 --> 00:20:56,000
Because they do not they do not test extrapolation and they do not test multiplication only one of those seven tasks actually test extrapolation on multiplication.

136
00:20:56,000 --> 00:21:06,000
And so that is a task we benchmarked on and that is quite a simple task so we also took one of the other tasks they had.

137
00:21:06,000 --> 00:21:13,000
And then we modified that a bit so it would also test multiplication instead of only addition.

138
00:21:13,000 --> 00:21:25,000
And so how are these tasks set up is pretty simple you have like one part that is like you have a hundred different inputs and then you take two subsets of this input here and then you sum that together.

139
00:21:25,000 --> 00:21:31,000
Now you have two numbers and then you multiply those two numbers and that's your target.

140
00:21:31,000 --> 00:21:40,000
And then this is pretty easy to test extrapolation on because then you just sample your input from a different range than you trained on.

141
00:21:40,000 --> 00:21:50,000
And then we have a more complex task because well you can show that this works really well but what if you had this in the context of a more complex neural network then maybe it wouldn't converge.

142
00:21:50,000 --> 00:22:01,000
And so what we did was also looked at endless numbers where we take like a sequence of endless numbers for example and then we try and add them together or multiply them together.

143
00:22:01,000 --> 00:22:16,000
And then of course you need like a convolution neural network to translate the image to a number and then you have our unit to then translate those numbers or multiply those numbers.

144
00:22:16,000 --> 00:22:36,000
Presumably the problems are set up as supervised problems so you're training on you said in the case of the first task a different range of the inputs and then you're running your testing against a range at the.

145
00:22:36,000 --> 00:22:53,000
Yeah, this is supervised to regression problems. Yeah, how do you ultimately perform on the various tests? I mean we perform quite a bit better so what's the right way to even measure performance on these kinds of tasks.

146
00:22:53,000 --> 00:23:14,000
Right. That I think is a very good and central question because there's not been a lot of work in extrapolation and we actually did two papers in this a workshop paper neural lips and where we actually just discussed how do you evaluate this and then our I clear people where we improve the model.

147
00:23:14,000 --> 00:23:37,000
And I think the fundamental idea you need to have so the never paper is they just evaluated on one seat and they just show the mean squared error and mean squared errors are great for comparing is this model better than this model but that is not really was interesting here because if you truly can extrapolate that means you found the perfect solution you're complete a solid problem.

148
00:23:37,000 --> 00:23:50,000
And you should have a mean square error that is very close to zero whatever error that is left is really just due to like computer precision if you follow the problem. Yeah, exactly.

149
00:23:50,000 --> 00:24:13,000
What the NL people does did was they compare this to a random all and as you can imagine a random all can have quite a big error especially if you multiply different numbers can the target becomes like a million and then you have a random all that produces something that's a million of that and then you square that and not something you have a like a really big.

150
00:24:13,000 --> 00:24:36,000
And so comparing really relative to this random model doesn't really make sense. So what we didn't status compare to a nearly perfect model so we simply just like develop the perfect model and then we add like a little epsilon to the weights and then we use that as a threshold for what the means squared error should be.

151
00:24:36,000 --> 00:24:49,000
And that's all about success criteria and once you have the success criteria then you can just do this a hundred times on a hundred different seats and then you can measure the success rate.

152
00:24:49,000 --> 00:25:02,000
Okay, I'm not sure I'm following the process here so you said you are comparing to a nearly perfect model. I mean the nearly perfect model is what you're trying to figure out in the first place.

153
00:25:02,000 --> 00:25:12,000
And you have you know that you know the absolute answer for multiplying the numbers where does this nearly perfect model come from.

154
00:25:12,000 --> 00:25:20,000
Well, it's really just the absolute value as you call it from multiplying the numbers like plus an epsilon.

155
00:25:20,000 --> 00:25:29,000
Got it. So you're you're taking the result that you know to be true and adding some noise to it and you're training against that.

156
00:25:29,000 --> 00:25:41,000
Exactly now this epsilon here you of course need to adjust to reflect the number of operations equivalently to perform this calculation.

157
00:25:41,000 --> 00:25:47,000
And why do you add the noise in the first place? Well, how does that help your model converge presumably that's why.

158
00:25:47,000 --> 00:25:52,000
No, it's not it's not added to the problem itself.

159
00:25:52,000 --> 00:26:01,000
It's added to the criteria. Okay, or the top like what do you actually like you get some mean squared error.

160
00:26:01,000 --> 00:26:04,000
And that's very close to to zero.

161
00:26:04,000 --> 00:26:08,000
And like let's say.

162
00:26:08,000 --> 00:26:19,000
But is this you you're not adding this in your or are you adding this in like your loss function or is it just after you train a model in the way you present the results.

163
00:26:19,000 --> 00:26:29,000
It's just a way of evaluating like the results like you need need to compare the mean squared error to something you cannot compare to zero because it's never going to be zero right.

164
00:26:29,000 --> 00:26:38,000
And so you need to compare to some epsilon and in order to compute that epsilon you essentially just run a perfect model on the problem.

165
00:26:38,000 --> 00:26:46,000
Except it's not a perfect model. It has like a really tiny error and then you get a really tiny mean squared error and that's what you compare against.

166
00:26:46,000 --> 00:26:57,000
Besides the the Nallu paper there are other results that were out there to compare against or was that the kind of state of the art in this space.

167
00:26:57,000 --> 00:27:01,000
It's a very new feel so that is the state of the art.

168
00:27:01,000 --> 00:27:11,000
There are other papers that are sort of also doing a written text or for example neural GPU is kind of a popular paper.

169
00:27:11,000 --> 00:27:16,000
That paper they're trying to learn algorithms. That's a bit different.

170
00:27:16,000 --> 00:27:20,000
And also they don't focus on extrapolation, I believe.

171
00:27:20,000 --> 00:27:24,000
But that's like the most similar.

172
00:27:24,000 --> 00:27:27,000
And but really it is just that paper.

173
00:27:27,000 --> 00:27:36,000
Yeah, that's also part that they made a attractive for me to research because they didn't need to spend like half a year just understanding the background.

174
00:27:36,000 --> 00:27:42,000
Yeah, yeah. What are kind of your next steps beyond this paper?

175
00:27:42,000 --> 00:27:54,000
Right now I'm just focused on trying to get into more permanent situation, a PSD program or a job or something similar.

176
00:27:54,000 --> 00:28:00,000
That's sort of my focus right now. Then I'm doing a collaboration with one from DeepMind.

177
00:28:00,000 --> 00:28:06,000
Unlike what will probably be a small workshop paper, but not Andrew Trask.

178
00:28:06,000 --> 00:28:09,000
Not Andrew Trask, no.

179
00:28:09,000 --> 00:28:15,000
Who have interviewed before on the podcast incidentally, it was at NURBS a couple of years ago, I think.

180
00:28:15,000 --> 00:28:16,000
Okay.

181
00:28:16,000 --> 00:28:21,000
But we talked about his privacy stuff and open mind and not any of this.

182
00:28:21,000 --> 00:28:25,000
Yeah, he really been public on that this year, I think.

183
00:28:25,000 --> 00:28:30,000
More into interpretability, I think that's a really important problem.

184
00:28:30,000 --> 00:28:40,000
We're creating all these very complex models here, but we how do we can we rely on them actually working.

185
00:28:40,000 --> 00:28:48,000
And so this sort of already is around what I'm already working on like I did the visualization of memorization.

186
00:28:48,000 --> 00:28:56,000
That is an empowered interpretability and also my work here relating to NALO is also about extrapolation,

187
00:28:56,000 --> 00:29:03,000
which is essentially about like can you trust that this model is also going to work once I see this data that it has not seen before.

188
00:29:03,000 --> 00:29:13,000
And this is something I wanted to do for a long time, but I was always discouraged from pursuing it when I was at university

189
00:29:13,000 --> 00:29:18,000
because they didn't really feel like you could publish in the big conferences and that subject.

190
00:29:18,000 --> 00:29:29,000
But Dennis was at NURBS 2019 because I did a workshop publication and saw this talk here by being came from Google.

191
00:29:29,000 --> 00:29:42,000
And I think she really like illustrated for me that it is possible to do like high quality research in this area here and get published at conferences that are used for your career.

192
00:29:42,000 --> 00:29:45,000
And so that is really what I kind of want to pursue.

193
00:29:45,000 --> 00:29:54,000
But preferably with with an organization, you know, whether academic or industry as opposed to independent.

194
00:29:54,000 --> 00:30:00,000
Yeah, probably.

195
00:30:00,000 --> 00:30:07,000
So you did it, you wouldn't recommend it and you're taking your own advice.

196
00:30:07,000 --> 00:30:15,000
Exactly. Well, Andreas, thanks so much for taking the time to share a little bit about your journey and research.

197
00:30:15,000 --> 00:30:19,000
Yeah, thank you.

198
00:30:19,000 --> 00:30:28,000
All right, everyone, that's our show for today. For more information on today's show, visit twomolai.com slash shows.

199
00:30:28,000 --> 00:30:38,000
As always, thanks so much for listening and catch you next time.

