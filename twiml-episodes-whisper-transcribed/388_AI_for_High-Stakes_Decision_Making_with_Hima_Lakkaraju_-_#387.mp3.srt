1
00:00:00,000 --> 00:00:13,120
Welcome to the Twimal AI Podcast.

2
00:00:13,120 --> 00:00:17,120
I'm your host Sam Charrington.

3
00:00:17,120 --> 00:00:26,920
Hey, what is up, good Twimal people?

4
00:00:26,920 --> 00:00:32,160
Before we jump into today's show from our CVPR series, I'd like to share a few quick

5
00:00:32,160 --> 00:00:39,120
details about the next great event in our continuing live discussion series.

6
00:00:39,120 --> 00:00:45,920
Join us on Wednesday, July 1st for the great machine learning language on debate as we

7
00:00:45,920 --> 00:00:52,400
explore the strengths, weaknesses and approaches of both popular and emerging programming languages

8
00:00:52,400 --> 00:00:54,360
for machine learning.

9
00:00:54,360 --> 00:01:01,320
People have great speakers representing Python, R, Swift, Closure, Scala, Julia and more.

10
00:01:01,320 --> 00:01:06,760
The session kicks off at 11 a.m. Pacific time on the first and you won't want to miss

11
00:01:06,760 --> 00:01:13,560
it, so head over to twimalai.com slash languages to get registered.

12
00:01:13,560 --> 00:01:18,280
At this point, I'd like to send a huge thank you to our friends at Qualcomm for their

13
00:01:18,280 --> 00:01:24,120
support of this podcast and their sponsorship of our CVPR series.

14
00:01:24,120 --> 00:01:30,680
Qualcomm AI research is dedicated to advancing AI to make its core capabilities, perception,

15
00:01:30,680 --> 00:01:34,840
reasoning and action ubiquitous across devices.

16
00:01:34,840 --> 00:01:39,120
Their work makes it possible for billions of users around the world to have AI enhanced

17
00:01:39,120 --> 00:01:43,520
experiences on Qualcomm technology's powered devices.

18
00:01:43,520 --> 00:01:49,440
To learn more about Qualcomm and what they're up to on the AI research front, visit twimalai.com

19
00:01:49,440 --> 00:01:52,120
slash Qualcomm.

20
00:01:52,120 --> 00:01:55,400
Now on to the show.

21
00:01:55,400 --> 00:01:56,400
All right, everyone.

22
00:01:56,400 --> 00:01:58,440
I am here with HEMA Lockaraju.

23
00:01:58,440 --> 00:02:04,000
HEMA is an assistant professor at Harvard University with joint appointments in both the

24
00:02:04,000 --> 00:02:07,360
business school and the Department of Computer Science.

25
00:02:07,360 --> 00:02:09,840
HEMA, welcome to the Twimal AI podcast.

26
00:02:09,840 --> 00:02:10,840
Thank you so much, Sam.

27
00:02:10,840 --> 00:02:12,800
I'm super excited to be here.

28
00:02:12,800 --> 00:02:13,800
Same here.

29
00:02:13,800 --> 00:02:16,760
I'm really looking forward to our conversation.

30
00:02:16,760 --> 00:02:23,160
We will start where we typically do on the show and have you share a little bit about

31
00:02:23,160 --> 00:02:29,600
your background and how you came to work in machine learning in particular, your focus

32
00:02:29,600 --> 00:02:39,320
on fair and interpretable ML and the implications and kind of mission critical high stakes domains

33
00:02:39,320 --> 00:02:42,800
like criminal justice, healthcare and public policy.

34
00:02:42,800 --> 00:02:44,800
How did you get started in all this?

35
00:02:44,800 --> 00:02:48,560
So, that's actually an interesting story.

36
00:02:48,560 --> 00:02:55,400
Let me try and kind of summarize it in a hopefully few sentences so that we are not hogging

37
00:02:55,400 --> 00:02:56,960
all the time here.

38
00:02:56,960 --> 00:03:01,760
So yeah, I was actually working in machine learning, right?

39
00:03:01,760 --> 00:03:08,320
So basically I come from India, I moved to the United States for my PhD in 2012.

40
00:03:08,320 --> 00:03:13,280
I have been working in machine learning since I was a student in India.

41
00:03:13,280 --> 00:03:19,360
I was publishing actively in machine learning, but my interest in sort of the applications

42
00:03:19,360 --> 00:03:25,000
of machine learning to some of these domains, like criminal justice or healthcare, that

43
00:03:25,000 --> 00:03:30,920
sort of started or it became a prominent threat in my research when I started my PhD.

44
00:03:30,920 --> 00:03:38,840
And this was, I guess, mostly due to a collaboration now between my advisor and another professor,

45
00:03:38,840 --> 00:03:43,560
and I have a couple of professors and economics who were dealing with behavioral economics,

46
00:03:43,560 --> 00:03:48,120
and they sort of introduced us to all these fascinating problems.

47
00:03:48,120 --> 00:03:52,360
As I was sort of, I think, by then I had already explored machine learning to a reasonable

48
00:03:52,360 --> 00:03:59,080
extent, and I was looking for applications which were more than just like ad recommendations

49
00:03:59,080 --> 00:04:04,480
or, you know, friendship recommendations and so on so that I could keep myself going

50
00:04:04,480 --> 00:04:10,200
in the field and also anchor on to something which is more applied in the sense of, like,

51
00:04:10,200 --> 00:04:11,760
real world settings and so on.

52
00:04:11,760 --> 00:04:18,720
So I guess my PhD was like one of the, you know, main times in my life where I got into

53
00:04:18,720 --> 00:04:24,600
both machine learning as well as its applications to some of these domains which are super fascinating

54
00:04:24,600 --> 00:04:29,440
and the broad field of fairness and interpretability enabled, yeah.

55
00:04:29,440 --> 00:04:37,880
You know, I suspect when we dig into your upcoming, well, actually your recent talk at CVPR

56
00:04:37,880 --> 00:04:43,440
where you were an invited speaker in the fair data efficient and trusted computer vision

57
00:04:43,440 --> 00:04:53,400
workshop, we will learn about a bit of your research, but kind of broadly, how do you frame

58
00:04:53,400 --> 00:04:58,360
out the kinds of questions that you're looking to answer with your work?

59
00:04:58,360 --> 00:04:59,360
All right.

60
00:04:59,360 --> 00:05:05,960
I guess in a broad sense the way I think about my research is it's about enabling machine

61
00:05:05,960 --> 00:05:11,240
learning to help with decision making in high stakes settings, right, and that involves

62
00:05:11,240 --> 00:05:17,320
some sub questions like how can we make sure that machine learning models which are of course

63
00:05:17,320 --> 00:05:23,440
getting more and more complex day by day are, you know, more palatable form to these decision

64
00:05:23,440 --> 00:05:27,040
makers who are not necessarily experts in machine learning.

65
00:05:27,040 --> 00:05:32,400
So how do we sort of explain these models, what are the algorithms that we can use which

66
00:05:32,400 --> 00:05:36,160
can in turn explain these models to people who are not machine learning experts.

67
00:05:36,160 --> 00:05:41,000
So that's of course one of the key questions and also other core question behind interpretable

68
00:05:41,000 --> 00:05:47,800
ML, right, and beyond that when we develop some of these tools which will assist these

69
00:05:47,800 --> 00:05:53,360
decision makers in important decisions, how do we ensure that the tools or the algorithms

70
00:05:53,360 --> 00:05:58,840
that we are developing are by default fair otherwise they can induce their own discriminatory

71
00:05:58,840 --> 00:06:04,160
biases and undesirable biases into the entire real world decision making.

72
00:06:04,160 --> 00:06:09,560
So that's of course another question and more broadly also just trying to develop models

73
00:06:09,560 --> 00:06:16,800
and methods to understand what kinds of biases exist as of now in human decision making

74
00:06:16,800 --> 00:06:21,600
like even if there was no algorithm involved in the picture as well as how to diagnose

75
00:06:21,600 --> 00:06:26,440
biases if someone gives me an algorithm, what is the best way to do that.

76
00:06:26,440 --> 00:06:29,120
So these are roughly the broad questions I think about.

77
00:06:29,120 --> 00:06:35,120
And so your talk is titled Understanding the Limits of Explainability in ML Assisted

78
00:06:35,120 --> 00:06:40,200
Decision Making and there's some interesting tidbits that I'm looking forward to digging

79
00:06:40,200 --> 00:06:46,800
into around like some of the explainability algorithms like Lyme and Shap, but before we

80
00:06:46,800 --> 00:06:52,840
even do that, thinking about the topic of your talk and the workshop makes me think of

81
00:06:52,840 --> 00:07:01,840
a podcast that I did with Cynthia Rudin last year and her perspective seems to come from

82
00:07:01,840 --> 00:07:06,600
a different direction which is we shouldn't even be using black box models for the kinds

83
00:07:06,600 --> 00:07:08,960
of problems that you're studying.

84
00:07:08,960 --> 00:07:17,560
We should be using models that are kind of more fundamentally understandable and in many

85
00:07:17,560 --> 00:07:21,320
conversations I've had in this topic, there's this tension between explainability and

86
00:07:21,320 --> 00:07:22,320
interpretability.

87
00:07:22,320 --> 00:07:28,400
So I'm curious kind of out of the gate, what's your take on all that?

88
00:07:28,400 --> 00:07:34,440
Oh, I think we're already starting with a very interesting and controversial topic.

89
00:07:34,440 --> 00:07:39,880
Yeah, I mean, Cynthia has been a mentor and a collaborator for several years but we somehow

90
00:07:39,880 --> 00:07:43,760
managed to coexist with this dichotomy.

91
00:07:43,760 --> 00:07:50,040
So yeah, I mean, so I agree with this point or rather might take on this would be that

92
00:07:50,040 --> 00:07:56,000
if at all it is possible for you to develop a model that is interpretable by default

93
00:07:56,000 --> 00:08:01,440
and is also accurate and you have the data to build such a model by all means you should

94
00:08:01,440 --> 00:08:03,120
go for it, right?

95
00:08:03,120 --> 00:08:08,240
So because there are no barriers here, but unfortunately the real world is always not

96
00:08:08,240 --> 00:08:09,240
like that.

97
00:08:09,240 --> 00:08:13,480
So in some cases you may not have enough training data for example to build a disease

98
00:08:13,480 --> 00:08:15,400
diagnosis model, right?

99
00:08:15,400 --> 00:08:21,680
So you might be then using another proprietary model that some other company has built and

100
00:08:21,680 --> 00:08:27,680
in that case you would still want to do some diagnostic checks to ensure that the model

101
00:08:27,680 --> 00:08:32,160
is you're doing what it's supposed to do and it's put the way it's making predictions

102
00:08:32,160 --> 00:08:34,320
is reasonable and so on.

103
00:08:34,320 --> 00:08:41,640
So those are the kinds of cases where explaining a given model or a black box model as we are

104
00:08:41,640 --> 00:08:45,440
calling it, you know, is probably the only option, right?

105
00:08:45,440 --> 00:08:50,320
So because you don't have the ability to sort of build such a model because of lack of

106
00:08:50,320 --> 00:08:56,880
data or resources and empty number of other reasons, but you have the capability to buy

107
00:08:56,880 --> 00:09:02,480
or get this model from a third party, but you still want to wet it or understand what's

108
00:09:02,480 --> 00:09:08,080
roughly going on with the tiny bit of data that you have or like just doing some diagnostic

109
00:09:08,080 --> 00:09:12,240
checks with whatever little amount of data that you have which may not be enough to develop

110
00:09:12,240 --> 00:09:17,560
and accurate model, but at the same time might be, you know, decent enough to sort of wet

111
00:09:17,560 --> 00:09:18,960
a given model.

112
00:09:18,960 --> 00:09:24,120
So if that is the context you're dealing with, then essentially explaining or understanding

113
00:09:24,120 --> 00:09:27,760
what the black box might be doing is probably the only option.

114
00:09:27,760 --> 00:09:32,600
So these are of course, like, you know, again, as I mentioned earlier, these are constraints

115
00:09:32,600 --> 00:09:37,960
that arise in the real world and that's what we are sort of thinking about, but yes,

116
00:09:37,960 --> 00:09:43,920
if you have the means to develop an interpretable model from scratch, you have all the data,

117
00:09:43,920 --> 00:09:48,840
you have the necessary means, I think that is definitely and it's also accurate of course.

118
00:09:48,840 --> 00:09:53,120
So that is definitely the way to go, but just that there are many other real world contexts

119
00:09:53,120 --> 00:09:57,240
where that might not be the case which you could actually pursue.

120
00:09:57,240 --> 00:10:04,040
In the scenarios where you can't do something that is more fundamentally transparent, you're

121
00:10:04,040 --> 00:10:12,920
doing something that's a black box and you want to explain it, there are some kind of

122
00:10:12,920 --> 00:10:18,280
known and popular methods for achieving some level of explainability and I mentioned

123
00:10:18,280 --> 00:10:24,880
a couple of those all right, lime and sap, but a part of your presentation references

124
00:10:24,880 --> 00:10:30,360
and previous research that you've done that has shown that, you know, that work can

125
00:10:30,360 --> 00:10:37,120
be vulnerable to attack, you know, with your presentation, what's kind of the broad landscape

126
00:10:37,120 --> 00:10:43,600
that you're looking to carve out and we'll get to the particular areas of lime and

127
00:10:43,600 --> 00:10:48,920
sap when we get to them, but you know, how do you, how do you kind of frame the, you

128
00:10:48,920 --> 00:10:53,800
know, the problem of kind of understanding the limits of the explainability tools?

129
00:10:53,800 --> 00:11:02,120
Yeah, so I think this talk and more broadly my recent research has been kind of exploring

130
00:11:02,120 --> 00:11:07,960
as we are thinking about the limits of explainability and what I mean by that is so far at least

131
00:11:07,960 --> 00:11:12,640
in the past few years, there has been a lot of interest in coming up with new algorithms

132
00:11:12,640 --> 00:11:14,520
which can explain black boxes, right?

133
00:11:14,520 --> 00:11:19,400
So there is like a huge research that has built up I think since 2016, pretty much like

134
00:11:19,400 --> 00:11:24,480
paper on top of papers, every paper comes up with another new method for explaining

135
00:11:24,480 --> 00:11:27,760
a given black box classifier or a prediction model.

136
00:11:27,760 --> 00:11:32,920
So one of the things that as we were seeing more and more work on this topic that, you

137
00:11:32,920 --> 00:11:38,480
know, me and some of my collaborators of this work, we got excited about is how to now

138
00:11:38,480 --> 00:11:45,040
start thinking about what are all the ways in which these explanation techniques can be

139
00:11:45,040 --> 00:11:51,960
gained or potentially even unintentionally misused to sort of generate explanations which

140
00:11:51,960 --> 00:11:58,480
could fool people or mislead and users into trusting something that they should not trust,

141
00:11:58,480 --> 00:11:59,480
right?

142
00:11:59,480 --> 00:12:04,440
So for example, maybe if your explanation, basically if your model, let's say if you have a black

143
00:12:04,440 --> 00:12:11,040
box model which is actually using, you know, lace or gender, some of these sensitive features

144
00:12:11,040 --> 00:12:16,040
which are prohibited to be the key aspects for making, you know, critical decisions like

145
00:12:16,040 --> 00:12:20,240
for example, who gets a bail or who gets a loan and so on.

146
00:12:20,240 --> 00:12:26,800
So if a black box model is using some of these features and if your explanation is somehow

147
00:12:26,800 --> 00:12:32,200
misled to sort of think that that's not the feature that it is using but instead it's

148
00:12:32,200 --> 00:12:38,120
using another correlate, for example, a zip code when making prediction and end user might

149
00:12:38,120 --> 00:12:43,760
look at this explanation and just be misled that, oh, this seems like a model that's not

150
00:12:43,760 --> 00:12:48,000
using grace, it's not rationally biased, it's using other correlates.

151
00:12:48,000 --> 00:12:50,240
So maybe it's fine to deploy it, right?

152
00:12:50,240 --> 00:12:55,760
So an explanation that can be misleading in terms of explaining what the black box is

153
00:12:55,760 --> 00:13:00,600
doing can have serious consequences in the real world as we can just see with this kind

154
00:13:00,600 --> 00:13:02,520
of example, right?

155
00:13:02,520 --> 00:13:09,120
So the line of work that I'm pursuing currently is how to identify some of the issues or

156
00:13:09,120 --> 00:13:14,240
vulnerabilities of existing methods which can potentially lead to these kinds of misleading

157
00:13:14,240 --> 00:13:19,760
explanations and also understanding what is a real world impact if there is a misleading

158
00:13:19,760 --> 00:13:21,080
explanation.

159
00:13:21,080 --> 00:13:25,400
So what would be the consequences of that in real world and for that I'm also doing

160
00:13:25,400 --> 00:13:30,880
a bunch of user studies with students from law schools and healthcare professionals and

161
00:13:30,880 --> 00:13:35,080
so on to see how a misleading explanation can affect their work.

162
00:13:35,080 --> 00:13:37,480
This is pretty much what the talk is all about, yeah.

163
00:13:37,480 --> 00:13:45,880
I've got to imagine that the effects of these misleading explanations vary pretty dramatically

164
00:13:45,880 --> 00:13:48,960
depending on the setting in which they're used.

165
00:13:48,960 --> 00:13:49,960
That is definitely true.

166
00:13:49,960 --> 00:13:50,960
Yeah.

167
00:13:50,960 --> 00:13:55,760
So I think like for example, I think you know since anyways we are discussing this, let

168
00:13:55,760 --> 00:14:00,800
me segue a bit into the second part of this talk which essentially talks about the effects

169
00:14:00,800 --> 00:14:07,080
of these explanations in a particular context and the context we are looking at is let's

170
00:14:07,080 --> 00:14:14,880
say if there is a model that is you know designed to predict who should get a bail, right?

171
00:14:14,880 --> 00:14:19,040
Or at least it's designed to assist a judge in determining who should get a bail, but

172
00:14:19,040 --> 00:14:24,440
in the process the model is also making predictions as to who should get a bail, right?

173
00:14:24,440 --> 00:14:31,080
So before such a model is even deployed in the real world, ideally the judges or the teams

174
00:14:31,080 --> 00:14:35,720
of the people it'll definitely go through a lot of vetting and people would like to look

175
00:14:35,720 --> 00:14:40,800
at what the model is exactly doing before they sort of decide to trust and deploy it in

176
00:14:40,800 --> 00:14:43,040
some sense, right?

177
00:14:43,040 --> 00:14:47,960
So in that context we designed a small experiment with law school students.

178
00:14:47,960 --> 00:14:53,640
Of course, it's all proxy because you know the time from judges and these senior domain

179
00:14:53,640 --> 00:14:58,320
experts is their time is much more valuable and it's not easily available.

180
00:14:58,320 --> 00:15:03,280
So we were trying to sort of mimic that with the law school students here at Harvard

181
00:15:03,280 --> 00:15:04,760
and Yukon.

182
00:15:04,760 --> 00:15:11,040
And basically what we did was we essentially built like a simple, in fact, Cynthia Rudin's

183
00:15:11,040 --> 00:15:16,400
style rule based model which is the actual black box which explicitly has some rules which

184
00:15:16,400 --> 00:15:19,960
are used to determine who gets a bail or not.

185
00:15:19,960 --> 00:15:25,160
And in that we specifically used all these bad features or undesirable features like race

186
00:15:25,160 --> 00:15:30,280
or gender to determine who gets a bail and then we constructed an explanation for this

187
00:15:30,280 --> 00:15:33,440
rule based model which is another set of rules.

188
00:15:33,440 --> 00:15:39,280
And then in that we explicitly avoided the usage of the features race and gender, but

189
00:15:39,280 --> 00:15:44,000
the explanation is free to replace it with its correlates like replace these features

190
00:15:44,000 --> 00:15:45,000
with its correlates.

191
00:15:45,000 --> 00:15:49,800
So race could be substituted by zip code or anything else, but the explanation is just

192
00:15:49,800 --> 00:15:55,160
kind of not allowed to show race or gender explicitly, right, but it's still, it should

193
00:15:55,160 --> 00:16:00,000
still make the same predictions as what would be made by the original black box model that

194
00:16:00,000 --> 00:16:01,000
we constructed.

195
00:16:01,000 --> 00:16:07,920
And there was an actual system that generated these explanations or was this, you know,

196
00:16:07,920 --> 00:16:13,000
did you create these explanations to simulate what a system might do under the set of conditions

197
00:16:13,000 --> 00:16:14,440
that you outlined?

198
00:16:14,440 --> 00:16:19,640
So one of the existing techniques is what we used to generate the explanations for this

199
00:16:19,640 --> 00:16:20,640
black box.

200
00:16:20,640 --> 00:16:25,360
So it's just that we put an additional constraint on that technique that just make sure

201
00:16:25,360 --> 00:16:28,240
that race or gender do not show up in the explanations.

202
00:16:28,240 --> 00:16:29,240
Got it.

203
00:16:29,240 --> 00:16:35,440
Now the experiment was something that revealed very insightful things to us, which is

204
00:16:35,440 --> 00:16:39,600
we showed we basically split people into a couple of groups or we took a bunch of

205
00:16:39,600 --> 00:16:42,440
law school students, we split them into two groups.

206
00:16:42,440 --> 00:16:47,960
For one set of people, we showed the actual model, which has race and gender, and then

207
00:16:47,960 --> 00:16:55,160
we asked them if this is the model or if this is an explanation of a model, which is showing

208
00:16:55,160 --> 00:16:59,640
you that race and gender are being used in making, you know, these predictions of who

209
00:16:59,640 --> 00:17:00,640
should be jail.

210
00:17:00,640 --> 00:17:05,800
Would you trust this model enough to deploy it in your code if you were a judge assuming

211
00:17:05,800 --> 00:17:07,480
you were a judge, right?

212
00:17:07,480 --> 00:17:11,800
And most of the people of course, as I expected, said, no, the model is using race and gender,

213
00:17:11,800 --> 00:17:15,760
I don't want to deploy it anywhere close to me, sorry.

214
00:17:15,760 --> 00:17:20,240
But then when we gave them the explanation, which was tailored to sort of hide or cover

215
00:17:20,240 --> 00:17:25,840
up some of these problematic features and then showed it to the other half of the people.

216
00:17:25,840 --> 00:17:31,000
And we said, this is an explanation generated by state of the art machine learning to explain

217
00:17:31,000 --> 00:17:36,960
a black box using this now do you trust the underlying model enough to deploy it?

218
00:17:36,960 --> 00:17:41,360
And most people said, yeah, sure, because it seems to be doing something which reasonably

219
00:17:41,360 --> 00:17:46,720
matches my intuition of how, you know, we should determine someone should be given a bail

220
00:17:46,720 --> 00:17:47,720
or not.

221
00:17:47,720 --> 00:17:52,200
I don't see you say, which of any prohibited features or problematic features, yeah, sure,

222
00:17:52,200 --> 00:17:53,200
let's go ahead.

223
00:17:53,200 --> 00:17:59,800
So the actual true model less than 10% of the people trusted it and the explanation that

224
00:17:59,800 --> 00:18:03,760
we are generated, which is essentially doing the same thing, but replacing race and gender

225
00:18:03,760 --> 00:18:07,960
with its correlates almost about 80% plus people trusted it.

226
00:18:07,960 --> 00:18:15,000
Oh, so it reminds me a little bit of some experiments that I on a Howard shared with me and her

227
00:18:15,000 --> 00:18:22,320
about her research into just the authority that we tend to confer on computing systems in

228
00:18:22,320 --> 00:18:25,800
her case robots, yeah.

229
00:18:25,800 --> 00:18:30,840
And the examples that she gave were, you know, a robot that is presumably supposed to lead

230
00:18:30,840 --> 00:18:35,960
you out of a fire or a dangerous condition in a building, you will like stand behind

231
00:18:35,960 --> 00:18:40,440
it banging itself against the wall and waiting it for it to, you know, all of a sudden do

232
00:18:40,440 --> 00:18:46,840
the right thing because we just want to believe that these things are, you know, more infallible

233
00:18:46,840 --> 00:18:48,160
than they are.

234
00:18:48,160 --> 00:18:49,160
Right.

235
00:18:49,160 --> 00:18:50,160
Yeah.

236
00:18:50,160 --> 00:18:51,160
No, definitely.

237
00:18:51,160 --> 00:18:56,400
I think this research, I guess, also fits in line with some of that work in the sense

238
00:18:56,400 --> 00:19:02,880
that people are probably already approaching models and model explanations from the perspective

239
00:19:02,880 --> 00:19:04,800
of some prior trust, right?

240
00:19:04,800 --> 00:19:10,000
So they are already willing to sort of trust them, which is why some of these, you know,

241
00:19:10,000 --> 00:19:14,400
issues are like the things that we are seeing are actually being seen.

242
00:19:14,400 --> 00:19:15,400
So, yeah.

243
00:19:15,400 --> 00:19:20,080
And realizing we've kind of jumped ahead to the end of the talk.

244
00:19:20,080 --> 00:19:28,080
But did you further explore around, you know, different ways to present that result that,

245
00:19:28,080 --> 00:19:32,760
you know, helped, you know, besides from, you know, the one example where you kind of

246
00:19:32,760 --> 00:19:38,560
show the race and the other where you hide it and are, are there things that you've played

247
00:19:38,560 --> 00:19:44,880
with like showing the different correlating features or other things that can help the

248
00:19:44,880 --> 00:19:46,960
human understand what's really happening?

249
00:19:46,960 --> 00:19:47,960
Yeah.

250
00:19:47,960 --> 00:19:52,880
So that's actually the ongoing research that we're actually doing, which is what is the best

251
00:19:52,880 --> 00:19:58,440
way in which we can educate people that, hey, the explanation that you saw, it is purely

252
00:19:58,440 --> 00:20:03,880
correlational and it's like when it says that zip code is being used, it could essentially

253
00:20:03,880 --> 00:20:09,680
mean that any zip code or its correlate could be actually used to make predictions, right?

254
00:20:09,680 --> 00:20:17,040
And in fact, one thing is we design like a very short 5 to 10 minute, like a primer or tutorial

255
00:20:17,040 --> 00:20:23,080
just highlighting some of the examples that just because you don't see race, you know,

256
00:20:23,080 --> 00:20:27,840
it could still be present because the correlation between race and zip code is like greater than

257
00:20:27,840 --> 00:20:29,720
0.8 or something, right?

258
00:20:29,720 --> 00:20:35,560
And then designing some examples like these and it's a very short 10 minute tutorial and

259
00:20:35,560 --> 00:20:41,440
using that we can already see some, you know, like mega improvements in terms of how people

260
00:20:41,440 --> 00:20:47,040
already like latched on to some of those ideas and the next time we ask them a similar

261
00:20:47,040 --> 00:20:50,440
question, they are like less likely to make this kind of a mistake.

262
00:20:50,440 --> 00:20:56,160
So we are also just thinking about what training is, might help people in like realizing some

263
00:20:56,160 --> 00:21:01,240
of these things because again, what we are designing or even the way that we are sort

264
00:21:01,240 --> 00:21:06,680
of producing these explanations are intention is that they would be used by someone who is

265
00:21:06,680 --> 00:21:08,560
not an expert in machine learning.

266
00:21:08,560 --> 00:21:13,360
So we should also be prepared to teach them how to think about these explanations and

267
00:21:13,360 --> 00:21:17,600
what they can or they cannot provide in terms of information.

268
00:21:17,600 --> 00:21:21,600
So I guess that's the next step or the next research that we are conducting.

269
00:21:21,600 --> 00:21:25,240
And again, going back to your earlier question, we are also looking at this, of course,

270
00:21:25,240 --> 00:21:29,600
one scenario as we talked about where misleading explanations had this impact.

271
00:21:29,600 --> 00:21:34,880
We are also looking at other scenarios, again, in healthcare and a bit in like the business

272
00:21:34,880 --> 00:21:40,120
domains where we are looking at different kinds of decisions, like some of which are more

273
00:21:40,120 --> 00:21:43,560
high stakes, the others which are a bit more low stakes.

274
00:21:43,560 --> 00:21:48,440
So in those cases, what would be the implications of misleading explanations?

275
00:21:48,440 --> 00:21:54,680
So let's maybe take a few steps back and talk a little bit about the explainability

276
00:21:54,680 --> 00:22:02,720
techniques that you are seeing in use and where you are seeing them in use.

277
00:22:02,720 --> 00:22:09,160
Have you done a survey of the various techniques and how they are being used in practice?

278
00:22:09,160 --> 00:22:13,200
I know you talk specifically about Lyme and Shapp, and I hear those come up probably

279
00:22:13,200 --> 00:22:17,160
more than any others, but I'm wondering if you look broadly at that.

280
00:22:17,160 --> 00:22:18,160
Yeah.

281
00:22:18,160 --> 00:22:24,720
So while it's not an active area of my research, so there are I think some other folks who

282
00:22:24,720 --> 00:22:30,280
are sort of thinking about these things, but in general, you're right that these two

283
00:22:30,280 --> 00:22:35,000
techniques, one of the reasons we also picked those was because they were being very widely

284
00:22:35,000 --> 00:22:40,560
used in practice and industry and in other real world settings, so that was also one reason

285
00:22:40,560 --> 00:22:45,120
to sort of see if there are any vulnerabilities in those first because they're so widely

286
00:22:45,120 --> 00:22:46,120
used.

287
00:22:46,120 --> 00:22:52,560
Beyond that also, there are several techniques which are probably much less popular, but they

288
00:22:52,560 --> 00:22:57,240
try to address some of the issues that are present in the first two techniques, Lyme and

289
00:22:57,240 --> 00:22:58,240
Shapp.

290
00:22:58,240 --> 00:23:03,520
Just to name a few, for example, Maple is another approach that has been proposed which

291
00:23:03,520 --> 00:23:09,960
tries to sort of get rid of some of the, you know, like some sort of an ad hoc perturbations

292
00:23:09,960 --> 00:23:15,440
or like some of the ad hoc pieces within Lyme and sort of make them more systematic.

293
00:23:15,440 --> 00:23:18,800
So that's another approach just to give an example.

294
00:23:18,800 --> 00:23:23,480
And of course, there are like several more which are like, you know, muse and a bunch

295
00:23:23,480 --> 00:23:25,880
of other things, anchors and so on.

296
00:23:25,880 --> 00:23:30,360
So there has been a lot of work just built up on, you know, this entire explaining black

297
00:23:30,360 --> 00:23:32,720
boxes, as I said, like since 2016.

298
00:23:32,720 --> 00:23:37,280
So by now, there are countless approaches, but I think the well-known ones among, the

299
00:23:37,280 --> 00:23:40,480
most well-known among these are Lyme and Shapp.

300
00:23:40,480 --> 00:23:44,200
Now coming to the second part of your question where you're thinking about how are people

301
00:23:44,200 --> 00:23:46,280
using these techniques?

302
00:23:46,280 --> 00:23:54,560
Honestly, the domains that I look at, so we are still kind of trying to make decision

303
00:23:54,560 --> 00:24:00,240
makers like doctors or judges kind of aware of these techniques and how they should even

304
00:24:00,240 --> 00:24:01,240
use them.

305
00:24:01,240 --> 00:24:05,240
So, but that's the domains that, you know, I deal with a lot, but I can easily imagine

306
00:24:05,240 --> 00:24:10,160
that if you're looking at, you know, maybe a startup or a tech company and so on, their

307
00:24:10,160 --> 00:24:14,920
people are like more widely, you know, more well-familiar with these kinds of techniques

308
00:24:14,920 --> 00:24:20,840
and they may already be using, be using some of them in their day-to-day, you know, job,

309
00:24:20,840 --> 00:24:24,720
whether as a developer or an engineer or a scientist, you are trying to understand

310
00:24:24,720 --> 00:24:29,080
what a particular model is doing, maybe to debug the model and so on.

311
00:24:29,080 --> 00:24:35,080
So I can imagine all those kinds of use cases already underway and, you know, where explainability

312
00:24:35,080 --> 00:24:40,680
is becoming a, you know, playing a bigger role in practice in day-to-day applications.

313
00:24:40,680 --> 00:24:45,520
But, yeah, the domains that I deal with, like especially where you are sort of a bit

314
00:24:45,520 --> 00:24:50,520
more detached from, you know, the core machine learning, you're dealing with people who make

315
00:24:50,520 --> 00:24:55,240
different kinds of decisions, they're not tech people, they're not experts, so it is

316
00:24:55,240 --> 00:25:00,560
like these kinds of approaches are sort of like reaching them at this point, barely,

317
00:25:00,560 --> 00:25:01,560
I would say.

318
00:25:01,560 --> 00:25:08,080
In your talk, did you kind of go over how the different techniques work and, you know,

319
00:25:08,080 --> 00:25:14,000
what some of the, you know, weaknesses or blind spots that are inherent to them are and

320
00:25:14,000 --> 00:25:15,760
where they come from?

321
00:25:15,760 --> 00:25:22,040
Yeah, so I think the first part of this talk is mainly about what are the weaknesses

322
00:25:22,040 --> 00:25:29,520
of Lime and Shap and just to kind of think about more broadly, the explanation techniques,

323
00:25:29,520 --> 00:25:33,200
you can roughly characterize them into like two categories.

324
00:25:33,200 --> 00:25:38,320
So one is local explanation methods and the other is global explanation methods.

325
00:25:38,320 --> 00:25:44,560
I guess as the name suggests, local means, you know, you just think of explaining a

326
00:25:44,560 --> 00:25:52,560
complaint behavior only within a particular tiny locality or neighborhood in the data,

327
00:25:52,560 --> 00:25:53,560
right?

328
00:25:53,560 --> 00:25:57,720
So a small piece of the feature space, you are trying to explain what the model is doing

329
00:25:57,720 --> 00:25:59,480
there as best as you can.

330
00:25:59,480 --> 00:26:01,760
So that's the local explanation methods.

331
00:26:01,760 --> 00:26:07,600
And then the global explanation methods is you somehow want to give an in the pick the

332
00:26:07,600 --> 00:26:12,720
entire picture of what the black box model might be doing like the whole big picture,

333
00:26:12,720 --> 00:26:17,040
you know, so that like someone like for example, the use cases of these two could be different

334
00:26:17,040 --> 00:26:22,800
where in the case of global explanation, the idea would be that someone who is deciding

335
00:26:22,800 --> 00:26:27,680
if a model is good enough or whether it should be deployed, like maybe a team of judges.

336
00:26:27,680 --> 00:26:32,720
Or, you know, a stakeholder who has a lot of authority on deciding if some model should

337
00:26:32,720 --> 00:26:39,000
be deployed or not, he or she might use those to wet and decide is this model even reasonable

338
00:26:39,000 --> 00:26:40,560
enough to deploy, right?

339
00:26:40,560 --> 00:26:45,600
So that's where the global explanations come into picture because you are giving like zoomed

340
00:26:45,600 --> 00:26:49,120
out view of what the model's behavior looks like, right?

341
00:26:49,120 --> 00:26:53,920
On the other hand, when you think of local explanations, it could be to just like as a

342
00:26:53,920 --> 00:26:59,080
model is deployed or after a model is deployed, let's say in a hospital to diagnose a disease

343
00:26:59,080 --> 00:27:04,480
or something, for every patient, the model will give you a particular, you know, diagnosis

344
00:27:04,480 --> 00:27:11,320
saying that this is basically what the diagnosis should be, say someone has diabetes or not

345
00:27:11,320 --> 00:27:12,720
for example, right?

346
00:27:12,720 --> 00:27:18,480
So in such cases, you also want to get an explanation for why that prediction is made

347
00:27:18,480 --> 00:27:19,960
the way it is.

348
00:27:19,960 --> 00:27:26,080
So then we focus more on the local or instance level or like singular predictions, whereas

349
00:27:26,080 --> 00:27:29,760
and that's the case where after a model is deployed, a doctor is just double checking

350
00:27:29,760 --> 00:27:32,760
that a single prediction makes sense, right?

351
00:27:32,760 --> 00:27:37,880
So that's the use case and then for the global, it is to decide if a model at a high level

352
00:27:37,880 --> 00:27:40,200
is even good enough to sort of beat it.

353
00:27:40,200 --> 00:27:49,760
And so do the local and global methods share the same weaknesses or issues or are they different?

354
00:27:49,760 --> 00:27:50,760
Yeah.

355
00:27:50,760 --> 00:27:56,000
So in fact, the weaknesses are specific to the exact techniques employed to generate

356
00:27:56,000 --> 00:28:02,200
a global or local explanations, like example, the first part of my talk is broadly focusing

357
00:28:02,200 --> 00:28:05,480
on what is called as perturbation based methods, right?

358
00:28:05,480 --> 00:28:09,160
And I'll get into the details of what I mean by that a bit.

359
00:28:09,160 --> 00:28:15,080
Then there are other methods which focus on, you know, kind of using gradients to sort

360
00:28:15,080 --> 00:28:18,960
of determine what features are being used when making a prediction.

361
00:28:18,960 --> 00:28:24,280
So the attacks, these two classes of techniques have are vulnerable to different kinds of

362
00:28:24,280 --> 00:28:25,280
attacks.

363
00:28:25,280 --> 00:28:29,720
So the same attack would not work both for the perturbation based methods as well as

364
00:28:29,720 --> 00:28:31,040
a gradient based method.

365
00:28:31,040 --> 00:28:36,760
So yeah, the attacks are specific to the exact techniques that these methods are using.

366
00:28:36,760 --> 00:28:41,520
So they're much more finely, fine grained than even just local and global.

367
00:28:41,520 --> 00:28:42,520
Okay.

368
00:28:42,520 --> 00:28:50,400
So in the case of a perturbation type of method, like lime, what does the attack look like?

369
00:28:50,400 --> 00:28:52,320
How are those attacks constructed?

370
00:28:52,320 --> 00:28:53,320
Right.

371
00:28:53,320 --> 00:28:58,000
So let me, I think, start by just giving an intuition about what line does so that it becomes

372
00:28:58,000 --> 00:29:00,360
clear what the attack would look like, right?

373
00:29:00,360 --> 00:29:06,600
So what line does is it is trying to at a very basic or code level, lime is trying to

374
00:29:06,600 --> 00:29:09,960
explain individual predictions of classifier.

375
00:29:09,960 --> 00:29:14,960
So for each prediction, it's trying to give you which features were important and what

376
00:29:14,960 --> 00:29:18,800
was their weightage in making this prediction, right?

377
00:29:18,800 --> 00:29:23,120
So now what lime does is it goes to sort of every data point.

378
00:29:23,120 --> 00:29:28,160
So basically if we want to explain a prediction of a particular data point, lime takes the

379
00:29:28,160 --> 00:29:32,000
data point and then it's sort of perturbs that data point.

380
00:29:32,000 --> 00:29:36,400
And when I say that, think of it as like you add some noise to different features of

381
00:29:36,400 --> 00:29:38,360
this data point, okay?

382
00:29:38,360 --> 00:29:40,640
And then that's what we call as perturbation.

383
00:29:40,640 --> 00:29:46,240
So you just kind of slightly massage the values of these data points, generate another artificial

384
00:29:46,240 --> 00:29:53,240
data point and keep doing this until you have a bunch of data points which were resulted

385
00:29:53,240 --> 00:29:58,800
from perturbing that initial instance or the data point you wanted to explain, right?

386
00:29:58,800 --> 00:30:04,440
So now that we have, let's say we got hundreds such perturbations or massage data points

387
00:30:04,440 --> 00:30:08,280
and then you have this actual data point that you wanted to explain.

388
00:30:08,280 --> 00:30:14,600
Now think of it as you just build a linear regression model on top of this so that that

389
00:30:14,600 --> 00:30:20,120
model is predicting what the black box models predictions are for these hundred data points,

390
00:30:20,120 --> 00:30:21,120
okay?

391
00:30:21,120 --> 00:30:25,800
So it's basically taking a data point, massage it to create some artificial data set around

392
00:30:25,800 --> 00:30:30,240
the data point, now just put a regression model and then it will give you what are the

393
00:30:30,240 --> 00:30:33,560
feature importance weights for each of the features, right?

394
00:30:33,560 --> 00:30:35,840
So that's what lime is doing.

395
00:30:35,840 --> 00:30:42,480
So now why this is called a perturbation based method is in order to even fit a linear

396
00:30:42,480 --> 00:30:48,840
regression model there or any local linear model there, you are generating some perturbations

397
00:30:48,840 --> 00:30:52,280
of this initial data point where you started from, right?

398
00:30:52,280 --> 00:30:57,800
So typically these are the, this is what we call as perturbation based methods, okay?

399
00:30:57,800 --> 00:31:03,160
So now the attack once you sort of know a key intuition becomes very clear and obvious

400
00:31:03,160 --> 00:31:08,880
which is, so what we found as part of like, you know, analyzing what lime is doing and

401
00:31:08,880 --> 00:31:09,880
so on.

402
00:31:09,880 --> 00:31:15,120
In fact, one of the PhD students that works with us found this was the perturbations that

403
00:31:15,120 --> 00:31:22,600
are being generated by lime are actually not the points that are in the data distribution.

404
00:31:22,600 --> 00:31:29,000
So these points look very different than the points that are actually in the data distribution

405
00:31:29,000 --> 00:31:30,960
that we care about, okay?

406
00:31:30,960 --> 00:31:36,640
So these could potentially be even off manifold data points or you know points that are like

407
00:31:36,640 --> 00:31:41,000
very far off because you are just massaging a point and assuming that you'll end up with

408
00:31:41,000 --> 00:31:43,000
a point that is sort of close enough, right?

409
00:31:43,000 --> 00:31:45,160
But that does not always happen.

410
00:31:45,160 --> 00:31:51,240
So given that the perturbations that you are generating using lime are not really the

411
00:31:51,240 --> 00:31:55,200
points belonging to the insample data distributions.

412
00:31:55,200 --> 00:31:59,120
That's a key, you know, point to sort of remember.

413
00:31:59,120 --> 00:32:05,520
Now that has happened or now that we realize that that was the case, what we did was design

414
00:32:05,520 --> 00:32:11,640
a simple attack or like build this kind of a what we call as an adversarial classifier

415
00:32:11,640 --> 00:32:18,880
which does something, you know, very sneaky which is this classifier, it looks at a point

416
00:32:18,880 --> 00:32:25,040
and then it basically decides if the point is in the data sample or if it's a perturbation.

417
00:32:25,040 --> 00:32:30,520
So it kind of makes this prediction as to is this point like generated by perturbation,

418
00:32:30,520 --> 00:32:34,040
is this a synthetic data point or was this point in the data?

419
00:32:34,040 --> 00:32:40,960
Now when a point is in the data, then this classifier says, yeah, let's use race or gender

420
00:32:40,960 --> 00:32:46,440
or any other undesired attribute that you could think of and make a prediction.

421
00:32:46,440 --> 00:32:52,000
But if a point is a synthetic data point, which means it's a perturbation, then look

422
00:32:52,000 --> 00:32:57,680
like you're super fair, like don't touch any bad features, remain like super pristine,

423
00:32:57,680 --> 00:33:02,440
be very careful, like do something very clean and innocuous looking, right?

424
00:33:02,440 --> 00:33:08,120
So when you do that, what happens is and when I give that classifier to line to explain

425
00:33:08,120 --> 00:33:13,960
since line relies so much on perturbations, it thinks, oh, this model is doing fine on

426
00:33:13,960 --> 00:33:18,520
perturbations, I don't see race as, you know, an important feature when it's making predictions

427
00:33:18,520 --> 00:33:24,240
on those, so it'll just assume that the model is not using race as a feature when making

428
00:33:24,240 --> 00:33:25,240
predictions.

429
00:33:25,240 --> 00:33:30,440
Whereas underneath what is happening is this like a wrapper, it's like an adversarial

430
00:33:30,440 --> 00:33:32,400
wrapper, you can think of that.

431
00:33:32,400 --> 00:33:39,320
It's kind of nicely shielding its kind of shady behavior for lack of a better word by

432
00:33:39,320 --> 00:33:44,600
doing all the shady things on in sample data points and then looking very innocent on

433
00:33:44,600 --> 00:33:45,600
any perturbations.

434
00:33:45,600 --> 00:33:50,960
So that is the attack, which is throwing off line and though the model uses race as the

435
00:33:50,960 --> 00:33:56,720
only and main feature in making predictions, because it looks like because it has an innocent

436
00:33:56,720 --> 00:34:01,880
behavior on the, you know, perturbation data points, line is just assuming that it is

437
00:34:01,880 --> 00:34:07,360
using some very innocuous features when making predictions and it can never catch this underlying

438
00:34:07,360 --> 00:34:11,280
racial behavior or that is our behavior which uses race, yeah.

439
00:34:11,280 --> 00:34:18,000
So the setting kind of goes back to your setup at the very beginning of our conversation,

440
00:34:18,000 --> 00:34:26,240
you, you know, maybe you can't use a, you know, a transparent model that you've developed

441
00:34:26,240 --> 00:34:33,240
yourself, so you're getting a model from someone else kind of shelf and the attacker in this

442
00:34:33,240 --> 00:34:40,240
case is whoever's creating the model and the scenario is kind of reminds me of Volkswagen

443
00:34:40,240 --> 00:34:45,720
gaming the EPA when the car is detected that they were, you know, being tested for emissions

444
00:34:45,720 --> 00:34:46,720
test.

445
00:34:46,720 --> 00:34:47,720
Right.

446
00:34:47,720 --> 00:34:51,280
They changed the way that they were, you know, throttled or whatever to make their emissions

447
00:34:51,280 --> 00:34:56,520
fall within spec, but, you know, out on the road, they were, you know, you know, seeding

448
00:34:56,520 --> 00:34:57,520
the levels.

449
00:34:57,520 --> 00:34:58,520
Exactly.

450
00:34:58,520 --> 00:34:59,520
Yeah.

451
00:34:59,520 --> 00:35:00,520
This is I think a very good analogy.

452
00:35:00,520 --> 00:35:01,520
Yeah.

453
00:35:01,520 --> 00:35:06,840
That's pretty much what this adversary who is designing this classifier is also doing.

454
00:35:06,840 --> 00:35:09,280
This off the shelf classifier is also doing.

455
00:35:09,280 --> 00:35:15,080
So the main idea is if people are just using like, for example, lime or shaft to determine,

456
00:35:15,080 --> 00:35:19,400
you know, are there any underlying racial or gender biases in this classifier, then

457
00:35:19,400 --> 00:35:24,000
the adversary can successfully fool them because they're able to fool these explanation

458
00:35:24,000 --> 00:35:27,000
methods.

459
00:35:27,000 --> 00:35:35,720
And were you in this, this work in this presentation, do you propose any protections

460
00:35:35,720 --> 00:35:43,040
for this or are you identifying the attack, the attack vector?

461
00:35:43,040 --> 00:35:44,040
Right.

462
00:35:44,040 --> 00:35:49,520
So that particular piece of work was just identifying the attack because that itself

463
00:35:49,520 --> 00:35:55,080
was, I think, one of the initial works, which even talks about attacks on explanation

464
00:35:55,080 --> 00:36:01,360
methods, but our ongoing work is definitely looking at how to sort of design these explanation

465
00:36:01,360 --> 00:36:06,280
methods, which are robust to those attacks, or which cannot be gained to sort of, you

466
00:36:06,280 --> 00:36:08,280
know, make these kinds of attacks accessible.

467
00:36:08,280 --> 00:36:09,680
So how to think about them?

468
00:36:09,680 --> 00:36:11,600
So that's an ongoing stream of research.

469
00:36:11,600 --> 00:36:12,600
Mm-hmm.

470
00:36:12,600 --> 00:36:20,280
And so it is a method like lime work if your perturbations are only producing or in

471
00:36:20,280 --> 00:36:26,280
the direction of kind of in distribution results, like is that a direction that you're

472
00:36:26,280 --> 00:36:27,280
looking at or?

473
00:36:27,280 --> 00:36:28,280
Right.

474
00:36:28,280 --> 00:36:29,280
Yeah.

475
00:36:29,280 --> 00:36:30,840
So that's one of the directions we are looking at.

476
00:36:30,840 --> 00:36:36,560
But then you can also think of this as there are some two problems, which are like two

477
00:36:36,560 --> 00:36:37,920
sides of a coin, right?

478
00:36:37,920 --> 00:36:44,880
So one is, then you can basically make these perturbations more and more similar to your

479
00:36:44,880 --> 00:36:46,120
data instances, right?

480
00:36:46,120 --> 00:36:48,680
So that will potentially subvert this kind of attack.

481
00:36:48,680 --> 00:36:53,000
So we can make a lime plus plus where your perturbations look more and more similar to

482
00:36:53,000 --> 00:36:54,000
your data instance.

483
00:36:54,000 --> 00:36:55,000
Yeah.

484
00:36:55,000 --> 00:36:56,000
Right.

485
00:36:56,000 --> 00:36:57,000
So that's one thing.

486
00:36:57,000 --> 00:37:01,040
There comes another problem that's a fix for the set attack, right?

487
00:37:01,040 --> 00:37:07,520
But then there comes another problem, which is your explanations will start becoming more

488
00:37:07,520 --> 00:37:10,040
and more data dependent, right?

489
00:37:10,040 --> 00:37:15,400
So because if you have a data set, then the explanation that you build will only hold

490
00:37:15,400 --> 00:37:17,520
for that data set.

491
00:37:17,520 --> 00:37:23,000
So which means if you change the data set or something, then the explanation is no longer

492
00:37:23,000 --> 00:37:24,000
going to hold.

493
00:37:24,000 --> 00:37:28,040
So that's another problem of this because you're making this explanation very tight to the

494
00:37:28,040 --> 00:37:29,040
data.

495
00:37:29,040 --> 00:37:32,240
So now how do we fix that is another problem.

496
00:37:32,240 --> 00:37:36,120
But I think this is again like a bit of a trade off where you can think of this like

497
00:37:36,120 --> 00:37:40,120
a scale where the more you move to one side, you're probably creating some issues on the

498
00:37:40,120 --> 00:37:41,120
other side.

499
00:37:41,120 --> 00:37:45,280
So we are also looking at sort of formalizing those trade-offs and like, you know, saying

500
00:37:45,280 --> 00:37:49,800
that yeah, as you try to achieve more of, you know, perturbations that look more and

501
00:37:49,800 --> 00:37:55,160
more similar to your data, yes, you subvert one attack, but then you're creating explanations

502
00:37:55,160 --> 00:37:57,320
that are only holding for your data.

503
00:37:57,320 --> 00:37:58,960
So is that good or is that bad?

504
00:37:58,960 --> 00:38:01,440
So like what are the trade-offs between these two?

505
00:38:01,440 --> 00:38:02,440
Okay.

506
00:38:02,440 --> 00:38:07,960
In your research, have you identified any other similar types of attacks?

507
00:38:07,960 --> 00:38:12,280
Are there others that have been proposed?

508
00:38:12,280 --> 00:38:17,800
So again, as I said, like this one of the initial ones, I think as a follow-up paper or as

509
00:38:17,800 --> 00:38:19,760
a follow-up work, there is another problem.

510
00:38:19,760 --> 00:38:26,480
Another team in Utah that has recent ICML paper on specific attacks to SHAP.

511
00:38:26,480 --> 00:38:32,000
So that is a follow-up work which again sort of plays on or builds on some of our earlier

512
00:38:32,000 --> 00:38:33,000
work.

513
00:38:33,000 --> 00:38:38,160
But I think so far, there have mostly been attempts at looking at like perturbation-based

514
00:38:38,160 --> 00:38:39,160
methods.

515
00:38:39,160 --> 00:38:43,640
So there is a lot of scope for open work on other kinds of methods including radiant-based

516
00:38:43,640 --> 00:38:48,480
methods and even, you know, global versus local what needs to be attacked and what is

517
00:38:48,480 --> 00:38:50,520
most vulnerable in each of these and so on.

518
00:38:50,520 --> 00:38:54,680
So there's like a whole set of open problems that haven't really been addressed so far.

519
00:38:54,680 --> 00:38:55,680
So yeah.

520
00:38:55,680 --> 00:39:03,080
And are there any interesting connections between the work that you're doing here and the

521
00:39:03,080 --> 00:39:09,040
broader research field of kind of adversarial machine learning attacks?

522
00:39:09,040 --> 00:39:12,840
You know, a lot of this is based on kind of perturbations and noise.

523
00:39:12,840 --> 00:39:16,520
And so there's at least a permanent creature overlap.

524
00:39:16,520 --> 00:39:20,240
Does one have something to offer the other and vice versa?

525
00:39:20,240 --> 00:39:21,240
Yeah.

526
00:39:21,240 --> 00:39:22,240
So yes, a lot.

527
00:39:22,240 --> 00:39:29,000
I mean, I think this work is actually inspired by, you know, some sort of the adversarial

528
00:39:29,000 --> 00:39:30,840
machine learning literature.

529
00:39:30,840 --> 00:39:39,320
So there the focus was more on finding examples or data points which can throw off a classifier,

530
00:39:39,320 --> 00:39:40,320
right?

531
00:39:40,320 --> 00:39:46,200
Whereas here the focus is now let's find something which throws off an explanation method.

532
00:39:46,200 --> 00:39:49,640
So like I guess that way there is like a very clear parallel.

533
00:39:49,640 --> 00:39:54,680
What adversarial machine learning was doing for classifiers and prediction models.

534
00:39:54,680 --> 00:39:56,720
We are trying to do that with explanations.

535
00:39:56,720 --> 00:39:59,800
So I guess that way there is like a pretty tight connection.

536
00:39:59,800 --> 00:40:05,280
And in fact, you know, as I was saying about some ongoing work which tries to address

537
00:40:05,280 --> 00:40:11,560
some of these vulnerabilities like in perturbation methods or otherwise, the way we also try

538
00:40:11,560 --> 00:40:18,560
to fix the vulnerabilities and come up with a new explanation method is also inspired by

539
00:40:18,560 --> 00:40:25,120
how people think of adversarial robust classifiers in the adversarial machine learning literature.

540
00:40:25,120 --> 00:40:30,040
So just like people had this, you know, let's first see what are the vulnerabilities where

541
00:40:30,040 --> 00:40:35,960
things are breaking down in terms of classifiers, then how can we develop a robust classifier.

542
00:40:35,960 --> 00:40:40,360
So the same thing is playing out in parallel in the explainability literature.

543
00:40:40,360 --> 00:40:42,160
So yeah, there is a pretty clear connection.

544
00:40:42,160 --> 00:40:47,240
A lot of that ended up saying, you know, more regularization is there approach.

545
00:40:47,240 --> 00:40:48,920
Is that going to be the answer here too?

546
00:40:48,920 --> 00:40:52,920
That's what I was is equally.

547
00:40:52,920 --> 00:40:58,840
Yeah, but I think so the I guess the there are a couple of things though, right?

548
00:40:58,840 --> 00:41:05,440
So for example, one thing is even beyond regularization, it's also about thinking about like

549
00:41:05,440 --> 00:41:11,000
formulations like maybe minimax, which is, you know, sort of like thinking about the maximum

550
00:41:11,000 --> 00:41:16,280
possible error that you could have on like a variety of distributions that you want your

551
00:41:16,280 --> 00:41:22,440
model to work on or like a variety of datasets you want your explanation to hold on and minimizing

552
00:41:22,440 --> 00:41:23,760
that maximum error.

553
00:41:23,760 --> 00:41:29,680
So I guess those kinds of formulations are also very helpful apart from, you know, regularization

554
00:41:29,680 --> 00:41:30,680
and so on.

555
00:41:30,680 --> 00:41:36,160
So I think those ideas are useful to sort of flow from that community to the explainability

556
00:41:36,160 --> 00:41:37,160
community.

557
00:41:37,160 --> 00:41:44,480
Beyond that, I also am hopeful that there could be other interesting challenges with explainability.

558
00:41:44,480 --> 00:41:50,920
The reason why I say that is because there is as algorithms are being like increasingly

559
00:41:50,920 --> 00:41:54,400
used for various decisions like whether someone gets a loan, right?

560
00:41:54,400 --> 00:41:59,840
Or whether, you know, someone gets a particular treatment or whether they are given a bail

561
00:41:59,840 --> 00:42:00,840
or not.

562
00:42:00,840 --> 00:42:06,640
So there is an increasing sort of like call from both legal scholars and social sciences

563
00:42:06,640 --> 00:42:12,560
scholars to sort of make these machines also provide resources to people.

564
00:42:12,560 --> 00:42:17,520
And when I say that, what do I mean is if I as a bank am using an algorithm and if I

565
00:42:17,520 --> 00:42:23,360
tell someone a loan is denied for you, I also need to tell them what needs to be changed

566
00:42:23,360 --> 00:42:27,440
in their profile so that they can come back and get a loan, right?

567
00:42:27,440 --> 00:42:34,080
So they are making these algorithms more accountable, which means the gaming of these kinds of

568
00:42:34,080 --> 00:42:37,400
methods is going to be like very real.

569
00:42:37,400 --> 00:42:42,800
Like when you think of, you know, classifiers and some adversaries sort of giving you an

570
00:42:42,800 --> 00:42:46,400
adversarial sample or example and so on.

571
00:42:46,400 --> 00:42:51,320
So I guess the danger of that somehow seems a little bit more limited to me than like

572
00:42:51,320 --> 00:42:56,480
when it comes to explainability where people are relying heavily on this and like things

573
00:42:56,480 --> 00:43:00,560
are moving increasingly in the directions that people are looking at these and making

574
00:43:00,560 --> 00:43:06,360
decisions of what models to use, making decisions like whether a prediction is reliable or not.

575
00:43:06,360 --> 00:43:12,280
So as you are hitting more and more of these real world scenarios, I think, well, we also

576
00:43:12,280 --> 00:43:17,760
need to be worried because the risk of these things being manipulated and game is very

577
00:43:17,760 --> 00:43:19,480
real and very high.

578
00:43:19,480 --> 00:43:26,000
But at the same time, I can see lot more real world applications or like usages of these

579
00:43:26,000 --> 00:43:31,120
scenarios probably way more than, you know, someone trying to change pixels and any

580
00:43:31,120 --> 00:43:32,320
image and so on, right?

581
00:43:32,320 --> 00:43:37,440
So that's while that's an interesting concept to think about, you know, to solve a problem

582
00:43:37,440 --> 00:43:42,360
or like from an engineering perspective, technical perspective, here the social implications

583
00:43:42,360 --> 00:43:43,720
are very real.

584
00:43:43,720 --> 00:43:48,680
So I'm hoping that this would also bring with that more interesting technical challenges

585
00:43:48,680 --> 00:43:50,000
as well.

586
00:43:50,000 --> 00:43:55,680
Is there a GAN application here where you've got some model that's trained to try to

587
00:43:55,680 --> 00:44:02,040
fool your, some model that's, you know, you've got kind of these two adversarial models

588
00:44:02,040 --> 00:44:07,960
that are trying to try to pick out, you know, the out of distribution samples or something

589
00:44:07,960 --> 00:44:11,760
like that and, you know, when the one model is trying to cheat the other or anything.

590
00:44:11,760 --> 00:44:12,760
Yeah.

591
00:44:12,760 --> 00:44:13,760
No, I think they're headed there.

592
00:44:13,760 --> 00:44:16,800
I think some of you in our ongoing work is sort of headed there.

593
00:44:16,800 --> 00:44:17,800
Okay.

594
00:44:17,800 --> 00:44:23,480
And I guess the way in which sort of at least like, you know, me and my group or some of

595
00:44:23,480 --> 00:44:30,160
the other researchers are approaching this problem is kind of trying to keep like a real

596
00:44:30,160 --> 00:44:37,120
year to the ground because a case where someone can build a classifier which can do something

597
00:44:37,120 --> 00:44:42,320
messy within sample data points and look very pristine and clean with these perturbations

598
00:44:42,320 --> 00:44:46,240
that the approach relies on is like a very realistic thing and it's not even like a

599
00:44:46,240 --> 00:44:49,440
super sophisticated attack if you think about it, right?

600
00:44:49,440 --> 00:44:54,080
So we are trying to sort of at this point, keep our year to the ground and look at those

601
00:44:54,080 --> 00:44:59,480
most plausible scenarios and how to fix them and then of course, you know, some of these

602
00:44:59,480 --> 00:45:03,480
will automatically happen which are definitely interesting from technical perspective and so

603
00:45:03,480 --> 00:45:04,480
on.

604
00:45:04,480 --> 00:45:05,480
Right.

605
00:45:05,480 --> 00:45:09,960
Well, Hima, thanks so much for sharing a bit about your presentation and your research.

606
00:45:09,960 --> 00:45:10,960
Yeah.

607
00:45:10,960 --> 00:45:11,960
Thank you so much.

608
00:45:11,960 --> 00:45:12,960
This was, yeah, this was amazing.

609
00:45:12,960 --> 00:45:15,440
I had a great time talking to you.

610
00:45:15,440 --> 00:45:16,440
I'm here.

611
00:45:16,440 --> 00:45:17,440
Same here.

612
00:45:17,440 --> 00:45:18,440
Thank you.

613
00:45:18,440 --> 00:45:23,440
All right, everyone, that's our show for today.

614
00:45:23,440 --> 00:45:29,240
For more information on today's show, visit twomolai.com slash shows.

615
00:45:29,240 --> 00:45:41,720
As always, thanks so much for listening and catch you next time.

616
00:45:41,720 --> 00:45:48,720
Bye.

