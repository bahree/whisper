1
00:00:00,000 --> 00:00:15,920
Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting

2
00:00:15,920 --> 00:00:20,880
people doing interesting things in machine learning and artificial intelligence.

3
00:00:20,880 --> 00:00:24,040
I'm your host Sam Charrington.

4
00:00:24,040 --> 00:00:29,760
Last week I spent some time at CES, the consumer electronics show in Las Vegas, exploring

5
00:00:29,760 --> 00:00:36,360
the vast sea of drones, cameras, paper thin TVs, robots, laundry folding closets and other

6
00:00:36,360 --> 00:00:37,760
smart devices.

7
00:00:37,760 --> 00:00:38,840
You name it?

8
00:00:38,840 --> 00:00:40,160
It was there.

9
00:00:40,160 --> 00:00:44,920
Of course, I was also able to sit down with some really interesting folks working on some

10
00:00:44,920 --> 00:00:48,360
pretty cool AI-enabled products.

11
00:00:48,360 --> 00:00:52,400
Head on over to our YouTube channel to check out some behind-the-scenes footage from my

12
00:00:52,400 --> 00:00:55,680
interviews and other quick takes from the show.

13
00:00:55,680 --> 00:01:01,960
And beyond the look up for our AI and consumer electronics series right here on the podcast,

14
00:01:01,960 --> 00:01:03,200
coming soon.

15
00:01:03,200 --> 00:01:08,280
The show you're about to hear is part of a series of shows recorded at the rework Deep Learning

16
00:01:08,280 --> 00:01:11,760
Summit in Montreal back in October.

17
00:01:11,760 --> 00:01:17,760
This was a great event, and in fact, their next event, the Deep Learning Summit San Francisco,

18
00:01:17,760 --> 00:01:23,200
is right around the corner on January 25th and 26th, and will feature more leading researchers

19
00:01:23,200 --> 00:01:28,320
and technologists like the ones you'll hear on the show this week, including Ian Goodfellow

20
00:01:28,320 --> 00:01:33,920
of Google Brain and Daphne Kohler of Calico Labs and more.

21
00:01:33,920 --> 00:01:40,160
Definitely check out the event and use the code TwimmelAI for 20% off of registration.

22
00:01:40,160 --> 00:01:46,480
In this show, I speak with Greg Dymus, Senior Computer Systems Researcher at Baidu.

23
00:01:46,480 --> 00:01:50,160
Greg joined me before his talk at the Deep Learning Summit where he spoke on the next

24
00:01:50,160 --> 00:01:52,680
generation of AI chips.

25
00:01:52,680 --> 00:01:57,480
Greg's talk focused on some of the work his team was involved in that accelerate Deep Learning

26
00:01:57,480 --> 00:02:03,600
training by using mixed 16-bit and 32-bit floating point arithmetic.

27
00:02:03,600 --> 00:02:07,920
We cover a ton of interesting ground in this conversation, and if you're interested

28
00:02:07,920 --> 00:02:12,440
in systems-level thinking around scaling and accelerating Deep Learning, you're really

29
00:02:12,440 --> 00:02:14,080
going to like this one.

30
00:02:14,080 --> 00:02:19,520
Of course, if you like this one, you're also going to like TwimmelTalk number 14 with

31
00:02:19,520 --> 00:02:25,040
Greg's former colleague, Shubo Sengupta, which covers a bunch of related topics.

32
00:02:25,040 --> 00:02:29,160
If you haven't already listened to that one, I encourage you to check it out.

33
00:02:29,160 --> 00:02:32,320
And now on to the show.

34
00:02:32,320 --> 00:02:43,400
Hey everyone, I am here at the Rework Deep Learning Conference in Montreal, and I've got

35
00:02:43,400 --> 00:02:49,480
the pleasure to be seated across from Greg Dymus from Baidu, and he's a Senior Researcher

36
00:02:49,480 --> 00:02:50,480
there.

37
00:02:50,480 --> 00:02:52,680
Greg, welcome to this week in Machine Learning and AI.

38
00:02:52,680 --> 00:02:53,680
Thanks for having me.

39
00:02:53,680 --> 00:02:54,680
Awesome.

40
00:02:54,680 --> 00:02:58,120
Why don't we get started by having you tell us a little bit about your background and

41
00:02:58,120 --> 00:03:00,680
how you got interested in ML and AI?

42
00:03:00,680 --> 00:03:02,280
Sure, absolutely.

43
00:03:02,280 --> 00:03:05,880
My background has traditionally been in high performance computing.

44
00:03:05,880 --> 00:03:10,640
I've been really excited about building really fast processors for important applications

45
00:03:10,640 --> 00:03:14,120
that enable new applications that people can use.

46
00:03:14,120 --> 00:03:17,800
You know, it's actually kind of a strange story how I got to AI.

47
00:03:17,800 --> 00:03:19,960
I used to be an AI skeptic.

48
00:03:19,960 --> 00:03:20,960
Okay.

49
00:03:20,960 --> 00:03:22,520
That's always a good place to start.

50
00:03:22,520 --> 00:03:23,520
Yeah.

51
00:03:23,520 --> 00:03:28,960
I always felt like AI would be valuable, but I just felt like there's no way that simple

52
00:03:28,960 --> 00:03:35,360
algorithms like Stochastic gradient descent could ever solve these complex, you know, highly

53
00:03:35,360 --> 00:03:38,280
multi-dimensional optimization problems.

54
00:03:38,280 --> 00:03:44,360
And then at one point, I remember sitting at Nvidia Research and hearing a talk from

55
00:03:44,360 --> 00:03:47,720
Jan LeCoon and just realizing, oh, I was wrong.

56
00:03:47,720 --> 00:03:50,480
I was totally wrong.

57
00:03:50,480 --> 00:03:55,880
And immediately after that, I joined by your research, Andrew was founding the Silicon

58
00:03:55,880 --> 00:04:01,000
Valley AI lab and it seemed like a great opportunity to learn more about AI.

59
00:04:01,000 --> 00:04:05,360
And man, it's been such a crazy ride to get to this point.

60
00:04:05,360 --> 00:04:06,360
I bet.

61
00:04:06,360 --> 00:04:10,160
So what was your path to get to that point that you even had an opinion that involves

62
00:04:10,160 --> 00:04:11,640
Stochastic gradient descent?

63
00:04:11,640 --> 00:04:12,640
Oh, sure.

64
00:04:12,640 --> 00:04:13,640
I mean, well, let's see.

65
00:04:13,640 --> 00:04:15,640
I like building things that are useful for people.

66
00:04:15,640 --> 00:04:20,760
I feel like computing in general has enabled many, you know, new capabilities like the

67
00:04:20,760 --> 00:04:26,680
internet and like videos and, you know, so many things that we take for granted every

68
00:04:26,680 --> 00:04:29,240
day, but it really make our lives better.

69
00:04:29,240 --> 00:04:32,240
And I was always really passionate about making computers even better.

70
00:04:32,240 --> 00:04:33,240
Okay.

71
00:04:33,240 --> 00:04:36,840
It's kind of this belief that although you might not know it going into it, if you make

72
00:04:36,840 --> 00:04:41,000
a faster computer or more efficient computer, someone will find a way of building something

73
00:04:41,000 --> 00:04:43,440
amazing on top of that.

74
00:04:43,440 --> 00:04:47,280
And so I spent a lot of time looking at applications.

75
00:04:47,280 --> 00:04:50,560
What are the things that you can use computers to do?

76
00:04:50,560 --> 00:04:52,440
And AI was always there.

77
00:04:52,440 --> 00:04:53,440
Okay.

78
00:04:53,440 --> 00:04:57,720
Just the feeling with AI has, you know, for me before deporning was that it would just

79
00:04:57,720 --> 00:05:04,440
be too hard that a lot of existing theory was kind of steering you down and had all of

80
00:05:04,440 --> 00:05:07,440
these really difficult challenges.

81
00:05:07,440 --> 00:05:11,960
And, you know, I'd seen a lot of people, very smart people spend a lot of time trying

82
00:05:11,960 --> 00:05:17,240
to tackle those problems and not quite getting all the way there.

83
00:05:17,240 --> 00:05:24,080
So do you recall what was it about Jan's talk that kind of made the light bulb go off

84
00:05:24,080 --> 00:05:30,000
and made you realize that it's to cast a green and to send was the answer?

85
00:05:30,000 --> 00:05:31,000
Sure.

86
00:05:31,000 --> 00:05:36,520
So you can look at these hierarchical feature representations and covenants.

87
00:05:36,520 --> 00:05:41,800
So when people, you know, look at images, you can tell, it's the world is hierarchical.

88
00:05:41,800 --> 00:05:43,400
You can break a chair down into pieces.

89
00:05:43,400 --> 00:05:44,400
It has arms and legs.

90
00:05:44,400 --> 00:05:48,440
You can break those pieces down, you know, recursively.

91
00:05:48,440 --> 00:05:54,480
And there's a lot of existing work that provides some evidence that vision algorithms will

92
00:05:54,480 --> 00:05:58,520
do similar things for recognition tasks.

93
00:05:58,520 --> 00:06:01,920
That wasn't ever really a question.

94
00:06:01,920 --> 00:06:07,400
And people were able to build by hand, you know, things like feature detectors and these

95
00:06:07,400 --> 00:06:09,920
hierarchical systems that worked reasonably well.

96
00:06:09,920 --> 00:06:12,600
They were just very difficult to build.

97
00:06:12,600 --> 00:06:18,520
And the interesting thing about Jan's talk was that, you know, systems could do it automatically.

98
00:06:18,520 --> 00:06:23,400
I thought before that that you would get stuck in these, you know, intractable optimization

99
00:06:23,400 --> 00:06:29,160
problems where even if a solution exists, you know, it's one out of some enormously large

100
00:06:29,160 --> 00:06:35,640
number, like, you know, two to the power of 10 to the power of 30, like something amazingly

101
00:06:35,640 --> 00:06:36,640
big.

102
00:06:36,640 --> 00:06:40,200
Like, you know, you think about different sizes of numbers.

103
00:06:40,200 --> 00:06:43,440
Sometimes I think of the number of atoms in the universe as being a big number.

104
00:06:43,440 --> 00:06:46,240
This is far, far bigger than that.

105
00:06:46,240 --> 00:06:50,680
And you were thinking that that number represents the represents what?

106
00:06:50,680 --> 00:06:55,120
How many things you'd have to search through to find a good search base for your, okay?

107
00:06:55,120 --> 00:06:59,880
Yeah, it's like the needle in an enormous haystack, more atoms than there are in the universe.

108
00:06:59,880 --> 00:07:02,880
How could you ever possibly hope to search through it efficiently?

109
00:07:02,880 --> 00:07:03,880
Wow.

110
00:07:03,880 --> 00:07:09,520
Actually, with these very simple algorithms, but, you know, reliably, I've seen since

111
00:07:09,520 --> 00:07:14,160
then for one application after another, for image recognition, for speech recognition,

112
00:07:14,160 --> 00:07:20,080
for synthesis, for language understanding, it works very reliably.

113
00:07:20,080 --> 00:07:24,400
Did you happen to catch any of Jeff Hinton's talk about this?

114
00:07:24,400 --> 00:07:28,680
What do you think about his kind of post SGD capsule?

115
00:07:28,680 --> 00:07:33,840
Well, it's not post SGD, actually, the starting point is that SGD is really the only

116
00:07:33,840 --> 00:07:36,680
thing that we know works, right?

117
00:07:36,680 --> 00:07:41,600
But it's more post kind of the traditional model of the neuron.

118
00:07:41,600 --> 00:07:42,600
Yeah.

119
00:07:42,600 --> 00:07:50,520
I almost think of it as like post-convenants, and, you know, one thing we've realized recently

120
00:07:50,520 --> 00:07:56,320
is just there is a lot of complexity in modeling, that while we like to think of deep warning

121
00:07:56,320 --> 00:08:01,520
as a general purpose learning algorithm, as you sort of playing it to different applications,

122
00:08:01,520 --> 00:08:06,080
like my experience has been spending a lot of time applying it to speech recognition.

123
00:08:06,080 --> 00:08:14,680
And you do get some benefits from more data and from some general purpose aspects of the

124
00:08:14,680 --> 00:08:19,080
learning algorithm to the extent that it's robust to different speakers or different

125
00:08:19,080 --> 00:08:24,720
variations in different environments, but you also get a lot of benefits from specialization.

126
00:08:24,720 --> 00:08:29,440
So finding the right neural network architecture seems like it matters a lot.

127
00:08:29,440 --> 00:08:34,560
And as we look into details for different applications, as we spend time tuning your

128
00:08:34,560 --> 00:08:40,080
own network architectures for different applications, you see very different structures emerge.

129
00:08:40,080 --> 00:08:46,560
So it wouldn't surprise me at all if there is a more efficient, more general purpose structure

130
00:08:46,560 --> 00:08:49,440
for vision than covenants.

131
00:08:49,440 --> 00:08:50,440
Yeah.

132
00:08:50,440 --> 00:08:56,520
One of the really helped me see that was a blog post by Stephen Merritti a while ago at

133
00:08:56,520 --> 00:09:01,560
May, be almost a year ago at this point, but he talked about network architecture being

134
00:09:01,560 --> 00:09:03,840
the new feature engineering.

135
00:09:03,840 --> 00:09:08,520
Hinton Salk was post-comments, but he did start it off by talking about, like calling

136
00:09:08,520 --> 00:09:14,360
into question the basic neuron structure, but he didn't necessarily, he did kind of pivot

137
00:09:14,360 --> 00:09:19,240
to talking about the network architecture at a higher level, right?

138
00:09:19,240 --> 00:09:26,960
Was there a piece in there where he suggested what might be the kind of successor to the

139
00:09:26,960 --> 00:09:30,000
traditional neuron architecture?

140
00:09:30,000 --> 00:09:36,160
I think it's about this concept of capsules, which might be groups of cooperating neurons.

141
00:09:36,160 --> 00:09:41,760
And then the way that they cooperate together might be more, more complex.

142
00:09:41,760 --> 00:09:46,080
Let's see, I feel like from a computational perspective, it's actually hard to get away

143
00:09:46,080 --> 00:09:51,600
from the formulation of neurons that we have as the basic building block where even if there

144
00:09:51,600 --> 00:09:57,720
is something that's algorithmically more efficient or more well matched to the problem, the computational

145
00:09:57,720 --> 00:10:02,640
building blocks that we have have been so highly tuned that if you made a very substantial

146
00:10:02,640 --> 00:10:08,320
change, it might be better kind of at the algorithm level, but it might be very inefficient

147
00:10:08,320 --> 00:10:11,080
on the type of computer that we know how to build today.

148
00:10:11,080 --> 00:10:17,200
It breaks this whole ecosystem that we've built up around this traditional way of building

149
00:10:17,200 --> 00:10:20,520
neurons and networks and solving them.

150
00:10:20,520 --> 00:10:25,800
Yeah, so much of the existing technologies are built on top of harder support and also

151
00:10:25,800 --> 00:10:30,280
algorithm support for linear algebra, like dense linear algebra.

152
00:10:30,280 --> 00:10:34,160
It's actually kind of surprising to me how effective that's been given that the primitives

153
00:10:34,160 --> 00:10:38,880
are so old and that they're so simple, it's very surprising to me that those building

154
00:10:38,880 --> 00:10:42,760
blocks have gotten us as far as they have.

155
00:10:42,760 --> 00:10:49,320
So we took a little digression, I guess, before we got to kind of what you're up to at

156
00:10:49,320 --> 00:10:50,320
Baidu.

157
00:10:50,320 --> 00:10:51,320
Sure.

158
00:10:51,320 --> 00:10:57,280
And Baidu, really the Silicon Valley AI lab is about building new breakthrough technologies

159
00:10:57,280 --> 00:11:02,760
in AI that especially have connections or enabled new products.

160
00:11:02,760 --> 00:11:09,760
We focus on things that we can't currently do today and there are really multiple ways

161
00:11:09,760 --> 00:11:11,520
we end up attacking this.

162
00:11:11,520 --> 00:11:16,840
The thing that I focus a lot on is just the idea of scale that as we have faster computers

163
00:11:16,840 --> 00:11:23,720
that can train larger or complex neural networks that it's not the only way, but that's a very

164
00:11:23,720 --> 00:11:28,520
reliable way of improving accuracy or enabling new capabilities.

165
00:11:28,520 --> 00:11:32,320
We've seen this in vision to a large extent.

166
00:11:32,320 --> 00:11:36,400
It's actually kind of interesting when we started working in Baidu, there is a question

167
00:11:36,400 --> 00:11:39,680
whether you could apply this outside of vision.

168
00:11:39,680 --> 00:11:42,800
We spent a lot of time looking into speech recognition.

169
00:11:42,800 --> 00:11:46,600
I think looking back on that, it works very reliably.

170
00:11:46,600 --> 00:11:51,800
You can definitely apply deporting outside of vision to many different applications.

171
00:11:51,800 --> 00:11:56,600
Sometimes now instead of thinking what is the new application that you can apply deporting

172
00:11:56,600 --> 00:12:02,000
to, I sometimes wonder, are there any applications that are not well matched that we won't be

173
00:12:02,000 --> 00:12:07,960
able to make significant progress on by just applying the simple recipe of deep architectures,

174
00:12:07,960 --> 00:12:11,040
large data sets, large scale computer?

175
00:12:11,040 --> 00:12:13,240
I haven't found one yet.

176
00:12:13,240 --> 00:12:16,280
We talked a little bit about before we got started.

177
00:12:16,280 --> 00:12:21,160
We talked a little bit about the fact that you worked with one of our previous guests,

178
00:12:21,160 --> 00:12:25,800
Shibos and Gupta, who was at Baidu and is now at Facebook, is that right?

179
00:12:25,800 --> 00:12:28,760
Yeah, he's playing Dota.

180
00:12:28,760 --> 00:12:39,400
Nice. He and I spoke pretty extensively about the speech translation, the specific name

181
00:12:39,400 --> 00:12:44,160
of the project with the Baidu speech, the deep speech, right?

182
00:12:44,160 --> 00:12:49,000
In the course of that conversation, we talked a little bit about some of the scalability

183
00:12:49,000 --> 00:12:54,160
challenges that your team ran into in tackling that problem.

184
00:12:54,160 --> 00:13:01,280
Here at this conference, you're talking about, you have to talk tomorrow, in fact, about

185
00:13:01,280 --> 00:13:03,760
some even further work that you've done.

186
00:13:03,760 --> 00:13:07,040
Can you tell us about what you're planning to talk about?

187
00:13:07,040 --> 00:13:08,640
Sure, definitely.

188
00:13:08,640 --> 00:13:12,760
This is definitely along the lines of scaling deep neural networks.

189
00:13:12,760 --> 00:13:18,560
We pretty consistently find that if you throw more data at the problem, it isn't the only

190
00:13:18,560 --> 00:13:24,840
way, but it is a very effective way of reducing error rates and improving accuracy.

191
00:13:24,840 --> 00:13:29,920
So oftentimes, when you keep throwing data at the problem, you eventually run into some

192
00:13:29,920 --> 00:13:31,320
limitation.

193
00:13:31,320 --> 00:13:38,000
Sometimes you run out of data, and sometimes you run out of patience to wait for your

194
00:13:38,000 --> 00:13:42,040
system to train.

195
00:13:42,040 --> 00:13:46,480
There's one example that I think drives this point at home that we once had a model that

196
00:13:46,480 --> 00:13:54,680
ran, I let it run on a large cluster, run about 64 GPUs for about six months.

197
00:13:54,680 --> 00:13:59,960
We were still getting improvements in accuracy at the time I decided to pull the plug.

198
00:13:59,960 --> 00:14:07,120
How are you, if you're still running your training model for, you're saying that kind of your

199
00:14:07,120 --> 00:14:11,360
incremental error is decreasing as you ran it?

200
00:14:11,360 --> 00:14:15,720
Yeah, this was a state of the art model, so it's actually improving the state of the art

201
00:14:15,720 --> 00:14:19,520
as it runs every minute, it's getting a little bit better than any model that we've had

202
00:14:19,520 --> 00:14:20,680
before.

203
00:14:20,680 --> 00:14:24,960
And then after six months, you really go back and look at that and say, well, I could

204
00:14:24,960 --> 00:14:31,360
let it run for another few years, but I'd like to use it now.

205
00:14:31,360 --> 00:14:34,480
So we're always looking for ways to improve speed.

206
00:14:34,480 --> 00:14:39,720
There's actually, oh man, there's something that's related to that that I can't talk

207
00:14:39,720 --> 00:14:40,720
about yet.

208
00:14:40,720 --> 00:14:46,480
It's kind of a weird situation to sometimes be in where like you know why something happens

209
00:14:46,480 --> 00:14:50,080
or you know that something will keep happening, but you can't talk about it.

210
00:14:50,080 --> 00:14:54,360
One of the things that I think I know is that for many applications, we will continue

211
00:14:54,360 --> 00:14:57,200
to see improvements in the state of the art from faster computers.

212
00:14:57,200 --> 00:15:01,280
I can't tell you why, but I'm pretty sure, I'll tell you why soon.

213
00:15:01,280 --> 00:15:02,280
Okay.

214
00:15:02,280 --> 00:15:09,160
Is this the, are we talking about a theoretical result or a, okay, yeah, I think so.

215
00:15:09,160 --> 00:15:11,560
We're nodding yes for those who can't see.

216
00:15:11,560 --> 00:15:13,480
I'm nodding yes.

217
00:15:13,480 --> 00:15:14,480
Yeah.

218
00:15:14,480 --> 00:15:17,800
This is one of those things that'll definitely be surprising to people when we can finally

219
00:15:17,800 --> 00:15:21,280
talk about it, but sorry, I can't talk about it today.

220
00:15:21,280 --> 00:15:24,920
But you, so you just have to take my word for it that we need faster computers.

221
00:15:24,920 --> 00:15:25,920
Okay.

222
00:15:25,920 --> 00:15:29,760
And the talk tomorrow is going to be about a way that we can make computers faster for

223
00:15:29,760 --> 00:15:30,760
deep warning.

224
00:15:30,760 --> 00:15:31,760
Okay.

225
00:15:31,760 --> 00:15:32,760
Yeah.

226
00:15:32,760 --> 00:15:36,520
I spend a lot of my time thinking about this, like what's the best that you could do?

227
00:15:36,520 --> 00:15:41,360
How fast could you possibly make a computer even our understanding of physics on our existing

228
00:15:41,360 --> 00:15:42,960
technology?

229
00:15:42,960 --> 00:15:48,960
I think one thing the industry is realizing is that we spend a lot of time focusing on general

230
00:15:48,960 --> 00:15:55,920
purpose computation, so building computers to run windows or your browser.

231
00:15:55,920 --> 00:16:00,280
But if you specialize, if you build a computer that's good, it only a few things and not

232
00:16:00,280 --> 00:16:03,080
everything, you can do a lot better.

233
00:16:03,080 --> 00:16:07,840
So we're exploring right now, how do you build computers that are good at AI for good

234
00:16:07,840 --> 00:16:08,840
and deep warning?

235
00:16:08,840 --> 00:16:14,920
And is this different than what others in the space are doing, like TPUs and things of

236
00:16:14,920 --> 00:16:16,320
that nature?

237
00:16:16,320 --> 00:16:17,680
Let's see.

238
00:16:17,680 --> 00:16:20,960
So this is about a very specific technique.

239
00:16:20,960 --> 00:16:25,280
It might be one technology that might go into a chip like a GPU or a TPU.

240
00:16:25,280 --> 00:16:26,280
Okay.

241
00:16:26,280 --> 00:16:29,840
Right now, many of these designs are using a lot of the same technologies.

242
00:16:29,840 --> 00:16:30,840
Okay.

243
00:16:30,840 --> 00:16:32,240
This is a new one.

244
00:16:32,240 --> 00:16:34,320
And this one has a pretty high upside.

245
00:16:34,320 --> 00:16:37,040
This one has a maybe order of magnitude upside.

246
00:16:37,040 --> 00:16:38,040
Okay.

247
00:16:38,040 --> 00:16:43,080
Well, before we dive into that, can we take a second to kind of characterize the thing

248
00:16:43,080 --> 00:16:48,320
that GPUs and TPUs are doing that's kind of gotten them the benefit and then we'll

249
00:16:48,320 --> 00:16:52,080
dive into, you know, this approach and what makes it different?

250
00:16:52,080 --> 00:16:53,080
Sure.

251
00:16:53,080 --> 00:16:54,080
Definitely.

252
00:16:54,080 --> 00:16:56,160
So, let's see.

253
00:16:56,160 --> 00:17:01,600
I feel like one of the, well, there are two, okay, there are a lot of differences.

254
00:17:01,600 --> 00:17:06,720
One thing that's worth keeping in mind is that modern processors incorporate probably

255
00:17:06,720 --> 00:17:12,040
thousands, probably even more optimizations.

256
00:17:12,040 --> 00:17:16,560
So these are technologies that will improve their performance in some way.

257
00:17:16,560 --> 00:17:20,240
They might be circuit level, they might be architecture level, they might be in the software

258
00:17:20,240 --> 00:17:21,240
stack.

259
00:17:21,240 --> 00:17:27,480
It's very hard because real designs are composed of, you know, you pick out your favorite

260
00:17:27,480 --> 00:17:31,800
out of this pool of thousands of technologies and that becomes the new processor that you

261
00:17:31,800 --> 00:17:33,040
build.

262
00:17:33,040 --> 00:17:38,320
These distinctions like TPU, GPU, CPU, they're very high level and they gloss over all of

263
00:17:38,320 --> 00:17:39,680
those details.

264
00:17:39,680 --> 00:17:40,680
Okay.

265
00:17:40,680 --> 00:17:47,600
So, I think what's more important than the name is what it does, how fast is it actually,

266
00:17:47,600 --> 00:17:50,320
like what is the result that you get from it?

267
00:17:50,320 --> 00:17:53,160
How fast does it run a model that you care about?

268
00:17:53,160 --> 00:17:57,800
We've seen things that were called CPUs being commonly used for training maybe 10 years

269
00:17:57,800 --> 00:17:58,800
ago.

270
00:17:58,800 --> 00:18:02,080
There was a transition where people started using GPUs.

271
00:18:02,080 --> 00:18:08,320
The important thing about GPUs was optimization for parallelism, that there is abundant

272
00:18:08,320 --> 00:18:11,680
parallelism in neural network computations.

273
00:18:11,680 --> 00:18:15,480
Some of the things that are, you know, a few, like a couple out of that list of thousand

274
00:18:15,480 --> 00:18:20,400
things that are being added into the next generation are optimizations around locality

275
00:18:20,400 --> 00:18:22,160
and low precision.

276
00:18:22,160 --> 00:18:27,880
So the technology I'm going to talk about tomorrow is focused on low precision, and there's

277
00:18:27,880 --> 00:18:34,960
a big difference when a lot of previous technologies have been discussed or proposed for low precision,

278
00:18:34,960 --> 00:18:38,840
it's mostly been focused on inference and not training.

279
00:18:38,840 --> 00:18:44,760
And this will be one of the first results, and especially the, as far as I know, largest

280
00:18:44,760 --> 00:18:50,240
skill result that focuses on using low precision for training, I think the high level conclusion

281
00:18:50,240 --> 00:18:52,960
is it finally works.

282
00:18:52,960 --> 00:18:54,920
It was enormously difficult.

283
00:18:54,920 --> 00:18:55,920
Wow.

284
00:18:55,920 --> 00:19:02,800
It was actually kind of a weird surprise that when you try doing low precision for training

285
00:19:02,800 --> 00:19:06,240
versus inference, we didn't really know that we would see this.

286
00:19:06,240 --> 00:19:09,840
We started looking into this, but it just turns out that for some reason, inference is

287
00:19:09,840 --> 00:19:15,480
so much easier than training, that even, you know, very drastic reductions in precision,

288
00:19:15,480 --> 00:19:20,400
like moving from double precision down to, you know, even 8-bit or possibly even lower

289
00:19:20,400 --> 00:19:22,920
fixed point representations.

290
00:19:22,920 --> 00:19:26,400
It works just fine across many different models.

291
00:19:26,400 --> 00:19:30,200
But if you try and do the same thing for training, things fail.

292
00:19:30,200 --> 00:19:35,360
It's actually kind of a funny point to me that we kind of made this implicit transition.

293
00:19:35,360 --> 00:19:39,200
CPUs commonly support a high performance double precision.

294
00:19:39,200 --> 00:19:46,200
CPUs don't, GPUs have historically optimized around single precision, so 32-bit floating

295
00:19:46,200 --> 00:19:49,200
point instead of 64-bit floating point.

296
00:19:49,200 --> 00:19:54,800
It turns out it's kind of expensive to do this in a GPU, to do 64-bit in a GPU, whereas

297
00:19:54,800 --> 00:20:00,720
it's pretty cheap in a CPU, because you don't have to replicate this thing, this unit

298
00:20:00,720 --> 00:20:01,720
very many times.

299
00:20:01,720 --> 00:20:02,720
Right.

300
00:20:02,720 --> 00:20:03,920
On a GPU, you have to replicate it a lot.

301
00:20:03,920 --> 00:20:08,160
So if you replicate something big a lot, it becomes expensive.

302
00:20:08,160 --> 00:20:13,960
It's kind of surprising that the whole industry, you know, when I started watching people train

303
00:20:13,960 --> 00:20:19,200
deep neural networks, they might write scripts in, you know, MATLAB or, you know, call CPU

304
00:20:19,200 --> 00:20:20,800
libraries directly.

305
00:20:20,800 --> 00:20:24,200
And those things, by default, use double precision.

306
00:20:24,200 --> 00:20:29,720
When the industry switched to GPUs, they switched from double precision to single precision.

307
00:20:29,720 --> 00:20:31,400
And we got so lucky.

308
00:20:31,400 --> 00:20:35,280
It turns out that it didn't really matter.

309
00:20:35,280 --> 00:20:37,320
But I think that was just by luck.

310
00:20:37,320 --> 00:20:39,000
And we tried doing the next step.

311
00:20:39,000 --> 00:20:44,920
We tried moving from single precision to half precision, so moving from 32-bit floating

312
00:20:44,920 --> 00:20:50,520
point to 16-bit floating point, things started failing all over the place.

313
00:20:50,520 --> 00:20:54,040
And what caused those failures?

314
00:20:54,040 --> 00:20:55,360
There were a lot of them.

315
00:20:55,360 --> 00:20:58,480
Let me try and draw a couple of big categories.

316
00:20:58,480 --> 00:21:02,000
One was just differences in range.

317
00:21:02,000 --> 00:21:06,320
So one of the points of having floating point, as opposed to fixed point, is that you have

318
00:21:06,320 --> 00:21:08,920
a very large dynamic range.

319
00:21:08,920 --> 00:21:14,680
You might, you know, be able to do an operation like an ad of a number that's, you know,

320
00:21:14,680 --> 00:21:16,480
where one number is a billion.

321
00:21:16,480 --> 00:21:20,360
And the other number is, you know, 10 to the minus 5.

322
00:21:20,360 --> 00:21:22,440
And that works.

323
00:21:22,440 --> 00:21:27,160
And so you need your range to extend from the smallest numbers that you want to deal

324
00:21:27,160 --> 00:21:30,440
with to the biggest numbers that you want to deal with.

325
00:21:30,440 --> 00:21:36,120
And it turns out that if you look at all of the operations that go on in four-propagation

326
00:21:36,120 --> 00:21:42,160
and backpropagation, the nonlinearities in the SGD algorithm, there's actually a pretty

327
00:21:42,160 --> 00:21:44,840
large dynamic range.

328
00:21:44,840 --> 00:21:48,800
Aren't we typically normalizing to try to get rid of some of that?

329
00:21:48,800 --> 00:21:50,520
Yeah, it's interesting.

330
00:21:50,520 --> 00:21:52,360
I'll come back to that point.

331
00:21:52,360 --> 00:21:55,360
It was an interesting thing related to that point.

332
00:21:55,360 --> 00:21:57,080
Yeah, let me come back to that.

333
00:21:57,080 --> 00:22:01,480
But I feel like the number one reason why when we just, so the first experiments we did

334
00:22:01,480 --> 00:22:08,520
were just convert all of the 32-bit numbers to 16-bit precision numbers and try using

335
00:22:08,520 --> 00:22:10,920
exactly the same algorithm.

336
00:22:10,920 --> 00:22:15,720
And also, and by due, you know, because we were working on speech recognition, we started

337
00:22:15,720 --> 00:22:18,480
doing this for recurrent neural networks.

338
00:22:18,480 --> 00:22:21,120
It turns out that was one of the harder examples.

339
00:22:21,120 --> 00:22:24,640
We started with one of the harder cases it turned out.

340
00:22:24,640 --> 00:22:26,720
And so we would see all sorts of failures.

341
00:22:26,720 --> 00:22:30,760
And what makes RNN particularly harder?

342
00:22:30,760 --> 00:22:33,920
I think it has to do with accumulated errors.

343
00:22:33,920 --> 00:22:39,680
So as you keep doing this repeated application of a matrix multiplication with the same weights,

344
00:22:39,680 --> 00:22:45,280
you're thinking about, or over time, this just encourages extreme values, either extremely

345
00:22:45,280 --> 00:22:47,720
small values or extremely large values.

346
00:22:47,720 --> 00:22:53,200
So sometimes people call this the vanishing gradients or exploding gradients problems.

347
00:22:53,200 --> 00:22:57,680
And for speech recognition, we see very long time series.

348
00:22:57,680 --> 00:23:02,560
We might see hundreds of iterations of an RNN or maybe thousands.

349
00:23:02,560 --> 00:23:07,360
And we saw, you know, large accumulated errors over time.

350
00:23:07,360 --> 00:23:16,000
One of the biggest sources we came across was when you're actually combining gradients with

351
00:23:16,000 --> 00:23:20,040
the gradient update with the master copy of the weights.

352
00:23:20,040 --> 00:23:25,320
So when you have this model, it turns out it seems like SGT just makes these repeated

353
00:23:25,320 --> 00:23:27,840
small updates to a model.

354
00:23:27,840 --> 00:23:31,880
And so if you look at it from a range perspective, there's a large difference in magnitude

355
00:23:31,880 --> 00:23:37,560
between the magnitude of the gradients and the magnitude of the weights.

356
00:23:37,560 --> 00:23:43,240
And so when you try and do operations on those, you know, numbers that have very different

357
00:23:43,240 --> 00:23:48,200
magnitudes, you get loss of information or you get errors.

358
00:23:48,200 --> 00:23:53,840
And that was one of the biggest problems we had with training in half precision.

359
00:23:53,840 --> 00:24:00,680
It seemed like, yeah, moving for some reason, the errors introduced from like the, in floating

360
00:24:00,680 --> 00:24:07,960
point, things work out well if the numbers are in different magnitudes, but not by too much.

361
00:24:07,960 --> 00:24:12,760
And so it turns out that the difference for single precision versus double precision

362
00:24:12,760 --> 00:24:17,960
was okay, but it ended up being borderline for multiple applications when we were looking

363
00:24:17,960 --> 00:24:21,720
at the difference between single precision and half precision.

364
00:24:21,720 --> 00:24:25,520
So we had to introduce some changes in order to deal with that.

365
00:24:25,520 --> 00:24:31,920
One of the questions that came up in this previous conversation with Shubo, in which we talked,

366
00:24:31,920 --> 00:24:37,240
we touched on some of this stuff, I think pretty tangentials, like the end of our conversation,

367
00:24:37,240 --> 00:24:43,960
I think if I remember correctly, but we're talking about a reduced precision and I think

368
00:24:43,960 --> 00:24:49,760
I asked the question like, you can reduce the precision in multiple places.

369
00:24:49,760 --> 00:24:57,280
You can reduce the precision in your weights, you can reduce the precision in your outputs.

370
00:24:57,280 --> 00:25:00,000
When you're talking about reduced precision, or you're talking, it sounds like you're

371
00:25:00,000 --> 00:25:04,720
talking about reduced precision everywhere, just running on reduced precision infrastructure

372
00:25:04,720 --> 00:25:10,080
on a reduced precision mode and not being particularly discriminating in terms of where

373
00:25:10,080 --> 00:25:11,280
you reduce the precision.

374
00:25:11,280 --> 00:25:13,040
Is that what you're referring to?

375
00:25:13,040 --> 00:25:17,160
Yes, we're trying to keep it simple.

376
00:25:17,160 --> 00:25:22,000
We feel like if it ends up getting very complex, then it's difficult for people to know how

377
00:25:22,000 --> 00:25:25,160
you would actually apply this to a real model.

378
00:25:25,160 --> 00:25:29,800
You get back into your architecture, feature engineering, complexity issues.

379
00:25:29,800 --> 00:25:35,280
We definitely didn't want to introduce this as another hyper parameter, or maybe this

380
00:25:35,280 --> 00:25:40,320
only works for a few layers, but it doesn't work in these places.

381
00:25:40,320 --> 00:25:45,360
You have to make this hard choice of deciding which ones to convert, which ones not.

382
00:25:45,360 --> 00:25:49,720
We wanted it just to be kind of like a switch, and you would turn on the switch, and you

383
00:25:49,720 --> 00:25:52,160
would get the performance improvement.

384
00:25:52,160 --> 00:25:58,560
I think we finally got to that point, but for this kind of reason, there were a lot of problems

385
00:25:58,560 --> 00:26:00,200
along the way.

386
00:26:00,200 --> 00:26:04,880
I mentioned the difference in magnitudes between the updates and the weights as a source

387
00:26:04,880 --> 00:26:06,720
of errors.

388
00:26:06,720 --> 00:26:12,160
The other big one was just accumulated errors in long dot products.

389
00:26:12,160 --> 00:26:19,520
It turns out that taking weights convert, quantizing them to 16-bit, and then doing multiplications

390
00:26:19,520 --> 00:26:24,280
of activations with those weights didn't introduce too many errors.

391
00:26:24,280 --> 00:26:28,520
But in neural networks, especially in recurrent neural networks, as layers get big, you end

392
00:26:28,520 --> 00:26:33,600
up with these long dot products, and so you're doing a running sum, kind of like over each

393
00:26:33,600 --> 00:26:40,200
row, or all of the inputs of a neuron.

394
00:26:40,200 --> 00:26:45,480
Each operation has an accumulated error, so everyone in the sequence is going to add some

395
00:26:45,480 --> 00:26:46,880
amount of error.

396
00:26:46,880 --> 00:26:52,360
And now we're not talking about error in the kind of machine learning modeling, since we're

397
00:26:52,360 --> 00:26:53,960
talking about floating point error.

398
00:26:53,960 --> 00:26:59,840
Yeah, we're talking about just, you know, you really wanted to do this multiply operation.

399
00:26:59,840 --> 00:27:01,200
You didn't get the exact result.

400
00:27:01,200 --> 00:27:04,800
We had to clamp it to a value that's representable by the computer.

401
00:27:04,800 --> 00:27:08,560
And so each time you do that, you introduce quantization error.

402
00:27:08,560 --> 00:27:14,920
And normally, as long as you have enough bits, the quantization error is small enough that

403
00:27:14,920 --> 00:27:18,200
it doesn't really affect the final result too much.

404
00:27:18,200 --> 00:27:22,800
Exactly what too much means is very application dependent, and into complex systems like neural

405
00:27:22,800 --> 00:27:27,240
networks, it's really hard to know how much error is too much error other than just trying

406
00:27:27,240 --> 00:27:29,480
it on a real application.

407
00:27:29,480 --> 00:27:34,600
But we found for real applications, like for speech recognition or for translation, the

408
00:27:34,600 --> 00:27:39,400
error introduced by doing ads in 16-bit was too much error.

409
00:27:39,400 --> 00:27:46,360
Models would diverge or models would achieve significantly worse accuracy than the 32-bit

410
00:27:46,360 --> 00:27:47,760
baselines.

411
00:27:47,760 --> 00:27:53,080
And so we went back to that and we tried a whole bunch of things, like we tried hierarchical

412
00:27:53,080 --> 00:27:58,000
reductions and a bunch of things that ended up just being complicated.

413
00:27:58,000 --> 00:28:01,760
And eventually we went back and looked at the circuits and came to the conclusion that

414
00:28:01,760 --> 00:28:05,800
it wasn't that expensive just to put in a 32-bit adder.

415
00:28:05,800 --> 00:28:11,520
So you have a bunch of 16-bit multipliers and then you have a few 32-bit adders.

416
00:28:11,520 --> 00:28:16,360
And if you look at the performance improvement that you get from that, it ends up being

417
00:28:16,360 --> 00:28:21,120
most of the performance improvement that you would have got if you would have built 16-bit

418
00:28:21,120 --> 00:28:24,440
multipliers and 16-bit adders.

419
00:28:24,440 --> 00:28:29,320
So yeah, we ended up with a mixed precision format.

420
00:28:29,320 --> 00:28:34,880
You end up doing multiplication in 16-bit but then the addition in 32-bit.

421
00:28:34,880 --> 00:28:36,520
And there are a lot of other things.

422
00:28:36,520 --> 00:28:39,720
We ended up looking at there's still some other failure cases but those are really the

423
00:28:39,720 --> 00:28:40,960
two big things.

424
00:28:40,960 --> 00:28:45,720
As long as you keep the master copy of weights in 32-bit and as long as you do all the

425
00:28:45,720 --> 00:28:52,120
additions in 32-bit, you can do all the multiplications and you can represent all the activations

426
00:28:52,120 --> 00:28:57,760
and intermediate copies of weights and weight gradients in 16-bit.

427
00:28:57,760 --> 00:29:02,400
To take a step back and make sure I understand why we're doing this, are we talking about

428
00:29:02,400 --> 00:29:07,640
performance and computational costs, are we talking about kind of unit compute costs

429
00:29:07,640 --> 00:29:12,840
for this chip by having narrower buses and things like that?

430
00:29:12,840 --> 00:29:17,000
Are we talking about training time performance?

431
00:29:17,000 --> 00:29:24,360
What are the factors that are driving us to say we want to do this and not just we can

432
00:29:24,360 --> 00:29:28,680
do it and reduce precision, we want to do this and reduce precision.

433
00:29:28,680 --> 00:29:29,680
Sure.

434
00:29:29,680 --> 00:29:31,760
Yeah, why do we want to do this and reduce precision?

435
00:29:31,760 --> 00:29:34,640
It's really so we can build more efficient hardware.

436
00:29:34,640 --> 00:29:36,840
Within without this technique you can just do a comparison.

437
00:29:36,840 --> 00:29:40,920
If you're building the same processor within without this technique, there's a fair amount

438
00:29:40,920 --> 00:29:42,240
of performance at play.

439
00:29:42,240 --> 00:29:48,360
It might be something like four to eight X difference in really both sides of it, total

440
00:29:48,360 --> 00:29:53,640
performance or energy per operation, which would translate into efficiency.

441
00:29:53,640 --> 00:30:02,240
By going to reduce performance or by going to reduce precision, we can increase some

442
00:30:02,240 --> 00:30:08,840
composite of performance and energy consumption by four to eight X, like nearly order of

443
00:30:08,840 --> 00:30:09,840
magnitude.

444
00:30:09,840 --> 00:30:15,280
Yeah, we could finish my six month model in maybe just a single month.

445
00:30:15,280 --> 00:30:19,040
And is it, I guess I'm trying to get at this question, I don't know if the question

446
00:30:19,040 --> 00:30:26,800
makes sense, but like, is it something inherent about the lower precision or is it the fact

447
00:30:26,800 --> 00:30:31,920
that the lower precision allows us to use new compute architectures that are faster in

448
00:30:31,920 --> 00:30:32,920
other ways?

449
00:30:32,920 --> 00:30:35,160
Oh, sure, definitely.

450
00:30:35,160 --> 00:30:39,600
So do you get this performance improvement on existing computers?

451
00:30:39,600 --> 00:30:43,440
You get some performance improvement because you're moving around less data, but it might

452
00:30:43,440 --> 00:30:47,800
be closer to two X, it really depends on whether you're compute bound or bandwidth bound,

453
00:30:47,800 --> 00:30:51,200
but the maximum might be more like two X.

454
00:30:51,200 --> 00:30:55,880
But if you build another computer, if you build a new processor that was optimized around

455
00:30:55,880 --> 00:30:59,520
this idea, you could do even better, you could realize the four to eight X.

456
00:30:59,520 --> 00:31:00,520
Okay.

457
00:31:00,520 --> 00:31:07,040
So low precision fundamentally allows you to do kind of train these neural nets by moving

458
00:31:07,040 --> 00:31:11,600
around less data, right, 16 bits instead of 64, for example.

459
00:31:11,600 --> 00:31:17,040
So you get some advantage in doing that, even if you're just in low precision mode on a

460
00:31:17,040 --> 00:31:24,120
general purpose computer, but it also allows you to build chips that are specific to running

461
00:31:24,120 --> 00:31:29,800
in low precision, and that gives you, that's where you get the big opportunity to bump up

462
00:31:29,800 --> 00:31:30,800
your speeds.

463
00:31:30,800 --> 00:31:31,800
Yes.

464
00:31:31,800 --> 00:31:32,800
Exactly.

465
00:31:32,800 --> 00:31:33,800
Okay.

466
00:31:33,800 --> 00:31:35,560
And so you were here talking about the actual chip.

467
00:31:35,560 --> 00:31:36,560
Is that correct?

468
00:31:36,560 --> 00:31:37,560
Oh, yeah.

469
00:31:37,560 --> 00:31:40,600
So we're going to talk about the Volta GPU from Nvidia.

470
00:31:40,600 --> 00:31:44,360
This is a, yeah, this is a collaboration within video.

471
00:31:44,360 --> 00:31:49,120
It's worth noting, you know, this hardware has been shipping for a while.

472
00:31:49,120 --> 00:31:53,160
But the side of it that we're talking about now is the validation that we've done on

473
00:31:53,160 --> 00:31:54,160
it.

474
00:31:54,160 --> 00:31:57,760
So we've actually shown that you can train models in low precision.

475
00:31:57,760 --> 00:32:04,640
We've looked at, you know, over 15 large scale, complete end to end deporting applications.

476
00:32:04,640 --> 00:32:07,920
So it's really easy to build hardware that gets great performance numbers, but isn't

477
00:32:07,920 --> 00:32:10,200
able to run any real algorithms.

478
00:32:10,200 --> 00:32:17,280
So from the point of view of low, of low precision, the Volta is like its general purpose, right?

479
00:32:17,280 --> 00:32:21,800
It's not a chip that specifically designed for low precision.

480
00:32:21,800 --> 00:32:25,840
Oh, it did actually have, they're called tensor cores.

481
00:32:25,840 --> 00:32:26,840
Right.

482
00:32:26,840 --> 00:32:31,600
It was the name for them is a tensor core that is this operation I'm talking about.

483
00:32:31,600 --> 00:32:32,600
Okay.

484
00:32:32,600 --> 00:32:38,880
It's a specialized unit that does 16 bit multiplication floating point with 32 bit floating point

485
00:32:38,880 --> 00:32:40,120
addition.

486
00:32:40,120 --> 00:32:43,000
That unit was designed as a result of the study.

487
00:32:43,000 --> 00:32:44,000
Got it.

488
00:32:44,000 --> 00:32:45,000
Got it.

489
00:32:45,000 --> 00:32:49,760
And now, if I remember correctly, when this was announced, they made a big deal about not

490
00:32:49,760 --> 00:32:53,920
the floating point side of things, but like N8 performance and things like that.

491
00:32:53,920 --> 00:32:55,320
How does that all fit in?

492
00:32:55,320 --> 00:32:56,320
Sure.

493
00:32:56,320 --> 00:32:57,320
Definitely.

494
00:32:57,320 --> 00:33:01,520
So I kind of alluded to this maybe in the beginning that inference just is easier for

495
00:33:01,520 --> 00:33:04,400
some reason than training.

496
00:33:04,400 --> 00:33:05,560
So that's all the inference.

497
00:33:05,560 --> 00:33:10,080
It's like we can do N8 on inference side and it is easy and it just works.

498
00:33:10,080 --> 00:33:11,080
And it's faster.

499
00:33:11,080 --> 00:33:12,080
Exactly.

500
00:33:12,080 --> 00:33:13,080
Got it.

501
00:33:13,080 --> 00:33:14,080
Okay.

502
00:33:14,080 --> 00:33:15,080
I don't know.

503
00:33:15,080 --> 00:33:17,880
I don't know that this whole topic has been really fully explored yet.

504
00:33:17,880 --> 00:33:22,840
Maybe someday in the future, we might see someone who gets in date training to work.

505
00:33:22,840 --> 00:33:26,560
But as far as I know, I've never seen it.

506
00:33:26,560 --> 00:33:30,920
I know there are a lot of, there's a lot of work on, you know, very reduced precision,

507
00:33:30,920 --> 00:33:33,120
like even down to binary.

508
00:33:33,120 --> 00:33:36,680
But one thing that's worth noting about these approaches is that they either have accuracy

509
00:33:36,680 --> 00:33:45,680
losses, so you trade precision for accuracy on the complete application or they only apply

510
00:33:45,680 --> 00:33:47,680
to inference and not to training.

511
00:33:47,680 --> 00:33:48,680
Mm-hmm.

512
00:33:48,680 --> 00:33:49,680
Okay.

513
00:33:49,680 --> 00:33:50,680
Got it.

514
00:33:50,680 --> 00:33:58,560
So, reduced precision, you did some validation that shows that essentially running in this

515
00:33:58,560 --> 00:34:01,360
mode is kind of a generalized approach you can take.

516
00:34:01,360 --> 00:34:06,160
Now, you know, things that you need to do or switches that you need to flip when you're

517
00:34:06,160 --> 00:34:11,760
training your model in order to get it to work accurately or to work correctly.

518
00:34:11,760 --> 00:34:12,760
Sure.

519
00:34:12,760 --> 00:34:16,560
So, one switch that you need to flip is you need to decide to do this.

520
00:34:16,560 --> 00:34:17,560
Okay.

521
00:34:17,560 --> 00:34:23,640
You need to decide to represent things in 16-bit and do your matrix multiplications or

522
00:34:23,640 --> 00:34:29,360
convolution operations in this mixed 16-bit, 32-bit format.

523
00:34:29,360 --> 00:34:33,160
That's somewhat of a global switch, you can just turn that on for the entire program.

524
00:34:33,160 --> 00:34:34,160
Okay.

525
00:34:34,160 --> 00:34:37,160
The other thing that you need to do that we found as essential is you need to master

526
00:34:37,160 --> 00:34:38,360
a copy of the weights.

527
00:34:38,360 --> 00:34:39,360
Mm-hmm.

528
00:34:39,360 --> 00:34:44,000
So, in your optimization algorithm, like your implementation of SGD, you need to have a

529
00:34:44,000 --> 00:34:48,840
separate copy in 32-bit of all the weights.

530
00:34:48,840 --> 00:34:53,360
And only when you're doing a Ford propagation or back propagation, do you convert from that

531
00:34:53,360 --> 00:34:54,600
into 16-bit?

532
00:34:54,600 --> 00:34:55,600
Okay.

533
00:34:55,600 --> 00:34:59,720
But, both of those changes we found are the first ones really easy.

534
00:34:59,720 --> 00:35:03,600
The second one can be encapsulated inside of the optimization algorithm.

535
00:35:03,600 --> 00:35:07,080
So at least when you're designing a network, you don't have to think about this.

536
00:35:07,080 --> 00:35:08,080
Okay.

537
00:35:08,080 --> 00:35:12,560
You know, I can envision how I might do this if I was writing the, you know, if I was

538
00:35:12,560 --> 00:35:18,000
implementing SGD myself to the higher level frameworks and toolkits all know how to

539
00:35:18,000 --> 00:35:21,600
do this or is that, you know, yet forthcoming.

540
00:35:21,600 --> 00:35:28,040
So it's straightforward to do this in most frameworks, but there needs to be developers who are

541
00:35:28,040 --> 00:35:30,920
working on those frameworks who will actually add support for this.

542
00:35:30,920 --> 00:35:31,920
Okay.

543
00:35:31,920 --> 00:35:36,120
You know, when we did this in the framework that we have in BIDU, it's something like a,

544
00:35:36,120 --> 00:35:37,920
you know, 15 lines of code change.

545
00:35:37,920 --> 00:35:38,920
Yeah.

546
00:35:38,920 --> 00:35:41,560
So it's really minor, but you still have to do that.

547
00:35:41,560 --> 00:35:44,920
Otherwise, you won't get access to the improved performance.

548
00:35:44,920 --> 00:35:45,920
Right.

549
00:35:45,920 --> 00:35:46,920
Okay.

550
00:35:46,920 --> 00:35:50,400
Anything else you talked about in your, or I keep saying it in past tense.

551
00:35:50,400 --> 00:35:54,760
Anything else you're going to talk about in your talk that you want to mention?

552
00:35:54,760 --> 00:35:59,480
I guess the last thing is that this is one piece.

553
00:35:59,480 --> 00:36:03,160
This is one technology that gives us a large improvement in performance.

554
00:36:03,160 --> 00:36:08,440
I think we're aware of a lot of them that haven't been realized yet.

555
00:36:08,440 --> 00:36:12,720
So you know, I mentioned before, the hardware industry for a long time has been kind of creeping

556
00:36:12,720 --> 00:36:13,720
along.

557
00:36:13,720 --> 00:36:18,960
It's actually very difficult to realize large improvements in sequential performance.

558
00:36:18,960 --> 00:36:25,600
But at least for parallel programs like graphics applications and things that would run on GPUs,

559
00:36:25,600 --> 00:36:29,040
performance has been increasing, you know, following something like the popular form of

560
00:36:29,040 --> 00:36:32,520
Moore's law, so exponential growth.

561
00:36:32,520 --> 00:36:37,680
For AI, if you're only thinking about running deep neural networks, you can probably do

562
00:36:37,680 --> 00:36:40,560
a lot better than that in the short term.

563
00:36:40,560 --> 00:36:43,960
So we might not have to wait 10 years to get 1,000 X faster.

564
00:36:43,960 --> 00:36:46,400
It might happen in just a few years.

565
00:36:46,400 --> 00:36:51,040
I'm going to mention some of the other ways that haven't been implemented yet, but that

566
00:36:51,040 --> 00:36:53,840
we know about it are likely to happen in the future.

567
00:36:53,840 --> 00:36:54,840
Can you rattle those off?

568
00:36:54,840 --> 00:36:58,160
This podcast will not be published before you're talked tomorrow.

569
00:36:58,160 --> 00:37:00,160
Sure.

570
00:37:00,160 --> 00:37:02,200
One of them is array parallelism.

571
00:37:02,200 --> 00:37:04,840
I think this is one of the other big ones is array parallelism.

572
00:37:04,840 --> 00:37:05,840
Okay.

573
00:37:05,840 --> 00:37:10,960
I had a Forbes article about this for us talking about locality, the importance of locality.

574
00:37:10,960 --> 00:37:17,880
If you build processors around the idea of locality and parallelism, not just parallelism, you

575
00:37:17,880 --> 00:37:22,960
end up with something that it looks, I call it like an array processor rather than a vector

576
00:37:22,960 --> 00:37:23,960
processor.

577
00:37:23,960 --> 00:37:24,960
Okay.

578
00:37:24,960 --> 00:37:28,640
You're thinking about the core instruction that you're doing instead of adding or multiplying

579
00:37:28,640 --> 00:37:33,120
two vectors together, you're adding or multiplying two arrays together.

580
00:37:33,120 --> 00:37:36,280
So you see things like this in designs like the TPU.

581
00:37:36,280 --> 00:37:40,160
I feel like the thing that is wrong with those designs is that they don't find the knee

582
00:37:40,160 --> 00:37:45,680
of the curve that this is beneficial, but you don't have to go all in on it to get most

583
00:37:45,680 --> 00:37:48,400
of the benefits and you actually are trading off.

584
00:37:48,400 --> 00:37:51,560
So you don't find the knee of the curve, what exactly does that mean?

585
00:37:51,560 --> 00:37:56,240
It means working on a raise is a good idea, but they don't have to be enormous arrays.

586
00:37:56,240 --> 00:37:57,240
Okay.

587
00:37:57,240 --> 00:38:02,280
And you actually are trading off flexibility for performance when you're making the arrays

588
00:38:02,280 --> 00:38:03,440
bigger.

589
00:38:03,440 --> 00:38:05,600
So you shouldn't make them enormous.

590
00:38:05,600 --> 00:38:09,840
You should make them big enough to get most of the savings and energy.

591
00:38:09,840 --> 00:38:14,520
In terms of order of magnitude, we're talking about like little teeny ones, like convolutional

592
00:38:14,520 --> 00:38:19,000
kernel sizes, are we talking about, you know, something bigger than that?

593
00:38:19,000 --> 00:38:20,000
Or...

594
00:38:20,000 --> 00:38:25,760
Yeah, it's like more like 16 by 16 than 256 by 256, that makes sense.

595
00:38:25,760 --> 00:38:26,760
Okay.

596
00:38:26,760 --> 00:38:27,760
Yeah.

597
00:38:27,760 --> 00:38:28,760
Interesting.

598
00:38:28,760 --> 00:38:31,120
Any others on that list that come to mind?

599
00:38:31,120 --> 00:38:36,160
One of the ones that doesn't work yet, but I think is very promising, is sparsity.

600
00:38:36,160 --> 00:38:37,160
Okay.

601
00:38:37,160 --> 00:38:40,400
We're talking with sparse representations rather than dense representations.

602
00:38:40,400 --> 00:38:46,600
And it might seem like that's incompatible with the one that I just said, the array parallelism.

603
00:38:46,600 --> 00:38:49,760
We haven't shown this yet, but I suspect that they're not incompatible.

604
00:38:49,760 --> 00:38:54,240
Well, what I'm hearing putting the two together is that, you know, we're living in a world

605
00:38:54,240 --> 00:38:58,120
that thinks about all this stuff as composite vector operations.

606
00:38:58,120 --> 00:39:02,640
And by thinking about this at the level of matrices, you know, there are opportunities

607
00:39:02,640 --> 00:39:03,640
there.

608
00:39:03,640 --> 00:39:04,640
Yes.

609
00:39:04,640 --> 00:39:05,640
Yeah.

610
00:39:05,640 --> 00:39:06,640
That's a good way of thinking about it.

611
00:39:06,640 --> 00:39:07,640
Interesting.

612
00:39:07,640 --> 00:39:09,360
Well, I really enjoyed this chat.

613
00:39:09,360 --> 00:39:11,240
Thank you so much for taking the time.

614
00:39:11,240 --> 00:39:12,240
Good to be here.

615
00:39:12,240 --> 00:39:13,240
Great.

616
00:39:13,240 --> 00:39:14,240
Thanks, Craig.

617
00:39:14,240 --> 00:39:15,240
Thank you.

618
00:39:15,240 --> 00:39:19,000
All right, everyone.

619
00:39:19,000 --> 00:39:21,120
That's our show for today.

620
00:39:21,120 --> 00:39:26,360
Thanks so much for listening and for your continued feedback and support.

621
00:39:26,360 --> 00:39:32,080
Thanks to you, this podcast finished the year as a top 40 technology podcast on Apple

622
00:39:32,080 --> 00:39:33,600
podcasts.

623
00:39:33,600 --> 00:39:38,240
My producer says that one of his goals this year is to crack the top 10.

624
00:39:38,240 --> 00:39:42,360
And to do that, we will need your help.

625
00:39:42,360 --> 00:39:45,960
Please head on over to the podcast app, rate the show.

626
00:39:45,960 --> 00:39:48,200
Hopefully we've earned your five stars.

627
00:39:48,200 --> 00:39:53,200
Leave us a glowing review and share it with your friends, family, co-workers, Starbucks

628
00:39:53,200 --> 00:39:56,640
baristas, Uber drivers, everyone.

629
00:39:56,640 --> 00:39:59,160
Every review and rating goes a long way.

630
00:39:59,160 --> 00:40:01,560
So thanks in advance.

631
00:40:01,560 --> 00:40:06,560
For more information on Greg or any of the topics covered in this episode, head on over

632
00:40:06,560 --> 00:40:10,760
to twomolei.com slash talk slash 97.

633
00:40:10,760 --> 00:40:16,240
Of course, we'd be delighted to hear from you, either via a comment on the show notes page

634
00:40:16,240 --> 00:40:19,400
or via Twitter at at Twomolei.

635
00:40:19,400 --> 00:40:37,240
Thanks once again for listening and catch you next time.

