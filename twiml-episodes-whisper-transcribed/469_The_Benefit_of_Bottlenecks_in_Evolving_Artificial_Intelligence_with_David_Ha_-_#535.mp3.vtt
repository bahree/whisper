WEBVTT

00:00.000 --> 00:16.240
All right, everyone. I am here with David Ha. David is a research scientist at Google Brain.

00:16.240 --> 00:20.080
David, welcome to the Twomal AI podcast. Thanks for having me, Sam.

00:21.280 --> 00:28.160
Hey, I'm really looking forward to diving into our conversation. I've been a long time follower

00:28.160 --> 00:34.400
viewers on Twitter, and I definitely recommend folks to check you out there at Hard Maroo.

00:34.400 --> 00:39.120
Why don't we get started by having you share a little bit about your background and how you came

00:39.120 --> 00:48.560
to work in AI? Yeah, sure. It's kind of a weird background. I was originally studying

00:49.680 --> 00:56.160
control systems back in the day in university. Eventually, for some one reason or another,

00:56.160 --> 01:06.400
I entered the finance industry. I started off as a quants on Wall Street, actually.

01:07.120 --> 01:13.200
I started working at banks, and then eventually I became worked on a trading desk as a trader,

01:13.200 --> 01:19.920
and I spent around 10 years or so of my life in the derivatives trading, various different investment

01:19.920 --> 01:28.480
banks, but things got a bit old and tried to learn different things. I was always interested in

01:28.480 --> 01:35.200
neural networks because they're always fascinating, especially the biological inspired component,

01:35.200 --> 01:43.280
and I started to do some reading and learning by myself. One thing that to the other, and

01:43.280 --> 01:52.800
around the five years ago, I was able to join Google in one of their research residency programs,

01:52.800 --> 02:00.720
and as a researcher, so that I was able to change careers and became a four-time AI researcher.

02:01.680 --> 02:10.400
And so this is where I am now. That's awesome. That's awesome. As the idea or the attraction,

02:10.400 --> 02:15.120
the initial attraction to the biological inspiration, has that held up for you? Do you

02:16.640 --> 02:26.320
feel like the biological inspiration continues to inspire you, or was it a let down to find out

02:26.320 --> 02:33.040
that the neural networks and computers are not all that similar to the biological ones?

02:33.040 --> 02:40.160
I think to this day is still continues to inspire me and drives some of my work,

02:41.040 --> 02:48.000
but we do have to recognize that modern deep learning or machine learning systems are very

02:48.000 --> 02:55.040
different than biological processes. For one thing, we can scale them up. We have lots of

02:55.040 --> 03:03.760
electricity and compute power, and the trend is actually having more compute resources,

03:03.760 --> 03:10.080
and for machine learning and training to increasingly scale to larger models and larger datasets

03:10.080 --> 03:16.800
and larger environments. And it's a bit different than biology, because in biological systems,

03:16.800 --> 03:24.800
it's more like a biological intelligent life is more like coming from and evolving because we

03:24.800 --> 03:30.320
have not because we have an abundance of resources, but more like we have a lack of it.

03:30.960 --> 03:39.200
And I was fascinated at how evolution seems to select systems that are able to always do more

03:39.200 --> 03:47.840
with less. But in a way, it's not just biological systems, but also the creativity process as well.

03:47.840 --> 03:55.200
Sometimes we see some creative works. It's always like you're able to express more with less.

03:55.200 --> 04:04.560
And I think the good, the interesting thing about being a researcher, especially at Google is

04:04.560 --> 04:10.880
you do have a lot of resources. So you get to see both ends of the spectrum. On one hand,

04:10.880 --> 04:20.320
you do get to see people who are really excited at scaling up the research and making very large

04:20.320 --> 04:26.000
systems to work on large datasets. And on the other end of the spectrum, you have people working

04:26.000 --> 04:34.240
on theory or on coming from theoretical physics backgrounds. And actually, they may not even

04:34.240 --> 04:43.200
do a lot of extensive computational modeling. So it's good to see a balance of the spectrum of

04:43.200 --> 04:53.600
a resource-heavy stuff, and also the things that concentrate on having very low resources.

04:54.480 --> 05:01.520
And ultimately, you need both. You can have large models and you have a smart chips that run them

05:01.520 --> 05:07.920
with less power. You alluded to this idea of constraints as playing a role in the way you think

05:07.920 --> 05:16.960
about machine learning systems. Can you elaborate a bit on that? Yeah. Getting back to the idea of

05:16.960 --> 05:25.760
constraints we see in nature, it certainly plays a role in shaping some of the research work

05:25.760 --> 05:34.320
that we've been doing. In nature, for example, there are lots of examples of these so-called

05:34.320 --> 05:41.120
bottlenecks that shape their development as a species. Just looking at the fascinating way

05:41.120 --> 05:49.040
of how our brain is wired, how our consciousness is able to process abstract thought.

05:49.600 --> 05:53.040
We have a language I'm talking to you in language, even though we have a video feed.

05:53.040 --> 06:00.000
And also how we're able to convey concepts to each other, not just using languages,

06:00.000 --> 06:07.200
but using drawings or gestures like this. And that's developed into languages, stories and

06:07.200 --> 06:16.800
cultures. I guess to me, it's kind of debatable whether these bottlenecks or constraints

06:16.800 --> 06:23.120
from our development is a requirement for intelligence to emerge. But it's also not

06:23.120 --> 06:30.320
deniable that our own intelligence is a result of these constraints. On one hand,

06:30.320 --> 06:38.400
maybe the argument is just because we have constraints that led to us, it doesn't mean we cannot,

06:38.400 --> 06:46.080
I have the development of a general, a strong AI needs to have such constraints.

06:46.080 --> 06:51.680
So there are opposite views of this spectrum. But for my research work, it's more like a

06:51.680 --> 07:00.800
led by the idea of constraints. And you can see this from some of the work I've done even very

07:00.800 --> 07:06.400
earlier, a few years ago, when I started to get into things like generative models.

07:08.000 --> 07:14.960
Back then, GANs were really taking off in 2017 or 2016. They started to take off

07:14.960 --> 07:23.680
and at the beginning, people were really excited at generating C-Fart 10 images, 32 by 32 pixels.

07:23.680 --> 07:32.720
But then they got bigger, 64 by 64, 128 by 128, pictures of a datasets of CD albums or something

07:32.720 --> 07:38.720
really cool. And so there's all this exciting work going on. And I had my share of playing around

07:38.720 --> 07:50.320
with these generative models as well. So a few early works I've done is to build a generative model

07:50.320 --> 07:58.640
for MNIST, the simplest dataset ever. But rather than taking the approach of generating pixels

07:58.640 --> 08:07.440
directly, I try to generate a parameterization of MNIST, which can be very abstract in nature.

08:07.440 --> 08:15.360
And with that led to like an early work, I combine some of the models from another researcher,

08:15.360 --> 08:23.120
Ken Stanley at the time, who designed this network called CPPNs. So if you're familiar with that,

08:23.120 --> 08:27.760
it's more like you take in the pixels. And you take in the coordinates and it outputs a pixel.

08:28.480 --> 08:35.440
So if you have a simple rule that can take in the coordinate and output a pixel,

08:35.440 --> 08:39.840
then you actually don't need to train the network to output the entire pixel. You just train

08:39.840 --> 08:45.680
the network to simply give you the coordinate, I'll give you the pixel value. So I train such a

08:45.680 --> 08:52.960
network to generate MNIST, so it's very elegant. And the end process is you can actually train it

08:52.960 --> 09:00.480
on an MNIST dataset, 28 by 28, into this half-stract generative model, which I call a CPPN,

09:00.480 --> 09:08.960
EAE or GANs. And then you can actually blow it up and generate MNIST digits that are like

09:08.960 --> 09:20.480
1,000 by 1,000 resolution back in 2016. So before people can do, now we can actually model GANs on

09:20.480 --> 09:29.680
1,000 by 1,000 resolution datasets with our exponentially increasing hardware. But I thought at the

09:29.680 --> 09:38.960
time, it was kind of cool to be able to train again in 2016. Right after the EN Goodfellow is

09:38.960 --> 09:44.960
again paid for K-vogue for a few months, and you're able to also produce 1,000 by 1,000 resolution

09:44.960 --> 09:52.400
images by skipping entirely the need to produce such big images. And the key is to abstract the

09:52.400 --> 10:03.520
principles of that image into an abstract representation using the CPPNs. And later work, I kind of

10:03.520 --> 10:10.080
follow the same trend. I looked at creating a generative model of doodles. There's a model,

10:10.080 --> 10:16.480
I don't know, you probably played around with it called a sketch RNN, that you can interactively

10:16.480 --> 10:22.560
draw something on the web browser. And the model is like an anguish model. It'll continue to

10:22.560 --> 10:28.640
predict what you're going to draw, like a stroke by stroke in a vector format. It's also like

10:28.640 --> 10:35.600
an auto encoder model as well. So you can draw a full pig and then you can compress it into a

10:35.600 --> 10:41.840
lean space and then redraw the pig out. So now is a very trivial in retrospect. But a few years ago,

10:41.840 --> 10:53.440
it was one of the different models, because most people working on GANs and generative models

10:53.440 --> 10:59.840
on pixels. And we're trying to do it on doodles. And at the time, the challenging thing was finding

10:59.840 --> 11:10.320
the datasets. And luckily, one of my colleague, Jonas, at Creative Labs, they created a viral

11:10.320 --> 11:16.640
game called a quick draw that collected some of this doodle data that we can use. So I thought

11:16.640 --> 11:26.080
that was kind of fun, man. For that project, it was more inspired by, rather than trying to create

11:26.080 --> 11:32.560
a representation, a minimalist representation, maybe we can use machine learning to study how

11:32.560 --> 11:42.240
we humans ourselves do like a representation learning. Because of our own inductive biases,

11:42.240 --> 11:49.440
you know, we're forced to draw doodles, maybe from the time we're cave people, because we have

11:49.440 --> 11:55.200
a we have hands and we have sticks. So we develop this type of drawing. So maybe it's, you know,

11:55.200 --> 12:01.200
a good idea, a good idea to get machine learning models to analyze how we develop this

12:01.200 --> 12:09.040
representation. And that could lead to other ideas. And one of those, the ideas after

12:09.040 --> 12:15.920
this sketch RNN paper was when I started to get into doing some work on reinforcement learning.

12:17.680 --> 12:23.680
And, you know, like, what I thought was we have all of these cool algorithms that can train agents

12:23.680 --> 12:31.120
to perform tasks when the agents are fed pixels like the entire screen, which was really amazing

12:31.120 --> 12:38.960
at the time. At the time, the DQM model from the mine came out and agents started playing

12:39.520 --> 12:46.240
pong or like Atari games, the entirety of pixels. I thought that was cool. But in a way, I kind of

12:47.440 --> 12:52.640
think that could be information overload as well as most of the pixels are not useful when you're

12:52.640 --> 13:00.880
playing like pong games or Atari games. So what I try to do is like, you know,

13:02.400 --> 13:11.120
have enforced a type of constraints onto the policy or the controller so that it's not allowed

13:11.120 --> 13:17.520
to see the full pixel information or the stream of pixels. And it's only allowed to see a

13:17.520 --> 13:25.840
representation of its environments. So that, that work led to a model, a paper called the

13:25.840 --> 13:33.360
World Models. It's kind of an exploratory paper that I published a few years ago with

13:33.360 --> 13:40.800
Jürgen Schmittuber. And it was a really fun project. So the idea is we have a really simple

13:40.800 --> 13:48.320
generative model. We use a variational auto-encoder to simply compress all of the screens into a

13:48.320 --> 13:54.640
load-dimensional latent space. And we have another recurring neural network that simply predicts

13:54.640 --> 14:00.320
the future latent space of the environments. So if you have a VAE that's trained on,

14:00.880 --> 14:06.800
trained on your your game that produces a load-dimensional latent vector, your R&M will predict

14:06.800 --> 14:11.200
your future latent vectors. That, that, like, depending on its current.

14:11.200 --> 14:12.960
Are you projecting the future state of the game?

14:13.600 --> 14:23.040
Yes, exactly, exactly. So that was a fun project because we're able to use these two simple concepts

14:23.040 --> 14:33.120
to build a neural simulator of, like, games if we're able to connect enough data on it. And

14:33.120 --> 14:38.560
what, what was fun about the project is we can, we show that we can just feed in

14:40.080 --> 14:44.880
the representations learned from such a model, like the VAE's latent vector and also the

14:44.880 --> 14:51.200
R&M's hidden state and feed it to an agent. And, like, at the time, this, we show that,

14:51.760 --> 14:59.120
this hidden vector, like, this small, this bottleneck allowed the agents to, to discover policies

14:59.120 --> 15:06.640
much, much more easier than compared to, you know, like, having to see the entire pixel information

15:06.640 --> 15:11.040
is from an optimization standpoint. It's easier to figure out what you have to do if you're only

15:11.040 --> 15:19.120
given, like, maybe 200 numbers and give me an action compared to if you're given, like, you know,

15:19.120 --> 15:23.760
a million numbers every time set, give me an action, right? So because of that,

15:23.760 --> 15:30.400
it was able to, like, solve tasks like the car racing game and open the agent.

15:31.760 --> 15:37.360
Back then, it was, like, considered a hard task. Now it's trivial. But no one was able to get

15:37.360 --> 15:43.520
the required score. I'm sure people tried hard enough, they could. But at the time, this was

15:43.520 --> 15:49.600
the first approach that was able to get the required score for that game, you know, which was,

15:49.600 --> 15:56.800
I guess, from, from the machine learning research point of view, it was considered state of the art.

15:59.200 --> 16:04.880
I'm sure it'd be tweaked. Any model of it enough, you can also beat it. But, but apart from that,

16:06.080 --> 16:15.840
with those kind of results, have we seen that idea of, you know, constraining your latent space

16:15.840 --> 16:24.080
who come generally used as part of state of the art approaches in RL and similar areas?

16:24.960 --> 16:31.120
Yeah, yeah, definitely. So from that, from that paper, I think, you know, whether we got state

16:31.120 --> 16:35.200
of the art or not, it doesn't really matter for, in general, in machine learning papers, because

16:35.200 --> 16:43.200
it'll always be beaten by later on. But the idea on that paper of that, you can, you can train

16:43.200 --> 16:50.080
and you can learn a janitor model and train the agents entirely inside of that model to produce

16:50.080 --> 16:58.800
a policy. That, that was the main idea that, that seems to have taken off in subsequent works.

17:00.240 --> 17:07.360
Like, for example, after the world model's paper, there was another paper about, about

17:07.360 --> 17:16.080
the model base learning for Atari. So, and where they literally caught their algorithm simple.

17:17.040 --> 17:21.440
And the idea is basically, okay, you would, you would, you would, you have your agents collect data,

17:22.160 --> 17:30.480
train a generative model of the environment to predict the future. And then, and then, and then train

17:30.480 --> 17:36.000
your policy inside of that environment only. Right? Of course, at the beginning, you're not going to

17:36.000 --> 17:40.240
get a good policy, but then it doesn't matter. You deploy that policy out and you collect

17:40.240 --> 17:46.560
more data. And then you would refine your model and then you would redeploy it. So, so at,

17:46.560 --> 17:53.920
when that work came out in 2019, at the time, that was then the state of the art for, for,

17:54.960 --> 18:01.680
for sample efficiency for various Atari games, because simply because the learning took place in

18:01.680 --> 18:09.600
the model. Because a lot of the sample efficiency, we've been, if we noticed, when we run an

18:09.600 --> 18:15.840
RL algorithm, is, you know, you have the data collection process, but you're also learning

18:15.840 --> 18:22.080
Indian environments. And that, that could, there could be some slippage in the efficiency.

18:22.080 --> 18:29.360
So, if you're able to isolate the data collection from, from policy learning and your interactions

18:29.360 --> 18:37.280
through the environment is strictly for data collection and for evaluation policy of your policy.

18:37.280 --> 18:40.960
And all of your learning is done in the model. Then, then intuitively, that would help,

18:42.560 --> 18:48.320
that would also help the data efficiency. And another line of work done by my colleague, Danny

18:48.320 --> 18:56.160
J.R. Halfner, who is also working at a Google, but based in Toronto is, is a new, he started using

18:56.160 --> 19:03.040
these latent based role models and combined them with planning algorithms. So, like traditionally,

19:03.040 --> 19:09.280
planning algorithms are really useful for robotics. But at the same time, they're kind of flaky as

19:09.280 --> 19:16.160
well, especially when it comes to, when you're getting the, like, for example, four video feeds

19:17.200 --> 19:25.840
of sensory data, maybe traditionally, a lot of the planning algorithms were used on the state

19:25.840 --> 19:32.320
observations. So, you're feeding, like, you know, like a really well engineered measurements

19:32.320 --> 19:37.280
of your robot controller, right? But, like, how, then, how the key question is, how do you get

19:37.280 --> 19:45.920
your robot controller or your control system to work on video feeds? So, then, I guess,

19:45.920 --> 19:52.640
something like a world model or a latent based world model with, with this latent bottleneck

19:52.640 --> 19:58.800
could be useful for planning algorithms, because then you, if they're really good at working with

19:58.800 --> 20:06.640
load-dimensional data. So, then you give it load-dimensional data. So, the key idea behind that line of

20:06.640 --> 20:13.760
work initially started by a model called a planet. So, it's a great kind of name is to, you have

20:13.760 --> 20:20.880
you have this kind of latent, latent spatial temporal world model that is constantly updated as you

20:20.880 --> 20:28.000
get more data, and you have a planner that would, that would get, that would figure out the optimal

20:28.000 --> 20:33.200
action within the model. So, then, you don't actually go into doing any learning.

20:34.480 --> 20:41.040
This is also helped with generalization. You'd think that a lower latent space model

20:42.080 --> 20:49.280
has gotten rid of some of the noise that the full, the full world or environment

20:49.280 --> 20:57.360
contains. And so, the agent might be able to perform, the agent performance might transfer

20:57.360 --> 21:01.760
from one specific environment to another better. Is that actually the case?

21:02.880 --> 21:12.160
That is still like an open question. For instance, if I naively train that world model based

21:12.160 --> 21:19.600
on based on the data that we collect, then like, no, it's not going to generalize to variations.

21:21.040 --> 21:30.400
An example is if we change the background color of your environments, then yeah, your VAE

21:30.400 --> 21:36.160
or your latent model has never seen that before. And that generalization can only be done

21:36.160 --> 21:45.280
in VAE learning. So, that algorithm would need to collect the new data and relearn its world again.

21:47.280 --> 21:56.800
And whether it can generalize or not will be a question of how many shots, how many time

21:56.800 --> 22:04.000
steps it has to generalize rather than a zero shot. But that being said, there is like a line of

22:04.000 --> 22:12.640
work on looking at generalization problems within latent space models. There's actually a few

22:12.640 --> 22:20.000
challenges. There's a deep mind robotic control, have a variation with explicitly introduced

22:20.000 --> 22:29.120
lots of distractions and changing the backgrounds. And then you can employ lots of all sorts of ways,

22:29.120 --> 22:34.320
like rather than training like an image-based latent space, you can do contrasts of learning.

22:34.320 --> 22:43.520
There's a line of work doing that and so on. But for me, around that time, I also step back a bit.

22:44.240 --> 22:48.960
There's all these, I have the same question as you, you know, we can this generalization.

22:48.960 --> 22:54.480
Yeah, of course, there's lots of different ways of doing these latent space models.

22:54.480 --> 23:02.320
But I looked at, along with my colleagues and my team, we started to explore them.

23:02.320 --> 23:08.800
It's maybe latent space bottlenecks is one solution, but it might not be the best solution

23:08.800 --> 23:16.160
for these generalization tasks. So we looked at maybe another bottleneck we can use is

23:16.160 --> 23:24.160
something like attention. Or in our case, we try to use a hard attention. So like in a paper,

23:24.160 --> 23:30.160
we published two years ago called a neural evolution of self-interpretable agents,

23:30.160 --> 23:37.360
which is let by my colleague, Eugene Tang, rather than using a latent space to do this bottleneck.

23:38.560 --> 23:46.480
The idea is we will only allow an agent to see 10 patches, for instance, of the screen.

23:46.480 --> 23:55.840
And its decision is solely based on those 10 patches. Or however, however, number of patches we want.

23:55.840 --> 24:05.920
So it's kind of like, you know, biological vision for our fovea type system where we have to really,

24:05.920 --> 24:11.200
you know, like a one when when we study how humans see things, it's always like, you know,

24:11.200 --> 24:20.640
attending to a bunch of points in front of us. But somehow we have a mental understanding of what

24:20.640 --> 24:26.000
we're seeing. But it's not like we're getting like, you know, full on HD resolution directly in my

24:26.000 --> 24:31.600
eyes. I'm actually seeing a bunch of things. So it's kind of inspired by that.

24:32.080 --> 24:39.920
And in this example or paper is the where the patches, were you trying to emulate like a visual

24:39.920 --> 24:44.800
field and the patches were kind of contiguous in a particular arrangement or were they,

24:45.920 --> 24:51.520
you know, randomly distributed across the image. Oh yeah, so for this work,

24:52.960 --> 25:01.200
it's part of the policy actually. So rather than having a randomized frame, the agent actually

25:01.200 --> 25:12.080
has to learn to decide first which 10 patches to choose. Right. So like it's like it's like how

25:12.080 --> 25:18.960
when you're looking at me, somehow you're deciding where to position your your eyeball on the

25:18.960 --> 25:26.160
screen. So in the same way, the agent has to decide which 10 or it doesn't have to be 10. It could

25:26.160 --> 25:32.080
be one or two that don't still work. But we do a sweep and we can be a bit more general and then

25:32.080 --> 25:39.680
choose five or 10. So it's less like I was originally thinking it was along the lines of masking

25:39.680 --> 25:46.800
for kind of generalization or regularization. This is more learning where to attend to within the

25:46.800 --> 25:55.920
image as a as a constraint. Yes, exactly. And it was it's also inspired by a line of work in

25:55.920 --> 26:03.680
psychology a few decades ago. The whole concept of a script you heard about is this in

26:03.680 --> 26:16.640
in a tentative selective attention. So this is an intentional blindness. So sometimes our brains

26:16.640 --> 26:22.560
just don't see part of the screen or part of what we see. So there was a psychology experiment done

26:22.560 --> 26:32.160
back then where the subjects were asked to to look at a scene and the scene had two two teams of

26:32.160 --> 26:38.960
basketball players. One wearing white shirts and the second one wearing black shirts. And I think

26:38.960 --> 26:45.600
the subjects have to count the number of times that's the ball was passed between the white shirt

26:46.400 --> 26:50.400
the players to the black shirt players and something like that. And then there's a gorilla working

26:50.400 --> 26:56.400
walking up from the background. And most of the time the subjects were not able to to see the gorilla

26:56.400 --> 27:03.920
because they're so focused on the ball and the colors of what people are wearing. So that kind of

27:03.920 --> 27:14.080
helped create some some analogies between okay we have this thing called whether we like it or not

27:14.080 --> 27:20.320
called intentional blindness. What if we try to do something like that with an RIO agents? What are

27:20.320 --> 27:25.920
the pros and cons? Does it give it more abilities or does it actually deduct some of the abilities?

27:25.920 --> 27:31.440
And that's that's what we're trying to explore. And it turns out that's using this simple scheme

27:32.000 --> 27:37.760
we were also able to train some simple agents to do the same task as the world models paper

27:37.760 --> 27:42.960
like getting a pretty good score on a car racing game from pixels and playing the dune game.

27:42.960 --> 27:50.240
But unlike the previous latent space models, this model we found can easily adapt

27:50.240 --> 27:58.960
to to augmentations to the environments. Like for instance, if we in doom in that doom

27:58.960 --> 28:06.720
this in environments if we change the color of the ground it will still work. If we add like a

28:06.720 --> 28:12.320
little blob on the side of the tracks on the car racing game, it will still work. And the reason

28:12.320 --> 28:20.960
is those patches are likely not to be selected by the pre-trained agents. So so it's just simply

28:21.600 --> 28:26.960
it works because it's just not attending to the things that deemed not to be that relevant

28:28.400 --> 28:35.760
to some extent. Of course this is very like a naive way of like approaching the problem

28:35.760 --> 28:41.520
because like in reality it's very nuanced like we do see a bit of it but it's kind of a simple

28:41.520 --> 28:49.040
model that that clearly demonstrates that in attentional blindness if we strictly enforce it

28:49.040 --> 28:55.200
in the context of NRL agents it will have these properties that because it's simply not allowed

28:55.200 --> 29:01.360
to see certain parts of the screen it may lack if you throw away information but you also gain

29:01.360 --> 29:10.880
ability to to generalize to to changes in the environments. Yeah yeah and does it how does it compare

29:11.760 --> 29:18.480
from a sample efficiency to the constrained latent space does it retain that advantage in some way?

29:19.440 --> 29:26.720
For for this one no no because it actually takes a bit more time to train or to evolve the policy

29:26.720 --> 29:33.680
for to be able to perform the task but but the way I think about these issues is that there's

29:33.680 --> 29:41.920
a few dimensions you know you can you can work on optimizing the sample efficiency like maybe reducing

29:41.920 --> 29:49.840
an RL algorithm from you know 200 million time steps to 100 million time steps to achieve some

29:49.840 --> 29:57.440
score or you can think of a sample efficiency in terms of of a zero shot transfer so one can

29:57.440 --> 30:04.080
argue that okay I spent all of this time figuring out the policy using a hard attention in this

30:04.080 --> 30:11.600
paper but if you give it a new environment which is not the same as the original environment but

30:11.600 --> 30:18.640
one that has has some augmentations to it we can argue that that's a new environment and and how

30:18.640 --> 30:24.000
many time steps would it take your agents to adapt to that new environment and here the case is

30:24.000 --> 30:30.800
zero because it's a zero shot so that's also like another way of looking at the sample efficiency

30:30.800 --> 30:37.360
as well not not on the training task but on the task that it has never seen in life kind of arguing

30:37.360 --> 30:43.600
for a global sample efficiency in a sense across multiple problems or versions of a problem

30:43.600 --> 30:50.640
is exactly or in in our case I think I'm really interested in in in sample efficiency across

30:50.640 --> 30:59.120
on the same versions of the problem and that that's basically what what one of the the goals

30:59.120 --> 31:07.520
of of AI is it's like of course given enough compute we're we're gonna solve every known problem

31:07.520 --> 31:12.880
that is well defined but one of the thing is that this thing which is us from machines so far is

31:12.880 --> 31:20.080
like our ability to to solve problems quickly that that we have not seen before with variations

31:21.120 --> 31:29.200
you mentioned earlier your work with Ken Stanley and you just mentioned this concept of evolution

31:29.200 --> 31:34.720
I spoke to him probably several years ago talking about his work in neuro evolution

31:34.720 --> 31:43.440
and this we're using evolution loosely or have you also studied the these ideas of neuro evolution

31:43.440 --> 31:50.000
and kind of evolutionary neural nets and machine learning oh yeah yeah for sure for example the work

31:50.000 --> 31:56.640
that I just talked about the neuro evolution of itself interpretive agents like we actually

31:56.640 --> 32:04.400
used evolution or computational evolution algorithms to train the agents so rather than using

32:04.400 --> 32:13.280
like like reinforcement learning to train them so like in in general I kind of like some of

32:13.280 --> 32:19.840
the evolution algorithms because they're kind of like we can use them as a black box optimizer as

32:19.840 --> 32:26.560
well we we don't necessarily need everything to be nice and differentiable which is one of the

32:26.560 --> 32:32.800
key properties of many many domains right now so one one of the things are differentiable

32:32.800 --> 32:39.280
than you can put it into the machine and you know you're you're you're you're gradient based

32:39.280 --> 32:45.280
optimizer would get the solution but because hard attention is it's kind of difficult to make

32:46.160 --> 32:52.240
it very differentiable or there are methods but there is challenging it's just easier sometimes to

32:52.240 --> 33:00.640
use evolution to solve these problems so one hand we we do like to use evolution and specifically

33:00.640 --> 33:11.040
evolution strategies and genetic algorithms as a tool to to help us find solutions but we I did work

33:11.040 --> 33:18.800
on some research projects where we're also developing these evolution algorithms as well

33:18.800 --> 33:31.360
so there there was another paper with with with these themes of constraints was done with with me

33:31.360 --> 33:40.080
and I was led to my former colleague and intern Adam Geier in a paper called weight agnostic neural

33:40.080 --> 33:48.240
networks so the the key concept in that paper is you know we want to find the neural

33:48.240 --> 33:55.920
network architectures that have a really strong inductive bias for certain like reinforcement

33:55.920 --> 34:04.080
learning or machine learning task and can we go to the extreme and find architectures that can work

34:04.720 --> 34:10.960
even without training weights so so usually when we think of neural networks you think of

34:10.960 --> 34:16.240
having a neural network architecture okay and then let's run the optimization algorithm using

34:16.240 --> 34:23.120
SGD to find the weights but here we okay what can we still find the architecture that can still work

34:23.120 --> 34:27.360
when we don't train the weights like when the weights are chosen from a random distribution

34:28.560 --> 34:37.280
so that that one is when we when we looked at we essentially looked at doing architecture search

34:37.280 --> 34:47.680
where we want to optimize the performance of the architecture with a given weight distribution

34:49.840 --> 34:55.600
right so of course your architecture is not going to perform as well as when all the ways we

34:55.600 --> 35:02.480
refine tune but this is still very useful because as you know a neural architecture search is

35:02.480 --> 35:08.640
extremely computational intensive so you you will have a batch of architectures and then you'll

35:08.640 --> 35:14.880
have to find the weights of all of these architectures and and then you would go on to find the

35:14.880 --> 35:22.880
next set of architectures you use your results but here we can simply find the architectures and

35:22.880 --> 35:30.560
evaluate their performance on on random weights and and then we can find architectures that are

35:30.560 --> 35:37.760
have have a very strong inductive or even like an innate bias for certain tasks so then

35:38.720 --> 35:47.120
the intuition is like is kind of inspired by by the biology sometimes the organisms have some

35:47.120 --> 35:54.640
ability the moment they're born to escape predators right you can imagine like maybe you can

35:54.640 --> 36:01.440
you can have a bipedal walker controller that can already still walk forward when the

36:01.440 --> 36:07.280
weights are not trained but if that's the starting point then then training the weights will be

36:07.280 --> 36:13.840
a lot more efficient if you want to fine tune the network later on so that that's like to answer

36:13.840 --> 36:18.560
your your question earlier is like this is like one example of the work that I was involved in

36:18.560 --> 36:27.760
where we actually try to extend and and improve upon architecture or neuro evolution methods

36:28.480 --> 36:36.080
to find neural networks that that are where we're not just an user of the evolution algorithm

36:36.080 --> 36:44.720
as a black box optimizer and this paper was was apparently like a talk about in the neuro

36:44.720 --> 36:49.440
science community a bit more than the machine learning community it was not so useful to them

36:49.440 --> 36:58.480
like it would be like that we got our best score on M this was like 92% or I forgot

36:58.480 --> 37:07.040
I'm there so it was like the the M this performance is is horrible so it's not going to be so

37:07.040 --> 37:13.440
useful for for the M this committee but but the the papers do still got accepted at the

37:13.440 --> 37:17.600
new groups conference you know and it got like a spotlight but it's probably one of those papers

37:17.600 --> 37:24.000
that where we we massively underperformed the state of the art with like 90% on M this but

37:24.000 --> 37:36.880
somehow still got in kind of luck done on that one nice nice I tend to hear neuro evolution coming

37:36.880 --> 37:45.120
up most in the context of architecture search are there other areas where you see it being used

37:46.400 --> 37:53.120
yeah well like as I mentioned earlier we we use it a lot just for policy search

37:56.800 --> 38:03.040
we also see it used quite often in in robotics as well

38:03.040 --> 38:11.680
huh like for example some of my colleagues in in the robotics team they they like to use

38:12.800 --> 38:21.760
simple evolution algorithms to to quickly find policies like the the one that is

38:22.800 --> 38:31.920
is used most often there's two that is really used often one one is a CMAES so that that is

38:31.920 --> 38:36.720
that kind of like the the defaults evolution strategies algorithm that people like to use as a

38:36.720 --> 38:43.920
black box optimizer the other one is caught a I think it's caught a augmented random search

38:43.920 --> 38:50.080
is basically evolution is a form of random search and then this one is a it's a it's a very simple

38:50.080 --> 38:58.720
random search algorithm that's directed in in a very simple way so so the robotics folks likes

38:58.720 --> 39:05.280
these simple approaches because they're they're they're explainable and they're intuitive so I see

39:05.280 --> 39:14.480
some people are using them to find the policies on on like on like like robots and then using them to

39:15.120 --> 39:22.880
to like a control control these like mini-tower robots that they have in the lab

39:22.880 --> 39:31.840
uh we I but I use them a lot for in general like especially if when I have a I have a neural net

39:31.840 --> 39:38.880
without so many parameters like which is very common in RL like unlike deep learning where you

39:38.880 --> 39:45.600
do have like you know 20 million parameter solutions in RL a lot of the the controllers like my

39:45.600 --> 39:53.520
might work uh when when we only have have like you know the the 10,000 parameters or even 1,000

39:53.520 --> 39:59.600
parameters so so it's like yeah and it could nice to say the RL is like a cherry on a cake

39:59.600 --> 40:05.680
and so so the trend is uh you have all these self supervised models that are trained with

40:05.680 --> 40:11.680
gradient descent with hundreds of millions of parameters but your actual policy network that

40:11.680 --> 40:17.920
could be using all of these things maybe perhaps via a world model and and those networks could

40:17.920 --> 40:24.560
might even just be a few thousand parameters and that can you know wipe out or using gradient descent

40:24.560 --> 40:30.480
to train them when uh like uh if we're able to use evolution to train them we can get away with

40:30.480 --> 40:37.840
doing things like non-differentiable environments and and whatnot so so we we tend to like to use

40:37.840 --> 40:47.520
those as as a baseline in that case in conjunction with with other RL methods and and approach

40:47.520 --> 40:55.200
this to policy or kind of um in isolation we usually like if we're able to get the solution we

40:55.200 --> 41:02.800
want then we can use them like in isolation okay I mean there's some of my colleagues have been

41:02.800 --> 41:11.120
working on ways to to combine the reinforcement learning with evolution so yeah then evolution

41:11.120 --> 41:18.160
can kind of be the outer loop and the RL can be the inner loop but in a lot of the work where I'm

41:18.160 --> 41:27.040
simply using like an end user of an optimizer then I simply use it to to get me get me a set of

41:27.040 --> 41:36.480
weights or get me a set of parameters and go on in a day. Did we talk about the sensory neuron

41:36.480 --> 41:48.240
paper? Oh no no no it's it sounds like I mean it kind of fits right into this idea of uh constraints

41:48.240 --> 41:56.480
and um applying constraints to to make problem solving easier can you talk a little bit about that

41:56.480 --> 42:03.520
paper? Yeah sure so like uh some of the previous work I I discuss it's it's more like you know

42:03.520 --> 42:08.800
the constraint is more like a bottleneck like an information bottleneck and maybe you're doing more

42:08.800 --> 42:16.480
with less but it doesn't always have to be like that so in in this paper we it was a really fun

42:16.480 --> 42:23.920
project also with with my teammates Eugene Tang we we looked at the problem of uh what what if we

42:23.920 --> 42:35.040
we gave an agent's uh an observation space that is shuffled around so like usually in a in these

42:35.040 --> 42:40.400
reinforcement learning environments or in machine learning in general you have to give a model

42:41.680 --> 42:50.320
very well-specified input data like like if if you give it like the the observation space of

42:50.320 --> 42:58.000
of a humanoid or an ant's robots like every single input means something like maybe a tour good

42:58.000 --> 43:04.640
of velocity or the positions or maybe the pixels on the screen this pixel corresponds to

43:04.640 --> 43:12.000
to this has to be this position so we we toyed around with the idea of what if we we can randomly

43:12.000 --> 43:20.480
shuffle the observations and the agent actually has to like figure out what each input like each

43:20.480 --> 43:29.600
sensory input means before you know deciding an action and if an agent is able to to solve

43:30.720 --> 43:36.880
a particular task or environments or or a machine learning problem from from uh shuffled

43:36.880 --> 43:45.280
observations we can also examine the properties whether it has extra benefits that it has compared

43:45.280 --> 43:51.600
to agents that are otherwise trained the normal way of just getting getting the inputs so this

43:51.600 --> 43:57.360
is another type of constraint that I don't consider to be a bottleneck or information bottleneck

43:57.360 --> 44:03.120
because I you're actually giving the agent the same information I guess like the dimensionality

44:03.120 --> 44:11.360
of the information is the same but but here we we try to just shuffle the order and surprisingly

44:11.360 --> 44:19.120
we're able to we're able to get it to work so like uh the the information of this work originally

44:19.120 --> 44:25.520
came from some ideas on in a meta learning space because we we're essentially trying to

44:25.520 --> 44:31.440
to get an agent to adapt to changing environments when it's like the agent will get a shuffled

44:31.440 --> 44:37.680
and re-shuffled screen and it has to re-adapt but also in the in the neuroscience there's the

44:37.680 --> 44:45.280
the area called a sensory substitution and uh it's a psychologist have to measure the human's

44:45.280 --> 44:53.440
ability to adapt to when when what our senses give us suddenly change like like there's this

44:53.440 --> 44:59.440
popular experiment done even a hundred years ago where you you're wearing this uh you're wearing

44:59.440 --> 45:05.200
an upside down goggle I'm not sure you saw that so there's a there's a simple mirror glass

45:05.200 --> 45:10.560
in front of your eyes so what you're seeing is is completely flipped and what people notice is

45:10.560 --> 45:17.040
it requires maybe maybe like ten minutes or half an hour of readjusting and you're able to walk

45:17.040 --> 45:22.800
perfectly fine with with this flip sensory but once you take off the glass then you're messed up

45:22.800 --> 45:27.760
again for another half an hour or so so that that's kind of I guess one of the easier tasks

45:27.760 --> 45:36.240
uh there there's a TED talk a few years ago where someone had a video of an inverted bicycle

45:36.240 --> 45:44.080
so this one is harder when when you turn left you actually go right and when you turn right you

45:44.080 --> 45:49.840
actually go left and they found this one really messed people up so you know like I guess because

45:49.840 --> 45:58.080
a riding a riding a bicycle is more like a it's like a human invention I guess so it takes it takes

45:58.080 --> 46:03.360
a long time for people to to read that because you actually have to balance as well you're like

46:03.360 --> 46:08.880
a complicated control system constantly balancing you so that one really messed people up

46:08.880 --> 46:15.280
I don't know if you've ever had the experience where your your screen controls get flipped

46:15.280 --> 46:22.480
I don't remember what caused it um but you know your you know trackpad right becomes left and

46:22.480 --> 46:29.040
up becomes down and and vice versa and that can be you know infuriated it's very difficult to

46:29.040 --> 46:36.240
to adjust to yeah exactly yeah especially like you know apple like whenever you use like

46:36.240 --> 46:41.600
apple products that okay but sometimes your trackpad goes the other way around when you're on

46:41.600 --> 46:48.400
another person's trackpad or when they have a new model so I booked or MacBook Pros you have

46:48.400 --> 46:54.880
you have a touch bar and you suddenly don't have a touch bar yeah no no we're back to the other day

46:55.760 --> 47:03.600
but hey there's another uh there's another uh neuroscientist uh Paul Paul back Rita who's kind

47:03.600 --> 47:11.920
of a pioneer of sensory substitution and and his claim to fame was uh he he had uh sub he had uh

47:11.920 --> 47:20.080
people who were unfortunately blind like cannot like uh see uh this lack vision and back in the

47:20.080 --> 47:27.440
end of the 60s he had an experiment where uh he put a low-dimensional uh video camera

47:27.440 --> 47:38.000
uh there's analog back in the day and he he uh he fed some of those uh signals from the analog

47:38.000 --> 47:44.960
video camera into a low-dimensional 2D grid of pokes into the person's back I've heard about this

47:44.960 --> 47:53.200
yeah so yeah one of the cool things is uh our our skin or our our touch senses is under

47:53.200 --> 47:59.440
utilized everywhere outside of our hands like maybe from evolution when we're hunter-gatherers our

47:59.440 --> 48:04.720
skin was really important but in kind of modern times that we we wear you know like a clothing and

48:04.720 --> 48:09.440
we we don't really use our touch senses but that does another interesting topic but it's kind of

48:09.440 --> 48:15.120
getting signed like but back for for this particular idea is that he poked a low-dimensional

48:15.120 --> 48:23.520
resolution of uh commissioned to the subjects back within a few weeks or months uh people gain vision

48:23.520 --> 48:30.560
they were able to to see and understand things uh by sitting on his chair uh so so he showed that

48:31.680 --> 48:39.280
through through uh touches or through pokes on a set person's back that person can can learn

48:39.280 --> 48:47.840
to to interpret those signals as if that person was was seeing uh what's in front of the camera

48:47.840 --> 48:56.880
and in the in the late 90s in turn of the century there was a variation of this uh from this team

48:56.880 --> 49:05.200
where they they fed in a higher resolution video feed into a 2D grid of uh electrodes that was

49:05.200 --> 49:12.240
placed on a person's tongue so so from from the the stimulation on the tongue the person's is

49:12.240 --> 49:19.200
able to the the subject is able to interpret uh what the video uh camera mounted on the subjects

49:19.200 --> 49:26.320
head is seeing and this was actually you know gain popularity uh so like uh people were able to

49:26.320 --> 49:33.760
to live their lives then like having a low-dimensional uh vision system simply by learning to

49:33.760 --> 49:40.800
predicting um how um the these sensory uh signals from the from their I guess from from their

49:40.800 --> 49:47.680
tongue but however um these these are incredible it shows how how great we are but they require

49:47.680 --> 49:54.160
like months if not years of training uh to gain mastery so it's it's kind of like okay sure

49:54.160 --> 50:00.320
you can you can change you can switch around your inputs and retrain your machine learning

50:00.320 --> 50:06.160
model from scratch even uh and using the new inputs and of course yeah then then you can deal

50:06.160 --> 50:12.880
with these sensory substitutions so what the what we're trying to ask ourselves is can we actually

50:12.880 --> 50:18.320
get an algorithm to do this without training in the in the traditional sense like with without

50:18.320 --> 50:26.320
like a retraining your model where so uh where the the the the agent is able to explicitly adapt

50:26.320 --> 50:35.840
to to these uh inputs uh so in the end uh even though this work is biologically inspired from

50:35.840 --> 50:42.800
from on the problem side the solution we use has nothing to do with biology uh we're lucky enough

50:42.800 --> 50:49.360
to to build on to previous work that gave us to do the tools to work with uh permutation

50:49.360 --> 50:56.080
invariant networks and some of these uh works have been pioneered by by people working on the

50:56.080 --> 51:02.160
transformer paper the original transformer paper uses the linear attention which was predate

51:02.160 --> 51:09.600
transformer by lots but those those were shown to be uh permutation acrovariants so if if you

51:09.600 --> 51:14.960
change the input order the output order changes the same way but uh there's another paper that came

51:14.960 --> 51:20.720
out later called a set transformer which had a really cool trick on making one of the the

51:20.720 --> 51:28.320
query matrix uh constant and that converted this attention mechanism to be permutation invariant

51:28.320 --> 51:38.160
so suddenly you're able to feed in a signal of like uh of any order and the output will be the

51:38.160 --> 51:46.640
same thing okay so it'll be like um it's it's a method to to take on a sets of an unordered

51:46.640 --> 51:53.120
variable length sets and and you're able to to get uh get a permutation invariant representation

51:53.120 --> 51:58.560
of it so we played around with this idea and applied it to reinforcement learning problems

51:58.560 --> 52:04.720
so it's a weather you can uh you can feed in all of your signals whether those signals could be

52:04.720 --> 52:11.120
the states of a pie bullets like locomotion environments uh or it could be like all of the

52:11.120 --> 52:19.200
the tiles of of uh of an Atari game uh and you can feed them in any order you want uh and you know

52:19.200 --> 52:26.960
you get the same representation coming up so we we tried to uh to to feed these representations

52:26.960 --> 52:33.680
into into a policy network and train the entire system to to perform the task

52:33.680 --> 52:41.280
and uh what we notice is that it is uh after after uh development we have to iterate on this method

52:41.280 --> 52:48.160
it doesn't work at the beginning so uh some some of the the improvements that my my colleague

52:48.160 --> 52:54.160
Eugene discovered is we actually have to feed in things like the previous action and have

52:54.160 --> 53:02.480
have each sensory neuron uh for certain tasks have its own internal state so so for for for example

53:02.480 --> 53:09.600
like your locomotion robot actually you know every sensory uh input goes into its own LSTM

53:09.600 --> 53:14.960
and that LSTM will output a broadcast signal to the attention mechanism that will generate

53:15.680 --> 53:19.520
this permutation invariance representation that can produce the action

53:20.400 --> 53:28.880
so it's fairly expensive yeah yeah but yeah so in a way abstracting like usually uh

53:28.880 --> 53:33.280
you know our in traditionally our networks are we just get the input right away

53:33.280 --> 53:40.720
into into our particular uh input node of a neural network but here we treat every input node

53:41.280 --> 53:48.000
as uh as a neural network itself so that's why uh yes the papers type of is the sensory neuron

53:48.000 --> 53:53.680
as a transformer because these neural networks is has been inspired by the transformer architecture

53:53.680 --> 54:04.880
so to to uh to I guess pay some give some credit to that and what we notice is like uh of course

54:04.880 --> 54:12.080
this it's going to work for permutation invariance observations but we were actually without

54:12.800 --> 54:19.440
additional training these agents tend to work even when we shuffle them during the observations

54:19.440 --> 54:25.920
during an episode like like like for example if you have a locomotion robot plot forward uh

54:25.920 --> 54:32.080
and of course it's going to work when you shuffle to input once at the beginning uh and keep

54:32.080 --> 54:38.160
keep that shuffle order the same for the rest of the one thousand time steps because like by the

54:38.160 --> 54:44.720
condition the the the representations don't change but what we notice is we can shuffle them

54:44.720 --> 54:50.240
many times during the environments like like if you if your episode is one thousand time steps

54:50.240 --> 54:55.280
you can shuffle them you know every one hundred time steps and the performance without additional

54:55.280 --> 55:01.520
training remains roughly the same so so there there's something to be said about the the power of

55:01.520 --> 55:08.240
uh of uh the agent's ability to to quickly re-adapt uh without explicitly learning to re-adapt

55:08.240 --> 55:17.120
uh to the environments um that you look at um or would you expect to see that if you then

55:18.160 --> 55:23.440
uh you train an agent with this capability or this constraint you know as you might say

55:23.440 --> 55:30.160
and then you give it unsuffled data does it perform better because it you know has learned to

55:31.040 --> 55:37.760
attend to important relationships in the scene as opposed to um an agent that you know

55:37.760 --> 55:43.840
hasn't been trained in this way for for this one um if if we give a shuffle or on shuffle data

55:43.840 --> 55:51.280
it'll perform exactly the same way um because the representations are consistent uh but but

55:51.280 --> 56:00.480
that being said uh we we could do things like um like take away information uh from the input

56:00.480 --> 56:07.360
or give it additional redundance information uh into input and have it still kind of work

56:07.360 --> 56:15.040
like like for instance if the agent expects like five input signals to do a task uh you can give it

56:15.040 --> 56:22.960
like you know 20 signals but five of them are the actual important signals and the other 15 are

56:22.960 --> 56:28.720
pure noise uh a little like a small amount of noise and the whole thing can be shuffled

56:28.720 --> 56:34.880
and without actually extra further training like uh it's only trained originally on the five inputs

56:34.880 --> 56:42.080
it's it's still able to identify like it work i guess uh it's able to to somehow learn

56:42.080 --> 56:49.600
that it should identify which signals are important without explicitly training to identify those

56:49.600 --> 56:57.280
so i feel it's it's kind of like um this is somewhere between learning and metal learning

56:57.840 --> 57:02.320
for something like a metal learning you're explicitly training the algorithm to learn an algorithm

57:02.320 --> 57:09.040
that learns uh and if we're for learning you're just getting the algorithm policy for the task

57:09.040 --> 57:16.320
here i think the the method is like indirectly learning uh self identification method

57:16.320 --> 57:21.920
to to identify um like which patches or or which uh inputs are important and

57:22.720 --> 57:30.640
the other uh interesting result is uh is from the robustness standpoint so if we apply this uh

57:30.640 --> 57:37.200
methodology to visual tasks like the car racing game or pong like Atari games uh we know

57:37.200 --> 57:44.160
this that we can do we can change the backgrounds uh of the game and the policy can still continue to

57:44.160 --> 57:50.800
work uh to some extent uh this was not possible in the earlier work on on the on the hard attention

57:50.800 --> 57:56.640
so when we change the background it still fails but here uh when we change the background for

57:56.640 --> 58:03.280
for the car racing task uh without explicitly training on these new backgrounds the policy can

58:03.280 --> 58:10.560
can still work to some extent and the the performance is is almost as well that is good for

58:10.560 --> 58:17.600
these generalization tasks compared to existing works in the literature that are explicitly designed

58:17.600 --> 58:24.320
to do such generalization uh but but here it's like a byproduct of of okay let's train our agent

58:24.320 --> 58:30.000
to work with shuffle inputs and oh by the way you know the generalization abilities are

58:30.000 --> 58:38.640
are like just a byproduct of of this this constraint um so when we duck into further like uh the

58:38.640 --> 58:46.560
hypothesis is if we if we shuffle up all of the the patches or the tiles of the screen uh we we

58:46.560 --> 58:54.080
could force the agents to to learn like the to essence the essential important things for the

58:54.080 --> 59:00.960
task and because it's it's forced to learn the essential properties that may help it generalize

59:00.960 --> 59:07.600
to to variations of the environment with different backgrounds and when we did further analysis

59:07.600 --> 59:13.920
we actually looked at the patches that they learned they learned to attend to like in in the

59:13.920 --> 59:18.880
car racing game and it turns out that even though the patches are all shuffled around

59:18.880 --> 59:26.320
it still learns to to attend mostly to the patches that correspond to the edge of the road

59:27.600 --> 59:33.760
even even when the screen is all shuffled around and so then this can help us explain why

59:33.760 --> 59:39.360
the generalization still works to to environments with with when we change the background

59:40.000 --> 59:46.160
because like it's not seeing it's not really attending to to the positions with with different

59:46.160 --> 59:52.720
backgrounds is still looking at at the road so some of the analysis is done in the paper to to

59:52.720 --> 01:00:01.920
explain why the the transfer works got it so given this uh you know body of research that

01:00:01.920 --> 01:00:08.400
that you've pursued focused on the ideas of constraints incorporating ideas like noravolution

01:00:09.120 --> 01:00:13.440
you know what are you excited about looking forward where where do you see your research headed

01:00:13.440 --> 01:00:21.920
yeah so like I'm really fascinated with the whole concept of of the self-organization

01:00:23.200 --> 01:00:30.560
so especially like I was I'm really inspired by this body of work that that my my colleague

01:00:31.280 --> 01:00:39.200
Alexander Mordensev did on a neurosurder automata and also self-organizing

01:00:39.200 --> 01:00:46.400
uh class and this classifiers which was uh recent articles on the disto pub platform and

01:00:46.400 --> 01:00:52.960
like one of one of the things that excited me about this self uh the the sensory you know neuron

01:00:52.960 --> 01:01:00.400
paper is uh the it is sort of like a self-organized system every input goes into uh an identical

01:01:00.400 --> 01:01:06.640
neural network with its own hidden recurrent state and somehow these neural networks learn to

01:01:06.640 --> 01:01:12.880
communicate via attention mechanism to have this emergent property which is the policy and I'm

01:01:12.880 --> 01:01:22.000
really excited about like going forward with exploring more of these uh collective intelligence

01:01:22.880 --> 01:01:30.480
themes where you where you have a you have an emergent property from thousands or even like

01:01:30.480 --> 01:01:39.280
hundreds of thousands of different unique agents or units that have their own local processing

01:01:39.280 --> 01:01:48.560
rules but somehow as a whole you have some global emergent property that that is a that is a result

01:01:48.560 --> 01:01:57.120
of maybe some evolutionary optimization and I want to explore like properties of these emergent

01:01:57.120 --> 01:02:04.640
prop uh behavior because maybe that will help us address some of the shortcomings we see in

01:02:04.640 --> 01:02:11.840
in reinforcement learning like like a like a lot of like some some of the issues in RL has to do

01:02:11.840 --> 01:02:22.960
with like robustness generalization um maybe like uh out like a sample efficiency and so on

01:02:22.960 --> 01:02:29.600
but uh we can get inspiration from other areas like like like for example swarm

01:02:29.600 --> 01:02:37.280
swarm computing swarm optimization uh like a multi-agent systems and and maybe if we look at

01:02:37.280 --> 01:02:44.960
if we try to break down a problem into into a large like a complex systems problem where you

01:02:44.960 --> 01:02:50.240
have lots of local computation perhaps that might give us some insight or or different types of

01:02:50.240 --> 01:03:01.200
solutions to to how we've been able to approach them so far so I'm excited about the general idea

01:03:01.200 --> 01:03:08.320
of like a collective intelligence complex systems and going forward we want to see you know how we

01:03:08.320 --> 01:03:17.040
can like a bridge between the complex systems uh research and incorporate some some of the good

01:03:17.040 --> 01:03:22.560
ideas into machine learning and also maybe look at the other way around maybe we can use machine

01:03:22.560 --> 01:03:30.320
learning to also help uh advanced state of complex systems research awesome awesome i'm looking

01:03:30.320 --> 01:03:37.280
forward to following along as you uh push forward in that direction david has been wonderful chatting

01:03:37.280 --> 01:03:48.720
with you thanks so much for joining us well thanks for having me Sam always thank you

