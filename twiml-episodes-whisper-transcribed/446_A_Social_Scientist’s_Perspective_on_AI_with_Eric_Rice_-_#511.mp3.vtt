WEBVTT

00:00.000 --> 00:15.760
Alright everyone, I'm here with Eric Rice. Eric is an associate professor at the University

00:15.760 --> 00:20.840
of Southern California and co-director of the USC Center for Artificial Intelligence

00:20.840 --> 00:24.600
in Society. Eric, welcome to the Twimal AI podcast.

00:24.600 --> 00:28.640
Well, thank you very much. It's a pleasure to be here. Hey, I'm really looking forward

00:28.640 --> 00:33.800
to digging into our conversation. Before we do, I'd love to have you share a little bit

00:33.800 --> 00:39.720
about your background. You are a social scientist, but you work extensively with the AI community.

00:39.720 --> 00:45.520
Tell us a little bit about how that came to be. Oh, God. Yeah, sure. As if that's a quick

00:45.520 --> 00:51.080
story, but I'll try to make it quick one. So yes, I am a social scientist. I'm a sociologist

00:51.080 --> 00:56.600
by training, and I'm a professor in the Susanne DeVorque Peck School of Social Work at USC.

00:56.600 --> 01:04.480
So I've been an applied social scientist working in social work for about 12 years. And

01:04.480 --> 01:12.400
I started working with computer scientists back in 2014. And my initial collaboration

01:12.400 --> 01:19.880
was with Mill and Tombay, who I know you've had on this podcast in the past. And really,

01:19.880 --> 01:26.760
he, he and I met at a kind of a meet and greet for faculty across schools. And I think

01:26.760 --> 01:35.200
the, and when he and I started talking, we realized that surprisingly, we shared an interest

01:35.200 --> 01:41.280
in social network influence problems, which I didn't realize the computer scientists were

01:41.280 --> 01:45.000
interested in it at all. But then it turns out that they are, especially in the context

01:45.000 --> 01:50.080
of, you know, Twitter networks and, and even in sort of modeling other sorts of networks

01:50.080 --> 01:55.440
because the interesting computational problems that go along with it. And I was a social network

01:55.440 --> 02:01.400
researcher since the 90s, really, when I was originally trained in sociology. And so

02:01.400 --> 02:08.080
I've been work, I had been working on some projects around social influence in youth

02:08.080 --> 02:12.800
experiencing homelessness and HIV prevention. And so when he and I started talking, we got

02:12.800 --> 02:17.680
all excited about this very odd, you know, a connection that we, that we didn't expect

02:17.680 --> 02:25.080
to have. And then when we, when we decided to start meeting together, things quickly seemed

02:25.080 --> 02:30.440
very promising. And so we started playing around and it became a paper. And then a paper

02:30.440 --> 02:34.840
became a grant. And then a grant became a center. And, you know, the next thing I know,

02:34.840 --> 02:38.760
I wake up and, you know, people expect me to say intelligent things about AI. And I

02:38.760 --> 02:42.760
am not a computer scientist, right? So I mean, I, I hang out. I've, I've got a lot of friends

02:42.760 --> 02:46.520
that are computer scientists, but I am not one. I mean, the last time I took a computer science

02:46.520 --> 02:52.640
class, it was 1994. And I was learning Pascal, right? So just to give you a sense of like what

02:52.640 --> 02:56.840
a dinosaur, you know, my computer science is person. That's awesome. Pascal's a great

02:56.840 --> 03:02.160
language. Sure. I don't think I could code a word in it anymore. But, you know, yeah,

03:02.160 --> 03:06.360
it was, it was cool when I was in college. Yeah, but it's, but, but, but, but, you know,

03:06.360 --> 03:09.880
I was a math nerd. You know, I, I mean, when I first started college, I thought I was going

03:09.880 --> 03:16.800
to be, I started off as a math major. And I took, sort of, as much math as the, um, folks

03:16.800 --> 03:22.440
that do economics and physics do just because a, I liked it. And it was like, and it was,

03:22.440 --> 03:27.240
math was an easy A for me. So it's like several, a couple years of math as electives was what

03:27.240 --> 03:30.920
I did, even though I was a sociology major, just because it was, I liked it. But I was really

03:30.920 --> 03:38.760
much more drawn, I think intellectually to social science and issues of inequality, uh, and

03:38.760 --> 03:42.560
poverty. I mean, I was living in, I was at the University of Chicago. I was an undergraduate

03:42.560 --> 03:48.280
in the south side of Chicago in the early 90s. And it was a really, you know, race relations

03:48.280 --> 03:53.680
were really tense. Poverty was really extreme. And it was, it was, and University of Chicago

03:53.680 --> 03:59.200
has a great kind of history of doing sociology. And so it was really hard not to get kind

03:59.200 --> 04:03.160
of drawn into this. And then as time went on, I realized that what I was really interested

04:03.160 --> 04:07.480
in was these very applied version of social sciences, which is what you get in social work

04:07.480 --> 04:12.000
where people are really interested in intervention work. And then all of that is sort of to say

04:12.000 --> 04:17.280
that, like, I become the domain expert, right? I mean, so I, I hang out with folks and talk

04:17.280 --> 04:22.160
to them about social problems. And then we try to come up with, uh, algorithmic solutions

04:22.160 --> 04:27.320
to, to, to make a difference is kind of like a high level, uh, of where, of where I'm at.

04:27.320 --> 04:34.560
Awesome. Awesome. And you recently participated in a workshop at ICLR, uh, where I clear,

04:34.560 --> 04:41.080
uh, on responsible AI. Uh, tell me a little bit about, uh, that workshop, the focus there

04:41.080 --> 04:50.120
and, sure, yeah. I've been very fortunate that, um, the computer science community has

04:50.120 --> 04:56.560
been very gracious and welcoming of me. I mean, I, I'm, I'm surprised pleasantly when

04:56.560 --> 05:01.120
I hear people talk about the value of working with social scientists and the value of social

05:01.120 --> 05:05.840
science, like social work. And I sort of, I was in, like, wow, this is weird that people,

05:05.840 --> 05:09.960
you know, people in this, in this room would say such a thing. But it's, it's, um, but

05:09.960 --> 05:15.080
it's really gratifying. And so I was, uh, this responsible AI workshop was really cool.

05:15.080 --> 05:20.120
There were, um, I don't know, about a dozen, um, people that they asked to give half an hour

05:20.120 --> 05:25.360
talks. And then they had some panel discussions where we all got to, in groups of about six,

05:25.360 --> 05:30.240
yeah, kind of riff on ideas with one another. And, you know, as usual for me in a computer

05:30.240 --> 05:35.040
science convening, I'm, you know, the one social scientist in the room. But, um, so I say

05:35.040 --> 05:40.640
probably, I don't know, maybe high level interesting things, but maybe in the details, like,

05:40.640 --> 05:45.680
dumb things, but I try my best, you know, and, um, and, and one of the things that I was talking

05:45.680 --> 05:50.080
about for myself when they asked me to give a talk was really, you know, what have I learned

05:50.080 --> 05:55.040
as a social scientist about doing interdisciplinary work? Because that's really been the

05:55.040 --> 06:01.920
focus since the beginning with Milland, um, Tom Bay, uh, back in 2014. And now in the center

06:01.920 --> 06:05.840
that I run at USC and the collaborations that I have with people like Phoebe Vianos and

06:05.840 --> 06:11.040
B. Stratelkina, um, you know, we still are doing all these interdisciplinary work. And one of

06:11.040 --> 06:16.160
the things I was talking about was some lessons learned for me. And, and I think I'll share this

06:16.160 --> 06:20.880
with, with, with your, with your audience, because I think that, you know, since most folks are,

06:20.880 --> 06:27.760
as you said, practitioners in the AI realm, um, that, you know, what I found in working collaboratively

06:27.760 --> 06:32.480
is that the, the, there's a few lessons that are valuable. I mean, the first is that you actually

06:32.480 --> 06:38.960
have to, what I say, well, say, like, collaborate for real. And that means don't just bring in domain

06:38.960 --> 06:44.080
experts and as like a token or if you're in social science, don't just bring in a machine learning

06:44.080 --> 06:48.640
person as a token person and say, hey, run these models. It's like, if you, but if you get people

06:48.640 --> 06:56.320
together to genuinely work on problems from the get go, the solutions and the, the con, the

06:56.320 --> 07:01.440
contours of the problem are much more detailed and the solutions are much more thoughtful and

07:01.440 --> 07:05.920
innovative and it's, it's much better work. Um, the other thing is you have to learn how to

07:05.920 --> 07:15.360
communicate, which is actually really difficult because you, um, in the social sciences and in

07:15.360 --> 07:20.720
computer science, there's elaborate professional languages that are very arcane and specific to those

07:20.720 --> 07:28.720
disciplines and they're very jargon-filled and sometimes the same words mean different things

07:28.720 --> 07:32.800
and different words mean the same things like, for example, you know, social scientists we talk

07:32.800 --> 07:37.600
about variables in the way that a computer scientist would talk about features of a data set.

07:37.600 --> 07:41.600
But when you talk about a variable, you're usually talking about something that's in the level of

07:41.600 --> 07:46.800
sort of like a, an algorithmic level model, whereas that's what we talk about when we're talking

07:46.800 --> 07:51.440
about features of a data set. And likewise, like a model, I mean, good lord. Like models mean like

07:51.440 --> 07:54.800
six different things because there's like theoretical models, but then theory means something

07:54.800 --> 07:58.560
different in social science than it means in computer science and so you can talk past each

07:58.560 --> 08:02.960
other really quickly, even though you think you're speaking the same language and it takes time

08:02.960 --> 08:09.440
to, to learn to communicate effectively. And then the example of this just prior to starting the

08:09.440 --> 08:16.720
interview, we were out, I was describing the, our audience as practitioners and to you that

08:16.720 --> 08:21.600
was social scientists. Do you mean that meant social workers? That meant practicing social workers,

08:21.600 --> 08:26.160
like not social scientists who would be running right from us, but people that actually work with,

08:26.160 --> 08:30.560
you know, a homeless individual and try to get them a house, you know, that's a practitioner,

08:30.560 --> 08:33.840
right? And so it's, it's, it's funny because yeah, we're using the same word to mean two

08:33.840 --> 08:38.560
totally different things. I mean, great example of that. Yeah, no, exactly. And it's, it happens

08:38.560 --> 08:45.920
all the time, all the time. And it's not insurmountable, it just takes time. And then, and then the

08:45.920 --> 08:52.560
last thing is really that you have to iterate, which is part, which is, you know, part of the processes

08:52.560 --> 09:00.880
that, you know, as you work collaboratively in these interdisciplinary spaces, the initial solutions

09:00.880 --> 09:05.680
may not actually encompass all of the depth of the problem and that you don't really even realize

09:05.680 --> 09:12.080
that you've left things out until you get to the next step down the road. And then you realize

09:12.080 --> 09:17.840
that while you've solved one aspect of the problem, new aspects of the problem are emerging and

09:17.840 --> 09:24.240
becoming visible to you simply by having solved that first part of the problem. But yeah, so anyway,

09:24.240 --> 09:29.360
we're, you know, it's, it's, it's these, these, these lessons learned about interdisciplinary work

09:29.360 --> 09:33.520
are things that, you know, I kind of came across over having done several projects over the years

09:33.520 --> 09:37.840
with several different computer scientists. And, and it's really, it's, it's really fun work.

09:37.840 --> 09:44.160
I mean, that's the other thing that, that I, I, I hope that I shared with the, the ICLR

09:45.600 --> 09:50.640
responsible AI is not only that, you know, we can be responsible and part of being responsible is

09:50.640 --> 09:59.360
doing this work in this collaborative interdisciplinary way, but also that it's a, it's a, it can be a very

09:59.360 --> 10:06.240
joyful process, you know, it's, it's, it's fun to, I mean, not that working on problems like HIV

10:06.240 --> 10:13.040
and homelessness, like those aren't fun things to talk about, but working with other people that

10:13.040 --> 10:18.400
are dedicated to making the world a better place and who want to solve problems and be thoughtful

10:18.400 --> 10:24.640
about it can be a very joyful and, you know, processed, you know, even though the issues that you're

10:24.640 --> 10:32.400
working on can sometimes be really heavy. I'd love to maybe carry those three lessons through

10:32.400 --> 10:38.000
as a frame for talking about some of the projects that you've worked on and some of the collaborations.

10:39.920 --> 10:46.240
One that you've already mentioned is this work with homelessness or this collaboration with

10:46.240 --> 10:54.320
Melentombe. You know, we talked quite extensively with him about it, but, you know, maybe before we

10:54.320 --> 10:59.920
jump into, you know, something new, maybe the kind of a fresh perspective from your side of the

10:59.920 --> 11:10.400
collaboration. Sure, sure. So when we start, like I was saying a couple minutes ago,

11:10.400 --> 11:16.960
part of this started because we realized that we had this surprising shared intellectual interest

11:16.960 --> 11:26.960
in how networks can be essentially mobilized to, with influence, maximization as sort of the

11:26.960 --> 11:35.920
basis for this. And so can you pick a set of, you know, nodes within this network that could be

11:35.920 --> 11:40.960
trained, this is in the context of the way that I think about, that could be trained to disseminate

11:40.960 --> 11:45.680
messages that would improve health and well-being in a community. In this case, we were really interested

11:45.680 --> 11:52.480
in youth who are experiencing homelessness and preventing HIV because almost one in 10 youth

11:52.480 --> 12:02.720
who are experiencing homelessness have HIV, which is compared to 0.0.2% of the housed population.

12:02.720 --> 12:08.880
So we're talking, you know, orders of magnitude more HIV risk. And that's because, you know,

12:08.880 --> 12:15.760
these young people are, you know, they're from very risky backgrounds where they're, you know,

12:15.760 --> 12:21.680
they're coming from very abusive households, lots of substance abuse in those families,

12:21.680 --> 12:27.680
substance abuse amongst the young people themselves. They get involved in exchange sex in the

12:27.680 --> 12:35.200
impartus survive. And also they're just having, you know, you know, sex in sexual relationships

12:35.200 --> 12:42.000
with people that are also in risky environments. And so it's just a, it's a dangerous

12:42.880 --> 12:50.080
from a sexual health standpoint life to live. But, you know, HIV is very preventable if people

12:50.080 --> 12:57.120
use condoms, if people get tested for HIV regularly, if you can inform people about, you know,

12:57.120 --> 13:01.760
some myths and realities about HIV testing. And so that's the sort of the public health goal.

13:01.760 --> 13:07.520
But then the challenge is that youth who are experiencing homelessness are really transient,

13:07.520 --> 13:13.840
their relationships come and go very quickly. They themselves are very mobile as a population. So

13:13.840 --> 13:18.160
they're kind of hanging out in a specific part of a city for a while. Then they move to another

13:18.160 --> 13:23.280
part of the city, maybe even to another city. So you want to, when you want to spread

13:23.280 --> 13:28.640
health information in that community, you have to do it, and you have to do it quickly and

13:28.640 --> 13:33.680
efficiently. So therefore the engineering process becomes really compelling because can we

13:34.640 --> 13:41.120
implement a version of these studies where we can, where we can pick the ideal set or nearly

13:41.120 --> 13:47.120
ideal set of young people to be that we would work with intensively on a short-term basis.

13:47.120 --> 13:51.120
And then they work in their communities to advocate for the health of their, of their friends.

13:51.120 --> 14:00.800
And so that project was really exciting. We had some graduate students in computer science

14:00.800 --> 14:05.440
who worked on the algorithms. I had some graduate students in social work who worked on developing

14:05.440 --> 14:12.960
the actual HIV prevention, intervention, you know, training and how to work with the young people

14:12.960 --> 14:21.440
face to face. And we had some successful papers that won awards, that students won on the computer

14:21.440 --> 14:28.160
science side, projects that got funded and had a lot of visibility on the social work side

14:28.160 --> 14:33.360
for my students. And we were lucky enough to get a grant funded by the state of California,

14:33.360 --> 14:39.680
California HIV research project. They gave us almost a million dollars to do a large-scale study,

14:39.680 --> 14:49.360
and we actually did an experimental study where we had 714 homeless youth over the course of two

14:49.360 --> 14:56.880
years that we enrolled in this program and watched, you know, whether or not we could use the AI-driven

14:56.880 --> 15:04.240
influence maximization peer leader selection process to improve the impact of this intervention.

15:04.240 --> 15:08.240
And we did. And it was, it was really gratifying. I mean, I remember the first time that I ran the

15:08.240 --> 15:14.560
data after the project had, we had gotten some initial, the initial data back from the field.

15:15.440 --> 15:19.680
And I just sat there shaking my head, oh my god, it actually worked. I mean, it's one thing as

15:19.680 --> 15:25.600
a social scientist to see these simulation models where, you know, you can see that, you know,

15:25.600 --> 15:32.320
from a, from a mathematical basis that this ought to be more efficient and it ought to have a

15:32.320 --> 15:39.120
greater impact. It's another thing to actually see the numbers of real human beings who were

15:39.120 --> 15:45.120
impacted be greater when you use those algorithms versus when you didn't. And it was just unbelievably

15:45.120 --> 15:50.720
exciting to have that happen. And so, you know, and that really took in terms of like those three

15:50.720 --> 15:55.920
lessons. I mean, you know, it was the social science team, the computer science team, but also the

15:55.920 --> 16:05.440
community. So, you know, the young people themselves and, and three big community agencies that

16:05.440 --> 16:09.920
work with homeless youth and Los Angeles worked with us very closely on that project. And it was

16:09.920 --> 16:15.920
the collaborations there, the long process of learning to talk to one another and really iterating.

16:15.920 --> 16:22.640
I mean, it was the, the final, the final algorithm that we tested with 700 people was, I think,

16:22.640 --> 16:30.400
the fourth or fifth iteration of the, the influence maximization algorithm. So it was not, you know,

16:30.400 --> 16:36.400
we had some of them were computational solutions that were discarded for improved computational

16:36.400 --> 16:40.000
solutions. And some of them were computational solutions that we tested and then said, well,

16:40.000 --> 16:44.880
now we've learned a new problem. We need to fix that as well. So here's a, here's a computational,

16:44.880 --> 16:50.320
new computational solution to an even more complex field problem. So it was, it was really interesting

16:50.320 --> 16:57.360
and gratifying work. And, you know, and I think the one, yeah, so, so I mean, that was, that was,

16:57.360 --> 17:02.480
that was kind of how it started. And that's the work that I did with, with, with Millen, and then,

17:04.000 --> 17:10.400
you know, and that was really the project that was the, the center piece really of him and my

17:10.400 --> 17:20.080
creating the center at USC back in 2016 and the fall, because we had just gotten funded

17:20.080 --> 17:26.640
to, for that study, we had just gotten, you know, I think the second paper that we'd written with

17:26.640 --> 17:33.440
some students in computer science had one on award working on this project. We had some other

17:34.000 --> 17:38.400
people that were really getting excited about the work that we were doing and we decided to

17:38.400 --> 17:43.680
create this center with some other faculty and just bring people are, you know, our students and

17:43.680 --> 17:48.560
faculty together to start working in these interdisciplinary projects. And, and, you know,

17:48.560 --> 17:53.760
and that's really what I've been doing for the last five years, you know, is, is, is now just doing

17:53.760 --> 18:01.840
social science, computer science, hybrid projects. And, and, and kind of talking about this

18:01.840 --> 18:08.720
idea of learning to communicate in different languages, it's, what extent did that extend to

18:09.600 --> 18:17.120
the different ways that you might approach assessment and measurement in a project like that as a

18:17.120 --> 18:22.400
social scientist, you know, versus, you know, Millen's approach and the computer science approach

18:22.400 --> 18:28.480
of kind of assessing the algorithmic performance of that, that specific model.

18:29.520 --> 18:37.280
Yeah, now that's an interesting question. I think, so computer scientists often get data

18:38.080 --> 18:45.360
that is relatively robust data. So, for example, you know, if you're getting, you know,

18:45.360 --> 18:52.320
click through rates from web pages like that data, it's pretty solid data. It's not messy

18:53.440 --> 19:00.080
data. In the social sciences, we often deal with very, very, very messy data because it's collected

19:00.080 --> 19:08.240
by human beings from the self-reports of human beings. And, you know, if you're interested in

19:08.240 --> 19:13.200
say studying, you know, youth who are experiencing homelessness over time, you've got to find those

19:13.200 --> 19:18.800
people again, right? And, you know, you've got to find those people again, you've got to create

19:18.800 --> 19:24.640
situations in which they feel like they can tell you the truth. You've got to create, you know, data

19:24.640 --> 19:33.280
collection techniques that are in language that those people understand, right? So, I mean, it's like,

19:33.280 --> 19:38.560
you know, you as an academic may understand a sentence that's got double negatives,

19:38.560 --> 19:45.200
but when you are talking to a 18-year-old who is a high school dropout, a double negative

19:45.200 --> 19:50.000
sentence may be completely, you know, confusing. I mean, it may not always be, but certainly,

19:50.000 --> 19:55.040
you know, you have to deal with with this as a reality. And, and then there's also, there's also

19:55.040 --> 20:01.200
messiness that I've experienced in working with computer scientists and social scientists together

20:01.200 --> 20:06.880
when we work with community partners and we're trying to use their existing administrative data.

20:06.880 --> 20:11.440
So, sometimes people get really excited about the fact that social service agencies

20:12.320 --> 20:18.960
collect a lot of data on their interactions with their clients over time. And that's a really

20:18.960 --> 20:25.440
exciting thing, but the challenge there is that the data is usually entered by social workers

20:25.440 --> 20:33.280
who are overworked, who may not necessarily prioritize data entry rigor over, you know,

20:33.280 --> 20:37.120
helping somebody find a house, right? And so, it's like, oh, at the end of the day, oh, yeah,

20:37.120 --> 20:40.320
right, I got to enter these things and then I'm trying to remember what it is that I did over the

20:40.320 --> 20:44.640
course of the day and, you know, and, you know, oh, you know, I'll do it tomorrow kind of thing,

20:44.640 --> 20:50.000
you know, and so things get forgotten, things get, get entered incorrectly, you know. And so,

20:50.000 --> 20:54.800
there's a lot of uncertainty, which is actually why it's been for me. One of the things that's

20:54.800 --> 21:00.480
been really fun is that as a social scientist, the way that we used to have to deal with uncertainty,

21:00.480 --> 21:04.320
or the way I used to have to deal with uncertainty is, okay, I'm going to do some basic statistical

21:04.320 --> 21:08.160
modeling and there's just going to be a lot of error. And at the end of the day, I'm going to have

21:08.160 --> 21:12.880
to talk about it at the end of my paper, what I understand as the complexity of the sources of

21:12.880 --> 21:17.360
these errors, like some of the errors, like some of the things I just described to you. But with

21:17.360 --> 21:22.560
computer scientists, sometimes they talk about, you know, robustness, right? So it's like, okay,

21:22.560 --> 21:30.320
well, if we can, if we can actually kind of delineate what it is, the world of uncertainty

21:30.320 --> 21:36.480
is, we can actually try to design some algorithms that are kind of best solutions in spite of the

21:36.480 --> 21:41.760
presence of this uncertainty or error, which as a, which as a social scientist was mind-blowing,

21:41.760 --> 21:45.680
when I first started talking to them, I was like, you, but it's really kind of the difference

21:45.680 --> 21:53.120
between treating data in sort of this very 20th century kind of way. It's like data as is versus

21:53.120 --> 21:59.600
treating data in a much more probabilistic fuzzy kind of way, which has become much more the,

21:59.600 --> 22:06.240
the realm of data science and computer science. And so it's really exciting, but still as a social

22:06.240 --> 22:11.120
scientist, I think one of the things that I can do is I can help the data science folks to understand

22:12.640 --> 22:17.920
what is, what are the sources of messiness in this data that we need to be paying attention to?

22:17.920 --> 22:22.960
You know, because you know, and there are things that, you know, I may know from sort of years

22:22.960 --> 22:30.080
of collaborating with community partners about the nature of these data that could be helpful,

22:30.080 --> 22:33.760
which isn't to say that you couldn't be a very thoughtful data scientist and make a good connection

22:33.760 --> 22:38.320
with a community agency and really talk to them in great detail about how they collected their

22:38.320 --> 22:42.960
data and what it all means and these sorts of things. But there's sort of a, I don't know,

22:42.960 --> 22:48.000
I think the three, the kind of this triangle, so to speak, of computer science, social science,

22:48.000 --> 22:56.000
and community collaborations really is a nice combination of partnerships where there's some,

22:56.000 --> 22:59.840
there's some real value added by everybody who comes to the table there.

23:04.880 --> 23:10.240
Let's maybe talk about some of the other collaborations and projects you've worked on.

23:10.240 --> 23:16.800
Sure, sure, no, I'd love to. So one of the ones that I'm, so first of all, I guess it's one

23:16.800 --> 23:23.040
thing to say is that, you know, as personally as an academic, most of my work has been around

23:24.480 --> 23:29.920
homelessness over the years. And so while HIV prevention is certainly one of the issues that

23:29.920 --> 23:34.240
people face, I've also done a lot of work around housing and housing intervention.

23:34.240 --> 23:38.720
And, and, and, and then my center, and I'll come back to the housing intervention in a minute,

23:38.720 --> 23:44.560
but the center itself, we've got several areas that we focus in on. So we're, we're interested in

23:44.560 --> 23:52.320
what we think of as, as sort of robust resilient communities, which is in a sense like planning for

23:52.320 --> 23:58.080
disasters and really the, the, the, the impact on human populations of global climate change is a big

23:58.080 --> 24:03.360
part of this. There's also some, some work that we've done on, on, on conservation as well. But

24:03.360 --> 24:08.400
then most of them are a little bit more, you know, hardcore social work driven. So it's, you know,

24:08.400 --> 24:12.960
health and mental health. We're, we're really interested in homelessness is a lot of the work

24:12.960 --> 24:20.480
that we do. We have a specific focus on suicide prevention. And we also have a big focus on

24:21.040 --> 24:26.960
substance abuse interventions as well, because of the, some of the other social scientist

24:26.960 --> 24:32.320
collaborators that, that are like, I have a colleague in front of Jordan Davis, who, who's an expert

24:32.320 --> 24:37.920
in substance abuse interventions. And he is, you know, he's, he's working in that area. And those

24:37.920 --> 24:46.720
are part of our portfolio. And so the, the two projects that I've been working on, other than the

24:46.720 --> 24:50.880
one that I just described with Millen, the most intensely over the last years, it is, our last

24:50.880 --> 24:58.320
couple of years has been one in suicide prevention and one in, in housing allocation. And so the,

24:58.320 --> 25:06.480
the suicide prevention project is working really on trying to use some machine learning predictive

25:06.480 --> 25:14.240
analytics to understand network level interactions between people. I'm really interested in social

25:14.240 --> 25:18.560
networks, which is going to come up over and over again. And trying to understand like really who it

25:18.560 --> 25:23.840
is that people turn to in times of need when their mental health is really, when they're really

25:23.840 --> 25:30.000
suffering and trying to understand if we can do some data mining of network data to try to understand

25:30.000 --> 25:38.480
better who might be people that could be eventually intervention allies, kind of in the same way

25:38.480 --> 25:44.320
that we thought about the intervention allies in the, in the HIV prevention study. So, you know,

25:44.320 --> 25:49.600
the HIV prevention study had a lot of social science work before I got involved in data mining

25:49.600 --> 25:56.400
types type of work that led to the, the, eventually the, the models that we created for interventions.

25:56.400 --> 25:59.200
And so likewise, we're doing stuff with suicide prevention. And we're working with a couple

25:59.200 --> 26:04.880
different populations, there are three actually. One is college students, another is homeless youth,

26:04.880 --> 26:09.680
again, homeless youth. I'm all about homeless adolescents. And then the third is actually working

26:09.680 --> 26:15.920
with folks in the army. And so we actually have a study that we, that we just finished doing the

26:15.920 --> 26:26.080
field work on where we, we, we, we looked at a, a battalion who had experienced a very high profile

26:26.080 --> 26:34.000
member of their community had died by suicide. And then we were allowed to come in and interview

26:34.000 --> 26:39.920
about 250 of these soldiers who had been impacted by this suicide about how they were talking with

26:39.920 --> 26:45.440
one another. You know, and so we've got, I don't know, something like, you know, 250 humans and maybe

26:45.440 --> 26:50.400
2,000 connections and the ways that they're communicating about ideas. And it's, it's really,

26:50.400 --> 26:55.680
we've just barely gotten into it. But it's, it should be really, hopefully really impactful to,

26:55.680 --> 27:00.960
to help the, you know, the folks in the army as well. And so it's like, there's a variety of

27:00.960 --> 27:07.120
populations that are impacted by suicide. You know, and unfortunately actually since COVID has happened,

27:07.840 --> 27:11.680
you know, suicide is up in the United States. And actually, I think across the world, although I

27:11.680 --> 27:15.360
don't know the data very well outside of the United States. And, and, and I think that, you know,

27:15.360 --> 27:20.400
so it's, it's timely to be, you know, thinking about these issues. But then the other project that

27:20.400 --> 27:27.440
we're jumping in to mention, because I remember going down this path and the conversation with

27:27.440 --> 27:32.400
Millen, where at time he said, network, I was thinking, oh, okay, so we're mining social network

27:32.400 --> 27:38.640
information for, you know, relationships. But in fact, that's not typically the case in your work.

27:38.640 --> 27:44.880
It's more about using social work techniques to try to understand the implicit networks

27:44.880 --> 27:49.360
among the participants in your studies. Is that right? You know, that's a really good thing to,

27:49.360 --> 27:55.200
to clarify, because I think in the most people when you say networks these days, think about

27:55.200 --> 28:00.400
Twitter networks or Instagram networks or Facebook networks, right? And what, what, what I'm talking

28:00.400 --> 28:07.040
about usually is, and at least in the context of homeless youth was face to face networks of

28:07.040 --> 28:13.120
conversation networks that people have that are these really, I guess you might think of as

28:13.680 --> 28:18.480
implicit networks that, you know, you can observe people talking to each other, right? But there's no,

28:18.480 --> 28:23.040
there's no, I friended you, right? On the streets, like, like, I'm talking to you, because you're

28:23.040 --> 28:28.080
my friend, but we don't, we don't like, you know, there's no formal linkage through a, through a

28:28.080 --> 28:34.000
software, right? And I think the same is true with folks that we're talking about in the army. So

28:34.000 --> 28:37.760
you've got in, in a sense, you know, when you're looking at people that are in a battalion,

28:37.760 --> 28:42.720
in a formal sense, they're all connected, right? Like these are, you know, these are platoons that

28:42.720 --> 28:46.160
are within, you know, there's like these nested structures and there's these formal networks,

28:46.160 --> 28:52.080
but then there's also informal networks, right? So not everybody that I work with, do I necessarily

28:52.080 --> 28:59.280
trust with the, you know, my deepest, darkest fears and secrets, that's a subset of people. And

28:59.280 --> 29:04.720
when you're talking about people that are, you know, sort of like on base, getting ready to gear

29:04.720 --> 29:09.920
up for deployment, which is who the folks that we were talking to were, you know, they're also talking

29:09.920 --> 29:14.640
to their spouses, if their spouse lives on base with them, their kids, if their kids live on base

29:14.640 --> 29:19.040
with them, their girlfriends, if they've, or boyfriends, if they've got a girlfriend or boyfriend that's,

29:19.040 --> 29:24.080
you know, in the, in the community, or frankly, friends from high school, or girlfriends or

29:24.080 --> 29:29.600
boyfriends that aren't on base and aren't even in, you know, the nearby, the, the base, but they're,

29:29.600 --> 29:33.440
you know, whatever, 3,000 miles away in California or something like this. And so,

29:34.320 --> 29:41.200
but some of those connections are Facebook, Instagram, Twitter, you know, Snapchat, whatever.

29:41.200 --> 29:45.200
But, but most of the time when I'm thinking about networks, I'm thinking about them in a more

29:45.200 --> 29:50.240
kind of, you know, abstract social work sense, which is the sort of sense of, like, you know,

29:50.240 --> 29:56.880
we're all networked together as human beings. And, but, but how we define those networks is, is,

29:57.840 --> 30:04.800
can be very important. And, and rarely do I do work where we are, say, mining Twitter networks

30:04.800 --> 30:07.840
or something like this. I mean, I've got some colleagues that do that work. And it's really cool.

30:07.840 --> 30:11.680
There's some really interesting questions to answer there. But most of the questions that I've

30:11.680 --> 30:17.040
been interested in over the years because of my focus on, especially people who are experiencing

30:17.040 --> 30:20.880
homelessness who have limited access to technology, right? I mean, it's like most of the people

30:20.880 --> 30:26.240
that I've worked with over the years, like they have a cell phone. But, you know, oftentimes,

30:26.240 --> 30:32.400
they don't have any, their data plan is, is, is, doesn't work. And so it's kind of like a Wi-Fi device.

30:32.400 --> 30:37.760
And so they're like finding a, you know, it's like, so yeah, they're using social media, but they're,

30:37.760 --> 30:41.760
but the homeless youth that I'm looking at are, they're using it on like a daily basis or a weekly

30:41.760 --> 30:47.120
basis, not an hourly basis, the way that you would think about like a young adult who, you know,

30:47.120 --> 30:52.480
has a, you know, typical 20-year-old in America is like glued to their phone. Whereas,

30:52.480 --> 30:55.120
a homeless youth would love to be glued to their phone. They just don't have the resources to

30:55.120 --> 31:03.520
be glued to their phone, you know. And so in terms of the, this applying machine learning to the

31:03.520 --> 31:10.000
social networks to try to identify and prevent suicide, what's the general approach and

31:10.640 --> 31:14.800
where does machine learning come into play there? Sure, sure. So, so, so what we're, what we're doing

31:14.800 --> 31:22.880
is that we've collected through some surveys. We ask people to talk to us, or to, to, to delineate

31:22.880 --> 31:28.320
in these surveys, the 10 or 20 people that they talk to the most frequently. And then we ask them

31:28.320 --> 31:34.160
in a, a battery of, you know, I think it's about 30 or 40 questions about who these people are,

31:34.160 --> 31:38.800
the frequency of which you talk to them, what are the means by which you talk to them, what do you

31:38.800 --> 31:45.120
talk to them about, what roles do they occupy in your life, you know, things like, you know,

31:45.120 --> 31:49.200
if you needed to borrow, you know, a thousand dollars, is there anyone on this network that you

31:49.200 --> 31:56.240
could do that from? When you are feeling, you know, if, if you were feeling yourself suicidal,

31:56.240 --> 31:59.280
who would you go into? Is there anyone in this network space that you would talk to?

32:00.400 --> 32:06.720
Who did you know before you joined the service? Who, who on here is a family member?

32:07.360 --> 32:12.000
Is there anyone on here that's a romantic partner? How frequently do you talk to each of these

32:12.000 --> 32:16.080
people, these sorts of things? And then, and then that becomes a data set that has

32:16.080 --> 32:23.040
thousands of diads. So thousands of relationships that have a lot of information, a lot of features

32:23.040 --> 32:27.840
about those relationships. And then rather than approaching it in kind of the traditional social

32:27.840 --> 32:33.360
science way, which is to say, well, we a priori hypothesize that these four or five factors are the

32:33.360 --> 32:37.200
things that are important. And so we're going to look at some statistical models and see what,

32:37.200 --> 32:42.560
and see if those four or five things are statistically significantly associated with, say, the,

32:42.560 --> 32:48.400
the ability to communicate about suicidal thoughts to a particular person as an outcome, you know,

32:48.400 --> 32:51.360
here we're going to, we're going to do some data mining, right? So we'll just do some predictive

32:51.360 --> 32:55.200
analytics, right? Like let's just throw in all the features that we've got in the data set,

32:55.200 --> 32:59.040
you know, and, and see what, and see what shakes out, you know, and, and so then it's, you know,

32:59.040 --> 33:03.200
and with, it was only a couple of thousand people, you know, you can, you know, yeah, I mean,

33:03.200 --> 33:06.400
convolutional neural networks probably aren't going to work, right? But, but certainly you can do

33:06.400 --> 33:12.480
things like decision trees and, and, and random forests and, you know, maybe even some, you know, SVMs

33:12.480 --> 33:18.720
or whatever. But, but even those, those contributions thinking about letting the data speak for itself

33:19.280 --> 33:26.720
is, is, is, is not the way that traditionally social scientists have, have, have done things. And,

33:26.720 --> 33:33.440
and, and there are some interesting ways in which, when you're, when you're stuck in, when you're

33:33.440 --> 33:38.720
stuck in the world of linear models, like, logistic regressions, like, like traditional statistical

33:38.720 --> 33:45.600
modeling, social scientists come up with very linear answers to relationships between variables.

33:46.400 --> 33:50.320
The nice thing about the way the computer scientists have, and data scientists have started to

33:50.320 --> 33:55.120
look at things is that sometimes non-linear combinations of variables are really, are really

33:55.120 --> 34:02.160
what's happening in the world. Like, the, and, and that is a very interesting new way of thinking

34:02.160 --> 34:07.680
about things that, that social sciences are, I think, benefiting from in recent, and recent years. But,

34:08.880 --> 34:12.080
yeah, so it's, it's, so that's kind of how we would, how would use it. And then, and then the idea

34:12.080 --> 34:19.360
would be, especially if we could use a, if the models are ones that have more transparency to,

34:19.360 --> 34:23.200
to how they work and what the, and what the underlying features are, then we can know what are the

34:23.200 --> 34:27.120
important features to track over time. And then as we move forward towards thinking about intervention

34:27.120 --> 34:32.400
models, you could think about creating small assessments that could then help you very quickly

34:32.400 --> 34:38.960
identify people who could be targets of intervention down the road. So, who are the types of people

34:38.960 --> 34:44.400
that, that, homeless youth need to talk, need to be having their, in their networks, cultivate

34:44.400 --> 34:49.760
in their networks to talk about suicide, or who are the people in, in these, you know, these,

34:49.760 --> 34:54.560
these, in networks of soldiers that people need to talk to, and trying to identify who those people

34:54.560 --> 34:59.840
might be. I don't know if that, that makes sense, but that's kind of the, the, the thrust of the ideas.

35:00.800 --> 35:07.680
It sounds like the initial phase of the research is to understand these communities and build a

35:07.680 --> 35:16.560
model. And then later on, you start to experiment with, or apply the model to determining which

35:16.560 --> 35:21.120
interventions are likely to be the, the most successful. And they're kind of distinct phases.

35:21.120 --> 35:25.360
It sounds like. Yeah, yeah. I mean, and this is interesting because I think this is something that

35:25.360 --> 35:32.400
both engineers and social workers think about a lot. Like there's, there's the, trying to understand

35:32.400 --> 35:38.560
the world phase of things, which oftentimes is sort of prediction, right? And then there's the,

35:38.560 --> 35:43.440
what do we do, phase of things, which is, which oftentimes, you know, I hear computer scientists say

35:43.440 --> 35:48.960
prescription is the, is the phrase I've heard like Eric Horowitz say from Microsoft research say,

35:48.960 --> 35:52.640
like here's prescription, prediction, then prescription. And the way that, you know, social scientists

35:52.640 --> 36:00.640
think about is oftentimes when you're looking at like trying to intervene with risky communities,

36:00.640 --> 36:04.720
you usually think about the first phase of the research is trying to understand what are the

36:04.720 --> 36:11.280
risk and protective factors that you could then leverage in your intervention phase of things.

36:11.280 --> 36:16.000
And that really basically boils down to prediction and then prescription. And then so it's like,

36:16.000 --> 36:23.120
again, language difference, right? Well, we're talking about two very analogous, you know, phases

36:23.120 --> 36:32.880
of work. And it's been interesting to me how much social work and engineering share in world

36:32.880 --> 36:38.320
views, even though the techniques are very different. Because in both engineering and this,

36:38.320 --> 36:46.080
I think, you know, when you end and in social work, the idea is to do not necessarily to do science

36:46.080 --> 36:53.280
for the sake of science, but it's to do science for the sake of creating solutions to problems.

36:53.280 --> 36:58.560
And in the case of social work, it's social problems. In the case of engineering, it can be,

36:58.560 --> 37:04.480
you know, physical problems as well as social problems. But now my experience with this sort of AI

37:04.480 --> 37:10.400
for social good, you know, universe is that there's a lot of computer scientists that want to work

37:10.400 --> 37:16.560
on social problems. And so, you know, we're, you know, trying, at least I'm trying my best to sort of,

37:16.560 --> 37:20.240
you know, work with those folks and see what else, you know, what new things can come out of those

37:20.240 --> 37:26.480
collaborations. Great, great. I want to make sure we talk about a third project that you mentioned.

37:26.480 --> 37:32.800
Oh, yeah. It sounds like, well, it's relating to systemic racism. And I believe it's housing

37:32.800 --> 37:39.120
specific. Yeah, thank you. It's like, the project, I'm probably most, I'm most invested in right

37:39.120 --> 37:44.400
now. And I forgot to talk about Phoebe Viandos, who's my partner, and this would kill me.

37:44.960 --> 37:49.280
So, yeah, so, so, so I've got this wonderful project that we're working on.

37:50.480 --> 37:54.480
We've got a partnership with the LA Housing Service Authority. I also have a partnership with

37:54.480 --> 38:00.480
some people at UCLA in the, in the, in the Semmel Institute of Nurse Psychiatry as well as the,

38:00.480 --> 38:05.440
as the, as well as the, the California Policy Lab, then people in, in the, you know,

38:05.440 --> 38:09.440
the School of Engineering at USC. So, there's this large group of people who've got community

38:09.440 --> 38:17.600
partners. And what we are doing there is we are, the problem that we're trying to solve is,

38:18.240 --> 38:24.480
how do we identify people who are in the greatest need for housing resources and make sure that

38:24.480 --> 38:31.280
they get the best interventions possible. And the challenge with this is that in a lot of communities

38:31.280 --> 38:36.240
like Los Angeles and many other big cities across the country, there are more people who experience

38:36.240 --> 38:40.960
homelessness than there are resources to go around, right? So LA, for example, every night has

38:40.960 --> 38:45.840
about 60,000 people who are either living in an emergency shelter around the streets.

38:46.880 --> 38:51.520
LA Housing Service Authority placed the most human beings of any community into housing

38:51.520 --> 38:56.720
last year, relative to anyone. It was 20,000 people, right? So you can see there's a huge disconnect

38:56.720 --> 39:02.240
between, you know, the available resources and, and, and what they're. So the challenge then is

39:02.240 --> 39:07.520
who do you serve? And so one of the thrusts has been to serve people that are considered to be the

39:07.520 --> 39:13.600
most vulnerable. And then, so then, okay, that sounds great. Like we don't want people to die on the

39:13.600 --> 39:17.120
streets. So we're going to try to, we're going to try to make sure that we get people who need

39:17.120 --> 39:22.560
these resources the most into housing. But then the challenge becomes, is this fair? And you can

39:22.560 --> 39:26.720
kind of think about this in a few different ways. One is, are the resources allocated fairly,

39:26.720 --> 39:32.720
meaning, you know, if there's 40% of the homeless population in LA that's black, which is what,

39:32.720 --> 39:37.600
which is what we have, are 40% of the resources going to black people. And what we find is the answer

39:37.600 --> 39:45.920
to that is yes. But then, if you ask the question, are, are black people succeeding in those housing

39:45.920 --> 39:53.280
allocation as it, at the same rates? What we're seeing is that you're the likelihood that people

39:53.280 --> 39:59.760
return back to homelessness after getting a housing intervention seems to be higher for black

39:59.760 --> 40:06.560
and Latinx folks than it is for white folks. So then the question is, okay, so we're trying to do

40:06.560 --> 40:13.760
this and we're trying to do this fairly. But can we, can we, can we really do this in a way that

40:13.760 --> 40:22.640
is genuinely mitigating a history in the United States of housing inequality that, that, especially

40:22.640 --> 40:28.480
bullet bot communities experience, but also the Latinx community is experiencing. And, and so what

40:28.480 --> 40:33.840
we're trying to do is we're trying to, and the way that this is done is that in the, in, in most

40:33.840 --> 40:40.400
communities in the US, they use these triage tools. It's like a survey instrument where we ask about,

40:40.400 --> 40:46.400
you know, about 40, 40 to 50 questions about terrible things that happened to you in your life.

40:46.400 --> 40:52.320
Like, have you been, have you been arrested? Do you have a substance abuse problem? Do you have health

40:52.320 --> 40:59.280
issues like HIV? And on and on and on. Have you been abused? And then the more of those risk

40:59.280 --> 41:04.000
factors that you have, the more vulnerable you are and the more likely you are then to be served by

41:04.000 --> 41:08.720
these, you know, the, the scarce housing resources. But there are a couple of different housing

41:08.720 --> 41:14.320
resources that most communities have. Some of them are short-term rental subsidies and others

41:14.320 --> 41:21.040
are more long-term programs that have extensive social services and social workers that are attached

41:21.040 --> 41:26.400
to them. And so one of the things that Phoebe Vianos and I have been working on and this is

41:26.400 --> 41:33.120
really her area of expertise is really these resource allocation problems. So given that we have

41:33.120 --> 41:38.720
this information about about, in this case with, with LA, we've got tens of thousands of people

41:38.720 --> 41:44.480
every year that are housed. And we've got this history of information about these risk factors.

41:44.480 --> 41:48.480
And then we have a history of information about, did you get a rental subsidy or did you get these

41:48.480 --> 41:54.400
more these the what we call permanent supportive housing, which is this more robust form of housing.

41:55.280 --> 42:00.720
Then we can look at what are the features of people in the past who've done well in each

42:00.720 --> 42:07.760
of those interventions. And then we can see whether or not we can allocate resources

42:07.760 --> 42:11.840
differentially. Because the current system basically says the highest risk people get the

42:11.840 --> 42:16.160
get the most intensive resources, the medium risk people get the get the get the interventions,

42:16.160 --> 42:20.400
which is kind of, which is better than nothing. It's it's thoughtful, but we can do better.

42:20.400 --> 42:28.960
Right. So and so the what we find, especially with the context of people who have experienced

42:28.960 --> 42:36.080
a lot of, you know, systemic racism and inequality, you know, in in the housing market is that

42:36.640 --> 42:42.240
when we reallocate resources such that, well, one of the issues is that when you have,

42:43.520 --> 42:48.800
you have to do fair machine learning, right. So you don't just want to, if you're, if you're,

42:48.800 --> 42:55.840
if your machine learning is based on a majority white population, but it's, but it's inaccurately

42:55.840 --> 42:59.840
then characterizing the black population and the Latinx population, you may actually

43:00.560 --> 43:06.960
misallocate people because they don't fit the, the, the, the, the, the, the, the modal categories,

43:06.960 --> 43:10.480
right. So there's those issues that come into play. And then there's issues about just

43:10.480 --> 43:15.680
allocating resources differently, depending on what people's histories within these systems

43:15.680 --> 43:24.480
have been. And so what we're trying to do now is, is, is redesign the, the use of these survey

43:24.480 --> 43:29.600
assessments that are these vulnerability assessments to try to serve the communities better. And at

43:29.600 --> 43:36.800
least our preliminary, preliminarily, the, the computational experiments that we've been working

43:36.800 --> 43:41.600
on would suggest that you can do a lot better than, than what we've done in the past by both

43:41.600 --> 43:48.720
improving the, the, the, the decreasing the bias in the machine learning processes and then

43:48.720 --> 43:57.200
reallocating resources to, to, you know, to match people in a more thoughtful way than just kind of

43:57.840 --> 44:02.720
really risky gets really good. Medium risky gets pretty good, you know, that you can do better than

44:02.720 --> 44:08.160
that. So, so that's, that's the other project that I'm really invested in. And, and I might have

44:08.160 --> 44:12.640
explained it in a much quicker way than, than the other ones, which is probably unfair to my

44:12.640 --> 44:17.520
commitment to it. But, but I also feel like I kind of rambled on about all of these other ones. So,

44:17.520 --> 44:20.320
but yeah, it's, it's, it's, but it's really exciting. I mean, one thing that's really

44:20.320 --> 44:25.600
exciting about that project is that because we have this partnership with the LA Housing Service

44:25.600 --> 44:33.520
Authority, if we can come up with a viable, some viable solutions, you know, we could potentially

44:33.520 --> 44:40.400
help thousands of people be allocated to more appropriate resources, which would increase the

44:40.400 --> 44:45.520
likelihood that people would succeed in those resources, which would mean potentially that over

44:45.520 --> 44:50.080
time, you know, thousands of people will not return to homelessness that would have returned to

44:50.080 --> 44:56.000
homelessness. And that's, that seems like a really big deal. And, and, and so the social impact of

44:56.000 --> 45:03.120
this could be, could be quite, could be quite large if we, if we do, if we do a good job. And,

45:03.120 --> 45:09.520
and it's exciting to, to get to work on a project like that, frankly. And what stage are you at,

45:09.520 --> 45:17.120
with that? Are you just getting started? So, so we, we are, we've been working on this for a

45:17.120 --> 45:22.480
couple of years. We have a couple of more years in the current grant project that we're working on.

45:22.480 --> 45:27.360
I don't know that will be done entirely by the time, you know, that project is done. It may be one

45:27.360 --> 45:31.760
of these things that, you know, it kind of becomes, you know, something that we work on for, for

45:31.760 --> 45:36.160
wildbies, as I said before, iteration is part of the game, right? So, I, but we will, you know,

45:36.160 --> 45:40.800
my, my, there, there's going to be things in the next, you know, year or so that we will be pilot

45:40.800 --> 45:46.720
testing with, with, with, with in Los Angeles to see how things work. But, you know, kind of larger

45:46.720 --> 45:50.160
scale implementation may take longer. I mean, there's also a huge, I mean, the reality is that

45:50.160 --> 45:54.720
there's a huge political process that's involved in all of this as well, because, you know, housing

45:56.160 --> 46:02.880
is not something that just, there's a, you know, the federal government provides a lot of this

46:02.880 --> 46:08.560
money. Local communities have a, a huge vested interest in how these resources get done. So,

46:08.560 --> 46:14.240
even if we come up with a genius engineering, so engineering slash social work solution to this,

46:14.240 --> 46:18.480
the community then has to still say, like, hey, we think this is important. And part of how we're

46:18.480 --> 46:24.560
doing that is that we have a really heavily involved community process, like we've got a lot of

46:25.280 --> 46:30.480
people that we meet with regularly who themselves have been home, lived through homelessness,

46:30.480 --> 46:36.240
or, and, or who are providers of homelessness resources. We work closely with the LA Housing

46:36.240 --> 46:41.600
Service Authority. We work closely with the departmental health. And we, we talk to all of these

46:41.600 --> 46:47.360
people every step of the way and really get them to help tell us where, you know, where our blind

46:47.360 --> 46:52.400
spots are and how to, and how to pivot. Because, like I said before, you know, social scientists may

46:52.400 --> 46:57.760
contribute a lot to the dialogue, but, but also the, the boots on the ground people who live these,

46:57.760 --> 47:03.040
who live with this, you know, these issues and, and, and with trying to implement solutions also have

47:03.760 --> 47:08.480
insights that, that are valuable. And so, we all come together and we're trying the best that we can.

47:08.480 --> 47:13.920
And, and hopefully that'll mean that people will be more eager to, to, to try these solutions out. But

47:13.920 --> 47:18.720
it's a, it's not, it's not a foregone conclusion that just a good engineering solution gets implemented.

47:19.360 --> 47:22.560
But hopefully this one will, and I think that there's a lot of reason to think that it will,

47:22.560 --> 47:24.880
because of the community participation that we have.

47:24.880 --> 47:27.280
Awesome. Awesome. Yeah.

47:27.280 --> 47:29.760
Well, Eric, thanks so much for joining us today.

47:29.760 --> 47:32.560
Hey, thank you for working on this very cool stuff.

47:32.560 --> 47:37.280
Well, I really appreciate it and thanks for inviting me and, and, and, and pleasure to meet you and,

47:37.280 --> 47:55.440
and, and good luck. Thanks, Eric. All right. Thanks.

