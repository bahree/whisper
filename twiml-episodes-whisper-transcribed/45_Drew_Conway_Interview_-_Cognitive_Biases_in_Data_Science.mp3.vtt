WEBVTT

00:00.000 --> 00:15.720
Hello and welcome to another episode of Twomble Talk, the podcast where I interview interesting

00:15.720 --> 00:20.640
people, doing interesting things in machine learning and artificial intelligence.

00:20.640 --> 00:25.240
I'm your host Sam Charrington, a few announcements before we get to the show.

00:25.240 --> 00:29.640
First, I'd like to take a second to give a virtual high five to everyone who entered

00:29.640 --> 00:34.280
our latest giveaway, in which two lucky listeners will get a chance to attend the recently

00:34.280 --> 00:39.640
rebranded AI conference in San Francisco, compliments of this week in machine learning

00:39.640 --> 00:42.920
in AI, and our friends over at O'Reilly.

00:42.920 --> 00:48.760
This has been our most active giveaway to date, with over 100 of you submitting entries,

00:48.760 --> 00:50.160
just wow.

00:50.160 --> 00:54.440
We haven't picked the winners yet, as of the time of this recording, but by the time you

00:54.440 --> 01:01.560
hear this, winners will be posted over at Twombleai.com slash AISF, and we'll give them a proper

01:01.560 --> 01:04.360
announcement on next week's show.

01:04.360 --> 01:09.880
Next up, about a month ago during my conversation with Chelsea Finn, I thought out loud about

01:09.880 --> 01:12.720
starting a virtual paper reading group.

01:12.720 --> 01:17.360
After receiving lots of positive support for the idea, we finally have a meetup to call

01:17.360 --> 01:18.600
our own.

01:18.600 --> 01:24.920
On August 16, we'll kick off the inaugural Twomble online meetup, where our first presenter,

01:24.920 --> 01:30.120
community member Joshua Manella, will lead a discussion on Apple's GANS paper, learning

01:30.120 --> 01:36.200
from simulated and unsupervised images through adversarial training, which is one of the

01:36.200 --> 01:40.360
best paper award winners from this year's CVPR conference.

01:40.360 --> 01:44.240
If you've already signed up, great, we look forward to seeing you there.

01:44.240 --> 01:49.200
If not, head over to twombleai.com slash meetup to get registered.

01:49.200 --> 01:54.360
Finally, if you've been paying attention, you know that after almost a year of procrastinating,

01:54.360 --> 01:57.160
I've finally launched my email newsletter.

01:57.160 --> 02:01.480
I've been having a blast with it, and you definitely want to subscribe.

02:01.480 --> 02:06.360
We've got some fun stuff in store exclusively for newsletter subscribers, so make sure

02:06.360 --> 02:11.200
you bounce on over to twombleai.com slash newsletter to sign up.

02:11.200 --> 02:16.720
Alright, as you all might know, a few weeks ago, I was in San Francisco for Wrangle, a great

02:16.720 --> 02:20.800
conference brought to you by our friends over at Claudeira.

02:20.800 --> 02:22.520
Wrangle is a super fun event.

02:22.520 --> 02:27.640
Each year, it brings an interesting and diverse community of data scientists to an intimate

02:27.640 --> 02:33.520
and informal setting, this year a music venue in SF's Mission District, for great talks

02:33.520 --> 02:39.280
on real data science projects and issues, not to mention cowboy hats and barbecue.

02:39.280 --> 02:44.120
While I was there, I had a chance to sit down with a few of the event's great speakers,

02:44.120 --> 02:50.640
including Drew Conway, founder and CEO of Aluvium, and a former data scientist with the CIA,

02:50.640 --> 02:57.000
Sherrath Rao, a listener and fan of this podcast and an engineering manager over at Instacart,

02:57.000 --> 03:02.080
and Aaron Schelman, a statistician and data science manager with Zimergen, a company

03:02.080 --> 03:06.280
using robots and machine learning to engineer better microbes.

03:06.280 --> 03:11.160
This show features my interview with Drew, whose Wrangle keynote could have been called

03:11.160 --> 03:14.960
Confessions of a CIA Data Scientist.

03:14.960 --> 03:19.880
The focus of our interview and the focus of Drew's presentation is an interesting set

03:19.880 --> 03:25.120
of observations he makes about the role of cognitive biases in data science.

03:25.120 --> 03:29.520
If your work involves making decisions or influencing behavior based on data driven

03:29.520 --> 03:35.280
analyses, and it probably does or will, you'll want to hear what he has to say.

03:35.280 --> 03:40.040
A quick note before we dive in, as is the case with my other field recordings, there

03:40.040 --> 03:44.880
is a bit of unavoidable background noise in this interview, sorry about that.

03:44.880 --> 03:47.480
And now on to the show.

03:47.480 --> 04:00.600
Alright everyone, I am here at the Wrangle conference, the guest of Claude Ara, who's sponsoring

04:00.600 --> 04:06.680
our series here, and I am with Drew Conway, who is the founder and CEO of Aluvium.

04:06.680 --> 04:08.480
It's going to be with you, Sam.

04:08.480 --> 04:10.600
It's great to have you on the show.

04:10.600 --> 04:14.880
So you just did a really interesting presentation that I tweeted a little bit about.

04:14.880 --> 04:18.040
Why don't we take a minute to have you introduce yourself to the audience?

04:18.040 --> 04:24.800
Sure, so I am the founder and CEO of Aluvium, we're a New York based company that builds

04:24.800 --> 04:30.160
what we call human-centered AI for the industrial industry, and what that means for us is we

04:30.160 --> 04:36.480
build software products that exist at the intersection of complex machine data, so

04:36.480 --> 04:43.520
think streaming data from an olive refinery, data from automated robotic systems, and human

04:43.520 --> 04:44.520
knowledge.

04:44.520 --> 04:47.960
One of the things that I can even talk a little bit about in the context of what I just

04:47.960 --> 04:53.800
presented here at Wrangle is me, my career has really been one that has kind of moved

04:53.800 --> 04:57.600
me through a path of working alongside and building software tools for people who need

04:57.600 --> 05:03.080
to make decisions from data, and Aluvium in many ways is a kind of culmination of a lot

05:03.080 --> 05:09.520
of thinking I've had over my career as to how best to extract maximum value from these

05:09.520 --> 05:15.040
complex streams of data and present a human being, you know, a man or woman who's working

05:15.040 --> 05:18.800
inside an industrial operation with the right information at the right time to make the

05:18.800 --> 05:19.800
best decision.

05:19.800 --> 05:22.400
So the company is just about two years old.

05:22.400 --> 05:27.240
We work primarily in what we call process manufacturing, so sort of distinct from the

05:27.240 --> 05:31.880
screen manufacturing and the way that I just grab is like screen manufacturing is screw

05:31.880 --> 05:37.360
in bolts and rivets and process manufacturing is typically, you know, pipes and boilers

05:37.360 --> 05:38.360
and things like that.

05:38.360 --> 05:44.080
And the reason that we focus on that second half is our approach to learning is really

05:44.080 --> 05:51.320
one where this continuous nature of information is more well suited for what we're doing.

05:51.320 --> 05:55.640
Interesting, particularly interesting because I've been spending a lot of time researching

05:55.640 --> 06:02.440
industrial AI or industrial applications of AI, and we're in the midst of a series of

06:02.440 --> 06:08.480
podcasts on industrial AI, although we're connecting here in a totally different context.

06:08.480 --> 06:11.200
And so why don't we jump into that?

06:11.200 --> 06:13.920
Tell us about your presentation here at Wrangle.

06:13.920 --> 06:14.920
Sure.

06:14.920 --> 06:15.920
So, yes.

06:15.920 --> 06:21.960
So the early part of my career when I first started a career was actually as a what was

06:21.960 --> 06:26.640
called then a computational social scientist inside the US intelligence community.

06:26.640 --> 06:32.880
And so this was a few years before data science was a profession, a sort of well-known profession

06:32.880 --> 06:37.000
and I think probably the people who were doing the work that I was doing then now are probably

06:37.000 --> 06:38.600
all data scientists.

06:38.600 --> 06:45.760
But what my job primarily was to think about how to build statistical programming and

06:45.760 --> 06:52.280
statistical software tools to support decision-making inside the intelligence loop, and basically

06:52.280 --> 06:56.000
that meant my work was split primarily into two big halves.

06:56.000 --> 07:00.720
One half was what I would call principally basic research.

07:00.720 --> 07:05.760
So things that are more academic in nature, I spent a lot of time thinking about graph

07:05.760 --> 07:11.880
theoretic models of network change and network moving over time.

07:11.880 --> 07:17.520
The other half of my work was sort of much more tactical in nature, so it was really building

07:17.520 --> 07:25.360
custom software and even kind of documentation systems for taking in a very wide breath

07:25.360 --> 07:26.600
of different kinds of data.

07:26.600 --> 07:32.080
So all the way from space-based assets and satellite imagery to signals intelligence,

07:32.080 --> 07:38.120
to ground sensing, to an unstructured written report from some PFC in the field and being

07:38.120 --> 07:45.600
able to distill all that information down in a reasonably timely way to help a sergeant

07:45.600 --> 07:51.200
major who is in the field and needs to know whether they go knock on the store as the

07:51.200 --> 07:55.560
person they're looking for going to be there, if they go inspect this shack, will they

07:55.560 --> 07:57.600
find the weapons that they're looking for.

07:57.600 --> 08:03.400
So it was a fascinating place to start my career and when I spoke about this morning

08:03.400 --> 08:10.280
was really part of the lessons that I learned from that with respect to how data science

08:10.280 --> 08:16.160
as a sort of profession and as industry can kind of get wrapped around the axle on bias

08:16.160 --> 08:20.320
and how to overcome that and how to help yourself as a professional data science as it really

08:20.320 --> 08:25.360
had to help the folks around you better leverage and use the tools that you have because everybody

08:25.360 --> 08:30.840
brings bias to their work and I think my experience in the intel community was one where I was

08:30.840 --> 08:35.440
sort of acutely aware of that because the stakes were relatively high.

08:35.440 --> 08:36.440
Right, interesting.

08:36.440 --> 08:42.920
So one of the silly questions that I had for you was, did you have to get your slides approved

08:42.920 --> 08:43.920
by me?

08:43.920 --> 08:51.760
No, no, no, so yeah, all things in my slides were sufficiently generalized that there's

08:51.760 --> 08:56.640
no need to do that but one thing that I did mention in the talk which is sort of interesting

08:56.640 --> 09:00.960
context for being a data science inside the intel community is sort of your access to

09:00.960 --> 09:01.960
tools.

09:01.960 --> 09:06.720
You know, so we, you know, we, the sort of collective community of people doing this work,

09:06.720 --> 09:10.960
we think about just this ready access to the latest and greatest open source tools and

09:10.960 --> 09:16.560
that as soon as Google or Facebook or whomever kind of open source this great new thing,

09:16.560 --> 09:20.160
well, let's figure out a way to play with it and we're going to get it into our workflow.

09:20.160 --> 09:24.360
That is absolutely not the case when the equipment that you're working, you know, literally

09:24.360 --> 09:28.440
the computers that you're doing your work on are classified pieces of equipment.

09:28.440 --> 09:36.160
And so one of the stories that I told in my talk was one where I was trying to overcome

09:36.160 --> 09:44.680
this motivated reasoning as an example of how that problem-motivated reasoning can actually

09:44.680 --> 09:49.920
be addressed through a kind of deliberate technical approach to analyzing data.

09:49.920 --> 09:54.520
And in this case, we were, we were looking at satellite imagery, the context here was the

09:54.520 --> 09:58.880
sort of never-ending search for weapons of mass destruction in Iraq, you know, the timing

09:58.880 --> 10:02.760
here is sort of mid-2000s, 2000s, 2006.

10:02.760 --> 10:07.520
And as I mentioned in my talk, there, I had the opportunity to work with these two, we

10:07.520 --> 10:12.800
would call image intelligence analysts who had been working for, I mean, by the time

10:12.800 --> 10:15.680
I got to know them, I think they've been passed their 30-year mark.

10:15.680 --> 10:20.280
And so these were, these were people with a tremendous amount of expertise and they were

10:20.280 --> 10:29.520
tasked with analyzing images, satellite images taken of Iraq and try to identify any places

10:29.520 --> 10:32.880
where you might see weapons of mass destruction.

10:32.880 --> 10:39.720
And one of the projects that we worked on, which they themselves asked for, was their

10:39.720 --> 10:43.640
intuition was that there were no, they were never going to find so.

10:43.640 --> 10:48.680
And unfortunately, because of this issue of motivated reasoning, they were continuing

10:48.680 --> 10:52.560
to be asked to just look and look and look at no end.

10:52.560 --> 10:56.080
And so we basically came up with this novel idea is like, well, why don't we try to automate

10:56.080 --> 11:03.440
this process so that you can, in aggregate show my automatically analyzing images and

11:03.440 --> 11:09.280
using very simple classification to try to identify where, oh, if there are any indications

11:09.280 --> 11:16.320
of suspicious activity, if that actually exists, because, you know, two men doing this on

11:16.320 --> 11:19.600
their own, they could be at this for the rest of their careers.

11:19.600 --> 11:24.320
And so the reason I bring this up, because it ultimately became a really interesting exercise

11:24.320 --> 11:26.480
in trying to get new tools in the building.

11:26.480 --> 11:32.280
So, you know, one of my greatest achievements, I think, as in this job was actually not technical

11:32.280 --> 11:38.360
but bureaucratic in that I was able to actually get early versions of scientific Python and

11:38.360 --> 11:43.080
OpenCV installed on a classified machine so that we could write a simple classifier to

11:43.080 --> 11:44.880
try to help these guys out.

11:44.880 --> 11:49.240
And ultimately, the success was that A, we were able to do that, but B, we were able

11:49.240 --> 11:52.280
to show an aggregate that there was really nothing there.

11:52.280 --> 11:56.440
And ultimately, we were able to use that as a way to dislodge these guys from having

11:56.440 --> 12:00.960
to continue to pursue something that ultimately they knew they were not going to find.

12:00.960 --> 12:05.680
And do you think that's changed at all the ability to get new technologies open source

12:05.680 --> 12:08.920
into these environments of a, you know, since you left that out?

12:08.920 --> 12:12.680
I think it's improved quite a bit, you know, and it's funny to think, I mean, 10 years

12:12.680 --> 12:19.080
is a long time in this sort of tech timeline, and they're, you know, large bureaucratic

12:19.080 --> 12:23.480
institutions like the military and the Department of Cancer are always trailing indicators,

12:23.480 --> 12:27.680
but one of the things that I've been very impressed by with the folks that I know that

12:27.680 --> 12:34.160
continue to work in this space is their sort of progression to being able to bring new

12:34.160 --> 12:35.160
tools in.

12:35.160 --> 12:40.160
But there, there's part of this, which is there I think there are better tools available

12:40.160 --> 12:44.800
commercially, and so that's always been an easy way to acquire new stuff, but having

12:44.800 --> 12:50.600
it not only in appetite, but a sort of avenue for bringing in open source tools, I mean,

12:50.600 --> 12:58.360
I know even, even AWS works with the Intel community now and creates, you know, distributed

12:58.360 --> 13:00.800
compute for them to actually use some of these tools.

13:00.800 --> 13:05.480
I mean, I think it's been obviously a real boon for their work, but I think more importantly

13:05.480 --> 13:09.400
for recruiting and retention for, you know, very talented people.

13:09.400 --> 13:10.400
Yeah.

13:10.400 --> 13:15.200
So you mentioned motivated reasoning in the context of this story of pulling technology

13:15.200 --> 13:22.080
into one of the agencies, tell us, you know, what that means, and spend some time talking

13:22.080 --> 13:26.560
about the other biases that you talked about in the year, because that was the bulk

13:26.560 --> 13:27.560
year.

13:27.560 --> 13:28.560
Yeah.

13:28.560 --> 13:29.560
Yeah.

13:29.560 --> 13:30.560
Yeah.

13:30.560 --> 13:31.560
So we see it all the time.

13:31.560 --> 13:36.880
It's basically, I ask you a question because I want you to give me the answer that I

13:36.880 --> 13:37.880
already know.

13:37.880 --> 13:38.880
Right.

13:38.880 --> 13:41.840
And motivated reasoning is a particularly sticky wicket when it comes to intelligence

13:41.840 --> 13:48.000
gathering, because policymakers will oftentimes have free-determined policy biases.

13:48.000 --> 13:52.200
And so your job, and as I mentioned in my talk, you know, particularly if you're working

13:52.200 --> 13:57.120
in civilian intelligence, you know, principally the CIA, you have one customer.

13:57.120 --> 14:01.000
Your customer is the president and by extension the White House.

14:01.000 --> 14:06.680
And so, you know, motivated reasoning can be very problematic if you have a customer

14:06.680 --> 14:08.960
with high levels of motivated reasoning.

14:08.960 --> 14:12.960
And there are a number of examples of that throughout history, you know, I spoke about

14:12.960 --> 14:18.880
the sort of search for weapons and mass destruction in the early part of the 2000s, but there

14:18.880 --> 14:20.200
are lots of other biases.

14:20.200 --> 14:21.200
Right.

14:21.200 --> 14:25.560
And I think the next stop and the one that I mentioned during the talk is this idea of

14:25.560 --> 14:26.560
confirmation bias.

14:26.560 --> 14:32.120
Which in many ways I think is sort of the, you know, the cousin or the direct relative

14:32.120 --> 14:37.720
of motivated reasoning, which is maybe I don't already have a predetermined policy outcome

14:37.720 --> 14:39.080
that I'd like to see.

14:39.080 --> 14:45.040
But I sure have a lot of bias in accepting, you know, analysis that confirms the thing

14:45.040 --> 14:46.040
that I already think is true.

14:46.040 --> 14:47.040
Right.

14:47.040 --> 14:52.800
And that is, again, this incredibly problematic lens through which to observe information

14:52.800 --> 14:55.880
that in and of itself is very hard to collect.

14:55.880 --> 15:00.640
You know, I think in the context of data science, people often talk about bias in the data

15:00.640 --> 15:02.120
generating process.

15:02.120 --> 15:03.120
And it's true.

15:03.120 --> 15:04.120
It's everywhere.

15:04.120 --> 15:08.560
You know, if you work at a big social media company, you have access to a tremendous amount

15:08.560 --> 15:09.560
of data.

15:09.560 --> 15:14.160
But some engineer along the way shows to collect that click stream.

15:14.160 --> 15:19.000
You know, that wasn't, that was done by a product manager who decided that they wanted

15:19.000 --> 15:21.680
to track a specific kind of action.

15:21.680 --> 15:22.680
Right.

15:22.680 --> 15:23.680
And that's bias.

15:23.680 --> 15:24.680
Right.

15:24.680 --> 15:29.200
So kind of intel context where there's policy decisions that need to be made off of it.

15:29.200 --> 15:33.680
You have an extremely limited set of kinds of information that you can get.

15:33.680 --> 15:38.400
And oftentimes that information is brought to you in sort of opportunistic way.

15:38.400 --> 15:44.240
You know, one of the, this is well after my time in the intel community, but of course,

15:44.240 --> 15:49.920
after, you know, after the raid, had a Salomon Bin Laden's base camp, a lot of the most valuable

15:49.920 --> 15:53.320
stuff that came out of that were all the laptop computers and information that they

15:53.320 --> 15:57.160
were like, lean there, you know, obviously that data is highly biased.

15:57.160 --> 15:59.480
And that was a, that was a collection of opportunity.

15:59.480 --> 16:03.720
There was no, there was no, like, a B test and no experiment to try to identify which

16:03.720 --> 16:06.040
would be the best pathway to do that.

16:06.040 --> 16:14.280
And so confirmation bias becomes extremely dangerous if you have sudden access to a new

16:14.280 --> 16:16.720
data set that you didn't already have.

16:16.720 --> 16:22.680
And without taking a delivered approach to analyzing it may reinforce that bias in a very

16:22.680 --> 16:23.680
dangerous way.

16:23.680 --> 16:24.680
Yeah.

16:24.680 --> 16:27.400
The last one that I mentioned during the talk and this is sort of where I left the talk

16:27.400 --> 16:32.400
and I think is something that we as a community really need to think about in the context

16:32.400 --> 16:41.880
of our work today now is sort of flat out denial that we, we as data scientists and we really

16:41.880 --> 16:46.880
as sort of technicians because I wouldn't bucket this only for people who, you know, writes

16:46.880 --> 16:55.600
just code, right, all folks who build tools with software have this attachment to both

16:55.600 --> 17:02.040
information that is biased and imperfect and tools that are, you know, sort of approximations

17:02.040 --> 17:06.120
of good ways of estimating what we think is occurring in the real world, the real world

17:06.120 --> 17:07.560
is really hard to measure.

17:07.560 --> 17:14.240
And so this kind of trifecta of imperfection means that everything that we do should be,

17:14.240 --> 17:19.320
you know, fraught with caveats and considerations for all of that stuff.

17:19.320 --> 17:24.640
But what that means is that we are, we accept the fact that we open the door to people who

17:24.640 --> 17:27.360
will try to poke holes and deny that that's true.

17:27.360 --> 17:32.280
Even if we can make a very confident assessment of something and we can show it to be true,

17:32.280 --> 17:37.680
but because of good hygiene around doing data science, it's very easy to poke holes in

17:37.680 --> 17:38.680
that.

17:38.680 --> 17:43.360
And I think there's, I think part of what we need to think about as a community is, well,

17:43.360 --> 17:45.400
how do we prepare ourselves for that?

17:45.400 --> 17:51.040
How do we get better at communicating this kind of, you know, these foibles in our work

17:51.040 --> 17:53.280
that we cannot pull away?

17:53.280 --> 18:00.640
But also, how do we get the consumers out of that information to be more willing to

18:00.640 --> 18:04.440
accept and more educated to a certain extent about that reality?

18:04.440 --> 18:10.080
I mean, one of the things that I mentioned during my talk was, you know, this, the election

18:10.080 --> 18:13.760
in 2016, the US election in 2016, I think it was a great example where people just had

18:13.760 --> 18:19.160
this expectation that, you know, you get, whoever has the higher percentage of winning

18:19.160 --> 18:20.160
is the winner, right?

18:20.160 --> 18:22.720
Because we have this kind of horse race mentality.

18:22.720 --> 18:27.040
But anybody who's ever been to a casino would know that if you, if there was a game at

18:27.040 --> 18:31.360
the casino that gave you, you know, 25% out of winning, you would never leave that game.

18:31.360 --> 18:32.360
Yeah.

18:32.360 --> 18:35.360
And that's essentially the game that we ended up playing and we just got the one in four

18:35.360 --> 18:41.200
chance that we didn't expect to see and I think part of that is this, you know, people

18:41.200 --> 18:47.120
just need to get a better understanding of, I mean, it's part of it, I think, is sort

18:47.120 --> 18:52.680
of becoming more numerate, but I think part of it is just we, we're responsible for that.

18:52.680 --> 18:56.200
The data community is responsible for kind of conditioning folks to understand that,

18:56.200 --> 18:58.920
but I think folks like Nate Silver do a great job and they're saying that I think there's

18:58.920 --> 19:02.120
a lot of success in the world and that, but we need more of it.

19:02.120 --> 19:03.120
Yeah.

19:03.120 --> 19:09.320
And it's, it's struck me that it's a bit of a fine line between, you know, denial and

19:09.320 --> 19:13.960
maybe it's not a fine, maybe it's a golf, but, you know, there's denial on one hand and

19:13.960 --> 19:16.760
then there's, you know, challenging the results.

19:16.760 --> 19:22.960
And I think, you know, we need to be, as a community, be open to having our results being

19:22.960 --> 19:27.440
challenged because the flip side of that is this idea that, you know, data and quote unquote

19:27.440 --> 19:28.440
AI is magic.

19:28.440 --> 19:29.440
Right.

19:29.440 --> 19:31.440
And whatever it says, that's what we got to do, right?

19:31.440 --> 19:35.640
And that's a whole different, just, you know, that poses a whole different set of challenges.

19:35.640 --> 19:36.640
I totally agree.

19:36.640 --> 19:39.640
And I think, you know, we are, we are right at the beginning of what I think will be a

19:39.640 --> 19:46.040
very interesting, you know, future, immediate future for, you know, sort of general consumer

19:46.040 --> 19:52.160
because it seems like the technology trend is to move in the direction of effectively

19:52.160 --> 19:55.000
using black boxes or solutions.

19:55.000 --> 19:59.840
And that means that we accept the fact that it will be very difficult to understand why

19:59.840 --> 20:01.960
a system makes a choice.

20:01.960 --> 20:07.480
And I think we are opening ourselves up to a really difficult set of circumstances that

20:07.480 --> 20:14.320
will very likely come sooner or later where, you know, intelligent software systems, whether

20:14.320 --> 20:18.960
you want to call it AI or sort of a general class of just not decision support systems

20:18.960 --> 20:23.600
but decision making systems that we don't understand.

20:23.600 --> 20:29.320
And, you know, I think the danger now is that in some sense, we've moved so quickly in

20:29.320 --> 20:33.280
the direction of having access to tools like this that the education component is just

20:33.280 --> 20:34.800
not kept up.

20:34.800 --> 20:38.800
And I think the flip side of this, which is in many ways a compliment to it, is I also

20:38.800 --> 20:41.240
think that there's an unnecessary amount of fear, right?

20:41.240 --> 20:46.720
I think now we have this pendulum swinging back the other direction where you have folks

20:46.720 --> 20:54.560
creating a kind of anxiousness around the arrival of tools like this and systems like

20:54.560 --> 20:58.240
this that doesn't really meet with the reality of how that technology trend is doing.

20:58.240 --> 21:05.440
So consumers are being conditioned now to not trust ATM machines and to not trust subway

21:05.440 --> 21:07.000
doors that open and close on their own.

21:07.000 --> 21:12.280
And I think this friction now is really, I mean, literally starting to heat up in a sense

21:12.280 --> 21:16.080
that, again, we're the folks that build these systems.

21:16.080 --> 21:19.600
And I think because of that, it's partly our responsibility to be able to go out and

21:19.600 --> 21:24.560
try to talk to folks and say, you know, this is how this works, you know, fine.

21:24.560 --> 21:28.400
We don't, I think we lack in sometimes good champions for this stuff, right?

21:28.400 --> 21:33.800
We have lots of really good entrepreneurs and technologists who can build this stuff.

21:33.800 --> 21:37.960
We need to find our champions who can actually kind of go out there and try to help folks

21:37.960 --> 21:42.680
understand how this is going to change their lives, you know, for good and potentially

21:42.680 --> 21:44.480
in ways that we don't even understand yet.

21:44.480 --> 21:45.480
Yeah.

21:45.480 --> 21:46.480
Yeah.

21:46.480 --> 21:49.680
I've got to ask the ATM and subway doors, a specific exam.

21:49.680 --> 21:53.880
No, no, I'm just thinking that you know, you hear, you hear all these, you know, a lot

21:53.880 --> 21:56.680
of kind of fear and certainty and doubt or stuff.

21:56.680 --> 22:00.000
And so, you know, actually, these would be more examples for my personal life where I'm,

22:00.000 --> 22:05.720
you know, asked by family members, you know, when am I going to show up to the ATM and

22:05.720 --> 22:10.560
it's going to be hacked or when am I going to show up, you know, autonomous vehicles going

22:10.560 --> 22:14.720
to turn on their own or use, you know, everybody, you know, people have seen, you know, fast and

22:14.720 --> 22:18.360
furious movies and they're like, is this how things are going to work?

22:18.360 --> 22:20.120
And I don't know.

22:20.120 --> 22:25.640
But I also know that that future is a little bit further away than Hollywood is presenting

22:25.640 --> 22:26.640
right.

22:26.640 --> 22:29.600
And there's a long way to go before we actually have to start thinking about that.

22:29.600 --> 22:35.360
I think because there is time, part of our job as a community is to try to fill that information

22:35.360 --> 22:36.880
gap in a little bit.

22:36.880 --> 22:44.560
The speaker after he said that in kind of running through similar sets of issues, more

22:44.560 --> 22:50.600
applied, societally, gave the audience the advice that, you know, as you're thinking about

22:50.600 --> 22:54.000
these systems you're building, think about the degree to which it resembles an episode

22:54.000 --> 22:55.000
of Black Mirror.

22:55.000 --> 22:56.000
Right.

22:56.000 --> 22:57.000
Yeah.

22:57.000 --> 23:01.880
I think that's, you know, if there's a useful rubric, that's probably a really good one.

23:01.880 --> 23:06.640
Like if you start to bleed into Black Mirror territory, and then you probably have made

23:06.640 --> 23:11.320
some choice that you might want to weakless enter.

23:11.320 --> 23:12.320
Yeah.

23:12.320 --> 23:14.560
And for those who don't know Black Mirror, probably the best analogy is like a British

23:14.560 --> 23:15.560
version of the Twilight Zone.

23:15.560 --> 23:16.560
Yeah.

23:16.560 --> 23:17.560
Yeah.

23:17.560 --> 23:19.040
It's a group heard of the Twilight Zone, but is, you know, the Twilight Zone.

23:19.040 --> 23:20.040
Very modern.

23:20.040 --> 23:22.400
Yeah, dealt with lots of different kinds of societal issues.

23:22.400 --> 23:28.080
Black Mirror tends to focus on technology, has a kind of specific or consistent thread.

23:28.080 --> 23:29.080
And well, I'm a big fan.

23:29.080 --> 23:30.080
Yeah.

23:30.080 --> 23:31.080
Same here.

23:31.080 --> 23:32.080
Same here.

23:32.080 --> 23:37.560
So what are the specific, you know, the top three things that you want folks to take

23:37.560 --> 23:41.000
away from your presentation and not just take away, but like go do.

23:41.000 --> 23:42.000
Right.

23:42.000 --> 23:46.560
I mean, I think the first one, you walk out of my talk, and you go back to your, you

23:46.560 --> 23:50.480
know, maybe tomorrow or this afternoon, you go back to your desk at the office.

23:50.480 --> 23:58.400
And sit down and think about how confirmation bias, motivated reasoning, and opportunities

23:58.400 --> 24:05.320
for denial exist in the current decision-making workflow in your company, your organization,

24:05.320 --> 24:10.280
and how your own work may be contributing to that, or helping to save it.

24:10.280 --> 24:14.680
And really, not to be overly negative, but, you know, I think that there is, everybody

24:14.680 --> 24:16.240
can think of examples of this.

24:16.240 --> 24:18.240
And I run a company, I can think of examples.

24:18.240 --> 24:25.200
I mean, I'm heavily motivated to sell my products, and that will impact my decision-making.

24:25.200 --> 24:32.200
And I have told my team, I have high expectation of them to step in and say, well, hold on, Drew,

24:32.200 --> 24:36.120
like, why are you, why do you think that that's a good choice to make?

24:36.120 --> 24:41.840
And this would be specifically in product building, even model selection for some of the,

24:41.840 --> 24:45.760
you know, algorithms that we may choose, you know, this stuff is, this stuff is not,

24:45.760 --> 24:46.760
you know, abstract, right?

24:46.760 --> 24:47.760
There's a real examples.

24:47.760 --> 24:48.760
Right.

24:48.760 --> 24:54.280
And I think the second one, you know, which is, I think, maybe more fun is, you know,

24:54.280 --> 25:01.440
go out and seek the first hand, you know, go try to, try to meet folks and see systems

25:01.440 --> 25:09.120
where this kind of intersection of bias data and decision-making exists in your community,

25:09.120 --> 25:15.680
you know, local government in, you know, in your local community organizing, you know,

25:15.680 --> 25:17.200
this stuff is everywhere.

25:17.200 --> 25:22.680
And one of the things that's nice about kind of professionally applying statistical methods

25:22.680 --> 25:29.720
and computing to this, in this context is that you can bring your own expertise to potentially

25:29.720 --> 25:34.120
a much smaller scale problem that impacts many, many more people's lives and allows people

25:34.120 --> 25:35.680
that you are your neighbors.

25:35.680 --> 25:41.320
And, you know, what, I've had a great opportunity in my life to work with, you know, the government

25:41.320 --> 25:44.480
in New York City and work with local communities there.

25:44.480 --> 25:47.760
And, you know, I was, I was, I was a co-founder of an organization called Data Kind, which

25:47.760 --> 25:54.280
really kind of grew out of this, really this question of how do you, how do you bridge

25:54.280 --> 26:00.200
the gap between talented data scientists and engineers and product managers and a social

26:00.200 --> 26:05.120
sector that has great data, really interesting problems, but doesn't have access to those

26:05.120 --> 26:07.240
talented data scientists and engineers.

26:07.240 --> 26:09.080
And Data Kind exists to do that.

26:09.080 --> 26:13.040
And, you know, you can go sign up on Data Kind's website, that's one way to do it.

26:13.040 --> 26:17.040
And the easier way to do it is just, you know, jump to a community meeting, see what,

26:17.040 --> 26:20.720
see what the Board of Ed is talking about, you know, they're making decisions about what

26:20.720 --> 26:25.120
data to collect on students, you can help, you know, help them make better choices.

26:25.120 --> 26:28.280
And the final one I'll say, which was the final, the sign I'll kind of call to action

26:28.280 --> 26:33.000
of my talk, which is, you know, if you're in a position of hiring people, which I'm

26:33.000 --> 26:36.520
sure many listeners are, you ought to really think hard about hiring veterans.

26:36.520 --> 26:44.040
I had an opportunity, as I said, to work alongside active service folks in the military.

26:44.040 --> 26:51.520
And there's some of the most brilliant, dedicated, you know, technically competent folks that

26:51.520 --> 26:54.000
even now I've ever had a chance to work with.

26:54.000 --> 26:58.560
And I think there are many underrepresented groups in technology and I think veterans

26:58.560 --> 27:01.840
is a group that people don't talk a lot about.

27:01.840 --> 27:10.160
And I think the transition from a career as a signal, you know, a signal analyst on a

27:10.160 --> 27:16.320
submarine to working as a data scientist is actually a lot narrower than certainly folks

27:16.320 --> 27:19.480
in that higher technologists think.

27:19.480 --> 27:24.560
And part of the issue is that a lot of people who come out of the service and then try

27:24.560 --> 27:28.680
to, you know, transition into a professional job, they just didn't even know that these

27:28.680 --> 27:29.680
jobs exist.

27:29.680 --> 27:34.680
You know, and I've worked with an organization in New York called the Iraq and Afghanistan

27:34.680 --> 27:39.880
Veterans of America to try to build, you know, access, you know, just to kind of make

27:39.880 --> 27:40.880
bring these two communities together.

27:40.880 --> 27:47.000
And again, you know, I think this is something that I do in New York, here in San Francisco,

27:47.000 --> 27:50.320
these, all these organizations exist and I would encourage folks in all their communities

27:50.320 --> 27:53.440
to try to work and do that.

27:53.440 --> 28:00.440
How do the biases that you talked about express themselves and your customers when they're

28:00.440 --> 28:07.120
trying to apply industrial AI and data driven decision making, what, how do you see them

28:07.120 --> 28:08.120
show up?

28:08.120 --> 28:16.320
That's a great question and, you know, we see a lot of the bias coming from customers

28:16.320 --> 28:22.280
at a point now where the buzz around AI and machine learning in the context of their

28:22.280 --> 28:23.280
work.

28:23.280 --> 28:29.080
So, you know, I know you're in, you're doing this series, I mean, the industry 4.0,

28:29.080 --> 28:34.280
you know, digital transformation, all these kind of terms, terms of art, basically what

28:34.280 --> 28:40.360
that means is, okay, we're not a software company, but we believe that we need to transform

28:40.360 --> 28:45.080
part of our business to rely more heavily on data and software, because that's what

28:45.080 --> 28:46.760
we think we need to do.

28:46.760 --> 28:50.520
But those are just words and so when you get to actual practical implications, you start

28:50.520 --> 28:56.760
talking to folks who, you know, their job is to make sure that the oil refinery has

28:56.760 --> 28:59.920
no safety issues, ever again, how do we do that?

28:59.920 --> 29:06.360
And because there's a potentially very large white space around what AI and machine learning

29:06.360 --> 29:12.280
can actually do to solve that problem, it's very difficult for that, you know, site

29:12.280 --> 29:17.920
coordinator or plant manager to think creatively about how to solve those problems.

29:17.920 --> 29:23.240
And so, the bias is that we see is, you know, particularly in the beginning, the first

29:23.240 --> 29:29.640
year of Ovaluvium, the bias that we saw is that when you, when you try to rely on customers

29:29.640 --> 29:36.280
creativity, they come back and say, I don't understand this.

29:36.280 --> 29:42.200
I don't, I don't see how it could possibly help my work, because there's no real tangible

29:42.200 --> 29:47.800
connection to all of this fancy math and compute that you want to throw at it and how this

29:47.800 --> 29:55.600
translates to my workforce getting home safely every day and my facility producing at

29:55.600 --> 29:56.600
maximum yield.

29:56.600 --> 29:59.760
And so I think for us, the journey that we've gone through as a company is actually trying

29:59.760 --> 30:05.200
to figure out how to more narrowly go after a set of, really a set of metrics and kind

30:05.200 --> 30:12.360
of value, values that can apply to answering that question because the denial bias, that's

30:12.360 --> 30:17.440
like, I'm going to poke holes in this because it's easier for me to say no than it is to try

30:17.440 --> 30:23.160
to do the work to understand this, you know, it's hard to, it's hard to fault the customer

30:23.160 --> 30:24.160
for that.

30:24.160 --> 30:29.240
Yeah, there, you know, our customers are folks who have worked 15, 20, 25, even 30 years,

30:29.240 --> 30:34.240
you know, in an oil rig, on a, you know, in a manufacturing floor and a power station,

30:34.240 --> 30:38.080
like software is not their job, software is my job.

30:38.080 --> 30:40.320
So I should be able to help them with that.

30:40.320 --> 30:45.600
And so how I can kind of minimize the gap in that is, is a way of minimizing that bias

30:45.600 --> 30:51.080
to try to help them understand and connect the dots between the work they do every day,

30:51.080 --> 30:54.920
the service that we can provide and then how that makes their work better.

30:54.920 --> 31:00.320
Interesting comments around kind of the role, the role of software, I think we go around

31:00.320 --> 31:04.840
ideas like software is eating the world and then, you know, you have big industrial companies

31:04.840 --> 31:10.840
like GE and Ford saying that they kind of committed themselves to become software companies,

31:10.840 --> 31:11.840
right?

31:11.840 --> 31:16.680
So, you know, on the one hand, I think those, you know, I guess maybe I'm a cheerleader

31:16.680 --> 31:20.720
for the industry and say, hey, you know, these companies, you know, every one of these

31:20.720 --> 31:25.160
companies need to start thinking a little bit more, at least like a software person in

31:25.160 --> 31:26.680
some way.

31:26.680 --> 31:32.080
But, you know, you're right, one of the things that has come up repeatedly in talking

31:32.080 --> 31:38.200
about the industrial AI in particular is how, you know, you are, you know, fundamentally

31:38.200 --> 31:43.880
trying to pair, you know, something is like the cutting edge of, you know, software and

31:43.880 --> 31:48.400
technology with, you know, someone who's, you know, been working in a particular, you

31:48.400 --> 31:54.240
know, working with a machine, you know, a CNC machine, you know, or a, you know, a boiler

31:54.240 --> 31:55.400
or something like that.

31:55.400 --> 32:00.120
And they know it's like they're the CNC whisperer, right, that thing makes a certain noise,

32:00.120 --> 32:04.000
they know that it's going to need some care and feeding and they know how to do it.

32:04.000 --> 32:09.320
And I think it's an interesting responsibility for us as technologists to try to figure out

32:09.320 --> 32:11.440
how to bridge these worlds.

32:11.440 --> 32:12.440
Yep.

32:12.440 --> 32:17.280
You know, at Aluvium, we, we, we really think about this as a first class part of what

32:17.280 --> 32:19.280
we're trying to do.

32:19.280 --> 32:24.680
No, we, we have, you know, set of company values and one that we, we really hold dear

32:24.680 --> 32:25.680
as an idea.

32:25.680 --> 32:26.680
We want to put people first.

32:26.680 --> 32:30.960
And I think, you know, that's, that's sort of easy to say and in the context of what

32:30.960 --> 32:38.120
our work, what that means is I have, I have no idea what a CNC whisperer knows and I'll

32:38.120 --> 32:39.120
never know.

32:39.120 --> 32:43.200
I will, I could, I could start today and work through a main in my career and never be

32:43.200 --> 32:44.200
as good as that.

32:44.200 --> 32:49.880
I mean, we've met people who are those, those, those folks, I mean, single, single individuals

32:49.880 --> 32:55.280
working in massive multinational companies that themselves have institutional knowledge

32:55.280 --> 32:59.040
that is probably invaluable to those organizations.

32:59.040 --> 33:03.160
And so to build software to support that, I think the, the admission that you have to

33:03.160 --> 33:07.320
make as a technologist is that you'll never know what they know.

33:07.320 --> 33:13.400
And so how can you build tools that shift the cognitive responsibility of that individual

33:13.400 --> 33:20.240
away from having to constantly be, you know, checking that CNC machine and pulling data

33:20.240 --> 33:26.200
off of it so that it knows exactly that series of vibrations or heat or spin that may indicate

33:26.200 --> 33:34.160
imminent failure to providing them with information in a timely manner that draws them to that,

33:34.160 --> 33:38.080
you know, opportunity to make a choice, to make a decision.

33:38.080 --> 33:40.120
And then they can get back to the work that they're really good at.

33:40.120 --> 33:44.000
And I think in some sense, it's just a matter of increasing cognitive margin.

33:44.000 --> 33:48.560
You know, there's, there's this, you know, this, this upward slope of data, right, constantly

33:48.560 --> 33:49.560
everywhere.

33:49.560 --> 33:52.560
And I think in the industrial space, which is, is largely not talked about outside of

33:52.560 --> 33:59.120
those kind of professional conferences is just as steep or, or more, you know, but the

33:59.120 --> 34:04.160
labor dynamics in those, in those industries are, you know, fixed or shrinking.

34:04.160 --> 34:05.160
Yeah.

34:05.160 --> 34:11.480
And so now you have this, this very problematic asymmetry between humans having to understand

34:11.480 --> 34:16.400
and, and deal with data and make decisions and systems that are just drowning them.

34:16.400 --> 34:17.400
Yeah.

34:17.400 --> 34:18.400
And information.

34:18.400 --> 34:23.200
And for us, he's the simplest way to bridge that gap as well, let's leverage that expertise

34:23.200 --> 34:29.480
that they have so that that, that, like, shifting of cognitive responsibility for the routine,

34:29.480 --> 34:37.080
boring, observational stuff can go to the computer and the, you know, imminent high value need

34:37.080 --> 34:41.640
to know now decisions can go to those experts and leverage that and really stitch that stuff

34:41.640 --> 34:42.640
together.

34:42.640 --> 34:43.640
Right.

34:43.640 --> 34:44.640
Right.

34:44.640 --> 34:45.640
Awesome.

34:45.640 --> 34:49.200
Sure, folks, to check out what you're doing, try to get down and engage with you.

34:49.200 --> 34:50.200
Yeah.

34:50.200 --> 34:56.720
So, you know, websites alluvium.io, that's A-L-L-U-V-I-U-M.io, alluvium.

34:56.720 --> 35:01.960
The easiest way to engage with me, you know, I'm, I'm on Twitter at Drew Conway, C-O-N-W-A-Y.

35:01.960 --> 35:04.400
I'm, I'm pretty easy, I'm a pretty easy Google.

35:04.400 --> 35:09.640
So, you know, I, I, I, I, I answer, I answer my emails, I like to, I like to engage with

35:09.640 --> 35:10.640
folks.

35:10.640 --> 35:15.360
So, if you have any questions about the company, we are, we are hiring as I think everybody

35:15.360 --> 35:16.360
is.

35:16.360 --> 35:20.520
So, you know, for us, the, the nucleus of, of our company is really around data science

35:20.520 --> 35:26.920
and engineering, but I think with a specific bent on streaming data and, you know, semi-supervised

35:26.920 --> 35:28.000
and unsupervised learning.

35:28.000 --> 35:31.880
So, if that's the kind of stuff with, you know, high volumes of streaming data that get

35:31.880 --> 35:33.720
you excited, we'd love to hear from you.

35:33.720 --> 35:34.720
Great.

35:34.720 --> 35:35.720
All right.

35:35.720 --> 35:36.720
Well, thanks so much, Drew.

35:36.720 --> 35:37.720
Thank you, Sam.

35:37.720 --> 35:38.720
It was a lot of fun.

35:38.720 --> 35:40.720
All right.

35:40.720 --> 35:45.840
All right, everyone, that's our show for today, thanks so much for listening and for your

35:45.840 --> 35:48.680
continued support of this podcast.

35:48.680 --> 35:53.400
For the notes for this episode, to ask any questions, or to let us know how you like

35:53.400 --> 36:00.880
the show, leave a comment on the show notes page at twomolei.com slash talk slash 39.

36:00.880 --> 36:05.280
Thanks again to our sponsor for the Rangle Conference Series Cloud Era.

36:05.280 --> 36:10.600
To learn more about Cloud Era and the company's data science workbench family of products,

36:10.600 --> 36:18.240
visit them at cloudera.com and be sure to tweet to them at at Cloud Era C-L-O-U-D-E-R-A

36:18.240 --> 36:21.400
to thank them for their support of this podcast.

36:21.400 --> 36:26.360
If you're interested in joining our meetup, you can register for that at twomolei.com slash

36:26.360 --> 36:33.040
meetup and don't forget to sign up for the newsletter at twomolei.com slash newsletter.

36:33.040 --> 36:43.040
Thanks again for listening and catch you next time.

