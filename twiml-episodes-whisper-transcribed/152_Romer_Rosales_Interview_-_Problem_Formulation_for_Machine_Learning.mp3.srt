1
00:00:00,000 --> 00:00:16,320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

2
00:00:16,320 --> 00:00:21,480
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:21,480 --> 00:00:34,080
I'm your host, Sam Charrington. Before we dive into today's show, just a few quick

4
00:00:34,080 --> 00:00:40,000
meet up related announcements. First, let me say thanks to everyone who participated

5
00:00:40,000 --> 00:00:45,200
in this weekend's fast AI deep learning course study group. We had a great discussion

6
00:00:45,200 --> 00:00:50,720
about lesson one. If you missed it, the recap will be posted later this week, so be sure

7
00:00:50,720 --> 00:00:56,880
to subscribe to our YouTube channel or follow at Twimble AI on Twitter for updates.

8
00:00:56,880 --> 00:01:03,120
Also, this is your final reminder that the next Twimble online meet up will be tomorrow

9
00:01:03,120 --> 00:01:10,800
Tuesday June 12th at 5pm US Pacific time. Kelvin Ross will be reviewing the paper, cardiologist

10
00:01:10,800 --> 00:01:15,880
level arrhythmia detection with convolutional neural networks, which is worked by researchers

11
00:01:15,880 --> 00:01:25,360
and Andrew Ings lab at Stanford. For more information, visit twimbleai.com slash meetup.

12
00:01:25,360 --> 00:01:31,120
In this episode, I'm joined by Romero Salas, director of AI at LinkedIn. We begin with

13
00:01:31,120 --> 00:01:36,000
a discussion of graphical models and approximate probability inference, and he helps me make

14
00:01:36,000 --> 00:01:41,680
an important connection in the way I think about that topic. We then review some of the applications

15
00:01:41,680 --> 00:01:47,240
of machine learning at LinkedIn and how what Homer calls their holistic approach guides

16
00:01:47,240 --> 00:01:51,640
the evolution of machine learning projects at the company. This leads us into a really

17
00:01:51,640 --> 00:01:56,560
interesting discussion about problem formulation and selecting the right objective function

18
00:01:56,560 --> 00:02:01,960
for a given problem. We then talk through some of the tools LinkedIn has built to scale

19
00:02:01,960 --> 00:02:07,400
their data science efforts, including large scale constrained optimization solvers, online

20
00:02:07,400 --> 00:02:12,680
hyperparameter optimization, and more. This is a really fun conversation that I'm sure

21
00:02:12,680 --> 00:02:17,680
you'll enjoy. Let's go.

22
00:02:17,680 --> 00:02:24,800
All right, everyone. I am on the line with Romero Salas, who is director of artificial intelligence

23
00:02:24,800 --> 00:02:30,040
at LinkedIn. Romero, welcome to this week in machine learning and AI.

24
00:02:30,040 --> 00:02:35,440
Thank you, Sam. How are you? Glad to be here. Really excited to talk a little bit more

25
00:02:35,440 --> 00:02:41,640
about AI with you. I'm really a listener of your show in the past, and I really enjoy

26
00:02:41,640 --> 00:02:46,720
them. So looking forward to this. Awesome. That's great to hear, and I am doing great.

27
00:02:46,720 --> 00:02:52,240
And I'm excited to dig into our conversation, which as a listener, you know that I'll

28
00:02:52,240 --> 00:02:57,080
start by asking you how you got into artificial intelligence. Tell us a little bit about your

29
00:02:57,080 --> 00:02:58,080
background.

30
00:02:58,080 --> 00:03:09,360
OK, yes, so I guess my background as an undergraduate student is on something that is called informatics

31
00:03:09,360 --> 00:03:14,040
or informatics engineering. And this is something a term that is not used here in the States

32
00:03:14,040 --> 00:03:21,200
because I study in Venezuela for my undergrad. And that's kind of like a combination of math,

33
00:03:21,200 --> 00:03:28,440
computer science and stats or operations research. And, you know, looking back 20 years ago,

34
00:03:28,440 --> 00:03:35,640
this is exactly what you need to get into machine learning, right? So I was really interested

35
00:03:35,640 --> 00:03:44,440
in these topics from very long ago. And once I go into graduate school, I try to explore

36
00:03:44,440 --> 00:03:51,400
those more and more, first studying computer vision, which was kind of like my first subject

37
00:03:51,400 --> 00:03:57,800
area in graduate school. And then realizing that, you know, you could do a lot more in

38
00:03:57,800 --> 00:04:03,640
computer vision with the right sort of machine intelligence, right? And that got me more

39
00:04:03,640 --> 00:04:09,960
and more interested in AI. For example, I was looking at things like how do you recover

40
00:04:09,960 --> 00:04:15,040
the 3D structure of an object? And of course, the first thing you do in computer vision is

41
00:04:15,040 --> 00:04:19,840
you're going to the 3D geometry. And, you know, all of these things related to, you know,

42
00:04:19,840 --> 00:04:25,720
epipolar geometry or recovering the camera parameters. But then I try to approach the same

43
00:04:25,720 --> 00:04:32,520
type of problems using machine learning. And it just felt much more natural to me. And

44
00:04:32,520 --> 00:04:38,480
then I can go in in that direction and exploring a little bit more of a, you know, in-depth

45
00:04:38,480 --> 00:04:46,920
machine learning, did a couple of postdocs in the area, one in what I call image analysis

46
00:04:46,920 --> 00:04:55,040
and another one more related to how we scale inference in systems that are so large or

47
00:04:55,040 --> 00:05:01,360
so complicated that you cannot really do exact inference, but you have to go into approximate

48
00:05:01,360 --> 00:05:08,160
inference. So the area is really called approximate inference in graphical models.

49
00:05:08,160 --> 00:05:13,760
So, you know, that area really reinforces more and more, you know, what I really wanted

50
00:05:13,760 --> 00:05:18,800
to do in machine learning, which is to deal with these very large problems, large systems.

51
00:05:18,800 --> 00:05:23,560
And even if you cannot come up with an exact solution to all of these problems, to come

52
00:05:23,560 --> 00:05:32,720
up with a good approximations. So after that, I went to industry and I started working

53
00:05:32,720 --> 00:05:41,240
in the area of healthcare. So, which is a great area. It's a very, very meaningful area,

54
00:05:41,240 --> 00:05:48,840
which really needs, you see a lot of applications of artificial intelligence in this area. I

55
00:05:48,840 --> 00:05:55,160
spent about five or six years in the area of healthcare. And then I moved to Silicon

56
00:05:55,160 --> 00:06:02,400
Valley and right now I am here at LinkedIn. I think I'm very happy and I think I've been

57
00:06:02,400 --> 00:06:11,560
lucky to work on two areas that I consider that are pretty useful in general for the socially

58
00:06:11,560 --> 00:06:17,880
one is healthcare and the other one is my work here at LinkedIn, which is, as you know,

59
00:06:17,880 --> 00:06:21,240
deals with the subject of creating economic opportunity for everyone.

60
00:06:21,240 --> 00:06:27,720
Awesome. Well, I am curious about the work that you mentioned on approximate inference.

61
00:06:27,720 --> 00:06:30,720
Is that model specific?

62
00:06:30,720 --> 00:06:39,040
Yes. So approximate inference in graphical models is this area of machine learning where

63
00:06:39,040 --> 00:06:46,200
you know that if you really want to compute a posterior distribution, what we call, for

64
00:06:46,200 --> 00:06:52,240
a given random variable, given all the information that you know about the system around this

65
00:06:52,240 --> 00:06:59,360
random variable, they say you have a system of 100 random variables and you know a lot

66
00:06:59,360 --> 00:07:03,760
about 99 variables, but you don't, you want to know what those 99 variables tell you

67
00:07:03,760 --> 00:07:10,920
about these one variable, right? How do you compute the distribution over that random

68
00:07:10,920 --> 00:07:17,560
variable, right? So for example, you know, everything about the state of a patient, right,

69
00:07:17,560 --> 00:07:22,560
the result of many tests, how, how you can calculate the probability that this patient

70
00:07:22,560 --> 00:07:29,040
has a particular disease, for example, right? So we know that in complicated systems, depending

71
00:07:29,040 --> 00:07:36,240
on how these variables interact with each other, you can easily go into the field of, you

72
00:07:36,240 --> 00:07:41,560
know, such large computations that you cannot really compute is even if you have a very large

73
00:07:41,560 --> 00:07:49,360
computational resources. And you have to go and explore areas of approximations, right,

74
00:07:49,360 --> 00:07:55,920
of this posterior distribution. So basically approximate probability inference is the field

75
00:07:55,920 --> 00:08:00,480
of trying to come up with the best possible approximations to these very large problems

76
00:08:00,480 --> 00:08:05,960
that you cannot solve because they are either too large or simply because you don't

77
00:08:05,960 --> 00:08:12,560
have the time to wait for, for an answer. So anyway, that's a, that's in general what,

78
00:08:12,560 --> 00:08:17,600
what this area is. And you will surprise how quickly you go into these intractable models.

79
00:08:17,600 --> 00:08:24,080
For example, you have a, you have random variables that can take 10 values and you have a maybe

80
00:08:24,080 --> 00:08:31,360
50 of those, right? The number of possible states that the system could be in is already

81
00:08:31,360 --> 00:08:36,320
quite large. It approximates, I guess, in the number of particles in the, in the universe.

82
00:08:36,320 --> 00:08:43,600
So, wow, this is why you need this type of approximate algorithms and, and they actually

83
00:08:43,600 --> 00:08:49,320
very useful for a lot of the things that we do. Is the common theme between the work you've

84
00:08:49,320 --> 00:08:54,440
done in healthcare and the work you're doing at LinkedIn, this notion of graphical models,

85
00:08:54,440 --> 00:08:59,320
do you, do you use graphical models much at LinkedIn? Thinking in terms of graphical

86
00:08:59,320 --> 00:09:06,680
models is, is, to me, has been a really great way to think in terms of probability, a general

87
00:09:06,680 --> 00:09:11,160
way to think in terms, in terms of probability. And if you're in the field of machine learning,

88
00:09:11,160 --> 00:09:18,320
I think thinking in terms of probability, it's extremely useful. And even though you can

89
00:09:18,320 --> 00:09:22,560
do a lot of things, maybe without thinking directly in terms of probability, this, I

90
00:09:22,560 --> 00:09:28,080
think for most of the things that I have done in the past, the notion of probability,

91
00:09:28,080 --> 00:09:35,040
probability distribution, approximations have played a very important role. So, I think

92
00:09:35,040 --> 00:09:41,600
of graphical models as an excellent way to think in general about probability distributions

93
00:09:41,600 --> 00:09:48,640
and, and useful in, you know, many areas. In the healthcare field, for example, every

94
00:09:48,640 --> 00:09:55,200
time that you need to think about, well, what is the probability that this particular

95
00:09:55,200 --> 00:10:01,520
patient has this disease, given that we know the result of this test and this test and

96
00:10:01,520 --> 00:10:09,120
this other test, you know, it is court where everything we do, right? In the same way, in,

97
00:10:09,120 --> 00:10:15,160
in a large internet company like LinkedIn, understanding what is the probability that

98
00:10:15,160 --> 00:10:20,280
this member is going to really find value in this information that we are providing to

99
00:10:20,280 --> 00:10:27,000
him via the application, right? It's core to a lot of the things we do, right? So, we

100
00:10:27,000 --> 00:10:34,160
want to, at the end of the day, maximize the value that this member gets from interacting

101
00:10:34,160 --> 00:10:41,280
with LinkedIn and encoding this in terms of probabilities, right? Probability that the

102
00:10:41,280 --> 00:10:47,120
member is going to engage with this post or the probability that the member is going to

103
00:10:47,120 --> 00:10:53,440
invite a colleague to join LinkedIn or the probability that this member is going to maybe

104
00:10:53,440 --> 00:10:58,840
disable this notification because this notification wasn't the appropriate one to send, is something

105
00:10:58,840 --> 00:11:06,280
that we consider every day at my work. So, yes, I mean, thinking in terms of probabilities

106
00:11:06,280 --> 00:11:12,760
is, is at the core of everything we do. Graphical models provide a language to speaking in terms

107
00:11:12,760 --> 00:11:19,200
of probability that is very general and this is why, you know, I, I find it really useful

108
00:11:19,200 --> 00:11:26,320
and I, I was lucky to explore this field back when I was in academia and, you know, turns

109
00:11:26,320 --> 00:11:32,440
out that I'm using it every day, basically. It's really interesting. I've done a number

110
00:11:32,440 --> 00:11:40,040
of interviews on topic of graphical models and I've tended to think of them in the context

111
00:11:40,040 --> 00:11:47,640
of either computational graphs or like graph databases, things like that and always think

112
00:11:47,640 --> 00:11:54,000
of them visually. And it wasn't really until you just, until I heard your description

113
00:11:54,000 --> 00:11:58,200
that it really clicked, clicked for me that the, the graph we're really talking about

114
00:11:58,200 --> 00:12:02,560
is an application of baserol and conditional probabilities and that's really what their

115
00:12:02,560 --> 00:12:05,600
relationships are between these nodes and the graph. Is that the right way to think

116
00:12:05,600 --> 00:12:11,160
about this? You, you, you, you, you hit right on target, basically, approximate inference

117
00:12:11,160 --> 00:12:16,360
in graphical models is a way to efficiently apply Bayes rule. That's a, that is what

118
00:12:16,360 --> 00:12:23,120
this reduces to really, right? So, yeah, that's a, for example, algorithms like belief

119
00:12:23,120 --> 00:12:32,240
propagation, right? In a graph, right? It's an exact, exact way to apply Bayes rule, right?

120
00:12:32,240 --> 00:12:38,840
In some cases, you cannot apply the Bayes rule exactly, right? Because the computation

121
00:12:38,840 --> 00:12:46,680
is too large and these, they believe propagation algorithm becomes intractable, right? The exact

122
00:12:46,680 --> 00:12:54,920
one and approximating what belief propagation should do is, is basically core part of these

123
00:12:54,920 --> 00:13:00,520
algorithms for doing approximate inference in, in graphical models. So, yeah, I think

124
00:13:00,520 --> 00:13:07,360
that, that's another way to put it, right? So inference equates in these probabilistic

125
00:13:07,360 --> 00:13:13,360
systems as applying Bayes rule and when you cannot apply Bayes rule, you have to go into

126
00:13:13,360 --> 00:13:18,800
approximations, right? So, so now there is, that is not to say there are a lot of, there

127
00:13:18,800 --> 00:13:23,760
is a large field of machine learning that maybe you don't have to think directly in terms

128
00:13:23,760 --> 00:13:32,760
of probability, right? And for example, you know, I'll argue that even though it is useful,

129
00:13:32,760 --> 00:13:39,080
in many cases, thinking of when you need to use a neural network, let's say to address

130
00:13:39,080 --> 00:13:44,440
a classification task, you may or may not think in terms of probability and will be fine

131
00:13:44,440 --> 00:13:51,000
with it, right? But introducing probability in, in the thinking, in the reasoning about

132
00:13:51,000 --> 00:13:56,360
how machine learning works is, is super useful. And it's a, it's a great tool, right? Even

133
00:13:56,360 --> 00:14:02,000
if you are using, you know, neural networks or, or any of the things that on the surface

134
00:14:02,000 --> 00:14:08,040
don't require you to think in terms of probability directly, right? So, for example, in, in, you

135
00:14:08,040 --> 00:14:15,280
have a neural network trained to predict, let me use the same example in healthcare. The,

136
00:14:15,280 --> 00:14:22,600
you want to predict whether the patient is healthy or not, you can use as input the result

137
00:14:22,600 --> 00:14:29,880
of all the tests that you apply to this patient. And the output is going to be an activation

138
00:14:29,880 --> 00:14:36,160
of this normally referred to as a, as a neuron in the, in the, in the neural network that

139
00:14:36,160 --> 00:14:41,000
tells you whether the patient is sick or is not, right? Based on what he has learned from

140
00:14:41,000 --> 00:14:49,760
the past. And so far, I haven't brought up the term probability. But if you start thinking

141
00:14:49,760 --> 00:14:57,160
about how the activation of the neurons, right, relate to this, how certain the system

142
00:14:57,160 --> 00:15:02,720
is that the patient is sick or not, then you can translate that to, to a more probabilistic

143
00:15:02,720 --> 00:15:11,080
concept and start doing, you know, additional analysis on, let's say, the, how reliable

144
00:15:11,080 --> 00:15:16,040
the neural network is or how, how much it captures the true relationship between the inputs

145
00:15:16,040 --> 00:15:22,400
and outputs, right? So, anyway, that is kind of like my summary of why, you know, thinking

146
00:15:22,400 --> 00:15:26,480
in terms of probabilities is, to me, it's always helpful, even if you are not dealing with

147
00:15:26,480 --> 00:15:30,720
them directly, like in, like in some versions of neural networks.

148
00:15:30,720 --> 00:15:35,280
Yeah, I'm really glad we went down this path. I don't, you know, it seems so, so obvious

149
00:15:35,280 --> 00:15:40,320
in hindsight, but that way of thinking about graphical models is, is going to be helpful

150
00:15:40,320 --> 00:15:45,720
for me, I think. You know, maybe let's jump over to, to LinkedIn and, and what you're

151
00:15:45,720 --> 00:15:51,400
up to at LinkedIn. Can you talk a little bit about why AI at LinkedIn? I mean, some of

152
00:15:51,400 --> 00:15:57,600
that is going to be obvious, but maybe give us a lay of the land of how LinkedIn thinks

153
00:15:57,600 --> 00:16:03,640
about artificial intelligence and machine learning. So, basically, in everything you see

154
00:16:03,640 --> 00:16:12,160
on the LinkedIn app has, to a large extent, some form of machine learning in the background,

155
00:16:12,160 --> 00:16:17,600
right? Let's, let's start from the things that you normally see when you open the LinkedIn

156
00:16:17,600 --> 00:16:22,680
app, let's say the feet, right? The first thing you see normally is, is the LinkedIn

157
00:16:22,680 --> 00:16:29,680
feet, which ranks the conversations or updates or information that, in general, we believe

158
00:16:29,680 --> 00:16:37,200
is the most relevant to you. So, this, the reason why we rank things in, in that way, is

159
00:16:37,200 --> 00:16:44,760
because of the, how you have interacted with the LinkedIn application in the past, right?

160
00:16:44,760 --> 00:16:51,320
So we try to understand what interests you, what is relevant, right? You have a limited

161
00:16:51,320 --> 00:16:57,560
amount of time to invest on, using the LinkedIn app, we want to make sure that that's the

162
00:16:57,560 --> 00:17:04,720
most relevant that we can show you. In the same way, connection recommendations, who,

163
00:17:04,720 --> 00:17:11,040
who would you like to connect to? We try to infer from past activities and your past

164
00:17:11,040 --> 00:17:16,800
connections, you know, who you would be interested to connect with. Follows recommendation, who,

165
00:17:16,800 --> 00:17:21,440
would you like to follow? What topics do you like to follow? How to make messaging interactions

166
00:17:21,440 --> 00:17:29,480
simpler, right? How to, what to notify you off, right? So we know that notifications,

167
00:17:29,480 --> 00:17:35,720
too many notifications could be negative because we don't want to overwhelm anybody by sending

168
00:17:35,720 --> 00:17:40,800
too many notifications all the time, but so we want to really, really choose what is the

169
00:17:40,800 --> 00:17:47,760
best possible use of your time if we were to send you a notification. So machine learning

170
00:17:47,760 --> 00:17:53,040
plays a role there in trying to decide what is, what, what, what notification for all the

171
00:17:53,040 --> 00:17:58,560
notifications that we could send you has the high chance of providing value to you. And

172
00:17:58,560 --> 00:18:06,320
even in other areas that are not directly consumer or member focus, we use artificial intelligence

173
00:18:06,320 --> 00:18:13,080
a lot, for example, preventing some forms of abuse to the site, right? That's another

174
00:18:13,080 --> 00:18:20,560
area that understanding the patterns in the past that we have seen of abuse, let's say,

175
00:18:20,560 --> 00:18:29,160
to the site. I think we can prevent future problems with it, right? So jobs recommendations,

176
00:18:29,160 --> 00:18:35,360
right? So how to connect you with the right job, with the right company requires of understanding,

177
00:18:35,360 --> 00:18:41,200
you know, your skills. How do they, how they relate to certain types of jobs? What is your

178
00:18:41,200 --> 00:18:47,280
job title? So what type of jobs probably makes the most sense for us to recommend to you? And

179
00:18:47,280 --> 00:18:54,920
things like that, in addition to that, maybe your job requires you to know or to learn certain

180
00:18:54,920 --> 00:19:03,520
subject areas. So we can also recommend to you learning courses that you can also find in LinkedIn

181
00:19:03,520 --> 00:19:13,040
that will maximize the chances of you getting that next job, for example. In the same way,

182
00:19:13,040 --> 00:19:19,440
you know, in our search results, when you search for a particular topic or member or product or

183
00:19:19,440 --> 00:19:26,640
company in the app, we use artificial intelligence to show you what are the most likely matches to

184
00:19:26,640 --> 00:19:36,320
what you are intended to identify in the app. So, you know, I could, I could talk about all of

185
00:19:36,320 --> 00:19:43,200
these subjects for hours, right? But it's basically everywhere, yeah. I'm curious a lot of what you've

186
00:19:43,200 --> 00:19:50,720
described sounds common across internet companies and social types of applications, feeds,

187
00:19:50,720 --> 00:19:57,200
recommendations, things like that. I'm curious where LinkedIn's requirements and use cases might

188
00:19:57,200 --> 00:20:04,320
be different from some other companies in ways that are interesting. There are going to be things

189
00:20:04,320 --> 00:20:10,400
that are probably in common with other companies and things that are special to LinkedIn.

190
00:20:10,400 --> 00:20:16,000
One of the main things that we truly care at LinkedIn is that notion of, you know, are we providing

191
00:20:16,000 --> 00:20:22,640
the most possible value, right? The highest possible value to the member by showing the member

192
00:20:22,640 --> 00:20:28,640
a particular recommendation, for example, right? So, we think about this whole

193
00:20:29,680 --> 00:20:35,120
interaction with the application in a holistic way. And I think that's pretty interesting. I

194
00:20:35,840 --> 00:20:43,920
am not too familiar with other industries or companies thinking this holistically, right? So,

195
00:20:43,920 --> 00:20:50,080
and what do I mean by that, right? So, for example, recently, or maybe a couple of years ago,

196
00:20:50,080 --> 00:20:58,000
not so recently, we receive a lot of feedback that maybe LinkedIn was sending too much email,

197
00:20:58,000 --> 00:21:03,680
right, to people, right? And some of this feedback was public and that we have acknowledged this. So,

198
00:21:05,440 --> 00:21:09,440
what, we decided to think about, you know, what we need to do something about this, right? And

199
00:21:09,440 --> 00:21:16,080
why is it that, you know, we're sending so much email? And if you think short term, right?

200
00:21:16,080 --> 00:21:20,480
Well, you know, the more messages you send, the more email you send, in this case, you know,

201
00:21:20,480 --> 00:21:26,240
the higher the chances of engagement, right? So, if you want to maximize engagement, you know,

202
00:21:26,240 --> 00:21:33,280
sure, at least in the short term, send more email, right? And, well, that is a very, you know,

203
00:21:33,280 --> 00:21:39,920
short-sighted strategy, right? So, instead, what if you start thinking about if you want to improve

204
00:21:39,920 --> 00:21:47,520
the overall ecosystem of LinkedIn, right? And at the same time, the overall value that the member

205
00:21:47,520 --> 00:21:54,000
gets from using LinkedIn, then you start thinking much differently, right? And one of the

206
00:21:55,200 --> 00:22:01,440
early projects that we did in this area was, well, you know, engagement, you know, as a proxy for

207
00:22:01,440 --> 00:22:06,960
value is interesting, but we should also take into account negative signals, like, for example,

208
00:22:06,960 --> 00:22:14,640
is a member disabling or marking the email as spam, for example, right? So, so let's speak one

209
00:22:14,640 --> 00:22:21,920
email type and let's try to change the problem from trying to maximize engagement to a constraint

210
00:22:21,920 --> 00:22:27,760
optimization problem, where we think a little bit more holistically and say, well, let's try to

211
00:22:27,760 --> 00:22:35,280
maximize some form of engagement, which is a proxy to value, subject to some constraints on, let's

212
00:22:35,280 --> 00:22:41,760
say, negative feedback, like marking the email as spam. And, you know, that turns out, you know,

213
00:22:41,760 --> 00:22:48,400
that's that relatively simple optimization problem, you know, only one email type to utilities,

214
00:22:48,400 --> 00:22:56,240
one is related to engagement, one is related to negative feedback, had a huge impact on that

215
00:22:56,240 --> 00:23:02,080
particular type of notification or email that we were sending, right? And how do you characterize

216
00:23:02,080 --> 00:23:11,200
that impact? So, the way that we can address that impact is how many, how much negative feedback

217
00:23:11,200 --> 00:23:16,400
are we reducing? And so how much how much of a reduction negative feedback we can we can measure,

218
00:23:16,400 --> 00:23:24,560
right? How many fewer emails we can send at what level of engagement? So, it turns out that

219
00:23:24,560 --> 00:23:32,480
we could reduce the number of emails notification that we send by a pretty large amount. I think

220
00:23:32,480 --> 00:23:38,640
for that initial test that we run, it was at least 40% of email reduction with a equivalent

221
00:23:39,120 --> 00:23:46,400
percentage of negative feedback. And the engagement, the proxy to engagement that we were using

222
00:23:46,400 --> 00:23:54,000
at the time, which were sessions, right? We're almost unchanged, maybe negative 0.5 reduction

223
00:23:54,000 --> 00:24:00,240
in sessions. So, that's that's a great trade off to make, right? So, we we provide a much better

224
00:24:01,360 --> 00:24:07,680
experience at a very similar level of engagement and we actually could measure that there is a

225
00:24:07,680 --> 00:24:12,720
reduction in the negative feedback that we were getting, right? So, thinking holistically,

226
00:24:13,120 --> 00:24:20,880
basically gave us so many more insight into how we can do this across LinkedIn. So, so we didn't

227
00:24:20,880 --> 00:24:26,400
stop here. We said, well, you know, this is this is great. So, it may sound crazy, but what if we do

228
00:24:26,400 --> 00:24:32,720
this across all notifications that we're sending and at the time we were sending the primary

229
00:24:32,720 --> 00:24:38,960
form of notifications that we were sending were emails. So, let's say that we now instead of just

230
00:24:38,960 --> 00:24:44,240
why stop it at one email type, let's let's across all the possible emails that LinkedIn could send

231
00:24:44,240 --> 00:24:52,400
you. What if we apply a similar way of thinking, right? And then you you're running to scalability

232
00:24:52,400 --> 00:25:03,200
problems and having to work with an entire set of, you know, product managers within the company.

233
00:25:03,760 --> 00:25:10,160
And how do you deal with that, right? So, and this experience really gave us a lot of insights

234
00:25:10,160 --> 00:25:16,960
about how we approach future problems, right? But what we did basically was less, less formulate

235
00:25:16,960 --> 00:25:22,000
the problem again as a constraint optimization problem where you are less minimize the amount

236
00:25:22,000 --> 00:25:28,320
of messages that we send. Let's say that we want to really, you know, if we don't have to send any

237
00:25:28,320 --> 00:25:35,200
messages and provide the same amount of value to the members, then why not do that, right? So,

238
00:25:35,200 --> 00:25:38,640
the way that we formulated the problem was, well, less minimize the number of messages that we

239
00:25:38,640 --> 00:25:46,800
send in this case emails, subject to a maximum amount of negative feedback that we're willing to

240
00:25:46,800 --> 00:25:53,360
tolerate. We want, you know, less than 2% of negative feedback or 0.2% of negative feedback.

241
00:25:54,080 --> 00:26:00,880
And let's also make sure that there is certain level of engagement. So, we don't want to reduce the

242
00:26:00,880 --> 00:26:06,400
engagement too much because that's our proxy for for member value, right? So, let's let's play

243
00:26:06,400 --> 00:26:12,960
second strain on on the member value that we believe we are providing, right? And since we have

244
00:26:13,600 --> 00:26:18,880
maybe 20 different products or 30 different products across LinkedIn that are all sending some

245
00:26:18,880 --> 00:26:24,960
form of email or notification, let's make sure that we don't decrease the engagement that we are

246
00:26:24,960 --> 00:26:31,840
directing to those products more than a percentage, right? And then you see how we can we keep

247
00:26:31,840 --> 00:26:37,280
increasing the complexity of the problems and the number of constraints, right? So, this turnout

248
00:26:37,280 --> 00:26:46,720
to be a project that we, you know, decided to do and which enormously reduce the amount of emails

249
00:26:46,720 --> 00:26:54,240
that we sent to our members, all the product partners were positive about the impact that this

250
00:26:54,240 --> 00:27:00,720
was having across the product and basically address this very critical problem that we really,

251
00:27:00,720 --> 00:27:06,240
really wanted to solve, which is provide better member value while at the same time reducing the

252
00:27:06,240 --> 00:27:11,520
amount of notifications that we were sending, right? So, I believe that at the time the reduction

253
00:27:11,520 --> 00:27:19,280
of total messages sent to members was close to 65% or close to 50% overall without decreasing

254
00:27:19,280 --> 00:27:25,840
in negative feedback of 65% and less than 1% reduction in sessions overall, right? But,

255
00:27:25,840 --> 00:27:31,760
you know, we believe this is the right thing to do and we did it, right? This, even though

256
00:27:31,760 --> 00:27:36,720
this came at some cost, which is, you know, 1% reduction in sessions that in a large company that

257
00:27:36,720 --> 00:27:43,120
may be a large cost, but it was a right thing to do and it opened the door to a lot of other

258
00:27:43,120 --> 00:27:48,160
things that a lot of our future thinking about how to approach these problems. This is a really,

259
00:27:48,720 --> 00:27:55,760
really interesting story, a great story and really timely for me, I was just giving a talk

260
00:27:55,760 --> 00:28:03,040
earlier this week and talked about how the metric that you choose to optimize around, you know,

261
00:28:03,040 --> 00:28:08,400
has a huge impact in the way you can approach a machine learning problem, even if it's fundamentally

262
00:28:08,400 --> 00:28:12,800
the same kind of problem, you know, a recommendation problem, you could look at it from the perspective of

263
00:28:13,440 --> 00:28:21,200
revenue or profit or a lifetime customer value and there are all kinds of choices to make there

264
00:28:21,200 --> 00:28:26,160
that have different implications on the ultimate performance of your models, but also the user

265
00:28:26,160 --> 00:28:32,160
experience, the customer experience. So it's really interesting to hear how you've applied this.

266
00:28:32,160 --> 00:28:38,880
Are there other areas outside of this email domain that you've applied this kind of holistic thinking

267
00:28:38,880 --> 00:28:46,320
to? Yeah, so we're now using these to all the types of notifications as well, but I think

268
00:28:46,320 --> 00:28:54,320
one interesting example is also in the area of forming connections, right? So in LinkedIn,

269
00:28:54,320 --> 00:29:02,320
we have this product called BYNK or people you may know and this product is basically powered by

270
00:29:04,240 --> 00:29:09,680
machine learning algorithm that determines for a particular person. What are the connections that

271
00:29:09,680 --> 00:29:16,640
in the past, you know, we also think about what are the connections that are most likely to respond,

272
00:29:16,640 --> 00:29:25,600
yes, I want to connect with you, right? And when you then rethink the problem and think, well,

273
00:29:25,600 --> 00:29:32,000
you know, maybe we should think about the problem in terms of what are the most valuable connections,

274
00:29:32,000 --> 00:29:38,320
right? Or what are the connections that will in the future, you think of value as how much

275
00:29:38,320 --> 00:29:44,320
interaction you will have with these connections, then, you know, you start thinking about different

276
00:29:44,320 --> 00:29:51,760
objective functions and different ways to provide what we think is a better value to the members,

277
00:29:51,760 --> 00:29:58,400
right? So what we started to do was to instead of creating connections after connections,

278
00:29:58,960 --> 00:30:03,520
trying to maximize the number of connections that a person creates, try to maximize the

279
00:30:03,520 --> 00:30:07,760
valuable connections or the connections that are, that the connections that matter, right, or that

280
00:30:07,760 --> 00:30:18,560
we think that matter, right? So, so we reformulate the problem into less maximize the chances that

281
00:30:18,560 --> 00:30:23,200
you're going to really engage with this person, you connect with the person, subject to some constraints

282
00:30:23,200 --> 00:30:29,440
on, you know, I want you to be connected, right? I don't want the members to be all severely

283
00:30:29,440 --> 00:30:34,560
undreconnected because we are too picky about the type of connection we're recommending to

284
00:30:34,560 --> 00:30:42,720
this member, right? So, this had an interesting impact on, well, you know, the new connections

285
00:30:42,720 --> 00:30:46,480
that you're making are actually, you're actually interacting more with them because of the new,

286
00:30:46,480 --> 00:30:54,960
this new recommendation algorithm and it's probably of higher value to you, right? So,

287
00:30:54,960 --> 00:31:02,080
as another example where we were thinking about maybe the more of the ecosystem value, you know,

288
00:31:02,080 --> 00:31:10,240
or long-term value to the member, right? And another example is in the area of, so for example,

289
00:31:10,240 --> 00:31:16,800
on the feet, right, which is a core part of LinkedIn, you know, we want to make sure that everybody

290
00:31:16,800 --> 00:31:22,000
in the feet feels hurt, right? So, you know, when I post something in the feet, I want to make sure

291
00:31:22,000 --> 00:31:30,480
that people, you know, can see what I posted and I get feedback, right? You want to maximize

292
00:31:30,480 --> 00:31:37,840
click to rate, which is a usual metric that you will first think about. You will normally

293
00:31:37,840 --> 00:31:44,000
promote members that are very, very popular and get a lot of clicks, right? So, but you want to

294
00:31:44,000 --> 00:31:51,600
create a really a better sense of community and people, you want people to feel that they're

295
00:31:51,600 --> 00:32:00,080
hurt, right? You want to also take into account that maybe some members that are less active, right,

296
00:32:00,720 --> 00:32:06,560
will deserve the chance to get feedback as well, right? So, we want to maximize overall

297
00:32:07,440 --> 00:32:16,320
engagement, not just the gain of a few members, right, while other members feel that they are not

298
00:32:16,320 --> 00:32:24,480
hurt, right? So, that's another area. And so, what we did is we basically focused on a group of

299
00:32:24,480 --> 00:32:36,720
members that are new sharers or not very common sharers or contributors and decided to make a

300
00:32:36,720 --> 00:32:43,680
trade-off, basically. We want to have this, we want to have most members receiving feedback

301
00:32:43,680 --> 00:32:50,800
as compared to just a few members receiving a lot of feedback. And again, this had a very

302
00:32:50,800 --> 00:32:58,640
positive effect on the members who contributed and they were not necessarily the most popular

303
00:32:58,640 --> 00:33:03,760
members or heavy contributors, but because of the feedback they got, there was a considerable

304
00:33:03,760 --> 00:33:09,280
increase in how much these members contribute again and share another article or share their views

305
00:33:09,280 --> 00:33:17,040
again on the site. And again, this is something that maybe it is obvious once you think about it,

306
00:33:17,040 --> 00:33:23,920
but it is not how usually most companies start when they want to optimize something like the

307
00:33:23,920 --> 00:33:29,920
feed, for example, or connection recommendations. Usually, the tendency is to optimize for the

308
00:33:29,920 --> 00:33:35,920
metric that you can see the most now and hope that that is the metric that in the long term is

309
00:33:35,920 --> 00:33:42,160
going to be the best metric to optimize. And we have realized that that's not the case in many

310
00:33:42,160 --> 00:33:46,000
instances and instead you need to think a little bit more holistically and come up with a better

311
00:33:46,000 --> 00:33:52,560
proxy of what is the metric that is better for the overall good of the member and the

312
00:33:53,600 --> 00:33:59,200
engagement overall in the site. Now, this is not to say that we have solved the problem of what

313
00:33:59,200 --> 00:34:04,720
is the best long-term metric that you should optimize for. I think that's a really difficult

314
00:34:04,720 --> 00:34:13,520
problem and in many cases you get counterintuitive results into I want to optimize

315
00:34:14,480 --> 00:34:21,120
how much daily active users they are in the site in one year. What should I optimize more

316
00:34:21,920 --> 00:34:27,280
in the short term in order to get there? And I think that's a tougher question. That's a tougher

317
00:34:27,280 --> 00:34:36,800
question. Basically, this reflects our idea that you should try to optimize whatever is closer

318
00:34:36,800 --> 00:34:43,920
to the long-term metric you want to see change. As long as you have a way to more or less

319
00:34:44,800 --> 00:34:52,160
approximate that metric with something that you can measure in the short term. If we wanted

320
00:34:52,160 --> 00:35:01,520
to apply this idea, I would like to optimize the feed or LinkedIn overall for a metric that says

321
00:35:01,520 --> 00:35:08,640
how many people in the global workforce are employed and believe that have economic opportunity,

322
00:35:08,640 --> 00:35:14,400
right? But what is the, is that my objective function? What did you get pretty broad?

323
00:35:14,400 --> 00:35:18,640
Yeah, what is the derivative of that with respect to the variables that I can control? That is

324
00:35:18,640 --> 00:35:26,720
a, that is a problem. So that is trying to fill in the intermediate proxy objective functions

325
00:35:26,720 --> 00:35:32,800
that you think will get you to that overall ideal, right, is where there is so much

326
00:35:32,800 --> 00:35:43,200
interesting work and ideas in machine learning that this is such a broad subject. But this is

327
00:35:43,200 --> 00:35:47,680
more or less how we want to think about it. What is the true metric that you really want to go

328
00:35:47,680 --> 00:35:58,000
towards? So now that you have this experience, do you find that teams are starting with more

329
00:35:58,000 --> 00:36:06,560
holistic models from the beginning of a modeling process like for new features or do you find that

330
00:36:07,440 --> 00:36:14,160
it needs to be more of a crawl walk run and, you know, they should still start simple and

331
00:36:14,160 --> 00:36:22,720
evolve and mature over time? Yeah, so I think that definitely I always go for the, you know,

332
00:36:22,720 --> 00:36:30,480
try the simplest thing first, right? And try to understand from the simple approaches first,

333
00:36:30,480 --> 00:36:38,960
right? And, you know, for a new company that needs to grow very fast, right, in order to get

334
00:36:38,960 --> 00:36:46,080
that engagement going, especially say you're starting a new application for which the,

335
00:36:47,200 --> 00:36:51,920
the social aspect is very important, you know, perhaps trying to maximize the number of connections

336
00:36:51,920 --> 00:36:58,000
as much as possible is the best, right? Well, the cake and not the icing or the cherry.

337
00:36:58,000 --> 00:37:06,400
Exactly, right? So now, once you start learning and realizing that, you know, maybe, you know,

338
00:37:06,400 --> 00:37:14,560
our members have a limited amount of time and energy and attention, what is the next best thing

339
00:37:14,560 --> 00:37:22,000
that we could do to provide that value to them? Right? Then you probably start thinking into

340
00:37:22,000 --> 00:37:28,480
more sophisticated way to optimize for that. Also, you know, at the same time, when you are early,

341
00:37:28,480 --> 00:37:32,240
you know, when a company is an early in the early stages, you probably don't have a very

342
00:37:32,240 --> 00:37:37,920
large investment in machine learning, so you need to try simple things first, right? Because

343
00:37:38,560 --> 00:37:45,280
the ROI is this largest, right? At that stage, right? But once you have understood a lot about

344
00:37:46,320 --> 00:37:56,480
the members or people who use your application, it's really, it turns into an ethical question,

345
00:37:56,480 --> 00:38:02,800
you know, what is the best thing that we could do for our members, given that we have this level

346
00:38:02,800 --> 00:38:08,400
of maturity, you know, the standing, how to bring value, right? And this is what we are trying to do,

347
00:38:08,400 --> 00:38:14,560
right? We try to do the best we can, given, given all that we have learned throughout the many years

348
00:38:15,520 --> 00:38:24,880
at LinkedIn. There are certainly implicit, if not explicit, tones to, you know, fairness implications

349
00:38:24,880 --> 00:38:29,280
of some of the things that you describe, like, you know, particularly around the feed and what

350
00:38:29,280 --> 00:38:33,600
goes into the feed and that kind of thing. Yeah, exactly. I think that's something that

351
00:38:34,640 --> 00:38:41,680
we always keep in mind. I think that we always try to think about all of these considerations

352
00:38:41,680 --> 00:38:48,480
in terms of fairness, you know, reducing bias, and I'm members' privacy, of course, right? That

353
00:38:48,480 --> 00:38:57,040
go into all the things we do, but as long as all of those aspects are satisfied,

354
00:38:58,320 --> 00:39:03,680
they're thinking about how to think holistically to provide the best possible value, right?

355
00:39:04,320 --> 00:39:10,080
To the members is something that is now reflected in many of the things we do. So

356
00:39:12,000 --> 00:39:17,920
and I guess he helps that my team is responsible for a lot of the products that I mentioned earlier,

357
00:39:17,920 --> 00:39:24,960
so we really try to make sure that this this way of thinking permeates across the organization

358
00:39:24,960 --> 00:39:32,960
and it starts at that level instead of thinking too narrowly or too greedily in terms of maximizing

359
00:39:32,960 --> 00:39:38,960
one particular metric at the expense of all the things that we may be missing, right? That can

360
00:39:38,960 --> 00:39:44,560
be better for the overall ecosystem, right? So anyway, so that's just wanted to summarize

361
00:39:44,560 --> 00:39:49,920
maybe how how it is that we're thinking about all of these problems at a high level and maybe

362
00:39:49,920 --> 00:39:55,680
provide a few examples like the email example that where we could touch a little bit at the low level.

363
00:39:56,720 --> 00:40:05,520
Yeah, absolutely. I'm curious as your team takes on more of these constrained optimization

364
00:40:05,520 --> 00:40:11,840
types of problems, has it changed or to what degree has it changed? The tools that you use,

365
00:40:11,840 --> 00:40:19,200
the data pipelines, the modeling process, you know, the general approach to rolling these out.

366
00:40:19,600 --> 00:40:27,920
Okay. Yeah, that's a good question because we had to build tools for doing this more efficiently,

367
00:40:27,920 --> 00:40:34,720
right? And some of the tools, for example, that we had to build are this large scale

368
00:40:34,720 --> 00:40:42,720
constrained optimization solvers that can use, can take advantage of some level of distributed

369
00:40:42,720 --> 00:40:48,640
processing, right? And provide a result to the problem very quickly, right? Because a lot of

370
00:40:48,640 --> 00:40:53,840
these problems really, what something that is common about all of these problems is that you're

371
00:40:53,840 --> 00:41:02,160
normally computing a trade off across many different objectives, for example, you know,

372
00:41:02,160 --> 00:41:12,960
a number of negative feedback that you get, a number of sessions that we see, perhaps sessions

373
00:41:12,960 --> 00:41:18,080
to a particular product and so on, right? A number of connections that you make. So what is

374
00:41:18,080 --> 00:41:23,040
common across all of these kind of applications, or examples that I've given, is that you always

375
00:41:23,040 --> 00:41:28,000
end up with how do you set the right trade off across all of these possible objectives?

376
00:41:28,000 --> 00:41:34,000
Right? So in optimization, I think that's what's normally referred as the Pareto curve, right?

377
00:41:34,000 --> 00:41:40,960
What is the right point in the frontier where you want to operate, right? So in two dimensions,

378
00:41:40,960 --> 00:41:48,560
this is just a curve and two variables and, you know, usually an increase in the return that

379
00:41:48,560 --> 00:41:54,960
you're getting one variable implies that you will get a negative impact on the other variable,

380
00:41:54,960 --> 00:42:01,760
right? So, you know, and that's a, you know, that's a very large set of combinations of variables

381
00:42:01,760 --> 00:42:07,120
that you could get, right? In just in two dimensions, what if you have three different

382
00:42:07,600 --> 00:42:11,840
objectives, or we call it utilities, three different utilities that you need to balance,

383
00:42:11,840 --> 00:42:17,040
or four utilities? How do you quickly solve these problems? Now, one thing you could do is that

384
00:42:17,040 --> 00:42:23,520
you could solve these problems offline, right? And wait for your cube quadratic programming,

385
00:42:23,520 --> 00:42:29,600
or linear programming, a solver to give you the solution, and then put the solution into the

386
00:42:29,600 --> 00:42:34,720
system and, you know, a measure just to make sure that things are going the way you expect.

387
00:42:34,720 --> 00:42:41,520
So that's one way to deal with the problem, and we continue to build ways to scale these

388
00:42:41,520 --> 00:42:48,880
constraint optimization problems in our systems. Another way that you could think of of these problems

389
00:42:48,880 --> 00:42:56,640
is let's say I try to do this online, right? And this is an interesting problem that we're working

390
00:42:56,640 --> 00:43:03,840
on these days. Let's say we have three utilities, right? And I want to learn what is the best

391
00:43:03,840 --> 00:43:09,440
combination of those utilities that satisfy my constraints as I serve the traffic so that I

392
00:43:09,440 --> 00:43:19,840
can adjust these things as the traffic is being served, right? So there are working online,

393
00:43:19,840 --> 00:43:28,160
we call it online model selection methods that allow you to, well, you know, a member comes,

394
00:43:28,160 --> 00:43:33,120
we serve this with a particular combination of parameters, and we determine, you know,

395
00:43:33,120 --> 00:43:40,080
how well the response to that combination of parameters was, and then the system automatically

396
00:43:40,080 --> 00:43:43,840
says, you know, well, you know, based on all the information that I have collected in the past about

397
00:43:43,840 --> 00:43:49,040
how how members are interacting given this parameter setting, what is the next best set of

398
00:43:49,040 --> 00:43:56,240
parameter settings that I could try to improve my utilities and still satisfy the constraints,

399
00:43:56,240 --> 00:44:03,520
right? So there are actually ways that we are trying to adjust solve this optimization problem

400
00:44:03,520 --> 00:44:10,320
where you have a few, we call it hyperparameters, right? That determine how much importance

401
00:44:10,320 --> 00:44:17,440
each of the utilities has, right? And, you know, the process of trying to adjust this in real time

402
00:44:17,440 --> 00:44:24,320
as we see more and more data is one of the areas that I think is very exciting and we have seen

403
00:44:24,320 --> 00:44:30,240
quite a few positive results in this direction. Of course, this method may not be able to scale

404
00:44:30,240 --> 00:44:35,520
for a very large dimensions where you need, you probably will need to solve a full offline

405
00:44:36,320 --> 00:44:41,760
constraint optimization problem, but in many practical situations, being able to do this

406
00:44:41,760 --> 00:44:48,320
parameter tuning in the online form seems possible. And I think this is one of the areas where

407
00:44:48,320 --> 00:44:56,080
I think we are we're very excited to see positive results these days. So in this context of an

408
00:44:56,080 --> 00:45:02,640
online system, you mentioned the dynamic model selection, which makes me think of, you know,

409
00:45:02,640 --> 00:45:10,240
not so much updating hyperparameters or parameters in real time, but you have offline developed as

410
00:45:10,240 --> 00:45:16,320
opposed to one single model, multiple models, and you're trying to fit a given user to a model

411
00:45:16,320 --> 00:45:22,560
online and then use that model. But it also, it sounds like you're doing a bit of both. I guess

412
00:45:22,560 --> 00:45:27,200
I'm trying to get some confirmation of my hearing right that there are two different types of

413
00:45:27,840 --> 00:45:34,400
things that work as you're trying to make the system more dynamic. Yeah, I think the two examples

414
00:45:34,400 --> 00:45:41,520
you provided are very related problems. I was and I think one could approach them in a very

415
00:45:41,520 --> 00:45:48,000
similar way. I was referring mostly about the problem of fitting hyperparameters, right?

416
00:45:48,000 --> 00:45:53,280
Okay. Which is basically when you have a linear combination of utilities, you have that

417
00:45:54,160 --> 00:45:59,440
hyperparameter that that is specific is how much of each utility, you know, what is the weight

418
00:45:59,440 --> 00:46:05,680
for each utility, right? And how you should change the that weight to satisfy your constraints and

419
00:46:05,680 --> 00:46:11,520
to at the same time maximize an objective, right? So I just seen those parameters in real time is

420
00:46:11,520 --> 00:46:17,360
what mostly I was referring to. I got it. However, however, that is you are right on target when you

421
00:46:17,360 --> 00:46:25,120
when you relate this with the with the other problem of trying to decide in real time what is

422
00:46:25,120 --> 00:46:31,680
what is the best model variant, right? That you could use instead of having to run different

423
00:46:31,680 --> 00:46:38,160
AB tests separately. You may want to run just one AB test and let the AB test itself out to

424
00:46:38,160 --> 00:46:43,760
tune it, right? Self-tune it. So that you can decide what is the what is the best model variant.

425
00:46:43,760 --> 00:46:49,280
And yeah, those those two problems are very related and we are exploring these directions as well.

426
00:46:50,160 --> 00:46:54,960
Just in terms of the time check, we've had a really interesting conversation so far. There are

427
00:46:54,960 --> 00:47:02,480
other things that you would add in and around this topic of trying to approximate more holistic

428
00:47:02,480 --> 00:47:10,960
metrics in the way we model. No, I think I think that reflects at a high level. I think what

429
00:47:12,000 --> 00:47:16,560
how we're thinking about it. There are there are some papers that I can also share with you offline

430
00:47:16,560 --> 00:47:22,160
into that they're going to more details about these. We have a pretty active group of

431
00:47:22,160 --> 00:47:29,840
scientists, engineers, analysts that publish papers and attend conferences and pre-active in

432
00:47:29,840 --> 00:47:36,400
the research community. And you know, we have the data website at LinkedIn. You can search for

433
00:47:37,280 --> 00:47:43,920
a few blog posts and articles that we normally publish there to go into a lot more detail into

434
00:47:43,920 --> 00:47:50,960
what I just said. You know, we're pre-active overall in the research community. And you know,

435
00:47:50,960 --> 00:47:54,960
something that I found I guess that I think it's also probably interesting to mention here is that

436
00:47:56,000 --> 00:48:02,080
you know, I spent a lot of times many years in academia and I actually did several postdocs and

437
00:48:02,080 --> 00:48:08,400
you know, was very interested in co-research in this area. But you know, working in industry,

438
00:48:08,400 --> 00:48:17,200
you you you never run out of problems. That that that is no answer for and or that the answers

439
00:48:17,200 --> 00:48:24,960
are just not that great when you try them, right? That this creates a constant influx of problems

440
00:48:24,960 --> 00:48:33,680
that you wish you have more time to solve that that that are pretty advanced or research driven

441
00:48:33,680 --> 00:48:40,400
problems that you know, you can publish and make a lot of impact on. So I found the working in

442
00:48:40,400 --> 00:48:44,960
industry as one of the main sources of interesting problems that I can think of

443
00:48:44,960 --> 00:48:51,440
you know, for publishing or for you know, just personal satisfaction of of being able to to

444
00:48:51,440 --> 00:48:57,360
approach the problem for for a greater good, right? So I think that that's that's one message that

445
00:48:58,160 --> 00:49:03,360
I also wanted to share because it took me some time to realize this but once I realized it as well

446
00:49:03,360 --> 00:49:08,800
this is this is so obvious, right? So this is this is such a great place to think of, you know, to

447
00:49:08,800 --> 00:49:14,000
realize what are the really important problems that matter and if you solve them now, it will have

448
00:49:14,000 --> 00:49:20,000
a big impact not only on on on just core research but also on the impact that you can have to

449
00:49:20,000 --> 00:49:26,080
to society in general, right? Right. So I think I find it a great place to to do that and

450
00:49:27,600 --> 00:49:34,960
LinkedIn has a different initiative is also with academia. For example, we run this what we call

451
00:49:34,960 --> 00:49:42,320
the economic graph research program, right? Which is another way for us to share, hey, you know, he

452
00:49:42,320 --> 00:49:48,800
here is here is a data that has been of course properly, you know, to identify and this information

453
00:49:48,800 --> 00:49:55,600
has been properly processed to be shared with a certain group of people, right? And we work together

454
00:49:55,600 --> 00:50:02,480
with professors or people from academia and other parts of outside of LinkedIn to be able to

455
00:50:03,120 --> 00:50:07,040
share, hey, you know, this is the data, this is a problem we have. What are all the problems that you

456
00:50:07,040 --> 00:50:14,240
see that for which you can use this data and actually, you know, to research make a positive impact,

457
00:50:14,240 --> 00:50:20,320
right? So I just wanted to mention that as well as one of the things that I'm pretty excited about.

458
00:50:21,280 --> 00:50:27,520
What LinkedIn does overall and if anybody is interested, I think this is something that you can

459
00:50:27,520 --> 00:50:34,480
check out also in our web pages. Okay, great. Well, we will include links to those sites in our

460
00:50:34,480 --> 00:50:41,280
show notes as well as any papers that you want to send over. But Romeo, thank you so much for

461
00:50:41,280 --> 00:50:46,720
taking this time. It was really a pleasure to chat with you. Thank you very much. It's my pleasure.

462
00:50:46,720 --> 00:50:47,440
Talk to you later, Sam.

463
00:50:52,320 --> 00:50:58,720
All right, everyone, that's our show for today. For more information on the Romeo or any of the topics

464
00:50:58,720 --> 00:51:07,280
covered in this episode, head on over to twimmolai.com slash talk slash 149. For the details of our

465
00:51:07,280 --> 00:51:13,840
upcoming meetup for the fast AI study group we formed, visit twimmolai.com slash meetup.

466
00:51:13,840 --> 00:51:28,880
As always, thanks so much for listening and catch you next time.

