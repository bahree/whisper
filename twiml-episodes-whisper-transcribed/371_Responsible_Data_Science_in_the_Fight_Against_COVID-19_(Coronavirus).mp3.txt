All right, I think we are live. Hey, everyone, welcome to the broadcast. I am super excited
for our conversation today. Since the very beginning of the coronavirus pandemic, we've
seen an outpouring of interest on the part of data scientists and AI practitioners wanting
to make a contribution. At the same time, some of the resulting efforts have been criticized
for promoting the spread of misinformation or just being disconnected from the applicable
domain knowledge. The question that we're here to address today is how can data scientists
get involved and do so in a responsible manner. And I've got a great panel lined up to help
us do that. Before we dive in, a couple of housekeeping notes. We really, really want
to ensure that this is an interactive discussion. So your comments, whether you're viewing on
YouTube, Facebook, or Twitter, are visible to me, and I'll relay them to our panel. And
it's really my sincere hope that you will help drive a good part of the discussion today.
Next, this is the first of many discussions that we'll be bringing you on a wide range
of topics. To be notified when we schedule future discussions, please just follow or subscribe
to us on whatever channel you are watching right now. Before introducing our panelists,
I'd like to send a shout out to our friends at IBM for their huge support of what we're
doing here at Twilmo generally and this discussion in particular. IBM has been at the forefront
of the fight against coronavirus. Through their leadership and the COVID-19 high performance
computing consortium, they've helped bring together the federal government and industry
and academic leaders in support of COVID-19 research. In addition, they're bringing together
trusted data from governments, the World Health Organization, and the CDC to provide detailed
virus tracking on the weather channel. And they're offering free access to Watson Assistant
to respond to common COVID-19 related questions. For more information on these resources and links
to dig in deeper, as well as to learn how you can get 30 days of free Coursera access when you
join the IBM data science community, visit the resource page for this program, twilmolyi.com slash
RDS COVID. With that said, I am really excited to introduce myself and our panelists for this
discussion. I'm Sam Charington, founder of Twilmo and host of the Twilmolyi podcast. Rex Douglas
is a computational social scientist and director of the machine learning for social science lab,
MSSL, at the Center for Peace and Security Studies at the University of San Diego.
Rex's research focuses on applying advanced technologies to research problems in
social sciences and policy world, particularly on issues of human conflict. Rob Monroe is a
former guest of the podcast and a long time friend of the show. He's currently CEO of the
machine learning, he's currently CEO of machine learning consulting. Hi, Rob.
Hey, everyone. Rob has extensive experience in crowdsourced data,
disaster response and the intersection of these areas. Leah Shanley is a senior fellow with the
Nelson Institute at the University of Wisconsin Madison, where she focuses on open science
information, innovation strategy and policy, responsible AI and data ethics and expanding diversity
in data science. Welcome, Leah. And GG UN Read is a distinguished engineer and executive
responsible for data science innovations in the healthcare space at IBM, with an emphasis on
the use of predictive models and clinical applications. All righty. So let's jump right in and get
started with a round of questions so that we can get to know our panelists a little bit better.
We'll start with you, Rex. You recently wrote an interesting piece on how to be curious
instead of contrarian about COVID-19, eight data science lessons from a coronavirus perspective.
We're not necessarily going into the specific lesson that you presented that article. We'll come
back to those. Tell us a little bit about the origin of that article and what prompted you to
write it and how that relates to your work at MSSL. Sure. So the big idea is that my note is
ostensibly a takedown of a specific piece that Richard Epstein wrote posted to Hoover,
where he predicted only 500 deaths in the US and made a long list of other false and
sophist arguments. In reality, though, my note is a Trojan horse intended to try to sneak some
basic social science research design methods to a broader audience who may not otherwise
receive that kind of information. I wanted to show by example that these kind of gatekeeping
or lanekeeping arguments are themselves another kind of anti-intellectualism. I argue that
during a crisis, it's exactly when we should be encouraging people to become more curious
and more scientifically rigorous. Right now, a good chunk of the planet actually wants to know
about causal inference and about confidence intervals and about Bayes rule. For God's sakes,
we should be telling them, especially before theaters reopen. Awesome. Rob, a lot of your
perspective on this issue comes from your experience working in disaster response. Share with us
a little bit about your background in this area and the concerns that it raises for you.
Yeah, absolutely. So I've worked as a disaster responder for about 20 years for about the same time
that I've been working as a data scientist. And in every major disaster I've responded to,
I've seen people die as a result of people cutting corners, including actions taken by well-meaning
data scientists, often grabbing media attention when experts should have had that attention.
But like I've also said, I think some of the biggest impact I've had is not when I've been working
in refugee camps or directly disaster response. Some of the biggest impact I've had
has been when I've been at large companies. So I ran AWS's and on the service for a while,
it was on comprehend. And it's when I've been working with large companies like that and
getting them to expand into more languages and think about general ways that tools can be adapted
to different parts of the world. I think that's built a foundational layer that I'm seeing right
now a lot of people responded to COVID using. So I'd love to share on this panel some of the ways
that you can take your existing skills as data scientists and help plug gaps in the COVID response.
Awesome. Leah, much of your work including co-founding citizen science.gov has focused on encouraging
everyday citizens to get involved in science and help accelerate innovations in this country.
My guess is that scientists don't always welcome the input of the quote unquote unwashed masses.
Can you bear a little bit about your experience in this area?
Well, like Rob, I've worked at the intersection and served as a connector between the
academic research community, the tech innovation community, crowdsourcers,
citizen scientists and disaster management practitioners. Like Rob over the last 10 years,
we saw a lot of enthusiasm and well-meaning open data, data scientists and others try to
contribute and innovate in the middle of a crisis. But they lacked an understanding of what the
disaster response community actually needed on the ground. And so we tried to work to build that
bridge between the practitioners, the research community and the digital volunteers.
For citizen science and crowdsourcing, yes, one of the first questions we get when we propose
crowdsourcing or citizen science approaches is what about data quality and that field has
become rather mature and there are a lot of great approaches now for assuring the data quality
levels that you need for a project just as you would with any scientific research or data analysis.
Awesome. And Gigi, your work at IBM involves what you describe as translational science,
so translating between cutting edge, machine learning and AI research and making that accessible
and scalable to your customer and end users. Can you share a little bit about your experience
coming from the clinical and traumatic side of things and how that applies to COVID?
Yeah, it's a very interesting time. I think based on my experience in trying to take cutting
edge technologies to our end users, sometimes a very sophisticated clinician or a consumer who's
trying to understand the risk for the health problem. Thinking through that really,
really drive home the need for transparency. I think near and resonable, we all kind of touch
on palm data. The fact that we got to be transparent, not only on the data and its quality,
but the assumptions that we can use. I think that tends to get overlooked by the end user,
and if it's our responsibility to make sure we work with the domain experts to validate those
assumptions. That's kind of the biggest piece that I'm advocating. I do think data science,
that's what we can do, even to overcome why the issues of data security, privacy,
and how to leverage the streaming data from IoT. I think that's a lot we can contribute on,
with the matter of how to restrict the balance.
Awesome. Once again, I want to remind our viewers to contribute questions via the chat. We will
incorporate those into the conversation, but I'd like to start out really exploring the idea of
what's at stake when data scientists get involved and talk through your examples of where you
seeing help, where you've seen harm, and what folks are really worried about when they kind of
push out their arms and say, hey, data scientists shouldn't really be involved in this without
working with domain experts, whether that be epidemiologists, virologists, what have you.
I'll let you start with this Rob, and then we'll incorporate others.
Sure. Happy to. First off, this isn't you. It's not new that we're seeing data scientists
stepping to disaster response efforts. We have a lot of past examples already, where there
have been very negative effects. I'll give three examples. I also presented all three of these
at the KDD conference last year, one of the largest data science conferences. These should
already be part of the dialogue that people are having around data scientists responding to
disasters. This is not something new, and if you haven't read the papers and the dangers in the
past, you should think carefully about the kind of work that you're undertaking. One of the
biggest negative impacts I've seen was during the Ebola outbreak in West Africa,
having worked in epidemic tracking and having lived in West Africa in the past,
and a lot of work involved in this. One of the biggest causes for unneeded deaths were data
scientists in the West coming up with really like doomsday prediction models about how bad a
Ebola could be. The people who were going on the big media programs during the Ebola outbreak,
they weren't the professional epidemiologists. These were data scientists from adjacent fields,
and the media just eats up their story like this could kill us all. As a result of the international
pressure from the media, filled by people who did not have that expertise, but who looked like
there were experts, there was a lot of misinformation, a lot of people being too scared,
and so we calculated that for every person who died from Ebola in West Africa,
10 more people died from treatable illnesses because they were avoiding hospitals and clinics
unnecessarily. This was a very clear example of where so many people died because of misinformation
that started from well-meaning non-experts. Some of the others relate to open data and data
security, like people have mentioned already. When I was hoping to respond to the previous
coronavirus outbreaks, the SARS-CoV-1 or MERS, we found that because a lot of these outbreaks were
in countries where the rule of law is very different to the West, people go into different
assumptions. For example, Saudi Arabia wanted to find people in social media who were reporting
the actual numbers of patients in hospitals that might have differed from the official
government numbers. We evaluated this at that time and decided that even if this project was
well-meaning and even if it only used open data from people's public tweets, these people could
be seen as disagreeing with the government. It could be persecuted as a result, and so we decided
not to go forward with this particular project. Then the last of three examples, also working in
the Middle East during the uprising in Libya when Gaddafi was overthrown, I was hoping the UN
track displaced people. It seems like a use case that just suggests to be very non-controversial,
like who's internally displaced or refugee in another country and needs help, but people
within the region saw the UN's involvement as being a Western involvement, and our security was
compromised and some people treated as collaborators with a Western invader, not as people helping
in humanitarian efforts. Again, this was set up by data scientists who didn't have a background
in disaster response, technologists who thought they could do good, and ultimately did far more harm
than good. Those are three very negative examples. I'll pass it over to the other panelists here.
I do want to assure everyone watching that. I do have a lot of good examples of ways that people
are hoping and can help that I want to come back to. I want a second Rob's comments. We saw the
same thing and working the disaster space curation of social media for who needs medical care,
who needs water, who needs rescue, etc. It's not just in other countries. It's here in the U.S.
we're seeing retaliations against certain vulnerable populations. While we espouse open data,
I think we need to be careful. As Gigi mentioned, ensure the privacy and security, particularly
around vulnerable populations. Yeah, and this conversation reminds me of it's about six months ago,
where there is a commonly used population health analytics to help stratify individuals for
intervention, to prevent hospital wear mission, where that work was built upon a national
national data set, to understand individuals' health first. What was overlooked is the fact that
the training data set was balanced. Certain demographic tends to have richer data than other.
The mathematics were great. The sensitivities, the specificity were
high quality. Unfortunately, there's a piece of my data bias that we overlook.
That's a piece that we should really think through. Pandemic is really
evolving very quickly. Every local county has very different data composition and data coverage.
As data scientists, I think it's responsible to really think hard about what is the data
that we bring into our models and being very type of local, one of the strengths and weakness
of doing this. We don't keep that in mind and have the discipline to do the bias and diversity
check at the end. We could really sort of advance the allowance. I think those are some great
examples, Rob. It's clear, I think, to everyone that the stakes are high. I'm curious for you,
Rex, based on a conversation from a comment you made in our earlier conversation.
When a lot of people see these kinds of, or think of these kinds of potential harms,
the immediate reaction is, if you don't have an epidemiology degree or a virology degree,
you shouldn't be involved. You should take us a backseat to the experts.
You're a data scientist who is involved in this work, but you don't have a degree in
epidemiology or background and disaster response. How does that occur for you and
how have you reacted to those kind of commentary?
The way I tried to lay out the piece is to make an argument that the difference between good work
and bad work is not your level of expertise in that field or the strength of the quality of your
priors. I define science as a curiosity about the true state of the world and the application
of evidence and methods to form true beliefs than we held yesterday. That holds for every
level of skill and every kind of question or form of empirical inquiry we could form.
There are parts about COVID that the experts don't understand currently. There are things we'd
like to know about what's going to happen in the future that almost no one understands,
and these are probabilistic forecasts. There are smaller things that individuals would like to
understand of the expert recommendations which ones should I follow. What happens when they contradict
each other? What assumptions are the models making and are they getting better or worse over time?
How do I be a smarter consumer of expert information? I don't know a single scientist no matter what
field they're in who isn't climbing up the walls right now trying to understand this global
pandemic and how it affects them, how it affects their family, what they can contribute or better
understand themselves. My big takeaway is that we're not going to be able to prevent bad work
through gatekeeping. The people who are doing bad work before COVID are doing bad work now and
will do bad work in the future. What we can try to do is we can try to encourage people to be
curious and methodologically rigorous and then keep giving them concrete examples and feedback to
positively support them on that journey as they move up the curve. Earlier you brought up a couple
of examples of even the so-called experts to epidemiologists, credentialed epidemiologists
publishing studies that were later shown to be later taken down. Can you mention a couple of those?
Sure, so there's a really good paper called Learning as We Go, which does sensitivity analysis
on that IHME is two for health and metrics evaluation forecast. That is an example of a
credentialed group according to the Lane Keepers, who is producing the most culturally dominant
forecast in the US right now. But they're 95% confidence intervals or write only about 27% of the
time and their accuracy doesn't improve when you shorten the time horizon for their predictions.
And if you actually eyeball the confidence intervals, they get more confident the further out in time
they go. And there's all sorts of problems with their model and all sorts of weird assumptions,
but this kind of picking apart someone else's work, if you have a background in statistics or
causal inference or research design, anyone with that training can start to see the pieces and
understand the underlying problems and maybe contribute some feedback and some review that will
help others in picking between models. Just because they were first to be pitched or they were
able to culturally position themselves as the most influential forecast doesn't mean that they're
the only forecast within epidemiology and it doesn't mean that we should just accept them
because they're provincial. So we had a really interesting question come in via the chat. David
Clement is referring to the peer review process that we've historically used in academic publications
to ensure the quality of research. What's the role of peer review and why or why isn't it helping
in this environment? I can speak to that briefly. So peer review is absolutely helping in this
environment. In many disasters in the past, which came on much faster than COVID,
there was no advantage in cutting the corners of science. So the science itself couldn't be rushed.
What can be sped up is the review process itself. And so this is something that I have seen
different in COVID to previous disasters. I think because it is so global that a lot of major
publications are speeding up this process and letting it be open. So for example, at the largest
computational linguistics conference, ACL, which is coming up, there's now a workshop specifically
focused on COVID. It's an open review. So it's allowed itself to be the right place for these
kinds of discussions to happen. And papers are being reviewed on a rolling basis. So as quickly
as possible, as soon as we've been submitted. So we've been able to really speed up a process
that taken nine months waiting for the right conference to come around in the past and
bring it down to a couple of weeks without cutting corners on the science itself. It's just
been great to see that across a lot of different disciplines at the moment.
And when you've worked to scale up these kinds of citizen science efforts, have you incorporated
or what kind of review processes or checks and balances or peer review have you been able to
incorporate into those kind of programs? Well, with citizen science, you would go through the same
kind of peer review that you would with any scientific research if the intent is to do scientific
research versus say strictly education. So you would still produce published papers that go
through the normal peer review process. And in fact, these days, a lot of citizen science projects
are including the citizen scientist as co-authors on those papers. And in some open peer review process,
you know, people can contribute to the peer review as well. I want to tag on though to Rob's comment
about, you know, a lot of this preprints of the research is being made available and speeding up
the peer review. The White House has issued a call to action to the tech community. They've made
the publishers have made the literature now openly available around COVID. It made a team of
different organizations have gotten to including Microsoft, the National Library of Medicine,
the Chan Zuckerberg initiative to make those articles machine readable. So they now have 29,000
scholarly articles on COVID that are machine readable. The White House now would like the data
science community to develop text and data mining techniques to help speed up, you know, synthesizing
these materials. And they are also provided a set of research questions to help guide some of that.
And that is available on Kaggle. Awesome. Awesome. So Ayadela gets us to our core question, you know,
what can data scientists do to help? And she's specifically asking for resources, but I think there's
a lot to dig into just on, you know, what are things that data scientists need to be thinking about
when they're considering getting involved? Rob, do you want to get us started on that one?
Yeah, sure. I can get through a low new list of things which are really important and for which
they're very little negative consequences. There's a lot of ways that you can jump in and help
without potentially endangering people. So content moderation is huge. If you work at a large
company doing content moderation or if you want to take one of the existing open data sets,
that's really important. So obviously there's a big NLP component to it, but also computer vision.
So for example, a lot of even the bigger companies right now can't do matching on a fake news image
if there's crops slightly differently or it's like in, you know, on below format, etc.
So there's some low hanging fruit there that already exists in open data sets that any research
would help us. Unfortunately, during any disaster, while crime goes down in general, some particularly
nasty people ramp up in scams and exploitation, especially exploitation of miners. So again,
if you're working in content moderation or content health, when you're able to identify scams,
artists or people looking to exploit children, that's incredibly important right now. And again,
this will exist in initiatives for that. Any work in low resource languages is really valuable
right now. So the majority of course that I've had with organizations like WHO have been especially
worried about how to get information out, how to start using natural language processing in low
resource languages, they're not captured in any of the large libraries that we have. Some of the
existing open source data sets that I've helped create for disaster response have low resource
languages or any research into those would be valuable. You see them in AI for all, some Stanford
MIT classes have used them. Udacity is nano-degree in data science uses a set that I created too.
Any of that research would be valuable. And then it's just like a ton of things around the edges.
Like cleaning data is always valuable. I mean, if you turn up to volunteer at your local hospital,
you're not going to be invited into do surgery, you might be given a mop and bucket.
Expect the same thing. If you turn up to...
Technology.
Yeah, so if you turn up to a big organization, they're not going to be sitting there with a
well structured data set ready to go and they just need 10 lines of high torch. But data cleaning
is really important. Scentitation in hospitals is equally as important, especially at the moment.
And so yeah, like that's how I started out for years to do an all the grunt work in disaster
response. And it's a very noble, valuable way to start. The last example, anything around
supply and logistics. So there is no world database of who has what kinds of medical supplies.
And a lot of the world governments are now implementing really basic crawlers and machine learning
systems to work out who has a supply of medical masks or respirators and things like this.
And in almost any disaster response, the supplies and logistics component of it ends up being
the biggest part of it, the most complicated part. And there's a ton of machine learning in supply
supply chains and logistics. But not enough because it hasn't been a popular area for people to
research. And again, like any research in this area would be really valuable for those of us in
disaster response. Anthony Garland asks, what's the role of IOT devices in this pandemic? He's
speaking specifically about personal IOT devices and health trackers like Fitbit, but GGM wondering,
you know, either personal or kind of within hospitals and health care systems or other IOT use
cases are there. Interesting opportunities for data scientists that arise in the intersection
of COVID and IOT? Right. I mean, to ask that to it, right, in terms of personal IOT device,
right? One is there's a lot of interest around better understanding more social policy,
social distancing adherence. The people actually following the orders or guidelines that
is local government. So the personal IOT device has rich rich data to support that.
But of course, we've got to be vigilant with the security and privacy. But we do, we are seeing,
there's a strong correlation to adherence to some other disease curves, right? So I think that's
an area that will be of interest. And then the area will be to help support some of the symptom
checks, right? As opposed to trying to get on a phone call and talk to someone and understand
this sometimes and putting it in the CDC website, there's a lot of opportunities in leveraging
the data that is being connected in this personal fitness devices to help supplement that.
I think about, you know, the fever monitoring devices,
think about, especially for one of the highest good is folks with hypertension and diabetes,
but about those IOT devices, how they can be incorporated to help detect people at
high risk. And trace together is another one where they're looking at the connectivity is
who's exposed to whom. And I'm going to bring it in a privacy way that secures people's privacy,
but alerts them if they've been exposed potentially, if people who've been diagnosed.
Yeah, yeah, that's a great one. In fact, I just had a ethics conversation,
internally, and I'd be on how to have a handle of that. But you may have seen in the latest
White House guidance, there's a point to as employer to support the employees' content resources.
So if you and I have gone to work, and in the next stage, we should be alerted by employers,
someone else on my floor has been tested possible. So how do we do that while protecting the
individual privacy and the instability? So Rob, you took on that question of how can data
scientists help from a use case perspective? Rex, maybe you can take it on from a practices
perspective. Sure. Well, there's two big ways that at least our shop is trying to contribute,
and I think are low-hanging fruit and opportunities for other people in this space.
The first is in the aggregation of existing data, not just cleaning, but munging different
data sources together creates a lot of problems. And my shop day-to-day, we're a knowledge graph
shop that works on aggregating conflict panel data or political science data measured across
countries or localities. And so when COVID counts started coming out of confirmed deaths,
number of tests, it looks just like the data we are working with already. And so we've rapidly
been prototyping a system to aggregate all of the COVID data globally at at least eight province
or state level and preferably below. So we're up to about 300,000 observations, a couple dozen
data sets, and doing that kind of work is something I think any data science shop can contribute to.
The other one is evaluation of models and predictions. If there's anything ML people know how to do,
it's how to create a test set and take a split. And I just want to do a shout out from one project,
I know of Nicholas Reich's lab at UMass Amherst, has been collecting the predictions and forecasts
from different groups, normalizing them, putting them into the same sheet, and then seeing how they
perform over time and even creating an ensemble prediction based on all of them. And that is something
we can immediately start trying to do is evaluate and combine the forecasts we have already. And the
faster we do that, the more informed decisions we'll be able to make in the next couple months
about reopening in the US or other places. Any other takes on that? Things that data scientists need
to be thinking about when they're considering jumping in and helping out?
Listening to the panelists, right, two things come to mind. I think we're out your earlier point
about NLP in addition to helping with clinical and genetic discovery, looking at various literature,
that there is a strong desire to better understand local government's policy. It's changing on a
daily basis. One of us, you know, to deal with bus models, we had to look at cost geography
while really being careful about local differences. So if you could spend more time
enabling policy insights to the NLP, it could be very valuable.
Oh, go ahead. Go ahead. GovLab did kind of a rapid assessment of different topic areas that
could be looked at and proposed at least 12 areas, going beyond tracking disease spread and supply
chain of the PPE to protecting human rights and promoting accountability, you know, oversight of
governments and their actions with COVID, alleviating pandemic related and unemployment and poverty,
education, upskilling, identifying which businesses may be at most risk and helping inform
policymakers of that as they make their decisions. And as we talk together in the pre-planning
meeting, identifying and tracking misinformation.
So misinformation is, you know, didn't come up in specifically in talking about some of the
potential harms, but it's kind of the backdrop of a lot of the, you know, angst that comes up in
this conversation. You know, any specific examples of misinformation or disinformation relating
to COVID that any of you have seen? Yeah, I've seen a lot related to, I guess, basically you could
you could call folk remedies and that's in every country in the world. So whether it's something
of no known benefits like COVID-opathy here in the United States or other local remedies in other
parts of the world, this happens in every disaster and every disease outbreak. It can be very
dangerous when that misinformation relates to poisonous things. So as soon as we introduce
something like bleach as a cleaning agent and then in disaster, we know some percent of people
are going to drink it. As what happened with, I think it was chloroquine or certainly some other
quinine derivative here where people drank it who didn't even absorb symptoms and then people died.
So yeah, this is, you know, off the back of well-meaning people, people who think there are solutions
or there are types of protective measures which they believe are way better or way worse than they
truly are. And this becomes the biggest worry for us when celebrities get behind it because no
public health organization can compete, you know, with the Hollywood celebrity with tens of
millions of followers on social media. So it's really tough for us when a lot of these celebrities
will double down, you know, maybe on remedies that they have a financial stake in, like that
stupid good product. And so that's, we see a lot of the danger because, you know, we can't
compete with those kinds of people in social media and a lot of them are being kicked off in social
media, YouTube and Facebook especially have been great at this. But it's always trailing. The
damage has often been done by the time this content gets removed. I think they also thought that 5G
causing coronavirus so people were burning down the 5G towers. There's an organization news guard
which has a misinformation tracker around COVID and I think the credibility coalition and the
missing folk on folks are also have efforts on this regard. Yeah, so I think as a data scientist,
this is one area where you can use your critical judgment. Similar to what we're actually saying
earlier, you know what a good report, what versus a bad report looks like. You should be able to
evaluate objectively whether a public official has experience in this area, whether they're
calling on peer-reviewed articles when they present their evidence versus someone who's
share in their end of total experience. And as the data scientist and your family, you probably
have a better critical thinking capacity than most other people. And so I hope in your family
become better critical thinkers and how to evaluate any advice skeptically and choose the best
advice when there's conflicting advice. That's a really valuable thing that you can do in your
community. There's also a big need for data scientists in this research space. So when I ran the
big one of the big data innovation hubs for the National Science Foundation, we had a community
of a hundred researchers, data scientists, digital anthropologists, political scientists, all
bringing to bear their different disciplines to understand, track and develop mitigation strategies
for misinformation. You know, I would like to comment too. Sometimes the information is true,
but it's not actionable to your situation. I think that's the piece that I tend to see often.
For instance, we hear how a lot of local hospitals are having resource storage,
patients that we turn away, or that's in a call to ask them on critical or elected surgeries to
take a pause. And it's very, it really depends on the local health system. And we're seeing that
there are a few conditions that shouldn't have gone down, are going down in areas. Like there,
less heart attack, there's less asthma attack, and there's less trauma cases. And in some sense,
it could make sense because people are not traveling as much, but the rate of the increase is a
little alarming. So hospital resource storage does apply to your local area. I think that's,
you know, we've got to help our community to think through that. It's true. If I have someone
city, it's a stature for the way I live, and that's one of the things. We started off talking about
examples of the potential harms, but what about specific examples of where data science is
helping out and making an impact? Leah, you had a few. There's like, there are lots of different
opportunities, I think, for data scientists and other data-related folks to plug in,
in very positive ways. I think in our discussions, we talk about, you not only need the data
science at the table, but you need the epidemiologists, the virologists, immunologists,
economists, and other kinds of professionals. So there's several opportunities where they're
bringing these kind of teams of folks together. MIT COVID challenges one where they're looking at
innovating, using data and other approaches for solutions in Africa. The DevPost Global Hackathon
is another place. We'll put provide those links afterwards. They've got to think something like
18,000 people signed up to contribute. The US Digital Response Team, which is started by some
former White House deputy chief technology officers, has now got over 4,000 volunteers,
and they're pairing those data science back end engineers, product managers with local government
folks to help build websites, things that are free, open source, and extensible or replicable
to other locations. And then in the citizen science space, we've got Folding at Home, which is
using distributed computing. So if you want to, like, study at home, donate your computing resources
to help solve some of the medical challenges around COVID, there's the coronavirus binder
design game, which is developed by Foldit. So we've got a serious gamers figuring out how a protein
related to coronavirus works, and figuring out how they might disrupt that protein.
Eternet is another one of these online series games that requires some complex problem-solving
that having a data scientist tell probably would be helpful. And then, you know, as I think Gigi
mentioned, there's not just COVID we have to worry about, but now we've got some fabulous air
quality, right? So doing things like monitoring air quality, doing things like monitoring water
quality, it's Earth Day, and how is this impact of COVID on our environment? There's a big
opportunity for folks with IoT skills, data science skills, Earth observation, geospatial mapping
to help with those efforts too. Other examples of where data scientists, non-epidemiologists,
virologists are jumping in and adding value. So no one's mentioned kind of the efforts from
journalism, and a lot of newspapers have data scientists on staff now, so I just wanted to
point out that the main source of tracking for testing in the US is the COVID tracking project,
which came out of an effort by the Atlantic and two journalists in India. COVID-19 India is a
massive crowdsourced effort. If you go on GitHub, you can go country name by country name and find
at least one data scientist or beginner who has started scraping the local health ministry website
and getting unstructured data into a structured form that we can then use. I think one of the
one of the main comparisons here is not between epidemiologist and expert scientific experts
and civilians or data scientists or amateurs. It's between government resources and capabilities
and the civilian capital, social capital to work on that. So if the New York Times is the one
who's putting together county level counts of COVID in the US and not the federal government,
that tells you something. New York Times and Rooters are both tracking that at the county level
because there isn't a strong federal system with reporting all the way up the chain and all the
way down the chain. It's newspapers collating this information for us. And so we have to step in
where the institutions fall short and that can only be done by regular people.
David Clement asked another really interesting question about how the role of data science shifts
as this pandemic continues. Early efforts have been involved in modeling and dashboards and the
like. But what does the role of data science need to evolve to as time goes on? Any thoughts on that?
Yeah, I can I can speak to some part of it. I certainly can't speak for for all the data
science and how it can respond. One of the most constant things I'm seeing working with really
large organizations doing public health and misinformation is the areas where we can get help
from data scientists. So almost all the systems I'm looking at don't have much data to begin with
and they need to bootstrap from not many examples. So human in the loop systems, smart active
learning, adapting to low resource languages with with inconsistent spellings and more complicated
suffixes and prefixes. Lots of really important problems that just aren't part of what most
data scientists learn. Most data scientists will learn about purely automated methods where the
data is constant and the data is never constant in in disaster response. So a lot of the things
that might look useful for multilingual NLP like zero-shot learning really aren't that useful.
Zero-shot learning requires fairly large unsupervised, unchanging corpus and the percentage difference
doesn't doesn't change a whole lot. What we need more is people who can think about the productization
human in the loop systems where it's the human either expert or non expert for example giving
medical advice who can look up a database or maybe the right answers and give that advice.
And so this is one of the areas that I've been given advice to. So a lot of major companies working
in this area have been introduced to a lot of data scientists. So data scientists that all the big
tech companies are willing to step up and do the right work. But they're going to introduce to
the wrong people. They get introduced to someone who can do a cutting edge paper in zero-shot or
one-shot learning. Whereas actually the data scientists who probably have the best knowledge,
they work in content moderation or they help out with customer success for customer service systems.
So I think a lot of that human computer interaction side of data science which I find really
fascinated. I think that becomes way more useful in a disaster response environment and
I would love to see more people picking that up. Gigi, I think you had something to add to this one.
Yeah, I was smiling because let's on the technology aspect. But as we think through the use cases,
as many countries are trying to figure out how to recover and get back to the new normal,
I think we have to think through like care delivery will not be the same anymore.
Like half of the care today in America is delivered to a telemedicine and I think that's
statistically even higher than that. So what can we do for the scientists to help improve
not only how monitoring but the experience between the clinician and the patient as they are
more longer able to physically touch right. So the signs of haptic skin tell me, the signs of
how the share and transmit monitoring data all that comes in. And another
we have to allow us whereas as a teacher, as an educator, we will be delivering our education
material in a very different way. How can we make them more interactive and how can we learn from
and imagine the data that we are collecting these online systems. How can we learn from that
and really provide it in much more rich experience for our students. So my mind actually went
in a way of, okay, in a new normal, not only in health, but in many many disciplines,
on their normal opportunities for us. The idea of the new normal is an interesting one. I saw
an article the other day that talked about how in Milan, they're taking advantage of the change in
the traffic situation in the core city to implement some pro cycling measures. So they're
going to be producing the vehicular traffic in the city core and adding bike lanes and things like
that. And you know, the new normal is going to be new and it will provide a lot of opportunities
for folks to jump in and help with the kind of modeling that they've done in non epidemiology domains.
So we've got a number of questions that have come in that relate to the kind of data that
we're seeing being published by media and experts talking about. I think a lot of these questions
are getting at kind of the, you know, a couple of things, a lack of confidence on the part of
the information consumer as in terms of what media we can trust as well as the rapidity with which
viewpoints are changing. We've got, you know, folks showing data on different sides of, you know,
the utility of masks and the position that local governments and organizations like WHO CDC
is shifting over time. We've got similar types of, you know, data on different sides of social
distancing. Any thoughts on like cutting through the, you know, cutting through these differing
perspectives and more importantly, how do data scientists help their communities cut through
these differing perspectives? Well, I'll say be very wary of the media for one, you know, most
journalists are great people and they want to report things honestly, but media companies are not
aligned with providing you the most important information that you need. A lot of media companies
have the same narratives in every disaster and it's that the responders aren't doing the right
job. Why is this being ignored? Why did somebody cover this up? Why is there a conspiracy here?
And even when journalists know that they, it's not a real story, it's part of the narrative
they have to tell. And you can be smart about this. If you see stories come out and there's no author
in a big publication that they normally know that they're spreading misinformation or they get
an invited expert opinion who is no expert at all, I'd just say something outrageous.
But it can be, yeah, it can be tough. I mean, I haven't said that like, I mean, like Rex said,
like some of the most useful sources of information have come from
den scientists and journalists working together. So yeah, I'll flag that there's a lot of variety,
but yeah, the media is tough to trust a lot of the time.
So you mentioned working with local communities. I had mentioned the US Digital Response,
which is working with local government, but there are other examples like the Data Science,
Federation, and Southern California. That was started by Gene Holm, who's in the CIO's office
at the City of Los Angeles, and they've partnered with 18 universities and organizations in the
area. So the students and faculty and staff are working closely with the City. Students and faculty
and the other volunteers bring the Data Science expertise. The City brings the problem space,
the use cases where they need that help, and that's kind of a local on the ground use case.
You know, I was listening to you, Rob, and I think what do I do? What do I advise myself
in my choosing to do when you're exposed to this different news channel and information? I think
the first thing I go to, of course, the author and the citation. I think the second thing I'll
advise you to pay attention to is the metric. You can very quickly tell the thoughtfulness of the
numbers behind the scene in how do we put the metric, right? It's just a number of cases, so a number
of cases per capita. That is a basic, for example, but many ways you can ask you if you focus on
the metrics that's being used and being reported, and you ask yourself, this is translatable.
I think that is a good indicator for me. A corollary is to look at the why
access of the graph and whether the numbers are jumping around like some of the ones that we've
seen going around. There's been some interesting variations on log scales. I thought that was
complicated. It's a log scale and check the units, right? Things that you learn in elementary
school, it's very, very relevant. Well, then there's even a scale. If there's not a scale in the
first place, that's a really good indication. It's something going on. It's actually, if I could
make one point, I think there's a piece that someone else could overlook their thoughts, right?
Especially coming into a new field, what is the right metric to use? What is the right
scaling path to use? We've got to be careful of that, and coach looks at what we work with.
One quick point on this that I raised in my piece is that not just the media, but some of these
political actors are producing speech that they know is wrong, or they're incurious about what the
right answer is. They just have to produce a particular viewpoint. Their mistake here is to
try to respond to that point for point. There's something called the Gish Gallup, which is a
yeah, it's a debating tactic where you just spam bad arguments as quickly as you can, because
the time it takes to explain why those are wrong is longer than it takes to make the argument
in the first place. We have a risk here where certain media or certain political actors are
producing false speech at a rate faster than we could possibly debunk it. And frankly, if you're
an expert in these fields, you shouldn't be spending your time debunking this. You should be
producing work and getting us to the right answer independently. So the only response that I found
has either been through speech like comedy and satire, where the daily show used to take apart
bad arguments for you in a very creative way quickly, or through positive education. We have to
provide scientific literacy to our families and to our population so that they can tell a bad
argument no matter which day or year or decade it's made in on what topic. That's getting it at
the bud is the only way to protect them. It's like a vaccine against bullshit. And it's the only
hope. Otherwise, you'll bankrupt yourself trying to go beat for beat with these people. On the satire
front, there was an article that was responding to another article that purported that public
transportation was one of the key spreaders of the virus in New York City, based on correlation
between, you know, I forget what the specifics were when the city shut down and public transit usage
or some random correlation and the satirists and author basically took the same argument and
article and rewrote it with the correlation being to bike sharing using the bike sharing data.
It's poke fun at the argument. We're coming up to the top of the hour. I want to take another
couple of audience questions to help us close out. So Megan asks a question referring to
COVID-19 data sets but brings up the broader issue of bias. And this relates to something that
you open up with Gigi as well as the idea of transparency. Can you speak a little bit to the
importance of bias and transparency and things that we need to be thinking about both generally and
when dealing with these data sets? Yes, absolutely. Really being, it's okay, I call the gating
requirement. It's first take a look at the completeness coverage and distribution of your data,
right? And I think it's more than important especially when we're dealing with public
health situation to consider the dimensions of not just demographic groups but different
local diversity sets. So I think that's the piece that we're very passionate about.
And secondarily, I'll go beyond just the data set. I think it goes back to my earlier point
about being transparent on the assumptions that we are making. It's amazing, right? If you look at
the epidemiology model, if you include as a atomic patient or if you don't include as a
tonic patient, you could get two different times differences in results. If you don't mention that
to your reader, then I think we're not doing our job. So transparency in data and transparency
in assumptions. Any additional thoughts from our panel on bias and transparency or data sets?
Stay tuned, we're working on a piece called how to be careful with COVID counts that's going to
investigate that in question exactly. And if I could add one thing, right? It's interesting how
we often report on cases and deaths as opposed to recoveries. It's also interesting how we don't
report on testing availability and data in a driver of what we see in the results. So I think not
that. I look forward to the article. Bernard asks, and this is a great question to help us wrap up,
is there a responsible data science 101 out there that folks can go to and have a look at?
I start with the article I wrote about five ways that data scientists can help and five mistakes
to avoid. There's a lot of links in there. If you want to do deeper reading about things that
worked and things that didn't work in the past too. And Rex's article as well. And in fact, we'll
link to all of these articles and other resources that have been mentioned in this conversation and
that our panelists have to share on the program page for this, which is at twomolei.com slash
rdscovid. Matume asks about that term that you used Rex in the context of debating. We'll get
that in there as well. But at this point, I'd like to thank all of our panel for participating in
this great discussion. Thanks, everyone. Thanks for having us. Thanks for joining us.
