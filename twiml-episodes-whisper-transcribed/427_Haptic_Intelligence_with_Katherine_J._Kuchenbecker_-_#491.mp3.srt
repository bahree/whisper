1
00:00:00,000 --> 00:00:13,760
All right, everyone. I am here with Katherine Krukenbecker.

2
00:00:13,760 --> 00:00:18,800
Katherine is a director at the Max Plank Institute for Intelligent Systems.

3
00:00:18,800 --> 00:00:21,360
Katherine, welcome to the Twoma AI podcast.

4
00:00:21,360 --> 00:00:24,160
Thanks so much. I'm really happy to be here.

5
00:00:24,160 --> 00:00:29,120
I'm really looking forward to digging into our conversation and your work, which is at the

6
00:00:29,120 --> 00:00:36,400
intersection of robotics and machine learning, and I'd love to have you start us off with

7
00:00:36,400 --> 00:00:38,640
a little bit of background. How did you come to work in the field?

8
00:00:39,440 --> 00:00:44,800
Yeah, so I grew up in California. Actually, although my name looks German and I live in Germany,

9
00:00:44,800 --> 00:00:50,480
I'm actually American, and I was raised by a research psychologist. My mom was a now retired

10
00:00:50,480 --> 00:00:56,240
professor and a surgeon, and I was always fascinated by how things work, and I wanted to create

11
00:00:56,240 --> 00:01:02,560
technology that helped people. I also did a lot of art, and I like writing, so I had many,

12
00:01:02,560 --> 00:01:06,800
many different interests, and I was an athlete, and it was that athletics that led me to study at

13
00:01:06,800 --> 00:01:12,720
Stanford University, which was also close enough to home, but far enough away, and I studied

14
00:01:12,720 --> 00:01:18,480
mechanical engineering. I really enjoyed understanding both physics, but then how I had to also design

15
00:01:18,480 --> 00:01:23,040
and build things that produced functionality in the world, and I was always drawn more towards

16
00:01:23,040 --> 00:01:29,600
smart systems with sensors and actuators and programming. I actually delayed taking my first

17
00:01:29,600 --> 00:01:33,280
programming class because I'd heard it was so difficult, all the other athletes, I was a

18
00:01:33,280 --> 00:01:39,840
viable player. They all said, oh, you know, the programming class, so hard, and I loved it beyond

19
00:01:39,840 --> 00:01:45,600
words. And then I took more computer science, I took more electrical engineering, and I decided

20
00:01:45,600 --> 00:01:50,640
to stay for a master's degree, and I worked actually as a teaching assistant in a machine shop

21
00:01:50,640 --> 00:01:56,240
for two years, helping like design, helping students learn design and manufacturing, like welding

22
00:01:56,240 --> 00:02:01,520
and casting bronze, and milling aluminum and making parts, staying in the shop late at night,

23
00:02:01,520 --> 00:02:06,240
helping, and I really fell in love with working with younger people, helping them design and create

24
00:02:06,240 --> 00:02:11,840
things. And I also took this amazing mechatronics class there, and like realized I wanted to

25
00:02:11,840 --> 00:02:18,800
begin my professor, and I needed a PhD, and I needed a PhD advisor, so I looked around and found

26
00:02:18,800 --> 00:02:24,480
a new professor, I was his first PhD student, his name is Kuntan Niemeyer, and he was one of the

27
00:02:24,480 --> 00:02:29,040
first engineers at Intuitive Surgical, a robotics surgery company that most of you probably know,

28
00:02:29,040 --> 00:02:36,160
they make the DaVinci robot. And we got along super well, he was also a volleyball player, and I think,

29
00:02:36,160 --> 00:02:41,280
yeah, the mechanical engineering computer science electrical intersection is really robotics,

30
00:02:41,280 --> 00:02:45,920
and then turning it to do something useful, whether that's in health or in consumer products,

31
00:02:45,920 --> 00:02:50,960
or I mean, we work on so many different things now. Yeah, so I got my PhD there, and

32
00:02:52,240 --> 00:02:56,560
moved to the East Coast after graduating. I did a brief postdoc at Johns Hopkins University

33
00:02:56,560 --> 00:03:00,560
with Alessandro Camaro, who's now a professor at Stanford, and then I started my faculty career

34
00:03:00,560 --> 00:03:05,360
at the University of Pennsylvania in the grass lab, which is a great robotics group, and I was super

35
00:03:05,360 --> 00:03:09,840
lucky to have colleagues in several different academic departments, and was there for nine and a half

36
00:03:09,840 --> 00:03:16,080
years, and we did, I mostly do haptics research, robotics, and while I was at Penn, I started doing

37
00:03:16,080 --> 00:03:20,720
more research in autonomous robots, giving autonomous robots a sense of touch, sometimes through

38
00:03:20,720 --> 00:03:26,640
machine learning, and then in 2017, January, I had the chance to move here to Germany to become

39
00:03:26,640 --> 00:03:31,200
a director at the Max Plank Institute for Intelligent Systems. We've been around for about 10

40
00:03:31,200 --> 00:03:36,880
years now. It's a wonderful place to do interdisciplinary research in robotics and AI.

41
00:03:36,880 --> 00:03:43,120
Awesome, awesome. Tell us a little bit about your research interest. You've given us a taste of

42
00:03:43,120 --> 00:03:51,520
them, but what are some of the various areas that you're focused on? Yeah, so a lot of our work

43
00:03:51,520 --> 00:03:57,600
comes back to the sense of touch. So haptics are physical interaction with the world, and I know many

44
00:03:57,600 --> 00:04:03,520
researchers study vision, whether it's computer vision or natural vision, or hearing auditory

45
00:04:03,520 --> 00:04:10,640
perception for humans and animals or artificial, we know far less about the sense of touch.

46
00:04:10,640 --> 00:04:15,040
And so from a researcher's perspective, that makes it really interesting, because it's still

47
00:04:15,040 --> 00:04:21,840
like relatively early days, for how do we create artificial systems that can perceive their world

48
00:04:21,840 --> 00:04:26,000
through touch, and then we also work on the dual problem, which is actually where I started,

49
00:04:26,000 --> 00:04:32,640
which is creating devices and programming them to present high quality haptics sensations to

50
00:04:32,640 --> 00:04:38,800
users. And so a lot of the work that we do is inspired by some fundamental insights that like

51
00:04:39,360 --> 00:04:44,960
the sense of touch is really important to people and to artificial agents. It's often under-appreciated,

52
00:04:44,960 --> 00:04:50,560
and it's not yet understood that well. And when I talk about touch, I'm kind of talking about

53
00:04:50,560 --> 00:04:57,120
two things. And one is your skin, like distributed all over your body. You have cells, receptors,

54
00:04:57,120 --> 00:05:02,240
that can process different kinds of physical stimuli. So whether something is touching you,

55
00:05:02,240 --> 00:05:08,160
or how big of an area it's covering, if it's sliding across, if it's shaking back and forth,

56
00:05:08,160 --> 00:05:13,520
vibration, if it's stretching, if it's warm or cool, taking heat from you, and that's distributed

57
00:05:13,520 --> 00:05:17,440
all over your body. So it's like a few tiniest sense. And I think you probably already know robots

58
00:05:17,440 --> 00:05:22,880
don't usually have any skin or almost any skin. They're really deficient in that. And then on

59
00:05:22,880 --> 00:05:26,800
the other side, there's also your movement through the world, like your joints and the muscles

60
00:05:26,800 --> 00:05:32,560
forces. And your brain is constantly combining all that streaming data from all over your skin,

61
00:05:32,560 --> 00:05:38,800
and the movements you're making with your body to help you understand your own physical,

62
00:05:38,800 --> 00:05:42,720
how you're moving through the world, if you're contacting something that's expected or unexpected,

63
00:05:42,720 --> 00:05:47,680
and processing that. And the final thing, cool thing about the sense of touch is it's very interactive.

64
00:05:47,680 --> 00:05:51,440
So vision is active, isn't it? You move around and you get different perceptions of a world,

65
00:05:51,440 --> 00:05:56,400
maybe see something from different angles, but touch is even more so. Because there's a really

66
00:05:56,400 --> 00:06:02,480
tight loop between when I touch this glass and I start moving my finger down, I can make movements

67
00:06:02,480 --> 00:06:07,200
that are almost not visible and really, really feel something very different or start moving in a

68
00:06:07,200 --> 00:06:11,760
different way. And so yeah, we're fast, I'm fast and you can probably tell I'm talking too long

69
00:06:11,760 --> 00:06:19,040
about this by touch-based interactions. And we start from this foundation and work on kind of,

70
00:06:19,040 --> 00:06:25,120
maybe a few different areas like social physical human robot interaction. So giving robots a sense

71
00:06:25,120 --> 00:06:28,880
of touch whether that's actually making sensors for them or how they should process the data

72
00:06:28,880 --> 00:06:34,320
and often to interact with people. Our more specifically just tactile sensing for robots and

73
00:06:34,320 --> 00:06:40,720
for manipulation. We also work on systems where a human is controlling a robot from a distance.

74
00:06:40,720 --> 00:06:45,200
So like the DaVinci robot where a surgeon remotely operates these little tools that are deep inside

75
00:06:45,200 --> 00:06:49,920
the person and we work on giving you a sense of touch and maybe using that to provide demonstrations

76
00:06:49,920 --> 00:06:55,360
for autonomous robots. And then last, we also work in like virtual reality devices and algorithms

77
00:06:55,360 --> 00:07:02,080
to let you feel simulations like maybe dragging across well on a cell phone. You probably felt

78
00:07:02,080 --> 00:07:07,520
some have to feedback but feeling able to feel textures of surfaces on a cell phone or in virtual

79
00:07:07,520 --> 00:07:12,000
reality that would make and break in contact with things and to make it feel more realistic.

80
00:07:12,000 --> 00:07:21,360
Nice, nice. I noticed the title of your recent presentation at the Eichler conference was

81
00:07:21,360 --> 00:07:26,320
haptic intelligence and I kind of had the sense that that was maybe a double-on-tondra like

82
00:07:27,280 --> 00:07:34,000
the sense of touch or haptic informs our intelligence but there's also intelligence required to

83
00:07:34,000 --> 00:07:41,840
process it digitally. And I thank you in kind of articulating your research portfolio.

84
00:07:45,120 --> 00:07:51,360
You spoke to the way that sense that that that sense kind of informs the way that we or robots

85
00:07:51,360 --> 00:07:56,560
move through the world. Yeah, I think the sense of touch is a little under-recognized and it's

86
00:07:56,560 --> 00:08:01,200
sort of coming up in importance in robotics. Of course, first if a robot's standing there it needs

87
00:08:01,200 --> 00:08:05,120
to be able to perceive its environment, to be able to move around and not hit anything. Actually,

88
00:08:05,120 --> 00:08:10,320
the goal is often don't touch anything, don't break anything, don't hurt the person. But then if we

89
00:08:10,320 --> 00:08:15,600
want to do something, if we want to manipulate, move things around, pack a box, clean up a room,

90
00:08:15,600 --> 00:08:21,200
help a person, robots are going to need to reach out and touch something. And then like many other

91
00:08:21,200 --> 00:08:25,840
researchers we often turn to humans and animals for some inspiration and are curious about

92
00:08:25,840 --> 00:08:30,320
understanding I could scientifically what is known about the sense of touch. Often as a researcher

93
00:08:30,320 --> 00:08:35,120
you might have some intuition about how you would design a system. And sometimes that works well.

94
00:08:35,120 --> 00:08:41,280
And sometimes it doesn't because there are aspects that you maybe don't properly anticipate. And so

95
00:08:41,280 --> 00:08:45,280
a common example is like often a roboticist we think I need to put a force sensor on my robot.

96
00:08:45,920 --> 00:08:49,680
And that means you have to cut the wrist and put a force sensor here and then between the hand

97
00:08:49,680 --> 00:08:54,800
and the wrist the robot can feel the load as it like picks something up. But then that force

98
00:08:54,800 --> 00:09:00,400
sensor here can't feel but things that happen higher up. And it also doesn't give you that much

99
00:09:00,400 --> 00:09:06,720
information about where contact is happening. And so skin is distributed and has like it's quite rich

100
00:09:06,720 --> 00:09:11,040
and a little bit different from what I would think I would need a force sensor. But maybe

101
00:09:12,160 --> 00:09:19,040
also another example is our skin is especially sensitive to transients, to changes in stimuli.

102
00:09:19,040 --> 00:09:22,880
And a lot of times as an engineer you might look at the levels and think oh it's really important

103
00:09:22,880 --> 00:09:28,960
that it's like 2.3 newtons or the illumination what level that is. But these sometimes the

104
00:09:28,960 --> 00:09:33,520
illumination in a room radically changes over the day and your vision system is adapting or your

105
00:09:33,520 --> 00:09:41,600
touch system is also kind of has these is able to focus in on changes which are often really relevant

106
00:09:41,600 --> 00:09:49,120
and important for behavior accomplishing what you want. So nice. You talked about some of your

107
00:09:49,120 --> 00:09:56,400
kind of early you know your early days and just liking to build and make stuff. And I just had

108
00:09:56,400 --> 00:10:02,960
this wonderful memory of when I was a kid I was into electronics and one of the things that I was

109
00:10:02,960 --> 00:10:08,800
playing with at some point was a force sensor. It was like some piezoelectric foam or something like

110
00:10:08,800 --> 00:10:13,520
that and you put it between two like printed circuit board elements and you can I guess the

111
00:10:13,520 --> 00:10:20,000
resistance varies based on the pressure that's applied. Actually we're working on something similar

112
00:10:20,000 --> 00:10:25,440
to that. We're actually working on something similar to that. We have a paper under review

113
00:10:25,440 --> 00:10:30,800
and a human computer interaction conference on something very similar but to give rope to like put

114
00:10:31,840 --> 00:10:35,920
touch sensors all over the body of a now robot. You need something cost effective and simple

115
00:10:35,920 --> 00:10:39,520
so it'll keep working but please go ahead. I'm glad you had a chance to make a force sensor.

116
00:10:39,520 --> 00:10:47,280
I hadn't thought about that in who knows how long but just kind of clearly came back to

117
00:10:47,280 --> 00:10:54,080
me so thanks for that. I'm glad I I hope that lots of young kids have a chance to test out

118
00:10:54,080 --> 00:10:59,360
engineering and computer science as you were telling your story I was flashback on I was lucky

119
00:10:59,360 --> 00:11:05,120
that for my 13th birthday and this was a long time ago this was in the 1990s my parents gave me

120
00:11:05,120 --> 00:11:12,480
a Dell 386 computer and it still had like a DOS 5.0 command line and windows 3.1 that you like

121
00:11:12,480 --> 00:11:18,720
ran from the command line and I like tried absolutely everything possible in windows and learned

122
00:11:18,720 --> 00:11:25,440
all about it and then learned some weird visual programming language and I was thought oh my gosh

123
00:11:25,440 --> 00:11:29,520
this programming thing is really cool at the time I had had a knee injury and I had to do physical

124
00:11:29,520 --> 00:11:36,160
therapy and it required me to like tense my muscles for 10 seconds and then pause for 5 seconds

125
00:11:36,160 --> 00:11:40,560
and tense and you always had to sit in water clock and I found that very cumbersome and I wanted

126
00:11:41,280 --> 00:11:45,200
something that would keep that time and I did three sets of 10 repetitions and I wanted to be

127
00:11:45,200 --> 00:11:50,000
able to read while I did my exercises so I used this visual programming language that I'd learned

128
00:11:50,000 --> 00:11:56,080
on my new old school windows 3.1 computer to make a program that would beep to tell me when to

129
00:11:56,080 --> 00:12:01,200
start and stop and so I could read and I didn't realize until I was later in college and I'd

130
00:12:01,200 --> 00:12:07,760
heard all these rumors how hard programming was amazing it's so powerful and let you out of

131
00:12:07,760 --> 00:12:13,760
me things and it also when I later realized it forces you to think much more clearly it forces

132
00:12:13,760 --> 00:12:18,800
you to think like a computer and be extremely logical so I think that's maybe why I fell in love

133
00:12:18,800 --> 00:12:25,120
with it as a bachelor student as an undergrad that's awesome that's awesome so tell us a little bit

134
00:12:25,120 --> 00:12:28,960
more about kind of where machine learning comes into the picture when you're thinking about

135
00:12:28,960 --> 00:12:37,440
haptics yeah so I mean machine learning is a wonderful tool especially for understanding patterns

136
00:12:37,440 --> 00:12:41,200
that are too complex for us to see with the eye I come from more of a mechanical engineering

137
00:12:41,200 --> 00:12:46,400
background we believe in physics we believe in Newton's second law and lots of things we can write

138
00:12:46,400 --> 00:12:52,320
a some we can write out equations of motion and then try to predict how things will behave but

139
00:12:52,320 --> 00:12:59,520
I think we have to make a lot of assumptions like linearity or yeah we have to sometimes you have

140
00:12:59,520 --> 00:13:05,200
to simplify things so much to make these equations work and like all models all of these like even

141
00:13:05,200 --> 00:13:09,360
no matter how beautiful they are they're all going to be wrong at some point in the real world

142
00:13:09,360 --> 00:13:16,960
is dirty and messy and complicated and not linear and not even not very well paved and so then if

143
00:13:16,960 --> 00:13:22,720
we want to have a robot or a system that's going through the world and physically interacting with

144
00:13:22,720 --> 00:13:28,000
things for example we work on designing and creating tactile sensors that incorporate machine

145
00:13:28,000 --> 00:13:33,520
learning in the flow we can do much more if we can free ourselves of some of these really rigid

146
00:13:33,520 --> 00:13:38,240
physical assumptions and open things up and then then we need a tool to really understand those

147
00:13:38,240 --> 00:13:45,920
complicated mappings and nonlinear transformations and yeah so the very very first project I did

148
00:13:45,920 --> 00:13:51,200
that involved machine learning was a DARPA funded project when I was an assistant professor at

149
00:13:51,200 --> 00:13:55,040
the University of Pennsylvania it was a collaboration with Trevor Daryl who's a computer science

150
00:13:55,040 --> 00:13:59,440
professor at Berkeley Peter Biel and Tom Griffith and some other people were also on the bigger

151
00:13:59,440 --> 00:14:06,080
project but I worked closely with Trevor on it was a grounded language acquisition for robotics

152
00:14:06,080 --> 00:14:12,240
so can we enable robots to associate words with their physical sensations and there were other

153
00:14:12,240 --> 00:14:17,280
cool parts of the project where they were working on verbs like seeing actions and nouns but we

154
00:14:17,280 --> 00:14:22,320
worked on adjectives like how you would describe how things feel through the sense of touch

155
00:14:23,600 --> 00:14:29,440
and we installed at the time state-of-the-art tactile sensors in a PR2 which is a humanoid robot

156
00:14:29,440 --> 00:14:34,480
and had it touch a set of about 60 objects that had many different physical properties and it

157
00:14:34,480 --> 00:14:39,840
touched them in different ways and we also had humans come and touch the exact same 60 objects

158
00:14:39,840 --> 00:14:44,480
and describe them in words and rate them with words so then we could pick out a particular object

159
00:14:44,480 --> 00:14:51,440
from this data set this set of objects it's called the pen haptic adjective corpus I think

160
00:14:51,440 --> 00:14:56,800
have pen haptic adjective corpus and we would know what words people describe and also the frequency

161
00:14:56,800 --> 00:15:00,880
with which different people describe the words and then we could have the robot touch the same object

162
00:15:00,880 --> 00:15:05,440
in different ways and we trained simple machine learning so we did at the time manual feature

163
00:15:05,440 --> 00:15:09,840
extraction and included support vector machines to have the robot be able to have all these

164
00:15:09,840 --> 00:15:16,320
clusters that could touch a new thing and like say is this slippery is this nice is this scratchy

165
00:15:16,320 --> 00:15:25,840
is this soft and that kind of opened my eyes to how much we could do beyond like me just looking at

166
00:15:25,840 --> 00:15:31,920
the signals or trying to write how can I write a formula for nice or scratchy and would I even want

167
00:15:31,920 --> 00:15:37,120
to sit there and try to come up with formulas it was painful enough to have to come up with features

168
00:15:37,120 --> 00:15:42,640
and now we can automatically figure out what features are and maybe go just end to end but

169
00:15:42,640 --> 00:15:47,040
as a time even that much was really powerful that we could then bring new objects to the robot and

170
00:15:47,040 --> 00:15:51,520
have it touch it and it would give labels it would tell us what it what would it how would it describe

171
00:15:51,520 --> 00:15:58,400
this object and it's actually pretty good and I thought this this is fun we should do more stuff

172
00:15:58,400 --> 00:16:07,520
like this and so that's where it began that's awesome so much of the emphasis and you alluded to

173
00:16:07,520 --> 00:16:15,520
this earlier in machine learning and robotics is around vision and the application of computer

174
00:16:15,520 --> 00:16:23,280
vision to all different aspects of robotics from you know local motion to grasping to manipulation

175
00:16:23,280 --> 00:16:34,880
are there things that you're working on that are you know to what degree have we kind of integrated

176
00:16:34,880 --> 00:16:41,520
vision and haptics is that an area of active research in the field it definitely is I hope that

177
00:16:41,520 --> 00:16:45,280
robot I think robots are now and there will be increasingly multi-sensory the section with the

178
00:16:45,280 --> 00:16:51,360
workshop that I was in that I clear was about with some multimodal embodied robot multi-modal

179
00:16:51,360 --> 00:16:57,520
embodied intelligence or learning learning I think different senses that a robot or a human has

180
00:16:57,520 --> 00:17:02,240
provide different kinds of information from the world and I'm convinced not everyone is convinced

181
00:17:02,240 --> 00:17:08,800
that that having some touch sensing and probably audio also will greatly benefit agents robotic

182
00:17:08,800 --> 00:17:12,800
systems in the future they have to have vision there are a few different places where there is

183
00:17:12,800 --> 00:17:19,920
crossover or synergy and one I briefly alluded to is we're using we have created a tactile sensor

184
00:17:19,920 --> 00:17:24,720
it's under review now so I can't talk too much about it I showed a little video just with some

185
00:17:24,720 --> 00:17:30,560
preliminary results during my talk at ike there and here we're actually using a camera like many

186
00:17:30,560 --> 00:17:35,120
others have been to doing a vision-based tactile sensor and we're basically then we have a soft

187
00:17:35,120 --> 00:17:40,080
structure this is joint work with gare of martes and his PhD student Juan Boson at this to

188
00:17:40,080 --> 00:17:45,120
tubing inside of our institute mexican super intelligent systems and so we built a soft structure

189
00:17:45,120 --> 00:17:51,760
that has a camera that's looking inside and can see the deformations of where soft sensors

190
00:17:51,760 --> 00:17:57,200
being touched and many others have made beautiful maybe thin or other shaped sensors and we

191
00:17:57,200 --> 00:18:01,440
tried to make one that was basically the shape of your human thumb so three-dimensional and can

192
00:18:01,440 --> 00:18:07,920
feel on all sides and here we use a camera to capture simultaneously the deformation feel but

193
00:18:07,920 --> 00:18:12,240
not even the deformation feel just the pixel values all over what's seen inside with some structured

194
00:18:12,240 --> 00:18:17,760
lighting and then we Juan Boson Gare created a beautiful deep learning architecture and we have

195
00:18:17,760 --> 00:18:21,840
a little robot that goes around a pokes on the finger and we get all this data to know exactly

196
00:18:21,840 --> 00:18:25,840
where it's been touched when and it can then generalize and we can touch it on the outside and it

197
00:18:25,840 --> 00:18:31,120
can show us a map of all the forces it's feeling and not just normal direction indented but

198
00:18:31,120 --> 00:18:36,160
sheer also so if you push in and you slide along it it can feel you're pushing up or down

199
00:18:36,160 --> 00:18:40,720
it can tell if you're touching it in a large area or small area if it's touching something in that

200
00:18:40,720 --> 00:18:47,040
way and so here's an example where we're taking great stock commercial off-the-shelf good camera

201
00:18:47,600 --> 00:18:52,560
and then adapting deep learning like vision-based deep learning but the thing we're learning is

202
00:18:52,560 --> 00:18:57,120
something haptic and it actually works pretty well and I think some other researchers around the

203
00:18:57,120 --> 00:19:02,320
world have done similar things and then certainly like I was already saying before robot should

204
00:19:02,320 --> 00:19:09,120
combine what they see in the world with what they feel I'm not working in that so much right now

205
00:19:09,120 --> 00:19:17,520
but accepting rather primitive ways but I think that's beneficial for sure and can you speak to

206
00:19:17,520 --> 00:19:26,320
the with that previous project you mentioned how that data that or that model might be used so

207
00:19:26,320 --> 00:19:31,760
you've got the the robot kind of grasping this thumb-shaped thing and you're using computer vision to

208
00:19:31,760 --> 00:19:40,000
um I maybe didn't explain it well so there's the the sensor eventually will be mounted on a robot

209
00:19:40,000 --> 00:19:45,760
but in the beginning it's just sitting on like on a bench and then another robot like a

210
00:19:45,760 --> 00:19:51,680
sort of 3d printer a gantry comes around and pokes it and we hope to mount we plan to I have a

211
00:19:51,680 --> 00:19:57,840
new code-wise PhD student with Garen who will both be working to miniaturize the sensor and adapt it

212
00:19:57,840 --> 00:20:02,640
maybe bring in other modalities and then also to put it on a robot so that the robot could move

213
00:20:02,640 --> 00:20:07,840
through the world and touch things and and learn so uh and be able to physically interact in a

214
00:20:07,840 --> 00:20:12,960
controlled way with things in the world and eventually maybe we imagine robots whole parts of the

215
00:20:12,960 --> 00:20:19,040
robot could be made in this way maybe the distal extremities the fingers are are the most natural

216
00:20:19,040 --> 00:20:25,520
components because you don't need that space so much for like joints and and wires to be running

217
00:20:25,520 --> 00:20:31,520
I think we'll have a combination of different technologies um deployed you don't need the same

218
00:20:31,520 --> 00:20:36,880
precision of and the same richness of tactile sensation like on your back or on your elbow as you

219
00:20:36,880 --> 00:20:43,680
do on your fingertips this is more like a fingertip finger hand um I didn't realize that you were

220
00:20:43,680 --> 00:20:51,520
developing the device that would then be the extremity um and so the idea is that the device

221
00:20:51,520 --> 00:20:58,000
that's ultimately deployed on a robot would continue to have a vision sensor in it and

222
00:20:58,880 --> 00:21:07,600
you're building a model that would um allow uh I'm ready your face here yes or no I think I'm

223
00:21:07,600 --> 00:21:12,240
not explaining myself well something maybe let me try one more time uh it's actually near the end

224
00:21:12,240 --> 00:21:16,240
of the day here in Germany so I may be uh not functioning on all cylinders we're basically trying

225
00:21:16,240 --> 00:21:20,080
to make a robot finger let's imagine we made two of them and the robot would have these in the hand

226
00:21:20,080 --> 00:21:25,520
there's a camera inside the robot's hand looking through its own fingers and seeing and yeah

227
00:21:25,520 --> 00:21:31,600
okay you get it the camera is inside the hand that is deforming the thumb or the camera's inside

228
00:21:31,600 --> 00:21:38,480
the thumb camera's inside the thumb right camera is definitely looking inside the robot's own body

229
00:21:38,480 --> 00:21:46,000
looking at seeing the skin from the inside yep and I got that and and that is what you I think the

230
00:21:46,000 --> 00:21:53,360
distinction that I was trying to draw is is the camera like a you know train time thing and then

231
00:21:53,360 --> 00:21:57,600
you've got something else or the camera is always going to be there and you're building a model

232
00:21:57,600 --> 00:22:03,760
that basically allows the computer to you know turn this video signal into something that's more

233
00:22:03,760 --> 00:22:09,280
useful from a a grasping perspective the second the camera is the transducer so we have the mechanical

234
00:22:09,280 --> 00:22:13,120
structure the concept mechanical structure that takes the physical contact and it's just passively

235
00:22:13,120 --> 00:22:19,200
deforming and then the camera sees that and then the deep learning and it's capturing frames

236
00:22:19,200 --> 00:22:24,560
over time and then each frame gets passed through this deep neural net which is based on a resident

237
00:22:24,560 --> 00:22:31,680
and then it outputs the force field so other people and also in one part of our paper we worked on

238
00:22:31,680 --> 00:22:36,240
just estimating where am I being touched like I'll put XY coordinates or in the magnitude of the

239
00:22:36,240 --> 00:22:41,200
force or something or how many like there that framework I personally it doesn't leave very far

240
00:22:41,200 --> 00:22:45,120
because what if I get touched one two three four five infinitely many I have a funny patch

241
00:22:45,120 --> 00:22:50,800
is thinking about discrete outputs it didn't really work and so we actually started this project

242
00:22:50,800 --> 00:22:57,440
in 2017 shortly after I came here sometimes important things or hard things take a long time and I

243
00:22:57,440 --> 00:23:01,200
think that you should help to have some patience and persistence and have many projects going in

244
00:23:01,200 --> 00:23:06,720
parallel so that some things will bear fruit sooner actually here you see behind me it always seems

245
00:23:06,720 --> 00:23:13,920
impossible until it's done this is one of my models and then yeah we output this map of basically

246
00:23:13,920 --> 00:23:19,040
little points each one has a force vector a 3d force vector so the robot can reconstruct

247
00:23:19,040 --> 00:23:23,600
the agent with it has this thumb this finger can reconstruct where it's being touched

248
00:23:24,480 --> 00:23:32,240
and the direction the force is happening against it so kind of like a vectorized point cloud

249
00:23:32,240 --> 00:23:38,880
yeah like a set of points and each one has a little vector that tells it is it being touched

250
00:23:38,880 --> 00:23:44,720
and in which direction is it being pushed got it yeah and it was trained so the trainee day

251
00:23:44,720 --> 00:23:48,880
that comes from a little thing poking it at all these different locations and it moves around and

252
00:23:48,880 --> 00:23:53,120
then we have all every frame with it new okay a little four millimeter diameter indenture

253
00:23:53,120 --> 00:23:57,440
touch me here or here and here and it presses down it pushes down at different force levels and

254
00:23:57,440 --> 00:24:03,440
also laterally the shear is really important and it vastly expands like the data you have to collect

255
00:24:03,440 --> 00:24:07,520
it's not just where did I get touched and how hard was it pushing but also it was translating

256
00:24:07,520 --> 00:24:14,640
laterally against my skin so okay and so that you have that external robot that's essentially

257
00:24:14,640 --> 00:24:19,920
generating the training data for your ResNet it's known ground truth of what the forces that

258
00:24:19,920 --> 00:24:28,000
it's imparting into this this new sensor that is the camera and the inside out camera that

259
00:24:28,000 --> 00:24:33,040
ultimately we want to use to create the point cloud exactly yeah and there's a force sensor

260
00:24:33,040 --> 00:24:37,680
on the robot that's probing our sensor which is called insight I should have called I sort of

261
00:24:37,680 --> 00:24:42,960
said that I shouldn't give it a name we're probing it from the outside and we know how the

262
00:24:42,960 --> 00:24:48,640
force vector and where it's being touched and yeah it's been a really fun project actually I really

263
00:24:48,640 --> 00:24:53,520
enjoy collaborating across disciplines so like I know a little about machine learning actually

264
00:24:53,520 --> 00:25:00,240
really like math I really like programming I really like making plots and drawing diagrams

265
00:25:00,240 --> 00:25:03,920
and understanding things and learning new things but it's even more satisfying that we're

266
00:25:03,920 --> 00:25:08,640
trying to fumble through alone to partner with someone really smart and a few really smart people

267
00:25:08,640 --> 00:25:13,760
and learn from them and yeah that's I think one of the most rewarding things about being a researcher

268
00:25:13,760 --> 00:25:19,440
like an academic researcher awesome awesome you mentioned the importance of having multiple projects

269
00:25:19,440 --> 00:25:25,360
going on in parallel so that you can wait out the the insights so to speak no pun intended

270
00:25:26,960 --> 00:25:32,320
what are how long it takes for an insight to arrive or yeah go ahead what are some other projects

271
00:25:32,320 --> 00:25:38,480
that you're working on in the lab oh my gosh I can't even count I have 15 PhD students eight postdocs

272
00:25:38,480 --> 00:25:43,840
three research scientists like four or five people who recently graduated or recently finished and

273
00:25:43,840 --> 00:25:51,440
have faculty positions of their own okay I'll tell you just a few one we are making a hugging robot

274
00:25:51,440 --> 00:25:56,400
it's called huggy back when my PhD student Alexis Block she was a master student with me at Penn

275
00:25:56,400 --> 00:25:59,760
she made huggy about one point oh the first one which was a lot of fun actually got a lot of press

276
00:25:59,760 --> 00:26:04,640
at the time and then for her PhD which is through our Center for Learning Systems which is joint

277
00:26:04,640 --> 00:26:12,240
with ETH she is designed and built from scratch a new hugging robot hug about 2.0 it can see users

278
00:26:12,240 --> 00:26:20,000
it adapts to you so it's soft and warm which is very important and then it embraces you at adapts

279
00:26:20,000 --> 00:26:25,360
to your body size and your position and it lets you go when you're ready to go this was our we had

280
00:26:25,360 --> 00:26:30,240
a paper ed HRI the human robot interaction conference this year on this and then we have a paper

281
00:26:30,240 --> 00:26:34,960
under review where we did some machine learning to give this robot a better sense of touch and be

282
00:26:34,960 --> 00:26:38,400
able to know what the user's doing so when you hug someone you don't just stand there and then

283
00:26:38,400 --> 00:26:44,480
like back away you might pat them on the back or squeeze them and the robot has a pneumatic

284
00:26:44,480 --> 00:26:50,720
torso so an inflated like a beach toy inflated torso that makes it soft and we heat it up

285
00:26:51,600 --> 00:26:57,680
but then it has a microphone and a pressure sensor inside the torso so we collected a bunch of

286
00:26:57,680 --> 00:27:02,400
training data where people come in and enter into hug with a robot and then we told them to pat

287
00:27:02,400 --> 00:27:08,560
the robot or squeeze it or rub its back or do nothing and we have data I don't know from 20

288
00:27:08,560 --> 00:27:13,440
something people doing these things in various states and we also had the robot respond like pat

289
00:27:13,440 --> 00:27:18,240
you back or squeeze you and got a mapping of like what people prefer people really like being

290
00:27:18,240 --> 00:27:23,120
squeezed by a robot and then we have what we call hug you about 3.0 where it does this in real time

291
00:27:23,120 --> 00:27:29,280
so it's feeling it's this microphone and pressure data in real time and making judgments like

292
00:27:29,280 --> 00:27:35,280
what did the user do and then deciding how to respond and it's pretty fun to hug so that's honestly

293
00:27:35,280 --> 00:27:40,800
I know people you'll be surprised hugging robots we have another study that we just finished

294
00:27:40,800 --> 00:27:44,960
that I can't talk too much about we're still analyzing the data but it looks like people might even

295
00:27:44,960 --> 00:27:49,840
sometimes prefer hugging a robot to hugging a person because there's no social pressure if you

296
00:27:49,840 --> 00:27:58,960
hug a robot you don't want to worry it's going to like judge you anyway I'm imagining the abstract

297
00:27:58,960 --> 00:28:06,240
of the paper saying something like hug you about 3.0 achieve superhuman performing and hug

298
00:28:06,240 --> 00:28:15,760
detachment priming or something oh yeah we're working on hugging robots it has been surprising

299
00:28:15,760 --> 00:28:21,120
a fun and also I mean Alexis Black and I have had so much fun she's going to graduate soon

300
00:28:21,120 --> 00:28:26,400
and she'll start a postdoc in the US and hopes for a faculty crew of her own but this is one of

301
00:28:26,400 --> 00:28:30,640
the most rewarding things is getting a young scientist she started working with me actually when

302
00:28:30,640 --> 00:28:38,320
she was an undergraduate back at Penn and then yeah now about to graduate and when we first came

303
00:28:38,320 --> 00:28:41,840
up with the idea of hugging robots we thought oh no that's too crazy and then we were like no we

304
00:28:41,840 --> 00:28:46,960
got to do it and it's been real fun we have collaborators too at ETH her co-advisors RogÃ©ic

305
00:28:46,960 --> 00:28:51,040
Gasser's and Ultramar Hiligas who've also been helping with like the computer vision for the robot

306
00:28:51,040 --> 00:28:56,400
to see and the evaluation so stay tuned for more stuff about hugging bat there's a little machine

307
00:28:56,400 --> 00:29:00,640
learning in there it has to be real time that's that's important for robot perception things have

308
00:29:00,640 --> 00:29:05,840
to be able to execute in real time perhaps it's gathering the training data what can be hard you

309
00:29:05,840 --> 00:29:09,280
have to bring real people in and have them really hug your robot and not hurt them and

310
00:29:09,280 --> 00:29:15,600
then get a big diversity people are really diverse and behave in strange ways sometimes

311
00:29:16,400 --> 00:29:19,680
let's see and then we do a bunch of other human robot interaction like robots for exercise or

312
00:29:19,680 --> 00:29:26,400
something like tactile sensing for robots we do some things on surface haptics like on a cell phone

313
00:29:26,400 --> 00:29:32,800
like understanding the physical contact between human finger and the screen I mean I can't even

314
00:29:32,800 --> 00:29:37,520
think about some augmented reality for robotic surgery have to be back in robotic surgery

315
00:29:37,520 --> 00:29:46,160
I don't even know like a lot of stuff and so the machine learning primarily come into play

316
00:29:46,160 --> 00:29:54,960
when you're integrating other senses or doing multimodal work or is there a a degree to which

317
00:29:55,680 --> 00:30:02,640
machine learning has become kind of part of the you know classical or part of the the standard

318
00:30:02,640 --> 00:30:10,000
uh processing you know work floor or tool chain for haptic data if there's enough standardization

319
00:30:10,000 --> 00:30:15,760
to even say something like that I mean machine learning plays a role in many of our projects not

320
00:30:15,760 --> 00:30:20,240
in all but I mean we have some projects that are really hardware focused like design and creation

321
00:30:20,240 --> 00:30:26,640
but I think it plays a role in many of our projects and sometimes it's not even rewrote the machine

322
00:30:26,640 --> 00:30:30,560
learning so for example we have a paper at Ikra the International Conference on Robotics

323
00:30:30,560 --> 00:30:36,240
in our nation this year about the robot interaction studio which we call a platform for unsupervised

324
00:30:36,240 --> 00:30:41,600
HRI but not unsupervised like a machine learning person would think we want unsupervised like no

325
00:30:41,600 --> 00:30:46,800
supervisor no experiment or no coach in the room just you had people come in and play with our

326
00:30:46,800 --> 00:30:53,040
it's a Baxter robot so it's a humanoid robot with two arms and um we used a commercial marker

327
00:30:53,040 --> 00:30:59,040
and less motion capture system called capturing um it's wonderful. Baxter are on the person

328
00:30:59,040 --> 00:31:03,600
is playing with the Baxter. You don't have to wear anything it's in the room so there we put 10

329
00:31:03,600 --> 00:31:10,080
cameras in the room. Oh okay. And we basically function like Baxter's eyes and it's constantly

330
00:31:10,080 --> 00:31:14,640
so there's you walk in the room you don't have to put anything on as a user and it fits a skeleton

331
00:31:14,640 --> 00:31:20,400
to you in like 20 seconds and then Baxter knows where you are in the room and the pose of your body

332
00:31:20,400 --> 00:31:26,880
and we programmed only for the paper at Ikra um we programmed only that the robot we had Baxter

333
00:31:26,880 --> 00:31:31,040
always track you in the room so that you know it knows where you are and it's just always facing

334
00:31:31,040 --> 00:31:37,440
you people respond it has a smiley face and uh and then we had it do a sequence of like commands

335
00:31:37,440 --> 00:31:42,080
like it was a coach like it would point around the room and it lift its arms up and it would like

336
00:31:42,080 --> 00:31:45,760
put its hand up like it wanted you to come to it and then we just looked at did people do those

337
00:31:45,760 --> 00:31:49,680
kinds of things and then looked at statistics of did they walk around the room did they mimic the robot

338
00:31:49,680 --> 00:31:53,040
and we could make heat maps and like where they put their hands and where they went in the room

339
00:31:53,040 --> 00:31:58,880
and actually people are cues which were designed just with intuition were um we're actually

340
00:31:58,880 --> 00:32:03,360
reasonably successful and people were remarkably interested in the robot and had a fun time playing

341
00:32:03,360 --> 00:32:09,600
with it and um then in data that hasn't been published yet we had the robot also notice what the

342
00:32:09,600 --> 00:32:14,000
person is doing and try to correct them or guide them and give them feedback on like and that

343
00:32:14,000 --> 00:32:17,200
became even more interesting because then there's this loop the robot I can see what you're doing

344
00:32:17,760 --> 00:32:22,080
and can like come back try to say oh yeah you're you're I'm doing this but you're doing something

345
00:32:22,080 --> 00:32:26,560
else and it's like no lift your arms up like we're supposed to be doing the sexual sacrifice

346
00:32:26,560 --> 00:32:31,440
and we did this with like no speech no instructions to the users um but here's an example where we're

347
00:32:31,440 --> 00:32:37,520
using this commercial marketless motion capture system that has deep learning in in it uh to

348
00:32:37,520 --> 00:32:42,400
deliver really great perceptual real-time perceptual capabilities to a robot robotic system

349
00:32:42,400 --> 00:32:47,680
and then using that to create new experiences um another example i mentioned briefly um

350
00:32:47,680 --> 00:32:54,560
augmented reality in um robotic surgery yeah we also are doing some computer vision on like what the

351
00:32:54,560 --> 00:32:59,440
so someone else trained uh the neural network but we can do computer vision on what the surgeon

352
00:32:59,440 --> 00:33:04,160
is seeing to figure out where the tools are and then use that to create interactive functionality so

353
00:33:04,160 --> 00:33:08,480
those are places where we're really grateful that other wonderful researchers or experts in

354
00:33:08,480 --> 00:33:13,520
machine learning and vision have delivered capabilities that we can then integrate and then um into

355
00:33:13,520 --> 00:33:17,760
interactive systems and then other times the machine learning is more at the core of what we're

356
00:33:17,760 --> 00:33:22,720
doing and not typically it is more on when we're designing and creating tactile sensors for robots

357
00:33:22,720 --> 00:33:29,200
or interpreting information from tactile sensors that robots already have um yeah

358
00:33:30,560 --> 00:33:39,040
nice nice um is part of your research or to what degree i guess i should ask is part of your

359
00:33:39,040 --> 00:33:46,880
research focused on the kind of the softer side of human robot interaction uh maybe for context

360
00:33:46,880 --> 00:33:53,280
i've had really interesting conversations on related topics to this with uh iana Howard for

361
00:33:53,280 --> 00:34:00,160
example talking about this um you know what i think of as the this deference relationship that

362
00:34:00,160 --> 00:34:09,040
she's observed between humans and robots also Kate darling um uh studies this as well uh is that uh

363
00:34:09,040 --> 00:34:14,480
is that a explicit focus in your research or something that you observe while doing other things

364
00:34:15,120 --> 00:34:20,560
well just as a side note it's great you've talked to iana and Kate i i know iana and greatly admire

365
00:34:20,560 --> 00:34:24,400
her and have read an article recently about Kate and i would love to get to meet her someday

366
00:34:24,400 --> 00:34:28,720
and it was really inspired by her advice uh that we should treat robots and we should have

367
00:34:28,720 --> 00:34:34,960
our paradigm more that they're like animals than like people we do hri it's like i actually let

368
00:34:34,960 --> 00:34:40,000
the topics that we study in my group mostly be guided by my students interests and i'm relatively

369
00:34:40,000 --> 00:34:44,160
flexible like there's various things i'm interested in i can get excited about a lot of stuff i have

370
00:34:44,160 --> 00:34:48,640
ideas not a lot of things and i have had over the last few years especially since i moved from pen

371
00:34:49,520 --> 00:34:55,040
more students really interested in human robot interaction but and yeah we do human subject studies

372
00:34:55,040 --> 00:34:59,840
we're curious but maybe something that's different um we don't usually use like a wizard of

373
00:34:59,840 --> 00:35:05,600
os paradigm where there's a human controlling the robot i'm usually trying to make actual technical

374
00:35:05,600 --> 00:35:10,960
systems that are functioning in real time because those constraints need to be overcome and

375
00:35:11,760 --> 00:35:17,280
i want to operate kind of within what would what could work in the next five years not what

376
00:35:17,280 --> 00:35:22,480
it's going to take some superhuman human level perception is beyond what robots are delivered right

377
00:35:22,480 --> 00:35:27,680
now and so we're often challenging i'm often challenging my students and we're trying to deliver

378
00:35:27,680 --> 00:35:33,440
interactive systems that can function in real time and i'm part of i'm certainly a roboticist

379
00:35:33,440 --> 00:35:38,640
when i was a PhD student there was kind of a joke that in robotics you could get away with proof by

380
00:35:38,640 --> 00:35:44,320
video like if you made a video of your robot doing something once and like submitted a nico paper

381
00:35:44,320 --> 00:35:49,200
in that one time it was impressive like no one might ask does it always do that how many times did

382
00:35:49,200 --> 00:35:56,240
you test it and it's a bit time in cheek but being able to deliver the same quality of performance

383
00:35:56,240 --> 00:36:00,960
over and over and especially robust to different things that might change and the environment is

384
00:36:00,960 --> 00:36:09,440
really hard and in haptics where we're creating sensations for people to feel if they if you read a

385
00:36:09,440 --> 00:36:13,680
paper and it's just the technical development and algorithms or the description of the system and

386
00:36:13,680 --> 00:36:18,480
there was never an experiment of people the reader the reviewers are skeptical they don't believe

387
00:36:18,480 --> 00:36:22,240
you your papers are not going to get accepted and not only that do you have to like bring real

388
00:36:22,240 --> 00:36:26,640
naive people who aren't members of the research team in and have them do stuff and look at their

389
00:36:26,640 --> 00:36:30,720
both their behavioral their performance differences and their opinions in a very unbiased way

390
00:36:30,720 --> 00:36:36,320
it's very scientific you also need to bring your demo to the conference and let people experts

391
00:36:36,320 --> 00:36:42,800
from around the world try your system and see if they can break it or there's this like pride of

392
00:36:42,800 --> 00:36:47,360
i brought a real system hti they do this also uh bring real interactive systems and other people

393
00:36:47,360 --> 00:36:52,480
try them and that's how you can gain real credibility as a researcher by showing your systems live

394
00:36:52,480 --> 00:36:58,240
and you also learn a lot by demonstrating your systems whether it's to school kids or visiting

395
00:36:58,240 --> 00:37:04,080
researchers or at a conference i strongly encourage demonstrating or work live if you can it's

396
00:37:04,080 --> 00:37:13,600
terrifying but exhilarating awesome awesome uh you also mentioned as we're getting started uh

397
00:37:13,600 --> 00:37:20,400
uh and throughout the the interview can be your passion for mentoring and uh the

398
00:37:21,280 --> 00:37:27,760
personal side of your work and the importance of diversity and i thought maybe to wrap things up

399
00:37:27,760 --> 00:37:34,240
you can share a little bit of you know how you think about those topics yeah thanks um you might

400
00:37:34,240 --> 00:37:40,720
have noticed i'm a woman and there aren't that many women in robotics there aren't that many

401
00:37:40,720 --> 00:37:47,200
women in machine learning i'm super lucky that i had great mentors and that uh people gave me

402
00:37:47,200 --> 00:37:54,160
opportunities i worked my butt off um to do well and to get into a good university to learn what i

403
00:37:54,160 --> 00:38:00,800
could i i was brought up to really my parents had like to those who are given much much is expected

404
00:38:00,800 --> 00:38:04,560
and like what are you going to do with your life kid like where are you going to make a contribution

405
00:38:04,560 --> 00:38:07,920
they're really wonderful people this is probably the best thing they gave me wonderful access to

406
00:38:07,920 --> 00:38:11,360
education and made it clear that they were interested to see what i was going to do with those

407
00:38:11,360 --> 00:38:17,760
opportunities and i and they also did encourage me even though i was interested in topics that they

408
00:38:17,760 --> 00:38:22,560
they're both pretty technical and so maybe i was got my mom with like fix the sprinkler system

409
00:38:22,560 --> 00:38:27,680
she's a PhD in psychology in the 1970s she was programming for trend on punch cards to to

410
00:38:27,680 --> 00:38:32,720
analyze experimental data that she came she she had collected so maybe i didn't drift that far

411
00:38:32,720 --> 00:38:36,800
away from my my parents in terms of what they do but in the field that i'm in mechanical engineering

412
00:38:36,800 --> 00:38:40,240
computer science electrical engineering there aren't that many women so i'm grateful that i

413
00:38:40,240 --> 00:38:47,520
was given opportunities and i believe that young people of all all types uh deserve a chance and

414
00:38:47,520 --> 00:38:54,320
to fall in love with this field and to bring their insights and um ideas and to have yeah to have

415
00:38:54,320 --> 00:38:57,680
a chance to contribute and so that's been something that's been important to me throughout

416
00:38:57,680 --> 00:39:02,960
have a really diverse team here at Penn also at Penn and here at max plank my team is more than

417
00:39:02,960 --> 00:39:10,080
50 percent women or about 50 50 and from all over the world and also many different majors like

418
00:39:10,080 --> 00:39:17,360
many different fields of study and i i see every day the benefits of those different perspectives

419
00:39:17,360 --> 00:39:22,320
and the cooperation there can be challenges there are more cultural like people from

420
00:39:22,320 --> 00:39:28,960
vastly different cultures i remember a time i met a PhD student uh who like what had never had a

421
00:39:28,960 --> 00:39:33,600
female professor before much and his policy was he didn't touch women and but we still managed

422
00:39:33,600 --> 00:39:39,520
to have a good conversation and he did a super good job on his quals um and came to we came to really

423
00:39:39,520 --> 00:39:45,440
have a good relationship um so i i think it's a good personal growth to work in a diverse group

424
00:39:45,440 --> 00:39:52,880
and uh i am very passionate about supporting young people and often that's simply by a few things

425
00:39:52,880 --> 00:39:59,200
like asking them how they are how's it going where they want to go in life and how can i help you

426
00:39:59,200 --> 00:40:05,360
how can i help you get there where what do you what are you aiming for and what are you excited by

427
00:40:05,360 --> 00:40:11,280
and have you thought about this or where can we go with that so i don't know if that's a good answer

428
00:40:11,280 --> 00:40:17,360
to your question but at the end of the day i think the people i work with the people i help train

429
00:40:17,360 --> 00:40:21,760
the people i learn from are much more important to me than the actual work that we do i'm a total

430
00:40:21,760 --> 00:40:27,040
nerd i love the research that i do i'm very proud of our papers and our findings and our

431
00:40:28,240 --> 00:40:35,520
videos and i do love it but at the end of the day i'm even more proud of the the people and what

432
00:40:35,520 --> 00:40:39,040
they've taught me what i've learned from them and the relationships that we build together

433
00:40:39,920 --> 00:40:45,200
that's awesome that's awesome okay then thanks so much for sharing a bit about what you and your

434
00:40:45,200 --> 00:40:56,960
team are up to it's been great chatting it has been great sam thanks so much for having me thank you

