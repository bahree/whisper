WEBVTT

00:00.000 --> 00:16.480
Hello everyone and welcome to Twimble Talk, the podcast where I interview interesting people

00:16.480 --> 00:21.040
doing interesting things and machine learning and artificial intelligence.

00:21.040 --> 00:22.800
I'm your host Sam Charrington.

00:22.800 --> 00:27.600
We've got another great interview for you this time around but first a quick update

00:27.600 --> 00:30.880
on the drawing we've been running in conjunction with O'Reilly Media.

00:31.600 --> 00:37.040
As you know if you've listened previously O'Reilly Media is holding their first ever AI conference

00:37.040 --> 00:43.520
on Monday and Tuesday, September 26th and 27th in New York City. The conference will span both

00:43.520 --> 00:49.840
low-level talks on implementing AI and high-level talks on the impact of AI in society

00:49.840 --> 00:54.320
and I'm personally looking forward to speeches by AI luminaries such as

00:54.320 --> 01:00.640
Google's Peter Norvig, Facebook's Jan LeCoon, and Intel's Slash Nervana's Navine Row.

01:01.440 --> 01:05.360
And we're giving away a ticket to one lucky winner here two day.

01:06.400 --> 01:13.040
In addition, right after the AI conference on Wednesday and Thursday, the 28th and 29th

01:13.680 --> 01:19.440
is the O'Reilly Stratta Plus Hadoot World Big Data Conference which is one that I've been attending

01:19.440 --> 01:25.440
for years now. You may have heard me mention this one before. Stratta is a much bigger event

01:25.440 --> 01:30.880
and while it's not strictly focused on AI, there are tons of really interesting AI machine learning

01:30.880 --> 01:36.720
talks at Stratta as well. Along with talks focusing on what I consider to be the core topics of that

01:36.720 --> 01:44.080
event, data infrastructure and data engineering. And O'Reilly has been kind enough to offer us

01:44.080 --> 01:49.840
a ticket to Stratta as well, which we'll be giving away today. So about that giveaway.

01:50.640 --> 01:56.480
If you went ahead and entered into the contest via either Twitter or the Twimlai.com website

01:56.480 --> 02:02.080
before the cutoff date, your name or Twitter ID went into a spreadsheet and you actually

02:02.080 --> 02:07.760
had a pretty good chance of winning as far as giveaways go. I chose winners using a random

02:07.760 --> 02:14.320
number generator to pick four numbers in the range of my spreadsheet rows. The first winner

02:14.320 --> 02:21.520
who was lucky number 17 is Lance Pool who entered via Twitter. Lance gets to choose either conference

02:21.520 --> 02:28.800
ticket for his prize. The second prize winner is Yinka also from Twitter and he gets the ticket

02:28.800 --> 02:35.040
remaining after Lance's choice. I've also chosen two runner-ups who may be called upon to

02:35.040 --> 02:41.440
fulfill the duties of one of our winners if either of them can attend the event. Our first runner-up

02:41.440 --> 02:49.040
is Samuel W and our second runner-up is Dennis A who both happened to have entered via the Twimlai.com

02:49.040 --> 02:56.560
site. If you hear this any of you please reach out to me to claim your prize. Now if you didn't

02:56.560 --> 03:02.800
win it's not too late to save 20% on your registration for either conference. You can do that by using

03:02.800 --> 03:10.640
the registration code PCTWIML when registering and I'll include a link to the registration page

03:10.640 --> 03:17.440
in the show notes. On behalf of the podcast and our partner O'Reilly thank you to everyone who

03:17.440 --> 03:29.760
entered and now on to the show. All right folks I am super excited to bring you this interview.

03:29.760 --> 03:35.920
My guess this time is Charles Isbel Jr., Professor and Senior Associate Dean in the College of

03:35.920 --> 03:42.400
Computing at Georgia Institute of Technology. Charles and I go back a bit and in fact he's the first

03:42.400 --> 03:49.360
AI researcher I ever met. His research focus is what he calls interactive artificial intelligence,

03:49.360 --> 03:54.640
a discipline of AI specifically focused on the interactions between AI's and humans.

03:54.640 --> 04:00.160
Charles and I spent a good chunk of time in our interview exploring what this means and some of

04:00.160 --> 04:05.200
the interesting research results in this field. One part of the discussion I found particularly

04:05.200 --> 04:10.160
interesting was the intersection between his AI research and the related fields of marketing

04:10.160 --> 04:16.320
and behavioral economics. Beyond his research Charles is well known in the ML and AI worlds for his

04:16.320 --> 04:22.080
popular machine learning course on Udacity which he teaches with Brown University Professor Michael

04:22.080 --> 04:28.400
Littman. In addition Charles helped launch the online masters of computer science program at

04:28.400 --> 04:33.040
Georgia Tech. We spend quite a bit of time talking about what's really missing in machine learning

04:33.040 --> 04:38.960
education and how to make it more accessible. Of course I'll be linking to Charles and the

04:38.960 --> 04:45.440
resources we mentioned in the show notes which you'll be able to find at twomolei.com slash talk

04:45.440 --> 04:53.920
slash four and now on to the interview. All right everyone so I'm here with Charles Isbell. Charles

04:53.920 --> 05:00.320
is senior associate dean and professor at Georgia Tech and actually Charles and I go way back.

05:00.320 --> 05:05.040
So this has been a weird conversation because we're already like 20 minutes in and just getting

05:05.040 --> 05:10.800
started with the interview. Charles say what's up to everyone and we'll get started.

05:11.680 --> 05:15.360
What's up everyone how are you doing? I'm happy to be here. I'm happy to be having this conversation.

05:15.360 --> 05:23.120
Awesome well thank you so much for joining us for this interview now. I think we figured out that

05:23.120 --> 05:29.760
that it's been like 20 something years since we met and that was pretty interesting in that

05:29.760 --> 05:39.120
we were roommates during a I guess summer internships at Bell Labs and that was when you were at

05:39.120 --> 05:46.640
MIT and studying AI. Tell us a little about your experience at MIT and what you said you're in the

05:46.640 --> 05:53.520
famous AI lab there right? I was although the AI lab no longer exists it merged with the laboratory

05:53.520 --> 06:01.200
for computer sciences now known as CSAIL. So I love my time at MIT and I love my time at Bell Labs

06:01.200 --> 06:07.200
and eventually AT&T labs. Sort of my journey through AI is a I don't know it's a bit of a

06:07.200 --> 06:12.800
wandering one so here I'll just give you my entire history up to now in like 15 seconds and we'll

06:12.800 --> 06:17.840
see how that goes. So as you can tell by my accent I was born in Chattanooga Tennessee but my

06:17.840 --> 06:22.400
earliest memory is arriving in a moving truck at the age of three and a half in Atlanta so I think

06:22.400 --> 06:27.120
of myself as being from Atlanta. But very very early on I cared a lot about computers and computer

06:27.120 --> 06:31.040
science and I knew when I was eight years old that I was going to do computer science although I

06:31.040 --> 06:35.040
didn't know what it was. I knew I was going to be a professor although I didn't know what it was

06:35.040 --> 06:39.360
and I knew I was going to do AI even though I had no idea what that was. Something about building

06:39.360 --> 06:45.120
robots and yeah eight years old. You know it took me a very long time to realize that not everybody

06:45.120 --> 06:49.040
thought they knew what they wanted to do and they were eight years old. I think I was probably a

06:49.040 --> 06:54.720
senior in college before I realized this but I had always sort of wanted to to build intelligent

06:54.720 --> 06:59.040
things although I couldn't have articulated that way when I was eight years old but I always wanted

06:59.040 --> 07:02.480
to build smart things I always thought I thought the computers were great at least what I thought

07:02.480 --> 07:07.600
computers were and I basically just wanted to build you know an intelligent friend that's

07:07.600 --> 07:12.640
basically what I was into at the time and so everything I kind of did from at that point on was

07:12.640 --> 07:20.000
about that. My actual first encounter with Bell Labs long before we met I was things the summer

07:20.000 --> 07:26.480
before ninth grade so I was 13 years old or so and I built a computer at Bell Labs as a part of

07:26.480 --> 07:31.040
this the summer science program. What I say I built a computer I mean there was a kit and another

07:31.040 --> 07:35.200
engineer did all of the work while I stood there and watched it but it felt like I was building

07:35.200 --> 07:45.200
the thing. It was a time X and Claire T 1000 and I remember. Yeah it was a little chicklet thing

07:45.200 --> 07:49.440
and it didn't have an on-off switch so when you turned it off you had to unplug it. It was great

07:50.320 --> 07:57.280
and the first program I ever wrote was a piece of code that would fill up the screen with inverse

07:57.280 --> 08:02.240
spaces and it ran out of memory before it could finish doing it and that was my introduction to

08:02.240 --> 08:07.280
real computer so you know that that's what I figured I needed to fix that and so that whole summer

08:07.280 --> 08:12.000
we spent well the two or so weeks that I was there for that program I spent a lot of time trying

08:12.000 --> 08:16.080
to figure out how how to make computer smart and how to make them do what you wanted to do and it

08:16.080 --> 08:21.040
just verified for me that that's what I wanted to do for all of my life so I kind of dove in from

08:21.040 --> 08:25.360
there and I kept getting you know bigger and better computers and convincing my mom that you know

08:25.360 --> 08:30.480
an Apple 2GS was the right thing and it was the best thing she could do for my education she kind

08:30.480 --> 08:35.360
of nodded politely eventually gave me the things that I wanted and I sort of moved through and one

08:35.360 --> 08:40.800
of the advantages of knowing what you want to do with your life is that you sort of moved towards it

08:40.800 --> 08:46.000
there's some disadvantages we can talk about those but really admit that you know I knew I wanted

08:46.000 --> 08:50.720
to go to Georgia Tech because I wanted to stay in Atlanta and I thought that it was the best place

08:50.720 --> 08:57.280
for me to be so I went to Georgia Tech as an undergrad I completely dove into AI didn't do a lot

08:57.280 --> 09:01.520
of research at the time because that you know in the the 1980s it was a little there weren't as many

09:01.520 --> 09:06.400
places where you could do the kind of research that you can do now as an undergrad that no matter

09:06.400 --> 09:10.560
sort of what you're into and then decided well there was basically one place for me to go to grad

09:10.560 --> 09:19.840
school and I applied to MIT and I went to MIT and I wrote this long essay about building robots and

09:19.840 --> 09:24.400
trying to make them smart and and trying to make certain that they wouldn't run out of memory and

09:24.400 --> 09:32.080
it was a it was a lot of fun so I ended up going to MIT immediately started diving into machine

09:32.080 --> 09:36.480
learning which at the time was sort of new for me I knew about AI and I knew I wanted to build robots

09:36.480 --> 09:42.000
but it didn't occur to me that you needed to do something separate to make machines learn and

09:42.000 --> 09:47.040
I decided almost immediately once I was exposed to it that this was the central question you couldn't

09:47.040 --> 09:51.360
be smart unless you could learn right and our machines were never going to be able to do the

09:51.360 --> 09:55.920
interesting things that I wanted them to do when I was eight nine years old unless they were smart

09:55.920 --> 10:00.800
enough to learn how to do them on their own and so I dove into that became a part of the AI lab

10:01.600 --> 10:06.480
went through a couple of advisors I'm still good friends with with all of them and eventually

10:06.480 --> 10:12.880
ended up where I was the side story where we met is I at the same time that I was going through grad

10:12.880 --> 10:17.840
school I got to go to bill labs every summer so part of this this fellowship program you know

10:17.840 --> 10:23.760
all about this of course and there I did a lot of really interesting things in AI that had

10:23.760 --> 10:29.680
absolutely nothing to do with what I was doing in grad school but it was so interesting what they

10:29.680 --> 10:33.520
were doing they're trying to build these knowledge representations and kind of really understand

10:33.520 --> 10:38.080
how it is you could think and you could represent thought that I just you know at the time it felt

10:38.080 --> 10:41.680
okay that I wasn't making progress in grad school because I was still getting to do these cool

10:41.680 --> 10:49.520
things and so by the time we met I was doing six months out of the year at Bell Labs and six

10:49.520 --> 10:54.960
months out of the year at MIT more or less oh wow I don't think I realized that at the time

10:55.840 --> 11:00.560
yeah because I take four and a half months over the summer I'd start before everyone else and I

11:00.560 --> 11:07.200
would end after everyone else and I would go back during the winter breaks okay okay uh so

11:07.200 --> 11:15.360
the I think the time that you kind of came up in AI was during the quote unquote AI winter

11:15.360 --> 11:20.720
is that right more or less yeah I was just sort of at the tail end of the AI winter nobody told

11:20.720 --> 11:27.280
me that I didn't figure that out until much later so how was that impacted your and your

11:27.280 --> 11:34.160
contemporaries perspective on AI and and the work you've done and how do you like what do you

11:34.160 --> 11:42.400
think about the current popularity of AI and where it's all going so I think basically

11:43.200 --> 11:48.160
what it's mainly done is it the people who are about my age and a little bit older who

11:48.160 --> 11:51.840
live through the AI winter I think basically spend a lot of their time wondering when the next

11:51.840 --> 11:56.720
AI winter is going to come so a lot of us are very very sort of naturally and reflexively worried

11:56.720 --> 12:02.240
that we're overhyping what's going on right it was it wasn't that it was difficult to get funding

12:02.240 --> 12:06.160
it wasn't that it wasn't it was difficult to do work it wasn't that there weren't people

12:06.160 --> 12:10.880
interested in the problem that we're interested in it's that any minute now the federal government

12:10.880 --> 12:14.240
would take away all of the funding and we would you know we would go from having 10 graduate

12:14.240 --> 12:18.000
students to having two graduate students and I kind of think that little fear is always there

12:18.000 --> 12:22.720
in the in the back of our heads and we find ourselves thinking please stop overhyping deep

12:22.720 --> 12:27.760
neural networks or you know getting people convinced that we're gonna uh we're gonna build

12:27.760 --> 12:32.800
the next data or the you know the next android and self-driving cars and any minute it can all kind

12:32.800 --> 12:37.120
of go wrong so I think it's probably made us somewhat more cautious at least it's made me

12:37.120 --> 12:40.720
somewhat more cautious and trying to think a little bit about the hype that's sort where

12:40.720 --> 12:46.160
where it's kind of driven me uh but you know the other advantage of being a part of you a part

12:46.160 --> 12:50.480
of sort of AI when it was during the AI winter is that you knew that you and the people you were

12:50.480 --> 12:54.720
talking to were in it because you were truly passionate and motivated about solving the problem

12:54.720 --> 12:58.960
as opposed to starting a company that would make you really rich or you know this is the hot thing

12:58.960 --> 13:03.200
you were doing it because you you actually cared about it and and I think that you know that's

13:03.200 --> 13:06.640
important right certainly when you're when you're doing research you have to be passionate about the

13:06.640 --> 13:10.800
the things that you you're doing and really believe that somehow it's gonna get you someplace

13:10.800 --> 13:19.200
interesting. And so do you think the fear notwithstanding do you feel like the is the industry

13:19.200 --> 13:25.200
structured in the same way such that the risk is the same or is it different and in particular

13:25.200 --> 13:32.480
I'm thinking about is there you know the funding sources more distributed now is the level of

13:32.480 --> 13:39.120
industrial activity you know more greater now or is it all you know from a research perspective

13:39.120 --> 13:43.840
all still fundamentally the government funding everything and you know when he decided to change

13:43.840 --> 13:52.480
there when the winds changed there at all collapses. Well I think structurally two things have

13:52.480 --> 13:59.440
happened one is computers computing and that sort of way of of crunching things and data are now

13:59.440 --> 14:04.480
ubiquitous they're everywhere so industry is deeply into this it's not going away

14:05.120 --> 14:10.960
Google exists right and everything is driven by data and it turns out that the parts of AI the

14:10.960 --> 14:15.760
parts of computer vision the all the sort of pieces of of building intelligent things they're

14:15.760 --> 14:20.560
driven by data now and since we everyone has access to data and everyone has access to

14:21.440 --> 14:26.480
computing everyone has access to really fast machines I'm not worried about sort of it structurally

14:26.480 --> 14:31.680
going away in fact the problem is sort of the opposite it's that everyone has a piece of it now

14:31.680 --> 14:37.040
it's it's driven as much by commercial interest as it is by sort of pure research and so really the

14:37.040 --> 14:42.240
difficult thing in some ways is that there's so many opportunities to do the what I would have

14:42.240 --> 14:45.680
thought of as AI what we would talk about is machine learning and those kinds of related things

14:45.680 --> 14:51.840
that it's easy for things to become diffuse in a way that wasn't true 25 years ago I don't think

14:51.840 --> 14:57.520
this is a bad thing I mean the the fact that Facebook exists the fact that Google exists the

14:57.520 --> 15:01.600
fact that everything is about your about data and about you know sort of modeling what people are

15:01.600 --> 15:05.840
doing and what things are happening is definitely a good thing and it does mean that there's always

15:05.840 --> 15:10.640
going to be funding for some piece of it even if it's not being called AI or it's not being

15:10.640 --> 15:16.560
called machine learning the kind of ideas metastasized so I'm not really worried about it going away

15:16.560 --> 15:24.000
the only thing that worries me is that people are concerned that bad things will happen because

15:24.000 --> 15:27.840
of what we're doing and for good reasons right they're concerned about their privacy we now have

15:27.840 --> 15:32.480
all these ability it's ability to track everything that you do I guarantee you Google is well aware

15:32.480 --> 15:35.760
that you and I are having this conversation right now they probably know what we're going to say

15:35.760 --> 15:40.560
before we say it you know they've got more data on us than you can possibly imagine and truthfully

15:40.560 --> 15:47.120
we I'm not entirely sure that we mind Facebook knows everything about us their companies out there

15:47.120 --> 15:51.120
neither was it heard of who know kind of everything about us as a people worried about privacy they're

15:51.120 --> 15:56.560
also worried about cars running off the road and killing other people they're worried about robots

15:56.560 --> 16:03.040
you know rising up in terminator style killing us all so the the kind of fears is the hype has

16:03.040 --> 16:07.120
actually gotten to the point of not what you haven't given us what we promised it's that you've given

16:07.120 --> 16:13.200
us more than what we asked for I think that's where the danger is coming from now but in terms of

16:13.200 --> 16:17.760
funding in terms of people being interested in these problems now that's driving everything even

16:17.760 --> 16:25.680
things you don't think of as being AI or being machine learning. It's interesting that in some

16:25.680 --> 16:32.240
ways it's in some ways the industry's given more in some ways like we're still waiting like

16:32.240 --> 16:40.720
you know if you if you survey sci-fi and you know even the Jetsons you know where where sci-fi thought

16:40.720 --> 16:46.800
we would be in you know 2016 and a lot of ways where we're not there yet right like a lot of

16:46.800 --> 16:53.840
movies would have had the self-driving cars all over the street but some of this stuff it takes

16:53.840 --> 17:00.160
longer it takes longer to develop than you think and some of the stuff is happening quicker than

17:00.160 --> 17:04.800
you think. No I think well some of the things are happening that nobody ever thought about I mean

17:04.800 --> 17:08.960
you go back and you start thinking about sci-fi it wasn't self-driving cars or self-driving jetpacks

17:08.960 --> 17:15.760
right it's still haven't got my jetpack yet I'm still waiting for that and it's true we we haven't

17:15.760 --> 17:20.720
gotten the flying machines we haven't gotten the really the smart butlers that are that are

17:20.720 --> 17:24.720
taking us everywhere another thing we've gotten a lot of other things right we've got access to

17:24.720 --> 17:30.160
information that we've never had access to before we can ask questions and we'll get the answers

17:30.160 --> 17:35.360
back we can look up anything we want to we can teach ourselves we've gotten a lot more things we

17:35.360 --> 17:39.520
never thought about than we thought we would and we've gotten less of the kind of obvious things

17:39.520 --> 17:45.760
that that I think people sort of hoped that we would one day get so you know it's a mix I'm okay

17:45.760 --> 17:51.360
with that I mean I I people ask me all the time you know when are the computers going to achieve

17:51.360 --> 17:56.640
sentience and and and take over the world and I think the answer is probably never or at least

17:56.640 --> 18:01.120
probably not for a very very long time not in the way that people think about it but we're going

18:01.120 --> 18:07.520
to have very smart machines and we already do doing a whole lot of things for us that we never

18:07.520 --> 18:12.400
sort of expected them to do and the interesting thing is we won't even notice and it won't seem

18:12.400 --> 18:18.880
like that big of a deal I mean for example with the Tesla and the autonomous cars Uber and all

18:18.880 --> 18:23.600
the things that they're doing that's amazing have you ever been in one of these cars

18:23.600 --> 18:29.600
if you ever to it led to this that's amazing that you can sit in that car and it can drive you

18:29.600 --> 18:36.000
through traffic on a highway at 65 miles an hour that's it's amazing if you would ask me how

18:36.000 --> 18:41.280
you would do something like that 25 years ago I mean I can barely forget how human beings do it

18:41.280 --> 18:45.280
and in fact being on the road it's pretty clear to me lots of human beings don't do a very good

18:45.280 --> 18:50.320
job of it but that's a miracle and we barely notice right every time you get an airplane right the

18:50.320 --> 18:56.240
pilot's not flying the airplanes flying itself right and we just take this everyday miracle is just

18:56.240 --> 19:01.920
a another little thing in fact you know one of the big complaints if you're into AI right is that

19:01.920 --> 19:08.080
you never actually get credit for the cool things that you do right AI is kind of the the science

19:08.080 --> 19:14.080
and the engineering of making computers act the way they do in the movies right but one of the

19:14.080 --> 19:19.600
things that sort of tied into that is if it's got to be intelligent then it's got to be like humans

19:19.600 --> 19:24.560
and if it's got to be like humans it has to be mysterious and something we can't understand so

19:24.560 --> 19:30.000
the problem is every time we do something even if it's amazing once we know how to do it and we

19:30.000 --> 19:34.880
understand it well that can't be real intelligence and so we don't give the credit to AI so AI sort

19:34.880 --> 19:41.440
of has this problem where you you can't ever win because anything interesting you do well

19:41.440 --> 19:47.600
we understand that and that's not real intelligence so it's no longer AI yeah or it's just it's

19:47.600 --> 19:52.640
just computers right it's never this thing where you succeed it it's just oh that's not the real

19:52.640 --> 19:57.360
part the real intelligent part is this thing and then when you can suddenly do be you know people

19:57.360 --> 20:01.280
at jeopardy well that's that's not really intelligence the real intelligence is other things so you

20:01.280 --> 20:06.080
you basically just keep you know innovating your way out of out of business and so AI gets sort

20:06.080 --> 20:10.560
of smaller and smaller and smaller and what it's allowed to to call itself because the mystery gets

20:10.560 --> 20:16.640
smaller and smaller is it smaller smaller or further further well it's always sort of infinitely

20:16.640 --> 20:22.160
far away right all right it's it's it's something that we can always look for but we can never

20:22.160 --> 20:30.080
quite get to sort of Zeno's paradox of AI but there's not like a you know there's not some finite

20:30.080 --> 20:35.440
set of things that we need to do to figure out AI and we're chipping away at it and it's getting

20:35.440 --> 20:43.760
smaller and smaller it's like the goalpost is moving yeah well so I think both of those things are

20:43.760 --> 20:48.080
true I think there are a finite number of things we need to do we're definitely chipping away at it

20:48.080 --> 20:54.080
and so the stuff we need to do sort of gets smaller and smaller though it's still really big but the

20:54.080 --> 20:58.880
goalpost keep moving right we've got cars that can drive themselves more or less and now that's

20:58.880 --> 21:04.080
no longer amazing so it's got to be something else but that's that's amazing and by the way

21:04.080 --> 21:09.920
it's not just amazing it has an amazing impact on the world have you seen this I know you're on

21:09.920 --> 21:14.480
Facebook see you remember this map that was going around for a while that showed the the most

21:14.480 --> 21:21.120
common jobs in every state you remember this yeah and do you remember what the most common

21:21.120 --> 21:26.560
job is in almost every state in the US truck driver right yeah truck driver delivery person taxi

21:26.560 --> 21:32.320
driver right that's something like 42 or 44 this item the right number but it's over 40 for some

21:32.320 --> 21:36.320
reason in other states it's elementary school teacher I don't know why but but mostly it's truck

21:36.320 --> 21:43.280
driver well you know we're five years away from all the cars delivering driving themselves right

21:43.280 --> 21:49.920
Uber is it's not going to have people involved anymore my old advisor my one of my PhD advisors

21:49.920 --> 21:54.960
you know is heading the work at primary right so things are going to be delivered to as by drones

21:54.960 --> 22:00.960
and people are going to be involved anymore well that's the most common job in the country

22:00.960 --> 22:08.480
and it's going away right so the the goalpost removing the the things we have to do or getting

22:08.480 --> 22:12.480
smaller or not and people have this sort of feeling what AI is but whether or not you want to call

22:12.480 --> 22:16.880
it AI or not it's going to have a massive impact on our day-to-day lives and it's going to have

22:16.880 --> 22:20.880
a massive impact on the economy it's going to have a massive impact on sort of how we see ourselves

22:20.880 --> 22:25.520
and how we interact with one another and whether you decide that it's AI or not or it's intelligent

22:25.520 --> 22:30.320
or not whether you move the goalposts or not it's changing everything around us in deep and

22:30.320 --> 22:37.920
profound ways yeah absolutely absolutely so I want to talk about a couple of of really specific

22:37.920 --> 22:46.560
things with you and we'll take these in in turn the first is in the the realm of education

22:46.560 --> 22:53.120
and the second is in the realm of your research focus area and reinforcement learning but let's

22:53.120 --> 22:58.480
start with the first of those we got we got through your grad school experience in MIT then you

22:58.480 --> 23:10.000
went back to Georgia Tech and most recently you've been doing a lot of work in online education

23:10.000 --> 23:18.000
around machine learning maybe walk us through what you're doing there and in particular I'm curious

23:20.000 --> 23:27.920
and maybe as a bit of background here I didn't go through your entire course but I took a look at

23:27.920 --> 23:33.840
the the course that you did with Michael Litman and it was I really enjoyed the presentation

23:33.840 --> 23:42.160
having gone through a number of ML MOOCs and it made me wonder like what you know what unique

23:42.160 --> 23:50.000
views do you bring to you know teaching and or learning machine learning in AI that you surfaced

23:50.000 --> 23:54.320
in the the coursework as well as you know which of the you know are there any views you have that

23:54.320 --> 23:59.120
you think kind of go against the grain of the way people other people are approaching it

23:59.120 --> 24:07.600
yeah so so I'm glad you enjoyed the the the classes Michael and I had a ball just a total blast

24:07.600 --> 24:12.480
doing it and if you haven't you should watch the the Michael Jackson parody video we did about

24:12.480 --> 24:17.440
machine learning you get to see get this you Michael dressed up as Michael Jackson and dancing

24:17.440 --> 24:23.440
which is well worth the price of admission which is free so the you talk about this kind of

24:23.440 --> 24:27.680
interaction we have one of the things that Michael and I I tried to do is we decided that we've

24:27.680 --> 24:30.720
been wanting to do things for a long together for a long time but you know he's on one part of the

24:30.720 --> 24:34.640
country having another part of the country we wanted to do this this machine learning MOOC and

24:34.640 --> 24:38.480
and this gave us the the opportunity to do it and the way we decided to come at it was it's

24:38.480 --> 24:44.400
much like we're doing this now we said you know what education like this should be more like a

24:44.400 --> 24:50.160
podcast you should have a conversation so every time we did one of these these lectures one of us

24:50.160 --> 24:54.240
would be the professor who would try to present the material and the other person would try to be

24:54.240 --> 24:58.640
the student so the professor would do all the preparation and come up with the sort of lesson

24:58.640 --> 25:02.800
and get everything together and the student would do no preparation at all and come in cold so you

25:02.800 --> 25:09.120
know in that way it's just like regular school and we would just talk and of course he's an expert

25:09.120 --> 25:13.280
I'm an expert and this is what we do all day so it's not like we didn't really kind of understand

25:13.280 --> 25:17.680
what was going on but it turned out and I think this really does come out in the conversations that

25:17.680 --> 25:22.720
we had that we actually have very different views of what's important right so Michael is much more

25:22.720 --> 25:27.200
of a theoretician if you asked him what AI and machine learning is he might say something like

25:27.200 --> 25:31.760
computational statistics I'm much more interested in thinking about it and it's kind of practical

25:31.760 --> 25:36.640
applications and you know sort of what you can do as a practitioner to to to use these tools

25:36.640 --> 25:41.360
to to make them work and get synthesis I want people to see that this thing over here is just like

25:41.360 --> 25:45.120
that thing over there which is just like this thing over there and they're all tied together

25:45.120 --> 25:50.000
and I'm much less interested in proving in the abstract what it is that that you actually

25:50.000 --> 25:53.440
can learn in what you actually can't learn it's not that these things are important I just you

25:53.440 --> 25:57.360
know I'm just less interested in them than than Michael is and so we would spend all of these

25:57.360 --> 26:01.520
times kind of arguing some of that sometimes obviously sometimes not about what's going on

26:01.520 --> 26:05.440
and what I hope came out of that and you can tell me if you if you think it's true or not

26:05.440 --> 26:09.680
is that the student was drawn into this conversation and at least got the feeling that not only

26:09.680 --> 26:13.920
were they learning some equation or getting ready for some test or doing some assignment but that

26:13.920 --> 26:18.160
there really is a deep conversation going on about AI and machine learning and there's lots of

26:18.160 --> 26:24.400
different ways to think about it and and really that kind of gets to my larger philosophy about

26:24.400 --> 26:28.800
the way education works and why I'm so excited about the online education that's that we've

26:28.800 --> 26:35.440
been able to do to me what's really missing in education is access right the ability for people

26:35.440 --> 26:41.600
to really to participate in the in the commons that is education that is research that that is

26:41.600 --> 26:46.240
learning and one thing that I think's important for people to understand is that when you say

26:46.240 --> 26:52.000
access some people turn that into affordability you know is it cheap enough you know

26:52.000 --> 26:56.720
tuition is too high you know and that is a part of access but access is actually very different

26:56.720 --> 27:02.080
access is just the ability to be able to participate in the conversation and that if you're

27:02.080 --> 27:06.080
capable of getting through it being able to have the real opportunity to get through it

27:06.080 --> 27:10.800
affordability is only a small part of that so one of the things that we've been doing and

27:11.840 --> 27:15.920
and I'm actually quite proud of this over the last three years is we decided that we

27:15.920 --> 27:20.720
wanted to push on this idea of access and affordability and that online education in MOOCs

27:20.720 --> 27:24.400
were one way of doing it and while we're working on this this machine learning class we wanted

27:24.400 --> 27:28.880
to make it a part of something bigger and so Georgia Tech when when I was there in my senior

27:28.880 --> 27:33.840
social dean role I guess I still am and in my professor role we wanted to build an entire degree

27:33.840 --> 27:41.120
a graduate level degree that anyone who could get access to the internet and then who had the

27:41.120 --> 27:46.240
time and had the desire would be able to get through an actual full-fledged course

27:47.120 --> 27:51.920
a full-fledged and not just a course a full-fledged degree a real program and so we created this

27:51.920 --> 27:57.840
online MS program it's exactly the same as our on-campus program same requirements same degree

27:57.840 --> 28:01.680
you get through this you get a you get a master of science computer science from a top 10

28:01.680 --> 28:06.800
department and it's indistinguishable from the one that you get on campus and here's the thing

28:06.800 --> 28:13.280
that we we did two things to sort of push on this notion of access one is we decided to make it

28:13.280 --> 28:19.760
as inexpensive as possible so the entire the cost of the entire degree is something around $6600

28:20.800 --> 28:24.240
wow depending on how fast you you get through the program so somewhere between $6,000

28:24.240 --> 28:29.520
and $8,000 sort of depending upon what you do you get an entire degree that's pretty inexpensive

28:29.520 --> 28:34.160
if you came on campus and you were out of state student it cost you more like $46,000

28:34.160 --> 28:37.680
so that was the first thing that we did make it affordable but the other thing that we decided

28:37.680 --> 28:42.960
to do is we decided to admit every single student we believed who could succeed

28:44.400 --> 28:49.520
this is a pretty big deal right if you if if we think about our on-campus degree we accept about

28:49.520 --> 28:55.120
10% of our applicants why do we accept 10% of our applicants because it's all the space we have

28:55.120 --> 29:00.000
right I'd estimate somewhere between 60 and 70% of the students who applied our graduate program

29:00.000 --> 29:05.120
are above bar but we only got a room for 10% of them so only 10% of them get in and by the way

29:05.120 --> 29:11.280
it's it's it's basically a lottery right I mean you know when when you've got your place like

29:11.280 --> 29:14.640
Stanford and you're accepting four or five percent of the people coming into your undergraduate

29:14.640 --> 29:18.080
program there's no way that that four or five percent really better than the next four or five

29:18.080 --> 29:22.000
percent the four or five percent after that you're you're almost closing your eyes and just picking

29:22.000 --> 29:25.280
people right yeah and this is but what we were doing at the graduate level we don't like that

29:25.280 --> 29:30.720
for our online degree which again is the same degree as our on-campus degree at this point we're

29:30.720 --> 29:38.240
accepting about 60% of applicants okay we've gone from zero students three years ago to 4,000

29:38.240 --> 29:45.360
students this term 4,000 currently enrolled students or is that a cumulative 4,000 currently

29:45.360 --> 29:52.560
enrolled students okay wow wow that's pretty good how many on campus about two or three hundred

29:52.560 --> 29:59.200
okay in fact right by the way it's not just that we've got 4,000 students they're performing

29:59.200 --> 30:04.080
as well as the on-campus students oh by the way it's not just that we have 4,000 students

30:04.080 --> 30:09.200
who are behaving who are performing as well as the on-campus students they look very different

30:09.200 --> 30:14.160
so if you look at our on-campus degree about 85% of the applicants are far national

30:14.160 --> 30:20.160
faster majority of whom are from India following behind China so about 15% are US citizens

30:20.160 --> 30:26.960
for our online degree it's the compliment about 85 80 to 85% of the applicants are US citizens

30:26.960 --> 30:31.840
or permanent residents okay right they're in their early 30s early in mid 30s not in their

30:31.840 --> 30:39.200
earlier mid 20s most of them are working full-time they've got you know jobs mostly in IT they're

30:39.200 --> 30:44.880
not all of them they've got mortgages they've got kids and they're trying to sort of get through

30:44.880 --> 30:50.480
their day but they can't take the time to get further education or to do that thing they want to

30:50.480 --> 30:54.320
do because again they've got mortgages and kids they've got responsibilities right so what's

30:54.320 --> 30:58.560
interesting is we've done studies of this we partnered with Harvard and looked at it we think that

30:58.560 --> 31:04.400
of the people who are coming through our program almost none of them would have pursued an advanced

31:04.400 --> 31:08.160
degree otherwise they weren't they because they just simply didn't have the option they couldn't

31:08.800 --> 31:14.080
take two years off from their lives to go and pursue a degree because they had too many other

31:14.080 --> 31:18.640
responsibilities and things that they had to do but this gives them the option of doing that and so in

31:18.640 --> 31:23.120
fact the overlap between them and the people who normally we get education is almost zero

31:23.120 --> 31:28.800
current estimate is that we'll add between eight and 10% every year to the number of graduate

31:29.360 --> 31:35.040
IT workers in the United States then we otherwise would have seen and have you looked at what what

31:35.040 --> 31:40.320
they're doing afterwards how long has the program been in place and how long have you been tracking

31:40.320 --> 31:46.160
that into what degree so it's been about three years in fact I think we're beginning our we'll be

31:46.160 --> 31:50.000
we're just ending our third year now and we'll be starting our fourth year so people have just

31:50.000 --> 31:57.760
begun to graduate we had 20 people graduate two terms ago and this semester we're expecting

31:57.760 --> 32:03.120
to see closer to about 250 and we're expecting to see a steady state of closer to a thousand people

32:03.120 --> 32:09.520
graduating a year most of them already had jobs so you know usually the way you measure success you

32:09.520 --> 32:13.840
say okay the people get jobs when they graduate well most of these people already had jobs so they

32:13.840 --> 32:18.160
didn't lose their jobs I guess that's a good thing but it's hard it's hard to know what that

32:18.160 --> 32:24.640
what that impact is because the usual measures don't really make sense but they're all they all

32:24.640 --> 32:30.480
seem to be happy 97% of them said that they would you know recommend this to other people many of

32:30.480 --> 32:34.320
them do get jobs while they're in the middle of the program a lot of them get promotions and they

32:34.320 --> 32:38.560
move through you'll have to ask me in five years what the what the real impact is but right now it appears

32:38.560 --> 32:43.200
that people are happy they're getting a lot out of it some of them are able to change careers get

32:43.200 --> 32:47.440
promotions and to do things they wouldn't otherwise be able to do because they just couldn't take

32:47.440 --> 32:52.880
the time off to do it so I'm very happy with that and happy with the sort of impact it appears to

32:52.880 --> 32:59.200
be having on students let me ask you this a lot of people who listen to the podcaster somewhere along

32:59.200 --> 33:07.040
the progression of learning and and entering machine learning as a field as a profession and

33:07.040 --> 33:16.880
I'm wondering what what do you think the right set of uh set of educational tools to take advantage of

33:16.880 --> 33:25.120
right MOOCs are a piece of that um but there's obviously other pieces that go into making a full

33:25.920 --> 33:32.480
kind of a well-rounded student of machine learning in AI how do you recommend that students

33:32.480 --> 33:37.760
approach that or do you have a philosophy around that well so I sort of do and I do think it comes

33:37.760 --> 33:42.880
out in my in my class if you actually take the class as opposed to watch the lectures you get my

33:42.880 --> 33:46.480
assignments and I'll just describe my first assignment to you because I think it actually captures

33:46.480 --> 33:51.920
a lot of at least what I believe matters in becoming a either a machine learning researcher or

33:51.920 --> 33:56.640
a machine learning practitioner or even AI or more broadly speaking so here's my first assignment

33:56.640 --> 34:01.520
first assignment is go find two datasets I don't care what they are so long as they're interesting

34:01.520 --> 34:05.360
they have to be interesting by themselves and they have to be interesting together and you have

34:05.360 --> 34:11.360
to convince me that they're interesting um then I want you to implement these five or six algorithms

34:11.360 --> 34:15.440
and when I say implement I mean steal the code I don't really care you'll get any credit

34:15.440 --> 34:19.600
whatsoever for implementing and running the code you steal libraries you know go get your

34:19.600 --> 34:23.920
your favorite implementation of K&N or boosting from somewhere else I don't really care

34:23.920 --> 34:28.160
and I want you to run all of those algorithms on those two datasets and I want you to do analysis

34:28.160 --> 34:31.760
and explain to me why you got the behavior that you did why did some of those algorithms which

34:31.760 --> 34:36.640
should all work why did some of them behave better on some data on one of the datasets than the other

34:36.640 --> 34:42.160
what sort of things did you learn by applying those algorithms and doing the data analysis convince

34:42.160 --> 34:47.040
me that you thought about it convince me what experiments you would need to run in order to really

34:47.520 --> 34:51.760
get the answers to the questions and then run those experiments do all of that and then write it

34:51.760 --> 34:58.560
up in 12 pages not 13 pages not 14 pages 12 pages why do I have an assignment like that I haven't

34:58.560 --> 35:03.680
assignment like that because I think much about machine learning much about the field that we're in

35:03.680 --> 35:10.000
is really about the practice of doing it you know theoretically all of these algorithms is

35:10.000 --> 35:13.840
particularly in supervised learning they're all very similar they all can learn the same kinds of

35:13.840 --> 35:18.560
things you know but there's no free lunch right so there has to be built into what you're doing

35:18.560 --> 35:22.960
deep assumptions about your data what is you're trying to accomplish and you have to be able to

35:22.960 --> 35:27.440
surface those things so if somebody want to ask me if I wanted to really do machine learning what

35:27.440 --> 35:31.840
do I need to learn I give them two answers one you need to learn the foundations and the fundamentals

35:31.840 --> 35:35.840
yes you need to know the math you understand information theory you need to understand you know

35:35.840 --> 35:40.000
what linear algebra is you need to not flinch or somebody mentions an eigenvector an eigenproblem

35:40.000 --> 35:44.720
to you you need to get the math yes and you need to get the computing because it's a fundamentally

35:44.720 --> 35:49.600
I'm computing pace discipline and computing is not math computing is not engineering computing is

35:49.600 --> 35:54.480
not science you need to internalize the computing part of machine learning but just as important

35:54.480 --> 36:00.000
and in many ways more important I believe is you have to really dive deeply into the empirical side

36:00.000 --> 36:04.800
of it you have to get dirty with data you have to understand what the difficulties are in and

36:04.800 --> 36:08.400
answering the questions you want to answer and you have to really realize that the questions you're

36:08.400 --> 36:13.680
asking aren't necessarily the right ones most of what traps us in machine learning and in lots of

36:13.680 --> 36:18.640
other things we do are the unspoken assumptions you have to surface what those things are and I

36:18.640 --> 36:24.080
think that the best way of doing that is by getting your hands and your feet dirty so my classes

36:24.080 --> 36:30.480
are designed to do that to force you to get into a messy ill-defined situation and to work your

36:30.480 --> 36:34.880
way out of it so if you want to do data analysis if you want to do machine learning that's great it's

36:34.880 --> 36:39.920
wonderful I can think of nothing more interesting to do but you have to get out of the textbooks you

36:39.920 --> 36:45.200
have to play through the data and understand why it works the way that it does why the algorithms

36:45.200 --> 36:49.760
have the effect that they do why you can learn some things you can't seem to learn other things

36:49.760 --> 36:54.480
and that I think is actually really missing I think people either dive down the empirical side

36:54.480 --> 36:58.240
and just try to get stuff working but with no understanding of the fundamentals they don't even

36:58.240 --> 37:02.960
know how to ask the questions or they get so caught up in the fundamentals they don't worry

37:02.960 --> 37:06.960
about whether it actually works in practice or how you would actually apply your ideas

37:06.960 --> 37:15.760
and you have to do both especially in a field like machine learning they use all the

37:15.760 --> 37:20.320
the social media tools that are out there to build community to talk to each other to talk to

37:20.320 --> 37:25.440
the faculty to talk to the advisors they really build an entire community around what they're doing

37:25.440 --> 37:29.200
and really the people who are in that community do well and the people who are not a part of that

37:29.200 --> 37:35.520
community do poorly so one of the things that's important about the trips that I've been taking in

37:35.520 --> 37:39.520
the traveling around the world I've been doing is making certain that we provide the tools so that

37:39.520 --> 37:43.200
people can build local community that makes sense to them because that's how the learning happens

37:44.000 --> 37:50.640
you guys you guys might be single-handedly propping up Google Plus I'm about helping Google Plus

37:50.640 --> 37:55.600
I think people haven't been nice enough to Google Plus I've never heard of anyone else saying they're

37:55.600 --> 38:02.640
using it well there's no lag because no one else is using it so you got that nice nice nice

38:02.640 --> 38:11.440
so let's switch gears a little bit and talk about your research your research is your research focus

38:11.440 --> 38:18.160
as I understand it anyways primarily around reinforcement learning or maybe you tell me tell us

38:18.160 --> 38:28.880
what your research focus is nowadays and how you think of that area yeah so I I you know I

38:28.880 --> 38:32.960
like I said earlier I really have been into AI machine learning for a very long time and it

38:32.960 --> 38:38.080
took me a while to figure out what it was about it that I I really cared about and it was I was

38:38.080 --> 38:42.960
easier to see when I was reflecting back on it what it is that you know I found interesting what

38:42.960 --> 38:47.680
I didn't the kind of machine learning that I care about the name that we kind of give it in the

38:47.680 --> 38:53.680
field is interactive machine learning and interactive AI I sometimes refer to as interactive AI

38:53.680 --> 38:58.000
because I care about the AI problem as much as I do the the machine learning problem and what it

38:58.000 --> 39:02.960
really is is about what happens when you instead of just saying oh look here's some data and I'm

39:02.960 --> 39:06.880
going to look at that data and then I'm going to build a function and now I can do some prediction

39:06.880 --> 39:10.720
you know that you're going to have a fundamentally incremental and interactive process so I want to

39:10.720 --> 39:15.440
model human beings because I actually care about messy data and there's nothing messier than people

39:15.440 --> 39:20.400
so I want human beings to be a part of the story of how I learned and when I say that I think

39:20.400 --> 39:24.320
that people learn only through social communities or they learn best their social communities

39:24.320 --> 39:28.960
I think that's actually true for our machines as well so that ends up looking a lot like and I

39:28.960 --> 39:33.760
spend most of my time worrying about reinforcement learning so you're right about that and the

39:33.760 --> 39:37.920
reason I care about reinforcement learning is that reinforcement learning is really I think trying

39:37.920 --> 39:42.560
to do something big and hairy which is actually model what it means to be an autonomous agent so

39:42.560 --> 39:47.360
when people ask me for the one sentence description of what it is that I what it is that I do I said

39:47.360 --> 39:52.560
I care about interactive machine learning I care about building intelligent agents that have to

39:52.560 --> 39:57.840
interact with other intelligent agents perhaps hundreds of thousands of them at a time and some of

39:57.840 --> 40:02.880
those intelligent agents might be human they don't all have to be human but some of them will be

40:02.880 --> 40:07.680
and since some of them are human you can't just go around sending XML packets back and forth you have

40:07.680 --> 40:12.400
to actually engage in conversation you have to worry about the fact that human beings change over time

40:12.400 --> 40:16.640
they're inconsistent they're errone they're highly non-marcovian there's all kinds of interesting

40:16.640 --> 40:22.160
things about people and you need to be partners with people and you need to be long lived in order for

40:22.160 --> 40:26.800
you to make progress in the area so that's what I really care about I care about building a system

40:26.800 --> 40:32.560
that doesn't just predict whether you know a car is going to run into the side of the road or not

40:32.560 --> 40:36.320
but actually deals with the fact that there are several million other people on the road at the

40:36.320 --> 40:40.400
same time and you have to interact with those other people and you have to learn by talking to

40:40.400 --> 40:48.880
them and interacting with them and so reinforcement learning is a subset of that yes that's right I

40:48.880 --> 40:52.960
spend a lot of my time worrying about game theory I spend a lot of my time worrying about

40:54.480 --> 40:59.920
marketing believe it or not about social behavior and how people tend to interact and

40:59.920 --> 41:05.520
and work with one another and how you can convince them to to work with you or how you can deal

41:05.520 --> 41:09.680
with them if they're trying to work against you so it's the whole gamut of what it means to interact

41:09.680 --> 41:14.400
with other intelligent beings that have their own set of goals and and interest that might not be

41:14.400 --> 41:21.040
the same as yours so tell me you mentioned marketing tell me more about how that plays into your

41:21.040 --> 41:28.320
research or or maybe even give us an example of some of the research topics you've been looking

41:28.320 --> 41:34.240
into recently so I like the marketing question so so I spend a lot of time with a friend of my

41:34.240 --> 41:41.360
with one of my students is now a professor in North Carolina on something called drama management

41:41.360 --> 41:45.600
so the short version of drama management is well you know you've played video games right

41:46.240 --> 41:51.680
uh yep and you know the thing about video games is the interesting ones are ones where you're

41:51.680 --> 41:57.200
you know involved an entire world and an entire story so what's actually going on is that you're

41:57.200 --> 42:02.240
the person building the system for you is trying to build a story but most stories you just read

42:02.240 --> 42:06.240
and you're a passive participant of and things like games you're actually an active participant

42:06.240 --> 42:10.160
which means there's this tradeoff between your sense of autonomy and agency on the one hand

42:10.160 --> 42:14.880
and me making certain that you have a good experience or a good story so you can actually think of

42:14.880 --> 42:18.560
lots and lots of things like this you can think about conversations that you have in the interviews

42:18.560 --> 42:22.560
and a podcast is like a story where you're negotiating back and forth and trying to

42:22.560 --> 42:26.160
trying to figure out how to tell the the story that you want to tell while still allowing people

42:26.160 --> 42:30.320
to say the things that they that they need to say or that they want to say you can think about

42:30.320 --> 42:34.560
all kinds of examples like those can kind of go on for a while but the the the thing that the

42:34.560 --> 42:41.520
thing there is that it turns out that because your player or the person who's participating

42:41.520 --> 42:46.240
in building the story with you has their own ideas they might take your ideas off track they

42:46.240 --> 42:51.120
might turn your murder mystery into a horror story they might turn your interview where you're

42:51.120 --> 42:54.880
supposed to be going back and forth and having a conversation into a series of you ask me a

42:54.880 --> 42:58.800
questions and I say yes or no and it's not much of an interview for you right so you have to

42:58.800 --> 43:03.760
influence what the player is doing what the human participant is doing or otherwise

43:03.760 --> 43:08.800
you don't end up with a good story that you want to have so there are two ways of doing that one

43:08.800 --> 43:13.280
and I think you know you and most your listeners if you ever heard the expression a game that's on

43:13.280 --> 43:18.640
rails sure so you know that's where well I'm sorry I'm just not going to let you go through this door

43:18.640 --> 43:23.040
because if you do it breaks the video game it breaks the story and so you're on rails and the

43:23.040 --> 43:28.000
thing about being on rails it takes you out of the story takes you out of the experience and that's

43:28.000 --> 43:32.400
what a lot of people do and a lot of the drama management stuff is about that as well but there's

43:32.400 --> 43:36.240
another way of doing and in fact the right way of doing it if you can make it work is you get the

43:36.240 --> 43:40.720
other person the person you're interacting with you're trying to learn with the story you're trying

43:40.720 --> 43:45.760
to get to participate in the story to actually accept your goals as his or her own and it turns out

43:45.760 --> 43:51.840
marketing is very good at this so we build this kind of system where you get people to do the things

43:51.840 --> 43:56.960
that you want them to do by putting them in situations where it's just natural for them to do those

43:56.960 --> 44:04.320
things so rather than lock every door except one door in a room so you go through it I make something

44:04.320 --> 44:09.120
happen maybe some noise or something interesting that makes you want to go through that door right

44:09.120 --> 44:15.440
so um those kind of like themes of behavior like economics and incentives and things like that

44:15.440 --> 44:19.040
coming into play here right yeah oh that's exactly right so in fact the example of this that

44:19.040 --> 44:24.160
everyone's familiar with is one called scarcity uh-huh so that's where it turns out that people

44:24.160 --> 44:30.560
if they believe that something is going away they suddenly find it more valuable right so anybody

44:30.560 --> 44:35.520
with kids knows certainly anyone with kids ten years ago know that Disney has his habit of saying oh

44:35.520 --> 44:40.720
we're going to release on DVD beauty in the beast and then we're never going to release it again uh-huh

44:41.360 --> 44:46.480
and so everybody buys it right because it's about to go away or I mean black fridays like this right

44:47.280 --> 44:51.600
you're gonna every year at the day after Thanksgiving you go to the store to buy a bunch of stuff

44:51.600 --> 44:55.760
it doesn't make any sense whatsoever there are not even things you want to have but they're going

44:55.760 --> 45:00.400
away you're gonna get a price right now it's on sale and so people react to that they can't help

45:00.400 --> 45:05.040
themselves uh it's a scarcity is just one of one of the particular they very easy to understand

45:05.040 --> 45:08.480
there's tons of others of these there's something called liking which is it turns out people will

45:08.480 --> 45:14.560
do things for you if they if they like you uh people react to authority actually my favorite

45:14.560 --> 45:19.920
example is something called consistency where if you can get someone to say something out loud

45:19.920 --> 45:25.040
that they believe something they haven't almost pathological need to be consistent with it over time

45:25.040 --> 45:28.640
so uh you know do you have anybody in your neighborhood who won't mold their lawn

45:29.360 --> 45:34.160
um here's the way you get them to mold the lawn you wait till it's winter right and so all the

45:34.160 --> 45:38.480
grass is you know kind of dead and it's all the the same height and you start up a conversation

45:38.480 --> 45:42.960
and you say man you know it really looks great around here when it's like this you know everything's

45:42.960 --> 45:47.200
the same color everything's the same height if you get the person to agree with that yeah it looks

45:47.200 --> 45:53.680
really nice and it's like this the next summer they'll mold the lawn because they basically believe

45:53.680 --> 45:57.200
that's the way it's supposed to be and you know it's really nice about it it's not that you got

45:57.200 --> 46:03.520
them to mold the lawn it's that they believe that they are in complete control of the idea that

46:03.520 --> 46:08.560
they're the ones who made the decision are in charge so that's a long story but the the short

46:08.560 --> 46:15.280
version is we built systems like this uh that basically convinced people uh to do the things that

46:15.280 --> 46:19.360
we wanted them to do we influenced them so i'm not using machine learning just to predict your

46:19.360 --> 46:23.680
behavior i'm using machine learning to figure out how to intervene to get you to do something and

46:23.680 --> 46:28.160
what i really want to happen is for you to believe it's your own idea so we built this little story

46:28.160 --> 46:33.600
just a quick example we built this little story uh where the whole goal was to get you to buy a fish

46:33.600 --> 46:37.440
at a market it's not the most exciting story in the world and there are lots of ways we can

46:37.440 --> 46:42.000
influence you to do this with scarcity and various other things and and and so we had people

46:42.000 --> 46:47.600
play this game and uh the people we tried to influence were much more likely than the people we

46:47.600 --> 46:53.200
didn't um in buying the fish and doing the things that we tried to get done sure now that's

46:53.200 --> 46:59.520
interesting but what's more interesting is that when you ask the people whether they felt manipulated

46:59.520 --> 47:04.320
or not the people who were not manipulated were much more likely to say they felt they were being

47:04.320 --> 47:08.400
manipulated than the people who actually were manipulated that's interesting why is that

47:08.400 --> 47:13.040
because the whole the whole way this works the whole way the kind of psychology works is you

47:13.040 --> 47:17.520
feel as if you have agency that you're making the decision when something goes on sale and you

47:17.520 --> 47:21.040
decide you're gonna buy it you don't feel that you've been tricked into buying it you made the

47:21.040 --> 47:26.720
decision to do it right and so what's really interesting this is why it's not just running the

47:26.720 --> 47:30.640
data and doing machine learning it's actually understanding about human behavior it's understanding

47:30.640 --> 47:34.960
behavioral economics it's understanding the way marketing tricks work it's it's all about getting

47:34.960 --> 47:41.360
the person to make the decision you know themselves that they want to do this thing and then they have

47:41.360 --> 47:46.000
agency they have control and they're much more likely to see it through the fact that you kind of

47:46.000 --> 47:53.440
trick them into doing it is neither here nor there so quick note uh for listeners anyone that's

47:53.440 --> 47:59.200
interested in digging deeper into some of these ideas uh there's a great book called Influence by

47:59.200 --> 48:05.760
Robert Chaldeini that is super accessible and is covers all the things that you you talked about

48:05.760 --> 48:14.480
consistency and scarcity things like that um but this brings up a uh question for me and that is

48:15.520 --> 48:23.280
a lot of the a lot of the work we read about reinforcement learning nowadays is you're training

48:23.280 --> 48:31.040
these agents to uh navigate a world right and then the work you're describing is you've got this

48:31.040 --> 48:37.280
world that's essentially training the human to navigate it and there's an interesting complementariness

48:37.280 --> 48:42.640
to it and I'm wondering if if that complementariness has been explored at all like the things that I'm

48:42.640 --> 48:46.880
thinking around like adversarial networks like can you have the one training the other thinking

48:46.880 --> 48:50.640
it's training the other does that make any sense is anything happening there oh yeah that's

48:50.640 --> 48:54.960
actually very commonly doing it so the way so the thing that really got me into reinforcement learning

48:54.960 --> 49:00.240
when I was a young graduate student a couple hundred years ago uh was actually playing games so

49:00.240 --> 49:05.440
there was this guy named Saro who had built something called TD Gammon which was a particular way

49:05.440 --> 49:09.920
of doing a reinforcement learning to to learn how to play backgammon and the way it learned to play

49:09.920 --> 49:16.320
backgammon was through self-play so it it played both sides of the game uh and it learned by playing

49:16.320 --> 49:21.360
itself how to get better uh and this is a well I think I'm pretty well understood sort of technique

49:21.360 --> 49:26.720
for learning right you it's difficult to it if it's too hard you can't learn if it's too easy you

49:26.720 --> 49:31.600
can't learn you need to be just about a little beyond your current level of understanding and so

49:31.600 --> 49:35.520
yeah this kind of thing happens all the time now it is true that a lot of people who worry about

49:35.520 --> 49:39.760
machine learning do not think about the kind of complementary nature that rather than there being an

49:39.760 --> 49:45.360
agent that's training in an environment the environment could be in fact training the agent and people

49:45.360 --> 49:50.880
don't always see that um in fact my biggest complaint or complaints not the right word but my

49:50.880 --> 49:55.120
biggest um I don't know let's say complaint my biggest complaint about the way machine learning

49:55.120 --> 50:00.160
is portrayed is that it's portrayed as a supervised learning problem you know I'm going to give you

50:00.160 --> 50:04.640
a bunch of input output examples and you're going to learn the function that maps input output

50:04.640 --> 50:09.440
and that's interesting but I think reinforcement learning is more interesting because it's this

50:09.440 --> 50:14.960
bigger problem you don't have inputs and outputs all you've got is actions you can take and feedback

50:14.960 --> 50:19.600
you get from the world and then from that you have to figure out how to behave that feels richer to

50:19.600 --> 50:24.240
me even though in some sense they're equivalent the unsupervised learning is a very different way

50:24.240 --> 50:28.160
of thinking about the world even though again sort of mathematically they're they're all kind of

50:28.160 --> 50:32.880
equivalent and that kind of breadth of what machine learning and AI is is something that I don't

50:32.880 --> 50:37.280
think we spend enough time really thinking about I think people tend to focus on the supervised

50:37.280 --> 50:41.120
learning part instead of the reinforcement learning in the unsupervised learning part at least in

50:41.120 --> 50:48.800
the kind of popular press okay so maybe taking a step back how do you think about the

50:49.680 --> 50:55.040
current state of reinforcement learning like can you characterize the the major research

50:55.040 --> 50:59.920
efforts or even is it possible to characterize the major research efforts into a handful of

51:01.280 --> 51:07.520
directions and kind of who's doing what so I think there's kind well so the answers no it's

51:07.520 --> 51:11.680
way too broad but there's a couple of things that I think are really interesting one is all this

51:11.680 --> 51:16.640
work on deep networks and deep neural networks which you know is the the current thing that everybody's

51:16.640 --> 51:20.320
really into and by the way it's really good work you know I know the people who've been pushing on

51:20.320 --> 51:24.560
that for years and years and years and and they've really been able to to do a lot of interesting

51:24.560 --> 51:28.960
things they they've got the kind of fundamentals right with the math and they're taking advantage of

51:28.960 --> 51:33.840
the fact that we have insane amounts of data so that we can actually sort of take advantage of those

51:33.840 --> 51:38.080
algorithms and do cool things a lot of what's going on at least in my world that people are paying

51:38.080 --> 51:43.280
a lot of attention to is figuring out how to use the stuff that we know from deep networks and

51:43.280 --> 51:48.320
deep learning and applying it to reinforcement learning okay and and rather than doing the

51:48.320 --> 51:51.920
supervised learning take where you said well okay here's a state of the world tell me what to do

51:51.920 --> 51:56.240
you're actually treating it the way you treat a reinforcement learning problem you're talking about

51:56.240 --> 52:01.200
building value functions over what the states are in the world and what things are better and then

52:01.200 --> 52:06.400
using that to figure out how to make a decision and use what you learn from making the decision

52:06.400 --> 52:11.520
to affect your view of what's valuable in the world and kind of having each one feed into one

52:11.520 --> 52:16.160
another and so recognizing that there's at least two parts of that problem instead of one part of

52:16.160 --> 52:21.040
that problem is a really big deal and being able to marry the kind of math that's come out of

52:21.040 --> 52:26.480
supervised learning has been I think really important that I think has has been really interesting

52:26.480 --> 52:31.360
is push forward a lot of a lot of of what we've been able to to learn in the last couple of years

52:31.360 --> 52:36.320
for sure the second thing which I think is interesting in part because it's it's my own work and

52:36.320 --> 52:40.160
and place where I lived is very related to what we just got through talking about and it's this

52:40.160 --> 52:45.120
interactive machine learning it turns out you know I mentioned earlier that there's no free lunch

52:45.120 --> 52:49.520
right so for those of you don't know the no free lunch there and basically just says that

52:49.520 --> 52:54.240
all algorithms are equally good and in fact not only are all algorithms equally good but none of

52:54.240 --> 52:59.280
them are any better than behaving randomly and the reason that's true is because over all the

52:59.280 --> 53:04.640
infinite number of problems that one could encounter you know any algorithm has just as good a

53:04.640 --> 53:08.960
chance of doing well as that as any other algorithm but it turns out in practice we don't care

53:08.960 --> 53:14.080
about every possible problem in the universe we care about a small set of problems in the universe

53:14.080 --> 53:20.320
and what allows us to get leverage over that small set are built in assumptions about that

53:20.320 --> 53:25.280
about that world so the problem of learning is difficult and in some ways impossible

53:26.000 --> 53:31.280
but it turns out people are really good at solving the problems that people are really good at

53:31.280 --> 53:34.880
what they're really bad about is explaining to you how they do what they do but they're really

53:34.880 --> 53:39.280
good at solving these problems so a lot of what's been going on in the reinforcement learning world

53:39.280 --> 53:44.400
in particular is taking advantage of people learning from getting people to tell you something

53:44.400 --> 53:49.680
or to demonstrate something to you about how to do something so that you can learn much much faster

53:49.680 --> 53:54.160
than you ever would and really what you're getting out of it is you're getting human beings the

53:54.160 --> 53:58.560
human beings assumptions about the way the world works and you're taking advantage of those

53:58.560 --> 54:03.280
assumptions to narrow down the to narrow down the search base so I'll give you really I'll give

54:03.280 --> 54:09.760
you a really quick example so it turns out that people do not think about things and

54:11.280 --> 54:15.680
atomic actions they tend to think about them in these big temporarily extended views of the world

54:15.680 --> 54:20.000
so that takes something like Pac-Man right if you asked if I asked you to explain Pac-Man to me

54:20.000 --> 54:23.360
you would be describing in terms of up down left right or what you would say things like oh well

54:23.360 --> 54:28.400
look you need to get the power pellet you need to avoid the ghosts you need to you know you need to

54:28.400 --> 54:34.000
do these four or five things and we run experiments on this where we ask people to to create buttons

54:34.000 --> 54:38.640
that they would use if to to make Pac-Man go faster and they come up with these interesting

54:38.640 --> 54:43.760
buttons these sort of long term things but dividing the world up like that not from up down left

54:43.760 --> 54:48.640
right but into get the power pellet avoid the ghost is something that is very difficult to learn

54:48.640 --> 54:53.360
from scratch but people have already figured this out so you build systems where people are able

54:53.360 --> 55:00.320
to express to you those tricks those shortcuts those assumptions about the world and then you can

55:00.320 --> 55:07.200
learn so much faster than you would ever be able to do on your own and that's kind of where we're

55:07.200 --> 55:11.040
getting a lot basically taking assumptions from the world and getting them automatically from

55:11.040 --> 55:14.960
humans I think that's incredibly important and one of the reasons I think it's important by the way

55:14.960 --> 55:22.160
is because so many of the problems that we actually care about involve people right they involve

55:22.160 --> 55:26.240
other people they involve interacting with people and so you have to understand the fundamental

55:26.240 --> 55:29.520
assumptions that people are living and and you have to take advantage of them if you're ever going

55:29.520 --> 55:33.520
to learn so those are two areas that I happen to think are are really cool in the reinforcement

55:33.520 --> 55:40.400
learning space right now are we also learning how to enable the machines to make the assumption the

55:40.400 --> 55:47.120
assumptions themselves like what's happening there yeah but the way they do it is they kind of do it

55:47.120 --> 55:52.080
they do it by dint of observing the world right there's a Michael Lippman always says a couple of

55:52.080 --> 55:56.720
things that I really like him and one is that you know if the person who's doing the programming

55:56.720 --> 56:01.280
is doing all the learning and writing down the data structures then you're stuck right you need

56:01.280 --> 56:05.600
the machine itself to be able to learn its own data structures through observation it needs to be

56:05.600 --> 56:10.000
able to to build its own assumptions and its own models if you're always giving it the model then

56:10.000 --> 56:14.400
it's always depending upon you to give it the model it has to be able to to build its its own model

56:14.400 --> 56:19.120
so fundamental to that is this idea that that you're going to learn these you're going to build in

56:19.120 --> 56:22.240
your own assumptions you're going to learn new assumptions and you're going to build models that

56:22.240 --> 56:27.520
you're you're willing to adapt and so yes yes that that's definitely built into it it's definitely

56:27.520 --> 56:33.360
a part of what's going on but the problem is absent nothing number absent anything you you can't

56:33.360 --> 56:38.720
know where to start and so this gets his back full circle to this idea that learning is a

56:38.720 --> 56:43.440
social exercise right as human beings we interact with other human beings that have a bunch of

56:43.440 --> 56:47.600
assumptions they built the world together and they kind of know how it works and a lot of your

56:47.600 --> 56:52.560
job is to figure out what it is they've built into the world as assumptions so that you can begin

56:52.560 --> 56:56.960
to learn and our machines have to be able to do the same thing or otherwise they're not actually

56:56.960 --> 57:03.920
living in the same world that we're living in. Interesting interesting one of the one of the

57:03.920 --> 57:11.200
papers that I pulled up of yours on archive is a paper perceptual reward functions which is

57:11.200 --> 57:17.680
pretty recently published and that goes into I think the the former of these two areas that you

57:17.680 --> 57:24.080
mentioned where you're trying to map kind of the deep learning to you know a broader set of

57:24.080 --> 57:31.680
problems. Can you describe that the paper? That is relatively new stuff there's a bunch of new

57:31.680 --> 57:36.240
things that are coming coming out about about that at the student mine actually Edwards is really

57:36.240 --> 57:45.520
buying into the fundamental idea there is that you know people have people's reward functions so

57:45.520 --> 57:48.080
if you're a machine learning guy right you're particularly reinforcement learning guy you start

57:48.080 --> 57:52.880
talking about rewards and you start talking about states and you divide the world up into the abstract

57:52.880 --> 57:57.120
space and you go that's how you solve problems but we spend most of our time never actually worrying

57:57.120 --> 58:02.400
about where these things come from they're just given to us and this paper is a part of actually a

58:02.400 --> 58:06.800
larger body of work that that I've been I've been paying a little bit of attention to the last

58:06.800 --> 58:11.120
couple of years of trying to figure out where those things come from are there principles about

58:11.120 --> 58:16.000
where reward functions come from are there principles about where state comes from at least with

58:16.000 --> 58:21.840
respect to the way human beings deal with it so that you can actually solve these problems in

58:21.840 --> 58:26.800
general and be more robust to small changes in the environment one of the things that that's true

58:26.800 --> 58:31.840
about reinforcement learning is you know there's a nice little math equation that you need in order

58:31.840 --> 58:36.320
to figure out how to learn and determine value and it's very nice and it's very elegant but it's

58:36.320 --> 58:41.200
actually quite fragile so if I were to build a system let's say a robot and I wanted this robot

58:41.200 --> 58:46.480
to get from one end of a hallway to another and along the way it might do some other interesting

58:46.480 --> 58:52.320
things I can construct all my little alphas and my my learning rates and I can put everything together

58:52.320 --> 58:56.560
so that eventually it will learn and that it will do exactly what you want it to do and it won't get

58:57.280 --> 59:01.760
so scared that something battle happened that it won't move and it won't get so distracted by

59:01.760 --> 59:05.120
some interesting thing over here to the left that it'll never get to the end of the hallway I can

59:05.120 --> 59:10.080
actually do that pretty well but then if I take that robot and all that it's learned and then I

59:10.080 --> 59:15.920
make the hallway five inches longer it will stop working right because the math is very brittle

59:15.920 --> 59:21.280
everything is set up just right so that everything kind of touches one another and what you

59:21.280 --> 59:24.720
want to do is you want to build systems that are robust to that you want to build systems that

59:24.720 --> 59:30.880
adapt to that and it turns out that human beings are very good I mean in fact optimized in some

59:30.880 --> 59:35.280
ways for dealing with you know it's still a niche environment right we we do pretty well on earth

59:35.280 --> 59:40.160
we don't we won't do pretty well on Mars right we don't do pretty well in space but but you know

59:40.160 --> 59:44.720
it's still a rich environment that we're in and you want to build systems that can do that and so

59:44.720 --> 59:52.640
the perceptual reinforcement learning stuff is about using what we get from our perceptions

59:52.640 --> 59:57.600
directly as the as the notion of state and as our notion of reward that we try to get things to

59:57.600 --> 01:00:02.720
look like what we see we try to imitate the things that we see through through our perceptions

01:00:02.720 --> 01:00:09.520
rather than you know build simple or actually complex optimization functions that tell us you

01:00:09.520 --> 01:00:13.840
know whether this thing actually is like that thing no you just think about what it is that you see

01:00:13.840 --> 01:00:17.600
what it is that you're what it is you're receiving and there's this sort of larger philosophy

01:00:17.600 --> 01:00:22.000
around that I'm actually quite excited about the work I think what it allows us to do is to stop

01:00:22.000 --> 01:00:28.240
thinking about reinforcement learning as five you know a five tuple where you have to set the values

01:00:28.240 --> 01:00:34.480
and start thinking about it as a larger programming problem where the whole thing is it's reinforcement

01:00:34.480 --> 01:00:39.040
learning is not the thing that you start with it's the mechanism by which you happen to solve the

01:00:39.040 --> 01:00:43.840
problem it is itself a programming language is itself a wave of viewing the world and you've got

01:00:43.840 --> 01:00:48.880
a step back to the level of task and problem instead of thinking about solving this particular

01:00:48.880 --> 01:00:55.120
equation interesting yeah I thought the example that was provided in the introduction to the paper

01:00:55.120 --> 01:01:03.920
was a good one that was tip training robot to to fold origami like what you know what's the state

01:01:03.920 --> 01:01:09.840
of an origami and how do you how would you represent that traditionally you know whereas the

01:01:09.840 --> 01:01:15.520
what's natural for us as humans is to see a picture of the final result and you know how do

01:01:15.520 --> 01:01:22.800
you define a you know a score metric or a distance metric from you know a given current origami

01:01:22.800 --> 01:01:28.800
to this target yeah it's and it's a rich problem too because as soon as if I asked you to explain

01:01:28.800 --> 01:01:32.080
me how to do origami which by the way I have absolutely no idea how to do something magic with

01:01:32.080 --> 01:01:35.840
your hands paper you do a flurry of things and then suddenly there's a dragon I don't really

01:01:35.840 --> 01:01:39.760
know what happens but you know you start saying oh well you start thinking about folding and you

01:01:39.760 --> 01:01:44.480
start talking at this very high level just like with with Pac-Man right and the way of dividing up

01:01:44.480 --> 01:01:48.800
that world is actually important because if you don't divide up the world in the right way you will

01:01:48.800 --> 01:01:53.760
never in a million years a billion year in the lifetime of the universe actually solve the problem

01:01:53.760 --> 01:01:58.400
because there's just too many possibilities right this goes all the way back to to language learning

01:01:58.400 --> 01:02:05.360
and you know it turns out that people do not actually correct their children right so you don't

01:02:05.360 --> 01:02:10.560
get any negative examples hardly at all when you're a kid and yet somehow children learn to speak

01:02:10.560 --> 01:02:15.600
their particular language even though nobody's telling them when they're you think you are but you

01:02:15.600 --> 01:02:19.360
don't actually correct your children and we can prove to you mathematically that you can't learn

01:02:19.360 --> 01:02:23.280
under those circumstances so the only way it can be happening is if the world has been divided

01:02:23.280 --> 01:02:27.280
up in the nice little ways and there's only a few possibilities and you're searching over those

01:02:27.280 --> 01:02:31.520
few possibilities because the world's already been divided up for you if you have to go to the trouble

01:02:31.520 --> 01:02:36.160
of dividing up the world yourself then you're just there is enough time there aren't enough examples

01:02:36.160 --> 01:02:43.520
there isn't enough time yeah yeah yeah so now at the risk oh go ahead no I'm just gonna say so to

01:02:43.520 --> 01:02:48.320
me if you pop up to the AI level instead of the machine learning level right uh that's really the

01:02:48.320 --> 01:02:51.680
interesting thing right but what's really exciting about AI right now what's really exciting about

01:02:51.680 --> 01:02:56.560
machine learning right now is that we finally have enough computing power we finally have enough

01:02:56.560 --> 01:03:02.320
mathematical sophistication and we finally have enough data that we can actually start solving

01:03:02.320 --> 01:03:06.960
really hard problems where we're going to be forced to move beyond you know the equation

01:03:06.960 --> 01:03:12.560
that we wrote down in 1965 that hasn't changed to thinking about bringing in all of these other

01:03:12.560 --> 01:03:17.440
things whether it's marketing and behavioral economics whether whether it's game theory whether

01:03:17.440 --> 01:03:21.440
it's well engineering whether it's control you know we actually going to have to bring in tons of

01:03:21.440 --> 01:03:25.920
other things in order to solve the problems we're now at the point where we can actually do that

01:03:25.920 --> 01:03:29.440
so we're actually meeting in the middle so that so this is why this is an exciting time for me

01:03:30.080 --> 01:03:36.000
nice nice so at the risk of asking a question that we've kind of touched on in a couple different

01:03:36.000 --> 01:03:42.880
ways already for someone who wants to dig deeper into the kind of stuff we were just talking about

01:03:42.880 --> 01:03:48.880
interactive machine learning and AI and reinforcement learning are there any places that you would

01:03:48.880 --> 01:03:54.880
point them to get started well I would start with just a basic machine learning class particularly

01:03:54.880 --> 01:03:58.240
one that covers reinforcement learning if you really are interested in reinforcement learning

01:03:58.240 --> 01:04:04.240
as a topic I mean you know rich sudden's book is freely available online it's a great place

01:04:04.240 --> 01:04:08.400
to start to kind of understand what's going on the class that I teach with Michael Litman is

01:04:08.400 --> 01:04:13.120
freely online there's lots and lots and lots and lots and lots of examples out there I would

01:04:13.120 --> 01:04:18.240
actually start with that and get the basics there's survey papers I mean Google is your friend in

01:04:18.240 --> 01:04:23.360
this case but if you're the if you're the kind of person who wants to have someone give you a

01:04:23.360 --> 01:04:29.120
nice brief overview of what's going on then you know hey start with my class just pick Michael

01:04:29.120 --> 01:04:32.720
Litman you can go to your desk you can get it for free just sort of skim through it and watch through

01:04:32.720 --> 01:04:40.480
it and you'll you'll figure out from there where to go and I would really I would really encourage

01:04:40.480 --> 01:04:45.920
people to pick a problem that they find interesting if you games are the things for you then start

01:04:45.920 --> 01:04:51.360
looking up the deep learn the deep reinforcement learning work on games there's a there's a bunch

01:04:51.360 --> 01:04:56.320
of work done recently on solving most of the Atari games using deep learning that's really

01:04:56.320 --> 01:05:00.800
interesting stuff the problem with starting there though is that oh now you have to know what

01:05:00.800 --> 01:05:04.000
convolution nets are and you know you're you're going to find yourself distracted for nine

01:05:04.000 --> 01:05:08.160
months while you learn enough math to figure out what's going on I would actually start top down I

01:05:08.160 --> 01:05:13.040
would start thinking about the problems what the issues are before I get so deep into the to the

01:05:13.040 --> 01:05:17.200
math that I get lost you don't want to lose the forest for the trees here and it is very easy

01:05:17.200 --> 01:05:20.880
to lose the forest for the trees because there's so much kind of interesting and very difficult

01:05:20.880 --> 01:05:25.520
math that's underneath all of this but really you want to keep sight of the goal right which is

01:05:25.520 --> 01:05:30.640
to build something that can learn over time can adapt over a year it can live for 20 years

01:05:30.640 --> 01:05:34.320
and continually learn and adapt and think about what that would mean think about what it would

01:05:34.320 --> 01:05:38.240
mean to you as a person and then start asking what kind of background you would need to have in

01:05:38.240 --> 01:05:44.240
order to build a system that does that that's great and I'll include links to a bunch of the things

01:05:44.240 --> 01:05:50.960
that you mentioned in the show notes um oh so I would let me add one thing to show notes

01:05:51.440 --> 01:05:55.680
you mentioned the book influence I would also recommend the media equation

01:05:56.160 --> 01:06:01.360
to media equation that is a fantastic book it's one of these it's a short book about how human

01:06:01.360 --> 01:06:07.840
beings actually behave and how it turns out that people will treat machines as if they're humans

01:06:08.400 --> 01:06:12.480
even though they know better because they'll treat anything that acts like it has intention

01:06:12.480 --> 01:06:17.520
as if it has intention and I think that fact alone should influence everyone who's thinking

01:06:17.520 --> 01:06:21.360
about building systems that have to interact with humans interesting we're not even all that good

01:06:21.360 --> 01:06:27.840
at describing intention other people the thought of applying it to machines is uh and we're going

01:06:27.840 --> 01:06:31.280
to have to work on that I actually think it's the other way around I think the problem is we're

01:06:31.280 --> 01:06:35.520
incredibly good at describing intentions to other people it's just not always the right intentions

01:06:35.520 --> 01:06:44.160
ah yeah yeah yeah um great so I think we uh this has been a great discussion um I appreciate you

01:06:44.160 --> 01:06:48.320
getting together with me for it especially on a Saturday morning and don't want to monopolize

01:06:48.320 --> 01:06:53.760
your Saturday so we'll wrap things up here anything else you'd like to uh toss out

01:06:54.480 --> 01:06:57.680
no just I really enjoy this and we should have this conversation again

01:06:57.680 --> 01:07:03.200
absolutely absolutely um and then for folks that want to get in touch with you

01:07:03.200 --> 01:07:08.960
they find you on google plus well if you go to google plus I'm the one guy who's still there

01:07:08.960 --> 01:07:14.640
so just send me an email it may take me a while to respond that I'm more than happy to respond

01:07:15.440 --> 01:07:21.920
just google in bell c.katek.edu okay and are you on twitter or any of the the the lesser use

01:07:21.920 --> 01:07:29.200
social networks I have a I have a twitter account uh and occasionally I even use it uh but really

01:07:29.200 --> 01:07:32.560
emails the only way to really get to me I'm unless you have my cell number and I'm not giving

01:07:32.560 --> 01:07:37.440
you my cell number nice nice all right great uh well thanks so much Charles really appreciate it um

01:07:38.240 --> 01:07:41.520
and uh next time look absolutely awesome

01:07:48.640 --> 01:07:53.040
all right everyone that's our show for today thanks so much for listening

01:07:53.040 --> 01:07:57.280
if you're one of our lucky winners or runners up please reach out to me

01:07:57.280 --> 01:08:04.160
at sam at twimlai.com a bunch of you have asked hey what's up with the newsletter

01:08:04.160 --> 01:08:09.680
no you haven't missed anything I've just been crazy busy and haven't had a chance to get one out

01:08:09.680 --> 01:08:15.600
I'm so sorry about that I'm still working on it and I'll keep you posted thank you so much for

01:08:15.600 --> 01:08:27.360
your support and catch you next time

