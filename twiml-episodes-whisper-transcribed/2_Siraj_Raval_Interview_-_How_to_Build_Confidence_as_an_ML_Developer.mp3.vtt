WEBVTT

00:00.000 --> 00:14.480
Hello everyone and welcome to the podcast.

00:14.480 --> 00:18.640
If you're a regular listener of the show, I want to start out by saying thank you so much

00:18.640 --> 00:19.640
for your support.

00:19.640 --> 00:24.040
It's been really great to get your notes and feedback about the show.

00:24.040 --> 00:28.360
I won't go into the backstory here, but going forward, I'm going to pivot a bit in my approach

00:28.360 --> 00:33.960
to the show and focus on interviews with interesting folks in machine learning and AI.

00:33.960 --> 00:38.680
And to accompany the podcast, I'm still going to bring you the news, but now via the email

00:38.680 --> 00:40.080
newsletter.

00:40.080 --> 00:44.400
If you'd like to know more about these changes, hop over to the show notes after listening,

00:44.400 --> 00:51.880
which can be found at twimlai.com slash talk, T-A-L-K slash two, the number two.

00:51.880 --> 00:56.000
Okay, so about the interview you're about to hear.

00:56.000 --> 01:00.520
If you've listened to a few of my previous shows, you've probably heard me mention the name

01:00.520 --> 01:02.520
Siraj Ravel.

01:02.520 --> 01:07.960
Siraj is a machine learning hacker and educator whose machine learning for hackers and fresh

01:07.960 --> 01:14.160
machine learning YouTube series are fun, informative, high energy and practical ways to learn

01:14.160 --> 01:18.240
about a ton of machine learning and AI topics.

01:18.240 --> 01:24.000
I had a chance to catch up with Siraj in San Francisco recently and we had a great discussion.

01:24.000 --> 01:27.960
Siraj has great advice on how to learn machine learning and build confidence as a machine

01:27.960 --> 01:33.280
learning developer, how to research and formulate projects, who to follow on machine learning

01:33.280 --> 01:35.160
Twitter and much more.

01:35.160 --> 01:41.280
I'll include links to Siraj's shows and some of the things we discuss in the show notes.

01:41.280 --> 01:43.320
A quick note before the interview.

01:43.320 --> 01:47.000
If you're new to the show, you should know that I've partnered with O'Reilly to give

01:47.000 --> 01:49.840
away a ticket to their upcoming AI conference.

01:49.840 --> 01:56.840
I'll talk about how to enter after the interview and end the show notes and now onto the interview.

01:56.840 --> 02:06.680
Alright, so I'm here with Siraj Ravel.

02:06.680 --> 02:08.520
Siraj, it's great to meet you in person.

02:08.520 --> 02:14.200
I've been talking about your YouTube videos on the podcast for I've talked about a couple

02:14.200 --> 02:18.880
of them and like I wanted to talk about you like every week because there's so many

02:18.880 --> 02:23.120
great videos but I've held back a lot, you know, I got to spread the love.

02:23.120 --> 02:27.560
So it's great to get a chance to meet you in person and I just wanted to spend a few

02:27.560 --> 02:31.360
minutes kind of talking about what you're up to and how you got here.

02:31.360 --> 02:32.360
Sounds good, yeah.

02:32.360 --> 02:33.360
I'm totally down.

02:33.360 --> 02:34.360
Appreciate you coming over.

02:34.360 --> 02:35.360
Nice.

02:35.360 --> 02:39.040
So, you know, let's start there like how did you get into machine learning?

02:39.040 --> 02:47.400
I mean, ever since I was in college, I was looking for something to really put all my

02:47.400 --> 02:52.840
energy into and what it was for me was a robotics lab at my school at Columbia and the robotics

02:52.840 --> 02:56.960
lab was my first for ray into machine learning and I found that there were all these problems

02:56.960 --> 03:02.720
that I wanted to solve that at the time deep learning wasn't really a thing that deep learning

03:02.720 --> 03:08.240
would then solve later like in two years and so I was looking into like the initial types

03:08.240 --> 03:12.960
of machine learning like support vector machines and things like that and just gradually

03:12.960 --> 03:16.560
over time I realized like, hey, neural nets, deep learning, this stuff is like going to

03:16.560 --> 03:20.360
solve so many problems.

03:20.360 --> 03:25.240
So yeah, I've just always been into intelligence and solving intelligence.

03:25.240 --> 03:29.800
That's pretty much my main driver in life like I want to help humanity solve intelligence

03:29.800 --> 03:32.360
because I think it's the most important thing we can do.

03:32.360 --> 03:36.200
So Columbia is, you know, not San Francisco and we're sitting here in San Francisco like

03:36.200 --> 03:37.200
what was the path?

03:37.200 --> 03:38.200
How did you get?

03:38.200 --> 03:39.840
How did you end up here and what are you up to?

03:39.840 --> 03:45.480
Yeah, so yeah, was that Columbia and honestly I didn't feel like I really fit into Columbia.

03:45.480 --> 03:50.440
I was, you know, I fit in really well here in San Francisco in like Silicon Valley culture.

03:50.440 --> 03:56.520
I think because I'm, you know, I'm not so much into like going to classes in person and

03:56.520 --> 04:00.200
just like studying subjects that I don't care a lot about.

04:00.200 --> 04:03.120
Like I just wanted to just study robotics and AI.

04:03.120 --> 04:07.520
So once I was at the robotics lab, I felt like, okay, this is, this is like my thing.

04:07.520 --> 04:09.160
I'm going to keep doing this.

04:09.160 --> 04:12.920
But that only lasted like a year and then I had a startup called Lucid Robotics where

04:12.920 --> 04:16.520
I was trying to create a robot for your home like a platform where each app would be a physical

04:16.520 --> 04:17.520
task.

04:17.520 --> 04:20.000
So you'd have an app for like cleaning the dishes and stuff.

04:20.000 --> 04:23.640
Clearly, this was way out of scope at the time, but at the time you couldn't tell me that.

04:23.640 --> 04:26.480
I had to see the computer science problems myself.

04:26.480 --> 04:30.080
What actually ended the startup, I mean, we raised funding from Sabir Bhattia, the founder

04:30.080 --> 04:32.360
of Hotmail, we had a team.

04:32.360 --> 04:36.880
What ended the startup was we couldn't get the robot to pick up a simple novel object

04:36.880 --> 04:39.240
they had never seen before.

04:39.240 --> 04:41.320
Deep learning now solves this.

04:41.320 --> 04:44.720
So then, so then after the startup failed, I dropped out.

04:44.720 --> 04:45.720
I dropped out of Columbia.

04:45.720 --> 04:49.200
I just was so disenchanted with so many things.

04:49.200 --> 04:54.320
And I felt like San Francisco was the place where I could go to rediscover myself.

04:54.320 --> 04:57.000
And it's been a, it's been a, you know, quite a journey.

04:57.000 --> 05:01.400
And there's been a lot of uncertainty in my life about what I should be doing, the path

05:01.400 --> 05:03.680
I should be moving towards.

05:03.680 --> 05:07.760
But I'm lucky enough to have come to the conclusions that I have that intelligence is the most

05:07.760 --> 05:12.760
important thing for us to solve in our lifetime.

05:12.760 --> 05:17.240
Because if we don't solve it, then some other catastrophe could wipe out our species, whether

05:17.240 --> 05:22.640
it's biochemical terrorism or some natural disaster or, you know, something like that.

05:22.640 --> 05:24.360
We have to solve intelligence.

05:24.360 --> 05:25.360
Yeah.

05:25.360 --> 05:29.240
So how did you, how did that bring you to doing a YouTube channel?

05:29.240 --> 05:30.240
Yeah.

05:30.240 --> 05:36.040
So, so I, you know, I had a few jobs here as an engineer at CBS Interactive and at Twilio

05:36.040 --> 05:37.240
and they were, they were incredible.

05:37.240 --> 05:42.560
I love these positions, but engineering itself just, I don't know, I felt like I could,

05:42.560 --> 05:44.720
I could be having more impact at Twilio.

05:44.720 --> 05:46.120
I mean, Twilio was a great place.

05:46.120 --> 05:48.600
They, I was doing company, it was a great company.

05:48.600 --> 05:52.520
I was doing developer education and like, that was my full time role.

05:52.520 --> 05:53.840
So I was doing technical writing.

05:53.840 --> 05:56.080
It was the first time I hadn't just been doing code.

05:56.080 --> 05:58.560
And I felt like, okay, this is, this is my thing, like technical writing.

05:58.560 --> 05:59.560
This is awesome.

05:59.560 --> 06:02.480
I get to, I get to combine my writing ability and my coding ability.

06:02.480 --> 06:03.480
Yeah.

06:03.480 --> 06:08.720
The thing for me, like the reason that I left was that I wanted to do video documentation.

06:08.720 --> 06:12.640
I believe in the future of video documentation.

06:12.640 --> 06:14.720
And I, I feel like Twilio was going on a different path.

06:14.720 --> 06:19.040
So I decided, okay, you know what, I'm just going to do this full time.

06:19.040 --> 06:22.520
And so I started the YouTube channel on the side while I was at Twilio.

06:22.520 --> 06:23.520
Okay.

06:23.520 --> 06:27.040
So I was making one video a week, but the quality wasn't at the level that I wanted it

06:27.040 --> 06:28.040
to be.

06:28.040 --> 06:30.560
I didn't have enough, like the production equipment wasn't good enough.

06:30.560 --> 06:31.560
Yeah.

06:31.560 --> 06:33.280
I wasn't giving enough time to the technical writing.

06:33.280 --> 06:36.120
So the only option I had was to quit and do this full time.

06:36.120 --> 06:37.120
Okay.

06:37.120 --> 06:39.160
And so then I was just like, all right, here we go.

06:39.160 --> 06:41.080
And how many have you done?

06:41.080 --> 06:42.080
Video so far.

06:42.080 --> 06:43.080
Yeah.

06:43.080 --> 06:47.200
I think it's like, it's at least, at least like 28 videos now.

06:47.200 --> 06:48.200
Wow.

06:48.200 --> 06:49.200
Almost 30.

06:49.200 --> 06:52.360
It's one video a week every week since like January 1st.

06:52.360 --> 06:53.360
Wow.

06:53.360 --> 06:54.360
Nice.

06:54.360 --> 06:55.360
Nice.

06:55.360 --> 06:57.960
And you've, the original show was called Machine Learning for Hackers.

06:57.960 --> 06:58.960
Is that right?

06:58.960 --> 06:59.960
Yeah.

06:59.960 --> 07:00.960
Machine Learning for Hackers.

07:00.960 --> 07:01.960
And you, you've just launched a new one?

07:01.960 --> 07:02.960
Yeah.

07:02.960 --> 07:06.000
And does that one replace Machine Learning for Hackers?

07:06.000 --> 07:10.600
Or are they like two parallel tracks that continue ongoing?

07:10.600 --> 07:13.560
You know, it's interesting because the idea with Machine Learning for Hackers is that

07:13.560 --> 07:14.920
it's meant for developers.

07:14.920 --> 07:18.160
And Fresh Machine Learning was also meant for developers, but it was like a different

07:18.160 --> 07:19.160
topic subset.

07:19.160 --> 07:20.600
It was like newer things.

07:20.600 --> 07:25.840
But what I've noticed is that I have, so my subscribers, I have three different types

07:25.840 --> 07:27.000
of people who are watching me.

07:27.000 --> 07:32.640
I have the research scientists, the cool kids who are like developing the novel algorithms.

07:32.640 --> 07:35.720
Then there's the developers who are, honestly, they're also the cool kids.

07:35.720 --> 07:40.920
And those are the people I really want to, you know, they were my main motivation from

07:40.920 --> 07:41.920
the start.

07:41.920 --> 07:43.480
I want to make things for developers.

07:43.480 --> 07:47.120
And then there's actually the third subset which I'm learning about, which are people

07:47.120 --> 07:49.560
who are not really technical, but they really want to be.

07:49.560 --> 07:53.800
So it's like, I have to make videos that catering to each of them.

07:53.800 --> 07:58.120
So I'm still kind of trying to figure out like, you know, because sometimes my video is

07:58.120 --> 08:01.960
catered to the research scientists, sometimes the developers, sometimes the, you know, people

08:01.960 --> 08:03.640
who are not very technical.

08:03.640 --> 08:08.200
So I think for now, I'm making videos that kind of cater to all three, but eventually I

08:08.200 --> 08:12.160
want to get to the point where I have channels, dedicated channels for each of these subsets.

08:12.160 --> 08:13.160
Yeah.

08:13.160 --> 08:14.400
And for that, I have to grow a little bit more.

08:14.400 --> 08:15.400
Okay.

08:15.400 --> 08:22.320
It sounds like, in a lot of ways, a parallel path to mine with this podcast, I, you know,

08:22.320 --> 08:27.120
my initial vision was, you know, I just couldn't get enough machine learning information.

08:27.120 --> 08:32.520
Like I, I, you know, spend the week like opening up web browser tabs of articles that I wanted

08:32.520 --> 08:35.360
to read or papers that I wanted to take a look at.

08:35.360 --> 08:39.240
And I'd end up in a given week with like 80 to 100 of these tabs open.

08:39.240 --> 08:43.760
And I'm like, this is ridiculous, not, and then you spend some time going through it and

08:43.760 --> 08:45.120
half of it is crap.

08:45.120 --> 08:50.280
And like if everyone's doing the same thing, then, you know, people would appreciate, you

08:50.280 --> 08:53.480
know, something that tries to figure out what's good and what's not and just spend some

08:53.480 --> 08:54.760
time talking about what's good.

08:54.760 --> 08:59.320
So hey, I don't have to spend my week collecting this bag of tabs.

08:59.320 --> 09:02.920
And you know, it's been super rewarding, but it's like a ton of work.

09:02.920 --> 09:03.920
It's a ton of work.

09:03.920 --> 09:08.600
And then I look at your stuff and like, I can't imagine what goes into, you know, your

09:08.600 --> 09:12.840
videos because you're like going deep into a topic and then, you know, you're writing

09:12.840 --> 09:15.760
code, you're like, you know, publishing code up on GitHub.

09:15.760 --> 09:16.760
What's the process?

09:16.760 --> 09:20.240
Is it, is it the same every, every week, or are you like still experimenting?

09:20.240 --> 09:21.240
Yeah.

09:21.240 --> 09:23.640
So I've developed a methodology for this over time.

09:23.640 --> 09:25.920
Like I'm building the process.

09:25.920 --> 09:30.360
So what it is is like the first part of research, like what is the topic I want to talk about

09:30.360 --> 09:32.720
and let me just learn about it.

09:32.720 --> 09:37.160
The second part is the code, like programming it.

09:37.160 --> 09:40.960
Like I'm going to program some very, very simple, what I like to call the quick start

09:40.960 --> 09:41.960
of X.

09:41.960 --> 09:46.200
So the quick start of auto encoders, the quick start of support vector machines.

09:46.200 --> 09:47.200
Yep.

09:47.200 --> 09:49.440
Then it's the technical writing.

09:49.440 --> 09:53.800
So research, code, technical writing, then it's the production.

09:53.800 --> 09:57.520
So the actual video, like shooting it, and then it's editing.

09:57.520 --> 10:02.440
And then there's marketing and release.

10:02.440 --> 10:05.880
So yeah, it's like five or six things in sequential order.

10:05.880 --> 10:11.720
And I can, I managed to fit, I'm able to fit all these things into a single week and

10:11.720 --> 10:15.360
it takes around 40 to 60 hours for a single video.

10:15.360 --> 10:17.600
Generally, it's closer to 60 hours.

10:17.600 --> 10:22.960
Now I have clients, so I'm increasing the output from one video a week to two.

10:22.960 --> 10:24.880
So that's like 120 hours a week.

10:24.880 --> 10:26.520
That's a lot.

10:26.520 --> 10:30.800
This is actually the first week where I have to make two videos in one week.

10:30.800 --> 10:37.080
So I'm hiring, yeah, I'm hiring a video editor, a technical video editor, which is like

10:37.080 --> 10:42.880
a new role because they have to be a video editor who also knows kind of like how to code.

10:42.880 --> 10:46.360
Because I have code and I have to, you know, they have to point those red arrows at what

10:46.360 --> 10:47.360
I'm talking about.

10:47.360 --> 10:48.360
They have to be at least.

10:48.360 --> 10:49.360
They need to know what's important.

10:49.360 --> 10:50.600
They need to know what's important, right?

10:50.600 --> 10:53.400
And they have to know the cards, you know, the cards where I'm talking, like what I'm

10:53.400 --> 10:56.880
saying, like, oh, this is a relevant what he's talking about, like support documentaries

10:56.880 --> 10:57.880
or whatever.

10:57.880 --> 10:58.880
Yeah.

10:58.880 --> 10:59.880
So I'm looking for unicorns, basically.

10:59.880 --> 11:00.880
Yeah.

11:00.880 --> 11:01.880
Yeah.

11:01.880 --> 11:02.880
Aren't we all?

11:02.880 --> 11:10.000
Well, so you're doing it all in one week, you're not like, you know, researching one week

11:10.000 --> 11:14.160
and producing the, you know, researching in next week's video one week and then producing

11:14.160 --> 11:15.800
it's all self-contained in that week.

11:15.800 --> 11:17.440
It's all self-contained in that week, yeah.

11:17.440 --> 11:23.560
And how do you determine what's, you know, what you're going to talk about next?

11:23.560 --> 11:25.560
That's a good question.

11:25.560 --> 11:32.600
I, so, yeah, so I browsed the machine learning subreddit, looking at what's hot, what

11:32.600 --> 11:38.040
it, whatever interests me, I look at hacker news, I look at Twitter, like Twitter is actually

11:38.040 --> 11:39.120
a great learning tool for me.

11:39.120 --> 11:43.000
I just follow people who I think are really smart and, you know, young lacunes and stuff

11:43.000 --> 11:44.000
like that.

11:44.000 --> 11:52.720
And I think my, the other data source is Facebook groups, I mean, a lot of machine learning

11:52.720 --> 11:54.520
Facebook groups.

11:54.520 --> 11:58.000
So yeah, whatever is like new and hot and the intersection of what's new and hot and

11:58.000 --> 12:02.880
like what I'm into, generally, I can figure that out in like one day, but it takes all

12:02.880 --> 12:03.880
day.

12:03.880 --> 12:04.880
Yeah.

12:04.880 --> 12:05.880
Yeah.

12:05.880 --> 12:08.680
The curation part is, yeah, it's hard.

12:08.680 --> 12:12.000
I mean, there's just, there's a lot of stuff out there, you know, like I said before and

12:12.000 --> 12:15.600
there's a lot of stuff that, you know, looks really, there's a lot of clickbait, right?

12:15.600 --> 12:16.600
It looks really interesting.

12:16.600 --> 12:19.560
And then you get, you dig deep and it's just nothing there.

12:19.560 --> 12:20.560
Totally.

12:20.560 --> 12:21.560
Yeah.

12:21.560 --> 12:25.400
Or it's like just way, way too technical and you didn't even think it would be.

12:25.400 --> 12:26.800
It's like a lot of math.

12:26.800 --> 12:29.040
It's like, ah, here we go.

12:29.040 --> 12:33.240
Is there an example of, you know, something that you thought you wanted to take on?

12:33.240 --> 12:40.120
And then you just, you know, found out that it was just, that the math was just too ridiculous.

12:40.120 --> 12:49.200
I think, ah, well, I can't, well, I, if I've decided I'm going to do it, I'm just like,

12:49.200 --> 12:52.720
I literally don't have time to like, not do it because I've, you know what I mean?

12:52.720 --> 12:53.960
Because I just have to keep going.

12:53.960 --> 12:58.440
But I can tell you that the closest I was to like, not being able to finish a video

12:58.440 --> 13:00.600
was generative adversarial networks.

13:00.600 --> 13:01.600
That was a very video.

13:01.600 --> 13:02.600
That was a very video.

13:02.600 --> 13:03.600
Thank you.

13:03.600 --> 13:07.480
That was the hardest video I've ever had to make because make, because that stuff, yeah,

13:07.480 --> 13:09.680
that stuff was pretty hard.

13:09.680 --> 13:12.680
Was that the video where you're like, well, this really should be two or three videos,

13:12.680 --> 13:15.520
but I'm just going to, you know, cram it down to one and see how it goes.

13:15.520 --> 13:18.280
It was one of those where you said something like that, I thought.

13:18.280 --> 13:22.200
Um, yeah, no, no, I could definitely have more than one on, uh, Gans.

13:22.200 --> 13:23.200
Yeah.

13:23.200 --> 13:27.160
Uh, so what, um, you know, that's the topic that's come up on my podcast quite a bit.

13:27.160 --> 13:31.000
Why don't you talk a little bit about, um, you know, talk a little bit about Gans, what

13:31.000 --> 13:34.160
you learn there in, in doing that project.

13:34.160 --> 13:35.160
Yeah.

13:35.160 --> 13:41.720
So yeah, Ian Goodfellow, who's now a research scientist at OpenAI, he's, he's the guy who

13:41.720 --> 13:48.160
authored the paper, but it's a, it's a generative model that can create, uh, so if you give it

13:48.160 --> 13:53.760
some input data, it's going to, it's going to have some output data that's similar to the

13:53.760 --> 13:55.360
input data, but different.

13:55.360 --> 13:59.200
So if you feed it like a collection of faces, it's going to generate faces that look similar

13:59.200 --> 14:00.720
but are different.

14:00.720 --> 14:03.440
And at first, I was like, well, how is this going to be useful?

14:03.440 --> 14:08.960
But it's a tool for any kind of engineer to design, uh, so if you feed it like, you

14:08.960 --> 14:16.160
know, uh, collection of living rooms, it's going to be able to generate novel living

14:16.160 --> 14:19.880
rooms that look photorealistic, which is super cool.

14:19.880 --> 14:24.600
So it's a tool to help engineers like envision their ideas better.

14:24.600 --> 14:33.400
And yeah, yeah, I, um, I like the idea of two dueling entities, you know, uh, uh,

14:33.400 --> 14:40.400
how the discriminator is always trying to, uh, fool, um, fool, fool, fool its counterpart,

14:40.400 --> 14:44.920
or the, the counterpart is always trying to fool the discriminator, which is always trying

14:44.920 --> 14:47.920
to detect like, oh, is this, is this false or real?

14:47.920 --> 14:48.920
Right.

14:48.920 --> 14:51.960
And just keeps doing that until eventually, you know, it just gets better and better.

14:51.960 --> 14:52.960
It's a brilliant idea.

14:52.960 --> 14:55.440
And like, you know, deep mind has done this stuff with like AlphaGo when they trained

14:55.440 --> 14:59.000
two dual neural neural nets against each other to play Go, so it just got better and

14:59.000 --> 15:00.000
better.

15:00.000 --> 15:04.480
So this idea of, you know, of having this adversarial nature can be applied to a lot of other

15:04.480 --> 15:05.800
things in machine learning.

15:05.800 --> 15:06.800
Have you seen examples of that?

15:06.800 --> 15:12.280
I've been looking for that, uh, as well, I've come across, uh, or at least ideas of, uh,

15:12.280 --> 15:16.520
of, you know, where, hey, if we can pit one machine learning algorithm or one AI against

15:16.520 --> 15:19.200
another, you know, and let them train each other.

15:19.200 --> 15:23.720
Have you seen, uh, besides from the, the generous stuff that was covered in the papers,

15:23.720 --> 15:25.240
other examples of that?

15:25.240 --> 15:33.600
Yeah, um, I think, uh, there's a lot of a potential for like game AI.

15:33.600 --> 15:38.280
So if you have, you know, a bot versus a human, or just two bots first thing each other.

15:38.280 --> 15:43.840
Um, I think so, so deep mind is like really into games, which is cool.

15:43.840 --> 15:44.840
Uh-huh.

15:44.840 --> 15:51.800
And I think there's a lot of potential for combining, uh, adversarial work with what

15:51.800 --> 15:54.080
they're doing in 3D games.

15:54.080 --> 15:57.880
Um, just as like a, I mean, it was, it's kind of like a suggestion on my part.

15:57.880 --> 16:05.200
I'm sure they've already thought about this, but if you, um, apply gans, if you were to

16:05.200 --> 16:09.400
apply gans to games, I think that would be really cool.

16:09.400 --> 16:10.400
I haven't seen a paper.

16:10.400 --> 16:14.760
No, we're talking about like first person shooters type games or the types of games that

16:14.760 --> 16:18.000
they're playing, you know, in deep mind, the Atari game.

16:18.000 --> 16:19.000
No, no, no, yeah.

16:19.000 --> 16:20.000
Okay.

16:20.000 --> 16:21.000
So, okay.

16:21.000 --> 16:23.960
So like, what I think is really cool, um, so open AI just yesterday.

16:23.960 --> 16:28.920
I think released this call for, um, research scientist on four problems.

16:28.920 --> 16:30.680
There was number four was, what was it?

16:30.680 --> 16:36.040
It was like create a simulation that where all the entities get better and better over

16:36.040 --> 16:37.040
time.

16:37.040 --> 16:41.800
Like you create an entity in the simulated world and then it learns to like what kind of

16:41.800 --> 16:45.560
food it needs, what kind of nutrition it needs to survive better and better.

16:45.560 --> 16:50.840
Like I think there's a lot of potential for adversarial, uh, algorithms there, um,

16:50.840 --> 16:54.280
two entities versus each other in this, in this simulated world.

16:54.280 --> 16:57.880
So maybe not necessarily just a game, but any kind of simulated environment where you

16:57.880 --> 17:02.360
have a set of constraints and you want, you want it, you want some kind of AI to get

17:02.360 --> 17:04.280
better over time.

17:04.280 --> 17:10.760
I think we're going to see a lot of, um, a lot of adversarial algorithms in the future.

17:10.760 --> 17:12.760
And a lot of one shot learning, mm-hmm.

17:12.760 --> 17:16.120
I'd like to see more of that because right now, you know, all this machine learning stuff

17:16.120 --> 17:22.600
is, is kind of siphoned off to these big companies like Facebook and Google and Apple.

17:22.600 --> 17:25.800
But with, you know, if, with advances in one shot learning, anybody who is going to be

17:25.800 --> 17:32.360
able to, uh, create these models and, and learning algorithms from sparse data, startups,

17:32.360 --> 17:35.880
for example, that only have like, you know, a hundred users, but they want to imply machine

17:35.880 --> 17:36.880
learning to that.

17:36.880 --> 17:37.880
Right.

17:37.880 --> 17:38.880
Right.

17:38.880 --> 17:45.800
Uh, so you dig into a topic like this, you know, Gans, there's, you know, research papers.

17:45.800 --> 17:52.120
Like how do you, how do you take, how do you make the leap from that to code to getting

17:52.120 --> 17:53.120
code up?

17:53.120 --> 17:57.640
Um, you know, a lot of the folks that listen to my podcast, I've, you know, heard from,

17:57.640 --> 18:00.800
you know, are in the process of learning and they're trying to figure out projects to work

18:00.800 --> 18:05.280
on and, you know, getting from some of the things that they're reading about to, you

18:05.280 --> 18:09.040
know, some working example, like, and you've got that down to a science, right?

18:09.040 --> 18:13.960
So at this point, yeah, uh, from repetition, how do you approach it?

18:13.960 --> 18:23.960
Yeah, so I think, um, for me, it's, it's, it's, it's, a lot of it is what I learned

18:23.960 --> 18:28.640
from Tulio, like the idea of having a quick start, like a bare bone skeleton that a developer

18:28.640 --> 18:30.080
can then build off of.

18:30.080 --> 18:34.640
What is the minimum viable product for, for demoing this, this idea that you have?

18:34.640 --> 18:36.920
However simple you can make it, do it.

18:36.920 --> 18:49.360
So if I read something like, you know, um, a paper on, like, for example, ah, there's

18:49.360 --> 18:57.320
so much, uh, auto encoders, what's the simplest thing I can do with an auto encoder?

18:57.320 --> 19:02.440
An auto encoder takes them input, compresses it and then, uh, reconstructs it.

19:02.440 --> 19:05.160
It's only a, it's a three layer, it's a very simple neural network.

19:05.160 --> 19:07.160
What's the simple, simple demo I can make with this?

19:07.160 --> 19:11.560
And I just think about it and I'm like, okay, compression, oh, compression, just compression

19:11.560 --> 19:12.560
alone.

19:12.560 --> 19:15.320
So just use it as a compression algorithm.

19:15.320 --> 19:18.080
So like a zip, you know, zipping, yeah, zipping and unzipping.

19:18.080 --> 19:20.880
So then I was like, okay, so then I'm like, okay, so how do I code this?

19:20.880 --> 19:22.560
So what I first do is I search GitHub.

19:22.560 --> 19:26.640
So it's happened like very, you know, auto encoder and I look under Python because Python

19:26.640 --> 19:28.640
is awesome.

19:28.640 --> 19:34.160
And I see what's been done before, usually, usually something has been done before.

19:34.160 --> 19:37.160
So I'll take that and I'll like kind of like strip away the unnecessary things and add

19:37.160 --> 19:38.160
documentation.

19:38.160 --> 19:39.160
And that's going to be the demo.

19:39.160 --> 19:40.160
Okay.

19:40.160 --> 19:42.160
And the rare case, it's not, then I have to code it myself.

19:42.160 --> 19:43.160
Okay.

19:43.160 --> 19:44.160
Yeah.

19:44.160 --> 19:45.160
How often does that happen?

19:45.160 --> 19:50.720
That code to get yourself, like entirely, um, I'd say like off the top of my head, probably

19:50.720 --> 19:52.520
like 15% of the time.

19:52.520 --> 19:53.520
Okay.

19:53.520 --> 19:54.520
Yeah.

19:54.520 --> 19:58.000
So one of the, you know, the two lessons I got from that are, you know, simplify, simplify,

19:58.000 --> 20:04.120
simplify, like, you know, you know, whether it's, uh, the actual coding or the, uh,

20:04.120 --> 20:08.920
you know, trying to parse the research, it's like, figure out what this thing is at its

20:08.920 --> 20:14.640
bare essence and focus on that, uh, and then like reuse, like, figure out what's been

20:14.640 --> 20:16.400
done and try to use that.

20:16.400 --> 20:17.400
Is there anything else?

20:17.400 --> 20:21.440
And like any other pieces of advice that you'd give to folks that are trying to work this

20:21.440 --> 20:22.440
process?

20:22.440 --> 20:29.800
Um, yeah, just like, don't be intimidated by papers, like, there is a lot of math and

20:29.800 --> 20:30.800
papers.

20:30.800 --> 20:38.200
Really, like when I'm reading a paper, um, it's the abstract and the background, the process

20:38.200 --> 20:42.400
and the conclusion, which matter the most to me.

20:42.400 --> 20:43.880
And there's really not a lot of math.

20:43.880 --> 20:50.240
It's, it's, it's when they start describing, you know, certain aspects of the process that

20:50.240 --> 20:55.200
it can get really, really confusing if you don't know math notation.

20:55.200 --> 20:58.920
But math notation itself is in serious need of an upgrade.

20:58.920 --> 21:01.080
Mm-hmm, so it's more human readable.

21:01.080 --> 21:02.080
Mm-hmm.

21:02.080 --> 21:05.920
Right now, it's kind of siphoned off to just these research scientists who look at this

21:05.920 --> 21:07.160
stuff every day.

21:07.160 --> 21:12.400
So I think, you know, we're going to start to see, um, innovations in, in how we publish

21:12.400 --> 21:15.200
scientific research so that anybody can read it.

21:15.200 --> 21:17.360
What that's going to look like, I'm not sure.

21:17.360 --> 21:22.560
But they're just, they're just too much coming out right now and it's too important, uh,

21:22.560 --> 21:26.360
for a few people, for only a few people to be able to read it.

21:26.360 --> 21:32.760
So, so I would say, if you just read the abstract of a paper and you feel like you get the

21:32.760 --> 21:35.240
gist, that's fine.

21:35.240 --> 21:37.280
You can go start searching GitHub with just that.

21:37.280 --> 21:41.040
Don't feel like, um, you know, guilty or, or something.

21:41.040 --> 21:44.200
And definitely look at videos and, and what I try to do when I'm trying to learn something

21:44.200 --> 21:48.400
is I try to get as many different types of data sources that can into my brain.

21:48.400 --> 21:49.400
That always helps.

21:49.400 --> 21:55.560
Videos, articles, conversations with people, you know, there's, there's a lot of content

21:55.560 --> 21:56.560
out there.

21:56.560 --> 21:58.120
And it's just going to increase exponentially.

21:58.120 --> 21:59.120
Mm-hmm.

21:59.120 --> 22:00.120
Yeah.

22:00.120 --> 22:04.920
I find the same thing and find also that, um, sometimes it doesn't work out like you expect

22:04.920 --> 22:11.560
like the, I did a review of the Google research wide and deep learning paper.

22:11.560 --> 22:15.080
And you know, they've got this cool YouTube video that, you know, simplifies everything.

22:15.080 --> 22:16.720
But I watched that and I didn't, I didn't get it.

22:16.720 --> 22:19.360
But then I went through the paper and, uh, it made sense.

22:19.360 --> 22:22.320
And then I went back to the video and was like, oh, yeah, I don't know why I didn't get

22:22.320 --> 22:23.320
that before.

22:23.320 --> 22:24.320
Yeah.

22:24.320 --> 22:30.640
I think that, you know, having lots of different types of input can make a big difference.

22:30.640 --> 22:38.520
So what's, like, what's your roadmap for, for upcoming topics and research?

22:38.520 --> 22:40.520
Yeah.

22:40.520 --> 22:45.800
Um, so in terms of like the topics themselves, I kind of decide them week to week.

22:45.800 --> 22:51.760
But the, for the larger vision is to just focus on machine learning, kind of be like

22:51.760 --> 22:54.560
Khan Academy for machine learning.

22:54.560 --> 22:57.560
And I'm going to start needing help and from other people.

22:57.560 --> 23:04.360
So I'm hiring and, uh, yeah, just try to get, I'm just optimizing for subscribers.

23:04.360 --> 23:08.240
I want to get, you know, I want to get every developer on the planet to at least do a little

23:08.240 --> 23:09.680
bit of machine learning.

23:09.680 --> 23:11.240
I think it's super important.

23:11.240 --> 23:15.160
Uh, there are about 10 million developers on the planet right now.

23:15.160 --> 23:21.240
And not nearly, there's not nearly enough that are even aware of how important machine

23:21.240 --> 23:28.040
learning is, um, architecture engineering is a new, uh, feature engineering.

23:28.040 --> 23:35.720
And if you want to win, if you have a startup, if you, if you have, uh, an idea, if you

23:35.720 --> 23:39.160
want to win, you, at this point, you have to implement some sort of AI because if you

23:39.160 --> 23:42.040
don't, someone else will, right.

23:42.040 --> 23:45.640
So I want to make machine learning as, you know, democratizing, making it as accessible

23:45.640 --> 23:48.640
and understandable as possible to as many people as possible.

23:48.640 --> 23:51.320
So I'm just going to keep going down that path and do whatever it takes to, to make that

23:51.320 --> 23:52.320
happen.

23:52.320 --> 23:54.520
And that's going to be lots and lots of videos in the future.

23:54.520 --> 23:56.080
You've got a new project that you're working on.

23:56.080 --> 23:57.560
Is that something that you can talk about?

23:57.560 --> 23:58.560
That's going to be public as well.

23:58.560 --> 23:59.560
That's going to be public.

23:59.560 --> 24:00.560
Yeah.

24:00.560 --> 24:01.560
And it's not going to be on my channel.

24:01.560 --> 24:02.560
It's going to be on theirs.

24:02.560 --> 24:06.160
But a big ML, like I'm partnered with, I've, I've now, you know, I've signed a deal

24:06.160 --> 24:07.160
with big ML.

24:07.160 --> 24:11.040
So I'm going to be making a video series for them, um, about their product and it's going

24:11.040 --> 24:14.200
to be, it's, it's called cloud machine learning.

24:14.200 --> 24:20.680
And it's using big ML to do a bunch of, uh, pragmatic, real world applications.

24:20.680 --> 24:26.880
So the first one, uh, is going to be, uh, about climate change and how we can use machine

24:26.880 --> 24:29.240
learning to prevent climate change.

24:29.240 --> 24:30.240
Okay.

24:30.240 --> 24:31.280
So I'm super excited about that one.

24:31.280 --> 24:35.640
And then so like, because video content takes up so much of my time, I don't really have

24:35.640 --> 24:39.880
time to do things like client acquisition and, yeah, you know, all this, all this stuff.

24:39.880 --> 24:43.120
So the, the clients that I do have are the people who have come to me and right now have

24:43.120 --> 24:48.560
like seven or eight and they're kind of in a queue, uh, and yeah, I'm just taking on

24:48.560 --> 24:50.520
as much as I can handle at a time.

24:50.520 --> 24:57.080
And as I grow, I'm going to start, start looking at more, you know, ideally, you know, my

24:57.080 --> 25:02.200
goal is to one day partner with deep mind.

25:02.200 --> 25:08.480
I want to make videos for deep mind, um, but they're like, I consider them like the Navy

25:08.480 --> 25:12.680
Seals machine learning, so I've got to get, I've got to get to that level.

25:12.680 --> 25:18.320
You know, the Apollo program for intelligence, uh-huh.

25:18.320 --> 25:22.320
You know, if we solve intelligence, we can apply it to anything like just think of it as

25:22.320 --> 25:26.880
an objective function or X any problem you can ever think of it.

25:26.880 --> 25:33.080
If you have the right learning algorithm and you say solve for X, it could solve it, scientific

25:33.080 --> 25:37.240
research problems or even existential problems, the questions that have plagued us in

25:37.240 --> 25:38.240
stay one.

25:38.240 --> 25:39.240
Who are we?

25:39.240 --> 25:40.240
Why are we here?

25:40.240 --> 25:41.720
What's the point of the universe?

25:41.720 --> 25:46.560
You might not be capable of figuring this stuff out ourselves, but a highly intelligent

25:46.560 --> 25:47.560
AI could.

25:47.560 --> 25:48.560
Mm-hmm.

25:48.560 --> 25:50.960
We might not like the answers.

25:50.960 --> 25:53.600
We might, we might not like the answers.

25:53.600 --> 25:58.200
We might not like the answers, but where, so where do you fall on the whole singularity

25:58.200 --> 25:59.200
thing?

25:59.200 --> 26:00.200
All right.

26:00.200 --> 26:02.280
That's what, wake, wake, that's why I wake up in the morning.

26:02.280 --> 26:07.400
I want to make it a benevolent singularity happen as soon as possible, uh-huh, as soon

26:07.400 --> 26:08.400
as possible.

26:08.400 --> 26:15.120
What do you think about the, uh, the open AI research stuff that they put out a few

26:15.120 --> 26:18.600
weeks ago on Safe Machine Learning, have you been following that stuff?

26:18.600 --> 26:19.600
Mm-hmm.

26:19.600 --> 26:26.360
So, so specifically like, uh, ways to, they published this framework for like four or

26:26.360 --> 26:31.680
five different areas of research that need to be kind of dug into so that we can ensure

26:31.680 --> 26:36.720
the safety and, you know, benevolence as you put it of, of AI like, you know, if we've

26:36.720 --> 26:44.400
got a AI powered robot, um, you know, how do we, how do we ensure that, you know, it

26:44.400 --> 26:50.480
doesn't learn how to game the system and, um, and, you know, for example, if it's being

26:50.480 --> 26:52.200
programmed to clean, right?

26:52.200 --> 26:56.520
How do, how do we know that, how do we program it so that it doesn't sweep stuff onto the

26:56.520 --> 26:57.520
carpet?

26:57.520 --> 26:58.520
Right.

26:58.520 --> 26:59.520
Yeah.

26:59.520 --> 27:03.520
I think, um, yeah, and then Google had like the kill switch paper, which I thought was

27:03.520 --> 27:04.520
super cool.

27:04.520 --> 27:07.400
Uh, I like that open AI is thinking about this.

27:07.400 --> 27:10.960
I love open AI in general, the, the concept behind it.

27:10.960 --> 27:23.040
I think, um, yeah, it's like preventing AI from doing bad things is going to be really

27:23.040 --> 27:24.040
important.

27:24.040 --> 27:26.480
I mean, technology has always been a double-edged sword, you know, with the, starting with

27:26.480 --> 27:27.560
the natural fire.

27:27.560 --> 27:32.560
And I think that, you know, with security, I think that's going to be one of the first,

27:32.560 --> 27:38.640
uh, where we're going to see, uh, we're going to see the power of AI, uh, when it comes

27:38.640 --> 27:45.160
to protecting humans, if you have an AI and you train it to get really good at breaking

27:45.160 --> 27:50.000
into systems, the only thing that's going to be able to stop that is an AI that's good

27:50.000 --> 27:53.720
at detecting an AI that can break into systems.

27:53.720 --> 27:57.360
So, uh, I think it's a great thing what they're doing.

27:57.360 --> 27:58.640
I think it's really important.

27:58.640 --> 28:00.120
I think it's really important.

28:00.120 --> 28:05.000
And what Mary is doing as well, and, uh, you know, the ethics committee that DeepMind

28:05.000 --> 28:11.080
has at Google to prevent, uh, you know, malevolent types of AI, all this stuff is super,

28:11.080 --> 28:12.080
super important.

28:12.080 --> 28:14.440
Mary's the machine intelligence research institute.

28:14.440 --> 28:15.440
Yeah.

28:15.440 --> 28:16.440
Yeah.

28:16.440 --> 28:17.440
Yeah.

28:17.440 --> 28:18.440
Yeah.

28:18.440 --> 28:23.600
And, you know, there's always a question like, can we stop it, um, you know, who knows?

28:23.600 --> 28:25.360
But it's, it's, it's good to try.

28:25.360 --> 28:32.720
And honestly, uh, if a malevolent AI, uh, doesn't kill us, then something else likely

28:32.720 --> 28:33.720
will.

28:33.720 --> 28:39.720
So this is something that, um, just something that's really important.

28:39.720 --> 28:40.720
Hmm.

28:40.720 --> 28:48.720
So what's your maybe taking a step back like for folks that are trying, uh, do you have

28:48.720 --> 28:52.320
a quick like, if someone, you know, a friend comes up to you and says, okay, I really,

28:52.320 --> 28:54.840
you know, I really want to learn this stuff now.

28:54.840 --> 28:56.440
Like what's your curriculum?

28:56.440 --> 29:01.000
What's your, you know, one, two, three, uh, list of stuff to do is it?

29:01.000 --> 29:04.480
Do you think are you trying to build your videos so that someone could just follow those

29:04.480 --> 29:08.840
and get everything that they need or, uh, are there some set of resources that you think

29:08.840 --> 29:10.840
are kind of canonical?

29:10.840 --> 29:11.840
Yeah.

29:11.840 --> 29:20.360
So I think that, um, my videos are good if you know some basic Python.

29:20.360 --> 29:24.800
If you know Python, then my good, my videos are a great starting point.

29:24.800 --> 29:32.200
But I think that my videos alone are not enough, um, you know, it's, it's one of the things

29:32.200 --> 29:33.640
of like combining different data sources.

29:33.640 --> 29:41.440
So I think my videos, in addition to some long form content, I think, uh, so for me, uh,

29:41.440 --> 29:52.000
big ML has some great long form content, um, there's, uh, so, you know, I actually don't

29:52.000 --> 30:00.360
think I think that, uh, there's a deep learning course on Udacity, uh, by a Google engineer

30:00.360 --> 30:03.520
who works at Google Brain, I forgot what it's called, but if you, if you Google just

30:03.520 --> 30:06.840
like Udacity deep learning, uh, that, that course is really good.

30:06.840 --> 30:10.560
That to me is even the TensorFlow course or not the TensorFlow course.

30:10.560 --> 30:12.360
That's a great one.

30:12.360 --> 30:21.960
Um, but there's one specifically on deep learning in general, um, um, there's just

30:21.960 --> 30:22.960
just so much.

30:22.960 --> 30:26.440
I think it's, it's one of those things where it's like, you, okay, so if you're saying like,

30:26.440 --> 30:31.960
I want to learn machine learning, I would say like, okay, first learn Python, um, uh, by

30:31.960 --> 30:37.560
reading the book, uh, learn Python the hard way, um, uh, and then once you, once you,

30:37.560 --> 30:41.800
once you feel like you're comfortable with Python, just start building things, just start

30:41.800 --> 30:45.240
building things and, and my videos are good because it's application specific and I make

30:45.240 --> 30:49.080
it really easy for you to, you know, just, when you hit compile and you see your model

30:49.080 --> 30:55.400
train, train and then you can apply to other things, that is like super useful for, for

30:55.400 --> 30:59.920
your confidence as a machine learner and also just as a developer.

30:59.920 --> 31:06.560
So and also just go to GitHub and search for machine learning projects like search for

31:06.560 --> 31:12.120
like machine learning demo or machine learning simple and just look at those readmeas, download

31:12.120 --> 31:16.640
them, compile them, open it in a text editor and just like go through them one by one and

31:16.640 --> 31:21.760
like really try to understand what's happening, you know, and, and I would say start off

31:21.760 --> 31:26.080
at a high level because, you know, some people would say the other way, like start off at

31:26.080 --> 31:29.800
a low level, like learn exactly how to implement these models from scratch.

31:29.800 --> 31:34.200
No, no, no, I would say start off at a high level and once you get it at a high level,

31:34.200 --> 31:42.760
then you can start like trying to rebuild, you know, uh, you know, you know, you know,

31:42.760 --> 31:43.760
neural net from scratch.

31:43.760 --> 31:48.200
Yeah, like custom and implement from ground up or implement some research or something

31:48.200 --> 31:49.200
like that.

31:49.200 --> 31:50.200
Yeah.

31:50.200 --> 31:51.200
Yeah.

31:51.200 --> 31:52.200
Yeah.

31:52.200 --> 31:53.200
Keras, torch, lots of great libraries these days.

31:53.200 --> 31:54.200
Mm hmm.

31:54.200 --> 31:55.200
Nice.

31:55.200 --> 32:00.200
Um, what, uh, Quora is well, sorry, Quora is awesome.

32:00.200 --> 32:05.120
I've learned so much from Quora, just like, you know, cause I'll find one question on

32:05.120 --> 32:08.880
Quora on deep learning and on the side bar, it's like, oh my god, all these questions

32:08.880 --> 32:10.360
are amazing.

32:10.360 --> 32:14.240
And then you have people like Jan LeCoon answering them and like Monica Anderson and like,

32:14.240 --> 32:16.040
all these like really famous research scientists.

32:16.040 --> 32:17.040
Yeah.

32:17.040 --> 32:18.040
So I've learned a lot from them.

32:18.040 --> 32:22.200
Are there people that you, is it primarily like search, uh, based that where you find

32:22.200 --> 32:28.160
stuff or are you following particular people and just kind of keeping up with them there?

32:28.160 --> 32:29.160
It's search based.

32:29.160 --> 32:30.160
Search based.

32:30.160 --> 32:31.160
Yeah, search based.

32:31.160 --> 32:32.160
How about on Twitter?

32:32.160 --> 32:35.320
Are there, you mentioned Yana, there are other folks that you, uh, yeah, find our good

32:35.320 --> 32:39.160
signal to noise, uh, machine learning folks on Twitter?

32:39.160 --> 32:40.160
For sure.

32:40.160 --> 32:46.640
I think, uh, for me, uh, I think Chris Dixon, he's a partner in Andreessen Horowitz.

32:46.640 --> 32:50.880
He, he's, he's good for like knowing, uh, you know, what's up and coming and machine

32:50.880 --> 32:51.880
learning.

32:51.880 --> 32:53.680
I think he has a good eye for that.

32:53.680 --> 32:58.760
One person in general that I really respect about technology is Bology Srinivasan, who's

32:58.760 --> 33:01.120
also a partner in Andreessen Horowitz.

33:01.120 --> 33:05.320
That guy knows he lives in the future.

33:05.320 --> 33:10.120
And, um, yeah, board, Jan LaCoon is also like a great Twitter handle.

33:10.120 --> 33:14.520
I don't think I've come across that one yet, but it sounds funny.

33:14.520 --> 33:15.520
I really like it.

33:15.520 --> 33:16.520
Yeah.

33:16.520 --> 33:17.520
Yeah.

33:17.520 --> 33:18.520
Nice.

33:18.520 --> 33:19.520
And, yeah.

33:19.520 --> 33:24.080
And then, and, and, and following these big companies like Amazon and Google is really

33:24.080 --> 33:28.000
important because you, you can see like, oh, they just released, you know, DSST&E,

33:28.000 --> 33:33.360
their new machine learning library, which needed to be renamed, but, yeah, destiny.

33:33.360 --> 33:34.360
It's great.

33:34.360 --> 33:43.920
I don't think so, I'm not a fan, but, I mean, in general, I think machine learning needs

33:43.920 --> 33:55.800
better marketing, like a lot, a lot, you know, not, I'm not going to diss anybody, so nice.

33:55.800 --> 34:02.800
So for folks that aren't familiar with your, your videos, are there, you know, two or

34:02.800 --> 34:10.600
three that like, oh, man, these were my favorite or these were my best are, yeah, um, I think

34:10.600 --> 34:15.160
so the, the one that ended up being most popular was AI composer.

34:15.160 --> 34:18.320
That was the second video I made for machine learning for hackers.

34:18.320 --> 34:21.920
So AI composer, so the top three would be like AI composer.

34:21.920 --> 34:26.200
The one I'm most proud of is generative adversarial networks because it was the hardest.

34:26.200 --> 34:35.120
And the one that I thought was the dopest was a build an AI artist because I just thought

34:35.120 --> 34:42.080
that application was really cool, like applying some style to some novel, you know, picture.

34:42.080 --> 34:44.600
This like the thing that Prism is doing now.

34:44.600 --> 34:45.600
That Prism is doing.

34:45.600 --> 34:46.600
Yeah.

34:46.600 --> 34:47.600
Nice.

34:47.600 --> 34:50.480
And so what is the, what's composer?

34:50.480 --> 34:56.040
Composer is generating machine, like machine generated music.

34:56.040 --> 35:01.400
Do you feed it to music, like a data set of like, you know, 500 songs, it'll learn the

35:01.400 --> 35:04.960
style of that song and then it can generate new music in that same style.

35:04.960 --> 35:05.960
Okay.

35:05.960 --> 35:09.720
And I trained it in the video over British folk music, but you could apply anything to

35:09.720 --> 35:10.720
it.

35:10.720 --> 35:15.440
One idea I thought would be really cool that someone should do is take Hans Zimmer music

35:15.440 --> 35:18.040
and generate music in the style of Hans Zimmer.

35:18.040 --> 35:19.040
So Prism for music.

35:19.040 --> 35:23.840
And that's kind of what the magenta magenta is trying, well, I'm, they're not trying

35:23.840 --> 35:29.880
to do specifically that, but did you use magenta, any of their code in your, in this project?

35:29.880 --> 35:30.880
I didn't.

35:30.880 --> 35:31.880
No, no, no, no.

35:31.880 --> 35:41.800
This was for that, it was not that, yeah, it was, I found it on GitHub and I modified it.

35:41.800 --> 35:42.800
Okay.

35:42.800 --> 35:43.800
Yeah.

35:43.800 --> 35:44.800
Nice.

35:44.800 --> 35:45.800
Interesting.

35:45.800 --> 35:46.840
What was I going to ask you?

35:46.840 --> 35:52.720
Oh, you've done a, you've done a couple of videos on chatbots and chatbot platforms.

35:52.720 --> 35:54.600
That was a good one.

35:54.600 --> 35:58.880
What do you think about that space and like, what would you learn and, you know, over a few

35:58.880 --> 36:02.160
attempts at playing around with that stuff?

36:02.160 --> 36:03.160
Yeah.

36:03.160 --> 36:12.200
I think, you know, I, with the marketing effort, I expected, I expected with AI, like Facebook's

36:12.200 --> 36:18.200
acquisition, that, that chatbot building technology to be way better than it was.

36:18.200 --> 36:23.640
But what ended up happening is I found that API.ai had a much better, it was much easier

36:23.640 --> 36:25.680
for me to build a chatbot with API.ai.

36:25.680 --> 36:26.680
Yeah.

36:26.680 --> 36:28.880
I think that chatbots in general are going to get really popular and we're going to replace

36:28.880 --> 36:30.440
all of our apps with chatbots.

36:30.440 --> 36:32.200
This is already happening in Asia.

36:32.200 --> 36:37.280
So like with WeChat, like, a lot of people don't even use apps anymore, you know, in China

36:37.280 --> 36:42.880
and stuff because it's so easy to just say like, you know, you can even combine different

36:42.880 --> 36:49.920
apps together, like book me an Uber in 30 minutes at this location and take me to my favorite

36:49.920 --> 36:50.920
restaurant.

36:50.920 --> 36:51.920
And that's querying.

36:51.920 --> 36:57.440
Like, Yelp, your Google or whatever, you know, your preferences locally and the Uber

36:57.440 --> 37:01.480
app or if it was in China, quality.

37:01.480 --> 37:03.400
So there's a lot of potential for chatbots.

37:03.400 --> 37:09.760
And if you are like right now thinking about, you know, building a startup, like if it

37:09.760 --> 37:14.000
was me, if it was me, I would be doing some kind of chatbot, chatbot for X where there

37:14.000 --> 37:18.400
is no chatbot because this is just going to get more and more popular.

37:18.400 --> 37:25.080
Like I already used chatbots in Messenger for like, you know, like detecting like scores

37:25.080 --> 37:27.120
and stuff like that.

37:27.120 --> 37:29.360
Scores of like sports and stuff.

37:29.360 --> 37:30.360
Yeah.

37:30.360 --> 37:31.360
Okay.

37:31.360 --> 37:33.800
No, I mean, not that I watch sports, but like I just play around with them.

37:33.800 --> 37:34.800
Yeah.

37:34.800 --> 37:35.800
Yeah.

37:35.800 --> 37:36.800
Yeah.

37:36.800 --> 37:41.240
But I haven't found anything that's particularly useful like the thing that I haven't played

37:41.240 --> 37:47.600
around with a bunch of them, but you know, they're all kind of, uh, yeah, which is the

37:47.600 --> 37:48.800
doesn't feel like we're there yet.

37:48.800 --> 37:49.800
Yeah, we're not there yet.

37:49.800 --> 37:50.600
We're not there yet.

37:50.600 --> 37:54.160
But we will be in like a year.

37:54.160 --> 37:55.320
That's how fast the space is moving.

37:55.320 --> 37:56.320
Yeah.

37:56.320 --> 37:57.320
Yeah.

37:57.320 --> 37:58.320
It's just going to it's just yeah.

37:58.320 --> 38:02.080
Right now there's a lot of people who are like really need deep in this stuff and

38:02.080 --> 38:04.880
they're building, but we're going to see a lot of releases.

38:04.880 --> 38:08.840
And like, you know, because the bigger players haven't caught on yet, that, you know, that's

38:08.840 --> 38:09.840
one of the reasons.

38:09.840 --> 38:15.080
But deaf, I promise you Uber has a team dedicated to this Airbnb has a team dedicated, right?

38:15.080 --> 38:16.320
Absolutely.

38:16.320 --> 38:19.880
And then Facebook's releasing M, which they're training right now full time with like humans

38:19.880 --> 38:22.920
and machines and just getting better and better and people internally at Facebook are

38:22.920 --> 38:23.920
using this.

38:23.920 --> 38:26.320
And I talked to, you know, some of these people and they really like it.

38:26.320 --> 38:30.360
Um, and I was like, please give me an invite to M like, wouldn't you like to have an invite?

38:30.360 --> 38:32.200
I'm like, oh my God.

38:32.200 --> 38:35.720
If anyone at Facebook is listening, we both want invites to ask.

38:35.720 --> 38:36.720
Yes.

38:36.720 --> 38:37.720
Please.

38:37.720 --> 38:38.720
Please.

38:38.720 --> 38:39.720
Nice.

38:39.720 --> 38:40.720
Nice.

38:40.720 --> 38:41.720
Well, that's been great.

38:41.720 --> 38:42.720
It's been great.

38:42.720 --> 38:43.720
Channel with you.

38:43.720 --> 38:48.440
Anything that you'd want to leave folks with or point them to or, you know, help them

38:48.440 --> 38:49.440
to check out.

38:49.440 --> 38:50.440
Yeah.

38:50.440 --> 38:56.200
Um, yeah, I would say definitely subscribe to my channel because I'm just getting started.

38:56.200 --> 38:59.600
And that's where I'm putting all of my effort into right now.

38:59.600 --> 39:08.640
And, uh, what else, I would say, if you're a unicorn video producer, if you're a

39:08.640 --> 39:12.320
universe machine learning, yeah, if you're, if you're a video editor who happens to know

39:12.320 --> 39:17.240
how to program as well, definitely, uh, you know, message me on Twitter, uh, because

39:17.240 --> 39:20.200
I'm looking for you because I need you.

39:20.200 --> 39:28.040
And, uh, yeah, just don't, don't give up if, if, you know, machine learning is, you

39:28.040 --> 39:33.440
know, it's, it's kind of hard, but it's a worthwhile endeavor and you can make a lot of money

39:33.440 --> 39:34.680
for it from it.

39:34.680 --> 39:41.280
And you can learn a lot and it's going to, and if you, if you get good at learning about

39:41.280 --> 39:45.800
machine learning, which is one of the, it can be one of the hardest things on the planet

39:45.800 --> 39:50.720
to learn, like solving intelligence, like the human brain, like how do we work is equivalent

39:50.720 --> 39:52.640
to asking like, what is the universe?

39:52.640 --> 39:56.000
If you can get good at that, it's just going to train your brain to be good at so many

39:56.000 --> 39:57.320
different things.

39:57.320 --> 39:59.320
So, yeah, don't give up.

39:59.320 --> 40:00.320
Awesome.

40:00.320 --> 40:01.320
Awesome.

40:01.320 --> 40:02.320
Well, thanks.

40:02.320 --> 40:03.320
Yeah.

40:03.320 --> 40:04.320
Thanks.

40:04.320 --> 40:13.240
All right, everyone, that's it for today's interview.

40:13.240 --> 40:18.200
Before we go, a reminder that this week in machine learning and AI and O'Reilly have

40:18.200 --> 40:24.280
partnered to offer one lucky listener a free pass to the inaugural O'Reilly AI conference,

40:24.280 --> 40:27.720
which will be held at the end of September in New York City.

40:27.720 --> 40:34.120
You can enter via Twitter or the twimmalei.com website by doing one of the following three things.

40:34.120 --> 40:36.640
The preferred way of entering is via Twitter.

40:36.640 --> 40:43.960
Just follow at twimmalei, T-W-I-M-L-A-I and retweet the contest tweet that I'll pin to the

40:43.960 --> 40:46.080
account and post in the show notes.

40:46.080 --> 40:48.760
Do those two things and you'll be entered.

40:48.760 --> 40:53.920
If you're not on Twitter, you can sign up for my newsletter at twimmalei.com slash newsletter

40:53.920 --> 40:58.240
and add a note, please enter me in the additional comments field.

40:58.240 --> 41:03.880
Finally, if you're not on Twitter and you aren't interested in the newsletter, no problem.

41:03.880 --> 41:09.440
Just go to the contact form on twimmalei.com and send me a message with that form using

41:09.440 --> 41:12.560
AI contest as the subject.

41:12.560 --> 41:16.080
The drawing will be open to entries through September 1st and I'll announce the winner

41:16.080 --> 41:18.280
on the September 2nd show.

41:18.280 --> 41:20.760
Good luck and hope to see you in New York.

41:20.760 --> 41:27.760
Thanks again for listening.

