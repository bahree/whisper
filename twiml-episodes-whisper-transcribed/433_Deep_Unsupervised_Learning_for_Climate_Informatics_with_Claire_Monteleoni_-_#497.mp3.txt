All right, everyone. I am on the line with Claire Montellioni. Claire is an associate professor at the University of Colorado Boulder. Claire, welcome to the Twoma AI podcast. Thanks. I'm excited to be here.
I'm super excited to have you on a show and looking forward to our conversation. We are of course going to dig into all of the amazing work you've been doing in the client climate informatics field.
But to get us started, why don't you share a little bit about your background and how you came to work at this confluence of data science, machine learning and climate change.
Yeah, so actually I grew up in New York City and was involved in environmental activism in high school. We organized the first environmental awareness day at our high school.
You know, this is in the late 90s, Mayor Jenkins came and spoke. This was in New York City. And I remember we serve lunch on frisbees, you know, so that they would be reusable.
So some fun images there. And then I came to college, you know, with science interests, there were major such as environmental science and public policy, which I dabbled in, but then I did earth and planetary sciences.
I really wanted to understand climate change, you know, there were issues of acid rain and ozone hole and certainly global warming even back then.
But I got more fascinated by the math of computer science. I had to take computer science to understand how these climate models worked.
But really got hooked in by things like logic and recursion and all the theoretical topics that fascinated me. So I studied AI machine learning for my PhD and really environmentalism was on the back burner as more of an activist area and not an area of research for many years.
But then when I was finishing my postdoc and applying for jobs, I wrote about this idea of climate informatics because during grad school, I had seen bioinformatics emerge as really revolutionizing various areas of biology by bringing in machine learning right.
And I really wanted to do the same for the study of climate change. And I was really lucky that along the way, there were more senior people because I was a junior person who believed in this vision.
So one was my late boss at the Columbia Center for computational learning systems, David Waltz, he hired me and he said, oh, climate informatics, this sounds great. We can hire a whole group on this if you'd like. I said, I'd like to start a conference and he said, you know, go for it.
He also was able to find the right climate scientist with which to do the first paper with who was Gavin Schmidt, he's now the head of the NASA office in New York City.
And he's, you know, part of the IPCC and we did a proof of concept showing that we could reduce uncertainty on an ensemble prediction and ensembles, of course, our big and AI and machine learning and something I had been working on, but that's how predictions are made to the UN panel.
To the intergovernmental panel on climate change by people running physics driven models and then trying to interpret the high variance and spread from models from different countries. I mean, you've seen, even just for storms that sometimes the European model says the cyclone is going to hit Florida, but the American model might say that it wouldn't.
So together we started working in this area and then we launched a conference. I gave a tutorial at NERIPS in 2014 trying to spur machine learners in this field.
And yeah, there's been some further developments, but the environmental science interest dates back to high school and then I've been doing computer science since late college.
Awesome, awesome. And now when you took your your break from working in the environmental area and you were doing your doctorate, what was your focus there?
Right. Yeah. So I was actually not on applied research at all. I was more in a theoretical area. So one area that was really interesting to me, which was inspired by how humans learn was online learning in particular when your data or the model that you would like to emulate changes over time.
So learning in a game where the rules are always changing, the analog I was thinking of at the time was the stock market. Right. But later it made a lot of sense to do this for climate change, right. We can't just, you know, regress a model and then expect that the distribution underlying the data is not changing.
So it's really focusing on learning with non stationarity, which there was some work, but there were a lot of open issues there.
And I also worked on active learning as sort of my my gateway to completely unsupervised learning. So what can you do when there's just really a limit on how much labeled data that you have.
And also some work on on privacy.
Okay. Privacy as in differential privacy and things of that nature.
Yeah. So differential privacy had come out in the theory community. But my colleague, Kamalika, Chaudhary, and I published the first nirips paper on privacy preserving machine learning.
Where we, you know, took the algorithms of the time logistic regression and support vector machine and tried to re you know reengineer them to match differential privacy guarantees.
It's not an area that I've pushed on in a while, but we did.
So I do, you know, with past students and postdocs did papers on semi supervised privacy preserving machine learning privacy preserving unsupervised learning so privacy preserving dimensionality reduction.
And it kind of falls into my interest in doing AI for the service of social good.
Of course, now I'm mostly focused on environmental areas that have social good.
And so can you, you shared a bit of this in your background, but can you kind of.
Kind of lay out the landscape of your research interests and what your group is working on now is it primarily.
Is it all applied in the environmental area, are you also doing theoretical work.
Good question. There are some students doing purely theoretical some purely applied and then I really like to operate at the sweet spot.
We have some interesting stories where, you know, I told you that we had been working on online learning with non stationarity so the data is varying with time.
The idea of tracking the best expert predictor or tracking a switching expert of the switching best expert.
And also parameter free learning we had an early parameter free algorithm for learning the rate at which, you know, how bursty or how changing is this, this distribution.
What did that with respect to time. And that was the first proof of concept I did with the climate scientist to show that we could robustify the ensemble prediction of the IPCC climate models using our algorithms.
But then I had a student.
So for his PhD, we looked at how you do this over space and time because all the data is space, your temporal, it'll vary over both latitude, longitude and time.
And so at triple AI ways back, we published this algorithm in the context of of climate models, this algorithm that would learn the level of sort of changeability or non stationarity over both time and space while doing the learning tasks.
So while trying to combine the climate model ensembles predictions at now a whole time space grid.
And so this was just an algorithmic paper, there were no theorems, but if you looked at the field of online learning, it was pretty open to also have a spatial dimension or really any dimension.
So then it turned out some finance people that I was talking about were interested in it for where the other dimension is markets and other countries.
So some proof proof of concepts in that domain. But this really came full circle, I was on sabbatical in Europe and I visited Niko Locheza Bianchi, who has written, like, probably, or co written one of the main important textbooks on online learning.
With with his PhD student, we talked about the theory, how would we evaluate algorithms and how would we even think about regret when you have online learners in a spatial grid with some interaction and some observation of the losses that the others are experiencing what are sort of positive and negative results that you can get there.
And we, you know, we ended up publishing it last year in algorithmic learning theory. So I do like trajectories that hit both the theory and the practice.
And I mean academia. So of course, that's good for the students careers as well. Right. They were trying to forge this new domain.
And we're succeeding. There are people that have crossed disciplinary careers, someone who did a master's with me and CS and is now in a climate science lab doing a PhD. But in the meantime, while students are here, they do need to publish and machine learning venues.
So I like to work at the sweet spot between the two.
Nice. Nice. Yeah, I think we want to get into your recent keynote at CVPR, but continuing the thread of your your broad research interests. I'm wondering if you can speak about the, this is the opportunity and applying machine learning to climate informatics climate change environmental science in general are there.
How do you think about the field? How do you taxonomize it? What's the research frontier? Yeah.
Well, first, you know, if some of your listeners are considering an application area, I would strongly consider or recommend the study of climate change.
One, I mean, there are a few unique things about it. So if we compare it, say, to biological applications, there's much more copious data here. The data has generally been collected by public missions of NASA publicly disseminated by USGS NASA and NOAA.
And sometimes data that is simulated, so the output of physics driven models, we use as input for our machine learning because it encodes a lot of scientific knowledge and a lot of conservation laws from physics.
And so even though we can't see the future, we have this artificial lens into the future via those climate simulations. So that's really unique. And I don't know of any other field where people have been working for 50 or 40 or, I don't know, since the late 60s to, to simulate the thing that you're trying to predict, you know, my colleagues in industry who want to predict whether a user is going to click on an ad.
That's a really hard problem. And in climate physics does give us some predictability and top physicists and meteorologists have been simulating the climate using mathematical models for years.
So I heard a climate scientist say that more data that is simulated from climate models has been generated and stored than all the satellite retrievals to date. So we actually have a lot of simulated data, but there hasn't been a huge amount of effort of analyzing it in an automated way.
So we're in a very data rich scenario. And then if you think about it, it's probably the cheapest key to unlock these insights is to use data science and an AI to find underlying patterns and, you know, latent structures that we can interpret.
So, sorry, I was just going to say you don't hear that very often from machine learning researcher that, you know, there are any data rich environment and data is not their primary challenge.
Right.
And then the other thing is interfacing with the domain experts. So there are a lot of areas to climate change research. And, you know, I've tried some of them, but not all of them, but I know from experience that the climate scientists themselves, many of whom are climate modelers are computer savvy.
They're running the high performance computing, they're writing the code. So it allows analytics to be this real value add that you can just plug in.
Whereas if you're starting from scratch, and I, you know, admire the work in other domains, but it's quite hard to start from scratch where it's hard to even speak speak the same language around computing.
So, yeah, and, you know, it's pretty important. It's been pretty hot this summer.
You know, you may have seen a lot of wildfires last fall.
I think it's pretty impactful.
Even the rich history of climate scientists in computing.
Can you maybe talk a little bit about how the use of computing has evolved now that you're, or we're kind of introducing machine learning and analytics.
And what were the kinds of analysis that analyses that they were doing before, and how did that, how did they differ from what you would do introducing in more analytical and ML and a methods.
So, um, just like building any inter to the plenary field, we want to read the literature and, you know, welcome everyone into sort of a big tent. And so, um, there were, there's a whole field on statistical climatology.
And, you know, Bayesian methods were being used. The, the special sauce that machine learning brings, um, beyond statistics or geospatial statistics is often algorithmic thinking.
And anyone who's done a computer science degree will be aware of that and how you have an analysis algorithms class in your in CS, but you don't necessarily have that in other departments.
Um, so making, um, things more efficient. We could be talking about running time. We could be talking about data efficient. We could be talking about efficient with respect to the amount of labeled data that you have, which can often be the bottleneck in these real domains.
So, I think computer science did have a lot to add, but we had to find the domain experts who were open to that. Um, and then we also was very important to us to us to have a big tent and include statistics geostatistic statistical climatology.
Um, and a range of, um, fields in computer science in, um, in data science. Actually, another just anecdotes. So, um, I got this job writing about climate informatics before ever having published about it, but hoping we could do for climate, the study of climate change what, um, bioinformatics had done for biology and, um,
the Gavin Schmidt who I eventually collaborated with came to the CS department at Columbia, looking for something. Now he didn't know what exactly he was looking for. He's a climate scientist with lots and lots of data.
But they, they got together this meeting of computer science professors that the chair had tapped kind of violin told you need to go attend that meeting. So everyone was sort of, you know, a little bit bored or something. And I was really excited multiple people had forwarded it to me because they knew I was looking for this.
And I was the kind of snarky person in the back of the room anytime Gavin explained something I said, have you tried machine learning for this problem.
And so, you know, right after the talk, we, we literally got lunch right then and there. And that's actually when we started hashing out the idea for using this online learning that you would usually use to say predict the stock market, use it to robustify the predictions of the IPCC ensemble.
Now there's a range of, you know, systems and communication challenges. He was talking about also a challenge where say you're trying to study an extreme event that lives in time and space say a cyclone.
So that's going to be a huge chunk of data in time and space.
And you have these data servers at USGS or other publicly hosted sites, but even just the communications complexity of finding the boundaries in space and time of the pattern that you care about.
He said he had a student that wrote a whole thesis on adaptive querying algorithms to just find the bounds of the data cube that you wanted.
So there's a lot of systems challenges.
Also mentioned Steve Easterbrook at University of Toronto, who now heads up, he's a computer science person in software engineering and he now heads up their school of environment.
But he did a study where he embedded himself in climate modeling teams. I think he went to the UK for a few months, he went to NCAR here in Boulder and observed them because when you're studying the process of software engineering, there's a lot of sociological aspects that come in who's sharing whose code.
You know, it turns out there's a model of sea ice that's used in multiple of the different models. So the, the climate models from different countries are correlated.
He did a bug analysis and said that, you know, at the time, the bug rate was lower than when Windows NT, which was actually kind of a not a very high bar.
So there's a lot of different directions and different areas of computer science that can come to bear.
Awesome. So I suggested this before, but you recently delivered a keynote at the earth vision workshop at CVPR. Tell us about the keynote and your focus there.
So that was an event, a workshop where it sounds like, you know, I wasn't an organizer, but it sounds like vision researchers are coming together around earth environmental and climate change problems as well.
And of course, they're, you know, looking at image data, I also wanted to step back and say that any geospatial data can be viewed as image data, because what do you have, you have a matrix over, you know,
longitude and latitude of some real value, it could be temperature, it could be precipitation. So there you've got an image as far as the back end math is concerned.
If you add time, you've got essentially video, right. And so all the ideas from computer vision can apply and all the ideas that we develop for this kind of data can also apply back to computer vision.
I wanted to empower sort of domain experts around using unsupervised learning, because often you're not going to have a lot of annotated data for your domain.
I showed some case studies one in avalanche detection, and there the idea was that ground truth data literally was from humans trekking on the ground in very dangerous avalanche carders in the Alps to observe whether the snow had the characteristics of an avalanche having fallen on it.
Now, if you're looking at the satellite imagery products, you're just looking at snow, and there's some subtle textural differences as to whether an avalanche has fallen.
But our collaborators at meteor France in France, which is like Noah for France, hadn't found much skill even in humans labeling whether there had been avalanche deposits.
So we had a situation which as a machine learner or computer vision researcher is more general, which is that you're looking for a rare event.
What does that mean huge class imbalance is binary classification, but you have very, very few positive labeled examples.
But in addition, there was just a limited set of label data and we weren't going to be able to get anymore. It was just what the people on the ground literally measures this ground truth survey, and we didn't have some in between product of images where humans had annotated it because they were still training the people and it was also quite hard for them to do that.
So our thesis was ultimately you're going to do a semi supervised pipeline because in the end you want to classify avalanche. So there's some supervision as like the outer wrapper, but we wanted as much of the training as possible to be completely unsupervised.
So you can learn an unsupervised model, say a variational auto encoder, and it would make sense intuitively that look, if I give the if I train only on images of snow without avalanches, then I'd have a model for sort of the non avalanche image or the non avalanche state.
And it would make sense that I could look for I could try to classify images by on an unlabeled image send it through the trained VAE.
And if the VAE kind of struggles to reconstruct it like it outputs a high reconstruction error where you're comparing the output of the network to the original input.
Then we could say above some certain threshold that's an anomaly. And in this case, we were lucky that the anomalies were only avalanches because it was just different textures of snow.
But if you think about where is the supervised learning needed for that pipeline, certainly tuning that hyper parameter, the threshold on reconstruction error above which you'll say that's an anomaly that, of course, will need a labeled validation set because that's a hyper parameter for an ultimately supervised task.
But the way I explained it to you at first was, OK, let's learn a VAE just on the negative examples. We certainly did that and it definitely got a lift from our CNN supervised approaches that had to deal with the huge class imbalance.
But it turned out that we said, you know what, we don't want there to be a bottleneck at the very beginning of our training where we have to look at the ground truth survey and compare it to images to find images and patches that don't have avalanches in them.
Because then we're using up some of our label data, we can't use that in our validation set for hyper parameter tuning.
So our kind of twist or creative thought here was, you know what, let's skip the supervised step at the beginning. Let's just lean on unsupervised learning.
Let's just use all the satellite data we have other than some hold out labeled set to train the VAE.
So we sort of argue that the supervision should only if you have a limited amount of supervision, it should only be used in tuning your hyper parameter.
And we surprisingly got a significant lift there, which at first seems counterintuitive because the first version of the auto encoder.
And you knew you were only training on on clean snow without avalanche. But the idea is in both cases, you know, whether it was only on the negative examples or on all the examples, you get your two VAE's.
But in both cases, there's going to be that threshold hyper parameter for whether you're saying your reconstruction error is above a certain value.
And in both cases, you do get to tune that on labeled data. And now the value of the thresholds might be different when you've trained on all the data that also had, you know, some noise in it because it actually had avalanches.
But you train it to properly label a labeled validation set. And so they can both perform as well as they can. It turns out that the one with entirely unsupervised training of the VAE performed better.
So our conjecture on that is in part, it's trained on more data, right, because, but secondly, to the extent that there's, you know, a few rare examples in there with avalanche in them, that gives you kind of more of an exploration of the feature space.
And so we view that as really a form of regularization, which helps your ultimate model to be more generalized.
So it was a cool trick. And I think it can apply really in any setting where you have a major class in balance, and you don't want to use a lot of labeled data.
And so did you, you find that the, your, your margins were narrower, but the,
the supervised training to find the threshold was, was it equally, were there any differences in the two cases and the ability to find the threshold.
Now, the threshold values were different, but the threshold finding procedure was the same we use the same labeled validation set, just like if you report results of any two algorithms in a paper, they each get their hyper parameters tuned independently, but on the same labeled validation data set.
And the, the conjecture of like fun that the introduction of the avalanche images, help the VAE kind of learn texture is, did you validate that in some way it's, it's conjecture, but did you, did you run any experiments to try to understand the kind of nuance there.
Not directly, we did things like drill down and test only on,
on even less imbalanced data sets like avalanche carders and the Alps that were known to have a lot of avalanches and that kind of actually increased the lift among the methods.
But it would be interesting to try to derive so I mean another thing my group is really interested in is interpretability, the other paper I was going to talk about was, you know, about interpolations through latent space to bring two domains closer together, called domain alignment, but you bring up a really interesting interpretive ability question that we can maybe look at in future work.
So is that the interpretability paper was that one that you also talked about at the.
Yes, so this was a completely unsupervised method and understanding why it worked so well, brings in the idea of self supervision as well.
And this is an extremely general method for the task of downscaling now that's the task coming out of, you know, climate, neurology, earth sciences, whether et cetera.
And so down is the opposite of up so in computer vision, we would call that the task is sort of like super resolution or up sampling.
But to be very clear then for this audience as I know your audience is AI folks downscaling means I have some gridded data say I have a map of temperature, but I'm at a course scale.
So you know there's there's fewer grid points for a given geographical region region then at the fine scale and I'd like to infer a finer scale.
But downscaling is called so this is super general, I mean we did proofs of concept for temperature and precipitation, but of course the this could be applied to anything.
And we appealed actually so Brian Gronkey who's the one who came here did a machine learning masters and then you know got radicalized around climate change and is now a PhD student in Germany in climate science.
He did spend one summer internship at Jupiter intelligence kind of a climate startup here in Boulder, just trying to use some of the super resolution techniques that were hot off the presses in computer vision.
And that others had applied oh sorry yeah.
I mentioned gans and techniques like that. Yeah well in the end we'll mention normalizing flows that have kind of an actor critic loss function but aren't aren't gans I do have another student passionate about evaluating how jam how gans generalize which is actually quite open.
He found that these super resolution techniques in computer vision would would not do the best job at inferring and discovering the fine grained detail that is so important in environmental domains we all know that in painting can work and you would maybe infer from the outside of an image what's inside an image but.
I really have a multi resolution scale in any climatological or environmental science problem and so he instead appealed to domain alignment.
There was a paper called a line flow that was in the 2020 triple AI by Grover and others I believe it was from Stefano Airman's group at Stanford.
So the idea so normalizing flows is the next step after VAE where a VAE does learn a distribution over your latent space your compact representation but the distribution is some variant of a Gaussian.
Normalizing flows allow you to get much more complicated and interesting distributions over your latent space so to match reality better now you might have a prior that simple like an isotropic Gaussian but you go through a series of invertible transformations to sort of complicate your distribution over the latent space.
And one thing for you can do downscaling you can also do upscaling there's this automatic cycle consistency that comes for free there's not a cycle consistency loss function that's being optimized but you have exact cycle consistency because the transformations that you're learning from one domain to the other are invertible.
So the idea is you've got your course screen data say you know temperature at a course grid over the continental U.S. You also do need examples of fine grain data but you don't need pairing you don't need to know that this is the temperature map at fine scale corresponding to this input at core scale you just need to you just need independently identically distributed samples at both scales.
They don't need to be paired so that's you know different from what the field of statistical downscaling had been doing where they did have methods but they always required paired maps for training so you just need samples from both domains so now it's the same variable temperature but the domains are core scale and fine scale.
And then this align flow method the idea is that there's a shared latent space and you'll learn normalizing flow from one domain to the latent space and from the other domain to the latent space but since these are invertible that allows you to then go in any direction.
You get is this really nice probabilistic unsupervised model where once trained you can do what's called conditional sampling so if the original goal was to take a course grid temperature map and feed it to the model it'll map it down to the latent space and then map out to a fine grid map.
Also map and and because you're sampling in the latent space you can sample as many times you can take conditional samples like we showed some course grid weather map and then in our model we could sample a bunch of like fine green precipitation maps and then show it to experts do these look plausible but you can get a whole bunch of them not just one.
And are the samples conditioned on some tweakable parameter are you just picking multiple samples and there so when I said condition we're conditioning on the whole input image so the input is the core screened you're saying what are the tweakable parameters that I mean the hyper parameters as usual trade off between your kind of fidelity which is again sort of again style lost.
Versus your generalizability which is looking at trying to minimize it to it's it's sort of a maximum likelihood score that compares your distribution after all these complicated mappings to your prior because of course we always favor favor simple priors in order to avoid overfitting and so the trade off is between complicating your prior so much that you're basically overfitting.
Versus getting really good fidelity maps at the fine resolution which does involve you know evolving away from a very simple prior.
But you could also take unconditional samples so what does that mean there's no input image to the model I could just sample from this distribution I have over the latent space.
Okay now I get some point in the latent space how do I interpret that well we don't exactly know but we can send it through the flow to get a course resolution image send it through the flow to get a fine resolution image.
And then we can also do interpolation so Brian looked at two images at different times in one domain map them to the latent space and now there's this whole field on how should you walk through the latent space.
Anyway this gets into what are the geodesics of the posterior distribution you've learned over the latent space and what is the efficiency of walks that are geodesic versus just you know walking in a Euclidean type path.
But for each sample in the latent space then you can generate a course resolution image of precipitation and a fine resolution image of precipitation and so you've.
This allows for interpretability you can say you know how does the model interpret interpolate between these two points of time at high and low resolution so it's a really flexible nice method and mostly he just extended a line flow from triple AI.
But he decided to use glow for the normalizing flow which had shown great success on celebrity faces right which is where all the gans are trained and wanted to see if we could get some good good fidelity around weather.
Nice that last example of pulling the samples reminding me of the interview I did with Sasha Lachioni on the her visualizing climate impact with gans.
Awesome so the you both of those are papers that you worked on and presented at the workshop.
So that the proceedings are in ACM because our climate informatics workshop turned into a conference last year and we actually have had archivable proceedings for the past six or so years.
But yeah I I talked about those those collaborative works with my students and others at this this keynote Saturday.
Awesome awesome awesome and you mentioned climate informatics and the the conference or there's a new conference that you're part of forming climate informatics can you talk a little bit about that.
Sure so you know Gavin Schmidt and I had presented this paper. Actually, you know, Yanlequin was an early supporter I went to this invited workshop that he used to run called the learning workshop at snowbird.
I didn't choose to either submit something as a poster or a talk and I was like, oh, this is so edgy and weird. I don't know if people are going to like it. I'm just going to ask for a poster. But he really liked it and said, would you be willing to give a talk.
So I was presenting things in in mainstream machine learning and I even applied to run a workshop at like their nirips or ICML on AI for climate change, but the community wasn't ready. It was early days, you know, this was.
Definitely pry it was maybe around 2014, but anyway, so we decided that we wanted a fully interdisciplinary community and so we wanted to have a standalone event. So that's why we didn't run this as a workshop at an existing climate or geosciences event or machine learning event.
So we launched in 2011 at the New York Academy of Sciences.
There was huge participation and very international. It was challenging because hurricane Irene was bearing down on New York. So in the middle of the technical discussions around climate change, people's hotels were getting evacuated.
And so yeah, every year we've persisted. We were in Boulder for many years before I even moved here because I used to be at Columbia. And one year there was this huge flooding event in 2013 and we had to arrive for the conference right after that and figure out hotels and everything.
I also learned a lot about building a community. You know, when we had it in Boulder, we always had a hike. One time when we had to avoid a rattlesnake that provided like a real bonding experience for folks.
And of course, beer in the beautiful nature of Boulder was great for that. And then in 2019 we went international. So the workshop went to Paris in 2019 Oxford in 2020, which turned out to be virtual.
And so since 2015 we've been running these hackathons. So a climate scientist will bring up a challenge problem and a data set.
And then a bunch of teams will be formed with students and all sorts of people that want to participate industries really interested in coming and sponsoring and meeting our students.
And we make sure each team is interested in plenary. So we've had hackathons on forecasting El Nino Southern oscillation on Arctic sea ice extent and storm storm intensity tracking for tropical cyclones.
This year we're most likely doing something on wildfire. It's still in development and that event will kick off in early September.
And it'll be in Boulder slash online. And we are transitioning the climate from a Maddox conference to be a springtime event. And it'll be held in Asheville, North Carolina at the NOAA center and some other places venues around town in spring 2022 date TBD.
I think the big sort of validation of climate informatics, you know, partly universities, like such as University of Toronto started hiring professors in climate informatics. So that's really interesting.
And then the world economic forum made a call to action in 2018. They put out a report on AI for the earth. And they, you know, named climate informatics as a big priority and cited us.
And then yeah, the next big thing is that we've finally launched a journal where people in this whole spectrum, you know, all of data science and AI and all of study of climate change and the environment are welcome and encourage to submit where we feel that it has been sort of a gap.
You know, if you're not publishing only in algorithm, you also care about the application that's not always the best place for it's not always best to publish in an ML venue.
And since, and I'm hearing from domain experts that they can innovate on methods, but they still have to stick it in the method section, which mostly gets ignored by the reviewers in the domain journals.
And this is open access. You can also get an extra badge for sharing your data, your notebooks, your reproducible workflows, and another extra badge for sharing your data.
Okay, awesome. Sounds like an exciting time in the field. What's going on.
Maybe we can close out by having you share a little bit of what you're most excited about looking forward.
Yeah, so I'm broadening from climate change where we hope that we can, you know, make some change.
And hopefully we can to things that communities can do to prepare and really thinking about environmental resilience.
And this brings in infrastructures. It also brings a lot of social issues. So if we talk about a warning system for forecasting an extreme storm or how a wildfire is going to spread.
We really want to represent all voices in and all stakeholders in thinking about how the system should work to interface with society.
And we also want to recognize a legacy of environmental injustices where you may have communities that have been put near, you know, factories where they already have poor air quality.
We have increased instances of asthma or other risk factors. And so if we're warning about something like wildfire, which has, you know, huge air quality degradations, we really want to prioritize vulnerable communities first.
And really exciting. We're in a college of engineering here at CU that is the first that I'm aware of that has a funded center called rise resilient infrastructure with sustainability and equity.
Bringing environmental equity and social justice right into an engineering school. I think that's really important for any any addressing of climate change both within our country. And then also internationally how you have countries bearing the brunt of warming that you know aren't even emitting very much.
So I'm really excited to bring all these interests full circle around inclusion, equity and climate change.
Awesome. Awesome. Well, Claire, thanks so much for taking some time to share a bit about what you're doing. Very cool stuff.
Thanks for the opportunity. Thank you.
