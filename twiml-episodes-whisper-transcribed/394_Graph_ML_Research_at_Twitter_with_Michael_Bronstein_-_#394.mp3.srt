1
00:00:00,000 --> 00:00:16,200
All right, everyone. I am here with Michael Bronstein. Michael is a professor at Imperial College London and head of graph machine learning at Twitter.

2
00:00:16,200 --> 00:00:27,320
Michael, welcome back to the Twomo AI podcast. Thank you, Sam. Oh, great to be here. Absolutely is great to have an opportunity to catch up with you.

3
00:00:27,320 --> 00:00:49,520
You joined Twitter about a year ago, following the acquisition of Fabula, which you co-founded in April of 2018, if LinkedIn is guiding me correctly, which was just a few months after the last time we spoke, which was in December of 17 at Nureps in Long Beach.

4
00:00:49,520 --> 00:01:15,120
So it's been an exciting two, two and a half years for you. Yeah, that's right. So I think when we talk, it was together with Jean-Brona, and we were talking after our tutorial that we gave on this topic at Nureps, and a lot of things have changed in this field from that date in these two years. It's been really, really quick and very fast-paced.

5
00:01:15,120 --> 00:01:33,520
Awesome. Awesome. Well, why don't you give us a little bit of an update with what you've been up to, your role at Twitter and any changes in what you've been up to at Imperial College London.

6
00:01:33,520 --> 00:02:00,920
Sure. So probably since we last met, I think, when at Nureps 2017, when we also presented with Jean this tutorial, it still was rather a niche or an exotic topic, basically using neural networks to do deep learning on graphs appear to be something that is quite removed from what the majority of people were doing at that time in machine learning.

7
00:02:00,920 --> 00:02:19,520
Nowadays, it's very different in a matter of these two years, graph neural networks have become one of the most prominent topics, and if you look at the statistics of I clear, for example, that happened a couple of months ago, that was one of the most frequent keywords in the papers that were submitted.

8
00:02:19,520 --> 00:02:49,120
So you really see graph neural networks everywhere at the machine learning conferences. This is probably one of the most frequent topics, and it is always why basically graphs are a very generic abstract mathematical models for systems of relations, interactions, you can model with graphs systems in practically every field of application from particle physics to social networks to biological sciences.

9
00:02:49,120 --> 00:03:03,720
And applications from these domains, drag development, neutrino detection, different things that we've been doing in these domain of work on proteins, I'll be glad to talk about it.

10
00:03:03,720 --> 00:03:12,120
So it's really a very broad, a vast spectrum of applications and problems, and that's why this has become a very prominent topic.

11
00:03:12,120 --> 00:03:33,520
We should say that there has not been a revolution as probably some of us that have been working in this field for a while, something similar to what happened in computer vision with convolutional neural networks about eight years ago after the famous Alex net paper that actually completely revolutionized the field of computer vision.

12
00:03:33,520 --> 00:03:45,520
We've seen really in the killer up some field where graph neural networks were to make such a dramatic impact, but it is probably more an evolution extremely fast evolution.

13
00:03:45,520 --> 00:03:50,920
So in a matter of few years, basically these methods are everywhere.

14
00:03:50,920 --> 00:04:04,920
And I think there are several reasons, so if you can think of what were the key drivers for success of deep learning, these are obviously large data sets that became available in the public domain.

15
00:04:04,920 --> 00:04:15,920
So not only data sets, but also carefully designed benchmarks that include data and tasks and the way for evaluating them, the image net is the great example, then computing power.

16
00:04:15,920 --> 00:04:29,920
And in this case, it was GPUs and also software with software libraries such as PyTorch or TensorFlow, you can very easily implement and prototype deep learning systems.

17
00:04:29,920 --> 00:04:35,920
So similar things started happening in the domain of graph learning as well.

18
00:04:35,920 --> 00:04:51,920
And we have the open graph benchmark that was announced just less than a year ago in the end of 2019. There are several libraries that are professionally implemented and maintained like the deep graph library or PyTorch geometric.

19
00:04:51,920 --> 00:05:02,920
The hardware is still the good old GPUs, you can argue whether they're well suitable for dealing with graphs, but at least they do the job.

20
00:05:02,920 --> 00:05:14,920
So basically we have these magic confluence of all the factors that made deep learning successful and that promise to make deep learning on graphs successful as well.

21
00:05:14,920 --> 00:05:27,920
In terms of basically water, the key challenges that we still need to overcome, one of them is scalability, basically so far most of the research really focused on small graphs.

22
00:05:27,920 --> 00:05:34,920
So small graphs like citation networks, maybe maximum five or 10,000 nodes. This is not what you really see in practice.

23
00:05:34,920 --> 00:05:41,920
So if you look at the graphs that we need to do at Twitter, they have hundreds of millions of nodes, multiple billions of pages.

24
00:05:41,920 --> 00:05:44,920
So this is the gap of orders of magnitude.

25
00:05:44,920 --> 00:05:54,920
And until I would say very recently and with exception of a few research groups, these topics have not been addressed in the academic community.

26
00:05:54,920 --> 00:06:09,920
So we only now start seeing methods that are developed for dealing with large scale graphs and being evaluated what is probably more important on large scale data sets and open graph benchmark tries to breach this gap.

27
00:06:09,920 --> 00:06:15,920
So basically it's only the only the beginning of seeing these.

28
00:06:15,920 --> 00:06:28,920
These methods use in settings that are closed or to real life applications in industrial settings. I'm aware of a few settings where these systems are already used in production.

29
00:06:28,920 --> 00:06:40,920
Some of them are confidential information so I cannot I cannot disclose the details, but there are several companies that are already using graph neural networks and their production systems.

30
00:06:40,920 --> 00:06:45,920
It starts to become real industrial systems.

31
00:06:45,920 --> 00:07:09,920
And is it the several companies that you're thinking of is it, you know, that there are a few companies that are operating at the scale that you suggested or is it that in general, graph neural networks, you know, we're just starting to, you know, get to that kind of edge of production deployment.

32
00:07:09,920 --> 00:07:23,920
Well, so to ask that in part because I hear about it all the time, a lot of conversations about it, I get the impression that it's being used, you know, more regularly by folks.

33
00:07:23,920 --> 00:07:27,920
But you're saying that, you know, you only know of a few.

34
00:07:27,920 --> 00:07:39,920
Well, at least a few officially. So there, of course, they are used whether it's used in production system or whether it's used for research. So Pinterest was probably one of the pioneers. So they used, used it.

35
00:07:39,920 --> 00:07:42,920
Probably already a couple of years ago.

36
00:07:42,920 --> 00:07:50,920
Li Baba last year published a paper where they showed that they use graph neural networks in some of their business applications.

37
00:07:50,920 --> 00:08:13,920
Yeah, I, in thinking about the killer app for this and some of the points you made about scale, I, you know, wonder if part of the issue is that, you know, the killer app is, you know, social networks and we only have a few of those, you know, we're not, not every company is out there starting their own social network.

38
00:08:13,920 --> 00:08:41,920
Do you think that social networks are kind of uniquely positioned to use graphs or are they, you know, is graph neural networks, you know, just as strong a tool for some of the non kind of obviously graphical types of use cases like we see in health care and, you know, you mentioned physics and medicine and other things.

39
00:08:41,920 --> 00:08:52,920
Yeah, absolutely. So to start with the first part, obviously social networks like Twitter, maybe Facebook and Google are the first candidates that come into mind.

40
00:08:52,920 --> 00:09:00,920
This is probably also the very obvious graph structure data that is that is produced by by people using these platforms.

41
00:09:00,920 --> 00:09:14,920
But there are many other things recommended systems, so let's say company like Amazon for example, so there are not a social network, but they have a lot of graph structure data about how people interact with their products and they have recommended systems.

42
00:09:14,920 --> 00:09:32,920
The companies like Netflix, for example, as well with their classical already Netflix challenge, one of the typical examples of the ecosystem. So there are many more than just the obvious two or three companies that would come to your mind when talking about graph structure data.

43
00:09:32,920 --> 00:09:47,920
Beyond these, there are many other applications where graph structure data is a very natural way of describing the data that is generated in these applications that are collected in these applications. So you mentioned health care.

44
00:09:47,920 --> 00:09:58,920
Basically, the way that we can think of our body, basically it's an interaction graph between a lot of biomolecules, whether it's proteins, whether it's drugs, whether it's metabolites.

45
00:09:58,920 --> 00:10:12,920
And recently, it shows that there is a lot of benefit of thinking of it in this way. So these are very complex systems. It's probably hugely simplified way of thinking of them as a static graph. It's probably dynamic system.

46
00:10:12,920 --> 00:10:27,920
There is a lot of factors and degrees of freedom that we are probably still unable to account for, but the bottom line of this is there has been a lot of interesting progress that comes from modeling biological systems.

47
00:10:27,920 --> 00:10:40,920
In health care applications from the position of graphs, I would mention just one of them. So there was earlier this year, paper and cell, which is top biological journal.

48
00:10:40,920 --> 00:10:52,920
I grew from MIT that showed a new class of antibiotics that was virtually screened using graph neural networks.

49
00:10:52,920 --> 00:11:02,920
One of the things that I noticed in the way you talk about graph neural networks today, and maybe this is evolved over the past couple of years.

50
00:11:02,920 --> 00:11:17,920
But to your point, you talk about it as being applicable to non-uclidean structure data today as opposed to what I remember from you or perhaps from others being more focused on graph structure data.

51
00:11:17,920 --> 00:11:44,920
So there's obvious applications of it in the case of social networks and other things, but it sounds like now you're seeing and going for people to understand the broader applicability of these types of a graphical formulation beyond the things that are obviously naturally structured as graphs.

52
00:11:44,920 --> 00:11:55,920
We do like to think of it as non-uclidean structure data, basically as opposed to what's a grid-like data as you see in images or in audio or in text.

53
00:11:55,920 --> 00:12:01,920
So graphs are probably the most generic models for that, but there are many other applications of many phones, for example.

54
00:12:01,920 --> 00:12:12,920
We actually started with using deep learning on geometric objects and meshes on many phones and then we moved to the more general graphs.

55
00:12:12,920 --> 00:12:23,920
But these methods are if you're talking about other types of data that is maybe slightly different from graphs in computer graphics in computer vision community.

56
00:12:23,920 --> 00:12:31,920
There is a lot of work nowadays of using deep learning on meshes on basically discrete representations of three dimensional shapes.

57
00:12:31,920 --> 00:12:42,920
Actually a little bit ironic that computer vision has always said that you know 3d data, it's computer graphics if you're working on this.

58
00:12:42,920 --> 00:12:45,920
So you're not from our community or from the secret community, but.

59
00:12:45,920 --> 00:12:55,920
But nowadays actually if you look at the best paper words or the candy days for best paper words, probably half of them are somehow related to 3d geometry.

60
00:12:55,920 --> 00:13:06,920
Interesting how fields evolved so bottom line a lot of interesting applications in computer vision as well in computer graphics that involve geometric deep learning.

61
00:13:06,920 --> 00:13:19,920
So maybe talk a little bit about the focus of your research at the next level of detail at at Twitter and the university.

62
00:13:19,920 --> 00:13:26,920
Are you you know what have you been focused on to push the research forward.

63
00:13:26,920 --> 00:13:32,920
Yeah so frankly I'm working on a lot of things I would say the major focus.

64
00:13:32,920 --> 00:13:40,920
The major focus is geometric deep learning graphs, many folds and non-euclidean structure data as you said.

65
00:13:40,920 --> 00:13:58,920
So let me start with Twitter basically we are working on deep learning on graphs basically we my ambition at least is to make these technology broadly applicable to many problems that that we need to press using machine learning systems.

66
00:13:58,920 --> 00:14:17,920
So Twitter basically one of the core data sets and data data assets is graph structure date and it comes in a lot of different forms whether it's follow graphs or whether it's different interactions engagements of users with with content with tweets like tweeting with tweeting and so on.

67
00:14:17,920 --> 00:14:25,920
Also some other graphs that are not exposed to the public that allow for example to detect a platform manipulation or reviews.

68
00:14:25,920 --> 00:14:38,920
So basically we we are trying to develop methods graph deep learning methods that would be able to take better advantage of this of this kind of information.

69
00:14:38,920 --> 00:14:46,920
And there are several challenges as I mentioned one of them is scalability basically we're dealing with very large data sets.

70
00:14:46,920 --> 00:15:04,920
So we need to make sure that these methods scale to these to these kind of data sets which obviously automatically rolls out some of the methods that exist in the literature that are even not designed to work with these scales latency efficiency and so on.

71
00:15:04,920 --> 00:15:21,920
Another elastic that that is typical of social networks in Twitter in particular that our graphs are dynamic so it's not really a static graph that that I kind of know biological network the way that proteins interact with each other basically it is changing every time every second.

72
00:15:21,920 --> 00:15:33,920
And it's actually a graph that is basically an asynchronous theme of events basically every interaction or a user joining the platform using following some somebody a user.

73
00:15:33,920 --> 00:15:48,920
Tweeting basically it's a graph where edges or nodes are created or deleted some asynchronous time points so being able to deal with with dynamic graphs is extremely important.

74
00:15:48,920 --> 00:15:57,920
There are some other aspects of that are probably more on the theoretical research side but still very important basically understanding how these systems work.

75
00:15:57,920 --> 00:16:20,920
Because if you want to to develop a system that eventually will be serving the public you need to at least understand better to make sure that there are no vulnerabilities that it cannot be misused or manipulated so for example understanding how powerful graph neural networks are whether they can be attacked in an adversarial way.

76
00:16:20,920 --> 00:16:28,920
So what I'm talking about is you know convolutional neural networks are are pretty sensitive to adversarial perturbations adversarial noise.

77
00:16:28,920 --> 00:16:35,920
So that's some works that show that they can change single pixel in an image and it will be misclassified by convolutional neural network.

78
00:16:35,920 --> 00:16:49,920
So there have been several works recently in the domain of graph learning that show that basically you can do similar things for graphs and there are several works in that domain basically adversarial attacks on graph neural networks that actually show that.

79
00:16:49,920 --> 00:16:59,920
You can provide certain robustness theoretical grantees of how graph neural networks can be can define the gains such attacks.

80
00:16:59,920 --> 00:17:05,920
Interesting and is this work that you and your team have worked on our others in the field.

81
00:17:05,920 --> 00:17:12,920
So adversarial noise depending on works in these domain were from the group of Stefan Guniman technical University of Munich.

82
00:17:12,920 --> 00:17:19,920
But you know they started all these trend of adversarial attacks on graph neural networks.

83
00:17:19,920 --> 00:17:37,920
Interesting interesting with regard to the the dynamic nature of graphs you published a paper just recently on temporal graph networks talk a little bit about that formulation and what you're trying to do in that paper.

84
00:17:37,920 --> 00:17:47,920
So basically we what we presented is a very general framework that allows us to do deep learning on continuous time dynamic graphs.

85
00:17:47,920 --> 00:17:52,920
So basically graphs that can be considered exactly as I said before is the stream of events.

86
00:17:52,920 --> 00:17:59,920
It's whether pairwise events basically edges between nodes or events that affect the nodes.

87
00:17:59,920 --> 00:18:10,920
So graph is a good example of such situation and basically it's it's a framework that generalizes the standard static graph neural networks.

88
00:18:10,920 --> 00:18:15,920
But also some previous approaches that were developed for continuous time graphs.

89
00:18:15,920 --> 00:18:24,920
I should say that we are obviously not the first to deal with these problems but most of the works consider graphs that are just given as snapshots.

90
00:18:24,920 --> 00:18:36,920
There are a few pictures of the same graph basically this is discrete time graphs, which is quite disadvantageous model if we were to apply it for graph that change continuously like like the graph.

91
00:18:36,920 --> 00:18:43,920
So the key element there are basically there are several key elements one of them we we have a way of using memory.

92
00:18:43,920 --> 00:18:56,920
Basically we attach to each node a state that allows us to compress basically the history of the interaction of this node this node and the event that happened to this node.

93
00:18:56,920 --> 00:19:08,920
Basically we update this memory from from the neighbors from the graph and then we can do node embeddings that represent the graph at a particular point of time.

94
00:19:08,920 --> 00:19:14,920
The variable solve tasks like doing predictions about the node at certain time.

95
00:19:14,920 --> 00:19:23,920
So we can say for example whether this user is a bad actor in the network and should be banned for example or we can predict future links.

96
00:19:23,920 --> 00:19:29,920
So I know from my past interactions with either other users or maybe some content.

97
00:19:29,920 --> 00:19:39,920
I can predict what kind of content I might engage with in the future. So that's the bread and butter of incremental systems link link prediction.

98
00:19:39,920 --> 00:19:48,920
And are you holding the nodes constant and assuming the edges are dynamic or is it all dynamic.

99
00:19:48,920 --> 00:20:01,920
So we didn't consider for simplicity node and age delicious, but basically the model allows new nodes to be added nodes to be updated and new edges to be created with the nodes as well.

100
00:20:01,920 --> 00:20:14,920
So technically speaking we are looking at a node together is a hyper graph or a multi graph that that basically where there are multiple edges between between the pair of nodes.

101
00:20:14,920 --> 00:20:32,920
And you mentioned the kind of the memory that you've got associated with each node and kind of this dynamic accounting is the idea that I think you also mentioned continuous it sounds like the ideas that you can have the system.

102
00:20:32,920 --> 00:20:48,920
So running and following a stream of you know additions of nodes and edges and making predictions or updating itself and and being able to make predictions on the new things that are added as well as the existing things is that the general idea.

103
00:20:48,920 --> 00:21:05,920
Absolutely, yeah, so that's a good idea, but it's a system that is always keeping up to date based on the stream of events that is happening to this graph. I should also say that one of the key findings in the paper is that training strategies extremely important.

104
00:21:05,920 --> 00:21:22,920
So what we show is is a more advanced training strategy that allows us to do correct patching and training neural network in a way that is significantly better than that with previous simple approaches.

105
00:21:22,920 --> 00:21:35,920
Can you give us an overview of traditional training for these kinds of networks and then some of the things that you needed to do to make it work in this temporal setting.

106
00:21:35,920 --> 00:21:56,920
Probably that will be a little bit too many technical details, but basically it has to do with the way that you that you do the that you don't create the patches in the training basically there are dependencies between between the nodes that you need to handle efficiently.

107
00:21:56,920 --> 00:22:14,920
Okay, awesome, and then one of the other things that I've seen pop up in your research quite a bit recently is talking about expressivity and expressive power of graphs. What's that line of work focused on.

108
00:22:14,920 --> 00:22:26,920
So this is very important topic basically even the very understanding of how and when craft neural networks work well is still lacking to a large extent.

109
00:22:26,920 --> 00:22:39,920
So what you see in experiment in experiments for example that in some settings craft neural networks work very well and in some other settings they are more or less as some simple baseline.

110
00:22:39,920 --> 00:22:45,920
The question is what makes them work and probably more importantly what makes them fail.

111
00:22:45,920 --> 00:23:08,920
And this is not a trivial question even for me waiting it basically what do you mean by expressive power of graph neural network because here you're not considering it just the function approximation capability like in the traditional setting where you have a fixed domain and you just your neural network essentially represent some class of functions and we know that even very simple neural networks are universal approximators.

112
00:23:08,920 --> 00:23:12,920
They can approximate any function to any desired accuracy.

113
00:23:12,920 --> 00:23:22,920
Any continuous function to any desired accuracy here you need to talk about both the domain so the graph itself and the function on the graph.

114
00:23:22,920 --> 00:23:46,920
So it has been a line of works recently starting from works from the group of less kids at Stanford and Hamilton in Canada. Basically they drew parallels between craft neural networks and what is called the vice failure lemon graphism or phism test, which is a classical construction.

115
00:23:46,920 --> 00:23:56,920
Graph theory basically it's a heuristic that tests whether to graphs are isomorphic whether they are the same after permutation of nodes the topological equivalent.

116
00:23:56,920 --> 00:24:01,920
And essentially that one of the simplest versions of this algorithm is graph color refinement.

117
00:24:01,920 --> 00:24:14,920
So you color the nodes of the graph basically you attach some discrete label to the nodes and then you look at the neighbors and basically color based on the on the unique structure of the neighborhood.

118
00:24:14,920 --> 00:24:20,920
It is technically speaking it's represented as a multi set set where the same element can be repeated multiple times.

119
00:24:20,920 --> 00:24:31,920
I should say for historical context that's not the first time that the device for a lemon construction was used in machine learning about 10 years ago.

120
00:24:31,920 --> 00:24:46,920
And the group from MPI was the used the WL test for for constructing graph kernels, but let's say they in the context of deep learning on graphs that's about a year ago, the first results were published.

121
00:24:46,920 --> 00:25:00,920
And basically was shown that standard graph neural networks, the message passing neural networks were the learning the network what the network does essentially exchanges information between adjacent nodes along edges.

122
00:25:00,920 --> 00:25:08,920
There are in the best case as powerful as the WL test the vice versa lemon test.

123
00:25:08,920 --> 00:25:21,920
And it is quite interesting result. So first of all, it's important it gives your clear idea when such networks work and when they fail at least on some class of problems such as graph classification.

124
00:25:21,920 --> 00:25:30,920
And second, it's quite disappointing because it is known that a vice for a lemon test fails on even very simple cases.

125
00:25:30,920 --> 00:25:43,920
So when I say fails, meaning that there might be two non isomorphic graphs that will produce the same coloring so the vice for a lemon test is a necessary, but insufficient condition.

126
00:25:43,920 --> 00:25:58,920
It says that these graphs might be isomorphic, but we cannot tell for sure it tells you for sure that they are not isomorphic if the coloring is different, but if the coloring is the same, they might be isomorphic.

127
00:25:58,920 --> 00:26:12,920
And one of the for example, one of the structures that the WL test cannot detect is triangles. So just a simple very simple straightforward triangle triangle motif that might exist in a graph cannot be detected by by by graph neural networks.

128
00:26:12,920 --> 00:26:19,920
And this is very disappointing because triangles are extremely important patterns in social networks in biological networks.

129
00:26:19,920 --> 00:26:31,920
So if you look at works and bioinformatics and complex systems, they use sub graph structures motifs all the time and triangles appear to be very important in many applications.

130
00:26:31,920 --> 00:26:43,920
So basically there they've been follow up works the WL test. It's actually not a single test. It's a hierarchy of tests. So there is a hierarchy of vice for a lemon test that they've hired the higher order.

131
00:26:43,920 --> 00:26:56,920
Instead of considering just single color refinement for single nodes, you can look at at two pulls of nodes. So you can look at basically at K nodes at the same time.

132
00:26:56,920 --> 00:27:17,920
So basically the complexity of such tests is significantly higher. And there were works in particular from from the group of young leap on the vice and institute in Israel. Kagaimaron was his PhD student. He is now at I think that Nvidia and I was a member of his PhD exam committee.

133
00:27:17,920 --> 00:27:29,920
They show higher order graph neural networks, which are equivalent to KWL test. The problem with these methods that their memory and computational complexity is very high.

134
00:27:29,920 --> 00:27:48,920
So let's say going beyond the three WL test makes no sense. Even that complexity is quadratic, which means that if you have a graph of let's say even modest size of let's say with one million nodes, the complexity will be probably prohibited for any practical application.

135
00:27:48,920 --> 00:28:07,920
So basically what what we try to do is we try to go beyond the device for a lemon hierarchy. We wanted to see if we can basically help the message passing neural network by explicitly encoding the structure around the node in the form of some structure of this picture.

136
00:28:07,920 --> 00:28:24,920
And the simplest way to do it is to count a graph substructions, whether it's clicks, whether it's cycles, whether it's passes. So that was the paper that you mentioned that was done with my PhD student in Imperial College, Georgos Pulizas and the colleague from theater for pizza for ask.

137
00:28:24,920 --> 00:28:50,920
So what we did is very similar philosophically to position and coding. So for each node, we can provide this extra bit of information that that allows you to do different convolutional like operation or different message passing dependent on how these nodes node looks like, basically, whether it's part of certain certain structure.

138
00:28:50,920 --> 00:29:13,920
This conveys the graph neural network significant more explicitly. So for example, we can show that if you use even very simple structures like four clicks, basically fully connected graphs for nodes, you can be at least not less power than three WL test or the equivalent hierarchical networks.

139
00:29:13,920 --> 00:29:33,920
I'm saying that it is not less powerful than three WL test because we can find country examples, we can find special family of graphs that are called the strongly regular graphs where our network that we call the graph substruction network works and the three WL test fails.

140
00:29:33,920 --> 00:29:53,920
Now, we couldn't find examples to the country where our network fails and the three WL test succeeds. It doesn't mean that they don't exist, but I should say that the GSM, the graph substruction network, it's also it's a class of networks basically depends on what kind of structures you use.

141
00:29:53,920 --> 00:30:11,920
You will get different results, you can get different expressivity. What we see is that with a strong, when you when you talk about these these substructures and achieving a certain level of expressivity is the idea that you are

142
00:30:11,920 --> 00:30:31,920
constraining the network in some way to these substructures and that's what gives you the expressivity or that you're identifying the substructures and kind of noting that as a property of the node and that allows you to determine its expressivity.

143
00:30:31,920 --> 00:30:50,920
So basically, it's a standard message passing architecture, so it has linear complexity like standard message passing graph neural networks. It receives an extra bit of information, which is a local descriptor that is given for each node or we also have a version where it's given per edge instead of a node that are pre computed.

144
00:30:50,920 --> 00:31:12,920
The pre computation, the counting of substructures, of course depends on the substructure that you're counting, it might have higher complexity. For some simple substructures, there are computationally efficient methods, but you do it only once, you do it as pre computation, then the training at the inference has linear complexity, which is obviously very appealing property.

145
00:31:12,920 --> 00:31:22,920
Basically, you're as efficient as standard graph neural network, which has exactly the same architectures standard graph neural network with the addition of these of these structural descriptors.

146
00:31:22,920 --> 00:31:34,920
So the on the theoretical side, there are many, I will say, exciting questions, because at least to my knowledge, there are no results in graph theory that that that tells.

147
00:31:34,920 --> 00:31:52,920
So basically, what we conjecture is that there exist small structures in the sense that they are order of land, they are independent on the size of the graph that allow the graph neural networks basically to to to disambiguate large graphs.

148
00:31:52,920 --> 00:32:05,920
It would be probably more powerful than KW tests with very large case. Now, the results are pretty, pretty scarce. So there is what is called the graph reconstruction conjecture.

149
00:32:05,920 --> 00:32:15,920
It tells you that you can reconstruct the connectivity of the graph from substructures that contain the same graph with single node removed.

150
00:32:15,920 --> 00:32:24,920
But this is obviously not very interesting because the size of these substructures as the graph itself. So the question is whether we can do it with small structures is still an open question.

151
00:32:24,920 --> 00:32:38,920
You can probably find some very pathological examples where it fails, but probably with very high probability, you can say that with small substructures, you can be extremely, extremely expressive.

152
00:32:38,920 --> 00:32:47,920
And that's basically that's what we conjecture, that's what we see in experience. You can also see that it's.

153
00:32:47,920 --> 00:32:58,920
Maybe taking a step back, you talked about this WL heuristic is being a hierarchical set of tests.

154
00:32:58,920 --> 00:33:08,920
But the tests as I understand them are relating to, you know, working or not working or isomorphic or not isomorphic.

155
00:33:08,920 --> 00:33:16,920
Where does the hierarchy, how does the hierarchy relate to these more binary classifications of the graph as a whole?

156
00:33:16,920 --> 00:33:31,920
Right, so well, this is obviously the premise of the entire problem, right, whether the graph isomorphism or whether the fact that two graphs are isomorphic is what you want in terms of describing the the the expressivity of your graph neural network.

157
00:33:31,920 --> 00:33:36,920
It's obviously simplification because we might want to look at.

158
00:33:36,920 --> 00:33:53,920
From a different perspective that what we actually want from our graph neural network, but this is a very simple formulation that is very well studied in graph theory and basically here the expressive power is whether you are able to tell that two graphs are isomorphic or not.

159
00:33:53,920 --> 00:34:08,920
And that's exactly what the hierarchy of KWL test does basically the subsequent test K plus one WL is strictly more powerful than KWL basically because there are graphs that on which it succeeds and the KWL fails.

160
00:34:08,920 --> 00:34:17,920
So you've got successfully more complex tests, but each one gives you a greater degree of certainty that the graphs are isomorphic.

161
00:34:17,920 --> 00:34:24,920
So basically it decides it succeeds on the bigger family of graphs.

162
00:34:24,920 --> 00:34:30,920
Now, I should say that this is probably not the right way of approaching the problem of graph specificity.

163
00:34:30,920 --> 00:34:40,920
It's very cool and it allowed to establish interesting bridges between what's a classical theoretical computer science and graph theory and the field of deep learning.

164
00:34:40,920 --> 00:34:49,920
What we probably in real applications, we don't really care about exactly isomorphic graphs because it seldom happens.

165
00:34:49,920 --> 00:35:01,920
So if you think even historically the WL tests were developed for applications in chemistry where chemists wanted to see if two molecules are the same or not.

166
00:35:01,920 --> 00:35:16,920
Now, in many cases, they are not exactly the same, but they are almost the same. So what you're really interested in is some kind of graph distance graph any distance or maybe from a browser of distance that allows you to compare to basically gives you a number that tells you how similar to graphs are.

167
00:35:16,920 --> 00:35:23,920
So the graph isomorphism testing is a binary thing. It tells you there are isomorphic that are prevalent or they're not.

168
00:35:23,920 --> 00:35:41,920
But what we are probably more interested is when there are not isomorphic how much non-isomorphic they are. And this is something that is not covered that if we had that that might be something we get incorporated into the last function or, for example, during training.

169
00:35:41,920 --> 00:35:54,920
Exactly, and basically this is also interesting because we can say whether graph neural networks in principle can distinguish between isomorphic non-isomorphic graphs, but we don't tell how this extends.

170
00:35:54,920 --> 00:36:01,920
Can you say, for example, that if the graphs have distance of, I don't know, epsilon between them.

171
00:36:01,920 --> 00:36:16,920
Whether the distance between the embeddings that will be constructed by such such a graph neural networks will be any close to this epsilon. So this is actually a different kind of branch of mathematics where these problems are dealt with.

172
00:36:16,920 --> 00:36:26,920
And this is called metric geometry. So basically you're trying to approximate some ground for distance between the graphs using let's say Euclidean distance between the graph embeddings.

173
00:36:26,920 --> 00:36:43,920
And once this the new distance between graph embeddings to be as close as possible to the original ground for distance. So in ideal world, which is obviously a wishful thinking you would like this to be an isometric embedding basically an embedding that preserves distance or at least some approximation of it.

174
00:36:43,920 --> 00:36:56,920
There are many ways or many forms you can make this approximation. You can say that it's for example that it scales the distances by by certain factor by the by leaves its constant of this embedding.

175
00:36:56,920 --> 00:36:59,920
You can add some noise. You can say that I can allow.

176
00:36:59,920 --> 00:37:12,920
Absolutely distortion that the distance can be a little bit bigger or a little bit smaller. You can also write it as in the probabilistic setting as something that holds with sufficiently high probability when it's going probably approximately correct.

177
00:37:12,920 --> 00:37:22,920
So I believe that that is probably the direction where we need to go next in this field of trying to express the power of graph neural networks.

178
00:37:22,920 --> 00:37:38,920
Just to be clear on that is it coming up with any formulation for the distance between two graphs or coming up with a good formulation that is able to be expressed as an embedding space and has these properties.

179
00:37:38,920 --> 00:37:54,920
So it obviously the results will depend on the distance you use. So some distances might be easier to embed in Euclidean space. Some distances might be harder. So I think you would probably choose a distance that makes sense in your application.

180
00:37:54,920 --> 00:38:03,920
Some distance between graphs. The graph is over phase will be a practical case, basically that will be the case when the distance equals zero.

181
00:38:03,920 --> 00:38:19,920
So we do have there are kind of current ways that we can express the distance between different graphs and that it sounds like the issue is again, how well they work in this embedding sense.

182
00:38:19,920 --> 00:38:42,920
And talking about the W all test, you mentioned the idea of convolutions of on the graph, what does that mean? How does that convolution that we kind of typically think of as something that's happening in a 2D space like an image, how does that translate to graph world.

183
00:38:42,920 --> 00:38:55,920
So well, there are several ways of thinking of it. You can think of it from the spectral perspective, and I think two and a half years ago when we last time talked, we mostly talked about this interpretation.

184
00:38:55,920 --> 00:39:05,920
So think of convolution as a kind of shared weight. So if you think of a neural network that takes a vector input into vector output, it can be fully connected.

185
00:39:05,920 --> 00:39:18,920
So basically the one output dimension is combined with it's a linear combination of all the input dimensions, right? So if you think of it as a matrix, it will be just full metrics with n squared parameters.

186
00:39:18,920 --> 00:39:25,920
You can think of it as a sparsly connected network. So let's say one output neuron is connected to let's say three input neurons.

187
00:39:25,920 --> 00:39:39,920
This spars matrix, it will be linear order of degrees of freedom in convolutional neural networks, all the parameters are shared. So the weights that you used to combine, let's say these three inputs to form a single output.

188
00:39:39,920 --> 00:39:49,920
They are used for all the output neurons. And in this case, you get a very special weight matrix that has a circle structure or a top lead structure. So that's exactly the convolution operator.

189
00:39:49,920 --> 00:40:02,920
And from the spectral perspective, this operator commutes with shift. That's what we call shift neck viviance. Actually, most people call it shifting variance, but the right mathematical term in shift neck viviance.

190
00:40:02,920 --> 00:40:10,920
You can first shift your signal and then apply convolution, and it will be identical to first convolving and then shifting.

191
00:40:10,920 --> 00:40:27,920
So commuting matrices are jointly diagonalized. So every convolution is diagonalized by the eigenvectors of the shift operator, which happens to be the Fourier transform in the individual case on the green.

192
00:40:27,920 --> 00:40:46,920
That's exactly why you can formulate conversions on graphs using this analogy. So you will use some analogy of the shift operator or the plus and operator or basically any local diffusion operator as the analogy of the free basis and you can do filters in that space.

193
00:40:46,920 --> 00:40:58,920
The different perspective, the spatial perspective, it's basically this perspective of weight sharing. So basically what you have is you have a node and you have its neighbors, right? On the grid, you can number these neighbors in a canonical order.

194
00:40:58,920 --> 00:41:11,920
I can say in an image, I have a neighbor, my top neighbor, my bottom neighbor, my neighbor to the left and neighbor to the right. On the graph, you usually, unless you provide some extra information, you don't have canonical ordering of your neighbors.

195
00:41:11,920 --> 00:41:16,920
So the way of aggregating information from the neighbors must be permutation environment.

196
00:41:16,920 --> 00:41:28,920
That's why the aggregators that were used in graph neural networks are permutation environment functions, whether it's average, whether it's some, whether it's maximum, they're all permutation environment functions.

197
00:41:28,920 --> 00:41:51,920
Now, the way that you combine the information from your neighbors can also be something that is learned and that's exactly the message passing. So each node in your graph receives a message from your neighbor that depends on the feature vector in that in that neighbor and the feature vector at the node.

198
00:41:51,920 --> 00:42:02,920
And they're aggregated by this protein function is again usually sound or maximum. And the same, the same mechanism is used at every node.

199
00:42:02,920 --> 00:42:10,920
So you see now how it's similar to convolution that it uses exactly the same parameters at each position in the graph.

200
00:42:10,920 --> 00:42:21,920
And what is different from images is that the number of neighbors can be very different, but the way you aggregate them is actually completely agnostic to the structure of the graph.

201
00:42:21,920 --> 00:42:35,920
That's why if you think of the of the structure of these structures, basically we add extra information that tells you what this, what this position in the graph looks like.

202
00:42:35,920 --> 00:42:44,920
And you recently published a paper on differential graph modules for these graph convolutional networks. How does that fit into into this?

203
00:42:44,920 --> 00:42:49,920
Yeah, so this is interesting. So I would say this is a little bit different direction.

204
00:42:49,920 --> 00:43:05,920
And in most works on graph neural networks, you assume that the graph is given like the Twitter or the Facebook Friday. So you already have the graph. You have some information on the nodes. Let's now use this information, combine the information from the nodes to do something with this graph.

205
00:43:05,920 --> 00:43:08,920
Let's say to classify the nodes or classify the entire graph.

206
00:43:08,920 --> 00:43:20,920
In many applications, you don't know the graph. The graph can be actually used to model the structure of your data. And in some cases, the graph itself can be more valuable than the downstream task.

207
00:43:20,920 --> 00:43:27,920
So imagine that you have, for example, some kind of mental point cloud where each point represents a patient.

208
00:43:27,920 --> 00:43:31,920
Let's say some features from health care data records.

209
00:43:31,920 --> 00:43:46,920
And what I want to do is to use the graph to represent some similarities between these patients. So when, for example, when I see your health care record and my health care record, probably the doctors will look at them differently depending on on different.

210
00:43:46,920 --> 00:44:02,920
Method data for example, if I'm a male or if I'm female, if I'm older or if I'm young, what is my maybe some genetic creations or maybe what is my disease history and so. So the way that they will treat every node will be different.

211
00:44:02,920 --> 00:44:13,920
And that's why the graph can be used to model the data structure, another problem, more typical application from machine learning and what's a computer vision is a few short learning.

212
00:44:13,920 --> 00:44:27,920
And you have just a few labeled examples, but you have your data space. So if you can construct some representation of geometric representation of your data space that in the simplest form can be represented as a graph.

213
00:44:27,920 --> 00:44:37,920
I can tell that for example that these two nearby nodes are similar somehow. So basically I can, I can learn the structure of these of these data space.

214
00:44:37,920 --> 00:44:45,920
So these methods, maybe for learning or non linear dimension, if you reduction, they have been around for at least 20 years, maybe even more.

215
00:44:45,920 --> 00:44:52,920
So basically methods that try to blow the structure of some kind of mental space and then represented in the lower dimensional space.

216
00:44:52,920 --> 00:44:55,920
So algorithms like iso-met for example.

217
00:44:55,920 --> 00:45:02,920
So one of the classical examples is you look at images, let's say images of digits from the amnest dataset.

218
00:45:02,920 --> 00:45:07,920
These images can be extremely high dimensional, right? So it's thousand dimensional for example for these digits.

219
00:45:07,920 --> 00:45:19,920
But if you look at the points that these images form in these thousand dimensional space, the intrinsic dimensionality of these data sets is much slower.

220
00:45:19,920 --> 00:45:27,920
So many fold many fold learning algorithms try to capture these intrinsic dimension types of the data set.

221
00:45:27,920 --> 00:45:43,920
Oh, imagine the same thing combined with graph learning. So we want to build a graph that captures the structure of the of these data set and then learn something similar to convolution or diffusion operation on these on these graph.

222
00:45:43,920 --> 00:45:47,920
And basically what in this paper what we show is a way of.

223
00:45:47,920 --> 00:45:57,920
Basically an efficient construction where where the graph is constructed on the fly. So we build both the graph and the filters that are applied on this graph.

224
00:45:57,920 --> 00:46:09,920
And we show that this way we can significantly outperform existing algorithms for different healthcare applications, automatic diagnosis for the moment we want to classify patients.

225
00:46:09,920 --> 00:46:17,920
This has been done already before us actually by colleagues from from Imperial College, the group of Daniel record.

226
00:46:17,920 --> 00:46:29,920
And basically they use some handcrafted construction of the graph. Here we we don't know a priori which features are useful to build the graph because it also depends on the downstream task.

227
00:46:29,920 --> 00:46:35,920
So the graph is constructed in an optimal way for the downstream task.

228
00:46:35,920 --> 00:46:49,920
And this work are you is it you mentioned some of the work that's the way that manifolds play into kind of compressing down the space.

229
00:46:49,920 --> 00:47:04,920
Are you trying to or is it inspired by some of the ways that manifolds are applied in in neural networks or is it independent of that work.

230
00:47:04,920 --> 00:47:19,920
Not really so many for this probably not an appropriate term at least from the pure differential geometric standpoint basically slow dimensional space on this low dimensional structure which represents the data in this high dimensional bedding space.

231
00:47:19,920 --> 00:47:24,920
It's technically speaking. It's not a manifold it might have different dimensionality at different points.

232
00:47:24,920 --> 00:47:33,920
But it's a convenient mathematical model to think of it think of it as a manifold plus maybe some noise rounded.

233
00:47:33,920 --> 00:47:44,920
And for learning is is exactly this type of construction that the assumption of the reason the underlying low dimensional space that has no nuclear instruction.

234
00:47:44,920 --> 00:48:07,920
Yeah, we're things have evolved quite a bit over the past two and a half years if we talk again in two and a half years what do you think you know looking into your crystal ball how do you think this will all evolve what are we you know maybe on the verge of in your opinion or you know what you most excited about.

235
00:48:07,920 --> 00:48:17,920
Well, I think. So let me try to make I don't know probably making dictionaries is very in grateful and dangerous business.

236
00:48:17,920 --> 00:48:24,920
But still let's say what would make me happy as somebody working till basically obviously wishing to be successful.

237
00:48:24,920 --> 00:48:45,920
So first of all industrial applications basically craft neural networks being standard tool used by by companies like nowadays deep learning deep neural networks are used so craft deep craft neural networks being used as in industrial settings.

238
00:48:45,920 --> 00:48:55,920
Second killer apps and if you're if you asked me about what I would bet on as a single application. I think there will be multiple applications social networks are obviously one.

239
00:48:55,920 --> 00:49:02,920
But what would probably be the most remarkable revolutionary field is probably health care and biological sciences.

240
00:49:02,920 --> 00:49:13,920
And this is probably something where we should be looking at groundbreaking results in the next five to 10 years if everything works as expected in the game might be sooner.

241
00:49:13,920 --> 00:49:29,920
And never come and basically that tool levels or two ways of looking at it. So first of all modeling the molecules themselves as graphs is extremely powerful and extremely promising.

242
00:49:29,920 --> 00:49:35,920
Basically, you can predict properties of molecules using graph neural networks. Now where where this is important.

243
00:49:35,920 --> 00:49:43,920
If you think of the problem of drag discovery and drag development. Basically, this is extremely expensive and extremely long process.

244
00:49:43,920 --> 00:49:53,920
It takes a couple of billion dollars and about 10 years to bring a new drag on the market. Now why it is so so expensive and so and so difficult.

245
00:49:53,920 --> 00:50:08,920
If you look at the search space, it is humongously large. I think estimates right, but we will have about 10 to the 60 at least synthesizable medium size molecules. So you need to choose your candy day drag out of this humongous number.

246
00:50:08,920 --> 00:50:11,920
It's more than the number of atoms in the universe.

247
00:50:11,920 --> 00:50:19,920
So you obviously need somehow to restrict this space. So there is that number of molecules that you can actually test clinically in the experiment.

248
00:50:19,920 --> 00:50:32,920
Probably a few hundreds or a few thousands. And then the top we have this 10 to the 60. So there is this computational funnel that you need somehow to narrow down to detect the promising candidates and this can be done by virtual screening.

249
00:50:32,920 --> 00:50:42,920
And this virtual screening can be done by machine learning techniques. So graph neural networks are the one of these components you can use graph neural networks to predict properties of these molecules.

250
00:50:42,920 --> 00:50:53,920
So one of the key papers in this domain was from deep mind by Justin Gilmer, the actually the paper that gave rise to the message passing neural networks to this term.

251
00:50:53,920 --> 00:51:10,920
And the predicted properties of molecules as well as DFT methods discrete functional theory, which is let's say a cheaper version of doing simulations for molecules, but about four to five orders for magnitude faster.

252
00:51:10,920 --> 00:51:20,920
And that was in 2017, I think. Now it is, you can do way better. So this is one side. Another side is basically thinking of biological systems.

253
00:51:20,920 --> 00:51:25,920
So if you think of our body, basically we have a lot of biomolecules and keep biomolecules protein.

254
00:51:25,920 --> 00:51:44,920
We have about 20,000 different proteins that are encoded in our genes, our genes basically, they before nucleotides that form about two billion letters in our genetic code, they encode amino acids that then form chains that then fold into proteins.

255
00:51:44,920 --> 00:52:06,920
And these proteins, basically, they are, it's not a metaphor to say that they are molecules of life. Basically, they're everywhere from metabolic processes, from carrying, carrying oxygen, from, from signaling different hormones in our body, from defense against pathogens, antibodies, the structure of our skin collagen.

256
00:52:06,920 --> 00:52:24,920
And it's all proteins. We are currently not aware of any life form that is not based on proteins. Now, when we injected drug against some disease, it is usually designed to bind to a protein or multiple proteins to disrupt certain biological pathway.

257
00:52:24,920 --> 00:52:39,920
So basically, that's when you're sick, one of these interactions between the proteins goes, goes wrong, and then then the drug is aimed to fix it either enabling or disabling some of these interactions.

258
00:52:39,920 --> 00:52:45,920
So predicting how a molecule will interact with proteins is of crucial importance.

259
00:52:45,920 --> 00:52:54,920
Now, we can do it to develop new drugs, and this is one of the exciting collaborations I have with the lab of Bruno Correa from, from a PFL in Mozambique.

260
00:52:54,920 --> 00:53:10,920
They are biologists, they design proteins, for example, proteins that bind to cancer targets in, in cancer, if you know, one of the, one of the currently promising therapies is what is called the immunotherapy.

261
00:53:10,920 --> 00:53:25,920
Given the Nobel Prize in medicine two years ago, basically, it is a way of reenabling the immune system to kill malignant cells that are, that are normally killed by our immune system.

262
00:53:25,920 --> 00:53:42,920
Some cancers develop proteins that signal to the immune system cells to the T cells that these are healthy cells, and that's where the tumor multiplies and grows and the person becomes sick and eventually dies.

263
00:53:42,920 --> 00:53:54,920
So immunotherapy, basically, the key problem is designing a binder that will, you know, will bind to one of these proteins and will basically will disable these techniques.

264
00:53:54,920 --> 00:53:58,920
And one of these potential binders can be a protein on this one.

265
00:53:58,920 --> 00:54:06,920
So what we are trying to do with trying to build proteins that will bind to these proteins, and we use geometric defining for these purposes.

266
00:54:06,920 --> 00:54:16,920
Now, cancer is just one example, think of the current plate that is leveraging for the human kind, the novel coronavirus, right?

267
00:54:16,920 --> 00:54:31,920
So you can also design potential therapies or maybe a way to block these virus by binding to the viral proteins or spike proteins on the coronavirus or some of the protein candidates.

268
00:54:31,920 --> 00:54:39,920
Now, this is about designing new drugs. One of the cheaper alternatives to designing new drugs is what's called reposition your drug repurposing.

269
00:54:39,920 --> 00:54:43,920
You can take an existing drug and try to find new users for it.

270
00:54:43,920 --> 00:54:50,920
So the drug is already approved by FDA, so you know that it's not toxic, that it's safe, you just find it in your application for it.

271
00:54:50,920 --> 00:54:54,920
And the particular interesting way is to combine multiple drugs.

272
00:54:54,920 --> 00:55:03,920
So in some cases, the effect is not linear. If you take two or three drugs at the same time, they might suddenly produce a significant stronger effect.

273
00:55:03,920 --> 00:55:12,920
So there has been a work also from the from the group of less cards by Marinka Zitnik, who is now faculty member, the broad institute.

274
00:55:12,920 --> 00:55:28,920
So they use graph neural networks to predict side effects of this polyformacy of basically using multiple drugs. Now they're trying to use similar, similar models to predict synergies between drugs.

275
00:55:28,920 --> 00:55:46,920
And myself, I'm involved in a project that I'm doing with colleagues from the Faculty of Medicine at Imperial College, Kilo Vasilkov and collaborators. We are trying to find a similar genetic combination of drugs against COVID-19.

276
00:55:46,920 --> 00:56:01,920
So that's also something that we are using craft neural networks for.

277
00:56:01,920 --> 00:56:23,920
This general space of health care and disease, you know, modeling the kind of underlying biological systems as graphs under modeling the drugs as graphs and potentially modeling the diseases as graphs and, you know, each of these has their own potential, you know, upside as you've illustrated a few of those.

278
00:56:23,920 --> 00:56:31,920
That's correct. I think that these are some of the more exciting future applications were graphed your little skin and shine.

279
00:56:31,920 --> 00:56:38,920
Awesome, awesome. Well, Michael, thanks so much for taking the time to catch us up on your work.

280
00:56:38,920 --> 00:56:47,920
This has been quite a deep dive into graphical neural networks and it's been wonderful to catch up.

281
00:56:47,920 --> 00:56:49,920
Thank you very much. Thank you, Sim.

282
00:56:49,920 --> 00:56:56,920
Thank you.

