WEBVTT

00:00.000 --> 00:15.920
Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting

00:15.920 --> 00:20.880
people doing interesting things in machine learning and artificial intelligence.

00:20.880 --> 00:23.880
I'm your host Sam Charington.

00:23.880 --> 00:28.080
Just a couple of quick announcements today related to the Twimble Online Meetup.

00:28.080 --> 00:32.800
First, the video from our December meetup has been posted and it's now available on our

00:32.800 --> 00:36.920
YouTube channel and at twimbleai.com slash meetup.

00:36.920 --> 00:41.760
It was a great meetup, so if you missed it, you'll definitely want to check it out.

00:41.760 --> 00:46.240
But you definitely don't want to miss our next meetup either.

00:46.240 --> 00:52.080
On Tuesday, January 16th at 3 o'clock Pacific, we'll be joined by Microsoft Research's

00:52.080 --> 00:57.440
Timnett Gebru, who will be presenting her paper using deep learning and Google Street View

00:57.440 --> 01:03.040
to estimate the demographic makeup of neighborhoods across the United States, which has received

01:03.040 --> 01:06.560
national media attention for some of its findings.

01:06.560 --> 01:11.000
Timnett will be digging into those results as well as the pipeline she used to identify

01:11.000 --> 01:15.720
22 million cars and 50 million Google Street View images.

01:15.720 --> 01:20.920
I'm anticipating a very lively discussion segment as well to kick off the session, so make

01:20.920 --> 01:25.880
sure to bring your AI resolutions and predictions for 2018.

01:25.880 --> 01:32.480
For links to the paper or to join the meetup group, visit twimbleai.com slash meetup.

01:32.480 --> 01:34.320
Alright, onto today's show.

01:34.320 --> 01:39.520
In this episode, we hear from Kenneth Stanley, professor in the Department of Computer Science

01:39.520 --> 01:45.720
at the University of Central Florida and senior research scientist at Uber AI Labs.

01:45.720 --> 01:51.960
Kenneth studied under twimble talk number 47 guest, Risto Mikulainen at UT Austin, after

01:51.960 --> 01:57.520
geometric intelligence, the company he co-founded with Gary Marcus and others, was acquired

01:57.520 --> 01:59.520
in late 2016.

01:59.520 --> 02:04.360
Kenneth's research focuses Neuroevolution, which applies the idea of genetic algorithms

02:04.360 --> 02:08.160
to the challenge of evolving neural network architectures.

02:08.160 --> 02:14.240
In this conversation, we discuss the Neuroevolution of Augmenting Topologies, or Neat, paper

02:14.240 --> 02:20.040
that Kenneth authored along with Risto, which won the 2017 International Society for Artificial

02:20.040 --> 02:26.080
Life's Award for Outstanding Paper of the Decade 2002-2012.

02:26.080 --> 02:30.720
We also cover some of the extensions to that approach he's created since, including

02:30.720 --> 02:35.680
Hyper-Neat, which can efficiently evolve very large neural networks with connectivity

02:35.680 --> 02:41.240
patterns that look more like those of the human brain and that are generally much larger

02:41.240 --> 02:46.840
than what prior approaches to neural learning could produce, as well as novelty search,

02:46.840 --> 02:52.000
an approach that, unlike most evolutionary algorithms, has no defined objective, but

02:52.000 --> 02:55.240
rather simply searches for novel behaviors.

02:55.240 --> 03:01.760
We also cover concepts like complexification and deception, biology versus computation,

03:01.760 --> 03:06.520
and some of his other work, including his book, and Nero, a video game, complete with

03:06.520 --> 03:08.880
real-time neural evolution.

03:08.880 --> 03:13.320
This is a meaty, nerd alert interview that I think you'll really enjoy.

03:13.320 --> 03:15.880
And now on to the show.

03:15.880 --> 03:26.120
Alright, everyone, I am on the line with Kenneth Stanley.

03:26.120 --> 03:30.680
Kenneth is a professor in the Department of Computer Science at the University of Central

03:30.680 --> 03:36.160
Florida, as well as a senior research scientist at Uber AI Labs.

03:36.160 --> 03:39.120
Kenneth, welcome to this week in Machine Learning and AI.

03:39.120 --> 03:40.120
Thanks very much.

03:40.120 --> 03:41.120
Real happy to be here.

03:41.120 --> 03:42.120
Fantastic.

03:42.120 --> 03:45.800
Why don't we get started by having you tell us a little bit about your background?

03:45.800 --> 03:46.800
Sure.

03:46.800 --> 03:50.920
I've been interested in artificial intelligence since I was a little kid, maybe around

03:50.920 --> 03:55.920
eight years old, went on to major in computer science because of that, and carried that interest

03:55.920 --> 04:01.640
into graduate school, where I was at the University of Texas at Austin, where I did my PhD,

04:01.640 --> 04:06.760
and there I became interested in particular in neural networks, artificial neural networks,

04:06.760 --> 04:11.440
which are what are now the basis of deep learning, which everybody's talking about.

04:11.440 --> 04:17.280
And also what's called evolutionary computation, which means kind of Darwinian type of principles

04:17.280 --> 04:20.200
being applied inside of computer algorithms.

04:20.200 --> 04:24.040
And so the intersection of those two things is what's called today, neural evolution.

04:24.040 --> 04:28.480
So it means like evolving neural networks or like evolving brains, you could think of

04:28.480 --> 04:30.400
it as in a computer.

04:30.400 --> 04:34.960
And I guess my particular interest is just how brains evolved, you know, these amazing

04:34.960 --> 04:37.520
astronomically complex things that are in our heads.

04:37.520 --> 04:43.480
I was always fascinated by how an unguided process, seemingly an intelligent process like

04:43.480 --> 04:45.760
evolution could just produce something.

04:45.760 --> 04:50.080
So astronomically complex and amazing as our own brains.

04:50.080 --> 04:53.440
And so as a neural evolution researcher, I've been trying to figure out how can you actually

04:53.440 --> 04:58.200
make algorithms that would evolve something of similar scale and complexity?

04:58.200 --> 05:03.600
Was there anything in particular that you came across at the age of eight or so that

05:03.600 --> 05:05.440
got you interested in AI?

05:05.440 --> 05:06.440
Yeah, yeah.

05:06.440 --> 05:10.200
So at the age of eight, that's when my family bought a computer.

05:10.200 --> 05:12.680
It was like a Commodore 64.

05:12.680 --> 05:13.680
Yes.

05:13.680 --> 05:17.640
And it was, it was also I, my parents put me in a programming class.

05:17.640 --> 05:21.240
And that was on a TRS 80, which is a very old computer system.

05:21.240 --> 05:22.240
And.

05:22.240 --> 05:23.240
Flash 80.

05:23.240 --> 05:24.240
Yeah.

05:24.240 --> 05:25.240
Exactly.

05:25.240 --> 05:30.400
And I guess for some reason, like, as a little kid, it just really made an impression on

05:30.400 --> 05:33.120
me that I could tell the computer to do anything.

05:33.120 --> 05:36.680
Like I had this feeling like there was like infinite freedom in the things that I could

05:36.680 --> 05:38.840
get the computer can do to do.

05:38.840 --> 05:42.080
If only I could just figure out how to tell it what I wanted.

05:42.080 --> 05:43.080
Right.

05:43.080 --> 05:46.680
And I felt like if I could just tell it how to have a conversation with me, then it would

05:46.680 --> 05:50.800
basically be my friend or like talk to me.

05:50.800 --> 05:55.240
And I was really, really interested in just getting the computer to have a conversation

05:55.240 --> 05:59.600
with me, like a casual conversation, like how are you doing, what's your name, that kind

05:59.600 --> 06:00.840
of thing.

06:00.840 --> 06:06.000
And at first, I would write really simple programs and basic, the basic computer language

06:06.000 --> 06:09.000
that would have like little conversations like this, like I'd say, what's your name?

06:09.000 --> 06:12.480
I'd say Ken, like basically in typing and they would say, hi, Ken.

06:12.480 --> 06:16.120
And I was very impressed that we could have this kind of conversation and that I got it

06:16.120 --> 06:17.120
to do that.

06:17.120 --> 06:22.640
But I quickly hit a wall where I couldn't get it to like really do anything interesting,

06:22.640 --> 06:25.560
you know, just a very star scripted thing.

06:25.560 --> 06:30.320
And at the time, like around age eight, I thought there's some way to do this that I just

06:30.320 --> 06:33.280
need to read a book or something like there's something that would just tell me how to get

06:33.280 --> 06:35.920
it to have a real conversation with me.

06:35.920 --> 06:40.040
And I didn't realize that this is like one of the greatest problems like facing humankind,

06:40.040 --> 06:43.120
like how to get a computer actually being intelligent, like a real person.

06:43.120 --> 06:46.520
And it took me a while actually for it to strike me that this is actually like an extremely

06:46.520 --> 06:47.520
hard problem.

06:47.520 --> 06:51.720
And there's not just like some manual you can read that can get the computer to do that.

06:51.720 --> 06:55.560
So I probably, within a couple of years, I realized this is like a huge problem and then

06:55.560 --> 06:59.720
I was really interested in hooked and like, wow, this is actually hard and like there's

06:59.720 --> 07:01.520
got to be a way to do this.

07:01.520 --> 07:06.120
And I guess I would just stay captivated by that problem like forever.

07:06.120 --> 07:10.080
But I guess I changed the shift a bit in my interest because if you look at that and

07:10.080 --> 07:13.760
you look at it from the lens of like today's subfields of artificial intelligence, you

07:13.760 --> 07:16.840
probably call that natural language processing or something like that.

07:16.840 --> 07:21.840
And I kind of shifted away from that over time to more like lower level stuff, like control,

07:21.840 --> 07:22.840
like neural stuff.

07:22.840 --> 07:27.080
That was like what initially hooked me into it and got me into the AI.

07:27.080 --> 07:28.080
Interesting.

07:28.080 --> 07:30.600
And you mentioned that you studied at UT Austin.

07:30.600 --> 07:35.600
I did an interview with Risto, Michaeline and did you study with him there?

07:35.600 --> 07:36.600
Yeah.

07:36.600 --> 07:41.240
So I guess it's just a coincidence that Risto is my advisor or was my advisor during the

07:41.240 --> 07:42.240
PhD.

07:42.240 --> 07:44.240
I worked with him for years there.

07:44.240 --> 07:45.240
Yeah.

07:45.240 --> 07:46.240
Awesome.

07:46.240 --> 07:47.240
Awesome.

07:47.240 --> 07:50.360
Can you tell me a little bit about your primary research focus?

07:50.360 --> 07:51.360
Sure.

07:51.360 --> 07:56.880
So my primary research focus is in an area called Neuroevolution.

07:56.880 --> 08:02.840
And it's an area that is probably less well known in the general public like you hear tons

08:02.840 --> 08:08.480
of stuff about deep learning today, but you don't hear so much about neural evolution.

08:08.480 --> 08:14.040
It's certainly related to deep learning because both of them are about in effect neural networks.

08:14.040 --> 08:18.320
But Neuroevolution has this twist, which is that we're interested in neural networks,

08:18.320 --> 08:24.600
which are for those who don't know basically these rough abstractions of what happens in

08:24.600 --> 08:25.600
brains.

08:25.600 --> 08:28.440
Like, you know, the word neural comes from neurons and neurons are in our brain.

08:28.440 --> 08:34.520
So neural networks are roughly motivated or inspired by brains in nature, although they're

08:34.520 --> 08:37.280
not at all accurate models of them.

08:37.280 --> 08:42.760
But then in neural evolution, we're combining that with evolutionary principles, which really

08:42.760 --> 08:44.600
means kind of like breeding.

08:44.600 --> 08:47.720
Like if you think about it, like it's like if you had a neural network that does something

08:47.720 --> 08:52.120
good, like say drives a robot and makes it able to do attacks, like say, walk, like it

08:52.120 --> 08:56.720
gets your biped robot to walk, then like, neural evolution is kind of like you're breeding

08:56.720 --> 08:57.720
those brains.

08:57.720 --> 09:00.000
So you're saying, okay, I have a bunch of brains.

09:00.000 --> 09:01.000
These are artificial brains.

09:01.000 --> 09:03.600
We'll call them neural networks though, because artificial brains exaggerates like how

09:03.600 --> 09:04.600
cool they are.

09:04.600 --> 09:09.080
They do their artificial neural networks, and we would then look at like, well, how well

09:09.080 --> 09:12.960
do they get the robot to walk, like a whole bunch of them, and they call that a population.

09:12.960 --> 09:16.960
And then like we choose the ones that do better, some will do worse, and some will do better.

09:16.960 --> 09:20.440
And those that do better will have children, which basically means like new neural that

09:20.440 --> 09:24.880
will be born as offspring of the old ones that we chose, or we call that selection,

09:24.880 --> 09:26.440
we selected those.

09:26.440 --> 09:30.960
And our hope is that the offspring of those better ones will sometimes be even better

09:30.960 --> 09:31.960
than their parents.

09:31.960 --> 09:35.680
And we keep on playing this game, which is just breeding, so like it's not hard to understand,

09:35.680 --> 09:39.160
like some areas of AI are kind of complex, and are to understand at first.

09:39.160 --> 09:42.760
But intuitively, this is easy, because this is just like breeding horses or breeding dogs.

09:42.760 --> 09:47.280
They just choose the ones that are better in respect to whatever criteria you have, and

09:47.280 --> 09:50.400
then just breed them, and hope that things get better over time.

09:50.400 --> 09:53.880
And so a nerve evolution is basically about breeding these artificial things, rather than

09:53.880 --> 09:58.320
real organisms, which are these artificial neural networks, and thereby getting them to

09:58.320 --> 10:00.680
get better over generations.

10:00.680 --> 10:05.400
And what is interesting about it to me is that, like, well, it's like a simple concept

10:05.400 --> 10:09.360
in principle, at least like the initial outline that I gave is quite simple just in terms

10:09.360 --> 10:10.440
of breeding.

10:10.440 --> 10:16.000
Like under the hood, there's like real mysteries here, because this is really the process,

10:16.000 --> 10:20.560
you know, that produced you and me, and like the high level of intelligence that we have,

10:20.560 --> 10:26.240
going all the way back to single-celled organisms, and it's quite amazing to believe that, like,

10:26.240 --> 10:32.040
there is some kind of path through that space just through breeding that can lead to something

10:32.040 --> 10:35.640
like us from something so humble and simple.

10:35.640 --> 10:41.240
And to get algorithms to do that is an enormous challenge, and not fully understood right

10:41.240 --> 10:42.240
now.

10:42.240 --> 10:45.480
And that's where kind of the research comes in in the field.

10:45.480 --> 10:46.480
Interesting.

10:46.480 --> 10:47.480
Interesting.

10:47.480 --> 10:51.480
And then you're also, again, a senior research scientist at Uber AI Labs.

10:51.480 --> 10:55.600
What can you tell us about Uber AI labs and how that came about and what the charter

10:55.600 --> 10:56.600
is there?

10:56.600 --> 10:57.600
Right.

10:57.600 --> 11:02.760
So there was no Uber AI labs around nine months ago, but I was one of the co-founders of

11:02.760 --> 11:06.560
a startup company called Geometric Intelligence.

11:06.560 --> 11:12.200
My co-founders were included Gary Marcus, Zubin Garmani, and Doug Beamus.

11:12.200 --> 11:16.760
Some of them are really quite well known and have very respected researchers themselves.

11:16.760 --> 11:23.000
And we were doing in Geometric Intelligence proprietary machine learning research and

11:23.000 --> 11:27.480
developing new technologies and building a team that we were hoping to be a world-class

11:27.480 --> 11:29.320
research team.

11:29.320 --> 11:34.760
And what happened was that Uber acquired us nine months ago in December.

11:34.760 --> 11:40.720
And when Uber acquired us, they had partly one of their aspirations was to start in

11:40.720 --> 11:47.480
AI lab like a real research lab in industry that researchers the cutting edge of artificial

11:47.480 --> 11:52.960
intelligence because Uber believes and believes at the time that artificial intelligence

11:52.960 --> 11:59.240
is a critical competitive component of the industry where Uber needs to be staying at the

11:59.240 --> 12:00.560
cutting edge.

12:00.560 --> 12:05.800
And Uber has, and had before, a lot of competence already in machine learning.

12:05.800 --> 12:08.760
So it's not like there was nobody here that were plenty of people here who were very

12:08.760 --> 12:10.600
qualified in the field.

12:10.600 --> 12:14.880
But they didn't have something that was really a fundamental research lab and where they're

12:14.880 --> 12:19.120
sort of just really pushing on the cutting edge of AI itself as opposed to just applying

12:19.120 --> 12:20.120
it to internal problems.

12:20.120 --> 12:25.640
Like, for example, Uber has a team focused already that was focused on autonomous driving.

12:25.640 --> 12:30.360
And so they already had that in place, but that's an applied aspect of artificial intelligence.

12:30.360 --> 12:35.440
And so the AI lab that was founded off of the company that we started, which we founded,

12:35.440 --> 12:40.160
was really intended to be focused more in advancing the algorithms themselves.

12:40.160 --> 12:44.560
And so what Uber got was basically all at once, like all of these researchers who had

12:44.560 --> 12:48.560
this capacity to push forward the field of AI.

12:48.560 --> 12:53.400
And so you can kind of think about it roughly in analogy with similar types of research

12:53.400 --> 12:57.920
labs at big tech companies, like maybe like something like DeepMind, which was originally

12:57.920 --> 13:02.160
acquired by Google or something like Facebook AI research, or just also Google Brain and

13:02.160 --> 13:03.160
Google.

13:03.160 --> 13:05.800
So there's some rough analogy there between us.

13:05.800 --> 13:08.200
And then we're much smaller though, because we're newer.

13:08.200 --> 13:13.520
But we have the kind of similar mandates in terms of researching the cutting edge of AI.

13:13.520 --> 13:17.320
And I should say that actually we're going to, we are going to engage with the outside world

13:17.320 --> 13:18.320
in the academic community.

13:18.320 --> 13:22.680
You'll be hearing from Uber AI labs and we're going to be publishing and we understand

13:22.680 --> 13:28.320
that like just we cannot be a successful AI lab if we are not engaged with the outside

13:28.320 --> 13:29.320
world.

13:29.320 --> 13:34.000
So we will be publicizing and publishing some of our work so people can see what we're

13:34.000 --> 13:38.560
doing and so that we can communicate with other other researchers and scientists across

13:38.560 --> 13:39.560
the world.

13:39.560 --> 13:40.560
Okay.

13:40.560 --> 13:41.560
Great.

13:41.560 --> 13:42.560
Great.

13:42.560 --> 13:50.640
So a little bit about the intersection between your work and evolutionary AI and the kind

13:50.640 --> 13:54.560
of things that Uber is doing around self-driving cars.

13:54.560 --> 13:55.560
Yeah.

13:55.560 --> 13:59.560
So I can't get into specifics about what Uber is doing with their self-driving cars for

13:59.560 --> 14:00.560
a obvious reason.

14:00.560 --> 14:04.400
But I can say that Uber AI labs is diverse.

14:04.400 --> 14:09.920
I mean, that was one of the original inspirations behind geometric intelligence or the predecessor

14:09.920 --> 14:16.520
to Uber AI labs was to have a diverse group that isn't just in one particular fad which

14:16.520 --> 14:20.760
you might say deep learning is although it's obviously an important one that's making

14:20.760 --> 14:22.600
a lot of important contributions.

14:22.600 --> 14:27.280
But our philosophy was that, you know, we need to not have all our eggs in one basket.

14:27.280 --> 14:31.400
And so Uber AI labs itself is like that too and that we have a lot of diversity in terms

14:31.400 --> 14:34.600
of the expertise and areas that we cover.

14:34.600 --> 14:40.360
And so among those, we clearly are world class in neural evolution, which is the field that

14:40.360 --> 14:44.200
I just described where I've focused at most of my career.

14:44.200 --> 14:51.000
And so this is a particular direction within AI and machine learning that offers some

14:51.000 --> 14:56.640
unique insights and angles on certain types of problems that other areas might have a

14:56.640 --> 14:57.640
different take on.

14:57.640 --> 15:05.120
So in terms of like autonomous driving, I mean, it's clear that the idea of the evolution

15:05.120 --> 15:11.920
of complexity and how really high level intelligence can be evolved in terms of complex, large

15:11.920 --> 15:18.120
deep artificial neural networks has a connection in principle to how you could get a really

15:18.120 --> 15:21.880
sophisticated controller for a vehicle or something like that.

15:21.880 --> 15:26.760
And so the insights of the field of neural evolution, both directly, which means like using

15:26.760 --> 15:31.960
neural gene itself as an algorithm and indirectly in terms of insights that we gain as a side

15:31.960 --> 15:37.720
effect of doing experiments in that area, can impact how we would create algorithms that

15:37.720 --> 15:40.520
might control things like autonomous vehicles.

15:40.520 --> 15:45.440
But I should also note that it's not that it's not the case that the only application

15:45.440 --> 15:50.720
or even necessarily the main application of AI at Uber is in that area.

15:50.720 --> 15:54.840
I mean, Uber has AI problems across the gamut of all of their business components.

15:54.840 --> 15:59.160
So there's a lot of different applications that are under consideration when it comes

15:59.160 --> 16:01.800
to like AI labs and what AI helps does.

16:01.800 --> 16:02.800
Sure.

16:02.800 --> 16:10.000
So can you talk a little bit about how your research focus kind of compares and contrast

16:10.000 --> 16:13.320
with what RISTO is doing down at UT Austin?

16:13.320 --> 16:14.320
Yeah, sure.

16:14.320 --> 16:20.360
So I mean, actually, there's a lot of overlap because I mean, I'm his advisor, so I've

16:20.360 --> 16:26.480
taken a lot of the original teachings that he gave me as a basis of my career and obviously

16:26.480 --> 16:31.280
collaborated with him for years to publish some of the, in the end, it turned out to be

16:31.280 --> 16:34.760
some of the seminal papers in the area, both together.

16:34.760 --> 16:40.800
And so I think we're not actually so different in terms of like the fields that we're interested

16:40.800 --> 16:47.600
in where we may differ is more just in like what particular algorithms have we contributed

16:47.600 --> 16:53.120
to inventing sensory parted ways when I basically graduated with the PhD.

16:53.120 --> 17:00.200
And so he's focused on his own set of innovations and I've focused on my own and there's some

17:00.200 --> 17:01.200
divergence there.

17:01.200 --> 17:07.680
But we really ultimately tend to be very close because like when I've invented new things

17:07.680 --> 17:12.560
like I don't know is it, and I'm still at the University of Central Florida as a professor

17:12.560 --> 17:17.120
RISTO would sometimes build on those things in vice versa.

17:17.120 --> 17:21.440
So we're very intertwined and it's not a surprise since we started out in same area.

17:21.440 --> 17:22.840
Absolutely, absolutely.

17:22.840 --> 17:28.520
And so folks that are interested in maybe some of the background on, you know, you talked

17:28.520 --> 17:32.680
about the kind of breeding process that are really high level, RISTO and I spent quite

17:32.680 --> 17:37.880
a bit of time digging into that in more detail, you know, so folks that are interested

17:37.880 --> 17:44.560
in that might want to refer back to to that podcast since you've graduated and now that

17:44.560 --> 17:50.920
you're kind of driving your own research agenda, like what are some of the specific algorithms

17:50.920 --> 17:55.920
that you've published research on and, you know, how do they build on kind of that, the

17:55.920 --> 18:01.600
core ideas of genetic or evolutionary computing or algorithms?

18:01.600 --> 18:02.600
Yeah, sure.

18:02.600 --> 18:08.880
So, so a neural evolution, which is this idea of evolving neural networks, like one interesting

18:08.880 --> 18:14.520
thing is that when, what we're, at least for me, what I find really interesting is not

18:14.520 --> 18:19.480
just optimization, like a lot of people in machine learning think in terms of optimization,

18:19.480 --> 18:24.480
which means just like how do you get this structure to get better and better and better

18:24.480 --> 18:25.920
with respect to a task?

18:25.920 --> 18:29.680
But I'm also interested in what you might call complexification, which means like how

18:29.680 --> 18:33.960
do we get increasing complexity, like the thing that really fascinates me is like how in

18:33.960 --> 18:37.960
nature things got more complex, like insanely more complex.

18:37.960 --> 18:41.800
Not just like a little bit of incremental increases in complexity, but like from a single

18:41.800 --> 18:47.080
cell to organism to something that has in our brain a hundred trillion connections among

18:47.080 --> 18:51.360
a hundred billion cells, approximately, or a hundred billion neurons, and that's just

18:51.360 --> 18:56.120
amazing to me that like some kind of unguided process could build something like that.

18:56.120 --> 19:01.360
This is not something that was engineered and so I'm sort of always have my eye on like

19:01.360 --> 19:07.600
what is it that allows really high level astronomical levels of complexity to emerge from

19:07.600 --> 19:10.280
this kind of process, kind of automated process.

19:10.280 --> 19:13.600
And so the interesting thing in neural evolution is that every time it seems like we have an

19:13.600 --> 19:17.480
advance where we kind of figure out something about how do you get increasing complexity to

19:17.480 --> 19:22.760
happen inside of an algorithm, and we've made some advances, including the first thing

19:22.760 --> 19:28.080
that I did in grad school, which was this algorithm called neat or neural evolution of

19:28.080 --> 19:33.720
augmenting topologies, which I did with Risto, which was basically an algorithm about how

19:33.720 --> 19:39.320
can we have the neural networks that are evolving in the computer, increasing complexity over

19:39.320 --> 19:42.960
the course of the algorithm running in the computer.

19:42.960 --> 19:48.160
And it was because I had this real fascination with increasing complexity that led to us introducing

19:48.160 --> 19:51.800
this algorithm that increases complexity, but then what's interesting is that every time

19:51.800 --> 19:58.000
we make an advance like that, it sort of uncover some like deeper underlying question, because

19:58.000 --> 20:02.760
it turns out that like the explanation for why it was possible to get from one cell to

20:02.760 --> 20:08.080
trillions is really, really subtle and nuanced and complicated.

20:08.080 --> 20:13.720
And when you say that, are you speaking biologically or from a computational context?

20:13.720 --> 20:14.720
Right.

20:14.720 --> 20:15.720
Good question.

20:15.720 --> 20:16.720
Yeah.

20:16.720 --> 20:20.320
So actually, those things constantly get intertwined in my mind, like whether I'm speaking

20:20.320 --> 20:25.840
biologically or computationally, because the way I look at it is kind of like the biology

20:25.840 --> 20:31.320
and computation aren't really necessarily different things, like in effect, like if you read

20:31.320 --> 20:35.760
a biology textbook, you know, you feel like you're reading about biology, but like in effect,

20:35.760 --> 20:40.440
it's also about computers because, or at least algorithms, you know, because you're talking

20:40.440 --> 20:45.640
about a principled process that basically follows some certain kinds of rules.

20:45.640 --> 20:49.200
Just these analog computers that we really don't understand very well.

20:49.200 --> 20:52.760
Yeah, you could think of like the universe as a big analog computer, we don't really

20:52.760 --> 20:53.760
understand.

20:53.760 --> 20:58.360
And so like, I mean, but like evolution is a very algorithmic thing, you know, you're talking

20:58.360 --> 21:01.960
about there are individuals and those individuals reproduce.

21:01.960 --> 21:06.200
And then the thing that, and who gets to reproduce is based on a formula, which is, which is

21:06.200 --> 21:11.000
obviously complicated, but basically some, some individuals reproduce some, some don't.

21:11.000 --> 21:13.920
And this can be formalized as basically like a program.

21:13.920 --> 21:16.560
You could imagine writing the rules of the system.

21:16.560 --> 21:19.320
And this is what inspired the field of evolutionary computation.

21:19.320 --> 21:24.600
I mean, people saw the theories evolution in biology and thought like, you know what?

21:24.600 --> 21:29.440
This is actually not that hard to write down as a program and actually make evolution happen

21:29.440 --> 21:31.960
artificially inside of a computer.

21:31.960 --> 21:36.040
And it turned out though that like, if you just read a textbook and then, you know, learn

21:36.040 --> 21:40.480
these principles that sound like good explanatory principles for like how evolution works.

21:40.480 --> 21:43.080
Like if you read a biology, text was like, well, they know how it worked.

21:43.080 --> 21:44.400
That's an explanation.

21:44.400 --> 21:48.680
It turns out that explaining something is easier than actually implementing it, which is

21:48.680 --> 21:52.640
basically something that we found across the field of artificial intelligence.

21:52.640 --> 21:55.920
You know, you can read about, you can read a neuroscience textbook and say, this is

21:55.920 --> 21:56.920
how brains work.

21:56.920 --> 22:00.800
Of course, biology will acknowledge we don't know everything, but this is what we understand

22:00.800 --> 22:01.800
now.

22:01.800 --> 22:05.920
It's a comprehensive explanation, but it's far, far away from like telling you how to

22:05.920 --> 22:06.920
actually build a brain.

22:06.920 --> 22:10.400
I don't know how to build a brain just because we have some understanding of how brains

22:10.400 --> 22:11.400
work.

22:11.400 --> 22:12.400
It's the same with evolution.

22:12.400 --> 22:16.360
Like, we don't know how to build a true evolutionary system at the scale and magnitude

22:16.360 --> 22:21.360
of what happens on Earth, even though we know a lot of the details about what goes on.

22:21.360 --> 22:25.640
And the missing details, like the gap between what we understand and what we can actually

22:25.640 --> 22:30.400
build, that's where the research is and that's where like a lot of fascinating insights

22:30.400 --> 22:31.400
occur.

22:31.400 --> 22:37.040
Like to me, I think that to some extent, like when we make advances in artificial intelligence,

22:37.040 --> 22:41.280
we're actually learning something about biology in a sense because we're realizing that

22:41.280 --> 22:46.280
the gap in our knowledge, like what we didn't understand, are actually filled by something

22:46.280 --> 22:50.720
that we didn't expect or that wasn't in the textbook about how things work.

22:50.720 --> 22:55.560
And it's true that sometimes we may be doing things that are not actually the same as biology,

22:55.560 --> 23:00.040
but at least they're revealing gaps in our knowledge of biology because like if in some

23:00.040 --> 23:03.280
sense, if we actually knew everything about how things works, then we could just program

23:03.280 --> 23:05.280
it in, but we clearly don't.

23:05.280 --> 23:09.480
And so it's kind of like, I think AI has like a higher bar in a way than biology where

23:09.480 --> 23:14.120
in biology, like you can explain something or statistically analyze it, but then I actually

23:14.120 --> 23:16.720
actually have to build it, which is much, much harder.

23:16.720 --> 23:20.720
So it sort of forces us to grapple with the problems of the gaps in our knowledge and

23:20.720 --> 23:21.720
biology.

23:21.720 --> 23:25.640
Now some people in AI would just sort of like say not like that way of looking at things

23:25.640 --> 23:29.480
because some people in AI don't care about the biology and they just want to build intelligent

23:29.480 --> 23:33.880
things and they don't really care, do these things correspond or not with biology.

23:33.880 --> 23:34.880
That's not the goal.

23:34.880 --> 23:38.720
The goal is just to build intelligent things, we aren't like adhering to biology or

23:38.720 --> 23:39.720
not.

23:39.720 --> 23:44.080
I tend to be more biologically inspired, but I also agree that like I don't really,

23:44.080 --> 23:47.560
honestly, ultimately care whether what I build is exactly the way it works in biology

23:47.560 --> 23:53.080
or not, but I just find it interesting and inspiring that biology has achieved things

23:53.080 --> 23:58.320
that are just so amazing, I mean like human level intelligence, and I find it fascinating

23:58.320 --> 24:03.480
that we just don't know how, and like trying to probe those gaps in my understanding,

24:03.480 --> 24:09.640
I find leads to over and over again, really deep insights in artificial intelligence

24:09.640 --> 24:13.080
because it's like we suddenly realize, oh wait a second, actually there's an explanation

24:13.080 --> 24:16.800
here which is much different than what we thought it might be.

24:16.800 --> 24:23.480
And so after a graduate school, like there was a succession of those that I went through,

24:23.480 --> 24:27.880
we would realize that, you know, there's something missing still after like for example

24:27.880 --> 24:32.800
the need algorithm, which actually became the most used algorithm in this sort of niche

24:32.800 --> 24:37.280
field of neuro evolution, but we realized, you know, there's limitations on what need

24:37.280 --> 24:38.920
can ever do.

24:38.920 --> 24:42.280
And so this will, wow, can we get around those limitations?

24:42.280 --> 24:44.760
How did nature get around those limitations?

24:44.760 --> 24:50.400
So like one example is that like in need, there's this artificial DNA, which encodes the

24:50.400 --> 24:53.440
neural network, so we have to do evolution, so we have like an artificial DNA, which we

24:53.440 --> 24:58.960
call a genome, well it would have one gene per connection in this brain that's evolving.

24:58.960 --> 25:04.040
And like this is clearly not going to scale, even though like this, this brain can keep

25:04.040 --> 25:08.400
expanding, but like if you wanted to get 100 trillion connections, this is what we have

25:08.400 --> 25:13.080
in our brain right now in biology, we would need 100 trillion genes in need.

25:13.080 --> 25:15.160
And there is no way that's ever going to happen.

25:15.160 --> 25:19.080
100 trillion genes is just astronomically insanely large.

25:19.080 --> 25:24.280
And like for example, our genome in biology only has 30,000 genes, or 3 billion base pairs,

25:24.280 --> 25:26.320
another way of thinking about it.

25:26.320 --> 25:31.520
So we had to invent new algorithms, and this is after grad school and after need that

25:31.520 --> 25:36.000
could encode much, much larger structures, we called these indirect encodings, and this

25:36.000 --> 25:41.520
led to something called hyperneet eventually, which is a new kind of genetic encoding that

25:41.520 --> 25:44.040
is much more compact than the original need.

25:44.040 --> 25:49.920
And so hyperneet was something that I did after I left UT Austin, and so where I did that

25:49.920 --> 25:55.920
independently of Risto, and led to the ability to evolve much bigger in effect neural networks.

25:55.920 --> 26:01.080
And then I think one of the biggest things probably that has had a lot of impact in the

26:01.080 --> 26:05.960
field after that was something called novelty search, which is a result of discovering

26:05.960 --> 26:11.720
that in some cases the best way to get something in a search process, in evolution to kind of

26:11.720 --> 26:16.000
a search process, like you're searching through space of possibilities, is to not be trying

26:16.000 --> 26:17.000
to get it.

26:17.000 --> 26:22.400
And this was a really counterintuitive and paradoxical insight, but really important I think for

26:22.400 --> 26:24.080
realizing how things are achieved.

26:24.080 --> 26:29.280
So in other words, if you say that you're trying to breed for something, like say we want

26:29.280 --> 26:34.520
to get human level intelligence, then that actually may doom you from the start.

26:34.520 --> 26:38.400
Like sometimes the only way to get to something is to not be trying to get it.

26:38.400 --> 26:42.840
And this is a hard kind of a bitter pill to swallow, but something that, what is the mechanism

26:42.840 --> 26:45.840
of frying that keeps you from being able to get it?

26:45.840 --> 26:49.240
Yeah, so the mechanism there is something called deception.

26:49.240 --> 26:52.480
And actually this is something that applies way, way outside just neural evolution.

26:52.480 --> 26:55.560
This is a general principle for everything in life.

26:55.560 --> 26:56.560
Is that deception?

26:56.560 --> 26:58.280
It's called deception, yeah.

26:58.280 --> 27:04.720
It's basically the situation when if you are observing that things are getting better,

27:04.720 --> 27:08.200
so it's like you have some metric for what it means to be doing well, like performance

27:08.200 --> 27:11.600
metric, like let's say, how well are you able to walk?

27:11.600 --> 27:14.200
And so you have some metric that says, well, how well am I walking?

27:14.200 --> 27:18.720
And so normally, like if I was trying to get something to walk, I would select things,

27:18.720 --> 27:24.960
meaning I would breed things that are apparently better walking compared to their predecessors.

27:24.960 --> 27:26.320
And I would call that their fitness.

27:26.320 --> 27:27.800
And so that's what I mean by trying.

27:27.800 --> 27:31.240
I keep on intentionally picking things that seem to be better.

27:31.240 --> 27:36.760
And this is a very intuitive idea, like everybody for a long time felt like this is obviously

27:36.760 --> 27:40.200
the way to get things to evolve is to pick things that are better.

27:40.200 --> 27:44.640
But it turns out that if you're in a deceptive situation, which it turns out unfortunately

27:44.640 --> 27:50.480
you often are in, that you can be moving in the wrong direction, even though your metric

27:50.480 --> 27:52.520
for performance is going up.

27:52.520 --> 27:55.560
And that's because like the world is really, really complicated.

27:55.560 --> 28:00.040
So it can appear that you're improving in some way when you're actually not.

28:00.040 --> 28:04.360
And so for example, like when it comes to walking, like lunging forward like a maniac and

28:04.360 --> 28:09.560
falling down like a few feet from where you started may appear to actually be an improvement

28:09.560 --> 28:13.280
in your ability to travel, you know, because basically you're getting farther than your predecessors

28:13.280 --> 28:17.200
by throwing yourself on your head like five feet in front of you.

28:17.200 --> 28:21.640
But this is actually not a good stepping stone towards really good walking behavior.

28:21.640 --> 28:26.600
In fact, like a good stepping stone might be discovering the concept of oscillation.

28:26.600 --> 28:27.600
Like that's what your legs do.

28:27.600 --> 28:29.000
They kind of oscillate when you walk.

28:29.000 --> 28:32.760
Well, it could be that when you initially discover oscillation, you fall on your face.

28:32.760 --> 28:35.440
And so it actually looks like you're not improving.

28:35.440 --> 28:40.560
And so, but because your metric is basically how far did you go, it causes you to basically

28:40.560 --> 28:46.120
be blind to the underlying discovery that's actually essential to making the progress that

28:46.120 --> 28:48.320
you need to make in the long term.

28:48.320 --> 28:52.720
And this problem of deception is just like universal across all kinds of endeavors.

28:52.720 --> 28:54.440
Not just neurovolution.

28:54.440 --> 29:00.400
It's like, is this analogous to almost like a kind of a local maxima kind of issue?

29:00.400 --> 29:02.760
Yeah, I mean, it's basically the same thing.

29:02.760 --> 29:06.560
It's related to local maxima or local optima or premature convergence.

29:06.560 --> 29:09.680
Sometimes people would call it to getting stuck on a local optimum.

29:09.680 --> 29:14.600
But I think that the insight that we have that's different from just saying, okay, well,

29:14.600 --> 29:17.600
we just rediscovered local optimum because we already knew about local optima.

29:17.600 --> 29:18.600
Exactly.

29:18.600 --> 29:22.720
It's just how utterly profound the problem is.

29:22.720 --> 29:26.880
Then like you cannot just like, I mean, people think, well, there's ways of getting around

29:26.880 --> 29:27.880
local optimal.

29:27.880 --> 29:29.200
You know, I mean, you can do your tricks.

29:29.200 --> 29:30.200
We have diversity.

29:30.200 --> 29:31.200
We have randomness.

29:31.200 --> 29:32.200
Docasticity.

29:32.200 --> 29:35.120
There are things we can do to kind of jiggle things around a little so we don't just get

29:35.120 --> 29:38.480
stuck on a peak, which is what kind of we think of local Optimus like getting stuck on

29:38.480 --> 29:40.280
a peak in a big space.

29:40.280 --> 29:43.680
That like, that's just not going to cut it in certain types of problems because they

29:43.680 --> 29:48.600
are just so absolutely complex that almost no matter what you do, deception is going to

29:48.600 --> 29:49.800
kill you.

29:49.800 --> 29:54.240
And we showed this when we introduced this algorithm called novelty search that in some

29:54.240 --> 30:00.880
problems that it was like shockingly terrible with deception could do to you in these spaces.

30:00.880 --> 30:05.520
And what was profound was that we showed that in certain problems like this where deception

30:05.520 --> 30:06.760
is a really big problem.

30:06.760 --> 30:09.680
And I would claim that deception is a really big problem in like almost any interesting

30:09.680 --> 30:10.680
problem.

30:10.680 --> 30:13.920
And we kind of demonstrate that later if we want to get into it.

30:13.920 --> 30:18.400
But when it is a serious problem, then we showed that with this novelty search algorithm

30:18.400 --> 30:23.160
that we introduced, which was basically not trying to solve a problem, but rather it

30:23.160 --> 30:27.600
was just driven by selecting things that are more novel.

30:27.600 --> 30:31.520
So not things that are better, but just more novel, that this would actually be better

30:31.520 --> 30:36.720
at solving a problem that was deceptive than an algorithm that was actually explicitly

30:36.720 --> 30:39.520
being driven by selecting things that were better.

30:39.520 --> 30:44.840
So the lesson it showed is it can be better sometimes to not be trying to solve the problem

30:44.840 --> 30:48.680
than to actually try to solve the problem in terms of getting a better solution.

30:48.680 --> 30:53.800
And this obviously really counterintuitive in paradoxical and upsetting maybe even because

30:53.800 --> 30:58.280
it's like embarrassing in a way for anybody who's like saying, okay, I've got this really

30:58.280 --> 31:02.960
good optimization algorithm to lose to an algorithm doesn't even know what kind of problem

31:02.960 --> 31:04.280
it's trying to solve.

31:04.280 --> 31:05.960
And that's sort of what novelty search is.

31:05.960 --> 31:11.520
It's a divergent search algorithm, so basically it's just trying to find things that are different

31:11.520 --> 31:13.280
than what it's found before.

31:13.280 --> 31:18.840
It sounds a little bit like, you know, explore, exploit where you're explore is kind of optimizing

31:18.840 --> 31:19.840
for newness.

31:19.840 --> 31:26.080
Yeah, yeah, yeah, it is related to this kind of exploration, exploitation dichotomy that

31:26.080 --> 31:29.480
a lot of people talk about machine learning, but it's also different, I think.

31:29.480 --> 31:34.360
So like there's an additional element of insight here beyond that, which is really important,

31:34.360 --> 31:38.680
which is that when we think of exploitation versus exploration, like often we think of

31:38.680 --> 31:43.640
exploitation as following some gradient, which means information towards something that

31:43.640 --> 31:44.640
we are trying to get to.

31:44.640 --> 31:48.600
So in other words, we're using information to move in a direction that's intelligent.

31:48.600 --> 31:53.280
But interestingly, exploration we tend to think of as sort of random moves that are

31:53.280 --> 31:54.920
sort of ignoring the informed gradient.

31:54.920 --> 31:57.840
So it's like, let's just go somewhere and see what happens.

31:57.840 --> 31:59.280
And that's what we think of exploration.

31:59.280 --> 32:03.720
But what novelty search showed is that there is a principled kind of exploration that is

32:03.720 --> 32:09.160
not random, that actually exploration is something that's also very informed.

32:09.160 --> 32:12.640
And so in the novelty search case, you're informed by where you've been, because novelty

32:12.640 --> 32:16.320
is basically a comparison between where I am and where I've been before.

32:16.320 --> 32:18.200
So it's anything but random.

32:18.200 --> 32:20.200
It's a very informed gradient.

32:20.200 --> 32:24.880
It's just that it's the gradient of novelty instead of the gradient of the objective.

32:24.880 --> 32:28.200
And this is actually a very information rich gradient, because if you think about it, you

32:28.200 --> 32:29.480
know a lot about where you've been.

32:29.480 --> 32:33.160
In fact, you know more about where you've been than you know about where you're trying

32:33.160 --> 32:35.880
to go, because the whole problem with where you're trying to go is you don't know about

32:35.880 --> 32:36.880
it.

32:36.880 --> 32:38.240
Otherwise, you would just go there.

32:38.240 --> 32:42.920
So novelty is actually more informed, I'd say, than the objective gradient.

32:42.920 --> 32:47.960
And for this reason, it's an extremely interesting gradient to follow, like the gradient of novelty.

32:47.960 --> 32:50.800
Because you're being pushed away from where you've been before.

32:50.800 --> 32:54.960
And it turns out that you will be inevitably pushed towards higher complexity.

32:54.960 --> 32:58.120
So it's really tied into this idea of increasing complexity.

32:58.120 --> 33:01.520
Because if you think about it, as soon as you exhaust all the simple things you can do

33:01.520 --> 33:06.120
in the world, like the only choice you have if you want to continue to create novelty is

33:06.120 --> 33:08.200
to do something more complex.

33:08.200 --> 33:12.080
And so ultimately, there's an inevitability that like with novelty search that you're

33:12.080 --> 33:14.880
going to be pushed towards increasing complexity.

33:14.880 --> 33:18.960
So I think of it as almost like an information accumulator, like in order to continue to do

33:18.960 --> 33:23.320
novel things in the world, you have to accumulate information about the world.

33:23.320 --> 33:27.360
So for example, like you could imagine if you were trapped in a room and I told you

33:27.360 --> 33:31.840
like to just do novel stuff, like for a while you could just run around randomly and you'd

33:31.840 --> 33:34.720
like bump into walls and everything you do would be novel.

33:34.720 --> 33:37.440
But eventually you'd bump into all the walls in the room.

33:37.440 --> 33:39.840
And so at some point you're going to have to learn how to not bump into walls.

33:39.840 --> 33:42.560
And when you do that, you're going to have to learn what a wall is and how to sense

33:42.560 --> 33:44.520
a wall and how to navigate walls.

33:44.520 --> 33:46.840
And eventually you have to learn how to open a door because you have to get out of the

33:46.840 --> 33:48.200
room eventually to do something new.

33:48.200 --> 33:49.200
Right.

33:49.200 --> 33:51.720
And eventually you're going to have to get off planet earth and go to Mars.

33:51.720 --> 33:56.720
And clearly like doing that requires like learning extremely deep and complicated facets

33:56.720 --> 33:59.200
of how the universe works, like physics.

33:59.200 --> 34:03.640
And so you're going to be forced to become an expert on the domain where you find yourself

34:03.640 --> 34:06.760
if you're going to be pushed towards doing more and more novel things.

34:06.760 --> 34:10.360
And so knowledge actually is a very deep and interesting kind of a process.

34:10.360 --> 34:14.920
And that's why sometimes it alone will do better than actually trying to solve the problem

34:14.920 --> 34:16.520
you're trying to solve.

34:16.520 --> 34:19.440
If you think about like evolutionarily, like if you think about like how could we get

34:19.440 --> 34:24.560
to human intelligence from a single cell, it'd be crazy to do selection based on the

34:24.560 --> 34:27.080
intelligence of single celled organisms.

34:27.080 --> 34:30.680
Like we wouldn't start out by applying IQ tests to single celled organisms.

34:30.680 --> 34:32.480
That would just kill the population.

34:32.480 --> 34:35.480
I mean because none of them are intelligent at all.

34:35.480 --> 34:39.760
And so it's funny, but in a sense, the reason that we got to where we are today is because

34:39.760 --> 34:41.440
we were not trying to get there.

34:41.440 --> 34:44.680
Like if we had started out where selection was based on intelligence, then everything

34:44.680 --> 34:48.200
would have died or we would have gotten nowhere and we wouldn't have gotten to where we

34:48.200 --> 34:49.200
are today.

34:49.200 --> 34:51.520
So we see this issue of deception come up over and over again.

34:51.520 --> 34:56.960
Like it turns out that like there was a turning point long ago, eons ago, where symmetry,

34:56.960 --> 34:58.960
bilateral symmetry was discovered.

34:58.960 --> 35:00.240
These are our ancestors.

35:00.240 --> 35:03.400
There's these bilateral, these symmetric flatworms.

35:03.400 --> 35:07.080
There's no indication that they'd say anything to do with being more intelligent, but actually

35:07.080 --> 35:09.920
it does in some kind of like really, really long term sense.

35:09.920 --> 35:14.040
Like that was an important discovery that led ultimately, or stepping still in the leads

35:14.040 --> 35:17.640
ultimately to human level intelligence, but you wouldn't be able to predict that on

35:17.640 --> 35:20.080
the basis of doing an IQ test.

35:20.080 --> 35:22.080
And yet we needed to lock that in.

35:22.080 --> 35:25.600
So in some sense, we could recognize that was interesting from a novelty perspective because

35:25.600 --> 35:31.080
it was a very new innovation, but we cannot recognize it from a performance perspective

35:31.080 --> 35:36.160
because at that long, long ago point in time, it's not an indicator at all from the point

35:36.160 --> 35:40.120
of view of performance, like if the ultimate indicator is intelligence.

35:40.120 --> 35:44.520
And this is another kind of example of deception and why many things are not going to be possible

35:44.520 --> 35:49.120
to discover if we just set them as a goal and just select based on those things.

35:49.120 --> 35:52.720
And this is a principle not just for evolution, but for life too.

35:52.720 --> 35:57.120
You know, like there are many inventions that would not have been invented if they had

35:57.120 --> 36:00.880
been our goal to invent them, which is again the paradox coming up.

36:00.880 --> 36:04.960
Like computers, for example, were the first computer for based on vacuum tubes, but the

36:04.960 --> 36:08.400
people who invented vacuum tubes were not trying to invent computers.

36:08.400 --> 36:11.840
Like if you had gone back to the 1800s and told all the researchers working on vacuum

36:11.840 --> 36:17.000
tubes who were interested in electricity, that like actually there's something more interesting,

36:17.000 --> 36:19.400
like a computer, and maybe you should just invent that.

36:19.400 --> 36:21.760
Like forget this boring vacuum tube stuff.

36:21.760 --> 36:25.320
You would neither have vacuum tubes nor computers.

36:25.320 --> 36:30.720
So like once again, we needed people to be exploring very diverse ideas without having

36:30.720 --> 36:31.880
their eyes on the prize.

36:31.880 --> 36:35.840
If you think of the prize as like a computer, in order to eventually get the prize.

36:35.840 --> 36:37.640
And so there's a paradigm right there.

36:37.640 --> 36:43.200
And so this concept is so general and connected to this novelty search idea that we wrote this

36:43.200 --> 36:46.880
whole book about it called Why Greatness Cannot Be Plan.

36:46.880 --> 36:51.320
After a long time researching novelty search, and a long time for me talking in various

36:51.320 --> 36:56.280
forums and venues about novelty search, and I realized that like the principles are really

36:56.280 --> 37:01.440
general about this paradox, this is what I call the objective paradox, that like it's

37:01.440 --> 37:04.760
actually relevant to all society, like how we run our institutions.

37:04.760 --> 37:09.720
Like we give money to people based on them making progress with respect to an objective,

37:09.720 --> 37:12.960
like this is what granting agencies do, like in the sciences.

37:12.960 --> 37:15.160
And it's actually not principled in the long run.

37:15.160 --> 37:18.920
Like we have, there are other processes that need to be recognized and respected if we

37:18.920 --> 37:23.400
really want to be able to achieve really, really ambitious ends.

37:23.400 --> 37:28.320
And so that's why we wrote this book basically to introduce these principles of deception

37:28.320 --> 37:31.760
and divergent search and the objective paradox to the general public.

37:31.760 --> 37:36.120
We are hoping that maybe this would actually provoke a discussion of these things in a larger

37:36.120 --> 37:40.760
sense because of the fact that it affects many of the kind of attempts at innovation that

37:40.760 --> 37:43.000
we as a society are engaged in.

37:43.000 --> 37:47.960
So it turned out to have really broad implications across culture and society.

37:47.960 --> 37:48.960
Interesting.

37:48.960 --> 37:54.080
And then one of the papers that I noticed is one called Galactic Arms Race, is that an

37:54.080 --> 37:56.920
extension of this pork or is that a different direction?

37:56.920 --> 37:57.920
It's related.

37:57.920 --> 37:58.920
Yeah, it's related.

37:58.920 --> 38:04.840
So like we, as we started to understand this idea of, we call it sometimes divergent

38:04.840 --> 38:05.840
search.

38:05.840 --> 38:09.280
Like searches that are not aimed at a particular goal, but rather which are diverging

38:09.280 --> 38:11.120
through the space of what's possible.

38:11.120 --> 38:15.680
There are kind of searches that show you all the cool stuff that you could find, not

38:15.680 --> 38:16.680
just one thing.

38:16.680 --> 38:18.000
Evolution on Earth is kind of like that.

38:18.000 --> 38:21.960
It's not like one thing it's trying to do, it wasn't trying to get human level intelligence.

38:21.960 --> 38:25.880
It's kind of illuminated all of the possible cool stuff that's out there in nature, all

38:25.880 --> 38:28.000
of the diversity of nature.

38:28.000 --> 38:32.920
And so we started to realize these algorithms are really cool that do stuff like that, perhaps

38:32.920 --> 38:35.640
for applications in the real world.

38:35.640 --> 38:38.240
In Galactic Arms Race, the application is a video game.

38:38.240 --> 38:43.120
And our idea there was like maybe we could put one of these divergent search algorithms

38:43.120 --> 38:46.280
in a video game so it would generate the content in the game.

38:46.280 --> 38:49.960
And you'd get more and more cool content just like flowing into the game from nowhere.

38:49.960 --> 38:52.640
Like no human has to actually design or invent it.

38:52.640 --> 38:56.720
And in the case of Galactic Arms Race, it was the weapons of the ships that you fly.

38:56.720 --> 39:00.320
Like people are familiar in video games like with playing games where like you have to pick

39:00.320 --> 39:03.960
up new types of lasers or weapons or guns or something like that.

39:03.960 --> 39:04.960
Right.

39:04.960 --> 39:07.240
So we said let's let evolution invent the weapons.

39:07.240 --> 39:11.320
But with a kind of a novelty search like process where it's not like aiming for like

39:11.320 --> 39:15.360
the optimal weapon, it's just diverging through the space of weapons.

39:15.360 --> 39:18.640
But with some information about how humans are actually using them.

39:18.640 --> 39:22.120
So it's informed by the humans in the game and in real time inventing new weapons for

39:22.120 --> 39:23.840
the humans to try.

39:23.840 --> 39:28.000
And so there's an interaction called interactive evolution between what humans do and what

39:28.000 --> 39:29.480
evolution does.

39:29.480 --> 39:33.240
And it caused like all these cool weapons to be invented things that I don't have never

39:33.240 --> 39:36.680
seen in any other game that were just invented by the computer itself.

39:36.680 --> 39:40.920
And it's kind of I think a really nice exposition of like the potential of like divergent

39:40.920 --> 39:46.000
search or novelty like searches to create kind of open worlds where things are just continually

39:46.000 --> 39:47.000
generated.

39:47.000 --> 39:50.600
And sometimes we call this open-ended evolution that are interesting and hopefully without

39:50.600 --> 39:51.600
end.

39:51.600 --> 39:55.880
What's an example of a type of weapon that was invented in this game?

39:55.880 --> 39:56.880
Okay.

39:56.880 --> 39:57.880
Yeah.

39:57.880 --> 39:58.880
There's a couple good ones.

39:58.880 --> 40:01.160
So like one was I, so there's funny we started naming these things after the fact

40:01.160 --> 40:03.840
because they don't actually have names because they're invented by the computer.

40:03.840 --> 40:10.280
Like one we call the tunnel maker which would basically generate like two streams of particles,

40:10.280 --> 40:15.120
these are all particle weapons that would sort of like very slowly shoot on the left and

40:15.120 --> 40:16.440
right side of your spaceship.

40:16.440 --> 40:20.040
So basically it created a protective tunnel that you could fly through.

40:20.040 --> 40:24.160
And then in the middle of that tunnel there was another faster stream that was actually

40:24.160 --> 40:25.800
used for shooting things.

40:25.800 --> 40:29.160
So you would be creating basically like a shield that would like shoot out from your

40:29.160 --> 40:31.640
sides that you could then fly through.

40:31.640 --> 40:35.120
There was another one that we called a lasso which would just look like, it looked like

40:35.120 --> 40:39.120
a cowboy's lasso, you know, just like shot out and like created like this spiral around

40:39.120 --> 40:41.400
the enemy and then like closed in on it.

40:41.400 --> 40:44.440
And it was really surprising that this thing was invented and it was kind of interesting

40:44.440 --> 40:49.240
because I actually, it's not a great weapon in an objective sense like the lasso one because

40:49.240 --> 40:52.720
like I think it's much better probably just to shoot straight at something and kill it.

40:52.720 --> 40:55.600
But like the players loved it because of the aesthetics.

40:55.600 --> 40:59.520
It's just so interesting and fun like to have the lasso weapon and to kind of show off

40:59.520 --> 41:03.160
because it was a multiplayer game so people could see each other's lassoes that it became

41:03.160 --> 41:04.160
popular.

41:04.160 --> 41:06.800
And the game just kind of went with it, you know, the game didn't say this is objectively

41:06.800 --> 41:08.560
worse or objectively better.

41:08.560 --> 41:12.480
It just saw that people were interested in lassoes who created more lassoes and diverse

41:12.480 --> 41:16.840
lassoes and we had all these lasso weapons proliferate in the world because people liked

41:16.840 --> 41:22.600
them whether they're, you know, optimal and some objective sense or not.

41:22.600 --> 41:28.520
Is there an argument that says that the, you know, the, you know, issues around, you

41:28.520 --> 41:33.320
know that you identified in novelty search and, you know, getting led down the wrong path

41:33.320 --> 41:37.240
they example, I guess you gave us with, you know, a robot trying to learn how to walk

41:37.240 --> 41:42.640
and kind of, yeah, using a motion that kind of allows it that kind of doesn't lead it towards

41:42.640 --> 41:46.480
walking and eventually let's fall on its face.

41:46.480 --> 41:52.440
I guess the thought is are, you know, it can all of us be boiled down to just not being

41:52.440 --> 41:56.840
able to express enough sophistication in our objective function or not being able to

41:56.840 --> 42:02.240
express our objective function in the right time frame or something like that.

42:02.240 --> 42:03.240
Yeah.

42:03.240 --> 42:06.800
Actually, there's an element of truth to that view that like, yeah, like if we knew

42:06.800 --> 42:10.720
enough about the world, we could just write the objective function to take into account

42:10.720 --> 42:12.880
how the world actually works.

42:12.880 --> 42:18.280
But the problem is that like in practice, that's just impossible because like you ultimately

42:18.280 --> 42:21.520
would have to know every single thing about all the stepping stones that you would have

42:21.520 --> 42:24.320
to go through to write the objective function to take that into account.

42:24.320 --> 42:28.440
So it's like, say there's like, you know, a million steps between here and a human level

42:28.440 --> 42:29.440
AI.

42:29.440 --> 42:32.760
So it will obviously if I wrote a fitness function where your score is literally how far

42:32.760 --> 42:33.760
you are along that path.

42:33.760 --> 42:37.520
Then of course, this is like the ideal objective function is going to work out fine.

42:37.520 --> 42:40.600
But the whole point, the whole problem that we're facing just begs the question of how

42:40.600 --> 42:44.160
are we going to figure out what the stepping stones are so we're back to square one again.

42:44.160 --> 42:48.160
And so in practice, like you're probably not going to be able to do that in even like

42:48.160 --> 42:52.320
a relatively simple problem because the whole problem of searches we don't know the stepping

42:52.320 --> 42:56.520
stones, if we did, we wouldn't be doing search because we would just build the thing because

42:56.520 --> 42:59.280
we would know all the steps to get it right.

42:59.280 --> 43:04.040
So this paradox is basically unavoidable, you know, like if the problem's not interesting,

43:04.040 --> 43:06.360
then we do know the stepping stones that we don't need to do these things.

43:06.360 --> 43:07.760
But the problem's not interesting.

43:07.760 --> 43:11.440
But if the problem is interesting, it's interesting because we don't know the stepping stones.

43:11.440 --> 43:13.560
Like that's what makes it an interesting problem.

43:13.560 --> 43:17.800
And so almost any interesting problem is going to be confronting this paradox.

43:17.800 --> 43:20.840
Now that doesn't mean that there aren't some cases where search will work.

43:20.840 --> 43:24.240
Obviously it will with an objective sometimes, there's no doubt about it.

43:24.240 --> 43:28.320
In fact, deep learning has exposed that like in really high dimensional spaces between

43:28.320 --> 43:32.600
spaces of many, many parameters, like many weights in their own network, that there's

43:32.600 --> 43:34.520
less deception than we thought.

43:34.520 --> 43:36.880
Like and this has been a surprise for everybody including me.

43:36.880 --> 43:41.400
And so sometimes we still can just push sort of a brute force through the objective function

43:41.400 --> 43:45.920
because high dimensional spaces have some very odd properties and succeed at solving some

43:45.920 --> 43:46.920
problems.

43:46.920 --> 43:51.040
So we shouldn't conclude from what I'm saying that like all objectives are completely useless.

43:51.040 --> 43:53.160
They do work in some cases.

43:53.160 --> 43:57.360
But I think that it's still the case that in very, very complex problems, we are going

43:57.360 --> 43:59.120
to be facing deception.

43:59.120 --> 44:03.400
We are not going to know how to write the correct objective function to go through all

44:03.400 --> 44:07.760
those stepping stones, which are basically reflecting eons of progress to get to some

44:07.760 --> 44:09.640
of these really ambitious ends that we have.

44:09.640 --> 44:10.640
And so it's an element.

44:10.640 --> 44:13.960
It's not like everything should be done this way, but it's an ingredient that's added

44:13.960 --> 44:18.960
to our toolbox now, which is going to be important in concert with sometimes explicit

44:18.960 --> 44:19.960
objectives.

44:19.960 --> 44:22.320
And so it gives us kind of a powerful new tool.

44:22.320 --> 44:26.280
And this has actually led to a field called quality diversity where we combine quality

44:26.280 --> 44:30.960
measures with kind of diversity measures and try to do both at once in order to make

44:30.960 --> 44:35.680
a principle attempt to leverage what we know about both of those kinds of searches.

44:35.680 --> 44:36.680
Hmm.

44:36.680 --> 44:37.680
Super interesting stuff.

44:37.680 --> 44:42.800
Kenneth, I really appreciate you taking the time to speak with us about neural revolution

44:42.800 --> 44:43.800
and your research.

44:43.800 --> 44:46.480
Is there anything else that you'd like to leave us with?

44:46.480 --> 44:52.000
Well, I just, I guess just to say that take a look at neural evolution, like it's actually

44:52.000 --> 44:56.520
becoming now more recognized in deep learning that, you know, we have actually a lot of synergy

44:56.520 --> 44:59.440
with deep learning because we're also doing neural networks.

44:59.440 --> 45:04.040
And so both fields, I think, are realizing today that we have something to offer each other

45:04.040 --> 45:08.440
perhaps, you know, like a revolution can evolve architectures and deep learning can apply

45:08.440 --> 45:12.360
really powerful learning algorithms to those new complicated architectures for just

45:12.360 --> 45:13.360
as one example.

45:13.360 --> 45:18.280
And our evolution can contribute to reinforcement learning in new ways because of the way that

45:18.280 --> 45:23.200
fitness can be a different kind of driver of progress than, say, the typical gradient

45:23.200 --> 45:24.600
based approach.

45:24.600 --> 45:28.840
And so in the end, we get a possible really powerful synergy.

45:28.840 --> 45:33.280
And so I think it's worth looking at how these two things can possibly feed into each other

45:33.280 --> 45:34.280
going forward.

45:34.280 --> 45:35.280
Awesome.

45:35.280 --> 45:39.160
And what's the best way for folks to learn more about what you're doing?

45:39.160 --> 45:43.240
I'd point people to, I mean, I'm guessing you probably have some links associated with

45:43.240 --> 45:44.240
the energy.

45:44.240 --> 45:45.240
We can include a link.

45:45.240 --> 45:47.280
And I know you've got a page on the UCF site.

45:47.280 --> 45:48.280
Is that the best one?

45:48.280 --> 45:49.280
Yeah.

45:49.280 --> 45:53.560
I point people to my home page, my research group home page, both their UCF and also I

45:53.560 --> 45:58.480
can provide a link to Uber and I labs where we actually are hiring too.

45:58.480 --> 46:01.280
So people are just interested in jobs in general.

46:01.280 --> 46:02.520
That's another opportunity there.

46:02.520 --> 46:04.280
So I'll also point to that.

46:04.280 --> 46:05.280
Fantastic.

46:05.280 --> 46:06.280
Well, thanks so much, Kenneth.

46:06.280 --> 46:07.280
Yeah, thanks.

46:07.280 --> 46:08.280
It's been a pleasure.

46:08.280 --> 46:16.320
All right, everyone, that's our show for today.

46:16.320 --> 46:21.080
Thanks so much for listening and for your continued feedback and support.

46:21.080 --> 46:22.480
Thanks to your support.

46:22.480 --> 46:28.440
This podcast finished the year as a top 40 technology podcast on Apple podcasts.

46:28.440 --> 46:32.840
My producer says that one of his goals this year is to crack the top 10.

46:32.840 --> 46:37.280
And to do that, we need you to head over to your podcast app.

46:37.280 --> 46:38.280
Keep the show.

46:38.280 --> 46:42.800
Hopefully, we've earned your five stars and leave us a glowing review.

46:42.800 --> 46:48.280
And more importantly, share the podcast with your friends, family, co-workers, the Starbucks

46:48.280 --> 46:53.000
Barista, your Uber driver, everyone who might be interested.

46:53.000 --> 46:56.000
Every review, rating and share goes a long way.

46:56.000 --> 46:58.760
So thanks in advance.

46:58.760 --> 47:03.520
For more information on Kenneth or any of the topics covered in this episode, head on

47:03.520 --> 47:08.480
over to twimmolai.com slash talk slash 94.

47:08.480 --> 47:13.400
Of course, we would love to hear from you, either via a comment on the show notes page

47:13.400 --> 47:20.320
or via Twitter to at Sam Charrington or at Twimmolai or at Twimmolai.

47:20.320 --> 47:39.160
Thanks once again for listening and catch you next time.

