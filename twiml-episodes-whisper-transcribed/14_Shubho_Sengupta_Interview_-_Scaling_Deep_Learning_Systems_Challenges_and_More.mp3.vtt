WEBVTT

00:00.000 --> 00:15.800
Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting

00:15.800 --> 00:20.640
people, doing interesting things in machine learning and artificial intelligence.

00:20.640 --> 00:23.840
I'm your host, Sam Charington.

00:23.840 --> 00:27.640
I'd like to start off by thanking everyone who's taken the time to check out our new Facebook

00:27.640 --> 00:30.280
and YouTube pages over the past couple of weeks.

00:30.280 --> 00:35.840
We really enjoy hearing from listeners, whether via Facebook, YouTube, Twitter, or email,

00:35.840 --> 00:39.200
so please, please, please continue reaching out to us.

00:39.200 --> 00:43.160
We'd also like to say thanks to the folks over at O'Reilly, who provided us a ticket to

00:43.160 --> 00:47.480
giveaway for next week's Strata plus a Duke World Conference.

00:47.480 --> 00:50.920
With that being said, we do have a winner to announce.

00:50.920 --> 00:53.120
Congratulations to Evan Wright.

00:53.120 --> 00:58.480
Evan is a Principal Data Scientist at Security Startup Anomaly, and he'll be receiving a bronze

00:58.480 --> 01:02.480
pass to the conference, looking forward to meeting you, Evan.

01:02.480 --> 01:09.040
Thanks to all who participated and be on the lookout for more contests in the near future.

01:09.040 --> 01:13.320
Speaking of events, last week I announced one that I'm organizing, and I hope you'll

01:13.320 --> 01:14.320
check it out.

01:14.320 --> 01:19.040
It's called the Future of Data Summit, and it'll be taking place in Las Vegas and May

01:19.040 --> 01:22.200
at the Interop ITX Conference.

01:22.200 --> 01:25.960
To give you a taste of the great content I'm pulling together for you at the event,

01:25.960 --> 01:30.840
last week's podcast featured James McCaffrey of Microsoft Research, who's just one of

01:30.840 --> 01:34.160
the 15 great speakers you're here from at the summit.

01:34.160 --> 01:38.240
We'll be covering a bunch of really exciting topics, including of course machine learning

01:38.240 --> 01:45.400
in AI, but also IoT and edge computing, augmented and virtual reality, blockchain, algorithmic

01:45.400 --> 01:48.760
IT operations, data security, and more.

01:48.760 --> 01:54.840
You can learn more about the summit at twimmolai.com slash future of data.

01:54.840 --> 01:56.960
And now about today's show.

01:56.960 --> 02:02.360
This week my guest is Shubo Sengupta, Research Scientist at Baidu.

02:02.360 --> 02:07.040
I had the pleasure of meeting Shubo at the rework Deep Learning Summit earlier this year,

02:07.040 --> 02:11.480
where he delivered a presentation on systems challenges for deep learning, and we dig into

02:11.480 --> 02:13.800
this topic in the interview.

02:13.800 --> 02:18.680
Shubo has tons of insights to share into how to do deep learning at scale, and even

02:18.680 --> 02:23.640
if you're not operating at the scale of Baidu, I think you'll learn a ton from our conversation

02:23.640 --> 02:30.200
about architecting, productionalizing, and operationalizing deep learning systems.

02:30.200 --> 02:34.040
We also spent some time discussing the role of GPUs and hardware in building scalable

02:34.040 --> 02:38.520
machine learning systems, and that's an area he has a lot to say about, as the author

02:38.520 --> 02:44.400
of the KUDPP library, the KUDA Data Parallel Primitives Library, which was the first parallel

02:44.400 --> 02:47.440
programming library for GPUs.

02:47.440 --> 02:49.440
And now on to the show.

02:49.440 --> 03:03.600
Hello everyone, I am on the line with Shubo Sengupta, a Research Scientist at Baidu.

03:03.600 --> 03:09.720
Shubo and I met at the rework Deep Learning Summit in San Francisco last month.

03:09.720 --> 03:15.560
I really enjoyed his presentation on systems challenges for deep learning, which did a great

03:15.560 --> 03:21.840
job of talking through some of the challenges associated with deep learning at scale.

03:21.840 --> 03:27.880
And I'm very grateful to Shubo for agreeing to join me on the podcast to discuss his presentation

03:27.880 --> 03:31.600
at the summit, as well as some of his other work.

03:31.600 --> 03:36.400
So welcome Shubo, and thanks so much for joining me on the podcast.

03:36.400 --> 03:38.840
Thank you Sam, thanks for having me.

03:38.840 --> 03:42.280
Yeah, I'm really looking forward to this conversation.

03:42.280 --> 03:48.640
There were a bunch of really good presentations at the summit, but as a guy who found his

03:48.640 --> 03:55.120
way to machine learning and AI, but also spends a lot of time thinking about cloud and big

03:55.120 --> 03:58.040
data infrastructure, yours really spoke to me.

03:58.040 --> 04:01.120
So I'm looking forward to digging into it.

04:01.120 --> 04:06.320
Thanks, I was a little bit afraid that people won't like it, because it was a little bit

04:06.320 --> 04:08.320
off track in some sense.

04:08.320 --> 04:12.160
I thought it was fantastic.

04:12.160 --> 04:17.880
Why don't we start with having you share a little bit about your current role at Baidu

04:17.880 --> 04:22.640
and some of the things you've done previously and how you got involved in machine learning

04:22.640 --> 04:23.640
and AI?

04:23.640 --> 04:31.760
Sure, so as you said, I'm a research scientist at Baidu Silicon Valley AI lab, and I've

04:31.760 --> 04:35.960
been with the lab for the past two and a half years, a little bit more than two and a

04:35.960 --> 04:40.080
half years, almost since the inception of the lab.

04:40.080 --> 04:48.080
I primarily work in speech and language, I would say more speech than language, over the

04:48.080 --> 04:52.880
last two and a half years I've primarily worked on what is called ASR, which is the piece

04:52.880 --> 05:01.000
of technology that takes voice as we speak and then converts it into text.

05:01.000 --> 05:10.960
And we have built one of the best English ASR systems and by far the best Mandarin ASR systems

05:10.960 --> 05:16.280
which powers most of our products in China.

05:16.280 --> 05:20.760
And currently I'm starting to work on other things like speech synthesis, a little bit

05:20.760 --> 05:26.840
of language modeling to keep in the same domain.

05:26.840 --> 05:29.800
Surprisingly enough, I actually did not start in speech.

05:29.800 --> 05:33.920
I started my career, so to speak, in computer graphics.

05:33.920 --> 05:40.040
I was very, very interested in special effects and films and I know this sounds a little

05:40.040 --> 05:46.800
bit like a fairy tale, but I came to do a PhD in computer science to this country because

05:46.800 --> 05:50.880
I was so blown away by the films that Pixar was making.

05:50.880 --> 05:56.880
So my goal was if I get to grad school, maybe I'll get to work at Pixar at some point.

05:56.880 --> 05:58.800
And did you ever work at Pixar?

05:58.800 --> 05:59.800
I did.

05:59.800 --> 06:07.120
I spent six months as an intern at Pixar developing some of the early kind of GPU-assisted

06:07.120 --> 06:08.600
what are called relighting tools.

06:08.600 --> 06:13.880
These tools give you a fast preview of the film scenes that the lighting designers are

06:13.880 --> 06:16.000
using to relight the scene.

06:16.000 --> 06:17.000
Okay.

06:17.000 --> 06:18.480
And that was an incredible experience.

06:18.480 --> 06:21.480
I still very fondly look back on that experience.

06:21.480 --> 06:27.720
I think it was back in 2006, so a while ago, but it still sticks in memory.

06:27.720 --> 06:34.120
And that's kind of what I started taking these GPUs that were only used for playing games

06:34.120 --> 06:40.760
and starting to use them for doing a general purpose compute because, you know, I thought

06:40.760 --> 06:43.800
of graphics as just another computation domain.

06:43.800 --> 06:50.800
So during my PhD, I came up with all the basic parallel algorithms that people used to program

06:50.800 --> 06:51.800
GPUs.

06:51.800 --> 06:57.040
So I was very lucky that I was very early in this field and made a lot of impact with

06:57.040 --> 06:58.040
the author.

06:58.040 --> 06:59.040
Yeah.

06:59.040 --> 07:05.840
Was the author of the first parallel programming library for GPUs and did a lot of basic algorithm

07:05.840 --> 07:07.440
work on GPUs.

07:07.440 --> 07:12.680
So, you know, I've been with GPUs for 12 years now almost since the early days when there

07:12.680 --> 07:18.280
was there were very, very hard to program for general purpose compute that were primarily

07:18.280 --> 07:19.520
used for graphics.

07:19.520 --> 07:27.000
So it's very heartening to see that, you know, so many different areas being so positive

07:27.000 --> 07:31.520
and impacted by GPUs on that.

07:31.520 --> 07:34.880
What was the name of the library that you worked on?

07:34.880 --> 07:38.800
CUDPP, CUDA data parallel primitives library.

07:38.800 --> 07:45.440
It's still around and not so much popular anymore because a lot of other libraries like thrust

07:45.440 --> 07:49.960
et cetera, which is good because other people have, I mean, once you leave grad school,

07:49.960 --> 07:56.040
it's very hard to keep on contributing code to that library because you have other engagements

07:56.040 --> 07:59.080
a full-time job, et cetera.

07:59.080 --> 08:03.800
But my advisor and his students are still putting in new algorithms into the library.

08:03.800 --> 08:06.960
So it's still around, people still use it.

08:06.960 --> 08:12.360
And that library itself has spawned other efforts like thrust from Nvidia and modern GPU also

08:12.360 --> 08:13.360
from Nvidia.

08:13.360 --> 08:14.360
Okay.

08:14.360 --> 08:20.760
And I, yeah, and I should also give a shout out to Nvidia because Nvidia had been extremely

08:20.760 --> 08:25.560
kind in financial terms and in resources throughout my research career and they still work

08:25.560 --> 08:28.800
closely with Nvidia even though I don't work there.

08:28.800 --> 08:33.280
But I work with a lot of people in Nvidia on almost a day-to-day basis.

08:33.280 --> 08:38.120
Well, feel free to give that shout out to Nvidia because in fact, I was at a conference

08:38.120 --> 08:43.480
last week where Speaker from Nvidia gave a shout out to this podcast.

08:43.480 --> 08:44.480
Awesome.

08:44.480 --> 08:45.480
Awesome.

08:45.480 --> 08:46.480
Yeah.

08:46.480 --> 08:48.640
I mean, Nvidia has supported my career ever since.

08:48.640 --> 08:50.640
I mean, literally my first day of grad school.

08:50.640 --> 08:56.640
My first day, I think my first week in this country, I was at a talk at Nvidia because

08:56.640 --> 08:58.160
my advisor was there.

08:58.160 --> 09:02.120
One of his students was giving a talk so it was in 2004.

09:02.120 --> 09:05.680
So it's been a while, but the relationship is strong.

09:05.680 --> 09:07.000
That's great.

09:07.000 --> 09:13.920
So I, you know, what my thinking for this conversation was about, was that we would

09:13.920 --> 09:18.720
really dive into the systems types of issues.

09:18.720 --> 09:27.000
And given your expertise in GPUs, I have so many questions about kind of GPUs and how

09:27.000 --> 09:32.320
they're used in ML and AI and some of the libraries that support them.

09:32.320 --> 09:37.680
So at some point, we want to make sure to touch on that stuff as well.

09:37.680 --> 09:38.680
Yep.

09:38.680 --> 09:40.920
Absolutely.

09:40.920 --> 09:47.920
So in your talk, you gave a specific example of a speech recognition that you worked

09:47.920 --> 09:52.000
on that had some interesting challenges.

09:52.000 --> 09:56.360
Can you talk a little bit about that project or one of the, you know, a specific type of

09:56.360 --> 09:58.560
project that you can talk about?

09:58.560 --> 09:59.560
Yeah, sure.

09:59.560 --> 10:04.600
So we have primarily been working on ASR, which is essentially speech recognition for

10:04.600 --> 10:08.000
the past two years or so.

10:08.000 --> 10:13.200
And when we started, one interesting thing about our lab was we didn't have any speech

10:13.200 --> 10:14.200
experts.

10:14.200 --> 10:17.640
We still really don't have any speech experts.

10:17.640 --> 10:22.520
So we decided that since we don't have any experts, then we'll have the data kind of guide

10:22.520 --> 10:23.520
us.

10:23.520 --> 10:27.640
So we knew, we kind of, I wouldn't say new, but we were confident that if we could get

10:27.640 --> 10:33.040
a lot of data, and we knew how to train really deep networks, and we will be able to

10:33.040 --> 10:35.520
build these systems.

10:35.520 --> 10:40.320
So one thing that kind of was a challenge from us for us, at least in the very beginning,

10:40.320 --> 10:42.360
was we had to do a lot of experimentation.

10:42.360 --> 10:49.640
And we still do at scale to give you an idea, typically a publicly available data set.

10:49.640 --> 10:53.240
I think the largest one is 3,000 hours.

10:53.240 --> 10:57.960
And we typically do all our experimentation of multiple of tens of thousands of hours.

10:57.960 --> 11:03.600
So it's an order of, yeah, so it's an order of magnitude difference, and this is hours

11:03.600 --> 11:05.440
of speech data that we're talking about.

11:05.440 --> 11:06.800
Yeah, hours of speech data, yes.

11:06.800 --> 11:11.120
So it's millions of training examples, essentially.

11:11.120 --> 11:19.680
So typically around 14 million utterances is about 10,000 hours if that gives you an idea.

11:19.680 --> 11:25.760
So we have many millions of these training examples that we need to train our networks

11:25.760 --> 11:27.880
on.

11:27.880 --> 11:30.360
And we know that deep learning is a very empirical field.

11:30.360 --> 11:32.960
I mean, nobody tells you that this is the network.

11:32.960 --> 11:38.400
You have to kind of hunt around and figure out what the network actually is.

11:38.400 --> 11:43.200
Which means running a lot of experiments, each of our experiments takes anywhere from

11:43.200 --> 11:45.760
20 to 50 x-flops.

11:45.760 --> 11:53.560
So that's like, you know, XI is 10 to the power 18, so it's a lot of flops for one experiment.

11:53.560 --> 11:59.400
And typically, you would want to finish an experiment within three weeks is pretty much the

11:59.400 --> 12:01.960
patience we have.

12:01.960 --> 12:07.120
So early on, we had to like build the systems that is very, very high-performance.

12:07.120 --> 12:12.280
And it still is, in my opinion, from what the numbers have seen is the fastest training

12:12.280 --> 12:19.360
system in the industry by at least twice as fast of what the best number have seen publicly

12:19.360 --> 12:21.360
so far.

12:21.360 --> 12:26.840
And we kind of had to invent this thing because there is nothing when we started in late

12:26.840 --> 12:32.480
014 around October of 014 is when we started September, October, that time frame.

12:32.480 --> 12:35.880
So we build this thing and we are, I think we're still are ahead.

12:35.880 --> 12:40.680
And what we are doing is slowly kind of open sourcing parts of this framework.

12:40.680 --> 12:47.480
So we want to kind of take our ideas in some sense, infect the field or infect the community

12:47.480 --> 12:53.400
with all these high-performance computing ideas, which I think are very, very appropriate

12:53.400 --> 12:55.120
for deep learning.

12:55.120 --> 12:58.960
Deep learning is this problem that needs a lot of compute.

12:58.960 --> 13:03.680
And that's kind of what also differentiates our group is not only do we do deep learning,

13:03.680 --> 13:08.560
but we do this deep learning through this very high-performance computing approach.

13:08.560 --> 13:14.720
And we do all our experimentation set at big scale, which kind of separates us.

13:14.720 --> 13:15.920
And other people are catching on.

13:15.920 --> 13:21.840
I mean, you see now Google and Facebook and others of a lot of data have kind of come

13:21.840 --> 13:25.440
around to this kind of high-performance centric approach.

13:25.440 --> 13:28.960
Yeah, and one funny thing, there's an anecdote and it is true.

13:28.960 --> 13:34.360
What I like to tell is when we first started training in Mandarin and built a very decent

13:34.360 --> 13:39.520
Mandarin ASR system, not the best, but very decent, there was not a single Mandarin-speaking

13:39.520 --> 13:40.840
person in the group.

13:40.840 --> 13:41.840
Wow.

13:41.840 --> 13:46.480
But this is the power of data.

13:46.480 --> 13:50.240
You can look at a training curve and you know that if the training curve is going down,

13:50.240 --> 13:51.240
you're doing well.

13:51.240 --> 13:55.480
You don't need to know the language.

13:55.480 --> 14:02.480
My colleague, Ryan and I started doing it over, I think the Christmas of O-14, when we

14:02.480 --> 14:06.480
got our first big Mandarin data set, we started training and immediately we started getting

14:06.480 --> 14:07.480
very good results.

14:07.480 --> 14:08.480
That's incredible.

14:08.480 --> 14:16.720
And essentially the same network architecture does both English and Mandarin, which you

14:16.720 --> 14:25.160
know kind of validates your belief that the neural network will learn the differences

14:25.160 --> 14:28.280
in the languages by itself.

14:28.280 --> 14:33.800
And English and Mandarin are very different languages, I mean, they're so different.

14:33.800 --> 14:39.080
And the fact that almost the same neural network architecture does both is still to me,

14:39.080 --> 14:41.480
you know, I've been doing this for two and a half years, is still to me one of the

14:41.480 --> 14:48.520
biggest surprises and kind of validates our approach that led the data guide you.

14:48.520 --> 14:55.360
So when you're approaching one of these problems and you're trying to kind of iterate to a

14:55.360 --> 14:58.920
neural network architecture, where do you tend to start?

14:58.920 --> 15:06.800
Do you start from, you know, published research like Google Med or one of the, you know,

15:06.800 --> 15:13.080
published network architectures that is kind of known good in a given class of problem

15:13.080 --> 15:20.880
or, you know, at this point, do you have, you know, your own best practices or proprietary

15:20.880 --> 15:21.880
network architectures?

15:21.880 --> 15:25.880
How do you tend to approach the network architecture problem?

15:25.880 --> 15:28.560
Yeah, that's a very good question.

15:28.560 --> 15:33.200
And speech is a little bit different from identifying images like you said, an image

15:33.200 --> 15:38.320
net and a Google net kind of architectures, which are stacks of convolutional networks,

15:38.320 --> 15:41.200
because speech has a time dependency, right?

15:41.200 --> 15:47.800
You can think of a speech as a sequence of audio samples, typically a good practice is

15:47.800 --> 15:50.440
to choose audio samples that are 10 milliseconds each.

15:50.440 --> 15:53.840
So every 10 milliseconds, you've got to get an audio sample.

15:53.840 --> 15:55.800
But there is a time dependency, right?

15:55.800 --> 16:01.560
Because what you say, you know, at time t equals something is dependent on what you said,

16:01.560 --> 16:03.520
t minus something.

16:03.520 --> 16:09.000
So the styles of network that we had to train was what are called recurrent nets.

16:09.000 --> 16:11.880
And recurrent nets have been actually around for a long time.

16:11.880 --> 16:17.400
But when we first started in 2014, I don't think anybody had used recurrent nets that

16:17.400 --> 16:23.160
widely in speech, but we kind of had a gut feeling that since recurrent nets captured

16:23.160 --> 16:26.680
this time dependency, that's where we should start.

16:26.680 --> 16:33.040
So we started off doing very vanilla bidirectional recurrent nets, which actually bidirectional

16:33.040 --> 16:35.280
means they go backward and forward in time.

16:35.280 --> 16:39.960
So there's one layer, which goes backward, multiple layers, which goes backward in time and

16:39.960 --> 16:43.360
other multiple layers, which was forward in time.

16:43.360 --> 16:45.840
That's kind of where we started off with.

16:45.840 --> 16:52.840
And Aoni Hanun, who is now back at Stanford, he had done some initial work on speech and

16:52.840 --> 16:55.640
he was at play around with your recurrent networks a bit.

16:55.640 --> 17:00.400
So he was the proponent of using recurrent networks.

17:00.400 --> 17:06.040
And since then we have evolved to be used gated recurrent units, which are a little bit fancier

17:06.040 --> 17:09.320
recurrent networks, LSTMs.

17:09.320 --> 17:16.320
On top of that, you can use even fancier constructs like potential models, which kind of look

17:16.320 --> 17:19.880
at capturing some sort of memory inside these networks.

17:19.880 --> 17:26.640
So in these networks, in some sense, are capturing memory of what you've said in the past.

17:26.640 --> 17:31.440
So yeah, you kind of look at your problem and then there are wide classes of these networks.

17:31.440 --> 17:40.200
So convolutional networks kind of capture features in kind of an image space, in a 2D space.

17:40.200 --> 17:46.400
And this recurrent networks kind of capture features in time dimension.

17:46.400 --> 17:50.680
So depending on where your problem lies, that's where you would start.

17:50.680 --> 17:56.720
For example, tomorrow, for example, I have to, you know, I'm trying to do something in video.

17:56.720 --> 17:57.720
Where would I start?

17:57.720 --> 18:02.280
I would probably start with a recurrent network as a first guess, because video also has

18:02.280 --> 18:04.960
a concept of time.

18:04.960 --> 18:07.920
So that's kind of where you start off with.

18:07.920 --> 18:09.520
And nothing is actually proprietary.

18:09.520 --> 18:14.840
I mean, one good thing about the communities, people are very open about sharing architectures.

18:14.840 --> 18:21.760
Like all our architectures are public, I mean, we publish yearly almost, yearly cadence.

18:21.760 --> 18:26.520
What networks we have been using, and they're all like standard known networks.

18:26.520 --> 18:30.600
Just the architecture is different, like how they're connected, how many layers, what

18:30.600 --> 18:33.640
is the cost function and so on and so forth.

18:33.640 --> 18:41.160
So what's the relationship between the network architecture and underlying systems that

18:41.160 --> 18:42.320
support them?

18:42.320 --> 18:50.640
That's a good question, typically, you know, a lot of people, I think, because Google

18:50.640 --> 18:57.160
Net and because ImageNet has become such a widely known competition that people focus

18:57.160 --> 19:04.760
on these stacks of convolutional nets a lot, recurrent nets because they go in time, they're

19:04.760 --> 19:11.040
a little bit difficult because say you have a really long audio transcript, which means

19:11.040 --> 19:16.320
it will be kind of enrolled in time, you can think of it as a for loop.

19:16.320 --> 19:21.160
So depending on how long the audio transcript is, you have to kind of unroll that loop in

19:21.160 --> 19:29.560
time, which makes it a little bit more challenging compared to like convolutional networks.

19:29.560 --> 19:37.880
So their memory kind of blows up because you have these long utterances that you're transcribing.

19:37.880 --> 19:44.680
Sometimes your deployment can be a challenge because you are doing this small matrix multiplies

19:44.680 --> 19:50.240
which are not very high performance depending on how you do the inference, inference is when

19:50.240 --> 19:53.080
you actually deploy these models.

19:53.080 --> 19:57.600
And you're also, and users are also very, very sensitive to latency, so which means you

19:57.600 --> 19:58.760
have to do it.

19:58.760 --> 20:03.560
Like when you're talking to Siri, for example, you spoke something to Siri and Siri took,

20:03.560 --> 20:08.800
I don't know, one minute to answer, obviously you're not going to be a very happy user.

20:08.800 --> 20:10.800
We've moved on by that point.

20:10.800 --> 20:11.800
Yeah, exactly.

20:11.800 --> 20:16.680
So you have to think about all these issues when you're doing inference for these kind

20:16.680 --> 20:18.000
of recurrent networks.

20:18.000 --> 20:25.000
And then very recently there are network architectures that are incredibly challenging things like

20:25.000 --> 20:31.360
wave net and byte net, which came out of Google DeepMind and PixelCNN, PixelRNN.

20:31.360 --> 20:37.320
So what these networks are doing, they're generating one sample at a time, and this sample

20:37.320 --> 20:45.080
is at a very fine time granularity, which means they're generating one sample every 61

20:45.080 --> 20:52.160
microsecond, which basically 61 microsecond is basically one over 16,000, like a 16 kilohertz

20:52.160 --> 20:53.160
audio.

20:53.160 --> 20:56.840
So these networks are used to send us to the opposite problem.

20:56.840 --> 21:00.080
So given a text, you're synthesizing speech.

21:00.080 --> 21:09.800
So the denure incredibly latency bound, you have to generate an audio sample every 61 microseconds.

21:09.800 --> 21:17.080
And this entire network is like 40 layers, 50 layers, deep, something like that.

21:17.080 --> 21:21.560
So this is actually a research frontier, like how do you make things like wave nets and

21:21.560 --> 21:24.280
byte nets, synthesize things.

21:24.280 --> 21:29.520
Same thing happens with the pictures, like the PixelCNN paper if you look at, I think

21:29.520 --> 21:32.120
it was presented last year's ICML.

21:32.120 --> 21:38.560
They're synthesizing images, one pixel at a time, and to generate each pixel, you have

21:38.560 --> 21:45.240
to do this inference through this deep, really deep network of 40, 50 layers, which makes

21:45.240 --> 21:47.000
this very, very challenging.

21:47.000 --> 21:55.640
So from the system side, I think these networks are kind of at the forefront of difficulty

21:55.640 --> 22:00.080
and challenge of deploying these kind of networks.

22:00.080 --> 22:04.480
Now having been in the field for so long, does it still amaze you that any of this stuff

22:04.480 --> 22:05.480
works?

22:05.480 --> 22:08.280
Oh, on a daily basis.

22:08.280 --> 22:15.360
And I think it's an Achilles heel, in some sense, that the interpretability of these networks

22:15.360 --> 22:18.880
is extremely hard, especially for speech.

22:18.880 --> 22:26.240
If you look at some of the work that has happened in the Google net kind of world where you

22:26.240 --> 22:33.480
can kind of deconvolve a layer and kind of say, okay, this layer is detecting like a

22:33.480 --> 22:34.480
face.

22:34.480 --> 22:39.640
This layer is detecting like a, I don't know, the chin or something, because humans, visually,

22:39.640 --> 22:40.640
we are very rich.

22:40.640 --> 22:43.440
We can look at these patterns visually.

22:43.440 --> 22:48.640
I have no idea how to visualize a recurrent network, so how do you visualize it?

22:48.640 --> 22:57.000
Audio form, I mean, I don't know, because audio has in some sense much less information

22:57.000 --> 23:02.800
compared to like an image, it literally is a picture of a thousand words, right?

23:02.800 --> 23:08.280
So in our domain, it's almost impossible to gain any sort of interpretability in the

23:08.280 --> 23:14.280
networks we train, which makes our life really hard, because we see almost every daily

23:14.280 --> 23:20.400
basis, we see these, you know, some, something we see that is like, how did that happen?

23:20.400 --> 23:23.240
And, you know, it's very hard to gain insight into these networks.

23:23.240 --> 23:27.440
They're very much a black box, which makes them quite hard.

23:27.440 --> 23:34.200
It's incredible to think that this, the neural network is looking at, you know, milliseconds

23:34.200 --> 23:39.440
of speech and is able to do anything with it, you know, let alone create words and have

23:39.440 --> 23:41.320
them be partially intelligible.

23:41.320 --> 23:44.600
Yeah, and it's doing for two languages, which are completely unrelated.

23:44.600 --> 23:49.800
I mean, we do like, when we first start in Mandarin, people were saying, oh, you need

23:49.800 --> 23:54.680
to do explicit tone modeling, because Mandarin is a tonal language and all that stuff, right?

23:54.680 --> 23:58.760
And you're like, you know, we're not linguists, we are not language experts or speech experts,

23:58.760 --> 24:02.400
you're like, no, we're not going to do it and we're going to just try kind of the quote

24:02.400 --> 24:06.680
unquote brute force method of like feeding it to the neural network and see if the neural

24:06.680 --> 24:10.480
network can figure out the tonality of the languages.

24:10.480 --> 24:11.480
And it does.

24:11.480 --> 24:17.480
I mean, you know, if you ask me what layer of the neural network is learning these tones,

24:17.480 --> 24:21.480
I have no idea.

24:21.480 --> 24:28.200
Well, I had a, I had a guest on the show Pascal Fung, who works on, who spent quite a

24:28.200 --> 24:32.720
bit of time in her career working on speech.

24:32.720 --> 24:40.120
And she quoted, I forget the name of the researcher, who's one of the pioneers in statistical

24:40.120 --> 24:46.960
modeling of speech, who said something to the effect of my speech recognition, accuracy

24:46.960 --> 24:51.440
goes up every time I fire a linguist from my lab or something like that.

24:51.440 --> 24:52.440
Yeah.

24:52.440 --> 24:58.720
That's a very popular saying and we kind of jokingly say it inside of her, especially when

24:58.720 --> 25:03.240
we first started that we had like no clue what to do with.

25:03.240 --> 25:07.480
So your accuracy should be at the top, right?

25:07.480 --> 25:12.640
But I think that as you get, I mean, going from say 80, when we first started, I think

25:12.640 --> 25:16.280
we were 80% accurate, 75% accurate.

25:16.280 --> 25:24.280
Going from that to 95, 96% is actually easier than going from 96% or 99%.

25:24.280 --> 25:25.280
Of course.

25:25.280 --> 25:27.080
It's that famous sigmoid core, right?

25:27.080 --> 25:32.960
I mean, you have to 10x more work to get, I don't know, 1% more better accuracy.

25:32.960 --> 25:33.960
Right.

25:33.960 --> 25:41.040
Because making Androang gives us really nice example that it's very hard to build systems

25:41.040 --> 25:44.200
that are better than humans in accuracy.

25:44.200 --> 25:49.760
Because then you kind of lose sense of like, normally when you develop the systems, you

25:49.760 --> 25:52.560
kind of get a sense of why the system is failing.

25:52.560 --> 25:56.280
Maybe the system is failing because it can, because it has background noise or something

25:56.280 --> 25:57.280
like that.

25:57.280 --> 26:01.520
But once it's into the range where the humans will make mistakes, but then you can judge

26:01.520 --> 26:06.040
something that is already better than you, because then you don't know what these

26:06.040 --> 26:07.480
failure cases are.

26:07.480 --> 26:08.480
Right.

26:08.480 --> 26:13.000
So the last 3% or the last 2% is really hard to bridge.

26:13.000 --> 26:16.440
And that's kind of where we are now.

26:16.440 --> 26:21.200
And becoming increasingly hard, we're making progress, but it's not as rapid of a progress.

26:21.200 --> 26:23.920
We are, we want to close the gap.

26:23.920 --> 26:29.040
And I think we can close the gap, but it's going to be incredibly challenging.

26:29.040 --> 26:33.720
And the flip side is, I don't think until you get to that 99% accuracy level, you can't

26:33.720 --> 26:38.200
really have a technology that kind of gives you the sense of magic, right?

26:38.200 --> 26:39.200
You know, right?

26:39.200 --> 26:40.200
It just works.

26:40.200 --> 26:41.200
Right.

26:41.200 --> 26:42.800
You're not there yet with ASR.

26:42.800 --> 26:43.800
Right.

26:43.800 --> 26:44.800
Right.

26:44.800 --> 26:47.080
It's amazing how close we're getting though.

26:47.080 --> 26:48.080
Yeah.

26:48.080 --> 26:49.080
Yeah.

26:49.080 --> 26:52.480
You said a couple of things that I thought were pretty interesting that I'd like to go

26:52.480 --> 27:02.520
back to, one was that you were starting to see results that were 2x better than, you

27:02.520 --> 27:06.760
know, what you were seeing publicly.

27:06.760 --> 27:12.120
Were those results in terms of accuracy, or I think it was training time that you were

27:12.120 --> 27:13.120
what were those.

27:13.120 --> 27:15.760
So those were training times, right?

27:15.760 --> 27:16.760
Yeah.

27:16.760 --> 27:22.360
Training so from the very outset, what we have focused on was not just absolute training

27:22.360 --> 27:26.600
time, but one thing that I learned from NVIDIA while I was NVIDIA is what is called

27:26.600 --> 27:28.560
the speed of light.

27:28.560 --> 27:33.240
The speed of light is basically under ideal circumstances, what is the fastest you can

27:33.240 --> 27:34.240
go?

27:34.240 --> 27:38.400
And that typically is if your processor is running at full speed all the time, which obviously

27:38.400 --> 27:39.880
doesn't happen.

27:39.880 --> 27:45.440
So then what you should aim for is what fraction of the speed of light are you at?

27:45.440 --> 27:49.880
And typically for any real world application, especially for applications that run for

27:49.880 --> 27:55.320
three weeks, if you are at 50% of speed of light, which means, you know, a GPU has a speed

27:55.320 --> 28:00.360
of light of, typically, are on six terraflops if you take a Titan X GPU.

28:00.360 --> 28:05.320
So if you can sustain three terraflops, that's actually incredibly good.

28:05.320 --> 28:10.920
So instead of focusing on absolute training time, we look at what fraction of speed of light

28:10.920 --> 28:12.600
are we?

28:12.600 --> 28:16.480
Because that's a good measure of efficiency, right?

28:16.480 --> 28:23.840
So we target at 50% and I just saw, I think, a paper couple of days back.

28:23.840 --> 28:27.320
I believe from Google, it's a very good paper, it's on a mixture of experts, and they're

28:27.320 --> 28:30.720
I think getting around 25% efficiency.

28:30.720 --> 28:36.280
So the field is in some sense moving there that people are caring about things like efficiency

28:36.280 --> 28:37.280
efficiency.

28:37.280 --> 28:39.600
I've been 25% of speed of light, basically.

28:39.600 --> 28:44.720
And we have been at 50% of speed of light since at least a year and a half.

28:44.720 --> 28:49.280
So in that sense, we have been 2x or more ahead of the field.

28:49.280 --> 28:54.720
And this is something that we as a group have been trying to kind of evangelize that it's

28:54.720 --> 28:59.920
not just important to tell you the absolute number, but also what fraction of speed of light

28:59.920 --> 29:00.920
are you?

29:00.920 --> 29:06.520
Because otherwise, it's hard to measure how efficient you are.

29:06.520 --> 29:08.960
And that's a different kind of thinking I feel.

29:08.960 --> 29:13.320
And I think it, I actually learned it from Nvidia when I was at Nvidia because it's a

29:13.320 --> 29:14.320
game.

29:14.320 --> 29:19.640
When you're developing these algorithms for games, games also have this kind of a hard requirement

29:19.640 --> 29:23.280
that you have to generate a frame every 30th of a second, because it's typically a game

29:23.280 --> 29:24.280
for 30th.

29:24.280 --> 29:25.280
Yes.

29:25.280 --> 29:26.280
Right.

29:26.280 --> 29:30.520
So then you need to do it very, very efficiently, with a computer game, for example.

29:30.520 --> 29:31.520
Right.

29:31.520 --> 29:32.520
Right.

29:32.520 --> 29:36.360
And do you, does your tea, is it reflected in your team composition as well?

29:36.360 --> 29:41.960
Do you have more or folks from a systems background than you would typically find doing

29:41.960 --> 29:43.960
in an AI research group or?

29:43.960 --> 29:44.960
Absolutely.

29:44.960 --> 29:47.720
So this was also something that is very unique to our group.

29:47.720 --> 29:54.880
When we first started kind of formulating the lab, Adam Coates and Andrew kind of understood

29:54.880 --> 29:59.960
that the systems would be a really important aspect of deep learning.

29:59.960 --> 30:02.560
So we have a really deep engine system.

30:02.560 --> 30:07.920
We don't have a lot of people, but the people we have in our group, I think combined have

30:07.920 --> 30:14.240
like 30, 40 years experience in building very, very high performance systems.

30:14.240 --> 30:19.640
And it's also a unique opportunity, because there's a group that people can wear different

30:19.640 --> 30:22.320
hats, in fact, encourage to wear different hats.

30:22.320 --> 30:26.480
So I come from a GPU background, other people come from GPU background, we learn about

30:26.480 --> 30:30.840
speech, about deep learning, about ML, and then ML people and the deep learning to come

30:30.840 --> 30:36.720
from the ML or no more ML deal background, they learn about how to build these systems.

30:36.720 --> 30:41.240
I mean, you can't just write code and, you know, hope for the best.

30:41.240 --> 30:45.120
I mean, you know, depending on what code you write and how you write it or how you architect

30:45.120 --> 30:49.880
it, your training run might take two months in sort of three weeks.

30:49.880 --> 30:50.880
Right.

30:50.880 --> 30:51.880
Right.

30:51.880 --> 30:53.880
And good luck with that.

30:53.880 --> 31:00.040
So it's a very interesting composition of our lab and I don't think it exists anywhere

31:00.040 --> 31:04.960
else, but I think other people like Google, Facebook, etc, they're all seeing the value

31:04.960 --> 31:09.520
of this kind of an approach and they're all kind of ramping up, I mean, the TensorFlow

31:09.520 --> 31:15.080
team is incredible and they're, they're also have a lot of really great systems people

31:15.080 --> 31:19.160
and they're, they've kind of realized the value of this as well.

31:19.160 --> 31:20.640
It's interesting.

31:20.640 --> 31:26.400
I had an opportunity to hear Andrew speak at this conference that I was at last week.

31:26.400 --> 31:33.640
I was at Giga OM AI conference and one of the things that he said that was really interesting

31:33.640 --> 31:42.080
was that, you know, in Silicon Valley, we've got various established practices for, you

31:42.080 --> 31:48.880
know, lots of things, building web apps, building mobile apps, you know, building backend infrastructures

31:48.880 --> 31:56.240
for those kinds of systems like even as, even in spite of how fast those things are moving,

31:56.240 --> 32:02.040
like we have these established, you know, principles and architectures and like, you know, DevOps,

32:02.040 --> 32:08.200
we've got a team composition that kind of works and an AI, we're kind of inventing all

32:08.200 --> 32:14.160
that stuff over again and don't have, you know, people, everyone's doing things differently,

32:14.160 --> 32:17.360
trying to experiment and figure out what works and what works best.

32:17.360 --> 32:19.800
Yeah, I think that's absolutely true.

32:19.800 --> 32:25.840
I think the, there is a notion of what an AI product should be, but I don't think AI

32:25.840 --> 32:29.120
products have had their, what I call the Photoshop moment, right?

32:29.120 --> 32:35.480
For the Photoshop was one of those early desktop apps, so to speak, which kind of defined

32:35.480 --> 32:43.520
what a desktop application should be or, or a, you know, a word perfect or the early days

32:43.520 --> 32:48.800
of Microsoft Word, where, you know, these applications is defined what a desktop publishing

32:48.800 --> 32:52.040
or a desktop content generation application should look like.

32:52.040 --> 32:53.040
Right.

32:53.040 --> 33:00.320
I don't think AI product has had that defining moment that this app or product or service,

33:00.320 --> 33:04.800
people will say, oh, my God, this is how, you know, AI products and services should be

33:04.800 --> 33:05.800
built.

33:05.800 --> 33:10.200
And again, yeah, there's a lot of experimentation, there's a lot of experimentation and personal

33:10.200 --> 33:17.320
assistance, bots, things like Siri and Google Voice and our, our services in China of doing

33:17.320 --> 33:23.560
a SAR, but I don't think I, I don't get the sense that we have had that Photoshop moment

33:23.560 --> 33:24.560
yet.

33:24.560 --> 33:32.800
So another interesting point you made early on was talking about, you talked about some

33:32.800 --> 33:39.240
of the deployment challenges of the neural networks that you work on.

33:39.240 --> 33:45.080
And we often, we often overlook those, like a lot of the conversation I think is around

33:45.080 --> 33:52.640
training and the training challenges, just because it's so time and tense, but you mentioned

33:52.640 --> 34:00.800
some of the matrix multiplication challenges and other things, you know, what do you find

34:00.800 --> 34:05.800
to be, how do you address those challenges and how do you, like, how do you more fully

34:05.800 --> 34:07.280
characterize them?

34:07.280 --> 34:13.760
Yeah, I think the deployment challenges are driven quite a lot by the, the application

34:13.760 --> 34:20.920
themselves, like as I said, any kind of service, like speech or, for example, you're doing

34:20.920 --> 34:27.280
image tagging, for example, in an app, for example, say an app, let's you tag pictures

34:27.280 --> 34:34.680
of food or something, users care, like all these services are very user-facing and, you

34:34.680 --> 34:39.880
know, our attention span or what we expect from a service has decreased over years, like

34:39.880 --> 34:45.480
we really want everything to be very, very snappy, especially for things like speech, right?

34:45.480 --> 34:49.760
Speech is like our interface, like when I talk to you, expect an answer, I don't expect

34:49.760 --> 34:54.320
the silence, like you, you're not going to speak to me after a minute, after I've been

34:54.320 --> 34:55.320
speaking.

34:55.320 --> 34:57.680
So latency becomes really, really important.

34:57.680 --> 35:03.000
And then this is a challenge if the, the models are deployed in the data center, because

35:03.000 --> 35:07.160
if you're using a mobile phone, then they need to take into account the time that takes

35:07.160 --> 35:11.520
for the request to come to your data center, and then you run the model, generate the

35:11.520 --> 35:16.240
output, whatever you want is given, give to the user, and then you send it back.

35:16.240 --> 35:21.440
And for speech, it also depends on like, you know, you're doing, taking a, you know,

35:21.440 --> 35:25.680
speech segment and converting into text, like when do you start dropping up these speech

35:25.680 --> 35:26.680
segments?

35:26.680 --> 35:32.320
So to the user, it feels like it's a completely seamless kind of interaction.

35:32.320 --> 35:37.640
Ideally, you would want these models to be pushed out to the edge, right?

35:37.640 --> 35:44.400
You, I would want my speech recognition engine, the ASR engine, to be running on my phone.

35:44.400 --> 35:45.920
It has not quite happened.

35:45.920 --> 35:49.960
So what people do is they take kind of a split model, where a part of the model is running

35:49.960 --> 35:54.960
on the phone, and the reason you can run it on these devices is because usually they're

35:54.960 --> 36:01.000
so compute intensive, I mean, I can actually take our model and run it on the, on the iPhone

36:01.000 --> 36:04.960
or Android phone or whatever, a top of the line Android phone, but I can bet you that

36:04.960 --> 36:10.240
you're the, I'll drain the battery in like 20 minutes.

36:10.240 --> 36:12.640
And that won't be a very happy user, right?

36:12.640 --> 36:13.640
Right.

36:13.640 --> 36:15.160
So people kind of split the model.

36:15.160 --> 36:20.280
So maybe a part of the model that does hey Siri or hey Alexa or whatever is resident on

36:20.280 --> 36:23.720
the device, and that only does that, that bit.

36:23.720 --> 36:28.920
And then whatever you say after a Siri gets shipped up to the cloud and then transcribed

36:28.920 --> 36:31.720
there and then brought back.

36:31.720 --> 36:36.280
So those are the kind of, so I think more and more as the devices get more capable and

36:36.280 --> 36:41.480
the other thing that people are also looking at is these neural networks that are very

36:41.480 --> 36:42.480
over parameterized.

36:42.480 --> 36:46.840
I mean, you know, typically a neural network would be say 100 million parameters.

36:46.840 --> 36:51.200
So after you train it, it turns out that you can actually shrink it down a lot.

36:51.200 --> 36:56.920
You can compress the models down to really small models without losing a lot of accuracy.

36:56.920 --> 37:01.760
So people have been, and we are doing it as well, taking a network and shrinking it down

37:01.760 --> 37:05.800
and seeing if we can deploy it to an edge device.

37:05.800 --> 37:10.520
And going back to the matrix multiplies, in training, what you do is this thing called

37:10.520 --> 37:14.360
mini batching rates and mini batch, you take a bunch of training examples, shove it into

37:14.360 --> 37:23.960
like one big, big matrix because big matrices are easier to get very high performance on.

37:23.960 --> 37:28.560
When you're doing speech recognition in the cloud for user-facing services, you can't

37:28.560 --> 37:32.320
really take a lot of utterances and shove it into this large matrix because then you have

37:32.320 --> 37:34.320
to wait.

37:34.320 --> 37:37.160
So, and, you know, users don't like to wait.

37:37.160 --> 37:42.320
So then you have this problem of then you are ending up multiplying what I call skinny

37:42.320 --> 37:45.880
matrices having one or two examples, which are not fast.

37:45.880 --> 37:51.600
So then you have to kind of write these special matrix, multiply, multiplication, routines

37:51.600 --> 37:57.160
that are not optimized for fat matrices or normal matrices, but for skinny matrices.

37:57.160 --> 37:58.160
And everybody's doing it.

37:58.160 --> 38:04.040
I mean, Google has a team, I believe, that who does this kind of, you know, special coming

38:04.040 --> 38:08.840
up with special libraries, which are optimized for the skinny matrix multiplication, read

38:08.840 --> 38:10.160
all the same.

38:10.160 --> 38:15.480
So, yeah, the training side is kind of a different world and has its own kind of challenges

38:15.480 --> 38:18.400
that I think doesn't get talked about a whole lot.

38:18.400 --> 38:23.480
But it's also extremely important because this is where you can, I mean, this is where

38:23.480 --> 38:24.960
kind of the rubber meets the road, right?

38:24.960 --> 38:28.520
This is where, you know, users start using your model to do something useful.

38:28.520 --> 38:29.520
Right.

38:29.520 --> 38:30.520
Right.

38:30.520 --> 38:31.520
Right.

38:31.520 --> 38:37.840
The architecture you were describing or at least the end goal of the architecture you

38:37.840 --> 38:45.160
were describing starts to sound a lot to me like, you know, almost like, if you think

38:45.160 --> 38:49.760
about Hadoopland, in one of your first slides in your presentation, there's a picture of

38:49.760 --> 38:51.080
the Hadoop elephant, right?

38:51.080 --> 38:58.400
In Hadoopland, there's kind of this, you know, migration from batch processing, map reduced

38:58.400 --> 39:01.840
to Spark and streaming processing.

39:01.840 --> 39:07.520
It almost sounds like you're describing a similar migration that many AI apps are taking

39:07.520 --> 39:09.600
or will need to take over time.

39:09.600 --> 39:14.720
Yeah, I think I haven't heard the characterization before, but that is exactly right.

39:14.720 --> 39:22.000
Yeah, during, increasingly during, when you deploy, you are, you're, you're, it's some

39:22.000 --> 39:26.960
kind of streaming and when you're streaming, you are very, very latency sensitive, especially

39:26.960 --> 39:30.040
if there's a user that's using the service.

39:30.040 --> 39:34.600
And typically during training, you are batching stuff up into these things called mini batches

39:34.600 --> 39:39.040
to get much more computational efficiency.

39:39.040 --> 39:43.640
And typically one, I think one of the first lessons I learned when I joined grad school

39:43.640 --> 39:47.920
as my advisor said, latency problems are really hard to solve.

39:47.920 --> 39:53.760
I mean, it's, if a problem has high latency and user scare about low latency, that those

39:53.760 --> 39:59.040
problems typically end up taking a lot of engineering effort, it's harder to parallelize

39:59.040 --> 40:01.080
your way out of a latency problem.

40:01.080 --> 40:02.080
It is very hard.

40:02.080 --> 40:07.400
You have to think about, you know, different algorithms and yeah, it's all like different

40:07.400 --> 40:08.400
hardware.

40:08.400 --> 40:09.600
It's, it's a mess.

40:09.600 --> 40:15.360
Like, if your problem is bandwidth challenged or flop challenged, you can, you can try,

40:15.360 --> 40:21.480
you can solve it relatively easily, then trying to solve something that is latency challenged.

40:21.480 --> 40:28.960
And user scare about latency, I mean, it is, we want stuff to work instantly.

40:28.960 --> 40:34.560
We'd be have very little patience for something that is, especially for things like ASR and

40:34.560 --> 40:37.840
then these kind of things.

40:37.840 --> 40:44.240
One of the techniques you described in your presentation was the graph transformation

40:44.240 --> 40:45.880
of some of these problems.

40:45.880 --> 40:52.680
Can you talk a little bit about the problem space there, the motivation and what you found?

40:52.680 --> 40:53.680
Yeah.

40:53.680 --> 40:56.040
So I think it was pretty forward looking.

40:56.040 --> 40:59.720
In fact, after the talk, I met some people who said, you guys are five years ahead of

40:59.720 --> 41:00.720
the field.

41:00.720 --> 41:01.720
Nobody cares.

41:01.720 --> 41:05.280
So, so what's really happening?

41:05.280 --> 41:08.640
It was a good place to be, right?

41:08.640 --> 41:13.840
So what we are seeing is you have, I mean, you can think of a neural network as an execution

41:13.840 --> 41:14.840
graph.

41:14.840 --> 41:18.200
I mean, execution graph has some operations that you're doing in the nodes and then you're

41:18.200 --> 41:20.720
sending some data across the edges.

41:20.720 --> 41:26.520
It's a very generic way or a very flexible way of saying what you're doing.

41:26.520 --> 41:33.080
What we have found out is as we develop these very, very interesting architectures, they

41:33.080 --> 41:35.720
are becoming pretty hard to scale.

41:35.720 --> 41:39.080
So typically, what we would do is we come up with a really interesting architecture.

41:39.080 --> 41:43.520
We start training in TensorFlow or any of the different frameworks because specifying

41:43.520 --> 41:48.520
these architectures is very easy thanks to the work that all the different tool vendors

41:48.520 --> 41:49.520
have done.

41:49.520 --> 41:52.040
We have excellent tooling for specifying architectures.

41:52.040 --> 41:53.040
So we get up and running.

41:53.040 --> 41:56.400
We are training on, say, 1,000, 2,000 hours of data.

41:56.400 --> 42:02.200
And now we need to scale to what we call our native datasets, which are tens of thousands

42:02.200 --> 42:03.840
of hours.

42:03.840 --> 42:09.640
This making this jump from very, doing experimentation on a couple of thousand hours of data to tens of

42:09.640 --> 42:15.200
thousands of hours of data becomes very, very hard.

42:15.200 --> 42:20.120
Because once you have to take these kind of graphs, which tens of locals graph dev and

42:20.120 --> 42:26.440
various tool vendors of their own name, into a bunch of cores, I mean, you can think

42:26.440 --> 42:30.480
of all these GPUs, this bunch of cores, and inside the GPU, there are also another bunch

42:30.480 --> 42:34.440
of cores, this mapping gets incredibly hard.

42:34.440 --> 42:39.960
So what we do is then a bunch of people, we get together and try to hand tune it, hand

42:39.960 --> 42:43.520
map it down to these kind of cores.

42:43.520 --> 42:50.400
Usually it takes us anywhere between eight months to a year where we can take the networks

42:50.400 --> 42:56.720
that we want that we are training on, say, a small amount of data, which is a couple of

42:56.720 --> 43:03.360
thousand hours of data to like a massive amounts of data, to a lot of hand engineering, hand

43:03.360 --> 43:09.640
tooling, which ideally I want to shorten it, like ideally I want to shorten it to two

43:09.640 --> 43:14.560
months, where typically we are looking at, say, a wave net or a bite net or an intentional

43:14.560 --> 43:19.600
model, which are this very, very flexible, very interesting architectures, but also very

43:19.600 --> 43:22.560
complicated to scale.

43:22.560 --> 43:29.160
So that's what I was trying to get at, is like, we really don't have this kind of intermediate

43:29.160 --> 43:36.560
layer, which takes a graph specification from a framework, be TensorFlow or MXNet or

43:36.560 --> 43:44.520
PyTorch or Torch, and takes this graph and then kind of efficiently maps it down to your

43:44.520 --> 43:49.800
GPUs or CPUs or what have you, and scale transparently.

43:49.800 --> 43:54.040
I mean, today you are doing an experiment on one GPU, tomorrow you flip a switch, you're

43:54.040 --> 44:00.960
doing an experiment on 64 GPUs, and you see no performance hit whatsoever.

44:00.960 --> 44:06.160
We just don't have it, and now it's a very extremely painful process.

44:06.160 --> 44:10.280
And then this really kind of gets at productivity of a group.

44:10.280 --> 44:16.760
I mean, we do it because there's no other way, but ideally, it's kind of a wake up call

44:16.760 --> 44:21.160
for the community, for the community to come together and build this software tooling

44:21.160 --> 44:25.200
or the software middle layer that's absent.

44:25.200 --> 44:29.120
And as I said, in full generality, it's an intractable problem.

44:29.120 --> 44:39.560
So you have to figure out what sort of restrictions you want to place, so you can kind of do both

44:39.560 --> 44:44.480
things that you can get the flexibility of looking at the networks that you want to

44:44.480 --> 44:50.080
look at and also be able to scale these networks out to the data set that you want to scale

44:50.080 --> 44:51.080
at.

44:51.080 --> 44:52.400
And they're usually opposing goals.

44:52.400 --> 44:59.320
So you have to carefully figure out what compromises you have to put in the system.

44:59.320 --> 45:03.680
Let's talk a little bit more about the eight months and what goes into that.

45:03.680 --> 45:14.880
Is it that it's a computationally iterative approach to solving this problem, meaning

45:14.880 --> 45:23.800
you've got this model that after one computationally iterative cycle of training a model and coming

45:23.800 --> 45:32.720
up with a model that you think works, you then are having to run it at scale, figure out

45:32.720 --> 45:36.960
where it breaks, tweak it, and then are you remodeling, I guess?

45:36.960 --> 45:46.200
Are you retraining in that process, or are there other steps taking place to get to

45:46.200 --> 45:48.720
a model that runs at scale?

45:48.720 --> 45:54.280
And now it's usually the same model, you just have to re-implemented, say you take an

45:54.280 --> 45:58.760
attentional model, for example, these are pretty complicated models.

45:58.760 --> 46:04.520
And typically what happens is you run out of memory because these models are large, there

46:04.520 --> 46:10.520
is so much intermediate computation and so much intermediate state that you have to save.

46:10.520 --> 46:15.520
You make the models larger, I mean when I say I make the model larger means the individual

46:15.520 --> 46:20.800
layer sizes changes, so the connectivity stays the same, but the individual layer sizes

46:20.800 --> 46:30.760
say go from a dimension of say 256 to say 1024, and when you increase the dimension, stuff

46:30.760 --> 46:36.560
increases by a square, because it's a matrix, so now you're suddenly out of memory, and

46:36.560 --> 46:40.840
then if you're suddenly out of memory, you can run on the GPU.

46:40.840 --> 46:46.040
And then you have to look at the graph of this and figure out, typically as I said, every

46:46.040 --> 46:50.360
edge in the graph is something that stores memory, you need to figure out, okay, I need

46:50.360 --> 46:55.160
to collapse this graph, and collapsing this graph essentially means writing a new program,

46:55.160 --> 47:00.920
because you can think of the nodes as essentially small programs, like a matrix multiply or something,

47:00.920 --> 47:04.840
so then you have to start fusing these nodes in some interesting way, or think about how

47:04.840 --> 47:09.760
do I partition this graph over two different GPUs, right now this entire graph doesn't fit

47:09.760 --> 47:14.760
on one GPU, so now I have to partition this graph over two GPUs, so as soon as you think

47:14.760 --> 47:19.120
of partitioning a graph over two GPUs, it's like, okay, how do I communicate?

47:19.120 --> 47:28.640
All right, all these problems start coming up, it's extremely handcrafted in that sense.

47:28.640 --> 47:35.560
Right, I understand, it sounds like you're starting with this, again, this model that you've

47:35.560 --> 47:42.880
constructed using these tools that isn't necessarily designed for scale, and then you systematically

47:42.880 --> 47:47.280
have to, if we think about this in a context of traditional compute, you systematically

47:47.280 --> 47:55.840
are denormalizing your database, distributing out, and rewriting everything in assembly.

47:55.840 --> 48:00.920
Yeah, pretty much, I mean, at the end of the day for some of the networks, we just have

48:00.920 --> 48:06.040
to rewrite an assembly because they're not fast enough, otherwise, so it's a very like,

48:06.040 --> 48:11.520
it's almost like an artisan chipping away at a problem, and in some sense, I mean,

48:11.520 --> 48:14.880
as soon as you have an artisan chipping away at a problem, it doesn't scale, right?

48:14.880 --> 48:21.240
It slows you down, I mean, that's why we invented compilers because people are tired of writing

48:21.240 --> 48:29.840
assembly by hand, so yeah, so that layer doesn't exist.

48:29.840 --> 48:43.160
It's interesting to think about, as we think about general AI and the path to getting towards

48:43.160 --> 48:52.680
it, to having an existence, a totally generalized AI, yeah, I think we think about a lot of things,

48:52.680 --> 48:57.080
but I'm not sure that in that conversation, at least I've not heard it explicitly called

48:57.080 --> 49:03.040
out, in order to do that, this system is going to have to be on the cloud, it's going to

49:03.040 --> 49:08.480
have to be massive, it's going to have to be trained on massive amounts of data, it's

49:08.480 --> 49:12.120
probably not going to be, well, there's certainly a lot of algorithmic work that needs to

49:12.120 --> 49:17.240
be done, and lots of other types of work that to be done, data, data management, et cetera,

49:17.240 --> 49:24.920
but there are huge systems problems, fundamental systems problems that we're so far from solving.

49:24.920 --> 49:31.320
Yeah, so that's why I was trying to say it's kind of trying to wake up call or whatever.

49:31.320 --> 49:36.360
I think one of the problems also is we as a research community have done a fairly bad

49:36.360 --> 49:43.000
job of having massive amount of publicly available data set, the only big publicly available

49:43.000 --> 49:49.240
data set is ImageNet, and the architectures to do well on ImageNet are actually fairly simple.

49:50.120 --> 49:55.720
We know how to run them fast, very fast, in fact, I mean, I would not be surprised if ImageNet

49:55.720 --> 50:00.680
like networks get embedded in an ASIC and be available in your form and your camera and whatnot.

50:00.680 --> 50:07.480
Right. But a lot of the problems that we deal with in speech generation and speech synthesis

50:07.480 --> 50:13.080
or speech recognition in language understanding, there are no public data sets. I mean,

50:13.080 --> 50:19.240
there are no public data sets that are big. So people do, we look at the research that happens

50:19.240 --> 50:25.560
in network architecture exploration, if you look at the papers that come out in ICLR or ICML

50:25.560 --> 50:30.680
and NIPs, they are all working on really small data sets, so they don't face the challenge of scale.

50:30.680 --> 50:34.360
And I don't blame them, it's not that they're doing it purposefully, just just no data.

50:35.000 --> 50:41.960
I mean, yeah, so that's also, I think, is a challenge that I didn't talk about in that talk

50:41.960 --> 50:47.000
itself because it wasn't the right thing, but this I feel is a really big challenge that we don't

50:47.000 --> 50:52.840
really have good, large, publicly available data sets for stuff that's not ImageNet.

50:52.840 --> 50:59.560
Because if you are thinking about AGI or whatever, we need a lot of publicly available data

51:00.200 --> 51:04.440
where we can actually scale. Unless we come up with algorithms that will work at small scale,

51:04.440 --> 51:11.960
which people are working on that too, but we haven't made as much progress as we have in other fields.

51:13.560 --> 51:22.040
I've heard this, someone described this problem to me as the industry having overfitted

51:22.040 --> 51:28.760
on ImageNet and some of these other data sets. I totally tell that. I mean, I tell that to people

51:28.760 --> 51:33.640
like whoever is willing to listen, that look, you're overfitting on ImageNet. I tell that to every

51:33.640 --> 51:37.720
vendor we work with, they came up with like every, we work with a lot of different hardware vendors

51:37.720 --> 51:43.640
and stuff. And they all come up with like all these results that look, we are XYZ fast on ImageNet

51:43.640 --> 51:50.680
and like, we don't care. We just don't care, it's a solved problem. I mean, ImageNet is solved.

51:50.680 --> 51:55.400
I mean, it just did. I mean, as I said, it's solved enough to the point that I expect to see it in A6.

51:56.440 --> 52:06.760
Yeah. Yeah. Wow. So we are, we're getting close to the end of our hour, but I'd love to hear you

52:08.440 --> 52:12.920
talk through, actually, I had one more question related to the separate talking about.

52:12.920 --> 52:21.000
So all of these system things that we're describing has what's the best way to ask this question.

52:21.000 --> 52:28.360
You have an HPC background, which is, you know, I think increasingly rare in the, you know,

52:28.360 --> 52:32.760
the, in a world where people are coming up from kind of web-based architectures.

52:32.760 --> 52:39.240
Yeah. Yeah. And I don't hear a lot about that in deep learning necessarily and machine learning.

52:39.240 --> 52:45.400
Can you talk about how that kind of thinking is influenced what you've done and where you're

52:46.360 --> 52:54.600
directly pulling from HPC? It's been a while since I've been involved in HPC community,

52:54.600 --> 52:59.960
but does it lead you, for example, to use like Infiniband and some of the exotic HPC stuff,

52:59.960 --> 53:05.080
or how does it influence you? Oh, yes. I mean, Infiniband is not exotic in our world.

53:05.080 --> 53:11.400
It's, it's pretty standard. I mean, when we first started building stuff,

53:12.200 --> 53:19.240
we started using Infiniband. So the HPC kind of thing is, as I said, as efficiency and speed

53:19.240 --> 53:24.200
of light is extremely important to us. So, as I said, the first thing we think about is how

53:24.200 --> 53:29.800
efficient we are in terms of the processor flops. So if the processor is, say, as I said,

53:29.800 --> 53:36.120
six terraflops or seven terraflops, how close to that number can we get over a period of time?

53:36.120 --> 53:41.160
I mean, it just doesn't have to be bursty. So if you say that, oh, I can reach seven terraflops,

53:41.160 --> 53:45.800
but I can only reach it for like, I don't know, two seconds while my whole application takes two hours.

53:45.800 --> 53:53.800
That's no good. So how much can you sustain over the entirety of an application runtime?

53:53.800 --> 54:00.280
Right. Which in, as I said, in our example, isn't many weeks. So that's, I think, the driving kind of

54:00.280 --> 54:06.920
HPC thought is how efficient can I get? And when you think about efficiency, you tend to build systems

54:06.920 --> 54:11.720
that are what I call our tightly coupled, which means, you know, a loosely coupled system is kind

54:11.720 --> 54:18.520
of a web style system, right? You have this one, you blades are connected by probably a gigabit

54:18.520 --> 54:25.240
Ethernet or maybe 10 gig, I don't know, I think it's still gigabit. But we build like really fat

54:25.240 --> 54:30.280
nodes, like each of our node is like eight GPUs in it because we want this computing elements.

54:30.280 --> 54:35.240
If you think of a GPU as a computing element, we want them to be as close together as possible

54:35.240 --> 54:40.200
because we know that, you know, there's a fundamental physical limit on how fast you can move

54:40.200 --> 54:45.960
electrons over a wire. So if you want to move stuff fast, you want the wire to be very close to

54:45.960 --> 54:52.360
each other, similarly with infinity band. So, um, infinity band, we have like infinity band switches

54:52.360 --> 54:58.120
in our rack. So we don't have many racks. Um, but each of our rack is pushing like 30 kilowatt,

54:58.920 --> 55:06.360
of, of power. And then typical, you know, data center rack in Google, um, Facebook or even us,

55:06.360 --> 55:10.840
I mean, our webst, I mean, we have huge data centers in China for web style workloads.

55:10.840 --> 55:17.320
Um, they're like at less than 10 kilowatts. Right. So it's a very different kind of mindset,

55:17.320 --> 55:22.920
which is a mindset that, okay, I need to push, you know, orders of exoflops. So if I have to push

55:22.920 --> 55:29.560
orders of exoflops, I need to be maximally efficient on, uh, on my system and I have to build

55:29.560 --> 55:36.040
system that have very, uh, small distances to transfer data, like really short wire distances.

55:36.040 --> 55:42.520
So you tend to build what I call fat nodes that are, we're very closely connected, uh, to each

55:42.520 --> 55:47.480
other using, um, in usually infinity band or, you know, Ethernet is also catching up, but there's

55:47.480 --> 55:53.000
also like, uh, also telling somebody that I would like to see more research in interconnects,

55:53.000 --> 55:58.280
because low latency interconnect is becoming really important with deep learning. And you see,

55:58.840 --> 56:03.480
research in that with, um, uh, not only from infinity band, from melanox, with also the

56:03.480 --> 56:10.040
mandeling from, uh, Nvidia and then omnipad from Intel. And I think, uh, we will see more and more

56:10.040 --> 56:14.120
of these low latency interconnects, uh, for workloads like deep learning.

56:14.920 --> 56:23.080
Hmm. Great. Great. Uh, so you tell me, uh, based on your time, do we have time to run through?

56:23.080 --> 56:34.040
Yeah, yeah. Ladies and GPU and, um, okay. Awesome. Yeah. So, you know, maybe we should start with, uh, you

56:34.040 --> 56:41.800
know, the, at the highest level, the evolution that you've seen and how folks are applying GPUs to

56:41.800 --> 56:50.360
these types of workloads, why they're important, um, you know, frameworks, uh, QDNN, I can only

56:50.360 --> 56:54.680
assume is related to the, the kuda stuff that you were working on. Like, how does all that

56:54.680 --> 57:02.520
stuff fit together? Um, so it, in some sense, uh, you know, deep learning is a, is a problem area that

57:02.520 --> 57:07.960
is, is squarely in the comfort zone of the GPU, because if you look at the fundamental operation

57:07.960 --> 57:15.560
in deep learning, uh, it is a major regular matrix matrix map. And if you look at graphics,

57:15.560 --> 57:21.640
it is also regular matrix map. And it's smaller matrices, but, but, uh, but it's also matrix

57:21.640 --> 57:27.000
map. So if you have dense matrix algebra, um, which is what essentially deep learning is, almost

57:27.000 --> 57:32.120
every operation and deep learning is a dense matrix map, it's squarely in the comfort zone of the

57:32.120 --> 57:39.640
GPU. And that is why GPUs are doing so well in this space. Um, and then toolkits like QDNN is

57:39.640 --> 57:45.800
essentially taking, uh, things like this special convolutions. I mean, a convolution is in some

57:45.800 --> 57:51.800
sense a dense matrix map, which is what, how kudianN does it? Um, so what kudianN is doing is taking

57:51.800 --> 57:57.960
the special kind of convolutions that can be in four dimensions, um, that is used in deep learning

57:57.960 --> 58:02.520
and putting it in a library, which is exactly the right thing to do. Um, because they're different

58:02.520 --> 58:09.160
from the convolutions that's used in say traditional, uh, vision or traditional, um, graphics or so.

58:09.160 --> 58:14.920
So these are special kind in that sense. Um, and then what kudianN is doing is also kind of moving

58:14.920 --> 58:21.560
up the stack. For example, now kudianN has, um, a GRE implementation, for example, a batch norm

58:21.560 --> 58:25.560
implementations. In some sense, I think kudianN is going to do a pivot and change its name because

58:26.360 --> 58:32.440
it does, it does a lot more than just, um, convolutions and stuff that's used in image net.

58:32.440 --> 58:37.320
There's a more and more they're putting in stuff, um, that's used in, uh, recurrent nets. And then

58:37.320 --> 58:45.880
you have these, uh, frameworks like TensorFlow and PyTorch and, uh, MxNet and Tiano, um, and many more

58:45.880 --> 58:52.440
who, uh, they're probably 20 odd frameworks, uh, that layer over, uh, these frameworks like kudianN,

58:52.440 --> 59:00.920
like mkldnN that I think intel has. Um, and then what these frameworks are doing is let's you, um,

59:00.920 --> 59:08.840
kind of very easily, uh, specify this network architectures, like an image net or a wave net or

59:08.840 --> 59:15.400
a bite net or a intentional net or a recurrent net. Um, uh, and you can think of each one of the, um,

59:15.400 --> 59:21.000
the nodes in these networks as something as a call into kudianN in some sense.

59:21.000 --> 59:26.360
Okay. Um, and they're all in python because python has kind of become the defect to, um, kind of

59:26.360 --> 59:36.440
language in the AI world or the DL world. So, so kuda itself is the API for programming the GPUs,

59:36.440 --> 59:41.400
basically. Yeah. So kuda is both, uh, kuda is a big thing. I mean, different people think of kuda's

59:41.400 --> 59:46.760
different thing ways. Um, kuda is also language in some sense. Okay. Um, it's got a little bit of

59:46.760 --> 59:53.320
its own syntax. It's very much like cc++, uh, and a little bit more syntax. Um, so kuda is a whole

59:53.320 --> 01:00:00.360
like you can, uh, so you use that and it also has a bunch of API calls that you call into the GPU.

01:00:00.360 --> 01:00:07.000
So it's both like a programming framework as API and also this language itself, uh, for programming

01:00:07.000 --> 01:00:14.680
the GPUs. Yeah. Okay. Okay. And then kudianN sits on top of that, uh, to provide higher level, uh,

01:00:14.680 --> 01:00:20.920
primitives for programming deep neural nets. And then the frameworks, uh,

01:00:20.920 --> 01:00:26.520
uh, tors, Deano, TensorFlow sit on top of that. Yep. That's exactly right. Okay. And kudianN is

01:00:26.520 --> 01:00:31.400
written, could be written as in kuda, definitely written as in kuda. You can also, I mean, for, uh,

01:00:31.400 --> 01:00:35.320
spiss reasons of speed, you can even write an assembly, which they might have done, but yeah,

01:00:35.320 --> 01:00:41.000
that's the right picture to have in mind. It's kuda, kudianN framework. Yeah. Going from bottom to top.

01:00:41.000 --> 01:00:47.720
Okay. And so where do you see the, what do you see things going on the hardware side? Um,

01:00:47.720 --> 01:00:54.040
you know, Intel is trying to, uh, work on some things. Um, the Google has their tensor processing

01:00:54.040 --> 01:01:01.560
unit. Um, how do you see how do you see all that's evolving? Yeah. That's a little bit of a hard

01:01:01.560 --> 01:01:07.080
question to answer, but it's the future. The future is kind of unclear in some sense. I mean,

01:01:07.080 --> 01:01:13.160
the sense that, uh, the network architectures are evolving so fast that it's kind of hard for

01:01:13.160 --> 01:01:19.160
the processors to kind of keep up. Um, so far, what has happened is, you know, everything is

01:01:19.160 --> 01:01:26.520
this nice matrix multiply thing that works really well on GPUs, but we do have, um, networks

01:01:27.320 --> 01:01:32.040
that are actually pretty hard to do on GPUs. For example, WaveNet is one such network.

01:01:33.400 --> 01:01:39.480
So I think for the, the chip meant, I mean, ideally you want both to like,

01:01:39.480 --> 01:01:44.440
co evolve like the hardware architecture would evolve with the network architecture,

01:01:45.000 --> 01:01:50.440
but the problem is, um, it's very easy for me to come up with a very interesting looking

01:01:50.440 --> 01:01:56.520
network, um, using TensorFlow, but it's all, it's very hard to build hardware. Uh, building

01:01:56.520 --> 01:02:04.200
hardware is this incredibly, um, you know, in resource intensive, money intensive, uh, process.

01:02:04.200 --> 01:02:10.600
Um, so it's a little bit hard to see where the hardware is going to evolve. I think the way

01:02:10.600 --> 01:02:15.400
Nvidia and Intel are approaching the problem is, okay, let's just make it fast for, uh,

01:02:15.400 --> 01:02:20.760
matrixes of some sizes. And we have this benchmark out called deep bench, which tells what

01:02:20.760 --> 01:02:27.640
matrix sizes it should be for some of, uh, our networks. And then Google has, uh, developed

01:02:27.640 --> 01:02:32.360
TensorFlow, which it's not a lot of details out, but what I suspect it is is more towards inference.

01:02:32.360 --> 01:02:38.840
And doing very low power inference, but not so much for training, uh, for some specific models.

01:02:38.840 --> 01:02:43.320
So you could know that I was saying that as networks like Google net and things like that

01:02:43.320 --> 01:02:47.400
mature, you'll probably see them burnt into a sick and then shipped in a phone or a camera or

01:02:47.400 --> 01:02:53.320
wherever you're doing some kind of image or object segmentation. Um, so yeah, I think people

01:02:53.320 --> 01:02:58.120
are kind of just hedging their bets and trying to see how the networks are going to mature. And

01:02:58.120 --> 01:03:02.920
it kind of affects both ways because I won't make a network, I won't come up to the network that

01:03:02.920 --> 01:03:10.600
is not efficient on any processor that I can run on. Right, right. So you have no incentive to

01:03:10.600 --> 01:03:15.320
push to envelope, uh, for the hardware folks and the hardware folks have no incentives to push to

01:03:15.320 --> 01:03:19.640
envelope because they don't have any networks to run on their stuff. Right. So like, uh, I wouldn't

01:03:19.640 --> 01:03:23.800
say no incentive, but I would say they're both looking at each other kind of rarely. Right,

01:03:23.800 --> 01:03:30.760
right. Who's going to dump the billions into this science project? Yeah. Exactly. Yeah.

01:03:30.760 --> 01:03:35.000
It's an interesting spot to be in. Yeah. I mean, it's going to cost billions. I mean, let's not

01:03:35.000 --> 01:03:40.360
get ourself. I mean, you know, that that's where it costs to build like fabs and yeah, all of that

01:03:40.360 --> 01:03:48.360
stuff. Yeah. Um, and one of the issues that comes up, uh, at least in, you know, Intel talking about

01:03:48.360 --> 01:03:55.720
uh, uh, night's bridge and their advanced processors that are trying to solve this problem or at least,

01:03:55.720 --> 01:04:02.920
um, you know, cut off and video at the past is the issue of floating point versus in eight.

01:04:02.920 --> 01:04:07.480
Like, can you kind of talk through what that's all about? Yeah. So reduced precision is this

01:04:07.480 --> 01:04:12.680
really interesting thing. And this is also why I feel the industry is optimizing towards Google net.

01:04:12.680 --> 01:04:19.320
So what? Yeah. We have been trying to do reduced precision for two and a half years.

01:04:19.880 --> 01:04:25.240
And doing reduced precision, like, which is what Intel is instead of say float 32 for

01:04:25.240 --> 01:04:32.840
recurrent nets like speech is incredibly hard. Uh, we have it is incredibly hard. So I thought,

01:04:32.840 --> 01:04:43.240
yeah, just to jump in here. Um, what I had envisioned this as a simplification of the chip that

01:04:43.240 --> 01:04:52.680
allows Intel, for example, to stuff more, uh, you know, to make a, a given silicon die able to

01:04:52.680 --> 01:04:57.960
do more math. I hadn't really thought of it as something that's harder. You're basically float 32

01:04:57.960 --> 01:05:06.600
is your 32 bit floating point. Uh, yeah. So what, when I say hard, I mean, um, uh, getting it.

01:05:06.600 --> 01:05:12.600
So if I say a train a network in reduced positions in float 16 or in data in software 32, uh,

01:05:12.600 --> 01:05:19.080
that network will not perform in terms of accuracy as well as a float network trained in float 32.

01:05:19.080 --> 01:05:25.000
That's what I meant. Um, so if, if I have chips that, you know, can only do reduced precision,

01:05:25.000 --> 01:05:31.000
then it doesn't help me at all because my models don't perform well. And why is that I was under

01:05:31.000 --> 01:05:37.480
the impression that the reason why in eight was great, we're using integers was great was because

01:05:37.480 --> 01:05:42.440
the networks themselves don't inherently take advantage of floating point. Is that, I guess that

01:05:42.440 --> 01:05:48.680
doesn't really make sense, right? That only makes sense in the Google net world. This is why I

01:05:48.680 --> 01:05:58.920
does. Yeah. Uh, it does not so far has made sense in, in recurrent nets. Um, we may be seeing some

01:05:58.920 --> 01:06:04.760
light at the end of the tunnel very early days. Uh, recurrent nets for speech, I feel are some

01:06:04.760 --> 01:06:10.360
of the hardest networks you can train. Uh, they're very, very hard to train. Uh, in float 32,

01:06:10.360 --> 01:06:14.120
we have figured out quite well, but we have been trying for at least two and a half years to do

01:06:14.120 --> 01:06:20.600
a reduced precision. Um, and we have not been able to do it. And when I say have not been able to do

01:06:20.600 --> 01:06:26.520
it, what I mean is the models we train using reduced precision is not as good as the models we train

01:06:26.520 --> 01:06:31.800
is in float 32. Uh, and if it's not as good, I'm not going to deploy. I mean, right, right.

01:06:32.840 --> 01:06:39.560
So that's, that's, I think reduced precision is another case of the industry, um, optimizing on

01:06:39.560 --> 01:06:45.560
Google map, um, which, which may be fine because they might have customers, uh, all they want is

01:06:45.560 --> 01:06:50.760
object detection, say automated driving, for example, object detection, object segmentation. So

01:06:50.760 --> 01:06:57.640
they have a market where they can sell these chips and, um, but, uh, it's not appropriate for

01:06:57.640 --> 01:07:01.800
all neural nets, especially not recurrent neural nets yet. I mean, we might able to figure out

01:07:01.800 --> 01:07:08.600
how to do it, but we're not there yet. And so I can make sure I understand, um, when we're talking

01:07:08.600 --> 01:07:17.720
about training, uh, network and float 32 versus N8 is the issue or it is the issue that, uh,

01:07:17.720 --> 01:07:23.880
between the neurons in our neural network, we've got, you know, these weights and the weights are,

01:07:23.880 --> 01:07:29.000
you know, multiplied by states to give us outputs. Is it that the weights are integers versus

01:07:29.000 --> 01:07:33.880
floats and or the states are integers versus floats? So all that. So, yeah. So,

01:07:33.880 --> 01:07:39.240
that's like, I mean, you, you have touched upon a very interesting thing. Like there are many

01:07:39.240 --> 01:07:44.680
things in the neural net that you can choose to keep in low position. Do you want to keep the weights

01:07:44.680 --> 01:07:48.280
in low position? Do you want to keep the intermediate state that you generate in low position?

01:07:48.280 --> 01:07:52.840
Do you want to keep both in low precision? Uh-huh. But do you want to keep only the gradients in low

01:07:52.840 --> 01:07:59.000
precision? I mean, there are many design choices you can make. Um, and there are,

01:07:59.000 --> 01:08:05.960
uh, all kinds of funny, interesting, unexplainable results depending on what you choose to do when

01:08:05.960 --> 01:08:12.040
we train these networks for speech. Uh, typically, uh, at the end of the training process, you spit

01:08:12.040 --> 01:08:17.320
out a model, which is basically the weights. Um, so typically you just want the weights to be,

01:08:17.880 --> 01:08:23.320
in low precision. But, uh, in recurrent nets, uh, the convergence is extremely sensitive

01:08:23.320 --> 01:08:28.040
because you're doing this serial operation through time. Right. Uh, this is, this is extremely

01:08:28.040 --> 01:08:33.240
sensitive floated floating point error. Um, to the point that if we take an algorithm and move

01:08:33.960 --> 01:08:39.560
from CPU to GPU, the floating point mat is now floating point addition, uh, is not associated,

01:08:39.560 --> 01:08:43.480
which means the sequence in which you do the addition. If it's different, you'll get a different

01:08:43.480 --> 01:08:48.920
number. Um, so say, uh, you move an algorithm from CPU to GPU, you will get a slightly different

01:08:48.920 --> 01:08:54.360
result. Uh-huh. And sometimes that can make and break a model. And that has happened with us

01:08:54.360 --> 01:09:02.360
a few times. Um, yeah. So that that space is, I think, it's, uh, it's, it's, it's a field of research.

01:09:02.360 --> 01:09:08.040
Like, how do we think of floating point in this new world of, uh, training neural nets? Um,

01:09:08.040 --> 01:09:12.680
and I wish people who deal with this kind of stuff, like, uh, the floating point committee or

01:09:12.680 --> 01:09:21.160
whatever, um, wouldn't research this more. And in terms of the, the architectures, the GPUs

01:09:21.160 --> 01:09:30.120
are more optimized for the lower precision, uh, but they also do. Yeah. So typically GPUs have

01:09:30.120 --> 01:09:35.800
only done float because that's what games and games don't need anything better or more than float.

01:09:35.800 --> 01:09:40.040
Uh-huh. And then, then GPUs started getting used in oil and gas, for example,

01:09:40.680 --> 01:09:45.480
oil and gas needs doubles, uh, for the different partial differential equations they're solving.

01:09:46.120 --> 01:09:50.360
And then comes along deep learning. And deep learning is saying that I can do with float and for

01:09:50.360 --> 01:09:56.360
things like Google net, I can get away with intake or float 16. Uh, so the GPUs are now starting

01:09:56.360 --> 01:10:02.840
to support float 16 as well natively. Um, so that, so the, so the now the GPU vendors have to kind

01:10:02.840 --> 01:10:07.320
of target three different markets, one which uses float and other which uses double and other,

01:10:07.320 --> 01:10:20.280
a part of the market. Wow. Okay. So this has been super interesting. Uh, before we go,

01:10:20.280 --> 01:10:25.560
anything, um, you know, what are you excited about? Anything that you're working on or anything

01:10:25.560 --> 01:10:31.080
that, any asks for the community, the industry? Uh, how would you like to close us out?

01:10:32.040 --> 01:10:37.960
Um, I, I am personally interested a lot in, in speech synthesis. I think, um, you know,

01:10:37.960 --> 01:10:44.520
you can build really natural interfaces. You need, um, speech that sounds natural. Um,

01:10:45.240 --> 01:10:49.800
we are far from there. Like that's why you have this robotic voice that talks to you for

01:10:49.800 --> 01:10:55.560
various devices. I think there's a lot of research to be done on very natural sounding, um,

01:10:55.560 --> 01:11:01.400
speech from all these devices that interact with voice. And, and, and going far out, I mean,

01:11:01.400 --> 01:11:06.040
I would really like to see like, you know, a conversation being a first class UI element with

01:11:06.040 --> 01:11:12.200
all the devices that we use. Right. Because as I said, conversation is our UI. Um, that'll be,

01:11:12.200 --> 01:11:16.680
uh, incredibly nice if we see it and say the next five years or so, it's challenging. It's going

01:11:16.680 --> 01:11:24.520
to be very challenging. Um, so that's kind of my, my dream in some sense. And, and in the shorter

01:11:24.520 --> 01:11:28.920
term, of course, the systems, uh, goals that I talked about, like this kind of a middleware or

01:11:28.920 --> 01:11:34.440
middle ground where we take these graph transformation. And starting to happen a little bit,

01:11:34.440 --> 01:11:38.760
TensorFlow has this thing called the XLA, uh, which is really, uh, kind of the first step in that

01:11:38.760 --> 01:11:43.000
direction, the XLA compiler that came out with the TensorFlow one release that was last week,

01:11:43.000 --> 01:11:49.240
I think. Yep. Um, so the, uh, yeah, so people are starting to take note. And hopefully we are,

01:11:49.240 --> 01:11:53.160
they're not like five years ahead of it. Maybe things will get softer,

01:11:53.160 --> 01:12:01.880
the next couple of years. Wow. Awesome. Well, Shubo, uh, how can folks find you online? How can they

01:12:01.880 --> 01:12:07.480
follow the work that you're doing? Um, so the work that I do at Baidu, we have a website,

01:12:07.480 --> 01:12:14.440
research at Baidu.com, where, um, let me see if I have the URL right. Um, uh, and a lot of the work

01:12:14.440 --> 01:12:20.200
is there. Um, I'm, you know, I'm very easily findable on LinkedIn, Shubo's and Gupta,

01:12:20.200 --> 01:12:28.120
S H U B H O and S E N G U P T A. Um, yeah, I'm happy for people to get in touch with me,

01:12:28.120 --> 01:12:32.760
ask me questions, whatever, I'd be happy to elaborate. And thank you for your,

01:12:32.760 --> 01:12:36.920
thank you for your time and this platform. Awesome. Well, thank you so much. I really

01:12:36.920 --> 01:12:41.800
appreciate you taking the time to join, uh, join in the show and I really enjoyed the discussion.

01:12:42.360 --> 01:12:44.760
Yeah, me too. Thank you. Thanks. Bye.

01:12:48.600 --> 01:12:54.040
All right, everyone. That's our show for today. Thanks again to Shubo for an amazing conversation.

01:12:54.040 --> 01:12:59.640
I hope you all enjoyed it. Once again, thanks so much for listening and for your continued support.

01:13:00.280 --> 01:13:05.080
Please remember that we want to hear from you. You can comment on the show via the show notes page,

01:13:05.080 --> 01:13:12.440
via the at Twomlai Twitter handle or my own at Sam Charrington handle via our new Facebook and

01:13:12.440 --> 01:13:19.720
YouTube pages or just good old fashioned email to Sam at Twomlai.com. Your likes and subscriptions

01:13:19.720 --> 01:13:26.920
really help support the show. So keep them coming. The notes for this show will be up on Twomlai.com

01:13:26.920 --> 01:13:33.720
slash top slash 14 where you'll find links to Shubo and the various resources mentioned in the show.

01:13:33.720 --> 01:13:37.240
Thanks so much for listening and catch you next time.

