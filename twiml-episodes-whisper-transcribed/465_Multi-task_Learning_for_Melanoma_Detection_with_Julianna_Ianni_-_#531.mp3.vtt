WEBVTT

00:00.000 --> 00:12.800
All right, everyone. I am here with Juliana Iyani.

00:12.800 --> 00:17.600
Juliana is vice president of AI Research and Development at Prussia.

00:17.600 --> 00:24.560
Juliana, welcome to the Twomo AI podcast. Thanks, Sam. Got to be here.

00:24.560 --> 00:29.440
I am really looking forward to digging into this conversation.

00:30.560 --> 00:36.320
In particular, we're going to learn a bit about some of the work that you and your team are doing

00:36.320 --> 00:42.800
on cancer diagnosis and as well as the process of commercializing that.

00:43.600 --> 00:48.640
Before we dig into that topic, I'd love to have you share a little bit about your background

00:48.640 --> 00:52.240
and how you came to work in the field. Yeah, sure.

00:52.240 --> 01:01.360
Sure. I guess I have always had an interest in being able to help people.

01:01.360 --> 01:03.760
That's always where I wanted my career to go.

01:04.720 --> 01:08.560
And so I pursued my undergraduate degree in biomedical engineering.

01:08.560 --> 01:10.160
I thought that was the best way to do that.

01:12.480 --> 01:15.680
During my undergraduate, I did several internships.

01:16.240 --> 01:18.880
One of them was in biomedical informatics.

01:18.880 --> 01:26.240
And I actually had kind of a bad mentorship experience during that internship.

01:26.240 --> 01:29.200
But I ended up shaping my career positively.

01:32.480 --> 01:35.840
So I, you know, I didn't have a lot of direction.

01:36.880 --> 01:40.960
I ended up really kind of teaching myself machine learning principles

01:40.960 --> 01:48.080
because of that lack of direction during that internship.

01:48.080 --> 01:54.960
Eventually, you got assigned a fantastic new mentor and learned quite a bit during that.

01:54.960 --> 01:58.480
But during that internship, I was learning all these things about machine learning,

01:58.480 --> 02:04.480
which wasn't really referred to as machine learning and anything that I was reading at the time.

02:04.480 --> 02:12.880
But, you know, all of the same ideas. And I realized that, you know, that is very powerful.

02:13.840 --> 02:18.400
But we have kind of the most, the most data in the medical field.

02:19.040 --> 02:23.840
It's an imaging. And so I got really, really interested in medical imaging.

02:24.480 --> 02:30.720
And, you know, pursued that, you know, thought I need to learn, learn more about that.

02:30.720 --> 02:36.240
And so that's how I got very interested in MRI. I took a whole bunch of classes at Vanderbilt,

02:36.240 --> 02:46.080
my undergraduate school in MRI. And was really fascinated by everything that I went there,

02:46.080 --> 02:55.680
wanted to pursue it. So I wanted to go to graduate school and continue research in MRI.

02:55.680 --> 03:04.000
But actually, I haven't haven't shared this publicly with anyone yet, but I didn't get in the first time.

03:04.000 --> 03:13.280
So I took a little bit of a detour. And I, you know, pursued kind of my second love, which was,

03:13.280 --> 03:20.080
which was neuroscience. I did some research at a lab at Vanderbilt in

03:20.080 --> 03:27.840
a visual attention and memory, studying the way, the way that brain works as, as you're doing

03:27.840 --> 03:35.360
those sorts of tasks. And that curiously kind of, kind of relates a lot to what I'm doing now.

03:36.400 --> 03:42.320
So I was really glad that I did that. Still was really interested in medical imaging. And,

03:42.320 --> 03:50.480
went on to pursue my PhD in that the next year. I tried again, succeeded. So, yeah, so I pursued

03:50.480 --> 03:59.280
my graduate degree in MRI. I joined Wilgerson's lab at Vanderbilt and studied kind of imagery

03:59.280 --> 04:09.200
construction and trajectory correction and trajectory optimization for MRI for non-cartesian trajectories.

04:09.200 --> 04:16.720
So kind of ways to make MRI faster, but still avoid having all these artifacts in your images.

04:16.720 --> 04:24.080
And then sort of the latter half of my PhD, I was pursuing

04:24.080 --> 04:34.720
machine learning algorithms to identify some patient tailored parameters. So for high field

04:34.720 --> 04:40.400
imaging, so for seven to you, which is not what's usually used in hospitals, usually it's

04:40.400 --> 04:47.840
lower field right now. But for seven to you, you can get better images, but you just have to be

04:47.840 --> 04:54.880
a little bit more careful and tailor things to two patients. And so a lot of that...

04:55.920 --> 04:58.800
Sorry, when you say seven to you, is that like a type of MRI?

04:58.800 --> 05:06.560
It's like the strength of the magnetic field, sorry. Yeah, so it's a stronger magnet.

05:07.440 --> 05:13.440
You tend to get kind of on even images, they're kind of unevenly lit, if you will,

05:14.720 --> 05:24.080
uneven signal. And so I was I was working on designing machine learning algorithms to predict,

05:24.080 --> 05:29.920
predict the RF-shin parameters, predict the parameters of the scanner that you need to

05:29.920 --> 05:35.200
to tune that and make the image kind of uniformly lit. Imagine those are, you know,

05:35.200 --> 05:39.440
those parameters were typically the domain of the radiologists, like they would have their hands on

05:39.440 --> 05:47.520
dials trying to figure out how to get the best image for a given patient. And now you're trying

05:47.520 --> 05:54.640
to automate that? Sort of. So it's actually it was be done by another algorithm. The issue was

05:54.640 --> 05:59.840
that it would take a really long time. So you'd have to do a scan. So you already had the patient

05:59.840 --> 06:06.080
in the scanner. And then you had to run this really extensive algorithm to get the parameters.

06:06.080 --> 06:11.360
And then go plug those into the scanner. And then run the scans that you actually wanted to do.

06:11.360 --> 06:19.600
So it's just a, you know, not a very feasible process for most patients and for bringing it kind

06:19.600 --> 06:24.720
of to the clinic. So that was what we were working on. And then doing all this in my PhD, I

06:24.720 --> 06:31.600
was really interested in machine learning, but seeing deep learning take off all around us,

06:31.600 --> 06:39.760
I was I was pretty excited about the potential of that. And so really, really wanted to get in on it.

06:39.760 --> 06:46.800
I was also kind of in my PhD, you know, realizing that what really, really drives me is seeing

06:46.800 --> 06:51.200
something put into practice. I wasn't I wasn't getting that in academia where I was.

06:52.320 --> 06:57.920
Some people do get that from academia, but I wasn't in the right place for that. And so I really

06:57.920 --> 07:03.120
wanted to go into industry and be a part of a company that was that was using deep learning

07:03.120 --> 07:08.000
from medical imaging. And in a way that it could could actually impact patient's lives in the short

07:08.000 --> 07:13.760
term. And was Prussia already doing deep learning on images when you got there?

07:15.760 --> 07:22.320
Yes, yes. So Prussia was very small when I joined. I think less than less than 10 people

07:22.320 --> 07:28.000
at the time. So I got in, got in kind of early days, although the company had been around for a

07:28.000 --> 07:36.000
few years already, I think, was founded in 2014. Now 10 people doing AI data stuff or 10 people

07:36.000 --> 07:42.080
total the entire company. 10 people total the entire company. Okay. Yeah. Yeah.

07:42.080 --> 07:50.320
The like the core products, you say image management like content management system for

07:50.320 --> 07:59.520
radiology or. So pathology images are kind of different different from radiology. So usually,

07:59.520 --> 08:06.960
you know, if you have made a suspected cancer, you'll get like an MRI or a CT scan that will be like

08:06.960 --> 08:12.960
the first thing that happens. But later down the road, if there's still suspicion, you'll get a

08:12.960 --> 08:19.760
biopsy. And that tissue gets sent off to a lab that processes it. They're eventually going to

08:20.720 --> 08:28.720
slice up the tissue into really thin pieces and dead it and wax and stain it and put it on a

08:28.720 --> 08:34.800
slide. So that, that like glass slide, like, you know, like you had in high school biology class,

08:34.800 --> 08:40.320
like gets looked at normally under a microscope to diagnose your cancer.

08:42.000 --> 08:48.720
But what Prosa deals with is actually digitized versions of those glass slides. So very,

08:48.720 --> 08:56.320
very high resolution images of very, very small pieces of tissue. Tell us a bit about where deep

08:56.320 --> 09:03.200
learning comes into the picture. So deep learning as it's kind of taken off in the field of pathology,

09:04.000 --> 09:12.720
in the field of kind of studying this type of image. And there are whole host of potential

09:12.720 --> 09:21.760
applications for it. What we focus on at Prosa is applications that are built in some way to make

09:21.760 --> 09:32.400
the pathologist's lives easier to make labs more efficient. So enable more efficient assigning

09:32.400 --> 09:40.000
of cases to different pathologists or specialists or enabling them to review their cases faster.

09:40.000 --> 09:51.760
We also focus on deep learning systems or as we probably, probably controversially refer to them

09:51.760 --> 10:05.680
AI systems. We also focus on systems that can improve accuracy of pathologists. So in some scenarios,

10:05.680 --> 10:14.560
there are certain diagnoses that are a lot harder to make than others. And pathologists often

10:14.560 --> 10:23.760
disagree on those diagnoses. And so any way that we can, that we can use AI to deliver information

10:23.760 --> 10:29.920
to the pathologists that will improve their accuracy, that is something that we're interested

10:29.920 --> 10:36.640
into. So that's maybe a good segue to talking about the paper that you and your team

10:37.440 --> 10:44.400
will be presenting at the ICCV computational challenges in digital pathology workshop.

10:45.840 --> 10:51.840
If that's not enough of a mouthful, the title of the paper is a pathology deep learning system

10:51.840 --> 11:01.280
capable of triage, of melanoma specimens utilizing dermatopathologist consensus as ground truth.

11:02.080 --> 11:10.320
Did I get all that? You did, you did. Sorry for that. We'll make our next title shorter.

11:12.080 --> 11:19.200
So what's those paper all about? So this paper is about, you know, new new technology,

11:19.200 --> 11:24.880
new AI system that we've developed that's really kind of building on our previous work. So

11:26.160 --> 11:36.080
previously we built a system that could sort of sort and triage pathology cases before pathologists

11:36.080 --> 11:45.520
reviewed them. It's sort of them into four different categories that were not specifically

11:45.520 --> 11:52.320
diagnostic categories, but, you know, related to the diagnosis, kind of diagnostic groupings,

11:52.320 --> 11:59.840
you could think of. And, you know, the reason that we wanted to do that was to kind of make

11:59.840 --> 12:08.000
pathologists work more efficient, you know, be able to have them review certain cases earlier

12:08.000 --> 12:14.000
in the day so they could order additional tests if they needed to earlier in the day if it was

12:14.000 --> 12:19.440
a group of cases that could, that was more likely to need additional testing, things like that.

12:20.960 --> 12:26.240
But that previous system didn't do something that pathologists were really, really interested in,

12:27.280 --> 12:36.560
and that was to classify melanoma. Some melanomas is not the most common form of skin cancer,

12:36.560 --> 12:47.360
but, you know, one of the most deadly. And so, it's also one that pathologists

12:49.760 --> 12:57.760
disagree on a lot. So, if something, if you go in and get a biopsy of your skin and

12:57.760 --> 13:02.560
make it sent in, and it's kind of one of the ones that looks suspicious for melanoma,

13:02.560 --> 13:10.560
it's fairly common for pathologists to disagree on whether, whether it actually is melanoma. And

13:10.560 --> 13:17.920
you tend to, tend to want a subspecialist to review that. So, there are, there matter pathologists is

13:17.920 --> 13:23.920
one of the long words in our title, who are the subspecialists, who review these cases,

13:23.920 --> 13:33.600
and not always, not always, but, you know, sometimes it's not a very obvious, not a very obvious one.

13:33.600 --> 13:38.640
You want that subspecialist to kind of give you a second opinion, essentially, on the case.

13:39.760 --> 13:42.960
So, it would be more efficient if we could route the cases to them first.

13:43.520 --> 13:49.760
Okay, okay. I guess one of the virtues of a long title is that it actually says what the

13:49.760 --> 13:59.840
paper is about. And this one, I think, does a good job of kind of raising one of the core challenges

13:59.840 --> 14:08.960
that it sounds like you must face. If the specialists and, you know, reviewers of these pathology

14:08.960 --> 14:17.920
images tend to disagree, that seems like that would make building datasets and, you know,

14:17.920 --> 14:24.960
collecting ground truth particularly difficult. Yes, yes, absolutely. And I don't think,

14:24.960 --> 14:30.000
I don't think we even realize how difficult when we first set out, set out to build this system.

14:32.080 --> 14:37.280
I don't think the pathologists that we were working with, realized, realized exactly how much

14:37.280 --> 14:45.200
they disagree. When we set out to build this, there's actually surprisingly not that much literature

14:45.200 --> 14:52.560
on the matter. How have you characterized, how would you and how do you, like, have you

14:52.560 --> 14:59.520
characterized the degree of disagreement among the pathologist looking at a given specimen?

14:59.520 --> 15:07.920
Yeah, it's hard. So, there are, I guess, you know, a few different ways of doing that.

15:07.920 --> 15:15.920
What we, what we did for this paper was to have three different amount of pathologists reviewing

15:15.920 --> 15:24.160
the cases. And specifically, the ones that were melanocytic or, you know, basically the category

15:24.160 --> 15:30.640
that, the broader category that melanoma is in. So, that ranges from benigniva, which are just kind

15:30.640 --> 15:39.280
of your basic mole, all the way to, you know, invasive cancer. But we had pathologists review

15:39.840 --> 15:46.800
all of those cases in three different ones and, and characterize, you know, how often,

15:46.800 --> 15:54.400
how often did they disagree for which original diagnosis? And what we ended up doing for this paper

15:54.400 --> 16:01.520
was to just say, okay, we're not going to use it if they didn't all agree, because very often,

16:01.520 --> 16:06.000
they didn't all agree, especially for the ones that were kind of on the more suspicious end of

16:06.000 --> 16:11.840
things. I think it was something like 40 to 50 percent of the time they didn't agree.

16:11.840 --> 16:19.520
Oh, wow. So, the, so the 60 percent of the time, 50, 60 percent of the time where they do agree,

16:19.520 --> 16:26.720
that becomes your training data set. And the outcome that they agreed on is your, your label.

16:28.160 --> 16:34.960
Yeah. But again, from the explicit title, it sounds like you're using some kind of consensus

16:34.960 --> 16:44.720
algorithm on, you know, real data, you know, to kind of predict the outcome.

16:44.720 --> 16:52.000
Um, or where's consensus coming in is just that, that three out of three pathologists agree,

16:52.720 --> 16:57.520
is basically what we're using as our ground truth. Okay. Yeah, I was, I think I was envisioning

16:57.520 --> 17:04.640
some kind of like ensemble weird thing and like doing a consensus among your, among predictions.

17:05.200 --> 17:12.160
Yeah. Well, you're actually, you're not far off from what, what we did end up doing with,

17:12.160 --> 17:17.280
um, one of the models in the system. So the system, um, that actually makes these predictions,

17:17.280 --> 17:26.240
makes these classifications is, is a hierarchy of models. Okay. And, uh, the one that is actually

17:26.240 --> 17:32.880
classifying these, these melanocytic specimens, um, is what we ended up building for that is a

17:32.880 --> 17:39.040
multi task model. Uh, and I think that a lot of the reason that we ended up wanting to do that

17:39.040 --> 17:45.760
was because of this, this coordinates, because the pathologists are disagreeing so often. Um, so

17:45.760 --> 17:52.000
we, what we wanted to do was divide these specimens into three different categories. So kind of,

17:52.800 --> 17:59.360
you can think high intermediate and low risk and, you know, with, with melanoma being the highest

17:59.360 --> 18:05.040
invasive melanoma being the highest and intermediate melanoma in C2 or the lesions that were

18:05.040 --> 18:12.320
severely atypical, uh, and then low being your benign ones. Uh, but so you could do that as

18:12.320 --> 18:19.440
like a three way classifier, right? Um, we didn't end up doing that, doing it that way because,

18:19.440 --> 18:24.160
at least in the early days and most building this, it didn't work very well. Um,

18:24.160 --> 18:30.080
That's a good reason we could do it that way. Yeah. Um, but I think a lot of the reason that

18:30.080 --> 18:35.280
it didn't work while it was because of the, the disagreement. And so what we ended up doing was

18:35.280 --> 18:43.760
building, uh, you know, a multi task classifier that just did, uh, a binary classification at a time

18:43.760 --> 18:50.160
essentially. So it was, can you distinguish the low risk from the high risk? Can you distinguish

18:50.160 --> 18:56.400
the low risk from the intermediate risk? And can you distinguish the intermediate risk from the

18:56.400 --> 19:02.480
low risk? Can we chunk up this classification into smaller tasks? Um, and just do that because,

19:03.280 --> 19:10.400
uh, our ground truth was just too noisy, uh, to do this as it's just a typical like three way

19:10.400 --> 19:17.760
classifier. And the approach to training was kind of an end-to-end approach, you know,

19:17.760 --> 19:27.680
they kind of multi-task trained, uh, this entire system, or did you, um, train, you know, them

19:27.680 --> 19:32.480
independently or hierarchically or something like that, hierarchically? We did end up having to

19:32.480 --> 19:39.040
train them independently. So it's not, not an end-to-end system. Uh, it would be cool if it was, uh,

19:39.040 --> 19:44.880
but the system we built has, uh, has kind of several pieces. So there's, there's, there's an

19:44.880 --> 19:51.040
and better, um, and then there's, there's kind of a hierarchy of three different models, um,

19:51.040 --> 19:58.000
that to arrive at a final classification. Um, so it would have been, would have been a lot to build

19:58.000 --> 20:03.760
this as an end-to-end system. Yeah. Um, probably a lot of issues that we would have run into, but not

20:03.760 --> 20:10.720
impossible. Uh, so tell us about the, the model. What is it, you know, what are the components? I mean,

20:10.720 --> 20:16.000
you mentioned high-level what the components are, but like, what, what are the components? And how did

20:16.000 --> 20:23.520
you, um, how did you arrive at them? You know, were they all custom? Was there some off-the-shelf,

20:24.240 --> 20:31.680
uh, stuff that you used? How did you ultimately arrive at an approach for this?

20:31.680 --> 20:37.200
Yeah. So this system, like I said, was built off a, uh, a system that we, that we had previously

20:37.200 --> 20:43.280
built. So it's a bit of an evolution, many different, many different pieces involved. Um, so one of,

20:43.280 --> 20:48.720
one of the issues I haven't discussed yet that we've run into was that, um, some of the images have

20:48.720 --> 20:58.640
artifacts that are kind of correlated, uh, in many cases with the, the ground truth. Um, and so we

20:58.640 --> 21:05.280
wanted to get rid of those, um, those, namely, the major ones that we encountered were that

21:05.280 --> 21:11.680
pathologists sometimes use penning to mark, um, mark cancer on the tissue. Yeah. Um, so that was,

21:11.680 --> 21:17.760
that was an issue we ran into. Um, so that was kind of a whole, whole other project that we,

21:17.760 --> 21:25.360
that we did and we had, we had a custom model built, um, to eliminate penning from our images.

21:25.360 --> 21:33.040
And did you try to remove it or just kind of, uh, lock it out? I guess the way to, the way to talk

21:33.040 --> 21:39.840
about this is to explain a little bit more about how our models work. Um, so first, um, first thing

21:39.840 --> 21:46.960
we have to do because these are very, very large images, um, we have to kind of chunk them up. Um,

21:46.960 --> 21:52.960
so we're first kind of detecting the tissue regions on the side. Um, and then we just kind of

21:52.960 --> 22:00.240
patch those up into what we call tiles. So there's maybe like a whole bunch thousands of 128 by

22:00.240 --> 22:10.080
128 pixel images, um, that are our tiles. Um, and so when we remove ink, we literally just drop

22:10.080 --> 22:17.280
tiles. Yeah, I realized as I said it that kind of blocking it out wouldn't be much better than

22:17.280 --> 22:24.640
just leaving the ink in if you fed, uh, image with blocked out ink to the downstream model.

22:24.640 --> 22:31.360
Uh, it, yeah, probably not, but it depends. I've seen, I've seen a paper that tried to use

22:31.360 --> 22:38.640
GANS to just, you know, generate, generate, you know, like tissue, look, yeah, in fill. Uh,

22:38.640 --> 22:45.200
but it didn't look like it worked very well. So I don't, maybe there's a strategy there.

22:46.880 --> 22:52.800
Yeah, to what degree was that 128 by 128 kind of a architectural hyperparameter that you played

22:52.800 --> 22:59.920
with? Was that just where that decision come from? Sam there are so many hyperparameters and so

22:59.920 --> 23:08.400
little time. Uh, we didn't, we didn't, we haven't played with that one yet, actually.

23:08.400 --> 23:17.520
It may be taking a step back, you know, we've seen over the, the years kind of many approaches to

23:17.520 --> 23:27.920
trying to detect cancer in, uh, images, um, you know, all different kinds of cancers, all different

23:27.920 --> 23:34.880
kinds of approaches, you know, to the point where like some very, very famous AI people said,

23:34.880 --> 23:40.400
oh, this is a solved problem. Like, you know, we're going to replace radiologists with, uh, with AI

23:40.400 --> 23:49.120
systems. I think we've all, uh, kind of dialed back, you know, that enthusiasm a bit or rhetoric,

23:49.120 --> 23:55.760
if you would go that far, but, um, you know, kind of give me a sense for or give us a sense for,

23:56.560 --> 24:02.720
you know, what distinguishes this particular work from the other work out there, you know,

24:02.720 --> 24:08.000
that is trying to do similar things. Definitely not a solved problem. I feel like, uh, we all

24:08.000 --> 24:14.960
generally in this industry now, I can understand how complicated the, even the self-driving cars problem is,

24:14.960 --> 24:22.160
and uh, this is at least that complicated. And that's actually, that's actually interesting,

24:22.160 --> 24:27.680
because as much respect that I have for the complexity of this problem relative to solution,

24:27.680 --> 24:34.080
to solving it, when I think of a self-driving car, I think of a much more complex system,

24:34.080 --> 24:39.920
uh, with lots of moving parts. And in your mind, they're kind of on the same order in terms of

24:39.920 --> 24:45.600
complexity, you know, elaborate, elaborate on it. Where's the complexity that's not obvious to us,

24:46.160 --> 24:49.920
you know, when we think of problems like the one you're, you're trying to solve here?

24:49.920 --> 24:54.320
Yeah, a great question. I mean, it wasn't obvious to me when I, when I got into this field.

24:55.040 --> 24:59.120
Um, you know, it's like, you know, with image and that is solved, right?

24:59.120 --> 25:06.240
Uh, what could be more complicated, but it is quite complicated. So when you think about what a

25:06.240 --> 25:12.320
pathologist does, they're not just, not just kind of looking at an image and classifying it.

25:12.320 --> 25:17.200
They kind of synthesize a whole bunch of things, but pathologists, when they're looking at an

25:17.200 --> 25:22.880
image, they're not just thinking about the image, they're thinking about the patient's history

25:22.880 --> 25:29.600
and clinical information, they're thinking about the patient's age, um, and they're thinking about,

25:30.000 --> 25:38.240
uh, you know, the last time that they saw a patient like this or, uh, you know, all these other

25:38.240 --> 25:44.160
inputs that they are kind of synthesizing to make a diagnosis. And then, aside from that,

25:44.160 --> 25:53.520
the, uh, you know, the diagnosis itself can be very, very complicated for, for some of these things.

25:53.520 --> 26:04.000
So, um, in dermatopathology, which is skin pathology, um, there are probably over 500 different

26:04.000 --> 26:08.560
diagnoses that you can have. So there's a lot of different types of patterns that you need to be

26:08.560 --> 26:17.280
looking for, uh, in these, in these, uh, type of images that, that we look at, um, they're very,

26:17.280 --> 26:23.040
very high resolution. And so the pathologist is kind of first looking at them, kind of, at a high

26:23.040 --> 26:29.280
level and just, um, you know, seeing, okay, what are the regions that I should focus my attention on?

26:30.240 --> 26:34.240
Um, and what are the reasons I, regions I can totally ignore? Because they,

26:34.240 --> 26:40.800
I probably, in many cases, literally can't go through a pixel by pixel. They have to decide, okay,

26:40.800 --> 26:46.320
what can I ignore? Uh, kind of like a driver does when they're driving a car. Uh, what a,

26:46.320 --> 26:52.000
where do I focus my attention? Uh, and then they're going in at higher power to those regions that

26:52.000 --> 27:00.480
may be concerning, uh, based on, you know, based on all of their knowledge, um, and looking for very

27:00.480 --> 27:06.880
particular associations between like one region of an image at another? Well, it, it sounds like the,

27:07.520 --> 27:15.120
you know, the high level, um, summary is that the problem that you're trying to solve is not the

27:15.120 --> 27:19.920
binary classification that we've been sold in the media. It's a much more complex and nuanced

27:19.920 --> 27:32.400
classification problem, um, and that among other things, increases the level of complexity,

27:33.040 --> 27:38.400
you know, on par with kind of the percept, perceptual problem that a driver, you know,

27:38.400 --> 27:42.640
something that's trying to drive might be experiencing. But that's kind of the origin of your,

27:42.640 --> 27:49.040
your, your comment. I think I interrupted you on a prior question and I'm trying to remember

27:49.040 --> 27:56.480
what that was, uh, but well, what that was was like, what's different about, um, the problem that

27:56.480 --> 28:02.480
you're trying to solve with this paper and, you know, some of the other kind of cancer detection

28:02.480 --> 28:07.920
systems that have been publicized or that are prevalent in other fields or being worked on in

28:07.920 --> 28:12.640
other fields. One of the big differences and other things that we're really excited about with

28:12.640 --> 28:20.560
this work is that, um, there hasn't been, there hasn't yet been any kind of system, even like a,

28:20.560 --> 28:26.400
you know, a different stain or a genetic test or something like that, that can identify melanoma

28:26.400 --> 28:32.400
before a pathologist reviews a case. Uh, and the reason we're really excited about the system

28:32.400 --> 28:38.960
being able to do that is because, uh, it means that you can send that case to the right person

28:38.960 --> 28:45.200
very quickly rather than it having to get passed along from one person to another to, to say, like,

28:45.200 --> 28:50.960
oh, this isn't what I should be diagnosing. Let me send it to my colleague down the hall or across

28:50.960 --> 28:59.840
the city. Um, so we, we think we can speed up, um, the timeline for getting melanoma diagnosed with

28:59.840 --> 29:06.560
a system, uh, which is, is pretty exciting. Uh, should bleed to, should lead to patients getting

29:06.560 --> 29:10.560
their diagnosis faster, like, getting treatment faster ultimately.

29:11.440 --> 29:15.680
Is it the case of other systems that are out there, like rely on some kind of

29:16.400 --> 29:21.040
metadata or coding or something like that that comes from the pathologist in order to make

29:21.040 --> 29:27.440
their decision? Like, good question. With, uh, skin pathology, really aren't that many AI systems

29:27.440 --> 29:34.240
out there at all. Um, so we're, we're one of the first to really explore this and, and definitely

29:34.240 --> 29:41.040
the first to really, uh, explore it at a, at a level where it's, um, you know, this close to the

29:41.040 --> 29:47.040
clinic. There have been, there have been a, a number of papers kind of with a more academic that,

29:47.920 --> 29:56.080
um, exploring the issue. Um, but the only other tests that are out there right now that can detect

29:56.080 --> 30:05.280
melanoma are, um, our tests that are run after the pathologist has lifted the image. So it's, um,

30:06.080 --> 30:12.800
basically, you know, they'll be looking at something that, um, looks suspicious for melanoma and

30:12.800 --> 30:19.680
it's maybe borderline. Um, so the pathologist is not sure. Is this melanoma? Is this not, you

30:19.680 --> 30:28.480
know, should I recommend that they get a really deep oxygen, uh, or not? Um, and so they'll send it off

30:28.480 --> 30:37.280
to, to a lab, um, that does, like, genetic testing or does, um, an additional stain on the tissue,

30:37.920 --> 30:42.800
um, so that they can tell a little bit more about it and, and those tests will kind of like give

30:42.800 --> 30:51.760
you like a score, um, for the likelihood of melanoma, um, but you can't run it on just like any

30:51.760 --> 30:58.000
skin specimen. Um, they're only intended for after the pathologist has, has looked at the case.

30:58.000 --> 31:07.040
Talk a little bit about the, the specimen versus image issue, you know, when we read about,

31:07.040 --> 31:12.640
you know, a system that might, you know, one of the early ones was, uh, identifying breast cancer

31:12.640 --> 31:20.800
in a, you know, a radiological image. Um, it sounds like one of the big distinctions you're

31:20.800 --> 31:27.200
making is that your system is looking at the specimen level. Why is that important?

31:27.200 --> 31:36.800
So with, with pathology, uh, there are usually a couple different, um, whole-side images that belong to a,

31:36.800 --> 31:44.160
a given specimen, um, and that specimen, by the way, might correspond to like a biopsy that has

31:44.160 --> 31:48.720
been taken, like, of a mole. So your, your mole might, might end up being kind of a specimen,

31:49.360 --> 31:53.760
uh, and it might end up with, uh, tissue on a couple different slides that the pathologist has to

31:53.760 --> 32:03.120
look at. Um, and some of those, some of those slides might not even have, um, any, any cancer,

32:03.120 --> 32:09.760
any mole on them. They might be totally clean, totally normal skin. Um, and so the pathologist,

32:09.760 --> 32:14.080
when they're making a diagnosis, they look at, they look at all the slides that are part of this

32:14.080 --> 32:23.120
specimen with a case. And we wanted, wanted our ASS to be able to do that too. Um, I, it was a challenge

32:23.120 --> 32:28.640
because there's so much tissue on a single slide and not, uh, you know, not to mention multiple slides.

32:29.360 --> 32:36.240
Um, but we're, we're excited to be able to do that, um, because it means that, uh, our, our system

32:36.240 --> 32:41.680
is a little bit closer to, to reality, to what the pathologist is, is actually doing.

32:41.680 --> 32:49.200
Is the model's ability to work at the specimen level kind of part of its training routine?

32:49.200 --> 32:54.640
And I guess in contrast, what I'm imagining is that there's a simple heuristic that is,

32:54.640 --> 33:00.160
hey, if any of these slides has cancer, then the specimen has cancer. Is that not? Yeah.

33:00.160 --> 33:04.960
Actually true. Yeah, it's a good question. And what you just mentioned, if any of these slides

33:04.960 --> 33:09.920
have cancer, then the specimen has cancer. That's what, that's what we've done for our previous system.

33:09.920 --> 33:16.080
Um, so it was kind of aggregating the, the classifications at the end. Yeah. Um, but it's,

33:16.080 --> 33:23.040
it's a, I guess it's a less accurate way of doing it. It introduces more room for, for error

33:23.040 --> 33:28.960
in your model. And so we did, um, we did develop a way to train basically with, with all the

33:28.960 --> 33:36.640
slides in this specimen. Um, and we, we do that basically by including all of the tiles from all

33:36.640 --> 33:44.560
of the, all of the slides, um, in one, um, one bag under the multiple instance learning paradigm.

33:45.680 --> 33:52.240
So what that's doing is basically, um, you know, within your network, you're kind of,

33:52.240 --> 33:58.240
you're processing the features from, from each of the tiles, um, kind of individually. And then you

33:58.240 --> 34:07.440
have this aggregation function. That's, um, basically kind of, um, combining, combining, um,

34:07.440 --> 34:12.080
what the model has learned from all those tiles in it, you know, in some kind of learned weighted

34:12.080 --> 34:18.960
fashion. Kind of continuing on this particular point is the idea that if a model makes this kind of

34:18.960 --> 34:26.400
simple aggregated decision, the decision, um, you know, the error rate is higher, uh, because that

34:26.400 --> 34:33.600
decision is just wrong sometimes. And, you know, the, the aggregated heuristic is just not correct,

34:33.600 --> 34:43.440
or is it more nuanced like by doing aggregation, the training is more efficient, or the model learns

34:43.440 --> 34:48.800
different things. And so because it learns those different things, it makes better decisions.

34:48.800 --> 34:57.680
I think the answer is both. Um, so yeah, I think it is a more error prone process. If you have to do

34:57.680 --> 35:02.640
that aggregation at the end, because you have the chance to like, like, you know, let's say you have

35:02.640 --> 35:07.600
three different images in a single specimen, you, you have three different chances for your model

35:07.600 --> 35:14.080
to make an error on one of those, and then you're going to aggregate them. Um, but the other side of

35:14.080 --> 35:21.760
the coin is that, uh, the way we do this shouldn't prove the training process as well, um, in some

35:21.760 --> 35:30.160
former fashion. So, um, we could do training by having a pathologist go in and label each of the

35:30.160 --> 35:38.320
slides. So, um, I have three slides from the specimen. Um, the specimen is a melanoma specimen.

35:38.320 --> 35:43.040
Two of the slides have melanoma and one of them, you know, is just normal skin. I could have a

35:43.040 --> 35:49.040
pathologist label that, but it's not how, um, the medical records are, the medical records just have,

35:49.040 --> 35:54.240
say, you know, this specimen has three slides and it's melanoma. Um, so it's like an extra

35:54.240 --> 36:00.800
data curation step. Um, if we can't directly use that, um, you know, the other thing that we could do

36:00.800 --> 36:06.960
is, you know, train a system with those three slides kind of independently and say they're all melanoma,

36:06.960 --> 36:11.760
but then you're introducing noise into your system. What's the overall structure of your

36:11.760 --> 36:20.240
model or was the architecture look like? So we talked a little bit about how, uh, you know, our first step

36:20.240 --> 36:29.040
is kind of removing ink from the slides. Yeah. Um, after we do that, um, what we want to, what we

36:29.040 --> 36:38.560
want to do is, um, basically create embeddings. Um, and so we actually just use an off-the-shelf

36:38.560 --> 36:46.480
model for that. We use, um, ImageNet trained on ResNet, uh, for this, for this work. Uh, and so we,

36:46.480 --> 36:52.640
we basically create feature embeddings for each of the tiles independently. Uh, and then we have

36:52.640 --> 36:58.800
a hierarchy of models that actually makes the classification, um, based on those embeddings. Um,

36:58.800 --> 37:07.920
and the first, I guess there's one model at the top of the hierarchy, um, and that is basically

37:07.920 --> 37:14.080
distinguishing melanoma from everything else, um, or I guess melanoma and suspected melanoma

37:14.080 --> 37:21.600
from everything else. Uh, the reason why we did that, uh, is because, uh, we wanted this system to

37:21.600 --> 37:29.440
be highly sensitive to melanoma. And since melanoma is only about two percent of the overall cases,

37:30.480 --> 37:40.480
um, we had quite a class of balance problem. Um, so, so that's the first step of the hierarchy.

37:40.480 --> 37:49.040
And, and once we have kind of, we have the sort of suspected melanoma group, um, we have a subclassifier

37:49.040 --> 37:59.280
that, um, is, uh, kind of refining that classification. Um, the, the, the, this is the multi-task model

37:59.280 --> 38:04.320
that I talked about before that's distinguishing the high, low, and intermediate risk specimens.

38:05.360 --> 38:12.240
Um, and then on the other side of things, um, you know, for those specimens that were, um,

38:12.240 --> 38:21.280
basically, uh, classified as not, not at risk for melanoma, um, we, we are providing four

38:21.280 --> 38:26.800
different classifications. So we have a separate classifier that's doing that. Um, and it's,

38:26.800 --> 38:34.320
it's basically, uh, distinguishing, um, the two most common types of, of cancer, um,

38:34.320 --> 38:44.080
so squamous cell carcinoma and basal cell carcinoma, uh, and then those, um, those lower risk

38:44.080 --> 38:50.320
melanocytic specimens. So basically, you're benigny of our moles. Uh, and then we have this

38:50.320 --> 38:59.600
wonderful group called other. And that, that is literally everything else, uh, which, um,

38:59.600 --> 39:05.600
I was like, I think I talked about earlier is there are hundreds and hundreds of diagnosis and

39:06.320 --> 39:12.560
in skin pathology. And so we couldn't possibly classify them all. And so we have this

39:12.560 --> 39:20.960
catch all group. I guess it'd be, it'd be really interesting to, um, you know, understand kind of the,

39:20.960 --> 39:29.440
uh, you know, cell images through or sent, you know, specimen images through the, the eyes of

39:29.440 --> 39:34.720
image and that I guess is what I'm thinking of here. Yeah. Yeah. It would be really interesting.

39:34.720 --> 39:41.120
And so you kind of put all this stuff together. You mentioned that you trained the components,

39:42.240 --> 39:48.640
um, independently or at least the classifier independently. Is that like the features of the,

39:48.640 --> 39:57.920
uh, that your training data is the, the feature in the embedding for, yeah. Yeah. Yeah.

39:57.920 --> 40:03.600
Okay. The embeddings for each of the three models in the hierarchy. Yeah. Okay.

40:04.560 --> 40:09.600
Cool. And so, you know, tell us about how it all, how it all worked and, and, you know,

40:09.600 --> 40:16.560
what kind of results you saw and, um, you know, any challenges you ran into that kind of thing.

40:16.560 --> 40:24.640
Yeah. Yeah. So this, this worked pretty well. We were, we were able to distinguish, uh, the melanoma

40:24.640 --> 40:33.440
specimens from, uh, from, you know, the others pretty well. We had area under the curve of, of point

40:33.440 --> 40:42.080
nine, three and classifying the melanocytic suspect specimens, um, and, uh, you know, reproduce that

40:42.080 --> 40:48.240
pretty well in, in the two labs that we had for, for validation. So we were pretty happy with

40:48.240 --> 40:55.760
this result. I'm sure I'm sure it could be improved on with, with more data even, um, the fact that we

40:55.760 --> 41:03.520
could get, um, we could get a, a decent number of the, the melanomas kind of pulled out into this,

41:03.520 --> 41:12.240
this, this suspect classification, uh, already means that you can prioritize those in your, in

41:12.240 --> 41:17.680
your case lists, if you're a pathologist. So, um, you know, versus, they're just kind of like

41:17.680 --> 41:26.320
randomly interspersed throughout your, your day. Otherwise, um, we thought this was, was, was pretty

41:26.320 --> 41:35.680
exciting. And now you're the head of AI research and development at Procia, like, how does this

41:35.680 --> 41:44.320
translate into something that has impacts, uh, you know, at health centers and, and with users of

41:44.320 --> 41:53.600
your system? Yeah, yeah, that's a great question. Um, so kind of have to, have to demonstrate, uh,

41:53.600 --> 42:00.560
that, you know, the system that we've built actually has utility in practice, um, and that you,

42:00.560 --> 42:07.120
you kind of see similar results in practice. Um, so we've already done, uh, a couple of

42:07.120 --> 42:14.480
deployments of this particular system at a couple, uh, academic medical centers. And you'd just kind

42:14.480 --> 42:21.360
of, uh, a trial, uh, so our first, our first step was to see, you know, make sure that it wasn't

42:21.360 --> 42:25.520
biasing the pathologists. So you don't want to, you don't want to impact their diagnosis in a

42:25.520 --> 42:30.880
negative way. So we were, we were able to show that that it didn't have a, uh, any significant

42:30.880 --> 42:37.280
impact on their diagnosis. The next step is showing that it does indeed allow us to reduce the

42:37.280 --> 42:44.160
turnaround time for these cases. And for that, I think we need to, um, you know, we need to show

42:44.160 --> 42:51.200
that at a, at a larger site. Um, and one that's, that's kind of actually, you know, experiencing these

42:51.200 --> 42:57.440
challenges that, um, you know, they have, you know, multiple dermatopathologists or subspecialists

42:57.440 --> 43:02.720
and multiple general pathologists. And, um, how do they, how do they allocate their cases in the

43:02.720 --> 43:09.120
most ideal way that makes them the most efficient? Got it. Got it. So some, um, it beyond the research,

43:09.120 --> 43:16.640
some promising results, kind of in practice, but, you know, work to be done to, I guess show that

43:16.640 --> 43:23.920
it has the desired benefit at scale. Yeah, yeah, exactly. Awesome. Awesome. Well, Juliana,

43:23.920 --> 43:31.120
thanks so much for sharing, uh, a bit about what you're up to and the, the paper. I'm sure there's

43:31.120 --> 43:37.440
lots more interesting stuff. I'm, I'm, this last question that I asked about kind of productizing it,

43:37.440 --> 43:43.840
um, you know, I can imagine going on for a long time, just talking about that, uh, particularly in a,

43:43.840 --> 43:55.280
kind of mission critical, highly regulated, uh, space like healthcare. Um, yeah. But, uh, super

43:55.280 --> 44:01.760
interesting project and, and thanks so much for sharing it. Awesome. Thanks, Sam. Got to be hearing

44:01.760 --> 44:10.400
and glad to share with you guys. Awesome. Thank you.

