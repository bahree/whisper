WEBVTT

00:00.000 --> 00:21.640
All right, everyone. I am here with Yoon Yao Lee. Yoon Yao is a senior research manager with IBM research working on natural language processing.

00:21.640 --> 00:27.120
Yoon Yao, welcome to the Twoma AI podcast. Thank you so much for having me here.

00:27.120 --> 00:43.440
I'm looking forward to digging into our chat. We'll be talking of course about NLP and some of your experience on the research side as well as making it useful for IBM's enterprise customers to get us started.

00:43.440 --> 00:48.040
I'd love to have you share a little bit about your background and how you came to work in the field.

00:48.040 --> 00:57.200
Sure. So as you mentioned right right now, I'm a senior research manager at IBM research before that.

00:57.200 --> 01:03.680
How did I get there? It's actually a long story. So I have to say, I grew up in a small town in China, right?

01:03.680 --> 01:15.280
Before I went to college, I didn't even see a computer. But when I picked my major, I picked the major automation because I thought if I automated everything, I don't have to do anything.

01:15.280 --> 01:22.480
It's like a part of really think about AI helping people.

01:22.480 --> 01:36.440
So when I went to college, I also did a secondary degree, I did a dual degree, I did automation and economics because I want to understand how technology impacting people, impacting the society.

01:36.440 --> 01:55.400
Then I went down to come to US to pursue my graduate studies. So I did two master degree again, I did a line in computer science, another line in information science because again, I want to understand the technology itself, but I also want to understand how technology impacting people.

01:55.400 --> 02:09.520
So my information science, more focused on HCI side and information economics. Then I pursue my PhD in database. However, it's not a traditional database, PhD.

02:09.520 --> 02:20.360
So what I did was to enable people to query database using natural language. So that's basically the longest topic today in natural language processing.

02:20.360 --> 02:30.920
So I will say my connection with AI basically started from the very beginning of when I started my higher education.

02:30.920 --> 02:46.120
Nice, nice. And now as a senior research manager, talk a little bit about your research interests and more broadly, the role and.

02:46.120 --> 02:57.080
It sounds like you you're focused on kind of traditional research, but you also have these customer engagements element to your role. Can you talk a little bit about that?

02:57.080 --> 03:11.880
Sure. So my general research interest is quite broad, but in general, I'm very passionate about building systems and the tools to enable people wide range of users to be able to use technology.

03:11.880 --> 03:25.880
In particular, since I work in the field of natural language processing, my passion is really to empower people to harvest information from text to build the next generation of AI applications.

03:25.880 --> 03:49.880
So within IBM, you know, we have a lot of products based on natural language processing capability, like once in L, L, U, once in discovery, and so on. So behind this technology, I'm very proud to say a lot of the products are part of our technology developed by my team, so really empower the product themselves.

03:49.880 --> 04:09.880
Meanwhile, you know, when we have since in the lab, they may not always mature enough to put in a product yet. So then we also often engage with customers and also a technical system to say, OK, for this particular problem, we don't have a product yet. What can we do for the customer?

04:09.880 --> 04:16.880
And that really helped me to understand what are the key challenges that we need to address from research point of view.

04:16.880 --> 04:24.880
I think that in my mind, you know, as someone working in industry research lab for over 10 years, right? I really enjoyed that.

04:24.880 --> 04:35.880
Because when I was in school, how do you always need to think about what's the motivation behind the work here in industry research? I don't have that problem. The problem come to me.

04:35.880 --> 04:53.880
And then the interesting part is, how do I generalize one person's specific problem into a research problem that can be solved in I stage the fashion, like both very ambitious from scientific point of view, but at the same time with the nervousness.

04:53.880 --> 05:13.880
Yeah, well, I want to dig into some of the specific challenges that you've identified in the NLP domain, but projecting from individual customers challenge to kind of a broader, interesting research problem is one that I'm also interested in.

05:13.880 --> 05:17.880
Can we maybe start start there and I'd love to get your take on that.

05:17.880 --> 05:30.880
So maybe I can share a story, right? When I joined IBM, I work on a project called a system T. So system T is a declarative system for natural language processing.

05:30.880 --> 05:41.880
And I have to see to a certain extent, I'm still working on the system. The reason is that when we have a larger research agenda, we cannot solve the problem at once.

05:41.880 --> 05:49.880
So for example, how system did come into being that because at the beginning, we want to build a better enterprise search system.

05:49.880 --> 06:01.880
So when we trying to support enterprise search internally within IBM, we found the traditional Ion system do not understand the document enough.

06:01.880 --> 06:14.880
But once in that it differentiate internet search and the internet search is in internet search, when you try to find the answer to a question, maybe it's owning a one page that has the answer.

06:14.880 --> 06:20.880
Unlike when you do a Google search on the internet, when you search for certain question, right, you may have hundreds of pages.

06:20.880 --> 06:37.880
So then internet is really important to understand every single document. And that really motivated the team to build a natural language processing system. And then at that point, what we found is there are a lot of problems in creating natural language understanding system.

06:37.880 --> 06:54.880
But the most basic one is really, for example, the two information instruction identify important entities, identify important events and so on. But at that point, we don't have a very scalable system to perform the task.

06:54.880 --> 07:18.880
So then we start building the system T where we kind of abstract out what are the common text analytics operations that we need to capture. Then we build a system at the beginning, our concern is about expressivity and round hand performance to be able to express the kind of task that are important for supporting enterprise search.

07:18.880 --> 07:31.880
Then as we go, we found out, okay, we can build a system that is really good to building extractors for supporting enterprise search. But then we want to enable more people to use them, right.

07:31.880 --> 07:41.880
Then we kind of say, okay, now we have a problem of how do we build a better tuning into it? How do we incorporate more advanced machine learning capability into it?

07:41.880 --> 07:52.880
So kind of at a different period of time, we have different focus, but overall because we have this one bigger project, we can build one thing on top of the other.

07:52.880 --> 08:03.880
So I think that's really the advantage of working in our industry lab where our research is not bounded by a fixed period of time for grant, right.

08:03.880 --> 08:14.880
But it is kind of generalize the overall life cycle, like we have a concrete business problem, for example, we started of how do we support a better enterprise search, right.

08:14.880 --> 08:22.880
Then we turned that into how do we build a system to support a class of such problems.

08:22.880 --> 08:33.880
And then we saw some of them, we build some prototype, we evaluate with concrete use cases, and then we get a feedback.

08:33.880 --> 08:44.880
When we can solve certain kind of concrete problems, we can push it out into the wild, and then we get additional feedback from the wild.

08:44.880 --> 08:54.880
Then we incorporate to say, okay, now we have the system should we focus on improving the system itself, or should we focus on the tuning, should we focus on other things.

08:54.880 --> 09:00.880
I think a lot of the work from the team really kind of come with the same life cycle.

09:00.880 --> 09:17.880
Maybe jump into some of the challenges that we referenced earlier, you know, when you are thinking about and trying to help teams productize NLP in the enterprise.

09:17.880 --> 09:27.880
What are some of the challenges that you run into and kind of how do you think about those, those challenges, how have you organized those in your head.

09:27.880 --> 09:34.880
Yeah, so that's a very good question. I will say we can think of the four category of major challenges.

09:34.880 --> 09:42.880
The one is complexity, complexity in terms of the, we can talk about not long processing, right.

09:42.880 --> 09:50.880
In terms of the complexity of the document, we have to deal with, and the complexity of the tasks themselves, we have to deal with.

09:50.880 --> 09:58.880
The second category is small data. So again, we want to talk about enterprise-facing challenges, right.

09:58.880 --> 10:09.880
We often have two new things that works for everybody, but we don't necessarily have data that are representing everyone's problems, right.

10:09.880 --> 10:27.880
So therefore, we need to be able to address the small data problem either based on what we have internally or have some way to enable people to provide data for us to help themselves.

10:27.880 --> 10:43.880
The third part is customization. No matter what we build, right, how well we build it, the other vast capability may not be sufficient for a customer specific use case or works as well as it could be on their own data.

10:43.880 --> 10:57.880
Then how do we enable the customers to quickly customize what we build to fit into their use case, their data very quickly, and it usually do not necessarily require someone like me, right.

10:57.880 --> 11:06.880
Someone with a big degree to work with them. So instead, maybe someone with a business knowledge to be able to do this work on their own.

11:06.880 --> 11:14.880
And then the fourth part of explainability. I know, you know, everybody talk about trusted AI and so on these days, right.

11:14.880 --> 11:25.880
One big part of trusted AI is explainability. Then here we need to understand what kind of explainability makes sense to what kind of audience, right.

11:25.880 --> 11:38.880
And how do we provide variety of explainability to help people. So I would say, yeah, those are the four major challenges we are focused on from enterprise, not along with process importantly.

11:38.880 --> 11:46.880
When you think about those challenges, do you think about them independently, meaning does.

11:46.880 --> 11:56.880
Do you have a set of solutions or a set of research directions to try to address complexity and a separate set to try to address small data, et cetera, et cetera.

11:56.880 --> 12:07.880
Or, you know, is there a way that you kind of unify these in the solutions that you're trying to apply to these various problems.

12:07.880 --> 12:16.880
Yeah, I will say we have a few common approach, a combination of them to solve all the problems, maybe with different combinations.

12:16.880 --> 12:24.880
I think the four things we often use as a phone number one is data augmentation, right.

12:24.880 --> 12:39.880
The second is have a very powerful declarative language that captures some of the main primitives that are important for conduct the task.

12:39.880 --> 12:57.880
And the third one is neural symbolic AI really leverage the best of both word in neural network, as well as the symbolic system. And the fourth one is human look, how do we involve human beyond just provide labor data.

12:57.880 --> 13:10.880
So those are the four things that the basic secret formula that we have been following and then for 12 different challenges, we may use a combination of two of them or three of them.

13:10.880 --> 13:25.880
Got it. Got it. I want to go through those in turn, but maybe to set some context, it'd be helpful to have you talk about, you know, some specific challenges and where each of those come comes in.

13:25.880 --> 13:33.880
Sure, I think maybe I can describe a particular system we have recently built and I think the system kind of representative for all the challenges.

13:33.880 --> 13:43.880
So what we have been working on with the Washington Discovery team was to build something called Washington Discovery content intelligence.

13:43.880 --> 14:01.880
What this system does is to enable lawyers and not professionals to quickly review contract documents. If you think about contract, right, it's probably one of the most important business documents in the business world.

14:01.880 --> 14:16.880
It's very expensive to review those contracts because you need to have legal professionals and the contract to often have many, many pages and contain a lot of information that is challenging even for you and me to understand, right.

14:16.880 --> 14:33.880
So to build such a system, what is required? First of all, the first step is a lot of contract in PDF format. So we need to convert the PDF into a form of a consumable by the machine, right.

14:33.880 --> 14:48.880
Then secondly, we need to build models to simulate what the no professionals would do so that can help them, right. So what contract understanding involved on hardware is a form.

14:48.880 --> 15:04.880
Then every single document identify the clause for simplicity content of sentences for every single sentence identify what are the categories of that particular sentence and what are the parties involved in the sentence.

15:04.880 --> 15:18.880
If I say the buyers share pay supplier one million dollars, right. So in legal terms, this is an obligation and the party involved is a buyer and supply, right.

15:18.880 --> 15:42.880
And then there is some amount. Then we turn this into a sentence, first of all, identify the sentence, right. Identify sentence from the document and then do classification so that we can categorize the sentence and also do extraction to extract what are the parties involved like supplier and buyer.

15:42.880 --> 15:55.880
So now, then once we have the system build, it will not work perfectly, right. So we need to surface it to a legal professional so the legal professional can say, okay.

15:55.880 --> 16:05.880
Now I want to review all the clause related to obligation and then look at those sentences and to be able to say, okay, this is indeed an obligation.

16:05.880 --> 16:16.880
I want to do something like market as high risk or low risk or no problem. But think of the challenge we just mentioned, like complexity.

16:16.880 --> 16:45.880
First of all, the complexity involved in the document itself, the document is the multi-page PDF document that we need to preserve all the structure, all the information and also be able to identify, like assuming the document conversion is correct, we also need to identify the document structure and to be able to identify individual sentences and the context, right.

16:45.880 --> 16:57.880
Then why is that small data problem because we come to contact nobody going to share the contact with IBM for us to create training data, right.

16:57.880 --> 17:07.880
If we want to view that model, we have to rely on IBM contact data, but at the same time what we build need to work for other people, right.

17:07.880 --> 17:25.880
Then why is that customization problem because even though we're talking about the legal professionals follow some general taxonomy related to the classification, every single company they may have some difference.

17:25.880 --> 17:40.880
For example, if I sentence mentions patent, right, or a trademark, it can be viewed as a clause related to intellectual property, but some company want to separate them.

17:40.880 --> 17:52.880
They want to say, okay, I only identify all the clause related to trademark as trademark, but everything else related intellectual property is intellectual property.

17:52.880 --> 18:15.880
And then finally explain infinity, when we surface the results to the legal professionals, we also need to, especially when there's a mistake from the prediction, we need to explain to the legal professional, like why we predict this sentence as something, right.

18:15.880 --> 18:36.880
Like this is obligation rather than something else, we need to explain that. And also what that will help us is when they say this is not correct, we can also look into potentially how to fix them, and they can explain to us, okay, why certain sense is not labeled correctly, like what I just described earlier related to customization, right.

18:36.880 --> 18:57.880
When they come back to say, oh, this sentence should be labeled as trademark, instead of intellectual property, they can explain to us to say, okay, for this particular thing, you identify this clause related to trademark, but in our company, this is regarded as trademark rather intellectual property.

18:57.880 --> 19:14.880
So they also have the model developers to make some changes. Yeah, so I hope this example kind of initiated all the different thread, but I will also be happy to talk about, you know, how do we solve them.

19:14.880 --> 19:41.880
Yeah, yeah, well, we'll definitely do that one question that does come to mind. Yeah, I've talked in the past with folks that are, you know, working on these production, NLP systems, and over time, the need to supplement the learn models with rule based systems and heuristics has decreased.

19:41.880 --> 19:59.880
And I'm curious in the cases that you deal with, you know, in addition to the data augmentation declarative neuro symbolic human and a loop, you know, do you also need to, you know, are you still worrying about, you know, rules and exceptions and all of these things as well.

19:59.880 --> 20:02.880
And do you see those decreasing over time?

20:02.880 --> 20:15.880
And depending on the use case, so for example, for some use case, when full explainability is important, we actually see requirement to combine both.

20:15.880 --> 20:29.880
So let me give you an example, right. So if we think about model development, it's a life cycle, right, we have data gathering, we have the developing the model self, we have past and validation.

20:29.880 --> 20:34.880
For some industry, for example, for retail banking.

20:34.880 --> 20:51.880
The fact, you know, the training test and the validation, what we learned is if you use a block box model, right, like a neural approach, assuming you have sufficient data, the training is very fast.

20:51.880 --> 20:56.880
Test also will not take too much time, but the validation will take a lot of time.

20:56.880 --> 21:12.880
So what they do in validation really ensures the model work as predicted under different variety of conditions and also be able to explain why certain predictions produce the particular results versus what is expected.

21:12.880 --> 21:16.880
Right. So this takes a lot of time.

21:16.880 --> 21:26.880
But if we use rule based approach, right, it's kind of opposite. So the developing of this rules take a lot of time.

21:26.880 --> 21:35.880
Testing also takes some time, validation doesn't take too much time. So overall, you cannot see the time and effort distributed in a different way.

21:35.880 --> 21:46.880
So what I might seem has been working on is actually kind of get the best of both work. So basically, what we do is we take a neural network.

21:46.880 --> 22:00.880
We leverage all the advantage of neural network to be able to learn from larger amount of data, right, but then we produce several rules that are completely transparent and explainable.

22:00.880 --> 22:20.880
So the advantage of this and then I don't my expert can actually come in to do further inspection and augmentation to ensure the model not just learn from the data, the model also learn from the domain experts, the corresponding domain knowledge, right.

22:20.880 --> 22:31.880
So the benefit of this approach is the training will not take as much time as the purely rule based system, right.

22:31.880 --> 22:46.880
It takes similar time as that whatever approach we are taking for black box model testing is also similar, but the validation takes significant and shorter amount of time than if you take a black box model.

22:46.880 --> 22:55.880
Depending on the use case, right. In some use cases, you really don't need to have full explainability. Then I think we are using neural model.

22:55.880 --> 23:01.880
Probably is a way to go, but in the use case is where explainability is important.

23:01.880 --> 23:19.880
Because it's important to hold somewhere accountable when the system make mistake, right. So then this approach works really well. Another aspect is remember what we talk about about small data problem, right.

23:19.880 --> 23:31.880
Actually, a lot of the area we don't have sufficient data. So when you learn from the data, it will not capture the whole domain knowledge, right.

23:31.880 --> 23:41.880
So I think what our experience, the domain experts really like the fact that they can not just provide input by labeling.

23:41.880 --> 23:51.880
They can also provide input by inspecting what learned from the model and make changes not just to understand how model performed, but able to make changes.

23:51.880 --> 24:00.880
For example, sometimes like what I just described earlier, right, to say, okay, share, pay, supply, I mean $1.

24:00.880 --> 24:13.880
Like a potential rule that we can learn is when there is a predicate indicating like a verb indicating purchase indicating some business transaction.

24:13.880 --> 24:30.880
And then there is a model, there is a modality of necessity modified on a particular verb, then it's potentially a indication of obligation. This is something completely transparent, right. We can learn such a rule.

24:30.880 --> 24:55.880
But what are considered as important business transaction verb, this can be given by the domain experts, not just by learning from the data, right. We can also let the domain expert to inspect the needs of dictionaries, renowned, miso rules, renowned so that we can ensure their knowledge is captured by a model.

24:55.880 --> 25:10.880
And then we can combine, right. So this model can be then combined with a block box model so that we can really ensure, you know, the overall performance is as good as possible.

25:10.880 --> 25:31.880
But at the same time, we're able to predict or explain significant portion of the prediction and how the two models are combined together, depending on use case, like for example, if you want to say, I want to have as good performance as possible, then if I can explain some of the results, that's good.

25:31.880 --> 25:44.880
Then we can combine it in one way or you want to say explainability is most important to me, right. Then with that, I also want how good performance in you can combine this to in a different way.

25:44.880 --> 25:59.880
So I think with this particular approach, we really can get a best of both words, you know, be able to leverage the fact neural network is very powerful to be able to really capture a model nuances from the data.

25:59.880 --> 26:10.880
That is very hard to express in rules, right. But at the same time, capture as much as possible, the domain knowledge, that can be clearly articulated through rules.

26:10.880 --> 26:22.880
Another thing I want to mention is the rules are not based on syntactical pattern, right. The rules are based on abstraction, we obtain through natural language understanding.

26:22.880 --> 26:34.880
For example, given the sentence, fair share pay supplier through natural language understanding, shadow semantic parsing, we will know there is an action of pay.

26:34.880 --> 26:54.880
And then the performer of the action is the buyer and the target of the action is the supplier and then the manner is pay with, you know, a million dollars. So this kind of semantics is captured through advanced natural language understanding.

26:54.880 --> 27:06.880
We can express rules in a very concise but very powerful way. I think that's another thing people need to think about. Rules are not necessarily one token for another, right.

27:06.880 --> 27:28.880
But if we build a rules on top of semantic abstraction, they can be much more powerful. And then behind the thing is also leverage the deep learning, right.

27:28.880 --> 27:36.880
So behind is in leverage in neural network model to do not long understanding.

27:36.880 --> 27:52.880
Got it kind of. And now just to make sure that one, the same page is what you describe, we kind of came at it from the perspective of rules, but is this the neural symbolic AI that you spoke about earlier.

27:52.880 --> 28:06.880
Correct. Correct. Yeah. So I think neural semantic, you can basically think of that two approaches, right. One approach is you embed symbolic information or expression into neural model, right.

28:06.880 --> 28:15.880
That helps people can ingest some domain knowledge into neural model. But then the neural model coming out still not completely transparent, right.

28:15.880 --> 28:28.880
The other way you can do this is you can use neural model to produce symbolic model or symbolic abstraction, then we can build a symbolic model on top of it.

28:28.880 --> 28:44.880
So right now, imagine we actually have both approach, but when I'm describing more of a ladder, where we leverage neural model to empower symbolic model because the symbolic model has a lot of advantages that people is fully aware, right.

28:44.880 --> 28:53.880
Explanability and the, and the part where, you know, it's easier for people in the art ways and understand.

28:53.880 --> 28:58.880
Yeah. So this is one particular neural semantic approach we have been taking.

28:58.880 --> 29:13.880
And then one of the other techniques that you mentioned was the use of declarative languages, imagining that, you know, you've used this neural model to create

29:13.880 --> 29:26.880
kind of this abstract rule, let's say about a particular scenario, you want to make that accessible to all the humans in the loop that we also talked about.

29:26.880 --> 29:31.880
A way to do that is to kind of represent this abstraction using some kind of declarative language.

29:31.880 --> 29:45.880
Exactly. So I think, yeah, I think that's a very, very good point, right. So you can express rules in grammar, but the challenges that grammar also detect how the rules will be executive, right.

29:45.880 --> 29:52.880
Then that makes the real creation much more challenging and less expressive and also how runtime performance issue.

29:52.880 --> 30:06.880
So we can overcome that by leveraging the declarative system we have mentioned earlier, right. Like the system I have been working with since I joined IBM is we can separate execution from the actual semantics.

30:06.880 --> 30:13.880
So you can specify the semantics using declarative language, but how it will be executed.

30:13.880 --> 30:26.880
It determined by the optimizer, the optimizer would determine what's most efficient way to execute. So for example, if my rule is to identify all the sentence,

30:26.880 --> 30:52.880
this business transaction and it's an include the modality indicating necessity, I can do it in two different way, right. One is I can identify all the sentences with necessity and then see whether there is the business transaction next to it, or I can do the other round, which was more efficient or decided by the optimizer.

30:52.880 --> 31:10.880
When I created a rule, whether manually or automatically, I don't need to worry about execution. The execution will be done automatically by the declarative system to ensure we get the most optimized plan in terms of execution.

31:10.880 --> 31:30.880
And when we talk about execution in this context, what do we mean we typically think of, you know, neural network, inference or, you know, even more broadly ML model inference as kind of this one, you know, you make a request you get a prediction back.

31:30.880 --> 31:40.880
What do you think about execution more from the perspective of like information retrieval and queries and that kind of thing.

31:40.880 --> 32:08.880
So here, when we have a rule based, like based on this kind of model, right. So for example, for simplicity, necessity, we do two part. One part is just to not to language understanding. The second part is just enforce the predicates, right. So the execution composed of a few different options. One option is that you perform this not to language understanding operations.

32:08.880 --> 32:24.880
The neural model we have right on every single sentence. Then for every single sentence, you also check whether it contains any business transaction mentions and also some words indicating necessity.

32:24.880 --> 32:44.880
That's a long way of execution. Another way of execution is that I can actually do some optimization. Instead of looking at every single sentence, I can first retrieve only the sentence contains the business transaction. Then I perform not to language understanding, then I check my predicates.

32:44.880 --> 33:06.880
So it's really how we produce a final result. So that's the executive time talking about it. Got it, got it. So to maybe kind of recap this and put it back in the context of this original problem where you're trying to enable legal discovery across lots of contracts.

33:06.880 --> 33:24.880
One approach one my take is to use traditional deep neural networks and maybe a supervised learning kind of thing or even unsupervised and kind of clustering the different clauses or sentences.

33:24.880 --> 33:44.880
But you know what you've found is that by combining by using the neural network instead of to classify, but rather to kind of do something akin to like entity extraction or or to or semantic parsing.

33:44.880 --> 34:04.880
And you can use that to generate this essentially a rule set that's kind of expressed in this declarative language. And then you've got a system that executes those rules against the given contract and uses that to essentially make sense of it.

34:04.880 --> 34:27.880
So you can identify whatever it is that you might want to identify it, you know, at a given time. Right. Yeah, on the high level, you can view it that way. But we can also do combination right. So you can basically think of I can either build a block box classifier or I can build a white box or I combine the block box and a white box together so that I can get it the best of both words.

34:27.880 --> 34:43.880
But in reality, we actually do both. So I think the challenge of doing this block box one another another challenge is especially when we start with building the product, we don't have even a lot of labor data.

34:43.880 --> 34:58.880
And also, as I mentioned, right, we have this small data problem. We have data from IBM, but we don't have data from other company. How do I ensure the model I build works well. So here, the other domain performance is very important. Right.

34:58.880 --> 35:17.880
So what we actually started with one of the different approach, right. You can think about in IBM research, we also build a lot of different neural networks. Right. That's really a thing that accessible to other people. We also have it. But we really don't side by side. What we found is that the

35:17.880 --> 35:37.880
domain performance for the deep neural network works really, really well. But then it drops very significantly when we do auto domain. So that really passionately motivated us to build more of this transparent model so that the auto domain performance is very similar to

35:37.880 --> 35:53.880
the intonement, then we can augment this particular model if we want to have a better performance to say if we have additional training that we can also kind of augment this with a block box model. But at the same time, we can benefit from the

35:53.880 --> 36:10.880
identity, transferability, and also be able to really quickly start with a particular customers use case without worry about training data. Because I think a lot of times, I mean, I'm deeply involved in not

36:10.880 --> 36:32.880
processing research, right. So if you if you look at the papers, people often are shown we already have labeled that are very nicely available. Right. Then if you don't have a label that you can just do crowdsourcing. But in the case of enterprise applications, like I cannot even label this data, right.

36:32.880 --> 36:49.880
The contact information is really, really rich. So I remember at the beginning, when we build this product, we look at all the sentences and like true sentence looks almost identical. We go to ask the lawyer to say, okay, why? Why those true sentence looks

36:49.880 --> 37:04.880
identical label differently. The lawyer tells us, oh, because one is within this section with this information, but the other do not. Right. Again, like, how do you even captured like, let's see, you have this information.

37:04.880 --> 37:17.880
I can quickly basically code that rule to say, okay, take this section information into account. Right. I can do that in a few minutes. But think about how do I return my model to do that? Like, now I have to.

37:17.880 --> 37:33.880
Yeah. So I think in general, I would say depending on the use case, right. There is the in my opinion, there is no one solution. We need to really use the combination of all these technology techniques available in your

37:33.880 --> 37:52.880
toolkit and take the practical challenges and also what's the final goal in mind. Right. Like, for example, in our case, our goal is not to say, build the best possible model for IBM contract. If that's my goal, I don't need to have this approach. Right.

37:52.880 --> 38:09.880
I can just do this training and then have a model that works really, really well for Indomie. But our goal is to build a good model for other people and enable them to customize and not really come to the solution that I mentioned.

38:09.880 --> 38:13.880
So we want to take advantage of both.

38:13.880 --> 38:25.880
In describing that solution, you mentioned a lot of different options, you know, the white box, the black box, et cetera.

38:25.880 --> 38:45.880
How do you, I'm wondering like where you're making those decisions, is it, you know, it's not at the level of product because you want, I'm imagining a platform that kind of does, you know, they can handle multiple use cases.

38:45.880 --> 39:07.880
You know, do you have different paths that you go down for different use cases or am I thinking about this incorrectly and, you know, it's more of you have this toolkit and you have to apply this toolkit kind of more in a consulting or orientation when you're faced with a particular use case.

39:07.880 --> 39:23.880
I think that I feel different ways to approach it, right. One is like, for example, when we build the one I just described this particular offering in Washington Discovery County Intelligence, it's one, it's one offering, right.

39:23.880 --> 39:29.880
And then in that offering, we take this particular approach and then we show it's really useful.

39:29.880 --> 39:45.880
But assuming like the customer comes to me with something else, then one thing we're actually doing right now is to take more of our auto AI approach where they usually give us the constraint.

39:45.880 --> 40:01.880
We will figure out based on the constraints they give to us, what's the most, like what's the optimal combination.

40:01.880 --> 40:19.880
One of a very trendy topic. So, for example, having faced recently pushed out a new feature called the auto AIP, right. Because even with the block box, you still have a lot of challenges like which models you use, how do you choose the hyper parameter and so on.

40:19.880 --> 40:31.880
Right, so here is actually think about the main parameter we introduce is we have additional type of models and we have additional ways of ensemble them together.

40:31.880 --> 40:44.880
And how do we do those could be automated when the user specified constraints. So, for example, the user can say, I want to have best performing model with some explainability, right.

40:44.880 --> 41:12.880
Then with those constraints, we can automatically figure that out. This is something we are building in our project called auto AIP for text. So, we're building all these operators that support different kind of model and different model have different properties, right. So then based on the user constraints, we can decide which combination models we use and then which is the same.

41:12.880 --> 41:24.880
The model to use and then which model to refine further look at it, you know, refine the hyper parameter tuning and refine, how do we do the final ensemble.

41:24.880 --> 41:38.880
Got it, got it, got it. Since you mentioned the auto NLP and hugging face, I'll quickly mention to folks that I interviewed one of the folks who worked on that.

41:38.880 --> 41:57.880
I'll be check thicker and we'll drop a link to that interview in the show notes. But I want to get back to data augmentation, because we haven't really talked about that so much here. And I think it's, you know, it's.

41:57.880 --> 42:17.880
Often easier to think about how to do that in the context of computer vision, you know, add noise or change the orientation or flip or rotate or that kind of thing. What does that mean in the context of NLP and how have you included it or or incorporated it into your projects.

42:17.880 --> 42:31.880
Yeah, sure. So talking about data augmentation in NLP, actually, there is an excellent tutorial or survey paper come out very recently and summarize some of the techniques I can, I can still be a link later on.

42:31.880 --> 42:37.880
But in the context of what we're working on, we do that augmentation in a few different ways.

42:37.880 --> 42:50.880
The first one is, so I mentioned we do not learn with understanding, right? We do, you know, give you a sentence we pass into some semantic structure.

42:50.880 --> 42:59.880
So we have enough data for English to do that. However, for other languages, we don't have this nice data, right?

42:59.880 --> 43:20.880
We don't have labor that are to perform the same task, but we want to also support other languages. So one technique we have been using in terms of data augmentation is we can leverage the fact there's the bi text we have sentencing one language like English and sentencing another language that is translation of the English sentence.

43:20.880 --> 43:33.880
We can do sentence alignment, right, align each token with each other and then we can project the annotation will produce an English onto other language.

43:33.880 --> 43:41.880
So now we have some very noisy label that are to start with, right? Because you can think about the always lost in translation, right?

43:41.880 --> 43:54.880
So when you do this projection problem is going to be some mistakes. So we also do additional filtering to ensure that the data will produce as good as possible.

43:54.880 --> 44:14.880
And then we can use this data to train a semantic posture. I challenge a man posture in the other language to further augment the data, kind of repair the data. So maybe after projection, for example, in my sentence, I only have some portion with annotation, the other portion with do not.

44:14.880 --> 44:27.880
Then we can do this with strapping to other additional ones. So this way we can automatically at scale produce larger amount of data. So this is like one way to do that augmentation.

44:27.880 --> 44:43.880
Another way you can also do it is, so for example, while my colleagues they have been working on dependency parsing for other languages, right? So again, we may not have sufficient data for other languages.

44:43.880 --> 44:54.880
So what they have done is they translate the English sentence of other language into English sentence and then back and forth.

44:54.880 --> 45:14.880
So this way you can also produce larger amount of what we call silver data. It's not good data because it's still contain good amount of noise. But when you train from this data, the neural model is able to distill the noise and to be able to learn a pretty reasonable model.

45:14.880 --> 45:25.880
So a lot of the data augmentation in natural language, kind of doing this synthetic data, all annotation projection, annotation transfer.

45:25.880 --> 45:38.880
There are also ways to generate the data, like for example, if you have this kind of a recent paper we published in ACL earlier this year.

45:38.880 --> 45:50.880
So one of the parts that are challenging for parser is handling questions because most of the training data for natural language processing are statement, right?

45:50.880 --> 46:05.880
News report or something. But often we have data load that actually are questions. So what we find is those the parser trend using the typical data set do not work well very well for questions.

46:05.880 --> 46:13.880
So then what we did, but then labor question is very expensive, right? Producing labor data for question, right?

46:13.880 --> 46:22.880
So what we did is there is a data set called a question bank is are some maybe a couple of thousand the sentence and so on.

46:22.880 --> 46:45.880
So we basically generalize from those questions from from the question bank to generalize into template. So with the template, we can generally generate a lot of labor data for question and then we can add those generated question back into the training data so that the parser can handle question very well.

46:45.880 --> 47:01.880
So that is actually to great extent similar to what people do in computer vision right is just how you manipulate the data is different because well you don't deal with tax care instead of vision right.

47:01.880 --> 47:17.880
Nice, nice, we haven't talked much about human in the loop. I think a lot of folks are familiar with some of the common uses of human in human in the loop.

47:17.880 --> 47:37.880
You know, of course, exception handling is one that comes to mind. Is that the kind of thing you are referring to when you mention it? That's a very good question because I can have very strong opinion about that.

47:37.880 --> 47:53.880
Human in the loop is actually very broad and I can describe a few different aspects in my opinion human can be very important as part of the model development process.

47:53.880 --> 48:22.880
We think about human in loop is more about labeling right. How do we involve people to provide labor data? That's very important. In fact, we have spent a few years trying to figure out how do we enable average person to be able to provide high quantity data for shallow semantic parsing because our initial way of doing this shows that the human cannot perform as good as our machine learning model.

48:22.880 --> 48:34.880
Then we need to come up with a more intelligent flow to enable average people to do as much as possible and then enable experts to do the rest.

48:34.880 --> 48:55.880
So I think that's one big part when we talk about a human loop is how do we enable human to generate high quality data but not necessarily all be done by crowdsource worker because not every task is possible. But then at the same time, we don't necessarily always fall back to the expert in the loop.

48:55.880 --> 49:13.880
So we need to kind of have crowd in the loop but at the same time involve experts or automatic method as much as possible. Then one thing people don't really talk too much related to human in loop is the model development.

49:13.880 --> 49:30.880
The reason is very simple. Most of the time when people talk about model development, you are talking about block box model. But when we talk about white box model or gray box model, we want to involve humans in loop in a few different ways.

49:30.880 --> 49:46.880
One is what I described as before. When you define your network, learn a transparent model. Now human can inspect the model, augment the model, modify the model. That's one big part of human and loop.

49:46.880 --> 50:04.880
The second part is maybe the user do not need to give us a lot of data at the beginning. Instead, we can have the model creation that are gathering in one intelligent simplified mode.

50:04.880 --> 50:19.880
For example, when I talk about classification of this particular cross. Instead, the user give me a few thousand labels that are a trim model and then we write what we can do.

50:19.880 --> 50:35.880
The user may not even specify a path up from the user to say, hey, this sentence is interesting. I will label a few interesting sentences as an example. Then that is the machine figured out what are other similar sentences.

50:35.880 --> 50:53.880
Again, the machine may not fully understand what I mean by similar. So the machine will propose some potential interpretation and ask the user to provide feedback. Then as the user provide feedback, the machine can learn a bit more and refine what the machine has learned.

50:53.880 --> 51:11.880
In the end, it will produce a model, but it's a very collaborative process. In between, the user may go beyond just give me an example. The user may even give me a rationale to say, hey, why I think this sentence is interesting or why I think what you propose is wrong.

51:11.880 --> 51:25.880
Then it's very interesting to incorporate this kind of what we call reach feedback into the machine learning process. Again, this is like a model development process.

51:25.880 --> 51:48.880
Another approach is, as I mentioned before, we have this declarative system. The declarative system is actually very good for tasks that have very obvious patterns. So rather than trying to let the black box model figure it out, we can specify that.

51:48.880 --> 52:11.880
But specify that can be expensive. So we can also have this process of user giving example. I can explain to the user in my declarative language what I have learned and then the user can give me either additional feedback or modify that particular declarative statement directly.

52:11.880 --> 52:29.880
That's one big part of human loop and model development. Finally, about user feedback. How do we enable the user to provide additional input to the system when the system already deployed?

52:29.880 --> 52:41.880
Again, that's also an important part. We're thinking about today in a lot of systems. If the user is not happy, you cannot really do anything.

52:41.880 --> 53:02.880
So the system produces whatever it produces, that's it. But you can imagine a system where, for example, the contract intelligence system I mentioned before. When the user sees outputs, they use the not company happy with it. The user can explain why she's not happy with the results.

53:02.880 --> 53:16.880
And then can also maybe provide some additional input to say, okay, here, let's say, for example, in my company, we were differentiated pretty much and IP.

53:16.880 --> 53:27.880
So we can learn from those and that can come back to our development team to be able to say, okay, is there something we need to address in our baseline model?

53:27.880 --> 53:41.880
Is there something we need to address by enabling the user to be able to do some customization? So that allows to do much more intelligent model improvement and maintenance.

53:41.880 --> 54:08.880
It sounds like the elements of this that you feel most strongly about are kind of a combination of closing the loop. So you put the human in a loop, but then close the loop so that it improves your model, but also an idea of like intelligent human in a loop use machine intelligence to optimize the way the human intelligence is being used in the loop itself.

54:08.880 --> 54:23.880
Exactly. I think we need to give human more agency. We need to empower human rather than just, you know, sometimes if you think about it, if all the user can do it, give you labor data and then leave everything to the machine.

54:23.880 --> 54:32.880
The human is almost powerless. You cannot do anything. You know the machine is not cracked, but other domain experts, you don't know how to modify the model.

54:32.880 --> 54:42.880
You don't know how to do, right? You're kind of waiting waiting waiting. Hopefully someone will incorporate the data or the knowledge properly.

54:42.880 --> 54:56.880
But when we actually give human agency, I think they have, you can do much more. So for example, actually in recent, in 2021, we can see a very interesting trend.

54:56.880 --> 55:12.880
So in 2021, we see three workshops from top ARP conference that are related to human and like human and ARP. So we have a workshop interactive machine learning.

55:12.880 --> 55:31.880
We have a workshop and ARP and HCI and we have a workshop and data science with human and loop language advancement. And next year in 2022, we're also going to have a special theme in NACO that is focused on HCI for ARP.

55:31.880 --> 55:48.880
So basically more and more, like when ARP become more and more used in practice, human become an important aspect. When it's just in the lab, you know, all you need to do is produce some numbers and then compare with benchmark data.

55:48.880 --> 56:09.880
You don't need to consider human, right? But now when the ARP actually impacting business, when it's actually used by people, you know, how do we use people to help with the entire development lifestyle or including evaluation and so on become one more important and you can see the trend.

56:09.880 --> 56:25.880
Well, you know, thanks so much for taking the time to share a bit about what you're up to and walk us through these four different tools that you've had some success within delivering enterprise and LP.

56:25.880 --> 56:27.880
Thank you so much for having me here.

56:27.880 --> 56:39.880
Thank you.

