All right, everyone. Welcome to another episode of the Tumel AI podcast. I am, of course,
your host Sam Charrington. And today I'm joined by Al Kay Wickers. Al Kay is a research scientist
at Qualcomm. Before we get going, be sure to take a moment to hit the subscribe button wherever
you're listening to today's show. Al Kay, thanks for joining me and welcome to the podcast.
Today we'll be digging into some of the work you and your colleagues are presenting at ICLR.
And especially your work on transformer-based transform coding. But before we do that,
I'd love to have you share a little bit about your background and how you came to work in
machine learning. Thanks a lot, Sam. Thanks for having me. So I started out in machine learning,
I think, around the time that I studied at UVA. So there was an AI course at UVA,
which I joined in 2012. And a couple of co-students, who were a year higher than me,
were quite business-oriented. And already during their masters, they were founding small companies.
And eventually they guided me into their startup, which was called Cypher. And I believe you've
actually interviewed some of the original founders. So that's Max Welling, who's a professor
currently at Microsoft. And some of the other original founders are Stella Qualcomm, Taco Cohen,
who has worked a lot on a group of variant networks and time in Blancofort. And so they kind of
talked me into joining their company. And I guess the rest is history, because this startup was
acquired after a couple of years by Qualcomm. And I've been at Qualcomm ever since, for the past four
and a half years or so, having worked on concepts like reinforcement learning, for autonomous
driving. And in the past two and a half to three years, mostly on neural data compression,
using generative models. Well, let's dig into that topic. Talk a bit broadly about your research into
the compression side of things, and we'll dig deeper. So I think neural data compression is
relatively new. I think that the seminal works came out around 2016, 2017. But it's quite a
beautiful application of generative modeling. Because most likelihood models, in the end, what they
give you is exactly what the name advertises, a likelihood for a data point. And information theory
tells us that you can then compress that data point with a certain number of bits. And what these
generative models can be used for, and that's how we were applying them, is to estimate likelihood
of incoming data points. So you could imagine that's images or audio or video. And using entry
coding, then squeeze out any redundancy, where this likelihood model tells you exactly how many
bits you're going to need in order to compress that data. So what we've done in the past couple of
years is mainly do this, the trained big generative models in sort of likelihood modeling tasks.
But of course, with the end goal that will eventually use these in practical setting.
So whereas in typical generative models, you could imagine that big hierarchical VIEs,
they train in a fully continuous way in the latent space is continuous. So a big difference in
neural data compression is that eventually you have to quantize this. You have to move to integer
representations instead of floating points in order to compress in a lossless way. So a large part
of the work we've done in the last three years is training these big generative models with a
quantized latent space. Whereas the general field of generative modeling, I would say is mostly
focused on continuous latent space models. Often when we're talking about quantization, it's
the focus of the conversation is around efficiency and performance. In this case, we're talking
about just the fundamental nature of how you're trying to use the output of the networks for
communication. The interplay between the quantization from an efficiency perspective and
the way you're using it in compression. Yeah, thanks for pointing it out. It's actually a good
distinction. So in our case, the only thing we need to quantize in theory as long as we don't
deploy these networks is the latent space. And the reason that we compress this is that this latent
variable is actually the compressed representation of the data. So as soon as you want to transmit this
to let's say different hardware, the only way you can do this in a truly lossless way is if this
compressed representation is quantized because of things like floating point precision, for example.
Now, the quantization and efficiency angle comes into play when you think about deploying these
generative codecs. So of course, when we move to device, we would likely want these models to run
in fixed point instead of in floating point. But most of the research that we've done so far,
leading up to academic publications, for example, has been on floating point models,
where the latent space is quantized. And then as soon as we go to prototypes where we deploy these
to the device, that's when we'll quantize the entire model. So the way it's activations and so on.
And that's when efficiency becomes most important. So we've been talking broadly about neural compression
thus far and you're working that area, but you've got a particular interest on the video side.
Can you talk a little bit about how the work you've done extends to video?
So video compression poses a couple of challenges that image compression, for example, does not.
And that's mainly about the subjective experience. So as a human observer,
you would want your video to look consistent over time and you would want the motions to be consistent
and so on. And also about exploiting more redundancies. So when you work on image compression,
if you want to exploit redundancies, what you'll mainly watch out for are things that are similar
across the image, you know, patterns that appear in different parts of it. Whereas for video,
there's also the temporal redundancy. Most of the time when you look at subsequent frames,
they will actually be very close to each other, especially for background of a video, for example.
And you can exploit this by making sure that any previously transmitted information,
you don't encode it again, so to speak. The other part, so this subjective quality part,
makes it a more challenging task and I think more interesting, therefore, than image compression.
As it's very noticeable if you have a codec that produces frames that are inconsistent over time.
Many years, you're just watching it and it's jerky and just visually unsettling.
Yeah, exactly. Or, you know, speckle noise and those kind of things. So a lot of our research
is aimed at exploiting this temporal redundancy apart from concepts that we borrow from image
compression, of course. And more recently, if you've also started looking more into how to
improve the perceptual quality when you're using this video codec. So that could go as far as
as GAN-based compression. So when you're really hallucinating parts of the reconstructions that are not
truly in the bitstream, but also things like region of interest-based coding. So for example,
now that you and I are talking, the background is actually not the most important part of the video.
You will want the face to be accurate and maybe text that appears in the video. So paying more
attention to that and spending more bits on that is another way to improve perceptual quality.
What's the relationship between the research you're doing on the neural compression side and the
historical research that's that's done into compression pre-neural networks, right? We
you know, figured out a lot of these same things, right? The background is not moving. We don't have
to spend as much bandwidth on that. Do you are you pulling kind of insights from the evolution
of that prior work or is the neural setting so different that you're coming up with new tricks
all the time? I wish the latter were always true. Of course, we're boring heavily from domain
expertise that's been that's been created over many years, maybe 40, 50 years even. So
handcrafted codex or standard codex, they apply a lot of domain knowledge that also goes for
neural codex and what you see most often in especially video coding nowadays is that certain concepts
are being pulled in like this motion composition that you mentioned. Of course, it's logical to
use previous information, already decoded information on the receiver end. So we're boring a lot
of those concepts from traditional coding and at the same time, many of the operations used in
handcrafted codex can be replaced in a sometimes more efficient way and by efficient, I mean,
from a rate distortion point of view, not necessarily from a computational point of view,
then what the handcrafted codex do? What are the key benchmark tasks in video compression?
So there are a couple of video data sets that are commonly used for evaluation of any video codec
really and these have been established by this standards community. The trick is you can't
just use any video. You want it to be raw video as high quality as possible, no compression apply
to it and somewhat counterintuitively because this is pretty expensive to obtain and store.
What most people train on nowadays is not raw video but actually already compressed video,
maybe down sampled or augmented in some way to get rid of compression artifacts.
But there are a few common raw video data sets that are commonly used for neural video coding
benchmarking. When you're using these raw video data sets and then you want to apply the
compressions in the real world, say with mobile phone
information or camera information, do you have domain adaptation issues where
you have to adapt the work? Yeah, that's an interesting question.
And mostly because there's no big cross domain benchmark, so it's hard to really judge the
impact until you deploy. We did notice this in a few cases and we have some work addressing this
exact issue. So we noticed for example, when we pre-trained our codex on some video data set,
we test it on some benchmark data set, then we get a certain score. But now if we fine tune on
a subset of that new domain and we test it on some whole that set, we actually see quite a big
performance increase. So these neural codex, they are generalizing beyond what they're trained on
and hopefully your training data set is so diverse that you can handle many cases.
But there is room for domain adaptation. And one work of my colleague,
this is actually aimed at exactly this issue. So the solution that they come up with
is to overfit on the data to compress and then transmit the parameter update, so the model update
along with the bit stream. So what you get is a sort of customized codec for every data point,
where the starting point is some global model that hopefully generalizes to many data points.
But by altering the model in just a tiny bit, you can gain quite a bit of rate distortion performance.
And another question on the data set given that a lot of the performance improvements that
you're trying to achieve are perceptual. How what's the evaluation process? How do you evaluate
performance in that environment? I think the best thing you can always do in perceptual quality
evaluation is user studies. So actually having people, actual people look at two videos and then
judge which one of these two is better by some criteria. And we have some tools to perform
user studies. Of course, user studies are quite expensive. So what we typically use until that time
are proxy metrics. So there are metrics developed by Netflix, for example, called VMAF,
which measures perceptual quality based on some statistics that they observed in their
own testing environment. There are many perceptual metrics that we borrow from the GAN literature,
such as Frichet Inception Distance. So these are not perfect. They were actually not really
intended for this use case. So Frichet Inception Distance, for example, uses an Inception Network
pre-trained on ImageNet to extract some of its inputs. So it's not a perfect fit to video,
but it's a decent proxy to gauge image quality before you finally do that expensive user study.
So one of the big things that has been happening in this space broadly, computer vision is the
introduction of transformers. How has that impacted the work you're doing around compression?
So there are actually a few works on transformer based compression on images and one of which
and is the one that we are talking about today, of course, the work by my colleague, Janau and Taco,
on transformer based video compression. And I think the main intuition behind this work is that
we know that these confolutions that we're using may not be the perfect building block or basic
operator. And we've seen that a lot of the work improving image codecs is aimed at improving
the likelihood model. So the model that you eventually use to compress these quantized latents
by modeling the distribution. And not a lot of it was focused on changing the the transform as
it's called. So the operation that takes the data and then transforms it into this compressed
representation. And what Janau and Janau did and found out was that you can use some of these
vision transformer building blocks in order to create a better transform. And somewhat counterintuitively,
because a lot of the transformer and vision transformer models are actually computationally
very expensive, they could do so with less computation than with the convolutional models that we've
been using up until then. So when I noticed this, there was of course quite quite a nice insight.
And we've been using these vision transformer based building blocks in our transform
architecture ever since. I don't think we've gone into much detail in the podcast about
vision transformers, fit and swim and things like that. Can you provide an overview of how
the transformer architectures being applied to vision oriented tests? I think the main intuition
behind most modern vision transformers is to treat the image as a collection of patches in a
sort of similar way that language transformers may treat language as a collection of
sentences, words, tokens. So the typical pipeline is something along these lines. We have an image
to encode, let's say. And we would extract patches from that image and bet these in a certain way.
And then we just act as if these are the tokens that are being produced by language transformers as
well. And we apply these transformer blocks on the resulting vision tokens. Of course, vision
transformers were originally proposed for tasks like detection and classification,
whereas we're using them in a compression setting. So I guess a big difference between how vision
transformers were used in classification and detection and what we're doing is that eventually we
have to also generate a dense reconstruction as opposed to single classification. Can you talk a
little bit more about the research that you mentioned with Taco and his co-author? Like I said,
the key idea behind this is to use this swing transformer, which is a type of vision transformer
that is more memory efficient than the original originally proposed vision transformer.
So the vision transformer, as it was proposed, it uses global self-attention and therefore
it's memory usage scales quadratically with the size of the image. Now we most of the time are
encoding pretty large images and video. So of course, that doesn't really work well. What swing
transformers propose is to instead use local windows in which you compute local self-attention
and then aggregate these windows in a actually quite similar manner to how the early convolutional
architectures aggregate information. So you start off with very small windows in which you aggregate
almost pixel information and then at higher levels of the hierarchy you aggregate whatever came
out of those smaller windows. So many inductive biases similar to the convolutional architectures
and because this attention, which is the most expensive operation, memory-wise is only computed
locally, it scales linearly with the memory usage scales linearly with the size of the image.
So, young you now figured this is a good fit for compression as well since we often use very large
image and video and what they did was take a hyper prior architecture, which is one of the seminal
image compression architectures. I think originally proposed by Google in 2018 and replace some of
the operators, especially the convolution and transpose convolution by their swing transformer
equivalent in order to see by making a relatively simple change, can we show that these swing
transformers are better suited for extracting information than convolutions are.
Was it relatively plug-and-play or were there some tricks that had to be involved to get
it all to work? I mean, there always are. There always are. Some tuning required.
But what's nice is that the swing codec work was open source and then this is work by
Microsoft Research Asia and so starting off actually was relatively simple. But yeah, it's largely
take the hyper prior architecture and modify it, tune this well and it turns out you get a similar
compute architecture that is much more efficient from a rate distortion point of view. So much
better performance in your head. One of the key elements of the transformer-based
transform coding paper is this idea of visualizing the effective receptive fields.
Can you talk a little bit about what that means and where it comes into play?
So of course, knowing that you can obtain better performance using these transformer blocks is
not enough. We also want to know why this is so that you could potentially make use of it.
So what Young and Jeno did was visualize this effective receptive field and how you could
kind of view this is asking your network for a certain feature what inputs do I need to change
and in what way in order to maximize or minimize this feature. So you can just use the property
that is network is fully differentiable and compute a gradient with respect to the input image.
And so what they found was that for this convolutional models, if you look at their use across
different compression tasks, the effective receptive field size was largely the same. So it was
something like 30 by 30 pixels, influence a local feature. But if they look at the results,
the same results for the swing transformer-based models, they noticed that in contrast
for tasks where the model only has to look at a single image, the effective receptive field
was very small, indicating that you only need a bit of local information in order to compress
efficiently. But when they applied it to a task where two images needed to be compared,
so a motion estimation task for this motion compensation that we talked about for video,
they noticed that the effective receptive field was much larger. And this makes sense from an
intuitive point of view. Because if you want to determine how across two frames a certain motion
appears, you would want to look at a lot of context. You wouldn't want to look just at a very tiny
area. So this kind of hints that these swing models are able to better determine how much
information they should take into account from a local area in order to make their decisions.
When I hear the description of the effective receptive field, it makes me think a little bit about
ideas like lime where you're kind of creating some perturbations and input and you're trying to
see how they flow throughout a network. Is it a similar idea? It sounds sort of similar. Yeah,
I think one major application, but that really is a blast from the past, is this deep-dream style
image generation where you're kind of asking your network, okay, how should I change
this input in order to make it resemble a dog or a cat or some concept.
But of course, similar techniques have been used in many different settings.
There was also some work in this paper that looked at the kind of characterizing the latent space.
Can you talk a little bit about what that showed? So, young and young, I'll perform a couple of
analyses and some of which were aimed at seeing how these swings, compression models,
utilize the latent space and whether they utilize it in a better way than the convolutional counterparts.
And one of the ways in which you can show this is by looking at how many bits are being transmitted
through every latent channel. So, the latent space has a certain number of channels and it also
has some spatial dimensions. And for each channel, you can count how many bits are going to be
spent in order to transmit the information in this channel. And when you order these channels by
the number of bits, you can kind of construct a progressive decoding scheme. You know, you only
transmit the first channel, you get some of the key information, maybe you get the shape tried,
and you transmit the second channel, you get some of the details right, and so on. And it turned
out that these swing models were able to distribute the information more evenly than their
convolutional counterparts. So, for progressive decoding style schemes, this is potentially interesting
because it means that you can opt to, for example, just transmit the first half your guaranteed
to get some information across. But it's also interesting in case of imperfect channels. So,
if some information is lost, let's say, could you resample these channels, or could you still
decode something that looks like what you meant to decode? And so, they have some experiments
in the paper as well, where they mask certain parts of the latent channels and show that the
swing transformer is more robust to this than the convolutional compression models.
I'm resulting in cleaner reconstructions, even if some of the information is missing.
It's occurring to me that as you've been using the word channel here, I've been thinking of channels
like in an image sense, like color channels and things like that. But you're not necessarily
using it in that sense here. I guess it's similar if you look at input images as a tensor.
Yeah, sorry. For me, whenever I think of images, I always see four-dimensional tensors,
batch channel height width. And for this latent space, it's actually quite similar. There's
also a batch dimension, a channel dimension. And in this case, it happens to be a large number of
channels because we choose it to be. But from a tensor perspective, it's the same thing.
Okay. But they don't necessarily correlate to a particular physical property, like a color
or something like that. They don't know. What we hope is that these transforms, the learned encoder
and decoder, that they're able to map to some space where all of these redundancies are being
squeezed out. But we have no idea what that space means inherently. We can visualize it. And
there actually are some visualizations in the paper as well, where you can see some correlation
to the original input and some spatial patterns appearing and so on. But there are no physical
properties that you can tie it to now. It sounds like it's not quite as clean as some of the early
work looking at layers of CNNs and seeing the, you know, textures, you know, shapes deep and then
textures and things like that. Yeah, I think for some channels, you can't trace it back to some
properties. But there's no guarantee. And what we've seen is that it sometimes is highly dependent
on the input image as well. So with you pass an image through it and you see some response and
you think, hey, great, I found one that corresponds to grass. And then the next image goes through it.
It also responds and it's not a grass image. So sadly, we can't really tie it to any one property now.
To what extent does this work then kind of fit into some of the other things that you're doing?
Like we alluded to some of the work on kind of intra frame for video and there's some other stuff
that you've done that we haven't talked about. P frame and B frame codex and I guess the
transformer base to a new foundation piece that you then can kind of plug in all the other
things that you're doing for video compression around. Yeah, I would say so. I would say so.
I think our team is working on on many directions simultaneously and that hopefully are complementary.
So like you mentioned, we have some work on on different schemes for neural codex.
And this work really is aimed at replacing some foundational building block for any codec.
So what John and Jena did was not apply this just to image compression, but also did some
feasibility studies that showed that it helped promise in in the video setting. And in this case,
it was only aimed at how do you call it? Not streaming, but a real-time video.
So in low the lay setting, it's referred to in the compression setting.
But there's no reason that this could not scale to the streaming use case as well.
And what are some of the future directions that you're looking at in terms of building on top of
this? So I think like you mentioned scaling it to some of these other codex that that we've
been building. The codex that are well suited for the streaming case, for example.
Seeing whether we can reduce computational complexity further. And as you may know,
we've been demoing some of our earlier codex throughout the past year. And of course,
it's interesting to consider whether we can actually make a feasible prototype out of this.
Something that would be runnable on the vice. On that note, what kind of results did you see
from a computational perspective with this type of coding? The convolutional ones are about
equally expensive in terms of max. So multiply and accumulate operations. So it depends on how
you measure compute. From that point of view, swing transformer models are somewhat counterintuitively
more rate distortion per Mac efficient. So you get much better performance for the same amount of
Macs. But there's a small caveat. The attention operation is memory hungry. Because of the
soft Mac operation, even these swing transformers, which only compute the attention in this local
window, there's still a big hit to the total memory. And this is noticeable. And young and
you know, also included some comparisons on things like peak memory usage, which showed that the
convolutional model is still the least expensive one there. So that is one of the papers that you and
your colleagues are presenting at ICOR. There are a couple of others that I'd love to hear a
little bit about. One of those is the confess paper. Can you share a little bit about that paper?
So they're using contrastive learning in a cross domain future setting. So what this means is,
for example, you were to train a model on ImageNet and you want to use this in a setting where x
rays are being used. So it really is a big jump from domain to domain. And the method data
files sort of combines three steps in order to facilitate this domain shift and enable it.
They do use a self-supervised pre-training in order to create a certain set of features.
Then they use a feature selection scheme where they sort of train a mask that is fit for a particular
target domain. And they perform fine tuning at the end. And it turns out that this three step
procedure of which the self-supervised pre-training is probably the most important one.
You can easily transfer from domain to domain without overfitting on the very few test samples
that you may have from the new target domain. So this is especially important when you think about
use cases like personalization. Let's say you've trained a big model on a huge data set for
many different users. And now you want to apply this to just your use case.
Then this fuchsia learning becomes all the more important, likely because your data is expensive to
obtain. And how is the self-supervised, how is self-supervision built into this?
So the first step of this process is mainly about learning a good representation.
And so contrastive methods have generally been used in vision in order to build a certain set of
features that could be used for many different tasks. So for many semi-supervised settings,
for example, it turns out that self-supervised pre-training on large unlabeled data sets
are a great way to kickstart the actually quite difficult semi-supervised process.
So it's kind of a way to use large unlabeled data sets through self-supervision
so that you don't have to go through the motion of obtaining this expensive labeled data set
beforehand. Another paper we wanted to talk a little bit about is the steerable CNN's paper.
What does that mean? What's a steerable CNN?
I would say the best way to introduce it is to talk about these group-equivariant nets that
you've probably talked about with Taka Ko and some time ago. Normal convolutional models,
they're equivalent to translations. So that means you shift the input, the output of the operator
shifts with it. And what group-equivariant nets are generally aiming to do is be
equivariant to different symmetry groups as well. Not just translation, but for example,
also reflection, flipping. I mean, you've talked to Taka about this. So steerable CNN's are one
of the most general ways to accomplish this. And what my colleague Gabriela has done in this work
is a theoretical analysis of the space of these steerable filters and kind of
come up with the permiss duration for the space. And what this will do is, if you're interested
in building a network that's equivariant to some certain symmetry group, let's say one that's
solely aimed at reflections and 90 degree rotations. And what you would have to do before
Gabriela's work is kind of work out an architecture and a method that could allow you to
to satisfy the constraints of that symmetry group. But using your work, you sort of have a general
almost automatic procedure for for deriving the steerable filters. And what's nice about it
is that the code is open source. So anyone could use this and kind of kickstart
building steerable CNN for their symmetry group of interest.
Got it. So you have a particular type of symmetry that is expressing your problem that you want
to exploit. And previously you'd have to kind of handcraft a method for taking advantage of that.
And this is, it doesn't sound like it's a general mechanism, but rather a procedure that you would
follow that leads to a mechanism that works for your specific use cases. Is that the right way to
think about it? Yeah, I think so. I think so. Yeah, like you mentioned, most of the works on steerable
CNNs so far have attacked a specific group. So for example, working on on globes, when you think
about things like weather data, of course, they're not actually 2D. They operate on around the
globe or on the earth. And so it's desirable to be accurate variant across that sort of space as
well. And, and so what's previously been done is you kind of handcraft for this specific use case
a set of steerable filters. Awesome. Well, I'd love to have you share a little bit about what you're
most excited about looking forward in, in the field that that you're focused on the neuro compression
and neuro video compression. What's exciting that you see down the pike? I think one excite or
direction that I'm particularly excited about is the whole perceptual quality direction. I really
believe that this is where neural codex shine, not just gang-based codex, but really any codec that's
aimed at improving perceptual quality. But for example, with the the advent of diffusion models,
a different type of diffusion probabilistic models, a different type of generative model. Again,
very good at generating high perceptual quality details, but not exactly straightforward for
using in the compression setting. We know that neural networks are able to generate these sort of
plausible details and trying to make use of that and trying to exploit that is something that
I'm particularly excited by. And at the same time, I'm also excited about bringing down the compute
and actually making practical codex, things that you or I could run on a mobile phone and watch videos
with, because that really moves it from the academic setting to the tangible setting.
When you talk about the increasing the perceptual quality and incorporating different pieces
like GANs, is it more often that you're taking elements of these different types of models and
building them into one end-to-end train thing? Or are there other cases where you've got
higher level components that you're bringing together into more of a system type of an approach
to solving the problem? There sure are hybrid approaches. I think the most elegant way is
always this end-to-end approach where you have an encoder that's a network, you have a prior model
that's also a network and a decoder is a network and there are some, I think, really well-made work
from the Google Perception Group, for example, on an end-to-end image codec, which I believe they call
high-fidelity, generative image compression. In this case, the decoder is a conditional GAN
mapping from this latent space to image space, but the encoder is a network and the
priors and network as well. You can train this entire thing end-to-end by mixing a few lost functions.
I think it's quite an elegant way, but it's not the only way and there definitely is something to
be gained from also looking at domain expertise in particular when you think about bringing down
complexity and making sure that you don't need a huge neural network in order to kind of rebuild
or learn what other people have already figured out for you. Right, right, and does that? What are the,
are there clear lines from, kind of, performance and efficiency perspective? In other words,
like is end-to-end always more computationally intense, more efficient, or does it depend on the
specific set of architecture and implementation? It's hard to predict where this will go. I mean,
at the moment, most of these end-to-end architectures, they are computationally fairly expensive,
because they're kind of replacing many components that make use of inductive biases,
and those inductive biases, especially when you think about standard codex, they're often
also aimed at bringing down the complexity, not just at obtaining the best rate distortion performance.
So some innovations in more traditional methods, they may have been chosen over others,
not necessarily because they are bringing out the very best rate distortion performance,
but because they have good rate distortion performance, but they're also efficiently implemented
on hardware. So with neural codex, especially end-to-end codex, given that it's a fairly recent field,
a lot of the focus has been on making models that scale well and making use of the compute we have,
and recently there's been a lot more attention towards making practical codex as well.
So for example, with some of our demos in the last year, and recently, David Minnan,
from the same Google perception team gave a keynote at ISIP, in which he also had a call to action.
Look at models that are computationally less expensive, but still obtain good rate distortion
performance. So I hope that answers your question. I think now end-to-end is still computationally
fairly expensive, but we've shown that it's doable. You can do on-device decoding, but there is
a change in the works, I would say. Awesome. Well, OK, it was wonderful chatting with you and
learning a bit about all the things that you and your team are working on, and best of luck with
your presentations at the conference. Thanks, and thanks a lot for having me.
