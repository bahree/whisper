WEBVTT

00:00.000 --> 00:05.500
As we gear up for Tumacon AI platforms, now less than two weeks away, I've been out

00:05.500 --> 00:09.800
and about talking up the importance of automating, accelerating, and scaling machine learning

00:09.800 --> 00:11.800
in AI in the Enterprise.

00:11.800 --> 00:16.440
Now, I'm usually the one doing the interviewing, so being on the other side for a change

00:16.440 --> 00:18.440
was a nice experience.

00:18.440 --> 00:23.280
Much thanks to my friend's Alex Williams, host of the New Stack Makers podcast, Mentor

00:23.280 --> 00:28.640
Dial, host of the popular podcast Mentor Dialogue, and James McGuire of DataMation.

00:28.640 --> 00:32.880
I had a great time chatting with each of them, and I encourage you to check them out via

00:32.880 --> 00:37.680
the Twumacon blog at twumacon.com slash news.

00:37.680 --> 00:41.520
And while you're there, be sure to check out the latest speakers and agenda updates, and

00:41.520 --> 00:46.040
if you haven't already registered, take that step to secure your ticket for what's shaping

00:46.040 --> 00:48.920
up to be an amazing event.

00:48.920 --> 00:51.360
And now, onto the show.

00:51.360 --> 00:59.400
All right, everyone, I am on the line with Mark Ryan. Mark is the author of Deep Learning

00:59.400 --> 01:05.400
with Structured Data, a book that is currently in Early Access with Manning and Do for Publication

01:05.400 --> 01:07.960
in the spring of 2020.

01:07.960 --> 01:12.000
Mark, welcome to the Twumacon AI podcast.

01:12.000 --> 01:13.000
Thanks, Sam.

01:13.000 --> 01:14.000
It's great to be here.

01:14.000 --> 01:15.000
Awesome.

01:15.000 --> 01:19.600
Let's get started by talking a little bit about your background, and in particular, Deep Learning

01:19.600 --> 01:21.080
with Structured Data.

01:21.080 --> 01:29.720
That is a topic that folks are starting to talk about, and in fact, via the Meetup group

01:29.720 --> 01:37.080
and association with the podcast, we have a fair amount of experience exploring this

01:37.080 --> 01:42.800
through the fast AI study groups that we do, that's a big part of one of the lessons

01:42.800 --> 01:44.840
in that course.

01:44.840 --> 01:50.960
But I'd love to get a sense for how you came to be interested in this particular topic

01:50.960 --> 01:52.960
enough to write a book about it.

01:52.960 --> 01:53.960
Sure.

01:53.960 --> 01:54.960
Sure, Sam.

01:54.960 --> 02:00.400
So my academic background is from Artificial Intelligence a couple of winters ago.

02:00.400 --> 02:06.480
So I studied, studied at U of T with Graham Hurst back in the late 80s, and it was all

02:06.480 --> 02:09.000
symbolic AI back then.

02:09.000 --> 02:14.800
And there were some interesting use cases, but to a large extent, it didn't work.

02:14.800 --> 02:20.120
And I went to work for IBM, had a great career there, learned a great deal, spent a lot

02:20.120 --> 02:25.240
of time in DB2, the relational database product for my VM.

02:25.240 --> 02:30.640
And about 2016, it became evident to me that Artificial Intelligence was starting to work.

02:30.640 --> 02:35.600
There were things that were actually working, became general knowledge, and that reignited

02:35.600 --> 02:37.160
the spark in me.

02:37.160 --> 02:42.560
So I did Andrew Anges intro course, and the fast AI course.

02:42.560 --> 02:46.400
So it had that of my introduction to deep learning.

02:46.400 --> 02:48.920
And I was very interested in deep learning and the promise of that.

02:48.920 --> 02:52.920
And one of the things I found a little bit frustrating is a lot of the use cases, particularly

02:52.920 --> 03:00.440
outside of the context of the fast AI course, were to do with images or audio, they weren't

03:00.440 --> 03:01.440
structured data.

03:01.440 --> 03:06.120
And what I was looking for was, can I find a way to use this in my day-to-day work?

03:06.120 --> 03:09.360
Because this looks like it'll be very useful, and I wanted to learn more about it, and

03:09.360 --> 03:12.800
the best way to learn about it is to use data sets that you're familiar with.

03:12.800 --> 03:18.120
So when I did the fast AI course, and this would have been version one of the course, so it's

03:18.120 --> 03:20.280
been through a couple of iterations since then.

03:20.280 --> 03:24.160
And there's this section on doing deep learning with structured data.

03:24.160 --> 03:29.280
And that really sparked my curiosity, I thought, wow, this is really cool.

03:29.280 --> 03:34.360
So we got to look around for some code to have as sort of a starter kit to get going.

03:34.360 --> 03:36.120
And it wasn't easy to find.

03:36.120 --> 03:39.920
But there were some Kaggle competitions where people had been working on structured data

03:39.920 --> 03:43.600
sets and applying it to deep learning, and some very elegant little design holes that

03:43.600 --> 03:45.840
Rothman stores and the like.

03:45.840 --> 03:47.720
Exactly, exactly.

03:47.720 --> 03:50.840
And I was really impressed by some of the work that I saw there.

03:50.840 --> 03:55.480
It was very elegant, very, very straightforward, and good for me at the stage I was at then

03:55.480 --> 03:58.880
to get started, start doing some coding.

03:58.880 --> 04:03.680
And at the time I was responsible for the support organization for DB2.

04:03.680 --> 04:08.520
So there were, there's tons of data there, hundreds of tickets coming in every day, lots

04:08.520 --> 04:10.120
and lots of data.

04:10.120 --> 04:14.280
And I thought, you know, it would be good if we could apply deep learning to this to sort

04:14.280 --> 04:17.280
of see, can we do some predictions that are useful.

04:17.280 --> 04:23.560
So I built up a prototype model to predict how long a ticket would take to get closed.

04:23.560 --> 04:27.120
And that's, you know, seemed to work reasonably well.

04:27.120 --> 04:33.920
And then did another project, sort of taking what I've learned to predict duty manager calls.

04:33.920 --> 04:37.480
So those are cases where a client reaches a point of frustration and says, I'm done.

04:37.480 --> 04:43.800
I'm going to pick up the phone and get something happening with this particular problem.

04:43.800 --> 04:50.720
So applying what I had seen from some of the kernels in Kaggle and using the data that

04:50.720 --> 04:53.600
was available, created these prototypes.

04:53.600 --> 04:55.160
And I've learned a lot doing that.

04:55.160 --> 04:58.840
And they were, I think they turned out fairly well.

04:58.840 --> 05:03.520
But one of the problems with those prototypes was that the data was obviously proprietary.

05:03.520 --> 05:04.520
It couldn't share that.

05:04.520 --> 05:09.000
And there's a very strong ethic, you know, in machine learning and data science to share

05:09.000 --> 05:10.480
results.

05:10.480 --> 05:16.960
So I started to look for a more, it's like a general data set that I could use to apply

05:16.960 --> 05:19.360
deep learning to structured data.

05:19.360 --> 05:26.360
And I've written a few blog posts on medium about my experience with the predicting time

05:26.360 --> 05:29.760
to resolution and predicting duty manager calls.

05:29.760 --> 05:32.920
And Manning got in touch with me and said, would you like to write a book, kind of pull

05:32.920 --> 05:33.920
this together?

05:33.920 --> 05:36.960
And I thought, wow, that sounds interesting, sure.

05:36.960 --> 05:40.160
It has been a lot of work.

05:40.160 --> 05:43.440
And I've certainly learned a lot in the course of doing that.

05:43.440 --> 05:50.760
And one of the things I've done is create a sort of a full end-to-end example using an

05:50.760 --> 05:57.560
open data set, which is to do with streetcars in Toronto, Toronto is my hometown now.

05:57.560 --> 05:59.840
And it has a very extensive streetcar network.

05:59.840 --> 06:04.520
These are a light rail system that runs on the regular roads.

06:04.520 --> 06:05.520
And they're great.

06:05.520 --> 06:06.520
They're efficient.

06:06.520 --> 06:08.040
They're relatively cheap to run.

06:08.040 --> 06:12.480
They're cheap to create much cheaper than subways.

06:12.480 --> 06:16.480
The problem is, because they share the roads with regular traffic, if they break down

06:16.480 --> 06:20.200
or there's a delay, it really exacerbates gridlock.

06:20.200 --> 06:25.120
So the city of Toronto publishes a data set that describes all of the delays that have

06:25.120 --> 06:28.240
happened for the last five years.

06:28.240 --> 06:31.800
And I thought, well, I'm going to roll up my sleeves and try to create a simple deep learning

06:31.800 --> 06:37.320
model to analyze this data and see if it can come up with predictions to predict where

06:37.320 --> 06:41.200
they're going to be streetcar delays and hopefully be able to prevent them.

06:41.200 --> 06:47.880
So that was kind of the path I took, and that's how that was the genesis for the book.

06:47.880 --> 06:54.000
And so should I assume that that worked, that you were able to come up with a model

06:54.000 --> 07:00.040
that predicted the streetcar delays or predicted streetcar delays with that data set?

07:00.040 --> 07:02.120
Yeah, it does a decent job.

07:02.120 --> 07:03.360
It's not a huge data set.

07:03.360 --> 07:05.720
There are about 90,000 records right now.

07:05.720 --> 07:06.720
Okay.

07:06.720 --> 07:10.760
So, you know, there's some limitations, but certainly for the purposes of helping somebody

07:10.760 --> 07:18.920
who's going to be taking a trip, the accuracy is good enough to be useful.

07:18.920 --> 07:25.280
But more importantly, in terms of as a learning exercise, I think it's useful because it's

07:25.280 --> 07:26.920
an open data set.

07:26.920 --> 07:31.840
It's big, but not so big, you have to deal with kind of the problems of big data.

07:31.840 --> 07:33.400
It's very messy.

07:33.400 --> 07:37.400
So there's a lot of work to be done to prepare the data, which I think is a good learning

07:37.400 --> 07:39.040
experience.

07:39.040 --> 07:41.960
And it has various different kinds of data.

07:41.960 --> 07:45.400
There's text data, there's categorical data, there's some continuous data.

07:45.400 --> 07:51.440
So it has a lot of the, it's big enough to be interesting, but not so big that it's overwhelming.

07:51.440 --> 07:56.760
And I think it's, you know, it kind of makes a decent end-to-end example to go through

07:56.760 --> 07:57.760
the topic.

07:57.760 --> 07:58.760
Awesome.

07:58.760 --> 07:59.760
Awesome.

07:59.760 --> 08:06.960
Jumping back to the couple of projects that you worked on when you were at IBM, in particular

08:06.960 --> 08:09.920
this looking at how long it took to close tickets.

08:09.920 --> 08:16.440
When I think of a trouble ticket use case, and when I think of that trouble ticket use

08:16.440 --> 08:23.040
case, I think of, you know, not just structure data as being useful, but also the content

08:23.040 --> 08:24.040
of the ticket itself.

08:24.040 --> 08:27.920
So textual data, more like the application of NLP.

08:27.920 --> 08:34.120
Did you use only metadata about the tickets to predict the close time, or did you also

08:34.120 --> 08:35.880
use that content?

08:35.880 --> 08:37.720
That's a great question.

08:37.720 --> 08:41.480
So I did use the content, there's, all the tickets had a description, the description

08:41.480 --> 08:45.640
could sometimes it be two lines like, you know, you suck, a little bit more elaborate

08:45.640 --> 08:46.640
than that.

08:46.640 --> 08:50.720
And sometimes it would be a paragraph of lots of detail.

08:50.720 --> 08:54.040
But that description was really essential, because that's kind of the initial customer's

08:54.040 --> 08:57.360
sense of, you know, what they found.

08:57.360 --> 09:05.160
And that was, we had a simple, the model included a simple recurrent neural network to deal

09:05.160 --> 09:06.160
with that data.

09:06.160 --> 09:15.400
So it, that text field was tokenized, used in beddings, and then there was a layer, an

09:15.400 --> 09:21.040
RNN layer, that was applied in the overall model to take that text into account.

09:21.040 --> 09:25.000
And it was interesting, the difference, because they did some experiments of including that

09:25.000 --> 09:29.840
as a feature and then excluding it, because it was fairly expensive, it added some links

09:29.840 --> 09:32.440
to the time it took to train the model.

09:32.440 --> 09:36.760
And it made a reasonable difference, like it was, you know, between three and four percent

09:36.760 --> 09:40.240
the accuracy, if this, if this field was included.

09:40.240 --> 09:42.680
And that really exciting, I thought, that's really something.

09:42.680 --> 09:46.040
And the other thing is that these descriptions weren't all, so the particular field is the

09:46.040 --> 09:49.440
description field, so all of the text of the ticket.

09:49.440 --> 09:55.240
It, no, it's just the description, the text of the ticket was, wasn't available to me

09:55.240 --> 09:56.240
at that time.

09:56.240 --> 09:58.080
So it wasn't full back and forth.

09:58.080 --> 10:02.920
Because that, there could be, you know, the equivalent of 100 pages of text.

10:02.920 --> 10:06.160
So all it was dealing with text wise was the, was the description.

10:06.160 --> 10:10.160
So it could be up to 500, 500, 600 characters altogether.

10:10.160 --> 10:11.160
Got it.

10:11.160 --> 10:17.760
And so that's typically the, the textual description of the issue, either as provided by the

10:17.760 --> 10:25.720
initial customer who's, who's submitting the ticket or whoever the support rep is that

10:25.720 --> 10:30.040
is taking their call, it would, it would always be the customer.

10:30.040 --> 10:33.400
And that was, it was intentional to say, and that was part of the whole idea of the model

10:33.400 --> 10:37.800
was to only take data that was available when a ticket first hit our system.

10:37.800 --> 10:38.800
Okay.

10:38.800 --> 10:39.800
So, and that description would be there.

10:39.800 --> 10:44.840
There were other things, obviously, like, whether the ticket had changed in severity,

10:44.840 --> 10:48.360
that wouldn't be available when the ticket was first opened, because that's kind of a

10:48.360 --> 10:49.600
data leakage problem.

10:49.600 --> 10:53.200
You start to peak over and see, oh, that looks, you know, use data that you don't actually

10:53.200 --> 10:55.760
have available to you when you're making the prediction.

10:55.760 --> 10:59.200
But the textual description of the problem coming from the client was always there when

10:59.200 --> 11:04.080
the ticket was opened, and that was the, that was one of the features that was fed into

11:04.080 --> 11:05.080
the model.

11:05.080 --> 11:06.080
Okay.

11:06.080 --> 11:12.200
And so you said that the importance of this feature, you know, this, this features presence

11:12.200 --> 11:17.720
gave you an additional 3% increase in accuracy.

11:17.720 --> 11:23.160
That is, you know, relative to what without it, how much of an impact did it have?

11:23.160 --> 11:25.640
So, that was in the, in terms of the absolute accuracy.

11:25.640 --> 11:31.840
So, I think at that time, it was probably going from 73 to 76% accuracy, leaving taking

11:31.840 --> 11:34.840
that, taking that field out or leaving it in.

11:34.840 --> 11:44.080
Which I think says a lot about the, the power, I guess, of doing deep learning with the

11:44.080 --> 11:49.760
structure data that adding the text, the descriptive text of the ticket only gave you an incremental

11:49.760 --> 11:55.120
3% accuracy in terms of predicting how long it'll take to close the ticket.

11:55.120 --> 11:56.120
Is that reasonable?

11:56.120 --> 11:57.920
Is that your interpretation as well?

11:57.920 --> 11:58.920
Yeah.

11:58.920 --> 11:59.920
That's right.

11:59.920 --> 12:02.520
I guess at the time I saw it, you know, it's, it's relative where you are.

12:02.520 --> 12:08.120
I was, I was very happy it had that increase, but you're right that the, the portion of the

12:08.120 --> 12:12.800
data that would have traditionally been dealt with with deep learning was only one of the

12:12.800 --> 12:14.640
feeds that was going into the model.

12:14.640 --> 12:16.680
And that feed by itself, well, it made a difference.

12:16.680 --> 12:20.240
It was a relatively small proportion of the difference in terms of the accuracy.

12:20.240 --> 12:21.240
Yeah.

12:21.240 --> 12:22.240
Yeah.

12:22.240 --> 12:25.480
I would have thought that without the text, it would be very hard to solve this, to solve

12:25.480 --> 12:29.240
this problem with any, any degree of accuracy.

12:29.240 --> 12:30.240
Interesting.

12:30.240 --> 12:35.640
And out of curiosity, did you try training a model only on the descriptive text to see if

12:35.640 --> 12:37.640
you were able to get any results?

12:37.640 --> 12:42.000
I'm wondering if there's a scenario where, if you didn't have it, you know, if you

12:42.000 --> 12:47.160
added it, it only gave you 3% incremental accuracy, but you could get a good part of the way

12:47.160 --> 12:50.600
there if you didn't have any of the other things either.

12:50.600 --> 12:51.600
That's a really good question.

12:51.600 --> 12:54.880
I didn't try that, but that's, that's an interesting question.

12:54.880 --> 12:59.000
I, I guess in this may be kind of a primitive way of thinking about it, but because there

12:59.000 --> 13:05.160
were, I guess, you know, there were another 13 or 14 features that were available.

13:05.160 --> 13:07.320
And sometimes this text was fairly short.

13:07.320 --> 13:11.040
I thought, well, this will make it'll, it'll have some impact, but by itself, it probably

13:11.040 --> 13:16.760
wouldn't be enough to actually come up with a reasonable, a reasonably accurate description.

13:16.760 --> 13:22.320
And then just looking at some of the, at some of the descriptions, they were pretty,

13:22.320 --> 13:23.320
pretty cryptic.

13:23.320 --> 13:26.800
But something I just wanted to mention is that these descriptions weren't even all in

13:26.800 --> 13:27.800
English.

13:27.800 --> 13:32.720
So they were, they were a fair number in Japanese or Chinese.

13:32.720 --> 13:37.680
And just given the aggregated size of the data set, it wasn't massively huge.

13:37.680 --> 13:40.600
It was about, about a little over a million records.

13:40.600 --> 13:43.520
It's still, it's still made a difference.

13:43.520 --> 13:45.920
Like it still provided some, some benefit.

13:45.920 --> 13:46.920
And I see that.

13:46.920 --> 13:51.400
I know some people are kind of critical of the idea of using deep learning, kind of backing

13:51.400 --> 13:55.440
up the dump truck of data and just tipping it open, seeing, seeing what comes out.

13:55.440 --> 14:00.640
But at the same time, it was, I was, I was impressed by what could be done without doing

14:00.640 --> 14:02.880
a whole lot of munking around with the data.

14:02.880 --> 14:07.240
It was just sort of taking the, the structure data as it was applying a relatively simple

14:07.240 --> 14:14.400
model to it, getting, you know, not, not bad results, I was, I was impressed, I was impressed

14:14.400 --> 14:15.400
by that.

14:15.400 --> 14:20.080
And particularly, because there have been so little, other than the fast AI course, so

14:20.080 --> 14:25.120
little spoken about in terms of using deep learning with, with structured data.

14:25.120 --> 14:26.120
Mm-hmm.

14:26.120 --> 14:27.120
Mm-hmm.

14:27.120 --> 14:34.360
So we, we tend to think of, rightly so, deep learning as requiring a ton of data.

14:34.360 --> 14:40.560
And certainly exacerbated by the fact that, you know, some of the things that deep learning

14:40.560 --> 14:48.240
is really good at are, you know, images which can be large speech, which can be large files.

14:48.240 --> 14:53.560
Structured data can be a lot more compact, but when we think of the number of, you know,

14:53.560 --> 15:01.600
rows or examples that you're feeding into a model is, do you require less data from

15:01.600 --> 15:06.920
examples or rows perspective to train a deep learning model accurately on structured

15:06.920 --> 15:12.880
data, or is it kind of about the same, but the data is just more compact.

15:12.880 --> 15:15.840
That's a good, that's a good question.

15:15.840 --> 15:22.080
I'd say that to get a, you know, the rule of thought that you need tens of thousands

15:22.080 --> 15:26.760
of records to have a starting point is probably applicable, as applicable to structured data

15:26.760 --> 15:29.080
as it would be to on structured data.

15:29.080 --> 15:30.080
Mm-hmm.

15:30.080 --> 15:34.000
But one of the things interesting, one of the things I heard is a critique, because certainly

15:34.000 --> 15:38.080
there are people who said, like, don't, don't use deep learning with structured data.

15:38.080 --> 15:39.880
And I'm thinking, well, why?

15:39.880 --> 15:43.200
Because that's where my problem space is, that's where the job I'm trying to do is all

15:43.200 --> 15:44.200
about structured data.

15:44.200 --> 15:45.200
Yeah, don't use that.

15:45.200 --> 15:49.280
You use XGBoost, use something simpler.

15:49.280 --> 15:51.160
And I said, well, why?

15:51.160 --> 15:55.040
And some of the answers I got back were, well, the structured data sets are too small.

15:55.040 --> 15:58.280
If they're not big enough to actually apply deep learning to.

15:58.280 --> 16:02.880
And I think that's a bit of a canard, because there are some huge structured data sets.

16:02.880 --> 16:09.040
They're data sets, certainly, in the world of DB2, which is all about structured data,

16:09.040 --> 16:15.240
structured tabular data, commonly had tables with billions of records in them.

16:15.240 --> 16:20.520
So that objection, I think, is a little bit, it's a little bit short-sighted, and obviously

16:20.520 --> 16:24.480
you're not going to get great results with tiny data sets, whether it's structured or

16:24.480 --> 16:31.000
unstructured, but I think as an objection to attempting to use deep learning with structured

16:31.000 --> 16:36.520
data, the data set size really is not material to the decision about whether it's worthwhile

16:36.520 --> 16:37.520
or not.

16:37.520 --> 16:45.720
Did you attempt to apply XGBoost or any other method to this kind of problem?

16:45.720 --> 16:46.720
I did.

16:46.720 --> 16:50.320
So I tried XGBoost on two of those problems that I described.

16:50.320 --> 16:57.200
And I guess one thing that is true, the results were not significantly better for deep learning.

16:57.200 --> 17:01.640
So I think that's one of the arguments people say, well, it's not a great idea to use

17:01.640 --> 17:06.200
deep learning with structured data, is if there's a simpler way to do it that doesn't have

17:06.200 --> 17:11.320
the complexities or the opaqueness of deep learning, then why not do that?

17:11.320 --> 17:15.040
And I think there's something to be said for that.

17:15.040 --> 17:20.240
But at the same time, if I'm looking down the road a little bit, the amount of

17:20.240 --> 17:27.000
effort that it takes to get a reasonable results with XGBoost versus deep learning, particularly

17:27.000 --> 17:33.000
the human effort required for it, I'm not convinced that XGBoost is going to, for example,

17:33.000 --> 17:35.040
is going to be the winner there.

17:35.040 --> 17:40.480
As there's been so much attention on the efficiency of tools related to deep learning, the

17:40.480 --> 17:46.560
libraries are getting better, the TensorFlow 2.0 coming out, making things much more

17:46.560 --> 17:49.120
straightforward and simpler.

17:49.120 --> 17:53.960
I think some of the objections are a little bit out of date.

17:53.960 --> 17:59.480
And there are certainly cases where a non-deep learning approach could produce better results

17:59.480 --> 18:01.400
and maybe the right way to go.

18:01.400 --> 18:05.400
I guess the argument I'm making is that people keep an open mind about applying deep learning

18:05.400 --> 18:06.720
to structured data.

18:06.720 --> 18:14.440
And I think particularly as the human cost is required to get results, becomes a bigger

18:14.440 --> 18:20.960
and bigger factor, may find that deep learning has more applicability to structured data,

18:20.960 --> 18:22.320
to keep an open mind about it.

18:22.320 --> 18:23.320
Yeah, yeah.

18:23.320 --> 18:24.320
Now that's a really interesting point.

18:24.320 --> 18:31.600
I think we've got an entire conference coming up on the tooling and technology platforms

18:31.600 --> 18:38.560
that are allowing enterprises to increasingly automate and make their ability to deliver

18:38.560 --> 18:42.640
deep learning models into production, as well as traditional machine learning models

18:42.640 --> 18:46.240
in a production more quickly and efficiently.

18:46.240 --> 18:53.040
And I think that's only going to serve to reduce the barriers.

18:53.040 --> 18:58.360
And as you mentioned, the frameworks are getting more powerful and easier to use.

18:58.360 --> 19:08.200
And so now you've got this, yes, kind of a more opaque method, but one that is highly

19:08.200 --> 19:12.560
automated in a sense that you don't need a lot of manual feature engineering to get

19:12.560 --> 19:18.880
good results versus one that requires a lot of human investment to get the same level

19:18.880 --> 19:22.760
of results or comparable results.

19:22.760 --> 19:28.200
As that barrier to entry on the deep learning side is reduced, in addition to all the software

19:28.200 --> 19:32.880
tools and frameworks that are improving, you've got hardware that's making it cheaper

19:32.880 --> 19:37.800
to build these models on the compute side, that's really going to tip the balances.

19:37.800 --> 19:39.640
That's what I'm hearing you say.

19:39.640 --> 19:42.120
Yeah, yeah, I really believe that's the case.

19:42.120 --> 19:48.600
And I think, and this is something in the FAST AI course, a Jeremy Howard said that this

19:48.600 --> 19:51.360
topic, people have kind of scratched the surface of it.

19:51.360 --> 19:54.480
And I think there's much more to be done there.

19:54.480 --> 19:59.440
And obviously my approach, I'm trying to sort of a beaten potatoes, trying to solve problems

19:59.440 --> 20:00.440
approach.

20:00.440 --> 20:05.080
But I hope that there were some research as well, people looking at it from a search

20:05.080 --> 20:07.600
point of view, you're saying what can be done.

20:07.600 --> 20:14.000
And the other thing that I think structured data has, there's sort of by definition, structured

20:14.000 --> 20:16.320
data has very rich metadata.

20:16.320 --> 20:22.600
So you have a database where everything, all of the tables in the database are described

20:22.600 --> 20:23.840
in tables.

20:23.840 --> 20:31.120
And it's very, it's all tooled and instrumented to be able to see what everything that's there

20:31.120 --> 20:32.640
in the database.

20:32.640 --> 20:39.040
And I think there's some potential for work that's kind of exploratory.

20:39.040 --> 20:44.440
I could see something like a web crawling to go through a database and see, are there

20:44.440 --> 20:46.640
potential for a useful model here?

20:46.640 --> 20:51.120
And you can see a situation where as the cost of compute drops, have something that's

20:51.120 --> 20:56.640
running in the background is sort of trying all sorts of different combinations of features

20:56.640 --> 20:57.640
in a large database sale.

20:57.640 --> 20:59.560
And maybe some interesting results there.

20:59.560 --> 21:00.560
And be able to do that.

21:00.560 --> 21:04.640
And that's something the structured data gives you that you don't get from unstructured,

21:04.640 --> 21:08.960
but you don't necessarily get from a structured data where there isn't that sort of rich metadata

21:08.960 --> 21:10.960
describing what's there.

21:10.960 --> 21:14.600
And we'll come back to the structure in just a moment.

21:14.600 --> 21:19.600
But one of the, you mentioned one of the points that Jeremy makes in the fast.ai course.

21:19.600 --> 21:26.240
And that is that in many of these examples, particularly the Kaggle ones, like I think

21:26.240 --> 21:33.400
it was Rothman stores, competition, you know, when you look at the leaderboard, I forget

21:33.400 --> 21:34.840
the details I'm going to watch them.

21:34.840 --> 21:40.920
I think, you know, out of the top, you know, the top N, a good number of them, if not

21:40.920 --> 21:47.480
the top spots were folks that, you know, applied deep learning based approaches.

21:47.480 --> 21:53.480
And the key takeaway is, and this is, you know, them competing against data scientists

21:53.480 --> 22:00.200
with, you know, oodles of experience in this particular retail domain.

22:00.200 --> 22:05.480
And, you know, in many cases, folks with no domain experience, plus deep learning were

22:05.480 --> 22:11.520
able to outperform folks that, you know, brought domain experience and, you know, handcrafted

22:11.520 --> 22:13.560
models to bear.

22:13.560 --> 22:14.560
Yeah.

22:14.560 --> 22:15.560
Yeah.

22:15.560 --> 22:19.880
That's, it's, it's a good observation use and, you know, Kaggle in a way can be, it's

22:19.880 --> 22:25.440
some artificial aspects of it, but it's sort of, it's raw capitalism and what works best

22:25.440 --> 22:27.040
rises to the top.

22:27.040 --> 22:30.360
So yeah, I think there's, there's potential there.

22:30.360 --> 22:37.280
And some of the big players are doing this and there are applications that I've heard

22:37.280 --> 22:43.960
of Google and Amazon are doing with structured data, but it just doesn't seem to be something

22:43.960 --> 22:48.360
that's really front and center in terms of what people, as both as they're learning

22:48.360 --> 22:54.560
about deep learning and where the research is, it seems to be much more on the, the unstructured

22:54.560 --> 22:57.480
that is the non, non tabular data side.

22:57.480 --> 23:01.200
One of the other objections I've heard is, and there's this heuristic thrown out there

23:01.200 --> 23:05.120
is that about 80% of the data in the world is unstructured.

23:05.120 --> 23:10.480
So, you know, if that's the case, it's only like one fifth of data is structured, then,

23:10.480 --> 23:12.520
you know, how interesting is it to apply that?

23:12.520 --> 23:14.040
Still a lot of data.

23:14.040 --> 23:15.040
It's a lot of data.

23:15.040 --> 23:19.400
And I know from experience from working with DB2, I know that the, you know, the world

23:19.400 --> 23:20.640
runs on it.

23:20.640 --> 23:26.840
Every bank, every insurance company, every, every government depends on, on structured data,

23:26.840 --> 23:34.640
you know, relational database may not be sexy, but it's, it underpins our modern lives.

23:34.640 --> 23:39.040
So there's a lot of important data that's, that's structured as well, even if the volume

23:39.040 --> 23:42.520
isn't as much as my structured data.

23:42.520 --> 23:48.320
You were making the point that it's a, the application of deep learning to structured

23:48.320 --> 23:53.200
data in particular is a topic that doesn't get a lot of play out there.

23:53.200 --> 24:01.080
And it made me think of embeddings, which is one of the key techniques that are used

24:01.080 --> 24:05.400
in applying deep learning to structured data.

24:05.400 --> 24:12.760
When I first heard about embeddings through the Fast.ai course, I don't know, a couple

24:12.760 --> 24:15.480
of years ago now, you didn't hear a lot about it.

24:15.480 --> 24:17.800
It was in a couple of obscure papers.

24:17.800 --> 24:21.000
And now everybody's doing it and talking about it.

24:21.000 --> 24:23.160
And it's almost, you know, a pass A, right?

24:23.160 --> 24:29.320
It's just a tool that we, we, we pull out of the toolbox to solve a great deal, a

24:29.320 --> 24:32.320
great number of problems.

24:32.320 --> 24:41.680
It was just an article I saw yesterday about StitchFix, the, the kind of clothing, styling,

24:41.680 --> 24:46.760
in a box subscription company, created a model.

24:46.760 --> 24:50.160
I was talking about some of the different models that they use internally.

24:50.160 --> 24:56.360
And a lot of them are based on style embeddings and things like that.

24:56.360 --> 25:01.560
And so kind of in that vein, I suspect that we'll start to hear a lot more about this.

25:01.560 --> 25:07.960
And the Fast.ai course is just kind of ahead of its time once again.

25:07.960 --> 25:13.400
But why don't we take a second to drill into some of the, you know, how it works and kind

25:13.400 --> 25:16.480
of the details that you'll be talking about in the book.

25:16.480 --> 25:17.480
It's a manning book.

25:17.480 --> 25:22.400
So I'm assuming it's going to be kind of rolled up the sleeves and kind of dig into how

25:22.400 --> 25:23.920
to make all this stuff work.

25:23.920 --> 25:29.280
And I would assume that embeddings is a big, you know, one of the chapters in that book

25:29.280 --> 25:31.960
somewhere, is that, is that the case?

25:31.960 --> 25:32.960
Absolutely.

25:32.960 --> 25:33.960
Absolutely.

25:33.960 --> 25:39.640
So dealing with, I guess one of this, this may sound a bit trite, but when they're first

25:39.640 --> 25:43.360
sort of at one hot encoding, I thought, I hate this idea.

25:43.360 --> 25:44.360
And I know it's necessary.

25:44.360 --> 25:47.400
I know it's something that, you know, we have to use, but I thought, wow, you know, this

25:47.400 --> 25:52.600
is, if you've got a category with more than a dozen values in it, this just gets crazy.

25:52.600 --> 25:53.600
Yeah.

25:53.600 --> 25:57.360
It's huge, huge, you know, pandas, data frames, for example.

25:57.360 --> 26:04.480
And embeddings give you a way you can, you can assign integer encodings to the values

26:04.480 --> 26:10.240
in a categorical feature and then use embeddings to learn their relationships.

26:10.240 --> 26:15.520
So you get away from having to use a one hot encoding and the inefficiencies that are

26:15.520 --> 26:16.920
involved there.

26:16.920 --> 26:17.920
So yeah.

26:17.920 --> 26:22.120
So just in terms of what you said about embeddings completely agree that so it's a very

26:22.120 --> 26:23.120
hot topic.

26:23.120 --> 26:30.520
I think the idea that something that came out of NLP and has more broad applications

26:30.520 --> 26:32.360
is really interesting.

26:32.360 --> 26:37.920
And the idea that you kind of, you kind of get unsupervised learning, not for free, but

26:37.920 --> 26:44.440
as some of some of the exhaust foam from doing supervised learning, solving a supervised

26:44.440 --> 26:45.440
learning problem.

26:45.440 --> 26:50.480
And you can use the embeddings to come up with categorizations, like, you know, movie

26:50.480 --> 26:54.680
categorizations for Netflix, that kind of thing.

26:54.680 --> 26:55.680
That's really interesting.

26:55.680 --> 26:58.440
You get a bit of a two for one value.

26:58.440 --> 27:07.680
One of the things that comes up frequently in our fast AI study groups as a point of confusion

27:07.680 --> 27:14.880
when we're talking about the structured data topic is kind of the relationship between

27:14.880 --> 27:19.800
embeddings from an embedding space perspective and the way it's used in NLP.

27:19.800 --> 27:25.960
And the embeddings that we use in the structured data problems, do you have a, like, is that

27:25.960 --> 27:26.960
real clear to you?

27:26.960 --> 27:29.160
Do you have a great way of explaining that?

27:29.160 --> 27:36.680
Well, I think I guess with a, with NLP, you can have, you know, tens of thousands, millions

27:36.680 --> 27:40.640
of values and just the number of individual words that are each with their own individual

27:40.640 --> 27:41.960
embeddings.

27:41.960 --> 27:48.240
And then in a structured data problem where you're leaving aside textual features, just talking

27:48.240 --> 27:53.000
about categorical features, you're going to have a smaller number of individual values.

27:53.000 --> 27:57.480
So the embedding space may have a smaller dimension.

27:57.480 --> 28:03.920
You may have a, if I could say a simpler embedding space than you would dealing with NLP.

28:03.920 --> 28:08.440
But the fundamental ideas is the same.

28:08.440 --> 28:14.120
And I guess to get back to your question before about how this sort of how things work.

28:14.120 --> 28:20.880
So as I mentioned, the book has the Toronto Streetcar data set as the problem is being solved.

28:20.880 --> 28:22.960
So go through the process of cleaning that up.

28:22.960 --> 28:26.240
So getting getting rid of bad values.

28:26.240 --> 28:31.680
One of the things that I've learned of the course of doing this was some geo coding.

28:31.680 --> 28:35.980
So there are address values that describe where the problem occurred or where the street

28:35.980 --> 28:38.440
car breakdown, where was there a delay.

28:38.440 --> 28:41.120
And those address values are just completely messy.

28:41.120 --> 28:49.560
They're totally free form, you know, say young, young and queen, their date, their misspelled,

28:49.560 --> 28:53.720
their references to sort of known areas in Toronto.

28:53.720 --> 28:54.720
So it's great.

28:54.720 --> 28:57.120
It's a very interesting problem to have.

28:57.120 --> 29:03.840
And it took advantage of Google's geo coding API to turn those values into latitude and

29:03.840 --> 29:05.200
longitude values.

29:05.200 --> 29:09.760
So that's probably that's the feature that required the most, the most effort to get something

29:09.760 --> 29:11.720
coming out the other end.

29:11.720 --> 29:16.040
And then for the categorical values, so that would be, for example, the street car route,

29:16.040 --> 29:21.840
the vehicle number, the day of the week, some Monday, Tuesday, Wednesday, whatever.

29:21.840 --> 29:26.520
Those get get translated to integer IDs.

29:26.520 --> 29:31.880
And then so all the all the categorical values get get translated that way.

29:31.880 --> 29:38.600
Text fields, individual words get translated into IDs.

29:38.600 --> 29:44.200
And then those get prepared to be fed into a fairly simple carous model.

29:44.200 --> 29:50.560
But the thing that is a bit of an innovation is that the model is the layers in the model

29:50.560 --> 29:55.560
are automatically defined based on the columns that are in the table, the input table.

29:55.560 --> 30:02.080
So if they're and all of the categorical columns get a set of layers in the model to get

30:02.080 --> 30:05.040
an embedding layer and a number of other layers.

30:05.040 --> 30:11.000
If there are any text columns in the input data set, they get, they get embeddings.

30:11.000 --> 30:19.800
They also get an RNN layer and then continuous values like temperature just kind of flow

30:19.800 --> 30:21.280
through.

30:21.280 --> 30:26.600
So that model gets built up layer by layer automatically based on the columns that are

30:26.600 --> 30:29.920
in the input, the input data set.

30:29.920 --> 30:35.840
And that's in a nutshell, how the model gets put together.

30:35.840 --> 30:43.080
So this automated generation of the model is this based on a tool that you've built?

30:43.080 --> 30:44.760
This is just Python code.

30:44.760 --> 30:51.760
So there are, there's part of the code is identifying which columns of the overall

30:51.760 --> 30:55.320
data set, which columns are going to be used to train the model.

30:55.320 --> 31:00.440
And then saying which columns are breaking them into three categories, a categorical.

31:00.440 --> 31:05.800
So those would be things like the day of the week, a continuous, like temperature or

31:05.800 --> 31:07.920
time duration and text.

31:07.920 --> 31:11.360
So a description of a of a problem, for example.

31:11.360 --> 31:16.160
This sounds like you've built this tool that you point at a data set and it will almost

31:16.160 --> 31:22.920
like AutoML, I create a model that at least as a starting place for making some predictions

31:22.920 --> 31:27.640
on the structure data as opposed to, you know, the book walking you through, like how

31:27.640 --> 31:31.600
you would perform this analysis by hand.

31:31.600 --> 31:32.600
That's right.

31:32.600 --> 31:36.560
So the book provides, the idea of the book is to generalize a little bit.

31:36.560 --> 31:40.360
So the, the streetcar problem is used as an example, but say more generally here's how

31:40.360 --> 31:41.360
you deal with it.

31:41.360 --> 31:46.320
But the intention of the code is that it could be applied to other structured data sets.

31:46.320 --> 31:47.320
That it wouldn't be limited.

31:47.320 --> 31:50.960
You could, that somebody could fairly quickly take a different structured data set and apply

31:50.960 --> 31:55.840
the code and get, try it out to see how it would work with a different, a different structured

31:55.840 --> 32:00.720
data set with different columns and different, a different mix of categorical texts and

32:00.720 --> 32:01.720
continuous columns.

32:01.720 --> 32:02.720
Okay.

32:02.720 --> 32:03.720
Cool.

32:03.720 --> 32:06.000
And how, how are the chapters structured?

32:06.000 --> 32:13.840
Are they different features of this, you know, you building up this Python code or do you

32:13.840 --> 32:18.320
assume it from the beginning and you're, you know, talking more theoretically about these

32:18.320 --> 32:21.440
topics, how do you organize things?

32:21.440 --> 32:24.120
Um, so Manning's pretty big on being practical.

32:24.120 --> 32:26.960
So they've, they've certainly encouraged me to stay close to the code.

32:26.960 --> 32:32.360
And this is one of the things, and I know that you've, and you've sponsored a number

32:32.360 --> 32:34.680
of sessions going to the fast AI course.

32:34.680 --> 32:38.920
So you've seen that ethic there of really trying stuff that, you know, you get as soon

32:38.920 --> 32:42.040
as you can, I try to apply it in code.

32:42.040 --> 32:43.800
And I've tried to do that in the book as well.

32:43.800 --> 32:45.560
So introduce bits and pieces.

32:45.560 --> 32:50.160
So talk about pandas data frames is one of the essential items, so it's been some time

32:50.160 --> 32:51.240
on that.

32:51.240 --> 32:55.400
Talk about the data set, talk about the different steps take, you need to take to clean up the

32:55.400 --> 33:00.360
data set, including the geocoding problem that I talked about before.

33:00.360 --> 33:06.720
And then a chapter on the automatic, how the, the layers of the carous model are automatically

33:06.720 --> 33:07.720
put together.

33:07.720 --> 33:11.600
And that's, that's one of the ones that's not that that I'm working on currently will

33:11.600 --> 33:14.600
be released a little bit later in the year.

33:14.600 --> 33:21.480
There'll be a, a section, as you said, talking a bit more to tail on embeddings where, what

33:21.480 --> 33:23.280
role they play.

33:23.280 --> 33:27.400
And then the other thing I really want to do, and this is, this has been quite a challenge

33:27.400 --> 33:32.120
is have a, is end to end a process as possible.

33:32.120 --> 33:35.120
So be able to talk about, here's how you deploy the model.

33:35.120 --> 33:40.000
And here's how you get a little, a little website that would let you pop in the description

33:40.000 --> 33:43.960
of a particular trip and get the prediction back whether or not that trip would incur

33:43.960 --> 33:45.520
a delay.

33:45.520 --> 33:50.000
Because that's one of the things that I found in terms of the my learning process, there

33:50.000 --> 33:53.840
was an awful lot that kind of took you up to the point where you had, you had trained

33:53.840 --> 33:57.520
the model and had some sense of its performance.

33:57.520 --> 34:01.600
And then things got a little bit sketchy about how would you actually deploy it?

34:01.600 --> 34:05.680
How would you get it into, into not even production, just get it to the point where somebody

34:05.680 --> 34:08.080
else could use it or play with it.

34:08.080 --> 34:11.200
So I want to, I definitely want to spend some time talking about some options for doing

34:11.200 --> 34:17.160
that and taking the reader through a particular process for deploying the model.

34:17.160 --> 34:19.000
Sounds awesome.

34:19.000 --> 34:29.160
And so the, you've kind of suggested this, but the early access process or program at Manning

34:29.160 --> 34:35.560
is one in which you are kind of incrementally posting chapters of the book.

34:35.560 --> 34:41.080
And folks can kind of buy in early and get access to these chapters, is that right?

34:41.080 --> 34:45.720
So they get access to the book and they get the completed book.

34:45.720 --> 34:48.520
The other part of it is supposed to be a two way thing as well.

34:48.520 --> 34:53.200
There's the opportunity to make comments and to frame the book either to say things

34:53.200 --> 34:58.480
that are that need to be corrected or adjusted or make recommendations for topics that need

34:58.480 --> 35:00.280
to be covered later on in the book.

35:00.280 --> 35:05.200
So people who get involved early, who take advantage of this, have a chance to really

35:05.200 --> 35:13.480
not just be passive participants, but also contribute and be part of making this book

35:13.480 --> 35:18.320
or other books that are in this program successful and meet the needs of the people who will be

35:18.320 --> 35:19.320
reading them.

35:19.320 --> 35:20.320
That's awesome.

35:20.320 --> 35:25.640
Yeah, one of the things that really jumps out at me in this conversation, we keep kind

35:25.640 --> 35:33.760
of coming back to a fast AI, I think, because I'd seen it impact a lot of people, myself

35:33.760 --> 35:40.200
included, and I'm seeing in this conversation how you, the echoes of that course are clear

35:40.200 --> 35:47.840
and what you're doing here and kind of extending what you got out of that course now into

35:47.840 --> 35:48.840
a book.

35:48.840 --> 35:57.120
And that's exactly how the course is kind of designed that you, there's a lot of self-direction

35:57.120 --> 35:59.720
required to go through the course and really get the most out of it.

35:59.720 --> 36:05.600
And a lot of that is taking what's taught and applying it to things.

36:05.600 --> 36:09.280
And so that is awesome to see.

36:09.280 --> 36:10.280
It is a great course.

36:10.280 --> 36:17.640
It's a fantastic opportunity, a really thing I learned so much going through it and also

36:17.640 --> 36:23.720
seeing, it comes up in all sorts of different contexts as well, I mean, in the sessions

36:23.720 --> 36:28.640
that you've sponsored, other people learning about deep learning, and I think it's not

36:28.640 --> 36:35.840
just the course, it's also the approach to teaching, that's saying, get to the coding

36:35.840 --> 36:38.680
as quickly as possible and try different stuff.

36:38.680 --> 36:43.720
And it's pretty bold as well, and making a number of significant changes in the platform

36:43.720 --> 36:49.560
being used, that's a lot of work and there's risk in that, but it got to tip the hat to

36:49.560 --> 36:51.040
the team that puts that course together.

36:51.040 --> 36:58.600
I think they've done a really, really fantastic job and had a really big impact on the industry

36:58.600 --> 37:01.680
and on people's lives.

37:01.680 --> 37:07.880
People have learned a lot, it's been a ladder up for a lot of people into a world that's

37:07.880 --> 37:11.080
really exciting and really interesting and really important.

37:11.080 --> 37:15.560
Yeah, absolutely, absolutely, and I would encourage anyone who, you know, hears us gushing

37:15.560 --> 37:25.240
over this course and is interested in taking the course to do so and Jeremy Howard, who

37:25.240 --> 37:29.960
teaches the course always says that taking the course is best done with other people,

37:29.960 --> 37:35.480
the people that do that are more successful, and that's really why we, with the support

37:35.480 --> 37:43.160
of a bunch of very dedicated volunteers have been doing, I don't even count now how many

37:43.160 --> 37:48.160
kind of cohorts of folks we've done through the various versions of the course.

37:48.160 --> 37:53.200
There are several versions and then a part one and a part two, but hundreds of people

37:53.200 --> 37:59.040
now have participated in our study groups to just get support in working through these

37:59.040 --> 38:00.040
courses.

38:00.040 --> 38:04.400
If, you know, that's interesting to you, encourage anyone who's listening to visit

38:04.400 --> 38:09.280
Twomlai.com slash Meetup, sign up for the Meetup and express interest in the study

38:09.280 --> 38:10.280
groups.

38:10.280 --> 38:16.400
And if we don't currently have one running, when you do that, you know, raise your hand

38:16.400 --> 38:22.960
in the Slack channel in our Slack and express some interest and, you know, we tend to kick

38:22.960 --> 38:26.240
these things off, you know, when folks are interested in them.

38:26.240 --> 38:33.760
So with that, Mark, thanks so much for taking the time to share with us about the book.

38:33.760 --> 38:36.480
It sounds super interesting.

38:36.480 --> 38:37.480
Thank you, Sam and Sam.

38:37.480 --> 38:38.800
I want to also thank you for the podcast.

38:38.800 --> 38:44.400
I've learned so much, been a faithful listener for several years now, and it's been a great

38:44.400 --> 38:45.400
asset.

38:45.400 --> 38:46.400
I learned a great deal.

38:46.400 --> 38:49.480
So thank you very much for all the work you do to get the podcast out.

38:49.480 --> 38:50.480
Really appreciate it.

38:50.480 --> 38:51.480
Wonderful to hear that.

38:51.480 --> 38:52.480
Thanks so much, Mark.

38:52.480 --> 38:53.480
Thank you.

38:53.480 --> 38:59.640
All right, everyone, that's our show for today.

38:59.640 --> 39:05.000
For more information about today's show, visit twomlai.com slash shows.

39:05.000 --> 39:10.000
Remember, less than two weeks to register for Twomlai platforms.

39:10.000 --> 39:13.680
So head over to twomlai.com now.

39:13.680 --> 39:23.680
Thanks so much for listening, and catch you next time.

