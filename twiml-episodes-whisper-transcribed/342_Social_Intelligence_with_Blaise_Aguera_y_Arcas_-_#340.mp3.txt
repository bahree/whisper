Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting
people, doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
All right everyone, still here in Vancouver at NERPs, continuing our coverage of this
incredible conference and I've got the pleasure of being seated with Blaz Aguera Yarkas.
Blaz is a distinguished scientist with Google AI.
Blaz, welcome to the Twimble AI podcast.
Thank you so much.
Thanks for having me.
Absolutely.
So, you are doing an invited talk here at the conference tomorrow morning on social intelligence
and we're going to dig into what exactly that means for you, but before we do, I'd love
to get a bit of your background.
Sure.
So, it's a little motley.
I started off in physics, it's an undergraduate at Princeton and I studied physics and applied
math there.
I took a year off between my third and fourth years because I was not a very good student.
And I really started to get into bio physics pretty heavily.
So you're all for after?
During, or a little bit before and then during.
So I worked for a little while in Stan Leibler's lab there.
He was working on bacterial chemotaxis and that actually is going to figure a little bit
into my talk tomorrow morning.
So it's the intelligence behaviors of bacteria and how it is that they find food.
They're obviously a really small, simple system but maybe not quite as simple as people think.
And then from there, my next advisor, Bill Bialik, is somebody with a physics background
as well, but also a computational neuroscientist and he ran this course in Woods Hole at the
Marine Biological Lab called Methods and Computational Neuroscientists, Methods and Computational
Neuroscience.
Sorry.
I don't know if you're familiar or how many of your listeners are with MBL with Marine
Biological Laboratory, but it's this place where a lot of Princeton?
No, no, it's on Cape Cod, so it's right on the elbow of Cape Cod across from Arthur's
Vineyard.
It's this little tiny town, it's very cute, and there's this kind of ramshackle lab that's
been there since the 19th century that a lot of visiting neuroscientists and biologists
have been going to for many, many years.
A lot of really basic discoveries in neuroscience were made there.
So it's kind of this cool place.
And at this course, at Methods and Computational Neuroscience, I met my now wife, Adrian
Fairhol.
Oh, wow.
So she also came up in physics and studied originally chaos and turbulence and fluid dynamics and
things like this and was making the switch to computational neuroscience, so we met
there.
And then she ended up getting a faculty job at University of Washington, which is how
we ended up moving to Seattle.
And around that time, I started a company and it was no longer really part of academia
at that point.
And the company got acquired by Microsoft a couple of years later.
And the company was doing computer vision type of work, or?
Yeah, somewhat.
somewhat.
It was doing sort of multi-resolution representations of documents of various kinds.
So it was a combination of wavelet-ish kind of tricks and UX.
I think wavelet is like Kryptonite for me.
That was the hardest thing that I studied in grad school for whatever reason.
It was very difficult to rock.
It was hard.
Yeah, my advisor in grad school in Applied Math was in Grin Dobeschi, who was one of the inventors
of wavelet.
Oh, wow.
And that might have not.
Yeah, it helped.
She was absolutely wonderful, very, very smart, very kind.
And I think one of the greatest living mathematicians, if I, I don't know, maybe unbiased.
But anyway, yeah, Microsoft acquired it.
And I did immediately turn the team toward more kind of computer visionary things right
after that.
So Photosynth, which started off as the Phototourism project by a University of Washington professor
and a Microsoft research scientist, together with their grad student, Noah Snavley, was doing
3D reconstruction environments from 2D images.
And that was really my introduction to computer vision.
That was very classical.
It wasn't like deep nets or anything like this, it was geometric computer vision.
But I kind of fell in love with that field.
And ended up at Microsoft, you know, sort of doing a lot of leading of teams doing that
kind of work.
So Microsoft's, you know, OCR team and their kind of photogrammetry type teams, the teams
that ended up doing a lot of work for HoloLens, for tracking the head using outward-facing
cameras, all that kind of stuff was part of my team at the time.
Oh, wow.
So I was at Microsoft for seven years.
So it was the CTO of Bing Maps, which also had some kind of computer vision, VR, photogrammetry
kind of stuff going on, and Bing Mobile.
And then I went to Google.
That was six years ago.
Come, of course.
So many people that are in this field that have some connection to Bing.
Yeah.
Yeah, I think I shouldn't bad mouth.
I mean, it was creative and scrappy at the time, you know, whether Microsoft was really
committed to running these things, I guess, you know, as anybody's guess.
Right.
Right.
But yeah, I mean, one of the reasons that I ended up leaving Microsoft was because about
six years ago, they had just kind of lost the phone war and it became clear that they
were going to be moving away from being a consumer-focused company and they were going
to start working on just, you know, enterprise stuff.
And that wasn't that interesting to me.
And that was around the same time also that the whole deep learning revolution was really
getting into full swing.
And I was very excited about sort of machine learning and computational neuroscience
re-converging.
And Google is the obvious place, you know, where the kind of hotbed of a lot of that.
So nice.
So what do you research at Google?
Well, at Google, I started a team called Saribra, which is not a name that we've generally
used in public, but that's not at all heady.
Well, well, that's the flow.
Thank you.
It's the plural of brain.
So there was a brain team already that, you know, Jeff, Jeff Dean started the brain team
a few years before.
And I went to Google to start a team that would take a much more decentralized approach.
So rather than one brain, it would be many brains, everybody would have a little brain.
And I had a kind of very augmentation-focused point of view, you know, that rather than
having, you know, one giant AI running in a data center, these things would have to shrink,
they would have to democratize, they would have to go into devices, run locally.
I had a lot of reasons for really wanting to push in that direction, including privacy,
which I will talk about a bit tomorrow.
So mobile nets and a lot of these kind of efficient ways of running neural nets locally came
from our team.
I, again, am running the, you know, the groups at Google, the two things like OCR and
face recognition and a bunch of other sort of image understanding primitives.
But we also power a lot of AI or ML features or whatever you want to call them in Android.
And also on other kinds of devices, including these little kind of coral boards, which
are sort of an IoT kit for doing, for doing local.
Yeah, those were, I think those were just, well, I guess it's maybe half a year ago at
the TensorFlow Developer Conference, I think I have one.
Yeah, that's right.
That's right.
So yeah, we're very excited about those.
Cool.
You mentioned OCR and of all the things that we've talked about, I think of that, or it's
probably easy to think of that as a solved problem and all the problem.
But there's probably a lot of, you know, I guess even just saying it, there's probably
like this last mile problem where in order to get to usable or better levels of accuracy
and performance, kind of that those last few percentage points are really hard to get
to.
So yeah, you say, I mean, it solves problem and yeah, I mean, it's good enough for practical
use.
There are a lot of engines that are good enough for practical use.
But A, of course, extra percentage points are always useful, you know, so a little bit
more is always better.
But also, the OCR team that I ran at Microsoft was still using a lot of these classical
techniques that would first, you know, they would have a whole pipeline of different stages,
first segmenting out letters and then, you know, doing template matching and then using
language model and all kinds of stuff like this.
And the direction that I think and that the people in the OCR team believe are really the
most fruitful now are much more end-to-end and much more neural.
So imagine it's more like a scanner that scans the entire line maybe bidirectionally and
emits a string of characters, kind of like a speech engine might.
If you do it that way, then, you know, joined letters and ligatures don't matter, cursive
doesn't matter, handwriting and print could be the same, Arabic and other languages that
don't have good, you know, distinctions between letters, I shouldn't say good, but that
don't distinguish clearly between letters and a more cursive sort of approach.
All of those things work.
And that sort of generalness and also just weird fonts, you know, there are a lot of things
that are easy for us to read that a classical OCR engine can.
So thinking about it more like a real vision problem, you know, with a brain behind it as
opposed to just a classical kind of letter clustering problem with the language model
tacked on.
So is the focus of that work today achieving the level of accuracy that we previously
achieve with traditional approaches, with deep approaches, or have we-
Oh, that's not a far surpasser past, yeah, we've already far surpassed.
Okay.
Right.
I mean, the goal now is to be able to do that in a way that is compact, real time runs
on devices, doesn't- it doesn't have to be told what language something is in or what
kind of script has a unified model for every imaginable kind of screen, you know, those
kind of goals, right?
So the kind of things that a person can do.
But yeah, the neural methods have long surpassed the classical methods.
Got it.
Yeah, Jeff has been on the podcast previously and has mentioned that the transition from
traditional machine translation to neural machine translation resulted among other
things in increased performance and a reduction in the size of that code base from half a million
lines of code to- I forget what the number was 50 or something like an astounding number.
Is there a similar-
It's exactly the same.
Exactly the same.
Exactly the same story.
And learning, you know, the code is very- it suddenly becomes very small because all
of the structure, all the statistical structure in the thing is being learned rather than
coded.
And it means that a lot of the assumptions that you might make in that code don't have
to be made at a programming time like letters being distinct or being read from left to
right or, you know, or not being slanted, being able to, you know, to be kind of characterized
in terms of connected components or boxes and so-
Right.
And so you're invited to talk here at NURBS is about OCR.
No.
No.
No, the team is pretty big and OCR is only three or four people I think.
Yeah.
Yeah.
What about social intelligence, which is more related to the concept of cerebris and many
brains.
I imagine.
What exactly are you discussing in the talk?
Right.
Well, it's a wide-ranging talk and, you know, I guess it has the shape of, you know,
like some- a lot of very broad considerations at the beginning and then some specific technical
work in the middle and then maybe broadening it back out a little bit at the end.
The broad themes are that, you know, I guess we've gone to the point like with OCR that
if you know what the goal is, if you're able to score or to make a loss function for
what you're doing and we have lots of training data like we do with OCR or faces or whatever.
A couple of my considerations.
Minor considerations.
Yeah.
Obviously they're not here.
Well, but this is the thing.
I don't think that they used to be considered, you know, all that limiting.
Right.
In the days of, I don't know, the Dartmouth Summer Project or something, you know, people
were like, we don't know how to solve AI, you know, think about it, you know, only brains
can do things like understand language and writing and so on.
Surely if we figure out how to do those kinds of things, we'll crack the secret of intelligence.
And now we're like the dog that caught the car, you know, like we, you know, any of those
things that can be characterized cleanly, we can achieve superhuman performance basically.
I mean, I'm making it slightly for looking statement, not all of these things are superhuman
yet, but it's, you know, like if you, if you say something isn't then like next year
it will be.
Right.
You know, so, so like we've solved it and yet none of the systems that we've built are
intelligent in a way that you or I would recognize as intelligence.
Right.
It's, they're just, it's just functions, you know, it's just progression really in
the end.
And that's why we say, you know, like AI or ML data, ML slash data science in a way, you
know, the, the projects of AI and data science could not be more different, you know, data
science is just about, you know, modeling data and AI is about making minds.
I mean, on the face of it, like really are these the same thing.
So, you know, I think if there's a single theme for my talk, it's like, well, what is that,
what is that gap and why and why is it there?
The reason that I called it social AI or social intelligence was that I've come to believe
in the last few years that, that sociality, which is to say, our interactions with each
other are not incidental to intelligence, but are actually fundamental to it.
In the sense that life isn't a one player game, it's not like we evolved intelligence
as a, as an adaptation in order to get by in a really hard video game environment where
nature is trying to kill us and we have to outwit it.
On the contrary, like an individual human is a lot worse at outwitting nature than our
A-band sisters, like drop one of us naked in the jungle and like only a handful of us
will make it, you know, maybe the piraha, you know, in Brazil or something, but it's
very few people who can make it.
We get to watch it on TV.
Yeah, exactly, exactly.
So, you know, our environment is each other.
And there, there is a famous researcher, Robin Dunbar, who proposed the social
cortex hypothesis way back in the 90s.
And he's not the first France V underval and France devalze and many other researchers
of the same kind of idea that we are essentially the role of intelligence is to model others.
And since you and I are of the same species, it's useful for me to be able to model you
and to understand what you're going to do.
And so my brain will grow bigger in order to model yours, but we share genes.
So in the process, your brain grows bigger as well, and you have a kind of, you know,
arms race or a feedback loop, and that's how you get these kind of intelligence explosions
that we've seen in the apes and in cetaceans and dolphins and whales and some other species.
Echoes in a lot of ways, Yvahar Ari's from sapiens, idea that our kind of core achievement
as a species is the ability to collaborate and communicate goals and stories to one another
and kind of project forward.
Yeah, I absolutely agree with that point of view.
There are a bunch of recent books, Harari's are among them, but also Nicholas Kristakis,
Blueprint, the human swarm, Moffitt's book, Pat Churchill's book, Conscience, a number
of books that I think really, you know, sort of start to expound that point of view.
But I don't think a lot of that has been heard in our community in the kind of AI or
ML communities.
So you're proposing this idea that intelligence is a collaborative idea, how does that manifest
itself or what are some of the kind of next layer points that you're making in the talk?
Sure.
Well, so one of them is that when you expand, when you kind of zoom out to think about
not just the intelligence of an individual, but the intelligence of a group of a society,
which, you know, most of our achievements are, of course.
Right.
And then you are in a multi-agent kind of universe, if you want to think about it in kind
of our terms.
And in a multi-agent universe, even if every individual agent is doing optimization, in
other words, has a clear loss function or objective and is doing, I don't know, gradient descent
to optimize it, when you zoom back and look at the whole, that isn't a longer the case.
And you can see that even in a really simple example, like, you know, the most minimal
ecology you can make is an ecology of two things, and GANs are an example of an ecology of
two things.
Right.
You have an actor and an artist and a critic, and their goals are different, and they're
kind of misaligned.
Right.
The artist is trying to fool the critic.
The critic is trying to winkle out, you know, to sauce out the artist.
And when you look at what happens in the interaction between those two, they pursue each other, and
there's a spiral, you know, so this is a dynamical system.
And dynamical systems have chaos, you know, they have vorticity, they have limit cycles.
And that does not look like gradient descent anymore.
Like, if you look at the dynamics of gradient descent, it looks like a curl free field.
Curl free?
Yes.
Meaning that there's no twist in trajectories.
Everything goes downhill.
So trajectories have no curve in them.
Whereas these GANs, you know, that's why they're so quite hard to train, you know, because
they have predator-prey kinds of dynamics, essentially.
So what happens is a lot more complicated, you know, it so happens that GANs were invented
to have a fixed point that coincides with the optimum of a function that can be written
down, which reproduces a distribution of PFX.
But that's just, you know, that's just an artifact of how we cooked that particular one
up.
And in fact, it's not as if GANs, when they converge or are in general, necessarily at
such a global optimum either, you know.
So that's like the minimal ecology, and when you consider that we're made out of cells,
you know, neurons are cells that each have their own objectives and so on.
It's not just ecologies when you look at, say, multiple people that have that property,
but a single brain or a single thing is in turn composed of some things that have their
own goals and agendas.
So it's, you know, it turtles all the way down and all the way up.
And when you start looking at it that way, then I guess I'm not saying that optimization
is dumb or is a bad way to look at things, but it's more like that's just a local force.
And when you look at it at the system, the behavior of that system cannot be determined
by just looking at those local forces, you have to look at the entire picture.
I mean, it's, it's especially fascinating when you think of it in the context of research
like, you know, how much of our behavior is controlled by our microbiome or I was just
watching some TV show, maybe it was like our Earth type of show that showed, you know,
how some parasite would infect these ants and cause them to do the, like, these zombie-like
behavior and control, which I think underscores your point that, you know, so much wall behavior
and, you know, manifest behavior that we see is controlled by kind of the interactions
of things as opposed to, you know, some optimization function.
Exactly.
Very between parasitism or predation or exploitation and symbiosis is, not only is it a fine line,
I'm not even sure the line exists.
Well, certainly.
If you step back and look at a system, right, right, right, right, what is the system's
optimization function may be very different from that of the ant.
That's right.
But even if you look at things, you know, even if you try to make firm boundaries around
say an individual person, I mean, like there was, there was a recent discovery that I thought
was really cool about the arc virus.
So there are a lot of retroviruses that have been incorporated into our gene, into our
germline, into our genome that are passed on.
Some of them for many, many hundreds of millions of years.
And the arc virus is one of those.
So you know, we've known that it's there for a long time, but nobody knew what it was
doing.
It was finally caught.
So I understand this.
You say past or kind of incorporated into our geneful meaning, the body fundamentally
creates this virus as part of existing.
That's exactly right.
Like the virus is no longer distinct from your genetics.
It's not that it's, you know, endemic in a population and gets passed.
It's literally part of your DNA.
Okay.
So these are retroviruses that inject themselves into the DNA and into the germline and propagate
along with you, just like mitochondria, you know, those are bacteria, of course, right?
We're full of some of these kind of symbiotic things and arc, it turns out, you know, it was
caught in this electron microscope actually forming its viral coat and, you know, forming
its capsid.
Okay.
And if you knock it out in mice, they can't remember anything longer than 24 hours.
Wow.
So like they're involved in some way that we don't fully understand in memory formation.
Huh.
You know, wow.
Right.
Some people think that AIDS, you know, it was kind of on the way.
I mean, hopefully we've now, you know, we've now sort of controlled it.
But we might have essentially been witnessing a kind of event that has happened many, many
times in human history, wherein, you know, a virus begins in very virulent and devastating,
but eventually kind of goes global as it were and becomes incorporated into our genes
for good.
And so to make this more concrete.
Yes.
Yes.
Yes.
Well, it may be before, you know, a pit stop before we go into more concrete and technical,
you know, it calls to mind.
Obviously, I guess kind of a lot of the research in, I think, maybe the 80s or 70s, like
swarming behaviors and things like that, which imagining are very simplistic models relative
to the things that you're thinking about.
Well, yes, yes.
Also, factable.
Yeah.
That's right.
So that's, I was, if I have time tomorrow, I'm going to show two things, two sort of technical
things, one of which is very simple and is very much along those kind of 80s swarming
sorts of lines.
It's a simulation of bacteria.
So trivial system, the learning is very, very simple and so you can characterize it completely.
And the other one is about neural nets and update rules for cells and synapses.
So that one applies to, you know, to the kind of systems that we build.
So the bacteria one, yeah, it's very much inspired by that kind of centiphanes to, you
know, sort of work from the 80s and 90s.
And the idea there is, you know, bacteria have E. coli in particular, can either run or
tumble.
So when they run, they're going in a straight line and when they tumble, they randomize
their direction.
It has to do with the direction the flagella are rotating.
And you know, they're too small to have a sense of like the global environment, you know,
they can just go straight or, or kind of reverse, kind of like those RC cars that have
a like go straight or, you know, reverse and turn, kind of, so it's a one bit output.
And so I did a little simulation of them and they have a food source that moves around
and if they don't get enough food, they die.
And if they, if their food gets, if their energy level gets high enough, they reproduce.
They can also conjugate when they touch, they can swap a little bit of DNA and they, and
they, they hand her go some mutations.
So it's artificial evolution.
The first experiment involves thinking about the genome as just being the Q table.
In other words, you know, in RL language, right, just that the table, you know, stimulated
to behavior.
And you can see how, you know, with this kind of evolutionary pressure, they evolve to follow
the food.
There's a kind of, you know, their algorithms that let you do that, essentially they involve
tumbling more when you're in the food and running more when you're far away from the
food.
And statistically that will, that will make the colony of bacteria follow the food around.
I'm imagining the picture that comes to mind for me is the kind of the classic picture
of an RL agent, I forget the game, but it's like a boat game where the RL agent kind of learns
that it can rack up points by just spinning around in this particular place.
Yeah.
Yeah, that's right.
Well, I mean, one of the fun things, of course, about RL is that any kind of machine learning
really is that, you know, it can, it'll, if there's a way to cheat, it'll figure it
out.
Right.
Like life.
Right.
That's right.
So, yeah, the, it figures out how to follow the food around this way.
Things get a little more interesting if you add an additional output and you let them
emit an extra chemical that they can also sense.
Okay.
So, that, and that's done by bacteria, they have chemo, chemo signaling.
So, you know, in the rules of the game that I set up, if they're signaling, they're losing
health faster.
So, it's costly.
It's costly to signal.
So, you, you would, you would first think that, well, the first thing to learn is to
not signal because, you know, it's, it has no advantage for them, basically, and it,
and it burns energy faster.
But they don't.
They keep signaling.
So, in, you know, generation after generation, you know, restart after restart, they, you
know, almost all of them retain the signaling capability.
And what you realize, of course, is that they become a super organism when they, when they
start to share genes.
And, you know, so, thinking about them as individuals or as a, or as a single, as, as a
single organism, as a tissue, you know, it's kind of like in the eye of the beholder.
There's no, right?
It's, it's, that's not determined by the rules of the game.
And so, are we talking about the observed behavior in the game or in the thing that you
model?
Yes and yes.
Yes, yes and yes.
I mean, in the model, you can reduce everything to very, that's a very, very simple principle
since, and see all of those behaviors emerge, which is, which is fun, because then you,
you know, then it becomes clear that we're understanding something.
So, yeah, they signal to each other.
They obviously are behaving like a super organism.
You can't say what is, you know, what is the agent, you know, is it one, is it many?
And it's also kind of hard to say what they're optimizing, you know, like, if you take a
very Darwinian red of tooth and claw kind of perspective, they're like, well, they're,
they're trying to, they're maximizing their energy intake, well, but who is maximizing
their energy intake?
Is it an individual bacterium?
Is it the whole colony?
How do you think about, you know, the population or the size relative to what is being taken
and what does optimization actually look like?
Because in simulation, you observe behaviors that are sub-optimal, either individually
or globally, but optimal otherwise, or well, even talking about optimal is hard.
So I, you know, you just, you know, you're just going to the optimization that you propose,
that it's all about energy intake and thus kind of speed of reproduction.
Well, all I said was that they die if they go to zero and they reproduce if they get
to one.
I didn't say what was being optimized.
Okay.
So, you know, the thing with evolution is that like, what you see is what makes it, you
know, what persists exists, you know, we're not actually writing down a laws function.
So you can ask what is optimized and that's actually an inverse reinforcement learning
problem.
In other words, you have a queue table, now you back out policy and what's the right,
what's the reward.
And I kind of cheated in order to get, so IRL inverse reinforcement learning is actually
a hard problem in general, but I cheated by switching things up so that rather than the
genome being the queue table, now the genome is the reward map.
So they essentially evolve a reward system or an emotional system if you want to think
about it that way.
And we see like, well, what's the reward system that actually results in survival.
And what comes out is kind of what should expect, which is that in general, you know,
you look at this many, many times, death is bad, signaling is bad, because that's, you
know, that hurts and you lose health, right, mating is good, survival is good, food
is good, you know, those are positive, these are negative, so it's, you know, it's kind
of what you'd expect, but the surprise is the error bars.
So when you look at specific winning solutions, that is solutions that, you know, converge
and that, and that persist, they're all over the map.
So those are averages that I just gave you, but the variance is huge.
They're all these different reward maps that all work, you know, they, some of them result
in, meaning dying off is a lot harder than continuing in a sense.
Well, the, the set of, if you can find a, I mean, the set of, of, of emotional systems
or rewards that work is small relative to the total imaginable space of reward maps,
but it's also very varied, right, and there's not just one, there's not just one, okay,
but there are many and some of them involve lots of exploration and therefore death is
not bad for those, they're not afraid of death in some sense, some of them are very conservative
and therefore have smaller populations, they hate death, they don't signal very much,
they follow the food run really closely and all of those are viable.
So, you know, this is kind of one data point, just so we don't get stuck in time on this
thing, because I can ask you a bunch of questions about this particular thing.
This is one data point that informs a lot, a larger perspective.
What's the other, you mentioned a second example?
Yes.
So, yeah, all of this is really just a kind of extended example of, you know, of some
of the problems that arise when you try and look at a, a real evolutionary system in
terms of optimization and try and ask basic questions like what is it optimizing?
So, I then take that to the home, I guess, to the thing that concerns all of us at this
conference, which is, all right, so how do you train a model?
There's been a bunch of work in recent years on meta-learning, which involves, you know,
learning to learn.
So, learning maybe what the update rule is at a synapse or learning how, you know, not
assuming that the learning algorithm is fixed.
And I really like that approach because, you know, our genome basically has in it the
learning rule for our neurons and synapses and that evolved.
So, I began playing around with systems that evolve, use evolution to determine the rules
for neuron state and synapse state over time.
And therefore, that have to learn to learn.
They learn to learn on an evolutionary time scale and they learn on a behavioral time
scale.
That makes sense.
So, the learning to learning includes instinct and imprinting and other kinds of things.
And the learning to learn, you know, establishes the ability to go from stimuli to generalization.
And if you do that, you basically have a little kind of LSTM at every neuron and every
synapse that has local state and has its own time scales and so on that it learns evolutionarily.
And you can get very, very fast learning of standard kind of machine learning models.
I give, you know, some M-nist type examples.
So learning from very, very few examples.
And that's interesting if you just think about it in terms of optimization.
You know, this is in the spirit of some other work that has been done, a Ravi and La Rochelle
wrote a really nice paper in 2016 or 2017 that did something similar.
My version of this is a little more general in that it's not designed necessarily to optimize.
It's kind of broader.
It's just like any old, you know, it's a very general kind of synapse rule in a very
general cellular evolution rule that I don't say.
So I understand this.
You've got the picture that's forming in my head is kind of multiple agents or entities
that have some kind of embedded, you know, memory, sequence, LSTM based thing.
And in their interactions, they can essentially learn and solve M-nist type problems.
So I do show an example like that, but I mean, if it's similar at all what you're saying.
Well, the simpler version is as follows.
Okay.
Imagine that you just have, you know, in one neural net, a single kind of LSTM, a single
set of weights for an LSTM that live at every cell and every synapse.
It's like your genome, right?
So that determines how the cell responds to inputs and how the weights change.
So it's kind of a dynamic weight model based on the parameters of the LSTM's at each
of the neurons.
Essentially.
Okay.
And it's a common LSTM.
So, you know, it's a small, it's like the difference with the genome, the connectome.
The genome is very small, connectome is very large.
The LSTM parameters are, you know, are the same everywhere, like a convolutional net kind
of.
Okay.
And so they comprise the genome of the neural net and whatever weights it learns over
time or the connectome.
The same everywhere in structure, not in value.
That's right.
That's right.
So different state at every neuron synapse, but the same weights, right, on the LSTM.
Right.
Which in turn determine the rule for updating the weights in the neural net, a little confusing
I know a bit.
Yeah.
It's hard to do without the whiteboard.
It's a little hard without the whiteboard.
We run into that a lot.
I'm sure.
I'm sure.
So, yeah, how do you train, how do you determine the genome?
Well, now you do have to have a population.
So you have a population of neural nets that in the beginning have random genomes and the
attempt to learn, meaning you feed them in these digits, you feed them in error signal,
and the ones that do a better job survive and reproduce, meaning they share their LSTM
weights and yield into generation, and the ones that do poorly die off.
So you can use evolution strategies to do this, I use CMAES, or you can use more classical
kinds of evolution.
CMAES.
CMAES is a, so the ES is evolution strategies, and CMA is a particular variant of this
that assumes that the set of parameters is a Gaussian blob, and essentially estimates
the gradient of the fitness along that Gaussian and uses that to kind of make a new Gaussian
in the next generation.
Okay.
So it's a toy model of evolution, and it's very robust optimizer for moderate dimensionality.
So you do that with the genes, and you end up evolving a kind of net that learns really
fast, if that's what you make your fitness function, like learn from the minimum number
of examples, learn as fast as possible.
So like in a normal, you know, MNIST kind of set up, you know, you measure how much training
data in terms of how many passes over, you know, all 50,000 training examples or something.
This one, you know, you measure its performance in terms of how many digits you show it.
So like after you've shown it 10 or 20 digits, it's already doing pretty well.
So it's, you know, really, really, really small in.
And are you able to infrospect into it and understand what it's learning, what the hell
are you exactly?
Yeah.
Well, I haven't thought enough of that yet, but I'm measuring like the texture and feature
maps of a neural network.
That's right.
Well, I don't think that what it's, what it's learned, what the network is learning,
that the weights are, yeah, weights is an ambiguous term here.
I don't think that what the synapses get set to is particularly different from a normal
neural net.
Okay.
What's different is the update rules.
Yeah.
And the update rules are governed by a little LSTM.
So figuring out what that is doing is, you know, it's kind of like a biology problem.
It's actually not that.
It's not that trivial.
But you can do things like, you know, normally when we do backprop, we assume symmetric
weights, meaning, you know, that though you use the same weights when you're backpropagating
is when you forward propagate, you have to, otherwise you can't take the derivative.
But we know that in real brains, that's not how it works.
Like, you know, signals don't propagate backpropagate through synapses in the top 10.
Right.
Right.
And so usually we think of MNIST as this toy problem, but for someone that works on OCR,
particularly relevant, are these things that you can, you have a path to actually using
putting production?
Oh, yeah.
And for what?
Well, I mean, in this case, the goals are to be able to learn from small data, which
I care about from the point of view of privacy.
And I care about because, you know, I think one of the Achilles' heels of deep learning
as we do it today is its reliance on mass and amounts of data.
And the reason we have that reliance is because we don't have very sophisticated learning
rules, and they don't embed any priors that have been learned over, you know, evolutionary
time the way ours have about the world, right?
It's all of those priors and all of that intelligence that lives in our genome that lets us learn
so fast.
So, you know, this is an attempt to say like, well, can we reproduce what evolution did
in order to learn statistical priors and learning rules that radically outperformed the feature
engineered rules that we have today, you know, Adam and Ada Grad and SGD and all that?
So that's interesting, just from the point of view of the table.
I give what I want it.
The question is, is there a catch, right?
You know, do we just throw LSTMs in everything and, you know, now everything converges way
quickly?
Yeah.
I mean, it's much more computationally expensive to do that, of course, but on the other
hand, if you can learn from many, many fewer examples, then, you know, it's still a good
thing, even computationally, and certainly from the point of view of data.
Is there a trade-off?
Yeah, absolutely.
The trade-off is that, you know, when you're learning from very few examples, that means
that you're bringing much heavier weight and sometimes rather opaque priors to bear on
the problem.
So, you know, you're subject to more cognitive fallacies and all kinds of, you know, all the
things that humans are subject to.
So all the issues that we talk about is bias that's being introduced in, you know, our
data distributions are potentially magnified manyfold because we're training on much less
data.
Possibly.
Although, I guess I would say a couple of things.
I mean, one is that when you have very small, when you can learn from very small amounts
of data, then you can perhaps be a bit more careful about how you curate that.
But, and also, of course, the fact that the genome is very small means that you maybe
have a little bit more hope of understanding, you know, what those biases and meta priors
are.
So I think it's still positive from the point of view of problems like ML fairness and
so on.
But that's definitely something that we have, you know, one has to look at very, very closely
because, yeah, you know, newborn babies, you know, will react with fear to snake
like objects.
So like, even at a very high level, object like ignition kind of level, you know, there's
something in the genome that, you know, that makes snake like things scary, and so you
can imagine the problems that can arise from, you know, from having, you know, priors
like that, hidden priors like that, you know, in the genome for learning that.
So yes, that's definitely an issue.
But at a more meta level also to connect this to the bacteria stuff, you know, what you
can imagine is that this rule, it's not just a learning rule, it's actually a rule for
how brains or how behavior works.
And in that sense, it's like an emotional system.
What has been learned by evolution is what is good or bad as well locally.
And that's something I think that this is a route toward, toward real AI in ways that
I don't think we can do with, I don't think we can get to that by hand writing either
update rules or fitness functions or loss functions.
And so in the, the model we were just talking about, the individual agents, actors, whatever
you want to call organisms are these LSTMs.
Right.
So that's kind of looking at a, looking at a neural net as a society of neurons if you
want to think about it.
Right.
You can also then take these brains and put them together and have them teach each other
you have the kind of things that you were pointing out, right, have them interact socially
and that's also super interesting.
And I think that that's how you actually get emotional systems that, you know, that will
work.
So, so yeah, I have come to the belief that our route, I mean, I don't know what to call
AI, I guess, right, but our route to like real brains has to go through this, like,
Stalin intelligence.
Yeah.
It has to go through the social route and stopping hand defining the loss functions and
the update rules.
Interesting.
One of the things that is common between this warm intelligence approach we were referring
to earlier and what you just described is kind of a, you know, everything's the same.
Like you're building these systems out of components that are the same and groups of components
that are the same.
Whereas, you know, maybe the counter example that we've talked about is GANS where you've
got these two distinct things with different goals.
Yes.
Do you have you started looking at more heterogeneous systems?
Yeah.
I have.
So, even within a single neural net, I've often played with having different genes for
every layer, for example, or having different brain areas that have different, that have
different genes in them that are interacting, you know, with, you know, that are sort of
feeding, feeding into each other with asymmetric weights that are, you know, whose interactions
are learned, or having multiple species together.
And have entirely different kinds of inputs and outputs and that, you know, so this gets
into very a life, a very artificial life kind of paradigm.
So yeah, I think those are all really interesting roots.
I mean, the challenge with a lot of this is, of course, that, you know, we've really relied
on the fact that these things look like scores or in games for a long time in order to talk
about, like, what is state of the art, what is better or worse, you know, it's research
groups competing or collaborating with each other with a well-defined metric.
And it's really hard to take these more social and organic kind of approaches and cope
with the same sort of clear metrics, you know, for what constitutes progress.
So that's one of the big challenges that I want to kind of leave the audience with, you
know, how do we keep that sense of clarity of progress about, you know, how we're advancing
in our understanding and our ability to solve these problems when we, you know, I believe
inherently need to start looking at things that don't have such well-defined scores.
And is that because the things that we should care about are internal metrics of these social
entities or if we want to get kind of near-termish value out of them, we're applying them, the
problems that have some type of score associated with them, you know, the things that we've
been doing, like translation and OCR and, you know, playing games and things like that.
Why aren't those metrics sufficient for these types of systems?
Yeah, yeah.
So both of the things you just said are correct.
I mean, I'm not saying that metrics are bad thing for OCR, you know, or for face recognition
where, like, you know, it's very clear what is good or bad, you know, so the problem with
an alphairness for face recognition, for example, is now much discussed and it's actually
really well posed.
But if it doesn't work as well at distinguishing faces for some group of people that we define
socially, then that's a problem.
And the answer is clear, you know, like sample better in that space, you know, measure
better in that space and, you know, and set a higher bar for, you know, for everything
to performically well.
The problem is that most real stuff doesn't actually fit into that paradigm.
You know, what is the loss function for, you know, for a credit score for the correct
assignment of a credit score or for couples counseling, you know, or for good or bad art,
or for, you know, how to rank notifications on Android for that matter, you know, so
like, really bread and butter stuff, you know what I'm saying, like, if you just optimize
for India, I can tell you a thing or two about the loss function for ranking notifications
on Android.
Well, you know, and actually there are rules.
Yeah, so at the moment, at the moment, it's unfortunately a lot of a lot of hard rules
that, yeah, that themselves, I mean, that's where back to Q tables again, like if you
try and back, back out using enforcement enforcement learning, like, okay, those rules
are all, you know, kind of carefully considered, like, what are they optimizing?
You cannot write down what they're optimizing.
Right.
Right.
They're coming from a whole bunch of different socially constructed intuitions.
So kind of your point is you're proposing this potentially much more powerful paradigm
and with this much more powerful paradigm, we should be able to push into areas that
we can't apply, you know, I always fault our saying traditional, you know, since, right,
but she learning as we know it today, she learning as we know it today, which is very much
based on kind of being able to write down these loss functions, but, you know, we're kind
of in it.
It's a little bit of a chicken and egg like we can't write down a loss function so we don't
know how to apply these new things or even how to measure success with these new things.
Well, and what I would argue is that, you know, it, again, it's social, right?
So the point of ranking, I mean, if you were doing the ranking and, you know, you're
like a little person living in somebody's phone doing the ranking for them and you were
able to consult also with the other rankers, you know, and everybody else's phone using
federated learning or something like that, then, you know, the fundamental tool that
you'd want to be able to do that effectively is empathy.
You know, it's not, it's not like maximized engagement, you know, or some kind of stupid,
you know, quantitative measure like that.
Right.
It's more like, you know, be good for the humans.
Right.
Right.
And that requires modeling the other and using that as a guide for how to behave collectively
and individually.
And there may be conflicts in that, like, you know, whether you model them at the individual
level or the collective level, you know, just like in medical ethics, you have conflicts
about, say, you know, I don't know, giving people antibiotics, right?
And in the aggregate, it's not not necessarily a good thing if you're only optimizing for
the individual, you'll do it more.
And so on.
The same issues come up here.
So, you know, you've got to kind of create an entity that has clear allegiances and clear
kind of, you know, empathic goals that don't quite look like lost functions, but look
more like being able to relate socially to the relevant kinds of entities.
And you're sounding at this from a biological perspective, I imagine that there are connections
to many other, you know, economics and sociology and other things if you were to really get
into the social aspect of this.
That's right.
And dynamical systems theory and various other fields.
Right.
There's a lot of other fields come into it, yeah, exactly.
Awesome.
Awesome.
Well, guys, thanks so much for taking the time to, you know, share what you're doing and
give us a preview of what you're speaking about tomorrow.
Of course, folks that are listening to this can actually go and check out the recording
of your talk.
Absolutely.
If they've listened to all of this, they've gotten a longer version than they've talked
tomorrow morning.
So, thank you for asking such great questions.
Nice.
All right, everyone, that's our show for today.
For more information on today's guests, visit twimmaleye.com slash shows.
Thanks so much for listening and catch you next time.
