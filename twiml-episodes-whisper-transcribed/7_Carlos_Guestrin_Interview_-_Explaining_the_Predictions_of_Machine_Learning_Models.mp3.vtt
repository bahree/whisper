WEBVTT

00:00.000 --> 00:16.120
Hello everybody and welcome to another episode of Twimble Talk, the podcast where I interview

00:16.120 --> 00:21.640
interesting people doing interesting things in machine learning and artificial intelligence.

00:21.640 --> 00:27.200
I'm your host Sam Charrington.

00:27.200 --> 00:31.360
The recording you're about to hear is the first of a handful of interviews I was fortunate

00:31.360 --> 00:36.960
enough to be able to record live in New York City from the O'Reilly AI and Stratoconferences

00:36.960 --> 00:39.000
that I attended last week.

00:39.000 --> 00:43.000
I'll be sharing these interviews on the podcast over the next several weeks and I think

00:43.000 --> 00:45.640
you'll really, really enjoy them.

00:45.640 --> 00:51.920
I'm especially excited to lead off this series with an interview with Carlos Gastron.

00:51.920 --> 00:56.960
Now if that name sounds familiar it's because I've discussed Carlos's work on the show a number

00:56.960 --> 01:04.000
of times, most recently when I discussed Apple's acquisition of his company Tury back in August.

01:04.000 --> 01:09.080
In addition to Carlos's new role at Apple, he's also the Amazon professor of machine learning

01:09.080 --> 01:12.280
at the University of Washington.

01:12.280 --> 01:17.760
Earlier this year Carlos, along with one of his PhD students Marco Ribeiro and postdocs

01:17.760 --> 01:23.600
Samir Singh, published some very, very interesting research into the explainability of machine

01:23.600 --> 01:26.000
learning algorithms.

01:26.000 --> 01:31.000
My conversation with Carlos is focused on this research and the paper that the group recently

01:31.000 --> 01:36.640
published called Why Should I Trust You, Explaining the Predictions of Any Classifier.

01:36.640 --> 01:39.880
This paper has been on my reading list for a while and I encourage you to take a look

01:39.880 --> 01:40.880
at it.

01:40.880 --> 01:45.720
Of course, you'll find links to Carlos and the paper in the show notes, which you can

01:45.720 --> 01:50.200
find at twimmelai.com slash talk slash seven.

01:50.200 --> 01:54.320
A quick note about the background noise and this and the other onsite recordings, they're

01:54.320 --> 01:59.640
not too bad considering the noisy caverns in which they were recorded, but some of you

01:59.640 --> 02:02.720
might find the murmurs and bumps a bit annoying.

02:02.720 --> 02:07.280
If you find yourself in this camp, please accept my apologies.

02:07.280 --> 02:16.480
And now on to the show.

02:16.480 --> 02:22.400
So hey everyone, I'm here at the strata conference in New York City and I happen to find Carlos

02:22.400 --> 02:26.360
Gestren, who we've talked about on the podcast before.

02:26.360 --> 02:31.480
He's the Amazon professor of machine learning at the University of Washington and we've known

02:31.480 --> 02:33.200
each other for a bit.

02:33.200 --> 02:34.880
So Carlos, say hi.

02:34.880 --> 02:40.440
Hi, thanks for having me here and it was great running into you, it's a great event.

02:40.440 --> 02:41.680
Yeah, absolutely, absolutely.

02:41.680 --> 02:47.680
In fact, I think we probably had a briefing like right at this very table a year or two

02:47.680 --> 02:48.680
ago.

02:48.680 --> 02:52.760
And I think we met at this event in this very place.

02:52.760 --> 02:54.360
Where's that room over there?

02:54.360 --> 02:55.360
Yeah.

02:55.360 --> 02:56.360
Yeah.

02:56.360 --> 02:57.360
Yeah.

02:57.360 --> 03:02.560
So I guess I'll say very briefly to the audience, we're not in the most convenience spot

03:02.560 --> 03:03.560
for podcasting.

03:03.560 --> 03:08.080
So if there's the occasional trolley rolling by, just try to block that out because if you

03:08.080 --> 03:12.280
want some lunch, it's right behind us.

03:12.280 --> 03:16.240
But I'm sure you won't remember that at all because we're going to have a great conversation

03:16.240 --> 03:26.840
here, first of all, congratulations on the acquisition of Toree, Neh, Dada, Neh, Graflab

03:26.840 --> 03:27.840
by Apple.

03:27.840 --> 03:28.840
That was amazing.

03:28.840 --> 03:31.200
Yeah, we're very excited to work with Apple.

03:31.200 --> 03:32.200
It's great.

03:32.200 --> 03:33.200
Awesome.

03:33.200 --> 03:34.200
Awesome.

03:34.200 --> 03:39.840
So why don't we just start with introductions, like introduce yourself, talk a little bit about

03:39.840 --> 03:40.840
your background.

03:40.840 --> 03:43.640
I think a lot of people kind of know what you've been up to.

03:43.640 --> 03:45.520
But sure, I'm happy to share.

03:45.520 --> 03:51.280
So well, I'm Carlos, Carlos Gaston, me working machine learning for a long time.

03:51.280 --> 03:55.640
So I was a professor at Carnegie Mellon for about eight years and then at University of

03:55.640 --> 03:58.480
Washington since about 2012.

03:58.480 --> 04:03.080
And I've been excited about machine learning for a long time and worked on many areas of

04:03.080 --> 04:05.320
machine learning.

04:05.320 --> 04:09.480
Most recently, a couple of years have been exciting to me, really around dealing with

04:09.480 --> 04:12.520
a big data and the two sides of that.

04:12.520 --> 04:16.600
So on one side, algorithms for machine learning that scale to very large data sets.

04:16.600 --> 04:20.760
So how can you scale up to deal with tons of tons of data.

04:20.760 --> 04:24.280
And the second side is what I think about is the human side of machine learning.

04:24.280 --> 04:26.800
So how can a human understand large data sets?

04:26.800 --> 04:32.000
How can a human understand what machine learning algorithm is doing and bringing some kind

04:32.000 --> 04:33.960
of human perspective into the mix.

04:33.960 --> 04:38.320
So I think about those two sides, a computer perspective and a human perspective of machine

04:38.320 --> 04:41.000
learning in large data sets.

04:41.000 --> 04:44.800
And I imagine there's also a fair amount of overlap and intersect between those two.

04:44.800 --> 04:46.080
And of course, right?

04:46.080 --> 04:52.200
So the bigger your data, in a sense, the harder it is to figure out how to make it work,

04:52.200 --> 04:54.640
but it's also hard to figure out what's going wrong with that.

04:54.640 --> 04:59.360
So debugging and machine learning algorithm that requires you to run in a cluster with tons

04:59.360 --> 05:03.200
of machines is just almost an impossible task.

05:03.200 --> 05:06.920
And honestly, the way I think about it is that there's no machine learning without humans

05:06.920 --> 05:07.920
in the loop.

05:07.920 --> 05:08.920
Right.

05:08.920 --> 05:15.120
And this incredibly intelligent application, they're going to be self-sufficient, but

05:15.120 --> 05:18.920
we'll always have humans be part of the process at some point.

05:18.920 --> 05:23.760
And so making that more human is a very important part of machine learning.

05:23.760 --> 05:28.840
It's been understudied in my field, but it's something that I'm very excited to engage

05:28.840 --> 05:29.840
in as well.

05:29.840 --> 05:30.840
Yeah.

05:30.840 --> 05:34.480
So there's humans in the loop in lots of places, actually.

05:34.480 --> 05:41.480
And one of the places that humans are most certainly in the loop is on the back end of a machine

05:41.480 --> 05:43.600
learning recommendation.

05:43.600 --> 05:51.200
And your group has done a lot of interesting work very recently, at least, on explainability.

05:51.200 --> 05:54.920
Can you talk a little bit about how you've arrived at that research area?

05:54.920 --> 05:55.920
Yeah, yeah.

05:55.920 --> 06:01.080
So just in one sentence, we're interested in being able to provide more transparency to

06:01.080 --> 06:02.080
machine learning.

06:02.080 --> 06:05.640
We'll explain why a machine learning model makes a particular prediction, or why it behaves

06:05.640 --> 06:06.960
a certain way.

06:06.960 --> 06:12.720
Now, we fell into this topic kind of interestingly in various ways.

06:12.720 --> 06:17.800
So for example, in academia, we're working with various folks in application domains,

06:17.800 --> 06:20.720
and we said, oh, come use machine learning, solve this problem, it's going to be awesome,

06:20.720 --> 06:22.360
we're going to change your life.

06:22.360 --> 06:28.080
And their response was like, sounds great, but why should I trust this model?

06:28.080 --> 06:29.080
What is it doing?

06:29.080 --> 06:30.080
Right.

06:30.080 --> 06:34.560
And I was like, it's got great accuracy.

06:34.560 --> 06:35.560
So that's one side.

06:35.560 --> 06:36.920
That's somehow unsatisfying.

06:36.920 --> 06:37.920
It's unsatisfying.

06:37.920 --> 06:39.520
Yeah, no, it's not enough.

06:39.520 --> 06:40.520
It's really not enough.

06:40.520 --> 06:42.000
We can talk about why it's not enough.

06:42.000 --> 06:49.520
And then on the other side, once we build a company around machine learning, three day

06:49.520 --> 06:54.360
to graph lab, we start working for a lot of companies that brought machine learning

06:54.360 --> 06:55.360
to production.

06:55.360 --> 06:59.360
And there was always a step that nobody talked about, but it was very fundamental.

06:59.360 --> 07:05.520
They trained the model to do something, recommendations or whatever, predictor and predict fraud.

07:05.520 --> 07:08.560
And you want to deploy it as a service every time you swipe your credit card, it makes

07:08.560 --> 07:09.920
a prediction about fraud.

07:09.920 --> 07:13.480
You don't just make that happen out of the box, you want to make sure that model is working

07:13.480 --> 07:15.840
well and is doing things for the right reason.

07:15.840 --> 07:18.520
Because if it's not, you're going to get fired.

07:18.520 --> 07:19.520
Right.

07:19.520 --> 07:22.760
You really want to understand why that thing is behaving the way it is.

07:22.760 --> 07:24.360
So by talking to those.

07:24.360 --> 07:28.760
And not just that, we've talked about on the podcast before in Europe, there's legislation

07:28.760 --> 07:33.440
that's coming down the line that mandates explainability for machine learning and predictive

07:33.440 --> 07:34.440
applications.

07:34.440 --> 07:37.040
Yes, there's legislation in Europe.

07:37.040 --> 07:43.200
And not just that, even in the US, for certain application domains in the financial sector,

07:43.200 --> 07:47.800
they mandate certain models you are allowed to use versus others because they believe those

07:47.800 --> 07:52.080
models to be more interpretable or more believable or something.

07:52.080 --> 07:58.320
And so for certain tasks in that sector, you have to use a particular kind of model.

07:58.320 --> 08:04.200
And that just blocks a lot of the high accuracy models you might want to use.

08:04.200 --> 08:05.960
So it is a real issue.

08:05.960 --> 08:09.520
And I think that issue gets bubbled up in three areas.

08:09.520 --> 08:12.880
So one is just kind of general user model.

08:12.880 --> 08:16.240
How can they gain trust that service is doing things for the right reason?

08:16.240 --> 08:23.800
So if I go to a movie content recommendation in Netflix, I want to know that I got recommended

08:23.800 --> 08:28.040
Lord of the Rings because I also like Star Wars, that gives me a sense that that thing is

08:28.040 --> 08:31.400
doing the right things, recommending things that make sense to me.

08:31.400 --> 08:35.640
And they can begin to gain a relationship of trust with that artificial intelligence

08:35.640 --> 08:36.840
system underneath.

08:36.840 --> 08:40.000
So that's kind of a more personal consumery thing.

08:40.000 --> 08:46.400
But if you think about from the decision that it's really important, a really high change,

08:46.400 --> 08:52.160
like a doctor making a decision about the treatment of a patient, there you want transparency.

08:52.160 --> 08:58.800
So if you have a system that says the patient is going to have cancer with 90% probability,

08:58.800 --> 09:02.600
most doctors are going to ignore that system because they might not trust it.

09:02.600 --> 09:07.680
And also because there's a holistic approach to medicine that we want to have where it's

09:07.680 --> 09:09.800
not enough to just make that one prediction.

09:09.800 --> 09:16.080
But if the system were to say, you know, this patient is likely to have cancer because

09:16.080 --> 09:22.440
if you look at their MRI results, you see this lump, and if you look at this related cases,

09:22.440 --> 09:28.400
they were diagnosed in the same way, and if you look at this latest study, this all corroborates

09:28.400 --> 09:32.440
to evidence, then I can gain a more holistic view and I can gain trust in the system.

09:32.440 --> 09:33.840
So that's the second piece.

09:33.840 --> 09:38.000
It's kind of getting deeper insights as to what's happening in that prediction.

09:38.000 --> 09:42.960
And the third way, which is more kind of personal for me, is as a data scientist, I want

09:42.960 --> 09:45.600
to be able to make the models always better.

09:45.600 --> 09:49.200
And I want to understand when it's working, what's not working, so they can improve it.

09:49.200 --> 09:54.520
And so those are the areas, public perception of machine learning, making more informed

09:54.520 --> 09:59.600
decisions, not just the prediction for machine learning, and improving the models through

09:59.600 --> 10:00.600
feedback.

10:00.600 --> 10:04.120
And so transparency and explanation are going to be indispensable to make that happen.

10:04.120 --> 10:08.080
And unfortunately, as a field, we haven't invested enough in that topic.

10:08.080 --> 10:09.080
Right.

10:09.080 --> 10:10.080
Right.

10:10.080 --> 10:14.400
But you guys have started to invest in this and you was it a month or two ago, you published

10:14.400 --> 10:15.400
a paper there?

10:15.400 --> 10:22.880
Yeah, the paper came out just a couple months ago, and it's a system called Lyme, that

10:22.880 --> 10:28.880
my student, Marco Ribeiro, and postdoc Samir Singh, wrote, Samir is now a professor you see

10:28.880 --> 10:31.040
or find.

10:31.040 --> 10:36.840
And we wrote this paper based on the feedback we're hearing and the need to do something

10:36.840 --> 10:37.840
more in this area.

10:37.840 --> 10:40.480
And there being some other works in the kind of explainability machine learning, but what

10:40.480 --> 10:49.040
was unique about the perspective that Marco and Samir brought into the world is how they

10:49.040 --> 10:50.520
approach the problem.

10:50.520 --> 10:54.720
So a lot of the working machine learning has been about finding models they're transparent

10:54.720 --> 10:55.720
or explainable.

10:55.720 --> 10:56.720
Right.

10:56.720 --> 10:57.680
Like we talked about in the financial sector.

10:57.680 --> 11:00.880
So these models have to be simple, so this somebody can understand it.

11:00.880 --> 11:05.280
But the problem with that is that simple models tend to be inaccurate.

11:05.280 --> 11:09.760
And so you're compromising accuracy for explainability.

11:09.760 --> 11:13.000
And that's what I think is the wrong compromise to have.

11:13.000 --> 11:17.440
In fact, when you were making the comment that you drawing the analogy with Netflix earlier,

11:17.440 --> 11:21.720
I was thinking to myself, you know, I'd actually kind of rather that Netflix recommends

11:21.720 --> 11:26.360
movies that I want to see and not tell me how it got that, then recommend movies that

11:26.360 --> 11:28.400
I kind of don't really like.

11:28.400 --> 11:33.040
But it says it's read, this is the reason, so, so, so, so, right.

11:33.040 --> 11:38.320
So accuracy is still a good, but for me, the way I'm thinking about it is accuracy is

11:38.320 --> 11:40.240
number one.

11:40.240 --> 11:44.400
You want to have high accuracy for the right reasons, but that's the main thing.

11:44.400 --> 11:48.200
Otherwise, you're not going to be able to solve the image processing task that we've seen

11:48.200 --> 11:50.200
solved really well deep learning today.

11:50.200 --> 11:53.520
If you don't have the most accurate models, if I want to use a very simple model, like

11:53.520 --> 11:59.280
a shallow decision tree, you're never going to be able to detect objects in an image.

11:59.280 --> 12:00.280
So what's the point?

12:00.280 --> 12:01.280
Right.

12:01.280 --> 12:03.360
You know, we're not going to build that kind of artificial intelligence system.

12:03.360 --> 12:11.920
So what we did was say, okay, let's take accuracy as a requirement.

12:11.920 --> 12:14.760
And so that means that we want to be able to give a data scientist the flexibility to

12:14.760 --> 12:17.280
choose any model they want.

12:17.280 --> 12:23.560
And the question is, can we provide an approach that can explain the prediction for any model?

12:23.560 --> 12:29.680
That was like the question, and I think it's a really beautiful question.

12:29.680 --> 12:34.120
And you know, the way that the work came together was really interesting.

12:34.120 --> 12:37.040
Of course, it's only scratching the surface of the possibility.

12:37.040 --> 12:43.640
But what basically, Marcus Mierde, was come up with a system that says, okay, I want

12:43.640 --> 12:45.400
to explain a particular prediction.

12:45.400 --> 12:48.600
Why did you like that movie or why does this patient have cancer?

12:48.600 --> 12:51.760
And the way we're going to explain it is in a simple way that's good just for this

12:51.760 --> 12:52.760
prediction.

12:52.760 --> 12:56.440
And we're going to do it by highlighting the pieces of the input that we believe the

12:56.440 --> 13:01.120
model was most important for the model to make this decision.

13:01.120 --> 13:07.720
So for the doctor's example, it is this particular area of the MRI, this particular studies

13:07.720 --> 13:10.040
are going on, this related cases.

13:10.040 --> 13:11.720
So that's kind of a small explanation.

13:11.720 --> 13:16.280
For the recommendation system analytics that you're unhappy with, it might be that the

13:16.280 --> 13:20.400
underlying system is very complex and very accurate, but the explanation we have to give

13:20.400 --> 13:23.760
you, it's kind of very simple, but somehow has to be faithful.

13:23.760 --> 13:24.760
Right.

13:24.760 --> 13:29.840
It behaves like the model for this particular prediction, it's not like the model only

13:29.840 --> 13:36.080
uses, you know, a lot of the rings for everything, but for this prediction, that's what was

13:36.080 --> 13:37.080
the model.

13:37.080 --> 13:38.080
Right.

13:38.080 --> 13:39.600
And so that's how we went about doing that.

13:39.600 --> 13:46.200
And, you know, we did a bunch of user studies that really showed that this can be very

13:46.200 --> 13:47.840
powerful and can be used in various ways.

13:47.840 --> 13:48.840
So it's pretty cool.

13:48.840 --> 13:50.480
What was the nature of the user studies?

13:50.480 --> 13:58.200
So one of the really cool user studies that Marco designed, let me just step back and say,

13:58.200 --> 14:01.520
it's really hard to evaluate explanations because it's a subjective thing, right?

14:01.520 --> 14:04.680
How do you even figure out it's doing anything interesting?

14:04.680 --> 14:05.680
Right.

14:05.680 --> 14:12.320
And maybe if it's better, I want to get into kind of how it works and what the research

14:12.320 --> 14:13.320
actually showed.

14:13.320 --> 14:18.120
If it's better to do that first, we can do that first and encircle back to the user studies.

14:18.120 --> 14:20.960
It is, you're the boss.

14:20.960 --> 14:25.720
So let's talk about how it works, let's start from there.

14:25.720 --> 14:32.000
So the way it works is to say, I want to have a particular prediction, I want to explain

14:32.000 --> 14:35.200
why it was made, why did the model make the prediction the same way.

14:35.200 --> 14:39.320
And the way we're going to explain it is, let's look at the behavior of the system, the

14:39.320 --> 14:42.800
behavior of this complex model around this prediction.

14:42.800 --> 14:45.280
So not everywhere, but around that.

14:45.280 --> 14:49.280
So for this particular patient, I'm going to try to explain why the model thought it

14:49.280 --> 14:50.280
had cancer.

14:50.280 --> 14:54.080
So let's look at patients around that, some that were predicted to have cancer by the

14:54.080 --> 14:57.040
model, some that were not predicted to have cancer by the model.

14:57.040 --> 15:03.680
And fit a simple explanation that explains the difference between those similar patients.

15:03.680 --> 15:07.840
It doesn't explain the difference between every patient, but just patients that kind of

15:07.840 --> 15:09.000
like you.

15:09.000 --> 15:16.040
So patients that had most of the same characteristics as you that have cancer had this thing going

15:16.040 --> 15:19.040
on, and patients with similar characteristics as you that don't have cancer had these

15:19.040 --> 15:20.320
other things going on.

15:20.320 --> 15:24.440
And that gives me a lot of insights for this particular decision, and that's more or

15:24.440 --> 15:25.440
less how it works.

15:25.440 --> 15:32.160
So as a course approximation of what I'm hearing, take the example of that you use in your

15:32.160 --> 15:36.720
course, predicting real estate prices based on a bunch of different variables, it almost

15:36.720 --> 15:44.320
sounds like you might have a model that's a regression model that's predicting based on

15:44.320 --> 15:53.400
house size, and the Lyme system is almost a reverse regression that's going the other

15:53.400 --> 15:56.680
way, like predicting what the inputs might be based on the output.

15:56.680 --> 16:02.840
In a sense, so one way to think about it is house prices are very complex, depends on

16:02.840 --> 16:07.920
the neighborhood you live in, the characteristics of the house, everything around it.

16:07.920 --> 16:13.320
So there is a simple explanation why your house costs showing you $5 million, right?

16:13.320 --> 16:15.360
That's how much your house costs.

16:15.360 --> 16:18.160
And so, can you give me a raise to it?

16:18.160 --> 16:20.160
So I can pay for my house.

16:20.160 --> 16:24.360
I'll double what I pay you right now.

16:24.360 --> 16:28.400
So why did your house cost five million?

16:28.400 --> 16:30.200
Why did the house cost certain amount?

16:30.200 --> 16:34.920
Who knows, really complex, the models can be very complex, but your specific house can

16:34.920 --> 16:38.920
explain why the model predicted just $5 million, that's very doable.

16:38.920 --> 16:44.200
I can look at your house and I look at similar houses that were like it, and I can fit locally

16:44.200 --> 16:49.720
a simple model with only a few variables, they were most important, that's basically,

16:49.720 --> 16:55.200
let's say linear, and it says around your house, the variables that were most important

16:55.200 --> 17:04.520
were square footage and zip code and number of bathrooms, that's really important for

17:04.520 --> 17:12.280
$5 million house, for $500,000 house, it might be a different thing is more important.

17:12.280 --> 17:18.480
And so that's the kind of thing that the model would say, right, so that's kind of how

17:18.480 --> 17:25.000
it works, it provides the key pieces of the input, they were most distinctive for particular

17:25.000 --> 17:30.920
prediction, and as I said, this is one way to do explanations which scratch in the surface,

17:30.920 --> 17:34.800
there's all sorts of other ways you can imagine doing it, and I think there's a lot of opportunity

17:34.800 --> 17:35.800
to do even more.

17:35.800 --> 17:40.640
But one of the challenges is how do you figure out if this makes any sense whatsoever, if

17:40.640 --> 17:44.640
these kinds of explanations are good, if the algorithm is working at all, like there's

17:44.640 --> 17:50.880
so many dimensions that this can go wrong, how can you even figure out if this is at all

17:50.880 --> 17:57.800
reasonable, right, and Marco Samir and I spent a lot of time buying your head against

17:57.800 --> 18:04.200
the ball to figure out how can you even test this, how can you even measure it, how do you

18:04.200 --> 18:11.720
explain my explanations, and Marco had a couple brilliant ideas that really surprised

18:11.720 --> 18:19.520
me, and so let me give you one of them, so he wanted to know if explanations are good

18:19.520 --> 18:23.880
and intuitive, that would mean that somebody who is not a machine learning expert, a layperson

18:23.880 --> 18:30.360
could look at it and make good decisions from it, so that's what he thought, so how can

18:30.360 --> 18:36.640
we test that hypothesis, so he did two tests of validated hypothesis in a brilliant way,

18:36.640 --> 18:44.000
so the first one was, if I can interrupt you, it sounds like the aim of the research was

18:44.000 --> 18:50.280
not just to spit out, like in order list of features in terms of, you know, waiting or

18:50.280 --> 18:56.680
importance in the output, but more generating human readable description, am I reading

18:56.680 --> 18:58.480
too much into this, or is that correct?

18:58.480 --> 19:02.560
No, no, no, no, no, it was a human interpretable description, interpretable, not necessarily

19:02.560 --> 19:07.800
readable, okay, that's one way to explain, just can be many things, right, we explored

19:07.800 --> 19:17.160
human readable explanations, the visualizations, we explored different ways to explain things,

19:17.160 --> 19:21.920
but yeah, so here's the experiment that he did, which was pretty brilliant, so he took

19:21.920 --> 19:31.720
a data set and just a little background, which is kind of a fun story that I just told,

19:31.720 --> 19:36.160
there's this famous data set called the 20 News Groups Data Set, like 20 News Groups

19:36.160 --> 19:46.040
Data Set has been around for about 30 years in a machine learning community, and it's

19:46.040 --> 19:49.280
from news groups, which you might not know what they are, but they're something they're

19:49.280 --> 19:53.840
called forums, now called Facebook pages, right, they have a topic and people talk about

19:53.840 --> 20:00.160
the topic and they post things, and the data set was famous because the idea was given

20:00.160 --> 20:04.640
the text of the posting, can you predict whether this was about Christianity or atheism

20:04.640 --> 20:09.960
or hockey or computers, whatever the topic was, and basically any modern machine learning

20:09.960 --> 20:18.120
approach gets 94% accuracy, so everybody used this data set in their classes, like I

20:18.120 --> 20:21.960
used in my class, I said, oh machine learning is so cool, I guess 94% accuracy, when

20:21.960 --> 20:27.000
Marco run his explanations on that, it turns out that the main features being used are

20:27.000 --> 20:34.360
things like the email address of the poster, okay, so sam at gmail.com, always post

20:34.360 --> 20:41.800
in the, I don't know what your interest are, sam, but then let's say podcast, news group,

20:41.800 --> 20:49.480
podcasting, yeah, recta podcasting, clearly, and obviously it's a great predictor, but

20:49.480 --> 20:53.680
it doesn't generalize to somebody else, and so it's not a good feature, it's a good

20:53.680 --> 20:59.000
feature for you, but not a good feature for the world, and so if you remove that, those

20:59.000 --> 21:05.880
kinds of features, the accuracy went down from 94% to only 57%, so this data set that

21:05.880 --> 21:11.600
everybody has used for decades, a machine learning is so well, actually it was great so

21:11.600 --> 21:15.960
well, and you were able to see that from the explanations, so the question that he wants

21:15.960 --> 21:21.720
to ask going back to the user study was, as an expert he discovered this with the explanation,

21:21.720 --> 21:26.040
can somebody who's not a machine learning expert discovered this and improved the performance

21:26.040 --> 21:32.320
of a machine learning system, so he took this data set and there was only getting 57%

21:32.320 --> 21:36.720
accuracy, and then clean the data, as much as scrub, scrub, scrub, scrub, remove all

21:36.720 --> 21:41.960
this bad features, then retrain the model and using what to get about 70% accuracy,

21:41.960 --> 21:48.960
by removing all the bad features like sam at gmail.com, and then coming up with a new model

21:48.960 --> 21:54.480
or new model training on that clean data set, so that's the go sound, clean data, and

21:54.480 --> 22:00.240
it was the dirty data, and the question was, using explanations could mechanical turkers

22:00.240 --> 22:05.560
who know nothing about machine learning, identify bad features, we said, look at explanation,

22:05.560 --> 22:09.560
just cross out things that you think should be relevant for this system, we didn't say

22:09.560 --> 22:13.840
anything else, just cross out things that you think are irrelevant, and we thought, okay,

22:13.840 --> 22:22.520
could crossing it out get performance of close to Marko's gold standard, from non experts.

22:22.520 --> 22:30.880
So you ran the line system, the explanation system, against the dirty data set, you came

22:30.880 --> 22:35.280
up with these explanations that included things like emails that should be irrelevant,

22:35.280 --> 22:38.800
and you asked if turkers, yes, turkers if they could figure that out.

22:38.800 --> 22:44.080
Figure out what parts of the explanation, in the sense what features, you thought should

22:44.080 --> 22:47.640
not have been used as parts of this decision, and how did that go?

22:47.640 --> 22:54.480
So after just three rounds of mechanical turkers crossing things out, they were able to

22:54.480 --> 22:58.720
get better accuracy than Marko's gold standard data set.

22:58.720 --> 23:03.560
So they were able to clean the data better than Marko did.

23:03.560 --> 23:04.960
What is around in this case?

23:04.960 --> 23:13.600
Around was showing the, so adding into details, but we showed the explanations to a number

23:13.600 --> 23:19.360
of mechanical turkers, they were able to cross it out and retrain the model, then we showed

23:19.360 --> 23:24.160
the new model to different types of mechanical turkers, the crossing things out and we showed

23:24.160 --> 23:25.160
it again.

23:25.160 --> 23:30.160
So we just did that three times, three iterations with non experts, but looking at explanations,

23:30.160 --> 23:34.600
they were able to find all sorts of problems with data, clean it, and get better performance

23:34.600 --> 23:38.880
than Marko did, like sitting down and like trying to clean the data himself.

23:38.880 --> 23:42.720
Which was surprising, so that means that non experts, this is just an example, so just

23:42.720 --> 23:51.640
a non-experts are able to understand explanations of complex machine learning system and provide

23:51.640 --> 23:55.920
some feedback to that system that can be used to improve the performance of that system.

23:55.920 --> 23:57.680
Which was really surprising.

23:57.680 --> 23:59.920
Wow, that's very cool.

23:59.920 --> 24:01.160
It was very cool.

24:01.160 --> 24:06.520
And then the second user, so then you know, Marko was bold and then went to come up even

24:06.520 --> 24:13.840
more interesting for this, so Mark, and the second one was also really exciting.

24:13.840 --> 24:18.200
So here's what he wanted to ask.

24:18.200 --> 24:22.440
When you train a machine learning model, you usually train it on some data and you evaluate

24:22.440 --> 24:29.320
it on some data that you hold out, it's called a test set, so that you don't kind of get

24:29.320 --> 24:32.240
a biased prediction on how well the model do.

24:32.240 --> 24:37.280
So you can imagine some models might do well on the training set, but don't do well on

24:37.280 --> 24:40.760
the test data set, so we want to throw out those models.

24:40.760 --> 24:45.160
And some models will do well on the test set, and then you want to keep those models.

24:45.160 --> 24:46.560
So that's what you typically do.

24:46.560 --> 24:50.800
If you go back to the training news groups data set, if we had just looked at the training

24:50.800 --> 24:56.000
news groups with the email address in the test set, you do well on the test set, so you

24:56.000 --> 24:57.000
think you're doing well.

24:57.000 --> 24:58.000
But it wouldn't be.

24:58.000 --> 24:59.000
We didn't validate very well.

24:59.000 --> 25:04.360
But if you had tested on some other data set, they didn't have some as email.com, then

25:04.360 --> 25:06.960
it would have them badly, then it would be able to throw it out.

25:06.960 --> 25:11.240
So here's the experiment that Marko did, which I thought was brilliant.

25:11.240 --> 25:19.320
He split the data into a training set and a test set, and he trained a bunch of models,

25:19.320 --> 25:23.880
a lot of models on different random subsets of that input data, of the training data.

25:23.880 --> 25:29.120
And some models did well on the training data, and some models did badly on the training

25:29.120 --> 25:30.120
data.

25:30.120 --> 25:32.480
He threw out everything that did badly on the training data, because we only want to keep

25:32.480 --> 25:34.520
high accuracy models.

25:34.520 --> 25:37.760
So he kept only things that were accurate on the training data.

25:37.760 --> 25:41.200
And then he looked at the test data set, and some models did well on the test data

25:41.200 --> 25:44.600
set, and some models did badly on the test data set.

25:44.600 --> 25:48.960
And then he said, oh, the model is still now, I mean, this is pretty standard, what any

25:48.960 --> 25:50.200
data scientists would do.

25:50.200 --> 25:51.200
Yeah, yeah.

25:51.200 --> 25:52.200
Pretty standard.

25:52.200 --> 25:54.200
Well, Z-Watt sticks against the wall.

25:54.200 --> 25:55.200
Yeah, exactly.

25:55.200 --> 25:56.200
Yeah.

25:56.200 --> 25:57.200
Now he did the following.

25:57.200 --> 25:58.200
Okay.

25:58.200 --> 26:02.160
He took mechanical turkeys with no nothing about machine learning, and he showed them explanations

26:02.160 --> 26:06.560
for the models that did, they didn't say anything about the process, and it was all

26:06.560 --> 26:09.000
randomized and blind and everything, right?

26:09.000 --> 26:14.000
So the explanations for models that were doing well on the test set, and models that did

26:14.000 --> 26:20.360
badly on the test set, and they both looked equally good on the training set.

26:20.360 --> 26:21.360
Yeah.

26:21.360 --> 26:27.320
And the test set, you mean the validation.

26:27.320 --> 26:30.840
Actually, it was a hidden, in this case, it was a hidden test set, but it came out of

26:30.840 --> 26:31.840
all this way.

26:31.840 --> 26:32.840
It was a hidden test set.

26:32.840 --> 26:36.840
So there was something that you wouldn't do as a data scientist, but the test, he held

26:36.840 --> 26:43.040
off some additional data, and he ran both, he ran lots of models, did well on the training

26:43.040 --> 26:50.040
set, and he picked out some that did badly on the test set, and did well on the test set.

26:50.040 --> 26:55.560
And then showed explanations for all those models to make out code turkeys and ask, which

26:55.560 --> 27:00.080
model do you think is going to be better in the real world, based on the explanations of

27:00.080 --> 27:01.840
why they're making their predictions?

27:01.840 --> 27:02.840
Yeah.

27:02.840 --> 27:08.080
And they were asked to pick between one, between two.

27:08.080 --> 27:09.080
Yeah, between two.

27:09.080 --> 27:11.280
And you were comparing them with the coin flip, right?

27:11.280 --> 27:12.280
Yeah.

27:12.280 --> 27:13.280
Compared to the coin flip.

27:13.280 --> 27:14.280
And they did better than a coin flip.

27:14.280 --> 27:16.880
So a coin flip gets 50% accuracy, 87% accuracy.

27:16.880 --> 27:18.720
Wow.

27:18.720 --> 27:26.680
So totally untrained, unwashed mechanical Turk masses are basically creating, you know,

27:26.680 --> 27:28.280
doing feature engineering on models in their head.

27:28.280 --> 27:29.480
So the first part was feature engineering.

27:29.480 --> 27:31.800
The second part was like model selection, basically.

27:31.800 --> 27:32.800
Right.

27:32.800 --> 27:36.240
They were able to look at explanations and figure out this model is stupid.

27:36.240 --> 27:37.240
Yeah.

27:37.240 --> 27:39.200
Even though on the training set, it looked great.

27:39.200 --> 27:40.200
Yeah.

27:40.200 --> 27:42.200
But in the real world, there's going to be bad.

27:42.200 --> 27:43.200
Wow.

27:43.200 --> 27:44.200
That's pretty amazing.

27:44.200 --> 27:46.200
And that was amazing to me.

27:46.200 --> 27:50.360
And the fact that we can do that, as I said, we're only scratching the surface here,

27:50.360 --> 27:57.080
but the fact we can do that, to me says humans will be in the loop in the long run.

27:57.080 --> 28:01.480
There are insights who have, I mean, humans in the loop, because even kind of the statistical

28:01.480 --> 28:05.760
problems that underlie this question, like if we discuss for a long time, we can talk

28:05.760 --> 28:09.720
about why there's a relevant statistical.

28:09.720 --> 28:13.920
Humans might be able to pick those out and they'll be able to do better feature engineering.

28:13.920 --> 28:18.040
They'll be able to understand problems they're going in the data, even on train folks.

28:18.040 --> 28:24.520
And now, if you imagine doing this to get more insight for the doctors or for systems

28:24.520 --> 28:27.120
in the real world, I think you could do really amazing things.

28:27.120 --> 28:30.840
So it's pretty exciting to me to start exploring this further and further.

28:30.840 --> 28:31.840
That's very cool.

28:31.840 --> 28:32.840
Let me ask you this.

28:32.840 --> 28:36.640
This is kind of in the weeds question, but were the features, what you might think of as

28:36.640 --> 28:39.000
natural features or engineered features?

28:39.000 --> 28:40.000
Yeah.

28:40.000 --> 28:45.640
That's the really interesting or a really interesting question.

28:45.640 --> 28:50.080
So the underlying models used the engineered features.

28:50.080 --> 28:54.200
So for example, he also showed this was good for deep learning models for images, which

28:54.200 --> 28:58.560
used really, you know, learn complex features of the data.

28:58.560 --> 29:01.720
But the way that he explained was from pieces of the input.

29:01.720 --> 29:07.400
So the assumption that he made was the input is interpretable.

29:07.400 --> 29:11.920
And by selecting pieces of images or part of the text, I used to look at that and say,

29:11.920 --> 29:13.840
oh, this makes sense.

29:13.840 --> 29:17.920
If we looked at like the seventh player of a neural network, I can say, oh, this is like

29:17.920 --> 29:19.720
a human being like, what is that?

29:19.720 --> 29:20.720
Yeah.

29:20.720 --> 29:22.040
And why do I care?

29:22.040 --> 29:24.760
And so that's why we biased towards this approach.

29:24.760 --> 29:29.160
It doesn't mean that in long run we want to invent something better that looks at the features

29:29.160 --> 29:32.600
because the problem might be down in the features, it might be down in the weeds.

29:32.600 --> 29:34.840
And that's kind of where the research should go.

29:34.840 --> 29:38.040
But as a first step, we looked at pieces of the input.

29:38.040 --> 29:42.600
And it's totally model independent, like you can work on neural network models.

29:42.600 --> 29:43.600
Yeah, yeah.

29:43.600 --> 29:44.600
We've done this with deep neural networks.

29:44.600 --> 29:46.400
We've done this with boosted decision trees.

29:46.400 --> 29:48.600
We've done this with lots of different kinds of models.

29:48.600 --> 29:49.600
Wow.

29:49.600 --> 29:50.600
Wow.

29:50.600 --> 29:51.600
So I know we need to get you off to your next session.

29:51.600 --> 29:53.440
Where can folks learn more about this?

29:53.440 --> 29:57.080
So if you just search for my name and line, you'll find our paper.

29:57.080 --> 29:58.680
It was a KDD this year.

29:58.680 --> 29:59.680
Line LIME.

29:59.680 --> 30:00.680
LIME?

30:00.680 --> 30:01.680
Okay.

30:01.680 --> 30:06.840
We'll find a GitHub project that Marco has been putting together, we hope it's some of

30:06.840 --> 30:08.840
these ideas.

30:08.840 --> 30:12.640
But yeah, it's been a pretty exciting work.

30:12.640 --> 30:18.320
And you just keep tracking, you know, Marco, Samir, there'll be a lot more in the pipeline.

30:18.320 --> 30:19.320
There's a really cool thing.

30:19.320 --> 30:20.320
That's awesome.

30:20.320 --> 30:21.320
That's great.

30:21.320 --> 30:24.320
And if folks want to reach out to you, you're on Twitter or what's the best way to get in touch

30:24.320 --> 30:25.320
with you?

30:25.320 --> 30:26.320
I'm on Twitter, Gastrin.

30:26.320 --> 30:27.320
It's my last name.

30:27.320 --> 30:28.760
It's the handle.

30:28.760 --> 30:29.760
Okay.

30:29.760 --> 30:30.760
And so reach out.

30:30.760 --> 30:31.760
Awesome.

30:31.760 --> 30:32.760
Give me some feedback.

30:32.760 --> 30:36.880
We're also, as you know, on a Coursera teaching machine learning and that's another place

30:36.880 --> 30:38.880
that I interact with folks.

30:38.880 --> 30:39.880
Yep.

30:39.880 --> 30:40.880
And it's a great course.

30:40.880 --> 30:41.880
I highly recommend it.

30:41.880 --> 30:42.880
Very case study focused.

30:42.880 --> 30:43.880
I really enjoyed it.

30:43.880 --> 30:44.880
Okay.

30:44.880 --> 30:45.880
Thanks.

30:45.880 --> 30:46.880
Great.

30:46.880 --> 30:47.880
All right.

30:47.880 --> 30:48.880
Thanks so much, Carlisle.

30:48.880 --> 30:49.880
Yep.

30:49.880 --> 30:50.880
I'll do a handshake here.

30:50.880 --> 30:51.880
On the audio.

30:51.880 --> 30:52.880
Thanks.

30:52.880 --> 31:01.400
All right, everyone, that's it for today's show.

31:01.400 --> 31:03.400
Thank you so much for listening and for your continued support.

31:03.400 --> 31:04.880
A quick story.

31:04.880 --> 31:09.560
If you follow me on Twitter, you know that I recently called out an iTunes review that

31:09.560 --> 31:12.000
I'm actually particularly proud of.

31:12.000 --> 31:18.040
In this review, a user that originally rated the podcast a two out of five based on their

31:18.040 --> 31:23.520
disappointment with the switch to the interview format came back and revised that review to

31:23.520 --> 31:28.520
a four, noting that the interviews were getting better and that the format was really starting

31:28.520 --> 31:29.920
to grow on them.

31:29.920 --> 31:32.320
Now don't get me wrong, please.

31:32.320 --> 31:37.480
I really, really, really appreciate those of you that left five star reviews on iTunes.

31:37.480 --> 31:40.640
And I hope the rest of you go run and do that right now.

31:40.640 --> 31:45.400
But it also felt great to see that in spite of his initial misgivings, the shows just kept

31:45.400 --> 31:49.160
getting better and the user eventually came around.

31:49.160 --> 31:51.920
That kind of feedback is great to read.

31:51.920 --> 31:56.000
Thanks to everyone who stuck with the show through the transition and I hope you're continuing

31:56.000 --> 31:57.760
to learn a ton.

31:57.760 --> 32:03.480
Please join the conversation by commenting on the show notes at the twimmalei.com website

32:03.480 --> 32:09.600
or by reaching out to me on Twitter where you can find me at at Sam Charrington or at

32:09.600 --> 32:10.800
Twimmalei.

32:10.800 --> 32:15.920
All right everyone, thanks again for listening and catch you next time.

