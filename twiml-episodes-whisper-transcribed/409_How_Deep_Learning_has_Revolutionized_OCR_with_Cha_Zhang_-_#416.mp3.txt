All right, everyone. I am here with Cha Zhang. Cha is a partner engineering manager at Microsoft Cloud and AI. Cha, welcome to the 20 AI podcast.
Thank you, Sam.
I'm really looking forward to our conversation before we dive into the heart of it. Let's get a little bit, actually, I'm going to, I'm going to start over if you don't mind.
We won't be doing much of this, but just so you know, if you at any point need to kind of pause or restate something, that's fine because we will edit that out in the back end.
All right, everyone. I am here with Cha Zhang. Cha is a partner engineering manager with Microsoft Cloud and AI. Cha, welcome to the 20 AI podcast.
Thank you, Sam. Nice to meet you.
Great to meet you as well. Before we dive in, I'd love to learn a little bit about your background. Tell us how you came to work in computer vision.
Sure, sure. I actually have been at Microsoft for 16 years. I joined Microsoft originally as a researcher at Microsoft Research. I was there for 12 years.
My research was primary applying machine learning to image audio video, all these different applications started a 2016. I joined on the product side and currently I'm working as a engineering manager and my primary focus is on OCR and document understanding.
Awesome. Awesome. So we will be focusing in quite a bit on OCR and some of your work in that space. And you know, I think people often think of OCR as a, you know, a solve problem, right?
It's, you know, we've been scanning documents and extracting text out of those documents for a long time. Obviously, the advent of deep learning, you know, changes things.
But I'd love to get the conversation started by having you share a little bit about what, you know, what's new and interesting in the space. How has it changed over the past few years?
Actually, it wasn't very long ago when people talk about OCR, you know, what comes out the mind was firstly scan documents in many people's eyes. OCR for scan documents is sort of a solve the problem.
More lately, I think there's two major development. One is with mobile first kind of word where everybody now have, you know, mobile phones and they take pictures everywhere. So there's a lot of demand to do text recognition out of images in the wild.
Certainly is a much more challenge problem than scan documents. And then technically, because of the events in deep learning.
We have realized that with deep learning, we can do OCR at a different level. We can make it a lot more accurate than before. And we can solve OCR problem in the kind of imaging the wild scenario.
So, I think it started 2000, early 2010-ish, I think there's a lot of big event advances in this area. And now we're seeing basically OCR becomes something really works.
People just don't need to worry about quality, et cetera, just mostly works.
Can you talk a little bit more about the challenges that arise when you're trying to do OCR in the wild?
Of course. So, I think for documents, usually it's white background and black text. But for images in the wild, essentially, it's a photo.
Also, in a photo, there's a lot of variations in the text. First, there's a huge scale variation. So some text, if you capture a picture of a street, there might be some store name that I super big.
And then there are some tiny text that's hard to see. So there's a big variation in scale of the text.
And the aspect ratio of this text can be really long, because text string can be very long compared to regular objects like a cat or a dog.
And because of the mobile capture scenario, usually it's difficult to enclose these text by and access align the rectangles. For example, you know, there might be perspective distortions of the text when the camera sees them.
And then, you know, the background in the image in the wild is much more complicated than the typical white background you see in scanned documents. And some of these backgrounds, such as fences, breaks and stripes, even though they appear quite simple for human beings.
And I think of like fences can be a perfect bunch of ones, you know, on the street sitting there and they look very similar to characters. And so those create additional challenges.
So one of the biggest one, I think technically for all CRS challenging is the localization accuracy. And so typically in object detection, the localization accuracy, if it's measured by intersection of a union.
That criterion is bigger than 0.5 people think this is good enough. But for OCR, if you actually the intersection is only half of the union, a lot of the characters will be missing. So usually OCR, we need a 0.9 0.95 level kind of accuracy in order to recognize all the characters properly.
Can you explain that in more detail, what is intersection over union and how is that used in perfect detection?
Yeah, so in order to measure the accuracy of a particular detection algorithm, you need to ground choose label the data. And so typically what people do is they create a bombing box object to be detected.
And then you use a automatic algorithm to figure out where the object is, and that will also create a bombing box. Now you have two bombing boxes.
And the question is how do you measure how well these two boxes align and a common measure is to take the intersection of these two bombing boxes.
And you take the union of these two bombing boxes, you get two areas, and you can imagine if the two bombing box are very close to each other, overlapping a lot, then that intersection of a union would be very high.
But if they are offset by quite a bit, then number is low. So that's kind of academia standard, how people measure detection accuracy with this criteria.
And so the, you were saying that the threshold that you need in the case of text is much lower, you were saying higher, much higher is a point nine, yeah, much higher.
And so the, so the threshold is higher because of what?
Because let's just think about, you know, you have a ground to text, let's say, hello word, and it's elongated rectangle. And you say, I have a text detection algorithm that creates also a bombing box, but have a intersection of a union, let's say, roughly 0.5.
And so what that means is that the intersection area divided by the union of the two bombing box is 50%.
So very likely the detected bombing box will miss a few characters because, you know, the overlapping is not there.
And so you might be missing an edge, you might missing a D at the end. And all this will cause the OCR to produce wrong results. And so that's, that's amazing challenge here.
So in the case of a traditional object detection scenario, you know, you may miss a half of the face, but you can tell that there's a face there.
In the case of OCR, you're just missing letters and it makes it a lot more difficult for the algorithm to guess what was there.
That's exactly got it.
And maybe taking us a step back just to the problem as a whole, the, you know, what what's granted mobile is driving that, you know, this transition to these in the wild, you know, these in the wild pictures and people trying to OCR them.
But what, what are the other, what are the high value use cases there? Like is it, you know, I'm thinking of some interesting ones is like the, you know, the, when it's in conjunction with translation, you know, maybe I'm in another country and I'm, I've done this, you take pictures of words and another character to try to read the menu or something like that.
And I've also done things like, you know, scan documents on a phone and you might want to OCR those, but those are that's kind of back to the traditional OCR problem in a lot of ways.
What are some of the other use cases that are common?
Yeah, so if you look at this kind of business opportunities, I still think the traditional document, you know, scan document, I think some traditional kind of OCR problems are like, for example, receipts, where people can scan in the old days, but nowadays people mostly do reimbursing by taking us and snapping a photo.
I think in terms of the market, the revenue, I think that's still quite a big one. There are a few others, the one that you mentioned, if you have a phone, you go to a foreign country, you snap a photo and you want to translate and that's one.
There's also a lot of applications in digital asset management, and so this is when you either you are a big company or you are a personal kind of you have some big storage of photos and where you want to organize these photos.
We have shown that, you know, with OCR capability, you can increase the accuracy of processing these photos and retrieve these photos.
As a matter of fact, you know, the big search engines like Google and Bing, when they search images, OCR is integral part of that as well, because the OCR content can help a lot in getting the best images.
And so you were mentioning kind of some of the technical challenges and localization of the text in these images is one of those challenges.
How do you go about it? Is it the case that deep learning is so powerful off the shelf, deep learning techniques just solves it for you or do you re-engineering the whole pipeline? How do you approach that?
Yeah, so in the text detection, usually the detection pipeline is different from a traditional object detection.
What's been most popular for kind of OCR for imaging the wild today is something called anchor free detection.
So in typical object detection, usually the most well-known algorithms like fast RSTN, faster RSTN, et cetera, they basically create these anchors and then they regress the actual bombing box of the objects.
The advantage of using that kind of approach is that these anchors need to be preset. And so typically for normal object detection, you set at a certain density and then you set a certain set of aspect ratios.
So your anchor box are one to two, one to three, one to one. Typically you go about there. But text as some of the tech can go like twenty to one. So really you cannot, it will be a huge complication of cost to go with anchor based approach.
Modern days for OCR we go anchor free and high level concept is essentially by using convolutional neural networks, you almost do kind of a per pixel level decision or classification saying, well, this region nearby this particular pixel looks like part of text.
There's a text non-text classification almost kind of per pixel level. And then you rely on a few algorithms to group these into text lines by looking at how well to, for example, to text the region are similar to each other and you can decide, well, these two looks like the same texture same color, maybe they should be connected.
And in this regard, there are quite a few well known algorithms to do this connection. Earlier days, people use a relatively kind of rule based approach like segment where they link based on some features, but it's kind of rule based.
More recently, people start looking to new networks like relation network. So they're kind of estimating the relation of two regions of features. And based on that to decide, well, these two should be connected or not.
So that way, you started to kind of bottom up, you start with a perfect, so kind of classification, and then you do grouping and you come up with text lines. Very powerful approach, they cannot only detect kind of straight lines, but even curved lines, you can handle them pretty well with those approaches.
And so it sounds like you're describing a pipeline that's not like, you know, end to end train single network that you give it images and train it on, you know, label data and it is, you know, telling you what the text is, but rather a bunch of independent steps.
Yes, that's a very good observation. Actually, so for OCR detection is only the first step, and after detection, we typically run a character model where you take the detected text lines, you normalize them into a straight line with fixed height, and then you run a character model to actually decode the image into a character list of characters.
And there are a lot of the approach actually similar to speech, where, you know, speech is going from acoustic signal to these texts, but here we're going from image to text, but a lot of the approaches that we use, like LSTM, language modeling is very similar.
Now, your question is certainly valid, because in speech today, you know, people do end to end training, you start from audio signal directly go to text.
For OCR, we are now the year yet. I think the main challenge is, well, first is, like, how much data you have. I think speech you can collect a lot more data compared with OCR.
OCR data are usually very expensive to collect in a label, and so going stage by stage at this point is more economically doable than, you know, do end to end training.
And why is that? It seems like we have tons of, you know, pictures with words in them that we, you know, that we know, particularly, is it, is it just of the in the wild?
The, in the wild examples where we don't have the label data or is also this document use cases, because I'm imagining, you know, Microsoft has probably, you know, labeled a ton of receipts and business cards and that kind of thing.
Yeah, I think certainly labeling is very, very expensive. And for Microsoft, we are company paying a lot of attention to privacy, you know, those kind of issues and collecting OCR data has been a major kind of, I would say, a blocking issue to go for this kind of end to an approach.
Because if you think about a lot of the document that we actually care, like if you say talk about invoice, talk about receipts, business card, they all contain PI information.
And those are data extremely difficult to obtain, and we follow very strict kind of guidelines, how we can collect them, how we can label them.
So in some way, we are limited hit by these privacy restrictions, but we do respect those a lot. And so, we, as a result, you know, we're not going into any point.
Got it, got it, makes me think a little bit about the, some of the issues with neural networks, remembering data. So, for example, there are examples where you train a, you know, CNN and there are some attacks that you can do that will reproduce some of the images, you know, it's a some degree or another that the,
the model was trained on likewise with these very large language models, you can start to see some of the text that the models were trained on come out in the, in the output.
I would imagine if you were training end to end, at least, then that becomes an issue as well, and maybe, you know, more so than in the case of images, what, what, what's your intuition there would it be worse or are better than images.
I would imagine it will be similar, I would say. So, after all, you know, OCR, you come from image to text, but during the learning of this OCR process, language model is actually very helpful to help improve the OCR.
And so, for example, during decoding of these text lines into text, we use some of the like LSTM or, you know, basically these very popular language modeling schemes and certainly it remembers the contextual information of the language in order to help those are to recognize these text properly.
And so, I think when you go to end to end, when the amount of data that you use for training is among us, I think, you know, it's difficult to imagine for me, you know, we'll have similar level of data for training like Bert models or, you know, those,
CBT models are huge, huge amount of data, but still you will learn something from the text and they might leak into the model as well.
Along those lines, what enabled Bert and many of the recent innovations around language models is a shift from supervised to the semi supervised way of framing the task. Is there a semi supervised framing for the OCR task that makes sense?
Actually, for OCR today, we are not, although I think it's definitely a very interesting research problem.
I think Bert is a super nice framework for transfer learning, you know, you go from pre-trained model and then, you know, unsupervised, you can, in the image world, I think, transfer learning probably exists earlier in image than language.
So, earlier days, when we have image net, we train like a resident, those are already being used for transfer learning.
So, unsupervised kind of image learning is also, I think, is still ongoing. There's a lot of interesting projects going on.
Yeah, I think, for OCR right now, we're not there yet, I guess, one of the main issues for building a product like OCR to use some of these pre-trained model is the computational cost.
And I think this happens in language as well, Bert model, the GPT model, three like this, multi billions of, you know, parameters is very difficult, turn them into a product. For OCR, it's also, you know, we have the same problem, computational cost is very sensitive, we need to make it fast.
And so, we're using relatively small models and normally we train from scratch, transfer learning does show some benefit, but when the data reaches certain amount, we found training from scratch is perfectly fine.
Many when, when you have a certain amount of data to train from, then, yeah.
And when you have started, we are in the very early days when we started doing deep learning OCR, we actually rely a lot on transfer on distillation, that's teacher student learning, where we first train a big model, and then we gradually use teacher student learning to create a small model so they can run efficiently.
Nowadays, we have figured out that, you know, you can train these small models from scratch, the amount of data that we have on the order of, you know, hundreds of thousands and millions of images sufficient to train from scratch on some model and reach about the same accuracy.
Can you elaborate a little bit on that? Do you, are you saying that you need more data to train smaller models?
No, I'm saying that, so take bird has example bird is super beneficial for transfer learning because it has seen so many documents giving any new language task.
Presumably, your data is not much, there's not much data that you have to train this new task, and therefore leveraging bird where it has seen so many documents will help through transfer learning to transfer some of the knowledge that the bird has learned from this huge set of documents to the small kind of task, and so that it can reduce the amount of documents required to train the smaller task.
The same thing happens in image net transfer learning where, you know, if it's a resonant train on image net, you learn a lot of visual information from the image net data set.
Then if you have a tiny detection task like detecting a helmet, let's say, and you can do the transfer learning and you can use a very small amount of data set
to actually train a very good helmet detector. What I'm saying, what I was saying just now was that for the problem with OCR where, you know, it is certainly a very important computer vision problem, and, you know, every company who invest in OCR tend to collect quite a bit of data, not to the level of, you know, billions, but, you know, hundreds of thousands, like millions.
To that level data model data is sufficient that you do not need to go transfer learning. You can train the model from scratch and you get very good result.
Got it. Got it.
And so the, when you were using transfer learning, were you using models based on image net, you know, along the lines of resonance and others, or, yeah, okay.
And so you're, let's see, so the smaller models that you're training, are they the, you know, some of the traditional architectures that we've already, you know, brought up, or you building out new architectures for the models themselves for this specific problem.
Right now we're using some of the traditional models. There are some active research going on regarding searching the best effective architecture for OCR.
We are, we haven't seen convincing results yet, but I think that's a very active research area that we're still kind of looking to.
Especially when we're trying to make it smaller and smaller, you know, faster and faster. Yeah.
And when you say searching the best architecture for OCR, are you speaking using the word searching generally, like you have researchers are looking at different models and trying to find the best one for OCR, are you suggesting a domain specific neural architecture search kind of.
So that certainly can be applied to OCR and we will still exploring it, but I think it's a very promising direction.
Okay, interesting, interesting.
Earlier in the conversation, you talked about the one of the, the big use cases is some of this semi structured data that we want to extract information out of invoices is one example.
It's a recent demonstration or I guess it's actually product now of the mobile version of Excel or something you can take a picture of a grid grid like data and that will, you know, both extract the text and organize it into a spreadsheet.
Talk a little bit about the product that you're working on the form recognizer, which is doing something similar.
Yeah, of course.
So OCR certainly is pretty low level and it's, you know, other than some of the application I mentioned earlier, like digital SMN and photo management, you know, translation, you can directly use OCR, but for many customers, what they want is not just OCR, they want extract information from documents.
It's about, you know, I need to process millions of invoices. I want to extract render name, you know, due date, total amount.
Or if it's an MS expense system where you want to process all the receipts and, you know, either it can be a verification purpose, for example, like, okay, how do I make sure employees are not putting random numbers.
And they don't match with the receipt that's actually filed. You know, it's actually, it sounds kind of silly, but in the, you know, today a lot of the company do this verification manually.
So because of the huge manual amount of effort needed, they often can only do sampling. So you sample like 5% of these receipts to validate, but you kind of miss a huge chunk that you never even look at it.
So we are looking at this space and we're trying to build essentially two category of products. One is a pre-built setup product. And so these are solutions that works out of the box.
For example, it can be a pre-built receipt, pre-built business card, pre-built invoice. So these are basically you sending a image or PDF file, it will extract all the fields that you'll be interested in.
Another big category that we think are super important is customization, because you know, the pre-built may never fit every need. So we have a solution called custom form where we allow customer to basically send us a few sample images.
You can either label or even, you know, not doing any labeling, but we will be able to extract key very pairs out of these documents.
Again, we see this as a much closer to what the customers need and that's what form recognizes position as.
So we've talked about a bunch of the interesting technical challenges at the lower level at OCR. The form level, you know, is that kind of a packaging of OCR, does it have its own technical challenges to overcome?
So it has a lot of very interesting challenges. So one of the work recently is coming out from Microsoft Research, whereas targeting exactly this problem.
And so for just think about it, you know, the language, I mean passing these invoices and receipts are essentially sort of a language problem because, you know, you have this text there.
The challenge here is that these are images, so you run OCR on them. But unlike a typical language data set where you scratch from the internet, you know, Wikipedia, this, you know, basically have this ordering of these words already.
If these data are coming from an image, essentially, you can detect these text lines, but it's actually very difficult to define the read order of these text lines.
And ordering of these text lines by itself is a very challenging problem when you have images in the wild. Paper can be curved, you know, can be crunched, can be rotated, perspective, you know, all kinds of issues.
It can have background packs and all this. And so the particular approach that MSRA came out is called layout LM. It's a, it's actually a modified Bert model.
It's also a language model. But in addition to the language, we also embed to the information, like what is the XY position of the bounding box of the text.
So with that information train, actually, this is all can also be trained without supervision and supervised pre training.
We're able to learn this kind of spatial relationship in these invoices without coming out with explicit read order.
And with that, we actually can do a lot of this key value extraction really well.
And there's also quite a lot of advanced research looking into, say, relation networks where you see two text lines nearby each other.
You can predict the relationship again, similar to the OCR where you have this bottom pixel level classification on the group them here is you want to group key and a value pairs.
And there's also a lot of advanced research in this graphical convolution networks where you do convolution networks over graph where the graph is defined by connecting nearby text lines.
Again, this is approach without requiring read order, but just look at the spatial relationship. And so these are all actually very exciting kind of extension of language, but also using visual information to help passing these vertical data more accurately.
Yeah, I think it's, you know, I at a quick, you know, quick thought would have imagined that, you know, maybe the top part of the stack there was more rule based than the bottom part of the stack was, you know, more machine learning base, but it sounds like they're even, I don't know, you know, relatively, but there are a bunch of really interesting.
We're doing another problem she learning stuff on the top as well as well. And I'm imagining the, you know, when you talk about relation that, for example, on an invoice, you could have, you know, date and then the date, you know, horizontally next to it, or you can have date and then the date beneath it.
You may have an address box and then a bunch of text that comes beneath it, and it would be nice to know that, you know, we're talking about the address here, that's that's part of the idea of a structured text extraction.
So in that you mentioned relation net and graphical CNNs are those two approaches to solving the same problem, or are they solving different aspects of the problem.
And they can be also used to solve the same. I mean, right now the main focus for us, for example, are extracting key value pairs. And this is both kind of pre-build and customization.
Think about, you know, if it's invoice and you want a vendor name.
So, and it's a name, certainly, you know, the text information, because, you know, you see this is, it looks like a vendor name, this probably is a vendor name.
You know, some invoice doesn't even have the key in the invoice, like you don't even have the word vendor name there.
So how do you figure out this thing is still a vendor name? So there, you rely on information that's language, and that's also kind of how the documents laid out, like, okay, the font size may matter, you know, the position of this thing may matter.
So we are looking to combining all these information to come up with a better decision on those fields.
And so how does a graphical representation or a way of thinking about the document, get you to a solution to, you know, these kinds of problems, you know, for example, the unlabeled vendor name.
Yeah, so, so the graphical kind of approach is basically, so you get a bunch of text lines detected by the OCR, and you connect these text lines with their neighbors.
And you define basically how strong these connections are. Actually, it's not defined, you actually learn these relationships by looking at the text, looking at their relative positions, like looking at their font similarity.
One, one issue that you actually just mentioned was like addresses, you can actually have multiple lines of addresses. How do you know they actually belong to the same address, right? So there's this kind of all these side information could be very helpful in determining that they should be grouped together.
And in the convolutional kind of graphical model, you learn a convolutional network by computing from all the labeling notes, where each note is a text line to aggregate basically at the center node.
And so basically, the model learns by not only looking at the current text line that's in focus, but also look at all the nearby text lines and decide, well, given all these contextual information, it does look like this is a vendor name.
I guess that's a very high level conceptual description of why it would work, but it's a it's a it's a data driven machine learning so that the model learns and learns or seems.
Yeah, as you're solving problems like this, are you often needing to relabel your data set, for example, imagining, you know, early on in developing an algorithm like this, you have a bunch of invoices and you, you know, draw a bounding box around the addresses and you say this is the address.
And you say, oh, while the font information, you know, as a whole new data set, you know, so you have to label, well, this is, you know, are you going in and having people label Helvetica versus Ariel, that seems a bit fine grain and hard to actually get experts to label, or is it more abstract than that.
And we usually only label the end goal, which is the key value pair, which is the fields that you're going to extract. So, for example, you want to extract a vendor name vendor address.
Total tags, you know, these basically draw a bounding box in those regions and use that as the as a ground choose data. Now, when I say, yeah, I think we're going to the we're going to the same place when you say font.
Actually, it's in some way implicit in the sense that we're taking these bounding boxes, we extracting image information.
So think of it as, let's say, run a convolution network to extract a feature of that part of the text region, text line.
This feature is essentially all the visual information that can be helpful in deciding or determining the relationship between text lines.
And so if features are similar, that probably mean they are similar font, they are similar size, you know, so those kind of.
So yeah, I think that seems to be efficient. Yeah, sufficient to kind of.
So you're not trying to kind of feature as your underlying images into these distinct things.
What I inferred when you said font, but have you, do you look at the, you know, is there an analogy to.
Looking at the layers of the network and, you know, you know, when we do this with CNN, you see like textures and things like that.
Is there some analogy that you've seen and looking at the layers of, you know, the network that says, oh, this layer is like, you know, identifying fonts.
No, we haven't been going there yet.
It's certainly interesting to look at it.
My take is most likely, you know, finally is just one attribute.
There are many other things.
And yeah, I think it'll be interesting to look at these features visually.
We've talked throughout the discussion about kind of the ways that OCR and this form recognition problem kind of blends the vision domain and NLP domain and, you know, language models has come up quite a bit.
Is there a little bit more kind of depth we can go into there, some of the ways that you see NLP.
And in particular, the advances in NLP over the past few years, kind of influencing the problem and the way you solve it.
Yeah, we certainly see NLP plays a very important role in these verticals, you know, after all, these invoice receipt, business card, you know, these are human artifacts.
They're kind of language artifacts in some way.
And so all the kind of latest, the state of the art in language modeling, we definitely want to leverage them.
The thing I mentioned earlier, like the layout or M, it's one way to leverage them by using the language model, but also embed additional visual information, hopefully to solve these problems effectively.
Because it's input is really different, right, you know, the bird is like you take text is input here, we're taking a bunch of text lines with the 2D locations and bounding boxes as inputs and the algorithm can naturally kind of solve these problems.
And is it also trying to do the traditional language model predicting the next character or word or, you know, set of text.
The way we train them are very similar, basically, you know, mask text, but you mask some words in trying to predict.
That's certainly you can use a lot of others, I think, you know, like I know recently people use translation targets, you can use auto variation encoder kind of targets.
And you know, this is a really active research area at this point. I don't think I think we're still just scratching the surface, although we already seen very, very promising results.
And so we definitely want to look deeper into this and see how well this really can push the state of the art.
But kind of continuing on that thread of the, you know, active research areas and what the future holds in this area.
What are you most excited about in this domain of, you know, OCR and in general, you know, extracting text from documents, vertical applications and the like.
Yeah, I think, you know, we have been working on this problem for quite a while, but I think there's still a lot of interesting problems.
And only when we start to work with customers, we realize, you know, there are problems we haven't been able to solve.
I can just name one, for example, like table extraction. It sounds trivial, but when you actually look at the all existing tables in the world, you know, the simplest one are those with explicit cell borders where you have straight lines.
And, but in reality, these tables can have no cell boundaries at all. It can be mixed on top with stamps, you know, all these things that are kind of making the problem extremely hard.
And so that's just, you know, another one that is extremely challenging, but we want to solve another saying that I sort of briefly mentioned about earlier was the customization part of this work.
How do you customize to customers own data instead of, you know, having these pre-built?
Because, you know, inevitably you will have data that doesn't work with these pre-built models. And how do you allow customers to have a way to build their own models to still work?
And that by itself is a very challenging problem because, you know, asking customers to label a lot of data is painful, they don't want to go there.
And so, either we go unsupervised or we go with a very, very limited number of supervision data. And in such a case, how do we adapt our model so that it can work on this document that customer realized that the pre-built model has failed?
That's also a very interesting kind of research problem that we were looking into.
So, you know, in vision, in languages, it's low shot to learning, it's also, you know, now definitely applicable to the problem here as well.
In the case of some of the productized vision offerings, you know, Azure does this as well. The user is able to upload, you know, its own set of label data and kind of the results for object detection or kind of fine tune against the user's data set is.
So, do the OCR and form recognition offerings, are they, are you providing something similar?
Yeah.
Like, can you upload, can I upload my own invoices and you're doing some kind of transfer learning or, well, if you can't, if you are, what are you doing to take advantage of what the user's providing?
So, we do have a product called custom form, which allow customer to upload a few samples here. We usually say minimum five samples.
So, say you have invoices that doesn't work with existing models and so you want to solve the problem.
You upload five invoices with similar, either from the same vendor or kind of looks or similar in structure.
And we can figure out these key value pairs and extract them either unsupervisedly or supervisedly.
So, unsupervised means customer don't need to label anything. So, you upload five documents.
The information we're gaining by looking at these five documents is, well, these documents are supposed to be similar and therefore they're going to be a bunch of words in this document that actually is common across these documents.
So, this commonality help us to tell, well, this is probably part of the empathy form or the template of the form.
Well, the thing that's varying across forms are like, these are must be information customer has filled in as kind of different from sample to sample.
And so, with that information, we can actually extract key value pairs without any supervision or you need these upload five similar documents.
Of course, that works to a certain degree, but if you're still not happy with accuracy, we provide a way for you to label your key value pairs.
And so, here we have a UX where you can go and label the fields you care by essentially highlight the OCR text lines where you think this is a value I want to extract.
And then we actually learn a model out of five samples and produce a model that can be used by the customer to extract these values.
The accuracy is actually normally pretty high in the 1995 percentage range actually.
And so, when the customer does this, is this process entirely learned or is there a human in the loop kind of exception handling element to it?
So, I guess this is probably kind of a take a step back. I think all the products, OCR parts today. OCR hasn't made a significant advance, but if you actually care about the numbers, think about invoice.
If your total is wrong, it's really bad. So, what we recommend is definitely we recommend people to have an agent kind of backup.
And for all the products we offer, we give people confidence, right? So, how confident we are about the extraction of a particular value.
And different customers can choose their own threshold and have an agent to look at them.
But I think at today's accuracy, we don't recommend kind of stray through unless you are handling certain specific applications.
I can give you an example. For example, if you are verifying a receipt against like a receipt image against an employee enter the data.
So, there you can go automatic, right? Because if the OCR produced a different number than the employee, or you will need somebody to look at them anyway.
But if they actually match, then well, that probably means it's okay. So, at application, you can, you can, you can automatic more.
Got it. Got it.
And so, and the question that I was asking is slightly different, though, when you, you know, so say you've got someone using automated form recognition, and they have their five examples that they haven't been happy with.
And they submit that in through, you know, some website, our API.
Someone at Microsoft taking those and, you know, going, taking them manually through some process to try to figure out why they're not working, or they thrown into some training job, and then the customer's result gets better.
Okay. No, no, we don't look at customer's data. So, this is a fully automated product, meaning, you know, customer basically label these files.
All a API to train a model. And, you know, the whole process is automated.
And is it?
And so, under the covers, you know, what's, are they, does, are they kind of forking off their own model and the, you know, last few layers are getting cut, cut off and it's fine tuning, or is it more elaborate than that?
It's more elaborate than that. Underneath the hood, there are multiple steps.
We leverage a lot of information in these sample documents. For example, you know, as I mentioned earlier, there will be words common across these samples.
And those are very strong indicators regarding, you know, this might be part of the empathy part of the form where, you know, you probably think these are not so interesting to the customer.
And the transfer learning is, you know, it's certainly one way of doing that. We are, right now, we are actually trained in these models without transfer learning.
So, it's actually the model is trained from scratch for every very few customers able to do this. We're able to do this.
Because some very interesting work that we have done to basically augment this data to make sure that you have sufficient data to still be able to train a model out of, you know, five samples only.
And it's a, there's can be a feedback loop as well. So, you know, if customers not happy with a model trained by five samples, you can upload more and we just train a new model for you.
So, every time you, you try and just get on your model. That way, you know, it's a, it's a feedback loop where customers keep improving their model until it to a certain stage where it's, it's really performing for the customer.
And so, when you say augmenting the five that they're providing, or are we talking about data augmentation in the sense of a transformation pipeline that kind of changes, you know, adds noise rotates that kind of thing, or we're talking about you've got some other data set that you're adding to their five and training it on that aggregate data set.
And, and that's how you're producing a better model.
Both, although I think the lateral lateral one is more because, you know, actually when customer label these data, they actually provide, we ask them to provide some additional information.
For example, you know, if they label this is a date, we know it's a date. And so, in this way, we can artificially create more data to feel the form so that we can produce more data to train the model.
Also, we use the very robust machine learning algorithms that are robust to very few examples. And so that way we can, we can learn with this limitation.
Normally, if you look at many of the other offerings that people provide, you know, you have to train with hundreds of examples here, we're pushing really down to five and we hope to push even lower in the future.
And so does that mean that the, so I'm assuming that this is a stacked problem and you've got, you know, some low level OCR, for example, models that are trained with many, many documents and, you know, what we're.
What you're doing with the, this form recognizer custom data is more at the top end of that stack is the, is the off the shelf model that I'm using without the five example customization is that also trained on relatively few examples.
What do you mean I guess what I guess maybe I'll jump ahead to the conclusion that I'm drawing on what's what's confusing me is, how are you getting good better results with few examples if you're not using any kind of transfer, I guess I heard in your explanation that you're not any kind of transfer.
Right, so I guess, so right now the custom form support training model and these models are usually each model is geared towards one particular form type.
And so in some way you can think this problem is actually restricted. It's actually a easier problem. It's not like a preview invoice where essentially you want to handle all invoices.
Here we are handling one particular invoice coming from and I would say one particular vendor, let's say right.
So they usually use a template.
Got it. And so the customer then do they call a unique API to resolve invoices of this type or is that then ensemble and then there's something that decides whether it's of the type that you built the new model.
Yeah, so here's the kind of the recommendation that we give to customers. So, so you maybe start with the preview model and the preview model may work and then your job is done, you know, your IP go.
And then you certainly say you have a lot of invoices and out of a thousand, 10 of them doesn't work. And so while we offer the customer as well, you can take these invoices and you can train specific models for these 10 different voices.
You might need to train more than one model, this special model because these invoices may look very, very different. So imagine you can train like 10 different custom models for this. We actually also offer kind of automatic invoice classification.
So a API called a model compose where we can compose these 10 small models into one. So you need to just call into that one. And by calling into that one, we also provide you a confidence to say, well, because, you know, during testing, the customer send the invoicing, we don't really know whether it's one that doesn't work with this preview one or whether it's part of this.
It works well with the preview. So you send this invoice first to the customized version of the model. And we will tell you, hey, it doesn't look like any of the 10 you have trained.
And in this case, you will remember back say, okay, now I'm calling the preview invoice because you sort of know that preview actually works well for that. So that's what we recommend customers to do. Okay, interesting. It's a dug into a little bit of the detail there, but it's interesting to see kind of how the end end problem is put together.
And, you know, in case like this, the, you know, the ends of that problem are, you know, on the customer side, not just the service that you're that you're offering.
And so seeing how the, you know, how the pieces are put together is kind of interesting.
Awesome. Well, child, thanks so much for taking the time and walking us through, you know, some of the interesting things that are happening in these domains.
Thank you for having me. Great. Thank you.
Thank you.
