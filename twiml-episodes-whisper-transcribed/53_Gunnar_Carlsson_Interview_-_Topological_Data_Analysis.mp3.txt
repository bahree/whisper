Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting
people doing interesting things in machine learning and artificial intelligence.
I'm your host, Sam Charrington.
This show you are about to hear as part of a series of shows recorded in San Francisco
at the Artificial Intelligence Conference, which was hosted by our friends at O'Reilly
and Intel Nirvana.
In addition to their support for the event itself, Intel Nirvana is also our sponsor for this
series of podcasts from the event.
A huge thanks to them for their continued support of this show.
Make sure you check out my interview with Navine Rao, VP and GM of Intel's AI Products
Group, and Scott Appland, Director of Intel's Developer Network, which you can find at
Twimbleai.com slash talk slash 51.
At the AI conference, Intel Nirvana announced DevCloud, a cloud hosted hardware and software
platform for learning, sandboxing, and accelerating the development of AI solutions.
The DevCloud will be available to 200,000 developers, researchers, academics, and startups via
the Intel Nirvana AI Academy this month. For more information on the DevCloud or the AI
Academy, visit intelnervana.com slash dev cloud for machine intelligence.
In our talk, we take a super deep dive on the mathematical underpinnings of TDA
and its practical application in software. Nerd alert, all right, onto the show.
Hey everyone, I am here at the O'Reilly and Intel Nirvana AI conference,
and I've got the pleasure to be seated here with Gunner Carlson, who is the president of IASD.
Welcome, Gunner. Thank you. Great to be here. Absolutely. Great to have you.
So why don't we start with having you tell us a little bit about your background and your
areas of research? So I come from an academic background. I did my PhD in mathematics, and for
the first 20, 25 years of my career worked very much in pure mathematics. Over time, I started to
become more interested in how could we apply things that we were doing in the pure math side,
down a shorter time frame, because oftentimes the applications have a very long, you know,
long time to go. So I tried to do some things more quickly, and my main area within mathematics
is topology, which is the study of shape. In a generalized sense, one can talk about shapes
and higher dimensions, and so I wanted to apply that to the study of large and complex data.
It turned out it led to a lot of things, basically a big career change. It wasn't just a little
hobby thing. You know, we found that it was a very hot topic, both for the National Science Foundation
and DARPA, the research arm, or the innovation arm within DOD. In the middle of that, we spun out
a company, a Yazdi Incorporated, which is so commercializing the ideas coming out of there,
and other things as well. So that's kind of where we are. I live in the Stanford campus,
a math professor at Stanford, or a retired math professor at Stanford, I should say,
and they've got the three grown kids who are in the area, and so. Yeah, awesome. So I'm like a busy
guy. A busy guy, but I don't want to not be busy. Absolutely. Absolutely. So let's talk a little
bit about topological data analysis and topology in general, which was a topic of a tutorial that
you did here at the conference today. You mentioned the study of shapes. The first thing that comes to
my mind is like high school geometry and trigonometry, but I imagine it gets a lot more interesting
when you're talking about higher dimensions and lots of data. It is, on the other hand,
sometimes what you can do is get very simple, small representations of complex data sets by
the things, the simple things that you mentioned. So what we do is we represent data by network
model. So when you think about mathematical modeling, one often thinks about algebra, one
thinks about equations, one thinks about various kinds of equations and so forth. But maybe one
should try to model data by something else, maybe something with a richer output than just
equations. And for us, the output of our models is a network in the computer science sense that
is nodes and edges. And it turns out to be a very useful, compressed representations and data sets
for many different applications. Okay. Interesting. Now, when you say network and nodes and edges,
I think of graphs, is this graph theory we're talking about? Well, one can view it in that way,
but you know, a lot of times graph theory deals with very nitty-gritty local discussions of degree
and so forth. Right. You know, this is sometimes thinking of it as a higher-dimensional shape. So
for example, you know, a graph with four nodes and it might actually encode a tetrahedron rather
than just its edges. And so we kind of think in those terms. So yeah, it is graph theory in some
sense that in the sense that we study graphs, I would say that we do it in a way that's different
from what is usually meant by graph theory in the math side. It's more a lot's meant by shape
and topology on the topology side of math, if that makes sense. Okay. And so how does this all play
into machine learning? One of the big things about machine learning is that it's great, but many
people, including people like regulators, like MDs, you know, all the people that one might want to
use it with often regarded as a black box. They regarded it as something which, you know, although it
seems to produce good answers, they can't put their head around, understand where it came from.
And that means that sometimes there's some difficulty in, you know, making use of it for those
reasons. And so we view ourselves, we view the topological data analysis as, you know, a part of
the growing area of machine learning. You know, we believe that it produces richer models than just
simply classifiers or linear regression models that come out or clustering that comes out of machine
learning. And so, you know, it's just augmenting and helping machine learning develop over time.
That's how we view it. Okay. So I heard a couple of things in there. I heard one that the models are
richer, and I'd like you to explain or elaborate on why that is, but I also heard that you suggest
that this approach, the taking things from a topological perspective, aids in explainability. And
that's a, you know, a huge issue for, you know, the, the constituencies that you mentioned,
the regulators, but also, you know, a business that's going to depend on, you know, machine learning
or artificial intelligence, whatever we want to call it. You know, they, they want more than just
the box set it, right? You know, kind of walk us through, you know, the next level of what TDA is
all about and how it lends itself to achieving those goals for machine learning.
So let me talk about an example, you know, for the explainability part for the machine learning.
So, and let me say, by the way, so what would produce a network? It can be viewed as a map
of your data in a sense. And so, okay, you know, for us, we're working with a bank that had failed
the stress testing, seek our stress testing process two years in a row. They had failed it in
part, most part because they had produced machine learning models, which, you know, were
reasonably, which were predictive, but which were non-understandable. That is to say, they came
as a large vector of numbers, vector of coefficients, if you like, and regulators couldn't understand.
So the stress testing process is the bank basically has to say, I've got this much reserves
based on my risk, and I've got to justify that some kind of way, and they produced this model,
but they couldn't explain what the model was doing. That's right. It wasn't explainable enough
for it. That's correct. And so, for us, now, the model was actually based on a lot of
features, a lot of macroeconomic and other kind of global economic indicators.
And so, we built one of our models on that. And within that model, we found several hot spots
for a correlation with revenue and a business unit. And, but more than one. And so, for each one of
those, we might join one or two features from each of those hot spots, those groups. And let me say,
and so the hot spot itself turns out to be a large collection of, you know, of these economic
indicators, but they were understandable to the regulators. So we tell them, look, we have a
model with four features, okay, first of all, much small. And then each feature is representative of,
you know, some class of features, which are recognizable as similar or related by regulators.
And that is what we would call, you know, an understandable model, a low-dimensional and
annotation in terms of a group for each one of the features. Okay. I've got some questions.
So you started out by saying you built, you were talking about their model and you said,
you built a model on on that. Did you build your model on their model? No, sorry, no, no,
we're building separate models. We're building their data. Building, you know, very,
yes, on their data, but not on their model. That's right. We're building, you know, there, we
involved many hundreds or even thousands of variables. You know, ours was, is a small number of
variables. And each one is understood as being representative of a class of indicators,
all of which have strong correlation. Yeah. It almost sounds like what you're doing is
you're like semantically clustering the features. That's correct. And kind of ranking the features in
their relevance to the prediction. That's correct. But here's the interesting feature in this.
You might say, well, why don't you just take all the features and find the ones that are the most
correlated? Right. You know, why do you need them? Well, the reason is that
features are perhaps correlated with revenue for different reasons. And so you have different
groups of things, which are correlated in different ways. If you put them all together, you know,
you don't get nearly the same kind of explainability as you do when you have to separate them out
and understand that, you know, each one is representative of a particular class of things that are
similar. So that's the key thing there. So I get the example and I kind of get what you're doing.
But still, how do you explain the TDA part of that? Like to at the next level of detail,
what do we do? What do we do? Let me tell you. And again, this may get a little geeky,
but let's go ahead. We love geeky. We love geeky. So, you know, the starting point for us is
always a data set equipped with a similarity measure of some kind. So we encode that as actually a
distance function, mathematical sense, which is an abstraction of ordinary distance that we have,
you know, in the plane or in space. And is this is your distance function? Something that might be like
your error function or is it distance of the, you know, your rows or your points within the
data set itself? It's distance of one row to another. Okay. Yeah. And so, so, you know, a normal
thing is supposing that it were, that we really only had two coordinates, you know, two or two
features. Then my rows would be two vectors, you know, with two entries and I could compute their
distance regarding them as being in the plane. And that distance would be, that would be a
perfectly good distance that we could work with. Right. Now, the thing is that oftentimes for,
you know, for different phenomena with more features, by the way, those formulas for distance
and dimension two, they extend to any number of dimensions. So if I have a spreadsheet with numbers,
you know, it doesn't matter whether I've got, you know, five or ten or even a thousand
fields, it's all for good. You can go ahead and compute with it. But supposing that you're in a
situation instead, you know, like you are in the study of genetics, where you have long sequences
and an alphabet of symbols, you know, you want what you might do is you might take the two sequences
and say, well, how many spots do they differ in? I keep a count of that. And that is a distance or
similarity measure as well. That's one that wouldn't be using that context. So in fact, you know,
for us, there are sort of many different choices of these distance functions, there's libraries
of them and so on. But, you know, I've just given you kind of two important ones. The first one
would be called Euclidean or general Euclidean. Second one would be called hamming. So it's called
correlation angle and cosine and so on. So there's a lot of variety of them. But the idea is always
to get at some notion of similarity of data points. So where the distance is small, we regard the
data points as similar. And if they're far apart, we regard them as dissimilar. So maybe let's take
this back to your example with the bank, you know, given a data set that consists of macroeconomic
factors and transactions, perhaps and portfolios and the like, like, what does a distance mean in
that context? So all those things though are their numbers, their hearts. So this is really just
a spreadsheet. So I could do the Euclidean distance there. I think there's a variant on Euclidean
distance, which is called, you know, variance normalized Euclidean, which means that if you've
got some variables that have much larger range than others, you might want to make those ranges
the same so that the one variable doesn't swamp the others. But fundamentally, it would be the
first one that I talked about, you know, in the notion of Euclidean. Yeah. But I guess maybe the
question that I'm asking is does Euclidean distance or any distance, I guess, in the general case,
have like a semantic meaning in a highly high-dimensional data set, or is it just the distance between
points and data set? You know, I think it does. It's not that one justifies it in terms of
semantics or theory, but what one observes is that, you know, it does typically coincide with one's
notion of similarity. If it does not, then, you know, maybe this is a data set for which some other
metric is, or distance is more usable. Again, it's more what you actually see in the data,
it's not about the theory that says, oh yes, you know, this is the one you want to use.
Okay. Got it. So you define this distance metric and apply it to the data and then what?
And then what? So now, what we want to do is, well, we have a projection of the data set,
which, and I won't go into detail on that, Scott, but basically what we do is we find we've been
the data set into overlapping bins. We do that in a systematic way, and it has to do with a
projection of some kind. And once that's done, we perform a clustering step within each of those
bins. Each cluster is now made the node of a network, and we connect to nodes if they share a
data point. So, you know, that's a short version of it. But it's a kind of what we call partial
clustering. They say we don't apply clustering to the whole data set. We apply it to a bunch of
pieces, and those produce points for a network or nodes for a network. Okay. Makes sense.
It does. It almost, it makes me think of a number of things, things like word embeddings.
It makes me think of things like, I don't even know what, I don't remember the general
terminology for it, but there's a company called cortical or nementa. Like they, they do something
similar to word embeddings. It kind of evokes that for me, but it also evokes for me like a
convolutional neuron that where you're like a windowing your bins or kind of like your
convolution windows that you're moving across your image. It is a little bit, I think they're,
actually, I think there are a lot of connections with that. We're just starting to develop those
now. So, you know, it's a slightly different, it goes through a part of this whole TDA business,
which we haven't talked about, which is about measuring shape through what's called persistent
homology. And, you know, that's, this is a very kind of, it's always been regarded as the most
esoteric part of mathematics for reasons that are kind of quite necessary to it, but nevertheless,
it's very powerful. It allows you to measure shape. It allows you to say, look, is there a loop
in your data? Is there a sphere in your data? You know, are there connected components in your,
all those kind of things that we think about? It allows you to actually measure those in a formal way.
So, this, this last step you describe your, taking your, your bins and I, I heard that is a
windowing kind of effective. It's kind of, it's key that the bins be overlapping. In this case,
that not that they'd be this joint is key that they'd be overlapping. Okay. Because we want the
clusters to have the ability to overlap so we can draw edges between them. Yeah. Okay. Yeah.
So, then tell me where you, what you do with your distance metric once you have these
bins. Actually, now we, that's how we use, that we only use the distance metric so to be able to
get the bins. Okay. And perform clustering within the bins. Got it. Once I've performed clustering
within the bins, you know, at this point, I can, for the time being, shelve the, the metric and say,
look, the representation I'm interested in, which can be thought of as a generalized Venn diagram.
If you like, you know, is this network model and this network model is something that we now want
to examine. Okay. All right. So let's talk a little bit more about that examination process. That
sounds like that's what's next. That's right. So, so let me say, by the way, first of all, that I'm
going to, what I'm going to describe is sort of the way to sort of interact with the model,
you know, visually and on the screen and so on. But one can also interact with it programmatically.
And that's what one wants to do to build applications, ultimately. But, you know, for some kind of,
some manual data analysis, what one does is one puts the network on a screen through a layout
algorithm. And now there's lots of things that capabilities that you have, you're able to select
parts of the network the way you would in Photoshop or Illustrator. And once I do that, I can
make that into a set of data points because the nodes correspond to collections of data points.
Okay. So now I have new sets that I can either perform other analysis on or I can ask for their
explanation. That is to say, what is, what are the features that characterize this subset?
And that's done in an appropriate mathematical and statistical sense. You know, there are some
choices on that, but we've made one particular choice. But now what's that choice? And one of the
same choices. Well, the main thing is that we're deciding we're selecting, you know, we have
distributions on the group of a particular variable on the groups. And the question is to choose
those variables, which are maximally different in terms of a so-called Comma-Gorov-Smirinov
distance on distributions. I'm just saying that I think there are other notions of distance on
distributions one could use to list those. And so when you say you've made that choice, you mean
in a given use case, you've selected one of many or the company's approach. The company's
approach is that one. Got it. That's right. Okay. That's right. But it tends to do a good job of
maximizing the distance for the cluster problems that you're going off. It does. We find that again,
yes, we find that the explain capability is quite useful. It works well. Okay. Yeah. Okay. Yeah.
So anyway, so that's something one can do with it. You can actually color by quantities of
interest. So if I'm interested in things like revenue new or survival or whatever it is,
I can color a node by the average value of quantity for each of the data points. And so that
becomes quite informative. And often what you see in the network is collections of hot spots,
you know, where some values high and more than one place. And it turns out that they're different,
they're high there for different reasons often. And that's what's quite why the network is so
informative. Because otherwise you would just take any aggregate you would study this quantity. And
since you're then, you know, putting together all the different ways in which that thing goes
high, you can't understand. You can't make sense of it in the way that you can when it's
split out like that. You've mentioned that that's this kind of ad hoc interaction with the
model is just one of one way to interact with the model. But the way you describe that makes me think
of use cases like forensic types of use cases like I associate with a company like Palantir. Do
you overlap with with them and the types of use cases you go after? Yeah. So, you know,
I think the answer is we don't fully know the interconnection. We know, I mean, what they're doing
is to sort of searching data that comes as a network. You know what I mean? Whereas in our case,
we're saying, well, actually all data can be represented as networks. And it provides a compression.
I actually think there are connections there between those things that could make things efficient. But
wouldn't want to speak to them because I don't, sorry, I don't have firm ideas in mind.
Do you tend to find yourself pursuing a lot of forensic types of use cases? Or, you know,
at the moment, we have been focusing on, you know, health care and financial services. We are
moving back into government in various ways. And so we may very well hit that. Okay. So we talked
a little bit about this, the ways that you can interact with this network that's created out of the
data. How else can you use these models? So, you know, another thing that one could do is suppose
that you have a linear regression or predictive model. Most likely, you know, it's gotten by
optimizing something and some kind of error function. But it's probably not also not perfect. It's
probably also the case that there are some areas within your data set, you know, some particular
phenomena that happen that make that there are some systematic errors that happen. You know,
you can't correct them within your own model and with the features that you've got. But what it
allows us to do is it allows us to say, let's take a network and let's color it by that model error.
Maybe we find some hot spots for model error. And maybe I try to correct around those hot spots
by adding features somehow. So that's another point of view. I would put it under the general
heading of model diagnosis and model improvement. So that's another situation. Interesting.
You mentioned health care. What are some of the use cases in health care? So we have something
called clinical variation management, which, you know, helps study both finding new and optimizing
care pads for particular, you know, procedures as well as tracking adherence to them. We have a
population health kind of application that is working on trying to understand trends in
population health. Who is going to go to the 5% group who are the most expensive? Who, you know,
is getting these on track to go bad and how can we improve their chances? That's a couple of
things. There are some on the financial side as well. We work with hospitals as well as payers
and the, you know, providers as well as payers and the health care side. And can you maybe just to
kind of tie all the terminology together? Maybe pick one of those examples and walk us through,
you know, what the data tends to look like, what the clusters might represent, what are some of
the findings that someone might see? Yeah. So let's talk about the clinical variation management,
for example. So, you know, the data there consists basically in all the events that happen during
the course of someone's stay. Say for some particular surgical procedure, lie in the replacement,
like bowel surgery and so forth. And so, what one can then do is one needs to put on, you know,
that set of things, some appropriate similarity measure and that, you know, distance function.
And that turns out to be, you know, quite a tricky, interesting problem. Probably maybe the key
part in solving that problem. And then ultimately it produces for us then sort of a consensus
care pass, maybe a few hair paths that are very good and one consensus together with some explanations
of what are the key features that differentiate it from others. So it's almost allowing you to
identify, you know, which outcomes or which features of the care, if you will, kind of correlated
with success and, you know, maybe where some outliers are. And the features might be, you know,
what drugs are administered, what doses they're administered, when they're administered,
things like that. That's exactly right. And just to give you a sense of how it can work in one
situation. When hospital systems are deciding on care paths, what they usually do is they get
together sort of the people, the smartest people that they can say who are working on this and they
get together in a room and they discuss it out and, you know, ultimately come to some kind of
answer about what it should be. There are a couple of problems with that model, you know, you may,
and maybe you haven't found all the best people who are doing this procedure. Maybe you haven't
chosen exactly the right group. And anyway, it's also the case that when people are just sort of
arguing things out in a room, sometimes, you know, it's the strongest personality rather than the
strongest case that comes out. So there's all sorts of issues with that. I think there's like
implicit versus explicit knowledge, right? I mean, there could just be things that some people do
and they don't really realize that that's that they're doing it different from the other doctors.
And so they don't know to argue it. Exactly. And then so in fact, that was, you know, what happened
for us with one of our customers was, you know, exactly some of that. We found that for one
surgical procedure, there was a group, a small group kind of out in the periphery of the system
that people hadn't really observed so much, but they were doing something that had good improved
effect on length of stay. Okay. And so, you know, so that was found. That was, you know, quite an
important contribution to them. So even just in terms of kind of a search thing like that, it was
quite quite useful that way. Interesting. So if someone wants to learn more about this, it sounds
like there's, it sounds like topology is, you know, an interesting place to start. Like what,
is there a canonical paper or a reference? Let me warn you that of course, if you go to study
topology, you'll be involved for years before you get to the case. So, so I wouldn't necessarily,
I mean, one could certainly read some things about it. But what I would say is, well, first of all,
our company has a lot of stuff on the web, on its website, basically, you can sort of acknowledge
Center a lot of technical papers and somewhat less technical as well. So I would kind of recommend
the reading survey paper route as opposed to, you know, taking a textbook and kind of chugging through.
Because this is a newly developing subject. And so, there are some textbooks in this persistent
homology side that I talked about. But, you know, the general notion of topological modeling,
you know, I think we have a lot of stuff on our web creating it. But actually come to think of it.
Oh, here we do have my colleague, FX Campion, Francis Campion, and I have written a book,
which is called Machine Intelligence for Healthcare. And so, it's available on Amazon, you know,
and I recommend that it's got the first half is kind of a discussion of this mathematical modeling.
And then the second half is specifically, how does this work in healthcare?
Okay. So, that sounds really interesting. Yeah. And did you elaborate on persistent
homology? Or did we, you know, I think we had to get, it's, let me just say that it is, it is a
very interesting way of sort of detecting shape features, certain kinds of shape features in the
data. And as it is, you know, on the pure math side, it detects features in, you know, in regular
spaces, spaces with complete information and where you've got the whole, the whole thing.
It can be used in two ways. The one way is it can be used as a way of recognizing what the
overall organization of the data set is, you know, is it, we found that, for example, in studying some
image processing data sets that, you know, the frequently occurring phenomena in three by three
patches lined around, you know, one circumstance, the circle in a slightly higher, you know,
level, understand around a mathematical object called a climb model, which is, you know, very,
it was very interesting for us. We used it for understanding image compression and also texture
recognition. So that was quite interesting. The second thing, though, is that it can be used,
and this I think is going to be a much more rapid application is where it gets used to generate
features in unstructured data. So when you have data that is complicated, but that somehow carries
a notion of distance on it, like molecules, you know, where the atoms can be regarded as the
points in the distance has to do with the bonds, then you can attach, so-called persistence
barcodes to those points, and that's quite useful in organizing and understanding, you know,
those kind of unstructured databases, databases of unstructured data. Interesting. You mentioned
earlier, and I saw it in the description of your session as well, something that I think is
related to this, like identifying loops in data. What does that even mean, loops in data?
Well, imagine, you know, I had a slide, imagine that you have a picture, you see your data,
suppose the data is actually in 2D, and supposing that you've got a bunch of dots,
so the data is a bunch of dots, and it looks like it's kind of surrounding, like it's a circle,
we see it as a circle. Right, right. So something that like a clustering algorithm
doesn't really know how to deal with very well, but you can identify this higher order primitive,
that hey, this is a, like a geometrical primitive, essentially. Exactly. That's exactly right,
and that's what we're trying to do. We're trying to mimic that fact. You know, we know what a loop
looks like, but we don't know what it is our brain does to recognize that, and so therefore,
you do this homology. So imagine that you're trying to understand how do you recognize a letter A
from a letter B. Right. That letter A has two loops, sorry, a loop and two legs, and the B has two
loops in it. So if you can find something that counts for you, the number of loops, you're going to
be able to characterize letter A from a letter B, you'll be able to differentiate it, and you'll
be able to differentiate it in a way that's font independent. That's independent of the fact that
you see it from, you know, one from an angle, perhaps, or that it's sitting in the surface of a
soccer ball. It's kind of, it's miraculously kind of deformation. It doesn't, it doesn't, it's not
sensitive to those deformations, and that's what homology is. And so that sounds promising,
but it also sounds, I guess I think about it in the context of deep learning, right? A deep learning
purist would say, well, you know, it's going to be a lot easier to just throw tons and tons of data
that have like all different kinds of Bs, you know, and A's, and just let the network teach it.
And I've had this conversation with some folks that specialize in deep learning around
combining other approaches to create higher level insights with the deep learning. And one of the
entrances, I just throw the data at it, and I think I'll figure it out. Of course, what you find is that,
you know, they're adversarial approaches to that way, you know, so, you know, even for something as
simple as MNIST, the MNIST dataset, which is of hand drawn numbers, where you find that if you
just mess with the background a little bit, in a way that, you know, people wouldn't see the
difference. People will see that, you know, a one is a one and a two is a two, but it messes up
the deep learner. Oh, yeah, absolutely. And that's a feature question. You see, it's doing a certain kind
overfitting is perhaps the wrong term, but it's focusing on some features that have to do with
the background that are not really relevant. And so to the extent that you can feed it features
that are kind of background independent like that, then you're in good shape. And that's what persistent
homology is a perfect tool for providing features to a deep learner, because in fact,
the output of the persistence thing can be regarded as an image, so it kind of fits
directly into that. So, yeah. Is this an application in like theory, in theory, in principle,
or are there demonstrable situations using, you know, MNIST or some other dataset that says
persistent homology outperforms deep learning or has some cost benefit analysis relative?
Well, remember, it's not outperforming deep learning. It is feeding into deep learning and
using it. So, the example I would point to. So, we're using the persistent homology to create
features that either replace the raw images or augment the raw images, and then we're still
using deep learning to learn. That's correct. Got it. That's correct. Now, again, most of this is
sort of looking into the future. However, this exact thing has been carried out by, you know,
a friend of mine or a colleague of mine at Michigan State in a gooey way, who has taken databases
of molecules, you know, candidates for drugs, you know, and kind of drug discovery, and build
persistent homology barcodes on them, and then use those, use deep learning on those,
extremely successfully. Okay. All right. I'll ask you afterwards for the spelling of that name
so we can... Sure. I'll write it down for you. Yeah. Included. That's right. Awesome.
Well, it was great to have you here. I learned a ton, and I feel like there's so much more to
learn about this stuff. There's a lot to learn. I feel that way every day. So, and you know,
thanks very much. I enjoyed the conversation. Great. Thanks a ton. Sorry. Thank you.
All right, everyone. That's our show for today. Thanks so much for listening, and of course,
for your ongoing feedback and support. For more information on Gunner and any of the other topics
covered in this episode, head on over to twomolei.com slash talk slash 53. For the rest of this series,
head over to twomolei.com slash AI SF 2017. And please, please, please send us any questions or
comments that you may have for us or our guests via Twitter, at twomolei or at Sam Charrington,
or leave a comment on the show notes page. There are a ton of great conferences coming up
through the end of the year to stay up to date on which events will be attending. And hopefully,
to meet us there, check out our new events page at twomolei.com slash events,
twomolei.com slash events. Thanks again for listening, and catch you next time.
