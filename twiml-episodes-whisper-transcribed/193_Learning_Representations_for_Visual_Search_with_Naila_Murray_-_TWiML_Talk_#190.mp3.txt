Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting
people, doing interesting things in machine learning and artificial intelligence.
I'm your host, Sam Charrington.
In this episode of our deep learning endoba series, we're joined by Nila Murray, senior research
scientist and group lead in the computer vision group at Naver Labs Europe.
Nila presented at the endoba on computer vision.
In this discussion, we explore her work on visual attention, including why visual attention
is important and the trajectory of work in the field over time.
We also discuss her paper Generalize Max Pooling and her recent research interests in learning
representations with deep learning.
Before we jump in, I'd like to send a big shout out to our friends at Google AI for their
support of the podcast and their sponsorship of this series.
Google AI recently opened up applications for their 2019 residency program.
The Google AI residency is a one year machine learning research training program, with the
goal of helping individuals become successful machine learning researchers.
The program seeks residents from a very diverse set of educational and professional backgrounds
from all over the world, so if you think that this is something that interests you, you
should definitely apply.
Find out more about the program at g.co slash AI residency.
And now on to the show.
All right, everyone.
I am on the line with Nila Murray.
Nila is a senior research scientist and group lead in the computer vision group at Naver
Labs Europe.
Nila, welcome to this week in machine learning and AI.
Thanks so much.
I'm happy to be here.
I'm happy to have you on as well.
And before we jump into the kind of heart of the conversation, I'd like to have the audience
get to know you a little bit, you did your PhD in Barcelona, Spain.
Tell us about your focus there.
Sure.
So I did my PhD at the University of Tata Aut√≥noma de Barcelona, so specifically in the
computer vision center in Barcelona.
And so the work I did there was very focused on subjective vision.
So this is the problem of being able to model subjective properties of human vision.
So that's things like, in particular, was focused on visual attention, in particular,
bottom-up attention or what we might call saliency.
So this is really understanding if you're given some kind of, if an individual is presented
with a visual stimulus, let's see an image or a video, how can we model which regions
of that visual stimulus would attract the user's attention.
And then we can have various degrees of attention that we can predict.
So that was one focus of my research through my PhD.
Another focus was on visual aesthetics.
This is another, let's say, subjective visual experience where you are trying to model
or at least trying to understand or predict, you know, to what degree with a specific visual
stimulus, let's say, an image or a video be considered visually appealing to someone.
And so you can consider different ways of approaching that.
There are some that are, let's say, some approaches that are a bit more in the computational
neuroscience perspective, really taking a sort of biologically inspired approach.
And I did investigate that to some degree, and they're also much more data-driven approaches.
So things that would be, you know, using machine learning techniques.
So I investigated both aspects through my PhD.
And during your PhD, you did a stint with Xerox Research Lab in Europe.
You went there after your PhD, and without leaving, you ended up at Naver Lab's Europe.
What's tell us about that whole story?
Sure, so it's very interesting time.
So in fact, I was a visiting researcher at Xerox Research Center Europe, XRC, for those
in the know, during my PhD.
So actually, I was visiting them and collaborating with them as a PhD student.
So after my PhD, you know, I had a great time at Xerox and so I decided to stay on as
a research scientist.
And eventually, I started leading the computer vision team at Xerox and at a certain
point last year, Xerox decided to transition, and you know, we were looking for acquirers
for the lab.
And so, Naver Labs, and so I can get in a little bit into what, you know, who are we as
Naver Labs?
Naver Labs acquired us eventually, acquired XRC.
And so now XRC is now known as NLE, or Naver Labs Europe.
And so the institute is the same, the people are the same.
But now we're under the umbrella of our new home, which is Naver Labs.
And Naver Labs does what?
Okay, so this is like an unintended, unintended story.
So let me start off with Naver Corporation.
So our parent company is Naver Corporation and this is a South Korean Internet technology
company.
We're a very dominant company in South Korea and then for different services in other parts
of South and Southeast Asia and East Asia.
So we're very well known, we're very dominant in search, for example, in web search and
mobile search in South Korea and then we have many other services.
We have a lot of internet and web services for things like for translation, for navigation,
for maps, for blogging.
We have more than 100 services that we provide over the web of the internet over mobile
phones.
We also, we own line corporation, for example, which is a very popular messaging app as
well.
So this is Naver Corporation.
And so Naver Labs is actually a spin-off of Naver Corporation.
And so Naver Labs, what we are, is we're a company with a vision of ambient intelligence.
So what this means is we are interested in having services that are intelligent and that
take context into account when it's deciding how to best provide some kind of service.
So that may mean understanding where you are, understanding what you're doing and using
that to provide you different services related to, for example, mobility or entertainment
or communications, things like that.
So that's the vision of Naver Labs.
And as a result of that, we do a lot of research on things like of computer vision, natural
language processing, you know, recommendation systems, a lot of machine learning tasks in
general and also applications like robotics, autonomous driving and quite a few other things.
You recently returned from the deep learning in Daba and in fact this interview will be
published as part of our deep learning in Daba series where you presented an overview
of tutorial on CNNs.
I'm curious about your experience at the in Daba.
Had you been to the in Daba event previously and tell me a little bit about your experience
there?
Sure.
So I hadn't been previously as far as I understand the first, this is the second edition
of the deep learning in Daba and I wasn't there last year.
I was really happy to be invited to give to give this overview of convolutional models
and I have to say it was really a great experience, you know, on all dimensions.
In terms of the organization, it was great.
In terms of the group of students and participants that the organizers put together, they were
really a very engaged, motivated audience.
I was a bit, you never know when you're giving a talk how engaged the audience is going
to be, you know, we got so many questions during the talk, which is always great after
the talk.
A lot of questions as well, a lot of students coming up to me afterwards saying, oh, you
know, I saw this, you spoke about this specific kind of flavor of a confnet.
Could do you think it could be use in my case?
Here's what I'm trying to do.
So people are really, really willing to explain to describe their problems, to look for advice,
to ask questions, to share knowledge.
So I thought the general spirits of the DL and Daba was great.
It was really, I think, in the best spirit of conferences, in the sense that people
are really there to share knowledge and to find collaborations and things like that.
So it was a very nice spirit.
I think I also learned a lot listening to some of the other talks.
They were great talks by people like Katja Hoffman and David Silver and others, many others,
where I learned quite a bit of stuff myself too.
So in all knowledge, it was really wonderful.
Oh, fantastic.
And so your recent work has been focused on areas like visual attention and learning different
representations for visual search.
Let's dive into some of your recent research.
Visual attention.
When we talk about visual attention, is it related at all to, you know, we talk about
attention within models?
What are the connections between those?
Or is it just a naming collision?
Sure.
So that's a great question.
I would say there's an intersection, but it's not a complete overlap.
So visual attention has a very specific meaning in the neuroscience community and in other
communities, right?
It's not something that computer vision people came up with.
But there is some overlap.
Basically, when I say visual attention in the context of my research, what I really mean
is human attention, let's say that's given, or that's a result of the human visual system.
And you can think of even higher up visual processes and cognitive processes.
The idea being that, so for example, to give you a typical setup, in terms of what we're
trying to model, let's say, for example, if you sit a human, you sit somebody in front
of like a computer screen, you show them an image, and then you have an eye tracker.
And the eye tracker is tracking exactly where fixations are landing on that image, right?
And the goal of a visual attention model will be to predict that, to be able to say, given
this image, I think that, let's say the average person, because of course, there's a lot of
subjectivity that goes into this, but the average person would pay more attention to this
part or to that part.
Or I think that part's going to gather a fair bit of attention.
Or this is a degree to which I think it would have that amount of attention.
So that's what I mean when I say visual attention, and that's the type of research I did.
But in a lot of CNNs now, and let's say general deep learning models, it's used in a very
similar sense, the idea being that, but it's not necessarily human driven, right?
It's basically saying, what does the algorithm, what does a model basically think needs to
be attended to in whatever stimulus you're talking about?
And that can be a visual stimulus, it can be textual data, it can mean many other things.
So it's not necessarily human driven, but the concept is similar in the sense that there's
a lot of information.
What information is relevant for me to make whatever decision I'm trying to make?
Why do we want to study visual attention?
Yeah, you could step back and think about it as more like a bug than a feature or limitation
feature, right?
It's like we, as humans, have this limited ability to focus due to a variety of kind of
neurophysical limitations the way we're wired, but a computer could do more, right?
Why do we need to worry about focusing on a specific part of an image?
You could say that, but I actually think that visual attention is kind of a miracle.
I wouldn't say it's a miracle, but it's actually a very wonderful feature of the human, let's
say human cognition.
In the sense that there is a vast amount of information that we're taking in every time
we just look around us, right, every time we look around us, every time we see.
If you think of the amount of data that you need to store it to just store like an hour
of video and just imagine that we're just like having this be input into our eyes constantly
as when we're awake and when we're looking around, there's a lot of information and it's
not rather than think that, okay, it'd be great if we could use all of this, frankly,
I mean, it's very smart for our visual system to say, a lot of this is not even necessary.
Why would I waste my time?
Why would I have waste my energy of my brain to process this stuff that's not even needed
for whatever time of task I'm trying to do?
The human brain is very computationally efficient and that's something that we struggle a lot
with in computation, especially in deep learning right now.
I think it's actually something very fascinating and something that we would love to be able
to replicate in, let's say, machines and computer systems as well.
That's just to start off with.
I really think, I find it really fascinating the ability we have to be able to really
attend to things that matter.
So that's one reason why I think it's interesting, just because if we could replicate that, I
think it would serve as a lot of computational power and many other things, right?
We know that right now, for example, there's a lot of issues with environmental concerns,
for example.
How do you cool all these, like, GPUs that we're all using and things like this?
So if we could manage to replicate that, that'd actually be pretty cool.
And then, so another reason we're interested in it is just for purely, let's say, scientific
purpose to really understand how the brain works.
So that's one thing.
And actually, a lot of this research came out of the neuroscience community, right?
Like, people who really want to understand how does the brain work.
That's just a fundamental question.
And attention is a big part of that.
Not just visual attention, but also attention to audio signals.
And then also just from for computational reasons, I just gave you a few, right?
It's really good for compression purposes and for just general computational efficiency.
Can you scope out the landscape of visual attention?
What's the research trajectory been in this field over time?
It sounds like it's been something we've been looking at for a long time from the perspective
of multiple disciplines.
How is our understanding of how to replicate visual attention and machines evolved over
time?
Sure.
So, yeah, I can try to take you through a very brief overview.
So visual attention, let's say computational models have been around for many, many decades.
There are some very seminal ones.
So for example, there's one by Triesman et al, which is related to what we call feature
integration theory.
And then from there, there have been some works by professors such as Laurent Eti and a
lot of his students who've worked on things like really taking an image and trying to decompose
that image into some sort of compressed representation.
The idea being that if you have an image, a lot of, there's a lot of redundancy in images,
right?
We all know that.
So images can be very highly compressed and anybody who's used, you know, JPEP compression
or any other types of compression techniques know that.
And the idea is that there's been a long history of work thinking about the fact that to understand
visual attention, we have to think about what can be compressed and what cannot be in order
to retain the same information in the image.
So that's been like a big theme throughout a lot of visual attention research.
In early days, let's say a lot of things was very, very much focused on trying to come
up with, let's say from first principles, first principles of neuroscience about, for
example, how the brain works, how the visual cortex works, how the primary cortex works,
et cetera, trying to, from that, come up with a model.
More recently, people have gone into very much data-driven approaches.
So by data-driven, I mean things like, you know, you have some sort of data set that
you collect.
So a typical data set that is used in this field is a data set of images and corresponding
fixations from different, different individuals, different viewers.
The data-driven approach would basically say, okay, I have some sort of model, and this
model can be a deep learning model, it can be some other type of model, and I want to
optimize this model such that it's able to predict as best as possible the these fixations.
So basically to give some sort of, for example, a probability score for given pixels, what's
a probability that a fixation would land on that pixel?
So a lot of work on this field has used things like image decomposition methods, for example,
using wavelet theory, using wavelet decomposition.
More recently, it's gone, you know, a lot of the research has gone into deep learning.
So for example, some of the recent research that I did used a very generic, let's say,
visual backbone, you can say, visual front end.
So for example, a ResNet model, and then for example, train the ResNet model architecture
for the final layer that gives you a prediction on a pixel basis, saying, okay, given this
image, what's the probability of this pixel being fixated upon?
And so your specific work in this field, what were some of the papers that you've done
in this area?
So the first, the first work I did on this, so this was before the deep learning era,
if we can put it that way, was really based on using a psychophysical model, a vision.
So this model was really a model that was developed initially to predict color perception.
So how do we perceive color in images?
And then we adapted this model to predict visual attention.
More specifically saliency.
So maybe before I get into this, I can give a little bit of a distinction between the
two things.
So when we talk about visual attention, this is something that's extremely complex,
let's say process in cognition, it involves many, let's say, what we might call low-level
cues and also many top-down cues.
By that, what I mean is that typically when you're looking around you and you're paying
attention to things, you have some sort of purpose in mind.
So for example, you might be reading or you might be looking at a movie or you might
be searching for something on your desk, and that impacts a lot where you look.
So this is why it's very subjective.
So there's a component of attention, though, that's been traditionally called saliency,
and this is something that people call more bottom-up.
So what that means is it's something that's more related to things like textures, like
low-level textures and colors and things that aren't really related to the task at hand.
So what I may mean by that, for example, is even though you're looking at your desk and
you're trying to find something, if there are certain things that are just, let's say,
fundamentally salient for lack of a better word in your desk.
So there are many patterns that have been found that have this property.
So this was more what I was focused on.
This aspect of really a bottom-up saliency or low-level saliency.
Is it fair to draw this distinction along the lines of bottom-up or saliency is specific
to the content of, let's say, an image, whereas top-down is more contextual or related to
the goal of the observer, or is that too simple?
Yeah, I think that's a fair distinction to make, exactly.
So we would say that, and this is why I think if you're trying to model attention, many
people have started off trying to model what we call bottom-up, bottom-up or low-level
attention, because that's the sort of thing that's really, it's less task-dependent,
right?
It really sort of depends on just the fundamentals of the visual stimulus that you have.
And so that weighs a bit more, there's a bit more continuity of that coherence in between
different viewers.
When you start to get into things like looking at different tasks, it can be very subjective
and very different.
So your specific research was taking a kind of a neuro-physical model, is that how you
described it?
So that was my initial work on this, which took a psychophysical model, and it attempted
to modify it to better predict, let's say, visual saliency and in particular to predict
visual fixations, so eye fixations.
So what that meant is that you take a data set with these fixations that were recorded
by different observers, and you try to really replicate that with your model.
So later on, in more recent years, I've looked at a more data-driven approach with some
learning involved.
So in this case, what we did was, we really focused on, so this is work I did with students
at MindSomniaJetly and also a colleague of mine, Noravig.
And so for this work, we focused on really trying to understand what kind of objective functions
would be appropriate to learn, to learn a model of saliency.
And so we modeled saliency as a probability distribution.
So we said, okay, we want to really predict the probability of fixations, we consider,
we consider all our pixels in our image to be potential fixations, and we try to figure
okay, what would be like the best objective function to learn to train this model.
And so we consider different ones, we propose some new ones, basically, that we're trying
to really capture this property of probability.
And so what were the field of objective functions that you considered?
So very much those related to capturing differences between, or let's say, to quantify differences
between probability distributions.
So they're, for example, we try things like, like, kale divergences, gents and shanen
differences, kite divergences, and then many others.
So then, basically, we, we, finally, we proposed like an objective function that's basically
a combination of a soft smack function, which will allow you to have a valid probability
distribution, then to apply different types of divergences, probability divergences on
top of that.
And we found that that worked that worked quite well.
And in fact, we found that the bathe charia distance worked quite well.
The what distance?
So there's a, there's a, it's called, it's called a bathe charia distance.
It's quite well known.
It's been used in many, many fields.
I think it could be used more, actually, than it is right now.
But actually, the kale diverge is actually works also very well.
We compared several ones, and then we found that these two were pretty good.
So in the case of, I'm trying to map this to, like, simple Gaussian type distributions,
where translating from one to the other is related to kind of the mean and the variance,
do those correspond to correspond one to one, two terms, and these different, these different
functions?
No.
So these are very generic.
So basically, so, so these, they're not necessarily some sort of generative model.
For example, it's really, you assume that you have for every, every value you have in
your distribution, you're assuming that, you know, you know, what the value is, right?
So you have, like, probability of X, one probability of X2, et cetera.
So that can, that can be modeled, for example, with a GMM or with something else.
But this is sort of agnostic to that, these types of distances.
They're saying once you have some sort of probability distribution, this is, this distance
is going to compute the difference between the two, but they can be modeled differently.
All right.
And so how do you use this ability to compute the distance between two distributions
to help you figure out attention?
So what we start off with, so our ground truth can be converted into a probability distribution.
This is what we did.
And when we're not the first people to do this, but this is something that's, that's, that's
very, that has been done before.
So for example, you start off with a set of fixations.
So you can consider it, you can consider that you have an image and you know the, you
know the X-Y coordinates of where somebody fixated on, on that image.
Is your data set consist of one image and a large number of captured fixations from different
observers based against that same image, or do you have a bunch of images each with their
corresponding fixations?
So both.
We have multiple images, ideally, in the beginning we had very, we have very small data sets, but
now we have pretty, pretty big ones.
So we have multiple images and for each image you have multiple sets of fixations.
So basically, for example, you might have like an image and you ask like 10 people to take
a look at the image, you know, during, during a very small amount of time.
So maybe like up to two seconds.
And so then you capture these fixations.
And normally what people do is they, they apply Gaussian blurring to that.
So you can imagine you have like an image, which is full of zeros and let's say you have
like ones at the locations of the fixations.
And then you can apply Gaussian blurring to this and what you're going to get is you're
going to get this resultant image, which is diffuse, right?
So you have high values at the fixation points.
And then these high values sort of diffuse in the immediate area of the fixation.
So what you end up with is sort of like, it's not a, it's not sort of like a binary image,
but you have some diffuse attention around the regions of fixation with the modes being
at the fixation points.
And so this can be what you have now is rather than have like an image full of zeros with
a few, you know, with just a few points where there's some support of the distribution,
you know, you've diffuse a distribution where you have basically some amount of non-zero
support at all points.
And if you normalize this resultant image appropriately, you can consider this image
to be a probability distribution.
And so then this becomes your ground truth and you just apply your typical machine learning
framework and you say, okay, this is my ground truth.
I have a model that takes us as inputs and image and produces some sort of predicted distribution.
And then I compare the two using my probability distribution, my difference measure.
And you backpropagate the loss, right?
You backpropagate the difference that you see.
And so why is that diffusion step key here as opposed to the more binary approach of looking
at the fixations?
As we have a somewhat subjective process, it's a little bit too strict of a task to ask
the model to predict exactly the fixation point because you could imagine that the user
could have very easily looked elsewhere and you can actually see this, right?
Because if two observers are looking at the same image, you're not going to look at
the exact same point.
So therefore, if you have a location, so for example, you have an image and there's
an image that contains a face, very likely there's going to be a ton of attention paid
to the face.
People like to look at faces.
But they're not going to look at the same points, not everybody's going to focus exactly
on the eye, right?
So if you have ten people, you might see like a distribution of fixations very much on
the face.
But you can imagine very easily if you had another person, if you had an eleventh person
that looked, they might look not exactly on that point, right?
But they would look in the general area.
So this is why it's nice to not use just the actual fixation that you have, but to sort
of do this diffusion process where you basically apply gouchions to those regions and have
this diffused distribution.
And you're doing this diffusion process to each of your fixation images, if you will,
as opposed to after aggregating or averaging across the different fixations for a particular
image.
Exactly.
So there are many different ways to do this.
You can see that there are different choices, right?
And none of them are exactly correct, right?
Right.
So in this case, we're trying to solve like a pseudo problem, like an adjacent problem,
because we're not actually solving the exact one, right?
Because this is the way the subject of it comes into play.
So yeah, there are different ways of doing it.
And of course, when you're applying this gouchion blurring, the gouchion filter itself has
its own parameters, right?
So you have to decide how diffused do you want this to be?
And that itself isn't a question that sort of left up to the specific researcher.
And do you, in that sense, is it, it's not something you're learning.
It's a hyperparameter that you're choosing the experimentation.
Oh, it's, it's, it's not even a hyperparameter because this is really the ground truth you're
setting at this point.
So really, what many people have tried to do is to set this in a somewhat principled way
by trying to look at things like, like peripheral vision and trying to understand, okay?
If someone actually fixates what does that, how localized is that fixation really?
And then you can kind of have a measure on what sort of local region is really being
attended to.
And there's a lot of psychophysical studies on that and that can, that can inform how
diffused you want this to be.
And so that's, are we just finishing up your, your first work in this, in this field?
It sounds like it.
Yeah, but we have a bit more to cover.
So where'd you go next?
You know, some of this work was done actually while I was at XRC now, NLE.
Some other work I've done while here is related to, as I said, learning visual representations.
So I also did a favourites of work on this once again before the deep learning era.
So this was using handcrafted features and trying to understand how to, how to learn embeddings
in an effective way for different tasks.
So things like fine-grained recognition and also visual retrieval.
So maybe I can start off with some of the work I did recently with colleagues from
NREA and also colleagues here at NLE related to what we call aggregation.
So this is also something that has become very well known in the deep learning context.
What I mean by that in particular is, for example, people very often refer to the same
sort of principle as pooling.
So, you know, for example, average pooling or actually exactly.
This is quite well known, right?
So this is basically saying you have some amount of it local information or maybe not even
not very localized information and you want to be able to summarise it somehow, right?
And very often what you want to summarise might be different vectors.
So let's say you have a set of vectors and you want to summarise them.
So for example, in the deep learning context, if you think of like a feature like a hyper
column and you want to to summarise that in some way or to, let's say, compress information,
you might use max pooling or average pooling.
You referred to a feature as a hyper column and I haven't heard that reference before.
What does that mean?
So when I say hyper column, this is sort of, it sounds a bit old school now.
It's a term that comes from neuroscience once again and it refers basically to, it's
what you might call this typical feature tensor that you find in many confnets where
you have, for example, you have an input image and you're applying different convolutions
to it.
Very often you end up at some intermediate point with multiple feature maps, right?
So you have like a feature map of size like h prime w prime and that's your feature
map and you have multiple ones of them, right?
Let's say you have d feature maps.
So in the end, what you end up with is a tensor that's like d by h prime by w prime and
this has often been referred to as a hyper column.
Okay.
So that's in a deep learning context, but you can think of many contexts where you have
feature vectors that you want to summarize in some way.
So for example, in my work, we worked a lot with what we call Fischer vectors and so Fischer
vectors are, it's a type of representation of visual content, particularly used a lot
with what we call local descriptors.
And so it's a way of basically saying, okay, I have some local region of an image and
I want to find some vector representation of that local region that's discriminative
and compact hopefully, okay?
And so imagine you have an image and you have a set of these descriptors that you extracted
let's say for different regions of the image and you say, okay, I have this image, I have
I don't know, like n of these descriptors and I don't want n.
I want like fewer.
I want maybe one or I want to.
So it is, how do I go from that end to that much, much smaller number while not losing
too much information?
So this is like a fundamental problem that has been tackled in many ways and so I've
done some work on how to construct these what we call aggregated representations from
a set of like a larger number of them.
It sounds like, and you mentioned the term embedding previously, it sounds like you're creating
some embedding space and then doing something akin to dimensionality reduction on that embedding
space.
So it's not quite in the sense that in the sense that when people talk about dimensionality
reduction, typically they mean that you have an embedding space, right?
It can be in, let's say, the dimension.
So for example, I don't know, 2000 dimensions and for for dimensionality reduction, what
you want to try to do is find some smaller dimensional space in which you're embedding
slip.
So for example, I don't know.
You might want to go down from 2000 dimensions to maybe 500 or 256 or something like this.
In our case, what we're doing is we're not changing the embedding space, but we're
just changing the amount of samples from that space.
That's it.
The amount of vectors that live in that space.
So for example, rather than have, let's say, 1000 vectors each of size, each of dimension
to K, you might want to have only one.
So this is what this aggregation is about.
Is that a typical example going from 2000 to one?
You can go even higher than that.
You can go even higher than that.
So very often, you might find on the order of anywhere from like 1000 to 10,000 factors
that you want to reduce to just one factor.
So you can see there that you can see what is that doing for you.
Yeah, what is that compression compression?
It's, you know, you're reducing, you're reducing the size of your image representation
by a factor of the number of descriptors that you have.
If you go down to one, I guess if I'm thinking of it in the sense of compression, then certainly
we would want to do that.
But you know, I would imagine there's a tremendous amount of loss in a scenario like that.
But when I think about it from the perspective of like an embedding space, I mean, I guess
it's also loss.
Like you just lose a ton of information.
And so maybe I'm asking you to convince me that there's, you're left with something of
utility after you do this 10,000 to one reduction.
Sure, so you're definitely right in the sense that they both, they have like an effective
compression, right?
That's for sure.
In one case, you're going to a smaller dimensional space.
In other case, you're just reducing the amount of representations you have.
In both case, you're losing information, that's for sure.
And this was the point of my work.
So my work was really on how to represent, how to, to perform this aggregation while maintaining
as much as possible, you know, as much information you can in this aggregated representation.
So what we proposed was a method called generalized max pooling, which aimed to, to sort of maintain
this property.
And of course, it's not perfect, but we found that it gave, you know, pretty, pretty interesting
results over other techniques, for example, like average pooling or max pooling.
And so the benefit there is, is once again, it's in terms of, of compression, which is
which is important.
So in many applications, you don't want, you can't afford to store, for example, 10,000
descriptors to represent an image.
To give an example, so we're very interested in my research center on the problem of image
search, right?
So let's, let's say, for example, you have a database of images, let's say, a billion
images, right?
And you have some image that you want to match to that.
And you can think of many applications, you can think of, for example, shopping, let's
say you want to, to find us, you know, you have an image of like some kind of clothing
item, and you want to find in a huge, let's say, catalog by multiple retailers, anybody
who has something similar, who has the exact same one.
If you have to, if you have to compute the similarity between that image and all the
images in those, in that database, and each image is represented by 10,000 vectors, each
of which has a dimension of 2,000, it's going to take forever, it's not practical.
So this is why very often people work on how do I get from 10,000 vectors to one, or
some smaller number, some much smaller number.
So this is really the utility there.
It's a compression, it's a compression utility.
You mentioned this, this algorithm is a generalized max pooling.
Are you taking kind of an off the shelf, CNN architecture, ResNet, for example, and kind
of swapping out, you know, wherever it says max pooling with this generalized max pooling
and maybe chopping off the last layer, your classifier, and that's creating your vectors
or is it more involved than that?
So that would make sense, as a good guess.
But actually, and this is something that could technically be done, but actually what
we think of this is something that's very generic.
So it's actually not related to deep, it's not specific to deep architectures.
It's very generic in the sense that we are trying to solve the problem where we have
a set of vectors and we want to reduce, we want to aggregate them in some way.
And so that can involve features extracted by deep networks.
So let's say, for example, you have a deep network and at some point you have this hyper
column I talked about, you have this feature tensor and you want to pull it in some way,
you want to aggregate it.
So you can think of generalized max pooling in this scenario as an alternative to max pooling
or average pooling, but you can also have other descriptors, right?
So you can have descriptors that are handcrafted or come from many different, many different
things we can think of.
It's more presented as a fundamental operation, you can apply to a set of vectors to aggregate
them as opposed to something that's specifically used in a context of a deep learning model.
Exactly, yeah.
And in fact, in the original work we didn't apply it on deep learning, although it's
been subsequently used with deep models in combination.
And so you've spent this time on the visual attention side.
It sounds like that was a bit earlier in your research.
More recently you're working on learning these representations.
Where do you, where you headed?
What's kind of interesting for you nowadays and where you investing your resources in terms
of future research?
Sure.
So actually, you know, everything sort of all becomes new again.
So actually I'm doing some work now on both things at the same time, because as you mentioned
very patiently before, attention is it's used everywhere, right?
In deep learning right now, in many things.
So actually right now some of the work I'm really involved in at the moment involves
how to learn representations using deep learning with attention involved as well.
So meaning how do you use some sort of attention mechanisms in order to pay attention to regions
of an image that you really need to focus on in order to solve your task, right?
So this is where once again the, this is like the overlap between the two views, let's
say, of attention.
Because once again, there's limited processing power, there's a lot of information images.
And so it pays to really only spend time computing over data that's valuable.
And then not only that, but it also can be considered some sort of clean name mechanism,
for example, because you can have a lot of things that you can just, that might be important,
but it can also be just distracting and can be considered clutter basically.
So let's say, for example, you're doing once again, let's look at the fashion example.
Let's say you want to train to learn a representation for visual search, right?
Let's use my previous example.
And you want to search for, you know, you're looking for shirts.
If you have an image, there are many parts of the image, which is not going to help you
find, you know, whether these two images match, right?
Whether they both have the same shirt.
So you might want to only attend to the regions of the image that are relevant for that.
So of course, like if there, if you might want to pay attention to the upper body of the
person, you might want to be able to find, okay, where is the upper body of this person?
Because this is where I'm going to really be able to tell if there's a shirt in the
image.
And if so, if it's the same shirt, if it's the shirt I'm looking for.
Now, there might be other parts of the image that are also useful for that.
So maybe, for example, if I find the head, I might have a better chance of finding the
body, but there are many parts that may not be, right?
So what I'm focused on right now is how to, how to incorporate different models of salience
or let's do different ways of attending to regions so that, you know, you can simplify
these tasks of learning representations for search.
Naila, thanks so much for taking the time to chat with us.
It's really interesting work and I'm excited to get to learn a bit about what you're up
to.
Thanks so much, Naila.
It was really great to have this conversation.
All right, everyone, that's our show for today.
For more information on Naila or any of the topics covered in this episode, head on over
to twimlai.com slash talk slash 190.
For more information on the entire deep learning and daba podcast series, visit twimlai.com slash
in daba 2018.
Thanks again to Google for their sponsorship of this series.
Be sure to check out the 2019 AI residency program at g.co slash AI residency.
As always, thanks so much for listening and catch you next time.
