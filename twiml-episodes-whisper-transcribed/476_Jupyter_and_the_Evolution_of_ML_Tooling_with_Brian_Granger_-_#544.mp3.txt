All right, everyone, I'm here with Brian Granger. Brian is a senior principle technologist
with Amazon Web Services. Brian, welcome to the Twoma AI podcast.
Hi, Sam. Thanks so much for having me. Looking forward to jumping into our discussion.
You are a co-founder of Project Jupiter, and that is, of course, the topic that we will
be digging into in this conversation. But to get us started, I'd love to have you share
a bit about your background and how you came to work in machine learning, and we'll
make our way to the founding of Jupiter as well.
Yeah, definitely can walk through that. So, as you mentioned, I'm a senior principle
technologist at AWS, and I've been here at AWS for three years, coming up in February.
Before that, for the sort of decade and a half prior, I was a physics professor, most recently
at Cal Poly San Luis Obispo, and then before that, it set a clear university. Even though
as a physics professor, most of the time at university, I built open source tools for
data science, machine learning, and scientific computing. And so I have a background in theoretical
physics, but that's evolved over time through software engineering, building tools, and
more recently, I spent a lot of time on UX design and research.
Nice. Nice. And so, how did Jupiter come to be?
Yeah, so it's a fun story. So if you rewind back to the early 2000s, Linux was really taking
off. Python had been around for a few years, but it became visible in the scientific computing
community. And a classmate of mine at CU Boulder in grad school, Fernando Perez, had started
to use Python in his research. And he's really the one that introduced me to Python, both
he and I during our physics education used Mathematica a lot. And we, even though we were doing
computational physics in other languages, we weren't necessarily using Mathematica, we had
always missed the notebook interface that Mathematica had. And in 2001, Fernando released
IPython, which is an improved and enhanced command line REPL for Python, that had some
of the ideas from Mathematica in it, although it didn't have the full notebook interface.
And so in those early years, I started to play with Python a bit, Fernando was working
on IPython. And then in 2004, he visited me in the Bay Area. I was a young professor at
Santa Clara University. And while he was there, we spent a lot of time talking about computing
what we were doing in our research, how we were using these tools. And it was really then
where the vision of creating a web-based notebook for Python came into focus. And there was
a couple different factors in that one. It was something that we wanted to use in our
own research. We have found over the years that this interactive computing in a notebook
-based interface, where you also have a document, was extremely useful. And we just wanted
it to exist. The other dimension was that by 2004, rich web applications were starting
to appear. And so it started to make a lot of sense to us at that time, that if we were
going to build something like this, it should be entirely web-based. Now, that was 2004.
It took us until 2011 to release the first version of the IPython notebook. And some of
it was us learning about this space. We were theoretical, computational physicists,
not web developers. We wrote a lot of code as physicists, but of a very different nature
than this, obviously. And the other is that modern web technologies, even in 2011, were
relatively primitive. And from 2004 to 2011, we were really waiting for modern web technology
to catch up to what we needed. And even in 2011, state-of-the-art at the time was JQuery
and Bootstrap. Web Sockets had just been turned on and all the browsers. And we were using
all the latest stuff in 2011, which now seems rather primitive compared to what we have today.
When we think about Jupiter or talk about Jupiter today, it's often in the context of ideas
like literate programming. Were you thinking about it from the perspective of literate programming
and some of the theoretical foundations of why a tool like this makes sense? Or is it
kind of strictly scratching your own itch and trying to bring to Python this interface that you
loved in Mathematica? The connection to traditional literate programming
came much, much later. And I think we, and so the phrase that we use, there's sort of two phrases
that we use. One would be literate computing rather than programming. And the other is distinction.
So in the traditional literate programming paradigm, there's nothing interactive about it.
You're not actually, as a human, interacting with the live code as you would in a repel.
You just write a source code file and then you use the literate programming tool to compile that
to the actual source code file that can run. There's nothing interactive about it. And in scientific
computing and data science and machine learning, that interactive experience of writing and running
a bit of code, seeing what the output is and having a stateful process that holds the state
of the program in memory that you can then write more code against. And so that's where we've
always thought of it as literate computing or interactive computing. Another phrase that we talk
about a lot is the idea of a computational narrative. And again, the focus there is on a narrative
that's contained in a document, but it's not about mirror programming as in typing code. It's
about actual computing. It's about writing code and running it, seeing the result of that and
using that to think about data in the case of machine learning. So I think you got us through
2011, 2012, all these new web tools kind of catching up to what you needed around that time. We also
saw an explosion in machine learning, deep learning interests. How did that shift? What was happening
with the Jupyter project? Yeah. So in the early years, when we thought about what would success
look like for us in building this web-based notebook for Python, I think our market, if you want
to phrase it that way, would have been academic researchers who are doing scientific computing.
That's the universe we were in at the time. Those were all the people we were talking to.
And every time, all through the early 2000s, whenever we talked to people in the industry,
they looked at interactive computing with a bit of a sort of, well, that's nice, but we never
need to do that. I think Fernando even had a conversation with Guido Van Rossum, who was the
creator of Python. And when Fernando described how we use Python interactively, Guido said,
wow, I always thought of the Python REPL as being a bit of a toy that no one would actually use for
real work. It's amazing to see how in the scientific computing context, you live in these interactive
shells. And so we had always thought that the commercial adoption of these tools would be very
slow at best. Now, it sort of came in from the side that around the same time, commercial entities
discovered the power of data through data science and machine learning in a way that they hadn't
before. And so they ended up needing these same tools for interactive computing, and they quickly
discovered Jupiter as one of the tools that they could use for this. But it took, I remember in the
early, so starting in 2011, going up to maybe 2015, was an amazing time for us in that it felt like
every month a new major organization discovered interactive computing and Jupiter and Python.
And by the time we got to 2015, 2016, it was a lot of people, a lot of companies were using these
tools as well. But it happened. It was a transition and growth that we had not seen coming and
definitely hadn't planned on. And it created a lot of challenges for Jupiter as a project.
Was it obvious that you should embrace these new use cases and new user communities,
or was there a bit of tension between kind of staying the course and building the thing that
your scientific computing users needed versus things that machine learning users might need.
It's in the extent that those are divergent in any way. Yeah, so I don't want to imply that
there's never been tension there. I think that the I don't want to speak for all Jupiter contributors,
but I can try to summarize some of the sentiments in the community. We even today, so today, Jupiter
is used by many large corporations and many contributors to Jupiter today work at those large
corporations. Back in 2011, it was close to 100% academics working on project Jupiter. And
even today, I think the core Jupiter team deeply values the role of Jupiter in research and education.
We recognize it's important in the commercial space. We definitely want to address those usage
cases, but I think broadly speaking, the Jupiter community holds the research and educational
usages with special importance. And so there has been some tension there. Now, the other question
you brought up is the potential for divergence between the needs of the academic users and commercial
users. I think to first order, the experience we've had is that there's no substantial difference
in the needs of those communities. It's even to the point where it's a little humorous in the
sense that even still to this day, we regularly talk to organizations that will tell us how they're
using Jupiter and start out by saying, you know, the way we're using Jupiter is really weird and
special and probably unlike anything you've ever heard. And then they'll tell us a story that we've
heard hundreds of times about. They're doing all the same things everyone on the planet's doing
with Jupiter. That's not to say that they're not differences. For example, in the academic context,
the requirements of teaching are really unique. And so when you're teaching a large class and
university with notebooks, being able to manage homework, assign homework that involves notebooks,
grade the homework involving notebooks. And so we've built special capabilities for things like
that. But at the core, I would say most of the functionality is common across to all Jupiter users.
Got it. Got it. And so you're now at AWS and you continue to work on Jupiter. How did that come
to be? Yeah. So as I mentioned, by the somewhere in maybe 2015, 2016 commercial and enterprise
usages of Jupiter had really taken off. And what that meant is that the Jupiter developers were talking
to a lot of users that were no longer individual users, but were people maintaining and operating
large scale Jupiter deployments in their enterprise. And the pain points and struggles that they were
having sometimes were related to Jupiter and would be either bugs or feature requests or enhancements
that we could make on the Jupiter side. In other cases, their challenges that they were running into
really were not a Jupiter problem or challenge. It was more of a deploying and maintaining
cloud infrastructure type of problems. A great example of that would be entities that have
some manner of private or sensitive data and need to deploy Jupiter to meet a certain compliance
regime such as HIPAA. And that's not really a problem that Jupiter's an open source project is
going to solve in an end to end way. Jupiter may offer building blocks that can be used to assemble
such a system. And so that was one of the, for me personally, starting to talk to these
enterprise users and realize, okay, there's a huge need here. We're probably not going to be
that the Jupiter open source community is not going to fully solve these needs. I'd love to start
to work in an organization that's really good at all those enterprise cloud computing challenges.
That was one dimension. The other dimension was us thinking about the long-term sustainability.
The as a result of the adoption and the enterprise commercial space, the user base of Jupiter
grew far, far faster than the size of the development team of Jupiter. I think that the Jupiter
user base has been growing exponentially with a doubling period of, I don't remember, it's about
a year since I think 2015. The Jupiter contributor population has not grown exponentially. We've grown
a lot. And so we've really faced a resourcing issue where we just as an open source project have
not been able to keep up. I love by the way that you said it was growing exponentially and
actually meant exponentially. I can show you the plot where I'm getting this from
is we have a chart that we periodically gather and that is the number of public notebooks on GitHub.
There's a number of different ways we measure how big the Jupiter user base is.
But you can see we have a chart in this repository in a notebook that shows
the growth of notebooks, public notebooks on GitHub, and it's been growing exponentially since I
think somewhere around 2015. So yeah, and so part of this was the Jupiter governance model has
always been multi-stakeholder. We've designed it and some of this came out of our own needs.
Fernando was at Berkeley. I was at Cal Poly. So almost by definition, there's multiple stakeholders
in organizations involved. As additional academic contributors came on board and then eventually
contributors from companies came on board, we embraced that multi-stakeholder nature.
And I think from this perspective, my moving to Amazon was an opportunity to bring a new stakeholder
into the mix of improving and making Jupiter sustainable and growing open source project.
And the governance model is it's Jupiter's part of the numfocus foundation. Is that correct?
Yeah, numfocus is a 501c3 nonprofit that's the umbrella organization for a number of open source
projects in this space that would include NumPy, SciPy, Pandis, SimPy, Jupiter, and dozens of others.
I know I'm forgetting. There's more than I can possibly name. So Jupiter's one of those
Jupiter in numfocus, each of those open source projects has its own governance model. It's not like
the Apache foundation where there's a single governance model that everyone adopts under the
foundation. And we actually in the Jupiter side have been refactoring and designing a new
governance model over the last two years to address the scope and scale of Jupiter.
And we've been rolling that out incrementally over the last year and still doing work to
finish that up. But the core idea is that it is multi-stakeholder and we're trying to build checks
and balances to include cooperation in a vibrant community among all these stakeholders.
So you've talked about the kind of what's in it for Jupiter in finding kind of an enterprise
cloud home, if you will. What's in it for AWS and how and why does AWS invest in Jupiter?
Yeah, this is this is a great question. So the story of Jupiter and AWS actually began before I
joined. And as AWS was diving into machine learning and data science, the question came up of what
do we do? Our customers are asking us about notebook platforms. What are we going to offer for
that? And a decision was made before I joined to embrace Jupiter. And it really came from feedback
from customers. The leadership team and the org that I'm in the AI ML org at AWS. But we spent a
lot of time talking to customers, understanding what they're doing, what their pain points are,
what existing open source technologies they're using. And like I said, even before I joined,
we heard the resounding chorus that people were using Jupiter and they needed help deploying Jupiter
in a secure cost effective way. And that they wanted actual Jupiter. They didn't want a notebook
like solution. They wanted real Jupiter. This is also something that made it possible for me to
join AWS and very attractive to join AWS. I didn't need to start at the sort of very beginning and
argue and make a case at AWS for why Jupiter? Why should we ship Jupiter versus build our own notebook?
That was already done and settled. And so since I joined AWS, it's more been a question of how do we
at AWS make sure that Jupiter continues to be the best notebook platform in a vibrant and growing
open source community? And at AWS, there, you know, across different projects, there are different
approaches to engaging with open source communities. It sounds like the way that AWS is engaging
with Jupiter in terms of, or rather, the way that AWS is incorporating Jupiter into its projects is
to try to stick close to kind of the core Jupiter as opposed to forking it off into something else.
Absolutely. Yep. Now, one of the things that, and this gets back to a technical and architectural
principle that Jupiter has used since its founding. And that is Jupiter at the end of the day builds
Lego building Lego pieces for notebook platforms or for interactive computing platforms. And the
idea is that enterprises and organizations can take those building blocks and assemble them in
different ways. And at AWS, that's exactly what we're doing. We're taking the open source building
blocks and assembling them in a particular way to serve the needs of our customers. And so we may
build, and we do, in fact, build additional things on top of it using the various extensibility APIs
that Jupiter has. So for example, our machine learning IDE at AWS is SageMaker Studio. It's based
on Jupiter Lab. But then on top of that, in the SageMaker team, we've written a bunch of Jupiter
Lab extensions that add machine learning specific capabilities to make it an end-to-end solution for
machine learning. And those extensions that we're building wouldn't make sense to be in Jupiter
from an open source perspective. They are, a lot of them are specific to AWS. Jupiter as an open
source project works really hard to have a essentially vendor neutral perspective. So if you look
across Jupiter's different code bases, you're not going to find a lot of code that's specifically
tuned to a particular cloud platform or deployment context. And so at Amazon, we're extending Jupiter,
we're building additional domain specific capabilities on top of that. But anytime we're looking at
either bugs or enhancements to the core of Jupiter itself, our approach is to work with the
Jupiter open source community and contribute back to those changes. And so we have a dedicated
team of engineers. It's a small team, but they're 100% focused on upstream contributions to
Jupiter. With the goal of making sure that Jupiter continues to be the best notebook platform out
there. And that is, and it's again, not just about the software, but it's also about the community,
the open source community. And so we're participating in that open source community in a way that we
hope makes sustainable and growing inclusive and diverse. Zooming out a bit, I'm curious how you
think about the broader ML tooling space. And you know, Jupiter is just one piece of what data
scientists or machine learning engineer might interact with to get an idea for a model from
that idea into production. How do you think about that broader space?
Yeah. And so for this, I'll start with the landscape of products we have at AWS for machine
learning under the SageMaker umbrella. And what we're seeing from customers is that there's
a bunch of different tools and capabilities they need to go all the way from the beginning of
the machine learning workflow where they're preparing data, importing data all the way to
building models and then deploying them and then using them to make predictions, whether it's
in a product through an API or make predictions that are more consumed by humans and something
like a dashboard. And we've been building the tools at AWS and SageMaker for the different parts
of that machine learning workflow in close collaboration with customers. As you know, one of the AWS
leadership principles is customer obsession, which means that we spend a lot of time talking to
customers and understanding what they're doing, what their needs are. And so all the different
things we're building in SageMaker are a response to what our customers need. Now with that said,
I think that the challenge that's emerging more broadly is one of complexity, that if you look
at all the tools, a large organization would have to string together to cover and span the complete
end-to-end machine learning workflow to have those tools address the different personas that
are participating in machine learning, whether it's data engineers, data scientists, ML scientists,
ML ops engineers, etc. There's just incredible complexity. And the complexity is along a number of
different dimensions. There's fundamental complexity in the data. People are working with,
there's complexity in the algorithms they're working with, there's workflow complexity,
and then there's the reality that this nice picture that we have about the machine learning workflow
that starts from importing data to exploratory data analysis, to data preparation, to model training,
to model evaluation, deployment, it's never linear in practice, right? Someone starts working with
a data set and the initial questions are going to be asking are very basic, such as what's even
in this data? What might we predict? What business questions might we predict with this data?
And they may get to the end of that initial pass and discover we're not even close to being ready
to building a model that we can deploy and make predictions against. We have to go back to the
beginning, clean up the data, gather more data, join it with other data that we don't have available,
and then they're come back and do that again. And maybe this time they get a little further
and start to feel like, okay, we may be able to predict this. Let's dive in and see how far we
can push it in terms of the quality of the model we can build. And then after that, they may have
to look at questions around bias and explainability and understand, okay, we have a model that's performing
well, but can we use it responsibly and ethically? And they may have to do another cycle through all of
this. And this iterative nature, and then the complexity of the overall workflow, I think is
something that we at AWS and everyone across this entire industry is just starting to grapple with.
I'm hoping that 10 years from now, we look back on this stage and say, wow, we've made incredible
progress. Really on the side of UX design and human computer interaction that our systems will
evolve to the point where they still have these capabilities, but allow human workers who are using
the tools to have a much simpler experience. And I think that's the main challenge we have right now.
Digging into the user experience and HCI aspects of this, I know that's something that you
are very passionate about and spend a lot of time researching. What,
you know, what, you know, have we learned or what have you learned and have applied into
Jupiter or, you know, what's kind of changed the way you think about Jupiter or, you know, what
from those spaces do you think will kind of impact the way that you, you know, build these tools
and guide these tools in the future? Yeah, there's a number of different dimensions here.
All maybe pick two, two of them to talk about briefly. One is that in organizations that are doing
machine learning and building machine learning tools, you're pretty much guaranteed that by definition,
they're engineering heavy. You, if you look at these organizations, you're not going to find a lot
of UX designers that just naturally work into the process. So that's the first challenge is that
is sort of the way to engineering required to build these systems and use these systems is massive.
And even in, even in organizations like SageMaker in the AWS AIML org, we have been very deliberate
to build out UX design teams. And yet, if you look across our organization, we're still very
engineering heavy because the problem requires it to be so. And so just the weight and momentum of
engineering presents, presents a lot of challenges to prioritizing the human experience of these tools.
And so a lot of what I'm working on right now at AWS is building mechanisms to help us include
the consideration of the human experience in this. And it's a lot of fun to be diving into that,
but certainly very challenging. And I think that the key is that at least at AWS,
it's rather new to build tools where the human experience is so primary and important.
And it's a growth area for us and something that we're spending a lot of time and energy and
investment on to improve in this space. The other dimension of this is I think that there's very
few situations where we as humans have tried to design tools that are this technically complex.
And what I mean, I'll use an analogy here. I'm a car nerd in addition to being a
data nerd. And if you think about how people approach car design, if you're designing a hatchback
for mass-produced market, you can have UX designers come in and look at the human needs.
And those UX designers will not need to know much about the technical implementation of that car.
They're not going to need to know about that. They can work with engineers who can handle all that
and it will work wonderfully. If on the other hand, your job is to design a formula and racing car,
anyone involved in that product, I mean, if you want to think of it as a product,
has to have an incredibly high level of technical knowledge. For example,
let's say you're the UX designer who's designing the steering wheel for an F1 racing car,
you need to understand what are all the technical capabilities that the driver needs to have
at their fingertips. What are the principles on the human side, the human factors that would enable
a driver to manage dozens of buttons driving 200 miles an hour. How on earth do they do
that? They're going to have to glance down in a split second to find that button to change the
brake bias or to change how the engine is tuned. And the designer going through that has to become
an expert in the technical details of that platform. They're going to need to know about
tire wear, brake bias. When do the drivers need to use these things? And this is, I think,
another fundamental challenge is that the designers who are helping to design these tools
need over time to get that technical expertise to understand these technical users and what they're
doing with the code and the data and the tools we're building. In kind of talking about the first of
those, the first of those directions for incorporating user experience into product,
the recent Canvas announcement came to mind. Can you talk a little bit about the way that
user experience design went into that product? Yeah, absolutely. So at reinvent this year,
we launched Amazon SageMaker Canvas, which is a tool that enables business analysts to train
machine learning models. So these are users that spent a lot of time working with tabular
data sets. And they're focused on answering significant business questions with tabular data sets.
Maybe Excel spreadsheets. They may have data in relational databases and running SQL queries
against them. And what we're hearing from customers is that these business analysts often would work
with data scientists or machine learning practitioners who can build models. But there's never enough data
scientists and machine learning practitioners to support the business analysts. And so the vision
of Canvas is basically let one of these analysts import a tabular data set. And then pick a target
column. We suggest what type of prediction is relevant, whether that's classification or regression.
And then we train a model and enable the business analyst to quickly make predictions. And it's a
no-code interface. And what's exciting about it is that use the same underlying platform
of SageMaker. So the models that are trained in Canvas use SageMaker Autopilot, which is our
AutoML service. And so the analysts when they train a model can then hand it off to the data scientists
who can then do additional work on that model as needed. For example, if there's multiple model
candidates that autopilot is suggested, the data scientists can come in and help the analysts at
that point figure out what for this business use case, what is the best possible model at that time.
And what we're seeing is that Canvas enables these analyst users to focus on the business
questions that they want to answer. And then understanding what types of things they can
predict using machine learning. And the, yes, even the name of the product kind of elicits this
visual approach to building machine learning. Do you see that as extending beyond what Canvas is
today, which is frankly a very simple approach to solving relatively simple problems?
The question that I would come back to is, is for these analysts, what are they doing on a daily
basis that where machine learning could help them? And how do we make them successful in doing that?
And today, Canvas does have some data preparation capabilities, but it's not as sophisticated,
for example, as the, what data scientists would do in a notebook or what they would do in a tool
like SageMaker Data Wrangler, which is a low-code data preparation tool we have in SageMaker Studio.
And so I think we have a question in Canvas right now around, or I guess it's more of a hypothesis,
that the analyst personas don't need to do heavy-duty data preparation, but now that we've
launched a product, we're going to get to figure out how much data preparation do they need?
Do they want to do it themselves? Do they want to be assisted in doing data preparation by data
scientists? At this point, our hypothesis is that they don't, they don't need to do a heavy-duty
data preparation. And a lot of this, you know, this is not just sort of a wild gas, but we spent a
lot of time talking to customers who have analysts who would be using a tool like this, and that's
our sense right now. And so, you know, I think part of what you're asking is how might, where might
Canvas evolve to over time? I think that's one question we have. Another question is the role of
collaboration between the business analysts and data scientists. We have collaboration capabilities
built into Canvas and SageMaker Studio to enable this to happen. I think our hypothesis is that
these users do need to work together. Time will tell, tell us more about the nature of that
collaboration and what additional things customers need. Yeah, I'd love to maybe spend a bit talking
a little bit more about collaboration and the way you see collaboration kind of taking place
in the context of the machine learning workflow in general, and notebooks in particular. I think
when we, it's easy to look at notebooks and what they, you know, taking you from this IDE and
a terminal or terminal that's kind of, you know, landlocked to your computer to a web page that,
you know, could be anywhere and offers the idea of, or the possibility of collaboration.
It strikes me that while that is a natural idea for notebooks, it's under-implemented maybe.
I don't see it being used in that way as often as, you know, I might expect. It's like the promise
of a Google Doc, but, you know, everyone just uses it as a regular word processor.
And I'm wondering, you know, what you observe about collaboration and the ML process in general,
and the way you see that applying to, you know, tools and notebooks in particular.
When we released the, the Python notebook in 2011,
and users started to work with it and began to open issues on GitHub to give us feedback.
And within a very short period of time, I think it was a month or two, one of the earliest feature
requests we had was for real-time collaborations similar to what you would have in something like
Google Docs. And it has continued to be probably the most significant feature requests we have
from the Jupyter community. And so we heard from the Jupyter user base very early on that they
wanted real-time collaboration, that they looked at these notebook documents in a similar way to how
they look at documents that they work within a word processor and wanted to collaborate with that,
that mode of interaction. And so we've, on the Jupyter side, we spent many years working on this,
we've had a number of sort of false starts. It's compounded by the fact that building a,
the needed infrastructure and architecture for real-time collaboration from a, from an algorithm
perspective is quite complex. Thankfully, the underlying algorithms have improved over the years.
And so it, just this year in Jupyter Lab 3, we've launched the first support for real-time
collaboration. And we're using a, another open source library that's been fantastic for this,
called YGS. It offers a very high-performance CRDT implementation in JavaScript. And so that's
really what is enabled us to build real-time collaboration in Jupyter Lab 3. And so if you,
any user of Jupyter Lab downloads the latest version of Jupyter Lab 3, there's a special flag
you can issue at the command line that enables the collaboration feature. With that said,
we're just getting started in terms of the full experience of this. There's a lot of additional
user experience, dimensions that we need to add, other technical dimensions, but it continues to be
a major focus of the Jupyter community. And something that users want and have wanted since the
very beginning. Now, the broader picture of collaboration that you mentioned in machine learning,
I think what, what we see both at AWS and in Jupyter is that there are many different personas
that participate in the overall machine learning workflow. And the key points of collaboration
are between those different personas. So, for example, one, a data engineer who's been
preparing and getting the data ready, hands off a data set to a data scientist for them to work on
it. And I think that that mode of collaboration between personas is really one of the main challenges
we see both in Project Jupyter and in in SageMaker products on the AWS side. And it's very different
from environments where collaboration happens primarily among the same persona.
And it adds that it's not to say that that that pattern of collaboration never happens in data
science and machine learning, but I think that the more the more challenging one is the cross
persona collaboration. Yeah, I could see arguments for that making things
easier in that you have these well-defined interfaces between, you know, not that they're inherently
well-defined, but there's an opportunity to define an interface between the personas, whereas
if you have people in the same, with the same role in the process working on the same thing,
it's easier for them to kind of walk on one another's work, so to speak. But it also defining
those interfaces can be challenging. Absolutely. And that's, it really is, and there's both the
interface from the perspective of a programmatic API, and then also from the perspective of a
like a graphical application. And the other, you know, we quickly get into the challenges of
distributed and shared data structures, and that is some personas tend to work with entities
that are immutable, others with entities that are immutable, and figure out how to
get those personas to collaborate, when the underlying entities that they're dealing with
are fundamentally different. So for example, software engineers are completely familiar with
collaborating using Git and a version control system. But when you look at other stakeholders
that want to interact with maybe the notebooks the data scientists are working with,
they're not going to be using Git, right? They probably want a graphical interface that allows
them to comment on a notebook in the same way that you comment on a word processing document,
right? They're not going to be on GitHub. They're not submitting pull requests and using the
Git command line or anything like that. And yet it's still the same entity underneath. It's still
a notebook at the end of the day. And so figuring out how even what the entities and data structures
are underneath the cross-role collaboration, I think is a really major challenge.
Nariah that I wanted to talk with through with you is the role of the notebook overall.
It's maybe circling back to the very beginning of the conversation. But this is I think a
conversation that's happening fairly broadly in our community. And that is, you know,
often comes up as, you know, our notebooks, the right tool for machine learning or, you know,
notebooks versus IDE's. And, you know, when you think about Canvas in the mix, you know,
maybe the question is, you know, no code versus notebooks versus IDE's. How do you react to
those types of questions? Yeah, this is a great one. So first, I'll tackle the question of
when should you use a notebook versus IDE or is the notebook a substitute for the IDE?
And really here, I think that the question to ask is, what is the fundamental activity or task
you're performing? And in the case of an IDE, typically that task is that you're building something.
You're building software, you're building a service, an API, a software product. And so the
fundamental verb of an IDE, I would say is build. Now, maybe the secondary verbs that would be test,
deploy, debug, et cetera, that go along with that. Whereas if you look at a notebook and what
the notebook was built for and how people use it, I would say that build is probably not even
secondary. And so what is the sort of fundamental activity? I think it's really that the notebook
is a tool for thinking with code and data. That when a user's working with a notebook at the end
of the day, they're trying to work in parallel with the computer to understand what is in the data
and what they might predict. And what the meaning of that prediction is, is their causation there,
is their bias. How can they use this to explain the result? Do they trust the prediction that the
model is making? Can they use it? Essically, these are all human questions. And so the notebook
really is a tool for thinking. And when you have this perspective, there's not really any confusion
between an IDE and a notebook. There are two different tools that are used for two very
different tasks. In the same way that an SUV and a two-seater sports car are two very different
vehicles use for a different set of purposes. And if you try to take an SUV and drive it and get
the sports car experience out of it, it's going to be pretty disappointing and vice versa.
And so when I want to hear people sort of complaining that Jupiter's not a very good IDE,
my sort of the filter I read that with is more along the lines of someone saying that an SUV is not
a very good sports car, namely, yeah, it's not. It wasn't designed to be, Jupiter was not designed
to be an IDE in the same sense that it's used for building and deploying and debugging software
products. With that said, there is a gray zone where users start to work in a notebook interactively
thinking about code and data. And at some point, a project gets to the point where it's mature enough
that people start to build things. That transition is still really painful. And it's painful whether
you try to keep working in Jupiter or you go from working in Jupiter with notebooks over traditional IDE.
And I think that's a major area of innovation that there's a lot of potential for the Jupiter
open source community and others to dive into and figure out what does this transition look like
from thinking with code and data to building software products. And how do you make that transition
and work in that sort of in between zone? Yeah, I was thinking there are a number of efforts,
you know, taking different approaches to try to productionize the notebook.
And I'm sure you've seen these as well. And up until the very end of your comment, I would have
thought you were thinking that those are all misguided. But it sounds like rather,
there's just attempts to figure out this confusing thing that we don't really know what it needs to
look like just yet. Yeah, there's a number of efforts in the Jupiter open source community and
other open source projects around taking notebooks and using them in a more production oriented way.
So one idea that's a it's a Jupiter sub project called voila, it allows users to tag cells in a
notebook and then turn those cells into an interactive dashboard that looks like a web application,
does not look like a notebook and deploy that to users. So that would be more of a human oriented
deployment of a notebook to a group of users who never want to look at the code and and the
notebook, but who want to interact with the outputs of that notebook. Maybe another example,
as you brought up was idea of scheduling notebooks. And that is that that at some level,
you can write a notebook like you can write a Python function, a notebook could be parametrized
by a set of arguments. And then you might want to run that notebook for a different set of
arguments on some schedule. And both of those usage cases are things that we're seeing. And I think
different Jupiter users and AWS customers are doing things like that. I think the more
challenging cases in this space are where you want to
build a machine learning model initially in a notebook, but eventually transition to
building a model as part of a broader pipeline that leads to the deployment of a model with an end
point, that transition between a notebook and the more traditional building that you do in an IDE
is still quite painful. And I don't know that these notebook-driven dashboards or notebook
scheduling are really the right answer to tackle those. Before we wrap up, I wanted to cover another
of the announcements that was made at ReInvent. And that is the new Amazon SageMaker Studio Lab
product that you and your team worked on. Tell us a little bit about Studio Lab and how it came about.
Yeah. So as you mentioned, Amazon SageMaker Studio Lab was launched at ReInvent this year.
And the origin of this really comes back to the following question. And that is
what's the minimum set of things that someone needs to get started with machine learning?
And I'm using the phrase getting started with machine learning very broadly here. This could be
students who are learning about machine learning and data science in a university class.
They could be learning on their own in a self-paced way. Or it even extends to
people who already have a good amount of machine learning expertise and are more enthusiast. And not
doing machine learning though in enterprise context where they have a support staff that maintains
cloud-based infrastructure for them. And really, if you look at how people learn machine learning
these days, there's a small set of things they need. One, they need notebooks. They need some
way of using Jupyter notebooks. Two, they need open source packages for machine learning.
They need tools such as NumPy, Pandas, TensorFlow, Scikit-learn, PyTorch, etc.
And then they need some place to run the code. They need compute of some sort and storage that
goes along with it. And those basic ingredients are really what is how we approach SageMaker Studio Lab.
So it really is an abbreviated version of SageMaker Studio that provides a notebook-based
development environment for users where they can use a Jupyter Lab-based environment.
And we offer free compute and free storage along with this. And so the real significance here
is that users don't need to have an AWS account for this. They can sign up with an email,
no credit card required. There's a simple account approval step that takes a few hours.
Once you have an account, you get 15 gigs of persistent storage for your project.
And then you can attach that storage to either a CPU or a GPU runtime and do data science
in machine learning and work with it in that context. And it's all free. And because there's
persistent storage behind this, even though we obviously behind the scenes, we shut down the
instance when you're not working, when you come back, all your files will still be there. And so
this is a real file system that's persistent and allocated to your project. You can check out
Git repositories locally. You can install Python packages persistently and save your data sets
and notebooks, notebooks alongside of all those things. And is it from the perspective of
building the products for the intended scale? Is it a, how did you think about this as a product?
Is it a ground up effort starting from components like EC2 and all the other components that AWS
has in Jupyter or, you know, is it a starting from SageMaker and kind of trim back? How should we
think about the way this came together? Yeah, that's a great question. So when we built Amazon
SageMaker Studio, that was launched at Reinvent two years ago, we built a platform that enabled us
to build these types of applications where you have an interactive user interface connected to
compute underneath with a persistent file system. And so that platform was already there with
SageMaker Studio. And we've reused that platform for SageMaker Studio Lab. Now there's some key
differences between the two and some common points. I'll start with the common points. So part of
the reason we built this platform for SageMaker Studio was that when we talked to customers,
they had security needs that could not be satisfied in a traditional Kubernetes environment.
And so the platform that we're using for SageMaker Studio and for Studio Lab is we're not using
Kubernetes. It's based on instances. Our customers have told us that they need instance level
isolation from a security perspective. And so a lot of the work we've put in on the SageMaker
Studio side with encryption at rest, encryption at transit, VPC support, instance level isolation.
We've been able to take that and apply it in the SageMaker Studio Lab case. And so one of the
sort of hidden features of SageMaker Studio Lab, even though it's not focused on large enterprise
usage cases, it still has all the enterprise security instance level isolation that we have on
the SageMaker side of things. So we we're able to reuse that. A lot of the the magic of this
platform is because it's instance space, there's a potential for to take a long time to start
instances. We've done a lot of work and innovated pretty incredible stuff that enables the
instance start times in SageMaker Studio Lab to be fast enough that it's not really going to get
in the way of most people. Obviously, it can't be hundreds of milliseconds or something like that,
but it's fast enough. I find when I'm using SageMaker Studio Lab that the the runtime
an instance start time is fast enough that I don't really think about it. And so we have been
able to reuse that platform for SageMaker Studio Lab. And the other point where the enterprise
security is important is that we wanted SageMaker Studio Lab to be a place where we could tell users
it's fine if you want to install your AWS credentials to another AWS account that you have a paid
AWS account and make calls out from the AWS SDK or command line from SageMaker Studio Lab.
And if we didn't have that enterprise security in place, we could not tell users great install
your AWS credentials on the on Studio Lab. Does that mean that there's a key store feature or that
you consider the instance level security to be robust enough that I can just put my
AWS keys in a cell in the Jupyter Notebook. So please don't put your credentials in a cell in
the Jupyter Notebook. That is definitely an anti-pattern. And we regularly see customers who do that
and then later forget about it and version control the notebook in the end of yeah the credentials
end up on GitHub. So that's why I was asking. It's a great question. So the model is that your project
gets 15 gigs of persistent storage and that storage is encrypted. And so you can install your AWS
credentials just like you normally would on your laptop. And because the entire drive is encrypted
and we handle it with all the necessary security precautions, you can install your credentials
that way. But yes, putting them directly in a notebook obviously is not a recommended.
In theory, not because it's any less encrypted, but because you're more likely to put it
someplace where it shouldn't be. Yes, if you never move a notebook out of SageMaker Studio Lab,
that notebook will be saved on that same encrypted volume and it will be fine. The risk is more
that that users would later do something with the notebook that expose those credentials outside
their original context. Maybe just one more kind of to wrap things up. Where do you see all this going?
What are you most excited about in terms of the future of ML development and
human computer interfaces for machine learning? I'll answer this personally.
And for me, I thrive on ambiguity and challenge. I want to be working on things where the answer
is not known, or it's not obvious, or there's significant challenges involved in coming to an answer.
I, when I look at this space, the amount of challenge in ambiguity that we have left is vast.
And so I find a lot of just a lot of enjoyment in working in a space where there's so many
unanswered questions. And some of it, this does come from my background in physics and
I love physics. And at the end of the day, I'm probably still a physicist. One of the challenges
in physics, though, is that as the field has grown and more and more is known in the physics space,
there are fewer and fewer unanswered questions that are available to be answered that are really
deep and interesting. Now, there are some obviously, but you really have to hunt in physics, honestly,
is my, my sense as a physicist is that you have to work really hard to find good problems to solve,
whereas in this space, there's an abundance of problems. And they're all really challenging
and really interesting. And there's a lot of people who will benefit by solving those problems.
So that, that's what I'm excited about in terms of looking forward in this space.
Awesome. Awesome. Brian, thanks so much for joining us and sharing a bit about what you've
been working on and some of the new stage maker and studio announcements coming out of reinvent.
Thank you so much, Sam. It's really great to be here and talk to you about all these things.
And I appreciate your time and questions about the early history of Jupiter, which is really a fun
story to tell.
