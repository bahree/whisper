Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting
people, doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington. Today we continue our exploration of trust in AI with this interview
with Richard Zemel, professor in the Department of Computer Science at the University of Toronto
and Research Director at the Vector Institute. In our conversation, Richard describes some
of his work on fairness in machine learning algorithms, including how he defines both
group and individual fairness, and his group's recent Norrop's poster predict responsibly,
improving fairness and accuracy by learning to differ. Thanks once again to Georgian
partners for their continued support of the podcast and for sponsoring this series. Georgian
partners is a venture capital firm that invests in growth stage business software companies
that use applied artificial intelligence, conversational AI and trust to differentiate
and advance their business solutions. Post investment Georgian works closely with portfolio
companies to accelerate the adoption of these key technologies for increased value.
To help their portfolio companies hire the right technical talent Georgian recently published
building conversational AI teams, a comprehensive guide to lead you through sourcing, acquiring
and nurturing a successful conversational AI team. Check it out at twomlai.com slash Georgian.
And now on to the show.
Alright everyone, I am on the line with Rich Zemel. Rich is a professor in the Department
of Computer Science at the University of Toronto as well as being Research Director at the
Vector Institute in Toronto. Rich, welcome to this week in Machine Learning and AI.
Thanks very much. It's great to have you on the show. I'd love to get started by hearing
a bit about your background and your path to your work in machine learning.
Sure, so I've been interested in machine learning or I've been interested in artificial
intelligence for a long time. In fact, since high school, I got lucky and had a summer
job and I was growing up in Pittsburgh and had a summer job at Carnegie Mellon working
with my sister neighbor was named Hans Berliner, one of the original person who worked on
road game playing programs for played games like Backam and then Checkers and had some of
the original ideas that later led to some of the ideas in chess and go the famous game
playing of these days. So yes, I've been interested in it since that time and worked for companies
back in the 80s, artificial intelligence companies back in the 80s when the AI went
to help first hit. So that was my first experience with AI. I went to graduate school after
that and got my PhD in machine learning, finished in the early 90s and I've been working
in it ever since. Oh, fantastic. And I mentioned your post with the Vector Institute in Toronto.
Can you share a bit about what the Vector Institute is up to there?
Happy to. So the Vector Institute is an independent non-profit institute that's a combination
of academia and industry. So there's a lot of faculty members here who have their home
in universities such as University of Toronto or Waterloo or Guelph and as well as more
distant universities like Delhause and UBC in Canada. And those people are all doing machine
learning research. We also have some research scientists who are with their primary employment
is here at Vector. And there's a lot of graduate students and postdocs associated with Vector
or affiliate with Vector all doing research in machine learning. And so that's my focus
is the machine learning research side of it. There's also another very strong side of it
which is industry and working with industry and trying to help industry grow in terms
of their machine learning capabilities. So Vector has funding from the federal and provincial
government in Canada as well as a lot of industrial sponsors to both to help the kind of industrial
business side of machine learning as well as the research end of it. Now I've taken a look
at some of your publications and you've got a pretty broad, what appears to be a pretty broad set
of research interests. Can you talk about where you tend to focus and some of the things that you're
working on? I've been interested in some of the kind of long-standing problems in machine learning
and in AI in general. Seen understanding is an example. So how can the computer really understand
what's in the scene? How can it pick out somebody saying something of interest to you across a
crowded room or find your friend in a crowded bar? So it's like the kinds of things that were
quite good at have been hard for computers for many years. So that involves good learnings,
a lot's taking a lot of data and trying to find patterns in it. And one of the things we've
learned over the years is that the important thing that machine learning can add to this is the idea
of learning good representations for inputs based on the data. So any kind of learning of
representations that involves what's known as unsupervised learning where you may not be told what
to take out of the data. You just try to do it. The system tries to figure out itself what are
good features for the learning. And so that's my research. And more recently I'm interested in
things where the aim is to come up with what we would call a more structured representation.
Something where the human has some input into this and can define what it wants. So it can say,
for example, we know that there's going to be, if we want to recognize a face that there's
going to be certain features of the face that we think are important. So the human is giving
some hints that the computer is then going to use. So we've made a lot of good progress on these
kinds of things, I think, much more so than I originally thought in some ways. I thought
it was going to be a long way off, but we're making a lot of good progress in this area.
That example you gave about the features in the face, it does resonate with this trend that I'm
seeing in machine learning research where we've started with these models that, in many cases,
at least, are very much grounded in the physical world, in the case of computer vision,
kind of feature detectors and edge detectors and things like that. We kind of swung to the
other end of the pendulum with deep learning where we don't explicitly do any of that stuff.
And now, more and more, we're kind of trying to find a middle balance where we're incorporating
in our knowledge of the physical world, but still trying to use the power of deep learning.
Does this work fit into that mold?
Exactly. So I think that, yeah, so for an example, something that we've recently made a lot of
progress in my group and many other groups is where the input to the learning system isn't just
like an image or something, or, you know, a sound bite. Instead, it's something that is represented
as a graph. So you have nodes and relations between the nodes. And so then you know,
so you have some sort of, that's what I meant by a structured representation. And the idea is that
then you can learn from that. So you're kind of given something, some sort of information that's,
you know, not starting from the basics, but given that, we're able to do a lot of interesting
processing on top of it. So I think that's something where it's combining some kind of information
that humans have in a lot of different forms, right? We have a lot know about objects and their
relations could be in big databases or, you know, descriptions of the world. And then we can learn
from those on top of it. And the biggest challenge is not to start with that kind of relational
information, but try to learn it as you go along. So that's the kind of the frontier, I would say,
in this area. And then you've also have an interest in fairness in machine learning and of
public several papers in that area. Can you talk a little bit about your work there? For example,
you've got, I noticed you've got a paper that has been accepted to the upcoming NURPS conference
that will both be added in Montreal. Maybe we can start with that one.
And I'll interrupt if you, if you would like to kind of broadly,
you know, contextualize your interest in that space and your, the kinds of things you're working
on, we can start. We can sit there. So I've done some other work with some colleagues on fairness
and was invited to a workshop a few years ago in Washington, D.C., where the discussion was about
computer algorithms and the courts. And so this was a workshop that had people studying
civil rights and as well as people from the legal system, judges and lawyers as well as some more,
people who are more on the activist side of things like the Million Hoodies group. And so I think
it was quite an interest and then there were some computer science people. And so it was an
interesting workshop. But one of the things I learned, and a lot of debates there, one of the things
that about how much influence computers should have and those kinds of things. So I learned a lot
of that meeting about how computers are being used to set bail and for sentencing or at least to give
advice. It's the kind of thing like I learned about a system in Pennsylvania where there was
the input to the system was a description of a defendant. And the system would then come up and say,
what's the probability that that person is going to commit a violent crime in the next few years.
Right. So these, these kinds of systems have become quite publicized in the popular press.
But this one was one that was developed specifically for the state of Pennsylvania. And it was
intriguing to me because it would came up with some probability, like I would say, probability of 0.7.
This this defendants is going to commit a violent crime. And then the question is, what does that
mean to the judge? How does the judge use it? And was that number really meaningful?
So one question I asked the person who was what? A description of the defendant, something about
the defendant's history. And it wasn't that clear. They said they weren't able to get access to
too much information about the defendant for various either privacy reasons or just not having
having enough information. So it'd be things like what kind of crimes the person has committed,
what they were arrested for, what they've already been convicted for, that type of thing.
And the shocking thing to me was just how little validation had gone into the system. So there
wasn't a real sense that they had tested the system and said that 0.7 really meant what you'd
hope it would, which would be that that in some unseen data, 70% of the people who were assigned
of probably 0.7, 70% of those people actually went on committed to violent crime. So you'd like to
have some, so we call it calibration, that's a well calibrated system. And in talking to the people
who developed it, they said there really wasn't that much enough data. And the data was quite noisy
that they couldn't do a very confident job of calibrating it. And so my, I was very surprised
to find that this was actually in use in the court systems in Pennsylvania. So my reaction was,
well, we got to do a better job before machine learning people. This is at its heart, a machine
learning problem. So we should be working on this and trying to figure out how our systems can
interact better and do a better job working in this. So that inspired the paper that we have in
the upcoming NURPS conference. We call it predicting responsibly. Here, a machine learning system
isn't making the ultimate decision. It's more of an assistive tool. It's something that's
providing an input to someone like a judge or a doctor or whatever it might be. And so you want
that assistive tool to produce information that's useful to the final decision maker. And so what
we were doing in this case was saying, imagine that the tool comes out and says not yes or no,
this person is going to commit a violent crime. But it could, you know, it's not going to even
produce a probability because it's unclear what a judge or somebody might do with that probability.
But a simpler problem is to say yes or no or kind of pass and just pass that information on and
allow the judge in this case to make the decision more clearly so that on their own. So it's kind
of like an initial system that's doing a call of kind of saying, you know, we're narrowing down
with the next step that the decision maker would have to pay attention to. That could be useful
in a lot of settings like, you know, and there's too many inputs and you want too many applicants
to a job and you want to have some initial calling system. But the key is this calling system,
the first system that we're building has to be fair in some sense. It has to not just try to
be accurate and say yes or no or, you know, pass it on, but try to take into account information
about the defendant in this case that would explicitly try to not discriminate against that
defendant. And that could be based on society definition of what discrimination is, right? So
discrimination could be based on, you know, race or ethnicity or gender or various things. So
that's what, in general, that's what the area of fairness is about is saying that, you know,
you're training up a machine learning system and you want it to make decisions that aren't biased
against any particular, and most of these, most of the work in fairness has been done where that
particular group is defined ahead of time, right? We like I said, it could be race or gender
or, you know, or age or whatever the attribute may be that you want to be fair to prevent
discrimination against. So that's what the aim is in this general and fairness. And that's what,
that's what we hope in this particular system is that when it makes a decision to say yes or no
or to pass it on, it's making those decisions in a way that is fair. That's taking into account
and making decisions that are ensuring that there's not discrimination against the, what we call
the sensitive attribute. We talk a lot in machine learning about how important problem definition
is and just listening to the way you are describing this system and it's clear that a lot of
the path that you take is set out by how you, you know, define the problem more so, you know,
perhaps than even in some other areas. Have you learned to observe anything about that particular
challenge as applied to this space in your work? Yeah, so you've, yeah, you've hit on the kind of
key key question in this work, which is about how do we define it? And I agree that's a,
it's in general a machine learning problem that's often not paid that much attention to in the
sense that, you know, we were used to machine learning defining classification problems or,
you know, something like an image classification is this, is this a hippopotamus in the image
or is it a dog? And for that, you can say, well, there's a right answer and you can judge whether
there's a right answer or not. And so that's just us, you know, not, but the answers you get depend,
depend a lot on how exactly you score your answers, right? So is it important to, you know,
is it better to say this is a hippopotamus if the right answer is a rhino than to say it's a dog,
for example? So if you have a scoring function that's sensitive to how similar the classes are,
then you'll get a very different learning system than if you use a definition of an error that isn't
sensitive to that. And that applies in spades when we come to the problem, the area of fairness,
because, you know, it's an area that, so I started working on it about six years ago. My wife,
Tony Potassi and I spent the summer with Cynthia Duwork and her colleagues and Microsoft Research
in Silicon Valley and we were working on exactly trying to come up with, we landed on this problem
with fairness and we worked hard on trying to figure out what is a good definition of fairness.
We spent the whole summer debating it. And we ultimately came, we wrote a paper where we came up
with two different definitions. So this is with Omar Ryan Gold and Moritz Hart in addition to
Cynthia and Tony myself. And we wrote a paper where we came up with a group definition of fairness
or an individual fairness. So a group definition of fairness would be one that would say,
you know, overall, for example, decision is fair if, let's say, the two groups have the same
number of positive outcomes assigned to them. Okay, that's a very simple thing that's like a kind
of thing like affirmative action gets at, right? So if you want to give admittance to a school,
you know, we have to say that the same number of males and females should get in, right? So that's
group fair. That's an example. And then there's kind of more, the individual level of fairness is
saying, well, what you really want is for an individual that other individuals that are similar to
that individual should get the same outcome, the same decision, right? So that's more in the individual
level. And since, since, so that we wrote a paper where we had that and talked about various
ways of carrying that out. And in the main idea was that when this decision made is made,
it should be aware of whether the person belongs to that group. So we called this fair
through awareness. And then in the years, since then, there's been a lot of debates about what
the proper definitions are of fairness and like different kinds of group fairness and different
types of, you know, individual fairness. Somebody wrote a paper about the, you know, 21 definitions
of fairness. So exactly like you said, you know, defining it is the key problem and that's what,
that's what makes it very interesting and challenging. It's a fun area to work in partly for this
because, you know, you can come up with a definition to debate whether it's right or wrong. I would say
in the end, none of them are actually right, but some of them are less wrong than the others.
And so in the paper that you were describing, one of the things that I kind of zeroed in on
quickly in your description was this system is designed to output, yes, no, it kind of almost
sound like yes, no, maybe yes, no pass in particular where pass is deferring on making a decision.
And the idea there is to with the pass presumably to kind of recognize the system's own uncertainty.
Yes, so it's a combination of things. So, yes, so people have worked on the idea of what's
called been rejection learning in the past. And that is, you know, saying, well, we can say,
I don't know or pass when the system's not confident, right? So back to the idea of, you know,
the defendant is defendant going to commit a violent crime. Well, we can say pretty confidently,
you know, within some error bars, it's going to say yes, or pretty confidently, no,
even though you can see in those cases that confidence is a little hard to assess, but
or and there might be a big space in the middle of between those two where the system isn't very
confident. And so the idea would be that if it's I don't know, it gets passed on to the next
to the decision maker. But in our in our work, the idea was that we aren't going to just pass on
when based on uncertainty, we're going to pass also taking into account what we know about the
downstream decision maker. So imagine that, you know, the system is trained up on judges decisions,
and it's also trained up on or in a doctor case rate, it knows a lot about the doctors. And so
it's going to say taking into account the kinds of decisions that the judge or the doctor tends
to make and how, where, what kinds of problems they're accurate on, what kinds of people they're
fair or unfair to, that the that could influence our machine learning system when it wants to say
pass versus yes or no. Okay, so that's the key idea. So it's going to defer when it it's it'll
be smarter in terms of when it defers based on knowing something about what's going to happen
downstream and not only smarter in terms of being more accurate, but also less discrimination.
So specifically, the the system, if it knows that it doesn't have a high degree of certainty
about a particular decision, but it knows that the the decision maker is worse in a particular
category of decision making, it might make the decision anyway. That's right. And so that's on
the accuracy side. And it could be that if it's not very confident, but this person happens to be,
you know, in this protected set, right? So it could be in a particular gender or something that
in it knows that this particular decision maker happens to be discriminatory against that group,
it could decide to make a decision rather than sit that deferring to the downstream decision maker.
Another thing that jumped out at me here is I think in the context of kind of these examples
with the courts, you know, what, you know, if you think about this relative to thresholds,
and you know, say that the yes represents some kind of, you know, guilty or something with
negative implications, right? So guilty or higher bail or longer sentencing or what have you.
You know, the threshold ban that I might want to attribute to a yes is going to be maybe different
than, you know, what I would attribute to a no because it has a much greater human impact.
Does the system come for that at all? Certainly. So that goes back to our definition in some sense
or what we would call the loss function, right? So it could be that making certain kinds of
errors are worse than others, right? So making a false positive, like, you know, some would say,
like letting somebody out of jail when they are going to commit the violent crime, potentially is
worse to society than, you know, then locking up an extra person who really shouldn't have been
locked up. That's debatable, right? But I mean, there's, but again, so it's just a lot of these
decisions are kind of these definitions have to come from society. And so, but I think, but certainly
on the machine learning side, it has the flexibility to do that to say, you know, what certain kinds of
errors are more costly than others and to set the thresholds appropriately. But I mean, but that
does lead to these questions about, you know, how are we going to decide, you know, who does what
kind of errors are more costly, right? And so that's a, I think something that society has to weigh
on in on. Similarly, you know, what kinds of attributes do we want to not describe,
what kinds of people do we do not want to be biased against? It's another kind of question.
So part of the aim here goes back to what we were saying earlier is that we really want to
add some, some, not my view is we're trying to add some knobs to a decision-making system.
So a machine learning system, add some knobs to a black box so that society or other people can
come in and add some control, has some insight into what's going on and add some control to it.
And so it's control in this way by like you're saying, you know, it could be that certain kinds
of errors are more important and it could be that we want to reduce the bias against this group or
that group and what does that actually mean? I'm curious for the decision maker, the system has to
have some, you've got to be able to represent the decision maker's decisions in some ways.
Is it you providing it a distribution or a set of rules or something else?
Well, the decision maker is going to make its decision, you know, based on whatever features
it was originally going to make its decision. So the, at least the way we formulated the problem
is that our system is either going to make a decision yes or no or pass it on. And when it gets
passed on and that the downstream decision maker, you know, gets to look at the case and just
review it on, on their own. So they get the same kind of input features. They would have
anyways, you know, another work that we've done in the fairness approach is that we're actually
taking in a little bit of a different attack on this. We're saying what we'd like to do is change
the representations of come up with new features that could be used by a downstream decision maker.
So that's not this particular paper you were talking about, but other work that we've done in
this area is, is about that is about constructing a different representation of somebody so that
the downstream decision has our own representation that we've constructed available to them rather
than the original representation. But in this paper, the model is learning based on some observations
from the decision maker. And so I guess my, my prior question was, are you, are you handing it
some kind of representation of the decision maker in the form of a set of rules or a distribution
or is it more like an active learning thing where it's actually observing, you know, independent,
I don't know, coin flips or whatever. I guess it's still a distribution of some sort.
I mean, yes, you're right. There's lots of different ways we can formulate what the downstream
decision maker's doing. So the way we're thinking about it would be that the
imagine that there's a database available of, you know, some dataset available of this downstream
decision maker or decision makers and how they've responded to cases in the past. So that's kind
of a separate training set that we're observing that. And we're taking that into account when we
build a model of what's the decision maker likely to say for this new, new case. And then that's
when our system is deciding yes or no or pass, it's, it's using it, the model we've developed
separately of that downstream decision maker. And is this, is the system is it something that
you've implemented? Is it something that you've modeled mathematically? Like how?
Yeah, so we, you know, we don't have good data for this. It's very hard to get, you know,
it's something that we've implemented and we, like it, like often happens, unfortunately,
in this fairness area is that there aren't great datasets out there. There's very few. There's one
that's well known as this compass dataset, which is a dataset like I was talking about that looks
at this exact case of here's a defendant. And what's probably in the system's aim is to come up
what's the probability that person's going to commit a violent crime in the next three years.
So that's a very well well known dataset. Some of the other datasets that are in use are ones
that we've actually constructed ourselves where we'll take a existing dataset and then we'll make
it into one that's relevant for fairness by identifying some sensitive attribute. Like we'll say
there's one known as the adult income dataset where you're trying to decide the decision to be made
is this person going to, you know, is there income greater than 50,000 or not, right? So that
could be useful for for loans or whatever, but the we made it into a fairness dataset by saying
taking one of the attributes gender in the gender of the person and saying we want to make sure
that the decisions are made are not only accurate about the income, but they're also fair with
respect to gender. So that's so we take existing datasets and they can be relevant for fairness by
identifying one or more attributes that are, you know, the relevant things for preventing discrimination.
So that's what we did in this case is that we took some existing datasets and implemented it.
And then what we had to do though, we had to simulate the downstream decision maker. We had to make
up, you know, what is this downstream decision maker? And so we tried to think of three different
cases. One case is a downstream decision maker that has more information available than the
the system we're building, right? So I imagine it's a judge and the judge gets to not only get the
same kind of information our system does about the defendant, but also gets to see the person face to
face and ask the person questions and gets additional information about the defendant that way.
So in a way, it's a more knowledgeable judge, but let's say that judge doesn't care about being
fair, okay? It just wants to take make make his or her best decision. So that was our model of
one version of a decision maker. We had other ones too, ones that were intentionally unfair,
ones that were intentionally discriminating. And so what we did is we trained up our system based
on these different simulated downstream decision makers and then observed what would happen.
How do you characterize the results? So the results are character, I would characterize
as saying that it depends what you want to compare to, right? So with it, so an interesting
comparison is to say, how would we do if we A, we could force our system to make a decision.
So there's that and not allow it to defer, right? So that's a kind of standard machine learning
classification problem. We can take the decision maker out of the loop and just use our system.
The other version of it is one where we take our system out of the loop and just use the ultimate
decision maker, right? So those are the two extremes. And then we have different versions of our
our system, one of which just says I don't know without paying attention to what it thinks about
the downstream decision maker. And then the fourth is the kind of ultimate goal, which is what we
wanted to really propose, which is this idea of learning about the downstream decision maker
and taking that model into account in making our decisions. And in general, and we looked at
these different scenarios, and in general, the effect was we wanted in the sense that we were able to
achieve a pretty good tradeoff of fairness and accuracy using the system as we had formulated it.
It's certainly better than either just relying on the downstream decision maker or relying on our
system alone. And the interaction where we were able to take it, build the model of the downstream
decision maker that was pretty good did improve overall both in terms of accuracy and fairness.
I came across another paper that you worked on recently learning adversarially fair and transferable
representations. Can you give me an overview of that one? So that one is along the lines of what I
was mentioning earlier where the goal now of the system isn't necessarily to make a decision,
but rather to come up with a representation. So this fits in with what we were talking about early
on that machine learning systems. One of the big advances that we've had is in coming up with
representations, right? So learning good features for visual recognition or for speech recognition.
And so this is along those same lines. And so what we've worked on for several years is what we
call fair representations. And that is coming up with representations for, let's say, individuals in
this case rather than images or text bite sound bites that are fair in some sense. So and obviously
again, it depends on what your definition of fair, but one intuition there would be what we'd like
to do is have a representation of an individual that is clean of that doesn't have any information
that kind of obfuscates any information about whatever the sensitive attribute is, right? So
for instance, we'd like to come up with a representation of you where it's not clear if you're
a male or female because if we want to ensure that that any classifier using that representation
will not have be able to discriminate against you based on your gender, then we want to remove
all information about gender in the representations. So then that will ensure that the downstream
classifier won't be able to make a decision about you based on your gender. So that's the
notion of fair representations. You kind of remove any information about gender before handing off
that representation to a classifier. You mentioned an image previously. Is that typically the domain
that you're working in for these fair representations like something that's characterizing, you know,
that starts from an image of me and then generates a genderless or raceless image, or is it more abstract
like you you're looking for some embedding space that isn't correlated with race or gender or
things like that? Yeah, so it's more of the embedding space that isn't correlated with race or
gender, and generally so far we aren't starting with though the original representation isn't an
image. It's, you know, think of it as a database record of your demographics or something, right?
It's about where you live and how old you are. This is something that you might be using
you know, and another kind of setting that's not based on images or hearing your voice or anything.
So it's, you know, a typical kind of let's say demographic record of an individual and that we
want to do is take that demographic record record and construct a representation of it like an
embedding of it as you describe where that embedding has lost information about your gender or your
race or something like that. Okay, so that's where they identified attribute we've removed information
from that. So that was, so that's the idea of the fair representation. So our paper now it's
paper with David Madras and Elliot Krieger and as well as Tony and myself and that paper the idea
was exactly, as you described, you want to cover it in embedding that doesn't have information about
that attribute, let's say it's gender. And the way to do that is to construct an adversary where
the adversary is going to take the representations or embeddings that we construct and try to
pull out the information to figure out what is the gender of that individual. So we're trying to
create a good embedding that will thwart this adversary, make it impossible for it to pull out that
information. And the interesting thing about this is, so that idea is, you know, this adversarial
approach, but we can then come up with a, there's been, as I said, there's been a bunch of
definitions of fairness that have gone beyond the original kind of group fairness that we described.
And so one of them is, it's called equalized odds. Another way of saying that is, you can think
of it as a balanced errors. So rather than making decisions that are balanced between the two groups,
you know, like 60% of the people of the males and females are going to get in. Instead, another
criteria for fairness is that you want that when the system makes an error, those errors are balanced.
So, you know, whatever the errors are, half of the errors are for males and half of the errors are
for females. Or, you know, if they have the same kind of base rate or, you know, if they're 70%
as many males in the databases as females and 30% females, then 70% of the errors are males
and 30% of them are females. Okay, so that's a different fairness criteria. And we can,
and we can adjust our adversary to reflect that fairness criteria. So it doesn't have to be that,
so it could be that it's only going to try to extract the information about gender on the error
cases. Okay, so that's the, and we want to thwart the adversary from doing that. So all I'm saying is
that the, you know, we can kind of tailor our system, our adversary to different definitions
of fairness and then train it up in that way. And so that's the main idea and what we call laughter
learning adversarily, fair and transferable representations. And the key notion is that,
so why do we want to come up with these representations that are fair is that this transfer idea.
So it might be that we want to use that same representation in other settings. So we might want
to say now I have this individual, I've taken your demographic information, I've taken out your
gender information. And now I want to see if, and now maybe many different advertisers may want
to decide whether they're going to advertise to you or not. And by making this new representation
that doesn't have your gender information, we're ensuring that all of those different advertisers
when they work on that on that new representation won't be able to discriminate against your,
based on your gender. So that's the transfer ideas that we can take that same representation and
use it in different settings for different kinds of classification problem. And if you come across
anyone doing something like that in practice, you know, identifying some representation that
is fair in this way and using that for downstream decisioning. I've talked to a number of people
in companies where they're certainly concerned about fairness and trying to ensure that they have
a classification system that is fair. And so they're interested in this idea of having a representation
that would be kind of, you could give a stamp of, you know, a fairness to that representation that
they could use multiple settings. I don't know anybody's actually using that idea in practice,
but certainly there are more and more interest in companies where they want to be able to
A, assess to what extent is their system fair that they're using and, you know, improve its
fairness. And so they, I think this notion that you have a representation that could be used
for multiple, multiple settings, multiple problems is of interest to people, but I don't know
if anybody actually doing that yet. You mentioned the assessment piece. So this can be used,
the, this method could be used for assessment independent of whether you're actually using
these kind of representations downstream, right? That's right. I mean, you can evaluate the kind
of, you know, based on whatever definition of fairness you might want to have, you could say,
how well does this, you know, to what extent is this system violating that, right? So that's
kind of like gives you an idea that says it, you know, are they errors? One definition I mentioned
is balanced errors, definition, or equalized odds. You can say how much is our current system
violating that? It's a little bit hard to quantify, you know, to the degree, but you can say
on what percentage of the cases is it violating that or having out. So, you know, so there,
so certainly people are interested in this notion that you can kind of audit an existing system
and say how well is it doing based on various fairness metrics? So we're talking about a couple
of your recent papers in this space. Can we take a couple of minutes and maybe get your perspective
on the broader landscape around fairness and machine learning and what some of the big
challenges and opportunity areas are? Yeah. So I think fairness is a kind of interesting
microcosm of the machine learning in general, right? So some of the same ideas we've talked about
about adversarial ideas is reflected in general and in machine learning, trying to come up with
representations that are have identify either, you know, separate out or in our case, reduce the
information in a particular case. And I think that's true of in general, but the rest of the
fairness work these days. So one one interesting bit of work I would highlight is on fairness and
causality. So in general, machine learning is very good at pulling out patterns and saying what
which things are what aspects of data are correlated with some label movement want to give.
But there's a big push now to try and come up with more causal reasoning. So it's not just
that things are correlated, but they're actually causal and that would enable us to do
counterfactual reasoning, right? So not only if you have something that's causal, you could say,
well, what would hypothetically happen if we flip that that bit and one of our causes changed it.
What do we expect to happen, right? It's a big challenge for the field because generally that
means that kind of data isn't available in most cases. And so it's, you know, there aren't great
data sets for it. And it's something that's, but it is an aim that's generally true in machine
learning. And I think it's really important for the case of fairness because understanding the
underlying causes for why some decision is made may be an important step to try to reduce
discrimination. So that's a, I think a very, very interesting area of research. We actually have
a paper in that direction coming out at the next fat star conference, but I think there's been
great work in the field. Some number of papers on things called counterfactual fairness in other
areas and other kinds of papers in that. That's one thing I would say is causality is another one
which is there's a simplification I've mentioned, which is that you have a single sensitive attribute
like its gender or race and in the real world, we know there's many sensitive attributes and
we might want to be ensuring that the system is fair with respect to several at a time.
And that in itself is not easy. So there's some nice work out of Stanford and some parallel
work out of Penn research groups. Well, something they call fairness gerrymandering, which is,
you know, just a cute name where it means like you can be fair with respect to one dimension.
And that may make it less fair with respect to another. So how can you simultaneously be fair
with respect to several attributes? So I think that's an important and interesting area in that.
It also makes me wonder if some of the recent work happening around multitask learning could be
applied in this space. Yeah. So I think the multitask learning was part of our inspiration for the
the laughter thing, right? Which is your transfer ability, right? So you want to have a same representation
that's useful for multiple tasks in that case. And I think the multitask, in this case, it's kind
of harder than multitask. So I think when you have this multi-attributes, you want to be fair
against all these different attributes. And the real challenge is that it's kind of like,
it's also related to these other areas of machine learning. Like I said that, you know,
I think it's a microcosm machine learning. It's a lot of work and few shot learning where you want
to be able to learn with little data that's available, little label data. So it may be that you have
some attribute that you want to learn, that you want to be fair to, but you have very little data
available for that. So how can we, that really taxes a machine learning system, right? It's
how it's very hard to do that. And then there's the kind of big open question. Maybe we don't really
know what what attributes we want to be fair to. But you know, so rather than humans deciding
that it's gender and race that are important, it could be some other group that's actually
being discriminated against. And how do we, you know, so that's a bigger kind of open question of,
you know, undefined attribute discrimination. And people are thinking about that these days as well.
So I think those are some of the main areas I would say of in fairness that are currently
of interest. And like I said, there's a lot of work that's paralleling what's going on and
other parts of machine learning in the fairness literature. Any recommended starting places for
folks that want to learn more or dive more deeply into this area? I know there's a book that's
being put out by Solon Barakus and more its heart on fairness. They've developed it online.
It's available. There's a number of very nice invited talks from people Kate Crawford gave
one at NURPers last year. And in terms of papers, I think, you know, there's a fair number of
papers that are looking at the definitions of fairness and interesting questions about
incompatibility of fairness. So some some work there. John Kleinberg with some colleagues
had some papers. So I would say that in these days, what's interesting is it used to be there
were one or two papers on fairness a year and now if you go to machine learning conference
like ICML or NURPers, you'll see there's five or six at least maybe eight or ten papers these
days. So, you know, I think picking up some of the recent papers in any of these areas is a good
good starting place. So that's my recommendation. Well, Rich, thanks so much for taking the time
out to chat. I really enjoyed it. Sure, then fun. Thanks.
All right, everyone, that's our show for today. For more information on Rich or any of the topics
covered in this episode, visit twomlai.com slash talk slash 2009. Thanks once again to the great
folks at Georgian Partners for their sponsorship of this series. Be sure to visit twomlai.com slash
Georgian for more information on their building conversational AI teams guidebook. As always,
thanks so much for listening and catch you next time.
