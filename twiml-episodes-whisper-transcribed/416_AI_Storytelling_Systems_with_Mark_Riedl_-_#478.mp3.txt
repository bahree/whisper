All right, everyone, I am here with Mark Redell. Mark is a professor at Georgia Institute
of Technology. Mark, welcome to the Twomla app podcast.
It's delightful to be here. Thank you.
Hey, I'm really looking forward to our chat. We've been Twitter friends, Twitter connecties
for quite a while, but it's been I think a long time in the works trying to get this conversation
going. So thanks once again, and welcome. You know, we usually start these conversations
by having you introduce yourself to our audience. Tell us a little bit about your journey
and how you came to work in AI.
Yeah, well, when I was a kid, I watched this movie called Tron, and I've been trying
to get it to the gaming grid ever since the one of the quality of Tron, but the maybe
that went back too far, right? How do you feel about the reboot?
You know, it's okay. They changed the lights, cycles can't be too happy about that, but
you know, ever since then, it's kind of been a series of happy accidents, but you know,
my voyage into artificial intelligence and machine learning came through human computer
interaction. So I get really interested in this in school, took way too many psychology
and human factors classes, and that was even before discovering this thing called artificial
intelligence. And the day I took that class, I was hooked. And then somewhere along the
way, I got invited to explore some research, got invited to look at AI for computer games,
and from that point on, like everything just kind of came together.
Nice, nice. When you started with this focus on AI and computer games, what were some
of the things that you were looking at?
Well, we were looking at, and this was, gosh, I don't know, I don't want to say how many
years ago this was, but we were looking at adaptive games. So we wanted to know whether
games could change based on what the user wanted to do or what the interests of the user
were. And this is sometimes called interactive storytelling, the idea that you don't have
to follow a linear course through a computer game with all these plot points, you could
decide to do something different. Like maybe you could even decide to be the bad guy, right?
And then the story would have to change, or twist, or add new elements into that. From
there, it became, you know, quest generation, then it became story generation. Then we kind
of eventually put the graphics aside and just said, well, you know, can we just write
a novel? Can we write a story that people would actually want to read? And I've been working
on that ever since. Nice. And so when you think about your kind of broad research, you
know, focus areas under this banner of storytelling, like how do you articulate all the various
things you're working on and how they tie together?
Well, the simplest ties that I get to work on are official intelligence and have fun
because it's stories and games. But but seriously, you know, I think broadly of my umbrella
of artificial intelligence is something called human centered artificial intelligence.
And this is turned out to be mean a lot of different things. But for me, it's about centering
the human inside the algorithm or with algorithms around the human, really instead of pushing
the human off to the side. And to for me, that means can we build AI systems that enrich
the human experience? So entertainment is one way of enriching the human experience storytelling
as another way. We can do it through design. So building explanation systems that help
people do their jobs better. But also, you know, you can kind of flip the equation around
and say, well, if you want to put humans, if you want to enrich the human condition, do
we have to build AI systems that actually understand the human condition? Because maybe
AI systems and humans are a little bit alien to each other. They think in very different
ways, the algorithms and the computer are not the same as the mental processes that
go on inside the human mind. So how do we reconcile that bridge or how do we build a bridge
between these two sorts of alien intelligences? And then how do we use it to do interesting
fun or even fundamental things? And so in the context of storytelling, where do those
two threads connect? I look at storytelling as actually this thing that brings a lot
of different AI problems together. So if you think about humans, let's start with humans,
right? Storytelling is this fundamental part of the human experience, the human life,
right? We do use it for entertainment, but we also use it for education. We use it for
training. We use it to build rapport human to human. We kind of chat around the dining
room table. What was your day like? These sorts of things. So this is this way of humans
connecting to other humans in a very rich modality of communication. But this is something
that computers don't engage in. They don't understand our stories. They don't understand
why we tell stories and they can't communicate back through stories even when that's the
most appropriate use of communication. And just to kind of like giving an example of
why this is such a, or how this is such a powerful modality, like one of my favorite
stories in the world is only six words long. It's attributed to Hemingway. It's, you
might have heard of it, it's baby shoes for sale. Never worn. Yeah. Right. Those six
words can convey, convey just a massive amount of emotion. There's a story behind it.
Why are these shoes for sale? Why were they never worn? Should I be sad? Is there something
else going on here? Right. But in just conveying that, I can convey these rich mental processes
inside the listener. And, you know, computers are not communicating like that. And the flip
side is, you know, if we said that back to the computer, they're not going to understand
all the nuance that went into those six words. Right. Right. So how do you begin to get
a computer to think about what the listener is thinking about? Yeah. Well, that is the
central challenge. Right. So because this is human, human communication, you do want
to think about what the listeners think about. You know, right now we do a lot with, um,
in artificial intelligence and machine learning with big data sets. Um, this has been a,
a godsend, right? I mean, we see amazing things. We see GPT 2 and GPT 3, these text generators
that just kind of spit out such human-like, um, language. But they're also not very good
at planning out what they're saying. They're kind of these, what are, there's sometimes
referred to as intelligent babblers or some outstanding babblers. They kind of say, here
statistically, if I have this at words, I should say this next word, then I should say
this next word and they should, I should say this next word. That's very good for creating
text that looks kind of plausible. Right. But stories are not just about babbling, right?
Stories have a goal. Stories have a point. So when I, here I am talking to you Sam and,
you know, you asked me a question, I have to figure out, you know, here's my goal. Here's
the point I want to drive to. How hard do I lay out those words and thoughts, um, to
get to my goal? So I've got to plan these things out. So the first part of it is, how do
you do planning and language? Instead of looking backwards to what I've said in the past
and whether my next word is statistically plausible, I have to be looking to the future and
saying, based on where I want to go, does this next word or next sentence, um, support
my goal? And then the next part, and that's hard enough, by the way. But the next part
is, um, you know, kind of a theory of mind, which is, um, am I choosing words or statements
or phrases that will have the cognitive effect on the hearer on the listener or on the reader
that I wanted to? Um, so going back to the baby shoes for sale example, um, you know,
if you're not a parent, you know, maybe that is not a particularly powerful story. If
you are a parent, it is powerful. So what are the, um, cognitive thoughts that are
vote by these particular words? And are they the sorts of thoughts that we want people
to have? Mm-hmm. Um, you, when you talked about the planning part of that, there is,
you know, lots of work around planning in a traditional sense and the application of
machine learning to, uh, you know, logistical planning and, and, um, you know, planning
in a shop floor for robotics and that kind of thing is your work able to draw any inspiration
or even techniques from, you know, other domains and applications of planning or is planning
in the storytelling sense, it's own unique, uh, animal that, that requires, um, ground
up thinking. Oh, no, the reason I love working on storytelling, um, again, is I get to work
on fundamental hard problems that are broader than just storytelling itself. So we, so when
I talk about planning out of story, like we, we take a lot of, we draw a lot from, um,
historical planning, literature, logistics, uh, reinforcement learning, all these sorts
of things. So what we're really trying to do is we're pulling threads together. We're
saying, all right, planning, planning plus language, how does language change the planning
process? Mm-hmm. Um, and so on and so forth, what happens when we put commonsense reasoning
inside the planning process? Maybe UPS doesn't need commonsense reasoning inside their
planning process? Maybe they do. But, you know, we're drawing another thread together and
we're saying we're going to connect a bunch of other things together that other people
have not had to put together because maybe they weren't kind of looking at this holistic
humanistic sort of problem. So when I started working on story generation, uh, we did a
lot of symbolic planning systems. Okay. And actually, we, we were able to generate some
pretty good stories because making a story is not that different from, you know, setting
up the logistics for, you know, a package or a store, right? Um, just, you know, just
we're talking about fairytale characters instead of, you know, packages and whatnot.
Um, you know, and then at some point you run into a limit and say, all right, well, you
know, we're using a lot of hard coded data, um, or models. Now we want to look at more
learned models. Can we now start to learn from reading stories as opposed to typing
in kind of logistical symbols and so on and so forth? So like anything, as we solve
a problem in, in the, in the way we think it's the right way to do it first, then we say,
are we there yet? And if we're not there yet, what do we layer on top? So we layer on top
data, then we layer on top theory of mind, then we layer on top commonsense reasoning and
so on and so forth. And we just build in that way. In the, the case of theory of mind,
but can you give us an example of how you've layered that in?
Yeah. So some of the work that we're doing now, and I should give a lot of props to one
of my former PhD students, Laura Martin, who just graduated as well as some of my other
team, um, we went in and we looked at these, um, neural language models that we see
on in the news all the time. And we said, why aren't they able to, um, why do they make
weird choices in terms of coherence? Why do they go off the rails and kind of go into
weird places? And we, we kind of looked at and say, well, because they're not really
thinking about whether what they say is building up a coherent, um, any connections to the
past. Right. So we then said, well, let's get a neural system to generate. All right.
So now we're dealing with neural networks. But then we said, well, you know what? Let's
actually go back to the past. Like in the past, we had symbols and symbols had meaning.
And we think symbols have meaning to humans. So let's build in parallel a symbolic layer
as well as a neural layer and have the two reference each other. So here's a fact.
I say something. My neural network says something. Let's convey that as a fact. Is this fact
consistent with previous facts? Does this fact build on previous facts? And so now what
we're kind of doing is we're kind of assuming if these are the sorts of facts that people
will recognize in the language itself, then maybe this set of symbolic facts is actually
a reader model. It is what we think the human is thinking about or inferring from the
language, kind of a high level semantic kind of orientation towards the language. And
if we can get that high level semantic model correct, then our stories that we generate
should come out more coherent because we can pick and choose. We can say if I say this,
this connects to my symbolic layer. If I say this other thing, it doesn't connect to
my symbolic layer. So it's probably off topic. And we were able to show that we can in fact
improve the coherence of stories. We can keep the stories on track longer. And people
report that they see more coherent as a whole.
Nice. And this is using off the shelf language models, like some of the ones you mentioned,
GPT2, GPT3, that kind of thing. Yeah, we do a lot with the, we might do some fine tuning
on them, whatever. But by and large, we're taking these very large models, much larger
than the ones that we can train or interested in training. And we're basically putting a
control layer on top. We're saying, you're not allowed to say something unless it connects
to my theory of how humans understand the language. So if you babble, and it doesn't connect
to my theory, my control theory, then I reject you and I ask you to come up with something
else. And your control plane control theory that sounds like it's not a learned layer or
mechanism, is it, or is it, or is it, you know, exclusively rules based, like how do you
create that symbolic layer? Yeah, it's a combination of multiple things. We've looked at multiple
ways of doing this. We find that there's a lot of heuristics or rules that we can apply
there, which kind of dips back into the old classical planning, sorts of literatures,
cognitive science, so on and so forth. Sometimes we also turn to learned models. You know,
we might say, well, what can we teach an AI system, a second AI system, to predict
what humans will think. And, you know, so teaching AI systems to understand common sense.
So we'll kind of, and now what we're starting to do is we're starting to experiment with
a combination of both kind of, kind of rule based cognitive models plus learned models
together, because I don't think any one system is quite the right way of doing it.
And so in describing the symbolic layer, at least the example that you gave early on
was essentially focused on maintaining consistency and coherence within the story that is a
part of making a story plausible or impactful for human, but there's also, you know, word
choice and other aspects of, you know, what the reader listener is thinking of. And it
brought to mind for me, you know, a task that's almost like, you know, invert sentiment
analysis, like sentiment conveyance or something like that. Is there an element in your work
that is focused on kind of the emotional impact of word choice by these systems?
Not at the moment, but our long term goal, yes. And so I'll kind of spill my beans a little
bit and talk about kind of where some of this work is going. What I really want to do,
what I think would be a really kind of amazing demonstration of these technologies is to
do build a system that can generate suspenseful stories. And suspense is emotion, but suspense
is a particularly interesting emotion where you see an idea. So think of James Bond, right?
So James Bond looks like he's in trouble. He's going to get captured and tortured and
killed by the bad guys, right? But then he always escapes at the end, right? So the way
to convey suspense is to see an idea in the mind of the watcher or the listener of a negative
consequence. And then at the last minute, you pull the rug out and you reveal that there's
a way out. So suspense, I think, is very powerful emotion, very hard to create accidentally.
We can do surprise. Surprise is easy to do accidentally, but suspense requires buildup.
So you have to see all these points in the story because you're planning ahead. So you
have emotion, you have planning, you have expectation. So what do humans think are going
to happen as opposed to what is actually happening? And so this, to me, this notion of suspense
brings, again, in all these threads of all these really kind of fundamental problems of
modeling, planning, so on and so forth in kind of a fun little demonstration.
Yeah, maybe think about it from, you know, there are all these story archetypes like heroes
journey and what have you that, in theory, you know, could serve as like a rule based
or some set of heuristics for a generative algorithm. Is that kind of what you had to
have in mind with this with suspense being one of those or are there other ways that
those kind of story models come into play in your work?
Yeah, so I think humans have a lot of expectations. So I talk about this idea of expectations,
right? So in some ways storytelling is self-referential. We watch a lot of movies, we build
this schema of what movies should be like. And then you play with that schema in interesting
ways. This is getting like into narrative theory here and away from the AI, right? But,
you know, but that's also important, right? So you don't want to tell stories that don't
fit people's expectations. You also have to deviate from those expectations, just modeling
what a, what is a human's expectation? Well, that's, you know, we're, we believe that
we can get that from data. So read a lot of stories, read a lot of movie scripts, you
know, those are the things you'll get to, you get a sense for what stories tend to do.
But data again is backwards looking, right? I'm not trying to emulate every story I've
seen before. I'm not trying to recreate the stories of the past. I need stories of the
future. So we start from that, we try to learn a model of expectation of what should or
should not be in stories from the data. But then the planning is the look ahead part
to say, well, what is the author's goal? You know, do they want the hero to win? Do they
want the bad guy to win? Do they want to create suspense? Do they want to create sadness?
Based on those two things coming together and the push and pull of those two things that
I think, at least that's my kind of idea of how, how stories can be told. It strikes
me that, you know, to some degree, if you figure this out, it would be very valuable to
these film studios who want to know if a story is going to resonate with an audience early.
Is that something that, you know, you're focused more on the generative side, but is that
something that is, you know, is that a solve problem? Because we can, you know, kind
of do, you know, predictive things, you know, we're think a bit further along with predictive
things with machine learning than, than generative. Or do you think they're interesting things
happening there as well? Is it even something that you're paying attention to?
Yeah, I pay attention to how AI is being used in various industries. I'm not 100% convinced
that predictive modeling in, let's say Hollywood studios to find the best, the next best,
you know, blockbuster hit is, is that big of a, is going to have that big of an impact?
I mean, I think someone will find a way of making a billion dollars off of this, regardless.
Right. But again, you know, it's backwards looking. It won't necessarily lead to better
films. Is that the core content in there? Yeah, I mean, if you're looking for things
that look like the past, then predictive models are very good. Because in some sense, all
machine learning is pattern matching. So I find patterns I've seen before. I, I activate
on that. Yeah. We're going to need something else to say, well, here's something that's
completely new. That's never been seen before, but could be a big hit. But I think that's
a big, that's a big ask for an algorithm. I mean, algorithms right now, we fully honest,
we're talking about baby shoes for sale and, and very complicated sorts of stories,
instruments, you know, we're lucky to put a couple paragraphs together, really, and have
it make sense all the way through without someone saying, wow, that was a weird direction
that this thing just went. No, that's, that's good context to, to ground on. You talked
about how story is self-referential and, you know, the role of human expectations and
it, it seems like there's a, there's a kind of a fine line there where, for the story
to be interesting, you wanted to have, you know, just enough surprise, but not so much
that it's kind of like you said off the rails or, you know, maybe avant-garde, if it was
produced by, you know, by an artist, how do you kind of control for that in the systems
you're building? I don't want to say I have all the answers yet, but, you know, what,
what we're, you know, I also, you know, while you look at storytelling, I also look a lot
at human creativity and AI creativity. And I think, you know, creativity is kind of this
weird thing. I think we all have an intuitive sense that some things are creative or not,
but I like to think about intentional creativity. So one of the big issues that machine learning
algorithms have right now, when they're used to create, whether it's stories or text
or dialogue or poems or pictures, even, is, you know, unintentional creativity versus
intentional creativity. So unintentional creativity is randomness, right? So sometimes AI systems,
they just make a statistically improbable decision because they're statistical machines.
And it ends up being a happy accident and there's something kind of beautiful and unexpected
that comes out of the computer, but it's completely non-replicable, right? You do it again
and you get something boring because a different random choice was made. So humans seem to
have this ability to say, and I think it's because we're goal driven, whereas AI systems
aren't always the machine learning systems aren't always goal driven. To say, well, you
know, I need this thing to be different than the things I've seen in the past. Now I have
to, in some sense, search through all the different ways to make things different. How
do I know which differences are good differences? Which differences are bad differences? Which
differences are just random unintentional noise that sometimes comes out good and sometimes
comes out bad? So I don't think we have a good theory of intentional creativity yet.
Specifically to AI or broadly in the sense of the human element of creativity.
Well, I think for both, I'm not a study psychology, of course, but I think definitely for AI
systems, we don't know how to build systems that can make kind of distinguish between
good and bad as well as they should. I think we're very good at building practical systems.
You know, if I have a practical objective function, I want to maximize X or I want to
minimize Y. I think we've gotten very good at that. But the problem is maximize enjoyment
like that's probably the goal for any creative system or maximize pleasure or aesthetics.
How do I define that mathematically? What is, what is, how do I define aesthetic or good
or pleasurable in art story, you know, visual arts, anything like that? I don't think you
know. And what we do is we turn to our kind of our toolbox and we say, well, what I can
do is I can at least compare it to the past. So train on a whole bunch of images. I'll
get more images like that. If I train on pretty images, I'll get pretty images. But you
know, who's creative there? The person who put together the data set, not necessarily
the algorithm. How do we get an algorithm to say, all right, you give me a whole bunch
of pretty pictures, but I think I could change this in a particular way and go off on this
weird tangent over here and get something better. That's what we don't know how to do.
And humans seem to be able to do it. And I don't know if it's just because humans have
a lived experience that is much broader and greater than any AI system or whether there's
some mechanisms in the human mind that we don't know how to model it. I think that's an
outstanding issue. Are there adjacent areas in machine learning that you're most excited
about for their, you know, potential contribution to the kind of research you're doing? Like, you
know, maybe a few years ago, you might have said, Transformers, if you foresaw all that
that was going to the brand, you know, what's the thing that's next? Is it some crazy reinforcement
learning thing or do you not know yet? Well, I'm very interested in, like, I think
story generation is going to result in new sorts of models and those sorts of algorithms.
But aside from like the big, grand AI challenges of, can we build systems that generate stories
from scratch? I'm also very interested in the applied side of storytelling. And to that
end, one of the things I've been working on is explainable AI systems. So explainable
AI systems is this idea that we have these machine learning systems. They're black boxes.
We don't understand what's going on inside of them. They make decisions, which might be
wrong or correct, but confusing. And so there's been a lot of interest in kind of quote-unquote
opening the black box. I've been thinking a lot about end users. So the people who are
going to use the Roombas of the Future, who are going to have the Roombas go off and do
really weird things. And they're going to ask, why? Why did you do that? Why did you run
over the cat or why did you wake up the baby or things like that? And the sorts of things
that we're doing now in an interpretable AI of, you know, I'm going to do data analysis
on this neural net and figure out which neurons are misfiring. That's not the sort of thing
that end users of Roombas are going to want to know. And my theory is that the explanations
that are going to be most useful to non-technical people are going to be the ones that are going
to feel like stories. So if the Roombas saying, well, I went over here because, et cetera,
et cetera. In some sense, they might be telling a little story. That story might be tapping
into a theory of mind that says, all right, well, how can I help my human user reconcile
their understanding of what the right answer action was? And my understanding of what
the right action was, I'm not sure who's right or who's wrong here. We just have a different
understanding of what was right. And can I explain through sharing experiential stories
why I did the things I did, I being the robot here. All right. And how far along are you
in that direction, that research area? We'd have some a few basic systems. Nothing
particularly complicated. Nothing, I would even say, is really telling a story. And then
at this particular point, we got into this work in end user explanation and then we realized
we actually didn't know what a good explanation was for end users. So my students and I have
been running human factor studies to say, well, what is it that the humans actually want
from an explanation? Okay. I think I understand the debuggers explanation in general are specific
to explanations of neural systems, for example, of general, right? So maybe like, whatever's
in that black box, you know, what is it? What is the explanation doing? Is it, is it helping
them understand? Is it helping them debug? Is it helping them just feel good about owning
a strange mysterious thinking device in their house? What kinds of explanations make them
trust the device and want to use it more versus less? What makes it a more pleasurable ownership
experience? Right. So it turned out there's a whole set of human, and this goes back to
my interest in human computer interaction, actually, because this whole set of questions
about what humans think about when they use complicated intelligent black box devices and
we didn't have the answers to any of these things. So we actually kind of set the algorithms
aside and say, well, before we build algorithms, we actually need to know what the algorithms
are going to target. I would imagine that there's not a single answer to that that, you know,
some people want to know a lot more about what's happening in order to help them, you know,
or an explanation to be satisfying to them, whereas others, you know, just get me to the
point. Yeah, absolutely. We're finding it's, it's vastly multi-dimensional. We just conducted
a study looking at people with computer science backgrounds versus people without computer
science backgrounds and doing explanations. And as you might predict, you know, people
with technical backgrounds have a very different orientation towards their devices, towards
their, you know, automated systems in their homes. Like to them, they want to figure out
what went wrong and how to make that never happen again. People who have, you know, aren't
engineers, you know, they had a very different orientation. They just, they just wanted to
feel that something was thinking inside, right? And just talking through the process of
what the AI system was doing was just enough. And it made them happier, right? Yeah, yeah.
And that was, you know, maybe in retrospect, obvious, but also kind of like the really
interesting result. And so, you know, granted that you kind of, you, you started with this
goal of trying to build explanations for these black box systems, you took a step back
to try to understand what a good explanation is. When are you done there? That sounds like
a huge challenge, but how far do you need to get there before you are able to return back
to this core challenge? Or are you kind of pushing on the different directions in parallel?
And how do you keep them synced up? Yeah, I think you can do some of these in parallel.
I think we know enough for now to start revisiting the design of algorithms. And so, like to
me, human centered artificial intelligence is you have to understand what, what it is
about the human experience working with an AI system before you know what to build.
So asking those sorts of questions about the human experience is the first step to any
good research, right? So I felt like it was important to step back. Now, I think we're
we know enough to say, well, we can start targeting different applications and different
use cases with algorithm again. Because now I know what I think the algorithms need to
do. And it wasn't necessarily what I thought they needed to do, you know, two years ago
when I first started thinking about this. Can you drill in on that point like what do
you think now versus what you thought then? Well, you know, we thought that explanations
were going to need to be a lot more technical than they were. And what we've really discovered
is that a lot of explanations for certain types of users and certain types of circumstances
and I don't know how, how universal this could be, but at least right now it seems that
you know, a lot of the explanation systems are going to be about confidence building.
Okay, so there's this black box. I don't know how it thinks. I don't know what is thinking.
Does it think like me? Does it think like something else? Does it even understand what
is doing or right? And so communicating in a way that sounds somewhat humanistic or human
system that says, you know, here's what I know about the situation that I'm in. Here's
what I'm seeing. Here's what I think should be done with that sort of situation. Maybe
the sort of explanations, which is very different from this is why my algorithm made choice
A versus B. Right, right. It also makes me think a little bit about the role of story
in the relationship between two things over time, like the system and the human. Maybe
the goal of the system is to gradually increase the human's confidence or understanding
or some other metric over time and is that something that we, how do we get to that?
Well, I think, you know, I mentioned one of the reasons why storytelling is as useful
from human to human is is rapport building. Before the pandemic, we used to go to conferences
and you know, there's the where are you from? How did you get here? You know, what are
you working on? You know, these these chitchat sorts of things. Often lightweight stories
who are not telling Steven Spielberg style epic thrillers, right? We're just kind of
saying here's my daily life, you know, here's who I am kind of conveyed in this interesting
kind of story like way. Can AI systems put people at ease? Make people want to use them
more? Have confidence in using them by doing this chitchat and folding in stories, but
also listening to stories. Oh, yeah, okay, you're this is the story you're telling about
you. Now, I have an understanding of how I need to respond to you and act with you.
That I think is would be kind of an amazing sort of step forward. Also one that has a little
bit of risk, right? We don't want to artificially create trust when we shouldn't artificially
create trust. So there's a little bit of a kind of a double sided kind of sword aspect
to this as well. So we want to convey confidence or and still confidence in users and convey
trust and build rapport when that's an appropriate thing to do, but we need to be very careful
that we don't step into manipulation and persuasion.
And so the what are your what are kind of the future directions in the explainability dimension
in that line of research? Yeah, well, I think there's still a lot of papers to be written
about the human centered aspect of non technical end user explanations. And then there's
I think going to be a lot of work to actually build the algorithms because the algorithms
that we might need to build might not be exactly the ones that that we know how to build
right now. Do you envision them starting from kind of existing explainability algorithms
like adding a story generation layer to lime or something like that, or are they kind
of ground up more likely? It's a little bit unclear, I guess. You know,
it's hard to predict the future, but what I do think is I think our explanation systems
are going to have to borrow a lot from the sorts of questions I've already been looking
at in the storytelling domain. So a theory of mind. So how do I answer this question
about what I just did? Well, what is it that the user actually wants to understand here?
What do they already understand about my behavior? I'm going to have the tune to that tune
to my reader, tune to my listener, tune to my audience sort of thing. Planning out the
explanation, right? So if it's going to be more than a single utterance, how do I now
I have a goal of helping you understand what I just did? Well, maybe I'm going to have
to say three or four or five different things to get you to understand that. So planning
ahead as opposed to just kind of using statistical language models is another thing that I think
is very likely to play an important role in here. So whether that whether we build from
there into something new or we take what's already out there and existing explanation
systems, we layer on top, I'm not sure. And I think it could go either way.
Nice. Yeah. One of you can maybe take us through some additional examples of papers
that you and your team have written around storytelling in general, not necessarily
the explainability, but that too. Just to give us some more concrete ideas of how you
start to kind of lay the bricks or put the pieces in place to push this broad research
agenda forward. Yeah. So I like to build big systems. So a lot of my papers are like,
here's a piece of the system, here's a piece of the system, here's a piece of the system.
So in 2017, 2018, I can't remember we had a paper at AAAI, which was on using neural
language models to build stories. So that was really kind of an early example of how to
use neural generators to build stories. But it was doing a lot of the things I just
talked about as kind of a naive way of doing it. We just statistically sample, we kind
of run the sampler forward until we get bored of the outputs. Doing that work was when
we realized we had to be much more forward looking, goal forward looking as opposed to statistical
backwards looking. So in Ijkai, I'm forgetting the dates on these, might have been 20,
1890. So about a year later, a year and a half later, we had an Ijkai paper that looked
at goal-driven storytelling. So given a goal, I want, you know, I want happily ever after
I want the hero and the hero and to dig it married. How do we force a neural network
to drive in a particular direction? And that was very successful. And then the next piece
of work after that, which is very recent, was Laura Martin's thesis, which should kind
of come out any day now, literally, was really now this neuro symbolic got to, got to talk
about her work is great. Was the, was the neuro symbolic thing to say, well, statistically
even if you drive towards a goal, you can still make these weird statistical leaps that
people just don't understand. So now we wanted to tie the neural generator to this symbolic
model of what we thought the humans would, how humans would link the different parts of
the story together to drive the story generator forward. So that was then the, the reader model
or the listener model being incorporated on top of that. And that kind of brings us
to where we are now. We've done a little bit with commonsense reasoning. We had a AAI paper
just this year about trying to incorporate commonsense. So each paper kind of had another
little dimension to the, to the story, the story of storytelling, I guess, nice, nice.
How did you approach the commonsense problem? Well, commonsense was just one of these things
that I felt was just missing from the beginning of storytelling. All right, it's really,
what is, you know, if you make commonsense mistakes, people observe them immediately. Okay,
so you make a mistake about telling you something as simple as going to a restaurant, you can
things out of order. And people like, whoa, like, what the heck is your career? Storytelling
is great like that. You can't, you can't get anything past the readers. The readers were
cold bullshit on you. Very fast, right? So we wanted work to, to be looking at how we
could use models of commonsense reasoning to, to factor into our AI systems.
And so did you use kind of existing off the shelf, quote unquote, existing research in
commonsense reasoning, or did you tailor something to the specific problem?
We did. There's this great piece of work out of University of Washington, not our team.
It's Yej Enchoy and Juan Boselet and the great crew over there. They put together some
commonsense modeling neural networks that can predict, you know, what people will think
about a sentence. So we were able to bootstrap off of that to save ourselves the trouble
of building big data sets and training off of those things. What we brought in though
was the notion of how it folds into the story planning process.
So not kind of naive filtering of things that don't pass the commonsense test, but incorporating
that signal into the planning process itself.
Yeah, we actually, we've done it both ways now. We can filter. So this sentence doesn't
pass the commonsense sniff test. So we're going to filter and we're going to regenerate
until we pass the sniff test. We can also use commonsense to actually plan forward
a little bit. We can say, well, if I see this, like, again, this is this idea of expectation
comes back again, right? If I see this, then my commonsense model says I would expect that.
So I better put that into the story. Are there kind of top line takeaways or lessons from
your work on story that you think should be more broadly applied by researchers or
practitioners in other areas of AI? Well, I think, I mean, again, I'll kind of harp
on this, right? But the idea that working on storytelling allows us to address these
fundamental problems. So, you know, I do think that some of the algorithms we're putting
together for storytelling can be pulled out of the story telling framework and be used
for, let's say, human AI or AI interaction, trying to use language to plan. So think
about getting together with your friends and trying to put the plan together for the
afternoon. Right? What are you doing? You're planning, but you're communicating back and
forth with language. So we think that this will apply to dialogue. We think it will
apply to, you know, series style personal assistance or Cortana or Alexa style personal
assistance. We haven't tested these application areas. I'll probably leave that to other
people. At the end of the day, you know, to me, this storytelling is what makes these
hard problems fun. So I kind of like to leave it in the sphere of having fun and let
other people kind of look for the applications. But you know, there's another valuable aspect
of working on storytelling is which is building big systems that are not one off papers that
say, well, I have to bring together a lot of things and build complex theories of how
all these different models fit together. You know, common sense models and planning models
they don't naturally fit together. So there's a lot to learn in terms of integrating multiple
types of AI styles problems together into one thing that and see it work and when it
works, it's great. Awesome. Awesome. Well, Mark, thanks so much for sharing a bit about
what you're doing with storytelling. Very cool stuff. It's been an absolute pleasure.
I'm finally glad to have had the chance to talk to you. Same here. Thanks, Mark. All right,
take care.
