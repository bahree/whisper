Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting
people, doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
Today we're joined by Hema Rogovon and Scott Meyer of LinkedIn.
Hema is an engineering director responsible for AI for growth and notifications and Scott
serves as a principal software engineer.
In this conversation, Hema Scott and I dig into the graph database and machine learning
systems that power LinkedIn features such as people you may know and second degree connections.
Hema shares her insight into the motivations for LinkedIn's use of graph-based models
and some of the challenges surrounding using graphical models at LinkedIn scale, while
Scott details his work on the software used by the company to support its biggest graph
databases.
Before we get going, I'd like to send a huge thanks to LinkedIn for sponsoring today's
show.
LinkedIn Engineering solves complex problems at scale to create economic opportunity for
every member of the global workforce.
AI and machine learning are integral aspects of almost every product the company builds
for its members and customers.
LinkedIn's highly structured dataset gives their data scientists and researchers the ability
to conduct applied research to improve member experiences.
To learn more about the work of LinkedIn Engineering, visit engineering.linkton.com slash blog.
And now on to the show.
Alright everyone, I am here with Hema Rogovon and Scott Meyer of LinkedIn.
Hema is an engineering director and responsible for AI for growth and notifications and Scott
is a principal software engineer.
Hema and Scott, welcome to this week in machine learning and AI.
Thank you.
Glad to be here.
I'm very glad to be here at LinkedIn and looking forward to our conversation and digging
in to some of the things you're doing with graphs for machine learning and powering
various LinkedIn features.
But before we dive into that, I'd love to introduce our audience to you.
So would you take a minute to share a little bit about your background and how you got
started working on graphs and machine learning, Hema?
Okay, so I've been in the field of machine learning for several years now.
I got my PhD back in 2006, largely focused on search and information retrieval, I've worked
on advertising, I've worked on question answering systems at IBM, IBM research in particular.
And then when I came to LinkedIn, I came in to actually work on content, on content
recommendations.
I spent a year doing that and I realized that one of the fundamental assets that LinkedIn
has is the underlying social network and that's where content propagates.
And I got really interested in the problem of building the graph in particular, thinking
about algorithms for people you may know like problems or follow recommendations and so
on. And that's what brought me from the general area of search and information retrieval
to graphs.
And it's been four years and I'm still having fun.
Fantastic.
Fantastic.
How about you Scott?
Well, let's see here.
I guess the story starts.
I did relatively well in the.com boom and actually quit for three years to go sailing.
And at the end of that, I was semi-relectorately looking for a job and I bumped into this company
called MetaWeb.
And they were like, hey, we want to build a world-rideable graph of all common knowledge.
And by the way, we're going to call this thing freebase.
And I was like, wow, I would really regret not taking that job.
So I wound up leading the storage team at MetaWeb.
MetaWeb was acquired by Google.
Freebase became Google's knowledge graph.
Spent four years at Google working on knowledge graph infrastructure.
I think the best summary of that is Google and I disagreed about graph databases.
LinkedIn found me actually through LinkedIn, which was a huge endorsement, and recruited
me to come here and lead a next-generation graph database project.
So that's what I've been doing for the last four and a half years.
So it's like 15 years in graph databases now.
Can't shake the habit.
So we're going to go deep into graphs in the way LinkedIn uses them.
We spoke a little bit about this with a previous interview with Romer, who's one of your colleagues.
Heima, but we'll dive even deeper here, but to kind of contextualize that for us, talk
a little bit about the motivation for the emphasis on graphs at LinkedIn.
Yeah.
So the social network for LinkedIn, we are an online professional social network.
The fundamental asset that a user has on LinkedIn, perhaps even comparison to our competitors
but indeed, or any of the other job sites, is the social network.
So in that context, a graph is the prime driver of all of the other parts of our business
at LinkedIn.
In particular, if you can just think about it intuitively, it makes sense that when you're
looking for a job, the most common thing for you to do is actually reach out to your
first degree connections.
If you are in a job that you like, you might want to stay informed through what your first
degree connections are talking about, and that's pretty much what your feed does.
And if you are likely missing out on a conversation, then your notifications are what are sent
to you.
And notifications from your first degree connections are more valuable than otherwise.
So at all stages of your professional network, your connections are your core asset.
And that's the core of what LinkedIn offers.
So in terms of the set of AI problems that surround graphs is first building the graph itself.
So when a member comes in, the most primitive way that a member would build their network
is Google buy memory and think about all of their colleagues that actually exist on LinkedIn
and go into a search.
Now you would not get a lot of coverage that way.
I would probably not remember colleagues from 10 years ago when I'm building my network.
So that's when a recommendation engine like people you may know comes in.
Likewise for the feed, then how do you know which of your first degree connections are
actually relevant to your current context?
So I may have colleagues from school, from three other jobs before, but maybe the conversation
that I really care about in today's context is deep learning and AI.
So which of those colleagues are contextually relevant to my feed?
So that's also a graph problem.
We can think about it as affinity modeling amongst your first degree connections.
Likewise, if you're on the jobs page, then which of your connections are relevant to you
and who might potentially give you a referral, right?
So all of these are graph problems.
They can be modeled as prediction.
They can come down to prediction problems given a certain context.
The people you may know problem is what edge should I build and that is most relevant
for the member.
And then the other problems like feed notifications or jobs say the member has a set of edges,
which of these edges are most relevant in that context.
So that's graphs everywhere at LinkedIn.
So LinkedIn is clearly already doing this.
When you think about the big challenges or opportunities in this space and among the
various features that you've talked about, people you may know in the feed, what are
those big challenges from a machine learning perspective?
Correct.
When I think about graph problems, I actually think the challenge, just compared to other
recommender systems, is often scale because in the limit, a lot of these problems are
n squared.
So if you have a graph that's 500 million members, then you're looking at n squared of
that.
So there's no way that you can talk about some of these problems without having a distributed
systems expert co-seated at the table with you, which is part of why we're at in this
conversation today together with Scott, right?
So in fact, that's the one piece we realize that even when we're training the models,
like just thinking about network propagation algorithms or about inference on these large
graphs, there is a community, a research community out there, but it's not as prolific
as perhaps, you know, other fields of AI, for example, Tex and LP have broader communities.
And it's perhaps also because the problem space is rather niche.
So those are the biggest challenges, the lack of large data sets for academia to publish
research that necessarily we can directly leverage from.
So oftentimes we are pushing the boundaries internally ourselves.
And there are a few problem spaces which sort of intersect so closely with distributed
systems and AI.
Scott, how about from your perspective, what are the big challenges in addressing this
scale?
Yeah.
Well, interestingly, I had a conversation with a recruiter when I came in and he said,
look, the magic in LinkedIn is in the second degree.
That is in, it's in connections that you could have, but probably don't.
So for example, any problem that you're experiencing, probably there's somebody in your second
degree who has solved it or who knows more about it than you.
And you may not know this person.
Conversely, with the first degree, like mostly you already know it, it's, you know, top
of mind.
From the standpoint of data, a second degree is very difficult to work with, right?
Because it's big, the way it got big is because it changes rapidly.
So if I talk about storing first degrees, this is very easy to do with a key value store.
However, if I were to try and materialize second degrees, the data footprint would be huge
and the right way it would be enormous.
And so there isn't a simple brute force solution to working with a second degree.
So one of the key things that a graph database does, probably the defining thing is joining.
And in particular, we join two first degrees to put together a second degree.
And we have to do this interactively in real time.
And talking about graph databases, where does the graph database sit in the process of
kind of serving up these graph based features?
Is it kind of at the end of the chain, you know, close to where a page is being rendered
or is it kind of early in the chain, serving up the models?
Certainly, our online, we are an online service.
So it's fundamental to LinkedIn.
If the graph is down, LinkedIn does not display web pages.
And you'll see it all over the site, for example, graph distances, how far apart you are
from someone you're looking at.
That's computed out of the graph.
So the graph itself is definitely online.
The graph, the contents of the graph, some of it is raw data that is curated directly
by users.
And some of it is the result of a relevance or AI process where we take data that may be
unreliable and figure out what's good, for example, filter out spam.
Okay.
So is it, is the graph a single monolithic system, an instance of a graph database, or
are there multiple systems that contribute to making the graph real for?
The graph is an instance of a graph database.
We actually have seven instances in four different collos to actually provide a continuous
service.
And the instance of the graph database itself is a cluster of machines, roughly 20 terabytes
of RAM per cluster.
And is this a LinkedIn kind of internally developed graph database, or is it an open-source
project?
This is a complete internal project, it's built, it's an in-memory database, it's built
on top of basically bare Linux.
Okay.
Oh wow, wow.
And so you talked about, Hema, you talked about across these different capabilities,
the common thing that you're trying to drive for, across the graph is relevant, some
aspect of relevance.
You mentioned affinity modeling as one of the types of technical approaches that you're
applying.
Can you talk a little bit more about that?
Sure.
So let's take affinity modeling for the people you may know problem, and then we'll talk
about it in the context of some other problems.
So the people you may know problem says, is A likely to know B, and, but we also want
to consider long-term value.
So if A may know B, but if they're not going to derive value from on LinkedIn in any
potential future, right?
Is it worth connecting to your neighbor down the road, like should we be showing?
It's someone you know, but using a top slot in a recommendation system for that versus
perhaps somebody who might be a good referral for a job for you down in the future.
Or someone who may share really good content, like someone that you know, but they just
write really well and you would benefit from their writing.
So you can think about predicting future edges or predicting.
So what we're doing is actually predicting future edges, right?
Is Hema likely to connect to Sam?
And so is Hema likely to send an invitation to Sam, and is Sam likely to accept that invitation?
So you have to know that, you know, Hema knows Sam, and Sam also knows Hema.
So it's a two-way problem.
But you want more than just that edge in building that edge in itself.
You want to be able to predict that once Hema and Sam connect, what's the likelihood
that they're going to interact?
An interact could be in the context of a job, interaction could be in the context of the
feed or a notification or some other context.
So affinity turns into a predictive problem.
You can generalize it to are you likely to interact in some context in the future?
And how likely is a person likely to accept an invitation from a person and derive value
from that?
So those are all the broader problems that we're trying to solve when we're building edges.
So one thing that comes to mind for me and think about that is you've got this graph
database that is presumably you're capturing the nodes which are the people in some aspect
of the relationships, the edges.
But you're also, when you're making these predictions, are you then taking the high
likelihood predictions and then putting them into the graph database?
I guess I'm trying to reconcile the graph as this kind of existent thing that's captured
concretely in a database and this predicted thing that doesn't exist.
And when does it get kind of materialized into the database?
God.
So the existing graph is the existing set of members and their connections.
And it can actually be richer than that.
You can have nodes in the economic graph of LinkedIn.
You can have nodes that are schools, companies.
You can have edges that people follow.
So that rich database in its current state.
So if you take a snapshot in time, that's what Scott Steen built serves for all of LinkedIn.
And then what we do is, given that snapshot, how do we query it to get the potential second
degree?
These are people who are not connected to each other.
And then potentially even third degrees for a low degree members because if your second
degree is too sparse, we might want to walk the graph out a little more.
So we look at the second degree, the third degree, and that itself is huge.
Even if a few of your second degree nodes have 30 odd connections each that can blow up
in computation.
So we take that large list.
And then we say all of these people are not connected to each other, which ones are
likely to connect to each other.
And that's the predictive problem.
And that's what you see on the people you may know or even follow recommendation pages
at LinkedIn.
And once people start sending invitations, so the bidirectional edge is a connection edge.
So when they send in an invitation and it gets accepted, it materializes back into the
database.
A follow recommendation will materialize right away.
And so Scott maintains the state of truth or of what connections have been accepted as
well as what follow links exist.
Okay.
I see the people you may know quite page all the time.
And you certainly wouldn't want to recompute that every time I'm going to see it.
So it needs to be cashed in some kind of way, is it cashed outside of the graph database?
That's also a graph.
Why not put the potential kind of secondary connections into the graph as well?
Like it is kind of a turtles all the way down kind of problem.
That's a very good question because the assumption around, you know, could we take a state
of the graph a time and point maybe once a week or once in every end days and then take
the second degree network and then actually build recommendations and cash it in some kind
of key value store, go as the premise with which LinkedIn built the people you may know
system for several years.
But something we discovered in more recent years was that network building is contextual.
So when people say people don't necessarily engage with people you may know application
or as a daily use case, but when they do it's long and deep sessions and say if you would
join the company and you connected with your manager, if we could in real time then explore
the second degree network from that new connection itself, right?
Then you start exploring nodes around that company, around perhaps, you know, the team.
You will start discovering some of these patterns and people go down PYM, can they go click,
click, click and then if we can actually refresh those results in each subsequent scroll based
on how we're walking that graph, we'd actually discovered that we can show significant
improvements in memory experience.
So actually walking the graph in your real time has been one of our internal discoveries.
So to speak the fact that we can actually do it and the fact that it gives huge improvements
in experience and performance has been one of the key insights in recent times.
And it strikes me that there are significant kind of systems level challenges there.
Is that in your realm or is that?
Yeah, I can speak to that.
So I think traditional graph processing, something like Pregel or GraphX, the notion is
I'm going to process the entire graph.
And if you're going to process the entire graph, the way that you serialize the graph is
really important because serial IO, you know, when you can read disc blocks in order is
much, much faster than random access.
And so a lot of graph processing focuses on clever ways to serialize the data and the
processing so that you do the most efficient processing.
The tack we've taken is what if we used random access, right?
So for random access, you've got to be in RAM.
Your basic problem is how fast can I get something from memory into the CPU?
So you're counting L3 cache misses.
Every link that you follow is an L3 cache miss, at least, that would be if you followed
it with a pointer.
In practice, we would like to follow links in constant time.
It's a little bit more expensive.
We typically average out between two and three L3 cache misses.
So by building a system which can do this, you can now use edges pretty much the way
you use pointers pretty casually.
And you can do, say, online materialization of a second degree in real time.
I'm not sure that this is a direction that I really want to go down, but you know, you're
talking about kind of optimizing around cache hits and avoiding cache misses.
You know, there's a whole emerging field of like baking machine learning into the low
level infrastructure here to make some of these predictions for you.
Is that, is that something that you're, are you thinking about it, what I do is a lot
more boring.
It's really nuts and bolts, retrieve data, figure out if it's the data that you want, you
know, pretty, pretty boring database stuff.
Okay.
Okay.
A lot of the challenges for building machine learning systems at scale have to do with kind
of the data pipeline and getting that in place before you even get to modeling.
Does having access to this graph database alleviate all of that for you?
Are you still wrangling with data pipeline types of issues?
So data pipeline issues exist definitely for modeling because oftentimes models are still
built offline prototyping is done offline.
I mean, a graph database comes in when you're actually serving.
So one of the pieces around making data pipelining easier has been what LinkedIn calls productive
ML or ProML and we have actually facilitated tooling to become much more easier for data
scientists to actually build prototypes.
Now that said, we have also started thinking about how do we make computing graph features
because if you have a data scientist who wants to try a slightly different variant of
a second degree algorithm because we talked about a very simple version which is get
or get me all the second degree notes.
And if you want to think about it as a probabilistic random walk and you want to weigh the edges
in terms of by a model, right, like how close people are based on some of these affinity
models that we talked about.
How would it be easy for a data scientist to not have to worry about scale?
So can we just provide an interface, a simple, maybe Jupyter notebook like interface
to actually query these kind of databases?
So we've actually, we've, we're working on these and that makes the life for our data
scientists easier.
So and that definitely takes away the data pipeline issues or the scale issues.
And when you get to modeling, can you talk a little bit about the modeling, the types
of models that folks are building to solve these types of problems are there.
You know, if you kind of go to models that you apply broadly or do you customize the
models very deeply for the specific types of problems you're attacking here?
Yeah.
So at the very basic, oftentimes for any kind of affinity modeling, we would start with
the simplest class of models, decision trees, logistic regression and so on and so forth.
That said, we do find benefits in deep learning like models.
So there are some, there are, there's published literature.
In particular, there is a graph sage algorithm that came out of work from graph sage.
Graph sage?
Yes.
And that has come out of Stanford and Pinterest.
So we've done variants of those, we see certain advantages to algorithms of that nature.
So deep learning definitely is one direction we're pursuing.
Another direction is just personalization.
So how do we think about different users who are in different stages of career building?
You may be a new user and you may be building your graph and what LinkedIn calls a novice
member.
So you're more tentative to send out invitations, you're showing you somebody that you're likely
to connect in that context makes a lot more sense.
So knowing the stage of the user, the degree of the of connectivity of the user and the
lifecycle of the users, particularly important and likewise for someone who's very well connected
already, the additional value of a connection may be much lower.
So how do we find that one connection that that person's missing?
So personalization is something that the simple models like logistic regression would
not achieve.
So we get to more advanced models like mixed effect models.
Okay.
I'd like to explore some of these.
So graph sage, what's that trying to do?
So graph sage looks at, so most deep learning models have, if you think about the body
of literature in text processing or NLP, typically looks at sequences of words.
There are a few models that actually look at general graphs for deep learning.
But essentially what graph sage is trying to do is similar to what to work or might be
trying to achieve, which is trying to learn a representation or an embedding, but it
takes into account properties of the node and it takes into account the graph structure.
So for example, you may be a sparse, leconnected node, but connected to one dense, leconnected
node with very rich profile features.
And I actually, in for some properties about the sparse, leconnected node, which with
an incomplete profile from the densely connected node.
So actually looking at network propagation, looking at the structure of the underlying graph
is the class of algorithms that we're going after.
Okay.
So thinking about it as a graph embedding kind of approach your, your building a model
that is finding relationships, ultimately between the nodes, that such that in this embedding
space, nodes that are close together have similar properties.
Yes, exactly.
And then you also mentioned mixed effect models, what are those?
Yeah.
So the way I want to sort of generalize that class of models is how do I learn models
that are specific to certain groups and groups that are learned automatically, right?
So we can think about it as a logistic regression is a simple, like simple formula, which does
not necessarily consider effects of the viewer.
So the member that's being, for whom the recommendations are being rendered, the, or the
in the, or doesn't take into account personalized features of the items being recommended, right?
Or you would actually have to hand code these deeply into a logistic regression model.
But classes of models that can actually learn subgroupings of people and learn those interactions
between the viewer, that is, for whom it is being recommended and the items that are being
recommended and learn interactions between them.
So is it fair to think of it as almost conditional clustering, like your clustering condition
on to a specific node or something?
The other way people can think about it, these are also hierarchical models, like in more
classical statistical statistics literature, you can think of taking all of your feature
data and actually building clusters.
So you know, you think about, you're called it conditional clustering, but just building
these groups and then almost learning a separate model for each group, but not having to
hand code these groups by themselves, letting the model info what these groups may be.
And then we jumped into some of the more advanced models that I wasn't familiar with, but in
thinking about the application of kind of your linear regression and other basic models
to graph types of data, is that are there unique kind of challenges or approaches to doing
that or is it kind of pretty standard to the way you do a linear regression on more simple
types of data?
Yes, so the challenges for a simpler model would often be just the volume of data we have
for training.
So just processing all of that data can often be a challenge.
The other part is feature computation, so if you actually work computing something like
just a number of common connections between two nodes, that can be a pretty heavy computation
and you have in an offline system like Spark or MapReduce.
And the other piece is the online serving, so actually inference is can be a challenge.
So if you're actually thinking about how the PYMK is rendered, we talked about neural
real time candidate generation.
So what happens is if Sam comes to his PYMK page, we have, we need to look at features
for Sam, but then we may have a few thousand candidates to rank for Sam.
So then we have to look up the, all of the features for those few thousand candidates.
So then we're pounding a key value store at our PQPS for that many candidates.
Then you start hitting against systems challenges to actually retrieve all of those features
at that rate.
So both at the data munging side for just offline analysis or feature engineering, some of
these graph features can be computationally complex and we, you're doing a lot of large
joints.
So we actually become smarter at the way we do joints, just simple, sequil it like pick
joints don't necessarily work that well for us at the scale that we're at.
And then again, online inference is the other piece in terms of those joints got what
does work.
Oh, what does work?
I think the two problems with, with joins are, are query planning.
So typically if you talk about a graph, it's represented as a table of edges.
This is a very, very easy to do in any SQL database.
However, when you start to query it, you're doing self joints.
And the problem with self joints is that the optimizer is based on statistics.
And after your first round of projections, you no longer have any statistics.
This gets exacerbated by the fact that graphs tend to have a lot of skew in them.
That is some nodes are very, very dense and others are very sparse.
And so basic techniques like, oh, I'll just take an average, don't work very well.
You can be looking at things, oh, this guy has 200, 150, 113, 25 million.
So if you have to look at 25 million things, you have to pay that price.
But if you're in a complex query, maybe you don't have to.
Maybe that highly skewed node will be knocked out by some other condition.
So you need an optimization framework that can respond to that problem of skew.
And then, so this typically shows up in offline jobs where you have a hot, you have a hot
shard.
All the other shards are done and one shard is just spinning at 100% CPU or it runs out
of RAM.
That's a very, very typical problem.
And the query planning we solve with a dynamic planner rather than a static planner.
So typical SQL database, you give it the query.
It looks at the statistics of the tables that are referred to.
It comes up with a plan and then it just executes the plan without change.
What we do is we look at the query, look at the sizes of sets, which we can determine
in constant time and we do the cheapest thing.
And then we see what that's done to the sizes of sets and we do the next cheapest thing.
So if we're pursuing a key brand and it turns out to be expensive, we'll go look at other
keeper things.
So we'll change the query plan in midstream as it were.
The hot shard problem that you were mentioning is that something that you deal with kind
of operationally or is that something that does this kind of dynamic element of the underlying
platform know how to deal with that?
It's something that would, I mean, so in liquid the graph database, it's something that
our planner handles.
So it's a problem that we don't have in the online world.
In the offline world, it's just a thing that can happen, right?
So very often people are grumbling about their offline job not finishing and oh, well,
we need to give everybody more memory or we need to fiddle with the query plan for this
particular thing that we're computing.
It's just a bad thing that can happen.
And when you say fiddle with the query plan, how much of that query planning is kind of
hand-tuned like that versus the system is creating the query plan?
It varies a lot.
The tendency is for things to be more SQL-like, where somebody else figures out the plan.
But a lot of people are still writing handwritten map-produced jobs, where you are actually,
the human programmer is figuring out a plan that the computer program will exit.
And the programmer has to realize, hey, if I want to talk about people who are following
Richard Branson, I probably don't ask about the shard that has Richard Branson.
I probably ask all the other shards like, hey, if you're connected, if anybody that you
have is connected to Richard Branson, I want to do some stuff with that, right?
So that's a typical way to work around that problem.
Is that type of thinking natural for the kinds of engineers that are working on these
problems?
Or is there a lot of education?
Yeah, I don't think it's natural for anyone.
Graph databases, it takes a while to, there's no top or bottom.
So I've always seen, it takes at least six months for people to come up to speed on the
domain and really start being productive.
That's like the new concurrency, right?
It's like something you just have to wrap your head around.
There's this fog and eventually you come out on the other side of the fog and it kind
of makes sense.
Yeah, yeah.
I think so.
I'm curious about if you're seeing anything happening on the hardware side of supporting
graph databases.
It strikes me that there's maybe an analogy in, there are a number of companies going
at doing machine learning in Silicon and one of the big innovations there is the way
where we do a lot of machine learning via TensorFlow and PyTorch and the like is
by constructing computational graphs, so let's build Silicon that is better suited towards
computational graphs as opposed to linear instruction sets.
Now we're talking about graph databases.
I don't think I've heard of like a graph server or graph hardware that's kind of tuned
for supporting graph databases.
Do you, have you heard of that or, and do you think that maybe I think at some point?
I mean, honestly, for my standpoint, hardware has steadily gotten worse, instruction pipelines
are longer.
People tend to focus on, well, if you can do serial IO, then it's much, much faster.
They want things to fit in the L1 cache.
So the sweet spot for modern hardware is I have figured out how to chop things up into
small enough pieces that one piece fits in the L1 cache and then I can use vectorized
operations really fast.
That's absolutely wonderful.
Unfortunately, that's not what a graph is.
A graph is really random access, right?
There's no easy way to sort a graph.
So for example, if I'm storing edges, well, the edge has, say, three fields, a subject
predicate an object, I have to sort in one way.
So if I sort it by subject, I can find all of your edges very easily.
But when I start looking at objects at the other end, it's unsorted, right?
So you really cannot escape the need for random access and unfortunately modern processors
are very heavily pipelined and don't handle random access very well.
So do you envision a hardware that's better, have you come across that?
I think people are doing the best they can.
It's very hard and expensive.
So we have L1 memory.
Why don't we just make L1 be like all of the RAM?
It's very expensive to do that.
Hey, man, when you think about the, you know, a lot of the challenges that we've been
talking about are challenges that arise from kind of the scale at which LinkedIn is manipulating
graphs and applying machine learning to graphs.
Not a lot of people are operating at that scale.
Do you think about kind of scaling down this kind of problem and like, you know, getting
started thinking about applying machine learning to graph types of problems, what are some
of the things that people can do to explore this area?
There are definitely lots of graph datasets out in the public domain.
The clue web corpus is a web graph in itself.
Clue web.
Okay.
There are academic publications that reference protein interaction networks.
You can think of those as graphs as well.
So there's lots of embodiments of graphs in different subdomains.
The set of problems that come to mind often is, it often boils down to link prediction,
like, does a link between A and B exist.
Another problem that actually many companies in particular financial institutions are generally
interested in is interaction graphs, but thinking in terms of the problem of abuse.
So the general problem of link prediction has many applications both in, you know, the
public domain as well as, you know, which the niche, you know, corporate domains.
So I think thinking about a newer classes of link prediction problems that go beyond
random walks, bringing embedding like algorithms, basically deep learning algorithms, but that
can operate at scale.
So basically, how can we distribute the processing?
Thinking about distributed graphs is itself a hard problem.
The other piece that we didn't cover too much about, but I think for folks who are academically
oriented, I would actually think about how do you do a B testing on a graph?
Because in some sense, nodes are not IID when you expose a node to a particular treatment,
its neighborhood is impacted too.
So a very nice theoretical problem may be just, you know, taking a look at how we might
actually do experiments on these graph structures.
So IID independent, identically distributed.
So where's the, what's the missing part of the academic literature in this area?
So think about this today when most companies do a B testing or they definitely, they partition
their user base into random buckets, right?
But when your users are connected by links, right?
So for example, let's take the example that you and I are connected and you share an
article and I view the article.
If we're in two different buckets for an AB test, I'm not really independent of you.
So that assumption completely breaks apart.
So how do we actually tackle this problem?
I think that's something you can study even in a smaller graph in a simulation like framework.
So I would, you know, sort of love to toss that problem out at the academics.
It's got any requests from your part for the academic community.
I think, I don't really have requests.
You were talking about, you know, how do you process this at small scale?
Yeah.
I think one of the, one of the insights here is it's actually pretty economic to have a
lot of RAM, right?
And so if I was, if I was wanting to do a graph work at a smaller scale, where a small
probably means, you know, up to three terabytes, I would just focus on a single machine and
use, use RAM.
It's, I think you can buy a one and a half terabyte machine from Dell at the rack price for
20,000 bucks, 25,000 bucks.
It's really quite economical.
And if you didn't have access to liquid, what graph database would you be exploring?
That's a harder one.
I know the guys at Franz, who, who build a leg rat graph.
And I think that's, to me, probably the most interesting of the extent graph databases.
Well, Hema Scott, thank you so much for chatting with me.
Thank you so much for having us here.
Yeah, thank you very much.
All right, everyone, that's our show for today.
For more information on Hema, Scott, or any of the topics covered in today's show, visit
twimmalei.com slash talk slash 236.
Thanks once again to LinkedIn for their support.
Be sure to check out what their engineering team is up to at engineering.linkedin.com slash
blog.
As always, thanks so much for listening and catch you next time.
