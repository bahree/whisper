WEBVTT

00:00.000 --> 00:15.920
Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting

00:15.920 --> 00:20.880
people doing interesting things in machine learning and artificial intelligence.

00:20.880 --> 00:23.520
I'm your host Sam Charrington.

00:23.520 --> 00:28.320
A big thanks to everyone who participated in last week's Twimble Online Meetup, and

00:28.320 --> 00:31.040
to Kevin T from SIGUP for presenting.

00:31.040 --> 00:35.360
You can find the slides for his presentation in the Meetup Slack channel, as well as in

00:35.360 --> 00:37.240
this week's show notes.

00:37.240 --> 00:41.640
Our final Meetup of the Year will be held on Wednesday, December 13th.

00:41.640 --> 00:46.840
Make sure to bring your thoughts on the top machine learning and AI stories for 2017

00:46.840 --> 00:49.000
for our discussion segment.

00:49.000 --> 00:54.640
For the main presentation, prior Twimble Talk guest Bruno Gonzalez will be discussing

00:54.640 --> 01:01.120
the paper understanding deep learning requires rethinking generalization, by Shi Huang Zhang

01:01.120 --> 01:04.480
from MIT and Google Brain and others.

01:04.480 --> 01:09.600
You can find more details and register at twimbleia.com slash Meetup.

01:09.600 --> 01:13.920
If you receive my newsletter, you already know this, but Twimble is growing and we're

01:13.920 --> 01:19.640
looking for an energetic and passionate community manager to help expand our programs.

01:19.640 --> 01:24.040
This position can be remote, but if you happen to be in St. Louis, all the better.

01:24.040 --> 01:27.760
If you're interested, please reach out to me for additional details.

01:27.760 --> 01:31.720
I should mention that if you don't already get my newsletter, you are really missing

01:31.720 --> 01:36.760
out and should visit twimbleia.com slash newsletter to sign up.

01:36.760 --> 01:42.680
Now, the show you're about to hear is part of our Strange Loop 2017 series, brought to

01:42.680 --> 01:45.520
you by our friends at Nexusos.

01:45.520 --> 01:50.440
Nexusos is a company focused on making machine learning more easily accessible to enterprise

01:50.440 --> 01:51.440
developers.

01:51.440 --> 01:56.000
Their machine learning API meets developers where they're at, regardless of their mastery

01:56.000 --> 02:01.200
of data science, so they can start cutting up predictive applications immediately and

02:01.200 --> 02:04.080
in their preferred programming language.

02:04.080 --> 02:08.440
It's as simple as loading your data and selecting the type of problem you want to solve.

02:08.440 --> 02:13.320
Their automated platform trains and selects the best model fit for your data and then outputs

02:13.320 --> 02:14.880
predictions.

02:14.880 --> 02:19.200
To learn more about Nexusos, be sure to check out the first episode in this series at

02:19.200 --> 02:27.480
twimbleia.com slash talk slash 69 where I speak with co-founders Ryan Sevy and Jason Montgomery.

02:27.480 --> 02:32.920
Be sure to also get your free Nexusos API key and discover how to start leveraging machine

02:32.920 --> 02:38.400
learning in your next project at nexosos.com slash twimble.

02:38.400 --> 02:43.400
In this episode, I speak with Sam Richie, a software engineer at Stripe.

02:43.400 --> 02:47.720
I caught up with Sam right after his talk at the conference where he covered his team's

02:47.720 --> 02:51.120
work on explaining black box predictions.

02:51.120 --> 02:56.440
In our conversation, we discuss how Stripe uses these predictions for fraud detection and

02:56.440 --> 02:59.400
he gives us a few use case examples.

02:59.400 --> 03:03.960
We then discuss Stripe's approach for explaining those predictions and briefly mention Carlos

03:03.960 --> 03:18.480
Guestren's work on the line paper, which he and I discuss in twimble talk number 7.

03:18.480 --> 03:23.360
Well hey everyone, I am here at the Strange Loop Conference in St. Louis and I've got the

03:23.360 --> 03:28.360
pleasure of being seated across from Sam Richie, who is a software engineer at Stripe.

03:28.360 --> 03:36.040
And Sam was in to talk about just so stories for AI, explaining black box predictions.

03:36.040 --> 03:40.520
And in fact, he just jogged over here from delivering his talk and it's like five blocks away

03:40.520 --> 03:42.280
at hustle.

03:42.280 --> 03:43.280
So welcome to the show, Sam.

03:43.280 --> 03:44.280
Yeah, thank you.

03:44.280 --> 03:45.280
Thanks for having me.

03:45.280 --> 03:48.080
Why don't we start by having you tell us a little bit about your background and how

03:48.080 --> 03:51.280
you got started working in AI and machine learning?

03:51.280 --> 03:52.280
Sure.

03:52.280 --> 03:54.680
Yeah, you want the just so story of exactly.

03:54.680 --> 03:59.400
So this is fairly new, I guess less year and a half I've been at Stripe, as I said, working

03:59.400 --> 04:02.080
on machine learning infrastructure.

04:02.080 --> 04:03.440
How did I get here?

04:03.440 --> 04:08.240
We were talking before this started about, you know, my kind of initial coding background

04:08.240 --> 04:12.880
is coming up in the functional programming world years ago, I guess the first thing that

04:12.880 --> 04:18.440
led to this was this large scale like deforestation monitoring system I worked on.

04:18.440 --> 04:22.600
This got me into like Hadoop and closure and this is sort of it was a machine learning

04:22.600 --> 04:26.160
application, I had no idea what I was doing when I was on this, right?

04:26.160 --> 04:27.560
And where were you working on that?

04:27.560 --> 04:32.800
So this was just a freelance thing with a guy I used to race like flat water kayaks

04:32.800 --> 04:33.800
with.

04:33.800 --> 04:36.200
And he moved off to the data science world.

04:36.200 --> 04:40.760
I was making apps like in my apartment in New York, I had a year in New York city.

04:40.760 --> 04:46.760
And yeah, just wanted to, well, the real, okay, so the reason I, it's like hard to put

04:46.760 --> 04:48.880
a narrative together in my life, man.

04:48.880 --> 04:53.160
The deforestation monitoring system was just this collaboration with a buddy who was doing

04:53.160 --> 04:54.760
work for the World Bank though, right?

04:54.760 --> 04:55.760
Okay.

04:55.760 --> 04:59.720
So the idea was to build like a model that could, on the basis of these studies that happen

04:59.720 --> 05:05.120
every like five years or so, build a logistic regression that could predict for certain pixels

05:05.120 --> 05:06.120
in the tropics.

05:06.120 --> 05:09.640
The idea was like any spot you looked at, the goal was to get out some prediction of the

05:09.640 --> 05:13.520
chance that that piece of forest would be deforested in the next month, right?

05:13.520 --> 05:14.520
Okay.

05:14.520 --> 05:15.520
And you're starting with satellite imagery?

05:15.520 --> 05:16.520
Exactly.

05:16.520 --> 05:17.520
Exactly.

05:17.520 --> 05:18.520
Okay.

05:18.520 --> 05:19.880
We came out every two weeks.

05:19.880 --> 05:23.320
The training data for this, again, I didn't know what I was doing at the time.

05:23.320 --> 05:27.880
The training data for this was a study that this guy Matt Hansen had done, which looked

05:27.880 --> 05:32.240
at, modus data from the year 2000, same data again in the year 2005.

05:32.240 --> 05:33.240
Okay.

05:33.240 --> 05:36.640
And then he just went through a manually like selected which pixels they thought were deforested

05:36.640 --> 05:37.640
in which work.

05:37.640 --> 05:41.960
So this is useful in the back, like a lot of policy, and there's a lot of work on, you

05:41.960 --> 05:46.880
know, reforestation and things like this, a lot of money that went out from this study,

05:46.880 --> 05:49.120
but the plan was to do it every five years.

05:49.120 --> 05:53.040
And so a lot of money went into Indonesia, for example, to like work on deforestation

05:53.040 --> 05:54.040
there.

05:54.040 --> 05:58.040
The next time he did the study or when our model came online, it was clear that like Indonesia

05:58.040 --> 05:59.040
was actually looking pretty good.

05:59.040 --> 06:00.880
It was not talking to charts anymore.

06:00.880 --> 06:01.880
Okay.

06:01.880 --> 06:03.320
And like me and Mar was now the issue.

06:03.320 --> 06:04.320
Okay.

06:04.320 --> 06:06.800
So the goal was to get new data every two weeks.

06:06.800 --> 06:11.360
So what we did is this is where the, you know, big data, functional programming stuff came

06:11.360 --> 06:12.360
in.

06:12.360 --> 06:13.360
Right.

06:13.360 --> 06:17.840
And then from satellite data, like time series of what was happening during that five

06:17.840 --> 06:19.240
year period, right?

06:19.240 --> 06:20.240
Okay.

06:20.240 --> 06:21.440
So use that as the training data.

06:21.440 --> 06:25.880
The prediction variable was what this guy Matt Hansen had said deforestation was.

06:25.880 --> 06:26.880
Right.

06:26.880 --> 06:31.280
And then we just marched the time series forward and predict, does this now look like deforestation?

06:31.280 --> 06:32.440
Does this now?

06:32.440 --> 06:36.400
You end up with a map that you can scroll through that just shows the trends of deforestation

06:36.400 --> 06:37.400
moving through the world.

06:37.400 --> 06:38.400
Oh wow.

06:38.400 --> 06:39.400
It's really amazing.

06:39.400 --> 06:40.400
Interesting.

06:40.400 --> 06:43.760
I don't know what's happened with that, but I think it's now the World Resources Institute

06:43.760 --> 06:47.000
was the organization that was sponsoring us.

06:47.000 --> 06:51.800
I think this is out as I can open data set now, but that's kind of my first taste of.

06:51.800 --> 06:55.840
And now I came at machine learning from the statistics side a little bit.

06:55.840 --> 06:58.320
And it turns out that these are like the same field.

06:58.320 --> 06:59.920
They just have different words for things.

06:59.920 --> 07:00.920
Right.

07:00.920 --> 07:01.920
So you move from one of the other.

07:01.920 --> 07:04.040
You don't really know if you can bring anything over.

07:04.040 --> 07:05.960
But yeah, that was the first thing.

07:05.960 --> 07:11.080
That took me, you know, I took all this like wonderful, non-profit, amazing work I was

07:11.080 --> 07:12.080
doing.

07:12.080 --> 07:16.640
And I let Twitter like recruit me to go do the same stuff on ads.

07:16.640 --> 07:19.120
The best minds of our world, that people put ads, right?

07:19.120 --> 07:20.120
That's it, man.

07:20.120 --> 07:21.120
Yeah.

07:21.120 --> 07:22.120
So it sold the soul for a little bit.

07:22.120 --> 07:27.240
But, you know, had a few years there building open source tech to do, again, like effectively

07:27.240 --> 07:31.440
the same sort of Hadoop based stuff that we were working on before.

07:31.440 --> 07:35.440
We ended up open sourcing a lot of that work as this library called Summingbird, which

07:35.440 --> 07:40.320
is, again, as we were saying, it's like all monads all the time.

07:40.320 --> 07:44.760
It's a library that lets you write these like big streaming data computations and then

07:44.760 --> 07:49.920
run them on Hadoop or on like a real-time streaming system or on both.

07:49.920 --> 07:54.520
And basically, like, separates what you want to compute from where you want to compute

07:54.520 --> 07:55.520
it.

07:55.520 --> 08:00.000
And it's only relevant really because the final piece of this puzzle is what drew me

08:00.000 --> 08:02.520
to stripe is that I was interested in machine learning.

08:02.520 --> 08:07.440
I was doing a lot of work on my own and studying and just trying to get up to speed and like,

08:07.440 --> 08:12.040
stripe had pulled this library in and was using it for their feature generation pipeline

08:12.040 --> 08:13.600
for a lot of their models internally.

08:13.600 --> 08:14.600
Okay.

08:14.600 --> 08:18.720
So it was like, I'm not the most qualified, like, I don't have this amazing like data science

08:18.720 --> 08:19.720
background.

08:19.720 --> 08:22.080
I did have a hook in the infrastructure side.

08:22.080 --> 08:27.080
And so a lot of the work I do now at this intersection between how do you make features?

08:27.080 --> 08:29.160
How do you run the stuff at production scale?

08:29.160 --> 08:30.480
How do you ship models?

08:30.480 --> 08:33.680
The intersection between that and like, what models are even worshipping?

08:33.680 --> 08:34.680
What should you care about?

08:34.680 --> 08:36.360
What should you put in the product?

08:36.360 --> 08:40.160
Super interesting for me and it's been a really fun, like, year and a half or so.

08:40.160 --> 08:42.680
Leading now to some work on this stuff we were talking about today.

08:42.680 --> 08:43.680
Okay.

08:43.680 --> 08:44.680
Awesome.

08:44.680 --> 08:45.680
Awesome.

08:45.680 --> 08:47.920
So why don't you tell us what you were talking about today?

08:47.920 --> 08:48.920
Yeah.

08:48.920 --> 08:54.520
So the talk today was on, in general, it was on this idea of how to explain the predictions

08:54.520 --> 08:55.920
that black box models make.

08:55.920 --> 08:56.920
Right.

08:56.920 --> 09:01.560
This is like a term that's tossed around like a black box model is a model, like, a neural

09:01.560 --> 09:05.120
network is a black box model, a random force is a black box model, right?

09:05.120 --> 09:09.720
Like, it's just kind of this term meant to express how, like, powerful and complicated

09:09.720 --> 09:11.440
these things are internally.

09:11.440 --> 09:14.160
The internal structure is very rich and varied.

09:14.160 --> 09:19.920
You know, a black box model is typically seen as difficult to understand or hard to explain.

09:19.920 --> 09:22.160
That's like, it's hard to know what that really means, right?

09:22.160 --> 09:25.280
Like, what does it mean to explain a black box model?

09:25.280 --> 09:27.320
So that was the general theme.

09:27.320 --> 09:31.360
The specific things I started with were the work we do at Stripe on fraud detection.

09:31.360 --> 09:35.120
You know, we use models that are as accurate as possible to try to catch fraud for merchants

09:35.120 --> 09:36.440
who sign up for Stripe.

09:36.440 --> 09:42.040
At the same time, this black box property of, we're just going to block charges and, you

09:42.040 --> 09:46.440
know, you as a merchant, like, you might know more about your business.

09:46.440 --> 09:49.480
We might block a charge that you think is a legit customer.

09:49.480 --> 09:53.480
And if we're just sort of telling you like, look, the world is a better place if you accept

09:53.480 --> 09:54.480
our decisions.

09:54.480 --> 09:56.360
It's not totally appropriate.

09:56.360 --> 10:01.480
And people don't trust us with this and often what they've done is not used our product

10:01.480 --> 10:05.440
and gone to the system of very manual rules.

10:05.440 --> 10:08.440
You know, you go in, you say what you think fraud looks like.

10:08.440 --> 10:12.440
You're probably like cutting a lot more tissue out than you should.

10:12.440 --> 10:15.560
But you know, you feel good about it because you made the decision.

10:15.560 --> 10:19.800
So there's a tension here between a black box model making decisions for you that you

10:19.800 --> 10:21.480
don't understand.

10:21.480 --> 10:26.440
And something is not as effective, just strictly, like, a merchant does not know as much about

10:26.440 --> 10:32.160
fraud as some like a big company that is dealing with tens of thousands of merchants can.

10:32.160 --> 10:37.480
So the talk was about some tech we developed at Stripe to give people alongside our internal

10:37.480 --> 10:40.840
sort of suggestion or decision about what we're going to block, an explanation of why we

10:40.840 --> 10:41.840
did that.

10:41.840 --> 10:42.840
Okay.

10:42.840 --> 10:43.840
So this is a subtle problem.

10:43.840 --> 10:46.680
It was about the solution we came up with for that.

10:46.680 --> 10:51.960
And then about why that solution is kind of not great and why it's not a real explanation,

10:51.960 --> 10:52.960
is it not?

10:52.960 --> 10:56.920
And then we talked about a bunch of other techniques that the goal was to build up in

10:56.920 --> 11:01.960
the mind of, I mean, I guess we'll talk about it here, like, there's a bunch of techniques

11:01.960 --> 11:02.960
to do this.

11:02.960 --> 11:07.040
And it turns out that black box models often are like the most explainable models.

11:07.040 --> 11:11.440
They have such a rich structure that you can ask so many questions that really tease

11:11.440 --> 11:15.880
apart subtleties of an individual example in a way that you absolutely cannot with something

11:15.880 --> 11:19.640
like a logistic regression or a very simple model that you can just simulate in your

11:19.640 --> 11:20.640
mind.

11:20.640 --> 11:21.640
As a human.

11:21.640 --> 11:22.640
That's counterintuitive.

11:22.640 --> 11:23.640
It is.

11:23.640 --> 11:27.440
Then a black box model, you'd consider black box models be the most explainable model.

11:27.440 --> 11:28.600
Yeah.

11:28.600 --> 11:33.600
And I mean, I do it by just taking the definition and twisting it a little, but I would argue

11:33.600 --> 11:36.800
that it's, it's fuzzy what people mean when they talk about this.

11:36.800 --> 11:37.800
Right.

11:37.800 --> 11:41.840
There's a number of ways to frame this problem of is a model explainable or not, right?

11:41.840 --> 11:47.680
And so when you, how are you defining it leading up to the conclusion you've drawn?

11:47.680 --> 11:51.680
So I think the way in which people think a black box model is not understandable, you

11:51.680 --> 11:58.000
know, is it's the same way that like, you could say, if you asked me why I decided to

11:58.000 --> 11:59.560
go or get straight for something else.

11:59.560 --> 12:03.720
And I just like printed out the contents of my brain and showed you the state of every

12:03.720 --> 12:05.120
neuron and every connection.

12:05.120 --> 12:06.880
This is not understandable, right?

12:06.880 --> 12:08.560
Like, it's the truth.

12:08.560 --> 12:14.040
This is why I did it, like the physical state of my mind just, I couldn't do otherwise if

12:14.040 --> 12:16.680
you buy a sort of free will argument.

12:16.680 --> 12:20.360
But that's not what people mean really when they talk about human explanations, right?

12:20.360 --> 12:24.120
Like, when you talk about a human explanation, you want to know, okay, well, give me some

12:24.120 --> 12:29.280
narrative, give me some plausible reason why in this case, like not the entire, your brain

12:29.280 --> 12:31.800
has information from everything you've ever done.

12:31.800 --> 12:37.880
Give me for this example, you know, maybe, maybe one way to explain the decision is what

12:37.880 --> 12:40.640
would have had to change to make you change your mind, right?

12:40.640 --> 12:42.000
So that's a form of explanation.

12:42.000 --> 12:47.440
That's a kind of question that you can sort of ask of a logistic regression.

12:47.440 --> 12:52.640
You can ask of simple models when you get to things like image processing, where the

12:52.640 --> 12:57.120
features, the inputs to the model are tens of thousands of different pixels, each of which

12:57.120 --> 12:59.880
has individual weights in say a logistic regression.

12:59.880 --> 13:04.640
Like, that ability to look at a feature and see how much it affected the output, that

13:04.640 --> 13:08.040
kind of stops being helpful, right?

13:08.040 --> 13:12.600
Whereas with a neural network or something like this, you can, for an individual example,

13:12.600 --> 13:15.720
you can start one technique I talked about in the talk, it's called line.

13:15.720 --> 13:16.920
And the idea here is that-

13:16.920 --> 13:18.600
That's Carlos Gastron's work.

13:18.600 --> 13:20.600
Yeah, yeah, it's so good.

13:20.600 --> 13:26.160
So the idea here is that for an individual example, you can probe the model and see what

13:26.160 --> 13:29.240
would have happened had any individual thing changed, right?

13:29.240 --> 13:34.920
So you can build effectively like a little local linear model inside of this wild space that

13:34.920 --> 13:37.400
the neural network is trained, right?

13:37.400 --> 13:42.760
So because of this rich internal representation, overall, totally with you, like, it's hard

13:42.760 --> 13:46.520
to explain why the model is doing what is doing in aggregate, just like it's hard to say

13:46.520 --> 13:51.360
why like, you know, the entire population of a country is doing something more.

13:51.360 --> 13:56.720
But for any individual case, you can, in fact, build a story or build more technically

13:56.720 --> 14:00.560
like a local linear model, which will tell you what the most important aspects of that

14:00.560 --> 14:01.560
particular example were.

14:01.560 --> 14:05.640
And it's been a while since I've talked to Carlos about this.

14:05.640 --> 14:08.560
We'll put a link to the podcast I did with him in the show notes.

14:08.560 --> 14:14.040
But I wasn't even impressed in that the line work, at least at that time, which was a year

14:14.040 --> 14:18.680
and change ago, like wasn't really being applied to neural nets.

14:18.680 --> 14:22.120
It was for, you know, other types of models.

14:22.120 --> 14:25.640
The stuff that we, that I mentioned in the talk is we're not using neural nets at strike.

14:25.640 --> 14:31.000
But the examples he's got are, like, his techniques don't really depend on, my understanding,

14:31.000 --> 14:32.000
the underlying model.

14:32.000 --> 14:33.000
The models themselves.

14:33.000 --> 14:34.000
Okay.

14:34.000 --> 14:35.000
Exactly.

14:35.000 --> 14:37.160
So you can sort of probe a neural net in that if you train a neural net, the example I

14:37.160 --> 14:42.000
showed in the talk was that he has actually in his paper, you have a neural net that recognizes

14:42.000 --> 14:44.760
Huskies, say, versus wolves.

14:44.760 --> 14:49.040
And so he had a beautiful example of a husky that was misclassified as a wolf.

14:49.040 --> 14:50.040
Right.

14:50.040 --> 14:52.680
And so you look at it and you go, yeah, they look kind of the same, like, I get why this

14:52.680 --> 14:53.680
is happening.

14:53.680 --> 14:54.680
Yeah.

14:54.680 --> 14:59.840
So the explanation that Lyme produces shows you the most relevant pixels to the decision.

14:59.840 --> 15:02.840
So like, what pixels that they change to have the most input?

15:02.840 --> 15:06.880
And in this pathological example, it's like the snow underneath the husky.

15:06.880 --> 15:07.880
Right.

15:07.880 --> 15:08.880
Interesting.

15:08.880 --> 15:09.880
Yeah.

15:09.880 --> 15:13.880
One example, and you can, it's clear that like the training set you used clearly just

15:13.880 --> 15:17.240
had every wolf associated with snow marked up on the wrong pattern.

15:17.240 --> 15:22.600
So you, you've explained this deep property of the neural net through one explanation, like

15:22.600 --> 15:25.960
just one image, right, which exposed this rich structure.

15:25.960 --> 15:26.960
Okay.

15:26.960 --> 15:27.960
One last thing.

15:27.960 --> 15:31.440
I'm not trying to make like a, this is this controversial claim that black boxes are actually

15:31.440 --> 15:32.640
easy to explain.

15:32.640 --> 15:37.520
It's more just for the things that we would care about at Stripe, which is how, like, that

15:37.520 --> 15:41.040
you care about with an image, like, why is this particular image misclassified?

15:41.040 --> 15:42.040
Right.

15:42.040 --> 15:45.800
Or why is this particular merchant getting blocked or why is this particular charge?

15:45.800 --> 15:49.000
You can come up with a much richer story about why that's true.

15:49.000 --> 15:52.840
And that seems to often have the property that if you look at enough of these and they're

15:52.840 --> 15:59.560
different enough, you can leach out some understanding of what the model is doing without

15:59.560 --> 16:04.120
actually having to go, you know, look at every path through every tree or the weights

16:04.120 --> 16:05.120
of every neuron.

16:05.120 --> 16:09.240
So you can treat the thing as a black box and get some insight into what patterns it

16:09.240 --> 16:11.920
extracted from the data set you gave it.

16:11.920 --> 16:12.920
Okay.

16:12.920 --> 16:17.840
So the, the line approach is doing what, I guess, sounds like almost like sensitivity

16:17.840 --> 16:19.600
analysis for the inputs, right?

16:19.600 --> 16:21.400
Is that maybe what I think about it?

16:21.400 --> 16:25.720
And then there are also approaches that are based on, you know, kind of introspecting the

16:25.720 --> 16:29.960
layers of a neural network, like deep dream and these other things trying to get, well,

16:29.960 --> 16:31.800
this, this network's looking at edges.

16:31.800 --> 16:35.720
And so if it's misclassifying something, it's because it sees an edge wrong or something

16:35.720 --> 16:37.520
like that.

16:37.520 --> 16:43.840
Did you develop your own technique or, okay, so tell me a little bit about that.

16:43.840 --> 16:44.840
Yeah.

16:44.840 --> 16:47.760
So the technique that we developed, the ones we were just talking about are really

16:47.760 --> 16:54.120
good for actually understanding, for some particular example of fraud or some merchant,

16:54.120 --> 16:56.040
why the model did what it did.

16:56.040 --> 16:58.240
So, or what would have to change to change the decision?

16:58.240 --> 16:59.240
Yeah.

16:59.240 --> 17:05.920
Now, you know, building a product that is meant to block fraud, you know, unfortunately,

17:05.920 --> 17:09.840
some of the merchants that sign up for Stripe, sign up to go just run stolen credit cards

17:09.840 --> 17:11.440
through the system and collect money.

17:11.440 --> 17:15.800
The whole issue is that there are people who are testing cards who are sort of committing

17:15.800 --> 17:20.480
fraud in this network, and because we're one level behind merchants, we have to be a

17:20.480 --> 17:24.680
bit careful about what we expose in terms of our explanations of why the models are doing

17:24.680 --> 17:25.880
what they're doing.

17:25.880 --> 17:31.080
If we were just to give, for every charge, our best sort of most beautiful explanation

17:31.080 --> 17:36.120
of what would have to be different to not get blocked, you know, there's, you could probably

17:36.120 --> 17:40.000
wait this, per merchant by trust and start to expose more and more as they're with you

17:40.000 --> 17:41.000
longer.

17:41.000 --> 17:45.960
You do that through, you know, individual like customer relationships, but as far as

17:45.960 --> 17:49.760
a product, like, what do you want to see on the screen when your card gets blocked?

17:49.760 --> 17:54.400
Like, the explanation that line produces is almost too good.

17:54.400 --> 17:55.840
It's like easy to game, right?

17:55.840 --> 17:56.840
Okay.

17:56.840 --> 18:00.280
So, the system we developed internally, I'll develop it here.

18:00.280 --> 18:05.880
It's a way that you can come up with an explanation for basically a post-hoc explanation

18:05.880 --> 18:11.440
of that looks like a rule that the merchant could have implemented themselves, where if

18:11.440 --> 18:15.280
they deployed that rule, like, it would have caught the charge and it would have agreed

18:15.280 --> 18:18.960
with the model in some high percentage of cases, right?

18:18.960 --> 18:23.160
But it's not necessarily tied to anything internal about the model structure.

18:23.160 --> 18:28.120
So I guess the subtlety here versus the other system is, and this is the title of the talk

18:28.120 --> 18:31.120
is just so stories for AI.

18:31.120 --> 18:35.880
The idea here is that you can develop a sort of, you know, ahead of time, it's hard to make

18:35.880 --> 18:36.880
predictions.

18:36.880 --> 18:39.320
That's why we have to use these models with rich internal structure.

18:39.320 --> 18:42.880
Once you know what's happened, once you know that the model thinks that this charge is

18:42.880 --> 18:48.880
fraudulent, it's much easier to go back and say, okay, given that I know that this is

18:48.880 --> 18:53.160
what the model said, I can go come up with an explanation for why the model did what it

18:53.160 --> 18:54.160
did.

18:54.160 --> 18:57.320
So, like, conspiracy theories are kind of an example of this, right?

18:57.320 --> 19:01.560
Once you know that the model went off in this particular building, you can come up with

19:01.560 --> 19:06.560
some crazy tale about how it got there and it's like a much easier problem.

19:06.560 --> 19:10.280
There's also different, many explanations that will fit some are more credible, some

19:10.280 --> 19:14.720
are less credible, but it's a much easier task than actually predicting a decision.

19:14.720 --> 19:20.560
Is what you're ultimately doing some form of, like, dimensionality reduction or dimension

19:20.560 --> 19:21.560
compression?

19:21.560 --> 19:26.080
Like, you've got all these internal variables in your model, but you're trying to, it sounds

19:26.080 --> 19:30.560
like you're trying to map your story down to, like, these four public facing, what aggregate

19:30.560 --> 19:31.560
thing.

19:31.560 --> 19:36.160
I wish it was, I'll tell you the algorithm, it's, yeah, I think it's a little as disciplined.

19:36.160 --> 19:40.520
Okay, so the simple way to say it, or the first pass of it, is that what we're trying

19:40.520 --> 19:45.720
to do is train, like, a decision tree on, it's basically a bad version of the model that's

19:45.720 --> 19:48.680
meant to predict what the model is going to do, right?

19:48.680 --> 19:54.200
So, we want it to look like a decision tree because we want to be able to present some,

19:54.200 --> 19:58.920
like, the form, the, the output, I guess, of this model, we want to be some set or, some

19:58.920 --> 20:03.560
rule that the user could, could see, or any, of course, using a decision tree because

20:03.560 --> 20:04.560
it's explainable.

20:04.560 --> 20:05.560
It's explainable, right?

20:05.560 --> 20:06.560
Exactly, exactly.

20:06.560 --> 20:12.120
It's not very good, but so, that's the shape of what we want and what the model looks

20:12.120 --> 20:19.240
like is basically a can set of potential explanations, any one of which, if it applies to some

20:19.240 --> 20:24.960
charge you give me, like, any one of which has a high precision with respect to what the

20:24.960 --> 20:26.440
model would have chosen, right?

20:26.440 --> 20:32.360
So, if my rule, if my explanation applies to some charge of the model calls fraudulent

20:32.360 --> 20:36.600
in the test set, I want to be sure that it also would have applied to many other charges,

20:36.600 --> 20:37.600
right?

20:37.600 --> 20:41.480
So, if you just tell me, like, if the explanation is, well, there was a charge on the card,

20:41.480 --> 20:44.960
this is very high recall, it catches a lot of fraud, it has incredibly low precision,

20:44.960 --> 20:45.960
right?

20:45.960 --> 20:48.400
There's no sort of information contained in that statement.

20:48.400 --> 20:53.920
So, again, the explanation model is a list of possible explanations sorted by their precision

20:53.920 --> 20:58.320
in descending order compared to the models, like, what the model claims.

20:58.320 --> 21:04.160
What we do then is, when a charge comes in, if the model claims that it's fraud, we then

21:04.160 --> 21:08.080
go to our explanation model and we just start looking through the explanations, and we find

21:08.080 --> 21:11.320
the first one that applies to the charge that has come in.

21:11.320 --> 21:14.240
So basically, like, what's a path through the decision tree?

21:14.240 --> 21:20.080
Looking at all the paths, not looking necessarily first at the features, just taking the outcomes

21:20.080 --> 21:24.080
and, like, trimming out anything that doesn't apply, which is the explanation that, as you

21:24.080 --> 21:27.880
go down the list, has the highest precision, but also applies to this one.

21:27.880 --> 21:30.040
That's the thing we exposed to the user, right?

21:30.040 --> 21:34.040
So, it's sort of like a decision tree, and then it has, it has the same structure as

21:34.040 --> 21:39.360
a decision tree, but you don't independently ask the explanation model and the forests

21:39.360 --> 21:40.960
and then present both of these.

21:40.960 --> 21:47.880
This probably wouldn't match, right? Because a lot of these explanations won't necessarily

21:47.880 --> 21:53.520
match up, but if you know what the model did, you can take your kind of bad model, your

21:53.520 --> 21:57.600
decision tree, you can trim all the paths that don't agree with the model and then show

21:57.600 --> 22:00.920
the highest precision path down to some leaf.

22:00.920 --> 22:07.480
Okay, so you've got your high fidelity model and your low fidelity model, the low fidelity

22:07.480 --> 22:13.440
model, the decision tree, you are, I mean, it sounds like you're running both of them parallel

22:13.440 --> 22:14.800
along your input data.

22:14.800 --> 22:20.160
Yeah, you run the first one and then the second one gets features as input, it runs its

22:20.160 --> 22:23.400
thing, and again, like, exactly like you're saying.

22:23.400 --> 22:28.760
So I'm not super clear on the part where you're trimming off the, like, you're pruning this

22:28.760 --> 22:30.560
decision tree in some way or...

22:30.560 --> 22:33.560
Okay, so let's see if I can, let's see if I can elaborate on that.

22:33.560 --> 22:38.760
Let's take a step back. So the input to this, the second model decision tree is, it's

22:38.760 --> 22:43.120
not your input data and it's not the output of the first models, the features of the first

22:43.120 --> 22:44.120
model.

22:44.120 --> 22:45.720
Are you talking about the training or the prediction process?

22:45.720 --> 22:46.920
The prediction process.

22:46.920 --> 22:54.600
So for prediction, yeah, it's, yeah, so the input is in fact, yeah, okay, let's step back

22:54.600 --> 22:55.760
and frame this.

22:55.760 --> 22:58.640
So your first, like you said, first model is the black box, right?

22:58.640 --> 23:03.320
Second model is the decision tree or another way to look at this list of predicates, right?

23:03.320 --> 23:08.440
So what you need to evaluate the second model is the output of your first model plus all

23:08.440 --> 23:10.160
the features, right?

23:10.160 --> 23:11.160
So the output of the...

23:11.160 --> 23:14.960
And my features are we referring to the inputs or, like, weights or some internal...

23:14.960 --> 23:15.960
Sorry, good call.

23:15.960 --> 23:20.720
See, again, the statistics machine learning, crazy terms, by features here, I mean, like,

23:20.720 --> 23:26.040
the things we know about the charge, like the dictionary, the inputs to the black box.

23:26.040 --> 23:27.040
Got it.

23:27.040 --> 23:28.040
Okay.

23:28.040 --> 23:29.040
So forget what's going on in the black box.

23:29.040 --> 23:30.040
You're totally right.

23:30.040 --> 23:31.040
Got your features as well.

23:31.040 --> 23:32.040
Okay.

23:32.040 --> 23:34.360
We're going to evaluate the charge, the variables.

23:34.360 --> 23:37.960
We first send those into the black box, we get out some decisions from the black box.

23:37.960 --> 23:38.960
Right.

23:38.960 --> 23:42.080
And we've chosen a threshold in advance, so all we care about is does the model think

23:42.080 --> 23:43.080
it's fraud or not?

23:43.080 --> 23:44.080
Right.

23:44.080 --> 23:45.080
Okay.

23:45.080 --> 23:49.920
Now, we take that, we go to the second model, we take the same inputs to the black box,

23:49.920 --> 23:55.480
and we go through this list, which is ordered from high precision to low precision, and

23:55.480 --> 24:00.160
we find the first path through the tree, again, like, it's important that they're ordered.

24:00.160 --> 24:05.280
We find the first path through the tree that one evaluates the true as well, so agrees

24:05.280 --> 24:06.600
with the black box model.

24:06.600 --> 24:07.600
Well, that's it.

24:07.600 --> 24:09.000
That agrees with the black box model.

24:09.000 --> 24:10.000
Okay.

24:10.000 --> 24:12.760
There might be multiple paths that agree with the black box model, right?

24:12.760 --> 24:17.040
So this is the difference between just evaluating the decision tree.

24:17.040 --> 24:21.200
So we find the one that has the highest precision that does agree, and we return that as the

24:21.200 --> 24:24.120
explanation model, or as the explanation user.

24:24.120 --> 24:25.120
Okay.

24:25.120 --> 24:31.160
How do you even measure the results from, I mean, if we're talking about, if we're talking

24:31.160 --> 24:36.360
about, you know, whether the fraud is the transaction as fraudulent or not, the easy to measure

24:36.360 --> 24:37.360
the results are.

24:37.360 --> 24:39.760
How do you measure like the explainability of the result?

24:39.760 --> 24:41.280
There's a couple answers to this.

24:41.280 --> 24:43.000
One is, I think, a good answer.

24:43.000 --> 24:44.840
One is kind of unfortunate.

24:44.840 --> 24:49.440
The first answer is the way you can evaluate your explanation model is you want each individual

24:49.440 --> 24:51.720
explanation to have high precision.

24:51.720 --> 24:55.800
You want overall the entire set of explanations to have I recall, right?

24:55.800 --> 24:59.680
So you want for any charge that comes in, you want a very high probability of having an

24:59.680 --> 25:01.560
explanation available, right?

25:01.560 --> 25:05.280
And then for each explanation that you give, you want that to be as high precision as possible.

25:05.280 --> 25:08.320
So internally, like, that's one way you can evaluate this.

25:08.320 --> 25:11.680
I think what you're actually getting at is like, what's the point of this?

25:11.680 --> 25:15.960
Like how can you evaluate the effect of giving these to merchants and seeing if they actually

25:15.960 --> 25:16.960
mean anything?

25:16.960 --> 25:19.880
That we were just kind of at the early days.

25:19.880 --> 25:21.600
We don't do a great job at this now.

25:21.600 --> 25:27.520
We don't have, I believe, much built into the product now to evaluate the actual thing

25:27.520 --> 25:33.480
we're trying to do, which is to take something that feels like a black box decision and give

25:33.480 --> 25:37.760
people some way to kind of relate or build a model in their mind for what this black box

25:37.760 --> 25:38.960
model is doing.

25:38.960 --> 25:44.360
Because again, each individual instance of fraud, they might be in a domain where they understand

25:44.360 --> 25:45.360
what's happening, right?

25:45.360 --> 25:47.800
They might be a charity site that is dealing with card testing.

25:47.800 --> 25:51.920
And if they see an instance of this and then the model gives them an explanation that's

25:51.920 --> 25:58.200
a rule that, you know, maybe it's not why the model did what it did, but it actually

25:58.200 --> 26:02.160
happens to match up in a lot of ways with the decisions the model would make for charges

26:02.160 --> 26:03.640
that look like this.

26:03.640 --> 26:08.040
Like that's a valuable piece of information that a merchant could then take that would

26:08.040 --> 26:14.040
one give them some insight into what fraud looks like and two, and this is probably the

26:14.040 --> 26:18.360
real thing you want down the road is to have people just trust what the model is doing

26:18.360 --> 26:20.360
in decision they understand less, right?

26:20.360 --> 26:21.360
Right.

26:21.360 --> 26:25.320
Now, a little bit of what I talked about in the talk is that this is a kind of pathological

26:25.320 --> 26:31.000
by itself, like you have to know, the explanation is there to get you to trust the black box.

26:31.000 --> 26:32.000
Right.

26:32.000 --> 26:35.400
But you shouldn't trust the explanation unless you have some reason already to trust

26:35.400 --> 26:36.960
the black box.

26:36.960 --> 26:42.200
So it's, it's like when Twitter says, oh, you should follow this person because, you

26:42.200 --> 26:44.680
know, Sam follows him and Oscar does too.

26:44.680 --> 26:45.680
Right.

26:45.680 --> 26:46.680
That's not a reason.

26:46.680 --> 26:47.680
Right.

26:47.680 --> 26:51.000
It's just, there's sort of an implied reason, but it's not what's actually happening behind

26:51.000 --> 26:52.000
the scenes.

26:52.000 --> 26:56.520
Well, I mean, part of what I was getting at was, you know, there's all kinds of questions

26:56.520 --> 27:00.960
that it raises, like how granular you want your explanations to be.

27:00.960 --> 27:05.880
The degree to which people actually care about the explanations, like there's, you know,

27:05.880 --> 27:11.600
the book influence, Robert Chaldeini, you know, basically we rejected this charge because

27:11.600 --> 27:14.720
we rejected this charge, like it's putting the word reject there.

27:14.720 --> 27:15.720
Yep.

27:15.720 --> 27:21.320
Just has a huge, you know, provides a huge degree of acceptance, just based on, you know,

27:21.320 --> 27:27.920
human biology and so how do you, how do you wrap your head around, like how granular

27:27.920 --> 27:31.680
you need these explanations to be or is it just like you started somewhere and that's

27:31.680 --> 27:35.240
kind of how much are you investing in that process, I guess?

27:35.240 --> 27:40.520
So internally, we care, well, we care internally and externally, but like the initial reason

27:40.520 --> 27:45.320
we created these, these kinds of models was to try to give our risk analysts and our,

27:45.320 --> 27:50.600
I guess account managers, some way to understand as deeply as they can, like what's happening

27:50.600 --> 27:54.880
with some decision with, you know, a charge that a merchant called to ask about.

27:54.880 --> 27:59.800
So this is a case where you're not really trying to persuade the risk analysts, they,

27:59.800 --> 28:03.880
they have a vested interest in learning as much as possible about this so that they can,

28:03.880 --> 28:07.360
you know, sound knowledgeable or be knowledgeable like when they're talking.

28:07.360 --> 28:12.120
But you still don't want to give them the internal representation of the black boss model.

28:12.120 --> 28:13.120
That's right.

28:13.120 --> 28:16.360
The case I've just described leaves it up to the, you know, the person on the phone to

28:16.360 --> 28:18.600
like hide information that seems sensitive.

28:18.600 --> 28:19.600
Right.

28:19.600 --> 28:25.440
Whereas when you have the model do it, you know, I don't have a great answer for this.

28:25.440 --> 28:29.080
We're not like, it's a, it's a really interesting question.

28:29.080 --> 28:33.800
What level of granularity, you know, what you're really trying to do for a merchant?

28:33.800 --> 28:38.000
How you, how you have them, how you give them the ability to like let you know what they

28:38.000 --> 28:43.280
care about or maybe you give them the ability to, you know, sort of request more information

28:43.280 --> 28:44.560
that you can expand this thing.

28:44.560 --> 28:45.560
Right.

28:45.560 --> 28:47.760
How many explanations do you have?

28:47.760 --> 28:50.680
Each model has roughly like 80 possible explanations.

28:50.680 --> 28:52.400
That usually gets like full coverage on this thing.

28:52.400 --> 28:53.400
Okay.

28:53.400 --> 28:54.840
There's like 80 possible explanations that could apply.

28:54.840 --> 28:55.840
Okay.

28:55.840 --> 28:57.840
I'm comparing it to like credit reporting.

28:57.840 --> 28:58.840
Yeah.

28:58.840 --> 29:02.280
There's like what all of eight explanations and these are ones that you commonly see.

29:02.280 --> 29:03.280
Yeah.

29:03.280 --> 29:06.480
And this, this might help us out that like the rest of the world is just like, there's

29:06.480 --> 29:10.120
a sort of argument by authority for like scary thing is happening in the world.

29:10.120 --> 29:11.120
Yeah.

29:11.120 --> 29:12.120
Go ahead and deal with it.

29:12.120 --> 29:17.480
So this is, I mean, the reason I kind of, I was interested in giving this talk and

29:17.480 --> 29:21.560
doing this work is, these are a lot of the open questions in this, in this field, right?

29:21.560 --> 29:26.760
Like, there's the goal here of getting a customer to trust us.

29:26.760 --> 29:32.040
That doesn't mean anything without some internal, you know, sense of accuracy or reason to

29:32.040 --> 29:33.040
actually trust the model.

29:33.040 --> 29:36.800
If you've got the Husky snow dog problem going on internally, but then you're talking

29:36.800 --> 29:39.960
people into accepting the decisions, that's kind of pathological.

29:39.960 --> 29:44.240
And so with the talk, what I, what I was interested in after this that kind of expands

29:44.240 --> 29:48.800
beyond stripe is that, though we probably will have to deal with this, or I probably will

29:48.800 --> 29:53.200
personally in the next year or so, you know, the EU has this, what's it, the general data

29:53.200 --> 29:54.200
GDPR?

29:54.200 --> 29:55.200
GDPR.

29:55.200 --> 29:56.200
Exactly.

29:56.200 --> 29:58.640
The GDPR, the GDPR gives you this, you know, right to an explanation.

29:58.640 --> 30:00.400
This is one of the clauses.

30:00.400 --> 30:06.240
Now, after working up to this talk and giving it one of the messages I have is, it's, it's

30:06.240 --> 30:08.040
not clear what this means, right?

30:08.040 --> 30:12.520
There's two forms of explanations, there's many forms, but of these two, if people are having

30:12.520 --> 30:16.880
a right to this sort of explanation, like you said, to just have some emotionally triggering

30:16.880 --> 30:21.760
word like rejected in their explanation that's meant to convince them of something that's

30:21.760 --> 30:26.320
going on, this is actually not good, like, I don't think this is the intent of what's

30:26.320 --> 30:27.320
in that law.

30:27.320 --> 30:28.320
Absolutely.

30:28.320 --> 30:33.560
And the other hand, on the other hand, if you're giving people like incredibly detailed

30:33.560 --> 30:39.760
explanations about why the model did what it did, well, this is good, but it's potentially

30:39.760 --> 30:44.400
gameable, you know, like, I guess the core issue here is that you started.

30:44.400 --> 30:51.280
There's an element of, I mean, like domain appropriateness for lack of a better term.

30:51.280 --> 30:59.520
If you, you know, you could, you know, one way to, you describe one way of kind of scurrying

30:59.520 --> 31:01.960
GDPR is by, by, you know, having just this, you know, this, you were rejected.

31:01.960 --> 31:02.960
That's the explanation, right?

31:02.960 --> 31:04.960
That's certainly not in the spirit of the law.

31:04.960 --> 31:09.760
You know, but on the other side, if you say, well, the, you know, the third neuron and,

31:09.760 --> 31:15.040
you know, layer five of our neural network, you know, spit out a 0.6, you know, that's

31:15.040 --> 31:16.040
why you're rejected.

31:16.040 --> 31:21.080
That's also inappropriate, even though it's way more detailed and way more granular.

31:21.080 --> 31:25.880
So I didn't really mean, though, it's, it's a good characterization of it.

31:25.880 --> 31:29.600
I wasn't necessarily saying that that would be a way to skirt the rule, though it certainly

31:29.600 --> 31:30.600
would, right?

31:30.600 --> 31:34.920
If you give people these, okay, you've been rejected, you know, that character of explanation

31:34.920 --> 31:39.360
though about, hey, we don't know why the model worked, but here's a plausible explanation.

31:39.360 --> 31:43.920
I guess in, in thinking about this, I think the problem with that, if you did, if that

31:43.920 --> 31:48.600
was what was adopted by say the courts or, you know, a car, for example, having to spit

31:48.600 --> 31:51.920
out an explanation of why it did, but it did the cost and damage.

31:51.920 --> 31:56.360
This doesn't teach you much about the core problem of, that I think we're trying to solve

31:56.360 --> 31:59.760
here, which is there are things we care about in the world.

31:59.760 --> 32:04.440
And then there are the goals that we give to our models when they get trained, right?

32:04.440 --> 32:06.200
You have some prediction target.

32:06.200 --> 32:09.840
And it's just the fact that we don't, we don't know enough about ourselves about what we

32:09.840 --> 32:16.040
care about, about how we work to encode all of our ethics, all of these things in the

32:16.040 --> 32:17.880
goals of a model.

32:17.880 --> 32:22.520
And so when you can ask a model, like, I think my understanding here, my interpretation

32:22.520 --> 32:26.640
of this is, when you can ask a model for an explanation of why it's doing what it's doing,

32:26.640 --> 32:30.480
what you're doing is you're monitoring whether or not it picked up the things you actually

32:30.480 --> 32:34.120
care about from the data that was easy to encode, right?

32:34.120 --> 32:36.560
And what do you do then?

32:36.560 --> 32:42.240
What you do if you find a mismatch, the only thing that makes sense is to take the thing

32:42.240 --> 32:46.440
that wasn't quite there and to figure out how to encode that in the goal of your model

32:46.440 --> 32:48.040
that you're training, right?

32:48.040 --> 32:54.720
So you have, on the one hand, this, again, kind of pathological case where models start

32:54.720 --> 32:57.960
optimizing for whatever they're optimizing for, right?

32:57.960 --> 33:02.320
Once you start feeding data back, like, model decisions back into its own training set,

33:02.320 --> 33:06.840
you start to get this odd effect of, like, putting a copy on the copy over and over.

33:06.840 --> 33:10.960
And then you've got this other parasitic model that's, like, just justifying whatever's

33:10.960 --> 33:11.960
happening under there.

33:11.960 --> 33:12.960
This is not good.

33:12.960 --> 33:13.960
Right.

33:13.960 --> 33:14.960
Right.

33:14.960 --> 33:15.960
That's the downside.

33:15.960 --> 33:20.320
So we're forced to really clarify what we care about in technical terms, right?

33:20.320 --> 33:24.280
Like, you can take these ethical concerns, this is what I guess the law attempts to do

33:24.280 --> 33:31.280
through almost like your sidecar model is like a, almost like a unit test for your regular

33:31.280 --> 33:32.280
model.

33:32.280 --> 33:38.080
Is it encoding the kind of the way of thinking about the relationship between the transactions

33:38.080 --> 33:39.080
and the judgments?

33:39.080 --> 33:40.080
Yep.

33:40.080 --> 33:43.280
Like in a conversation, if you explain, you say something to someone, they repeat it back

33:43.280 --> 33:45.520
in a slightly different way, what's the point of this?

33:45.520 --> 33:49.760
Or you're just demonstrating that you've absorbed the content, right, stating it in a different

33:49.760 --> 33:50.760
way.

33:50.760 --> 33:55.080
And if you go, yeah, totally get it, you know, and just say something insane, right?

33:55.080 --> 33:57.120
We get to talking again, you know?

33:57.120 --> 34:03.480
So that I think is what's so interesting about this tension between the different kinds

34:03.480 --> 34:06.760
of explanations, the subtleties of what's going on inside the model.

34:06.760 --> 34:07.760
Right.

34:07.760 --> 34:12.760
So one example of, like, an ethical concern that people have been talking about, there's

34:12.760 --> 34:18.600
a couple of folks internally, it's right, where actually speaking of the GDPR, like one

34:18.600 --> 34:23.760
concern for a model is that it's going to pick up on some kind of implicit like pattern

34:23.760 --> 34:27.400
in the data and start treating like people of different genders, different race, like

34:27.400 --> 34:28.400
different groups.

34:28.400 --> 34:30.160
It's going to have different considerations for them.

34:30.160 --> 34:35.160
Now this first kind of model, there's a Peter Norvik quote I gave in the talk, can give

34:35.160 --> 34:39.160
you an explanation that just kind of ignores what's actually going on and gives you a plausible

34:39.160 --> 34:43.520
reason why, you know, somebody might have been rejected from a job, like say it's right,

34:43.520 --> 34:46.600
we start turning a model loose on hiring, right?

34:46.600 --> 34:52.000
If you can figure out what is different about what you care about, which is this sense of,

34:52.000 --> 34:56.960
this kind of vague sense maybe that everyone like deserves a fair shot, et cetera, you

34:56.960 --> 35:01.280
find that your model is not doing that, you're then forced to encode this more formally.

35:01.280 --> 35:06.360
So Google has a great post which says this in a nice way, which is like one way to encode

35:06.360 --> 35:11.080
this is to say that along any split you care about, along any group, along any like business

35:11.080 --> 35:15.400
type or something like this, let's make sure that the false positive rate is identical

35:15.400 --> 35:16.720
across all these things.

35:16.720 --> 35:21.680
And so this then can get fed back into the, like once you realize that if this is not happening,

35:21.680 --> 35:25.560
you can then feed this back into the training process for your black box models, they then

35:25.560 --> 35:30.360
will kind of formally encode the ethics you care about in the world and then you move

35:30.360 --> 35:32.760
on to the next problem, of course, right?

35:32.760 --> 35:38.840
Yeah, it's really interesting, even circling back to the kind of the beginning, the notion

35:38.840 --> 35:45.440
of explainability, the model you're describing, like the second model, you could argue it's

35:45.440 --> 35:51.240
not really explaining the first model, it's explaining, it's offering a justification,

35:51.240 --> 35:52.240
right?

35:52.240 --> 35:56.720
And you kind of said that, but we talk about this broadly as solving the explainability

35:56.720 --> 36:01.160
problem, but it doesn't necessarily offer any insight whatsoever to what's actually

36:01.160 --> 36:02.560
happening inside your model.

36:02.560 --> 36:07.840
That's right, which I guess, you know, we're kind of getting into semantic land here,

36:07.840 --> 36:11.520
but you know, explainability of the model versus explainability of the output.

36:11.520 --> 36:17.560
But I think they're, you know, at the very least, I think, you know, for folks that are,

36:17.560 --> 36:20.960
you know, thinking about this stuff, it makes sense to at least try to be clear on which

36:20.960 --> 36:22.600
one you're trying to achieve.

36:22.600 --> 36:28.480
I think that's a great takeaway, I mean, to wrap back to what you, when you were talking

36:28.480 --> 36:32.720
about like, if you give someone an explanation on the state of the neurons, you know, one

36:32.720 --> 36:37.040
thing that comes up a lot in these, in these papers, these papers that are trying to talk

36:37.040 --> 36:41.840
about how to formally encode explanations is this idea that, or these casual references

36:41.840 --> 36:45.320
to how humans reason and how humans explain their own decisions.

36:45.320 --> 36:49.560
And it's just kind of taken for granted at this point that the way we work is separate

36:49.560 --> 36:52.560
from the explanations we give for our actions, right?

36:52.560 --> 36:59.160
And so this is one thing that comes up when you read about the GDPR is that one critique,

36:59.160 --> 37:03.360
I guess, of this clause is that we're asking for things from our black box models that

37:03.360 --> 37:07.000
we don't ask from judges, because there's just no conceivable answer, right?

37:07.000 --> 37:11.680
So it's like you said, it's sort of getting into semantics is kind of getting into sort

37:11.680 --> 37:16.960
of whatever philosophy I can sort of pull out of a software engineering career.

37:16.960 --> 37:19.200
But I think it's really important to contrast the two, right?

37:19.200 --> 37:21.480
And say, you know, what do we mean?

37:21.480 --> 37:22.760
What are we asking for?

37:22.760 --> 37:26.800
There's this naive idea that because it's a model, because it's an algorithm, there's

37:26.800 --> 37:30.360
like a clear answer to why it did what it did, or because it was something you could turn

37:30.360 --> 37:31.360
the crank on.

37:31.360 --> 37:32.360
Right.

37:32.360 --> 37:33.360
But that's not right at all.

37:33.360 --> 37:34.960
It's much more subtle than that.

37:34.960 --> 37:35.960
Awesome.

37:35.960 --> 37:36.960
Awesome.

37:36.960 --> 37:39.960
So is any of the work that you've done in this area published?

37:39.960 --> 37:44.000
We're planning in the next few weeks on writing more about this, and I think publishing

37:44.000 --> 37:45.000
the algorithm.

37:45.000 --> 37:46.000
Okay.

37:46.000 --> 37:47.000
That'd be great.

37:47.000 --> 37:49.440
Yeah, I'm sure folks would be very interested in learning more about it.

37:49.440 --> 37:54.480
And once again, we'll drop the link to Lime and the conversation with Carlos.

37:54.480 --> 38:00.080
And are there any other efforts at explainability that figured heavily into the work that you

38:00.080 --> 38:01.080
did there?

38:01.080 --> 38:03.920
I'll give you some links to put in the show notes too, but we've got, I mean, there's

38:03.920 --> 38:04.920
so much stuff going on.

38:04.920 --> 38:09.960
There's a DARPA program now, an explainable AI, Stuart Russell at Berkeley has, I think

38:09.960 --> 38:13.920
it's called, I cannot remember the name, not human compatible, maybe human compatible

38:13.920 --> 38:14.920
AI.

38:14.920 --> 38:15.920
Okay.

38:15.920 --> 38:16.920
There's a track at MIPS about this issue.

38:16.920 --> 38:21.720
I mean, this is really coming up as, of course, we have to get into this, it's coming up

38:21.720 --> 38:27.280
as one of the core approaches toward this problem of like AI and algorithmic safety, right?

38:27.280 --> 38:30.160
Which can often be this fuzzy, scary conversation.

38:30.160 --> 38:34.240
For me, I think there's grounds it very heavily, like what are we worried about kind of one

38:34.240 --> 38:38.280
thing is the leaching of meaning from just letting things that are loose that are really,

38:38.280 --> 38:42.360
really accurate, but haven't encoded the things you care about in the world, right?

38:42.360 --> 38:44.240
And explanations are kind of our tether on this.

38:44.240 --> 38:45.240
Yeah.

38:45.240 --> 38:48.960
So I'll send more links and I'm going to do some writing on this next week that I'll

38:48.960 --> 38:49.960
send over as well.

38:49.960 --> 38:50.960
Okay.

38:50.960 --> 38:51.960
Just the full brain dump.

38:51.960 --> 38:52.960
Awesome.

38:52.960 --> 38:53.960
Looking forward to it.

38:53.960 --> 38:54.960
Well, thanks so much, Sam.

38:54.960 --> 38:55.960
Always very to have another Sam on the show.

38:55.960 --> 38:56.960
Yeah.

38:56.960 --> 38:57.960
Thank you, sir.

38:57.960 --> 38:58.960
Enjoy the rest of the conversation.

38:58.960 --> 38:59.960
Thank you, Sam.

38:59.960 --> 39:05.440
All right, everyone, that's our show for today.

39:05.440 --> 39:10.240
Thanks so much for listening and for your continued feedback and support.

39:10.240 --> 39:15.040
For more information on Sam or any of the topics covered in this show, head on over

39:15.040 --> 39:19.960
to twomolei.com slash talk slash 73.

39:19.960 --> 39:27.440
To follow along with our Strange Loop 2017 series, visit twomolei.com slash ST loop.

39:27.440 --> 39:33.200
Of course, you can send along your feedback or question via Twitter to at twomolei or

39:33.200 --> 39:38.080
at Sam Charrington or leave a comment right on the show notes page.

39:38.080 --> 39:41.360
Thanks again to Nick Sosis for their sponsorship of the show.

39:41.360 --> 39:48.600
Check out twomolei.com slash talk slash 69 to hear my interview with the company founders

39:48.600 --> 39:55.960
and visit NickSosis.com slash twimmel for more information and to try their API for free.

39:55.960 --> 40:14.160
Thanks again for listening and catch you next time.

