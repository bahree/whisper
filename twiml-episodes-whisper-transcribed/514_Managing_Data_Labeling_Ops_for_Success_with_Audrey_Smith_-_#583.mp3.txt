All right, everyone. Welcome to another episode of the Twomo AI podcast. I'm, of course,
your host, Sam Charington. And today I'm joined by Audrey Smith, the Chief Operating Officer
at ML Twist. Before we get into today's conversation, be sure to take a moment to head over
to Apple Podcasts or your listening platform of choice. And if you enjoy the show, please
leave us a five-star rating and review. Audrey, welcome to the podcast. Thank you, Sam.
Thanks for having me. Absolutely. I am really looking forward to our conversation. For those
listening in with us, if you've been listening to the show recently, you know that we've been
digging deep into data-centric AI. And this conversation will continue in this theme. And I'm
super excited to have Audrey on. She's got a ton of experience with labeling, label ops,
and so much more in this space. So Audrey, once again, welcome. Thanks, Sam.
Let's start with a little bit about your background and how you came to work in machine learning.
Sure. So I actually studied law. I am a lawyer. I studied in France. You can hear my accent, of
course. And I was a lawyer in France for three years before I decided that I wanted to have an
international experience. So I went to the UK, worked there for five years. And then when I moved
to the US in 2014, I was really looking forward to finding my place into the tech industry.
Until then, I had no technical background. I didn't know anything about machine learning.
So I didn't know exactly how I would start. But I applied for jobs basically where you needed to
have French speaking skills. And my first job was to listen to Siri and listen, especially to
French speakers, talking to Siri. And that's the start of my data labeling journey. Really,
that's when I learn about machine learning and how data labeling is so important to it.
And I wanted to dig into it a little bit more, went to Google, worked on way more projects,
linked to GPR compliance, user experience, ads policy, and all around data labeling. And I got
hooked. So I went to Amazon, stayed there for four years, working data labeling operation as well,
helping Amazonian team with their machine learning projects. Got lucky to work on a lot of
different projects, pertaining to different formats like video image, text. And then after four years,
went to label box, when they were still Series A. And I was the director of labeling operation
over there for a couple of years. And then joined ML Twist a few months ago as their chief operating
officer. Awesome, awesome. Why don't you give us a quick summary of ML twist? Sure.
So really, ML Twist is coming from GID that they are the space in the data labeling space for
machine learning is very crowded at the moment. There are so many different players on the market.
And they're all offering different solutions. And they're like even newcomers on the market.
So if you think about it, there are over 80 data labeling platforms out there. And so much more
when it comes to workforce labeling companies. And now you have the newcomers that are the
synthetic data platforms, the augmented data platform and so on and so on. And this is all great,
all this technology is great, but they don't connect to each other in a very easy way. And they
are like, it's pretty siloed. And a lot of them are even specializing certain verticals or
in certain formats. And so GID behind ML Twist is to connect all this ecosystem and give companies
the choice to use the right tool for the right use case. It's a kind of like middleware for
your labeling software and systems. Yeah, we were called middleware. I think that's okay,
that's okay to call us middleware. And really like the idea is to, you know, be the glue
and connect all the different tooling so that like data science team doesn't spend time looking
for the right tool, the right workforce, the right everything and just easily connect all the
pieces of the puzzle together to get their machine learning model trained and performing at the
right level. So let's maybe start by talking about some of the commonalities you've experienced
as you've tackled labeling across many different companies and customers. What's the typical
journey for an organization getting started with labeling? Yeah, that's a great question. So
whether it's a small startup starting the ML journey or a big company who wants to enter the AI
world, the journey is very similar. As I mentioned earlier, the space is very crowded with a lot of
very great tools on the market. And the idea for these companies that just start their journey
is that they don't have a data labeling operation person in house. Usually data scientists,
machine learning engineers or product managers are the ones like who are really trying to get
all the right pieces in place. And without any knowledge, without any background, it's quite
overwhelming to go after it and you have to find your right data labeling tool. And as I said,
there are many on the market which one is the best one for your use case and then the right
workforce and depending on what you want to label, each workforce is going to have their own
strengths and their own weaknesses. So like you have to find, assess them and then make your
own choice and selection. And that's just it takes time. It takes like probably a few months to
get there. And once you get that in place, there is also the formatting issue, right? Because you
have your data in house with a certain format and you need to connect that to the data labeling
platform that you're going to be using. So data scientists are going to have constantly to change
the format to be able to plug into the data labeling tool. And that's also another issue that
needs to be tackled. So yeah, definitely you need to think about all the species and once you find
the species and you connect them, you have to train, you have to create your task, put them on
the platform, train the workforce on the task and go quality control, look at the quality because
it's not only about maintaining, like sorry, it's not about just reaching the high quality of
your data, it's also maintaining it as you go through all your rounds of data labeling. And
that can be very challenging. In the early days of labeling when folks would use, you know,
crowdsource, like commoditized crowdsource platforms, like mechanical Turk, you know, data quality
was and continues to be a huge issue. And you know, folks would do like, have multiple lablers and
try to kind of abstract away from the individual labeler to achieve levels of quality. You know,
when you're using one of these labeling services, is that done for you? Or are you still
having to kind of manually implement these kinds of tricks in order to achieve high quality labels?
So you still have to do that, unfortunately, like the idea is that, you know,
the mist that you can just like create a task and then set it up on the platform and have
lablers in another part of the world, being able to nail it from the first pass is like,
it's not gonna work. It can work. So crowdsourcing is a great option. Definitely when it comes to
easy task, to task where you think that the knowledge is universal, right? Like is there a dog or
is there a cat on the picture? You can trust anyone to know the difference. When it comes to complex
tasks, then you need guidelines. You need rules in place that you're gonna train people on so that
they can again annotate with high quality, but also consistently. And that's part of the challenge.
And that means that companies going through that journey have to accept that feedback is
very important on the top of quality control. They need to train the people. It's like going to
school. You can't know like first you train and then you get tested and then you can you can
deliver high quality data. And that's exactly the mindset that companies should use when they
start working with outsourced labelling workforces. I had a really interesting conversation recently
as part of this series with Cheyne Mahanti at Watchful. And we were talking about this same
journey. And in particular, when companies do labelling inside in-house versus one to outsource
it. And the key idea that he raised was this idea of context. Like you can outsource when
there's no context required to correctly label. Like you said, you know, anyone can know a dog
from a cat or even, you know, a stop sign on a road. But if, you know, some deep business
context is required, you know, that is going to have to be done in-house. Is that your experience
as well? Not exactly. So I had a chance over the years to test some of those ideas. And we were
even able to test. So companies were coming to us and asking us, look, I really need doctors
to annotate my data because that requires medical knowledge. And on the side, we were testing
something else. We were saying, okay, let's do it with doctors. But let's do it also with
regular laborers. And maybe a doctor comes in and train the people on what to do and what to
recognize. And actually, we got pretty similar quality from boss ranges. So it doesn't have to be
all, you know, done internally because these people have the right knowledge. It can be that we
are able to transfer the knowledge. It might take longer, but we can get to very similar quality
over time. That's going to be tremendously less expensive because of that. So that there are
pros and cons, but it's not entirely true for my experience. But it sounds like you have to be
committed to that training and knowledge transfer and feedback loop in order to achieve the
level of quality you'd want with non-subject matter expert laborers. Exactly. And then that's
where every company is going to make a choice. If they have to get a very fast turnaround time
with high quality, obviously, you have to go with an internal team that's going to be very
knowledgeable on the topic they are labeling. But if you have a bit of time and if your cost
sensitive, then you might go the other route, take a bit longer, but get to the same quality
after a while. Yes, maybe continue on this training, this training angle are there. Things that
you've learned about how to effectively train and support laborers in the process.
Yes, absolutely. I think that as a data labeling operation, people really like one of the
skills that you have to master is being able to translate technical requirements into simple
task. And that's really like what it is about. When you get to talk to machine learning engineers
or data scientists about what they want, what the image in that the task should be about and
how to handle it and so on. It can be very confusing and it can go like in a different places
at the same time. So Jager is to get all these requirements and put them on paper and get Jager,
okay, what do they really want and make it as simple as possible when you translate that into
guidelines that's going to be helping the team getting trained on the task. So really,
that's actually something I learned doing over the years is that you have to translate,
you have to think as a labeler that has no technical background and tell them what they need to do.
And one thing I really like doing with the labelers is once they have worked on a product for months,
giving them more context about why they are doing that is very important because that give them
the purpose of why they are doing this task on a daily basis. Once the product has been released
publicly and there is like press release or a website talking about it, I love sending that
to the laboring team that worked on it so that they really understand and they can really see
and share like the success of the product. To help them be part of the team. Exactly.
Can you maybe give us an example of, you know, when you think of kind of the most complex
labeling tasks that you've tackled, give us an example of one of those and some of the things
that you did from a training and support perspective to help that team be successful.
One of the most complex use cases I worked on was like touching to augmented reality. I worked on like
J.J. about having an item that would be you would be able to see in your living room with like
the real size to see if you would like to buy this couch or this lamp or this kettle and that
was actually a very complex workflow. The task in itself was not super complex but on my end,
you know, like on the labeling operation side of things that was very complicated because
of the technology that we were using, we were not able to use any item that we had. We couldn't
like for instance have some sort of spiky item going through that technology that would render
this like a vision of the of this item. So there was like this like all selection of the items
that could go through that technology that we were using and then you would have to after that
talk with the product management team that had like all these like dates to release all these
products so that they could be available on the marketplace and so we had to juggle all of
this like huge volume and at the same time quality issues that were happening. On the top of that
once there was the ear reproduction of the item like some of them were not good at all and that
couldn't be publicly released on the website so we had to go through some quality control.
Anyway, the entire workflow was very complex and was handled by a lot of different stakeholders,
a lot of different teams inside and outside of the company and that was pretty stressful to handle
but that worked really well in gym. It sounds like one of the takeaways there is that
just the place of labeling in the overall kind of process of delivering a product it's
you know maybe easy to think of hey you know before you start this project you've got this big
data set you send it off to some labelers you know you get back you know a label data set and then
you kind of start with the product of productizing or building whatever you want but it sounds like
at least in the case of this workflow that you're describing you were kind of in the loop of
productizing this ARVR experience and you know the the things that you were the challenges that you
were experiencing just related to the complexity of delivering the product overall.
Yeah that's that's very true and I think that's like really the the core of the data labeling
operation team is that is that we are in the middle we are not we can be at the start of it but
there is always this idea that there is a product that's going to be released at the end and
what does that mean is that as a data labeling operation person you are dealing with humans you are
dealing with like you know a labeling team you are dealing with the product managers you are
dealing like with so many people having their own requirements but also their own limitations
and you have to juggle all of that to make sure that the product is going to be released on time
and that's true whether or not you are data labeling operation team internally or you are just
you know like these companies outsourcing all of that piece this is this is very central to the
success of a product release for sure. You've mentioned data labeling operations a few times
you know how how kind of ubiquitous and mature is that as a role a job title a function
the most large organizations that are you know have a significant investment in ML and labeling
you know have that team in place. That's a really good question I think that I was really at the
early stage of that that journey like data labeling operation when I started I mean I got lucky
right because basically there was no knowledge or degree in data labeling operation and so they
were opening the doors to anyone who wanted to give it a try and so as the years go by now you get
people who have been in the space for a few years like I've been in the space for the past seven years
and that's the type of people that you want to have if you want to lead data labeling operation
team at the big company and additionally like the complexity of the data labeling space not only
in the ecosystem as I mentioned earlier there are so many players on the market right now but also
the complexity of the labeling tasks themselves all of that have grown exponentially over the years
and that's just like you you need people who are specialized in the domain to make the right
decision but also to make decision very fast and that's still not completely I think understood
but we're coming we're getting there I see more and more people with like you know a lot of
experience getting hired to other places so there is this idea now that you're looking for people
with experience in the data labeling but now you know when you start up or when you only start
your journey you don't have that in house you I think it takes a little bit of time to to to
realize that this is a nation and it's a short wall in the in the entire AI loop you mentioned that
when you started there was no degree program is there now or have you seen like certifications or
that kind of I know not yet but I think I think it's going to happen in the in the next few years
I think it has to happen that has become its own specialty and and I really truly believe that
it's going to happen soon so if you're in this role if you're the the data labeling ops team
and you know you're at a larger company that has multiple projects and you're approached with a
new project you know walk us through kind of the steps of onboarding a new a new project or initiative
or customer however you would think about it what are the things you're thinking of what are the
things you're asking them for and just kind of how do you think about kind of spinning up a new
effort basically you need to talk to the machine learning engineers you need to talk to the
scientists and understand their needs what are they looking to accomplish with that labeling task what
is their model about what do they want to recognize or predict with that model that they are they
want to get the data for that's really what's going to help make sure that you're going to get the
right task to the laborer so that you can get the right data labored and then you're going to be
able to feed your model and train it so really there is this discussion on like where they want to go
and then once you frame it you talk about the task and either they have already an idea about the
task or they don't they can just tell you you know I have 50,000 images of dresses and I want to
recognize all the attributes on the dress do it good luck I need that for like in three months time
and then you go about it and then so the idea is to show them the task that you're going to be
creating the guidelines that will go with it but also like the examples that you're going to give
because the the best way to train labeling team on a task is to show them example about how to
train so about how to label a specific task so what I used to do for instance even at amazon is
create my own task and then work on it myself label a few images myself to see if that was making
sense if I was covering all the different use cases and once it was done I was submitting it to the
machine learning team they were telling me if they like it or not if that works for them and then
from there I would train the team and that goes back to what I was talking about which is feedback
loop tell the machine learning engineers hey you're going to have to do some quality control
after the first pass to see if what you're getting is what you want or not give feedback and so on
and so on so JJ is to have like a great relationship with the technical team and just like you know
give them in advance the knowledge get them used to the fact that it's not going to be one one
of thing that's going to be a project that's going to be on and on that's going to be very repetitive
but that we will be there to help them get there with high quality annotations and a lot of
ways it sounds like a product management type of role like you're the labeling product manager
yeah yeah I like that idea a lot of a lot of people doing labeling operations some like they end
up also being product managers for machine learning products that's correct so you've you've developed
this task and you've got this feedback loop you know it sounds like the task will often evolve
quite a bit from what you originally thought it should be to you know what you what you do at scale
can you talk a little bit about that evolution yeah I think it's very important to keep in mind that
there will be a feedback loop and and that's what I was trying to to say earlier is that you know
like you're you're going to have two types of feedback loops basically you're going to have the one
that's going to be about training the laborers making sure they understand your requirements
making sure that they are reaching the the right quality and once you get there you're going to get
your data back and then what happens is that you're going to feed your model with that with that
data and that's why you're going to realize that that your model is responding well to everything
that's supposed to be doing or sometimes it's responding well to some areas but not to others
and then in that case that's actually a feedback loop that's sent to the data science team
so that they know that they need to be framed and reworked that task to cover those areas that
were not working well when they tested their model and so again you get into another task that
you're going to train the laborers on and you're going to give them another feedback loop and so on
and so on so are there some common ways that tasks evolve I imagine you know if you've
if you've done similar types of use cases you know an engineering team may you know
habitually come to you and say I want this and you say no you really don't want that because
you're going to end up changing it to to something else because you've just seen over and over
it doesn't work the way that that they think but I'm wondering if there are examples of
you know those kinds of evolutions or transitions where you know that a task starts out as
as X and ends up evolving to Y for whatever reasons I would say yes I'm trying to think about an
example but that's not coming to my mind right now but I I would say what what really blew my
mind over the years is that I've seen a lot of different companies going after the same
problem trying to solve it and they were doing it in a very different way which was also like very
very intriguing for me because I was like I saw that okay you want to recognize that and
the machine learning teams are going to be convinced that the way they are going about it is
the right way and and and actually that's where like it creates like even more like complexity
into that space is that there are like a lot of different ways to tackle a problem exactly the
same problem so that I've seen a lot for sure so we've talked about setting up these tasks
we've talked about the the training part is there an element of workforce selection
that comes to bear when you're spinning up a new effort yes absolutely so as you like it depends
on the task if you have a task that requires specific knowledge you want to find a workforce
that's going to be able to get that type of special specialized people to work on the task whether
or not it's to become labor laws or to be training the labor laws on the task
that that's one now you can have also task that requires language skills and if you want to
understand the context let's let's take the example of you know content moderation on on social
major you can you you you want to do it on a lot of different languages not only in English so
you're going to have to find a workforce that's going to be able to understand Spanish and
Brazilian Portuguese and so on and because it's social major there is a cultural context to it
it's not only knowing or understanding Spanish it's also understanding the context of the country
right like why would they talk about this politician and and so on and so on and and so that's
where like it becomes very tricky to have only one workforce to do all your needs if you have a
lot of different needs you're going to have to have a portfolio of workforces that you're going to
be obviously testing before partnering with them but that will be helping you with all your
different data labeling needs and sometimes it's even like better to have workforces from
different parts of the world because they're going to be able to see things with a different angle
now when you look at the various labeling company websites they all
you know portray themselves as solving all the problems you know with equal level of excellence
and they all want to kind of be the one stop shop but it sounds like your experience is that
that's not really the case and it does it shouldn't necessarily be the case I think that it was
probably true a few years back when you were mentioning mechanical talk or like maybe there was
like crowd flower at the time like it was just like the space was like very new there were
very few players and and you had to go to one of them to make it work somehow or even you have
the idea about having companies that were bridging their own in-house labeling tool because they
couldn't find anything that was working for them on the market I mean even Elon Musk just said
that on the podcast recently that they had to build their own data labeling platform so
it's it's I in my experience I don't think it's true anymore because what happens is that
companies are going with one data labeling tool and one workforce and they try and and we know
why because it's so difficult to get all the species together it's so time consuming and
like as I said earlier these people are data scientists they should be working on models they
shouldn't be working on finding the right partners but they do that and then and then all of a sudden
the quality drops all of a sudden the cost gets higher or the turnaround time is not good enough
and then everything it's kind of like falling apart but you know you don't want to go through that
process all over again it's it's so time consuming to find another another partner so I believe yes
the JECO system is is is great right now it's crowded but in a good way meaning that there are
so many options on the market that a company doesn't need to choose one option they can just play
with them depending on what they need at the moment they need it depending on the model they're
working on I think that we need to keep in mind that nowadays companies have not not one model in
house they have hundreds of models that they need to work on with different formats and and and
thinking that there will be one stop shop I think is is something that's going to be hard to to
believe moving forward you mentioned that you've seen kind of this recurring pattern of when you're
working with a vendor at some point the quality drops off and the turnaround time gets extended
and that kind of thing is that is there something inherent about labeling the causes that to happen
or is it more kind of just working with a vendor and they're really focused on the new customer and
they kind of let the the older customers languish a little bit I think yeah part of it is working
with human beings like it's you know like you're not like working with tech you're working with
people and they have lives and they have issues and they get sick and you know so we have to take
that into account again like the idea about having like this like data labeled very fast with very
good quality it's going to be hard to maintain that over weeks and weeks just because
you are dealing with human people and and human beings and most of the time this this like
workforce labeling partners they are doing the best they can and they they want to do a good job
and and that's just that sometimes it's very hard that's the nature of the business and I think
we need to keep that in mind when we are using one of those partners is that I think you're you've kind
of your your answer kind of suggests this but it is a big part of your experience in this label
ops role helping you know engineers understand that there are humans on the other side of this
API that you know the labeling companies kind of position themselves as like this API that
abstracts the messiness of labeling do you think that that also abstracts away from the humans on
the other side of those APIs and that's a big part of the role is helping them understand that
I think it's trying to juggle like what's going on on like every side of the you know the workflow
so yes it's like talking to machine learning engineers and and educating them on how it works and
how quality is going to be rich and how quality is going to be maintained that it takes time
but in the end they're going to gain from it so Jager that like you know they understand a
bit more what's going on in this entire loop that's not only tech but are so human beings involved
in the process and on the other end like trying also to juggle you know what's happening with
the data labeling companies and and see how I can help them and that happens a lot that we have
discussions with them about how they can have a better workflow how they can have a better
QA process in place so that there is not a need for too many feedback loops so it's kind of like
understanding what's happening you know in the entire workflow and find a way to make it work
at the best the best the best you can if we haven't talked much about tooling uh obviously
important part of the overall workflow how have you seen that evolve over the years
when when I started a lot of the tasks were bound in boxes so really like you didn't have to have
very high tech tooling to make it work into the machine learning world but over the years I've
seen so many data labeling platform trying to become more sophisticated trying to respond to
special needs from certain machine learning companies they have like very great features they
can do segmentation they can do relationship between worlds like in a text it has become very
sophisticated and all these companies are tacking in a different way I would say which is very
interesting to see and they they are like pretty some of them are even like really good for certain
verticals but not for others or certain format but not for others so definitely data labeling
as become more and more complex and if you add on the top the new comments as I mentioned earlier
about synthetic data or augmented data then it becomes even more complex so really there is a lot
to play with and it's just a connection of being able to select the right one and to connect
them with each other when you need it talk a little bit about measuring quality for labeling
efforts so I mean you know quality everyone wants to reach 100% I I talk to people who are saying
well I don't want anything else than 99 or 100% that's that's a that's a bit complicated to reach
but the idea is to be able to check the quality and you have to do it regularly once once you
able to check the quality of the work that has been done by the laborers what does that mean is
that you're going to have a sample of your data that you're going to look at and you're going to
look let's say you look at 100 images and you know 95 of them have been labeled correctly
so you have a 99 95% accuracy rate you're happy with that right like how you're going to maintain
that accuracy level weeks after weeks and I think again it goes back to what I was talking about
which is the feedback loop you want to make sure that you keep an eye on the quality again you're
working with people who are labeling even though they are using like performant tooling in the end
if you don't have a good labeling workforce doing the labeling correctly the quality is going
is not going to be good so so really like the idea is to being able to do the quality control
regularly and make sure that if you see that something is dropping you are able to address it
as soon as possible so that the laborers can retrain and can perform well moving forward and
even correct the laborers that were done incorrectly in the past so yeah it goes back to this idea about
this cycle of quality control and is that is that quality control being measured against
you know kind of what's that process is it you know having internal folks spot check or
you know where does that that come from yeah so everyone is going to get it in a different way but
ideally I think as you mentioned earlier the internal you know laborers in a company are going to
be the one with most of the knowledge and ideally you would use those people for QA you would use
those people to check the quality of the work that has been outsourced to make sure that it's
maintained but also to give some feedback and retrain more than anything else because
cost wise it probably makes more sense to do it that way than to have like an army of people in
house that would do the the whole labeling and the whole QA and are there established kind of norms
about you know what percentage of labels you want a spot check or that kind of thing
there are a lot of opinions on that what I've been taught over the years is that if you look at
a hundred every every other day you're going to be able to have a good idea about what's going on
even if it's like 10,000 images that have been labeled that's still going to give you a good idea
about what's going on in your data sets okay so it sounds you know in some ways it's less about
um you know establishing kind of statistical significance of sampling or anything like that
more about just having a feel for how it's going yeah and it has to be done regularly so um
obviously if you do that you know once a month you're not going to get a good idea about it but
if you do it like every other day you're going to have a really good idea about what's going on
definitely mm-hmm and I asked this question earlier I want to re-ask it you know when you see a drop
in quality you know it sounds like one of the tools uh in your toolbox is training and feedback
and and that kind of thing you know how often are you implementing strategies like you know
aggregating labelers uh and taking a quorum that kind of thing versus you know training and
what's the thought process around you know these kinds of strategies yeah absolutely that's a really
good question uh definitely they're like different ways to go about um you know making sure that
quality is going to be high so on one of the projects I worked on in the past the idea is to have
we were like having uh several people on the same image at the same time and uh you know like
if we had like three agreements on the way they were labeling if the three people were
labeling the same way then we were considering it as correct even if you were not eyeballing the data
that was delivered um and we were doing that as long as the quality was not great and was not
maintained and once it was done we were dropping from like three people to two people and then from
two people two people to one person and why we had to do that it's a question of budget right so
you have to be able to balance um you're like search for like quality and and the money you have
to spend on it right if you get three people per image obviously it's going to cost way more
but that's also a good way about doing crowdsourcing right if you go into uh use mechanical
torque or happen and you have three people on the same image even though you feel that the
knowledge is probably you know you don't you're not sure if people know about what they are labeling
you can have like certain degree of um certainty that this is correct if three people out of three
are responding the same way so you're right they're like a lot of different ways to go about
quality again depending on the complexity of the task the budget the turnaround time you're
going to have to juggle all these options and find the best one for your your case how often is it
you as the kind of data label ops on the customer side that is thinking about that and having
to come up with the solution versus um you know the vendor that is you know promising to kind
of manage the quality for you like how hands-on does quality management need to be
I mean I'm pretty hands-on I would say I think I think that's where also it's one of the strengths of
the data labeling oppression people is that they are um they can be like very objective in the way
of doing things um so ideally like I mean I was the one proposing and there is this idea about
also rejecting right like understand how much it's going to cost and how long it's going to take
and they're like those three matrix that are really important that help you measure if your
data labeling project have been successful or not uh it's obviously the quality of the data
the turnaround time how fast they can deliver uh I have customers asking for a five minute turnaround
time how do you get high quality with a five minute turnaround time when you have people who are trained
and that are like on the other side of the world with like the time difference and so on um and
then you have the budget how much money are you able to spend on data labeling and so um you have
to take the three matrix into consideration each time that you are you know starting a data
labeling project that's that's the only way you can understand fully what you want to do about it
and in other areas of of tech and I guess life you have this classic like uh time cost quality
pick any two does that idea recur in uh in labeling well I believe you can you you can reach three of
them but obviously you're right like depending on the solution yes you're going to have to to
privilege which of those two are the most important ones to you and then you you might be able to
reach the third one as well but you need to be more flexible on the third one so for instance
I'm going to give you an example if you use crowdsourcing uh for your labeling it's going to be
super cheap uh you're going to have a fast turnaround time and quality hmm you're not 100% sure
about it uh if you have an internal team doing the labeling super high quality but also super expensive
and uh and the fast turnaround time it depends on the size of your team if you have five people
they can do only what they can do you know during that day so they might not have a big volume
done within 24 hours or week or so um so yes you're right each time there is like there are like
several options and you have to write to think about what matters to you the most I love to get
your take on the various ethical considerations with regards to labeling particularly with
uh many of the the labeling workforces being uh remote and with um you know workforces that are
used so much uh lower kind of income than you know in the in western countries um there have been
some recent articles uh there's a wired article uh not wired it actually MIT tech review article by
Karen how talking about how uh one of the label labeling companies was kind of taking about
taking advantage of economic turmoil and Venezuela um and uh uh his Facebook is was getting sued
by a labeling company in Kenya I think I watch how you kind of thought about and managed the various
ethical considerations before answering your question I think I would like to ask two questions
the first one is is like how does it work for outsourcing in general not only for data labeling but
if you think about recycling we are doing that already in other domain in other verticals
are we comfortable with it do we need to revisit but that's that's the bigger discussion that's
broader discussion that we should probably have as well the second one is you're probably mentioning
so data labeling in general but I think that the article was also mentioning content moderation
because you can have like some disturbing content and that might affect you know psychologically
some people um is it needed do we need content moderation I think as a parent myself I I want to
feel that the internet is a bit safer for my kids uh and I want to have a choice to you know
protect them if I can so in my view it's very important that we keep doing content moderation
uh not not to answer and and one thing I would like to add is that I yes content moderation has
been outsourced but it's not entirely outsourced I know for a fact that content moderation is
currently done uh in the United States and also in Europe um so um the question and I've done
I mean I when I work my first job I was not doing content moderation but I had some pretty
disturbing things when I was doing the job and so you know it's it's unfortunately it's part of
like the low income jobs in the data labeling space to jump in I think you know you're already
raising uh uh several important issues one I think is that um you know there are
multitude of potential issues it's not just one issue that and it's not you know solely
you know outsourced versus insourced there's a lot of complexity to it uh but I also thought the
first point you made around you know the questions in labeling and and label an outsourced labeling
in particular are the same as other kinds of outsourcing and even more broadly
um other types of commerce like the first thing I thought of was like in in coffee we've got fair
trade right you know uh and so you know maybe the future is like fair trade labeling or something
like that. Absolutely absolutely I think that's uh that's a great comment that you're making right
now I think that that's the future and that's what I was going to talk about is that I
think that jays that yes content moderation is needed but how do you do it is what needs to be
improved and definitely um uh throughout like the years I've seen companies taking action in that
in that field so for instance um they're going to be having like a therapist on site so that
people who feel like they need to talk to someone they can straight away go and talk to that person
uh they can also uh do content moderation only on a voluntary basis it's not something you're not
going to lose your job if you don't want to do it you have the choice um and these people also
work less hours than other regular laborers working on regular labeling tasks just because it's a
way to work on my stack okay you're doing a very very difficult job and you don't need to work you
know as many hours as as the other people are doing uh other tasks I think it's the beginning and
every company has their own vision on what it should look like but I've seen a lot of companies
especially feng um making decisions to work only with companies that have made that type of
efforts for their own workers and you're right you're right that I I believe that the future is going
to be that there will be some sort of committee uh that would like you know um
eject the rules in terms of fair data labeling uh how does that work how you protect the workers
and so on and and that's definitely something that I would like to see happening
if anything the unifying thread between you know the various aspects of this conversation
is recognizing the humanity of the folks that are doing the labeling and the implications of that
both from your process as well as the you know now the ethical considerations we're discussing
yeah definitely I one thing I would like to add to that is that um you know all these people like
a lot of them are not you know educated they don't have like degrees and and that's their way
also to go and start having like an education uh around IT how to use a computer how to label
and a lot of the companies I've been working with us for the years have you know promoted
internally all these people from doing the labeling to doing quality control to being teamly
to being program manager so there is like all this like new industry that's going to generate
you know educated people that we'll be able to get like um you know good salaries and and and
grow into their career so I I want I was one of them even if uh if uh I was uh I was doing the same
thing but in the United States but I started you know at at that level and then I went up so I
think that's very important also to to talk about that uh what do you see as the future of data
label operations I think that the ecosystem needs to get unified I think that it's it's very
there are like so many players that it's just like it's going to be hard to moving forward for
company going into machine learning to find their way uh even though they're incredible um
tools on the on the market right now so that's one so unifying the ecosystem is is a very important
one and uh for the role in itself the data labeling operation I think that we're going to see more
and more of data labeling operation people uh hired uh even in smaller companies or new companies
going into the the journey instead of you know having uh only machine learning engineers and and
think okay we're good we can start our journey there will be this idea that um other non-technical
people are really um important to the journey well Audrey thanks so much for joining us and sharing
a bit of your wealth of experience and labeling well thank you I was very honoured to be on your
podcast thank you Sam thank you
you
