WEBVTT

00:00.000 --> 00:11.200
All right, everyone. Welcome to another episode of the Twomel AI podcast. I am your host,

00:11.200 --> 00:18.080
Sam Charington. And today I'm joined by Arash Bebudi. Arash is a machine learning researcher

00:18.080 --> 00:23.840
at Qualcomm Technologies. Before we get going, be sure to take a moment to hit that subscribe

00:23.840 --> 00:28.640
button wherever you're listening to today's show. Arash, welcome to the podcast.

00:28.640 --> 00:33.440
Yeah, hi, Sam. Thanks for having me. It's great to have you on the show and I'm looking forward

00:33.440 --> 00:38.640
to the chat. I'd love to have you start out by sharing a little bit about your background and

00:38.640 --> 00:46.320
what led you into the field of machine learning. Well, my background is information theory,

00:46.320 --> 00:51.840
a mathematical signal processing on the one hand, that my PhD was about basically deriving

00:51.840 --> 01:00.160
Shannon theoretic bounds on information capacity of certain cooperative networks on their

01:00.160 --> 01:07.760
channel on certain T. So kind of mathematical PhD work. And after that, kind of transition into

01:08.480 --> 01:15.520
in the field of compress sensing, it's quite fascinating to me because the core problem

01:15.520 --> 01:22.480
in compress sensing was you have set an observations of an unknown signal, but you don't have enough

01:22.480 --> 01:27.680
observation to recover it fully. So you have to rely on some prior assumption about the signal.

01:27.680 --> 01:33.520
And the question is, if you know that prior, what is a good, you know, how you can recover your

01:33.520 --> 01:40.080
signal and what are theoretical bounds on, basically, sample complexity, how many observations you

01:40.080 --> 01:45.120
need and all that. So kind of dealing with that notion of a structure, signal structure,

01:45.120 --> 01:51.280
and how this can be used in different kind of inverse problems. That was very, very interesting

01:51.280 --> 01:57.600
for me and kind of I was working on that afterward. But then of course deep learning comes with a lot

01:57.600 --> 02:07.040
of a lot of interesting promises and challenges. So immediately I said, you know, maybe you know,

02:07.040 --> 02:13.120
with deep learning can even do better with, you know, to do a better job of working with structures

02:13.120 --> 02:21.520
and learning better recovery algorithms. And even from theoretical side, I was very

02:21.520 --> 02:26.400
interested in understanding like mysteries of deep learning. So I think kind of that dragged

02:26.400 --> 02:35.680
me into deep learning. But funny enough, you know, on the side note, when I was undergrad,

02:35.680 --> 02:42.240
I could one of my main interests was philosophy. And kind of I did that in parallel doing whole,

02:42.240 --> 02:49.040
in my whole life. And when I was doing my PhD in parallel, I also did master degree on that. And

02:49.040 --> 02:54.160
kind of a lot of topics that kind of studied there, philosophy of mind, language and all that.

02:54.800 --> 02:59.920
Question of meaning, intelligence, consciousness, all that's kind of re-emerged in this field

03:00.800 --> 03:06.400
of machine learning again. And kind of it's it's it became a very exciting area to work on.

03:06.400 --> 03:12.240
And kind of I'm happy that all my interests are coming together within within this field.

03:12.240 --> 03:21.120
Oh, that's fantastic, fantastic. What, you're at obviously Qualcomm AI research now. What are your

03:21.120 --> 03:28.240
primary research interests on that team? Yeah. So, you know, information, I talked about

03:28.240 --> 03:32.080
information theory and compress and seeing a mathematical signal processing. And

03:32.080 --> 03:37.520
wireless communication, of course, you know, Shannon's theory is mathematical theory of communication.

03:37.520 --> 03:42.560
So kind of communication and wireless communication in particular has always been very also

03:42.560 --> 03:47.600
interesting to me. And if I worked on that also as kind of a motivation for a lot of problems,

03:47.600 --> 03:56.560
I was I was formulating. And so what we're trying to do here, we're basically looking at wireless

03:56.560 --> 04:03.120
communication from machine learning perspective, put it that way. We're trying to understand how

04:04.000 --> 04:12.000
we can design new machine learning architectures useful for different wireless tasks and how we

04:12.000 --> 04:17.760
can improve different parts of wireless system design using machine learning. And this kind of

04:18.800 --> 04:23.040
relates to the current technologies that we have and the next technologies that are going to

04:23.040 --> 04:28.880
come. And so the research that we have, I like to call it wireless AI research. So it's a kind of

04:30.480 --> 04:36.720
doing machine learning research for wireless communication. And one of the papers that we're

04:36.720 --> 04:42.560
going to talk about that you have had accepted at ICML this year is in the same field of

04:42.560 --> 04:48.080
compress sensing that has come up a few times. It sounds like it's an area that you're continuing

04:48.080 --> 04:54.080
to explore pretty deeply. Yeah, absolutely. I think, you know, the inverse problems there

04:54.080 --> 05:01.600
everywhere. We kind of from medical imaging to all sorts of kind of you can talk about drug discovery

05:01.600 --> 05:08.480
problems by, you know, you know, protein unfolding is another example. So we can find a lot of

05:08.480 --> 05:15.680
a lot of areas where inverse problems are important. And wireless communication is actually

05:15.680 --> 05:21.920
full of those problems like from child estimation to other topics. So you can find a lot of inverse

05:21.920 --> 05:29.920
problems. And yeah, I mean, the compress sensing is definitely like a core core interest for me there.

05:29.920 --> 05:35.840
Now, I've talked to some of your colleagues about some of the work that other teams are doing

05:35.840 --> 05:42.480
in compression. How does compression and the setting there compare with compress sensing?

05:42.480 --> 05:49.600
Is it the same? Is it different? It is different. So in compress sensing, the idea is that you want

05:49.600 --> 05:57.760
to efficiently sense the environment in order to infer something. In compression, the idea is you

05:57.760 --> 06:04.720
have a source, you have information source, and you want to efficiently compress it. So there is no

06:04.720 --> 06:11.600
sensing medium there. The only thing that's important is how, how will you compress and reconstruct

06:11.600 --> 06:17.600
your information source? In compress sensing, the idea is how you sense the medium given the

06:17.600 --> 06:22.960
constraint that is given to you. You do not choose that. And then how well you can reconstruct

06:22.960 --> 06:28.560
based on based on the observation you have. So they they're conceptually they might be connected

06:28.560 --> 06:35.760
but if they're kind of a parallel field, I would say. And so with that said, what is the motivation

06:35.760 --> 06:43.920
for your ICML paper, which is called equivalent priors for compress sensing with unknown orientation?

06:43.920 --> 06:50.240
Great. So let me let me start with the first talking about generative priors and its relation

06:50.240 --> 06:57.840
to compress sensing. This is probably a good start. So when I talked about the structure,

06:57.840 --> 07:03.840
so assuming some some prior about the structure of the signal, for example, the typical example

07:03.840 --> 07:09.920
of a structure in classical compress sensing was sparsity. You assume that your signal is sparse

07:09.920 --> 07:15.600
in a given basis, for example. And then you use that to some sort of a one minimization to basically

07:15.600 --> 07:24.400
find your signal. Now with deep learning, with generative models specifically, we notice that

07:24.400 --> 07:34.880
actually generative models provide a way of parametrizing space of signals of interest using

07:34.880 --> 07:40.800
basically the latent space of the generative model. So and specifically, if you cannot

07:40.800 --> 07:46.560
tractably represent a signal, in this case, let's say images, if you cannot tractably represent

07:46.560 --> 07:53.920
them, represent the prior generative models provide a way of parametrizing. And there was a work by

07:53.920 --> 08:01.280
Bora and co-authors around 2015 that showed actually you can use generative priors,

08:01.280 --> 08:10.000
generative models, and do gradient descent on the latent space of that generative model

08:10.000 --> 08:17.040
in order to estimate the signal. So generative models in that sense provide a structural

08:17.040 --> 08:24.720
structure prior on your signal. And the paper showed quite interesting results and it was quite

08:25.440 --> 08:32.000
quite an encouraging work with theoretical guarantees on top, which is always good to have.

08:32.640 --> 08:38.480
So that was the original work. Now a lot of work came afterward basically using flows,

08:38.480 --> 08:42.880
using different techniques for doing a better job in this thing. We're extending it into

08:42.880 --> 08:51.520
different type of non-linear problems and values extensions. Some challenges that these models have

08:51.520 --> 08:58.240
are, let's say, convergence and latency, meaning that sometimes when you do gradient descent

08:58.240 --> 09:03.040
on the latent space, you might need to restart the whole process because it's not converging.

09:04.080 --> 09:11.920
So the convergence and the way you optimize your problem can become an issue,

09:11.920 --> 09:17.040
especially if you're interested in low latency, let's say solutions, that can become a bottleneck.

09:17.040 --> 09:25.040
So there were actually some works about how you can effectively invert a generative model.

09:25.040 --> 09:31.200
And actually Alex Demarkis from the TU Austin had some follow-up works. Actually it was a quarter,

09:31.200 --> 09:36.480
a quarter of the original papers, some follow-up works on kind of a layer by layer inversion and

09:36.480 --> 09:43.600
some other tricks following that as well. So that's one part, like how why generative models are

09:43.600 --> 09:52.400
relevant for compressency. The other part was, okay, in many applications, we might not have,

09:52.960 --> 10:00.320
when we want to measure the signal, the signal might go under some transformation. Let's say

10:00.320 --> 10:05.840
some rotation before the measurement. This can happen. A typical example is a cryo-electron

10:05.840 --> 10:12.560
microscopy. So cryo-electron microscopy is a way of basically taking a picture of a biomolecule.

10:12.560 --> 10:17.520
You can look at it like that. And the problem is that when you take this picture, of course,

10:17.520 --> 10:22.080
the picture is very noisy and all that, but the molecule that you have is in an unknown orientation.

10:23.680 --> 10:28.160
And if you have multiple pictures, these orientations are not the same, so are not aligned.

10:28.160 --> 10:35.600
Also, you can find in different, so you can, in general, you can think of a signal that you

10:35.600 --> 10:40.240
actually can turn a generative model on, but the orientation of that signal might change

10:41.520 --> 10:46.960
before the measurement. So you might say, okay, let's, you know, what I can do probably is I try

10:46.960 --> 10:52.560
to first figure out the orientation and then reconstruct the signal and or doing it iteratively

10:52.560 --> 10:59.040
basically finding solving the problem like that. What we thought was, oh, why do we need to

10:59.840 --> 11:06.640
kind of separate orientation discovery and the signal recovery? So we can do this jointly.

11:07.360 --> 11:15.280
And the way we can do it is we can train a generative model that has already disinformation,

11:15.280 --> 11:20.560
has already disinformation embedded in it. Of course, you can say, oh, you know, what we can do,

11:20.560 --> 11:27.760
we can get a generative model, use data augmentation and basically, now we have it already there somehow

11:27.760 --> 11:32.640
because we have seen all these different orientations. But when we know that we want to have this

11:34.240 --> 11:41.520
this steerable basically transformable latent space, equivalence models are excellent candidates.

11:42.320 --> 11:49.280
So what are equivalent equivalence models? So the equivalence models are models that when you

11:49.280 --> 11:55.760
transform the inputs of the network according to certain group transformations, let's say rotation.

11:55.760 --> 12:01.600
When you rotate the input, then the features that are outputs of that network also rotate.

12:02.880 --> 12:08.560
So therefore, a rotation, a change of orientation at the output correspond to the change of orientation

12:08.560 --> 12:15.120
as the input. This also imposes some sort of a structure on the latent space of your generative model.

12:15.120 --> 12:22.080
So the main idea was, let's try to use equivalence priors so that we can just solve this whole problem

12:22.080 --> 12:29.440
altogether. Awesome. Now I've talked with colleagues in the past of yours about Equavareans as well.

12:30.400 --> 12:36.400
I think in the context of those previous conversations, we're primarily talking about supervised

12:36.400 --> 12:44.320
types of problems, has Equavareans been applied to generative types of problems previously?

12:44.320 --> 12:51.760
Yeah, this is a good question. Actually, the answer is yes. The answer is in context of

12:52.480 --> 12:58.400
generative model, there have been some works that try to incorporate this equivalence into the

12:58.400 --> 13:04.000
model architecture. The way we kind of did that in our work, they're kind of equivalent VAE

13:04.000 --> 13:09.520
that we built, as far as Vino, as far as I know, this is the first time that we have such an

13:09.520 --> 13:16.960
equivalent VAE constructed. But at the end of the day, I think another example that comes to my

13:16.960 --> 13:20.800
mind is the kind of equivalence normalizing flows, that's something that has been

13:22.320 --> 13:29.280
published I think last year. So one challenge of those works are very nice and elegant,

13:29.280 --> 13:37.280
but kind of scaling them up to kind of a problem that wants to have low complexity, low latency,

13:37.280 --> 13:42.480
which is actually the main motivation at the end of the day. Scaling them or bringing them to that

13:45.680 --> 13:51.200
to that type of problem, it becomes a bit challenging. For example, for normalizing,

13:51.200 --> 13:56.800
Equavare normalizing flows, you know, you had to work with this type of a kind of continuous

13:57.760 --> 14:03.120
time and neural networks, like ODE type models. And training those models are difficult,

14:03.120 --> 14:09.360
scaling them up are difficult. And what we're targeting here is just having a nice small

14:09.360 --> 14:15.360
equivalent models that can already, can perform already quite well compared to the counter-counter parts.

14:16.000 --> 14:23.200
How did you, assuming you started with a traditional VAE type of an architecture, how did you

14:24.240 --> 14:30.240
evolve it to be equivariant based or to understand equivariants?

14:30.240 --> 14:35.840
Equivariants, you know, the variational auto encoders, you have an encoder network and decoder

14:35.840 --> 14:45.360
network. The decoder network is the network that is going to be used for our recovery task,

14:45.360 --> 14:51.520
because that is the generator part, and we're going to use the latent space. So we want the

14:52.320 --> 14:59.200
output orientation change. So basically transformation of our signal translates into the transformation

14:59.200 --> 15:03.920
of our input. So what we want is that the generator of the network should be equivariant,

15:03.920 --> 15:10.480
so that the decoder network will pick an equivariant network off the shelf from the existing models.

15:10.480 --> 15:16.880
Now, what happens to the encoder network? The encoder network basically in VAE gives

15:17.760 --> 15:23.840
parameters of your approximate posterior. So if you're assuming that we have a Gaussian type

15:23.840 --> 15:30.320
of posterior, you have a mean value, and you have a Koreans matrix basically. Now, when you rotate

15:30.320 --> 15:37.840
the image, you, since this, you're going to, your encoder network is going to give you a

15:37.840 --> 15:43.280
distribution basically approximate posterior, you will get a new distribution when the input

15:43.280 --> 15:50.880
is rotated. What should be this distribution? So the random, consider random variable corresponding

15:50.880 --> 15:57.680
to the untransformed image, right? Then you transform that image, you want to get a new random

15:57.680 --> 16:04.960
variable, a random variable, that is the transformed version of that. So basically, if the

16:04.960 --> 16:09.120
distribution that you want, you want it to correspond to the transformed random variable.

16:10.160 --> 16:16.320
So, okay, so that is, that would, let me know. Now, what happens to the parameters of this

16:16.320 --> 16:23.520
distribution? Let's start with mean value. The mean value, as we wanted, is when we transform

16:24.240 --> 16:29.600
a random vector with the rotation matrix, for example, then the mean value of that random

16:29.600 --> 16:35.280
vector is also transformed according to a, to rotation matrix. So this basically means that

16:36.560 --> 16:43.440
the, the part of the network giving us mean value should be equivalent. So that is also

16:43.440 --> 16:50.000
a figure out. The challenge is for the covariance matrix. Then you transform your random vector

16:50.000 --> 16:55.440
according to a matrix, let's say, A, you have A times your random variable, random vector.

16:55.440 --> 17:01.120
The covariance matrix changes according to A times the previous covariance matrix times

17:01.120 --> 17:08.480
a hermitian. So we do not have like the classical equivalence anymore because if you transform

17:08.480 --> 17:13.920
your input according to A, the covariance part should be transformed according to A, A hermitian.

17:13.920 --> 17:20.880
First conclusion out of that is that we cannot use the typical diagonal covariance matrix assumption

17:20.880 --> 17:26.480
used in VAs. So we need to consider like a full covariance matrix. Another thing is how we're going

17:26.480 --> 17:32.000
to build this kind of, this transformation, this, this type of network. So the way we did that,

17:32.000 --> 17:37.760
we said, okay, with covariance matrix is a positive definite matrix. So we can already

17:37.760 --> 17:44.000
transform, you write it down as a V, V transpose. We considered other ways of parameterizing that

17:44.000 --> 17:49.120
as well. So we write the, the covariance matrix, the positive definite covariance matrix as VV

17:49.120 --> 17:57.920
transpose. And then if that V matrix is equivalent. So it's just transformed. Then the whole

17:57.920 --> 18:02.960
covariance matrix satisfies the condition that we want. So we built the, the, the, the, the

18:02.960 --> 18:08.400
decoder network giving us the encoder network, giving us the covariance matrix in a way that,

18:09.360 --> 18:15.360
that part of the network is, is equivalent and it's just parametrizes basically the V presentation.

18:15.360 --> 18:19.680
There are other ways of doing that, but we tried that also in the paper more or less they give

18:19.680 --> 18:28.880
same, same performance, all of them. And does the way that you approach that, does that apply

18:28.880 --> 18:34.800
constraints to your inputs? There's no, there's no constraints necessary on the input.

18:35.520 --> 18:41.120
So the only thing you need to know is how the transformation that you're interested in,

18:41.120 --> 18:47.840
that's a rotation, is acts on the input space. That's what you need to do in order to build your

18:47.840 --> 18:53.120
equivalent networks. You need to know how the input is transformed so that the equivalence is

18:53.120 --> 18:56.800
built into the architecture. That's the only thing you need to know. Otherwise, you don't need to

18:56.800 --> 19:02.720
put any constraint on it. And so how did you go about assessing the performance of the

19:02.720 --> 19:08.240
architecture? So, yeah. So what we did is said, okay, let's, of course, we are interested in

19:08.240 --> 19:14.960
recovering signals with unknown orientation, right? So we said, let's try to pick like a powerful

19:14.960 --> 19:20.320
flow networks with, you know, the most powerful flow network that, the normalizing flow network

19:20.320 --> 19:28.240
that we can have, like a typical real MVP setup. And then let's try to iteratively find

19:29.040 --> 19:35.760
the orientation and kind of a latent space latent code by just the gradient descent on the

19:35.760 --> 19:39.840
latent. This is not, this is the so-called vanilla or like a baseline that we had.

19:41.680 --> 19:49.200
And then we compared that with our VAE, equivalent VAE method. We had other baselines as well,

19:49.200 --> 19:52.640
but I think the normalizing flow is the most interesting part because it usually gives the best

19:52.640 --> 19:58.480
result in terms of reconstruction. So we noticed that our equivalent VAE is much simpler,

19:58.480 --> 20:08.240
it's much smaller, and it already gives same or better result than flows in many cases, which

20:08.240 --> 20:12.080
is more complicated and then you have to do all those iterative assumptions and all that.

20:12.800 --> 20:18.720
And then something interesting happened as well that even if the orientation is known,

20:18.720 --> 20:24.400
so or you don't have any random orientation, actually equivalent VAE turned out to perform

20:24.400 --> 20:30.080
quite well in this task. And our conjecture is that actually the equivalence

20:31.280 --> 20:38.720
adds certain additional structure on the latent space that makes the life of optimization

20:38.720 --> 20:44.720
algorithm much easier. And therefore, you know, we can do, we can do a lot, a lot with it.

20:44.720 --> 20:50.080
So that was an interesting thing. So the gain was not only in terms of reconstruction quality,

20:50.640 --> 20:57.920
not only in terms of finding unknown orientation, but also in terms of latency, convergence,

20:57.920 --> 21:02.720
which are kind of important metrics for these generative priors. I really think like

21:03.280 --> 21:09.440
lattice in convergence or do metrics that are quite important for applicability of these

21:09.440 --> 21:16.960
methods in practice, especially if we could target something that requires, you know,

21:16.960 --> 21:22.000
implementation on edge device, low latency constraint, that's quite crucial.

21:22.560 --> 21:30.640
You talked about wireless and cryo electron microscopy as the use cases in the microscopy case,

21:30.640 --> 21:37.600
there's kind of this obvious image that you're trying to work with. And wireless are you

21:37.600 --> 21:41.920
applying, are we talking about applying this to images that happen to be transmitted wirelessly,

21:41.920 --> 21:48.640
or is there some application of the same idea to broader wireless problems?

21:48.640 --> 21:58.960
That's an excellent question. Actually, so let's start with the new kind of frequency band that is

21:58.960 --> 22:09.360
being introduced in 5G. It's kind of a millimeter wave frequency bands. So when we go higher in

22:09.360 --> 22:17.680
frequency for wireless communication, what happens is that the beams become very, very, basically,

22:17.680 --> 22:24.080
you need to use directional beams to get the performance that you want. These are some artifacts

22:24.080 --> 22:29.520
of like having a smaller antenna aperture and all that. So to to summarize the whole thing is that

22:29.520 --> 22:34.880
you you just cannot rely on kind of a rich scattering environment that you have in order to get

22:34.880 --> 22:40.560
signals that you want. You need to really be able to steer your beams, your antenna beams,

22:40.560 --> 22:46.480
in a direction that kind of gets the most energy and also direct your beams and design your beams

22:46.480 --> 22:53.120
accordingly. Now, if you want to do that on your device and kind of I leave it at a high level

22:53.120 --> 23:01.040
like that, if you want to do that on device, you're usually kind of keeping your device and you're

23:01.040 --> 23:06.880
talking and then you put it on the table or walk around, your device is permanently rotating.

23:08.320 --> 23:12.880
But the environment is the same environment. It's just the way you view it is changing. So it's

23:12.880 --> 23:19.040
kind of undergoes some transformation. So that is the main main idea in wireless communication.

23:19.040 --> 23:27.040
So that's why kind of having incorporating those geometric priors in our design, wireless design

23:27.040 --> 23:30.640
becomes really, really crucial, especially when we talk about these high frequencies.

23:31.440 --> 23:37.040
Speaking of wireless, what are some other research areas that you're pursuing in that domain?

23:37.040 --> 23:48.480
I think, you know, look at wireless communication. At the end of the day, we are working with

23:48.480 --> 23:56.080
with lots of physics there. You have Maxwell equations and at the end of the day, there's an

23:56.080 --> 24:02.960
electromagnetic wave moving from one point to another undergoing a lot of effects in an environment,

24:02.960 --> 24:09.680
reflected from different objects scattered, diffracted and all that. That's the underlying

24:09.680 --> 24:17.600
nature of wireless communication is kind of physics, right? So modeling these complicated

24:19.440 --> 24:26.240
complicated propagation effects precisely, real precisely, it's a challenging task.

24:27.200 --> 24:35.200
And the way people traditionally build models, at least so far, so you have two class of models

24:35.200 --> 24:41.120
for wireless. One is kind of a ray tracing type models, which is based on just, you know,

24:41.920 --> 24:47.440
kind of deterministically following paths of the ray. And you have what I can call statistical

24:47.440 --> 24:53.840
channel models, that kind of build an average case statistical model that you put a distribution

24:53.840 --> 24:59.280
on the channel gains that you have on the delays that you get and you work with those hard-coded

24:59.280 --> 25:06.880
assumptions. Now, with machine learning, we know that using data, we can build much better

25:07.840 --> 25:13.680
models if we can just use those models to build either better statistical channel models or

25:13.680 --> 25:18.720
better kind of a more especially consistent channel models. So one of the research areas that are,

25:18.720 --> 25:26.560
you know, or is quite important for us is exactly this channel modeling, which basically is about

25:26.560 --> 25:32.000
kind of learning physics. This is kind of a topic. It's not limited to wireless. You know that

25:32.000 --> 25:38.000
there are other folks working on neural networks, a physics-inspired neural network or

25:38.000 --> 25:42.240
neural networks that can learn physics. So that's a topic that is quite interesting. And one

25:42.240 --> 25:49.680
thing that like we did was, you know, let's try to gather very simple field data, not requiring

25:49.680 --> 25:57.360
really kind of hard really like special devices. And let's build a generative model on that that

25:57.360 --> 26:02.400
generates the underlying channel. So implicitly learns the underlying channel. So that's kind of one

26:02.400 --> 26:07.360
of the one of the works we had and we continue to work on on those topics as well.

26:08.720 --> 26:17.840
The other thing that is quite interesting is wireless perception. So again, our visual perception

26:17.840 --> 26:22.400
is it's still based on electromagnetic waves. It's just a different frequency. Of course,

26:22.400 --> 26:28.240
it's much higher frequency than what we're working right now in wireless. But on the other hand,

26:28.240 --> 26:35.760
in wireless we might have access to much more details, much more refined concepts than what we

26:35.760 --> 26:44.240
probably have visually. So the question of wireless perception and sensing is quite important as well.

26:44.240 --> 26:49.840
Like how we can build right now, let's say, machine learning models that can perceive the

26:49.840 --> 26:57.040
environment through the learns of wireless signal. And also that was one of the topics that we're

26:57.040 --> 27:03.520
working on basically trying to solve a simple slam problem by incorporating physics of

27:03.520 --> 27:10.480
propagation into the machine learning model and try to learn that from a wireless signal.

27:10.480 --> 27:16.160
Slam being the same kind of slam that comes up in robotics, simultaneous location and mapping,

27:16.160 --> 27:21.200
I think. Precises, yeah, some sense of localization mapping. So that's exactly the same thing.

27:21.200 --> 27:26.560
But this time you want to do it with wireless signal. Awesome, awesome. So kind of returning to

27:26.560 --> 27:34.480
ICML, what are some other papers that you and your team or other teams that Qualcomm are presenting

27:34.480 --> 27:40.800
there? Yeah, I mean, there were a couple of very interesting works from my colleagues at Qualcomm.

27:41.520 --> 27:49.120
And the first paper that I want to mention is a paper that has an oral presentation paper this

27:49.120 --> 27:59.280
year at Qualcomm. And it is about basically oscillation of, you know, a quantization of a training.

27:59.280 --> 28:05.440
So basically, and dealing with that, those problems. So to give a little bit of background on that,

28:05.440 --> 28:11.680
you know, again, you you probably talked with some other colleagues before that they work on

28:11.680 --> 28:15.360
quantization and compression of neural networks. So if you know that,

28:17.360 --> 28:23.200
yeah. So if you know that kind of with quantization, let's say with post training,

28:23.200 --> 28:28.400
a quantization techniques, you train a network, you go and quantize it, you can actually get

28:28.400 --> 28:33.920
very good performance decent performance with 8-bit quantization out of these networks, right?

28:34.640 --> 28:38.960
So this is quite quite quite interesting. However, if you want to push it down, push this

28:38.960 --> 28:46.000
quantization down to four bits, then just post quantization post training quantization techniques

28:46.000 --> 28:51.680
are not sufficient for pushing the performance. So we need to basically do what people call

28:51.680 --> 28:57.360
quantization of their training. This basically means that the quantization noise that kind of

28:57.360 --> 29:01.920
is added because of quantization operation, you include that in the training process. So what you

29:01.920 --> 29:08.000
are doing is that you quantize and then you backprop through the quantization operation.

29:08.480 --> 29:15.040
Now, back and learn to minimize those errors as it's training. Exactly. Exactly. Now,

29:15.040 --> 29:20.800
then you backprop through the through the quantization operation, basically the technique that's

29:20.800 --> 29:27.760
used is straight to estimator, one of the techniques. So the straight to estimator, what it does is that

29:27.760 --> 29:35.440
you basically have your shadow weights, basically weights that they live in real valued numbers.

29:36.240 --> 29:41.680
And they get quantized by quantization operation and gives you your quantized value.

29:42.240 --> 29:48.240
And now, when you backprop at inference, you work with only quantization quantized value.

29:48.240 --> 29:52.640
So that's the ultimate network. However, when you're training it and you backprop through that,

29:53.360 --> 29:59.680
you straight through estimator is roughly speaking means, okay, forget about the quantization

29:59.680 --> 30:05.920
operation that you have here, just directly backprop through the shadow weight. That was the real number.

30:05.920 --> 30:13.040
So that is the rough idea. Okay, that already helps kind of overcoming some of the some of the

30:13.040 --> 30:19.040
difficulties. However, it's not it is not sufficient. And one interesting phenomena that can happen

30:19.040 --> 30:26.080
is that then shadow weights are close to quantization threshold. So basically threshold between two

30:26.080 --> 30:34.720
different quantization beans, they start to actually oscillate around that. And if you look at the

30:34.720 --> 30:42.640
quantized value now, they oscillate between different quantization. And this oscillator behavior has

30:42.640 --> 30:47.600
been reported before and it's kind of a peculiar phenomena. The interesting thing about this

30:47.600 --> 30:52.080
phenomena is that, first of all, the learning grade, but by choosing different learning grade,

30:52.080 --> 30:57.360
you cannot control this oscillation. The oscillation, the amplitude might change, but the frequency

30:57.360 --> 31:05.520
of oscillation remains. And what my colleagues noticed that is actually this frequency is dependent

31:05.520 --> 31:14.880
on the gap between the best, the ground truce value of the optimal value and the quantized value.

31:14.880 --> 31:18.880
If the gap is larger, that oscillation is going to be bigger. If the gap is smaller,

31:18.880 --> 31:23.360
the solution can be smaller. So something something like that, the more details are in the paper.

31:24.800 --> 31:33.120
But they observed such such a behavior. And then what are some drawbacks of this oscillatory behavior?

31:33.120 --> 31:41.280
The first drawback is the discrepancy in bachnormous statistics, meaning that the bachnormous statistics

31:41.280 --> 31:49.040
that you compute during training is going to be different from what you will get at test because

31:49.040 --> 31:57.600
of this oscillation. So what you need to do, you need to recompute the bachnormous statistics,

31:57.600 --> 32:05.280
which manages to solve this discrepancy. However, the problem is not only that. Actually, it has

32:06.800 --> 32:11.600
negative impact on the optimization process as well. This basically means that

32:14.480 --> 32:18.800
if you look at the optimization process, this oscillation actually prevents network to

32:18.800 --> 32:24.880
converge to a better local minimum and gets better performance. So it is important to kind of

32:24.880 --> 32:33.200
overcome this oscillatory behavior. And on that last point, the idea is that beyond just

32:34.960 --> 32:38.640
resulting in a larger error, it will prevent converges altogether.

32:40.000 --> 32:45.120
Or converges to a good good point. So you might converge to something that is not a good place.

32:45.120 --> 32:54.080
So that is the main issue. So the other thing is that, okay, what are some solutions? How

32:54.080 --> 33:00.800
you can actually deal with that issue? So my colleagues actually proposed two methods. The first one

33:00.800 --> 33:07.680
is a dampening regularizer. The idea is, when you're close to quantization threshold, you're going

33:07.680 --> 33:13.280
to see this oscillatory behavior. So what if you put a regularizer on training that pushes the

33:13.280 --> 33:19.440
weights to be close to the center of beings instead of quantization thresholds? And that already

33:19.440 --> 33:30.560
is quite helpful. The other approach that they have is about freezing the weights. What does that

33:30.560 --> 33:38.160
mean? This means if you see that the weights are oscillating, then you freeze them. You just

33:38.160 --> 33:44.160
don't update them because you know that this is going to have a negative impact. But there are

33:44.160 --> 33:51.680
subtleties in the way you do that. First of all, how do you decide if you freeze a weight or not?

33:51.680 --> 33:58.800
You have to monitor the oscillatory behavior and oscillation frequency. And you basically have to

33:58.800 --> 34:03.360
compute that oscillation frequency, which they do by using some kind of exponential moving average.

34:04.160 --> 34:12.000
And then put a threshold on that. So this is, are you tracking this and managing this on a weight

34:12.000 --> 34:18.880
by weight basis? Or is it, are you looking at some notion of oscillation that's kind of vector

34:18.880 --> 34:27.440
based? So as far as I know, this is weight by weight basis. Again, I encourage everyone reading

34:27.440 --> 34:35.680
the papers in all details to make sure I'm not destroying my colleagues' work. But I think it's

34:35.680 --> 34:42.960
a weight by weight because you compute it in a weight by weight basis. So at the end of the day,

34:42.960 --> 34:50.560
what happens there is that, yeah, you monitor that after a certain threshold, you just freeze it,

34:50.560 --> 34:54.640
and then you freeze it to a value that kind of occurred most. So there's actually like a mechanism

34:54.640 --> 34:59.680
for selecting that. As a matter of fact, it manages with this technique, you can actually bring the

34:59.680 --> 35:08.160
quantization with down to, let's say, four bits, three bits, for, let's say, mobile net efficient

35:08.160 --> 35:14.400
net type architectures on image net data, which is quite remarkable result. Wow, wow.

35:15.200 --> 35:20.880
And is oscillation, you know, to your knowledge, is oscillation been the primary impediment to

35:20.880 --> 35:28.400
reducing the level that we're able to quantize to? Or are there other key challenges there?

35:28.400 --> 35:37.920
Yeah, I mean, again, I have to give a caveat that my colleagues are definitely more knowledgeable

35:37.920 --> 35:46.560
in that area. But that definitely was one of the main factors, for sure, because after fixing

35:46.560 --> 35:55.600
that oscillation problem, you could do a great job that you couldn't do before. So this is

35:55.600 --> 36:00.320
definitely one important factor. And there are another couple of papers that your colleagues

36:00.320 --> 36:07.360
are presenting. Exactly. We can rapidly cover some of those as well. So on, on personalization,

36:07.360 --> 36:14.880
there's this paper of my colleagues on variational, on the fly personalization paper. So

36:15.920 --> 36:22.160
when you deploy a model on edge device, sometimes you would need kind of personalization of that

36:22.160 --> 36:29.680
model specifically to to the user that is using it. Let's say, for example, let's talk about

36:29.680 --> 36:34.000
the speech verification, right? You want to be able to personalize the machine learning model

36:34.000 --> 36:40.480
that you have for a specific user that is using that service, right? Now, there are some challenges

36:40.480 --> 36:47.680
for personalization. You really don't want to do on device training because of the kind of the

36:47.680 --> 36:52.880
you don't know what's happening. You have to monitor everything and it's kind of challenging. You

36:52.880 --> 36:58.960
want to do it in an unsupervised way, right? You want to do it few shots. So you really don't want to

36:58.960 --> 37:06.000
get that much label from the user and you want to do it with few samples. And yeah, I mean,

37:06.000 --> 37:10.000
you don't want to basically send data back to a source. So you really want to be able to do it

37:10.000 --> 37:17.120
on device and kind of without requiring great training and all that. So then this personalization

37:17.120 --> 37:25.680
problem was formulated using a variational principle. The idea is that you actually train so-called

37:28.160 --> 37:36.960
variational hyper-personalizer, so we can call it that way. So there's this hyper-personalization

37:36.960 --> 37:45.520
basically network that gets those few samples and basically based on that updates

37:45.520 --> 37:53.280
weights of the model according to those samples. So it's just a way of adjusting the distribution of

37:53.280 --> 37:58.000
those weights to the model that you're getting. So of course, there are more details in the paper,

37:58.000 --> 38:04.080
but the core idea that we have is we have there is that, you know, this is the first time that you

38:04.080 --> 38:11.520
can do on the fly personalization. And yeah, this is I think it's very, very important for

38:11.520 --> 38:21.040
age device machine learning model deployment. Awesome. Awesome. And the last one that we wanted to

38:21.040 --> 38:30.000
cover was on kind of causal identifiability for or from temporal intervened sequences or citrus.

38:30.000 --> 38:36.880
What's that one about? Sounds juicy. Indeed, indeed. That's a that's a that's a 40 page paper. So

38:36.880 --> 38:42.320
also encourage everyone to to go and read all the details. It's a very exciting topic. I mean,

38:42.320 --> 38:49.920
we all know that causality and identifying causal factors are quite quite important. So

38:51.840 --> 39:00.880
so the core idea of the paper is that you so first you want you represent your causal factors

39:00.880 --> 39:07.200
as a multinational vector. So that's the that's the first thing. This is representation of causal factors.

39:07.200 --> 39:12.400
The second thing is that the data that you have and based on which you try to solve this problem,

39:12.400 --> 39:20.560
this causal identification is basically what it works based on access to a temporal sequence.

39:21.280 --> 39:26.800
In which there are some intervention is happening. So basically there are some interaction with

39:26.800 --> 39:34.240
with the scene. So through that you can actually figure out some causal factors out of those

39:34.240 --> 39:42.640
those interactions and out of the temporality that you have. So then there is a V.A. model

39:42.640 --> 39:51.120
is proposed where the the causal fact that the latent space of that are basically in the latent

39:51.120 --> 39:57.600
space of that you can disentangle causal factors represented as this multidimensional factors.

39:57.600 --> 40:03.360
So basically you can in the latent space of that V.A. you can actually disentangle causal factors

40:04.480 --> 40:11.120
using having access to those things. And the other one interesting thing about the paper is that

40:11.120 --> 40:17.440
this is just not like a you can use that V.A.E. but as a matter of fact if I give you an arbitrary

40:17.440 --> 40:25.040
autoencoder, pre-trained autoencoder without knowledge of this, then it is possible to train

40:25.040 --> 40:32.640
a normalizing flow that manages to disentangle causal factors from the latent space of that autoencoder,

40:33.200 --> 40:39.760
which is actually quite an interesting thing. So definitely encourage everyone to go and read

40:39.760 --> 40:47.280
the details of the of the paper. Awesome. And before we finish up we also wanted to touch

40:47.280 --> 40:54.800
on a workshop that you're participating in your colleagues. This one on Bayesian optimization.

40:55.600 --> 41:03.040
Yes exactly. So that's also an interesting interesting paper by my colleagues on micro based

41:03.040 --> 41:09.600
on optimization for macro placement. So what is macro placement? So if you look at kind of a

41:09.600 --> 41:15.840
cheap placement, cheap placement problem, you you basically have a macro unit. Still though those

41:15.840 --> 41:21.360
are memory blocks that you have and then you have standard cells that you can put on on the chip.

41:22.080 --> 41:31.840
So usually you design the you know you place all those objects in chip under some constraints.

41:31.840 --> 41:39.760
These constraints basically are basically you know you can say power performance area. Those are

41:39.760 --> 41:47.200
very important constraints, but to evaluate these for each placement that you choose to evaluate

41:47.200 --> 41:52.320
those that the objecting function that you have is very costly because it has to go like a lengthy

41:52.320 --> 41:57.600
simulation and kind of compute everything. So you really cannot integrate that in the optimization.

41:57.600 --> 42:03.040
So you have to use some sort of a black box optimization for that. And people kind of

42:03.040 --> 42:07.520
traditionally use simulated annealing type methods, but Bayesian optimization actually

42:07.520 --> 42:13.280
provides an elegant way and alternative way of solving this problem. It's a combinatorial

42:13.280 --> 42:19.040
optimization problem. So it's a very tough problem. And Bayesian optimization roughly like you build

42:19.040 --> 42:24.880
a surrogate model and you have access to use a Gaussian process to build a surrogate model and

42:24.880 --> 42:31.360
basically integrate that into use that for your optimization. So the contribution of the paper

42:31.360 --> 42:38.160
is basically solving this macro placement problem using Bayesian optimization. Also very,

42:38.160 --> 42:44.080
very interesting work and yeah of course quite important for what we took. Got it.

42:44.080 --> 42:50.320
And that one's at the real ML workshop. Do you know what the real ML acronym is?

42:51.840 --> 42:59.360
I cannot remember. But it was I think it has to something to do with active learning for real

42:59.360 --> 43:04.080
world systems. Real world active learning and machine learning or something like that.

43:04.080 --> 43:08.560
Exactly, exactly. But it cannot remember. Those are complicated names sometimes.

43:11.600 --> 43:19.200
Awesome. Well, Arash, in terms of your research on kind of machine learning for wireless, what are

43:19.200 --> 43:25.600
some of the future directions that you're excited about? Yeah, I would say I think I touched a

43:25.600 --> 43:32.400
little bit on this modeling part. The fact that the wireless channel modeling at the end of

43:32.400 --> 43:40.080
today is about learning physics. Neural networks that are integrated with this bias coming from

43:40.080 --> 43:45.360
physics or they can learn part of physics. That's a very, very interesting and exciting

43:45.360 --> 43:51.440
the research direction for me and kind of investing a lot of time on that. I hope that in

43:51.440 --> 43:59.040
a couple of months I can be back and talk about that. But that's a very interesting research

43:59.040 --> 44:03.760
topic and I think it can be quite game changer. Well, thanks so much for joining and sharing

44:03.760 --> 44:10.880
a bit about what you're up to and your work, your presentations at ICMA. Yeah, absolutely.

44:10.880 --> 44:22.080
Thanks for having me. It's a lot of fun.

