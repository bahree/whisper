1
00:00:00,000 --> 00:00:07,000
.

2
00:00:07,000 --> 00:00:14,000
.

3
00:00:14,000 --> 00:00:21,000
.

4
00:00:21,000 --> 00:00:28,000
.

5
00:00:28,000 --> 00:00:34,000
.

6
00:00:34,000 --> 00:00:42,000
All right everyone this is Sam Charrington host of the Tumel AI podcast and today I'm coming to you live from the future frequency podcast studio at the AWS

7
00:00:42,000 --> 00:00:52,000
Reinvent Conference here in Las Vegas and I am joined by Amad Mostak. Amad is founder and CEO of Stability AI. Amad welcome to the podcast.

8
00:00:52,000 --> 00:01:11,000
Thanks so much Harmi. Super excited to talk to you. You are of course the founder and CEO of Stability. Stability is the company behind stable diffusion, which is a large multimodal model that has been getting a lot of a lot of fanfare I think.

9
00:01:11,000 --> 00:01:15,000
And I'd love to jump in by having you share a little bit about your background.

10
00:01:15,000 --> 00:01:25,000
Yeah, I think it's been super interesting. I think several diffusions kind of a specific text image model. I think it's that large, but we can talk about that a bit later, which is one of the fun parts.

11
00:01:25,000 --> 00:01:35,000
As for me, I started off mass computer science at uni and enterprise developer and then we came a hedge fund manager and one of the largest video game investors in the world and then artificial intelligence.

12
00:01:35,000 --> 00:01:54,000
I was doing that. There was a lot of fun and then my son was diagnosed with autism and they said there was no cure treatment. So I quit switched to advising hedge funds and built an AI team to do literature review all the autism literature and then buy molecular pathway analysis of neurotransmitters to repurpose drugs to help them out.

13
00:01:54,000 --> 00:02:00,000
And it kind of worked. He went to mainstream school and super happy. It's awesome. It's kind of cool. Good trade. Good trade.

14
00:02:00,000 --> 00:02:12,000
Then I went back to the hedge fund world once more wars as I was boring. So then decided to make the world a better place. So first off took the global ex price for learning. That was 15 million dollar prize from Elon Musk and Tony Robin.

15
00:02:12,000 --> 00:02:22,000
So the first app to teach kids literacy and numeracy about internet. My co-founder and I have been deploying that around the world and now we're teaching kids and refugee camps literacy and numeracy in 13 months and one hour a day.

16
00:02:22,000 --> 00:02:33,000
And about AI the craft out of that in 2021. I designed and led the United Nations one of the United Nations AI initiatives as COVID-19.

17
00:02:33,000 --> 00:02:40,000
Collective and mental intelligence against COVID-19 launched a Stanford that by the WHO in ESCO and the World Bank.

18
00:02:40,000 --> 00:02:53,000
And that was really interesting because we're trying to use the world's made the world's knowledge free on COVID-19. So there's a 500,000 paper data set freely available to everyone and use AI to organize it because it's really confusing.

19
00:02:53,000 --> 00:03:03,000
During that lots and lots of interesting tech kind of came through but I realized these foundation models are super powerful. You can't have them control by any one company.

20
00:03:03,000 --> 00:03:15,000
It's bad business and it's not the correct thing ethically. So I thought let's widen this and create open source foundation models for everyone because I think it can really advance humanity. And again, I think it'll be great to see these things collaborate.

21
00:03:15,000 --> 00:03:22,000
So we can have an open discussion about it and also have the value created from just these brand new experiences.

22
00:03:22,000 --> 00:03:26,000
That's awesome. And when did you get started down that that part of the journey?

23
00:03:26,000 --> 00:03:38,000
About two years ago, stability has been going for about 13 months now. Yeah, when I think about the, you know, a lot of stable diffusion goes back to this latent diffusion paper, which was, you know, not even a year ago.

24
00:03:38,000 --> 00:03:44,000
It's not only a year ago. I think the whole thing kind of kicked off with the clip released by open AI in January of last year.

25
00:03:44,000 --> 00:03:47,000
So I said COVID during that time while doing my COVID thing. Okay.

26
00:03:47,000 --> 00:03:59,000
My daughter came to me and said, Dad, you know, all that stuff you do, taking all that knowledge and squishing it down to make it useful for everyone. Can you do that with images like we can. So a bit of system for her based on VQ gang and clip.

27
00:03:59,000 --> 00:04:08,000
So image generating model and then clip is an image to text model where she created like a vision board of everything she wanted a description what she wanted to make.

28
00:04:08,000 --> 00:04:17,000
There is 16 different images and then she said how each one of those is different and change the latent. And then generate another 60 another 60 another 60 and then eight hours later.

29
00:04:17,000 --> 00:04:22,000
She made an image that she went on to sell as an NFT for three and a half thousand dollars.

30
00:04:22,000 --> 00:04:25,000
Wow. And donated the proceeds to India code relief. Okay.

31
00:04:25,000 --> 00:04:28,000
I thought it was awesome. She's seven years old. Wow.

32
00:04:28,000 --> 00:04:32,000
And then I was like, this is transformative technology.

33
00:04:32,000 --> 00:04:41,000
The image is the one it's out language. You're already at 85% we're going to get a 95% image. We're at 10%. We're not visual species like the easiest way for us to communicate is what we're doing now.

34
00:04:41,000 --> 00:04:48,000
We're having a nice chat, you know, then text is the next part of an image like the images or PowerPoints are impossible.

35
00:04:48,000 --> 00:04:55,000
Let's make it easy. This tech can do that. So we started funding the entire sector, Google call up notebooks models, all these kind of things.

36
00:04:55,000 --> 00:05:03,000
Late in diffusion was done by the conference lab at the University of Munich, who are led on the stable diffusion one as well.

37
00:05:03,000 --> 00:05:11,000
Amazing lab led by BjÃ¶rn Jomrod and led by Robin Rombach, who was one of our lead developers here at Stability.

38
00:05:11,000 --> 00:05:18,000
And then there was work by Catherine Krausen, Rivershave Wings, a Twitter handle on clip condition models and things like that.

39
00:05:18,000 --> 00:05:20,000
And the whole community just came together and built really cool stuff.

40
00:05:20,000 --> 00:05:25,000
Then he had entities like mid-journey where we just gave grants for the beta that started operationalizing it.

41
00:05:25,000 --> 00:05:30,000
And it's all come together now to the finality of stable diffusion that was released on August 3rd.

42
00:05:30,000 --> 00:05:39,000
So that was led by the conference lab and then we ourselves a stability runway ML, a Luther AI community that we kind of helped run and lie on.

43
00:05:39,000 --> 00:05:49,000
We came together to put out 100,000 gigabytes of image lab text pairs, two billion images turned into a two gigabyte file that runs natively on your Macbook.

44
00:05:49,000 --> 00:05:54,000
That can create anything. It's kind of insane. I think it's yeah.

45
00:05:54,000 --> 00:05:57,000
And the speed of which it all came together is mind modeling.

46
00:05:57,000 --> 00:06:05,000
Yeah, like our model was to have a core team and then site contributors and partners from academia and then these communities that we kind of built and accelerated.

47
00:06:05,000 --> 00:06:13,000
So like tens of thousands of people from open by ML were doing protein folding work to a Luther with language models to harmonize with audio.

48
00:06:13,000 --> 00:06:18,000
And it turned out that's a really good system to just iterate and experiment with these things and exactly the right time.

49
00:06:18,000 --> 00:06:27,000
And now it's progressed. So like when we started with stable diffusion and launched it in August, 5.8 seconds for a generation on an A 100.

50
00:06:27,000 --> 00:06:31,000
As of yesterday, 0.86 seconds.

51
00:06:31,000 --> 00:06:36,000
As of two weeks from now, it'll be 20 times faster with our new still models.

52
00:06:36,000 --> 00:06:43,000
So you're getting to 24 frames a second high resolution image creation from basic blobs a year ago.

53
00:06:43,000 --> 00:06:54,000
I don't think we've ever seen anything that fast. And the uptake has been crazy. So I believe on Monday, the number of GitHub stars was stable diffusion overtook Ethereum and Bitcoin.

54
00:06:54,000 --> 00:07:00,000
So we're taking Kafka, everything else thing. I'll overtake kind of PyTorch and TensorFlow and like a month or two.

55
00:07:00,000 --> 00:07:07,000
And that's since inception, like over the last month, I think Master Don has had 6000 GitHub stars over the last week.

56
00:07:07,000 --> 00:07:13,000
Stable diffusion to has had 6000. Yeah, and stable diffusion to was just released this month.

57
00:07:13,000 --> 00:07:19,000
Yeah, we were last month. It was a week ago. Yeah, last month. These times I lay stable. So stable diffusion one.

58
00:07:19,000 --> 00:07:26,000
We kind of use the Lyon data set to create the image model and then we use open a eyes clip L 14 to kind of condition it.

59
00:07:26,000 --> 00:07:28,000
So we combine the text model and the image model.

60
00:07:28,000 --> 00:07:41,000
We're stable diffusion to we instead use something called open clip run by kind of the Lyon charity, whereby we had an open data set for both because open added amazing work open sourcing clip.

61
00:07:41,000 --> 00:07:46,000
But we didn't know what data was inside it. So it learned all these concept. I'm like, how does it know that?

62
00:07:46,000 --> 00:07:54,000
And so when we launched able to fusion kind of as a collaboration, we had all these questions about attribution about what's in the data set, say for work, not say for work.

63
00:07:54,000 --> 00:08:02,000
But you can't control that if you don't control half the data set right off the learning. So still diffusion to have that, but I also had a better texting code a model.

64
00:08:02,000 --> 00:08:08,000
So now basically it's heading towards photorealism. Yeah, you can get photorealistic outputs from it if you can process.

65
00:08:08,000 --> 00:08:16,000
Yeah, and again, kind of insane. Like you just see these things generated in a second. You're like, it can be completely like artistic or completely photorealistic.

66
00:08:16,000 --> 00:08:28,000
These people do not exist. This landscape or this interior does not exist. I don't think we've ever actually seen anything like this because the majority of him out he doesn't believe they can visually create.

67
00:08:28,000 --> 00:08:33,000
Just like before the Gutenberg press, you couldn't write or read.

68
00:08:33,000 --> 00:08:48,000
But now hundreds of thousands of developers think we've had like 380,000 developers sign up for a thing on hugging face. And now using this to create ridiculous things. And now that it gets to real time, what does that even look like when people can just seem to see communicate visually.

69
00:08:48,000 --> 00:08:56,000
Like we could literally in a few months a year, definitely this podcast, you could generate a live video almost on it.

70
00:08:56,000 --> 00:09:08,000
But all the topics that we're talking about, which is insane. One of the examples that you like to use is killing PowerPoint. So we've got the text that's where you usually start and you go through this long process to make it pretty or engaging aesthetic, right?

71
00:09:08,000 --> 00:09:18,000
Yeah, because you know what these models do, like this attention based models, like it's interesting. So with my son with his autism, autism is kind of a social interaction disorder.

72
00:09:18,000 --> 00:09:26,000
It's caused, in my opinion, largely by a gap of glutamate imbalance in the brain. So Gabba calms you down when you pop a value and glutamate excites you.

73
00:09:26,000 --> 00:09:35,000
And obviously in our industry, a lot of people kind of have people, they know on the spectrum or they're just highly there because it lends itself sometimes where there's a dual wedge thing.

74
00:09:35,000 --> 00:09:42,000
Because of all that stuff, what happens is that there's too much glutamate. It's like, you know when you're tapping your legs, there's too much going on in your brain.

75
00:09:42,000 --> 00:09:45,000
Imagine that was like that all the time. You couldn't think straight.

76
00:09:45,000 --> 00:09:52,000
Yeah. And so you can't form the connections of like a cup means cup your hands or a cup or a world cup in your brain.

77
00:09:52,000 --> 00:09:55,000
That's why there's a lot of cases where they can't communicate properly.

78
00:09:55,000 --> 00:10:01,000
Addressing those factors and calm it down, then you basically start teaching them just like when you have a stroke.

79
00:10:01,000 --> 00:10:06,000
A cup means it's a cup means that a cup means that and they can start talking or you know progress.

80
00:10:06,000 --> 00:10:18,000
With these attention-based models, you've moved from kind of giant extrapolation of data to paying attention to the most important parts between words and pixels, which is kind of crazy for the denoising process of diffusion.

81
00:10:18,000 --> 00:10:26,000
The latent is that it built up there where it has all the concepts of a cup means that if you have a cup in a sentence, it understands what that is in that context, a world cup or cup in the hands.

82
00:10:26,000 --> 00:10:31,000
And then can do these images, which is kind of insane. So it works like that part of the human brain.

83
00:10:31,000 --> 00:10:36,000
I think that's what's so exciting. That's what lets you have the compression of knowledge.

84
00:10:36,000 --> 00:10:43,000
Like so 100,000 gigabytes into two gigabytes is like we're pie-pype from that Silicon Valley HDO show, right?

85
00:10:43,000 --> 00:10:47,000
I think it doesn't make sense. Yeah. You know, like a lot, but that's because...

86
00:10:47,000 --> 00:10:51,000
A hundred thousand gigabytes. A hundred terabytes.

87
00:10:51,000 --> 00:10:54,000
Was our input data and the output files two gigs. Yeah.

88
00:10:54,000 --> 00:10:58,000
And it's not optimised yet. We reckon we can get that to 400 megabytes.

89
00:10:58,000 --> 00:11:06,000
Oh wow. A 400 megabyte file that now works on an iPhone that can generate any image in seconds by description.

90
00:11:06,000 --> 00:11:07,000
Yeah.

91
00:11:07,000 --> 00:11:10,000
And you can go the other way as well. You can take an image and turn it into text.

92
00:11:10,000 --> 00:11:16,000
And that text encoding is only a few lines that can generate a high resolution masterpiece.

93
00:11:16,000 --> 00:11:18,000
It's insane.

94
00:11:18,000 --> 00:11:19,000
That's nuts.

95
00:11:19,000 --> 00:11:30,000
And I think we were kind of a bit misguided by not mismigod, but you know, the focus was on scale is all you need 540 billion parameter, trillion parameter, large language models.

96
00:11:30,000 --> 00:11:34,000
Stable diffusers 890 million parameters.

97
00:11:34,000 --> 00:11:37,000
How does that kind of work about large earlier?

98
00:11:37,000 --> 00:11:40,000
It's not large. It's actually quite small.

99
00:11:40,000 --> 00:11:46,000
And this is kind of pointing something to the future because like, you know, OpenAI took GPT-375 billion parameters.

100
00:11:46,000 --> 00:11:52,000
And they instructed it. So reinforce with only human feedback by getting annotators to use it.

101
00:11:52,000 --> 00:11:55,000
And then seeing which neurons kind of lit up these kind of latent space things.

102
00:11:55,000 --> 00:12:02,000
Instruct GPT had equivalent performance. I think they probably use a larger version of that at 1.3 billion parameters.

103
00:12:02,000 --> 00:12:07,000
Because kind of you don't need all the information of the world completely to do stuff.

104
00:12:07,000 --> 00:12:09,000
You just need some of it.

105
00:12:09,000 --> 00:12:12,000
Image models though are surprisingly small.

106
00:12:12,000 --> 00:12:17,000
Like the largest we've seen was the 12 billion parameters. Are you Dali model?

107
00:12:17,000 --> 00:12:23,000
But now, like I said, we're 900 million parameters and we've had great success with our 400 million parameter models.

108
00:12:23,000 --> 00:12:29,000
Our 4 billion parameter models are better. Actually, the largest is party, which was from Google at 220 billion.

109
00:12:29,000 --> 00:12:36,000
We don't know what an optimal data set is, what optimal parameter size is for these particular non-text models.

110
00:12:36,000 --> 00:12:41,000
Text models themselves. Text is quite a dancing coding. I think we'll tend larger.

111
00:12:41,000 --> 00:12:44,000
But combining these models is going to be super interesting as we move forward.

112
00:12:44,000 --> 00:12:52,000
It's a lot of the efforts as far have been on shrinking the model to make the performance better, to make it smaller, faster.

113
00:12:52,000 --> 00:12:55,000
Do you see a pull towards larger models?

114
00:12:55,000 --> 00:13:02,000
Or do you think it's a different paradigm altogether where there's not going to be that kind of drive to make the model bigger and bigger?

115
00:13:02,000 --> 00:13:10,000
I think there'll be a mixture of things. Again, like what we saw with the deep-mind chiller paper was that the scaling laws weren't necessarily appropriate.

116
00:13:10,000 --> 00:13:18,000
So that showed that a 67 billion parameter model trained on 5 epochs would outperform 175 billion parameter model.

117
00:13:18,000 --> 00:13:23,000
But actually what it really showed if you dig into the details is that data is what you need.

118
00:13:23,000 --> 00:13:27,000
And what does that data look like? We haven't done the proper data augmentation in other studies.

119
00:13:27,000 --> 00:13:32,000
But this is also like, you can think of these models like, stable diffusion one was a pre-crecious kindergarten.

120
00:13:32,000 --> 00:13:37,000
And we talked about the whole internet, so it occasionally turned a little bit off in some of the outputs.

121
00:13:37,000 --> 00:13:40,000
Yeah, stable diffusion too, you're getting to like grade school now.

122
00:13:40,000 --> 00:13:45,000
Let's still see what we're pre-crecious. And you know, we made it safe for work and then it's safer for work and a whole bunch of other things.

123
00:13:45,000 --> 00:13:48,000
Did you do the data sets? We're still not feeding it the right information.

124
00:13:48,000 --> 00:13:51,000
Once we know what the information to feed it will make it even better.

125
00:13:51,000 --> 00:13:54,000
And I don't think that translates to large. I think it turns to more efficient.

126
00:13:54,000 --> 00:13:57,000
And I think one of these things is the accessibility.

127
00:13:57,000 --> 00:14:06,000
Because we optimize stable diffusion kind of as a group and collective to be available on low energy devices, not just like 3090s or A100s.

128
00:14:06,000 --> 00:14:09,000
You can download it on your MacBook right now.

129
00:14:09,000 --> 00:14:15,000
And MacBook M2, as of today, can generate an image in 18 seconds of any type.

130
00:14:15,000 --> 00:14:17,000
In a couple of weeks, it'll be less than a second.

131
00:14:17,000 --> 00:14:21,000
So you can have PyTorch, you can have jacks or whatever.

132
00:14:21,000 --> 00:14:23,000
And you can just stop coding.

133
00:14:23,000 --> 00:14:27,000
And so that opens it up to so many people. It's a new type of programming primitive.

134
00:14:27,000 --> 00:14:30,000
You know, this hashed file that can create anything.

135
00:14:30,000 --> 00:14:34,000
That went into the connection between programming and stable diffusion.

136
00:14:34,000 --> 00:14:37,000
So if you think about it, you're creating an experience for your programming, right?

137
00:14:37,000 --> 00:14:44,000
And so if you use the diffuses library from hugging phase, it's like a couple of lines you can be using stable diffusion in a code base.

138
00:14:44,000 --> 00:14:47,000
And again, it can run your MacBook with no internet.

139
00:14:47,000 --> 00:14:54,000
So what type of experiences can you do when you have this verifiable file, words go in, images come out.

140
00:14:54,000 --> 00:14:56,000
It opens up a whole world of its possibilities.

141
00:14:56,000 --> 00:14:59,000
It's like an ultra library in a way.

142
00:14:59,000 --> 00:15:02,000
Like the library condense in AI model.

143
00:15:02,000 --> 00:15:04,000
And we're not really used to that.

144
00:15:04,000 --> 00:15:07,000
Like, you know, we've had birds and kind of some of these other things.

145
00:15:07,000 --> 00:15:11,000
But nothing that has this massive range, shall we say.

146
00:15:11,000 --> 00:15:16,000
Like two billion images, a snapshot of the internet can press down.

147
00:15:16,000 --> 00:15:17,000
Yeah.

148
00:15:17,000 --> 00:15:19,000
You're kind of thinking more broadly like we.

149
00:15:19,000 --> 00:15:24,000
A lot of the conversation about stable diffusion today is about art.

150
00:15:24,000 --> 00:15:29,000
And kind of, you know, the creation part of that process.

151
00:15:29,000 --> 00:15:32,000
I'm thinking more broadly about practical applications.

152
00:15:32,000 --> 00:15:38,000
And this is maybe getting into something I wanted to speak about later, just where you see the company going.

153
00:15:38,000 --> 00:15:46,000
You know, talk about some of the other things that are disrupted beyond just, you know, making pretty pictures, arts and crafts, right?

154
00:15:46,000 --> 00:15:47,000
Yeah, man.

155
00:15:47,000 --> 00:15:51,000
I think art is like, we think about it as like, if my artist never make money, right?

156
00:15:51,000 --> 00:15:54,000
Unless they do, you know, like my seven-year-old daughter.

157
00:15:54,000 --> 00:15:57,000
She's obviously, you know, one of the OGs now in general is about.

158
00:15:57,000 --> 00:16:02,000
I actually asked her, why don't you make any more art anymore? And she's like, well, dad, there's this thing called supply and demand.

159
00:16:02,000 --> 00:16:10,000
If I reduce the supply and you can make this whole industry, the bond from my stuff will go up so the value can't be like, you're paying for your own university.

160
00:16:10,000 --> 00:16:14,000
Creative industry is worth hundreds of billions of dollars a year.

161
00:16:14,000 --> 00:16:18,000
Video games, 170 billion, like movies or 80 billion.

162
00:16:18,000 --> 00:16:21,000
This will all be disrupted by this technology.

163
00:16:21,000 --> 00:16:26,000
If you think about the creation process, like one of our directors, he was doing a shoot with a famous actress.

164
00:16:26,000 --> 00:16:29,000
And with a rating, it's going to be $113,000.

165
00:16:29,000 --> 00:16:33,000
It gets flyer out and do all this and get all these other people just for three days.

166
00:16:33,000 --> 00:16:37,000
He fine-tuned a stable diffusion model.

167
00:16:37,000 --> 00:16:40,000
Did it in three hours, 2,000 shots.

168
00:16:40,000 --> 00:16:42,000
Go to realistic.

169
00:16:42,000 --> 00:16:45,000
Meaning being entire shoot was generated as opposed to?

170
00:16:45,000 --> 00:16:46,000
Yeah, like all the shots.

171
00:16:46,000 --> 00:16:51,000
Because there's going to be a shoot to kind of put her in different things to go into the movie kind of process.

172
00:16:51,000 --> 00:16:54,000
So concept artists are using this to become more efficient.

173
00:16:54,000 --> 00:16:56,000
There's a group corridor digital.

174
00:16:56,000 --> 00:17:01,000
They created a Spider-Man everyone's home, which is like a two and a half minute trailer,

175
00:17:01,000 --> 00:17:06,000
in the spider-verse style by having a spider-verse model.

176
00:17:06,000 --> 00:17:08,000
But they train on like 100 images.

177
00:17:08,000 --> 00:17:11,000
And you can't tell it's like, wow, this is an amazing animation.

178
00:17:11,000 --> 00:17:12,000
No, it isn't.

179
00:17:12,000 --> 00:17:16,000
They just interpolated every single frame and used stable diffusion to kind of do image damage.

180
00:17:16,000 --> 00:17:17,000
It's the craziest thing.

181
00:17:17,000 --> 00:17:19,000
It would cost millions of dollars before.

182
00:17:19,000 --> 00:17:21,000
It did it like a few days.

183
00:17:21,000 --> 00:17:27,000
I think media is going to be the first to be disrupted here, because that creation process is hard.

184
00:17:27,000 --> 00:17:28,000
And now it's easy.

185
00:17:28,000 --> 00:17:31,000
I would think industrial design, for example, wouldn't be too far back behind.

186
00:17:31,000 --> 00:17:32,000
Like auto desk.

187
00:17:32,000 --> 00:17:36,000
You know, they spend a lot of time thinking about ways to use machine learning help designers.

188
00:17:36,000 --> 00:17:38,000
Yeah, they've got amazing kind of data sets.

189
00:17:38,000 --> 00:17:42,000
You know, you've got the cameras of the world that have every single click on design.

190
00:17:42,000 --> 00:17:46,000
It can make all of those easier, because the system learns.

191
00:17:46,000 --> 00:17:50,000
There's a foundational model in some ways, because also like a base foundation.

192
00:17:50,000 --> 00:17:52,000
You can then train on your own things.

193
00:17:52,000 --> 00:17:55,000
And it learns physics and all sorts of other stuff, which is a bit creepy.

194
00:17:55,000 --> 00:18:00,000
But it can learn about that specific type of design that you might want to do.

195
00:18:00,000 --> 00:18:05,000
We work with car manufacturers right now, who want to have custom models based on their entire back catalog.

196
00:18:05,000 --> 00:18:08,000
And they want to iterate and combine different concepts.

197
00:18:08,000 --> 00:18:11,000
And then they automatically stick together these cars and combines them.

198
00:18:11,000 --> 00:18:13,000
You know, we also didn't just release the model.

199
00:18:13,000 --> 00:18:15,000
We also released an in-painting model.

200
00:18:15,000 --> 00:18:20,000
So you can delete parts of a picture and have seamless edits based on your text conditioning on that.

201
00:18:20,000 --> 00:18:21,000
You've got an image to image model.

202
00:18:21,000 --> 00:18:22,000
We can define it into any style.

203
00:18:22,000 --> 00:18:25,000
We have a four soon to be eight times up scaler.

204
00:18:25,000 --> 00:18:29,000
That's like enhanced and enhanced on a TV show, you know.

205
00:18:29,000 --> 00:18:33,000
And all of these are going sub-second now in terms of the speed of iteration on them.

206
00:18:33,000 --> 00:18:35,000
So I think creative is the first.

207
00:18:35,000 --> 00:18:37,000
But then I said some of this design kind of things.

208
00:18:37,000 --> 00:18:40,000
Then it goes into more visual communication, like I said, slides.

209
00:18:40,000 --> 00:18:45,000
If you go to image model, combine with language model, combine with code model, you never need to make a presentation again.

210
00:18:45,000 --> 00:18:47,000
And understand what aesthetics are.

211
00:18:47,000 --> 00:18:55,000
Like one of the things we do with stable diffusion thing is that we create a discord bot where everyone rated the outputs of stable diffusion 1.1.

212
00:18:55,000 --> 00:19:03,000
And then we use that to filter down our giant 2 billion image data set into the most aesthetically pleasing things using clip conditioning on that.

213
00:19:03,000 --> 00:19:06,000
And then we trained on that and it became more aesthetic and pleasing.

214
00:19:06,000 --> 00:19:11,000
Bit weird in some ways, but again, these feedback leaves become very, very interesting.

215
00:19:11,000 --> 00:19:18,000
Because to get the wide range of viability on these image models, language models, audio models, others.

216
00:19:18,000 --> 00:19:20,000
The human and the loop factor is essential.

217
00:19:20,000 --> 00:19:23,000
Because your typical training data is quite diverse.

218
00:19:23,000 --> 00:19:29,000
But you want to customize it to the needs and wants of the humans or the sector or the specificness of that.

219
00:19:29,000 --> 00:19:31,000
There are other models out there.

220
00:19:31,000 --> 00:19:38,000
You mentioned the journey a few times, you mentioned Dali, we've talked about performance as a kind of a target differentiator.

221
00:19:38,000 --> 00:19:47,000
What are some of the other ways that you see stable diffusion kind of defining itself relative to the other things that are popping up?

222
00:19:47,000 --> 00:19:50,000
Open source will always lack closed source.

223
00:19:50,000 --> 00:19:54,000
Because they can always just take open source and upgrade it, especially foundation models, right?

224
00:19:54,000 --> 00:19:56,000
I think data is kind of a key thing.

225
00:19:56,000 --> 00:20:01,000
There's been a recurring theme that's come up in our conversation a lot.

226
00:20:01,000 --> 00:20:08,000
This is the idea of the human and the loop and data, refining the data versus evolving the model, the whole data-centric AI idea.

227
00:20:08,000 --> 00:20:14,000
Yeah, so it's kind of a data-centric thing where if you look at people adapt to these models right now, they're doing few shot learning.

228
00:20:14,000 --> 00:20:16,000
Or they're doing basic fine tuning.

229
00:20:16,000 --> 00:20:20,000
Because there's no point in training your own model because it's freaking moving so fast.

230
00:20:20,000 --> 00:20:28,000
We'll have stable diffusion version three in a few months. We had a 20 time speed up yesterday on the model.

231
00:20:28,000 --> 00:20:30,000
It's insane, these kind of moves.

232
00:20:30,000 --> 00:20:32,000
I don't think we've ever seen anything quite this exponential.

233
00:20:32,000 --> 00:20:38,000
But what happens then is that if you go via an API, there's only so much you can do.

234
00:20:38,000 --> 00:20:40,000
That's what all of these companies do.

235
00:20:40,000 --> 00:20:43,000
Or if you go via an interface like the mid-genius and like that or a Dali.

236
00:20:43,000 --> 00:20:46,000
If you've got the model yourself and you can play, you can experiment, you can adapt it.

237
00:20:46,000 --> 00:20:50,000
So the language models from the Elite, the community, GPT Neo, JNX.

238
00:20:50,000 --> 00:20:54,000
They're GPT level models, but only up to 20 billion parameters.

239
00:20:54,000 --> 00:20:58,000
They've been downloaded 20 million times by developers.

240
00:20:58,000 --> 00:21:02,000
And they need to tell anyone, they just get on with things.

241
00:21:02,000 --> 00:21:06,000
And so one of the interesting things for me is that the positioning is the tooling around this.

242
00:21:06,000 --> 00:21:14,000
Because once you've got those primitives, you can build stuff around just like you've seen loads of community web UIs and other interfaces to interact with stable diffusion.

243
00:21:14,000 --> 00:21:16,000
And for our own company, it's a very simple thing.

244
00:21:16,000 --> 00:21:18,000
This is like a database on steroids.

245
00:21:18,000 --> 00:21:20,000
You're thinking about it.

246
00:21:20,000 --> 00:21:22,000
Like it's a database that comes pre-filled with interesting stuff.

247
00:21:22,000 --> 00:21:24,000
And that's how much people are using it right now.

248
00:21:24,000 --> 00:21:28,000
But soon when we upgrade it a few bits and it comes mature.

249
00:21:28,000 --> 00:21:30,000
It's a database, it's a database, it's a kind of a magic bot.

250
00:21:30,000 --> 00:21:32,000
Bot's database of images and your query is your prompt.

251
00:21:32,000 --> 00:21:34,000
Exactly. It's a data store.

252
00:21:34,000 --> 00:21:36,000
Except for a super efficient data store.

253
00:21:36,000 --> 00:21:38,000
100,000 gigs to two.

254
00:21:38,000 --> 00:21:40,000
And it can do all sorts of wonderful things.

255
00:21:40,000 --> 00:21:42,000
So right now everyone's using like the pre-baked version.

256
00:21:42,000 --> 00:21:44,000
Like the Laura Mipson version, right?

257
00:21:44,000 --> 00:21:46,000
But then in a few years, everyone wants their own custom ones.

258
00:21:46,000 --> 00:21:48,000
So obviously, it's very simple.

259
00:21:48,000 --> 00:21:50,000
Take the exabytes of data from content companies.

260
00:21:50,000 --> 00:21:52,000
Convert them into these models and make them useful.

261
00:21:52,000 --> 00:21:54,000
Because we think content is turning intelligent.

262
00:21:54,000 --> 00:21:58,000
And it goes beyond media companies to bio, farmer, and others.

263
00:21:58,000 --> 00:22:02,000
And we're probably the only foundation model company building cutting edge AI.

264
00:22:02,000 --> 00:22:04,000
That's willing to work with people and go to the data.

265
00:22:04,000 --> 00:22:08,000
So models to the data, I think, are a very interesting thing.

266
00:22:08,000 --> 00:22:12,000
Based on open frameworks, so you don't have to lock in

267
00:22:12,000 --> 00:22:14,000
of some of these other ecosystems.

268
00:22:14,000 --> 00:22:16,000
You'll be like, I'll trade a model for you,

269
00:22:16,000 --> 00:22:18,000
but you have to be locked into my thing.

270
00:22:18,000 --> 00:22:20,000
Yeah. Yeah.

271
00:22:20,000 --> 00:22:22,000
One of the things that you mentioned in passing

272
00:22:22,000 --> 00:22:24,000
is that you've seen the model learn physics.

273
00:22:24,000 --> 00:22:26,000
What does that mean?

274
00:22:26,000 --> 00:22:30,000
So like, if you type in a lady looking across a still lake,

275
00:22:30,000 --> 00:22:34,000
it will do her reflection in the water.

276
00:22:34,000 --> 00:22:36,000
You know, raindrops, it gets correct and things like that.

277
00:22:36,000 --> 00:22:40,000
And as you train it more, it learns more and more concepts

278
00:22:40,000 --> 00:22:44,000
of how things interact, which again is a bit insane.

279
00:22:44,000 --> 00:22:46,000
Like, you can show it the sides.

280
00:22:46,000 --> 00:22:50,000
You can train it on like a experimental call like a cyber truck.

281
00:22:50,000 --> 00:22:52,000
You can see how much effort has gone into

282
00:22:52,000 --> 00:22:56,000
the visualization community trying to get that stuff right.

283
00:22:56,000 --> 00:23:00,000
Exactly. So like, you can show it parts of like a cyber truck.

284
00:23:00,000 --> 00:23:02,000
Yeah.

285
00:23:02,000 --> 00:23:04,000
And it doesn't know a cyber truck, say for instance.

286
00:23:04,000 --> 00:23:06,000
What the back of the cyber truck looks like and it will guess

287
00:23:06,000 --> 00:23:08,000
and it'll probably get it right.

288
00:23:08,000 --> 00:23:10,000
It knows the essence of truckness.

289
00:23:10,000 --> 00:23:12,000
Yeah. So rather than having these very specific models

290
00:23:12,000 --> 00:23:14,000
that learn stuff, you can now have something

291
00:23:14,000 --> 00:23:16,000
that can do just about anything in terms of lighting

292
00:23:16,000 --> 00:23:18,000
and, you know, they've got prompt to prompt

293
00:23:18,000 --> 00:23:20,000
where you can say, make this picture sadder

294
00:23:20,000 --> 00:23:22,000
or, you know, turn them out into a clown or a stormtrooper

295
00:23:22,000 --> 00:23:24,000
and automatically does that.

296
00:23:24,000 --> 00:23:26,000
Because it understands the nature of these things

297
00:23:26,000 --> 00:23:28,000
and the physics and balancing of that,

298
00:23:28,000 --> 00:23:30,000
which again is kind of insane.

299
00:23:30,000 --> 00:23:32,000
This has been implications for the rendering industry

300
00:23:32,000 --> 00:23:34,000
and other things because this is a far more efficient renderer

301
00:23:34,000 --> 00:23:36,000
that can do image-to-image

302
00:23:36,000 --> 00:23:38,000
and transform something into something else.

303
00:23:38,000 --> 00:23:40,000
Nobody's quite sure how it works.

304
00:23:40,000 --> 00:23:42,000
And I've got theories.

305
00:23:42,000 --> 00:23:44,000
And this is one of these things

306
00:23:44,000 --> 00:23:46,000
with these foundation models like

307
00:23:46,000 --> 00:23:48,000
they're just an alien type of experience

308
00:23:48,000 --> 00:23:50,000
when you first really start pushing it.

309
00:23:50,000 --> 00:23:52,000
Most people are surface level when you start pushing through it

310
00:23:52,000 --> 00:23:54,000
you're like, it's really curious that you can do this.

311
00:23:54,000 --> 00:23:56,000
It doesn't have agency.

312
00:23:56,000 --> 00:23:58,000
It's too key what file.

313
00:23:58,000 --> 00:24:00,000
But the fact that you can have that compression

314
00:24:00,000 --> 00:24:02,000
of those concepts

315
00:24:02,000 --> 00:24:04,000
is really interesting.

316
00:24:04,000 --> 00:24:06,000
Would that always be a fundamental limiter

317
00:24:06,000 --> 00:24:10,000
meaning, you know, if you want a quick and dirty approximation,

318
00:24:10,000 --> 00:24:12,000
use something like stable diffusion.

319
00:24:12,000 --> 00:24:14,000
But if you want a precise rendering, you know,

320
00:24:14,000 --> 00:24:16,000
you have to turn to traditional techniques.

321
00:24:16,000 --> 00:24:18,000
I think it's going to be, I always say it's part of a process.

322
00:24:18,000 --> 00:24:19,000
Yeah.

323
00:24:19,000 --> 00:24:21,000
Architecture, you shouldn't try to do zero sure everything.

324
00:24:21,000 --> 00:24:23,000
That's what people tend to pull into a travel.

325
00:24:23,000 --> 00:24:25,000
Like, yeah, I just wanted to know, like, have kind of

326
00:24:25,000 --> 00:24:27,000
K&Ns or knowledge graphs

327
00:24:27,000 --> 00:24:29,000
or retrieval augmented systems or kind of whatever.

328
00:24:29,000 --> 00:24:31,000
Put it as part of a process pipeline.

329
00:24:31,000 --> 00:24:33,000
But definitely a quick and dirty,

330
00:24:33,000 --> 00:24:35,000
it does very, very, very well.

331
00:24:35,000 --> 00:24:37,000
Better than anything.

332
00:24:37,000 --> 00:24:39,000
But then I think that also this way we have our

333
00:24:39,000 --> 00:24:41,000
impainting and all these other models.

334
00:24:41,000 --> 00:24:43,000
It's going to be part of multiple models

335
00:24:43,000 --> 00:24:45,000
doing multiple things for multiple purposes.

336
00:24:45,000 --> 00:24:47,000
Sometimes there might be a giant model

337
00:24:47,000 --> 00:24:49,000
once you get to a certain stage.

338
00:24:49,000 --> 00:24:51,000
Other times, you might just want to have a quick and dirty

339
00:24:51,000 --> 00:24:53,000
256x256 iteration loop.

340
00:24:53,000 --> 00:24:55,000
And so what we've seen as well.

341
00:24:55,000 --> 00:24:57,000
Like, we're stable diffusion too.

342
00:24:57,000 --> 00:24:59,000
We actually flattened the late in space through

343
00:24:59,000 --> 00:25:01,000
DDP and all bunch of other things.

344
00:25:01,000 --> 00:25:03,000
So it's more difficult to prompt.

345
00:25:03,000 --> 00:25:05,000
Stilt diffusion one was quite easy to prompt.

346
00:25:05,000 --> 00:25:07,000
Stilt diffusion two is more difficult,

347
00:25:07,000 --> 00:25:09,000
but it's got much more fine grain control.

348
00:25:09,000 --> 00:25:11,000
But where we're going, we're not going to use prompts.

349
00:25:11,000 --> 00:25:13,000
Because it will learn from...

350
00:25:13,000 --> 00:25:15,000
What do you see taking the place of prompts?

351
00:25:15,000 --> 00:25:17,000
Well, I think it will just be a case of like,

352
00:25:17,000 --> 00:25:19,000
you will have your own embedding store.

353
00:25:19,000 --> 00:25:21,000
That points to points in the late in space

354
00:25:21,000 --> 00:25:23,000
and then pulls up like the things that you like most commonly.

355
00:25:23,000 --> 00:25:25,000
So it learns and then kind of there's that interaction

356
00:25:25,000 --> 00:25:27,000
two things. So, you know, embedding is being

357
00:25:27,000 --> 00:25:31,000
a multiplier representation of kind of what's in there.

358
00:25:31,000 --> 00:25:33,000
So I think that people's own context is important.

359
00:25:33,000 --> 00:25:35,000
And AI models have, we understand people's

360
00:25:35,000 --> 00:25:37,000
AI person context in that.

361
00:25:37,000 --> 00:25:39,000
Or companies or other things.

362
00:25:39,000 --> 00:25:41,000
And again, this is fine tuning effect where you can

363
00:25:41,000 --> 00:25:43,000
with a two gigabyte file.

364
00:25:43,000 --> 00:25:45,000
Actually have your own model.

365
00:25:45,000 --> 00:25:47,000
And then why do you need to prompt training on

366
00:25:47,000 --> 00:25:51,000
art station 3D, obtain render and all these things.

367
00:25:51,000 --> 00:25:53,000
When it learns that that's what you want to have.

368
00:25:53,000 --> 00:25:54,000
That's this type of style that you like.

369
00:25:54,000 --> 00:25:55,000
Right.

370
00:25:55,000 --> 00:25:56,000
Having said that.

371
00:25:56,000 --> 00:25:58,000
I think prompting is just very difficult.

372
00:25:58,000 --> 00:26:00,000
Likewise, we've been trying to prompt me for 16 years

373
00:26:00,000 --> 00:26:02,000
if she has a quite managed.

374
00:26:02,000 --> 00:26:06,000
You've touched on a couple of things.

375
00:26:06,000 --> 00:26:08,000
Open source versus API.

376
00:26:08,000 --> 00:26:12,000
And very briefly.

377
00:26:12,000 --> 00:26:16,000
This idea of kind of customization.

378
00:26:16,000 --> 00:26:20,000
And I think, you know, based on stuff that I've heard you talk

379
00:26:20,000 --> 00:26:22,000
about in the past, like.

380
00:26:22,000 --> 00:26:26,000
You're very strongly opinionated around the.

381
00:26:26,000 --> 00:26:30,000
The model that through which you're kind of delivering the technology.

382
00:26:30,000 --> 00:26:32,000
Beyond just the technology itself.

383
00:26:32,000 --> 00:26:34,000
Can you talk a little bit about.

384
00:26:34,000 --> 00:26:36,000
Your thoughts there and kind of what's driving.

385
00:26:36,000 --> 00:26:38,000
You were driving that.

386
00:26:38,000 --> 00:26:39,000
So I think.

387
00:26:39,000 --> 00:26:41,000
This is incredibly powerful technology.

388
00:26:41,000 --> 00:26:43,000
I think it's one of the big epoch changes in humanity.

389
00:26:43,000 --> 00:26:45,000
Because you have a model that can do anything and approximate.

390
00:26:45,000 --> 00:26:47,000
There's two types of thing in type one and type two.

391
00:26:47,000 --> 00:26:49,000
Logical thinking and then principle based thinking.

392
00:26:49,000 --> 00:26:52,000
This kind of get to principle based thinking.

393
00:26:52,000 --> 00:26:55,000
We still don't have AI that does good fashion reasoning with logic.

394
00:26:55,000 --> 00:26:56,000
This can take leaps.

395
00:26:56,000 --> 00:26:57,000
That's what we said.

396
00:26:57,000 --> 00:26:59,000
Like quick and dirty approximation.

397
00:26:59,000 --> 00:27:00,000
You can do that.

398
00:27:00,000 --> 00:27:03,000
Your type of end you get like a hundred different images of.

399
00:27:03,000 --> 00:27:05,000
Like a book or a vase or something like that.

400
00:27:05,000 --> 00:27:07,000
You can then iterate and improve.

401
00:27:07,000 --> 00:27:09,000
Just very different experience.

402
00:27:09,000 --> 00:27:11,000
So I think was like, again, put this out as foundation models.

403
00:27:11,000 --> 00:27:14,000
Like again, benchmark models that people can then develop around.

404
00:27:14,000 --> 00:27:17,000
Because the pace of invasional outpace anything that's closed.

405
00:27:17,000 --> 00:27:20,000
But also addresses things like the digital divide.

406
00:27:20,000 --> 00:27:22,000
And my route is nothing.

407
00:27:22,000 --> 00:27:25,000
So like with open AI and Dali too.

408
00:27:25,000 --> 00:27:31,000
They introduced the anti bias filter, which automatically for non-gendered word added a gender and a race.

409
00:27:31,000 --> 00:27:35,000
So we type in suma wrestler or do Indian female suma wrestler.

410
00:27:35,000 --> 00:27:36,000
Yeah.

411
00:27:36,000 --> 00:27:38,000
Which I suppose could exist.

412
00:27:38,000 --> 00:27:39,000
Probably not many of them.

413
00:27:39,000 --> 00:27:40,000
That's probably not an intent.

414
00:27:40,000 --> 00:27:41,000
It's kind of limited.

415
00:27:41,000 --> 00:27:43,000
Whereas with our model, what happens we release it.

416
00:27:43,000 --> 00:27:46,000
And then a team manager pan created a Japanese texting code.

417
00:27:46,000 --> 00:27:47,000
That's our alternative.

418
00:27:47,000 --> 00:27:51,000
So salary man rather than meaning man with lots of salary, men and race have man.

419
00:27:51,000 --> 00:27:55,000
In a county's local context, these local elements, these local fine tunes.

420
00:27:55,000 --> 00:27:56,000
I think we're essential.

421
00:27:56,000 --> 00:27:58,000
And also widening the discussion.

422
00:27:58,000 --> 00:28:03,000
Because a lot of the stuff that occurs with these big powerful models is that we won't release them.

423
00:28:03,000 --> 00:28:04,000
Because we're scared about what's going to happen.

424
00:28:04,000 --> 00:28:06,000
Because the no, no, no, no, no, no.

425
00:28:06,000 --> 00:28:07,000
So that's fair.

426
00:28:07,000 --> 00:28:09,000
That's an opinion.

427
00:28:09,000 --> 00:28:13,000
That shouldn't mean that it shouldn't be available to other people because of the power of this technology.

428
00:28:13,000 --> 00:28:16,000
Because otherwise, they'll just go to corporate first and it won't be available.

429
00:28:16,000 --> 00:28:20,000
That's why the fact it could uplift them creatively and communicatively and other things.

430
00:28:20,000 --> 00:28:24,000
One way to think about it, if I'm really mean in some discussion sometimes, is like,

431
00:28:24,000 --> 00:28:27,000
why don't you want Indians or Africans to have this technology?

432
00:28:27,000 --> 00:28:28,000
Because there's no comeback.

433
00:28:28,000 --> 00:28:32,000
You can't say that more education is needed or it's too dangerous and they're not responsible.

434
00:28:32,000 --> 00:28:34,000
Because the reality is this is technology of the humanity.

435
00:28:34,000 --> 00:28:38,000
And there's an echo of what happened with cryptography.

436
00:28:38,000 --> 00:28:44,000
We can't let cryptography be open and the government pacified it as a weapon here in the US.

437
00:28:44,000 --> 00:28:46,000
Because bad guys might use it.

438
00:28:46,000 --> 00:28:49,000
But we use it now to protect ourselves as well.

439
00:28:49,000 --> 00:28:53,000
Open source will always be more secure than close source if the community rounds together.

440
00:28:53,000 --> 00:28:57,000
Because what do we run our infrastructure on here at AWS?

441
00:28:57,000 --> 00:28:58,000
It's not really Windows.

442
00:28:58,000 --> 00:29:00,000
It's Linux.

443
00:29:00,000 --> 00:29:03,000
Our databases are mySQL and things like that.

444
00:29:03,000 --> 00:29:07,000
Because the community can come together and build stronger systems and more effective systems.

445
00:29:07,000 --> 00:29:10,000
But it's crazy how far this is going.

446
00:29:10,000 --> 00:29:13,000
And so it's difficult lines to throw a toe down.

447
00:29:13,000 --> 00:29:14,000
Yeah.

448
00:29:14,000 --> 00:29:19,000
You mentioned that more recent versions of sable diffusion include, like, safe work filters, that kind of thing.

449
00:29:19,000 --> 00:29:26,000
So it sounds like something that you're thinking about and care about and not just putting out without any kind of controls.

450
00:29:26,000 --> 00:29:27,000
Yeah.

451
00:29:27,000 --> 00:29:31,000
So the original version, look, again, it was led by the Confis Lab.

452
00:29:31,000 --> 00:29:35,000
And we said very specifically, you guys get to decide and we will advise.

453
00:29:35,000 --> 00:29:42,000
Because it wasn't academic endeavor, you know, even if the people like one of them works for us and then works for runway and et cetera.

454
00:29:42,000 --> 00:29:44,000
Is the nature of the thing.

455
00:29:44,000 --> 00:29:49,000
And so we're very respectful of kind of entities that we collaborate with because it can be a minefield, right?

456
00:29:49,000 --> 00:29:51,000
You know, trying to whitewash anything.

457
00:29:51,000 --> 00:29:57,000
So it's released under a creative ML open rail license, which is a new type of license from Hungry Face that said you have to use ethically.

458
00:29:57,000 --> 00:30:02,000
Add a safety filter because the decision was made by the developers not to filter the data.

459
00:30:02,000 --> 00:30:06,000
So it could be a baseline from which we could then figure out biases and other things.

460
00:30:06,000 --> 00:30:11,000
And that removed a lot of nudity and kind of other things, especially because it was accidentally creating it.

461
00:30:11,000 --> 00:30:16,000
Sable diffusion too was trained on a highly safe for work data set.

462
00:30:16,000 --> 00:30:17,000
So it's massively more safe.

463
00:30:17,000 --> 00:30:20,000
It doesn't have a filter because of the need one.

464
00:30:20,000 --> 00:30:30,000
It has some drawbacks such as one of the things that we saw during the fine tuning after stele diffusion one is that people trained on not safer work images.

465
00:30:30,000 --> 00:30:33,000
Internet is for that whatever they fine tune it.

466
00:30:33,000 --> 00:30:36,000
They took lots of images that were not safe for work.

467
00:30:36,000 --> 00:30:37,000
Right.

468
00:30:37,000 --> 00:30:41,000
So obviously there was a standard effect of that because again, they're free to kind of use it as a community.

469
00:30:41,000 --> 00:30:47,000
But the side effect is that when you actually used it for safe for work prompts, it did amazing humans.

470
00:30:47,000 --> 00:30:53,000
Like photorealistic because it learned about anatomy from these not safer work images.

471
00:30:53,000 --> 00:30:54,000
It was quite funny.

472
00:30:54,000 --> 00:31:00,000
So stele diffusion two out the box is a bit less good at anatomy because we removed a lot of those things.

473
00:31:00,000 --> 00:31:01,000
Not much.

474
00:31:01,000 --> 00:31:03,000
And again, we're adding it back in safely.

475
00:31:03,000 --> 00:31:04,000
We really care about that.

476
00:31:04,000 --> 00:31:07,000
The other thing that we care about a lot is, you know, we read this community as big.

477
00:31:07,000 --> 00:31:10,000
We're creating millions, hundreds of millions of artists.

478
00:31:10,000 --> 00:31:12,000
So artists are part of the community.

479
00:31:12,000 --> 00:31:15,000
They were asking, can we opt out of the data sets?

480
00:31:15,000 --> 00:31:18,000
Some are actually asking, can we opt in because we're not in the data set.

481
00:31:18,000 --> 00:31:23,000
And so we worked with spawning and lying on others on opt in an opt out mechanisms.

482
00:31:23,000 --> 00:31:25,000
Because I think that's the right thing to do.

483
00:31:25,000 --> 00:31:29,000
Like, I think that it's ethical to use web scrapes to create models like this.

484
00:31:29,000 --> 00:31:34,000
Especially because the diffusion process doesn't create copies or photo mashes.

485
00:31:34,000 --> 00:31:35,000
It actually learns principles.

486
00:31:35,000 --> 00:31:36,000
It's like a human.

487
00:31:36,000 --> 00:31:40,000
For the same time, people don't want to have their data in the data set.

488
00:31:40,000 --> 00:31:41,000
They should opt out.

489
00:31:41,000 --> 00:31:42,000
If they want to end, they should opt in.

490
00:31:42,000 --> 00:31:45,000
In fact, we've had thousands of artists sign up for the system.

491
00:31:45,000 --> 00:31:48,000
It's been 50-50. Opt in an opt out.

492
00:31:48,000 --> 00:31:51,000
I think it's really interesting and not maybe what some people would expect.

493
00:31:51,000 --> 00:31:53,000
Yeah, interesting, interesting.

494
00:31:53,000 --> 00:32:00,000
Maybe shifting gives a little bit to stability as a company as an organization.

495
00:32:00,000 --> 00:32:04,000
I've heard it described as, you know, very, you know, variously an art studio.

496
00:32:04,000 --> 00:32:07,000
Kind of looks and feels a little bit like a research lab.

497
00:32:07,000 --> 00:32:12,000
Feels a little bit like a funder of things, a provider of GPUs and instances.

498
00:32:12,000 --> 00:32:14,000
How do you describe what it is?

499
00:32:14,000 --> 00:32:17,000
I mean, stability AI is a platform company.

500
00:32:17,000 --> 00:32:20,000
So we're trying to build a layer one for foundation model AI.

501
00:32:20,000 --> 00:32:22,000
And we think the future will be open source on this.

502
00:32:22,000 --> 00:32:27,000
So our research lab is, you know, researchers who have loads of freedom.

503
00:32:27,000 --> 00:32:30,000
And they can, in their contracts, open source and they create.

504
00:32:30,000 --> 00:32:33,000
And there's a revenue share for when we run the models on the API.

505
00:32:33,000 --> 00:32:36,000
Even if the researchers don't work at stability, they still get cut checks.

506
00:32:36,000 --> 00:32:38,000
Which thing is very interesting when we're doing things.

507
00:32:38,000 --> 00:32:39,000
Yeah.

508
00:32:39,000 --> 00:32:43,000
We've got a product team that takes the open source stuff, just like anyone can,

509
00:32:43,000 --> 00:32:45,000
and productizes it into things like Dream Studio.

510
00:32:45,000 --> 00:32:49,000
We have Dream Studio Pro coming up, which is a full enterprise level piece of software

511
00:32:49,000 --> 00:32:54,000
with like 3D keyframing, animation, video, audio, everything.

512
00:32:54,000 --> 00:32:57,000
We're going forward to deploy a team whereby for our top customers,

513
00:32:57,000 --> 00:33:00,000
with the most content that we've transformed in foundation models,

514
00:33:00,000 --> 00:33:03,000
we're basically embedding teams inside there and saying,

515
00:33:03,000 --> 00:33:05,000
you don't need to build a foundation model team.

516
00:33:05,000 --> 00:33:09,000
We're your team because we do all the modalities from the text to language to audio.

517
00:33:09,000 --> 00:33:11,000
I can add some of this super appealing to people that we've got infrastructure team

518
00:33:11,000 --> 00:33:15,000
that is supporting our five, six thousand to eight one hundred's

519
00:33:15,000 --> 00:33:21,000
and the infrastructure to scale API's to billions in support with Amazon and others as I am.

520
00:33:21,000 --> 00:33:25,000
Can you talk a little bit about some of the ways that you engage with enterprises?

521
00:33:25,000 --> 00:33:29,000
Like, what are the kinds of things that they want to help doing with these models?

522
00:33:29,000 --> 00:33:32,000
So, the pace of ML research is literally exponential,

523
00:33:32,000 --> 00:33:34,000
with a 23 month wobbling.

524
00:33:34,000 --> 00:33:35,000
It looked crazy.

525
00:33:35,000 --> 00:33:36,000
They can't keep on top of this.

526
00:33:36,000 --> 00:33:39,000
And there's very few people that publish papers on market.

527
00:33:39,000 --> 00:33:40,000
Yeah.

528
00:33:40,000 --> 00:33:41,000
Okay.

529
00:33:41,000 --> 00:33:43,000
It's always nice when you see a casual exponential.

530
00:33:43,000 --> 00:33:46,000
In AI to help with that.

531
00:33:46,000 --> 00:33:47,000
Yeah.

532
00:33:47,000 --> 00:33:48,000
Yeah.

533
00:33:48,000 --> 00:33:51,000
But like, when you look at this,

534
00:33:51,000 --> 00:33:53,000
they're realizing they need to be on top of this technology now.

535
00:33:53,000 --> 00:33:56,000
And they come to us as kind of almost consultants.

536
00:33:56,000 --> 00:33:57,000
They take a parentier type model.

537
00:33:57,000 --> 00:33:58,000
Yeah.

538
00:33:58,000 --> 00:34:00,000
Where we're like, we'll fine tune some models for you

539
00:34:00,000 --> 00:34:02,000
and we'll make them usable through Dream Studio.

540
00:34:02,000 --> 00:34:05,000
But you shouldn't train your own models now.

541
00:34:05,000 --> 00:34:07,000
Because the models aren't going to mature for another year.

542
00:34:07,000 --> 00:34:09,000
When that time comes, we will train the models for you.

543
00:34:09,000 --> 00:34:10,000
We will fine tune them for you.

544
00:34:10,000 --> 00:34:12,000
We will create custom models for you.

545
00:34:12,000 --> 00:34:15,000
That's our highest touch engagement with a couple of dozen entities.

546
00:34:15,000 --> 00:34:18,000
And when you're telling them, they shouldn't train models.

547
00:34:18,000 --> 00:34:20,000
Are you talking about from scratch from scratch?

548
00:34:20,000 --> 00:34:21,000
Sure.

549
00:34:21,000 --> 00:34:22,000
They should.

550
00:34:22,000 --> 00:34:23,000
Okay.

551
00:34:23,000 --> 00:34:24,000
They will be able to eventually.

552
00:34:24,000 --> 00:34:26,000
But right now, it's not a sensible thing to train a model from scratch.

553
00:34:26,000 --> 00:34:27,000
Yeah.

554
00:34:27,000 --> 00:34:31,000
I stable the fusion to 200,000 a 100 hours to like 600k you spent on it?

555
00:34:31,000 --> 00:34:32,000
Yeah, 600k.

556
00:34:32,000 --> 00:34:34,000
Well, we actually spent less because of our discounts.

557
00:34:34,000 --> 00:34:36,000
But I can't say what I discount.

558
00:34:36,000 --> 00:34:37,000
You know what I mean?

559
00:34:37,000 --> 00:34:38,000
You can figure out the retail.

560
00:34:38,000 --> 00:34:39,000
Yeah, retail.

561
00:34:39,000 --> 00:34:42,000
Retail-style diffusion to about 800,000 hours.

562
00:34:42,000 --> 00:34:44,000
Retail-open clip.

563
00:34:44,000 --> 00:34:47,000
Because we had to make the clip model about $5 million.

564
00:34:47,000 --> 00:34:49,000
So, you know, these things add up.

565
00:34:49,000 --> 00:34:50,000
Yeah.

566
00:34:50,000 --> 00:34:51,000
Quite a large bill.

567
00:34:51,000 --> 00:34:53,000
So, I think that when you kind of look at all of these,

568
00:34:53,000 --> 00:34:56,000
now's not the right time to do big trainings for big companies.

569
00:34:56,000 --> 00:35:01,000
Because again, the model architecture is just increasing on ridiculous rate.

570
00:35:01,000 --> 00:35:03,000
But there's going to level off.

571
00:35:03,000 --> 00:35:05,000
You can't keep improving forever.

572
00:35:05,000 --> 00:35:07,000
And then that's the right time to train up your own models.

573
00:35:07,000 --> 00:35:09,000
So, it'll be better than these fine-dune models.

574
00:35:09,000 --> 00:35:11,000
But then you have models with multimodalities.

575
00:35:11,000 --> 00:35:14,000
You know, this is part of the dear reason we've kind of partnered with SageMaker.

576
00:35:14,000 --> 00:35:17,000
Because people need to get used to this technology now.

577
00:35:17,000 --> 00:35:19,000
And they'll have all these different primitives,

578
00:35:19,000 --> 00:35:22,000
these different models they can mix and match to create brand new things going forward.

579
00:35:22,000 --> 00:35:24,000
And SageMaker makes it kind of easy to do that.

580
00:35:24,000 --> 00:35:26,000
And it makes it easy to address the tail.

581
00:35:26,000 --> 00:35:29,000
Because, apart from the top couple of dozen companies,

582
00:35:29,000 --> 00:35:32,000
we just want to have a SaaS solution for everyone else to be able to access,

583
00:35:32,000 --> 00:35:34,000
use and modify these models.

584
00:35:34,000 --> 00:35:38,000
And following up a little bit on the SageMaker and the AWS announcement,

585
00:35:38,000 --> 00:35:42,000
kind of red as, you know, you selected AWS.

586
00:35:42,000 --> 00:35:46,000
From my understanding, you've been using AWS for some extent all along.

587
00:35:46,000 --> 00:35:48,000
Yeah, so AWS built the core cluster.

588
00:35:48,000 --> 00:35:50,000
And now, you know, we reached this point.

589
00:35:50,000 --> 00:35:52,000
So, it was originally a 4,100 cluster,

590
00:35:52,000 --> 00:35:54,000
which on the public top 500 listed,

591
00:35:54,000 --> 00:35:58,000
about number 10, super computer in the world, which is kind of insane.

592
00:35:58,000 --> 00:36:00,000
So, AWS did a great job building that.

593
00:36:00,000 --> 00:36:02,000
But then we have to decide what's next,

594
00:36:02,000 --> 00:36:04,000
like the managing the resilience through some of these other things.

595
00:36:04,000 --> 00:36:06,000
We build our next cluster.

596
00:36:06,000 --> 00:36:08,000
Amazon came and they said,

597
00:36:08,000 --> 00:36:11,000
let's use the SageMaker service to offer a high level of resilience optimization.

598
00:36:11,000 --> 00:36:13,000
So the SageMaker crew, for example,

599
00:36:13,000 --> 00:36:17,000
took our language model, GP Neo X,

600
00:36:17,000 --> 00:36:19,000
again, 20 million downloads of this family.

601
00:36:19,000 --> 00:36:20,000
Yeah.

602
00:36:20,000 --> 00:36:23,000
They went and took the efficiency of a 512-A 100 trading

603
00:36:23,000 --> 00:36:26,000
from 103 telephlops with GPU to 163,

604
00:36:26,000 --> 00:36:29,000
by optimizing it for Amazon, EFA, Fabric,

605
00:36:29,000 --> 00:36:32,000
and pipeline powerism, and cost attention, and kind of all these things.

606
00:36:32,000 --> 00:36:34,000
And that was amazing things.

607
00:36:34,000 --> 00:36:36,000
So they're helping us optimize our entire stack,

608
00:36:36,000 --> 00:36:39,000
from inference to training, through to having resilience.

609
00:36:39,000 --> 00:36:41,000
So when GPUs fail, they come back up.

610
00:36:41,000 --> 00:36:44,000
And the final part of it was just how do you make this accessible?

611
00:36:44,000 --> 00:36:47,000
Through SageMaker and the services,

612
00:36:47,000 --> 00:36:49,000
the ecosystem they built around that.

613
00:36:49,000 --> 00:36:51,000
Now, we're going to make our models available on everything.

614
00:36:51,000 --> 00:36:52,000
Right.

615
00:36:52,000 --> 00:36:56,000
Right? So, like today, they became available on the MacBook M1,

616
00:36:56,000 --> 00:36:59,000
with native neural engine support, one of the first models ever to have that.

617
00:36:59,000 --> 00:37:01,000
You know, that's massive.

618
00:37:01,000 --> 00:37:02,000
We've got it working on Qualcomm.

619
00:37:02,000 --> 00:37:04,000
We're going to work on iPhones, all these things.

620
00:37:04,000 --> 00:37:05,000
But Amazon is a really great partner,

621
00:37:05,000 --> 00:37:07,000
because they're infrastructure players,

622
00:37:07,000 --> 00:37:09,000
one of the biggest cloud drivers in the world.

623
00:37:09,000 --> 00:37:11,000
And so that's why we kind of picked them as our preferred partner.

624
00:37:11,000 --> 00:37:13,000
Also, you know, we're super grateful in that.

625
00:37:13,000 --> 00:37:16,000
We wouldn't be here if they hadn't philosophically an enormous cluster,

626
00:37:16,000 --> 00:37:17,000
and really believed in us.

627
00:37:17,000 --> 00:37:19,000
Because we're only a 13-month-old startup.

628
00:37:19,000 --> 00:37:20,000
Yeah.

629
00:37:20,000 --> 00:37:21,000
So everything's been in the cloud the entire time?

630
00:37:21,000 --> 00:37:22,000
All the entire time.

631
00:37:22,000 --> 00:37:23,000
Yeah.

632
00:37:23,000 --> 00:37:27,000
So we had a machine learning ops team of four people,

633
00:37:27,000 --> 00:37:29,000
managing 4,000 A-100s.

634
00:37:29,000 --> 00:37:30,000
Wow.

635
00:37:30,000 --> 00:37:32,000
Now we're up to nearly 6,000.

636
00:37:32,000 --> 00:37:36,000
Was that team managing that cluster, you know,

637
00:37:36,000 --> 00:37:38,000
kind of bare metal with your own tooling,

638
00:37:38,000 --> 00:37:41,000
or how much of the Amazon tooling have you?

639
00:37:41,000 --> 00:37:43,000
So it was easy, too.

640
00:37:43,000 --> 00:37:46,000
And then the Amazon had a system called parallel cluster,

641
00:37:46,000 --> 00:37:48,000
with Slurm, that was used to kind of manage it.

642
00:37:48,000 --> 00:37:51,000
And so we've been working for the last four, five, six months,

643
00:37:51,000 --> 00:37:52,000
just constantly improving it together.

644
00:37:52,000 --> 00:37:53,000
And again, it's open source.

645
00:37:53,000 --> 00:37:54,000
Yeah.

646
00:37:54,000 --> 00:37:55,000
If you go to the stability AI GitHub,

647
00:37:55,000 --> 00:37:58,000
you can literally download all our configurations

648
00:37:58,000 --> 00:38:00,000
to run your own parallel cluster on that.

649
00:38:00,000 --> 00:38:01,000
Okay.

650
00:38:01,000 --> 00:38:02,000
And again, this is part of what we really like.

651
00:38:02,000 --> 00:38:04,000
The fact that the stack is open source.

652
00:38:04,000 --> 00:38:06,000
And anyone can take it and they can build their own clusters.

653
00:38:06,000 --> 00:38:09,000
Maybe not quite to the size that we did,

654
00:38:09,000 --> 00:38:11,000
unless you're feeling really punchy.

655
00:38:11,000 --> 00:38:12,000
Yeah.

656
00:38:12,000 --> 00:38:14,000
But still, I think these knowledge and these things should be shared.

657
00:38:14,000 --> 00:38:18,000
Because you find that large model training isn't really an art.

658
00:38:18,000 --> 00:38:19,000
Is it really science?

659
00:38:19,000 --> 00:38:20,000
It's more of an art.

660
00:38:20,000 --> 00:38:22,000
Like one of the most interesting reads you can do

661
00:38:22,000 --> 00:38:26,000
is the Facebook OPT175 logbook for the 175 billion parameter model.

662
00:38:26,000 --> 00:38:28,000
They just try stuff and it often fails.

663
00:38:28,000 --> 00:38:30,000
And there's the occasional weird thing,

664
00:38:30,000 --> 00:38:33,000
like really was the Azure kind of customer support

665
00:38:33,000 --> 00:38:35,000
on the 23rd of December.

666
00:38:35,000 --> 00:38:37,000
It deleted the entire cluster.

667
00:38:37,000 --> 00:38:40,000
And you're like, man, I feel for you guys.

668
00:38:40,000 --> 00:38:41,000
It's kind of that.

669
00:38:41,000 --> 00:38:44,000
But like I said, this is not just an easy click and play kind of thing.

670
00:38:44,000 --> 00:38:46,000
These models are difficult to train.

671
00:38:46,000 --> 00:38:47,000
Yeah.

672
00:38:47,000 --> 00:38:49,000
The smallest hardware thing can throw it out.

673
00:38:49,000 --> 00:38:50,000
They can be just weird stuff.

674
00:38:50,000 --> 00:38:53,000
We're making it up and figuring out as we go along.

675
00:38:53,000 --> 00:38:55,000
Because remember, transformer architectures

676
00:38:55,000 --> 00:38:57,000
are literally only five years old.

677
00:38:57,000 --> 00:38:58,000
Yeah.

678
00:38:58,000 --> 00:39:02,000
It's thinking about open source and that direction broadly

679
00:39:02,000 --> 00:39:03,000
that the company is taking.

680
00:39:03,000 --> 00:39:08,000
One of the challenges that comes up in open source

681
00:39:08,000 --> 00:39:11,000
as it matures is this idea of governance.

682
00:39:11,000 --> 00:39:14,000
How do you think about, maybe it's early,

683
00:39:14,000 --> 00:39:17,000
talking about governing a community that's just months old.

684
00:39:17,000 --> 00:39:21,000
But do you have thoughts on how the community,

685
00:39:21,000 --> 00:39:23,000
you know, governs itself over time?

686
00:39:23,000 --> 00:39:24,000
Yeah.

687
00:39:24,000 --> 00:39:25,000
So again, it's complicated one, right?

688
00:39:25,000 --> 00:39:26,000
AI governance.

689
00:39:26,000 --> 00:39:27,000
And does it policy led?

690
00:39:27,000 --> 00:39:28,000
Is it community led?

691
00:39:28,000 --> 00:39:30,000
Who are the voices at the table?

692
00:39:30,000 --> 00:39:31,000
Because there's some important things.

693
00:39:31,000 --> 00:39:32,000
This is such powerful technology.

694
00:39:32,000 --> 00:39:35,000
It's going to be essential, I believe, to the future of humanity.

695
00:39:35,000 --> 00:39:37,000
So like, for example, Luther AI is two and a bit years old.

696
00:39:37,000 --> 00:39:39,000
That's our language model community.

697
00:39:39,000 --> 00:39:42,000
15,000 people and 12 of us.

698
00:39:42,000 --> 00:39:44,000
We're kind of incubating at the moment.

699
00:39:44,000 --> 00:39:47,000
We're going to spin it out into its own separate 5 and 1 C3.

700
00:39:47,000 --> 00:39:49,000
Because it shouldn't be else influencing the direction

701
00:39:49,000 --> 00:39:51,000
of open source large language models, right?

702
00:39:51,000 --> 00:39:53,000
It should be a collective effort.

703
00:39:53,000 --> 00:39:55,000
But now we're really going through the governance thing

704
00:39:55,000 --> 00:39:56,000
and looking at different examples.

705
00:39:56,000 --> 00:39:58,000
And the Linux Foundation is an excellent example of that.

706
00:39:58,000 --> 00:40:01,000
So PyTorch has just been given to the Linux Foundation.

707
00:40:01,000 --> 00:40:04,000
And so Rin talks with them, a whole bunch of others to say,

708
00:40:04,000 --> 00:40:05,000
what are best practices here?

709
00:40:05,000 --> 00:40:07,000
And what should really look like given the power

710
00:40:07,000 --> 00:40:09,000
of these, some of the decisions need to make about that?

711
00:40:09,000 --> 00:40:12,000
As stability itself, we're setting up a subsidiaries in every country

712
00:40:12,000 --> 00:40:15,000
such that, first off, a temp set of equity in those

713
00:40:15,000 --> 00:40:17,000
goes to the kids using our tablets.

714
00:40:17,000 --> 00:40:19,000
Because I think they should influence it.

715
00:40:19,000 --> 00:40:20,000
Because that's the next generation.

716
00:40:20,000 --> 00:40:22,000
And this AI will be important to them.

717
00:40:22,000 --> 00:40:24,000
But then we want this to be independent entities that run the AI

718
00:40:24,000 --> 00:40:29,000
for India or Vietnam or kind of Malawi, et cetera.

719
00:40:29,000 --> 00:40:31,000
Because we need to train up a next generation of people

720
00:40:31,000 --> 00:40:33,000
to make those decisions for their own country.

721
00:40:33,000 --> 00:40:35,000
Because right now, what we have is a situation

722
00:40:35,000 --> 00:40:38,000
where you've got a few people in San Francisco

723
00:40:38,000 --> 00:40:40,000
making decisions in the most powerful infrastructure

724
00:40:40,000 --> 00:40:42,000
of the world for everyone.

725
00:40:42,000 --> 00:40:44,000
Because that's not to deny ourselves.

726
00:40:44,000 --> 00:40:46,000
This AI is infrastructure.

727
00:40:46,000 --> 00:40:48,000
It's essential for where we're going to go.

728
00:40:48,000 --> 00:40:51,000
And it shouldn't be controlled by any person or entity.

729
00:40:51,000 --> 00:40:53,000
Like, I'm very supportive of the whole ecosystem.

730
00:40:53,000 --> 00:40:56,000
The one time I, by almost the very direct.

731
00:40:56,000 --> 00:40:58,000
So I spoke out against open AI.

732
00:40:58,000 --> 00:41:01,000
Because for Dali too, they banned Ukrainians from using it.

733
00:41:01,000 --> 00:41:04,000
They removed any Ukrainian entities from that as well.

734
00:41:04,000 --> 00:41:08,000
And this is doing the time when they're being oppressed.

735
00:41:08,000 --> 00:41:12,000
I said, basically, you have excluded and removed and deleted

736
00:41:12,000 --> 00:41:13,000
and oppressed people.

737
00:41:13,000 --> 00:41:15,000
And that is ethically and morally wrong.

738
00:41:15,000 --> 00:41:18,000
But is their prerogative as a private company?

739
00:41:18,000 --> 00:41:21,000
And if it wasn't for us, there would be no alternative.

740
00:41:21,000 --> 00:41:24,000
And so I literally took Ukrainian developers,

741
00:41:24,000 --> 00:41:27,000
and houses were destroyed, and brought them to the UK.

742
00:41:27,000 --> 00:41:29,000
And so this is part of the thing as well.

743
00:41:29,000 --> 00:41:31,000
If you have control of this artificial intelligence

744
00:41:31,000 --> 00:41:34,000
given to an unregulated entity like these big companies,

745
00:41:34,000 --> 00:41:37,000
they can't help themselves but behave in certain ways

746
00:41:37,000 --> 00:41:38,000
and they can't release it.

747
00:41:38,000 --> 00:41:40,000
More than that, they tend to optimize.

748
00:41:40,000 --> 00:41:42,000
So I did a lot of counter-extremism work,

749
00:41:42,000 --> 00:41:43,000
advising multiple governments.

750
00:41:43,000 --> 00:41:45,000
The YouTube algorithm got hijacked by extremists

751
00:41:45,000 --> 00:41:47,000
because the most engaging content was extreme.

752
00:41:47,000 --> 00:41:49,000
Again, that's not YouTube's fault.

753
00:41:49,000 --> 00:41:51,000
That's full of great people.

754
00:41:51,000 --> 00:41:53,000
Add driven AI companies.

755
00:41:53,000 --> 00:41:55,000
They will use this technology to create the most amazing

756
00:41:55,000 --> 00:41:56,000
manipulative ads.

757
00:41:56,000 --> 00:41:58,000
Against this not their fault, it's kind of what they are.

758
00:41:58,000 --> 00:42:00,000
So regulation needs to come in appropriately.

759
00:42:00,000 --> 00:42:01,000
Governance needs to come in appropriately.

760
00:42:01,000 --> 00:42:04,000
But we need to educate and widen the discussion on this.

761
00:42:04,000 --> 00:42:06,000
And the only way to do that is open source.

762
00:42:06,000 --> 00:42:08,000
Otherwise, it will never happen.

763
00:42:08,000 --> 00:42:11,000
And so you will have AI basically being a colonial tool

764
00:42:11,000 --> 00:42:14,000
in some ways with very Western norms.

765
00:42:14,000 --> 00:42:16,000
When this is essential infrastructure,

766
00:42:16,000 --> 00:42:18,000
like I said, I believe for everyone.

767
00:42:18,000 --> 00:42:22,000
And I think the common retort to that

768
00:42:22,000 --> 00:42:25,000
is it needs to be controlled because it's so powerful,

769
00:42:25,000 --> 00:42:26,000
so dangerous.

770
00:42:26,000 --> 00:42:28,000
Yeah, so who are you to control it?

771
00:42:28,000 --> 00:42:29,000
I mean, this is a thing.

772
00:42:29,000 --> 00:42:31,000
Like, I've had it, it likes into a nuclear weapon.

773
00:42:31,000 --> 00:42:34,000
I'm like, it's a nuclear weapon that can allow humans

774
00:42:34,000 --> 00:42:36,000
to create visually.

775
00:42:36,000 --> 00:42:37,000
And so you're restricting it.

776
00:42:37,000 --> 00:42:38,000
And when you get, come and answer a question.

777
00:42:38,000 --> 00:42:40,000
Like, I've asked this, I've never had a question.

778
00:42:40,000 --> 00:42:43,000
Why don't you want Indians to have this for Africans?

779
00:42:43,000 --> 00:42:45,000
And the only answer is, because they need more education,

780
00:42:45,000 --> 00:42:47,000
so educate them more.

781
00:42:47,000 --> 00:42:49,000
Because they can't use it responsibly,

782
00:42:49,000 --> 00:42:51,000
and you can, it's racist.

783
00:42:51,000 --> 00:42:54,000
Like I think fundamentally, if you think about digital divide,

784
00:42:54,000 --> 00:42:56,000
we've seen this with technology being restricted

785
00:42:56,000 --> 00:42:59,000
from minority groups and from the rest of the world frequently.

786
00:42:59,000 --> 00:43:01,000
It's fundamentally racist because we think we know better

787
00:43:01,000 --> 00:43:02,000
in the West.

788
00:43:02,000 --> 00:43:04,000
When it's reality, we don't, because people can take this

789
00:43:04,000 --> 00:43:05,000
and extend it.

790
00:43:05,000 --> 00:43:07,000
And people are generally good.

791
00:43:07,000 --> 00:43:09,000
People are not bad, and if people are bad,

792
00:43:09,000 --> 00:43:11,000
as a society, we build systems to regulate that.

793
00:43:11,000 --> 00:43:13,000
So even if they create deep fakes,

794
00:43:13,000 --> 00:43:16,000
we build our social networks and those type of curation mechanisms.

795
00:43:16,000 --> 00:43:19,000
You know, we build authenticity schemes like content

796
00:43:19,000 --> 00:43:21,000
or authenticity.org that we back.

797
00:43:21,000 --> 00:43:23,000
That sounds like the core of your answer

798
00:43:23,000 --> 00:43:25,000
is that the ecosystem will solve the problem.

799
00:43:25,000 --> 00:43:26,000
The bad actors come in.

800
00:43:26,000 --> 00:43:29,000
They use these tools to cause whatever

801
00:43:29,000 --> 00:43:32,000
having their cause, and then, you know, we'll find faces.

802
00:43:32,000 --> 00:43:34,000
The bad actors have the tools already.

803
00:43:34,000 --> 00:43:37,000
They have tens of thousands of A-100s and people.

804
00:43:37,000 --> 00:43:39,000
I mean, since you're the proof point of this, right?

805
00:43:39,000 --> 00:43:42,000
Open AI was keeping Dolly closed behind, you know,

806
00:43:42,000 --> 00:43:44,000
APIs and wait lists and things like that.

807
00:43:44,000 --> 00:43:46,000
And, you know, you came up, I don't know where,

808
00:43:46,000 --> 00:43:48,000
and released something.

809
00:43:48,000 --> 00:43:50,000
And look, 4chan has had this technology for three months.

810
00:43:50,000 --> 00:43:52,000
What if they created nothing, right?

811
00:43:52,000 --> 00:43:53,000
Yeah.

812
00:43:53,000 --> 00:43:55,000
You know, like this isn't going to topple humanity,

813
00:43:55,000 --> 00:43:56,000
and have more and more people know about it,

814
00:43:56,000 --> 00:43:57,000
so we can bring this discussion.

815
00:43:57,000 --> 00:43:59,000
You know, we took a lot of flak.

816
00:43:59,000 --> 00:44:00,000
We had a lot of benefits.

817
00:44:00,000 --> 00:44:01,000
Yeah.

818
00:44:01,000 --> 00:44:02,000
We brought this discussion into the open,

819
00:44:02,000 --> 00:44:04,000
into policy and other fields as well.

820
00:44:04,000 --> 00:44:06,000
Like, again, it's my hope that now,

821
00:44:06,000 --> 00:44:07,000
we access a forcing function.

822
00:44:07,000 --> 00:44:09,000
So I reckon Dolly III will be open sourced.

823
00:44:09,000 --> 00:44:11,000
You know, it's like the open source whisper.

824
00:44:11,000 --> 00:44:12,000
And I think this will be fantastic.

825
00:44:12,000 --> 00:44:13,000
Let's bring it out into the open,

826
00:44:13,000 --> 00:44:15,000
because, again, this is foundational infrastructure

827
00:44:15,000 --> 00:44:17,000
for extending our abilities.

828
00:44:17,000 --> 00:44:18,000
It should not be closed.

829
00:44:18,000 --> 00:44:19,000
Yeah.

830
00:44:19,000 --> 00:44:22,000
But I don't believe that it should be free forever,

831
00:44:22,000 --> 00:44:25,000
and, like, even, actually, it's not open source,

832
00:44:25,000 --> 00:44:28,000
because it doesn't confirm with rule zero of open source

833
00:44:28,000 --> 00:44:29,000
in a pure open source way.

834
00:44:29,000 --> 00:44:31,000
The creative analysis is not,

835
00:44:31,000 --> 00:44:33,000
because we say you must use it ethically.

836
00:44:33,000 --> 00:44:36,000
You know, do we have to move it to open source?

837
00:44:36,000 --> 00:44:37,000
Yes.

838
00:44:37,000 --> 00:44:39,000
Under CC by a five, or MIT license,

839
00:44:39,000 --> 00:44:40,000
just like our other models,

840
00:44:40,000 --> 00:44:42,000
like our Korean language model,

841
00:44:42,000 --> 00:44:43,000
the polygote one from the Luther

842
00:44:43,000 --> 00:44:45,000
or open-clapal things like that.

843
00:44:45,000 --> 00:44:46,000
Yeah.

844
00:44:46,000 --> 00:44:47,000
But, again, this needs to be an open discussion, I think,

845
00:44:47,000 --> 00:44:49,000
rather than who is deciding it.

846
00:44:49,000 --> 00:44:50,000
I don't know.

847
00:44:50,000 --> 00:44:51,000
Right.

848
00:44:51,000 --> 00:44:52,000
If regulators want to come and regulate it,

849
00:44:52,000 --> 00:44:54,000
again, that's a democratic decision.

850
00:44:54,000 --> 00:44:56,000
And so I'm a big supporter of democracy

851
00:44:56,000 --> 00:44:57,000
and, kind of, these things.

852
00:44:57,000 --> 00:44:59,000
But let's use our institutions on processes

853
00:44:59,000 --> 00:45:01,000
rather than trying to make these decisions ourselves

854
00:45:01,000 --> 00:45:02,000
in closed rooms.

855
00:45:02,000 --> 00:45:03,000
Mm-hmm.

856
00:45:03,000 --> 00:45:04,000
Awesome.

857
00:45:04,000 --> 00:45:05,000
Awesome.

858
00:45:05,000 --> 00:45:07,000
Well, Emma, thanks so much for taking the time to chat.

859
00:45:07,000 --> 00:45:08,000
It's been wonderful.

860
00:45:08,000 --> 00:45:10,000
Speaking with you, I'm learning a bit more about what you have to.

861
00:45:10,000 --> 00:45:12,000
It's a pleasure, and I hope you have a seat as well.

862
00:45:12,000 --> 00:45:13,000
Nearly done.

863
00:45:13,000 --> 00:45:14,000
Nearly done.

864
00:45:14,000 --> 00:45:15,000
Thanks so much.

865
00:45:15,000 --> 00:45:27,000
Take care.

