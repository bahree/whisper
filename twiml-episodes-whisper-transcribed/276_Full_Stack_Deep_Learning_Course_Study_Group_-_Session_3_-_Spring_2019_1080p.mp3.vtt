WEBVTT

00:00.000 --> 00:21.800
So, other one, it's another full stack, the learning static group, and we have more

00:21.800 --> 00:28.040
presentations for the day, so we have, honestly, we're ready.

00:28.040 --> 00:37.560
We have Avvynier's presentation on the model inter-operability and overview, and also

00:37.560 --> 00:50.360
Sanyam's and Akash's presentation on some OPPOSOR still, JVN, IO, so it's cool.

00:50.360 --> 00:54.080
Let me open my chat so I can also see what's in it.

00:54.080 --> 00:59.320
I'll keep an eye on you in the chat, okay, that's great, thank you.

00:59.320 --> 01:03.040
So let's maybe start with the, so we have about half an hour for each presentation, I think

01:03.040 --> 01:04.840
that should be fine.

01:04.840 --> 01:14.800
So let's maybe start with the full stack deep learning thing, so for today we had quite

01:14.800 --> 01:22.000
challenging agenda, a couple of lessons, 7, 8, 9, 10, 11, but actually 7, let me show

01:22.000 --> 01:33.640
a look, so 7, yeah, so 7 is about the machine learning teams, and that was about half an

01:33.640 --> 01:38.880
hour of video, less than 8, it's not available, so that's easy.

01:38.880 --> 01:44.080
Less than 9 was optional, because that's a guest speaker and that's about 45 minutes

01:44.080 --> 01:45.080
of video.

01:45.080 --> 01:52.200
And then less than 10, it's about troubleshooting, that's about an hour and a half.

01:52.200 --> 01:58.520
And then we said we're going to try to attempt the lesson 11, which is labs, but we said

01:58.520 --> 02:04.200
not all of them, but 6 and, 6 and 7.

02:04.200 --> 02:14.040
So as you remember, labs were in the Geek app somewhere, so the labs were kind of

02:14.040 --> 02:19.920
splitting the four sessions, session 1, 2, so the third session is actually the lesson

02:19.920 --> 02:34.760
7 and 8, that was the plan anyway, so we can start, actually, so I can open slides,

02:34.760 --> 02:57.320
I'm not going to go through all the slides as last time, because there are too many of

02:57.320 --> 03:04.000
them, and I'm just going maybe to scroll, just ask questions or make some comments,

03:04.000 --> 03:11.200
or anything related to the course, what was interesting, what was fun, what was not clear,

03:11.200 --> 03:16.080
would try to talk about this.

03:16.080 --> 03:25.520
Yeah, so he was talking about how many companies are struggling to hire AI experts.

03:25.520 --> 03:33.920
I believe that was the, yeah, that was what he was talking about, so it's a good thing

03:33.920 --> 03:38.440
for people looking for jobs, but at the same time, I think it's not easy to find a job

03:38.440 --> 03:46.160
without experience, that's another kind of comment that is there around machine learning,

03:46.160 --> 03:47.480
finding jobs.

03:47.480 --> 03:56.720
You guys have some experience about finding jobs, so looking for people, for experts in AI.

03:56.720 --> 03:59.160
I have experience of getting rejected from Google.

03:59.160 --> 04:06.160
Oh, yeah, I kind of converse up from one point, people say that's a lot of, is it a big

04:06.160 --> 04:11.480
demand for AI experts, on the other hand, yeah, I also heard that, and I kind of seen

04:11.480 --> 04:19.240
my couple of cities themselves, it's, yeah, it's interesting.

04:19.240 --> 04:30.720
Yeah, so there are different roles, so in machine learning, data science, AI, that's

04:30.720 --> 04:38.000
I think there are a couple of roles we could split that into, so like DevOps, could be

04:38.000 --> 04:47.600
one role, so people who make the deployment of the machine learning models, the data engineers

04:47.600 --> 04:55.360
kind of might be a bit similar, but maybe more on a data, so they keep the look after

04:55.360 --> 05:01.520
the databases and stuff like that, then the machine learning engineer and other similar

05:01.520 --> 05:06.760
kind of like we have machine learning researcher and machine learning engineer, so that's kind

05:06.760 --> 05:13.880
of very similar, but different again, machine learning engineer is more about software, software

05:13.880 --> 05:19.840
engineering site and then machine learning researcher is more about the deep learning or machine

05:19.840 --> 05:26.480
learning algorithms models, making them better, and then the data scientist likes is like

05:26.480 --> 05:39.720
everything else, it's like very broad topic, yeah, I think that's, that's, yeah, so again,

05:39.720 --> 05:48.640
the DevOps, DevOps, data engineering, yeah, building data pipelines, machine learning engineer

05:48.640 --> 05:54.160
or trying to deploy, research to try and better models, yeah, data science is everything

05:54.160 --> 05:58.720
else, so data scientists could be someone who works with Excel, it could be someone who

05:58.720 --> 06:03.680
works with TensorFlow, yeah, it's just to check the description of the job, I think that's

06:03.680 --> 06:08.760
what it's trying to sell as well, I think it's also worth mentioning that many startups

06:08.760 --> 06:14.080
especially use these terms very interchangeably, so someone might be hiring an ML engineer

06:14.080 --> 06:18.440
but they might pitch it as a data scientist position, I've seen that for quite a bit of

06:18.440 --> 06:40.160
course, yeah, yeah, and I've seen a lot of, like, if you just type, when you try to open

06:40.160 --> 06:52.360
the machine learning, and just, yeah, those type of kind of, you see those allow this kind

06:52.360 --> 06:58.080
of like, what's data science, what's computer science, what's math, and every time, every

06:58.080 --> 07:02.520
time you look at it, they are different, so there is no like common understanding, some

07:02.520 --> 07:09.400
of them like very interested in complex, some of them are more like basic, like, like

07:09.400 --> 07:14.680
the data scientist is someone who knows how to work with software, who know math, and

07:14.680 --> 07:19.520
who is good at data communication, and that's the true data scientist, but then, again,

07:19.520 --> 07:28.280
I've sold so many of those, and there is just not common understanding of what that is,

07:28.280 --> 07:34.640
it's, yeah, so this one's different, this one's got data science on the right side,

07:34.640 --> 07:43.320
and IT skills, business skills, there's just no, yeah, this one, yeah, this one's like

07:43.320 --> 07:50.040
again, data science is like, I'm growth term, and then you have the AI, very growth field,

07:50.040 --> 07:55.560
and then machine learning, and then deep learning as a subset of machine learning, so, yeah,

07:55.560 --> 08:01.720
I think this is fun, but we just need to check the job description, and some companies

08:01.720 --> 08:08.360
maybe has got more specific descriptions, some companies who have like more growth,

08:08.360 --> 08:12.320
maybe they don't know themselves, or they don't know what they want to do, so just someone

08:12.320 --> 08:17.120
is mentioning the chat, that they found the working of data kind was a great deal to

08:17.120 --> 08:22.480
recently, but like popular projects, then what to talk about, and also very little, it's

08:22.480 --> 08:31.480
difficult to join the team, even as a volunteer, yeah, yeah, data kind, I'm afraid about

08:31.480 --> 08:41.360
this, so, it's not, I'm not sure, I think it's an NGO, only a few are supposed to, yeah,

08:41.360 --> 08:48.240
it's a non-profit, and they basically data scientists and developers work for free,

08:48.240 --> 09:00.000
and they have a small, small team of project managers, and that basically organize work,

09:00.000 --> 09:10.600
I think on this slack, I saw there was a call for volunteers to help judgegoogle.org challenge,

09:10.600 --> 09:15.760
I think that was a great experience, I think a few people from here were working there,

09:15.760 --> 09:24.040
but after that I started working with a local data kind in Washington, DC, and then once

09:24.040 --> 09:29.520
I started working on the project, because they're kind of open, it's great that you can

09:29.520 --> 09:35.000
talk about the project, and the data kind has a good reputation, so it was, it was a good

09:35.000 --> 09:43.440
way of, you know, presenting skills. So you can apply for like, yeah, they, what I found

09:43.440 --> 09:50.600
is, I registered online with them, but I never got a call for any project through registration

09:50.600 --> 10:02.280
online, but because I was on two slacks, one for Google ML challenge, and then it was

10:02.280 --> 10:09.040
for Washington, DC chapter, periodically they post different projects, and the skill

10:09.040 --> 10:21.200
match, you know, I think it was cool to accept it. Oh yeah, nice one, okay, I'm going

10:21.200 --> 10:23.120
to do it now, but, and the other one you mentioned, there are a couple of different places,

10:23.120 --> 10:27.360
there was a slack, you saw it? Oh yeah, so basically they have, they have a bunch of local

10:27.360 --> 10:37.280
chapters, data kind of local chapters, okay. Yeah. Okay, yeah, the other one that was

10:37.280 --> 10:44.880
hard about this was the, I think the Havan in India, I said, I'm not sure, maybe in Bangalore.

10:44.880 --> 10:49.600
Yeah, I think the one in Bangalore, yeah. Yeah, I think they had something, let's, let's

10:49.600 --> 10:58.200
have a look. Yeah, UK, Bangalore, India, Washington, DC, Singapore, San Francisco, yeah.

10:58.200 --> 11:05.480
So, yeah, so my, my advice would be, I mean, just register, like, didn't have, but like

11:05.480 --> 11:10.920
because I started, I was able to start on one project, then getting on slack, like,

11:10.920 --> 11:17.320
then people start sharing opportunities. Yeah, that was nice, that was useful. Can you

11:17.320 --> 11:25.880
do that remotely? Yeah, so local chapters, they usually, they want local people, but a lot

11:25.880 --> 11:33.640
of times they post remote projects. Okay, nice. If I, if I hear, if anybody hears about

11:33.640 --> 11:39.800
another call for volunteers, like outpost, yeah. That would be great. Another one I've heard

11:39.800 --> 11:52.920
was the fellowship. Yeah. I don't see this website working. I've also heard about inside

11:52.920 --> 12:02.360
fellows program. I think it's it's along similar lines. So I have some experience with inside

12:02.360 --> 12:10.520
fellow fellowship. I actually got accepted there to start last January, but I decided against

12:10.520 --> 12:17.640
joining just at work. They had, like, they had a lot of interesting projects starting and I was

12:17.640 --> 12:23.880
basically could do the same at work and, you know, get paid. But inside, it's an interesting,

12:23.880 --> 12:29.960
it's basically, they don't have any regular classes, but you do research like with them,

12:29.960 --> 12:37.080
and they don't, they sort of like lightly kind of supervise you, but they help you, they help

12:37.080 --> 12:44.200
you sell yourself. And then they, and then they work as, as you, as a job placement, basically,

12:44.200 --> 12:49.960
they get like some, some money from the company if you get once you get hired. So it's completely

12:49.960 --> 12:56.760
free. Interesting. They have two programs like that. So one is fast AI, not fast, one is inside

12:56.760 --> 13:08.920
fellowship, and another is data, the data incubator. But inside, I actually see a lot of people,

13:09.960 --> 13:15.000
a lot of people went through, especially coming from PhD, a lot of people went through that program,

13:15.000 --> 13:21.160
like I see people who are working. Actually, a lot of people who some interviewed went through

13:21.160 --> 13:32.040
inside. Okay. Yeah, interesting. So they also fellowship, guys, I guess something similar.

13:32.600 --> 13:38.760
I think you have to apply for them and complete some sort of a challenge project to get accepted.

13:39.800 --> 13:45.960
And then you kind of work with them for free. Again, it's not paid, but I guess you can learn

13:45.960 --> 13:53.160
for like this September, October, like four months. And after that, and they have a lot of

13:53.960 --> 13:59.880
different locations around the world. So because they don't accept remotes, so we have to be

13:59.880 --> 14:05.880
in one of those positions. And then I think they help you to find a job or something like that. So

14:05.880 --> 14:17.000
that it's again, for four months, full time, do something for them and learn a lot, but yeah. Okay, nice.

14:22.520 --> 14:27.480
The other side of balance, through some scenarios, through Slack, was kind of Delta helpers.

14:27.480 --> 14:34.600
So it's not to find a job or a staff, but there's a lot of people who sit on Twitter. I'm okay to help

14:34.600 --> 14:44.760
with questions and stuff. And at least it's quite long. So I guess that's also a second way to kind

14:44.760 --> 14:51.800
of ask people questions, how to get into data science or obviously a data scientist's work

14:51.800 --> 15:06.520
at a day-to-day on its show. I don't know. Okay. Interesting. We've not seen you screened for sharing it.

15:13.000 --> 15:19.560
So I was showing the data helpers side. Data helpers.org. So there's like someone

15:19.560 --> 15:24.840
asking right now. Could you see? Yeah, good. Someone said on Twitter, hey, can you help

15:24.840 --> 15:28.920
put some questions in their science? And there's a lot of people said, yeah, I'm happy to help,

15:28.920 --> 15:35.640
since the list is quite long. People that said yes. And yeah, some of the research.

15:37.800 --> 15:40.920
We're asking some people about data science.

15:40.920 --> 15:53.320
Yeah. So, yeah. So they're not talking about skills. So like machine learning, skills wise,

15:53.320 --> 15:58.200
and software engineering. What do you need to know to kind of work in such a domain?

16:01.960 --> 16:09.400
So like they develop more mostly software engineering, data engineering is software engineering

16:09.400 --> 16:13.800
with some ML experience. And of course, the machine learning engineering.

16:14.600 --> 16:17.480
So again, machine learning class and software engineering skills.

16:18.520 --> 16:23.960
Machine learning research is something that Jeremy Howard teaches us what to do. So that's mostly

16:23.960 --> 16:29.240
the fast AI courses, I guess. And the data scientists have to be everything.

16:32.280 --> 16:38.920
And then they were talking about teams. And because they did some interviews about

16:38.920 --> 16:43.080
some companies about how to structure a team, but they didn't find any consensus anywhere to

16:43.080 --> 16:48.920
do it. So they're like everything is different. And they do it in different ways.

16:58.200 --> 17:02.040
Yeah, I think that was quite interesting. And on the guest lecture, the

17:02.040 --> 17:08.440
I forgot the person who was talking about the founder of the Sweden biases.

17:09.480 --> 17:14.680
They found actually that the improvements of the accuracy of the model

17:15.480 --> 17:20.120
is mostly coming from the first like two weeks of working in the model. After that,

17:20.120 --> 17:24.920
you're not going to get a lot better. So I thought that was quite interesting. And that's

17:24.920 --> 17:38.440
similar from the other alukas. Alukas was talking about this as well. And there was no slides for that.

17:40.040 --> 17:46.040
But he showed the same if I can find it. Yeah, I think that's exactly the same graphs.

17:46.040 --> 17:55.160
And that's maybe the bar 20. Okay, so I'm going to go through that. So what I found is that

17:56.440 --> 18:00.280
I think that's the one in the middle. The accuracy of the best model

18:01.720 --> 18:07.880
goes up a lot in the first like two weeks or even that's even like two days. And after that,

18:08.520 --> 18:13.960
it doesn't help. Even like I think the one on the right is like number of participating teams

18:13.960 --> 18:18.840
like on the Kaggle. It increases a lot, but the improvement is not getting better.

18:19.720 --> 18:26.120
So this is kind of like the sweet spot between accuracy and how much time you want to spend.

18:27.080 --> 18:29.640
I think they wanted to highlight that. I found it quite interesting.

18:31.240 --> 18:38.040
And of course in Kaggle, you're going to win. Even if it was just like 0.001%,

18:38.040 --> 18:43.960
you're in normal projects. It's not that important to get such an accuracy.

18:47.240 --> 18:52.200
As far as the managing ML teams, like any team, just kind of challenging the ML teams.

18:52.920 --> 18:53.960
Different I guess.

18:59.240 --> 19:04.680
Yeah, they're hiring. We're talking about more hiring like LinkedIn. And we talked about some other

19:04.680 --> 19:18.600
also nice recruiting that conferences this course. Like they talked about interviews a little bit.

19:19.400 --> 19:26.120
Some people still do the whiteboard. Some people ask it to do like part programming.

19:26.120 --> 19:35.480
They would give you some quizzes. I think what's quite interesting, I think, makes most sense these

19:35.480 --> 19:45.320
days. This for me is like to take home ML projects. Although this can be a lot of work from you.

19:46.120 --> 19:53.080
But at least it's kind of more like a more real life project. Because if you take that project home,

19:53.080 --> 19:59.080
you can Google, you can even find help from other people. This is how this works. Even if you work

19:59.080 --> 20:05.880
at the company, you're not going to do stuff on the whiteboard. You're going to use all the resources

20:05.880 --> 20:12.200
you can to solve that project. For me, that kind of like makes sense. But it's going to take

20:12.200 --> 20:17.320
a lot of time from you. It's like a week or maybe even more sometimes. So that depends.

20:17.320 --> 20:23.560
They were talking about how to prepare for interview.

20:30.440 --> 20:34.920
I think they were joking there's eggs on maybe the walls. I don't know. How many of the walls?

20:42.120 --> 20:46.200
Oh, there are some questions. I was watching those ideas some time ago for a

20:46.200 --> 20:53.160
god about that. Do you want to answer those questions? Why does there is no outblock in the

20:53.160 --> 21:00.520
ResNet architecture help with the vanishing gradient problem? Well, I think because the

21:00.520 --> 21:16.360
image of that. Oh, yeah. We can find some some ResNet to the ResNet.

21:18.760 --> 21:25.240
Yeah, I think that's the ResNet stuff. So I think in ResNet, we get this kind of blocks.

21:25.240 --> 21:32.360
Yeah, so compared to the kind of like a normal 34 layer plan layer network, we have those.

21:33.240 --> 21:39.160
They call the ResNet blocks when they have the skip connections. So they go through the

21:39.160 --> 21:48.760
convolution layers and then they get added the same signals signal as it goes into the layers.

21:48.760 --> 21:52.200
But without going through the layers, the kind of called the skip connection.

21:52.200 --> 21:58.600
So I believe those skip connections, they help you to avoid the vanishing gradient problem.

22:00.440 --> 22:02.280
That would be my answer to that question at least.

22:07.080 --> 22:10.200
I also have the following learning curve from the training on single batch,

22:11.000 --> 22:13.640
which in which of the following could be the course.

22:13.640 --> 22:24.440
And this is, oh, this is the error. Okay. So the error is going up and down, but it's still roughly

22:24.440 --> 22:29.080
about 50%. It's not improving. Sometimes it's improving. Sometimes you're getting worse.

22:30.120 --> 22:31.400
So what could be the problem? But,

22:32.920 --> 22:36.520
cheerful flybals, learning rate too low, learning rate too high,

22:36.520 --> 22:45.320
the American stability too big, too big a model. That's interesting. We have any idea?

22:47.960 --> 22:48.760
I'm not so sure.

22:55.720 --> 22:59.480
The learning rate is too low. Yeah.

22:59.480 --> 23:09.480
Maybe just if the learning rate is too low, you would be going like slowly to slow to your

23:11.000 --> 23:15.880
minimum. But in this case, with iterations, we're not really going

23:17.240 --> 23:21.160
for lower error rate. With the two high learning rate, they'll be going

23:21.160 --> 23:28.440
maybe quicker to the minimum, but then you will go, I think, to some higher values.

23:30.680 --> 23:36.360
The shuffle flybals. What do I mean by shuffle flybals? I'm not so sure. I'm with like a mixed

23:36.360 --> 23:46.440
flybals. I don't know. It'll be more than I don't think so. Maybe it's a learning rate, I think.

23:46.440 --> 23:55.640
Question number three. For each of the following prediction tasks,

23:55.640 --> 23:59.960
select the loss function that is best suited for. So the predict sale price of a house listed

23:59.960 --> 24:07.560
for a sale. And then we'll have four, five answers. Okay. That's also a nice one.

24:07.560 --> 24:16.680
So we have the mean squared error, we have categorical cross entropy, binary cross entropy,

24:16.680 --> 24:21.480
CTC loss and gannels. Okay, maybe I need to go through that slacker again.

24:26.760 --> 24:31.960
That was basically the lecture seven,

24:31.960 --> 24:41.400
lecture eight, and then lecture nine. It was a thing was interesting, but I'm going to treat

24:41.400 --> 24:50.760
this as the optional lecture. And then lecture nine, lecture eight was about troubleshooting.

24:50.760 --> 25:04.920
I don't have a lot of time, but I like this picture. So you throw like a lot of data in the model,

25:04.920 --> 25:10.600
I just trust, yeah. And you just trust that you're going to get right output. And if it's not

25:10.600 --> 25:18.440
right, just draw more data or just mix whatever the data will be. So it's interesting.

25:18.440 --> 25:20.440
Yeah.

25:28.680 --> 25:32.840
Yes, I again, a lot of times going to take for you to kind of debug your model.

25:38.680 --> 25:39.560
So there are a couple of,

25:39.560 --> 25:49.400
I'm just going quickly because just ask questions or comments from that lecture because we don't

25:49.400 --> 25:57.880
have that much time to go in detail. Actually, I think that was the, this is the answers for those

25:57.880 --> 26:03.000
questions, because this is the same graph, right? And they're talking here about the labels side

26:03.000 --> 26:09.560
of us. I guess the result is like this, that's going to be because of the shuffle labels.

26:10.520 --> 26:12.680
That must be the case. Okay, interesting.

26:16.120 --> 26:23.080
Yeah, they were talking how to find out if your model is performing poorly. And I like the idea,

26:25.160 --> 26:29.640
yeah, I think this is quite interesting graph. So if you have your learning rates set right,

26:29.640 --> 26:35.480
do your minimum. But if you learn your rate is too high, you're going to get quicker to your

26:35.480 --> 26:41.800
minimum, but it will not get as low as it should. But it's too high learning rate. It will go

26:43.800 --> 26:49.000
up, but if the learning rate is too low, it will get to low point, but it's going to be slow.

26:51.480 --> 26:54.120
But I think it was interesting for me was,

26:54.120 --> 27:07.080
was that like they said, we should start. Yeah, and this slide was quite interesting. So the

27:07.080 --> 27:14.600
common data set construction issues, so we have not enough data. So deep learning models, they need

27:14.600 --> 27:19.800
quite some data. So maybe if you can use transfer learning, maybe not that many data points,

27:19.800 --> 27:25.800
but if you start training from scratch, then you need quite some data. The class imbalances,

27:25.800 --> 27:31.160
so say if one class got like 500 images, another class got just one couple of images,

27:31.160 --> 27:37.880
that's going to be an issue. The noisy label. So I guess this is when your labels not necessarily

27:37.880 --> 27:43.640
are true for the picture. So say if a picture of a car and then on a picture, there's no car.

27:43.640 --> 27:49.960
And then when you're training or your test set is coming from different distributions.

27:49.960 --> 27:56.040
So I think he was given an example when you're training on a car like in an autonomous vehicle

27:56.040 --> 28:00.840
problem, when you're training pictures from like day pictures and then a test are like

28:00.840 --> 28:09.480
two degrees. So that could be a complete different start. Yeah, and it's difficult to

28:09.480 --> 28:13.800
troubleshoot because you might have issues with your data, you might have issues with your code,

28:13.800 --> 28:19.880
you might have issues with the math behind it. So it's not so easy. So what they

28:21.880 --> 28:27.240
what they recommend, and that's actually quite true, just start simple and then gradually

28:30.440 --> 28:35.480
and that's I was thinking about because what we what we've learned on the first day,

28:35.480 --> 28:43.400
we've learned is already quite quite complex models because by default Jeremy uses this like one

28:45.560 --> 28:51.400
one cycle policy, there's a lot of like the dropouts and weight decays and all that stuff.

28:52.440 --> 29:00.040
So I was thinking about that and maybe maybe the next step I'd like to to find out the simplest

29:00.040 --> 29:06.920
model and just to try and a very simple model. But the fast AI works and maybe fast AI is also

29:06.920 --> 29:16.840
a good start. Yeah, so that's that's kind of nice flow chart. Start simple, implement and debug,

29:16.840 --> 29:23.080
evaluate the model, choose hyper parameters, improve your data in the model, and if it is quite

29:23.080 --> 29:35.400
done, start simple, so choose like simple architecture. I'm just going to go very quickly with that

29:35.400 --> 29:39.480
just ask questions or comments from this lecture because I think there's a lot of slides.

29:40.680 --> 29:43.400
It's like 100 40 slides and we don't have that much time.

29:43.400 --> 29:55.960
Yeah, so that's again kind of like because Jeremy is using resident as a first model and

29:55.960 --> 30:01.400
that naturally they recommend something more simple like Linux. So there's different approaches

30:01.400 --> 30:20.600
to this and everything but something to think about. Yeah, but they give you like some default

30:20.600 --> 30:26.120
options for optimizers. For example, possibly other optimizers are the same as in fast AI. It's

30:26.120 --> 30:31.560
also animal optimizer. The learning rate three and like four, I think Jeremy used like three

30:31.560 --> 30:37.240
and like three or something like that. So it's close. Activation is radio, so that's the same with

30:37.240 --> 30:49.800
Jeremy's using. He or had initiation was also the same. Regularization, they suggest to start with

30:49.800 --> 30:59.720
none. I think when the fast AI we get some, I'm not so sure. That's more about

31:05.400 --> 31:10.120
also they recommend to start with a small trading set about 10,000 examples.

31:11.960 --> 31:17.640
And there's a fixed number of objects classes and create as a simpler synthetic trading set.

31:17.640 --> 31:22.840
I think it's a good device. I was I was working recently in a Kaggle dataset and that's like

31:22.840 --> 31:33.720
268 training, 126,000 training images. And and for some reason the model is working very, very slow.

31:33.720 --> 31:40.200
So if you start with the huge number of images, you might have issues somewhere and that's going to

31:40.200 --> 31:47.480
take you time to debug it and so on. So if you start with some make sure you're

31:47.480 --> 31:51.480
model works and then try to increase that maybe more sense.

32:05.640 --> 32:13.080
And also this kind of the same start with like small number of lines. I'm going to mute everyone.

32:13.080 --> 32:17.640
I want to. So just unmute yourself.

32:24.360 --> 32:27.880
Yeah, just first of all, good thing with the ground. That's the first step.

32:31.240 --> 32:36.840
I apologize I'm going so fast with that, but it's just too much of a step to cover in one,

32:36.840 --> 32:45.800
have an hour. Yeah, so and the next step they say it's overfit a single batch.

32:45.800 --> 32:50.280
So if you kind of overfit a single batch, then then there's some issues with your model.

32:50.280 --> 32:56.360
So I think that's interesting, interesting things so they say just overfit your model first.

32:56.920 --> 33:02.040
And once you can do that, then you can try to optimize performance of your model. I think that's

33:02.040 --> 33:10.280
quite interesting. I'm going to skip some slides now to step three. And it's also good if you have,

33:10.280 --> 33:16.760
if we have like known results or like image net, of course, we can compare ourselves to a lot of

33:16.760 --> 33:21.880
other guys who try to do the same thing, but in some cases it's not possible. I guess like Kaggle

33:21.880 --> 33:27.400
gives you that possibility. You can compare your solution to other guys on a leaderboard,

33:27.400 --> 33:31.880
but in some cases it's not possible, but at least if you can.

33:41.160 --> 33:46.040
Yeah, and then once you just keep iterating until your model performs up to expectations,

33:46.760 --> 33:52.120
again, we don't have to make very precise up to 0.001 percent model,

33:52.120 --> 34:00.040
just whatever makes sense for our project.

34:06.040 --> 34:11.880
And then they talk about what pull is a test error. It's a combination of more errors. It's just

34:11.880 --> 34:22.040
to be aware of that. And again, we was talking about the training data could be from

34:22.040 --> 34:27.320
day scenes, and then test data could be from the night. And that's going to cause an issue for

34:27.320 --> 34:29.320
for your model.

34:38.280 --> 34:38.760
What else?

34:44.520 --> 34:48.200
Right, so in the deep plan, you can have underfeiting.

34:48.200 --> 34:57.000
So try to fix that, try different network, different model, more detailed model, like more parameters.

34:59.400 --> 35:04.680
Once you can overfeed, then you need to deal with overfeiting. So that's next step.

35:05.320 --> 35:10.280
So once you start overfeiting, then you need to address that by some sort of dropouts and

35:10.280 --> 35:14.360
other regularization, other techniques that helps you with overfeiting.

35:14.360 --> 35:19.160
I'm going to scroll quickly.

35:26.520 --> 35:32.200
The point see of me. So the address the distribution shift. So again, if you have different test

35:32.200 --> 35:39.080
set, different training set, different data sets, something to think about. And the rebalance

35:39.080 --> 35:45.560
data set if applicable. So yeah, if you have, again, night and day pictures, try to make them balanced.

35:46.680 --> 35:51.400
And that's another problem I had with that data set from Kaggle. I had to like find a thousand

35:51.400 --> 35:59.160
classes. And some classes had like 500 pictures in. And some classes only had like a couple of them.

36:00.040 --> 36:04.600
So I believe that also can cause an issue. So one technique for that would be to over sample.

36:04.600 --> 36:12.600
So it just is basically take the same image a couple of times to make up for the missing numbers in

36:12.600 --> 36:24.440
that class. And then hyper parameter tuning. So like the hyper parameters, like how many layers

36:24.440 --> 36:29.880
in a network. So you can look and choose between the rest of the 34 or 50, 150, I believe.

36:29.880 --> 36:36.840
The white installation, the kernel size could be different stuff like that. You can choose different

36:36.840 --> 36:45.160
optimizers, different batch sizes, different learning rates. Yes, there's a lot of stuff to

36:45.160 --> 37:00.920
play with. And we can do this manually. So just try an error. Another one is like a grid search.

37:02.280 --> 37:07.640
So I believe there will be like certain tooling that would help you to search different

37:07.640 --> 37:16.440
hyperparameters. I don't know if they show any tools for that. Might be in a different lecture or

37:16.440 --> 37:35.080
something. Another one would be just random. I'm not sure about this method by

37:35.080 --> 37:48.040
as an hyperparameter optimization. So then at the end he says that the debugging is hard.

37:51.400 --> 37:57.000
So we start iterative process, start simple and then increase complexity.

37:57.000 --> 38:05.880
They're simple, implement in debug, evaluate, tune hyperparameters and improve model on data.

38:05.880 --> 38:15.080
So that kind of like a workflow they propose can make sense. And they give some of resources

38:15.080 --> 38:21.000
where we can learn more, so the machine learning, yearning. I think this is some sort of a book

38:21.000 --> 38:28.680
or something. I don't know what that is actually. Maybe you can check. So you can sign up for a draft

38:28.680 --> 38:35.160
free copy. Okay. I think it's a book, right? It looks like a book. It's a book I'm doing.

38:35.880 --> 38:47.800
Okay. Yeah. Never heard about that one. And the car part is three. It's a long three.

38:47.800 --> 38:59.400
Maybe I need to read that one. Okay. And there's a blog post.

39:03.720 --> 39:11.000
Okay. It's just something for me to still read about. Some homework.

39:11.000 --> 39:20.680
Yeah. So that was lesson 10, which brings us to a lab session. But because I didn't do lab,

39:20.680 --> 39:25.800
I kind of talked much about the lab. So I still have lab to do as a homework. But if there's

39:25.800 --> 39:42.600
anyone else on the call that did the lab work. And maybe once we talk about this, we can listen to that.

39:42.600 --> 40:00.440
You can chat as well, but I don't see anyone volunteering to. So maybe we can shift labs to another session

40:00.440 --> 40:07.240
on the week. And on the next week, that's what we've done. So do we have any more questions in the

40:07.240 --> 40:24.280
chat? Let's see below. Charters, do you know? No questions. Okay. That's great. So next week,

40:24.280 --> 40:29.960
we're going to have our last session of that. No, no, is it? That's right. That's going to be the last

40:29.960 --> 40:40.520
session. So we're going to finish the labs. So lecture 11, the labs 9, 6 to 9. And that's the

40:42.040 --> 40:51.800
habit. So that's end of the lab sessions. And then the lectures 12, 13, 12 is a testing

40:51.800 --> 40:59.480
on deployment. And 13 is the research directions. Lecture 14 is a guest lecture from Jeremy. And

40:59.480 --> 41:07.880
lecture 15 is a guest lecture from Richard. So those two again, optional. So if we don't count

41:07.880 --> 41:15.400
the optional, we have two video lectures and the labs 8 and 9 for next week. Okay, excellent.

41:15.400 --> 41:21.160
So we also in the agenda, the Avingash's presentation, we have Avingash in the call.

41:23.000 --> 41:29.800
So there's a question from Sagar. He mentions, he says if anyone, he wants to ask if anyone has

41:29.800 --> 41:35.960
had the chance to try the tools mentioned in the data management or infrastructure and doing lecture.

41:35.960 --> 41:53.880
Yeah, so I didn't. And they, they showed a lot of tools in the presentation. They showed a lot of tools.

41:53.880 --> 42:09.720
I'm just trying to show that page again. This kind of is from Google, from some paper from Google

42:09.720 --> 42:16.280
they said, this is your RML called. And it's so small compared to the whole ecosystem of deep learning.

42:16.280 --> 42:23.160
So that just shows the how big that is. And then they show this picture, which shows like all

42:23.160 --> 42:31.480
sorts of different tools for different problems. I've logged into the white and biases, but because

42:31.480 --> 42:38.680
I could not run that lab because yeah, I couldn't install the torch libraries for some reason.

42:39.560 --> 42:45.400
I couldn't run it, but there's a lot of tools to try, but also in a lot of time.

42:45.400 --> 42:52.360
Actually, we'll also be doing the presentation of tools. Yeah,

42:52.360 --> 42:55.640
into that. Excellent, excellent.

43:01.800 --> 43:05.400
Okay, so we can move on to next presentation.

43:05.400 --> 43:20.440
Well, there are more questions. Yeah, the question, how to practice labs. I don't know actually to

43:20.440 --> 43:28.280
answer the best answer to that. So all the code is here in a GitHub. And in a lecture, they use

43:28.280 --> 43:35.160
the suites and biases, and they have the code to run the Jupyter notebooks from there. But because

43:35.160 --> 43:40.040
we don't have that code anymore, it doesn't work anymore, we have to find a way to do it. And so far,

43:40.040 --> 43:48.200
I did not find a way to do it. I know some people did, it's like Michael, he managed to run the code,

43:50.040 --> 43:56.840
and he used call lines instead of the pip end of as they recommend here. Because it's TensorFlow,

43:56.840 --> 44:04.520
I had some issues installing some TensorFlow libraries. I think Pythos requires Google 10 and

44:04.520 --> 44:11.480
TensorFlow requires Google 9. I don't know if that seems or maybe that position. Yeah, because they give

44:11.480 --> 44:19.480
you like installation setup, you have to set up thing, but so I follow that, but it didn't work,

44:19.480 --> 44:28.440
so I don't have time to debug it. But if someone know how to do it and wants to shout to us,

44:28.440 --> 44:37.880
other people have to do it, they'll be great. I haven't tried to do it. Yeah.

44:39.720 --> 44:45.720
Also, hands up to everyone if you try installing TensorFlow, I need a break in everything.

44:45.720 --> 44:54.600
To an extent, but even Zoom is crashing, so a world of caution. Exactly.

44:57.320 --> 45:03.000
So that's kind of, because this was created as a bootcamp thing, it wasn't created as a MOOC,

45:03.560 --> 45:13.720
so they never really, you know, if we go back. Sagar has mentioned it's a problem with TensorFlow's

45:13.720 --> 45:19.720
newest version. Yeah, I think it's common as related to the lab. Yeah, because that was created as a

45:19.720 --> 45:25.720
bootcamp one of them, it's not a MOOC officially, so I guess no one is really maintaining that repo

45:25.720 --> 45:31.000
or making sure it works still. So it's nice we can use it and nice we can see all the videos,

45:31.720 --> 45:41.000
but yeah, maybe the code needs some updating or, you know, my telling. So that's the thing.

45:41.000 --> 45:49.400
But we'll try it for next week, if we cannot make it work. It's a shame, but yeah.

45:51.000 --> 45:54.040
So Sunyan, do you want to take covert presentation and present the

45:55.400 --> 45:57.480
Sure. JVM too.

45:58.520 --> 46:01.960
Oh, definitely. I just requested Avina, she's agreed that I can go first.

46:02.760 --> 46:09.400
My Zoom is crashing constantly because of the Swift thing, so Akash will do the presentation and I'll

46:09.400 --> 46:20.280
monitor the chart. Hi, everyone. This is Akash here. I hope you can see me here. I can see my video.

46:20.280 --> 46:26.520
Great. Yeah, great. It's great to be. I was actually listening in for the past half an hour.

46:26.520 --> 46:32.600
Great discussion. He wonder why I haven't been participating. I've been part of this earlier, but yeah.

46:32.600 --> 46:39.880
So maybe I can I share my screen? Yeah, sure. I'm sharing.

46:40.440 --> 46:46.120
Meanwhile to those unaware, there's been this huge meetup in India that was being run by Akash.

46:47.240 --> 46:52.920
So all of the Bangalore meetups around pasta and even data science would have been mostly

46:52.920 --> 47:01.800
promo cards. Those who might have a take. Excellent. I think this is like the pie torch from zero to

47:02.600 --> 47:05.800
whatever. Yeah, it is. Excellent.

47:07.640 --> 47:17.480
All right. Great. Thanks. Thanks. I am. So, okay, I'm going to share my screen. I hope this was great.

47:17.480 --> 47:23.880
Cool. Can you see my screen? Yep. Yes. Okay. Awesome. So, yeah, quick introduction guys.

47:23.880 --> 47:30.600
My name is Akash. I'm joining from Bangalore. The first time I'm joining the Twimmil AI Study

47:30.600 --> 47:36.920
Group call. So just quick background about myself. I was a software engineer for a few years

47:36.920 --> 47:42.680
and then switched over to machine learning after doing the first AI course online.

47:42.680 --> 47:49.560
And that was just that just completely changed my perspective on how we were looking at data

47:49.560 --> 47:53.800
science and machine learning. Like in college, it was so mathematical and so dry, but faster,

47:53.800 --> 47:58.120
I was so hands on. So that's how I got involved and started actually switched over my career

47:58.120 --> 48:05.640
completely. Started doing ML consulting work and freelancing while also in the mean and as part

48:05.640 --> 48:10.520
of this, I also like along with a friend of mine, we realized that although there are a lot of

48:10.520 --> 48:17.320
people who are interested in data science in India and Bangalore, but there's no group

48:18.680 --> 48:24.520
community, especially having a lot of technical talks. There's too much networking and that kind

48:24.520 --> 48:30.040
of stuff. So we started the data science network about eight months ago and it has been a wonderful

48:30.040 --> 48:35.000
journey that so far we've had about 5,000 members in our meetup groups and we have meetups every

48:35.000 --> 48:39.880
other week. So, yeah, just really excited to be part of the data science community and learning

48:39.880 --> 48:49.560
new things every day. So what I'm here to present today is there's a tool that I've been working

48:49.560 --> 48:55.800
on with a few friends and like Simon has also been involved recently. This is a tool called

48:56.360 --> 49:01.960
Jovian. This is something that was born out of our own problem that we really love Jupiter

49:01.960 --> 49:08.920
notebooks, the interactive nature, the way we can plot graphs and go back and forth and fix things.

49:10.200 --> 49:19.000
I felt like that really opened up things for me in terms of trying a lot of experiments out,

49:19.000 --> 49:23.720
but it also led to a lot of problems, especially coming from a software engineering background.

49:23.720 --> 49:28.280
I was used to having like get kind of a workflow where you sort of branch out, make some change

49:28.280 --> 49:34.520
and come back. But what really happens, what we've seen happens in a lot of machine learning

49:34.520 --> 49:39.800
projects is something like this. If you're using, so the question we asked us is how we'd be tracking

49:39.800 --> 49:45.320
our machine learning experiments, all the different ideas that we're trying. Now notebooks are great

49:45.320 --> 49:49.480
to try things out, but then because they're so rich, they have so many graphs and things like

49:49.480 --> 49:54.200
that, they can be large in size. What we tend to do is we tend to keep creating copies and copies

49:54.200 --> 49:59.880
of data. So this is just like one particular project that I was working on, where we had

49:59.880 --> 50:03.960
over a course of three months, we had like 60 different notebooks and then we would try to put

50:03.960 --> 50:08.600
everything into the just the file name so that and this would be sitting there lying on a cloud

50:08.600 --> 50:13.400
machine and there's no sense getting putting this into gate because this is like 200 MB of Jupiter

50:13.400 --> 50:19.480
notebooks. So that became, you know, that we found that very tricky. We also tried using log files,

50:19.480 --> 50:23.480
but then log files are sort of even trickier where you have like a code in a notebook which you

50:23.480 --> 50:28.120
have updated, but then your log file looks something like this where you have to sort of go into it

50:28.120 --> 50:34.440
and try and parse things out. It's a big mess, right? We tried spreadsheets as well where we would

50:34.440 --> 50:41.000
sort of keep track of things in a spreadsheet like on what date, which experiments, which network

50:41.000 --> 50:46.440
we tried, what hyper parameters we used, and what kind of results we got. But that also sort of

50:46.440 --> 50:50.840
made things really messy because now you have your notebooks somewhere and then you have your data

50:50.840 --> 50:55.160
somewhere and then you have maybe your checkpoints and models somewhere and nobody is really kept

50:55.160 --> 51:00.920
track of the library. So TensorFlow has updated, PyTorch is updated and it gets all very messy.

51:00.920 --> 51:06.200
So we started creating a tool for ourselves and that's what I want to show you. The goal was to

51:06.200 --> 51:11.400
start with Jupiter notebooks and make them more shareable, make them like collaborative so that

51:12.440 --> 51:17.880
people can work or work together on a single notebook, try different ideas and manage the entire

51:17.880 --> 51:24.760
workflow around them. So I will quickly jump into an example. I guess that's the best way to do it.

51:24.760 --> 51:31.560
So this is Jupiter notebook running on my local host. It's an example of classifying handwritten

51:31.560 --> 51:38.040
digits from the MNIST dataset by just training a CNN using Keras. It's pretty straightforward.

51:38.040 --> 51:43.560
You prepare some data, download the data, take a look at the data and then do some pre-processing,

51:43.560 --> 51:49.240
one-hot encoding, have some like a two-categorical which does like one-hot encoding here. Then,

51:49.240 --> 51:55.080
as you start out, as we just saw in the lecture that you start with a very simple model. So I have

51:55.080 --> 52:00.920
a very simple model of single conglare 32 filters and then I just flatten that and pass it through

52:00.920 --> 52:08.840
a fully connected layer. Then I compile it, I train it, I look at the accuracy, okay that looks

52:08.840 --> 52:13.640
pretty good. Maybe it also helps to look at how the model was training so I can plot

52:13.640 --> 52:19.160
how the accuracy is changing. So I guess the dots are the training laws and the validation

52:19.160 --> 52:26.120
laws is the line. So the pretty standard model, right? And then finally, you evaluate it on the

52:26.120 --> 52:31.320
test set and then get the accuracy and laws. Now, suppose you want to share this with somebody

52:31.320 --> 52:35.000
so that they can try it out as well, right? Maybe you're doing this on your local machine or your

52:35.000 --> 52:40.760
Google machine, on your on your DCP cloud machine. You want to send it across to somebody,

52:40.760 --> 52:46.040
just get quick thoughts, get maybe some inputs, maybe they're helping you learn. This is where our

52:46.040 --> 52:51.080
tool comes in, Jovian. So you can just can simply install, install Jovian.

52:57.000 --> 53:03.800
And yeah, I think I already have it. Then you import the library and then you just say

53:03.800 --> 53:11.240
jovian.com it. Okay. And when you say that, it asks you to, it asks you to provide an API key.

53:11.880 --> 53:17.160
An API key is something that you can find by simply logging into the website. It's a, okay,

53:17.160 --> 53:22.440
I'm already logged in, I guess. So I'm just going to copy, it's a one click login with GitHub.

53:22.440 --> 53:27.400
So I just get it, it just gives me an API key, which I can enter here. Now, once I do this,

53:27.400 --> 53:32.920
what we do is we take a snapshot of the Jupyter notebook. We take a snapshot of the entire

53:32.920 --> 53:39.640
backing environment. So here I'm using Anaconda. So we capture the Anaconda environment behind the

53:39.640 --> 53:43.880
scenes. And I'd like to ask you, do you know how to export the Anaconda environment? Because there's

53:43.880 --> 53:48.760
a really complex command that you have to try, especially to make sure that the environment is

53:48.760 --> 53:56.920
reproducible across between like Mac OS and Linux and Windows. But anyway, we do a bunch of these

53:56.920 --> 54:03.320
things behind the scenes. And then we give you, we upload it to the cloud and you get a quick,

54:03.320 --> 54:07.400
like a URL where you can see the notebook. So you don't need to have Jupyter running.

54:08.440 --> 54:12.600
You can, you know, the notebook is there. Apart from that, we also have,

54:13.640 --> 54:18.200
all the file, we also have the environment that has been captured. So these are all the libraries

54:18.200 --> 54:25.240
that I'm using here, right? And then you can simply take this link as I'm taking right now. And

54:25.240 --> 54:35.080
you can simply take this link and send it to someone. And there, that's, that's, you know,

54:35.080 --> 54:37.960
then now you can click the link and you can try it out. Now, if you want to quickly, if you want

54:37.960 --> 54:42.200
to try this out on your own machine, all you need to do is you can click the clone button.

54:43.080 --> 54:47.960
Now go back to your terminal. Let me open the new one here.

54:47.960 --> 54:57.320
What, let me just rename this to something nice. Yeah, there you go. Okay, so I'm just going to,

55:02.360 --> 55:06.600
I'm just going to run the clone command. Again, to run this clone command, I need to have

55:06.600 --> 55:16.440
the Jovian library installed. So I, I'll do like pip install Jovian, which in my case, I already do

55:17.480 --> 55:23.640
better just to show you. And this installs a CLI command called Jovian. So that's where you can

55:23.640 --> 55:28.600
paste the command that gets copied when you click the clone button. And that downloads all the files.

55:28.600 --> 55:33.240
And it gives you a bunch of instructions for what you need to do next, right? So I'm just going to

55:33.240 --> 55:39.320
enter this directory, run this Jovian install command. So what this does, the Jovian install,

55:39.320 --> 55:44.600
depending on like, as you can see here, I have an environment or YAML file, environment or Mac

55:44.600 --> 55:49.880
West file. So it looks into if there's a environment specific, if there was specific file, it uses

55:49.880 --> 55:54.440
that. Otherwise, it uses environment or YAML. The reason we need to do this is because Konda

55:54.440 --> 56:00.120
environments and Konda packages have different versions across different platforms. So sometimes

56:00.120 --> 56:05.240
things don't work. Sometimes things fail and then it just errors out. So what we do is we try to

56:05.240 --> 56:12.840
install. If there's an error, we, like, comment out those lines from your environment, try to

56:12.840 --> 56:17.240
make a best set and try this three four times till the entire environment is installed, right?

56:17.240 --> 56:21.880
And this has been an open issue on Konda for over a year where people are saying, I just can't

56:21.880 --> 56:28.040
reuse my environment fine. But we try to solve that with just doing a few hacks and trying the

56:28.040 --> 56:34.840
next best thing. But yeah. So what this does, you know, you could do some work on your local

56:34.840 --> 56:39.880
machine and then maybe push it to Jovian and then open it up on your cloud machine, pull from

56:39.880 --> 56:43.960
there, clone from there and start running, right? So now at this point, if I do, if I run the

56:43.960 --> 56:49.560
Konda activate command, that activates my new environment, which is just created and I can go to

56:49.560 --> 56:57.800
put a notebook and start playing around with that notebook, right? So simple push and simple

56:57.800 --> 57:05.400
commit and clone flow that that we do. Now, the, now the next thing that we've also done on top of

57:05.400 --> 57:10.680
this is maybe you're just looking for some feedback or some inputs. So what we've done is we've

57:10.680 --> 57:15.720
built out a commenting interface on top of Jupiter. So for instance, if you have a question,

57:15.720 --> 57:20.840
you know, very specifically about, let's say, about the model. Somebody has a question saying,

57:20.840 --> 57:30.360
why are you using kernel size 32? So they can just comment it on that particular cell and then

57:30.360 --> 57:34.840
I will get a email notification where I can click through and reply to them and then they will

57:34.840 --> 57:39.560
get an email notification, right? So this allows for very quick feedback and discussion on Jupiter

57:39.560 --> 57:44.200
notebooks. When you're, you know, one person can actually do the coding and the others can all

57:44.200 --> 57:49.240
come in and give their thoughts on it. This was something that we found missing. So we just

57:49.240 --> 57:56.280
decided to add that as well. So next, the way we use it is we run some workshops here. So often,

57:56.280 --> 58:01.400
we put all the, we put all the Jupiter notebooks, environments, everything on Jogin and then let

58:01.400 --> 58:05.960
people clone it, try out something on their own and then push the new versions and then get feedback

58:05.960 --> 58:12.360
from us, right? So that, it, it helps to do that very easily. Okay. So coming back to this,

58:12.360 --> 58:17.720
now one thing is that, okay, the, it's great that I have the Jupiter notebook. It's great that I

58:17.720 --> 58:23.400
have the environment I can reproduce it. But what ends up happening is, you know, even if you just,

58:23.400 --> 58:28.200
you make some changes, you try, you create a new version, you, you ultimately you end, start

58:28.200 --> 58:33.960
ending up with like a half a dozen dozen or so Jupiter notebooks, right? And what you really want

58:33.960 --> 58:41.000
is, and what you really want is you want to be able to track specific things about notebooks.

58:41.000 --> 58:45.480
Like for instance, in this notebook or in this experiment, what are the hyper parameters that

58:45.480 --> 58:51.000
tried and what were the success metrics I got, right? So that's where we have created a very simple

58:51.000 --> 59:00.520
API as well. Is there a question? Okay. Yeah. That's where we have created a very simple API as well.

59:00.520 --> 59:09.080
You know, all you, all you need to do is just import Jogin and then we have a couple of APIs,

59:09.080 --> 59:15.880
we have log hyperparamts. Okay. So this is two log hyper parameters. So for instance, here I'm

59:15.880 --> 59:25.720
just going to add a small note about the architecture that this is the simpler con 32 plus dense,

59:25.720 --> 59:35.880
right? Maybe I'll also add a note about the batch size. You know, the batch size I'm using is 128.

59:35.880 --> 59:46.600
I'm using three epochs and what else? Yeah. What else is interesting here? I think I'm using the RMS

59:46.600 --> 59:54.040
prop optimizer, right? So this is currently a bit manual, but what we're trying to do is,

59:54.040 --> 59:59.160
now when you do a model.fit, we're trying to create a callback so that all of this information

59:59.160 --> 01:00:03.640
can be picked up automatically, but I just wanted to show you the basic API as I do that.

01:00:03.640 --> 01:00:10.840
So that logs are hyperparameters. Then once the model is trained, I might also want to actually

01:00:10.840 --> 01:00:19.160
keep track of the metrics. So that's where I can do jogin.log metrics. And here I'm just going

01:00:19.160 --> 01:00:28.680
to put in these things. So I want to mention that even for category competition, like this is how

01:00:28.680 --> 01:00:33.080
most people usually track, like they have multiple notebooks and then they create spread sheets.

01:00:33.080 --> 01:00:38.280
So this is essentially looking at replacing it with just one single interface.

01:00:39.160 --> 01:00:45.080
Yeah. And right now, I'm actually having to type a lot, but the idea is this will be just like a

01:00:45.080 --> 01:00:51.480
single callback that you introduced into your history, you know, at least for the popular frameworks.

01:00:51.480 --> 01:00:55.320
So now I've captured some metrics. I've captured them hyperparameters and we'll take a look at

01:00:55.960 --> 01:01:00.680
what they look like. But apart from that, the other thing is also that, you know, maybe I might

01:01:00.680 --> 01:01:07.560
this code, this might be some common code that I use across all my notebooks. So I might

01:01:07.560 --> 01:01:14.440
actually put this in like a utils file, right? So maybe I'll do like create a utils for utils file.

01:01:14.440 --> 01:01:34.760
And we put this here, call this plot history. Okay, very simple. And I'm just going to import

01:01:34.760 --> 01:01:45.080
the from utils import plot history and then this plot history does the same thing, right?

01:01:45.080 --> 01:01:49.880
So now I also have a dependency on a certain file. And another thing is like I've trained this

01:01:49.880 --> 01:01:55.400
model. Maybe this is a simple example, but then in certain cases, your model can actually have

01:01:56.680 --> 01:02:00.920
been trained for a long time and you might want to just keep the save the weights. So that's where

01:02:00.920 --> 01:02:05.560
you probably want to do something like, you want to save the model as well. So I'm just going to

01:02:05.560 --> 01:02:21.080
save the model, model.savemscnn.h5. Okay. And yeah, so now I have like a dependency on a file,

01:02:21.080 --> 01:02:25.800
I've lost some hyperparameter, some metrics. This time when I commit, what I'm going to do is I'm

01:02:25.800 --> 01:02:31.320
also going to include the files that are depend on. So this is utils.py and the artifacts that I've

01:02:31.320 --> 01:02:43.880
generated. So this is like amnestcnn.h5. Right. So now I commit once again, what it does is it create

01:02:43.880 --> 01:02:48.760
updates that notebook. So it creates a new version essentially. Once again, it captures the environment

01:02:48.760 --> 01:02:54.840
to make sure if you've created, I did any new libraries that get captured. Any additional files and

01:02:54.840 --> 01:02:59.240
any artifacts are also captured. And also the metrics and hyperparameters, everything is then

01:02:59.240 --> 01:03:05.640
taken. And then now you have a more start to have a more complete picture of your experiment.

01:03:05.640 --> 01:03:11.400
Right. So you have the notebook, you have environment, you have the utils files right here, you have

01:03:11.400 --> 01:03:16.200
the artifact. Now we are trying to differentiate between sources and artifacts because, you know,

01:03:16.200 --> 01:03:22.920
as you might have faced, if you train a resident 152 and then create, save a checkpoint,

01:03:22.920 --> 01:03:27.080
that's going to be like a 120 MB file. If you check that into Git and then you train two or three

01:03:27.080 --> 01:03:31.640
more of these, suddenly you have a Git repository, that's one GB. So every time you push and somebody

01:03:31.640 --> 01:03:36.280
tries to pull it, they're downloading, you know, one GB worth of stuff because the weight change

01:03:36.280 --> 01:03:40.920
every single time, right. So that's why we differentiate between artifacts and sources and,

01:03:40.920 --> 01:03:46.520
you know, artifact and we provide an option to actually clone without actually downloading the

01:03:46.520 --> 01:03:49.800
artifact because if you just want to reproduce it, you probably don't need it. On the other hand,

01:03:49.800 --> 01:03:55.400
if you just need the artifact, you can download it right here as well. So that's the files, the

01:03:55.400 --> 01:03:59.960
dependency and the artifact. But also, this is where it starts to get interesting, where,

01:04:00.760 --> 01:04:04.920
you know, now you start to get, as you log some hyper parameters and some metrics, you start to get

01:04:04.920 --> 01:04:09.400
a timeline of what was actually happening in this project. So, you know, I have here, it's a

01:04:10.360 --> 01:04:17.000
corn 32 with a dense, three pox, RMS prop, and I go to an accuracy of 97%. That's pretty good,

01:04:17.000 --> 01:04:24.440
but maybe I can do better. And as you start to sort of keep doing this for multiple times,

01:04:24.440 --> 01:04:30.440
start to record multiple versions, what you can do is, so here I have another, you know, the same

01:04:30.440 --> 01:04:34.840
example that I've tried a few versions. So you can check the versions, okay, not here.

01:04:39.160 --> 01:04:43.160
Yeah, so same example I've tried, I've recorded a few versions and all the versions are visible

01:04:43.160 --> 01:04:48.680
here and the version drop down. And I can actually go click on compare versions, right? So now,

01:04:48.680 --> 01:04:55.480
whichever version has a record, all those records show up here. So I can see when it was created,

01:04:55.480 --> 01:05:01.720
who created it? So come back to this, you know, the notes that I've added here and the hyper parameters

01:05:01.720 --> 01:05:06.680
and the metrics. So it starts to give you a good overview. One thing that we've added and we're

01:05:06.680 --> 01:05:10.040
adding more things is, you know, you can select which columns you're interested in. Maybe

01:05:10.040 --> 01:05:15.880
epochs is not that important, maybe you don't need hyper parameters at all. So you can just look at

01:05:15.880 --> 01:05:20.920
the metrics. Another thing that we're doing is, you know, if you have some runs which you're not

01:05:20.920 --> 01:05:25.640
interested in, you can actually just select them and remove them, you can reorder the columns,

01:05:25.640 --> 01:05:32.280
you can actually, you can add in notes here, come and change, change in entry because maybe it was

01:05:32.280 --> 01:05:38.360
recorded incorrectly, right? So I'm going to try to make this very powerful, pretty much almost

01:05:38.360 --> 01:05:44.680
as powerful as a spreadsheet, but then more targeted for the data science use case, okay?

01:05:44.680 --> 01:05:51.560
And then finally, the last thing that I want to show you is collaboration. So now this is just me

01:05:51.560 --> 01:05:55.320
working and maybe me sharing with somebody, but you can see here that there are other authors here as

01:05:55.320 --> 01:06:01.240
well. That's where we have an option. So a bunch of things, one is a visibility that if you want to,

01:06:01.960 --> 01:06:06.920
so currently this is a public notebook, anybody can go on my profile and find it. I can make it secret

01:06:06.920 --> 01:06:12.520
so that only people with the link can access it. We're also working on private notebooks so that

01:06:12.520 --> 01:06:18.360
you have to be logged in and added as a collaborator. I can archive this notebook once it starts to

01:06:18.360 --> 01:06:23.080
get filled up on my profile and then finally, I can add collaborator so I can just enter that username

01:06:24.360 --> 01:06:33.160
and yeah, and that will, that will add them as a collaborator, right? So now what happens is

01:06:33.160 --> 01:06:39.320
every time they clone and they try to commit, this is going to add to this particular project itself,

01:06:39.320 --> 01:06:44.440
right? So slowly starting from a notebook, we're just trying to build a complete story of a machine

01:06:44.440 --> 01:06:50.280
learning, of a machine learning project and trying to make it as seamless as possible so that,

01:06:50.280 --> 01:06:55.800
you know, as a ML student or engineer or data scientist, you have to do minimum work.

01:06:56.680 --> 01:07:01.560
And we have been trying this for about four or five months now using it at workshop.

01:07:01.560 --> 01:07:07.960
It has been pretty helpful for us. A few community related, a few inputs for the community were,

01:07:07.960 --> 01:07:12.440
hey, this is great. But what if I could, I want to be able to run this immediately as well.

01:07:12.440 --> 01:07:17.560
So that's where we have created integrations with binder and Kaggle. So if I click run on binder,

01:07:17.560 --> 01:07:23.080
what this does is this immediately goes to mybinder.org. I don't know if you're familiar. Mybinder is

01:07:23.080 --> 01:07:27.560
like a free hosted Jupyter notebook which can create a rather Jupyter notebook

01:07:27.560 --> 01:07:34.520
instantly from any repository, get repository. So that's what we've added. It's going to take a time

01:07:34.520 --> 01:07:39.160
over. Okay, so it looks like, and what it does is it actually caches. So first time you click,

01:07:39.160 --> 01:07:43.480
it's going to install all these things, but next time it's just going to keep a Docker image around

01:07:43.480 --> 01:07:49.000
so that this can start immediately. Another thing is if you want to use a GPU, you can click run on

01:07:49.000 --> 01:07:56.520
Kaggle and then this would run immediately on your, on your Kaggle account, right? And you just

01:07:56.520 --> 01:08:10.200
need to connect your Kaggle account. There you go. So yeah, so that's that's pretty much it from my,

01:08:10.200 --> 01:08:18.440
I don't know why this is not working. Okay, I'll look into this. We're also adding colab and a bunch

01:08:18.440 --> 01:08:25.080
of other platforms soon, but yeah, that's it. So that's pretty much a lot and just wanted to show

01:08:25.080 --> 01:08:29.480
you what we are building and if it is useful. So we are still, I would say we are still in beta.

01:08:29.480 --> 01:08:34.120
We're still building. We have a lot of things working, but with this, still a long way we want to

01:08:34.120 --> 01:08:39.160
go. So would love, you know, if you could try it out, give us feedback, tell us what you would want

01:08:39.160 --> 01:08:44.920
in this, because we are very open to just trying to just building what, what will serve the,

01:08:46.040 --> 01:08:52.840
serve the user's best, right? Yeah, and we have been personally using it more as like in Kaggle

01:08:52.840 --> 01:08:59.160
competitions as like a internal leaderboard to keep track of who's doing what and you know,

01:08:59.160 --> 01:09:03.800
we can quickly click through see their approach, see their loss, see their notes and things like that,

01:09:03.800 --> 01:09:10.200
right? So building it as we use it. Yeah, so that's that's pretty much it. If you have any questions,

01:09:10.200 --> 01:09:17.000
I'm happy to answer. I think you know, it's very good, very nice, very like complete picture.

01:09:17.000 --> 01:09:23.320
So I've not, I didn't know about this too, so I know a lot. I've been trying to use also Kaggle,

01:09:23.320 --> 01:09:29.400
it's got something kind of not similar, but at least they have this idea of committing your

01:09:29.400 --> 01:09:37.160
kernels. Yes. So Kaggle, you can commit and then it saves your data, saves or not book and then

01:09:37.160 --> 01:09:42.200
it saves your output files. Yeah. But it doesn't, it doesn't save all the nice information as you

01:09:42.200 --> 01:09:49.400
have like architecture. It doesn't have that much. It's not that rich. Yeah. Yeah. One thing also is

01:09:49.400 --> 01:09:55.800
that Kaggle, it has to be, you sort of have to commit it on Kaggle, so you can't really run it

01:09:55.800 --> 01:10:00.440
on your local machine. It's right. We wanted to have it flexible and have it open.

01:10:02.120 --> 01:10:09.960
Yeah. Another quick note was is that this is most of this is open source. So the Jovian Python

01:10:09.960 --> 01:10:14.520
library is completely open source. We're trying to open source a backend as well, but it's just,

01:10:14.520 --> 01:10:19.800
you know, we have to pull out all the specific backend specific things before we can, you know,

01:10:19.800 --> 01:10:25.800
make it generalized enough. So yeah, I'll post this link here as well in case you want to post,

01:10:25.800 --> 01:10:29.560
create, you know, try it out. If you're facing issues, you can always switch out to a

01:10:29.560 --> 01:10:40.680
share of cellar poll requests. The other thing, whatever isn't open source is free and will stay that. Yeah. Right. Thank you.

01:10:41.480 --> 01:10:47.640
How I like envision it is like, especially for Kaggle, like if you run something locally,

01:10:47.640 --> 01:10:52.520
and then you also want to run it on a kernel, and you like want to get away from the flakiness

01:10:52.520 --> 01:10:56.840
of the kernel, if you run it locally, and say if you're participating in a kernel's only

01:10:56.840 --> 01:11:02.520
competition, you get to run it on a kernel. Or if you want to check it out on an AWS instance,

01:11:02.520 --> 01:11:07.320
like if you want to run your BFI, it's your most model, you could run the 32 core machine,

01:11:07.320 --> 01:11:13.400
just run it there, get the trained weights that have the inference run on Kaggle kernel,

01:11:13.400 --> 01:11:19.800
inference will be a competition or even locally. That's once more something you can do with this.

01:11:19.800 --> 01:11:29.800
Very easy. Yeah, folks, right. Okay. All right. And you can reach out to me in case you have any

01:11:29.800 --> 01:11:36.440
questions, I think. Sayam will share the contact details, possibly. Yeah. Right. Okay. So I'll

01:11:36.440 --> 01:11:42.920
start. Yeah. You could join our slack as well. So people can ask them. Yeah. Yeah. Yeah. That's great.

01:11:42.920 --> 01:11:48.200
I would love to. Yeah. I'll be there. I'll slack them. Yeah. Then we can ask questions. They're

01:11:48.200 --> 01:11:54.200
all right. I'll share the link in the slack as they're like, if anyone has any feedback,

01:11:54.760 --> 01:12:00.440
I think it's very open to community feedback also to please ping me or him once he joins for any feedback

01:12:00.440 --> 01:12:03.640
or suggestions. Next on. Thank you.

01:12:09.080 --> 01:12:13.800
So thank you for the presentation. We have we have one more presentation for today.

01:12:13.800 --> 01:12:22.120
It's Avinish. It's going to present. Yeah. Hi, Michael. Hi. I'm a properly audible. Yeah. Hi.

01:12:22.120 --> 01:12:26.840
Yeah. So do you want me to take the presentation today or?

01:12:29.480 --> 01:12:37.480
Yes, it's 720. Maybe you can at least tell us what you want to talk about. And if that's not

01:12:37.480 --> 01:12:45.080
enough time for you, then maybe you can reschedule. But what do you think?

01:12:54.600 --> 01:12:55.640
Come on here, Avinish.

01:12:55.640 --> 01:13:11.960
Hello, Avinish. Still there? Can you hear me now? Yeah. Yeah. So the presentation that I wanted to do

01:13:11.960 --> 01:13:17.160
was on model interpretability and visualization. In that, I will be walking through a Kaggle kernel,

01:13:18.040 --> 01:13:24.440
which so it's on food classification like hot dog version 2 where it doesn't it doesn't just say

01:13:24.440 --> 01:13:30.840
whether it's hot dog or not, it can also detect the other classes of food. So using that,

01:13:30.840 --> 01:13:36.280
I'll be explaining how the Inelaya Activations look and also a bit about activation maps.

01:13:37.000 --> 01:13:43.480
So this is what the talk is going to be. But I feel it is going to take at least 22 to 25 minutes.

01:13:43.480 --> 01:13:49.560
Okay. And you'll try to talk next week. Yeah, I'm fine with that.

01:13:49.560 --> 01:13:56.120
Okay. If you could just share the link with us, we'll check it out for the week. Sure. Yeah.

01:13:57.000 --> 01:14:03.080
I will actually really sorry if I took up too much time. Sorry. I'm saying I'm really sorry if I

01:14:03.080 --> 01:14:09.720
took up some of your time. Not at all, Akash. It was very good presentation. Okay. Thanks.

01:14:10.680 --> 01:14:18.040
Yeah. So I'll share the link, the Kaggle kernel link. And we can discuss that in the next

01:14:18.040 --> 01:14:34.200
in the next week's session. Okay. Great. So I'll move that for next week. Do you have anything else

01:14:34.200 --> 01:14:41.480
to share for today? And then use? Yeah. I have some open slots for the Chai time data science

01:14:41.480 --> 01:14:46.760
conversation that I've shared for this upcoming week. So if anyone wants to talk about anything,

01:14:46.760 --> 01:14:57.000
please feel free to use any of the topic. What is the most, what's the most things people

01:14:57.000 --> 01:15:01.800
like to talk about in this discussion? Can you? I don't want to hear like names and things.

01:15:01.800 --> 01:15:09.160
There might be some statistics. Was the most interesting box most asked things about?

01:15:09.160 --> 01:15:19.320
But it's also the thing, but I guess Kaggle and Fastie, then machine learning jobs.

01:15:20.120 --> 01:15:24.920
Okay. Nice. Nice. That was very nice of you, Sanyam.

01:15:26.520 --> 01:15:30.040
Thank you. Just thank you for helping me. Yeah. That's nice. That's nice.

01:15:32.360 --> 01:15:36.920
So when I found this page on data helpers, maybe something you would be

01:15:36.920 --> 01:15:43.240
interested to have your name to it, I was thinking. I just learned about it today. I have the tab open

01:15:43.240 --> 01:15:51.000
and check it out after this. Yeah. That's excellent. That's excellent. Okay.

01:15:53.400 --> 01:15:57.160
So if that's everything for today, I guess we'll just close for today and

01:15:58.360 --> 01:16:02.920
we'll meet next week, the same time, the same day, for our last session of

01:16:02.920 --> 01:16:17.240
I'm going to show it again. If I can still open for suggestions. Hopefully, a Fastie is released

01:16:17.240 --> 01:16:22.680
by then to the public. We could start our Fastie session as soon. I'll be happy to

01:16:22.680 --> 01:16:27.640
volunteer to take those notes. Yeah, indeed. Good. You mentioned that. I forgot about this.

01:16:27.640 --> 01:16:33.960
So, indeed, it's so next week, we're going to have our, I hope we can see my screen now.

01:16:33.960 --> 01:16:38.920
Yeah. Yeah. So next week, we're going to have the last Fast full stack deep learning static

01:16:38.920 --> 01:16:47.320
group thing. And then, and then say it's a blank page. So I've collected from all the,

01:16:47.320 --> 01:16:51.160
from Slack, from what people were saying that what potentially you could do next,

01:16:51.160 --> 01:16:54.280
like secure and private AI, there were some people interested doing

01:16:54.280 --> 01:17:00.680
ML course from open data science, which starts September 2nd, I think, for the last session

01:17:01.320 --> 01:17:07.960
of that course in the current format. And there was like advanced NLP, which spacey,

01:17:07.960 --> 01:17:13.560
all of those are free, by the way, so it's a good look. We can do all of them. Fastie, I released,

01:17:13.560 --> 01:17:19.160
kind of silently released, quietly released, NLP because the videos are not released yet,

01:17:19.160 --> 01:17:25.640
so it's only notebooks. The statistical learning is more like machine learning from Stanford,

01:17:25.640 --> 01:17:31.560
free introduction with PyTorch, free, there are some Stanford courses on MLP,

01:17:32.920 --> 01:17:38.920
recommend that the deep unsupervised learning, that we cannot do all of them at once.

01:17:39.800 --> 01:17:45.720
Well, we could, if we create like separate groups, led by different people and meeting at

01:17:45.720 --> 01:17:52.440
some times, maybe even overlapping with some others. So that's another option. We could do more

01:17:52.440 --> 01:17:57.240
courses, but then there would be like smaller groups doing the courses. Or we could try to keep

01:17:57.240 --> 01:18:03.000
it as one group and then just select a course. Of course, something that a lot of us,

01:18:03.880 --> 01:18:09.880
this is how the group started, was the Fastie I deep learning course, part one, was by June last

01:18:09.880 --> 01:18:17.800
year. So the part two, it's not released to public it, and I didn't hear when they're going to do it.

01:18:19.400 --> 01:18:25.880
Didn't you do, but I'm not sure if that still works. Yeah, exactly. I think they now busy with,

01:18:28.760 --> 01:18:36.680
they now busy with updating the Fastie I library to V version two. So I'm not sure what's the

01:18:36.680 --> 01:18:44.520
priority for them now, because they also promised a couple of lectures, livestream lectures to the

01:18:44.520 --> 01:18:48.360
part two course, which also it's not sure when they're going to be available.

01:18:51.560 --> 01:19:00.680
Yeah, so we can continue that discussion on a slack, but it's just up to us to decide what to do next.

01:19:00.680 --> 01:19:06.840
I want to suggest that I gave these courses a look, and in my opinion like most of these scenes are

01:19:06.840 --> 01:19:13.800
covered in Fastie, except for a security AI. So and personally, I'd also like to like this

01:19:13.800 --> 01:19:19.800
through the, anyways, I will be going through the materials again from part two. So it'll be really

01:19:19.800 --> 01:19:25.400
cool to have other people, other people also joining the study group, but again, like I'll wait for

01:19:25.400 --> 01:19:32.520
the common opinion. Yeah, it's a fair point, because the Fastie I part to was closed, was only

01:19:32.520 --> 01:19:39.080
open to some people. So when we open this to the public, to general public, I think I think it

01:19:39.080 --> 01:19:46.840
makes sense to kind of do it again. I tentatively put like one lecture per two weeks, because those

01:19:46.840 --> 01:19:53.160
lectures are quite heavy, I guess. Very dense. Yeah. So maybe that's an idea to do like one lecture.

01:19:53.160 --> 01:19:59.240
So we can meet every week, but then we can split the lecturing two halves and then talk about one

01:19:59.240 --> 01:20:12.200
lecture for two weeks, maybe. Okay. Excellent. So let's continue the discussion on the slack.

01:20:15.400 --> 01:20:21.960
And next week we still talk, Fastie, sorry, full stack deep learning, and I'm going to move

01:20:21.960 --> 01:20:31.000
the ethnic presentation for next week. So at least we have also agenda. Also the Daniel and

01:20:31.000 --> 01:20:35.400
Abin, they also wanted to talk about deployment of deep learning models, but then they put

01:20:35.400 --> 01:20:46.040
it on hold. So maybe they also talk about this in future soon. Okay. Excellent. So thank you for joining

01:20:46.040 --> 01:20:53.640
today. Talk to you next week. Thank you for hosting, Michael. Thank you.

