WEBVTT

00:00.000 --> 00:16.320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

00:16.320 --> 00:21.480
people, doing interesting things in machine learning and artificial intelligence.

00:21.480 --> 00:31.880
I'm your host Sam Charrington.

00:31.880 --> 00:36.800
While at this past NURBS, I attended the second annual Black NAI workshop, which gathered

00:36.800 --> 00:41.680
participants from all over the world to showcase their research, share experiences and support

00:41.680 --> 00:43.080
one another.

00:43.080 --> 00:47.560
This week, we continue our Black NAI series with interviews with some of the great presenters

00:47.560 --> 00:49.280
from the workshop.

00:49.280 --> 00:55.440
Today I'm joined by Justice Ammo, a PhD student at Dartmouth, Thayer School of Engineering.

00:55.440 --> 00:59.720
Justice presented his work on an optimized recurrent unit for ultra low power acoustic

00:59.720 --> 01:03.400
event detection.

01:03.400 --> 01:08.200
In our conversation, we discuss his goal of bringing low cost, high efficiency wearables

01:08.200 --> 01:10.720
to market for monitoring asthma.

01:10.720 --> 01:15.920
We explore the many challenges of using classical machine learning models on microcontrollers,

01:15.920 --> 01:20.800
and how he went about developing models optimized for constrained hardware environments.

01:20.800 --> 01:25.360
We'd also like to wish Justice the best of luck as he should be defending his PhD any day

01:25.360 --> 01:26.360
now.

01:26.360 --> 01:27.360
Enjoy.

01:27.360 --> 01:32.040
Alright everyone, I am on the line with Justice Ammo.

01:32.040 --> 01:37.120
Justice is a PhD student at the Thayer School of Engineering at Dartmouth.

01:37.120 --> 01:40.080
Justice, welcome to this week in machine learning and AI.

01:40.080 --> 01:43.480
Thank you Sam, I'm happy to talk to you today about my wake.

01:43.480 --> 01:49.000
Fantastic, so to kind of lead us into what you're up to, I would love if you share a little

01:49.000 --> 01:53.600
bit about how you got started working in machine learning and AI.

01:53.600 --> 01:57.080
Yes, I'm happy to share a bit about my background.

01:57.080 --> 02:02.760
So I was born and raised in Ghana and came to Dartmouth for undergrad.

02:02.760 --> 02:07.560
So this was in like 2009, I mean, undergrad when I was coming in, I always wanted to do

02:07.560 --> 02:08.560
electronics.

02:08.560 --> 02:10.760
So that's where I started.

02:10.760 --> 02:12.160
Pretty much embedded system tried.

02:12.160 --> 02:15.360
How do you build devices that was always fascinating to me?

02:15.360 --> 02:21.720
So I finished undergrad becoming sort of good at building devices for embedded systems.

02:21.720 --> 02:28.280
And then when I stayed a year after undergrad to do research, one of my mentors in undergrad

02:28.280 --> 02:34.040
was he had this research project and he was like, hey, I think you really do good on

02:34.040 --> 02:35.040
this.

02:35.040 --> 02:41.960
He was interested in building a wearable monitor for monitoring asthma, like monitoring

02:41.960 --> 02:42.960
the lungs, right?

02:42.960 --> 02:47.640
Like if you can listen to what is going on in the lungs, then maybe you can better advise.

02:47.640 --> 02:52.720
So he asked me to come on board and I was excited to come on board and just explore that project

02:52.720 --> 02:54.440
for a year.

02:54.440 --> 02:56.280
And it really came down to two things.

02:56.280 --> 03:01.600
One was how do you build the hardware that can really listen to, you know, can acquire

03:01.600 --> 03:03.560
the sound really well, right?

03:03.560 --> 03:07.480
And then the second phase is once you have that hardware in place, right?

03:07.480 --> 03:09.880
How do you detect these sounds well enough?

03:09.880 --> 03:15.760
How do you detect sounds and symptoms like coughing, whizzing or even when someone is

03:15.760 --> 03:16.760
panting?

03:16.760 --> 03:18.560
How do you detect them and do that very well?

03:18.560 --> 03:24.760
And so that one year I did, we came pretty far on, like, device in a good hardware.

03:24.760 --> 03:27.880
And so I started looking at, you know, how do we do detection?

03:27.880 --> 03:30.240
What are the algorithms that exist in for detection?

03:30.240 --> 03:34.720
That's what brought me to classifiers and machine learning and looking at that.

03:34.720 --> 03:38.600
And so after that one one year, we're all excited about the work as I say, hey, let's

03:38.600 --> 03:43.480
explore this as a PAD work and I start going to work on it.

03:43.480 --> 03:48.720
And it was mainly from the standpoint of your, okay, how do I, now that I have some hardware,

03:48.720 --> 03:50.840
how do I make the signal really great?

03:50.840 --> 03:55.160
And then how do I really detect these sounds very well?

03:55.160 --> 04:00.720
So started from traditional classifiers, like, you know, decision trees, hidden macaw models

04:00.720 --> 04:03.600
because those were the things that people were doing around that time.

04:03.600 --> 04:08.600
And then all the way meeting to, you know, deep learning, looking at, like, convenants

04:08.600 --> 04:11.400
and then more recently into recurrent.

04:11.400 --> 04:16.000
So that's kind of been the very broad journey, yeah, of mine.

04:16.000 --> 04:22.880
One thing that jumps out at me hearing the story is that you started with the hardware

04:22.880 --> 04:29.560
and then went on to think about the software, is that it stretched me as the opposite of

04:29.560 --> 04:34.960
the way that won my purchase, is that like an academic thing or was there something novel

04:34.960 --> 04:40.040
about the way you were approaching the hardware that you thought was the right thing for this

04:40.040 --> 04:41.040
application?

04:41.040 --> 04:46.240
Yeah, actually, it's interesting you point that out because like, I really started out

04:46.240 --> 04:47.720
interested in hardware, right?

04:47.720 --> 04:49.680
How do I build really good hardware?

04:49.680 --> 04:54.720
And the point wasn't just, you know, build whatever hardware, like, you know, build the

04:54.720 --> 05:01.280
best hardware, I was really constrained for very efficient but affordable hardware.

05:01.280 --> 05:05.720
The initial boost, where, like I mentioned, I'm from Ghana, so our initial boost, where

05:05.720 --> 05:10.880
how do we build devices that can be used in, you know, and the resource communities,

05:10.880 --> 05:15.080
communities where, you know, they cannot afford to buy really expensive devices, right?

05:15.080 --> 05:21.640
In areas that you can have doctors to do diagnosis, can use some of these tools to, you know,

05:21.640 --> 05:22.640
better do diagnose.

05:22.640 --> 05:25.120
So that was really the real motivation.

05:25.120 --> 05:30.520
And so when I was building hardware, even in building the hardware, I realized that,

05:30.520 --> 05:34.240
you know, hardware is expensive, software is cheap, right?

05:34.240 --> 05:37.120
So there is this trade off you can do, right?

05:37.120 --> 05:46.160
You can build a just okay hardware and do more of the signal processing and machine learning

05:46.160 --> 05:48.440
so that you don't have to use, you know, the best sense.

05:48.440 --> 05:52.520
So you can have really cheap hardware and then software is much cheaper, so, you know,

05:52.520 --> 05:53.960
you can duplicate it, right?

05:53.960 --> 05:57.840
So that's where it really, like I followed that trajectory, where I built a hardware

05:57.840 --> 06:01.920
that was like, you know what, this hardware is good enough, I could spend more time making

06:01.920 --> 06:06.000
the best hardware that would be very expensive, but I think this is affordable.

06:06.000 --> 06:10.600
And so now, how do I use software to bridge that gap to go the extra mile, right?

06:10.600 --> 06:15.920
And so that's where it came in and machine learning became a really good answer to that,

06:15.920 --> 06:16.920
right?

06:16.920 --> 06:23.600
And if you have data that is not like 100% like, you know, clean, you can still use advanced

06:23.600 --> 06:29.760
patent recognition tools to extract information from that and make really reasonable inferences

06:29.760 --> 06:30.760
of that.

06:30.760 --> 06:34.760
So it was like that practical approach that brought me into machine learning.

06:34.760 --> 06:41.520
And so before we jump into the machine learning elements, can you give us an overview

06:41.520 --> 06:48.000
of the hardware and the various components? It sounds like acoustics, maybe some microphones

06:48.000 --> 06:50.680
or something like that.

06:50.680 --> 06:54.160
And, you know, what is the processing capability?

06:54.160 --> 06:56.640
What are the various components of the hardware system?

06:56.640 --> 07:02.880
So as I said, I'm interested in this application, we're interested in basically escortating

07:02.880 --> 07:04.240
sounds from the body.

07:04.240 --> 07:06.760
So that's like what a status group does, right?

07:06.760 --> 07:11.280
And so you want an acoustic sensor that you can collect sound from the body.

07:11.280 --> 07:19.040
You could use really advanced microphones, like MEMS, but then those are really, they

07:19.040 --> 07:20.640
can be more expensive.

07:20.640 --> 07:26.880
But also, when you think of collecting data from the body, you can, and you can use pz

07:26.880 --> 07:30.400
electric transducers that have contact sensors, right?

07:30.400 --> 07:34.600
And so when you have that compared to a microphone, it would only pick up sounds that is coming

07:34.600 --> 07:35.960
from the patient.

07:35.960 --> 07:40.520
So already you start off eliminating a lot of the external background sounds, right?

07:40.520 --> 07:46.960
So a huge part of the hardware system is a contact pz electric transducer, also a pz

07:46.960 --> 07:49.360
electric transducer can generate power.

07:49.360 --> 07:51.320
And so it's not that power hungry.

07:51.320 --> 07:56.040
You don't have to, it's going to be really good for low power applications, right?

07:56.040 --> 08:00.720
And so from there, from the pz electric transducer to your raw sensor, you have some analog

08:00.720 --> 08:04.200
front end secretary to condition the signal, right?

08:04.200 --> 08:07.960
You're looking for things like cove sounds and we sounds.

08:07.960 --> 08:10.680
But a person is going to be wearing this most of the time, right?

08:10.680 --> 08:12.680
And people speak all the time.

08:12.680 --> 08:17.280
So you want to make sure that you're actually filtering out a lot of this, you know, the

08:17.280 --> 08:20.880
speech sounds and all of the other possible background sounds.

08:20.880 --> 08:23.680
So that's what a lot of the analog front end does.

08:23.680 --> 08:28.400
So these are things like low pass, high pass, notch filters, that kind of thing?

08:28.400 --> 08:30.000
Exactly, exactly.

08:30.000 --> 08:35.280
So you have, you know, a couple of stages of, you know, classical analog filters where

08:35.280 --> 08:40.280
you are really in the, in the kind of the range that we are interested in, you're looking

08:40.280 --> 08:43.520
for sounds between 100 heads and two kilohertz, right?

08:43.520 --> 08:48.480
So you're attenuating anything outside of that band, that band and sort of really amplifying

08:48.480 --> 08:49.760
the signal there.

08:49.760 --> 08:53.400
Because in that way, you can capture the re sounds, the cove sounds and the things that

08:53.400 --> 08:55.200
are of interest, right?

08:55.200 --> 08:56.200
Yeah.

08:56.200 --> 09:01.000
So once you condition that signal, then now you come to, you know, you need a microprocessor

09:01.000 --> 09:07.280
to sample the, to sample the analog signal and then, you know, maybe preprocess and identify

09:07.280 --> 09:09.720
what events actually okay, right?

09:09.720 --> 09:12.680
And so there you have a lot of options for microcontrollers.

09:12.680 --> 09:16.800
But if you think of something that is going to be wearable for, you know, there's something

09:16.800 --> 09:21.800
that someone will wear for maybe a whole day, that leaves you with only ultra low power

09:21.800 --> 09:22.800
microcontrollers, right?

09:22.800 --> 09:28.440
You cannot think of some of the fancy, you know, and for even M7, you have to go to something

09:28.440 --> 09:30.480
that is really, really low power.

09:30.480 --> 09:36.200
So the huge specification of this was to go for the lowest power microcontroller.

09:36.200 --> 09:38.720
So we are using an ARM M0, right?

09:38.720 --> 09:43.760
It's like the lowest power, the most energy efficient ARM microcontrollers are out there,

09:43.760 --> 09:44.760
right?

09:44.760 --> 09:48.560
So once you go there, that's where all the constraints begin to happen because you don't

09:48.560 --> 09:54.480
have, you don't have floating point operations, you don't have DSP instructions.

09:54.480 --> 09:56.200
So you are really constrained, right?

09:56.200 --> 09:59.800
And you still have to be able to get inference and working at that level.

09:59.800 --> 10:03.760
That's, I guess, pretty much sort of the set up for the hardware.

10:03.760 --> 10:11.720
And so that led you down the path of trying to figure out how to get the various types

10:11.720 --> 10:18.320
of software that you wanted to have working on this hardware, you had to work within kind

10:18.320 --> 10:21.400
of these constraints.

10:21.400 --> 10:28.880
You've got some constraints that are set up by the choice of microcontroller.

10:28.880 --> 10:34.680
Are you also thinking about power envelope and that kind of thing, or is that all inferred

10:34.680 --> 10:37.160
by the microcontroller choice?

10:37.160 --> 10:41.360
So actually, the ultimate and fundamental constraint is power, right?

10:41.360 --> 10:44.200
Power is always, right?

10:44.200 --> 10:45.200
Exactly.

10:45.200 --> 10:47.080
So you start out with power.

10:47.080 --> 10:50.480
And if you really look at the power range, you are talking about, right?

10:50.480 --> 10:52.040
To put it in perspective.

10:52.040 --> 10:58.800
So I like smartphones around the thousand milliwatts, even some of these fun embedded systems

10:58.800 --> 11:03.080
are around hundreds of milliwatts, but we are talking of tens of milliwatts.

11:03.080 --> 11:05.680
So that already really constrains you.

11:05.680 --> 11:10.640
And when you look at all the microprocesses available within that low, ultra-low power

11:10.640 --> 11:11.640
range, right?

11:11.640 --> 11:17.120
You realize they have so many things in common, like they don't have, you know, they don't

11:17.120 --> 11:22.440
have floating point operations, the clocks are normally much lower.

11:22.440 --> 11:25.640
We are looking at things less than 48 megahertz.

11:25.640 --> 11:30.280
And so the constraints become very apparent for that range.

11:30.280 --> 11:36.120
I guess what I was curious about was, you know, certainly within a given power constraint,

11:36.120 --> 11:38.960
you choose your microcontroller.

11:38.960 --> 11:46.920
But I guess I was envisioning a scenario where you want to do things at the software level

11:46.920 --> 11:52.320
to even further manage the power consumption given a specific microcontroller.

11:52.320 --> 11:56.800
Are you having to deal with that kind of thing or did you just constrain the power by choosing

11:56.800 --> 12:00.360
the microcontroller and then you can go hog while you're with?

12:00.360 --> 12:01.360
No.

12:01.360 --> 12:06.360
So that's a perfect question actually, because even with the microcontroller, you have

12:06.360 --> 12:11.400
in place, if you just run it all the time, you're not going to make your power budget.

12:11.400 --> 12:20.000
So you have to do this, you know, event pre-event detection task that can, you know, your

12:20.000 --> 12:23.000
microcontroller is mostly in a very low power state.

12:23.000 --> 12:27.240
And you wake it up when you think that the events you are seeing correspond to, you need

12:27.240 --> 12:29.000
to make an inference on it.

12:29.000 --> 12:33.480
So you have all of those like low levels sort of software already implemented.

12:33.480 --> 12:37.840
It's kind of like the very first layer of your entire detection pipeline, right?

12:37.840 --> 12:41.960
This wake up webbit has an sound event okay and at all, right?

12:41.960 --> 12:44.160
Before you wake up, other things to be processing.

12:44.160 --> 12:46.480
So you have that block link.

12:46.480 --> 12:52.640
It sounds like a low level version of like the wake word for an Alexa type of device.

12:52.640 --> 12:54.840
Exactly, exactly, exactly.

12:54.840 --> 12:55.840
Yeah.

12:55.840 --> 13:00.320
But putting like the whole pipeline is very, very interesting because you have to get all

13:00.320 --> 13:04.160
these blocks in place in order to make all your specifications.

13:04.160 --> 13:10.360
And so you have the hardware in place, I'm imagining that the traditional way of solving

13:10.360 --> 13:17.840
this problem is using kind of time-tested digital signal processing algorithms.

13:17.840 --> 13:21.200
Is that the direction that you'd first go with this or is there something else?

13:21.200 --> 13:22.200
Yeah, yeah.

13:22.200 --> 13:28.080
So when I started my research very early on, right, the traditional way was, you know what

13:28.080 --> 13:34.240
you get your signal and you do like a smart signal processing as you can.

13:34.240 --> 13:37.720
So you're doing things like, you know, what is the best feature extraction that you can

13:37.720 --> 13:38.720
do, right?

13:38.720 --> 13:43.840
So people have come up with so many sort of kind of fifth features that once you extract

13:43.840 --> 13:47.480
those features, it makes it so easy to fail your event, right?

13:47.480 --> 13:51.640
So I'm imagining things like FFTs would come into play here.

13:51.640 --> 13:53.640
Exactly, exactly, exactly.

13:53.640 --> 13:59.120
So FFTs being fast for your transform form is a way to kind of extract the frequency

13:59.120 --> 14:02.640
components of an incoming signal.

14:02.640 --> 14:04.440
Exactly, exactly.

14:04.440 --> 14:10.320
So doing something like that and even other sort of steps on top of the FFTs, so get

14:10.320 --> 14:16.920
something like the spectrails, centroid, the fundamental frequency and all of these things

14:16.920 --> 14:22.240
make it much, much easier so that now you can just use a very simple, say, like, you

14:22.240 --> 14:26.360
know, thresholding to detect when an event has happened, right?

14:26.360 --> 14:29.400
So that was kind of like the assistant work, right?

14:29.400 --> 14:33.840
How do you extract the salient features from this so that classification becomes so,

14:33.840 --> 14:35.040
so much easier, right?

14:35.040 --> 14:36.040
Okay.

14:36.040 --> 14:40.040
But of course, the handicap of that is, what is the best feature, right?

14:40.040 --> 14:46.080
So FFTs spend so many years sort of identifying the best features for say, curve detection

14:46.080 --> 14:47.680
for whiz detection.

14:47.680 --> 14:50.320
And I started out with looking at all of that.

14:50.320 --> 14:54.960
And that's when the promise of deep learning became really exciting because in deep learning

14:54.960 --> 14:58.240
now, you don't have to hunt engineer features, right?

14:58.240 --> 15:02.440
You can do the learning of both the features and the classifier jointly.

15:02.440 --> 15:07.680
So that's actually what pushed me into that direction that, hey, what I'm sitting here

15:07.680 --> 15:12.480
trying to hand crab the best feature sets, but it can actually be automated as part of

15:12.480 --> 15:13.480
the process.

15:13.480 --> 15:16.400
So how about we look at that and see how that compares?

15:16.400 --> 15:21.240
And so as a, you know, someone who's equally excited about deep learning, I kind of get

15:21.240 --> 15:31.120
that as a direction, but I wonder if you've got, you know, a very well defined kind of body

15:31.120 --> 15:35.560
of work for that's, you know, there's already handing you the features, kind of telling

15:35.560 --> 15:37.400
you the way to do it.

15:37.400 --> 15:40.080
What's the real advantage that deep learning is giving you?

15:40.080 --> 15:41.080
Yeah.

15:41.080 --> 15:46.360
So one of the key things to start realizing was these models with that, these kind

15:46.360 --> 15:50.600
of like handcrafted features were there, but one of the areas that they really struggled

15:50.600 --> 15:55.920
to do with was how do you handle, you know, the temporal nature of the data, right?

15:55.920 --> 15:56.920
Okay.

15:56.920 --> 15:59.240
That means it's pretty key.

15:59.240 --> 16:04.760
So a typical part actually has very key characteristics.

16:04.760 --> 16:07.120
You have about three phases.

16:07.120 --> 16:13.120
So there's a phase phase that is sort of the explosive phase.

16:13.120 --> 16:19.240
You literally have like these three phases that are very distinctive to the core, right?

16:19.240 --> 16:23.080
And the properties in any of these three states are very different, right?

16:23.080 --> 16:28.320
But then most of the previous methods were all sort of pretty much like think of them as

16:28.320 --> 16:33.640
a static way of approaching and not really taking advantage of the temporal properties.

16:33.640 --> 16:36.840
So they could only do so well, right?

16:36.840 --> 16:42.120
And people had now started using things like hidden Markov models to try to model those

16:42.120 --> 16:46.920
temporal patterns and those were sort of like working very well, but then hidden Markov

16:46.920 --> 16:52.040
models are pretty, you know, basically algorithms to start dealing with, right?

16:52.040 --> 16:59.800
And so a good segue was, hey, and I started out also like implementing HMMs for this application.

16:59.800 --> 17:05.160
And at that level, you've already gone into the complexity that, you know, you can now

17:05.160 --> 17:09.920
use some of that even deep learning models that these days much easier than, you know,

17:09.920 --> 17:12.480
a full blown hidden Markov model, right?

17:12.480 --> 17:18.600
So you quickly run into the area where you realize that this traditional approach was

17:18.600 --> 17:21.120
not doing the work quite right.

17:21.120 --> 17:26.720
And congestion becomes even much more complex when you start thinking of different patients,

17:26.720 --> 17:32.480
or main patient to patient variability, so the sounds, the cough sounds sound very different

17:32.480 --> 17:34.000
from different patients.

17:34.000 --> 17:37.920
And when they have different conditions, it makes it sound like, you know, very different.

17:37.920 --> 17:43.120
So you quickly run into the issue where you just feel like, no, I need better models

17:43.120 --> 17:44.840
to really capture this.

17:44.840 --> 17:48.600
How far down the path of the traditional models did you go?

17:48.600 --> 17:49.760
Did you try?

17:49.760 --> 17:54.640
Did you have a working HMM that you could later compare to deep learning?

17:54.640 --> 17:55.640
Yeah.

17:55.640 --> 17:59.000
So that was actually one of my very first papers.

17:59.000 --> 18:04.720
I did a whole survey on all of these traditional algorithms that folks have been using.

18:04.720 --> 18:09.520
They had decision trees, you had your SVM, you had the HMM with like, you know, Gaussian

18:09.520 --> 18:11.640
make some model observations.

18:11.640 --> 18:16.200
And then you had like the, you know, the early neural networks I started working on.

18:16.200 --> 18:20.320
So the first one I started being was to use convolutional neural networks.

18:20.320 --> 18:25.880
And they compared very well, they compare like, you know, much better than the HMM, right?

18:25.880 --> 18:27.200
Which was very, very impressive.

18:27.200 --> 18:33.440
And at that time, I was using the convolutional neural net on top of the spectrogram, right?

18:33.440 --> 18:36.480
I was using, yeah, on top of the spectrogram.

18:36.480 --> 18:41.280
So it was being able to learn both the, you know, the temporal and spatial in terms of

18:41.280 --> 18:47.720
the frequency components of the sound and helping you make decisions, like make good inferences

18:47.720 --> 18:49.040
based off on that.

18:49.040 --> 18:54.400
So I started off actually validating that, hey, actually, these deep learning approaches

18:54.400 --> 18:59.280
work much better than the traditional, you know, approaches, those are some of my

18:59.280 --> 19:03.560
anyways, okay, before jumping or into it, okay.

19:03.560 --> 19:11.480
So I'm imagining the early work into CNN's, you, you validated all this, but you were

19:11.480 --> 19:15.960
doing this on desktop computers and then you had to figure out, okay, how do I get this

19:15.960 --> 19:18.960
thing to actually run on the devices?

19:18.960 --> 19:21.240
Exactly, exactly, exactly.

19:21.240 --> 19:25.680
Before we dig into into that, I just remember a question I had from your earlier description

19:25.680 --> 19:28.440
of the hardware.

19:28.440 --> 19:37.800
You mentioned that you were using traditional analog stages to condition the signal.

19:37.800 --> 19:42.120
One of the great things, and you mentioned this about deep learning is that you don't

19:42.120 --> 19:51.560
have to do a lot of feature engineering and often you find that by giving the network,

19:51.560 --> 19:55.840
you know, as much data and not doing as much pre-processing as you might otherwise do,

19:55.840 --> 20:00.840
you can find patterns that haven't traditionally been used, like maybe a cough has like these

20:00.840 --> 20:05.840
really high frequency components that no one really thought about, but that can be predictive.

20:05.840 --> 20:08.640
Did you explore that angle at all?

20:08.640 --> 20:13.440
Yeah, so I did, I did with sort of the first combination, right?

20:13.440 --> 20:21.120
So this was around, I started this week, around 2014, 2015 using the combination, and

20:21.120 --> 20:27.160
so I was using like piano at that time, and I started visualizing the features it was

20:27.160 --> 20:28.760
learning, right?

20:28.760 --> 20:33.560
Of course, some of them were, there were some that looked really interesting, so if you

20:33.560 --> 20:38.560
look at the spectrogram, normally in speech, you have these harmonics, right?

20:38.560 --> 20:43.720
Where you would have these kind of distinct fine lines in the spectrum, but cough is just

20:43.720 --> 20:45.600
a burst of energy, right?

20:45.600 --> 20:47.920
So it's quite over the entire place, right?

20:47.920 --> 20:50.440
It's like the whole place is just full of energy.

20:50.440 --> 20:54.400
And you could see that what the network was actually doing was for the cough.

20:54.400 --> 21:00.400
It was really activated by like a broad range across the spectrum.

21:00.400 --> 21:05.920
For us, like for the speech, it was mostly activated by these sort of lines, right?

21:05.920 --> 21:10.200
And in that space, I thought I was discriminating between cough and speech.

21:10.200 --> 21:15.200
So you could definitely see that it was learning this notion, and was being activated by the

21:15.200 --> 21:20.760
fact that in cough, you have a burst of energy spread across a wide range of frequency,

21:20.760 --> 21:22.760
and it would highlight those.

21:22.760 --> 21:27.000
Given that, why is it still important to do the conditioning?

21:27.000 --> 21:33.080
Oh, so the reason why it's actually important to do the conditioning is because of the sense

21:33.080 --> 21:34.680
that we are using, right?

21:34.680 --> 21:41.480
So like I mentioned, I'm using this piezoelectric transducer, which is, it's not like you

21:41.480 --> 21:46.360
know, your best microphone is actually a good, it's used for like guitar pickups.

21:46.360 --> 21:47.960
So the sense is not great to begin with.

21:47.960 --> 21:51.600
So your signal is actually really bad to start out with.

21:51.600 --> 22:00.680
So the conditioning that we do on there is one to sort of make sure that hey, your signal

22:00.680 --> 22:06.040
is at least clean enough that it's even audible when you hear that, you know, a person

22:06.040 --> 22:11.040
can discern something because without that conditioning, you can even hear the different

22:11.040 --> 22:17.800
events that okay, but yeah, but I think what was really critical is you do have a point

22:17.800 --> 22:23.360
because we could have done more conditioning on there to get a really great signal, right?

22:23.360 --> 22:25.720
But that would have defeated the point.

22:25.720 --> 22:31.240
And so because you could have added more stages, just like really good operational amplifiers,

22:31.240 --> 22:33.480
but we really did the minimal.

22:33.480 --> 22:36.960
So we have like a one stage filter, for instance, right?

22:36.960 --> 22:41.240
A minimum processing so that you can use the rest of the deep learning pipeline to take

22:41.240 --> 22:42.840
advantage of that, right?

22:42.840 --> 22:48.800
So we do sort of really use the benefit of the not having to clean up the signal too much,

22:48.800 --> 22:50.800
except we do do a little cleaning.

22:50.800 --> 22:51.800
Got it, got it.

22:51.800 --> 22:55.040
Yeah, that was the intuition that I was trying to explore.

22:55.040 --> 22:56.040
Yeah.

22:56.040 --> 22:57.040
Awesome.

22:57.040 --> 23:02.480
So for your latest paper, which you presented at NURP set the the black and AI workshop,

23:02.480 --> 23:10.280
which is all about an optimized recurrent unit for ultra low power acoustic event detection.

23:10.280 --> 23:15.520
Tell us about that paper and the key challenges you're trying to solve there.

23:15.520 --> 23:16.520
Yeah.

23:16.520 --> 23:17.520
So that's great.

23:17.520 --> 23:20.600
And I think it follows very well, we kind of like what we've been discussing up on

23:20.600 --> 23:21.600
to now.

23:21.600 --> 23:25.600
So like I mentioned, I started with convolutional neural nets and the comments are great,

23:25.600 --> 23:26.600
right?

23:26.600 --> 23:27.600
We all love them.

23:27.600 --> 23:28.600
They do awesome.

23:28.600 --> 23:33.760
And even these days there is an argument that hey, recurrent neural nets are no longer

23:33.760 --> 23:39.040
necessary, because if you see some of Google's work with on like wave net, you see these

23:39.040 --> 23:44.480
direct solutions are doing great for speech recognition and like speech synthesis.

23:44.480 --> 23:48.640
So why do we care about recurrent net at all?

23:48.640 --> 23:53.040
And the motivation for that was first recurrent net when you're thinking of like really

23:53.040 --> 23:59.880
constrained sort of applications where you don't have a lot of time for you can't admit

23:59.880 --> 24:01.280
a lot of late things, right?

24:01.280 --> 24:06.680
In combination, if you think about it, you literally have to buffer up the entire event

24:06.680 --> 24:11.200
so you can apply the confidence, and you have to apply windows, but recurrent net have

24:11.200 --> 24:17.120
this natural nature where they can process data one time step at a time, right?

24:17.120 --> 24:18.120
So that's really great.

24:18.120 --> 24:21.080
It's great for application because it's just like a digital filter.

24:21.080 --> 24:25.560
You can implement it in such a way that a sample comes in, you process it, the next sample

24:25.560 --> 24:30.160
comes in, you process it, and you have really, really low latency, right?

24:30.160 --> 24:33.240
So that was one of the big promises for starting this, right?

24:33.240 --> 24:36.360
But then once we started, okay, we have this recurrent net.

24:36.360 --> 24:39.360
We can train them and they can do very well on the task.

24:39.360 --> 24:41.320
How do we put them on the device?

24:41.320 --> 24:48.920
Then it's like, wow, we open a whole box of challenges because it's interesting.

24:48.920 --> 24:54.040
And there are a lot of people working on deploying deep learning to the edge.

24:54.040 --> 24:58.920
And you know, you have, you know, really great hardware systems, like the Nvidia drive.

24:58.920 --> 25:06.120
You have like Jetson, which is like this small embedded system board by Nvidia that you can

25:06.120 --> 25:09.440
true, you know, neural nets on there and they run just fine, right?

25:09.440 --> 25:14.840
But when you look at all of these hardware, you are just above our power budget, right?

25:14.840 --> 25:19.880
We are running at either tens of thousands of milliwatts or thousands, and we are literally

25:19.880 --> 25:25.440
looking for something that's within, you know, tens of milliwatts, right?

25:25.440 --> 25:31.560
Something that looked very promising was this library, it's called, like, new tensor or

25:31.560 --> 25:33.360
micro tensor.

25:33.360 --> 25:38.760
It's supposed to be a really lightweight tensor tool like we that can run on certain embedded

25:38.760 --> 25:39.760
boards.

25:39.760 --> 25:44.800
But then when you look at that, it's still above the power range of these really, really

25:44.800 --> 25:48.240
ultra-low power, like, you know, microcontrollers, right?

25:48.240 --> 25:54.480
And so the challenges, can we take the classical models and put them on these microcontrollers?

25:54.480 --> 25:58.320
And the first thing realized that no, if you look at the specifications on the, you

25:58.320 --> 26:04.520
are thinking of, you know, memory of, you're thinking of memory of like less than 32 kilobytes,

26:04.520 --> 26:08.560
you don't have floating point operations, that's like a no, no, right?

26:08.560 --> 26:13.920
So the real challenges was, well, how do we look at the, you know, some of these recurring

26:13.920 --> 26:14.920
neural networks?

26:14.920 --> 26:20.360
I was looking at particularly the gated recurring unit, now can I optimize it all the way down

26:20.360 --> 26:23.520
so that it can run on my target device, right?

26:23.520 --> 26:25.280
So that was the motivation, right?

26:25.280 --> 26:29.880
But when you look at the current systems, they are limited precisely in terms of power,

26:29.880 --> 26:30.880
right?

26:30.880 --> 26:35.120
And if we can come up with a way of optimizing them, such that you can run on this unit,

26:35.120 --> 26:36.920
then that would be great, right?

26:36.920 --> 26:42.920
So once that problem was formulated, I run into the, you know, the main areas in which

26:42.920 --> 26:45.040
I can start optimizing them, yeah.

26:45.040 --> 26:46.040
Okay.

26:46.040 --> 26:53.120
And just to get a sense of the scope of this, you said you're trying to optimize this gated

26:53.120 --> 26:58.920
recurrent unit all the way down, does that mean you're like writing it in assembly and,

26:58.920 --> 27:03.680
you know, and writing it to, you know, to run on this physical device at the instruction

27:03.680 --> 27:04.680
level?

27:04.680 --> 27:05.920
Like how far down are we going here?

27:05.920 --> 27:06.920
Oh, okay.

27:06.920 --> 27:10.320
So I'm writing it to run just at C level, right?

27:10.320 --> 27:11.320
Okay.

27:11.320 --> 27:18.320
So it's just, yeah, see, not assembly, I think with me, but just at C, right?

27:18.320 --> 27:24.840
But that means, you cannot just, you know, grab your TensorFlow model and just run it on

27:24.840 --> 27:25.840
TensorFlow Lite, right?

27:25.840 --> 27:30.760
That means you're going to have to write everything from scratch in a very optimized way.

27:30.760 --> 27:37.600
And so this, how did you approach this process of writing the, of kind of writing it to run

27:37.600 --> 27:38.600
on the device?

27:38.600 --> 27:44.440
Was it, and we, you talked a little bit about challenges, but was it was, was it just

27:44.440 --> 27:50.440
kind of work or were there's particular problems that you ran into that you had to overcome?

27:50.440 --> 27:51.440
Yeah.

27:51.440 --> 27:57.160
So there were, there were actually like three main areas I identified that, you know,

27:57.160 --> 28:01.760
of like optimization that could really make this feasible.

28:01.760 --> 28:07.760
So just sort of stat, and a lot of these had been, you know, already resets and explored,

28:07.760 --> 28:08.760
right?

28:08.760 --> 28:10.280
In isolation, but they had been done.

28:10.280 --> 28:12.600
That's one of the great things about the deep learning community.

28:12.600 --> 28:16.400
There are so many researchers working on incredible things.

28:16.400 --> 28:21.200
And so just taking the time to look at all the work you would see that a lot of people

28:21.200 --> 28:25.560
have worked on, how do you really quantize which, right?

28:25.560 --> 28:27.600
So there was a lot of work there.

28:27.600 --> 28:34.800
And how do you make which run on, run, how do you see, maybe build new networks without

28:34.800 --> 28:37.040
even using a lot of multiplication?

28:37.040 --> 28:39.960
So there were a lot of these, you know, research out there.

28:39.960 --> 28:45.360
And when I really sat down and started like, you know, thinking of implementing, I did

28:45.360 --> 28:51.440
this in steps by the four main areas that I did was, the first was sort of in the gated

28:51.440 --> 28:53.120
recurring unit.

28:53.120 --> 28:58.720
So if you think of the gated recurring units, actually an optimized form of the LSTM unit,

28:58.720 --> 28:59.720
right?

28:59.720 --> 29:02.160
Where, you know, in LSTM, you have a lot of gates.

29:02.160 --> 29:05.280
In the gated recurring unit, you just have two gates.

29:05.280 --> 29:08.320
You have your reset gate and your data gate.

29:08.320 --> 29:13.880
So I was like, wait, if we can optimize LSTM to goo, then we can optimize goo even down,

29:13.880 --> 29:14.880
right?

29:14.880 --> 29:17.160
What happens if we just use one gate, right?

29:17.160 --> 29:21.200
And there's a lot of, there were some, there are some, a few weights out there actually

29:21.200 --> 29:29.240
from 2017, 2017, 2018, there were a few researchers who had just woken on this idea of, hey, what

29:29.240 --> 29:33.960
if we true out one of the gates, particularly the reset gate?

29:33.960 --> 29:40.200
So two main papers stood out, there was, there's one paper from Banger's Group, which said

29:40.200 --> 29:45.680
in speech recognition applications, you can actually omit the reset gate.

29:45.680 --> 29:52.880
And because speech doesn't change so much, it doesn't change too quickly, like you don't

29:52.880 --> 29:59.440
have these sudden discontinuities, it tends to work just as okay as having a group.

29:59.440 --> 30:04.880
And so I took that and sort of tried to apply that to my case of like, okay, maybe I can

30:04.880 --> 30:08.640
get rid of the reset gate.

30:08.640 --> 30:12.760
And when I, in my first simulations, when I did it, I realized that for my application,

30:12.760 --> 30:14.800
it actually tends to work well.

30:14.800 --> 30:19.760
And yes, conditection is different from speech because you do have, you know, sudden changes

30:19.760 --> 30:26.360
when the event happens, but then I validated that you can actually omit the reset gate

30:26.360 --> 30:29.200
and get it to work as much, right?

30:29.200 --> 30:34.760
The next, once I was able to do that, so you cut down one gate and get that expensive

30:34.760 --> 30:41.080
by the way, because it gets means you need weights, you need a lot of weights you need,

30:41.080 --> 30:45.120
you need a lot of computations to do the dot product, like do the weight and the activation

30:45.120 --> 30:46.120
function.

30:46.120 --> 30:51.760
So just by removing one gate, it's like you cut down the network by 33% in terms of memory

30:51.760 --> 30:53.960
and, you know, computation.

30:53.960 --> 30:59.120
Like, so once I got rid of the gates, the next, the main area that everyone thinks of

30:59.120 --> 31:04.640
when you're thinking of, you know, making optimizing neural net is quantization, right?

31:04.640 --> 31:06.240
So I have to do quantization.

31:06.240 --> 31:09.880
But what was popular out there was 8-bit quantization.

31:09.880 --> 31:16.720
I think even today you can use TensorFlow or even PyTorch and you can get 8-bit weights equivalent

31:16.720 --> 31:18.920
of whatever network you train, right?

31:18.920 --> 31:22.920
But I knew 8-bit wasn't going to cut it for my publication, so we have to go all the

31:22.920 --> 31:25.360
way down to just three bits, right?

31:25.360 --> 31:27.320
Three-bit quantization for the network.

31:27.320 --> 31:34.840
So I also did a lot of experiments and simulations to ensure that with three weights, I can still

31:34.840 --> 31:39.840
sort of train the network and have it run, well, and the trick there was to do what people

31:39.840 --> 31:46.520
have done in the past, which is during training, you apply the quantization as kind of like

31:46.520 --> 31:52.280
functions within your computational graph so that you are simulating the quantization process

31:52.280 --> 31:53.280
while you are training.

31:53.280 --> 32:00.520
And the next trick is learning about it and shifting the weights to the right place.

32:00.520 --> 32:04.320
Something else too with the quantization was rather than just quantizing to three-bit

32:04.320 --> 32:11.320
weights, you could quantize them such that they are just, you know, integer exponents of

32:11.320 --> 32:14.760
two or like, you know, the inverse of integer exponents of two.

32:14.760 --> 32:18.640
And if you did it that way, then you don't need multiplications anymore because you can

32:18.640 --> 32:24.320
replace all multiplications with, you know, with base shifts, right?

32:24.320 --> 32:29.640
So to recap the two main areas, one was true, I will show a way one of the gates and then

32:29.640 --> 32:36.080
you know, cut down the network computation and memory by 33% do this kind of exponential

32:36.080 --> 32:41.600
with quantization all the way down to three-bit and that saves you a lot of memory, but also

32:41.600 --> 32:46.280
enables you to just do base shifts and no multiplications, which you can do on the

32:46.280 --> 32:49.160
ultra-loop power microcontroller, right?

32:49.160 --> 32:56.200
And then the other main area of optimization was we don't have, we don't have two team

32:56.200 --> 32:58.880
points units on the microcontroller.

32:58.880 --> 33:04.800
So to get rid of every floating point of operation and replace that with integer operations, right?

33:04.800 --> 33:09.440
So the whole network on the microcontroller has to be implemented in such a way that you

33:09.440 --> 33:11.640
are just using integer operations, right?

33:11.640 --> 33:17.840
So all the activation functions, all the, you know, the linear transformations, the

33:17.840 --> 33:21.640
weak multiplication and the dot product, they all have to be done in such a way that

33:21.640 --> 33:27.400
you're operating within integer, you know, framework, I was using like 16-bit integers and

33:27.400 --> 33:31.960
then you are applying all the necessary collecting so you don't overflow, right?

33:31.960 --> 33:38.800
So those were the main sort of key areas of quantization, the integer arithmetic throughout,

33:38.800 --> 33:44.800
train out the single gate, like, train out the reset gate, and then also using fast activation

33:44.800 --> 33:47.240
because those are ourselves, right?

33:47.240 --> 33:49.080
Using what was that last one?

33:49.080 --> 33:50.080
Fast activations?

33:50.080 --> 33:55.560
Yeah, I have to, like, you know, so in the traditional, you know, recurring unit or

33:55.560 --> 34:01.560
updated recurring unit, you have the sigmoid and a time each and those are expensive on

34:01.560 --> 34:02.560
the microcontroller.

34:02.560 --> 34:06.600
Whenever you have to do an exponential on the microcontroller, if you are thinking all

34:06.600 --> 34:09.280
child do power, they become very painful.

34:09.280 --> 34:15.240
And so what I did was to use soft sign function instead, you can have like, soft sign variance

34:15.240 --> 34:19.880
of the time each and a soft sign and it works just as well, right?

34:19.880 --> 34:24.560
So these are the four main areas I had to, I had to bring together in order to really

34:24.560 --> 34:26.040
pull this off.

34:26.040 --> 34:31.160
One of the things that occurs to me here is a lot of times when we're trying to get networks

34:31.160 --> 34:36.120
to converge, we're doing things like drop out and other things that are like adding noise

34:36.120 --> 34:41.960
which kind of helps with regularization, but your process is inherently adding noise.

34:41.960 --> 34:49.400
So does that change the way you do regularization or eliminate the need to do regularization

34:49.400 --> 34:51.040
when you're trying to train the network?

34:51.040 --> 34:57.480
Yeah, so I don't do any additional regularization apart from these methods that I'm talking

34:57.480 --> 34:58.480
about.

34:58.480 --> 35:02.600
And almost every one of them you can actually think of as a regularization and you see

35:02.600 --> 35:03.600
that effect.

35:03.600 --> 35:10.640
So if I train on a really big network, it actually trains, you are able to convey to

35:10.640 --> 35:17.200
about the same or even it's slightly better than what you would with like a full precision

35:17.200 --> 35:22.600
network, right, but then when you go like really when you are training a very, very small

35:22.600 --> 35:29.080
network, the full precision will, you know, do better, but yes, you do see the regularization

35:29.080 --> 35:34.840
effect of these and you don't need to do any additional external regularization.

35:34.840 --> 35:40.240
I think one thing also that I have to point out is you see the whole simulation, the quantization

35:40.240 --> 35:46.480
but also the fixed point in the gyrismatic, it pretty much means you are sort of clipping

35:46.480 --> 35:50.520
how high the outputs of certain nodes can go, right?

35:50.520 --> 35:55.920
So nothing can go above the highest number in say 16 bit into the presentation.

35:55.920 --> 35:58.240
So it's almost like you're doing with clipping, right?

35:58.240 --> 36:02.320
You're doing all of these things that you traditionally do with regularization.

36:02.320 --> 36:05.120
In fact, really strict regularization.

36:05.120 --> 36:10.480
So it ends up training very well and then you're able to get a network that function.

36:10.480 --> 36:18.360
So you made a comment about how for large networks, it works as well or better than,

36:18.360 --> 36:23.960
than non, you know, networks where you're not doing these four things.

36:23.960 --> 36:29.000
How broadly does that generalize, is that, you know, just for this specific problem?

36:29.000 --> 36:35.000
Or, I mean, we always care about, you know, compute and usually care about power.

36:35.000 --> 36:40.880
Like could you just take this and apply it to something totally, you know, a totally

36:40.880 --> 36:41.880
different kind of problem?

36:41.880 --> 36:45.680
And do you think it would work or there are specific things about your problem that

36:45.680 --> 36:48.800
allows you to make these compromises?

36:48.800 --> 36:55.720
So actually in my paper, in the work that I presented in the paper, I actually wrote

36:55.720 --> 37:02.320
a paper on this that would be coming out for you become, like maybe later this year.

37:02.320 --> 37:06.520
What we realized was we tried on two additional like tasks, right?

37:06.520 --> 37:10.640
One was recognizing spoken digits, right?

37:10.640 --> 37:13.000
And you see that it still waits, right?

37:13.000 --> 37:18.080
In like, when you're trying to Google has this speech command dataset, it pretty much has

37:18.080 --> 37:25.480
like one second audio recording of sounds like on or, and you have 20,000 flat examples,

37:25.480 --> 37:26.480
right?

37:26.480 --> 37:31.320
And the way it's just as well, where you begin to see it hurting is when you look at the

37:31.320 --> 37:36.560
test as we consider this was called like a band sounds where you have really, really long

37:36.560 --> 37:37.560
recordings.

37:37.560 --> 37:41.720
You have recordings that are about four seconds long, right?

37:41.720 --> 37:47.560
And sequences that are about 100 plus, you know, instances that you begin to see it hurting.

37:47.560 --> 37:51.320
I think one of the reason why it begins hurting is because when you don't have the reset

37:51.320 --> 37:57.560
gate, you are pretty much now like you're limiting the long-term memory of the network, right?

37:57.560 --> 38:02.960
Like the very advantage of the group and then LSTM, like your handicrafting, the network

38:02.960 --> 38:07.480
is that because it can't really remember for too long, right?

38:07.480 --> 38:13.120
So I think these optimizations make it very suitable for applications like, you know,

38:13.120 --> 38:18.480
keywords, potten, or where you are, literally detection sounds that are very short, you

38:18.480 --> 38:21.400
know, or not very short, but relatively short.

38:21.400 --> 38:27.240
So, you know, anything one second, two second and below, then it becomes really promising.

38:27.240 --> 38:31.400
And then once you are looking at really long sequences, like you would in speech recognition,

38:31.400 --> 38:33.560
then it would not be ideal, right?

38:33.560 --> 38:38.640
And so you mentioned two additional tasks was one the digits and the other was the speech

38:38.640 --> 38:40.440
commands or were those one?

38:40.440 --> 38:45.080
Yeah, those were one the speech commands has the spoken digits.

38:45.080 --> 38:47.440
The test was this a band sounds.

38:47.440 --> 38:52.240
If the data says it's available online and they have, it's actually a really difficult

38:52.240 --> 38:53.240
task.

38:53.240 --> 38:59.240
You have like recordings of environments, you have things like child playing, car honking,

38:59.240 --> 39:02.280
and you know, you're supposed to identify these classes.

39:02.280 --> 39:07.880
You have about 20 classes or 10 classes, 10 to 20 classes, one of those.

39:07.880 --> 39:11.960
And sometimes it's really even hard for a person to discriminate between them because

39:11.960 --> 39:15.960
you know, the sounds are long and then the context is also very broad, right?

39:15.960 --> 39:21.200
So those, that was a very difficult task for our model.

39:21.200 --> 39:28.080
So we're performing maybe 10 to 15% ways than using a full precision, like, you know,

39:28.080 --> 39:29.080
green network.

39:29.080 --> 39:30.080
Okay.

39:30.080 --> 39:33.440
What was that data set called again?

39:33.440 --> 39:35.560
Abans sounds data set.

39:35.560 --> 39:41.520
So I think if I remember correctly, it's from, yeah, they have two.

39:41.520 --> 39:49.400
There's the Abans sound 8K and then like a more Abans sound like with even longer recordings

39:49.400 --> 39:52.200
and we have a paper out there on it.

39:52.200 --> 40:00.000
I think it's a data set in taxonomy for Abans sound research was published in an ACM conference

40:00.000 --> 40:01.360
on multimedia.

40:01.360 --> 40:04.800
So it's a data set that has been out there since 2015.

40:04.800 --> 40:05.800
I'm sorry.

40:05.800 --> 40:06.800
Yeah.

40:06.800 --> 40:07.800
All right.

40:07.800 --> 40:12.280
So you've developed this GRU kind of all the way down.

40:12.280 --> 40:18.840
You have demonstrated that it works for your task and reasonably well for other tasks.

40:18.840 --> 40:22.200
Are you done then or is there more work that you had to do here?

40:22.200 --> 40:23.200
Yeah.

40:23.200 --> 40:30.120
So I think right now for, and I actually had deployed it to the device and have it like,

40:30.120 --> 40:35.280
you know, run on examples in the data set and get like the decent results.

40:35.280 --> 40:38.040
I think one key area to was late, right?

40:38.040 --> 40:41.440
You need this to not just fit, but be fast enough.

40:41.440 --> 40:47.440
And that also we were meeting the right spec actually where I'm going about 60 to 70 times

40:47.440 --> 40:53.480
faster than if you were to just manually put a GRU onto any of these devices.

40:53.480 --> 40:57.520
And so while they all work and everything is like great, you've been able to embed

40:57.520 --> 40:58.520
the model.

40:58.520 --> 41:02.880
The entire pipeline that I talked about, right, also needs to be in place, right?

41:02.880 --> 41:09.200
So right now, it means, you know, our our GRU, our optimized recovery unit can, when

41:09.200 --> 41:11.840
you train it, they can't really detect these sounds.

41:11.840 --> 41:16.480
But then once you put it, once you start thinking, you know, okay, I have a whole streaming

41:16.480 --> 41:17.840
input, right?

41:17.840 --> 41:23.880
How do I ensure that I'm telling off my microprocessor, like at the right time, I'm waking up quickly

41:23.880 --> 41:26.240
enough so that I don't miss the events.

41:26.240 --> 41:32.520
And then my, you know, recurring neural network is also like, it's also processing the incoming

41:32.520 --> 41:33.520
sequence.

41:33.520 --> 41:38.080
And and finally also, how do I, so you get some outputs from your recovery net, right?

41:38.080 --> 41:43.920
But how do you smoothen over those posterior probabilities and actually record the event

41:43.920 --> 41:44.920
at what time?

41:44.920 --> 41:49.120
So those are that sort of additional work, that's what I've been working on, sort of completing

41:49.120 --> 41:51.000
that whole pipeline, right?

41:51.000 --> 41:56.720
Because I don't see the, the RNA is one block in that whole pipeline and you want to get

41:56.720 --> 42:00.520
the whole pipeline working, so this can be used in real time, right?

42:00.520 --> 42:04.400
So that's kind of like the work that I'm finishing on right now.

42:04.400 --> 42:05.400
Nice.

42:05.400 --> 42:10.480
And you're finishing up your PhD, what's next for for you in this project?

42:10.480 --> 42:11.480
Yeah.

42:11.480 --> 42:16.080
I'm very excited, I'm hopefully going to be defending in a month.

42:16.080 --> 42:17.080
Oh wow.

42:17.080 --> 42:20.800
And yeah, so very exciting things.

42:20.800 --> 42:27.400
But actually, my advice and I have found that a company called Clewis to commercialize

42:27.400 --> 42:32.560
these, this Asma monitoring device that I've been talking about, this world would device

42:32.560 --> 42:34.120
for monitoring the lungs, right?

42:34.120 --> 42:37.800
And the company is called Clewis and we are very excited about it.

42:37.800 --> 42:43.160
We just, I was just awarded an SBIR grant to sort of see that too.

42:43.160 --> 42:47.480
So I think for the next year, that's what I'll be, that's what I'll be fully dedicated

42:47.480 --> 42:54.080
to trying to bring the technology out of the lab and into the hospitals to actually use

42:54.080 --> 42:55.720
in patients on patients.

42:55.720 --> 42:56.720
Nice, nice.

42:56.720 --> 43:03.440
And so I imagine that this will have to go through the, kind of the FDA approvals, is this

43:03.440 --> 43:07.200
subject to like clinical trials and all of those kinds of things?

43:07.200 --> 43:08.200
Yes.

43:08.200 --> 43:11.560
So, eventually, it's going to go to FDA.

43:11.560 --> 43:17.080
We've actually been sort of prepping on the right materials and sort of lining up the

43:17.080 --> 43:18.520
right studies we have to do.

43:18.520 --> 43:23.120
I have done some preliminary studies at the hospital on patients' population of even

43:23.120 --> 43:29.800
up to 30, actually testing on patients, but we have to do like a really full-blown study.

43:29.800 --> 43:36.040
But for the next year, we are hoping to work with some medical companies who are already,

43:36.040 --> 43:42.760
you know, doing clinical trials for their respiratory therapeutics, and they need data, they need

43:42.760 --> 43:47.640
data on, you know, have these patients responding to particular drugs, right?

43:47.640 --> 43:53.000
And often, the available data is just subjective reports from patients, oh, I think these

43:53.000 --> 43:57.200
recalls COVID-19, but if they could have something that's more objective, huh?

43:57.200 --> 44:02.720
They were within X, RS throughout this week, and that reduced by 10% the next week, or

44:02.720 --> 44:06.600
their cough went down by this, they'll be really, really, you know, actionable for them

44:06.600 --> 44:08.080
and really valuable to them.

44:08.080 --> 44:14.680
So we are talking with some pharmaceuticals about, you know, the need and positioning the

44:14.680 --> 44:17.200
device in such a way that it can fulfill that need.

44:17.200 --> 44:18.200
Awesome.

44:18.200 --> 44:23.880
And will you also be getting into physical manufacturer of the devices themselves?

44:23.880 --> 44:24.880
Yes.

44:24.880 --> 44:30.840
So, for some of the studies that I did, I had to do like batch sort of manufacturing of

44:30.840 --> 44:36.000
these processes of these devices, because, you know, you're running a small study where

44:36.000 --> 44:41.440
you have maybe 50 subjects, you still have to have a lot of, you know, devices, and you

44:41.440 --> 44:45.400
have to have like packaging down how they're going to arrive at the hospital, how do you

44:45.400 --> 44:49.760
get the information in and out, and so I have had to do that, and I anticipate I'm going

44:49.760 --> 44:55.440
to have to do much more of that, like in this year, once I finished school.

44:55.440 --> 44:56.440
Wow.

44:56.440 --> 44:57.440
So, yeah.

44:57.440 --> 44:58.440
It's exciting.

44:58.440 --> 45:02.880
It sounds like a really exciting project, and you will certainly have your hands full

45:02.880 --> 45:05.200
over the coming months.

45:05.200 --> 45:08.560
I really appreciate you taking the time to share it with us.

45:08.560 --> 45:09.560
Oh, thank you.

45:09.560 --> 45:10.560
Well, this was fun.

45:10.560 --> 45:11.560
It's taking a while.

45:11.560 --> 45:12.560
Fantastic.

45:12.560 --> 45:13.560
Thank you.

45:13.560 --> 45:18.680
All right, everyone, that's our show for today.

45:18.680 --> 45:24.240
For more information on justice or any of the topics covered in this show, visit twimlai.com

45:24.240 --> 45:27.280
slash talks slash 2 3 0.

45:27.280 --> 45:34.240
For more information on our Black and AI series, visit twimlai.com slash Black and AI 19.

45:34.240 --> 46:01.000
As always, thanks so much for listening, and catch you next time.

