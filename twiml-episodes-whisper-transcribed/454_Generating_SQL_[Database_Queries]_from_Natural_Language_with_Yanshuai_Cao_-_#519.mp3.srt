1
00:00:00,000 --> 00:00:15,440
Alright everyone, I am here with Yenshwai Tsau. Yenshwai is a senior research team lead at

2
00:00:15,440 --> 00:00:21,240
Borealis AI. Yenshwai, welcome to the Twomol AI podcast.

3
00:00:21,240 --> 00:00:23,560
Thank you for having me, Sam.

4
00:00:23,560 --> 00:00:27,760
Awesome. I am really looking forward to digging into our conversation. We'll be talking

5
00:00:27,760 --> 00:00:33,280
about Turing, which is a recent project that you've been working on that does text to

6
00:00:33,280 --> 00:00:38,920
SQL. But to get us started, I'd love to have you share a little bit about your background

7
00:00:38,920 --> 00:00:41,960
and how you came to work in machine learning.

8
00:00:41,960 --> 00:00:50,200
Alright, so I did my undergrad at the University of Toronto in Computer Science and Math and

9
00:00:50,200 --> 00:00:57,480
stats and I took a number of AI courses during undergrad and that gave me the opportunity

10
00:00:57,480 --> 00:01:06,720
to work in the computer vision lab run by David Fleet at U of T. And so I did two summer

11
00:01:06,720 --> 00:01:12,840
research projects there and afterward went down to do my PhD with David, close device

12
00:01:12,840 --> 00:01:21,160
by her husband and did my PhD in actually in Gaussian processes, not in computer vision.

13
00:01:21,160 --> 00:01:27,480
And so Gaussian processes is this class of Bayesian and parametric models that learn

14
00:01:27,480 --> 00:01:34,920
very quickly from small number of data points and my work was focusing on scaling the

15
00:01:34,920 --> 00:01:43,160
compute aspect of GP. And on the side, I also worked a little bit on adversarial robustness

16
00:01:43,160 --> 00:01:49,440
of models. So this was the time when people just found out that you can actually apply

17
00:01:49,440 --> 00:01:55,880
this imperceptible perturbation to images and then change the class labels. So we thought

18
00:01:55,880 --> 00:02:01,040
that a, you know, can you change more than just a class label, can actually change the

19
00:02:01,040 --> 00:02:07,000
features to make the features like, you know, another image completely, not just a change

20
00:02:07,000 --> 00:02:13,720
label. And surprisingly, in terms of actually, you can, you can actually take a picture

21
00:02:13,720 --> 00:02:21,280
of me and then apply a perturbation change the feature of me to look like a feature of

22
00:02:21,280 --> 00:02:27,240
a car, for example, not just any car, but a particular car of particular color in that

23
00:02:27,240 --> 00:02:35,560
pose, et cetera. And so that was very surprising at the time, but without, you know, this

24
00:02:35,560 --> 00:02:43,160
adversarial robustness issue is going to be solved very quickly, but turns out to help

25
00:02:43,160 --> 00:02:47,960
me more further from truth, things still consult the problem.

26
00:02:47,960 --> 00:02:51,160
Do you still follow that space?

27
00:02:51,160 --> 00:03:02,120
Not super closely, but it still motivates a lot of things that I do, especially later,

28
00:03:02,120 --> 00:03:07,720
after I joined Borealas, you know, I look at that literature and people found out that

29
00:03:07,720 --> 00:03:12,520
not only can do this on vision, you can do this on an LP as well. And obviously, there

30
00:03:12,520 --> 00:03:17,960
the perturbation is not imperceptible, you know, change to pixels, but you add some extra

31
00:03:17,960 --> 00:03:23,720
text. And also, you can do this in the physical world, apply some patches to images that

32
00:03:23,720 --> 00:03:27,720
are not, you know, changing pixels, but some patterns that people don't pay attention

33
00:03:27,720 --> 00:03:36,080
to. And so, and they're actually very hard to get rid of. So, so what that tells is this

34
00:03:36,080 --> 00:03:42,080
old, you know, theoretical approach of studying and understanding this adversarial tax, we're

35
00:03:42,080 --> 00:03:47,200
not actually capturing what really under the hood, like these people were considering

36
00:03:47,200 --> 00:03:54,760
adversarial tax with like some sort of perturbation with in a ball centered at the image and then

37
00:03:54,760 --> 00:03:58,960
look at the robustness model that way, but that clearly doesn't capture all the other

38
00:03:58,960 --> 00:04:07,680
ones. And fundamentally what it looks like is what the model, how the model represent the

39
00:04:07,680 --> 00:04:14,480
data, represent the word is different from human, you know, from people's representation

40
00:04:14,480 --> 00:04:20,360
and they're not aligned. And it looks like their models are picking up on sort of a short

41
00:04:20,360 --> 00:04:26,640
cause or a speed of correlations or other type of, you know, associations that, you know,

42
00:04:26,640 --> 00:04:34,840
we don't use. And fundamentally, it looks like that's the problem. And, and it looks like,

43
00:04:34,840 --> 00:04:44,840
you know, one has to really, you know, go beyond pattern matching to really, to be able

44
00:04:44,840 --> 00:04:50,040
to get to the root of this problem. So, to look at, you know, model that can reason that

45
00:04:50,040 --> 00:05:00,040
can try to discover the, you know, the type of relationship that people use in recognizing

46
00:05:00,040 --> 00:05:06,280
understanding and reasoning. So, that was, that was my thought at the time. And I think

47
00:05:06,280 --> 00:05:13,200
it was also, you know, thinking share by, you know, lots of people in the field. And so

48
00:05:13,200 --> 00:05:19,920
that led me to work on an LP, because an LP is a lot closer to reasoning, I felt, because

49
00:05:19,920 --> 00:05:25,880
language is already a model of the word. And, yeah, also because, for us, it's part of

50
00:05:25,880 --> 00:05:37,240
a robot Canada. There's a lot of potential application of text of an LP inside a bank.

51
00:05:37,240 --> 00:05:42,520
Tell us a little bit about touring and the motivation for it. How did the project get

52
00:05:42,520 --> 00:05:43,520
started?

53
00:05:43,520 --> 00:05:51,480
Right. So, touring is, is this natural language database interface? It's a demo of a natural

54
00:05:51,480 --> 00:05:59,480
language database interface that we built. And it's really just putting a lot of our work

55
00:05:59,480 --> 00:06:07,600
on semantic parsing this space together in this academic demo. So, natural language database

56
00:06:07,600 --> 00:06:14,040
interface, the, from an application perspective, the, the potential users to allow non-technical

57
00:06:14,040 --> 00:06:19,080
users to interact with structured data set, because there's a lot of insight in there.

58
00:06:19,080 --> 00:06:26,080
And, you know, we want to give it opportunity for non-technical users to get those insights.

59
00:06:26,080 --> 00:06:34,840
And, from a research perspective, it's a very challenging natural language processing

60
00:06:34,840 --> 00:06:43,640
problem, because the underlying problem is you have to parse, question us in English

61
00:06:43,640 --> 00:06:50,200
or any other natural language and, and convert to SQL. And, we all know natural language

62
00:06:50,200 --> 00:06:56,680
is ambiguous. Machine language is all ambiguous. So, you have to resolve all the ambiguity

63
00:06:56,680 --> 00:07:03,080
in order to parse correctly. Furthermore, what's different from something like convert to

64
00:07:03,080 --> 00:07:09,560
Python or other program language is the mapping from patterns to SQL is underspecified if

65
00:07:09,560 --> 00:07:15,600
you don't know the schema. It really depends on, you know, what is the structure of schema.

66
00:07:15,600 --> 00:07:21,800
And so, so model has to really learn how to reason using it, and in order to resolve

67
00:07:21,800 --> 00:07:30,040
all the ambiguity and correctly predict the SQL. And lastly, you know, this, you train

68
00:07:30,040 --> 00:07:33,960
the model on some domain, you don't want to just work on this domain, you want to work

69
00:07:33,960 --> 00:07:38,800
on domains and databases that you've never seen before. So, that's the cross domain,

70
00:07:38,800 --> 00:07:44,840
cross database part of it. And that is a very, very challenging, because it's a completely

71
00:07:44,840 --> 00:07:49,360
different distribution once you move to a different domain.

72
00:07:49,360 --> 00:07:58,480
So we're talking about touring, you know, shortly after the release of OpenAI's Codex.

73
00:07:58,480 --> 00:08:03,160
And so I imagine as you're talking to people about it now, that's a correlation that

74
00:08:03,160 --> 00:08:11,160
folks make, because that project is also focused on code generation. I'm curious, you know,

75
00:08:11,160 --> 00:08:18,080
if you might compare and contrast the different projects and, you know, maybe the complexity

76
00:08:18,080 --> 00:08:24,400
of the challenge or, you know, different aspects that would help us understand how they,

77
00:08:24,400 --> 00:08:27,760
you know, might be similar or not.

78
00:08:27,760 --> 00:08:34,920
Right. So, first of all, Codex, I saw the demos, and the demos are really amazing and

79
00:08:34,920 --> 00:08:42,760
impressive. I think if you look at the where it's applied, it's focusing on product language,

80
00:08:42,760 --> 00:08:49,960
like Python, JavaScript, and all this language where you can find a lot of public, available

81
00:08:49,960 --> 00:08:55,360
training data, like, you know, on GitHub and other sources. I think there are training

82
00:08:55,360 --> 00:09:04,720
on code from GitHub. And so, the availability of data is a one major difference. When you

83
00:09:04,720 --> 00:09:11,120
try to do SQL, as I said earlier, you really have to know the schema, right? There's not

84
00:09:11,120 --> 00:09:17,560
a lot of training data for SQL, public, available, like, people don't post their schema online

85
00:09:17,560 --> 00:09:25,480
generally. So, the amount of data is very, very small that can be used for, you know, even

86
00:09:25,480 --> 00:09:31,560
for, like, modeling will SQL, you can't find a ton of it for training purpose. And so,

87
00:09:31,560 --> 00:09:38,600
so that is, I would say, like, one major difference. So, that's also why Turing is a academic

88
00:09:38,600 --> 00:09:44,320
demo that we built, where it's Codex, obviously, with the, you know, all more data, the model

89
00:09:44,320 --> 00:09:49,320
is, it is, you know, fairly good, and you can build a product of it.

90
00:09:49,320 --> 00:10:00,640
Okay. You mentioned earlier that one of the elements of this project is kind of reasoning

91
00:10:00,640 --> 00:10:07,720
and reasoning on text, and that that's a fundamental aspect that allows you to transform

92
00:10:07,720 --> 00:10:12,480
natural language to SQL. Can you talk a little bit more about the role of reasoning in

93
00:10:12,480 --> 00:10:20,240
this problem? Right. So, so one of the aspects is what I mentioned earlier, like, it's

94
00:10:20,240 --> 00:10:26,480
underspecified. And it's underspecified due to, you know, you have to, you know, really

95
00:10:26,480 --> 00:10:34,240
leverage schema. And the other part is you have to really have common sense. A lot of times,

96
00:10:34,240 --> 00:10:43,080
for example, if I say, you know, which cities have the most number of employees under 30,

97
00:10:43,080 --> 00:10:48,000
under 30 that refers to age of employee. I assume that I have a scheme, a table with

98
00:10:48,000 --> 00:10:55,240
age column. It's never mentioned explicitly. But, you know, the model has to learn to,

99
00:10:55,240 --> 00:11:00,040
that under 30 refers to age and have to make that association. So, that's common sense.

100
00:11:00,040 --> 00:11:08,040
And learning, you know, reasoning using schema, an example is, if there's textual evidence

101
00:11:08,040 --> 00:11:13,680
for two of the tables, but these two tables are, you know, unrelated to each other, and

102
00:11:13,680 --> 00:11:20,560
there's no foreign key links. And the model really has to look at how the schemas link

103
00:11:20,560 --> 00:11:26,360
together, you know, the relationship of elements in order to infer that, hey, maybe there's

104
00:11:26,360 --> 00:11:33,680
a third table that I need to bring in in order to produce the correct SQL. So, all of

105
00:11:33,680 --> 00:11:43,320
this is an example where something is missing, causing it to be ambiguous. And the reasoning

106
00:11:43,320 --> 00:11:50,120
feeling in this missing information allow the model to deconfond the relationship. Otherwise,

107
00:11:50,120 --> 00:11:55,480
you know, if the information is not there, and the model can just pick up, you know, on

108
00:11:55,480 --> 00:12:01,120
spherical correlations, shortcuts that makes learning possible during training, which

109
00:12:01,120 --> 00:12:06,000
of course doesn't generalize, especially on a task like this, where you try to generalize

110
00:12:06,000 --> 00:12:13,480
to a new domain, a new schema, and with very small amount of data.

111
00:12:13,480 --> 00:12:22,360
And how is reasoning implemented in the model? Is it, it is something that's primarily

112
00:12:22,360 --> 00:12:30,600
done at training time, and at inference time, it's a single step, a single step inference,

113
00:12:30,600 --> 00:12:35,560
or is it more something that's done at inference time, where you're, you know, going through

114
00:12:35,560 --> 00:12:40,200
multiple steps to try to reason about a particular query?

115
00:12:40,200 --> 00:12:49,600
Right. So, there's few elements in the model that can, you know, that express some structural

116
00:12:49,600 --> 00:12:57,080
prior about the problem that performs some form of reasoning. One is, you know, it's

117
00:12:57,080 --> 00:13:04,880
overall, it's encoder decoder type of architecture, architecture on the decoder side. The decoder

118
00:13:04,880 --> 00:13:14,120
actually knows the grammar of C-pole, and it's baked into the, you know, the decoder

119
00:13:14,120 --> 00:13:20,360
knows has some sort of knowledge that we designed into the, into the model, and it leveraged

120
00:13:20,360 --> 00:13:26,440
the grammar of C-pole to, to figure out at this step, what are the legal things that can

121
00:13:26,440 --> 00:13:33,200
be produced, and that allows us to prune search space during inference, and during training

122
00:13:33,200 --> 00:13:42,240
and, and it learns what are the things that are likely. And on the encoder side, it's,

123
00:13:42,240 --> 00:13:48,280
the model is, is basically a transformer, but, you know, a special type of transformer,

124
00:13:48,280 --> 00:13:53,040
uh, called a relational way of transformers, and can consider the different religions

125
00:13:53,040 --> 00:14:01,080
of schema elements, and, uh, as well as, you know, association between the, the question

126
00:14:01,080 --> 00:14:08,480
tokens with, with, with the, um, elements in the schema, like a table name, column name,

127
00:14:08,480 --> 00:14:14,680
and you can give something, uh, prior knowledge about this, uh, foreign key relationship,

128
00:14:14,680 --> 00:14:19,960
and, uh, some initial evidence about, uh, which token, the question is likely to be linked

129
00:14:19,960 --> 00:14:26,600
to which column, uh, and, and then during training, uh, the model learns to basically clean

130
00:14:26,600 --> 00:14:31,680
up the sort of relationship, and in, uh, during inference, it can basically, uh, starting

131
00:14:31,680 --> 00:14:38,680
from an initial guess, and then try to refine, uh, the potential link, and then, uh, remove

132
00:14:38,680 --> 00:14:47,040
some ambiguities. It's, uh, it's still some, I'll say it's still, um, rough form of, uh,

133
00:14:47,040 --> 00:14:50,840
reasoning, it's, uh, obviously it doesn't capture all the different type of things that

134
00:14:50,840 --> 00:14:56,480
we wanted to, to continue to do, but, uh, it already, uh, make a big, big difference.

135
00:14:56,480 --> 00:15:02,880
Okay. Um, you, you talked about how you're part of what makes the model work, is that

136
00:15:02,880 --> 00:15:09,080
you're providing information to the transformer during training. Can you talk about how

137
00:15:09,080 --> 00:15:17,920
the, the data is prepared and generated to train the model? Uh, right. So, um, during training,

138
00:15:17,920 --> 00:15:26,680
um, basically the model considers both questions and schema, and, uh, so, uh, the, uh, the, uh,

139
00:15:26,680 --> 00:15:33,280
the model is actually a, uh, consists of a pre-trained, uh, deep transformer, like a Roberta,

140
00:15:33,280 --> 00:15:38,520
and on top of it, it has some extra fresh layer of, uh, relational wear transformer

141
00:15:38,520 --> 00:15:45,640
layers, and, uh, the relational wear transformer layers encode, uh, some prior knowledge, um,

142
00:15:45,640 --> 00:15:52,400
between elements of, uh, you know, columns, relations, and, uh, words, and, you know, uh,

143
00:15:52,400 --> 00:15:57,360
words in the questions, and, uh, words in the, uh, the column, uh, in the table, for

144
00:15:57,360 --> 00:16:05,400
example, if, for instance, if we see a column, uh, in the questions, um, if we see a mention

145
00:16:05,400 --> 00:16:11,960
of, uh, you know, a column of the, uh, name in the questions, it, it provides some initial

146
00:16:11,960 --> 00:16:18,040
evidence, and so it has a link there, and, uh, during training, um, basically the model

147
00:16:18,040 --> 00:16:24,480
tried to maximize the, uh, the, the correct, uh, prediction, and, uh, during that process,

148
00:16:24,480 --> 00:16:30,080
it can learn to adjust the weight and, uh, uh, remove some of the, the noisy, initial

149
00:16:30,080 --> 00:16:40,080
links. Um, so, so the relational layers aren't trained, those are, um, you know, built,

150
00:16:40,080 --> 00:16:47,560
as priors, and then added to the transformer. Uh, the, they, they are trained. So everything

151
00:16:47,560 --> 00:16:55,000
is okay. Okay. And, uh, the relational layers, um, they are able to take, um, so maybe

152
00:16:55,000 --> 00:16:59,560
let me take a step back. But are they trained separately from the rest of the transformer?

153
00:16:59,560 --> 00:17:04,880
Okay. Everything is trained together into end. Um, so maybe let me take a step back. So,

154
00:17:04,880 --> 00:17:10,600
the transformer is generally like, they can encode, uh, basically, they encode the fully

155
00:17:10,600 --> 00:17:15,320
connected graph, right? It, it looks, uh, if we ignore the position in bedding, it looks

156
00:17:15,320 --> 00:17:20,480
like it considers the full sentence as, uh, as a fully connected graph, it's every word,

157
00:17:20,480 --> 00:17:26,040
it's, it's, it's every word. And if you take a schema and, uh, say that you flatten the

158
00:17:26,040 --> 00:17:31,680
schema, uh, into, like, uh, just a long stream, like, uh, question and put it together with

159
00:17:31,680 --> 00:17:38,480
the, uh, with, with the, uh, with the, with the question and it looks, consider everything

160
00:17:38,480 --> 00:17:45,600
as a fully connected graph. Um, what relational wear transformer does is, it, uh, as, you know,

161
00:17:45,600 --> 00:17:50,880
some additional prior information saying that beside this, uh, fully connected, uh, knowledge

162
00:17:50,880 --> 00:17:55,840
about, you know, everything could potentially be related to everything else. Uh, you know,

163
00:17:55,840 --> 00:18:01,640
we have this knowledge of special links, like foreign, foreign key relations. And, uh, if

164
00:18:01,640 --> 00:18:07,360
we see, you know, mention of, uh, car in the passion and, uh, there's a column, uh, called

165
00:18:07,360 --> 00:18:12,880
a car type, uh, it's more likely that these two are related. So it, uh, makes this, uh,

166
00:18:12,880 --> 00:18:17,920
as this additional inducted bias into the model. And later on during training, it can

167
00:18:17,920 --> 00:18:24,520
refine what, uh, the initial given, uh, evidence into, uh, the, you know, try to infer what

168
00:18:24,520 --> 00:18:31,640
is truly there. Okay. Uh, I think that, I think that's consistent with what I thought,

169
00:18:31,640 --> 00:18:36,600
but I'm still not clear on the, you know, where does the initial inductive bias come from?

170
00:18:36,600 --> 00:18:42,760
Is that trained, you know, separately or is that, uh, you know, baked in as kind of an engineering

171
00:18:42,760 --> 00:18:51,400
process? Um, so the foreign key relations that's known from the schema and the initial, uh, link

172
00:18:51,400 --> 00:18:57,320
that's, uh, given by some heuristics. So, so that there's some, uh, a little bit of engineering

173
00:18:57,320 --> 00:19:03,080
there. Okay. But even if the schema link can miss some of the initial relations, it's able

174
00:19:03,080 --> 00:19:12,040
to, uh, to, to, to, uh, pick those up and training. Yeah. Okay. Okay. Um, interesting. And so

175
00:19:12,040 --> 00:19:18,680
then the, the entire model is kind of trained and to end, uh, you know, starting with those

176
00:19:18,680 --> 00:19:25,960
biases and what are, and the training data, the training data set consists of, uh, set of

177
00:19:26,680 --> 00:19:35,880
questions that are, um, appended, can catnated with schema information? Uh, yes. Uh, in a way,

178
00:19:35,880 --> 00:19:42,760
there's more information, uh, like there's, uh, information about, uh, the, the, uh, the, uh,

179
00:19:42,760 --> 00:19:49,880
the type of, uh, columns, uh, that can be used in there. Um, but, uh, uh, conceptually,

180
00:19:49,880 --> 00:19:55,640
you can think of this as just a graph that, uh, the model looks at. So there's, uh, essentially

181
00:19:55,640 --> 00:20:04,120
question, uh, natural language queries, plus, um, some features related to the, the database

182
00:20:04,120 --> 00:20:15,560
essentially. Yeah. Is that right? Um, and the, um, um, there's a, uh, kind of a label that

183
00:20:15,560 --> 00:20:23,000
is the query that you want returned. Is it the query, is it the query that you want to execute

184
00:20:23,000 --> 00:20:28,440
or the results of the query, uh, against the database? It's the, uh, query that you want to ask

185
00:20:28,440 --> 00:20:39,400
you. Okay. Okay. Um, there's, uh, so, so, um, uh, this system, we, we trended on this, uh,

186
00:20:39,400 --> 00:20:45,720
public benchmark dataset called spider. Uh, it's, it's a very, uh, small dataset, uh, you know,

187
00:20:45,720 --> 00:20:51,480
for, uh, from deep learning, you know, standpoint, it, uh, but because collecting data for this

188
00:20:51,480 --> 00:20:57,080
generally is pretty hard. Spider is the de facto, you know, uh, benchmark dataset for this problem.

189
00:20:57,080 --> 00:21:05,160
Mm-hmm. It has, uh, uh, 200 different databases, but overall, just, uh, a little bit over 5,000

190
00:21:05,160 --> 00:21:11,400
queries. So it's, uh, it's a very small, uh, data problem. Uh, yet you have to learn to

191
00:21:11,400 --> 00:21:20,440
generalize to complete new, uh, developments. Mm-hmm. And would it be practical? Would you think to train,

192
00:21:20,440 --> 00:21:25,880
not on the query that you want, but on the resulting data so that, you know, you can get the same

193
00:21:25,880 --> 00:21:31,960
data with many different queries and the database, some will be more optimal than others. Um, you know,

194
00:21:31,960 --> 00:21:38,200
there's, you might want to let the network figure that out. Yeah. So, so there are other works, um,

195
00:21:39,160 --> 00:21:46,440
uh, in this base that actually looks at, uh, use execution result. Okay. So, uh, it's, it's possible,

196
00:21:46,440 --> 00:21:53,720
though you have to, um, either change how your model works. So, so your model still has to produce

197
00:21:53,720 --> 00:22:00,920
these discrete objects that, uh, execute. Uh, and so you either have to change that to, to some sort

198
00:22:00,920 --> 00:22:06,680
of, uh, differentiable structure, uh, or you have to, you know, use, uh, reinforcement learning or

199
00:22:06,680 --> 00:22:12,280
other related, uh, uh, techniques to be able to do credit assignments through that. Um, um, so,

200
00:22:12,280 --> 00:22:19,560
yeah, so, so it's possible other people have explored, um, on this problem, um, it doesn't, uh,

201
00:22:19,560 --> 00:22:28,200
work super well, um, generally, um, but we are, we, we have explored, uh, combining with our

202
00:22:28,200 --> 00:22:35,000
techniques and the denotation, the execution result to help, uh, perform data augmentation

203
00:22:35,000 --> 00:22:40,520
afterward and that improves performance. So, in a way, yes, we are, uh, we have additional step that,

204
00:22:40,520 --> 00:22:47,640
that, uh, can leverage the execution results. Okay. You mentioned, uh, a couple of things, one,

205
00:22:47,640 --> 00:22:54,520
that the, the data set is, you know, fundamentally a small data set, uh, you just mentioned data

206
00:22:54,520 --> 00:23:02,600
augmentation is one possible way to address that. Are there other things that you have done, uh, in,

207
00:23:02,600 --> 00:23:07,400
in this project to kind of optimize the transformer model around the small data set problem?

208
00:23:08,040 --> 00:23:16,200
Yeah. So, so, um, actually one of the, the key, um, difference that, uh, in our approach, uh,

209
00:23:16,200 --> 00:23:22,280
so a lot of things that I mentioned, uh, previously was techniques that were already invented in,

210
00:23:22,280 --> 00:23:28,680
in the field. So, so, relational wear transformer, we didn't invite, invent that, um, but we're,

211
00:23:28,680 --> 00:23:36,280
you know, we, we found out is, uh, if you put a very deep relational transformer, um, you can train

212
00:23:36,280 --> 00:23:43,320
it very well. Um, the models, uh, you know, even if it has ability to perform sounds or,

213
00:23:43,320 --> 00:23:49,880
form of reasoning, if you can train it, then it's no use. So, so we found a way to, uh,

214
00:23:49,880 --> 00:23:57,240
train deep transformers, uh, in the very small data set in the stable fashion. And, uh, so,

215
00:23:57,240 --> 00:24:04,280
this is, um, very, um, you know, counter to what people used to believe, like, uh, you have,

216
00:24:04,280 --> 00:24:08,920
you know, you, you, you train deep transformers, you have to have large data set,

217
00:24:08,920 --> 00:24:14,440
um, yeah. With, with this technique that we, we came out, it's published that ACR this year,

218
00:24:15,160 --> 00:24:19,000
uh, it's possible to train it in a stable fashion or small data set.

219
00:24:19,960 --> 00:24:26,920
And so, what are the key elements of doing that? Right. So, um, it's, uh, in the nutshell,

220
00:24:26,920 --> 00:24:33,640
it's, uh, it's a better way to initialize these models. Okay. And, uh, it also builds on

221
00:24:33,640 --> 00:24:41,800
prior works that, uh, looks at, you know, initializing deep transformers models. And, uh, we're,

222
00:24:41,800 --> 00:24:48,200
we're, you know, we make a big difference here is, uh, we make it possible to actually add

223
00:24:48,200 --> 00:24:53,400
these layers on top of pre-trend and then train everything together. So, previous techniques that,

224
00:24:54,280 --> 00:24:59,320
uh, improve stability of this transformers, they were only considering training from scratch.

225
00:24:59,320 --> 00:25:04,760
But when you print on, on a small data set, you want to leverage pre-trend. You don't want to train

226
00:25:04,760 --> 00:25:11,000
transformer, you know, a fresh transformer from scratch completely. But the previous techniques

227
00:25:11,000 --> 00:25:17,480
just doesn't work when you have, you know, put the new layers on top of a, uh, pre-trend bird or

228
00:25:17,480 --> 00:25:25,240
Roberta. And, uh, in, in, in the nutshell, well, you know, the, how this works is, um, it turns out

229
00:25:25,240 --> 00:25:31,960
the problem with deep transformer, with transformer training generally is the layer norm, uh, layer.

230
00:25:31,960 --> 00:25:39,480
And, uh, previous work have found out that, you know, if, if you can remove that somehow and, uh,

231
00:25:39,480 --> 00:25:45,480
still make training stable, the performance is much better. And training transformer generally

232
00:25:45,480 --> 00:25:51,000
require you to, like, have layer norm large batch size and learning rate warm up. It turns out

233
00:25:51,000 --> 00:25:57,480
you want to remove layer norm, uh, don't do warm up. And here, you know, on small data set,

234
00:25:57,480 --> 00:26:02,120
you have to use small batch size. So we make all, you know, these three possible.

235
00:26:03,800 --> 00:26:10,120
And when you say you make all those three possible, how do you, how do you do that?

236
00:26:10,920 --> 00:26:19,160
Right. So, um, the, um, basically in the nutshell, the idea is, um, you want to, there are

237
00:26:19,160 --> 00:26:25,560
certain parameters that are, uh, in the transformer, either read vanilla transformer or relational

238
00:26:25,560 --> 00:26:32,120
wire transformers, uh, you want to, during initialization, you want to scale them by a factor that's

239
00:26:32,120 --> 00:26:37,800
proportional to the depth. And in the paper, we have, uh, some derivation show exactly what that

240
00:26:37,800 --> 00:26:44,440
scale factor is. Uh, in turns out, when you do this on top of a pre-trained model, that scale

241
00:26:44,440 --> 00:26:51,080
factor, uh, needs to be data dependent. And so, so our matter is called the DT fix up,

242
00:26:51,080 --> 00:27:00,680
um, data dependent transformer fixed update initialization. So when you do that, uh, the,

243
00:27:00,680 --> 00:27:08,360
the, the, the model is initialized to, uh, basically, uh, in a stable regime, uh, for

244
00:27:08,360 --> 00:27:14,600
optimization with, uh, Adam, and, uh, it doesn't also lead, which is the typical problem with, uh,

245
00:27:14,600 --> 00:27:20,520
that learning rate, warm up, and later on, try to fix, but, uh, uh, fail set, uh, doing. So,

246
00:27:20,520 --> 00:27:26,040
when you do all of this together, uh, you know, training is stable since the beginning, and you can

247
00:27:26,040 --> 00:27:32,840
train your deep transformer, uh, we can, you know, the maximum we train was 48 layers on this small

248
00:27:32,840 --> 00:27:41,160
dataset, um, with very small batch size. Uh, the overall dataset is, is just, uh, you know,

249
00:27:41,960 --> 00:27:47,160
5,000 less than, you know, for training is less than 5,000 queries. So you can't use large,

250
00:27:47,160 --> 00:27:53,800
very large batch size. So instead of the learning rate warm up and the layer norm and the large

251
00:27:53,800 --> 00:28:02,040
batch size, you kind of substitute that with, uh, data aware initialization that sounds like it

252
00:28:02,040 --> 00:28:07,960
does a pass on the data and then scales the parameters and set your initial weights and then

253
00:28:07,960 --> 00:28:15,640
you get a stable result from the, uh, your training. Yeah. And, um, what's amazing is, um, once you

254
00:28:15,640 --> 00:28:21,480
are able to train, uh, this in a stable fashion, the transformer plus relational wear transformer

255
00:28:22,120 --> 00:28:29,160
can already perform a lot of the, you know, the, the reasoning it, the improvement on this hard cases

256
00:28:29,160 --> 00:28:38,040
is, it's, uh, it's huge. So it turns out, it's, so, so what's, uh, the learning here is,

257
00:28:38,040 --> 00:28:42,680
it's already, you know, this model can already do, uh, some form of reasoning like this,

258
00:28:42,680 --> 00:28:48,760
it's just we're not able to trend them, uh, on this, uh, in a stable fashion.

259
00:28:48,760 --> 00:28:58,760
Mm-hmm. And with the initialization approach, uh, and its data dependencies, is it, um,

260
00:28:58,760 --> 00:29:05,640
did you find that it was broadly applicable to, uh, a variety of data sets or is it very specific

261
00:29:05,640 --> 00:29:12,440
to the, you know, the benchmark data sets that you use or, you know, other, um, characteristics

262
00:29:12,440 --> 00:29:20,120
of the data set? Right. So we, uh, we also tried this, um, a completely different, uh, problem,

263
00:29:20,120 --> 00:29:26,120
but that also requires, uh, uh, reasoning. So, so a problem for logical reading comprehension.

264
00:29:26,120 --> 00:29:33,000
Mm-hmm. So, so we actually did this after, you know, we made all this working, uh, on, on the

265
00:29:33,000 --> 00:29:38,120
semantic parsing problem and we took that data set and, uh, it's also very small and very hard

266
00:29:38,120 --> 00:29:44,120
problem and we just, uh, applied the technique and got, uh, very, very good results like, uh,

267
00:29:44,120 --> 00:29:49,560
without any special engineering on the, on that task, we thought, uh, near state of art on that

268
00:29:49,560 --> 00:29:57,560
problem as well. So generally, yeah, it, uh, it looks like it's, um, uh, obviously if, if your

269
00:29:57,560 --> 00:30:03,640
data set is already huge and then you can use large batch size and, uh, that, that, that makes training,

270
00:30:03,640 --> 00:30:08,040
you know, much more stable, but when you have small data set, you can't, you can't do that.

271
00:30:09,880 --> 00:30:18,600
And we're the, uh, the data sets, the, the benchmark data set that you use, was that, um,

272
00:30:18,600 --> 00:30:24,440
was that a multilingual data set or is it monolingual and, um, you know, what kind of support for

273
00:30:24,440 --> 00:30:31,000
multilingual do you have? Uh, these, these problems are, are just, uh, English like the reading comprehension

274
00:30:31,000 --> 00:30:40,360
and, uh, semantic parsing. But, but, uh, yeah, that, um, there, yeah, we, for, for semantic parsing,

275
00:30:40,360 --> 00:30:48,760
there is, uh, another data set, but, but there, um, but, but there's no, uh, cross language.

276
00:30:51,480 --> 00:30:55,800
At least in the, the, the popular ones for cross database and semantic parsing, there's not

277
00:30:55,800 --> 00:31:01,080
simultaneous cross database and cross language. Mm-hmm. Is multilingual something that you're

278
00:31:01,080 --> 00:31:07,160
looking at or interested in? Uh, yeah, I think it's, it's very challenging problem.

279
00:31:07,160 --> 00:31:13,960
Um, uh, at the moment, we're not studying this, this problem, uh, is there, there are other like,

280
00:31:15,080 --> 00:31:17,080
unresolved, uh, calendars.

281
00:31:18,920 --> 00:31:23,080
Uh, let's talk a little bit about those. What are the kind of the big challenges are next steps

282
00:31:23,080 --> 00:31:31,080
with this work? Right. So, so generally, um, you know, data, if you want to continue to improve

283
00:31:31,080 --> 00:31:38,680
the accuracy, um, data is a problem. How to perform data augmentation in this space. Um, I think

284
00:31:38,680 --> 00:31:45,960
that's, uh, that's a fundamental, uh, blocker. And we are looking at, uh, how to, uh, how to do that.

285
00:31:45,960 --> 00:31:52,680
In fact, it sounds like you did a bit of data augmentation already, right? Uh, yes. Um, actually,

286
00:31:52,680 --> 00:31:58,760
we, we, um, so for the non, uh, academic research part of this project, we actually done a lot of

287
00:31:58,760 --> 00:32:06,920
data augmentation to make something like this work. Um, but it's, it's challenging. Um, so other people

288
00:32:06,920 --> 00:32:12,760
who have done this, their, their technique is, um, to basically engineer some sort of a grammar

289
00:32:12,760 --> 00:32:18,920
that can simultaneously produce, uh, seagull query and, uh, natural language, natural language.

290
00:32:20,440 --> 00:32:25,480
But it's, it's very, very tedious hard task to do. And we, we actually also tried it. And that

291
00:32:25,480 --> 00:32:31,880
was also, uh, our first intuition when it comes to do data augmentation in this, um, but to actually

292
00:32:31,880 --> 00:32:36,760
engineer this grammar to produce questions that are very natural. It's, uh, it's very, very tough.

293
00:32:37,480 --> 00:32:42,600
So, so how to do data augmentation without, uh, you know, going through

294
00:32:43,400 --> 00:32:50,840
and engineer a, a generative model for this, basically. It's, uh, yeah. It's something that we're

295
00:32:50,840 --> 00:32:57,480
looking into. And we have some working progress. It's not yet released that, uh, that allows us to

296
00:32:57,480 --> 00:33:04,920
leverage, um, uh, you know, knowledge from pre-trained model to actually improve, uh, the,

297
00:33:04,920 --> 00:33:12,520
effect, effectiveness of data augmentation. Um, and before we go to the, the next challenge, um,

298
00:33:13,320 --> 00:33:20,120
the, the data augmentation conversation kind of sparked a question around the complexity of

299
00:33:20,120 --> 00:33:27,160
the questions and the queries. Can you, uh, can you kind of speak to that and characterize the,

300
00:33:27,160 --> 00:33:32,040
the level of complexity that the, that turns able to, um, to deal with?

301
00:33:32,680 --> 00:33:39,560
Yeah. So, so in terms of query that can produce, it can produce, um, you know, queries that involve

302
00:33:39,560 --> 00:33:47,800
joints, uh, multiple joints, uh, subperies. Um, so sometimes you have questions that are,

303
00:33:47,800 --> 00:33:53,800
in English, very benign, very simple, like, um, this is not an example that our own shell spider,

304
00:33:53,800 --> 00:33:59,160
but, but, you know, say, like, what's the average return of all the stocks that performed above

305
00:33:59,160 --> 00:34:05,640
market average? So, so there's compositionality there. And, uh, market average referred to, like,

306
00:34:06,360 --> 00:34:11,560
you know, actually, well, depending on the structural schema, well, will require you to first compute

307
00:34:11,560 --> 00:34:18,040
that number by doing some sort of, uh, averaging, abbreviation, and the plug it into, you know,

308
00:34:18,040 --> 00:34:26,120
the, the top level query. And, uh, turning is able to, to do things like, like this. And, uh,

309
00:34:27,320 --> 00:34:33,400
also, obviously, it's also able to handle some of the other extreme, like, if you write questions in,

310
00:34:33,400 --> 00:34:39,400
you know, very verbosely, you know, with lots of different conditions, uh, and, uh, you know,

311
00:34:39,400 --> 00:34:48,120
it can, you know, handle a lot of that as well. Um, now, obviously, you know, it will have,

312
00:34:48,120 --> 00:34:57,080
you know, mistakes, uh, it definitely is not, uh, yet a system that, uh, is quite a level of accuracy,

313
00:34:57,080 --> 00:35:04,360
it's, you know, useful, uh, directly as, uh, you know, if you want to just trust the result,

314
00:35:04,360 --> 00:35:12,040
extreme result, 100%, it, it's not quite there yet. That, that's also why, like, for the Turing

315
00:35:12,040 --> 00:35:20,280
demo, uh, we built in some, you know, system, uh, in presenting the end result, we help user, um,

316
00:35:20,280 --> 00:35:25,160
you know, really makes sense of the result and try to judge for themselves whether it's what they want.

317
00:35:25,720 --> 00:35:30,680
So, so what do we do is, um, uh, Turing is able to produce multiple hypotheses,

318
00:35:30,680 --> 00:35:36,760
uh, during its generation, and we explain the hypotheses back to the user in natural language,

319
00:35:37,560 --> 00:35:43,080
um, and we highlight the difference across the different hypotheses, and then they can see for

320
00:35:43,080 --> 00:35:48,520
themselves, okay, the difference between these three is, you know, instead of this column,

321
00:35:48,520 --> 00:35:52,520
we mentioned that column, and here instead of this value, it's using some other value,

322
00:35:52,520 --> 00:35:57,480
or maybe the overall just structure law is completely different, and, uh, then they can see

323
00:35:57,480 --> 00:36:03,640
for themselves, which one corresponds to their, uh, intention originally, and can choose that.

324
00:36:05,000 --> 00:36:10,840
Now that, that explainability model sounds like its own research project. Is that something you

325
00:36:10,840 --> 00:36:15,160
worked on internally, or is that kind of an off-the-shelf technique that you're able to go by?

326
00:36:15,160 --> 00:36:21,960
It's, uh, our, our own work. Okay. So, so that part is, um, so, so we had a number of different

327
00:36:21,960 --> 00:36:28,840
publications at ACL this year, um, so the work on optimizing deeper transformers, that's one, uh,

328
00:36:28,840 --> 00:36:35,960
paper, and the Turing demo paper actually consists of the core semantic parser, the component that,

329
00:36:35,960 --> 00:36:42,040
you know, uh, tries to fill in the value after you get to the SQL sketch, and as well as the

330
00:36:42,040 --> 00:36:48,680
explanation module, and the explanation module here, um, it's kind of different from normal

331
00:36:48,680 --> 00:36:54,600
natural language generation, where you, you know, for, at least for research, there's a lot of, uh,

332
00:36:54,600 --> 00:37:00,520
neural language generation. Here we actually don't, explicitly don't want neural language generation,

333
00:37:00,520 --> 00:37:07,160
because we want, uh, you know, the difference across the hypothesis to be only due to the hypothesis

334
00:37:07,160 --> 00:37:14,040
themselves, not that the natural language generation part. So we have a, a, a, a very, you know,

335
00:37:14,040 --> 00:37:19,960
computer linguistics type of, uh, grammar-based model that can take the SQL and compositionally

336
00:37:19,960 --> 00:37:26,280
produce the, the natural language explanation step by step, and the only difference there is due

337
00:37:26,280 --> 00:37:34,440
to the difference in the SQL. Mm-hmm. And just to make sure I understand that, so you're the,

338
00:37:34,440 --> 00:37:46,920
um, the, um, the explainer is explaining the result, not the process for obtaining the result.

339
00:37:47,640 --> 00:37:56,120
All right. That interesting nuance. Um, um, it, uh, it doesn't sound quite like, you know,

340
00:37:56,120 --> 00:38:02,840
not wanting to do generation, but it, it, it sounds like I'm wondering if, you know, there's

341
00:38:02,840 --> 00:38:08,760
something, you know, missing in not trying to explain the process that won't capture, you know,

342
00:38:08,760 --> 00:38:14,360
for the end user, what the model may have done to get to the result. But I don't, you know,

343
00:38:14,360 --> 00:38:23,400
does it even matter? Uh, well, we think it doesn't, we think, because, um, normally, you know,

344
00:38:23,400 --> 00:38:28,200
user just care about the final answer, right? So you, you can think of SQL as just some sort of,

345
00:38:28,200 --> 00:38:35,560
uh, um, some sort of, uh, intermediate representation. Um, but this is an intermediate

346
00:38:35,560 --> 00:38:41,720
representation that actually, you know, could potentially be aligned with how humans think about

347
00:38:41,720 --> 00:38:46,920
the problem. This goes back to what I mentioned, you know, at the beginning, like, uh, you know,

348
00:38:46,920 --> 00:38:52,600
the machines representation of the data of the domain of the world, how can we make that

349
00:38:52,600 --> 00:38:58,440
align with people's representation? So there's a lot of intricacy of about how the neural net maps

350
00:38:58,440 --> 00:39:04,680
the questions and the reason and maps to the final. But, uh, that details might not be interesting

351
00:39:04,680 --> 00:39:10,120
to end user. Sort of like when you and I, we speak, I don't know, everything that goes on inside

352
00:39:10,120 --> 00:39:17,720
the neurons of your brain, like you don't have shared language. And that description allows us to,

353
00:39:17,720 --> 00:39:23,560
to have a shared understanding. Yeah. Maybe another way to put it, the, the problem that you're

354
00:39:23,560 --> 00:39:31,240
trying to solve is that the machine is going to spit out SQL that the, the, the user didn't write.

355
00:39:31,240 --> 00:39:35,880
And so the user may have some difficulty understanding what's actually happening. If you can explain

356
00:39:35,880 --> 00:39:41,560
to the user what the SQL is doing, they can more easily determine if it's what they expected or

357
00:39:41,560 --> 00:39:46,680
what they need. And it doesn't really matter what the network did to, uh, it's, that's not the

358
00:39:46,680 --> 00:39:52,040
explainability problem that you're trying to solve. You're just trying to explain the end result.

359
00:39:52,040 --> 00:39:58,520
Yeah. Yeah. Yeah. The intermediate result, not really the end result. Yeah. Yeah. Okay. Um,

360
00:39:58,520 --> 00:40:02,840
and you mentioned to some other challenges that you're looking forward to solving?

361
00:40:03,960 --> 00:40:14,200
Uh, yeah. So, um, in terms of, uh, you know, how to, um, just, uh, so, so there's some,

362
00:40:14,200 --> 00:40:19,320
some problems that, uh, in the field that people identify like, uh, you know, the cross-domain

363
00:40:19,320 --> 00:40:25,000
generalization. So, despite our benchmark, uh, looks at, uh, cross-domain generalization already,

364
00:40:26,040 --> 00:40:33,000
but it turns out when you're, uh, if you try to generalize to a different data set that are

365
00:40:33,000 --> 00:40:37,720
collected different, under a different, um, policy like, uh, for data collection,

366
00:40:37,720 --> 00:40:44,600
um, uh, the accuracy drop is, is pretty big as we all expect from engineering systems. Yeah.

367
00:40:44,600 --> 00:40:53,240
Like, um, so, so how to, you know, um, you know, make a model more robust, you know, not, uh,

368
00:40:53,240 --> 00:40:59,080
just, uh, across-domain, across, uh, database that are, you know, sort of more or less collect

369
00:40:59,080 --> 00:41:06,280
under the same data collection protocol, but to, you know, things that are very different, uh,

370
00:41:06,280 --> 00:41:14,440
in the wild, uh, that's still a big open problem. Mm-hmm. Possibly relating back to the data

371
00:41:14,440 --> 00:41:21,960
augmentation problem as well, right? Yeah. Uh, yeah, that's, that's definitely a big part of it.

372
00:41:22,840 --> 00:41:29,800
Awesome. Awesome. Well, Yenshwai, uh, thanks so much for taking the time to share a bit about

373
00:41:29,800 --> 00:41:38,120
what you've been working on. It's very full-stop. Take our exercise and we're having it. Thank you.

