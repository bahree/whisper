WEBVTT

00:00.000 --> 00:16.320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

00:16.320 --> 00:21.480
people, doing interesting things in machine learning and artificial intelligence.

00:21.480 --> 00:32.040
I'm your host Sam Charrington.

00:32.040 --> 00:38.280
Today we're joined by Gary Brockman, Senior Director of Product Management at Qualcomm Technologies.

00:38.280 --> 00:43.080
Gary, who got his start in AI through music, now leads AI in machine learning strategy

00:43.080 --> 00:47.440
and product planning for the company with a focus that includes the Qualcomm Snapdragon

00:47.440 --> 00:49.640
mobile platforms.

00:49.640 --> 00:54.520
In our conversation, we discuss AI on mobile devices and at the edge, including popular

00:54.520 --> 00:59.320
use cases and explore some of the various acceleration technologies offered by Qualcomm

00:59.320 --> 01:01.600
and others that enable them.

01:01.600 --> 01:05.960
We also dig into the state of AI on devices from the application developer's perspective

01:05.960 --> 01:10.680
and how various acceleration technologies fit together to help developers bring new products

01:10.680 --> 01:13.160
to market.

01:13.160 --> 01:17.120
Before we get going, I'd like to send a huge thanks to Qualcomm for sponsoring today's

01:17.120 --> 01:18.120
show.

01:18.120 --> 01:22.080
As you'll hear in my conversation with Gary, Qualcomm has been in the AI space for well

01:22.080 --> 01:25.920
over a decade now, powering some of the latest and greatest Android devices with their

01:25.920 --> 01:28.120
Snapdragon chipset.

01:28.120 --> 01:32.520
From their strong footing in the mobile space, Qualcomm now has the goal of making AI at

01:32.520 --> 01:34.720
the edge ubiquitous.

01:34.720 --> 01:38.440
To find out more about what they're up to and how they plan to get there, visit twimble

01:38.440 --> 01:48.440
AI dot com slash Qualcomm and now on to the show.

01:48.440 --> 01:53.600
Gary is the Senior Director of Product Management at Qualcomm.

01:53.600 --> 01:56.400
Gary, welcome to this week in Machine Learning and AI.

01:56.400 --> 01:58.160
Thank you and happy new year.

01:58.160 --> 01:59.160
Happy new year to you.

01:59.160 --> 02:04.800
I am excited for this new year and as I mentioned to you, as we're chatting before we started

02:04.800 --> 02:10.920
rolling today is my birthday making this a, well, it's always very cool to kind of have

02:10.920 --> 02:13.640
the new year on my birthday line.

02:13.640 --> 02:18.000
But I'm excited to make this my first interview of the year.

02:18.000 --> 02:21.480
And I'm happy to share your birthday with you, Sam.

02:21.480 --> 02:22.480
Fantastic.

02:22.480 --> 02:23.480
Fantastic.

02:23.480 --> 02:29.080
Why don't we get started by having you tell us a little bit about your background.

02:29.080 --> 02:35.040
You got into artificial intelligence initially by way of music, is that right?

02:35.040 --> 02:36.040
Yeah, that's right.

02:36.040 --> 02:40.040
I guess my, my interest in AI started way back.

02:40.040 --> 02:43.880
I guess it would have been at the turn of the century.

02:43.880 --> 02:48.640
I was working at a company called Music Match, which had developed a jukebox software application

02:48.640 --> 02:49.640
for the PC.

02:49.640 --> 02:56.160
But part of that offering was a personal music recommendation engine that was a proprietary

02:56.160 --> 02:59.240
engine that Music Match had created.

02:59.240 --> 03:06.000
And it was using collaborative filtering algorithms to monitor the listening behavior

03:06.000 --> 03:11.200
of the users of our jukebox software and what they, what they were listening to, what they

03:11.200 --> 03:13.680
skipped, what they listened to fully.

03:13.680 --> 03:19.680
And we were able to really develop some really rich correlations between artists and listeners

03:19.680 --> 03:26.400
that had, that transcended any traditional tagging data like genre or year or era would

03:26.400 --> 03:27.880
have you.

03:27.880 --> 03:32.800
Just looking, listening to or actually watching what people are observing, what listeners

03:32.800 --> 03:37.360
were actually doing, not what they were saying, not what they said they liked or disliked.

03:37.360 --> 03:42.240
But what they were actually listening to and then taking the correlations between individual

03:42.240 --> 03:45.640
listeners and then myself or another somebody else.

03:45.640 --> 03:53.040
And that combined, the combined influences came up or resulted in some very unique recommendations.

03:53.040 --> 03:57.440
My background early on was in music and digital music and MP3 compression.

03:57.440 --> 04:04.000
And when I came across, when I experienced what Music Match had to offer and help promote

04:04.000 --> 04:10.320
their recommendation technology, which also powered personalized radio and on demand streaming

04:10.320 --> 04:13.720
recommendations, that got me hooked.

04:13.720 --> 04:18.840
And I carried that with me for a number of years and up until four years ago, when I had

04:18.840 --> 04:24.280
an opportunity at Qualcomm to bring some AI machine learning technology at a corporate

04:24.280 --> 04:30.760
R&D into the commercial side of the house and release it across our Snapdragon platform,

04:30.760 --> 04:36.080
kind of tying those things together over the past four years has really been a great experience

04:36.080 --> 04:37.800
and quite rewarding.

04:37.800 --> 04:42.520
Why don't we maybe spend a little bit of time talking about Qualcomm for those who are

04:42.520 --> 04:46.600
not familiar with the company and what you're up to?

04:46.600 --> 04:52.080
I know in our newsletter, we've talked about Qualcomm quite a bit, particularly over

04:52.080 --> 05:02.280
the last year, everything from the Snapdragon platform launch in December, prior to that,

05:02.280 --> 05:08.800
the extended reality release that you did in June.

05:08.800 --> 05:16.280
Some new chips in April and I think maybe in January or something, there was a neural

05:16.280 --> 05:22.280
processing engine and a hexagon vector processor, all really interesting stuff that I'm excited

05:22.280 --> 05:24.720
to talk to you about today.

05:24.720 --> 05:29.680
But for folks who've kind of seen those pieces and don't really know what the big picture,

05:29.680 --> 05:32.280
what's Qualcomm up to in this space?

05:32.280 --> 05:39.320
Well, if you go back in time, we've been a mobile innovator for over 30 years now, most

05:39.320 --> 05:46.440
of it's starting with cellular and since the early 90s, we've been focused on the connectivity

05:46.440 --> 05:51.560
side with every G transition, we call it from 3G to 4G and out of 5G.

05:51.560 --> 05:57.400
We also got into the chip business in the 90s because others were having some difficulty

05:57.400 --> 06:04.000
developing 2G and 3G chips and then that was a springboard for us to then move the internet

06:04.000 --> 06:06.760
from the PC onto a mobile phone.

06:06.760 --> 06:11.680
We saw this trend happening very early on and tried to hasten that with investment in

06:11.680 --> 06:15.280
both connectivity and silicon.

06:15.280 --> 06:19.760
And then we introduced the Snapdragon mobile platform a little over a decade ago.

06:19.760 --> 06:24.360
In fact, we're probably coming up on 11 years now and that and Snapdragon is now what

06:24.360 --> 06:29.360
powers the majority of Android phones in the marketplace globally.

06:29.360 --> 06:34.360
What we've done on the Snapdragon side specifically, when it comes to compute on a handset or in

06:34.360 --> 06:41.760
any other embedded device, IoT, automotive, etc., we have leveraged the compute capabilities

06:41.760 --> 06:47.440
that we developed over that past decade to drive what we have seen over the past four

06:47.440 --> 06:52.680
years as being a very quick movement of an artificial intelligence and machine learning

06:52.680 --> 06:59.800
based workloads that have been mostly relegated to the cloud on the server side or the data

06:59.800 --> 07:03.920
center, but are matriculating to the edge.

07:03.920 --> 07:09.160
Mobile phone is really kind of the primary focus for us, but we do look at other verticals

07:09.160 --> 07:15.760
like IoT and automotive as I mentioned, but the activity from Qualcomm standpoint has

07:15.760 --> 07:21.680
been very strong on 5G, but also an AI over the past four years and specifically on device

07:21.680 --> 07:31.160
AI and ensuring that any device for the Snapdragon processor is able to efficiently run and accelerate

07:31.160 --> 07:33.880
AI algorithms in a power efficient way.

07:33.880 --> 07:40.520
Okay, so to make sure I understand that trajectory, the companies started out basically building

07:40.520 --> 07:48.120
the or not started out, but one of the companies big moves was really building the chips that

07:48.120 --> 07:57.760
allowed devices to connect to wireless networks like 3G, 4G, 5G, and then from that kind

07:57.760 --> 08:04.600
of presence in the mobile space moved into, you know, as smartphones arrived, moved

08:04.600 --> 08:12.800
into providing the compute platform or the compute chips for the smartphones and now

08:12.800 --> 08:18.680
are providing an of the AI acceleration extensions to that compute platform.

08:18.680 --> 08:20.680
Is that the right way to think about it?

08:20.680 --> 08:22.800
Yeah, I think that's the right continuum.

08:22.800 --> 08:25.880
Those are the three big pillars, if you will.

08:25.880 --> 08:34.200
Okay, and your responsibility there is focused on AI strategy and product planning.

08:34.200 --> 08:37.480
How long has the company been thinking about AI?

08:37.480 --> 08:39.800
That's a good question.

08:39.800 --> 08:44.000
We've been, it's interesting that most folks don't know how long we've been focused on

08:44.000 --> 08:45.440
this, but it's been over a decade.

08:45.440 --> 08:51.760
We really started investing in deep learning back in or machine learning back in 2007.

08:51.760 --> 08:55.960
We've had a heritage in computer vision, but in the research side of the house, we've

08:55.960 --> 09:00.120
been looking at, you know, everything from spiky neural networks to deep learning going

09:00.120 --> 09:04.440
back as far as 2007, 2008.

09:04.440 --> 09:10.600
Our research group has actually been driving most of the activity over that 11, 12 year

09:10.600 --> 09:19.440
period, but in the past four years, as I say, four of the last 11 years, we've actually

09:19.440 --> 09:26.120
been releasing commercial grade software as well as hardware acceleration to accommodate

09:26.120 --> 09:31.680
what we see is at this present, a tidal wave of workloads that are arriving on mobile.

09:31.680 --> 09:38.280
We started our first, or at least we introduced our first mobile SOC that was optimized for

09:38.280 --> 09:44.280
on device machine learning with a Snapdragon 820 back in 2015, and we're now in our fourth

09:44.280 --> 09:46.480
generation.

09:46.480 --> 09:51.880
Taking a little quick step back, I mean, my role at Qualcomm and our team's role is to

09:51.880 --> 09:57.280
look at what we need to do to accommodate this new class of software and algorithms on

09:57.280 --> 10:00.160
device and all as well as in the device.

10:00.160 --> 10:02.920
So at Qualcomm, we don't focus just on the end product.

10:02.920 --> 10:07.640
We're actually looking at AI and machine learning as being integral to how we develop products

10:07.640 --> 10:08.920
and also run our business.

10:08.920 --> 10:15.760
So my team's responsibility is to look across the Snapdragon portfolio where we can optimize

10:15.760 --> 10:19.880
up and down the tiers, whether it's in a lower tier all the way up to the premium tier

10:19.880 --> 10:26.960
SOC, but also look at ways that as we develop our products within the organization, how can

10:26.960 --> 10:33.520
we apply machine learning to make those processes more cost efficient, reduce time, and in

10:33.520 --> 10:38.400
cases where it's possible, generate a creative revenue from that effort.

10:38.400 --> 10:44.600
You mentioned SOC, and that's system on chip, which is basically the effectively the CPU

10:44.600 --> 10:46.000
for these devices.

10:46.000 --> 10:50.880
Yeah, it's actually, the SOC would be a complete system.

10:50.880 --> 11:00.040
So the CPU, maybe the heart of the chip itself is the primary processor, but the SOC is

11:00.040 --> 11:07.160
defined as all the components that are necessary to drive what that device is capable of doing.

11:07.160 --> 11:13.480
So in the case of Snapdragon, the entire Snapdragon portfolio is an SOC portfolio and

11:13.480 --> 11:24.640
each has at least our cryo CPU, our Adreno GPU, and in the mid to high to premium tier,

11:24.640 --> 11:28.120
you'll find our hexagon.

11:28.120 --> 11:32.680
We do have DSPs, or I say the hexagon DSP.

11:32.680 --> 11:39.000
The DSP is pretty prevalent across the portfolio from a modem standpoint, but from an AI standpoint,

11:39.000 --> 11:47.360
the hexagon family includes vector processors, which are present in our premium tier and

11:47.360 --> 11:52.560
top tier SOC, so 800 down to 600.

11:52.560 --> 11:57.320
And that vector processor up until recently has been a primary engine for running on

11:57.320 --> 11:59.000
device AI.

11:59.000 --> 12:07.800
What are the primary use cases that you're focused on from a on device AI perspective?

12:07.800 --> 12:13.440
So there are a number that are I guess today and literally over the past 18 to 24 months

12:13.440 --> 12:19.280
that have become kind of table stakes have been things like just basic object recognition.

12:19.280 --> 12:23.400
So if you hold up your camera, you can detect whether something in the field of view is

12:23.400 --> 12:28.960
a specific class of object, let's say a car, a cat, or a dog, or what have you.

12:28.960 --> 12:33.360
And then taking that a little bit further to the facial recognition.

12:33.360 --> 12:39.880
So now if you look at any phone today, either you're going to have a phone that has a specific

12:39.880 --> 12:44.840
camera module that allows the device to take kind of a 3D depth mapped image of your face

12:44.840 --> 12:50.240
so that you can unlock the phone, but also maybe even make mobile payments depending on

12:50.240 --> 12:53.600
the type of phone you have and the region that you're in.

12:53.600 --> 12:59.840
With AI, we're seeing that the specifically in facial recognition, all the extra hardware

12:59.840 --> 13:06.000
that's been necessary to drive that really isn't necessary across the board because AI

13:06.000 --> 13:10.520
algorithms specifically are able to do depth mapping on their own with single images

13:10.520 --> 13:13.080
now and single camera.

13:13.080 --> 13:18.240
So facial recognition for unlocking a phone and making mobile payments is becoming fairly

13:18.240 --> 13:20.920
common voice activation.

13:20.920 --> 13:27.280
So being able to do detecting keyword on a mobile phone or any other device, I think that's

13:27.280 --> 13:31.320
becoming commonplace and again, that's happening locally, it's not happening in the cloud,

13:31.320 --> 13:37.520
that recognition is immediate so that you can have pretty low latency.

13:37.520 --> 13:42.560
There are other camera effects like bouquets, you can segment your portrait or they may

13:42.560 --> 13:47.520
call it portrait mode, what have you, where you segment your body or the subject's body

13:47.520 --> 13:53.120
from the background so you can blur that out or create a separate background and put

13:53.120 --> 13:58.840
the individual in a completely separate environment visually.

13:58.840 --> 14:04.120
The entire camera pipeline, most of the features that you see in a mobile camera today, whether

14:04.120 --> 14:10.440
it's HDR or even being able to do things like super resolution where you bring clarity

14:10.440 --> 14:15.480
to a picture that doesn't have, that isn't high resolution to start with.

14:15.480 --> 14:21.800
All of those features are now, they started off as computer vision based and are now becoming

14:21.800 --> 14:26.320
driven by deep neural networks on the device to solve for the same problem to achieve better

14:26.320 --> 14:33.560
accuracy and in many cases give the developer a more generalized approach to how they

14:33.560 --> 14:36.120
instantiate those features.

14:36.120 --> 14:38.720
Those are probably the most common today.

14:38.720 --> 14:46.680
It doesn't surprise me that the camera pipeline is all being processed on device but I'm

14:46.680 --> 14:54.120
a bit surprised to hear that things like object recognition, face recognition, voice recognition

14:54.120 --> 14:56.400
are being done on device.

14:56.400 --> 15:01.960
The thing that I'm thinking of most is like OK Google or the facial recognition or the

15:01.960 --> 15:06.840
people recognition and like Google photos and I just assume that that's the app and

15:06.840 --> 15:13.240
it's talking back to Google's cloud and all of the machine learning AI is happening between

15:13.240 --> 15:18.600
those two pieces of software but it sounds like more of it's happening on the device than

15:18.600 --> 15:19.600
I might think.

15:19.600 --> 15:21.120
I think that's right.

15:21.120 --> 15:26.720
If you look at anything that deals with biometric data, whether it's your face or your voice,

15:26.720 --> 15:31.960
that's very specific to you and that data needs to be kept secure and private and there

15:31.960 --> 15:42.000
is today given the state of kind of compute on device as well as the ability of CNNs and

15:42.000 --> 15:45.680
RNNs to be able to run locally in a pretty efficient way.

15:45.680 --> 15:53.120
There's really no reason why you as an individual should have to rely on a cloud and present

15:53.120 --> 15:58.560
your personal data to that, to the data center of the cloud in order to achieve some level

15:58.560 --> 16:02.160
of utility or some utility on the phone.

16:02.160 --> 16:07.120
So specifically with facial recognition and I guess I should take a step back and say

16:07.120 --> 16:13.040
that there's a security and privacy of been probably one of the primary reasons we focus

16:13.040 --> 16:18.320
so much on device as opposed to relying solely on the cloud for some of these functions.

16:18.320 --> 16:22.560
That in performance, so low latency because everybody wants things immediate.

16:22.560 --> 16:27.640
But there is again no reason why you should have to share your voice print or your voice

16:27.640 --> 16:33.960
data or your facial information or any other biometric data for that matter with a third

16:33.960 --> 16:40.120
party to let's say unlock a phone and do what you need to do on that device.

16:40.120 --> 16:48.720
So what you see today, if you're looking at OK Google or if you're on an iOS device,

16:48.720 --> 16:54.120
there's a balance between what's happening on the phone and what's happening in the cloud.

16:54.120 --> 16:57.040
Some cases as a consumer you're not going to know and I don't think that and the intention

16:57.040 --> 17:00.560
is that you shouldn't know which you should get as the best user experience.

17:00.560 --> 17:05.480
What we try to do is focus on ensuring that whatever workload does run on the device

17:05.480 --> 17:09.040
is executed in a power efficient and performant manner.

17:09.040 --> 17:11.640
But yeah, there's a variance.

17:11.640 --> 17:15.600
Some things do happen on the device and some things are happening in the cloud.

17:15.600 --> 17:21.720
And how about from an application developer perspective, if I'm developing an app and

17:21.720 --> 17:29.360
maybe everything's probably different if I'm Google, but if I'm just an app developer,

17:29.360 --> 17:39.240
Android app developer or iOS app developer, and I want to use AI, either via your mobile

17:39.240 --> 17:49.080
TensorFlow or something like that or the iOS extensions, do I have to think a lot about

17:49.080 --> 17:55.640
on device AI and how to take full advantage of the hardware or we at the point where

17:55.640 --> 18:00.200
that's all transparent to me and whatever framework I'm using, figures out the best way

18:00.200 --> 18:02.400
to do things.

18:02.400 --> 18:07.800
I think we're still at a point where there's experimentation, there's very little standardization

18:07.800 --> 18:13.840
across the industry, whether it's from benchmarking or common APIs to access the underlying

18:13.840 --> 18:20.240
compute on different devices, irrespective of the chip that is driving that device.

18:20.240 --> 18:25.120
In our portfolio, we have right now we have a handful of tools that are all part of what

18:25.120 --> 18:32.080
we call the Qualcomm AI engine and this AI engine is the sum of a number of parts.

18:32.080 --> 18:38.680
It's the primary compute engines that we have on our chip, so the CPU, the GPU, our hexagon

18:38.680 --> 18:44.240
vector processor, and something that we just recently introduced with Snapdragon 855,

18:44.240 --> 18:51.000
we call a tensor accelerator, which is a dedicated AI processor embedded in the Snapdragon

18:51.000 --> 18:56.560
855 chip, we can talk about that at another time or later, but those are the hardware

18:56.560 --> 18:57.560
elements.

18:57.560 --> 19:01.640
That's where the job gets done and each of those cores has a different power and performance

19:01.640 --> 19:09.000
profile to accommodate what would be, let's say, KPIs that the developer has set out

19:09.000 --> 19:13.720
for a specific user experience, let's say we're going to say facial recognition or detecting

19:13.720 --> 19:20.960
a keyword or somebody's voice, each of those features has

19:20.960 --> 19:28.680
a different tolerance for latency and a different tolerance for power, so by providing multiple

19:28.680 --> 19:33.920
compute engines from a hardware standpoint, it gives the developer choice.

19:33.920 --> 19:40.000
On the software side, specifically in our portfolio, we have SDKs like the neural processing

19:40.000 --> 19:45.120
engine, which I think you mentioned at the beginning, which is a very easy to use tool for

19:45.120 --> 19:49.960
a developer who has trained in neural network offline and wants to run it on a Snapdragon

19:49.960 --> 19:56.600
device, when they bring that model to the device using the neural processing SDK, all they

19:56.600 --> 20:01.800
have to do is make a simple API call to run on any of the compute elements, whether it's

20:01.800 --> 20:07.320
the CPU, GPU, or the vector process, or tensor accelerator, and then a way they go.

20:07.320 --> 20:08.400
It's very straightforward.

20:08.400 --> 20:14.920
You don't have to get down to the lower level close to the metal and rule up your sleeves,

20:14.920 --> 20:18.200
but we do have tools for those that want to get their hands dirty.

20:18.200 --> 20:25.160
We have libraries like Hexagon NN, which is a neural network library for Hexagon, if

20:25.160 --> 20:30.920
you want to just write directly to the Hexagon processor and not worry about the other cores.

20:30.920 --> 20:39.280
We have math libraries for CPU, and then if you're familiar with OpenCL, the Adreno GPU,

20:39.280 --> 20:43.520
our Adreno GPU supports OpenCL, so you can program directly with the GPU, so there's

20:43.520 --> 20:50.240
a variety of different ways with our own technology that you can access the compute on Snapdragon,

20:50.240 --> 20:54.040
and all of that is part of what we call the AI engine.

20:54.040 --> 21:02.360
There's also in mobile specifically, and I'd say with Android, Google released the Android

21:02.360 --> 21:08.520
NNAPI or neural networking API back in Android O, which we supported from the very beginning

21:08.520 --> 21:10.760
on Snapdragon.

21:10.760 --> 21:17.800
Another time we expect that that API will become the primary, the dominant way, that in Android

21:17.800 --> 21:26.960
if a developer wants to run a built-in application that's running DNNs online, they'll use Android

21:26.960 --> 21:30.240
NN or NNAPI as the primary interface.

21:30.240 --> 21:37.120
That would be, I guess, the first movement toward standardization when it comes to how

21:37.120 --> 21:42.440
you would access different chips in the Android environment.

21:42.440 --> 21:46.720
But I'd say by and large, there's still quite a bit of activity.

21:46.720 --> 21:53.600
I use the term Wild West a lot because it really is the Wild West, and it's exciting.

21:53.600 --> 22:02.000
There's so much activity, the amount of innovation, the collaboration between organizations that

22:02.000 --> 22:09.320
have found themselves to be the most contentious competitors are now collaborating to advance

22:09.320 --> 22:16.200
overall machine learning and artificial intelligence at a pace that I think most of us, we look

22:16.200 --> 22:17.200
at it.

22:17.200 --> 22:22.040
There's no hyperbolic term that could be used that doesn't apply, it actually does apply,

22:22.040 --> 22:24.360
it's moving so fast.

22:24.360 --> 22:33.520
Again, putting the developer head on, it sounds like particularly in the Android world,

22:33.520 --> 22:39.280
which kind of historically suffers with a very high degree of fragmentation.

22:39.280 --> 22:44.320
It sounds like it may still be a frustrating experience to figure out which of these tools

22:44.320 --> 22:52.640
and APIs and things like that I need to use to take advantage of the underlying, all of

22:52.640 --> 22:55.600
the potential underlying hardware possibilities.

22:55.600 --> 22:58.520
Yeah, I guess it depends too on the device class.

22:58.520 --> 23:02.760
In mobile, it's probably a little bit more complicated because there are multitude of

23:02.760 --> 23:06.600
SOCs that are out there that are driving different mobile phones.

23:06.600 --> 23:10.360
That's where Android NN as an API or as an interface may actually solve a lot of that

23:10.360 --> 23:15.480
problem because if you have a common way to interface with the underlying hardware and

23:15.480 --> 23:21.960
each of the SOC manufacturers, chip manufacturers like Qualcomm are doing our job under the hood,

23:21.960 --> 23:27.480
which we have on our side, developing drivers for each of our compute cores that are optimized

23:27.480 --> 23:33.880
specifically for, let's say, NNAPI, then that frustration from a developer standpoint

23:33.880 --> 23:34.880
is minimized.

23:34.880 --> 23:37.240
In fact, it's reduced considerably.

23:37.240 --> 23:42.160
When you deal with proprietary SDKs, they still have an impact and still have a benefit

23:42.160 --> 23:47.480
in a broad mobile environment, but they then become a little bit more, I guess the

23:47.480 --> 23:53.120
priority around those become increases when you get into dedicated devices like connected

23:53.120 --> 23:58.720
cameras or speakers where you know that the SOC that's powering that is the same one

23:58.720 --> 24:06.360
across the board and as a developer, you have less variability so you can be more confident.

24:06.360 --> 24:12.000
But I do think that there's, for at least for the next few years, there's going to continue

24:12.000 --> 24:17.400
to be a lot of experimentation, there'll be a balance between standardization and maybe

24:17.400 --> 24:25.000
a slight degradation in performance versus proprietary software execution and the ultimate performance

24:25.000 --> 24:29.720
that balance will always be there, but the good thing is that there will be choice.

24:29.720 --> 24:36.280
If I go back where we started this conversation and music, now I've tried to develop an analogy

24:36.280 --> 24:42.120
or a comparison when I was doing early on in my college days when I was doing digital

24:42.120 --> 24:47.960
music production, everything was very manual, it was all purpose built, you didn't have

24:47.960 --> 24:53.920
any way to generalize, cut and paste and sample and sampling was actually done with analog

24:53.920 --> 24:59.200
devices like cassette tapes and vinyl records.

24:59.200 --> 25:05.880
Over time, music production software became more advanced and abstracted a lot of that

25:05.880 --> 25:13.200
heavy lifting and purpose built programming that you had to do as a producer and then cut

25:13.200 --> 25:15.880
and paste became kind of the way that you went.

25:15.880 --> 25:21.280
So if you want to replicate a specific phrase, you could do that, just cut and paste, cut

25:21.280 --> 25:26.040
and paste, and now it's so easy to make music as a novice.

25:26.040 --> 25:32.720
All the tools are there at your disposal, it's very, it's democratized, that's probably

25:32.720 --> 25:34.240
the best way to put it.

25:34.240 --> 25:41.560
The pace that we see in, let's say machine learning and AI tools and frameworks like TensorFlow

25:41.560 --> 25:48.760
and PyTorch and the efforts like the OpenNural Network exchange is an example, which is another

25:48.760 --> 25:54.360
way to give a developer the latitude to use whatever framework they wish to express

25:54.360 --> 26:00.200
their model in and not have to worry about what the underlying hardware supports.

26:00.200 --> 26:04.440
Monarchs as an interchange format is one that, again, makes that job a lot easier.

26:04.440 --> 26:10.040
With all these different tools and advancements, I think you're going to see a democratization.

26:10.040 --> 26:15.240
In fact, there's already a democratization of AI happening outside of just the technology

26:15.240 --> 26:21.840
development, like an education, you know, Andrew Ng's courses on Coursera.

26:21.840 --> 26:26.360
There are government entities around the globe that are pushing AI machine learning

26:26.360 --> 26:29.200
education in high school and in college.

26:29.200 --> 26:31.240
It's, it's, it's remarkable.

26:31.240 --> 26:38.080
So I think this short term, there's certainly pain and, you know, a lot of experimentation.

26:38.080 --> 26:42.280
But we're, I think you're already seeing a number of examples where things are consolidating

26:42.280 --> 26:46.800
and I'll use that term, the democratization is already underway.

26:46.800 --> 26:53.360
You made an interesting point earlier in, you know, when I, in kind of progressing through

26:53.360 --> 27:01.560
this conversation, I've been thinking mostly mobile devices, i.e. handsets, smartphones.

27:01.560 --> 27:07.080
But you reference kind of device classes and, quote unquote, IoT devices, whether they're

27:07.080 --> 27:13.880
smart speakers or, you know, by extension, kind of industrial, IoT devices, can you give

27:13.880 --> 27:20.120
us a sense of the relative size of each of those markets from, I don't know what makes

27:20.120 --> 27:25.240
sense like a number of devices or do you have a sense of that?

27:25.240 --> 27:30.840
I don't have, I don't have, you can slice and dice that in a number of ways.

27:30.840 --> 27:35.320
I think the one, I think the, the one data point that keeps getting thrown around when

27:35.320 --> 27:42.360
it comes to specific devices outside of mobile, I think the, the smart speaker market, I've

27:42.360 --> 27:45.600
unfortunately, I have to forgive me, I don't have the number off the top of my head and

27:45.600 --> 27:47.720
I don't want to give you an incorrect number.

27:47.720 --> 27:56.360
But the smart, the smart speaker market driven it primarily by the echo in the Alexa platform,

27:56.360 --> 27:58.600
I think is probably one of the standouts.

27:58.600 --> 28:07.000
Security cameras are very prevalent, especially from an enterprise standpoint, but the smart

28:07.000 --> 28:13.360
speaker is a consumer device is probably the standout category leader, if you will,

28:13.360 --> 28:18.840
on IoT, even any other embedded device outside of mobile.

28:18.840 --> 28:24.040
That particular device, and I have to speak for my own personal, I'll speak for my own

28:24.040 --> 28:29.160
personal view first and then I'll talk about it from an industry standpoint.

28:29.160 --> 28:34.680
I'm one of these individuals that despite my comfort with technology and an audio, which

28:34.680 --> 28:39.320
is really at the core of what I've been focused on most of my career and my personal life,

28:39.320 --> 28:45.680
talking to an inanimate object has been uncomfortable, whether it's a speaker or a mobile phone

28:45.680 --> 28:51.760
or what have you, I see people do it and I had been awkward for me for the longest time,

28:51.760 --> 28:56.640
but what that, you know, echo class device was able to do is kind of break down that wall

28:56.640 --> 29:02.640
between a consumer and the device and create the beginning of what is a relationship.

29:02.640 --> 29:06.680
And I think the relationship that we will have with the devices around us will be based

29:06.680 --> 29:13.680
in large part on voice and how, you know, not just about command and control, where you

29:13.680 --> 29:23.000
know, ask Alexa or Cortana or Siri or what have you, for a to provide something, so make

29:23.000 --> 29:30.640
a request, over time that's going to become more real, more lifelike, today it's very,

29:30.640 --> 29:36.640
it's still a little bit, I don't say awkward, but it's not, it's not like you're talking

29:36.640 --> 29:41.200
to a human being, you know you're talking to a device, whether that device or whether

29:41.200 --> 29:48.240
the intelligence is in the cloud or on the device, you know that it's not, it's not a

29:48.240 --> 29:53.400
companion, it's not somebody you have a relationship with, but the movements toward

29:53.400 --> 29:58.000
making a lot of that work happen on the device, a lot of the algorithm processing on the

29:58.000 --> 30:04.000
device is going to allow for that conversation to become really more of a conversation.

30:04.000 --> 30:12.200
And the gaps between the device providing a response or are you talking to the device,

30:12.200 --> 30:16.840
those gaps will be reduced or lower latency, but then also being able to detect more about

30:16.840 --> 30:22.560
what you as an individual or how you feel, like what is it in your voice that the device

30:22.560 --> 30:30.240
itself or the algorithm can detect, so sentiment, like are you happy or you sad, angry, and

30:30.240 --> 30:38.680
then being able to provide even richer, a richer response and maybe even proactively engage.

30:38.680 --> 30:44.880
So that category and without providing a, without providing a number, which again, I don't

30:44.880 --> 30:49.280
want to, I don't want to call attention to a number that may be incorrect because I

30:49.280 --> 30:54.480
don't have that off the top of my head, but I think that device category specifically

30:54.480 --> 31:01.040
and the size of it, which is I think the largest of all IoT devices, that's going to be one

31:01.040 --> 31:04.560
of the most interesting ones to watch for all the reasons that I just mentioned.

31:04.560 --> 31:09.840
I think that that interaction, that real relationship, the companionship that you're going

31:09.840 --> 31:13.320
to have with a device over time is going to be largely driven by voice.

31:13.320 --> 31:15.040
That's why that category is so hot.

31:15.040 --> 31:16.040
It's interesting.

31:16.040 --> 31:26.440
We thought about the lack of the ability to do robust on device, audio processing, AI

31:26.440 --> 31:32.760
audio processing as a key limitation of the user experience, but now that you say

31:32.760 --> 31:37.200
that I totally get it, like you say something to one of these devices, it kind of you're

31:37.200 --> 31:41.000
waiting for a long time, and a lot of that is just kind of the latency of talking to

31:41.000 --> 31:44.600
a cloud and having that then have to come back.

31:44.600 --> 31:45.600
Exactly.

31:45.600 --> 31:51.160
I don't know how many times your internet's going down at your house, but if you have

31:51.160 --> 31:56.520
one of these devices, the minute you ask for something, you get a response, sorry, I

31:56.520 --> 31:57.520
can't help you right now.

31:57.520 --> 31:59.280
It's like, well, you should be able to help me.

31:59.280 --> 32:05.960
You should at least engage with me in such a way that it minimizes that frustration.

32:05.960 --> 32:08.880
There's enough processing capability.

32:08.880 --> 32:13.680
The algorithms are becoming far more efficient to run on device.

32:13.680 --> 32:19.560
You can handle more keywords, natural language processing, and overall voice UI as a category.

32:19.560 --> 32:21.920
There's so much movement there.

32:21.920 --> 32:27.440
There's no reason why the device could not provide more utility and that level of personal

32:27.440 --> 32:33.200
interaction moving forward, and it'll just get better over time.

32:33.200 --> 32:34.200
Yeah.

32:34.200 --> 32:35.720
I imagine that's coming very soon.

32:35.720 --> 32:39.320
There's no reason why the device shouldn't at least be able to turn off the lights or something

32:39.320 --> 32:41.320
like that when it's disconnected.

32:41.320 --> 32:50.200
So maybe taking a bit of a step back, one of the first things I mentioned was the Snapdragon

32:50.200 --> 32:57.560
855 platform launch you did back in December.

32:57.560 --> 33:05.320
I kind of want to use that as an on-trade of talking about what these platforms mean.

33:05.320 --> 33:12.880
And specifically, when we, you mentioned some of the components of a platform like the

33:12.880 --> 33:21.160
CPU GPU, hexagon, AI accelerator, I want to get a mental model for these sound like in

33:21.160 --> 33:26.760
some ways like overlapping components in terms of functionality, or at least they could

33:26.760 --> 33:31.000
be used in overlapping ways.

33:31.000 --> 33:37.320
What are the different pieces and how to developers, how should developers or users think about

33:37.320 --> 33:41.760
using them and what directions are each of them going?

33:41.760 --> 33:48.000
So if we use the Snapdragon 855 introduction as the springboard for this one, Snapdragon

33:48.000 --> 33:54.040
855 is what we consider to have our fourth generation AI engine.

33:54.040 --> 33:59.360
I mentioned the AI engine as being the kind of the sound of the parts that make on-device

33:59.360 --> 34:01.960
machine learning acceleration possible.

34:01.960 --> 34:06.680
In Snapdragon 855, we've done a number of things to all the compute elements.

34:06.680 --> 34:13.560
We've added more arithmetic logic units to our CPUs, about 50 percent more.

34:13.560 --> 34:21.040
On the CPU side, we've incorporated dot product instructions that increase AI performance

34:21.040 --> 34:26.680
specifically at 8-bit fixed by four times or 4x.

34:26.680 --> 34:28.880
And then on the hexagon side.

34:28.880 --> 34:35.200
If you followed our path over these fast forward generations, hexagon has had a vector

34:35.200 --> 34:40.720
processor where we at least we've had the hexagon vector processor in our chips going

34:40.720 --> 34:46.560
all the way back from Snapdragon 820 to 835, 845, and now 855.

34:46.560 --> 34:51.680
And that's been one of the primary engines for doing on-device AI.

34:51.680 --> 34:58.600
So that we actually doubled the size or doubled the number of hexagon vector extensions

34:58.600 --> 35:03.120
as we call them between 845 and 855.

35:03.120 --> 35:08.280
And then we added for the first time a dedicated AI accelerator.

35:08.280 --> 35:10.360
We call it a tensor accelerator.

35:10.360 --> 35:19.280
It's part of the hexagon family and its sole purpose is to process DNNs on device.

35:19.280 --> 35:26.200
The hexagon vector extensions had been the prior in prior generations had been a primary

35:26.200 --> 35:31.800
engine for that, but vector extensions also have utility and doing vision processing

35:31.800 --> 35:33.280
and other compute functions.

35:33.280 --> 35:38.320
So it wasn't dedicated solely to AI.

35:38.320 --> 35:44.040
But even with the addition of this tensor accelerator, that does not mean that the rest of the

35:44.040 --> 35:50.560
family of compute engines doesn't participate to your point, to your question, are there

35:50.560 --> 35:55.960
specific reasons why you would want to use one engine versus another or one compute

35:55.960 --> 35:59.520
element versus another to run an AI algorithm?

35:59.520 --> 36:03.720
And the answer goes back to something, I think I mentioned earlier, and it has to do with

36:03.720 --> 36:10.360
power and performance and what you want to get out of that particular user experience

36:10.360 --> 36:13.280
or how you want that user experience to perform.

36:13.280 --> 36:16.400
Also, what else is happening in the system?

36:16.400 --> 36:24.880
So if the phone is doing other tasks or as part of that user experience, you're doing

36:24.880 --> 36:33.080
graphics processing or you need the display to be highly performant.

36:33.080 --> 36:38.280
You may want to choose a different place to run an AI algorithm if the GPU has a higher

36:38.280 --> 36:43.560
workload or the same is true for the vector extensions.

36:43.560 --> 36:48.400
You may want to have a little bit more flexibility in where you run that AI algorithm.

36:48.400 --> 36:54.680
And then from a pure AI standpoint, what we've done to combine or in the system where

36:54.680 --> 37:01.880
combining hexagon vector extensions and our new tensor accelerator is a developer, you

37:01.880 --> 37:06.360
can utilize both of those, which gives you a kind of a balance between what would be

37:06.360 --> 37:13.360
programmable AI processing in the form of the vector extensions and then your dedicated

37:13.360 --> 37:16.040
acceleration with the tensor accelerator.

37:16.040 --> 37:21.040
You can combine those two to give, I guess I'll say I don't like to use the term ultimate,

37:21.040 --> 37:25.840
but the best performance possible by combining those two compute elements.

37:25.840 --> 37:32.840
But again, taking one step back, we don't look at AI as a one-size-fits-all problem,

37:32.840 --> 37:38.400
or at least it's not one-size-fits-all, so a one-compute architecture is not going to

37:38.400 --> 37:44.880
solve all the problems that you see or satisfy all the use cases that you see on device.

37:44.880 --> 37:48.840
It's just we're not at a stage and I don't know if we'll ever get to that stage to be

37:48.840 --> 37:50.640
quite honest.

37:50.640 --> 37:56.320
But at least with A55, we've provided a balance between what is our standard portfolio of

37:56.320 --> 38:02.120
compute with different programming capabilities and then this new dedicated accelerator, again

38:02.120 --> 38:07.240
each with a different power and performance profile to solve for the various use cases

38:07.240 --> 38:14.520
and tolerances that the developer has for the, when they're developing those features.

38:14.520 --> 38:20.400
In addition, and I guess I'll build on top of that, there's a big movement in the industry

38:20.400 --> 38:26.000
across the board, whether it's in, whether it's in the data center or it's on device,

38:26.000 --> 38:31.560
but dedicated acceleration, dedicated AI processors, there's various terms that are used.

38:31.560 --> 38:37.760
There's NPU for neural processing or there's VPU or IPU.

38:37.760 --> 38:43.880
All of these, there's a very big movement in that respect across the board in mobile,

38:43.880 --> 38:49.720
in IoT and automotive, and there is a demand for that and a need for that today.

38:49.720 --> 38:52.880
But that doesn't mean that that's where everything's going to go.

38:52.880 --> 38:58.840
That doesn't mean that a dedicated processor, dedicated processor is the answer to all the

38:58.840 --> 39:03.680
problems that you're going to see or satisfying all the use cases that you're going to see

39:03.680 --> 39:05.240
in various device classes.

39:05.240 --> 39:10.640
You do need a wide portfolio of compute to accommodate the level of variability.

39:10.640 --> 39:15.120
That is certainly there today and it's going to be there to, it'll be there for the foreseeable

39:15.120 --> 39:16.120
future.

39:16.120 --> 39:25.560
When you speak about these dedicated processors, does the tensor accelerator, for example, fit

39:25.560 --> 39:31.480
into that or are you specifically thinking about kind of an off board, off SOC processor?

39:31.480 --> 39:37.680
No, I think the tensor accelerator is an example of that.

39:37.680 --> 39:41.040
If you look at dedicated acceleration, there actually are two different vectors.

39:41.040 --> 39:45.400
One is embedded, which would be in the SOC or in the chip.

39:45.400 --> 39:55.440
In mobile, Qualcomm and others are or have introduced dedicated acceleration.

39:55.440 --> 40:01.400
Some of those are kind of reworked digital signal processors.

40:01.400 --> 40:09.840
In our case, we have a ground up architecture or from the ground up custom built IP.

40:09.840 --> 40:13.120
It's specifically designed for DNN processing.

40:13.120 --> 40:17.360
In tensor accelerator, but hexagon is kind of the reworked DSP.

40:17.360 --> 40:25.040
Yes, so hexagon, yeah, hexagon is a, it does actually a number of DSPs in the hexagon

40:25.040 --> 40:26.040
portfolio.

40:26.040 --> 40:30.360
There's a modem DSP, there's a compute DSP.

40:30.360 --> 40:34.560
In the past, hexagon has been associated with digital signal processing and even the

40:34.560 --> 40:39.400
vector extensions are part of our compute DSP.

40:39.400 --> 40:45.880
But the tensor accelerator is a totally unique architecture.

40:45.880 --> 40:53.840
In the market today, there are embedded accelerators in SOCs, in ours and others.

40:53.840 --> 41:00.560
There are also folks that are developing kind of off-chip dedicated AI chips that just

41:00.560 --> 41:03.680
do AI processing.

41:03.680 --> 41:08.880
They can augment what is on the primary application processor where they can be used independently.

41:08.880 --> 41:16.200
But there's again, going back to the, there's such a big movement and innovation is happening

41:16.200 --> 41:22.000
so fast that I guess the more, the bigger the sandbox that you provide a developer, the

41:22.000 --> 41:23.960
more they'll use it.

41:23.960 --> 41:30.440
In the case of mobile, there's a balance between what you provide in terms of overall compute,

41:30.440 --> 41:33.720
the variety of compute and the cost associated with it.

41:33.720 --> 41:41.040
You have less latitudes when it comes to kind of a primary SOC like Snapdragon to provide

41:41.040 --> 41:47.080
a massive AI accelerator and there's really not any need to in mobile, but there might

41:47.080 --> 41:52.080
be other instances where you do need a pure dedicated AI processing and quite a bit of

41:52.080 --> 41:55.160
it outside of mobile.

41:55.160 --> 42:03.480
On the software side of things, thinking about software platforms, I would call that

42:03.480 --> 42:11.480
kind of idea like a very unopinionated platform versus platforms that have very, this is the

42:11.480 --> 42:15.560
way to do it, very, very strong opinions.

42:15.560 --> 42:21.160
Do you think there's that analogy apply here and there are some implications of that on

42:21.160 --> 42:23.560
the software side.

42:23.560 --> 42:30.400
Oftentimes, developers want more opinionated platforms because they, they're less choices

42:30.400 --> 42:31.880
that they have to make.

42:31.880 --> 42:36.120
Is that analogy apply and what are the implications of it here?

42:36.120 --> 42:42.020
Yeah, I think if you had a going back in our conversation talking about developer pain

42:42.020 --> 42:49.680
points and what makes the developer's life easier difficult.

42:49.680 --> 42:56.040
We're not seeing a consistency across the board because there's still a ramping up of

42:56.040 --> 42:57.040
expertise.

42:57.040 --> 43:03.040
When we started this, I'll just speak from where I sit from four years ago when I began

43:03.040 --> 43:12.680
this process in this project in Qualcomm, developer savvy was, well, there wasn't much.

43:12.680 --> 43:17.920
A lot of abstraction and providing what would be turnkey solutions like an object classifier

43:17.920 --> 43:23.080
or image classifier or seeing classifier was what we started off with because nobody

43:23.080 --> 43:27.400
really understood how to do the job.

43:27.400 --> 43:32.200
They could train a model, but they didn't know where they had to run it on target.

43:32.200 --> 43:40.160
We've gone so far ahead or moved so far ahead just in four years that now developers are

43:40.160 --> 43:41.160
very savvy.

43:41.160 --> 43:42.160
The tools are there.

43:42.160 --> 43:44.400
There's a variety of them.

43:44.400 --> 43:52.000
The education is there and I think the desire across the desire from many of the developers

43:52.000 --> 43:56.720
that we deal with today and customers that we have today, they want more flexibility.

43:56.720 --> 43:59.760
They want more of an ability to experiment.

43:59.760 --> 44:08.920
They don't want to be stuck in a box and focus on a very specific programming method or

44:08.920 --> 44:13.160
way to access a particular compute engine.

44:13.160 --> 44:15.320
They want more variability.

44:15.320 --> 44:17.480
They want more flexibility.

44:17.480 --> 44:20.760
The openness is becoming more prevalent.

44:20.760 --> 44:25.600
It's not to say that proprietary tools and being very fixed in the approach to, let's

44:25.600 --> 44:31.120
say, running a natural language processing algorithm or something on a purpose-built

44:31.120 --> 44:38.240
device isn't helpful, but we are actually seeing more developers looking for choice and

44:38.240 --> 44:40.440
flexibility and modularity.

44:40.440 --> 44:44.200
They want to get closer to the metal when it comes to programming and they'd like to

44:44.200 --> 44:48.560
be able to do it in multiple ways.

44:48.560 --> 44:51.440
That's a trend, but I wouldn't say that that's a dominant trend.

44:51.440 --> 44:57.160
I think that's still, there's nothing that settles out to say that a proprietary approach

44:57.160 --> 45:01.720
and a fixed approach is the best, maybe over time that will be depending on the use case

45:01.720 --> 45:06.840
for the device class, but I still think it's highly variable at this point.

45:06.840 --> 45:11.640
We're speaking very early in January, it'll be a couple of weeks before folks hear this,

45:11.640 --> 45:16.320
but we're at the very beginning of 2019.

45:16.320 --> 45:23.440
What do you see happening in the space in 2019 and beyond?

45:23.440 --> 45:29.240
There are a couple of areas that I think we're looking at, and we see kind of emerging

45:29.240 --> 45:30.240
over time.

45:30.240 --> 45:37.320
One is the area of on-device learning, so today the lion's share of all the workloads that

45:37.320 --> 45:43.200
we see in mobile and in other devices, it's all inference, so it's the application or

45:43.200 --> 45:48.160
the utilization of a train model running on target to deliver that use case or solve

45:48.160 --> 45:49.160
that problem.

45:49.160 --> 45:56.000
There's very little learning happening, but we see a kind of a movement toward taking

45:56.000 --> 46:00.160
a train model, having it run on target, but also being able to take in data from the

46:00.160 --> 46:06.520
various sensors on device and augment that model to become more aware or contentionally

46:06.520 --> 46:12.840
aware of its environment to provide more personalization for consumers.

46:12.840 --> 46:16.760
Many of different techniques that are being explored in the industry, there's reinforcement

46:16.760 --> 46:24.400
learning and others, but I think that's going to be a trend that we see in 2019.

46:24.400 --> 46:28.840
Hardware acceleration or dedicated hardware for AI processing will continue to be a big

46:28.840 --> 46:41.160
movement, both in the embedded side as well as off-chip standalone AI processors.

46:41.160 --> 46:45.880
Unchmarking is probably one of the areas that I've mentioned Wild West a minute ago, but

46:45.880 --> 46:48.880
I think that certainly applies here.

46:48.880 --> 46:53.120
There's a lot of, I guess, confusion in the marketplace today when it comes to what

46:53.120 --> 46:59.280
is, first off, what is AI, and then secondarily, how do you measure its effectiveness?

46:59.280 --> 47:05.880
How do you measure the performance of running a particular convolutional neural network

47:05.880 --> 47:12.680
for a particular use case, or a specific network type or class like a ResNet or VGG or

47:12.680 --> 47:13.680
what have you?

47:13.680 --> 47:19.000
How do you actually measure that in a real-world setting and then compare hardware platforms

47:19.000 --> 47:20.400
to each other?

47:20.400 --> 47:26.080
Right now, there's a variety of entities that have sprung up to try to tackle that problem,

47:26.080 --> 47:34.440
but there's no consistency with methodology or formula or the underlying network classes

47:34.440 --> 47:38.680
that are used to actually do the measurement and ultimately what the benefit is.

47:38.680 --> 47:43.360
What is the final outcome of this and why does it matter?

47:43.360 --> 47:48.520
Benchmarks and graphics and CPU and in other cases are all very well settled, and there's

47:48.520 --> 47:54.400
still a lot of gamesmanship that goes on in those categories, but specifically in AI,

47:54.400 --> 48:02.920
that's an area that is quite fluid and it changes, I think, day-to-day.

48:02.920 --> 48:07.040
We see some of that settling out this year, and it would be good because it will help

48:07.040 --> 48:14.880
with OEMs making choices about who to pick as their SAC vendor, and it will help consumers

48:14.880 --> 48:19.320
understand a little bit better what is the benefit of what's happening on the device?

48:19.320 --> 48:20.320
Why does this matter?

48:20.320 --> 48:26.520
Why is this making my life more interesting and compelling and enjoyable?

48:26.520 --> 48:31.960
And then the last one, I think, is the movement that we're pushing very hard and something

48:31.960 --> 48:35.520
that we've been leading the charge on for a few years is 5G.

48:35.520 --> 48:40.680
So 5G connectivity isn't just going to make that connection between the data center and

48:40.680 --> 48:48.920
the device more efficient and lower latency, but devices will be able to share information

48:48.920 --> 48:52.000
with each other in ways that they haven't previously.

48:52.000 --> 48:59.120
And the combination of 5G and AI, albeit 5G, will be at the starting point in 2019, you'll

48:59.120 --> 49:06.640
probably see developers trying to take advantage of that connectivity platform in a way that

49:06.640 --> 49:10.280
they haven't in the past and being able to make devices connect to each other in a more

49:10.280 --> 49:11.760
intelligent way.

49:11.760 --> 49:16.280
We see a big future with a combination of 5G and AI, in fact, that's where we're spending

49:16.280 --> 49:19.200
most of our time and effort these days at Qualcomm.

49:19.200 --> 49:23.920
One of the things you mentioned kind of peak my interest in the context of on-device learning

49:23.920 --> 49:29.640
is reinforcement learning. I typically think of that as like running lots of simulations

49:29.640 --> 49:34.720
taking a long time, which doesn't seem like something that I'd necessarily want to do

49:34.720 --> 49:42.040
on a device. Are you aware of specific things that are happening to address that in the

49:42.040 --> 49:44.520
context of on-device learning?

49:44.520 --> 49:48.280
I don't have any examples to share with you now and you're right that a lot of what's

49:48.280 --> 49:53.440
happening today and we look at different processes that we're trying to explore, like, reinforcement

49:53.440 --> 50:03.920
learning, usage, off-target or not on-device, but improving processes that would end up

50:03.920 --> 50:09.120
running on the device. I don't have any examples and I just pulled that out as one potential,

50:09.120 --> 50:17.760
but nothing specifically that I could point to. But on-device learning specifically, the

50:17.760 --> 50:25.800
desire or the demand from developers and even our OEM partners to be able to take better

50:25.800 --> 50:31.600
advantage of the incoming data from the various sensors on the device, you know, microphone

50:31.600 --> 50:36.960
and camera and accelerometer to be able to provide a more personalized experience in

50:36.960 --> 50:42.000
general. There's a there's high demand there and I think you're going to see, you'll

50:42.000 --> 50:47.280
see more effort, maybe some experimentation, but there'll be more emphasis place on trying

50:47.280 --> 50:56.240
to make the device itself, not just a kind of fix with a specific intelligence level,

50:56.240 --> 51:01.160
but one that would be heightened by way of additional context that it's able to grok

51:01.160 --> 51:04.400
from the world around it. Well, Gary, thanks so much for taking

51:04.400 --> 51:09.480
the time to chat with us. I really learned a lot and I enjoyed the conversation.

51:09.480 --> 51:14.000
Yeah, thanks very much, Sam. I appreciate it and thanks for let me share your birthday

51:14.000 --> 51:19.200
with you. Awesome. Thanks. Take care. You too.

51:19.200 --> 51:27.040
All right, everyone. That's our show for today. For more information on Gary or any of

51:27.040 --> 51:33.760
the topics covered in this show, visit twimmalei.com slash talk slash 223. Be sure to check out

51:33.760 --> 51:40.880
what Qualcomm is up to at twimmalei.com slash Qualcomm. As always, thanks so much for listening

51:40.880 --> 51:47.880
and catch you next time.

