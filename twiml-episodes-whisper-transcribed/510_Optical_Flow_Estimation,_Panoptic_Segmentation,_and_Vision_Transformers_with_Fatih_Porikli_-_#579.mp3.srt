1
00:00:00,000 --> 00:00:11,320
All right, everyone. Welcome to another episode of the Twimmel AI podcast. I'm your host,

2
00:00:11,320 --> 00:00:16,960
Sam Charrington. And today I'm joined by Fatih Pratikli, Senior Director of Artificial

3
00:00:16,960 --> 00:00:22,440
Intelligence at Qualcomm. Before we get into today's conversation, be sure to take a moment

4
00:00:22,440 --> 00:00:27,840
to head over to Apple Podcasts or your listening platform of choice. And if you enjoy the show,

5
00:00:27,840 --> 00:00:33,160
we'd greatly appreciate your five-star rating and review. Fatih, welcome to the podcast.

6
00:00:33,160 --> 00:00:38,640
Thank you so much for hearing me, Sam. This is pleasure. I'm very excited. I'm all yours.

7
00:00:38,640 --> 00:00:43,520
All righty, I'm really looking forward to digging into our conversation. We'll be talking a lot

8
00:00:43,520 --> 00:00:49,920
about your research in the areas of computer vision and perception. But before we do that,

9
00:00:49,920 --> 00:00:54,160
I'd love to have you share a little bit about your background and how you came to work in the field.

10
00:00:54,160 --> 00:01:02,160
I am a computer scientist and electrical engineer. I was on the both sides of the fence,

11
00:01:02,160 --> 00:01:08,560
the fence, dividing academia and industry several times. You know, as everyone, I started my PhD.

12
00:01:08,560 --> 00:01:14,320
I did some research. I was a research assistant. I stayed at the university. Then I moved to

13
00:01:14,320 --> 00:01:20,400
industry. Many years after that, working on real problems. We're challenging problems.

14
00:01:20,400 --> 00:01:27,280
Maybe 13 years after that, I switched back to academia again. I was a full professor,

15
00:01:27,280 --> 00:01:36,160
tenor professor for a long time. Then I found myself intentionally, of course, in industry again,

16
00:01:36,960 --> 00:01:44,320
even trying to solve bigger problems, trying to create bigger impact for everyone. So now I'm with

17
00:01:44,320 --> 00:01:51,040
Qualcomm. And the whole time, have you been working on computer vision or have you switched areas of

18
00:01:51,040 --> 00:01:59,760
interest? Computer vision was always there. That is one of the things really excites me,

19
00:02:00,480 --> 00:02:09,280
amazes me, because if we consider human brain electrical activity, maybe 70-75% of what we

20
00:02:09,280 --> 00:02:18,400
actually consume in our brain dedicated to visual perception, significant portion of brain is also

21
00:02:18,400 --> 00:02:26,160
goes to visual understanding. So vision is the way that we understand, makes sense of the

22
00:02:27,120 --> 00:02:33,600
world life, everything around us. Actually, if we close our eyes, you know, I just close,

23
00:02:33,600 --> 00:02:40,800
a loss of vision might be the most devastating disability. You know, it comes so naturally to us,

24
00:02:40,800 --> 00:02:47,360
the way that we understand the 3D scene, recognize people, recognize faces, recognize objects. So

25
00:02:48,000 --> 00:02:55,040
then I was almost always interested in how we can make computers, machines to, you know,

26
00:02:55,920 --> 00:03:01,200
have that capability as well, this visual understanding, visual perception. So computer vision

27
00:03:01,200 --> 00:03:08,240
has been always there. Before maybe, let's say, 25-30 years ago, it was more conventional,

28
00:03:08,240 --> 00:03:15,840
engineered solutions, you think about, okay, what would the human perception do? How would

29
00:03:15,840 --> 00:03:21,840
brain work? And how I'm going to sit down and try some mathematical description to convert it

30
00:03:21,840 --> 00:03:30,640
to something a computer would understand. Now we are kind of as many of us are very familiar,

31
00:03:30,640 --> 00:03:42,480
we are using AI artificial intelligence to make it natural, look into data and learn something

32
00:03:42,480 --> 00:03:48,880
automatically from observing the environment around us. Awesome, awesome. Can you share a little bit

33
00:03:48,880 --> 00:03:54,800
more around your areas of interest from a research perspective? Under perception, there are

34
00:03:54,800 --> 00:04:02,800
several modalities. One is working with image and video data. This would be directly related to

35
00:04:02,800 --> 00:04:10,960
computer vision and then there is 3D data, point cloud and 3D representations. That's also,

36
00:04:10,960 --> 00:04:17,840
I will say, that computer vision as well. But perception is not only in these two modalities,

37
00:04:17,840 --> 00:04:25,040
3D point cloud and image video visual data, there is also RF radio frequency signals all around us.

38
00:04:25,040 --> 00:04:32,560
And they are, in a way, kind of the lights that we see, they are all around us feeling

39
00:04:32,560 --> 00:04:38,800
the space. We also look into those invisible frequencies and try to understand everything about

40
00:04:38,800 --> 00:04:47,520
the scene, about the world, about how everything works, objects, intrex. So these are the modalities that

41
00:04:47,520 --> 00:04:53,680
I am very much interested in it. What do we do with these modalities? For instance, in images

42
00:04:53,680 --> 00:05:02,640
and videos and 3D point cloud data, RF signals, any signal actually, including X-ray and ultrasound,

43
00:05:02,640 --> 00:05:14,160
we detect things. For instance, whether there is a vehicle or a person on the street, we reconstruct

44
00:05:14,160 --> 00:05:20,160
3D model of the world around us. That's also very interesting, very challenging, actually,

45
00:05:20,160 --> 00:05:26,160
if you want to just use one single image, not two, like we do. We have two left and right eye,

46
00:05:26,160 --> 00:05:32,800
so we use stress-coffee vision, but can you do it just using a single camera image? And the answer

47
00:05:32,800 --> 00:05:40,160
is yes, for a while, you know, I was really impressed with that one, and recognizing activities,

48
00:05:41,600 --> 00:05:47,760
labeling everything in the scene. In a way that what goes on the lower level in our brain, we

49
00:05:47,760 --> 00:05:56,800
want to do all of these, accomplish all of those processes, perception, and all of them were

50
00:05:56,800 --> 00:06:04,400
very interesting to me personally, and these tests, these understanding goes into many applications

51
00:06:04,400 --> 00:06:11,120
from, let's say, XR, augmented reality and virtual reality to autonomous vehicles, to robotics,

52
00:06:11,120 --> 00:06:19,600
IoT, you can imagine, wherever there is a human being, and if you replace or put a machine in front

53
00:06:19,600 --> 00:06:26,800
of it, kind of those applications exist, all around us and a computer vision enables all of those,

54
00:06:26,800 --> 00:06:34,480
and that's why I think it's very exciting to me. And some of these problems are big problems,

55
00:06:34,480 --> 00:06:42,160
they are not solved problems, they are presenting a big challenge, so that's another attractiveness

56
00:06:42,160 --> 00:06:50,000
for many people. So I want to dig into a few of the perception-related papers that you've got

57
00:06:50,640 --> 00:07:03,920
at CVPR this year, and the first of the ones is a paper on panoptic segmentation. The full title is

58
00:07:03,920 --> 00:07:09,840
Panoptic Instance and Semantic Relations, a relational context encoder to enhance

59
00:07:09,840 --> 00:07:16,400
panoptic segmentation. Let's start at the beginning, what is panoptic segmentation?

60
00:07:16,400 --> 00:07:25,680
Yeah, it's a long title, San, panoptic segmentation. So there are things and stuff around us,

61
00:07:25,680 --> 00:07:30,960
right? Things are the comfortable things like there is one vehicle, another vehicle, another vehicle,

62
00:07:30,960 --> 00:07:36,080
one glass, another glass, one person, another person, but there are also uncomfortable things like

63
00:07:36,720 --> 00:07:43,680
sky, like building, like road, is not comfortable, right? So segmentation, the goal of segmentation,

64
00:07:43,680 --> 00:07:51,280
take this visual information, images and video, or point cloud, and then label every pixel,

65
00:07:51,280 --> 00:07:57,600
every region with the identity of that region. For instance, if it's a sky, if we see sky,

66
00:07:57,600 --> 00:08:03,600
it will tell, computer will tell, okay, this is a sky pixel, that's specific pixel and the region,

67
00:08:03,600 --> 00:08:09,280
if there is a person, it will tell, this is person, but it's not going to just say person,

68
00:08:09,280 --> 00:08:15,680
it's going to say that this is person A, and another person B, even though they are occluded

69
00:08:15,680 --> 00:08:21,920
each other, maybe half of the person B is visible, it will still distinguish. So this is a very

70
00:08:21,920 --> 00:08:30,800
challenging task. You are trying to label all data pixels in this case with these corresponding

71
00:08:30,800 --> 00:08:38,160
things and stuff identities. That is what penoptic segmentation does. And so from that,

72
00:08:38,160 --> 00:08:45,520
in that sense, it is kind of a superset of instant segmentation, which is identifying the things

73
00:08:45,520 --> 00:08:50,960
and semantic segmentation, which is more focused on the stuff. Very good, absolutely. Yes, actually,

74
00:08:50,960 --> 00:08:58,720
that's the right technical description, instant segmentation and semantic segmentation together

75
00:08:58,720 --> 00:09:05,760
will give, would be under the penoptic segmentation. Describe the setting for this paper,

76
00:09:05,760 --> 00:09:11,360
what is the problem that you set out to the solve? So now we understand what penoptic segmentation is,

77
00:09:12,080 --> 00:09:18,880
and as maybe I should point out that recently there has been significant attention and

78
00:09:18,880 --> 00:09:26,320
excitement around a new technology in AI, which is called as transformers. So transformers,

79
00:09:26,320 --> 00:09:35,360
let me briefly mention that when we give an image data video algorithm learns which parts of

80
00:09:35,360 --> 00:09:43,040
for instance, image are related to each other. It knows to collect such supportive information,

81
00:09:43,040 --> 00:09:49,040
pay right attention to the right parts. For instance, if there is a vehicle part,

82
00:09:49,040 --> 00:09:55,840
a tire would, you know, put more confidence, if less everyone to detect a vehicle,

83
00:09:57,280 --> 00:10:04,000
with the hood of the vehicle, door of the vehicle. But road pixels, sky pixel, they will,

84
00:10:04,000 --> 00:10:11,200
even though they may look similar, attention mechanism will ignore those. So it will focus on the

85
00:10:11,200 --> 00:10:18,480
better parts, supportive evidence to, you know, make such deductions. So transformer is

86
00:10:19,680 --> 00:10:26,800
self-attention, advanced self-attention mechanism. It is important to relate these areas,

87
00:10:26,800 --> 00:10:35,760
image areas, and it has been applied to semantic segmentation before. Now the challenging part for

88
00:10:35,760 --> 00:10:42,160
penoptic segmentation, there are, you know, unknown random number of things like these instances

89
00:10:42,160 --> 00:10:49,600
of the other classes, like people and vehicles in the scene. How are we going to use transformers

90
00:10:49,600 --> 00:10:58,080
to make sure that these instance of, let's say, this person kind of is different. This transform

91
00:10:58,080 --> 00:11:04,240
mechanism would distinguish from the other person. Otherwise, since they are, they look same,

92
00:11:04,240 --> 00:11:10,560
they will make the transformer will think that they are same thing, and it may not distinguish

93
00:11:10,560 --> 00:11:19,600
these two. For penoptic segmentation, we want to label them separately. So this paper explains,

94
00:11:19,600 --> 00:11:27,600
first time in the world, how you can actually kind of combine this type of attention mechanism

95
00:11:27,600 --> 00:11:35,680
into a segmentation framework. Got it. And so what have prior approaches tried to do

96
00:11:36,640 --> 00:11:44,480
for solving penoptic segmentation? On a high level, we can consider there are a single shot or,

97
00:11:46,320 --> 00:11:53,280
you know, multi-shot setting, single-shot setting. It aims to take an input image and directly

98
00:11:53,280 --> 00:12:01,680
label each pixel as a different object, instance of an object, like this person and the other person.

99
00:12:02,320 --> 00:12:10,720
And there are multi-shot algorithms, multi-shot meaning first, we find these regions of interest.

100
00:12:11,680 --> 00:12:17,360
You can think that those are like boxes. You say, okay, this is like a box. There's a person here,

101
00:12:17,360 --> 00:12:24,000
another person here, another person here, then in the second stage, we go and look, okay,

102
00:12:24,000 --> 00:12:31,840
these two, this box contains a single instance of a person, and it doesn't overlap with anything,

103
00:12:31,840 --> 00:12:36,560
okay, good, then I will just go, you know, do segmentation between these boxes and I will find

104
00:12:36,560 --> 00:12:43,360
segmentation to person. If there is overlap, then I will kind of infer which one, which pixel

105
00:12:43,360 --> 00:12:49,920
is blocked, which person there are two or multiple persons. So this is the other way of doing that.

106
00:12:50,560 --> 00:12:58,720
Usually these algorithms lack an attention mechanism across different instances. We can do it

107
00:12:58,720 --> 00:13:04,240
pixelized, but if I have one person here, another person behind it, another person far away,

108
00:13:04,240 --> 00:13:14,240
how I'm going to learn where to focus if I want to detect all of them at the same time. So this

109
00:13:14,240 --> 00:13:19,760
is what we accomplish. For a different number, varying number of instances,

110
00:13:21,360 --> 00:13:32,160
CVPR paper shows that you can learn or train an algorithm that would learn to focus on the

111
00:13:32,160 --> 00:13:38,480
right areas of the image, which then improves the accuracy of the segmentation. That's what we

112
00:13:38,480 --> 00:13:44,800
show in the paper. Got it, got it. I think this is maybe related to the single shot versus multi-shot,

113
00:13:44,800 --> 00:13:49,440
but I got the impression from the paper that one of the big things that you're doing differently

114
00:13:49,440 --> 00:13:57,440
here is that previous attempts have tried to, hey, in order to solve penoptic segmentation,

115
00:13:57,440 --> 00:14:04,800
let's do image segmentation or instance segmentation and then semantic segmentation

116
00:14:05,440 --> 00:14:12,640
separately and kind of put the results together, whereas you're doing them as part of a consistent

117
00:14:12,640 --> 00:14:20,480
coherent system. Absolutely, very good point. Now, the same network can do in an end-to-end

118
00:14:20,480 --> 00:14:27,200
fashion, these two tests together. And when you do it together in an end-to-end fashion in the same

119
00:14:27,200 --> 00:14:34,880
network, they support each other. They don't kind of dismiss, but for instance, semantic segmentation

120
00:14:34,880 --> 00:14:42,160
will generate or instance segmentation will generate the leverage together and which generates

121
00:14:42,160 --> 00:14:50,400
better improved numbers, honestly. What were some of the biggest challenges to this approach?

122
00:14:50,400 --> 00:14:57,040
Transformer architecture or self-attention architecture. One challenge, I can say, that they are

123
00:14:57,040 --> 00:15:07,520
computationally, you know, intensive and how to make them efficient was a challenge. And also,

124
00:15:07,520 --> 00:15:14,480
you know, our people is not specific to any specific backbone. A backbone usually considered

125
00:15:14,480 --> 00:15:22,480
as a pre-processing neural network takes image or video and then creates useful features for the

126
00:15:22,480 --> 00:15:28,000
downstream test, like semantic penoptic or instance segmentation or many other tests.

127
00:15:28,000 --> 00:15:36,880
Our algorithm, our idea actually, can apply to any backbone, any, you know, kind of a segmentation

128
00:15:36,880 --> 00:15:43,680
framework, it could be plugged into improve their performance. In the paper, we tried maybe more than

129
00:15:43,680 --> 00:15:52,960
15 or 20 different segmentation algorithms and every time we plugged in this type of transformer-based

130
00:15:54,160 --> 00:16:04,640
instance self-attention with, you know, kind of semantic segmentation, the results were much better.

131
00:16:04,640 --> 00:16:13,520
How do you have both the ability to plug in whatever segmentation model you want to use?

132
00:16:15,200 --> 00:16:19,760
Was that specifically for the instance segmentation or for either of the components?

133
00:16:20,640 --> 00:16:28,800
What our algorithm does, it leverages these features coming from the previous segmentation algorithm

134
00:16:28,800 --> 00:16:36,000
and then it takes them and it learns, reweighting them. In a way, that's what transformer does.

135
00:16:36,000 --> 00:16:44,160
The input transformer is some kind of, let's simplify it, let's say it is an image, output is

136
00:16:44,160 --> 00:16:54,720
another image, let's say, but what you see, like maybe now much clear and focus on the right parts.

137
00:16:54,720 --> 00:16:59,840
Maybe input image, you can't think that it's a noisy image and there are some, you know, kind of like

138
00:16:59,840 --> 00:17:07,280
things not very visible in the output, now much sharper and kind of these things are clearly

139
00:17:07,280 --> 00:17:14,880
distinguishable. Of course, it is not an image but goes into this network, it is a set of feature

140
00:17:14,880 --> 00:17:23,280
vectors and it's called as the features for image. Each pixel has a feature descriptor,

141
00:17:23,280 --> 00:17:31,440
those descriptors goes into this component and comes in a better, more trustable,

142
00:17:32,960 --> 00:17:42,800
kind of more useful manner. You know, then we do that, then we make the features better,

143
00:17:42,800 --> 00:17:49,360
any downstream test will benefit from that. So all these segmentation algorithms, they have

144
00:17:49,360 --> 00:17:55,040
this type of feature generators either at the beginning or at the end. So this idea can plug

145
00:17:55,920 --> 00:18:02,880
kind of and directly apply to those feature maps. Got it, got it. So the core of what you've done

146
00:18:02,880 --> 00:18:08,320
is this transformer that takes as input these feature vectors and you don't really care how the

147
00:18:08,320 --> 00:18:16,160
feature vectors are created, any of these algorithms that you've tried, it worked just fine as part of

148
00:18:16,160 --> 00:18:23,040
your network. Yes, this is a strength of it, it can take any of the features and make them better,

149
00:18:23,040 --> 00:18:30,240
you know, more representative of the test that we want to accomplish. However, this is also

150
00:18:30,240 --> 00:18:37,440
something known in the paper, then we use this thing and we also go back to the previous

151
00:18:37,440 --> 00:18:44,880
stages like feature generator and other branches to make them even more accurate. So kind of

152
00:18:44,880 --> 00:18:52,880
when we do end-to-end training using PISR, the paper, the idea that we talk about in the paper,

153
00:18:53,760 --> 00:19:01,600
all network becomes kind of updated and even the previous part, feature generation parts

154
00:19:01,600 --> 00:19:09,760
improves. So overall kind of that further improves the accuracy. Semantic segmentation, you know,

155
00:19:09,760 --> 00:19:15,680
penoptic segmentation is one of the most challenging tests in computer vision. It's very difficult

156
00:19:15,680 --> 00:19:21,040
for a human to segment. By the way, if you want to do, if I, if you ask me to go, you know, label each

157
00:19:21,040 --> 00:19:27,920
instance, I will do something, but if you ask another person, it will do differently, you know,

158
00:19:27,920 --> 00:19:36,080
even for humans, there is significant variation in the outputs of how we, you know, do penoptic

159
00:19:36,080 --> 00:19:43,040
segmentation, instance and semantic segmentation for a computer vision algorithm for a machine to

160
00:19:43,040 --> 00:19:51,200
do it even more challenging. So this type of ideas really kind of pushes the state off to our

161
00:19:51,200 --> 00:19:59,200
such that, you know, they are becoming more and more feasible for bigger use cases to improve

162
00:19:59,200 --> 00:20:05,840
our daily lives through these applications in XR and auto and other type of use cases.

163
00:20:06,480 --> 00:20:14,240
How did you measure the performance of your model? There are very recognized metrics and there

164
00:20:14,240 --> 00:20:22,960
are benchmark data sets. So the huge benchmark data sets, those benchmark data sets have the

165
00:20:22,960 --> 00:20:29,440
ground truth information one way or another defined. These are real also data sets, real images,

166
00:20:29,440 --> 00:20:41,040
real videos. We use those metrics, for instance, mean IOU or similar metrics. It defines how well

167
00:20:41,520 --> 00:20:50,320
one mask, object mask overlaps with another one. So this is very common in semantic segmentation.

168
00:20:50,320 --> 00:20:56,000
For instance, segmentation, there are similar, you know, advanced versions of this thing now

169
00:20:56,000 --> 00:21:04,560
considering whether kind of we are confusing identity of the instances or not. So there are

170
00:21:04,560 --> 00:21:11,280
these common metrics and benchmarks that this how we evaluate the algorithm. And what kind of results

171
00:21:11,280 --> 00:21:18,480
did you see? Then we submitted the paper, we look at all the existing state of the art,

172
00:21:18,480 --> 00:21:25,440
existing work including the archive, things appeared on the archive, not maybe published,

173
00:21:25,440 --> 00:21:32,400
but very fresh things. So just before the submission deadline, the previous week we applied

174
00:21:32,400 --> 00:21:41,680
whatever we found and every time we observed improvement on this segmentation pipelines

175
00:21:41,680 --> 00:21:49,280
and our results also when we submitted to different or investigated, you know, generated results

176
00:21:49,280 --> 00:21:55,520
on these benchmarks were the top of the line. And in the paper, we showed that, you know,

177
00:21:55,520 --> 00:22:02,480
those are the best segmentation results possible. Of course, the field is changing, maybe

178
00:22:03,600 --> 00:22:09,600
next CVPR, there might be even better numbers that might be coming from us or other people,

179
00:22:09,600 --> 00:22:16,800
but at the time, it was the number one on multiple datasets also, not one dataset.

180
00:22:16,800 --> 00:22:24,000
Okay. And where do you see this particular line of research heading? Is it a solve,

181
00:22:24,000 --> 00:22:32,960
is panaptic segmentation a solve problem now? I think when the conditions are right, it is,

182
00:22:32,960 --> 00:22:40,880
this performance is, you know, almost product quality level, but there is significant variation

183
00:22:40,880 --> 00:22:47,600
in the input quality. For instance, it could be a dark image, it could be a very noisy image,

184
00:22:47,600 --> 00:22:53,040
it could be a blurred image, you know, you can imagine, you know, there might be many problems

185
00:22:53,040 --> 00:23:00,480
in the image, things may be very small, some of the objects might be very tiny and not maybe only,

186
00:23:00,480 --> 00:23:05,680
let's say, a hand of a person would be visible, you know, significantly up to the country,

187
00:23:05,680 --> 00:23:11,440
our goal is actually to take that hand as well, you know, kind of even the only is the hand.

188
00:23:11,440 --> 00:23:19,520
So those type of very difficult settings still require more work to make the algorithm

189
00:23:20,400 --> 00:23:25,440
to be more robust, generalize those type of challenging situations. And also,

190
00:23:25,440 --> 00:23:33,040
another thing, if, for instance, we use a data set and that data set collected in a specific

191
00:23:33,040 --> 00:23:39,200
manner using maybe the similar type of cameras and labeling might be similar and lighting might

192
00:23:39,200 --> 00:23:46,720
be similar, but now in a specific application, the environment lighting, everything would be

193
00:23:46,720 --> 00:23:52,560
different, how to adapt to that such domain changes, domain variations is one thing.

194
00:23:52,560 --> 00:24:01,360
Another thing, some, I mean, we don't, these things, instances could be any class,

195
00:24:01,360 --> 00:24:06,800
right? I may be repeated many times, like it could be a person, it could be a vehicle,

196
00:24:06,800 --> 00:24:12,240
but it could be anything, right? It could be, you know, a piece of machinery,

197
00:24:12,240 --> 00:24:18,400
it could be a kind of component in an assembly line, you can imagine, you know,

198
00:24:18,400 --> 00:24:24,400
it could be a bird, multiple birds, you know, for kind of, you can imagine, this is like a commodity,

199
00:24:24,400 --> 00:24:30,000
this type of AI solution, people would like to take it and count the number of ends, you know,

200
00:24:30,000 --> 00:24:37,600
in a lab setting, those type of things. So how to adapt automatically with minimal labeling

201
00:24:37,600 --> 00:24:43,760
to such different classes, different type of things, different type of stuff for semantic

202
00:24:43,760 --> 00:24:50,640
and instant segmentation, it is, I think, still a challenge and we are working on all of those

203
00:24:50,640 --> 00:24:56,640
problems, which would, for instance, take our solution and automatically adapt to a very

204
00:24:56,640 --> 00:25:02,960
different, completely different, you know, class, set of classes, different type of objects.

205
00:25:02,960 --> 00:25:12,880
So there is still work to be done, but the quality of the segmentation results for key applications,

206
00:25:12,880 --> 00:25:20,080
for instance, camera essentials for autonomous vehicles, for XR applications, for robotics,

207
00:25:20,080 --> 00:25:28,160
I think it's very promising and soon such solutions, either from us or maybe, you know,

208
00:25:28,160 --> 00:25:36,080
everyone in the world using our solutions will appear in products. I'm very confident about it.

209
00:25:36,080 --> 00:25:43,360
Awesome. Does this approach assume prior knowledge of the classes? I was envisioning this like

210
00:25:43,360 --> 00:25:49,040
an autonomous vehicle type of scenario where you, you know, you have a camera off of,

211
00:25:49,760 --> 00:25:55,040
you know, a picture of a road, a camera off of the front of the vehicle and there's something

212
00:25:55,040 --> 00:26:02,240
in the road and you're trying to differentiate, you know, not something in the road versus something

213
00:26:02,240 --> 00:26:06,720
in the road, but that's a different problem than what we're talking about here. Oh, this may be

214
00:26:06,720 --> 00:26:13,680
obstacle detection or, you know, we have a class of unrecognizable things as well. There are ways to

215
00:26:13,680 --> 00:26:21,360
solve it. Yes, that is also possible, but if it's supervised solution, if the target classes

216
00:26:21,360 --> 00:26:28,640
changes, it's a problem. You have to do this transfer learning. There is a specific term for that,

217
00:26:28,640 --> 00:26:37,440
you know, my goal target classes now change. How I'm going to leverage on the previous network

218
00:26:37,440 --> 00:26:43,200
that I trained and maybe some portion of the previous data, anything semantically related,

219
00:26:43,200 --> 00:26:51,200
semantic information and now I can adapt to this type of things. Two, the day things are not even

220
00:26:51,200 --> 00:26:59,600
be defined. Usually, there is a class of unidentified things. We don't know what they are,

221
00:26:59,600 --> 00:27:07,200
like these are unidentified object classes. You have all, like you all see something, you know,

222
00:27:07,200 --> 00:27:14,320
we can give it a name, but then maybe some other intelligent mechanism has to decide,

223
00:27:14,320 --> 00:27:19,520
oh, is this something that I should worry about it? If I'm approaching that thing, is it going to

224
00:27:19,520 --> 00:27:25,200
be a problem for us or not? We are, of course, that's a more higher level inference. There are

225
00:27:25,200 --> 00:27:31,520
ways of doing that. Supervised learning, of course, limited. Now, at Qualcomm, also, we are looking

226
00:27:31,520 --> 00:27:37,600
at self-supervised solutions or unsupervised solutions. Exactly for, this is one of the reasons,

227
00:27:37,600 --> 00:27:43,520
you know, we cannot expect people to generate these data labels, ground truth, to train these

228
00:27:43,520 --> 00:27:49,760
algorithms over and over again. And human beings, we don't learn that way, right? I mean, it's not

229
00:27:49,760 --> 00:27:57,680
like here's an example of a cat, example of a dog, example of a tiger, and then we remember that

230
00:27:57,680 --> 00:28:06,400
we know when we see an animal, that is an animal, you know, it is not like, I don't need a training

231
00:28:06,400 --> 00:28:12,240
data. Even if I don't see it, I would infer, you know, I would deduce that, okay, it looks like a

232
00:28:12,240 --> 00:28:22,240
tiger, so it might be some type of tiger. So we do it using this continual self and unsupervised

233
00:28:22,240 --> 00:28:29,120
learning. And we are, we have solutions actually, you know, kind of applied to different tests.

234
00:28:29,120 --> 00:28:35,600
In this paper, we don't really talk about that, but that is something we are very active on it as

235
00:28:35,600 --> 00:28:42,080
well. Another paper that we wanted to chat about was the imposing consistency for optical

236
00:28:42,080 --> 00:28:49,040
flow estimation paper. Tell us about the problem that you're trying to solve there.

237
00:28:49,760 --> 00:29:01,040
Yeah, absolutely. So optical flow is finding where each pixel in the current image was in the

238
00:29:01,040 --> 00:29:11,200
previous image. So in a way, it is motion. It describes pixelized motion. Why this is important,

239
00:29:11,200 --> 00:29:17,600
because if I know where the pixel was in the previous frame, then, you know, first of all, I know

240
00:29:17,600 --> 00:29:25,440
how things are moving in the scene. I can deduce about the camera motion, and then I can also

241
00:29:25,440 --> 00:29:34,160
understand object motion. For instance, we have a headset, XR headset, and or AR glass, we are

242
00:29:34,160 --> 00:29:41,120
moving our head. And this is most, but then some other people also most. So we know this type of

243
00:29:41,120 --> 00:29:49,920
motion, which is important. And also, I can relate the previous frame to when I compute or make

244
00:29:49,920 --> 00:29:57,680
deduction for the current frame. So motion is important. And optical flow is what you will

245
00:29:57,680 --> 00:30:08,240
to obtain if you correspond this pixel with its previous location in the previous frame. So it

246
00:30:08,240 --> 00:30:14,000
looks like a field, you know, last so kind of like motion vector from individual pixels to the next

247
00:30:14,000 --> 00:30:19,440
to where that pixel ends up in the next frame. Of course. Yeah, it could be from current frame to

248
00:30:19,440 --> 00:30:25,120
the previous frame or previous frame to the current frame or current to the future frame. We can

249
00:30:25,120 --> 00:30:32,560
predict also. Absolutely, sir. Got it. And so what's the approach that you took with this paper?

250
00:30:33,200 --> 00:30:41,200
So one challenge, again, in AI, in data driven learning, the dataset, what I mentioned before,

251
00:30:41,920 --> 00:30:51,920
we need ground-through data for supervised training. Here is, let's say, two images like the

252
00:30:51,920 --> 00:30:59,120
current frame and the previous frame, video frames. And this is the optical flow motion between them.

253
00:30:59,840 --> 00:31:07,680
So we can, you know, if we synthesize those images, we can we know we control everything,

254
00:31:07,680 --> 00:31:13,920
we might have a game engine, for instance, or any, you know, any software can generate this type

255
00:31:13,920 --> 00:31:19,920
of different, we can, for instance, move the image, things in the image in a game, and we know how

256
00:31:19,920 --> 00:31:25,600
they moved, the type of ground-through is available. But if we have real images, you know, how we are

257
00:31:25,600 --> 00:31:33,520
going to find the ground-through, like how each pixel is moved is not like something measurable.

258
00:31:33,520 --> 00:31:37,040
You can think that there's a much harder labeling problem than what we just talked about.

259
00:31:37,040 --> 00:31:45,280
Absolutely. So such datasets, I mean, still computed datasets, there are some datasets,

260
00:31:45,280 --> 00:31:52,320
smaller scale. And in AI, we want big datasets, you know, huge datasets with tens of thousands of

261
00:31:52,320 --> 00:31:59,680
samples. It doesn't exist. So if we don't have the dataset, we will not have a good model.

262
00:32:00,480 --> 00:32:06,640
So in this paper, but we show that, you know, you do not need such a big dataset. We will do

263
00:32:06,640 --> 00:32:12,640
self-supervised learning, unsupervised learning. We will, for instance, take the previous image,

264
00:32:12,640 --> 00:32:20,160
and we will do some transformations on it. We will rotate it, we will warp it, and we will

265
00:32:20,160 --> 00:32:25,520
apply lots of different degradations, you know, without really destroying the image, still,

266
00:32:25,520 --> 00:32:30,320
you know, it's like similar scene, and then we look at it, you know, it would maybe look a little

267
00:32:30,320 --> 00:32:37,600
bit noisier or less noisier or the color is different, but maybe it's also warped. So we do all

268
00:32:37,600 --> 00:32:45,520
type, this type of transformations, and we know, because we applied those, we defined those

269
00:32:45,520 --> 00:32:51,600
transformations, so we know the ground truth in that way. So what about leveraging on that thing,

270
00:32:51,600 --> 00:32:58,080
and if we do that, you know, kind of optical flow should be consistent with the way that we warp

271
00:32:58,080 --> 00:33:08,080
transform the image, this one thing, we also look, okay, if I do forward, if I go backward, what would happen.

272
00:33:08,080 --> 00:33:17,040
Let's say there's, my hand is moving, right, in front of my face, and then it moves, either it

273
00:33:17,040 --> 00:33:24,240
occludes some parts or reveals some other part, and that's important. If I have two frames, it's

274
00:33:24,240 --> 00:33:31,440
not like every pixel is going to be visible in both images, right. This is called as occlusion,

275
00:33:31,440 --> 00:33:41,840
and occlusion map, we want our network to not, you know, handicapped by the occlusion areas. So

276
00:33:41,840 --> 00:33:48,560
if we detect such occlusion areas, automatically, and if we manage them, automatically again,

277
00:33:48,560 --> 00:33:55,200
in the network, maybe as a separate, you know, channel estimating those occlusion maps, it will,

278
00:33:55,920 --> 00:34:02,080
overall, it will benefit during training and in the inference time also explicitly by estimating

279
00:34:02,080 --> 00:34:10,320
such occlusion maps. So this paper does all of these things that I mentioned. And just to elaborate on

280
00:34:10,320 --> 00:34:15,360
what you just said, it sounds like what you're trying to do is not necessarily

281
00:34:15,360 --> 00:34:24,160
teach the network to predict occlusion or anything like that for its own benefit, but rather,

282
00:34:24,160 --> 00:34:29,920
you're trying to teach the network to identify when there is occlusion. So it doesn't take

283
00:34:31,600 --> 00:34:37,360
the ground truth that it's creating otherwise, and it does, it knows if that date is going to be

284
00:34:37,360 --> 00:34:45,600
bad because it's, you can't relate the one pixel to the next. Absolutely. Occlusion masks are not

285
00:34:45,600 --> 00:34:54,320
available. I mean, we synthesize them. This is a sub supervision part, and also these transformations,

286
00:34:54,320 --> 00:35:02,640
we define them, and it will be, it is a large set of transformations, and we then apply all these

287
00:35:02,640 --> 00:35:09,840
training, you know, improvements, enhancements, novelties to a network, which is

288
00:35:11,680 --> 00:35:20,560
one of the state-of-the-art models for optical flow motion computation. It creates these cost volumes

289
00:35:20,560 --> 00:35:27,520
and in different scales, and then it starts with a previous optical flow or just random, you know,

290
00:35:27,520 --> 00:35:35,200
initialize optical flow, and every time it trace itself at these iterations, the optical flow,

291
00:35:35,200 --> 00:35:41,040
estimates optical flow becomes more and more refined, more accurate and especially higher resolution.

292
00:35:41,040 --> 00:35:49,360
So this is what we do, then we, for instance, take these ideas, training ideas, self-provised,

293
00:35:49,360 --> 00:35:54,880
unsupervised training ideas, and modify the network such that now it can also do occlusion,

294
00:35:54,880 --> 00:36:01,280
reasoning, and kind of train it in this manner, with the existing data sets, you know,

295
00:36:01,280 --> 00:36:07,760
still simple data sets, even on those data sets, it improves the performance.

296
00:36:09,040 --> 00:36:13,760
And there is a benchmark, there are multiple benchmarks, actually one is called as Kitty,

297
00:36:13,760 --> 00:36:20,960
the other one is Sintal. In both those benchmarks, our solution was when we submitted,

298
00:36:20,960 --> 00:36:29,840
and later also, because it's an open benchmark, it was ranking on the top of the data board,

299
00:36:29,840 --> 00:36:38,240
and if I'm not mistaken, there are more than 200 solutions, 100 of them is somehow AI-based,

300
00:36:38,240 --> 00:36:48,160
deep learning-based. So that was quite good news. We weren't expecting, but we were confident,

301
00:36:48,160 --> 00:36:53,520
this is the right solution to do, and yeah, it went to the top of the leader board with the

302
00:36:53,520 --> 00:37:01,840
solution. What's the future direction for this particular approach? Very good question, Sam.

303
00:37:02,640 --> 00:37:11,920
This solution, since it requires big cost volumes, and iterations, they are computationally

304
00:37:11,920 --> 00:37:18,400
expensive, and they require a lot of memory because of the cost volumes, and because of

305
00:37:18,400 --> 00:37:26,480
iterations, they are slow. So what we are working on now, and you know, kind of we will have a demo

306
00:37:26,480 --> 00:37:34,720
very soon, now we show that the same solution could run on a mobile phone, on a call-com platform,

307
00:37:34,720 --> 00:37:45,360
in real time, for a large input size, input image size, large video size. So this has been

308
00:37:45,360 --> 00:37:53,600
quite an exactment for us. We put a lot of effort to make it more efficient. Yeah, this is our

309
00:37:54,480 --> 00:38:00,000
current work, and we also want to extend it to other things. You can do this type of things

310
00:38:00,000 --> 00:38:07,760
for one camera, and this is one video, right, on the same camera, but you can do optical flow,

311
00:38:07,760 --> 00:38:15,840
which is called a scene flow, across multiple cameras, and you will find 3D motion, not 2D motion.

312
00:38:15,840 --> 00:38:22,960
So we are extending to that, and optical flow is core for many other, you know, perception

313
00:38:22,960 --> 00:38:29,760
test, and higher level understanding. We are now plugging this solution into different pipelines to

314
00:38:29,760 --> 00:38:35,200
see how much improvement we would get. Super solution is one of them, for instance. Yeah.

315
00:38:36,800 --> 00:38:43,840
Okay. Well, there's one more paper that you have at CVPR, and we wanted to make sure to touch

316
00:38:43,840 --> 00:38:50,560
on that one as well. The next one is dense vision transformers for single image inverse rendering,

317
00:38:50,560 --> 00:38:55,840
and newer scenes. This particular one is focused on inverse rendering. What's that problem?

318
00:38:55,840 --> 00:39:04,400
So usually, when we synthesize a scene, we know about 3D, we know about the objects, like there's a

319
00:39:04,400 --> 00:39:10,480
catch, there's a chair, there's a ball, and we know the color of those. We also know their properties,

320
00:39:11,280 --> 00:39:18,400
reflectance properties. We want to generate a scene, computer graphics. So we know the location

321
00:39:18,400 --> 00:39:25,680
of the light. We know many things about the scene, like surface, normals, albeda, roughness,

322
00:39:25,680 --> 00:39:33,600
you know, you can imagine. So it will look real time, real life. So inverse rendering, so this is

323
00:39:33,600 --> 00:39:39,600
rendering, this synthesizing that I mentioned, inverse rendering starts on the other end, it takes

324
00:39:39,600 --> 00:39:46,800
an image, natural image, and then it tries to find these components, for instance, the lighting,

325
00:39:46,800 --> 00:39:56,560
location, lighting direction, lighting intensity, room shape, I mean, for indoors, and the properties

326
00:39:56,560 --> 00:40:03,680
of the everything, objects, all the objects in the scene, their shape, their color, their

327
00:40:03,680 --> 00:40:09,360
materials, whether it's leather or, you know, metal or wood or cloth, those type of things. So

328
00:40:09,360 --> 00:40:18,160
inverse rendering takes an image, real image, and kind of finds these components. Each one of them

329
00:40:18,160 --> 00:40:23,520
you can think that is either an image, you know, a reflectance image, a color image, albedo image,

330
00:40:24,560 --> 00:40:31,040
or a 3D model, you know, or lighting location, you know, the heat maps and those type of things.

331
00:40:31,040 --> 00:40:39,680
This is the inverse rendering pipeline, and these people use a transformer based idea to accomplish

332
00:40:39,680 --> 00:40:46,080
that. And so how is inverse rendering previously done when you're not using transformers?

333
00:40:46,080 --> 00:40:52,640
Previously, but recently also, I shall say, because deep learning based solutions for inverse

334
00:40:52,640 --> 00:41:02,240
rendering are not that old either, maybe at most, a couple of years, but people did, okay,

335
00:41:02,240 --> 00:41:09,760
here is the input image, and I know 3D model, because I generated that input image, and now I'm going

336
00:41:09,760 --> 00:41:16,480
to switch the order, I will give input image and try to estimate the 3D shape, and I know also

337
00:41:16,480 --> 00:41:21,440
the surface normals, and I know the color, you know, I know the color of the objects, I know the

338
00:41:21,440 --> 00:41:26,320
locations of the objects, I know the reflectivity of the objects, I know the lighting location.

339
00:41:26,320 --> 00:41:34,080
So, kind of this is done in a supervised manner, and separately. I think kind of the

340
00:41:34,080 --> 00:41:42,800
keyword here is these type of things done separately, and there has not been any attempt to

341
00:41:42,800 --> 00:41:51,920
learn for each test, and across multiple tests, where to focus when we are doing this type of

342
00:41:53,200 --> 00:41:59,600
inverse rendering. If there is an image, if I want to, let's say, generate the lighting location

343
00:41:59,600 --> 00:42:07,120
and lighting direction, which part of the scene image provides the right information that has

344
00:42:07,120 --> 00:42:15,360
not been done before. And so, this particular approach in using the transformer, again,

345
00:42:15,360 --> 00:42:21,760
you referenced this idea of the transformer's ability to attend to the right, and the most

346
00:42:21,760 --> 00:42:27,440
important parts of the image. That's a big part of what's making this work. Yes, transformers

347
00:42:27,440 --> 00:42:35,280
does that, in this case, we incorporated such self-attention or cross-attention mechanisms into

348
00:42:36,320 --> 00:42:44,320
our work, into inverse rendering to improve the accuracy of each of these inverse rendering

349
00:42:44,320 --> 00:42:50,800
tests, and also lighting estimation. So, when we have that, when we decompose an image into

350
00:42:50,800 --> 00:42:57,440
this type of components that we factorize it, then we can, for instance, put anything in the scene,

351
00:42:57,440 --> 00:43:02,400
you know, the lighting location, you know, how it reflects from other objects in the scene,

352
00:43:03,920 --> 00:43:10,240
and it would look more realistic, more natural, much more natural. So,

353
00:43:11,840 --> 00:43:18,560
and for instance, one application will be here is an input image, and then we put a completely

354
00:43:18,560 --> 00:43:24,320
virtual objects in a way that, you know, all the shadows are correct. It's very difficult to,

355
00:43:24,320 --> 00:43:31,920
you know, distinguish what we inserted, edited in the image than any other things already exist in

356
00:43:31,920 --> 00:43:37,920
the image. Yeah, I'm so excited about that work. That's what I forgot, but I should have said it.

357
00:43:37,920 --> 00:43:45,920
For instance, we have an image of a real scene, a house, let's say, and walls have a specific color

358
00:43:45,920 --> 00:43:52,720
set. Now, I can change the wall color, or I can change, for instance, there's the catch here,

359
00:43:52,720 --> 00:44:00,240
I can make the leather catch to a cloth, you know, some different fabric. So, we can really modify

360
00:44:00,240 --> 00:44:06,400
the scenes in a very realistic manner. Right, in a way that preserves that realism without it

361
00:44:08,080 --> 00:44:15,440
falling apart. And so, what were the, I'm imagining challenges here, again, you know, with

362
00:44:15,440 --> 00:44:19,680
using transformers or the computational intensity of the approach as one of them?

363
00:44:21,200 --> 00:44:28,720
In this case, we didn't focus on a computational kind of intensity. Yes, it's computation very heavy,

364
00:44:28,720 --> 00:44:36,240
and we planned to kind of make it also very efficient. There's no question about that, but one of

365
00:44:36,240 --> 00:44:42,800
the challenges was lighting direction estimation is not a straightforward problem, because it's

366
00:44:42,800 --> 00:44:48,560
difficult to take an image, and before knowing about the location of the light, and also 3D

367
00:44:48,560 --> 00:44:54,480
scene structure deduce about the lighting direction. I mean, you can imagine this lighting,

368
00:44:54,480 --> 00:45:00,640
there's a window, there's a sun outside the window. By the way, this window is not visible in

369
00:45:00,640 --> 00:45:06,480
the image. It is like right in front of me, there is a window, and you don't see it, right?

370
00:45:06,480 --> 00:45:12,960
In this test, our goal is to find where that window is and where the sun is, you know, not this direction,

371
00:45:12,960 --> 00:45:20,000
but this direction. So, this is kind of what we want to do, if we want to really put an object

372
00:45:20,000 --> 00:45:25,680
in a way that, you know, shadows are correct, ambient lighting is correct. So, we are imagining

373
00:45:26,400 --> 00:45:33,680
estimating the invisible things in the image. If it's visible, it's much more easier. So,

374
00:45:33,680 --> 00:45:39,760
we are estimating the remaining part of the scene room, for instance, and that cannot be done,

375
00:45:39,760 --> 00:45:45,760
you know, using just an input image directly going to there. You need to go step by step,

376
00:45:45,760 --> 00:45:51,200
first get an idea about the room layout, 3D scene structure, you know, reflectivity,

377
00:45:51,200 --> 00:45:57,920
all type of cues, then leveraging on those, and with attention mechanism transformers,

378
00:45:57,920 --> 00:46:04,800
back to believe in the previous computer information, okay, there is the light now, invisible lights.

379
00:46:04,800 --> 00:46:12,640
What was the direction of the light? What was the properties of lights? So, that was the challenging part.

380
00:46:14,160 --> 00:46:23,120
What kind of constraints are you making on the image? For example, you have a good number of

381
00:46:23,120 --> 00:46:28,880
candles behind you. Are you limiting the number of light sources that you're assuming to be in the

382
00:46:28,880 --> 00:46:34,400
image? For example, it's a good question. We don't actually, that is quite restricted to

383
00:46:34,400 --> 00:46:41,840
number of light sources. Type of light sources are maybe shape of the light sources. There are some

384
00:46:41,840 --> 00:46:48,720
assumptions for windows, you know, there are maybe some additional assumptions. It is retained

385
00:46:48,720 --> 00:46:56,160
for indoor scenes. Maybe I should mention this is specifically due to the data set that we use,

386
00:46:56,160 --> 00:47:03,280
open rooms, data set, and scan it. It is for indoor scenes, but it could be any number of lights,

387
00:47:03,280 --> 00:47:11,040
you know, kind of, yeah. Before we wrap up, Qualcomm has a number of other activities at CVPR. Let's

388
00:47:11,040 --> 00:47:18,480
briefly have you share a little bit about those. One is a workshop on wireless AI perception.

389
00:47:18,480 --> 00:47:25,840
What's that one about? Absolutely. That is the first time a wireless AI perception, wireless

390
00:47:25,840 --> 00:47:36,080
itself is becoming a workshop at CVPR. CVPR is more visual data and, you know, there has been some

391
00:47:36,080 --> 00:47:43,200
other models as well, but not at a degree of a workshop. And if you look at the field, we see that

392
00:47:43,200 --> 00:47:52,160
people are using cameras in addition to together with, let's say, Wi-Fi or, you know, 5G or

393
00:47:52,160 --> 00:47:59,440
terrorist imaging. So, for instance, there is a Wi-Fi around me right now and there is a camera

394
00:47:59,440 --> 00:48:04,400
and together they accomplish more things than just using the camera or the Wi-Fi

395
00:48:04,400 --> 00:48:13,440
separately. In this workshop, we bring the leaders in the field, they will, they are going to give

396
00:48:13,440 --> 00:48:23,840
us several keynotes, maybe 7-6 keynotes and very exacting presentations, talks about tools

397
00:48:23,840 --> 00:48:33,120
available, data sets available for this type of research. So, we bring these leaders and create a

398
00:48:33,120 --> 00:48:41,200
platform so people can discuss further improvement ideas and share information, share their

399
00:48:42,480 --> 00:48:48,240
observations. So, that is going to accelerate more research in this area.

400
00:48:48,240 --> 00:48:53,680
When you mentioned Wi-Fi in that context, I know that Qualcomm has done some research around

401
00:48:55,280 --> 00:49:01,040
using Wi-Fi signals to determine presence in a room, that kind of thing. Is that the sense in

402
00:49:01,040 --> 00:49:06,320
what you're using it or more traditionally as a communication? That we showed we can do it.

403
00:49:06,320 --> 00:49:12,880
It is not only sensing the person in the room, but some we actually know when person moves,

404
00:49:12,880 --> 00:49:18,720
where the person is less than 10 centimeter accuracy, you know, depending on the number of

405
00:49:18,720 --> 00:49:24,640
like the Wi-Fi access points, it could be even better. So, we know kind of like, and we can

406
00:49:24,640 --> 00:49:31,200
track people in these Wi-Fi or 5G environments, it could be your phone, could be track, for instance,

407
00:49:33,040 --> 00:49:41,120
using these access points. But in addition to that, we can also estimate the body pose of the people,

408
00:49:42,240 --> 00:49:48,000
for instance, people know that they actually pull down to ground and they need help or not.

409
00:49:48,000 --> 00:49:53,520
You know, those are the things that we are trying to accomplish. If there's a camera in the system,

410
00:49:53,520 --> 00:50:01,760
it would make even stronger. In this workshop, we are not really presenting our work,

411
00:50:01,760 --> 00:50:10,480
but our goal was to accelerate further research innovation in this area and support

412
00:50:11,040 --> 00:50:20,000
everyone, academia and, you know, anyone interested in virus perception, and we will release

413
00:50:20,000 --> 00:50:27,680
some data says as well. Okay, great. There's also a omnidirectional workshop. What's that one about?

414
00:50:31,520 --> 00:50:38,800
So, the previous workshop initiated at Qualcomm and kind of several Qualcomm members are

415
00:50:38,800 --> 00:50:43,360
in the organizing committee. They are sharing the event. The only directional

416
00:50:43,360 --> 00:50:50,240
computer vision workshop is another one. We have the similar setting. It is the third time of

417
00:50:50,240 --> 00:50:58,880
the third workshop, and in this edition, there are many people from all around the different

418
00:50:58,880 --> 00:51:05,920
companies, autonomous vehicle companies, and academia joining this event, showing, presenting

419
00:51:05,920 --> 00:51:14,560
ideas for a setting where cameras may not be just like our phone cameras, but could be

420
00:51:14,560 --> 00:51:21,520
fisheye cameras or 360 cameras. The only directional kind of indicates in colors,

421
00:51:22,240 --> 00:51:27,200
cameras with wider, much wider field of view. Of course, those cameras have different

422
00:51:27,200 --> 00:51:35,520
geometry and different type of images. If you look at, for instance, maybe people who are

423
00:51:35,520 --> 00:51:44,560
holding a stick 360 images, those images look like not like the pictures we take in our phones,

424
00:51:44,560 --> 00:51:54,880
right? And because of that solutions, computer vision solutions kind of has to work in that setting

425
00:51:54,880 --> 00:52:01,280
rather than, okay, I'm going to take this funny image and create kind of like regular rectangular

426
00:52:01,280 --> 00:52:07,280
version of it. Let me do that, we lose information. So there's a reason why there are dedicated

427
00:52:07,280 --> 00:52:15,280
solutions for omnidirectional cameras, and this workshop combines such recent, latest state of

428
00:52:15,280 --> 00:52:25,280
the art research work and provides a platform for people to discuss and learn from each other,

429
00:52:25,280 --> 00:52:33,440
share their experiences. One big application is autonomous vehicles. As you know, autonomous

430
00:52:33,440 --> 00:52:39,920
vehicles have multiple cameras anywhere from, you know, four, five, six, seven, including the

431
00:52:39,920 --> 00:52:48,160
internal one, external one. So you get a 360 feeling, a perception around the vehicle, and

432
00:52:48,160 --> 00:52:57,520
how you are going to make sure all these information coming from these different cameras,

433
00:52:57,520 --> 00:53:02,560
some of them are like the fisheye cameras, and there are other sensors can be combined,

434
00:53:02,560 --> 00:53:08,160
in a way that, you know, all the process optimizes works better at a higher accuracy,

435
00:53:08,160 --> 00:53:13,520
you know, more robust manner. So these are the things I think people will be discussing,

436
00:53:13,520 --> 00:53:22,480
and these are the kind of some of the talks in the workshop as well. Awesome, awesome. And your

437
00:53:22,480 --> 00:53:30,320
team also usually is showcasing some number of demos at conferences like CVPR, do you have any

438
00:53:30,320 --> 00:53:36,320
demos this year? Yeah, usually we brought, we tried to bring mini demos, this time they are

439
00:53:36,320 --> 00:53:45,280
only bringing two. There are much more demos at CVPR from Qualcomm. I mean, I'm just talking about

440
00:53:45,840 --> 00:53:54,640
kind of my team, Qualcomm AI research, perception part. And one of the demos is called

441
00:53:54,640 --> 00:54:05,440
this auxilire adaptation for semantic segmentation. The other one is 4K image super resolution. Oh, wow.

442
00:54:06,560 --> 00:54:16,000
So folks who, whether you saw that at CVPR or not, there's a blog post that we'll link to in

443
00:54:16,000 --> 00:54:21,280
the show notes and you might be able to catch those demos there. Well, Fatih, it was great chatting

444
00:54:21,280 --> 00:54:28,240
with you and congrats on so many accepted papers at the conference and looking forward to

445
00:54:28,240 --> 00:54:33,920
catching up again soon. Thank you so much, Sam. Thank you so much for having me. It was a pleasure.

446
00:54:34,480 --> 00:54:39,360
I hope, you know, sometimes I state very high level, but there are many things in the papers,

447
00:54:39,360 --> 00:54:45,600
you know, kind of I'm sure people will love the details we provide in the paper. Please let me know

448
00:54:45,600 --> 00:54:51,920
if you have any questions also later. We'll definitely link to those papers in the show notes

449
00:54:51,920 --> 00:55:21,760
and encourage folks to reach out if they have any questions.

