WEBVTT

00:00.000 --> 00:13.400
Welcome to the Tumel AI Podcast.

00:13.400 --> 00:16.400
I'm your host Sam Charrington.

00:16.400 --> 00:24.480
Hey, what's up everyone?

00:24.480 --> 00:29.920
I recently attended the AWS re-invent conference in Las Vegas, and I'm excited to share

00:29.920 --> 00:38.960
a few of my many interesting conversations from that event here on the podcast this week.

00:38.960 --> 00:44.160
Before we dive in, I'd like to thank our friends at Capital One for sponsoring our re-invent

00:44.160 --> 00:45.760
series.

00:45.760 --> 00:50.160
Capital One has been a huge friend and supporter of this podcast for some time now, and

00:50.160 --> 00:54.600
I'm looking forward to sharing my interview with Dave Castillo, Capital One's managing

00:54.600 --> 00:58.640
VP of Machine Learning with you on Thursday.

00:58.640 --> 01:02.400
Dave and I discussed the unique approach being taken at the company's Center for Machine

01:02.400 --> 01:07.480
Learning, as well as some of the interesting AI use cases being developed at the bank,

01:07.480 --> 01:11.880
and the platform they're building to support their ML and AI efforts.

01:11.880 --> 01:20.960
To learn more about Capital One's Machine Learning and AI efforts and research, visit capitalone.com-slash-tech-slash-explore.

01:20.960 --> 01:27.960
And now on to the show.

01:27.960 --> 01:28.960
All right, everyone.

01:28.960 --> 01:33.640
I am here at AWS re-invent in Las Vegas, and I am with Fung Yen.

01:33.640 --> 01:38.440
Fung is an assistant professor at the University of Nevada, Reno.

01:38.440 --> 01:41.280
Fung, welcome to the Tumel AI podcast.

01:41.280 --> 01:42.520
Thank you.

01:42.520 --> 01:49.320
So you just did a talk here on one of your research projects, which is focused on,

01:49.320 --> 01:56.280
or it's called alert wildfire, and it's focused on detecting wildfire smoke using machine

01:56.280 --> 01:58.280
learning.

01:58.280 --> 02:03.240
We're going to dive deep into that, but before we do, how did you get started working on

02:03.240 --> 02:04.240
this problem?

02:04.240 --> 02:11.760
What's your background, and how did your interest align with this particular problem?

02:11.760 --> 02:16.640
So my background actually, you know, I see machine learning and the computer system,

02:16.640 --> 02:20.560
so I thought it was going to cross this plane between these two areas.

02:20.560 --> 02:27.600
So actually, when I'm doing my PhD, I actually have like a more computer system background,

02:27.600 --> 02:33.600
so we kind of build a, you know, efficient systems for data centers for, you know, cloud.

02:33.600 --> 02:39.680
And then, you know, I had an internship in Microsoft research, and that's where I actually

02:39.680 --> 02:41.640
started working on machine learning.

02:41.640 --> 02:47.120
So I still remember like back then, you know, we had like machine learning system.

02:47.120 --> 02:54.920
At that time, we didn't have tens of loads, and we had a system called Adam.

02:54.920 --> 03:00.880
So that's actually, you know, the state of the ad system back then, and you know, I'm

03:00.880 --> 03:07.240
extremely lucky to have the opportunity to work with the group to work on this system.

03:07.240 --> 03:12.240
So that's where I started actually working on the machine learning, and it's actually

03:12.240 --> 03:15.680
back in 2014.

03:15.680 --> 03:21.240
So it's actually, you know, before, you know, even the AI machine learning, you know,

03:21.240 --> 03:22.920
becomes really hot and hot.

03:22.920 --> 03:23.920
It's current wave.

03:23.920 --> 03:24.920
Right.

03:24.920 --> 03:25.920
Before I treat the wave.

03:25.920 --> 03:26.920
Yeah.

03:26.920 --> 03:35.680
So, and then later I joined the University of Nevada Reno, and become faculty there.

03:35.680 --> 03:37.600
One like a special thing, right?

03:37.600 --> 03:43.280
Because I used to be in the East Coast after I moved to the West Coast, we do say, right?

03:43.280 --> 03:48.480
Like one specific, like, situation here is about the White Files.

03:48.480 --> 03:54.240
They actually, Nevada and California, they have a lot of White Files, which, you know,

03:54.240 --> 04:00.080
significantly impacts the air quality and the people's life, because, you know, the smoke

04:00.080 --> 04:04.400
contains a lot of particles that's bad for your hairs.

04:04.400 --> 04:10.280
And also, it will, you know, create these, like, low visibility situation that's, you

04:10.280 --> 04:12.680
know, dangerous for a lot of things.

04:12.680 --> 04:20.280
And it also creates this dry condition, and which will lead to even more White Files.

04:20.280 --> 04:21.280
Okay.

04:21.280 --> 04:26.440
So that's why, you know, I started to get kind of curious about, right?

04:26.440 --> 04:32.120
How, whether there's any efforts has been made into kind of monitoring, control, you

04:32.120 --> 04:36.160
know, the White Files and also the White Files smoke and air quality.

04:36.160 --> 04:47.160
So, and, like, very likely, like, Yonger has a team, a lead by Dr. Ken Graham Ken, and

04:47.160 --> 04:54.360
Ken Smith's daily seismology lab there, and they actually had an effort to build like

04:54.360 --> 04:58.960
a camera system to help, you know, to monitor the White Files.

04:58.960 --> 05:04.560
However, current stage, they are basically, you know, doing things manually.

05:04.560 --> 05:08.600
So from, like, computer science, you know, perspective, I'm thinking, right?

05:08.600 --> 05:13.960
So with my background in machine learning and the cloud computing, I'm wondering, with

05:13.960 --> 05:17.800
her, you know, I can help them to build, like, a more intelligent system.

05:17.800 --> 05:22.520
So that's, you know, where are we actually started to think about this project.

05:22.520 --> 05:30.160
And later, we worked together and actually got, like, an SF and AWS cost months or the

05:30.160 --> 05:34.080
big data, you know, grant to work on this project.

05:34.080 --> 05:38.840
So, you know, that's how I actually get into, you know, this topic.

05:38.840 --> 05:44.960
And so, yeah, and that's very fortunate that you walked into this environment where they've

05:44.960 --> 05:50.120
already got these cameras deployed to try to solve this problem, but no automation system

05:50.120 --> 05:51.120
in place.

05:51.120 --> 05:52.120
Right, right.

05:52.120 --> 05:57.360
How many cameras have been put in place to monitor the, the wildfire situation?

05:57.360 --> 06:02.240
Right now, we have, like, more than 150 cameras already deployed, and then there are hundreds

06:02.240 --> 06:05.520
more, you know, it's going to be deployed as soon.

06:05.520 --> 06:10.760
This is the camera system that's specifically built for monitoring wildfire.

06:10.760 --> 06:17.440
And the technology is way developed to actually, you know, use machine learning, age computing,

06:17.440 --> 06:23.280
cloud computing to help, you know, the monitoring accurate can be extended to, you know, the

06:23.280 --> 06:28.960
general camera system, we then have to be constrained by, like, a specific, you know,

06:28.960 --> 06:34.160
network, because right now, there's a lot of camera system, right, the use for traffic

06:34.160 --> 06:35.160
and that's the purpose.

06:35.160 --> 06:37.200
They're already being there, right.

06:37.200 --> 06:41.680
That's a technology can actually can be extended to other systems if needed.

06:41.680 --> 06:42.680
Okay.

06:42.680 --> 06:45.680
And how are the cameras deployed?

06:45.680 --> 06:53.080
Are they in a grid, like, in forested areas, or are they in more populated areas, but,

06:53.080 --> 06:55.760
you know, facing edges of forested areas?

06:55.760 --> 07:00.840
What's the rationale or methodology behind the camera system?

07:00.840 --> 07:06.800
The camera systems, they basically based on, you know, the areas that are more like prone

07:06.800 --> 07:07.800
to fires.

07:07.800 --> 07:08.800
Okay.

07:08.800 --> 07:09.800
Right.

07:09.800 --> 07:13.320
And of course, the specific actually technology about the camera itself is actually, you

07:13.320 --> 07:17.520
know, lead by, you know, it's a group of the system, only your lab.

07:17.520 --> 07:20.360
So we are more kind of on the software side.

07:20.360 --> 07:28.560
And so when I think about, you know, monitoring for wildfires and images, one of the immediate

07:28.560 --> 07:30.480
thoughts is, like, satellite imagery.

07:30.480 --> 07:31.480
Oh, yes.

07:31.480 --> 07:33.320
That's a very good question.

07:33.320 --> 07:38.040
Conventionally, you know, people are using satellite imaging, as well as mediaology to,

07:38.040 --> 07:45.400
you know, to do the wide-file smoke for casting and also the air quality prediction for

07:45.400 --> 07:46.400
casting.

07:46.400 --> 07:47.400
Okay.

07:47.400 --> 07:52.680
So the limitation of this conventional technique is the resolution.

07:52.680 --> 07:57.360
So actually, wide-file smoke, they can transfer accurate very fast.

07:57.360 --> 08:02.160
Like we had like a video, you know, this morning to show to the audience that, you know,

08:02.160 --> 08:08.000
for example, for like a major area of arena, like the whole major area has close to 1 million

08:08.000 --> 08:09.000
people.

08:09.000 --> 08:13.520
So it's not as big as Vegas, but still quite a big area.

08:13.520 --> 08:23.520
So actually, in a windy day, the smoke actually can spread over the entire city in just 22-30

08:23.520 --> 08:24.520
minutes.

08:24.520 --> 08:25.520
So, yeah.

08:25.520 --> 08:34.280
So for the conventional methodology, their data is very coarse-grand, for example, right?

08:34.280 --> 08:38.320
In terms of both time dimension and also the spatial dimension.

08:38.320 --> 08:44.400
For example, for the time dimension, actually, the data, right, no matter satellite image,

08:44.400 --> 08:50.600
data or media origin data, they refresh like every a few hours or even feel, you know,

08:50.600 --> 08:52.320
like at the day's level, right?

08:52.320 --> 08:53.320
Right.

08:53.320 --> 08:55.320
Because you've got to wait for a satellite to be overhead.

08:55.320 --> 08:56.320
Right.

08:56.320 --> 08:57.320
Right.

08:57.320 --> 09:04.640
So, and another thing about the spatial dimension, right, they basically have like, for example,

09:04.640 --> 09:08.880
the typical resolution is like 10 by 10 kilometers.

09:08.880 --> 09:14.640
So this is like pretty big, right, in terms of, you know, the forecasting.

09:14.640 --> 09:20.320
And think about, right, if we do want to like accurate forecasting, for example, for

09:20.320 --> 09:24.160
an area of a city or like a neighborhood, right?

09:24.160 --> 09:30.960
So we do need a much finer grand way to track the smoke and also to the air quality prediction.

09:30.960 --> 09:35.920
So that's why, right, we are thinking about, you know, using the camera data, right?

09:35.920 --> 09:38.640
Because camera data is very fragrant, right?

09:38.640 --> 09:44.160
You can, you know, capture the image like a dozen of them per second, right?

09:44.160 --> 09:50.440
And you can basically have infinite resolution as well as you have enough cameras, right?

09:50.440 --> 09:56.920
We thought to use like a hybrid approach to combine both the conventional data, like satellite

09:56.920 --> 10:03.600
imaging and as well as the media-oriented data together with this camera data.

10:03.600 --> 10:08.480
So that, you know, we can have like a much finer grand data.

10:08.480 --> 10:13.560
And another important thing about the traditional approach is, for example, for the satellite

10:13.560 --> 10:18.000
data, since it's shooting from the app, right?

10:18.000 --> 10:22.520
Actually like cloud and other situations can block the view, right?

10:22.520 --> 10:25.640
So that you will have lots of missing data.

10:25.640 --> 10:31.120
And while camera is actually on the ground, so it basically have like, you know, like a

10:31.120 --> 10:32.720
better view, right?

10:32.720 --> 10:36.520
But of course, problem with camera data is, right?

10:36.520 --> 10:41.160
They only have local, you know, views, they didn't have like a very good global, you

10:41.160 --> 10:42.480
know, view.

10:42.480 --> 10:45.960
So that's why it's kind of complementary to each other, right?

10:45.960 --> 10:47.920
That's why we need both of them.

10:47.920 --> 10:52.800
Your collaborators that deployed the camera system, did you say they're with the Department

10:52.800 --> 10:53.800
of SizeMology?

10:53.800 --> 10:57.720
Like, are they originally trying to detect earthquakes with the cameras or?

10:57.720 --> 11:04.160
Actually not, like, they, you know, I think they just had this idea, right?

11:04.160 --> 11:11.440
Because, you know, in Reno, we have a lot of, you know, wide file and smoke like situation.

11:11.440 --> 11:17.920
Like, by right now, they sort of just deliver the camera data to the, for example, the file

11:17.920 --> 11:18.920
fighters, right?

11:18.920 --> 11:25.280
They can manually check whether this area has a fire, and if so, they can use it to monitor

11:25.280 --> 11:26.280
it.

11:26.280 --> 11:32.240
But it doesn't have any kind of intelligent way to automatically alert when there's a

11:32.240 --> 11:37.360
fire or smoke, and it doesn't actually track, you know, it's a fire or smoke, right?

11:37.360 --> 11:38.760
Automatically.

11:38.760 --> 11:48.280
So you've got this data coming off of these cameras, 150 today, sounds like 10 frames

11:48.280 --> 11:49.280
a second.

11:49.280 --> 11:50.280
You were saying?

11:50.280 --> 11:51.280
10 frames a minute.

11:51.280 --> 11:52.280
I remember 10.

11:52.280 --> 11:53.280
Right.

11:53.280 --> 11:55.520
This is actually a parameter you can control, right?

11:55.520 --> 11:56.520
Sure.

11:56.520 --> 12:03.520
You can, you know, make it like one frame per second, or even one frame per minute, right?

12:03.520 --> 12:07.960
But you can also add to like maybe 30 frames per second, right?

12:07.960 --> 12:08.960
Yeah.

12:08.960 --> 12:11.280
So that's why, you know, you have a lot of data actually.

12:11.280 --> 12:12.760
It's significantly getting more.

12:12.760 --> 12:13.760
Exactly.

12:13.760 --> 12:14.760
Yeah.

12:14.760 --> 12:22.000
The scale is so much bigger than conventional satellite data or the media-oriented data.

12:22.000 --> 12:25.280
And so that's actually our focus, right?

12:25.280 --> 12:28.120
So think about in practice, right?

12:28.120 --> 12:30.480
Even though you have these cameras really, right?

12:30.480 --> 12:36.800
You can, you know, have a lot of data coming from the sensors, and then you also have

12:36.800 --> 12:42.600
the models, right, for conventional statistical models, as well as the new kind of machine

12:42.600 --> 12:44.640
or any different models, right?

12:44.640 --> 12:52.120
But between the data and the model, right, you actually need to collect and deliver the

12:52.120 --> 12:55.240
data to the model, right, in a timely way, right?

12:55.240 --> 12:56.240
Yeah.

12:56.240 --> 13:02.240
So that's why, you know, one of our efforts is how can we do some pre-processing at the

13:02.240 --> 13:10.160
age side, and then, you know, to make the communication or transferring the data more

13:10.160 --> 13:13.320
efficiently to the cloud.

13:13.320 --> 13:18.880
So that's why we have this age and the cloud working together to actually deliver the

13:18.880 --> 13:21.120
data in a timely way.

13:21.120 --> 13:27.120
And another important thing is when you have this massive amount of data, right?

13:27.120 --> 13:31.200
You do need to find a efficient way to process it, right?

13:31.200 --> 13:38.440
Because if your prediction detection is too slow, then basically if the file or the

13:38.440 --> 13:40.920
smoke is already passed by region, right?

13:40.920 --> 13:41.920
Right.

13:41.920 --> 13:42.920
Then it doesn't matter, right?

13:42.920 --> 13:48.320
So you have to do this, you know, fast enough to make it proactive, right?

13:48.320 --> 13:55.000
So that's why, you know, we, a lot of effort also being spent on how can we do like since

13:55.000 --> 14:02.400
in like low latency, give good scalability, and as well as more efficient, right?

14:02.400 --> 14:08.200
Because whenever you talk about this massive amount of data, and we're complicated

14:08.200 --> 14:10.320
machining the algorithms, right?

14:10.320 --> 14:13.880
So they basically consume a lot of computing resources, right?

14:13.880 --> 14:20.360
A lot of energy, a lot of computing resource, that's actually at this scale, that's matter,

14:20.360 --> 14:21.360
right?

14:21.360 --> 14:29.720
So that's why, you know, we consider both latency, scalability, and efficiency into consideration

14:29.720 --> 14:32.280
for designing our system.

14:32.280 --> 14:38.960
This may be dig into the model itself, and the process you took to develop the model.

14:38.960 --> 14:43.680
Sounds like the inputs are these images, or there are other features that you're feeding

14:43.680 --> 14:48.520
into the model, and what is the model ultimately trying to do?

14:48.520 --> 14:51.480
Yeah, so yeah, this is just between us.

14:51.480 --> 14:56.320
So actually, you know, since this project starts from the beginning of this year, so we

14:56.320 --> 14:58.720
are still at quite early stage.

14:58.720 --> 15:06.640
So some of our efforts in the, you know, models and the training pathway is still ongoing.

15:06.640 --> 15:12.760
So that way, it's, you know, not yet to show, because some of them are submitted for

15:12.760 --> 15:13.760
publication.

15:13.760 --> 15:16.560
Some of them are still, you know, you know, okay.

15:16.560 --> 15:24.160
So actually, to talk this morning, of course, we showed some preliminary results about how

15:24.160 --> 15:31.440
can we use machine learning models to actually detect the smoke, and I can share a little

15:31.440 --> 15:32.440
bit about that.

15:32.440 --> 15:33.440
Okay.

15:33.440 --> 15:42.400
And then actually another thing we have already accomplished at this stage is, after you

15:42.400 --> 15:47.520
have the model, right, you do need to do the inference or classification or detection,

15:47.520 --> 15:48.520
right?

15:48.520 --> 15:51.080
So, so this is also like a big thing, right?

15:51.080 --> 15:53.720
You need to do this in like a real time, right?

15:53.720 --> 15:54.720
Yeah.

15:54.720 --> 15:55.720
I can't talk a little bit more about this.

15:55.720 --> 15:56.720
Okay.

15:56.720 --> 15:57.720
Well, okay.

15:57.720 --> 16:02.960
So maybe let's start with the results that you presented this morning.

16:02.960 --> 16:03.960
Okay.

16:03.960 --> 16:09.960
Maybe a good place to start there is the, how are you formulating the problem that you're

16:09.960 --> 16:10.960
trying to solve?

16:10.960 --> 16:15.520
Is it a classification problem that you're trying to express it as smoke or not smoke?

16:15.520 --> 16:19.200
Or are you trying to determine how much smoke there is?

16:19.200 --> 16:23.520
Are you trying to determine the, you know, predict the future likelihood of smoke?

16:23.520 --> 16:25.520
How do you structure the problem?

16:25.520 --> 16:26.520
Okay.

16:26.520 --> 16:34.480
So, for the problem, first of all, right, for the processing the camera data is actually

16:34.480 --> 16:36.320
our focus of this project.

16:36.320 --> 16:37.320
Okay.

16:37.320 --> 16:42.080
So, of course, we do have a lot of fancy machine learning models right now, right?

16:42.080 --> 16:47.280
However, you know, these are more kind of towards like this benchmarking, right?

16:47.280 --> 16:50.280
So the data is more or less defined, right?

16:50.280 --> 16:55.200
So one big challenge of detecting smoke is think about it.

16:55.200 --> 16:59.520
The smoke is actually has like a various ship, right?

16:59.520 --> 17:04.240
I would say, right, in this world, maybe no two smoke looks the same.

17:04.240 --> 17:05.240
Right.

17:05.240 --> 17:06.240
Right.

17:06.240 --> 17:10.200
So, in other words, like you've got all these, you know, we've got, you know, these models

17:10.200 --> 17:14.360
like ResNet and other things that are, you know, trained on things like ImageNet where

17:14.360 --> 17:18.840
you have these very well-defined objects in the scene.

17:18.840 --> 17:24.320
And I guess they also have kind of technical parameters like, you know, they're 224, you

17:24.320 --> 17:25.320
know, dimension images.

17:25.320 --> 17:32.360
And you've got this stuff coming off of a camera, you know, so real images, you know, of

17:32.360 --> 17:36.280
things that for, you know, most of the time you see them in a picture aren't even really

17:36.280 --> 17:37.280
there.

17:37.280 --> 17:38.280
Right.

17:38.280 --> 17:39.280
Right.

17:39.280 --> 17:40.280
Exactly.

17:40.280 --> 17:43.080
So, actually, there are two challenges, right?

17:43.080 --> 17:45.320
One is about the environment, right?

17:45.320 --> 17:50.000
Thinking about in a real situation, you have different light conditions, right?

17:50.000 --> 17:54.040
And then you also have a various background, right?

17:54.040 --> 17:58.240
For example, in like a mountain area, right, when there's cloud, right?

17:58.240 --> 18:02.280
And basically, the cloud and the smoke can look very identical, right?

18:02.280 --> 18:03.280
Yeah.

18:03.280 --> 18:07.080
Even, you know, humans are difficult to tell the difference, right?

18:07.080 --> 18:12.280
And also, the smoke itself, right?

18:12.280 --> 18:17.280
First of all, it has various shifts, and the second is actually changing, right?

18:17.280 --> 18:19.640
The shift is changing over time, right?

18:19.640 --> 18:25.480
So that's, you know, the two challenges when you, you know, want to classify this, right?

18:25.480 --> 18:29.320
And so what has been your approach for overcoming those challenges?

18:29.320 --> 18:30.320
Yeah.

18:30.320 --> 18:34.440
So, yeah, actually, you know, we are at this stage, right?

18:34.440 --> 18:40.480
We tried some, you know, standard kind of models and we did transfer learning.

18:40.480 --> 18:46.560
And for some, like, either, like, things we actually can, you know, do already very well,

18:46.560 --> 18:52.280
you know, with existing techniques, better for some more kind of challenge, right?

18:52.280 --> 18:59.000
Background, or some, like, faster changing, you know, smoke, so we are still trying to

18:59.000 --> 19:01.200
find a way to solve it.

19:01.200 --> 19:03.560
It's still like ongoing project.

19:03.560 --> 19:04.560
Okay.

19:04.560 --> 19:05.560
Okay.

19:05.560 --> 19:10.040
It sounds like at this stage, the model formulation or the problem formulation would probably

19:10.040 --> 19:15.040
be something on the simpler side, like, just a classifier, smoke or no smoke.

19:15.040 --> 19:16.040
Yeah.

19:16.040 --> 19:19.600
So, for smoke and no smoke, right?

19:19.600 --> 19:25.680
We actually have already done some, you know, preliminary testing, and it's actually

19:25.680 --> 19:30.200
can work very well, except some, like, really challenging, you know, since, yeah.

19:30.200 --> 19:32.560
And our eventual goal is, right?

19:32.560 --> 19:39.880
So we can actually not only identify with smoke or no smoke, but also we can identify

19:39.880 --> 19:41.520
the density of the smoke, right?

19:41.520 --> 19:42.520
Okay.

19:42.520 --> 19:48.320
So, and then use that to map to, like, air quality, you know, measure.

19:48.320 --> 19:53.720
So that we can directly tell from the image about the air quality, basically.

19:53.720 --> 19:54.720
Okay.

19:54.720 --> 19:55.720
Yeah.

19:55.720 --> 20:03.920
And another big challenge when you want to do the machine learning in this classification

20:03.920 --> 20:07.840
task is you don't have the label data, right?

20:07.840 --> 20:17.040
So, so currently, we are, you know, still trying to use some supervised ways to train the

20:17.040 --> 20:22.880
model and to do the classification, but eventually, we do want to, you know, utilize, you

20:22.880 --> 20:30.000
know, semi-supervised learning and on-supervised learning, so that way, you know, use the camera

20:30.000 --> 20:31.480
data directly, right?

20:31.480 --> 20:36.960
So data to help building models and improve the models.

20:36.960 --> 20:43.120
So you've got some preliminary models that you've developed that can start to differentiate

20:43.120 --> 20:49.120
between smoke and non-smoke and you're continuing to work in this area so that you can do things

20:49.120 --> 20:53.800
like identify density patterns and things like that.

20:53.800 --> 21:00.040
But the main focus of the work that you've done thus far and if published on is, it sounds

21:00.040 --> 21:06.040
like on the inference side and maybe some of the characteristics related to the edge nature

21:06.040 --> 21:12.160
of the camera deployment, are you taking advantage of, actually, it is more of a question

21:12.160 --> 21:13.160
than the statement?

21:13.160 --> 21:16.440
Are you doing, like, inference at the edge?

21:16.440 --> 21:20.360
Is that part of what you're, what you're trying to build towards?

21:20.360 --> 21:21.360
Oh, yes.

21:21.360 --> 21:24.120
So, for the edge pattern, it's still ongoing.

21:24.120 --> 21:29.480
So what we have done so far is about how you can do efficient inference as a cloud

21:29.480 --> 21:30.480
side.

21:30.480 --> 21:31.480
On the cloud side.

21:31.480 --> 21:32.480
Got it.

21:32.480 --> 21:41.840
So on the cloud side, if you think about it, about a camera network system, when you

21:41.840 --> 21:48.440
do this tracking and prediction, basically, there will be cameras joining the network and

21:48.440 --> 21:55.360
leave the network because of smoke spreading to a new area as well as the network condition

21:55.360 --> 21:57.640
is very unstable.

21:57.640 --> 22:04.160
So basically, you have this kind of dynamic workload, because the number of cameras and

22:04.160 --> 22:11.640
number of requests send it to the cloud for classification and detection is actually

22:11.640 --> 22:14.280
changing over the time.

22:14.280 --> 22:22.880
And especially, for example, when the smoke moving to a metro area, we do need to pay more

22:22.880 --> 22:27.600
attention, which means we may need a final grand prediction.

22:27.600 --> 22:28.600
Right?

22:28.600 --> 22:35.880
Because the smoke can transport over to a neighborhood in a second level.

22:35.880 --> 22:42.440
So that's why, usually, when the smoke is approaching to a metro area, you will have

22:42.440 --> 22:49.560
suddenly a very high number of requests you need to process, because one thing is you

22:49.560 --> 22:53.560
have a lot of cameras in metro areas.

22:53.560 --> 23:03.600
The second is, it's very important to detect and to classify and detect things timely

23:03.600 --> 23:11.120
in a metro area, because there are denser populations of people.

23:11.120 --> 23:18.800
So in this case, when you have this kind of suddenly increased demand for inference,

23:18.800 --> 23:25.800
then you do need to have a good way to scale in the cloud.

23:25.800 --> 23:34.560
So we have a joint work with Hong Kong University of Science and Technology, where we did

23:34.560 --> 23:37.280
like auto scaling work.

23:37.280 --> 23:46.240
So right now, Amazon do have something called SageMaker that can automatically scale

23:46.240 --> 23:53.560
your number of instances to a high level when it observes increased load.

23:53.560 --> 23:58.320
However, this is a very coarse grand approach.

23:58.320 --> 24:06.000
First of all, it's a feedback approach, so it observes and then acts, which means it

24:06.000 --> 24:13.200
takes quite a while between it observes the high load and its actually scales.

24:13.200 --> 24:17.160
Usually, this is at a few minutes' level right now.

24:17.160 --> 24:25.120
However, if you want to do this real-time fine-grained monitoring of smoke, then basically you

24:25.120 --> 24:28.040
need to do things at a second level.

24:28.040 --> 24:37.080
So that's why we created a new system called Mark that can have much quicker scalability

24:37.080 --> 24:39.160
than SageMaker.

24:39.160 --> 24:41.520
So the key idea here is, right?

24:41.520 --> 24:46.880
So SageMaker is a sort of feedback way of doing things, right?

24:46.880 --> 24:49.880
It's observed when they interact, right?

24:49.880 --> 24:53.240
And there's another way you can do it over provisioning, right?

24:53.240 --> 24:58.400
So basically, I predict there might be like a high load, right?

24:58.400 --> 25:03.640
And then I increase the resources in advance, right?

25:03.640 --> 25:04.640
And of course, right?

25:04.640 --> 25:08.840
In this way, you will have extra cost.

25:08.840 --> 25:12.560
And another thing is you cannot always predict, right?

25:12.560 --> 25:13.560
Correctly.

25:13.560 --> 25:18.680
For example, if you look at the load curve, usually just like a stock market, right?

25:18.680 --> 25:25.400
It's randomly and can surge very high sometimes and suddenly, you know, jobs, right?

25:25.400 --> 25:32.560
So in this case, if you can not predict the load like accurately, right, then what can

25:32.560 --> 25:33.800
you do, right?

25:33.800 --> 25:40.800
So our solution is combining, you know, the infrastructure as a service together with

25:40.800 --> 25:46.880
function as a service or, you know, the popular service provided by a call the providers.

25:46.880 --> 25:48.880
And the idea here is, right?

25:48.880 --> 25:54.480
We still would like to use infrastructure as a service because it's much cheaper in terms

25:54.480 --> 25:55.880
of cost.

25:55.880 --> 26:01.360
However, for function service, it uses the container, right?

26:01.360 --> 26:05.000
And it can scale very fast at like a second level.

26:05.000 --> 26:06.000
Right.

26:06.000 --> 26:12.320
So for the normal load, right, we just use the infrastructure as a service, those virtual

26:12.320 --> 26:17.000
machines to provide the main computing power.

26:17.000 --> 26:20.280
And better when there is a surge of load, right?

26:20.280 --> 26:24.040
Of course, we will do some predictions, right?

26:24.040 --> 26:28.040
If the prediction is accurate, then we can do provisioning.

26:28.040 --> 26:38.760
But if it fails, we will use the serverless instance as a transition period so that it can

26:38.760 --> 26:45.200
immediately, you know, take the new load while you are starting new virtual machines.

26:45.200 --> 26:51.640
And once the new virtual machines has been studied, basically you can, the serverless instance

26:51.640 --> 26:56.520
can transfer the work back to the infrastructure service so that you will have like instant

26:56.520 --> 27:02.280
scalability while also, you know, control the cost.

27:02.280 --> 27:06.840
Thinking about this in the context of cloud, you know, this is maybe, I don't know if we

27:06.840 --> 27:11.120
talk about this a whole lot nowadays, but we used to talk about this concept of cloud

27:11.120 --> 27:17.320
bursting where you'd have like some normalized infrastructure capacity within your enterprise.

27:17.320 --> 27:22.400
And then you have some burst of activity and you process that off in the cloud.

27:22.400 --> 27:27.440
This is like function bursting or something like that where you're running everything

27:27.440 --> 27:35.640
on instances in the cloud and then bursting into Lambda for access capacity.

27:35.640 --> 27:36.640
Oh, yes.

27:36.640 --> 27:37.640
That's the general idea.

27:37.640 --> 27:38.640
Yeah, yeah, yeah.

27:38.640 --> 27:44.440
It's very similar to the concept, but instead of using purely, you know, Lambda, for

27:44.440 --> 27:51.920
example, right, we actually, you know, we actually do some experiment and find it, right?

27:51.920 --> 27:56.480
If you are, everything is on Lambda, since, you know, machine learning is very expensive,

27:56.480 --> 27:57.480
right?

27:57.480 --> 28:02.560
You actually need to like much higher, you know, cost eventually.

28:02.560 --> 28:04.320
So, so that's why, right?

28:04.320 --> 28:10.600
So we, you know, had this idea to combine the infrastructure as a service together with

28:10.600 --> 28:15.120
the function as a service to enjoy the benefit of both worlds, right?

28:15.120 --> 28:16.120
Right.

28:16.120 --> 28:17.120
Right.

28:17.120 --> 28:18.120
Right.

28:18.120 --> 28:22.440
So, the function is instant, you know, scalability.

28:22.440 --> 28:29.360
Yeah, another thing, you know, we didn't use the function as a service at this stage is

28:29.360 --> 28:35.200
they do have some limitations about, you know, the model size like the function size you

28:35.200 --> 28:36.200
can actually have.

28:36.200 --> 28:37.200
You only get certain amount of memory, right?

28:37.200 --> 28:38.200
Right.

28:38.200 --> 28:40.200
Certain amount of execution to that kind of thing.

28:40.200 --> 28:41.200
Yeah.

28:41.200 --> 28:46.320
And we actually have some effort is pushing on, you know, making, enabling the functions

28:46.320 --> 28:51.400
as a service to support these larger models, but it's still, you know, on their going.

28:51.400 --> 28:52.920
So I cannot show too much.

28:52.920 --> 28:53.920
Okay.

28:53.920 --> 28:55.160
That's the stage.

28:55.160 --> 29:01.320
So what's interesting about this is it, to me, is that, you know, independent of the

29:01.320 --> 29:07.640
domain that you're applying this to, yeah, as more folks are moving machine learning

29:07.640 --> 29:11.880
workflows to the cloud, I guess, more and more important to figure out creative ways

29:11.880 --> 29:18.680
to cost optimize, like, you know, building systems to move stuff over the spot instances

29:18.680 --> 29:22.040
versus, you know, regular instances.

29:22.040 --> 29:27.880
And this is another kind of way to arbitrage the cost difference between, you know, one

29:27.880 --> 29:32.080
service, the functions versus the infrastructure.

29:32.080 --> 29:33.080
Oh, yes.

29:33.080 --> 29:37.680
So actually, the cost of place are very important role here, right?

29:37.680 --> 29:42.440
Because think about this is that service, this is that continuous, you know, process, right?

29:42.440 --> 29:44.640
It's just not just one time effort, right?

29:44.640 --> 29:45.640
Right.

29:45.640 --> 29:46.640
So that's why, right?

29:46.640 --> 29:52.320
You know, if you think about the accumulated, right, the cost and the benefits is actually

29:52.320 --> 29:54.520
it's quite significant.

29:54.520 --> 30:00.000
Especially for these machine learning, right, models, they are quite kind of intensive,

30:00.000 --> 30:01.000
right?

30:01.000 --> 30:03.360
They consume a lot of resources.

30:03.360 --> 30:11.360
And so have you been successfully doing inference on Lambda, or is it, are you using some

30:11.360 --> 30:14.320
other functions capability?

30:14.320 --> 30:15.320
Oh, yes.

30:15.320 --> 30:22.600
We did actually being able to deprive some relatively smaller models on Lambda.

30:22.600 --> 30:28.440
However, for some later models, right now, we asked there are, you know, have some ongoing

30:28.440 --> 30:32.560
efforts to make it to be deprived on Lambda.

30:32.560 --> 30:35.320
There are two aspects of what you're doing.

30:35.320 --> 30:39.680
One is focused on how to achieve the scalability requirements.

30:39.680 --> 30:43.960
And this is this idea of bursting the Lambda or the functions.

30:43.960 --> 30:48.560
You also mentioned an element of this that's focused on latency.

30:48.560 --> 30:50.560
Now, what's the driver?

30:50.560 --> 30:55.640
There's just decreasing the amount of time it takes to get a prediction out to someone

30:55.640 --> 30:58.840
who can act on it, or is there another consideration?

30:58.840 --> 30:59.840
Oh, yes.

30:59.840 --> 31:01.480
This is a very good question.

31:01.480 --> 31:06.160
So about the latency part, actually, you know, it's very critical because all these, you

31:06.160 --> 31:10.920
know, machine learning models, they do take a lot of time to process a request, right?

31:10.920 --> 31:14.960
Think about, for example, an inception model, right?

31:14.960 --> 31:19.120
They have hundreds or even thousands of operations, right?

31:19.120 --> 31:25.600
So basically, each image you send, they need to go through all these operators, right?

31:25.600 --> 31:31.880
So if you're doing the sequential way, of course, it would be like a lot of time, right?

31:31.880 --> 31:34.640
It can be like seconds to even minutes, right?

31:34.640 --> 31:42.400
So one way to accelerate the processing speed is through parallelism and also batching.

31:42.400 --> 31:50.320
So for the parallelism part, so basically, almost all modern machine learning inference

31:50.320 --> 31:54.480
or serving frameworks, they do provide these control knobs.

31:54.480 --> 32:01.440
Basically you can have the request-level parallelism as well as operator-level parallelism.

32:01.440 --> 32:08.480
For example, as a request-level basically means you can process multiple requests in parallel,

32:08.480 --> 32:09.480
right?

32:09.480 --> 32:13.040
This is like, you know, just like traditional servings, it's very easy to understand.

32:13.040 --> 32:17.280
For the operation-level, basically, it means how many threads or how many cores will

32:17.280 --> 32:20.680
be assigned to execute each operator.

32:20.680 --> 32:25.240
So since operators, they do have dependency, right?

32:25.240 --> 32:30.280
So it's quite complicated, you know, how you configure, you know, this parallelism, right?

32:30.280 --> 32:33.880
Some people may say, I just said it to maximum, right?

32:33.880 --> 32:39.240
If my CPU have 20 cores, why I just said, you know, 20, right?

32:39.240 --> 32:47.520
However, this way, the experiment shows it actually doesn't, you know, work idea like

32:47.520 --> 32:53.600
this because there are more threads, you have, you will have more contention for the results

32:53.600 --> 32:59.800
because we know, you know, we have limited cache bandwidth also the cache capacity, right?

32:59.800 --> 33:04.600
So if you have too many threads working on just one operation, sometimes it will cause

33:04.600 --> 33:10.960
contention and actually this will create even more overhead and slow down your processing.

33:10.960 --> 33:18.520
So that's why, you know, it's not always like your since that maximum would give you the benefit.

33:18.520 --> 33:22.280
We actually, you know, find most of the time, actually, you know, it's just a random

33:22.280 --> 33:28.840
value in between depending on your model, depending on the specific system, yeah?

33:28.840 --> 33:32.160
And another important aspect is batching.

33:32.160 --> 33:34.760
So the batching basically-

33:34.760 --> 33:40.600
Before we move on to batching, when we're talking about operator parallelism, what's

33:40.600 --> 33:42.520
the operator in this context?

33:42.520 --> 33:44.960
Oh, yes, that's a good question.

33:44.960 --> 33:51.720
So for the operators, right, basically you can think about, right, all the, no matter,

33:51.720 --> 33:54.840
you know, what models you have, we're talking about things like low level things like

33:54.840 --> 33:59.920
multiplies and accumulates and that kind of thing or, yeah, so modern machine learning frameworks

33:59.920 --> 34:05.840
they actually take your model as input and then they will create a computational graph,

34:05.840 --> 34:12.120
right? For example, some operators, you know, doing like a matrix multiplication, right?

34:12.120 --> 34:16.920
Some just like a symbol, like element wise operation, right?

34:16.920 --> 34:23.080
So all these operations basically are executed in your system.

34:23.080 --> 34:26.960
So we're talking about kind of unrolling the computational graph and understanding how

34:26.960 --> 34:32.600
it can be parallelized across multiple cores, right, exactly.

34:32.600 --> 34:39.320
So however, since all these frameworks, they provide, you know, a control knob so that

34:39.320 --> 34:45.680
you can set, you know, different paradigms for operators as well as for request, for request

34:45.680 --> 34:48.240
basically, it's admission policy, right?

34:48.240 --> 34:54.440
However, the sense they don't tell you how to set it for a specific model and deploy

34:54.440 --> 34:56.640
the specific system.

34:56.640 --> 35:02.400
And we find if you set it in a different way, it actually can significantly impact your

35:02.400 --> 35:03.400
latency.

35:03.400 --> 35:04.400
All right.

35:04.400 --> 35:08.720
And so you're about to mention the second part of latency, which is the batch size.

35:08.720 --> 35:09.720
Yeah.

35:09.720 --> 35:10.720
Yeah.

35:10.720 --> 35:18.160
So another important thing people do for accelerating the processing speed is batch.

35:18.160 --> 35:26.160
So basically batch means you just, you don't execute a request like one by one, but rather

35:26.160 --> 35:31.840
you will form like you will put several images together into a batch.

35:31.840 --> 35:37.920
So the benefit of batch is, right, it creates more opportunities for optimization, right,

35:37.920 --> 35:39.960
parallelism optimization, right?

35:39.960 --> 35:47.400
For example, right, if you have like a small matrix, right, because it will do, for example,

35:47.400 --> 35:49.280
matrix multiplication, right?

35:49.280 --> 35:55.640
If you have two small matrix, of course, since can be done in power, but in like, you

35:55.640 --> 35:57.400
know, very limited degree.

35:57.400 --> 36:02.960
However, if you have two big matrix, right, then basically you can, you know, divide

36:02.960 --> 36:06.320
ism and, you know, beta-accelerate, right?

36:06.320 --> 36:12.160
So that's why, you know, the batching can actually help, you know, the efficiency of the

36:12.160 --> 36:13.640
computation, right?

36:13.640 --> 36:19.000
So all these, you know, low-level libraries, they actually, you know, develop to optimize,

36:19.000 --> 36:25.560
you know, the computation when you have like a larger degree of input dimension.

36:25.560 --> 36:26.560
Okay.

36:26.560 --> 36:30.760
So, however, right, batching has two sides, right?

36:30.760 --> 36:37.040
One is it can increase the computational efficiency and then accelerate, you know, to all

36:37.040 --> 36:38.040
computation.

36:38.040 --> 36:43.600
However, if you think about it, in order to form like a large batch, right, you're increasing

36:43.600 --> 36:44.600
latency.

36:44.600 --> 36:45.600
Right, right?

36:45.600 --> 36:46.600
It's not in a training, right?

36:46.600 --> 36:48.640
Because training all your data is there, right?

36:48.640 --> 36:51.600
You can create whatever batch size you want, right?

36:51.600 --> 36:58.240
But in the real-time inference, actually, request arrives in a random pattern, right?

36:58.240 --> 37:02.160
Sometimes, you know, there are more requests, sometimes less requests, right?

37:02.160 --> 37:07.160
And if you want to create like a larger batch, means the earlier arrived the request, they

37:07.160 --> 37:10.520
actually have to wait the later request, right?

37:10.520 --> 37:13.200
Then this penalized the earlier request.

37:13.200 --> 37:18.000
Another thing is, right, even though when you, for example, pretend requests together,

37:18.000 --> 37:23.200
right, it's faster than, you know, it's secure to the individual of them.

37:23.200 --> 37:25.600
However, it still takes longer, right?

37:25.600 --> 37:26.600
Right.

37:26.600 --> 37:32.720
Say like each request has takes 100 million seconds to process, right?

37:32.720 --> 37:37.240
Then if you do one by one, then you need to once again, right?

37:37.240 --> 37:43.840
However, if you do it in a batch, maybe it's just a 500 million seconds, right?

37:43.840 --> 37:49.360
Then still think about the first request, originally it only takes 100 million seconds, but not

37:49.360 --> 37:51.600
it takes 500 million seconds, right?

37:51.600 --> 37:56.720
Of course, the later request will benefit a lot from this, because think about the

37:56.720 --> 37:58.880
killing, waiting, perspective, right?

37:58.880 --> 38:02.080
They need to wait a little request finish, right?

38:02.080 --> 38:06.680
They can immediately, you know, go through the process and it's, you know, doing faster,

38:06.680 --> 38:07.680
right?

38:07.680 --> 38:14.240
So that's, you know, the reason you cannot do an arbitrary batch size, right?

38:14.240 --> 38:19.800
Actually, more than machine learning frameworks, they do provide two control knobs here.

38:19.800 --> 38:24.920
One is the batch size, maximum batch size, which means once you hit the threshold, it will

38:24.920 --> 38:27.960
be sent to the system, right?

38:27.960 --> 38:34.160
Even you have even more to, you know, still just send this size.

38:34.160 --> 38:41.160
The second is with something called the waiting timeout, which means you don't want to wait

38:41.160 --> 38:43.440
forever to create a batch, right?

38:43.440 --> 38:45.280
Since we are talking about latency, right?

38:45.280 --> 38:47.400
Every request matters, right?

38:47.400 --> 38:52.560
So I will say these two parameters in a real system is really complicated, right?

38:52.560 --> 38:54.800
It depends on a lot of factors.

38:54.800 --> 39:00.280
So if you think both parallelism and batch parameters, right?

39:00.280 --> 39:03.960
We have a lot of parameters here, you need to tune, right?

39:03.960 --> 39:08.640
I would say even system expert and machine learning expert, right?

39:08.640 --> 39:14.360
They work together, it's difficult, you know, to find a way to just manually tune these

39:14.360 --> 39:15.360
parameters.

39:15.360 --> 39:21.600
Plus, you know, based on different system situation and different workload, right?

39:21.600 --> 39:24.040
These parameters need to be changed, right?

39:24.040 --> 39:26.960
So that you can achieve the optimal, right?

39:26.960 --> 39:33.560
So that's why, you know, we created an automatic way, you know, to help people to configure

39:33.560 --> 39:35.040
these parameters.

39:35.040 --> 39:43.960
So I will actually first work published in supercomputing 2016, we actually built it like a coding

39:43.960 --> 39:50.080
model to actually, you know, to model the situation, you know, when you have, you know, these

39:50.080 --> 39:51.080
different parameters.

39:51.080 --> 39:52.080
Okay.

39:52.080 --> 39:59.040
And then we can mathematically, you know, compute our optimal solution for the scheduling.

39:59.040 --> 40:02.840
And this is all bringing me back to the stuff that I did in grad school.

40:02.840 --> 40:03.840
Oh, really?

40:03.840 --> 40:04.840
Wow.

40:04.840 --> 40:13.160
Doing theory and stochastic modeling and MMN cues and all that kind of stuff, right?

40:13.160 --> 40:17.320
And of course, the limitation of that work is, right?

40:17.320 --> 40:20.520
You do need to have some assumptions, right?

40:20.520 --> 40:23.040
For example, I have the arrival.

40:23.040 --> 40:24.040
Yeah, yeah, yeah.

40:24.040 --> 40:31.200
You do need to assume the random arrival, right, which follows the ID distribution, right?

40:31.200 --> 40:37.120
And also, you know, for that work, we also assume, you know, the request size deterministic,

40:37.120 --> 40:42.320
which means it's actually true for a lot of computer vision tasks, right?

40:42.320 --> 40:46.320
Think about all the pictures they have the same dimension, right?

40:46.320 --> 40:47.600
And the model is the same.

40:47.600 --> 40:50.920
Of course, they take roughly the same time to process.

40:50.920 --> 40:57.720
However, this is not true, actually, for some other applications like, you know, speech

40:57.720 --> 41:02.640
recognition, right, for natural language processing, right?

41:02.640 --> 41:05.880
So depends on your sentence is short or longer, right?

41:05.880 --> 41:08.720
The present time can be different.

41:08.720 --> 41:09.720
Think about it, right?

41:09.720 --> 41:14.920
So basically, the model based approach, they do need some assumptions, right?

41:14.920 --> 41:22.360
So, and make, if we want to make it more general in practice, we actually, you know, there's

41:22.360 --> 41:25.800
another way is doing model free approach, right?

41:25.800 --> 41:31.240
Of course, there are tons of different learning based approach for model free, right?

41:31.240 --> 41:36.040
So, of course, there are some simple things like a patient optimization, right?

41:36.040 --> 41:38.760
It's something is also very popular, right?

41:38.760 --> 41:45.640
So the reason we choose reinforcement learning is because, think about the dimension here,

41:45.640 --> 41:46.640
right?

41:46.640 --> 41:49.480
We actually have a lot of parameters you need to tune, right?

41:49.480 --> 41:55.200
So it's actually, you know, multi-dimensional problem, so quite complicated.

41:55.200 --> 42:00.160
So that's why we choose reinforcement learning as our approach, right?

42:00.160 --> 42:04.360
However, for reinforcement learning, there are some big limitations.

42:04.360 --> 42:08.600
It needs a lot of training samples, and it converges very slowly, right?

42:08.600 --> 42:14.240
So this is actually not ideal, right, in like a real time system, right?

42:14.240 --> 42:15.240
Right.

42:15.240 --> 42:18.000
So also, it quits a lot of, you know, overheads.

42:18.000 --> 42:23.440
Those problems are training time problems as opposed to inference time problems, yes or

42:23.440 --> 42:24.440
now?

42:24.440 --> 42:29.960
Well, if you want to use a reinforcement learning to configure your system, right, then this

42:29.960 --> 42:33.520
is like a training problem, but of course it's different then.

42:33.520 --> 42:35.240
So we have two machine learning parts, right?

42:35.240 --> 42:37.240
One is machine learning itself, right?

42:37.240 --> 42:41.840
And the other is to configure the system parameters, right?

42:41.840 --> 42:42.840
Right.

42:42.840 --> 42:49.680
So we actually use the machine learning model to find, you know, an optimal solution to

42:49.680 --> 42:52.240
do the machine learning inference, right?

42:52.240 --> 42:53.240
Right.

42:53.240 --> 42:54.240
Right.

42:54.240 --> 42:55.240
Yeah.

42:55.240 --> 43:01.840
So the conventional reinforcement learning, right, they do have like a relatively long learning

43:01.840 --> 43:03.640
cycle.

43:03.640 --> 43:08.160
So what we observe is for this specific program, right?

43:08.160 --> 43:10.120
So think about it, right?

43:10.120 --> 43:14.360
When you have, you know, this complex computational graph, right?

43:14.360 --> 43:19.880
You may have, you know, hundreds of different operators, right?

43:19.880 --> 43:26.440
So some operators may be more sensitive to this, you know, a parameter, right?

43:26.440 --> 43:29.920
Others may be more sensitive to other system parameters, right?

43:29.920 --> 43:32.760
So when you do like a very slight change, right?

43:32.760 --> 43:37.880
For example, just one of the parameters increased, maybe from two to three, right?

43:37.880 --> 43:45.040
So in this case, actually, a lot of maybe only some of the parameters they, you know, have

43:45.040 --> 43:50.000
like pronounced change, but most of others may be not so much, right?

43:50.000 --> 43:56.120
But all global, you know, situation is that it doesn't show much of the change, right?

43:56.120 --> 43:59.880
However, if you did a big change, right?

43:59.880 --> 44:08.240
Then almost all of them have different behaviors, then this will show like a more kind of pronounced

44:08.240 --> 44:10.080
changes globally, right?

44:10.080 --> 44:12.480
So if you draw like a hit map, right?

44:12.480 --> 44:13.480
You will see, right?

44:13.480 --> 44:19.080
When you have a big change, the latency will be totally different.

44:19.080 --> 44:21.760
However, if you do mean, right?

44:21.760 --> 44:24.200
See, I just do a small change.

44:24.200 --> 44:27.160
Actually, it's locally, it's very smooth.

44:27.160 --> 44:30.760
It's actually doesn't, you know, change latency much, right?

44:30.760 --> 44:32.680
So then we are thinking about, right?

44:32.680 --> 44:38.600
Why we just, you know, do less of the learning samples?

44:38.600 --> 44:44.280
And then we use, you know, the long-running sample to actually help estimate, right?

44:44.280 --> 44:46.120
Then you're by regions.

44:46.120 --> 44:51.080
So that's, you know, why we created an approach called region-based reinforcement

44:51.080 --> 44:54.760
running, or L in short.

44:54.760 --> 44:55.760
Okay.

44:55.760 --> 44:56.760
Yeah.

44:56.760 --> 45:02.360
So we find, you know, compared to state-of-the-art, a reinforcement running approach, you

45:02.360 --> 45:08.560
know, used for system configuration, we can significantly reduce, you know, the learning

45:08.560 --> 45:09.560
curve.

45:09.560 --> 45:15.520
So basically, the reinforcement running models can converge so much faster.

45:15.520 --> 45:18.360
And it also, think about, right?

45:18.360 --> 45:20.880
It's a continuous learning process, right?

45:20.880 --> 45:26.400
So whenever you have your models changed, or, you know, even, you know, the system can

45:26.400 --> 45:28.440
have, you know, changes, right?

45:28.440 --> 45:30.760
Especially if it's a cloud system, right?

45:30.760 --> 45:33.920
It's self-have-lot of randomness, right?

45:33.920 --> 45:36.120
So whenever there are such change, right?

45:36.120 --> 45:40.360
Basically, the models will learn to adapt to it, right?

45:40.360 --> 45:44.360
So that's why, you know, the faster convergence is critical here.

45:44.360 --> 45:51.640
I feel like I'm close to understanding what you're doing with the region-based RL.

45:51.640 --> 45:58.480
What I am envisioning is you're doing something where you're changing your optimization,

45:58.480 --> 46:07.640
your cost function, so that you're more focused on kind of this coarse-grained sensitivity

46:07.640 --> 46:10.040
analysis kind of thing.

46:10.040 --> 46:12.440
If you think about reinforcement running, right?

46:12.440 --> 46:15.760
It basically, you know, have two key things, right?

46:15.760 --> 46:17.920
One is action, one is state, right?

46:17.920 --> 46:18.920
Right.

46:18.920 --> 46:22.640
So it's just like, you know, we are running scenes or playing a game, right?

46:22.640 --> 46:23.640
Yeah.

46:23.640 --> 46:25.680
We would try different actions, right?

46:25.680 --> 46:28.120
And then we would get different rewards, right?

46:28.120 --> 46:33.000
Of course, in different states, you'll try different actions, you'll get different rewards,

46:33.000 --> 46:34.000
right?

46:34.000 --> 46:40.280
So that's basically the space of the whole learning process, right?

46:40.280 --> 46:46.400
So our approach basically says, right, if you try one action in a certain state, right?

46:46.400 --> 46:47.400
State.

46:47.400 --> 46:54.640
Basically, you probably didn't need to try another very close-by action because it gives

46:54.640 --> 46:59.760
you, well, be roughly, you know, similar amount of reward.

46:59.760 --> 47:05.320
So in other words, you're kind of quantizing the action space a little bit differently,

47:05.320 --> 47:06.840
more coarse-grained.

47:06.840 --> 47:10.440
Yeah, you can imagine kind of think about it, right?

47:10.440 --> 47:11.440
Sort of like that.

47:11.440 --> 47:12.600
Yes, yes, yes.

47:12.600 --> 47:18.680
So, and of course, you know, we do introduce like a hyper-primiter, right?

47:18.680 --> 47:21.440
To see how big is the region, right?

47:21.440 --> 47:29.320
So, if you have like, you know, too large region, then, you know, it will sort of, you know.

47:29.320 --> 47:32.720
So, so the region size basically means, right?

47:32.720 --> 47:38.760
If you have like larger region size, you are then faster, but if it's too big, then

47:38.760 --> 47:40.760
you will never converge, right?

47:40.760 --> 47:42.400
It's kind of like a learning rate.

47:42.400 --> 47:43.400
Right, right.

47:43.400 --> 47:48.920
If you have like two small ones, of course, you know, it will have less risk, but it's

47:48.920 --> 47:49.920
longer, right?

47:49.920 --> 47:56.640
If you have size one, basically, it's traditional conventional reinforcement, right?

47:56.640 --> 47:57.640
Right, right.

47:57.640 --> 48:04.720
I think the key message here, right, is even if you have good training model, right?

48:04.720 --> 48:12.080
So in order to do this in a real time detection, it's actually not that easy.

48:12.080 --> 48:16.840
So you do need to consider, you know, the machine learning actually are quite expensive

48:16.840 --> 48:20.360
compared to conventional statistical models.

48:20.360 --> 48:26.640
And as well as, right, you do have this changing demanding of the workload, right?

48:26.640 --> 48:27.800
So that's why, right?

48:27.800 --> 48:32.600
You need to consider, you know, the latency, as well as the scalability.

48:32.600 --> 48:35.200
And of course, right, if you want to go to cloud, right?

48:35.200 --> 48:37.560
The efficiency is always a problem, right?

48:37.560 --> 48:40.560
If you have infinite amount of budget, right?

48:40.560 --> 48:45.080
Of course, you just go for the most expensive equipment, right?

48:45.080 --> 48:46.080
Right.

48:46.080 --> 48:49.080
Actually, it's quite costly, right?

48:49.080 --> 48:53.480
So it's usually for machine learning inference, right?

48:53.480 --> 48:58.880
If you want to continuously rent, you know, a lot of machines, right?

48:58.880 --> 49:05.080
The actual computing resource and energy is tremendous, right?

49:05.080 --> 49:09.800
So any savings that on this line would be, you know, give you like a huge benefit.

49:09.800 --> 49:10.800
Awesome.

49:10.800 --> 49:11.800
Awesome.

49:11.800 --> 49:15.640
Well, thanks so much for taking the time to share with us what you're up to.

49:15.640 --> 49:16.640
Okay.

49:16.640 --> 49:17.640
Thank you.

49:17.640 --> 49:18.640
Thank you.

49:18.640 --> 49:24.360
All right, everyone, that's our show for today.

49:24.360 --> 49:32.360
To follow along with our reinvent series, visit twimmelai.com slash reinvent 2019.

49:32.360 --> 49:36.480
Thanks once again to Capital One for their sponsorship of this series.

49:36.480 --> 49:42.280
Be sure to check out capital one dot com slash tech slash explore to learn more about their

49:42.280 --> 49:45.040
ML and AI research.

49:45.040 --> 49:47.960
Thanks so much for listening and catch you next time.

