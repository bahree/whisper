Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting
people, doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
Contest alert.
This week we have a jam-packed intro, including a new contest we're launching.
So please bear with me, you don't want to miss this one.
First, a bit about this week's shows.
As you may know, I spent a few days at CES earlier this month.
While there, I spoke with a bunch of folks applying AI in the consumer electronics industry,
and I'm including you in those conversations via this series of shows.
Stay tuned as we explore some of the very cool ways that machine learning and AI are being
used to enhance our everyday lives.
This includes work being done at Anki, who built Cosmo, the cutest little computer vision-powered
robot.
Nighthouse, whose smart home security camera combines 3D sensing with deep learning and NLP.
Intel, who's using the single-shot multi-box image detection algorithm to personalize video
fees for the Ferrari Challenge North America.
First beat, a company whose machine learning algorithms analyzed your heartbeat data to
provide personalized insights into stress, exercise, and sleep patterns.
3AI and Koito, who have partnered to bring machine learning-based adaptive driving beams
or automatically adjusting high beams to the U.S.
And last but not least, aerial.ai, who applies sophisticated analytics to Wi-Fi signals to
enable some really interesting home automation and healthcare applications.
Now, as if six amazing interviews wasn't enough, a few of these companies have been so
kind as to provide us with products for you, the Twimmel community.
And keeping with the theme of this series, our contest will be a little different this
time.
To enter, we want to hear from you about the role AI is playing in your home and personal
life, and where you see it going.
Just head on over to twimmelai.com slash myaicontest, fire up your webcam or smartphone camera,
and tell us your story in two minutes or less.
Go post the videos to YouTube, and the video with the most likes wins their choice of
great prizes, including an Anki Cosmo, a lighthouse smart home camera, and more.
Submissions will be taken until February 11th, and voting will remain open until February
18th.
Good luck.
Before we dive into today's show, I'd like to thank our friends at Intel AI for their
continued support of this podcast.
Intel was extremely active at this year's CES, with a bunch of AI autonomous driving
and VR-related announcements.
One of the more interesting partnerships they announced was a collaboration with the Ferrari
Challenge North America race series.
Along with the folks at Ferrari Challenge, Intel AI aspires to make the race viewing experience
more personalized by using deep computer vision to detect and monitor individual race
cars via camera feeds, and allow viewers to choose the specific cars feeds that they'd
like to watch.
Look for my conversation with Intel's Andy Keller and Emil Chindicki later in this series
for an in-depth discussion about this project, and be sure to visit ai.intel.com, where you'll
find Andy's technical blog post on the topic.
And now a bit about today's show.
In this episode, I sit down with Alex Teichmann, CEO and co-founder of Lighthouse, a company
taking a new approach to the in-home smart security camera.
Alex and I dig into what exactly the Lighthouse product is, and all of the interesting stuff
inside, including its combination of 3D sensing, computer vision, and natural language processing.
We also talk about Alex's process for building the Lighthouse network architecture.
The tech stack the product is based on, and some things that surprised him in their efforts
to get AI into a consumer product.
And now on to the show.
All right, everyone.
I am here at CES, and I've got the pleasure of being seated with Alex Teichmann.
Alex is the CEO and co-founder of Lighthouse.
Alex, welcome to this weekend machine learning and AI.
Excellent.
Thank you.
Absolutely.
Great to have you on the show.
So, why don't we get started by having you tell us a little bit about your background.
You've done some interesting things in the AI sphere.
Oh, thanks.
Thanks.
Yeah.
So, my background is in perception systems for self-driving cars.
This is all about getting them to understand what they see in the world.
What is a car, and what is a bicyclist, and what is a pedestrian, and that sort of thing.
So, I joined Sebastian Thruns Lab back in 2007, right when the DARPA challenges were
wrapping up.
What were some of the specific things you were working on there?
So, my focus was on how you use 3D sensing, lidar in particular, in that case, to do a better
job of understanding what you're seeing in the world, you being a self-driving car or
a computer more generally.
So, this is very different from using a regular color camera to understand what you see in
the world.
When you have a 3D sensor, you've got the full structure to work with in real time.
And that opens up a variety of different computer vision techniques, and it makes many of
the very difficult computer vision subproblems quite easy, not all of them, but it makes many
of them easy.
Can you give us an example of that?
Yeah.
So, for example, segmentation and tracking of objects for which you have no computer vision
model is extraordinarily difficult with regular video, but when you have a 3D sensor and when
certain assumptions are met, then you could do a very good job segmenting and tracking
objects, even if you have no idea what they are, if you have no semantic information whatsoever.
And this is something that's made use so very heavily in the self-driving car world, where
you can see that there is a physical thing in the structure of the environment, and it's
moving around.
You don't have to know what it is to drive safely around it.
Yeah, there seems to be a, you know, there's a school of thought in and around the self-driving
cars that is taking advantage of what you're describing using LIDAR and things like that,
but then there's another school of thought where folks are saying LIDAR is too expensive
to be on every production vehicle, and we're going to try and do things with just cameras.
Any thoughts on that?
It's hard.
Which one?
That's all of it.
Yeah, no, no.
Well, getting all this stuff to work with just regular color cameras, it will eventually
happen, right?
The information is all there, and humans do it with what essentially amounts to just a
camera.
It's not very effective at that range, right?
We are just machines in some sense, very complex, very sophisticated machines, but we are
able to do it.
The information is there, and eventually we will get computers to be able to do that sort
of thing, but we seem to be a long way off from that.
It is quite hard.
This is why virtually every self-driving car project is using LIDAR, because it makes
many of those hard problems a lot easier.
So, fast forward to LIDAR, what's LIDAR up to?
Yeah, so the story of LIDAR, so we were talking a lot about self-driving cars here.
What we're doing is basically we're taking that set of computer vision and machine perception
techniques, and we're translating that from the self-driving car world into the home.
That's the technology perspective on what LIDAR is, that's the machine learning perspective.
From the customer perspective, LIDAR is, imagine you had a traditional home camera, but it
had the intelligence of something like Alexa or Google Home.
It's a new kind of interactive assistant that's based on this 3D sensing and computer vision
and cameras that lets you tell it what you care about, and then it tells you when it sees
those things happen.
And is the application, is it security or personal virtual assistant or something beyond?
It's both.
Okay.
Okay.
So, give me an example of how I might use it.
Yeah.
So, one thing you can do with Lighthouse is you can say, tell me if you don't see the kids
by 4 p.m. on weekdays, and you literally just say those words, that's it.
It understands what you're asking for.
It has a very good computer vision model for what children are, it knows what they look
like, and it knows that you're asking for, you know, by 4 p.m. on weekdays, Monday
through Friday, and if it doesn't see children by that time, Monday through Friday, it'll
send you a notification.
Okay.
And if it does, then it won't bother you.
Right.
Interesting.
A few years ago, I was, I had some crazy project that I was going to do around the house
and one of the first things I started trying to figure out was presence, and this was
pre-deep learning, CNNs, all that kind of stuff, and I started looking at NFC and all
these other kinds of things, and it's just so obvious now that the cameras and vision
is the way to do this.
What are some of the challenges associated with deploying a kind of a vision appliance,
I guess, in the home environment?
Well, everything in computer vision is hard to celebrate, because it's new or kind of
just like the dawn of artificial intelligence here, and all of these different techniques
are very cutting edge.
So we're really pushing the boundaries in what's possible with deep learning, and combining
that intelligently with the sorts of techniques you can use with 3D sensing, in particular
around segmentation and tracking.
There is a lot of complexity and a lot of difficulty around building the hardware to do this, too,
because this is the first 3D sensor that has a 95-degree diagonal field of view that
can see how it depends on the details, but 7 to 10 meters is typical.
It's quite challenging to put all that stuff together.
Hardware is hard, is the phrase for a reason.
Right.
So is the device itself a kind of a connect-plus-a-camera?
You can actually kind of think of it that way, so it uses a different underlying depth
sensing technique.
It actually depends on which connect you're referring to.
So the original connect was structured lights, and it's kind of like stereo.
There's like a texture pattern going out, a projector, and you know where that projector
is, and there's an infrared camera, and you can triangulate from that.
That was the original connect.
It was actually what we prototyped Lighthouse on in the very beginning, and what we're
using now is a time-of-flight camera, where that sends out modulated light, and then you
look at the phase shift between that modulated light as it returns, and a reference signal
in that phase shift, tells you how far away things are, essentially.
So at every pixel in the image, not only do you see, you know, oh, it's like this shade
of brown, you also see it's 3.72 meters away, you get that for the whole scene.
Interesting.
Trying to remember the name of this thing, there was a kickstarter that I backed.
I haven't done anything with this thing yet, but it was like a mini-lightar scans, I think,
was the name of it.
Have you ever come across that?
I haven't come across that one.
Was it for scanning your face or your self?
No.
It was kind of, you know, for hobbyists, you could, you know, put it on a mobile robot and
just experiment with it, that kind of thing.
I don't think there was any, like, specific end user use case associated with it.
Okay.
So it was focused on kind of, I think it was just an example of, you know, the, the, you
know, how to scale, light our down to something that fits in upon your hand and is relatively
cheap.
I see.
Yeah.
Yeah, I'm not familiar with that one, but, you know, both generations of the Connector,
a good example, the iPhone X, there's a 3D sensor built into it and that's also a good
example.
They use that to make face ID actually reliable.
And then, you know, self-driving cars, obviously, with all the different varieties of light
that's out there, you know, maybe a little bit more detail around the, the, some more
examples of kind of use cases for the device itself might be helpful.
Yeah.
Yeah.
So I mentioned the one about, like, if the children don't come home by a particular time.
And is it just children or is it like if Bobby doesn't come home or Susie doesn't come
home?
Like, do you, are you able to identify specific faces and associate them with, with kids or
is it just children?
Yeah.
So you can do either, in fact, with lighthouse.
So lighthouse has the ability to understand, oh, that is a child generally, it also has
the ability to understand faces of specific people.
And in particular, what, what that is most useful for is so you can do something like
say to lighthouse, you know, hey, tell me if you see someone you don't recognize while
Cindy and I are away, for example.
And that lets you get at, you know, I don't know if your children bring home a new friend
while you and your wife are out at work or something and you might just want to know,
like, oh, who is this new person and it'll tell you about it, it'll send you a push notification
when it sees that.
Or if, you know, you have a dog walker or a babysitter and one day it's somebody different
or somebody new is there, right?
It'll proactively notify you about this.
You don't have to go back and check every day, right?
Because you have set up this alert with natural language.
Mm-hmm.
Yeah, it was just occurring to me that as you were describing these use cases that, you
know, as complex as the, you know, the computer vision and the 3D sensing is there's also an
NLP challenge like how do you capture, you know, the full breadth of what someone's going
to want to ask this thing?
Are there, you know, we've talked a bit on the podcast about some of the underlying
NLP technologies and spoke with someone on the Alexa team.
Like are there unique challenges associated with the way you're using NLP in the context
of this device?
So I wouldn't say there's necessarily, I don't know, unique research challenges on
the natural language process inside.
There are difficult and important engineering challenges that we need to nail on that side
of things.
It's the computer vision where the really, the really heavy duty, you know, research
grade techniques are being deployed, at least for the current generation of Lighthouse.
I mean, you can imagine, you know, Google Assistant needs to answer virtually any question
you could throw away.
Right.
Right.
With Lighthouse, there's actually a more restricted set of things.
You know, if you ask Lighthouse for, you know, directions from like, you know, here or
two, wherever, like we don't do that, that's not what we do.
But for the set of things that we understand on the perception side, like we're, you know,
it's actually we're very good at being able to answer those questions.
It's a more constrained space.
That makes the problem easier.
There's more structure in it.
Okay.
And so do you, you know, when you're providing kind of the user manual for this thing, like
are you telling someone these are the 10 things you could ask it or are you setting the expectation
that they should just be able to ask it, things related to the kinds of stuff that it can
do.
Yeah.
So we float rotating suggestions in front of people in the app, right?
So like, so when you're in the kind of the natural language interface screen, you'll see
here is, you know, here's the set of things you might consider asking some examples of
these things in this category and it kind of guides you through what categories of things
we understand.
And that includes, you know, for object recognition, it's, you know, people and children and pets
and that kind of thing for action recognition, recognize waving at the device.
So you can say something like, hey, tell me if you see someone waving hello while I'm
out.
Okay.
That kind of thing.
I understand, you know, time ranges, we allow you to set up alerts for things that happen
in the future.
And so I'm going to kind of guide you through those different categories of what we do.
Okay.
And so on the computer vision side, what are kind of the key research level challenges
that you're tackling?
So what it is really coming down to is applying deep learning at a large scale with 3D sensors
combined with color cameras.
And there's, there are particular things that this set up let you do that you just can't
do in any other domain.
So for example, the 3D sensor lets you segment and track objects through the space without
you having to have any sort of semantic understanding.
You don't have to know what that thing is.
You just know it's a thing and it's moving through the space.
Now your unit of classification from a deep learning point of view is that segmented object
track through space and time.
And this enables several things.
One, it's just more accurate because you have more views of an object as it's moving
about and you can integrate all of that information.
And two, it's a very, very natural setup for doing action recognition because you've got
this thing moving through space and time and you can ask questions like is this a dog
but also is this a dog jumping up on my couch or is this a person waving hello or you
know, and so on.
So it's a great setup for working on these kinds of very challenging computer vision problems.
Okay.
You've talked about kind of segmenting these objects and I'm thinking about this primarily
being driven by the 3D sensor.
In what ways does having the camera augment what you're able to do beyond just the 3D point
cloud?
Oh, well, so at deep learning time, it's a specialized architecture that's using both
of those channels.
So the almost the attentional mechanism, if you want to, if you want to call it that,
that's primarily driven by the 3D sensor.
But then once you're kind of analyzing what is this thing, now we use everything we
have.
Okay.
And that is including the 3D sensor data, the point cloud of the object.
As well as the color camera data, we combine these things in a deep learning architecture
that uses both of those and then, you know, merges them and then goes into an LSTM
for doing like, you know, understanding of what is happening over time, right?
Okay.
And so how do you, what was the process for kind of coming up with the network architecture
for this thing?
Did you start with something off the shelf like inception or, you know, name your network
architecture or did you build it up from the ground up?
Yeah.
So, I mean, in this kind of context, it always makes sense to start from a baseline.
It's reasonably easy to just, you know, pull a thing out of the box and deploy it, see
what happens, right?
And so we did that with, you know, Google and that 100 years ago just to see what would
occur.
And yeah, it was, you know, it did something, it was good.
But it was pretty clear that we need to customize this thing to get the level of accuracy
that we really want.
Yeah.
And then the process from there is, well, are you familiar with the phrase graduate student
descent?
Sure.
Okay.
Yes.
But, I mean, it really, it's, you know, it's intuition combined with significant perseverance
combined with lots of compute, right?
Yeah, I think the current way of saying that post-Nips 2017 is alchemy.
Yes.
There's a lot of that.
I mean, it's kind of sad, actually, in that like a lot of my, a lot of my PhD work was
kind of like during the age when, you know, proper machine learning techniques should
be, you know, convex and just like, yeah, it's a descent method, like you're always going
downhill and just like roll to the bottom and you'll find the solution.
It'll be great.
Right.
Right.
And now it's just, it's non-convex and just like, maybe it's working and maybe it's not
working.
And, you know, oh, I don't know, try a, try a different momentum term and like maybe
it'll work this time.
Yeah.
Right.
And then it has, it has challenges and advantages too, right?
Like now things actually work.
That's pretty cool.
Mm-hmm.
So folks that are trying to productize around deep neural networks, like what, I mean,
I just, you know, I guess I struggle with the, the graduate student descent as the answer,
right?
I guess, probably we all do a little bit.
Have you developed, you know, any intuition or rigor around or methodology rather around
kind of the way you, you know, the way you build out network architectures for, for
this problem space or even maybe another question as background is like, was the network
architecture like up front work that you did and it's kind of static or is it, what, how,
how rapidly does that evolve?
So, so that is an ongoing effort in many different ways.
So in one way, we are, you know, collecting new annotated data all the time, you know,
both from our own early access testers who provide us access to their data for us to
use for training purposes.
And also, if there is a mistake in the field, you can, you can annotate it as such and
we'll make use of it and improve the models and we have, we have a stream of annotated
data coming in.
And so we're always taking the same network structure and taking that new training data
and turning the crank and redeploying.
And that cycle, I mean, it depends on the details, but that's on the order of days, right?
The new architecture deployment cycle, that's more like weeks or months as we, you know,
we come up with some new idea of like, oh, what if, you know, maybe we can compress the
network this way or maybe it would make a lot of sense to, you know, build out this piece
of the network and then we'll go work very hard and validate that new network and find
out, oh, indeed, this, you know, reduces compute time on our end and produces a better
experience for the customer.
Great.
Let's go deploy this.
You know, it's all about large scale quantitative testing.
And you mentioned compressing the architecture.
Are you deploying the network on the device or are you doing inference in the cloud or something
like that?
It's largely in the cloud.
Okay.
There's a variety of reasons that make sense, although I should mention it is not entirely
in the cloud.
It really is a distributed computer vision system to squeeze all the last, you know, bits
of performance out of it that we can.
You really do want it to not all run in one place.
It makes sense to have some of it run the device, some run in the backend.
So talk a little bit about that in more detail, like how do you, you know, what is running
on the device, how do you partition, what's running on the device and what's running in
the cloud?
Yeah.
So the device is doing the attentional mechanism.
It's doing the segmentation and tracking of what is interesting and new.
Okay.
And then there's nuance here, but at a high level, it's doing that.
So at kind of a simplistic perspective, you're not sending a bunch of frames out to the
cloud if there's nothing happening.
That's largely correct.
Yeah.
We do have to send some data once in a while, you know, one frame every, you know, a few
seconds, basically.
Okay.
Okay.
This is actually so we can present to you a beautiful summary of the day.
Okay.
Right.
So you, we call it a smart time lapse or a daily recap where you, you know, you press one
button and you get a, you know, a 10 second or one minute kind of summary of what happened
during the day and it goes fast during the boring parts and it goes slow when there's
something of interest to you.
Oh, interesting.
Yeah.
Okay.
But yeah, generally, we actually, we don't have to stream 30 frames per seconds because
it's actually not what customers really care about.
Users don't care about, you know, what were the RGB pixel values at like 3.47 AM, you
know, yesterday, what they care about is, you know, did my kids come home on time and
what has the dog done since I left the house because I just think it makes me feel warm
and fuzzy inside or, you know, was anybody new here?
Who was new here last week?
Just, you know, show me these things.
Yeah.
That's what they care about.
We're, with traditional home cameras, we're kind of a wash in, in data, but we don't
have much, you know, useful information and that's what Lighthouse is all about is taking
that enormous stream of data and compressing it down into just the bits that you actually
care about.
Interesting.
One thing that I'm curious about, you know, being kind of here at CES and seeing, you
know, tons of different consumer-oriented products that are trying to incorporate AI
in one way or another, are there any things that you've kind of learned that were surprising
about, you know, pulling AI into consumer-oriented products?
Yes.
I actually, when we started Lighthouse, myself and my co-founder, I thought it would be
the AI problems that were the hardest across the board.
And they are hard, for sure.
There's no question of that.
It turns out there's other hard problems that you have to solve along the way.
For example, getting the UX right, like getting the UI and the interface and the user experience
really right, that's really quite difficult.
It's something we spend a lot of time on because what we, you know, ultimately the reason
we exist is to deliver a delightful and useful experience to our customers.
And we're able to do that with AI, but like that's not the only thing.
Yeah.
And it's actually, it can be quite hard to get those things right.
Especially in, you know, breaking new ground in a new kind of interactive assistance,
how does one actually, you know, build the best interface to this kind of thing?
It takes a lot of work and iteration.
You know, do you have kind of the lighthouse laws of effective, intelligent user experience
design?
Like, have you, you know, boiled, you know, what you've learned down into key ideas that
you tell a new team member?
You know, I'm not sure we've refined it to that point, where I could concisely communicate
something.
Yeah.
Yeah.
I've asked people this on and off for the last couple of years, I think that it strikes
me that, you know, we've developed a fair amount of, you know, fair amount of methodology
around traditional user experience via mobile, via the web.
And it strikes me that there's, you know, some sort of rules that will evolve around designing
intelligent systems, or not that's kind of too broad, but presenting intelligent experiences
to consumers, but I haven't really found, you know, no one said, oh, yeah, I read this
book about it.
We're still too early for that.
Too early for this.
There is one guiding principle, actually, that is worthy of mention here, that's something
that's always kind of in the back of my mind with this kind of interface.
The reason it exists is to make useful information accessible to you as quickly as possible.
That's the reason natural language interfaces are good.
So, you know, stepping outside of lighthouse, looking at something like Alexa or Google Home,
one of the reasons they're so good is because, you know, you don't have to go find your phone
or pull your phone out of your pocket and like unlock it and go to this, you know, go
to the right app and then play your music and say, no, like, play it on this interface
and then finally, it comes out where, you know, no, you just, you know, you just yell
across the room, hey, play this thing and it just works, right?
And the reason that's amazing is because it saves you 10 seconds.
Right.
And it seems so trivial, right?
But it's not.
It really, really, really matters.
And when you look at this from the, I don't know if you want to call it the nerd point
of view, certainly says me, right?
But it's all about reducing latency and increasing bandwidth in the human machine interface.
That's the point of natural language is that you have a thought in your mind.
There's a thing you want to do.
And right now, generally, you have to translate that into, okay, I'm going to pull up my phone.
I'm going to tap on these buttons to get to the right app and then I'm going to tap on
some more buttons to do the thing I'm trying to do and I have to go to this menu and adjust
the slider buttons.
It's like, right?
It's just like, it's terrible.
What you should do is there's that thought in your mind.
Just say the thought.
Right.
And it just happens.
Right.
That's what that is.
That is just so much better.
But I don't know, it's an order of magnitude improvements in latency in that interface
between this intelligence and my head and this intelligence in my phone.
On the NLP side of things, did you start out with any of the kind of popular cloud-based
platforms for doing that kind of stuff?
Like the, I forget what it's called, not x.ai, what that is, but all the cloud vendors
have their own.
Or did you kind of roll your own?
You know, they are useful prototyping platforms.
And there may even be some applications where they get you all the way.
But that is not the case for Lighthouse.
I mean, I can tell you that for sure because I used one of them over a weekend to produce
a little demo of like, hey, this is what I have in mind.
I think this might be a way to really nail the user interface for this thing.
By the way, actually, I mean, when we started Lighthouse, we knew the direction to go
into to solve the perception problems, but we didn't know how to solve the UX problems.
And it was only along the way that we discovered that like, oh, my God, natural language interfaces
are the way to do this, like it is actually not possible as far as we are aware to produce
an interface with buttons and sliders and whatever else it might be to get you to be able
to say, hey, tell me if you're seeing anyone new at the doorstep while Cindy and I are
away next week.
Oh, yeah.
Right?
Like, how would you do that?
Like, you just can't, right?
But with natural language, it just works.
It just works.
Yep.
Right.
I've gotten that feedback quite a lot from folks that are trying to productize NLP, like
the platforms are an interesting way to start, but you run out of runway in terms of their
flexibility and ability to get you all the way.
Yeah.
So we built all around.
Okay.
It's the only thing to do in this area.
Can you tell me a little bit about your tech stack generally?
Yeah.
Yeah.
Happy to.
We use a lot of C++, because this is build on device and in cloud.
Both.
Okay.
It's a real-time, performance, memory-intensive, computer vision, right?
Running at scale, well, either at scale on the back end or on a limited compute device
out on the front end that is touching hardware, right?
And so in both of these places, C++ is the right thing to use at that level.
Now when we're prototyping a new architecture for our deep learning system, it's totally
reasonable to twitle around in Python to have faster iterations on that.
But ultimately when it's building and deploying real systems, it ends up being C++.
So did you build out the NLP platform on C++ as well, like the whole system for intense
and all that kind of stuff?
I'm simplifying a bit, of course.
So the core computer vision system is in C++, there's a Java layer around that because
that's easier to interface with your phones, for example.
And it turns out that's also a good place to build your natural language processing.
For whatever reason, in academia, at least my circles of academia back in my Stanford days,
natural language processing was generally done in Java.
And computer vision on self-driving cars, for example, was like all C++.
And it probably is the case that on self-driving cars, C++ is a more natural fit because
you have to interface with sensors and you have real time requirements.
And it's like very heavy data, whereas natural language processing is often less.
So in any case, that is a natural fit, our natural language system is lives out there.
And is the Java ecosystem for the natural language stuff as mature as the Python ecosystem?
Or more maybe?
That's a good question.
I don't think I actually know.
They were from a company perspective, where are you in kind of the life cycle of bringing
this product to market?
We are very close to general availability.
So in fact, you can go to our website right now, www.light.house and enter your email.
And we will add you to our special offer lists and if you're lucky, you might get one.
And if not, we will be available for anybody to buy in the not too distant future.
We are quite close now.
Nice.
As seen pictures of the device, it's like it looks like a, it's not a mobile device.
It's stationary.
You put it on a countertop or something like that.
You know, you either have to be very, very strategic about where you put this thing or
you have to envision a world where you've got ten of these all over the place.
Kind of like a Lexus becoming, right?
You have one in every room or something like that.
Is that the way you're thinking about the world?
Like you've got, you eventually have a full 3D and, you know, three color map of everyone's
home or is it something different?
Yeah, actually not.
I mean, maybe I'm not doing my job as like, you know, CEO of this company and like, oh,
you should have one in every room or something, but I actually don't see it that way.
I think several in a normal sized, you know, middle class American house, two to three is
probably the right number.
And you get a ton of value out of one.
And so you can kind of, you know, you get one, you play with it and you're like, oh my
god, this is amazing.
You get two and two and three.
I actually don't think it makes that much sense to have every single room covered.
It's usually particular areas of interest.
And so, you know, we often see the first one goes in an area that is kind of near the front
door.
So you see like what traffic is coming and going, but you also see a reasonable amount
of the kind of the floor plan of the home.
So you get a sense of what's going on there.
What other common places for Lighthouse to end up in or in the garage, because often
the door might get left open and you want to know if somebody's in there or you might
have tools out there and children and you want to know like, you know, are the kids going
out there when I'm at home, things like that.
Or, you know, upstairs in the kids room or just outside of the kids room to see if they're
getting up out of bed in the middle of the night.
I mean, you would literally just say, you know, hey, Lighthouse, you know, tell me if you
see the kids out in the hallway between, you know, 10 p.m. and 6 a.m. and then it works.
We need to implement the call me if you see this features coming down the road.
But it's those kind of areas of, you know, particular interests and it depends on the
particular homeowner.
Another common place is kind of in the living room, looking into the area where the dog
hangs out so that you can, you know, just get the warm and fuzzy feelings of like, hey,
what's my dog been up to since I left home?
Right.
So if you had this and you pointed it at the front door, can it, you know, can it effectively,
you know, track the state of the home and kind of be a general purpose presence detector
like, you know, keep track of someone walked in.
That person walked out so they're no longer inside and at any given time, like query it
and determine who's in the house.
So we can make that query and that does work.
But we don't do it with computer vision, actually.
Okay.
And the, you know, the reason is we, there's often many entrances and exits to a home
and we don't expect that you buy one lighthouse for every entrance and exit, necessarily.
Right.
So the way we do presence, absence detection is just, you know, is with phone presence
and absence.
GPS is part of that, but also looking at blue to the signals coming out of the devices.
All right.
Just going to say I just started playing with the Samsung smart things and it does it the
same way and it kind of sucks like it's, it's very coarse.
You have to work hard at it to get to work well, but I mean, there is a big advantage
in that we have a blue to the signal coming out of the device.
Ah, okay.
That's going to work a lot better than, you know, the GPS you're within a mile of your
house.
So therefore you're in your house.
If you just use location services, like as provided by the standard phone API's on
its own, it would be hard to make it really good.
Yeah.
Now to be fair, also like we will not cover the case where you walk out of your house
and you go to your neighbor's house, it's going to be hard for us to tell.
Right.
And like it will still think you're home.
But when you get far enough away, then it, you know, this gets you almost all of the
way.
Right.
Assuming that you've got your phone with you.
That is correct.
Yep.
Interesting.
I mean, that's one of the reasons that the children in classifier is a big deal because
they often don't have phones at all.
And you want to know what are they up to and did they get home on this, you know, on their
schedule and so on.
And so does this, does the lighthouse have an API?
Is it something that you envision people kind of getting and hacking on or is it more,
you know, just kind of the stated use cases as far?
You know, we have seen a tremendous enthusiasm for adding lighthouse capabilities to other
parts of the, the IoT world of the smart home.
Right.
You know, actually to this smart home, you know, device when you see something or other
kind of thing.
And I'm really excited to get to the point where we can actually start to tap into that.
We're not there just yet, but it's certainly on the roadmap.
We will be deploying something like that, some integration with other smart home capabilities
that, you know, that early adopters can plug together.
We will be providing that sometime this year.
It will not be immediate.
Yeah.
Yeah.
What's the long term, you know, view further company?
What are you trying to accomplish?
So when I take a step back and look at why lighthouse exists, the home is a piece of it,
for sure.
And it's a very exciting piece, but it's not the only thing.
The reason lighthouse exists is to improve human life by augmenting our physical spaces
with useful and accessible intelligence.
And that stated very broadly, quite deliberately.
Like there's sensors beyond computer, beyond cameras, beyond time of flight cameras, and
you know, beyond vision generally that are very interesting and that we absolutely should
integrate into this kind of thing.
And it also goes beyond the home.
There's many different AI service domains that are quite interesting to us.
We're not spending a lot of time there right now because, you know, it's hard enough to do
one of these things.
So we're very, very focused on delivering the home product into the world and having
that be a big success and make people's lives better.
But once that is established and growing more or less on its own, then it'll be time
to take our attention to another AI service domain.
What's an example of another one beyond the home that's interesting?
Elderly care is a big deal.
Okay.
It is a particularly big deal and we are particularly well suited to solve problems in that area.
And we're actually, we're starting to see hints of this already, even in the home,
we're aging in place in particular where you have an elderly loved one who may be there,
maybe they might need to go into a facility, like a nursing care facility, but you kind
of want to extend their time in their own home as long as you possibly can.
And a system like Lighthouse is actually really good for this use case because they get
a great security camera out of it or a camera they can see what their dog was up to or
whatever it might be.
And then the adult child gets the early warning system where you don't have to be looking
at it every day.
You just say, hey, Lighthouse, if you don't see anyone in the kitchen by 8 a.m. every day,
just let me know.
And then it might be that they just let in, but maybe today is a good day for you to call
and just see how are you doing?
Yeah.
Yeah.
Now, it seems like there are tons of folks kind of nibbling away at pieces of this space,
like how many devices does Amazon have alone?
Like they've got the key thing, which has a camera, they've got the look thing, which
is your kind of fashion visual system, but you know, they seem to be very gung ho of
getting cameras in your home, right?
You know, how does a consumer react to all these people trying to push cameras into
their houses and point clouds and all of this stuff?
Yeah.
So, there is a very fundamentally different perspective on the space when you're at a
place like Amazon.
Their goal is to magnify their marketplace, right?
Like they're trying to sell things.
That's why they're trying to put a camera into your house so that, oh, we can deliver more
things to you, right?
Or we can, you know, understand that like, oh, this scarf would look really good on you,
I'll try to sell this to you, or whatever it might be, right?
I mean, that's legitimately the stated purpose of that device.
With Lighthouse, it's very different.
We exist to provide this delightful AI service to you in return for money, and that's
the end of the transaction.
We're not looking to, you know, sell you a better hat or something, but, you know, taking
a step back from all that, it is super interesting what's happening in the home generally,
you know, at this CES in particular, there's this, you know, I, I, I, I, I, I, I, I, I, I,
I almost want to describe it as an epic slug fast between, you know, Alexa and Google
assistance to, to like, you know, with, like, oh, this is the AI is coming to the home in
this particular form.
Right.
And it's really interesting.
And, you know, who knows where it's going to be in a year from now, but what is very
clear is that adding perception capabilities and having, then that same kind of conversational
capability is super exciting, and that's going to write where Lighthouse is.
Mm-hmm.
Yeah, there's often this question about, like, is, you know, thing X, is it like a product
or a feature, and, you know, what you're doing in a lot of ways is, like, bringing together
the, you know, the vision piece, which is, you know, I guess I'm wondering, like, long
term, like, does, you know, does something like Lighthouse and Alexa, do they converge?
Like, do you, do you want to, if Alexa was more open, like, do you want to have to deal
with the NLP, you know, or do you, you know, want the, the vision to, to kind of tack
on to that or take advantage of the broader ecosystem, and I guess I'm mostly thinking
about this from the perspective of a consumer, like, how many of these devices do I want
in my house listening to, you know, listening to everything, and, you know, already I've got
like, you know, the, the Google Home, and you have to, it has its wake word, and Alexa,
I've got two different wake words in the house, it's like, it's already getting a bit
maddening.
Mm-hmm.
Yeah.
You know, with, with Lighthouse, you actually don't talk to the device itself.
Oh, really?
Yeah.
But because usually the responses we're providing are video, and there's, there's no, there's
no screen.
Like talking to your device.
Yeah.
And, and usually you're out and about, right?
That makes a lot of sense.
It's usually, you're, you're at work or, you know, you're, you're on a train or something,
and you're just like, you know, hey, what did the dog do since I left, or like, you
know, hey, it had, you know, what did the kids do while it was out yesterday?
I got it.
And then you, you see the results there.
Right.
And it's all about delivering video, answers in video form.
Mm-hmm.
Right.
So, so we won't be adding to that confusion about like so many different things that, that
like can respond to you in the home.
Mm-hmm.
I don't have an answer to that problem, but I don't know, a good chat with the Alexa folks,
I guess.
Mm-hmm.
Interesting.
Interesting.
All right.
Well, Alex, thank you so much for taking the time to chat with me.
I enjoyed learning about you, your background, Lighthouse, sounds like an interesting space,
and good luck here at CES.
Cool.
Well, thank you so much.
It's been fun.
All right.
All right, everyone.
That's our show for today.
Thanks so much for listening and for your continued feedback and support.
Remember, for your chance to win in our AI at home giveaway, head on over to twimolei.com
slash my AI contest for complete details.
For more information on Alex, Lighthouse, or any of the topics covered in this episode,
head on over to twimolei.com slash talk slash 103.
Thanks once again to Intel AI for their sponsorship of this series.
To learn more about their partnership with Ferrari North America Challenge and the other
things they've been up to, visit ai.intel.com.
Of course, we'd be delighted to hear from you, either via a comment on the show notes page
or via Twitter directly to me at at Sam Sharrington or to the show at at twimolei.
Thanks once again for listening and catch you next time.
