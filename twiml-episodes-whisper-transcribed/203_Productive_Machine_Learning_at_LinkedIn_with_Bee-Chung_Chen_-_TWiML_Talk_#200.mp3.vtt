WEBVTT

00:00.000 --> 00:16.320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

00:16.320 --> 00:21.480
people, doing interesting things in machine learning and artificial intelligence.

00:21.480 --> 00:31.640
I'm your host Sam Charrington.

00:31.640 --> 00:36.120
For those challenged with promoting the use of machine learning in an organization and

00:36.120 --> 00:41.680
making it more accessible, one key to success is to support data scientists and machine

00:41.680 --> 00:46.240
learning engineers with modern processes, tooling and platforms.

00:46.240 --> 00:50.480
This is a topic that we're excited to address here on the podcast with this AI Platforms

00:50.480 --> 00:55.640
Podcast series as well as a series of eBooks that we'll be publishing on this topic.

00:55.640 --> 00:59.560
The first of these eBooks takes a bottoms up look at AI Platforms and is focused on the

00:59.560 --> 01:04.880
open source Kubernetes project, which is used to deliver scalable machine learning infrastructure

01:04.880 --> 01:09.920
at places like Airbnb, booking.com and open AI.

01:09.920 --> 01:14.040
The second book in the series looks at scaling data science and ML engineering from the top

01:14.040 --> 01:19.240
down, exploring the internal platforms that companies like Airbnb, Facebook and Uber

01:19.240 --> 01:23.040
have built and what enterprises can learn from them.

01:23.040 --> 01:27.120
If these are topics you're interested in and especially if part of your job involves

01:27.120 --> 01:32.720
making machine learning more accessible, I'd encourage you to visit Twimbleai.com slash

01:32.720 --> 01:38.480
AI Platforms and sign up to be notified as soon as these books are published.

01:38.480 --> 01:45.560
Alright, in this episode of our AI Platforms Podcast series, we're joined by Bicheng Chen,

01:45.560 --> 01:49.640
principal staff engineer and applied researcher at LinkedIn.

01:49.640 --> 01:56.160
Bicheng and I caught up to discuss LinkedIn's internal AI automation platform ProML, which

01:56.160 --> 02:01.080
was built with the hopes of providing a single platform for the entire life cycle of developing

02:01.080 --> 02:06.000
training, deploying and testing machine learning models at the company.

02:06.000 --> 02:12.080
In our conversation, Bicheng details ProML, breaking down some of its major components including

02:12.080 --> 02:17.800
its feature marketplace, model creation tooling and training management system to name just

02:17.800 --> 02:19.360
a few.

02:19.360 --> 02:24.160
We also discussed LinkedIn's experience bringing ProML to the company's developers and the

02:24.160 --> 02:28.880
role of the company's AI Academy has played getting them up to speed.

02:28.880 --> 02:32.960
We're excited to have LinkedIn as a sponsor and supporter of the show and to include their

02:32.960 --> 02:35.400
story in this series.

02:35.400 --> 02:43.560
And now on to the show.

02:43.560 --> 02:46.400
Alright everyone, I am on the line with Bicheng Chen.

02:46.400 --> 02:51.400
Bicheng is a principal staff engineer and applied researcher at LinkedIn.

02:51.400 --> 02:54.000
Bicheng, welcome to this week in machine learning and AI.

02:54.000 --> 02:57.000
Yeah, it's very nice to be here.

02:57.000 --> 03:02.240
I'm excited to have you on and to learn more about what LinkedIn is doing to support its

03:02.240 --> 03:03.240
AI efforts.

03:03.240 --> 03:08.560
But before we dive into that, you currently lead machine learning algorithms and tooling

03:08.560 --> 03:09.560
at LinkedIn.

03:09.560 --> 03:14.280
Can you tell us a little bit more about your role and your background, how you got to where

03:14.280 --> 03:15.760
you are?

03:15.760 --> 03:22.520
My current responsibility is to ensure LinkedIn has the right machine learning and AI technology

03:22.520 --> 03:27.680
and also ensure our developers are productive using this technology.

03:27.680 --> 03:30.840
I have been always interested in machine learning since college.

03:30.840 --> 03:37.960
I still remember that probably 20 years ago, I'm doing neural network for face detection.

03:37.960 --> 03:46.320
Then in my PhD study, I was in a database group, but my research is always on how to apply

03:46.320 --> 03:51.560
machine learning to database systems to make data systems more intelligent.

03:51.560 --> 04:00.240
And after I graduated, I joined Yahoo Research and started my career in recommender systems.

04:00.240 --> 04:06.400
So in addition to publishing papers and also a book on statistical methods for recommender

04:06.400 --> 04:13.440
system, we also designed the recommending algorithms for the Yahoo homepage, Yahoo News

04:13.440 --> 04:15.680
and some other applications.

04:15.680 --> 04:21.080
And something we find to be very interesting is that for recommendation system to work

04:21.080 --> 04:27.760
very well, human and algorithm combination is very, very important.

04:27.760 --> 04:34.080
So human being are very good at selecting good candidate articles for recommendation.

04:34.080 --> 04:42.400
And that algorithms are very powerful to identify the user's interest and do depersonization

04:42.400 --> 04:48.160
and quickly react to changes in the user behavior.

04:48.160 --> 04:53.720
And after that, I joined LinkedIn about six years ago.

04:53.720 --> 05:01.720
I started by working on news recommendation and also feed ranking algorithms.

05:01.720 --> 05:07.120
And I'm amazed that LinkedIn has a very unique and a very rich data sets.

05:07.120 --> 05:15.360
So we have a pretty large member base like more than 500 million members and more than

05:15.360 --> 05:23.520
30 million companies and also a lot of like open jobs that people can apply to you.

05:23.520 --> 05:31.040
And another unique aspect is for every member, LinkedIn has very good profiles of those

05:31.040 --> 05:37.400
members, which are difficult to find other places for web companies.

05:37.400 --> 05:43.960
And in addition to those profiles, we have people's connection, we have people's interaction,

05:43.960 --> 05:50.440
we have users activity, people are seeking jobs, people are consuming content on LinkedIn

05:50.440 --> 05:54.080
and also learning courses on LinkedIn.

05:54.080 --> 05:57.520
And each member also have different roles.

05:57.520 --> 06:03.120
Some people are seeking jobs, some people are hiring people, some people are consuming

06:03.120 --> 06:07.760
content, some people are using LinkedIn as a platform to publish content.

06:07.760 --> 06:13.120
So there's a lot of connections we want to make between users so that everyone has the

06:13.120 --> 06:16.000
best opportunities.

06:16.000 --> 06:24.520
After around probably two years ago, I started to lead a group which designs the algorithms

06:24.520 --> 06:31.200
and also tools to help LinkedIn to be able to use machine learning and AI technology in

06:31.200 --> 06:32.560
all of our products.

06:32.560 --> 06:39.520
So you've talked about some of the unique assets that LinkedIn has in terms of its members

06:39.520 --> 06:43.560
and the profiles and the connections between those members.

06:43.560 --> 06:50.800
What are some of the machine learning and AI applications that are in place at LinkedIn?

06:50.800 --> 06:51.800
Yeah.

06:51.800 --> 06:56.520
So machine learning is used almost in every product in LinkedIn.

06:56.520 --> 06:58.920
So just give a few examples.

06:58.920 --> 07:07.240
If you open LinkedIn homepage, you see a feed of updates from your connections.

07:07.240 --> 07:14.160
So those are articles shared by your connections and also their professional change.

07:14.160 --> 07:17.800
For example, they change a job or they have a job anniversary.

07:17.800 --> 07:24.160
So we use machine learning to rank these different types of items so that we can maximize

07:24.160 --> 07:26.120
users engagement.

07:26.120 --> 07:30.400
And also in the feed, we also have sponsored updates.

07:30.400 --> 07:39.760
So company, they are paying thing to put updates in the feed so that they can get attention.

07:39.760 --> 07:45.460
And we use machine learning to predict the click rates of these different more like

07:45.460 --> 07:57.400
device advertisements and so that we can maximize the revenue and also the optimizer's value.

07:57.400 --> 07:59.760
And also we recommend people to people.

07:59.760 --> 08:05.600
So for social network, growing a person's network is very important.

08:05.600 --> 08:12.640
So we look at people's connection structure and try to identify the connection strength

08:12.640 --> 08:17.280
between two users and then make recommendations that you may be interested in connecting

08:17.280 --> 08:19.080
to these people.

08:19.080 --> 08:23.680
And we also use machine learning in different search problems.

08:23.680 --> 08:28.160
So for example, recruiters come to LinkedIn to search for potential candidates.

08:28.160 --> 08:32.920
And we use machine learning to help recruiters to find the best candidate that can feed their

08:32.920 --> 08:33.920
need.

08:33.920 --> 08:40.120
And also in addition to recruiting product, we also have a sales solution product to help

08:40.120 --> 08:46.800
sales people to find leads that can lead to potential like sales opportunity.

08:46.800 --> 08:50.960
And this is also using machine learning to find the right connection between the people.

08:50.960 --> 08:57.200
Can you talk a little bit about the evolution of tooling for machine learning at LinkedIn?

08:57.200 --> 09:08.120
So we started by developing machine learning algorithms on Hadoop.

09:08.120 --> 09:15.120
And I think several years ago, we find that using Hadoop may not be the best or the most

09:15.120 --> 09:18.600
efficient way of doing machine learning computation.

09:18.600 --> 09:23.840
So we start to develop algorithms on Spark.

09:23.840 --> 09:30.400
And we find that the ML leap coming from the Spark package is not efficient enough.

09:30.400 --> 09:40.360
So we develop our own training algorithm called PhotonConnect PHOTON Photon ML.

09:40.360 --> 09:45.280
And we open source that so that like everyone can also use that.

09:45.280 --> 09:54.480
And in addition to algorithms, we also look at how people can deploy model into our production

09:54.480 --> 09:56.000
systems.

09:56.000 --> 10:03.200
And in the past, deploying model into product systems has been pinpoint.

10:03.200 --> 10:07.400
There's a lot of co-development that people need to do.

10:07.400 --> 10:12.120
And our current effort is to make this whole process as automatic as possible.

10:12.120 --> 10:18.520
OK, and is Photon ML is that a replacement for ML lib that also runs within Spark or

10:18.520 --> 10:19.520
does that?

10:19.520 --> 10:21.800
Is there another engine for that?

10:21.800 --> 10:24.880
That is another engine.

10:24.880 --> 10:29.960
And internally, we basically mainly use Photon ML for our model training purpose.

10:29.960 --> 10:36.880
And we are not using ML leap because ML leap, at least in our experience, doesn't really

10:36.880 --> 10:41.000
scale to the amount of data that we need to process.

10:41.000 --> 10:48.280
And so it's Photon ML replacement for Spark or just the ML lib piece.

10:48.280 --> 10:49.960
And do you still run within Spark?

10:49.960 --> 10:51.480
Yeah, that's correct, right?

10:51.480 --> 10:53.440
Just a replacement for ML leap.

10:53.440 --> 10:56.920
We are still running using Spark.

10:56.920 --> 11:02.640
Maybe before we dig deeper into kind of the tooling and platform side, can you talk a

11:02.640 --> 11:09.120
little bit about the developer audience for your tooling efforts at LinkedIn or you targeting

11:09.120 --> 11:15.880
primarily kind of applied researchers or how diverse is the audience that you're supporting

11:15.880 --> 11:19.640
with the various tools that you're building?

11:19.640 --> 11:26.000
The initial audience that we support are the machine learning developers, right?

11:26.000 --> 11:33.440
So those are the people who design algorithms for our application, use machine learning

11:33.440 --> 11:39.240
to make the best decision for a different LinkedIn product.

11:39.240 --> 11:41.320
That's our initial focus.

11:41.320 --> 11:47.680
But at the same time, we also recognize that we really need to scale machine learning development

11:47.680 --> 11:49.320
and LinkedIn.

11:49.320 --> 11:56.480
We made an effort to educate our self-engineers to be able to apply machine learning.

11:56.480 --> 12:05.240
We call that AI Academy LinkedIn, so that is a five week training program that our engineer

12:05.240 --> 12:10.440
can participate to learn about machine learning and later, they can also apply machine learning

12:10.440 --> 12:13.160
to their application area.

12:13.160 --> 12:21.480
So gradually, the tooling that we're building will also support those people who graduate

12:21.480 --> 12:23.720
from AI Academy.

12:23.720 --> 12:28.680
They don't have a lot of past machine learning background, but they are learning to use machine

12:28.680 --> 12:32.880
learning and apply machine learning to our applications.

12:32.880 --> 12:40.080
Before we kind of dug into the question about the developer audience, you walked us through

12:40.080 --> 12:47.960
this evolution from Hadoop to Spark ML Lib to Photon ML.

12:47.960 --> 12:52.920
LinkedIn also has a machine learning platform that you call ProML.

12:52.920 --> 12:53.920
That's correct.

12:53.920 --> 12:59.920
Tell us about ProML and the relationship between it and these other tools that you've

12:59.920 --> 13:00.920
mentioned.

13:00.920 --> 13:01.920
Okay.

13:01.920 --> 13:16.640
So ProML is an initiative to double productivity for ML developer, and this platform provides

13:16.640 --> 13:21.920
functionalities for the entire machine learning life cycle.

13:21.920 --> 13:30.480
So starting from a feature marquee place, in which people can share and find potentially

13:30.480 --> 13:35.680
useful features and manage features for their machine learning model.

13:35.680 --> 13:39.880
And the second is model creation tooling.

13:39.880 --> 13:47.920
So this involves all the tools we use to train machine learning model and also specify

13:47.920 --> 13:51.160
the computation logic of machine learning model.

13:51.160 --> 13:55.280
So Photon ML belongs to this category.

13:55.280 --> 14:02.640
So Photon ML is one of the tools in ProML ecosystem.

14:02.640 --> 14:11.800
And we also develop algorithms to automatically select features and also automatically selecting

14:11.800 --> 14:16.160
hyper parameters for your machine learning models.

14:16.160 --> 14:25.240
And a third area is model deployment in which we develop tools to manage all the previously

14:25.240 --> 14:34.440
trend models and allow users, like LinkedIn internal users, to click on a button to publish

14:34.440 --> 14:41.760
the model and then deploy the model into various places in our production systems.

14:41.760 --> 14:51.320
And a third, the fourth area is model inference engine in which we provide the runtime environment

14:51.320 --> 14:55.800
to run the model and to serve user traffic.

14:55.800 --> 15:01.400
And the fifth area is what we call health assurance.

15:01.400 --> 15:07.640
So in which we provide tools to automatically monitor the model performance and also data

15:07.640 --> 15:11.800
quality to ensure that our models are performing well.

15:11.800 --> 15:20.120
And we also provide anomaly detection capability so that when something goes wrong, we can quickly

15:20.120 --> 15:21.680
identify them.

15:21.680 --> 15:29.680
And after we identify issues, we also provide debug and explain tooling so that people can

15:29.680 --> 15:33.800
investigate and find the reason that caused the issue.

15:33.800 --> 15:39.920
OK, it sounds like there's a ton for us to dig into the air.

15:39.920 --> 15:45.880
Maybe let's start at the beginning and talk about the feature marketplace.

15:45.880 --> 15:50.760
What's the motivation for the feature marketplace?

15:50.760 --> 15:57.320
So I think feature marketplace is also, I think, one of the quite unique area that we are

15:57.320 --> 15:59.720
looking at.

15:59.720 --> 16:08.560
So the reason that we start this feature marketplace effort is that in the past, different teams,

16:08.560 --> 16:12.240
they have different pipelines to produce features.

16:12.240 --> 16:19.080
And what we observe is that there are different teams.

16:19.080 --> 16:21.040
They develop very similar features.

16:21.040 --> 16:24.680
There's a very little leverage across the teams.

16:24.680 --> 16:30.480
And also the pipeline that generate features over time get very complicated, right?

16:30.480 --> 16:37.000
So we get into a pipeline jungle that at some point, for some application, become very

16:37.000 --> 16:38.840
difficult to manage.

16:38.840 --> 16:46.640
So the effort of feature marketplace is to simplify all these so that we provide a

16:46.640 --> 16:55.520
abstraction layer. In this abstraction layer, the feature producers, they use simple configuration

16:55.520 --> 17:01.840
to set up features by defining the location of the feature and the extraction logic of

17:01.840 --> 17:03.600
the feature.

17:03.600 --> 17:12.240
After that, the feature users, they can just use a unique name to refer to the feature.

17:12.240 --> 17:16.480
And the system will automatically figure out where to get the feature and how to extract

17:16.480 --> 17:17.480
the feature.

17:17.480 --> 17:21.680
And when you say the feature location, what are you referring to there?

17:21.680 --> 17:28.560
Is that like a source system or is it, what are we referring to?

17:28.560 --> 17:37.440
Yep. So location refers to the source of the features of example.

17:37.440 --> 17:44.280
We need features for model training offline, for example, Hadoop.

17:44.280 --> 17:51.000
So then the location will be a path on HDFS.

17:51.000 --> 17:59.000
And when we do online serving, the features may come from, for example, a key value store.

17:59.000 --> 18:04.760
So in that case, the location will be the address of that particular service.

18:04.760 --> 18:09.440
So you've got kind of the source of the feature data.

18:09.440 --> 18:17.000
And then it sounds like you're also providing a way to declaratively specify some sequence

18:17.000 --> 18:19.360
of feature transformations.

18:19.360 --> 18:20.360
Is that right?

18:20.360 --> 18:21.360
Is that part of it?

18:21.360 --> 18:22.520
Yeah, that is correct.

18:22.520 --> 18:28.920
So there are a few common logic that people use to produce useful feature.

18:28.920 --> 18:38.520
One is sliding window aggregation. So basically, you look at a user's past activity and aggregate

18:38.520 --> 18:41.760
the activity to identify their interest.

18:41.760 --> 18:49.320
So for example, if we saw that a user's frequently clicked on an article that mentioned a particular

18:49.320 --> 18:50.320
company.

18:50.320 --> 18:54.240
So then we know that the user probably is interested in that company.

18:54.240 --> 19:01.000
So one of the concepts that comes up in some of my conversations around features and

19:01.000 --> 19:08.400
repeatable pipelines is the notion of a DAG or a graph for applying these different transformations

19:08.400 --> 19:11.680
are you using some kind of metaphor like that?

19:11.680 --> 19:14.480
Yes, that's correct.

19:14.480 --> 19:19.640
There are two kind of DAGs in our system.

19:19.640 --> 19:24.800
So one DAG is for this feature computation.

19:24.800 --> 19:29.480
Another DAG is for the model computation logic.

19:29.480 --> 19:39.720
So in the feature computation DAG, that can include, for example, aggregation and also join.

19:39.720 --> 19:42.440
So from a key, we can look up.

19:42.440 --> 19:53.760
So for example, if we look at a feature for a member, so from the member, we know the

19:53.760 --> 19:54.760
indenting, right?

19:54.760 --> 20:00.800
We know the job title of the member and from the job title, we can look up for more information

20:00.800 --> 20:06.680
about the job title that in term can become a feature for the member.

20:06.680 --> 20:11.640
So such a join look up are all in the DAG that process the feature.

20:11.640 --> 20:18.200
Do you run into the issue of kind of the leaky abstraction problem around features where

20:18.200 --> 20:22.400
a team will define a feature?

20:22.400 --> 20:28.320
And it has kind of very specific semantics to the problem that they're trying to solve

20:28.320 --> 20:33.600
and someone else might find it in a marketplace or catalog and want to use it, but it doesn't

20:33.600 --> 20:34.600
quite fit.

20:34.600 --> 20:38.720
And if you do run into that, how do you address that?

20:38.720 --> 20:47.160
Yes, I think different teams, always, right, may define some feature that is very specific

20:47.160 --> 20:49.200
to their application.

20:49.200 --> 20:55.400
And in that case, while those features probably won't be that shareable, however, there

20:55.400 --> 21:01.120
are a large number of features that can be shared across different applications, especially

21:01.120 --> 21:04.360
on the features that capture a user's interest, right?

21:04.360 --> 21:11.360
So for example, users' interest in different kinds of jobs can be used to, for example,

21:11.360 --> 21:16.880
rank the content like articles for the user also, right?

21:16.880 --> 21:22.880
So for example, you look at job activity that a particular user is interested in jobs

21:22.880 --> 21:28.400
from a particular company, then when we rank articles, then we can potentially rank

21:28.400 --> 21:34.480
articles about that company higher, right, so that user can get a value.

21:34.480 --> 21:40.600
And are the features that groups define and publish out to this marketplace, are they?

21:40.600 --> 21:48.040
I'm thinking of like different version control metaphors, like forkable and I guess mainly

21:48.040 --> 21:49.120
that's the one I'm thinking of.

21:49.120 --> 21:54.600
Like can you fork these things and extend them to make them more, to tailor them to specific

21:54.600 --> 21:58.920
use cases or are they kind of more static?

21:58.920 --> 22:07.120
So the way that we are managing the features is the following, we try to reduce the dependencies

22:07.120 --> 22:14.040
of features on other features, because I think when we have more and more dependency, then

22:14.040 --> 22:18.280
it becomes something that would be difficult to manage.

22:18.280 --> 22:27.920
So we tend to treat each feature as a independent unit, and if you need to modify the feature,

22:27.920 --> 22:31.720
to some extent you fork or you copy and then you make all the changes and become another

22:31.720 --> 22:33.600
like independent features.

22:33.600 --> 22:40.640
I'm curious, what are the, it's just kind of wrapping up the feature marketplace component.

22:40.640 --> 22:49.240
What are the key things that you've learned in kind of producing and supporting this

22:49.240 --> 22:56.320
particular component in terms of the way folks use features and the most important

22:56.320 --> 23:02.480
things to consider when you're thinking about creating some kind of reusability for

23:02.480 --> 23:04.480
features?

23:04.480 --> 23:13.120
I think something that we learn is that a abstraction layer for feature access is very

23:13.120 --> 23:15.480
important.

23:15.480 --> 23:23.360
This abstraction layer gives the user a very simple view of how you can use a feature,

23:23.360 --> 23:27.960
basically you just refer to a user by its name, right?

23:27.960 --> 23:33.760
And then the system basically just automatically figured out where to find the feature and

23:33.760 --> 23:35.200
how to extract feature.

23:35.200 --> 23:41.400
I think this greatly simplify the way that people use feature.

23:41.400 --> 23:46.920
And when you refer to features here, it sounds like you're referring to kind of a higher

23:46.920 --> 23:57.800
level concept that is analogous to a pipeline as opposed to a low level, you know, I've

23:57.800 --> 24:05.640
heard it, you know, sometimes referred to as like a snapshot in time of data, right?

24:05.640 --> 24:10.200
And so you've got these features, you've got features, then you can kind of fast forward

24:10.200 --> 24:16.600
or kind of go back in time and kind of access features at different time points and things

24:16.600 --> 24:17.600
like that.

24:17.600 --> 24:22.200
Are you addressing, does your model address that as well?

24:22.200 --> 24:26.080
I think that that is a great question.

24:26.080 --> 24:34.960
We are currently building that capability, I think yeah, so the ability of looking at

24:34.960 --> 24:40.600
the feature at different time point is a very important functionality.

24:40.600 --> 24:51.400
The next part of the promo that you mentioned was kind of the interface for kind of building

24:51.400 --> 24:52.640
new models.

24:52.640 --> 24:59.000
What does that look like from a user perspective?

24:59.000 --> 25:04.400
Just make sure I understand this correctly, when you say user, you mean the user of machine

25:04.400 --> 25:05.400
learning tooling.

25:05.400 --> 25:06.400
Right.

25:06.400 --> 25:07.400
Right.

25:07.400 --> 25:08.400
The developer.

25:08.400 --> 25:09.400
Yes.

25:09.400 --> 25:10.400
Over the year, right?

25:10.400 --> 25:17.920
We found that there are three types model that are very useful for LinkedIn application.

25:17.920 --> 25:28.440
One is the tree models, usually that is a set of trees like a gradient boosted trees.

25:28.440 --> 25:34.840
And a certain type of model we find to be very useful is like deep learning models, right?

25:34.840 --> 25:37.120
Those are the neural networks.

25:37.120 --> 25:44.240
And a third type of model that we find to be also very useful is a deep personalization

25:44.240 --> 25:45.240
model.

25:45.240 --> 25:52.280
Those models try to learn a set of model parameters for each individual user.

25:52.280 --> 26:01.000
And the tool we provide is a method to allow people to connect or combine different type

26:01.000 --> 26:04.000
of model together, right, into a deck structure.

26:04.000 --> 26:05.000
Right.

26:05.000 --> 26:10.560
So for example, you can first apply tree models to learn the interactions right between

26:10.560 --> 26:16.600
users and for example, jobs, right, if we are talking about like job recommendation.

26:16.600 --> 26:22.920
And then after that, you may have another model neural network, right, that try to learn

26:22.920 --> 26:29.840
the representation of the member and also learn the representation of jobs, right?

26:29.840 --> 26:36.440
Through a neural network, we generate a vector that represent the behavior of the user and

26:36.440 --> 26:41.640
that behavior of job, and we can combine all these together into features into our deep

26:41.640 --> 26:46.680
personalization model and the tooling will building basically give people the flexibility

26:46.680 --> 26:52.680
of peak and choose different type of model and then combine them together into a deck.

26:52.680 --> 26:58.760
And then we do model training to train all these models together is the implication there

26:58.760 --> 27:07.840
that the feature marketplace and these DAGs has some kind of first class notion of the

27:07.840 --> 27:15.200
type of a feature or the type of features inputs or outputs that can be used to validate

27:15.200 --> 27:16.200
this DAG.

27:16.200 --> 27:17.200
Yes.

27:17.200 --> 27:23.720
So at the type of features, what we are currently developing is, well, tensor, right, so

27:23.720 --> 27:27.400
which is pretty much the same as other tools.

27:27.400 --> 27:33.920
However, we want the tensors to also carry semantic meanings, right?

27:33.920 --> 27:44.040
So if you look at say tensor flow, each tensor basically just a numeric array of multiple

27:44.040 --> 27:51.040
dimensions, but to be able to like validate the features and also help the developer to

27:51.040 --> 27:56.320
understand the feature, we want to add semantic to the dimensions, right?

27:56.320 --> 28:02.800
So for example, one dimension of the tensor may represent, for example, skills of the

28:02.800 --> 28:03.800
user.

28:03.800 --> 28:12.360
But another dimension may represent, for example, companies of a job, for example, right?

28:12.360 --> 28:17.680
So we won't be able to capture all these semantic meaning and so that our tensor type, when

28:17.680 --> 28:20.840
user look at the tensor, they can understand the tensor better.

28:20.840 --> 28:24.000
And that can also be used in our later validation.

28:24.000 --> 28:29.120
And now that I'm envisioning this, you know, almost kind of a wizzy wig drag and drop kind

28:29.120 --> 28:34.600
of thing, is that the direction you're headed or using more of a Jupiter notebook or kind

28:34.600 --> 28:38.320
of a code based way of constructing these graphs?

28:38.320 --> 28:44.760
Yeah, we are using code based approach rather than imagine that would be your, yeah, I

28:44.760 --> 28:48.800
think yeah, drag and drop is easy to create some small DAG, right?

28:48.800 --> 28:54.240
But if you manage a large DAG, I think that code based is important.

28:54.240 --> 28:59.800
Are notebooks a paradigm of choice at LinkedIn?

28:59.800 --> 29:10.200
Yes, we use notebook mainly for data analysis and also the first stage of modeling.

29:10.200 --> 29:16.800
Usually people do like portal typing, but I want to try out different ways of specifying

29:16.800 --> 29:20.080
your model, we use notebook for that.

29:20.080 --> 29:26.840
But for our production model training workload, we are not using Jupiter notebook.

29:26.840 --> 29:27.840
Okay.

29:27.840 --> 29:35.280
And so it sounds like there's not particularly any effort or interest in integrating the

29:35.280 --> 29:39.800
notebooks, like automating the notebook to production pipeline.

29:39.800 --> 29:45.400
It's a kind of ad hoc analysis tool and then if someone's going to produce something

29:45.400 --> 29:50.120
that eventually is targeting production, they're starting from, you know, they're starting

29:50.120 --> 29:52.720
in an IDE more often than not.

29:52.720 --> 29:53.720
Yeah, that's correct.

29:53.720 --> 30:01.120
That's the current usage, however, we are also evaluating the potential of using notebook

30:01.120 --> 30:06.000
for manage production training, but that's still in the exploration phase.

30:06.000 --> 30:15.360
So you've got these, you've got models that are developed in notebooks and in code connected

30:15.360 --> 30:18.120
via these graphs.

30:18.120 --> 30:26.280
I think the next component of Fremel is a component that manages the training environments

30:26.280 --> 30:29.320
and training clusters, is that right?

30:29.320 --> 30:42.440
Yes, the way that we manage training is mainly through Hadoop and the Spark.

30:42.440 --> 30:52.440
Our data all stored on Hadoop cluster and we use Spark to coordinate training of different

30:52.440 --> 30:54.960
types of models.

30:54.960 --> 31:03.760
And after training, we need to deploy the models into our production environment.

31:03.760 --> 31:11.240
There's another workflow management to take the model we trained and the data available

31:11.240 --> 31:17.200
on Hadoop, the model available on Hadoop and deploy that into different places in our

31:17.200 --> 31:19.480
production environment.

31:19.480 --> 31:28.680
And I think that is actually quite challenging problem in our system.

31:28.680 --> 31:34.120
The reason is that our models are usually very large, right?

31:34.120 --> 31:41.920
In order to be able to model each individual user's behavior, in the model training process,

31:41.920 --> 31:46.400
we generate model parameters for each individual user, right?

31:46.400 --> 31:53.800
So if you look at the size of a machine learning model, usually that's in the order of tens

31:53.800 --> 31:57.760
of billions of parameters.

31:57.760 --> 32:03.600
That amount of model parameters usually cannot fit into a single container.

32:03.600 --> 32:11.680
So in reality, we need to deploy parts of the model into, for example, key value store,

32:11.680 --> 32:21.960
parts of the model into the scoring service and also part of the model into the index

32:21.960 --> 32:25.160
that provide all the items.

32:25.160 --> 32:31.560
So that's the complexity that we are dealing with for the model deployment tooling.

32:31.560 --> 32:33.560
Okay.

32:33.560 --> 32:42.320
And the model, is there kind of an automated pipeline to kind of overuse the term between

32:42.320 --> 32:45.120
the training tooling and the deployment tooling?

32:45.120 --> 32:46.800
Yeah, that's right.

32:46.800 --> 32:56.840
So we are building a web interface, we call that model explorer.

32:56.840 --> 33:04.520
People can go to this interface and look at all the previously trained model and you can

33:04.520 --> 33:07.560
publish a model from the interface.

33:07.560 --> 33:16.160
And after that, we start the deployment process and we have a centralized release tool which

33:16.160 --> 33:25.480
monitor the deployment process and this process usually start with validating the model in

33:25.480 --> 33:35.080
a test environment and then deploy that in one instance in the production service and

33:35.080 --> 33:42.720
to validate that it works well in that one instance and then deploy to all the instances

33:42.720 --> 33:46.800
right after we validate that it works well for a single instance.

33:46.800 --> 33:50.880
So yeah, so this is a tooling we are currently building.

33:50.880 --> 33:57.360
I remember seeing a while ago, some work that I think was done at LinkedIn around distributed

33:57.360 --> 34:00.360
TensorFlow training on Hadoop.

34:00.360 --> 34:01.360
That's correct.

34:01.360 --> 34:06.360
I think you open sourced a project, is that still in use there?

34:06.360 --> 34:07.360
Yes.

34:07.360 --> 34:08.360
Yeah.

34:08.360 --> 34:16.880
So in when we train deep learning models, we are mainly use TensorFlow and in the past,

34:16.880 --> 34:25.640
we evaluated using Spark to manage a cluster for TensorFlow model training.

34:25.640 --> 34:30.840
However, that framework didn't work very well.

34:30.840 --> 34:42.800
So we develop TensorFlow on Yang, which we call like Tony, T, Stanford TensorFlow, ON,

34:42.800 --> 34:49.400
Y, Yang, which is a Hadoop cluster management system.

34:49.400 --> 34:58.400
And that helped us to effectively manage a cluster of machines that we can run distributed

34:58.400 --> 34:59.400
TensorFlow.

34:59.400 --> 35:09.840
Yesterday, I talked to colleague in Spark, apparently with a new release for Spark, Spark 2.4,

35:09.840 --> 35:11.880
they provide better support for TensorFlow.

35:11.880 --> 35:14.840
I think that's also something interesting to look at.

35:14.840 --> 35:19.880
There are a lot of folks working on different ways to do distributed TensorFlow.

35:19.880 --> 35:24.640
There's the horror vod stuff, there's the distributed TensorFlow that's kind of baked

35:24.640 --> 35:25.640
in the TensorFlow.

35:25.640 --> 35:34.680
It seems like just based on the level of activity that there are either a lot of kind of decisions

35:34.680 --> 35:42.640
that don't work well for everyone or the current solutions aren't, folks aren't very

35:42.640 --> 35:44.600
happy with what's currently available.

35:44.600 --> 35:48.200
Do you have a perspective on that?

35:48.200 --> 35:54.360
I think this is still a quite active area.

35:54.360 --> 36:02.240
I think probably after half a year, we will probably start to see like clear winners.

36:02.240 --> 36:10.240
And in terms of the actual computation, TensorFlow itself provide distributed training.

36:10.240 --> 36:16.840
However, I think most of the effort is on how to set up a cluster for machines such that

36:16.840 --> 36:21.160
we can start TensorFlow distributed training.

36:21.160 --> 36:31.800
And the key is how to enable developer to very easily set up these set of machines.

36:31.800 --> 36:39.000
And the set up mechanism also interact with TensorFlow very well.

36:39.000 --> 36:43.280
We've talked about training and deployment.

36:43.280 --> 36:51.000
And you also have an aspect of ProML that's focused on what you call health assurance.

36:51.000 --> 36:55.000
And I'm taking to be model evaluation and performance assessment.

36:55.000 --> 36:57.760
Can you talk a little bit about that?

36:57.760 --> 36:58.760
Yes.

36:58.760 --> 37:06.760
I think after we deploy the model into production and we start to serve user traffic, it is

37:06.760 --> 37:12.480
very important to make sure that the model continues to run properly.

37:12.480 --> 37:16.560
There are many things that we want to monitor.

37:16.560 --> 37:22.640
One is that the data quality, especially the feature data quality, we want to make sure

37:22.640 --> 37:30.720
that in the online service, the feature data we receive is consistent with the feature

37:30.720 --> 37:34.800
data we observe in our offline training process.

37:34.800 --> 37:43.600
And we also want to continue to continuously monitor our feature data distribution.

37:43.600 --> 37:50.920
So whenever we see a distribution change, then we want to be able to react to that quickly.

37:50.920 --> 37:58.060
Usually, distribution change may indicate some problem in our feature generation or

37:58.060 --> 38:00.760
feature serving systems.

38:00.760 --> 38:09.040
And after people received alert, people need to be able to investigate the problems in

38:09.040 --> 38:11.560
our production system and also the model.

38:11.560 --> 38:18.760
So in that area, it's important that we provide a user interface.

38:18.760 --> 38:24.000
People can go and look at to be able to explain for a particular recommendation.

38:24.000 --> 38:29.240
Why we are making this recommendation, people should be able to see all the features and

38:29.240 --> 38:33.000
also the model decisions based on these features.

38:33.000 --> 38:34.840
How is the distribution even specified?

38:34.840 --> 38:41.320
Are you assuming kind of simple Gaussian types of distributions and just looking at means

38:41.320 --> 38:47.960
and variances, or is it agnostic to the distribution of the actual data?

38:47.960 --> 38:52.240
We start by looking at basic statistics, right?

38:52.240 --> 38:57.960
So like, meaning variance, those are like first, old and second, older moments.

38:57.960 --> 39:01.920
And we also look at some higher older moments.

39:01.920 --> 39:10.120
And we also look at the cumulative distribution and compare the difference between the two.

39:10.120 --> 39:18.520
And you find that there's been a shift in distribution and or a model has degrade and

39:18.520 --> 39:20.560
predictive performance.

39:20.560 --> 39:25.200
Are you doing anything where you're kind of automatically triggering retrains or things

39:25.200 --> 39:26.200
like that?

39:26.200 --> 39:32.760
Or is it more about notifying whoever owns that particular model to take the right action?

39:32.760 --> 39:41.920
So now we start with notification and we are building methods which can also trigger

39:41.920 --> 39:42.920
this retrain.

39:42.920 --> 39:45.280
But we have not yet gone there.

39:45.280 --> 39:48.000
But that's an important next direction.

39:48.000 --> 39:56.760
And are you also doing things like canary models or AB testing, where you're kind of varying

39:56.760 --> 40:02.040
the amount of traffic that you're sending to a model on the fly?

40:02.040 --> 40:07.160
Yeah, AB testing is a very, very important aspect.

40:07.160 --> 40:18.200
And for all of our machine learning development, we use AB testing to verify and quantify

40:18.200 --> 40:22.720
the improvement of a new model that we launched in production.

40:22.720 --> 40:30.000
And a LinkedIn, we have a AB testing platform that is widely used in all the products.

40:30.000 --> 40:36.080
It's not only for machine learning, but for any product feature change, we are also doing

40:36.080 --> 40:42.960
AB testing because we want to make decision based on measurable results.

40:42.960 --> 40:50.320
You're able to use the existing AB testing platform that I imagine was in place for switching

40:50.320 --> 40:56.960
out copy and images and things like that on web pages with models as well.

40:56.960 --> 41:03.920
Did it require a lot of retooling to be able to support the use with the machine learning

41:03.920 --> 41:05.600
models?

41:05.600 --> 41:09.800
The way it works is pretty much the same.

41:09.800 --> 41:18.200
So basically, we have different treatments, and we allocate user traffic to different

41:18.200 --> 41:19.400
treatment.

41:19.400 --> 41:27.600
And each treatment has a key, and that key basically depends on how we interpret that

41:27.600 --> 41:28.600
key.

41:28.600 --> 41:36.280
So for experimenting with product feature, then in our product, we use that key to design,

41:36.280 --> 41:41.080
which experience we want to show to a user.

41:41.080 --> 41:48.080
And for machine learning experiment, we use that key pretty much to decide which model

41:48.080 --> 41:51.480
ID we want to pick up to serve the user.

41:51.480 --> 41:54.960
So at the infrastructure level, it's pretty similar.

41:54.960 --> 42:03.280
One thing that we didn't talk about earlier on in the relating more to model developments

42:03.280 --> 42:08.760
and training to some extent is experiment management.

42:08.760 --> 42:17.280
Does ProML provide a feature set for data scientists to help them manage the various experiments

42:17.280 --> 42:20.200
that they're running?

42:20.200 --> 42:25.440
So this is an area that we will be looking into in the future.

42:25.440 --> 42:35.200
So currently, our AB test platform provides reasonably support for managing the experiment

42:35.200 --> 42:36.600
that we run.

42:36.600 --> 42:39.960
But these are mostly user-facing experiments, right?

42:39.960 --> 42:42.080
Those are running online.

42:42.080 --> 42:51.280
For offline experiments, that is an area that we are currently building to be able to

42:51.280 --> 42:58.080
allow a user to see all the past experiments and enable them to compare different models

42:58.080 --> 43:03.760
in terms of their performance and also the difference in their features and also the difference

43:03.760 --> 43:07.280
in their type of models are using.

43:07.280 --> 43:10.520
This is one of the next steps that we are looking into.

43:10.520 --> 43:16.360
And that ties to hyperparameter optimization, which is something that you mentioned earlier.

43:16.360 --> 43:21.520
It sounds like you do offer via the platform some capability there.

43:21.520 --> 43:23.320
That's correct.

43:23.320 --> 43:29.320
So to train a machine learning model, usually we need to set up different like tuning

43:29.320 --> 43:30.320
parameters.

43:30.320 --> 43:37.000
For example, if you do a regression model, then usually you have regularization terms,

43:37.000 --> 43:41.840
then you need to specify the regularization weights.

43:41.840 --> 43:49.040
And also when you do neural net models, there are also many hyperparameters you need to

43:49.040 --> 43:50.520
decide.

43:50.520 --> 44:00.240
What we do is that we develop a Bayesian optimization method that sequentially try out different

44:00.240 --> 44:07.960
hyperparameter settings and based on that, where we learn the distribution of a potential

44:07.960 --> 44:12.960
best parameter setting and then based on that, we pick the next step.

44:12.960 --> 44:19.960
And through this iterative process, we find that better and better parameter settings.

44:19.960 --> 44:27.320
Sounds like ProML as one would expect is kind of in constant evolution.

44:27.320 --> 44:37.040
That's been the experience in bringing these features to the users of the system, the

44:37.040 --> 44:38.040
developers.

44:38.040 --> 44:44.320
Do you have, I'm curious really about observations in terms of, you know, for folks that have

44:44.320 --> 44:50.280
some community of machine learning engineers or data scientists that they're supporting.

44:50.280 --> 44:57.880
And it wants to bring platform features to them to make them more efficient.

44:57.880 --> 45:04.720
Do you have any advice as to, you know, where to start or how to proceed or the best way

45:04.720 --> 45:10.200
to work with that community to give them the tools that they really need?

45:10.200 --> 45:18.680
I think when for people to start using machine learning, I would recommend to start from

45:18.680 --> 45:24.240
simple methods and then gradually build complexity.

45:24.240 --> 45:34.240
And I think currently many cloud platforms provide capability of building and managing simple

45:34.240 --> 45:35.760
models.

45:35.760 --> 45:43.120
By simple, I mean that the model size is not very large and also the size of data is

45:43.120 --> 45:45.640
not very large, right?

45:45.640 --> 45:53.520
And then gradually you can add more complexity and by and for those simple models, I think

45:53.520 --> 45:58.520
Jupiter notebook is a perfect place to get started.

45:58.520 --> 46:05.480
And then when you add more complexity, then you may need to train your model on a large

46:05.480 --> 46:07.200
amount of data.

46:07.200 --> 46:13.920
And for that, I would probably recommend the listeners to take a look at the photon ML,

46:13.920 --> 46:21.160
which is a LinkedIn open source project, which can enable you to train your models on a

46:21.160 --> 46:30.040
large amount of data using Spark and do very good and deep personalization.

46:30.040 --> 46:38.080
And then after that, when you have those complex and large models, you need to start to

46:38.080 --> 46:42.480
worry about how to deploy model into your production systems.

46:42.480 --> 46:46.040
I think currently that is still a challenge.

46:46.040 --> 46:53.760
I think after we solve the problem, and we can share more of the fact that things.

46:53.760 --> 46:57.560
Watch this space is definitely a quickly evolving area.

46:57.560 --> 46:58.560
That's right.

46:58.560 --> 47:08.720
So you started this and describing a goal of increasing developer productivity and in fact

47:08.720 --> 47:12.560
ProML stands for productive machine learning.

47:12.560 --> 47:18.920
Have you kind of benchmarked productivity gains of the developer community as they've

47:18.920 --> 47:21.680
used this platform?

47:21.680 --> 47:33.440
We start to measure productivity in our LinkedIn machine learning applications.

47:33.440 --> 47:43.040
We just get started and what we look at is the number of successful experiments per engineer.

47:43.040 --> 47:52.560
So we want to improve or increase the value that a single machine learning engineer can

47:52.560 --> 47:53.560
produce.

47:53.560 --> 47:59.200
And the way that we measure the value is by looking at how many successful experiments

47:59.200 --> 48:04.200
that a single engineer can do by using the tooling.

48:04.200 --> 48:10.720
I think over time, we will be able to quantify the gain of the tooling we are building.

48:10.720 --> 48:14.200
And this is still a process that we are going through.

48:14.200 --> 48:15.200
Awesome.

48:15.200 --> 48:19.720
Well, Bichong, thank you so much for taking the time to chat with us about what you're

48:19.720 --> 48:20.720
up to.

48:20.720 --> 48:24.200
It is really interesting stuff and I learned a ton.

48:24.200 --> 48:25.200
Cool.

48:25.200 --> 48:26.200
Thank you so much.

48:26.200 --> 48:31.320
All right, everyone, that's our show for today.

48:31.320 --> 48:36.360
For more information on Bichong or any of the topics covered in this episode, visit

48:36.360 --> 48:40.480
twimmalai.com slash talk slash 200.

48:40.480 --> 48:46.520
To learn more about our AI platform series or to download our eBooks, visit twimmalai.com

48:46.520 --> 48:48.760
slash AI platforms.

48:48.760 --> 48:58.760
As always, thanks so much for listening and catch you next time.

