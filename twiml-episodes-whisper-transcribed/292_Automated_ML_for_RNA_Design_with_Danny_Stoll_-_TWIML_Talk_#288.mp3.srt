1
00:00:00,000 --> 00:00:16,320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

2
00:00:16,320 --> 00:00:21,480
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:21,480 --> 00:00:31,600
I'm your host Sam Charrington.

4
00:00:31,600 --> 00:00:35,960
Alright everyone, I am on the line with Danny Stoll, Danny is a research assistant at the

5
00:00:35,960 --> 00:00:38,520
University of Freiburg in Germany.

6
00:00:38,520 --> 00:00:41,600
Danny, welcome to this week in machine learning and AI.

7
00:00:41,600 --> 00:00:43,200
Thank you for having me.

8
00:00:43,200 --> 00:00:50,560
So Danny, I interview a lot of folks that are practicing machine learning professionally,

9
00:00:50,560 --> 00:00:56,480
as well as PhD students, but you are actually an undergraduate student working as a research

10
00:00:56,480 --> 00:01:00,040
assistant and you've been doing deep learning since high school.

11
00:01:00,040 --> 00:01:03,000
How did you get introduced to deep learning?

12
00:01:03,000 --> 00:01:08,000
So I guess it all started with a general interest in math for the math sake.

13
00:01:08,000 --> 00:01:16,320
So I learned a lot about analysis and linear algebra, stochastic, things like that.

14
00:01:16,320 --> 00:01:22,440
And at some point I got introduced to machine learning and deep learning via some blocks

15
00:01:22,440 --> 00:01:24,480
or things like that.

16
00:01:24,480 --> 00:01:26,360
And that really caught my attention.

17
00:01:26,360 --> 00:01:34,640
So I started with a famous curse offered by Andrew and G, a basic machine learning curse.

18
00:01:34,640 --> 00:01:40,680
And when I worked on that curse, which is an online curse, I really got hooked and I

19
00:01:40,680 --> 00:01:46,480
never looked back from that on and though deeper and deeper, mostly using online lectures

20
00:01:46,480 --> 00:01:49,600
because at the time it was still a high school student.

21
00:01:49,600 --> 00:01:54,080
Is that what you're studying at the University of Freiburg as well?

22
00:01:54,080 --> 00:02:00,680
So the program I'm in at my university is a general computer science program.

23
00:02:00,680 --> 00:02:07,360
So we learn all the basics of that for sure, but there are a lot of mandatory curses

24
00:02:07,360 --> 00:02:12,800
which cover the basics. But in the later stages of this program, you have a lot of freedom.

25
00:02:12,800 --> 00:02:15,520
You can choose different curses.

26
00:02:15,520 --> 00:02:18,840
You can hear reinforcement learning, machine learning, deep learning.

27
00:02:18,840 --> 00:02:21,520
You can have different seminars.

28
00:02:21,520 --> 00:02:24,240
You can even listen to very advanced seminars.

29
00:02:24,240 --> 00:02:32,120
But what I did when I started this program, I joined the reading groups with the topics

30
00:02:32,120 --> 00:02:37,040
related to machine learning and I joined the seminars just sitting in there and I really

31
00:02:37,040 --> 00:02:38,760
enjoyed that.

32
00:02:38,760 --> 00:02:46,120
And so the group here in, as a research assistant, automated machine learning, what's the

33
00:02:46,120 --> 00:02:49,120
focus of that group?

34
00:02:49,120 --> 00:02:57,840
So as the name says, the focus is on automatic machine learning because for once it's not

35
00:02:57,840 --> 00:03:02,760
everyone has experience in machine learning, it's pretty hard to do.

36
00:03:02,760 --> 00:03:10,160
But a lot of people would like to use machine learning or even AI for their products, for

37
00:03:10,160 --> 00:03:15,080
their research, people that might not even have a background in computer science.

38
00:03:15,080 --> 00:03:19,920
So one goal of automating machine learning is to really democratize machine learning

39
00:03:19,920 --> 00:03:26,000
to make it available to everyone and you do that by automating it to really remove the

40
00:03:26,000 --> 00:03:29,280
necessary expertise to run machine learning.

41
00:03:29,280 --> 00:03:34,720
And so we are going to talk a little bit about a paper you co-authored called learning

42
00:03:34,720 --> 00:03:37,440
to design RNA.

43
00:03:37,440 --> 00:03:43,040
Tell us a little bit about the origins of that paper and the problem that it sets out

44
00:03:43,040 --> 00:03:45,440
to address.

45
00:03:45,440 --> 00:03:51,440
So I guess I should start with automating why we care about RNA and what it means to design

46
00:03:51,440 --> 00:03:53,000
RNA.

47
00:03:53,000 --> 00:03:59,840
Many people or most people know RNA from their high school education and it's rural during

48
00:03:59,840 --> 00:04:02,840
the synthesis of proteins.

49
00:04:02,840 --> 00:04:09,120
But RNA can actually regulate biologic processes directly to match like proteins and there's

50
00:04:09,120 --> 00:04:13,880
a lot of focus on proteins because they can regulate these processes.

51
00:04:13,880 --> 00:04:18,560
But RNA can actually do the same thing and it has been connected to all kinds of different

52
00:04:18,560 --> 00:04:23,600
diseases like Parkinson's, Alzheimer's, epilepsy.

53
00:04:23,600 --> 00:04:28,680
And in a very recent study also autism, which was a 2019 study.

54
00:04:28,680 --> 00:04:37,480
So we're even still learning about where RNA is involved and it's only growing in size.

55
00:04:37,480 --> 00:04:43,640
So what actually matters about RNA in terms of which function it has in the body is its

56
00:04:43,640 --> 00:04:51,600
structure, its spatial structure, the weight holds, not the basic structure which is just

57
00:04:51,600 --> 00:04:55,840
a sequence of four nucleotides.

58
00:04:55,840 --> 00:05:03,280
But you don't really know which kind of sequence the desired way.

59
00:05:03,280 --> 00:05:10,040
So once you can have designed a spatial structure that you would hope has a certain function

60
00:05:10,040 --> 00:05:16,840
in the body, you still need to design an RNA sequence which has this structure which holds

61
00:05:16,840 --> 00:05:18,200
in the desired way.

62
00:05:18,200 --> 00:05:22,120
So you can actually have the desired function in the body and that's what RNA design tries

63
00:05:22,120 --> 00:05:23,360
to address.

64
00:05:23,360 --> 00:05:30,560
So given a high order structure design an RNA molecule, a sequence which has a structure

65
00:05:30,560 --> 00:05:34,120
which has the function that you want to have.

66
00:05:34,120 --> 00:05:40,920
So it's the function determines the structure that you need and the folding or the structure

67
00:05:40,920 --> 00:05:47,560
then determines the folding that's required to kind of create that structure.

68
00:05:47,560 --> 00:05:56,600
Yes, it's the structure determines the function it has and the basic sequence determines the

69
00:05:56,600 --> 00:05:59,000
structure, the higher folding.

70
00:05:59,000 --> 00:06:06,240
But we want to inverse that process, we want to go from function to structure to low level

71
00:06:06,240 --> 00:06:10,240
structure to the basic molecule itself.

72
00:06:10,240 --> 00:06:17,040
Great and so you kind of set out to tackle this problem.

73
00:06:17,040 --> 00:06:23,720
What led you to the particular approach that you ended up using here?

74
00:06:23,720 --> 00:06:29,240
So as we talked about in the start of this interview our group focuses mostly on automating

75
00:06:29,240 --> 00:06:35,360
machine learning and deep learning and one recent field there around recent approach

76
00:06:35,360 --> 00:06:41,120
what people are doing is called neural architecture search where you try to find the best

77
00:06:41,120 --> 00:06:48,680
architecture, you try to search of architectures of neural networks which perform best.

78
00:06:48,680 --> 00:06:55,000
And early times of neural architecture search people use reinforcement learning and basically

79
00:06:55,000 --> 00:07:00,360
decided for each building block or for each stage in the network what you want to use.

80
00:07:00,360 --> 00:07:08,440
And we saw some similarities with RNA design because there you also can choose which nucleotide

81
00:07:08,440 --> 00:07:15,720
at which point in the sequence you want to place and that led us to try this out and we

82
00:07:15,720 --> 00:07:19,400
got a proof of concept and it seems to work nicely.

83
00:07:19,400 --> 00:07:22,920
So from there on we went with it.

84
00:07:22,920 --> 00:07:29,760
Was there an existing body of research that you or kind of implementations that you were

85
00:07:29,760 --> 00:07:36,440
trying to automate through this neural architecture search or did you have to come up with, yeah

86
00:07:36,440 --> 00:07:39,680
I don't know if this question even makes sense but did you have to develop the algorithm

87
00:07:39,680 --> 00:07:48,560
to do the underlying kind of folding? Yeah that's actually a great question.

88
00:07:48,560 --> 00:07:52,880
So there has been a lot of research into RNA design because it's such an important problem

89
00:07:52,880 --> 00:08:00,480
and the research goes back almost 30 years and you ask about the folding itself.

90
00:08:00,480 --> 00:08:06,880
Folding has actually been solved for RNA and that's nice because if you compare to proteins

91
00:08:06,880 --> 00:08:11,920
there we are very far from solving folding. People are mostly working on folding, not designing

92
00:08:12,800 --> 00:08:19,120
and so we had this folding algorithm available to us. It's called the Zaka algorithm,

93
00:08:19,120 --> 00:08:25,200
there are other algorithms and our approach basically works with all of them but we concentrated

94
00:08:25,200 --> 00:08:29,520
on this one algorithm because mostly everyone else was trying that as well.

95
00:08:29,520 --> 00:08:36,560
As for the state of RNA engineering before our publication it was there were not many

96
00:08:37,200 --> 00:08:42,720
machine learning approaches but there were probably none that I know of. There was one other

97
00:08:42,720 --> 00:08:50,560
machine learning approach published in parallel to us but before that there was no really

98
00:08:50,560 --> 00:08:56,880
machine learning approach. What people usually did is they did a basic search, a local search for

99
00:08:56,880 --> 00:09:06,000
example to progressively improve the sequence that you have, that you want to then fold and see

100
00:09:06,000 --> 00:09:13,440
how well it folds and then try something else at some side of the sequence but that doesn't work

101
00:09:13,440 --> 00:09:21,120
very well we found because it doesn't scale well with sequence size. What we did instead is

102
00:09:21,120 --> 00:09:27,680
we formulated it as a reinforcement learning problem and that is in a generative way so

103
00:09:27,680 --> 00:09:34,480
we would put out the complete sequence that we want to fold at once and then we learned to do that

104
00:09:34,480 --> 00:09:41,280
very quickly and our approach scales really well with sequence length. The other machine learning

105
00:09:41,280 --> 00:09:51,760
approach which was published basically implements a heuristic for a local search so that's actually

106
00:09:51,760 --> 00:09:58,480
very different from what we have done. What's the relationship between your use of reinforcement

107
00:09:58,480 --> 00:10:05,360
learning and neural architecture search here? In the other days people were using reinforcement

108
00:10:05,360 --> 00:10:12,240
learning to do neural architecture search so reinforcement learning is the type of learning where

109
00:10:12,240 --> 00:10:17,600
you get a state or rather you have an agent and an environment and the environment provides a state

110
00:10:17,600 --> 00:10:24,320
and the agent can based on the state choose which action the agent wants to take and what people

111
00:10:24,320 --> 00:10:30,560
did with neural architecture search is they said okay our actions basically choose building blocks

112
00:10:30,560 --> 00:10:36,960
for different layers of the network say for example what do you want to choose a

113
00:10:36,960 --> 00:10:43,920
relu or what do you want to choose a different activation do you want a basic convolution or do you

114
00:10:43,920 --> 00:10:52,320
want a separable convolution things like that and we thought well that's the same for RNA design

115
00:10:52,320 --> 00:10:59,040
that's let's choose which nucleotides we place at different points in the sequence and so are

116
00:10:59,040 --> 00:11:08,560
you using neural architecture search or rather was this approach inspired by neural architecture search?

117
00:11:09,520 --> 00:11:16,320
So it's both so okay we already talked about how approach how basic approach was inspired by it

118
00:11:17,040 --> 00:11:23,920
but then we only had this basic idea and it's a very novel setting for machine learning algorithms

119
00:11:23,920 --> 00:11:27,920
and we didn't know what would work well which kind of architectures would we have to use

120
00:11:27,920 --> 00:11:33,360
what we have to use recurrent neural networks do we have to use convolutional neural networks

121
00:11:33,360 --> 00:11:40,160
do we need some embedding which we learn or what a basic embedding work and we said to ourselves

122
00:11:40,160 --> 00:11:45,360
okay we could try these things by hand but we are actually experts on doing this automatically

123
00:11:45,360 --> 00:11:53,840
and optimize it and then we went ahead and formulated our approach in a general way such that we

124
00:11:53,840 --> 00:12:00,560
could search the best architecture for it but we didn't stop there we also we also didn't know

125
00:12:00,560 --> 00:12:04,880
which kinds of hyper parameters we would need which how our training would need to work

126
00:12:05,600 --> 00:12:10,000
and finding the right hyper parameters is a very tedious and time consuming

127
00:12:11,360 --> 00:12:18,560
process as probably our machine learning practitioners know and our group usually tries to

128
00:12:18,560 --> 00:12:23,120
develop algorithms that automate this so we used our state-of-the-art algorithms or

129
00:12:23,120 --> 00:12:28,880
used algorithms and searched for the hyper parameters and the architectures jointly

130
00:12:29,440 --> 00:12:36,800
to to really optimize for that and we didn't even stop there we we thought okay but now we have

131
00:12:36,800 --> 00:12:42,320
to choose our environment how do we need to choose our environment what what kind of reward do we

132
00:12:42,320 --> 00:12:49,760
need how would our states need to look like how how do we need to choose our actions or what do

133
00:12:49,760 --> 00:12:55,600
our actions really correspond to all these kinds of choices of the environment and we also included

134
00:12:55,600 --> 00:13:01,120
these choices in our search so we searched for the best agent and the best environment jointly

135
00:13:01,120 --> 00:13:06,880
that's what we really did and also the training hyper parameters we we really automated our L here

136
00:13:06,880 --> 00:13:14,160
I mean it sounds like you kind of you know through the the kitchen sink at this problem you got

137
00:13:14,160 --> 00:13:20,240
a little bit of you know it sounded like multi-task learning in there reinforcement learning

138
00:13:20,240 --> 00:13:26,720
neural architecture search you know when you throwing so many you know different cutting-edge

139
00:13:26,720 --> 00:13:34,880
techniques at a problem like this you know I'm wondering what the you know are there some drawbacks

140
00:13:34,880 --> 00:13:40,000
or costs associated with doing that do you end up with you know something that you wouldn't want

141
00:13:40,000 --> 00:13:44,320
to implement because it's too complex or because there's no way for you to really understand what

142
00:13:44,320 --> 00:13:51,680
is doing you know or did you find that the you know the result met all the criteria that you you had

143
00:13:51,680 --> 00:13:56,560
for it and maybe you know that's a good place to start is what are the criteria that you you know

144
00:13:56,560 --> 00:14:03,280
would put in place for a solution to this problem so one basic criterion that that I tried to

145
00:14:03,280 --> 00:14:10,400
follow in my work always is do it simple you said we threw all these sophisticated things at it but

146
00:14:10,400 --> 00:14:17,680
in actuality our solution and or even our automation is extremely simple it sounds very complicated

147
00:14:17,680 --> 00:14:25,280
and but it but it's not it's the final result it's just a few layers it's it's it's very basic we

148
00:14:25,280 --> 00:14:34,000
didn't do any sophisticated techniques in there like we didn't even have learning rates get yields

149
00:14:34,000 --> 00:14:41,520
or we we basically only experiment with basic convolutional networks basic fully connected

150
00:14:41,520 --> 00:14:48,320
networks different very simple embeddings such that we could search over them efficiently and

151
00:14:48,320 --> 00:14:57,120
also our automation is very simple we had we formulated a 14-dimensional search space of integer

152
00:14:57,120 --> 00:15:03,360
parameters like how many convolutional layers do you want how many recurrent layers do you want

153
00:15:04,000 --> 00:15:11,280
but also continues parameters like what should the learning rate be or how should we shape the

154
00:15:11,280 --> 00:15:21,040
reward and we used a standard approach or our approach for hyper parameter optimization which is

155
00:15:21,040 --> 00:15:28,000
used in quite a lot of papers and through that at the search space and look what kind came out

156
00:15:28,000 --> 00:15:33,360
I guess a drawback when you do it like that is that you have computation overhead in terms of

157
00:15:34,080 --> 00:15:39,440
how you the big because you have to search over these hyper parameters you do that on

158
00:15:39,440 --> 00:15:46,320
many machines simultaneously but I think that's the better way than doing it manually and

159
00:15:47,600 --> 00:15:52,560
have it cost developer time because a lot of bottleneck in machine learning and deep learning is not

160
00:15:53,520 --> 00:15:59,600
on the computational side but on the developer time and that's also a great problem that can be

161
00:15:59,600 --> 00:16:04,800
solved with this automated type of parameter search and architecture search removing the need for

162
00:16:04,800 --> 00:16:10,320
a machine learning developers and researchers to really fiddle around the type of parameters because

163
00:16:10,320 --> 00:16:17,520
it is a very tedious and time consuming process and just formulate a search space run a hyper parameter

164
00:16:17,520 --> 00:16:24,800
optimization on top of it and see what works best and so were there any unique challenges or

165
00:16:25,600 --> 00:16:30,800
approaches that went into formulating that search base is it you know as simple as the things

166
00:16:30,800 --> 00:16:34,880
you outlined previously the number of layers and the types of layers that kind of thing or were there

167
00:16:36,960 --> 00:16:46,000
interesting interdependencies or you know did you have to kind of craft objective functions in

168
00:16:46,000 --> 00:16:55,040
an interesting way what was the kind of scope of that part of the problem so so there were some

169
00:16:55,040 --> 00:17:03,520
some things like the state space and the first few layers which had to which are conditional on each

170
00:17:03,520 --> 00:17:11,280
other we have some conditional parameters there but one other thing that that we had to come up with

171
00:17:11,280 --> 00:17:17,840
and it actually had a lot of impact on the final performance is the way we do the reward so what

172
00:17:17,840 --> 00:17:24,480
we do is we once we have designed a sequence we fold it and then we compare the folding the higher

173
00:17:24,480 --> 00:17:30,160
order structure with the one that we want to obtain and then we take the distance of these two

174
00:17:30,800 --> 00:17:38,400
but if you think about it if you're folding if your structure is 90% correct then that's not

175
00:17:38,400 --> 00:17:45,680
really good because you really need the structure to be exact and then we had to reshape the reward

176
00:17:45,680 --> 00:17:52,560
to put a lot more emphasis on if you only get 0.9% if you only get 90% right then your reward should

177
00:17:52,560 --> 00:18:00,720
almost be zero but if you get 0.98 correct then then the reward then there should be some reward

178
00:18:00,720 --> 00:18:07,440
because they're very close but we did not know how much we had to shape this reward in this

179
00:18:07,440 --> 00:18:15,520
regard so we formulated the reward in a way such that we had one parameter which could influence

180
00:18:15,520 --> 00:18:23,760
this kind of shape and then that's one part of a 14-dimensional search space with the reward

181
00:18:23,760 --> 00:18:29,120
function that needs to be shaped like that I imagine at one of the challenges that you run into

182
00:18:29,120 --> 00:18:37,280
is that you're not able to give your agent enough information as to how well it's doing unless it

183
00:18:37,280 --> 00:18:43,200
stumbles on to the perfect solution or something very close did you run into that challenge and

184
00:18:43,200 --> 00:18:50,800
how did you address it so that's a major challenge and when you when you start off with a single

185
00:18:50,800 --> 00:18:58,160
sequence and you run an algorithm on that you might get very close and then you get some reward

186
00:18:58,160 --> 00:19:06,240
yes but if it's a very complicated molecule then then it's very unlikely that you you will hit

187
00:19:06,240 --> 00:19:15,200
something successful and that's a major drawback of algorithms before ours but what we did is

188
00:19:15,840 --> 00:19:23,360
we incorporated this meta-learning on multitask learning across many molecules so we could

189
00:19:23,360 --> 00:19:29,440
actually transfer knowledge from for example a simple molecule where we can get good solutions

190
00:19:29,440 --> 00:19:36,320
quickly to more complicated ones so we can first learn the basics of the RNA folding dynamics

191
00:19:36,320 --> 00:19:45,760
or the inverse of that and then transfer this to some more complicated molecules so that's also

192
00:19:45,760 --> 00:19:51,920
where where where a very great advantage of our approach lies in because we can actually perform

193
00:19:51,920 --> 00:19:58,960
very well very fast on very long sequences which others cannot and if you take this component

194
00:19:58,960 --> 00:20:05,920
out of our approach which learns across these molecules then we do not perform as well but

195
00:20:05,920 --> 00:20:13,760
we would still have the state of the art so is the this transfer learning aspect is this across

196
00:20:13,760 --> 00:20:19,680
different training runs meaning you train up a model and then you apply it to a separate problem

197
00:20:19,680 --> 00:20:28,640
or you training across different problems or different molecules so it's kind of both

198
00:20:28,640 --> 00:20:37,360
because what we do is we do a kind of multitask learning very run on sequences on RNA molecules

199
00:20:37,360 --> 00:20:45,840
in sequence and in parallel and update parameters according to all of these RNA design tasks

200
00:20:46,720 --> 00:20:55,680
and then we use the resulting parameters to initialize our approach on a novel RNA design problem

201
00:20:55,680 --> 00:21:01,360
that's what we do but what we also do is we explicitly optimize the hyperparameters environment

202
00:21:01,360 --> 00:21:10,880
and the architecture to do this transfer well so we also optimize for being able to transfer well

203
00:21:11,760 --> 00:21:18,400
and how do you do that the objective that our hyperparameter and architecture search and environment

204
00:21:18,400 --> 00:21:29,760
search optimizes is given a training set of RNA design tasks and you can train on that how well

205
00:21:29,760 --> 00:21:38,960
do you on a few novel ones and we optimize this objective with our meta search and so when you

206
00:21:39,680 --> 00:21:46,720
kind of taking a step back and thinking about this from the the the perspective of the problem

207
00:21:46,720 --> 00:21:57,600
the RNA design problem what's your ultimate performance metric there and how do you how do you

208
00:21:57,600 --> 00:22:05,120
assess it is it are you comparing to other non-machine learning based approaches or something else

209
00:22:06,000 --> 00:22:14,000
we are comparing to the to non-machine learning based approaches that people proposed before us

210
00:22:14,000 --> 00:22:18,880
but we're also comparing it to the one machine learning based approach that I also mentioned

211
00:22:19,760 --> 00:22:25,520
and the kind of metric people used in the past for RNA design and which we also adapted

212
00:22:26,720 --> 00:22:37,040
is at each time point or in a for a given time how many RNA sequences or how many RNA design problems

213
00:22:37,040 --> 00:22:45,520
rather can you solve and solving means getting the structure completely right and what people

214
00:22:45,520 --> 00:22:51,360
usually do is they say okay there there we have a benchmark there we get 10 minutes per sequence

215
00:22:51,360 --> 00:22:58,000
or you have a benchmark where you get one day per sequence and then compare only the final

216
00:22:58,000 --> 00:23:06,320
performance but we looked at the complete trajectory of how long at each time point how many sequences

217
00:23:06,320 --> 00:23:13,440
have we solved and what we can see especially with our meta learning our multitask learning

218
00:23:13,440 --> 00:23:20,160
approach is that we solve a lot of sequences very very rapidly so for one of the benchmarks

219
00:23:20,960 --> 00:23:27,760
we we take some time for our parameters to load and everything but after that it takes one second

220
00:23:27,760 --> 00:23:34,880
and we basically beat most of the other approaches which take like five minutes to really

221
00:23:34,880 --> 00:23:43,280
solve softness many sequences so for one of the benchmarks we achieve state of the other results

222
00:23:43,280 --> 00:23:49,120
a thousand times faster than the previous state of the art and for another one we do it 60 times

223
00:23:49,120 --> 00:23:54,480
faster in this case of the thousand times faster you said loading your parameters is that

224
00:23:55,200 --> 00:24:00,880
so that's you're comparing a traditional approach that's non machine learning based with just

225
00:24:00,880 --> 00:24:07,440
the kind of inference against your model yeah we run the inference of our model on

226
00:24:09,120 --> 00:24:14,800
or the complete time it takes for our algorithm to receive the sequence to load the parameters

227
00:24:14,800 --> 00:24:21,120
and everything and to start predicting something so that's part of our evaluation and if some

228
00:24:21,120 --> 00:24:26,080
other algorithms also need some initializations then we measure that as well yes but

229
00:24:26,080 --> 00:24:33,600
but it's basically running the algorithms and looking at how quickly they solve a molecule or

230
00:24:33,600 --> 00:24:39,440
if they solve it at all I guess I'm I'm curious when you're talking about a thousand times faster is

231
00:24:39,440 --> 00:24:44,080
is it you know we really at that point trying to compare apples and oranges or they really

232
00:24:45,040 --> 00:24:53,520
comparable in the sense of a lot of your work is done in training and then you produce this model that

233
00:24:53,520 --> 00:25:01,920
is you know once you've got the model the time it takes to get the inference is going to be much

234
00:25:01,920 --> 00:25:07,280
less whereas if you have an algorithm that's doing the computation at the time of you know trying

235
00:25:07,280 --> 00:25:13,040
to figure out the the answer it strikes me as not being particularly comparable am I thinking

236
00:25:13,040 --> 00:25:17,760
about that correctly or yeah you can think about it that way but that's actually a nice thing about

237
00:25:17,760 --> 00:25:22,960
our approach that once you have trained it which doesn't take a lot of time in our case it took

238
00:25:22,960 --> 00:25:31,120
an hour or three hours and once you have that you can you can apply to novel RNA design problems

239
00:25:31,120 --> 00:25:37,760
which you haven't seen before and it just works that's that's definitely something that you should

240
00:25:37,760 --> 00:25:44,880
aim for like if you if you can pre-train your model or your algorithm beforehand and make

241
00:25:44,880 --> 00:25:50,640
and make use of information that you have for other problems and apply to new problems then

242
00:25:50,640 --> 00:25:56,240
that's a that's a very nice thing right and we also have one instantiation of our algorithm

243
00:25:56,880 --> 00:26:01,680
which does not do that does not transfer knowledge but just runs at the problem

244
00:26:01,680 --> 00:26:08,240
correctly and that also achieves state of the art performance I mean in one sense that it's saying

245
00:26:08,240 --> 00:26:13,360
you know a machine learning approach is where you produce a model and you can run inferencing

246
00:26:13,360 --> 00:26:18,880
as this model is better than an analytical approach where you're kind of doing the computations

247
00:26:18,880 --> 00:26:29,680
from scratch every time but you're you're saying that because the algorithm because your model

248
00:26:29,680 --> 00:26:36,560
generalizes it's kind of a valid comparison yes that's what I'm saying and there's also the other

249
00:26:36,560 --> 00:26:42,640
machine learning approach that I talked about also uses reinforcement learning but it only

250
00:26:42,640 --> 00:26:48,800
implements a heuristic via the reinforcement learning for a local search for these more traditional

251
00:26:48,800 --> 00:26:57,200
methods and um transfers knowledge for that as well but I'm not sure how well it actually does

252
00:26:57,200 --> 00:27:03,600
that yes but and it's and it still cannot just generate a sequence from scratch directly which

253
00:27:03,600 --> 00:27:12,080
which works yeah so from my point of view being able to transfer knowledge from from a set of

254
00:27:12,080 --> 00:27:20,240
training tasks to to novel tasks and make the performance better is a great thing but I might be

255
00:27:20,240 --> 00:27:26,560
biased since a lot of my work also focuses on meta learning where they try to do the same thing

256
00:27:26,560 --> 00:27:35,760
you try to learn to learn or you try to learn to effectively solve novel tasks if you

257
00:27:35,760 --> 00:27:41,760
someone's listening and they you know they hear this and they want to try it or their particular

258
00:27:41,760 --> 00:27:49,680
things that they need to be aware of or think about that are specifically related to maybe any

259
00:27:49,680 --> 00:27:55,040
interactions that all these things have with one another or they pretty straightforward to apply

260
00:27:55,040 --> 00:28:03,280
in conjunction with one another so because what we did is this is very simple we didn't run into

261
00:28:03,280 --> 00:28:10,880
really a lot of problems with using all these things in conjunction but if you want to combine

262
00:28:10,880 --> 00:28:16,000
very sophisticated meta learning algorithms with something like neural architecture search

263
00:28:16,000 --> 00:28:21,600
that's that that's a hard thing to do and that's an open research question how do you effectively

264
00:28:22,240 --> 00:28:27,440
meta learn to do architecture search or how do you do architecture search for meta learning

265
00:28:27,440 --> 00:28:34,160
that's that's what people are researching but in our case it was a very simple thing about a very

266
00:28:34,160 --> 00:28:41,360
simple formulation which ended up working great and that's also a strength of our approach that's

267
00:28:41,360 --> 00:28:46,800
very simple it's it's using hyper parameter search and architecture search and all these things

268
00:28:46,800 --> 00:28:52,880
but the way you formulate it it basically reduces to hyper parameter search for all of these

269
00:28:52,880 --> 00:28:59,760
things so we formulate a neural architecture search as a hyper parameter search and the same

270
00:28:59,760 --> 00:29:06,560
with the environment and that way you can very effectively search over all these choices at

271
00:29:06,560 --> 00:29:16,080
the same time and we talked before how we explicitly set the objective of our optimization

272
00:29:16,880 --> 00:29:25,280
for to the to how well at the configuration transfers so that's the thing you can do as well

273
00:29:25,280 --> 00:29:31,440
you can explicitly optimize for how well your meta learning optimizes or how you multitask

274
00:29:31,440 --> 00:29:38,720
learning transfers in the paper you talk about some ablation studies that she did can you talk a

275
00:29:38,720 --> 00:29:45,680
little bit about this yeah sure so in general I think it's very important to the ablation studies

276
00:29:45,680 --> 00:29:51,040
for your systems for your algorithms because if you do not you do not know what actually

277
00:29:51,040 --> 00:29:57,680
is the thing that performs well and if you do not do an ablation study then you end up doing

278
00:29:57,680 --> 00:30:03,440
things that don't help and it just makes your approach more complicated and more complicated for

279
00:30:03,440 --> 00:30:10,080
other people to use so I think in general it's very important to do an ablation study and what we did

280
00:30:10,080 --> 00:30:19,360
is we we ablated or we disabled the meta learning the transfer learning across molecules learning

281
00:30:19,360 --> 00:30:25,840
across molecules and what we observed is that while we still achieve very good results when

282
00:30:25,840 --> 00:30:34,400
we compare to other methods we do solve things much more slowly and also solve some molecules

283
00:30:34,400 --> 00:30:42,800
not at all one other thing that we disabled and saw how important it was was building a model

284
00:30:42,800 --> 00:30:48,960
itself like do we actually need to build a model or is it trying to just randomly predict something

285
00:30:48,960 --> 00:30:55,680
and as you would expect that that does not work very well yeah we also had an additional step

286
00:30:56,240 --> 00:31:04,560
in the reward or in the folding where once we are very close to a solution like only

287
00:31:04,560 --> 00:31:14,080
for sites or very small number of mistakes were made then we do some more search for it like

288
00:31:14,080 --> 00:31:22,560
not go through the our reinforcement learning agent again but just try some things just change

289
00:31:22,560 --> 00:31:30,320
some things and see how it affects the folding and if we can solve it and in early stages of our

290
00:31:30,320 --> 00:31:36,880
approach it was very important so we kept it in there but once we optimized for the transfer

291
00:31:36,880 --> 00:31:44,320
learning and everything we observed that it's important Scott less and less and only on very hard

292
00:31:44,320 --> 00:31:50,560
problems and very hard on a design problems it does make a difference so we ended up using that

293
00:31:51,760 --> 00:31:59,440
because it does help in these very hard cases but we could have disabled it for for easier benchmarks

294
00:31:59,440 --> 00:32:04,240
as well and that's something we learned from our ablation study that's that's very important I

295
00:32:04,240 --> 00:32:12,320
think to really learn what difference do your do your components make of your algorithm but what

296
00:32:12,320 --> 00:32:18,480
happens if you disable them are they really critical do they help at all and people do not do that

297
00:32:18,480 --> 00:32:26,320
enough I think did you develop any intuition for why that particular component only had the

298
00:32:26,320 --> 00:32:34,000
impact with the more complex molecules that's a good question I don't have a good answer I guess

299
00:32:34,800 --> 00:32:40,800
that that's not something we have looked too much into because only at the very late stages

300
00:32:40,800 --> 00:32:48,880
we found out that it's not actually important anymore so our intuition before that was like

301
00:32:49,760 --> 00:32:55,040
our model predicts a complete structure and it has to get everything right but our model is also

302
00:32:55,040 --> 00:33:03,920
very stochastic so it basically learns a distribution over molecules and you can make some mistakes

303
00:33:03,920 --> 00:33:11,520
there and we added this local adaptation step to really make up for this generative approach where

304
00:33:11,520 --> 00:33:17,280
you and allow the model to make some small mistakes that's how we thought about it but

305
00:33:17,280 --> 00:33:23,760
once you got a very good model and really learned how to transfer learn here we

306
00:33:24,640 --> 00:33:31,680
we then observed that that it did not make that many mistakes anymore and it it was more certain as

307
00:33:31,680 --> 00:33:38,480
well in which areas of the molecule space it wants to predict so it basically did this kind of

308
00:33:38,480 --> 00:33:47,120
search itself that's that's that's how I would interpret it I guess but for hard cases then for for

309
00:33:47,120 --> 00:33:55,200
very long molecules a very big molecules it cannot cannot do that very good still so there

310
00:33:56,000 --> 00:34:02,640
you still want to allow the model to do some mistakes and have it explore around good regions I guess

311
00:34:02,640 --> 00:34:10,240
one of the themes that comes up and pretty frequently in my interviews is this pendulum swinging

312
00:34:10,240 --> 00:34:15,920
between traditional model based approaches to the world where you're you know we're solving

313
00:34:15,920 --> 00:34:22,960
problems like this based on chemical or biological principles to more of a statistics based

314
00:34:22,960 --> 00:34:30,400
approach and how in a lot of research the pendulum is kind of centering again where we're trying to

315
00:34:30,400 --> 00:34:35,600
marry you know these model based approaches with the statistical based approaches is that

316
00:34:35,600 --> 00:34:42,960
something that you could see an opportunity for here so your question is whether the more

317
00:34:42,960 --> 00:34:48,640
traditional approaches for RNA design can be marriage with something like our approach in the

318
00:34:48,640 --> 00:35:01,120
future to even get better performance essentially yes so so so I see potential in in using our

319
00:35:01,120 --> 00:35:10,080
agent to produce a good initial guess for how the molecule should look like and then use more

320
00:35:10,080 --> 00:35:18,240
traditional methods to then work with this initialization because a lot of the drawbacks way

321
00:35:18,240 --> 00:35:24,880
of these traditional methods which are mostly search based are that it takes a lot of time to get

322
00:35:25,520 --> 00:35:32,480
close to a good solution so if you can already get very close to a solution right from the start

323
00:35:32,480 --> 00:35:42,160
like using our agent then then the drawbacks of the traditional approaches might not exist anymore

324
00:35:42,160 --> 00:35:49,120
and I see a lot of opportunities there thanks so much for taking the time to chat with me it was

325
00:35:49,120 --> 00:35:57,440
great chatting with you on about this stuff yeah it was my pleasure all right everyone that's our

326
00:35:57,440 --> 00:36:05,440
show for today for more information on today's show visit twomolai.com slash shows make sure you

327
00:36:05,440 --> 00:36:13,600
head over to twomolcon.com to learn more about the Twomolcon AI Platforms conference as always thanks

328
00:36:13,600 --> 00:36:43,520
so much for listening and catch you next time

