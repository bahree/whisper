1
00:00:00,000 --> 00:00:18,320
All right, everyone. I'm here with Sandra Vockler. Sandra is a professor at Oxford. Sandra,

2
00:00:18,320 --> 00:00:24,880
welcome to the Twomol AI podcast. Thank you so much for the invitation. I am looking forward to

3
00:00:24,880 --> 00:00:30,720
digging into our chat. Of course, to get things started, I'd love to have you share a little bit

4
00:00:30,720 --> 00:00:38,800
about your background and how you came to work in the field of artificial intelligence as a lawyer.

5
00:00:38,800 --> 00:00:43,680
I guess it started with the fact that I always was very, very excited about technology and I grew

6
00:00:43,680 --> 00:00:50,400
up with an understanding that technology could actually help us bring the world closer together.

7
00:00:50,400 --> 00:00:55,760
I'm just started off in the field of health tech. Actually, that was the first area that I looked

8
00:00:55,760 --> 00:01:02,800
at because medical devices that makes immediate sense that people understand that this can be helpful

9
00:01:02,800 --> 00:01:10,160
for a lot of people in our society. And then over time, and especially in my PhD, I branched out

10
00:01:10,160 --> 00:01:15,440
much broader and got interested in all types of technologies and how they can be used for

11
00:01:15,440 --> 00:01:21,920
good or bad and got interested in the question of how it affects laws, how it affects society,

12
00:01:21,920 --> 00:01:26,400
and what it is that we can do to reap the benefits and mitigate the risks.

13
00:01:26,960 --> 00:01:33,280
Nice. Were you ever been a practicing lawyer? Did you go through law school and all that?

14
00:01:33,280 --> 00:01:42,560
Yes, I did go for law school. I have a special degree in medical law. I do have a PhD in law.

15
00:01:42,560 --> 00:01:49,680
So, yes, full-fledged lawyer in dead regard, yes, in state and academia.

16
00:01:49,680 --> 00:02:00,960
Nice. Nice. What's your take on the... Actually, before we jump into that,

17
00:02:00,960 --> 00:02:08,400
tell us about your research. I was going to ask what's your take on the state of AI and the

18
00:02:08,400 --> 00:02:17,680
law, but that is a very broad topic. But it's not one that I've talked about extensively here

19
00:02:17,680 --> 00:02:25,920
on the podcast. So, there's lots to dig in there, but maybe we'll be a bit more focused and I'll

20
00:02:25,920 --> 00:02:32,560
ask you to talk a little bit about your research interests, which you spend a lot of time thinking

21
00:02:32,560 --> 00:02:41,120
about specific areas that AI intersects law. What are those areas? Yes. So, in general, I focus on

22
00:02:41,120 --> 00:02:48,480
the legal and ethical aspects of emerging technologies, and at the moment, I have free, very

23
00:02:50,960 --> 00:02:56,000
distinct research interests that have to do with AI. There is much more, so I don't want to say

24
00:02:56,000 --> 00:03:00,480
those are the only three people have to care about, but those are the only three I have time to

25
00:03:00,480 --> 00:03:11,280
care about at the moment. And there are free, free aspects that I think we all need to think about

26
00:03:11,280 --> 00:03:18,480
whenever we use algorithms for decision making. One definitely has to do with the Blackpops problem,

27
00:03:18,480 --> 00:03:26,080
the other one has to do with the protection issues, and the third one has to do with bias. So,

28
00:03:26,080 --> 00:03:32,560
those three areas, regardless of where you deploy an algorithm, whether this is in the US,

29
00:03:32,560 --> 00:03:38,640
whether this is in Europe, whether this is in New Zealand, and whether this is in banking or

30
00:03:38,640 --> 00:03:48,160
education, or in the health sector, those three areas will be important, because you always

31
00:03:48,160 --> 00:03:54,240
going to want to understand how a decision was made, how did the Blackpops come to the conclusion

32
00:03:54,240 --> 00:03:57,680
that you shouldn't get insurance, or that you have to go to prison, for example,

33
00:03:57,680 --> 00:04:02,000
always going to be a Blackpops problem. There is always going to be a data problem,

34
00:04:02,000 --> 00:04:07,120
because an algorithm is useless without data. So, whenever you talk about algorithms,

35
00:04:07,120 --> 00:04:11,520
you have to talk about data, and whenever you talk about data, you have to talk about algorithms,

36
00:04:11,520 --> 00:04:18,480
because data is only worth anything if you can analyze it in a way. And the genius thing about

37
00:04:18,480 --> 00:04:26,160
inferential analytics and AI is that you can learn so much about people, but the scary thing about

38
00:04:26,160 --> 00:04:33,040
the AI is that it can learn so much about people. So, the question is how can we navigate through

39
00:04:33,040 --> 00:04:38,320
that, that I have a powerful tool that is able to learn a lot of things about me that can be helpful,

40
00:04:38,320 --> 00:04:44,160
right, and be good, can help me diagnose cancer, but can also infer whether I'm gay or not,

41
00:04:44,160 --> 00:04:52,560
or whether I've voted in the last election, or whether I'm a woman, or whether what my

42
00:04:52,560 --> 00:04:58,240
sexual orientation is, right. So, the question is what needs to be done from a data protection issue,

43
00:04:58,240 --> 00:05:03,680
and then the last one I have to deal with both bias and discrimination, and again,

44
00:05:03,680 --> 00:05:09,680
this is prevalent regardless of country and regardless of sector where I deployed,

45
00:05:09,680 --> 00:05:16,960
because the data that we collect is, unfortunately, most instances not fair. Reason being because

46
00:05:16,960 --> 00:05:23,440
the world is not fair. It's not fair in any part of the world, fully at least, and it's usually not

47
00:05:23,440 --> 00:05:29,760
fair, fully fair in any of the sectors where AI is being used. So, the bias will be inherited

48
00:05:31,200 --> 00:05:36,000
when you train the algorithms on it, and just think about all the different types of

49
00:05:36,000 --> 00:05:42,240
sectors where we use them, right. We use them in education, we use them in traditional sector,

50
00:05:42,240 --> 00:05:49,200
we use them in employment, those are all areas where we know that biases exist, so it is no

51
00:05:49,200 --> 00:05:54,800
surprise that we're just going to transport human biases into algorithmic biases. So, regardless

52
00:05:54,800 --> 00:06:00,320
of where you are in the world, regardless of what you're using the technology for, you need to

53
00:06:00,320 --> 00:06:06,480
think about those things, and those are the three areas that I am trying to make an effort to come

54
00:06:06,480 --> 00:06:12,000
up with a contribution at least. Nice, nice. Well, let's maybe explore those in turn

55
00:06:13,280 --> 00:06:22,160
in talking about the black box challenge. We talked about that quite a bit on the podcast,

56
00:06:22,160 --> 00:06:30,080
the need for transparency to some degree, or explainability to some degree, often

57
00:06:30,080 --> 00:06:38,080
from the perspective of, yeah, I'm a business person, I'm relying on this algorithm to help me

58
00:06:38,080 --> 00:06:46,000
make a decision. I kind of, in order to develop a level of trust, I kind of want to know why it's

59
00:06:46,000 --> 00:06:55,280
recommending the thing that it's recommending. When you bring in the element of law and maybe

60
00:06:55,280 --> 00:07:01,600
more broadly regulation, how do the ways I need to think about that problem change?

61
00:07:02,480 --> 00:07:09,440
Right, so, I mean, you can do it with a brute force instrument and just say,

62
00:07:09,440 --> 00:07:15,680
I, as a regulator, I'm only going to allow algorithms that are explainable, period,

63
00:07:15,680 --> 00:07:20,960
exclamation point, and just live with those consequences, and that is something that has been

64
00:07:20,960 --> 00:07:27,440
discussed. And whenever this discussion comes up, then people will immediately say, hold on,

65
00:07:27,440 --> 00:07:32,640
hold on, hold on, you can do this because you cannot explain algorithms. So if you do that,

66
00:07:32,640 --> 00:07:40,560
then you let out banning them. That was the research, you know, there was a wisdom of the day when

67
00:07:40,560 --> 00:07:46,160
I entered the stage to get interested in this topic that it's just impossible. So when I thought

68
00:07:46,160 --> 00:07:53,280
about this a bit more, I came to the conclusion was, yes, there are probably two reasons why people

69
00:07:53,280 --> 00:07:58,800
don't want to give you an explanation. Right, the first is because they can't, and the second is

70
00:07:58,800 --> 00:08:05,760
because they don't want to. And those are two different topics. One is definitely because they

71
00:08:05,760 --> 00:08:11,760
don't want you, one to, and that has to do with trade secrets that they could tell you how

72
00:08:11,760 --> 00:08:18,480
an algorithm works because it's not so complex that you wouldn't know. So you could actually tell

73
00:08:18,480 --> 00:08:23,280
somebody why somebody has to go to prison because the algorithm is not that complex. So that's

74
00:08:23,280 --> 00:08:28,400
something where the law can come in and say, well, you know, just open up that black box

75
00:08:29,120 --> 00:08:35,200
because we know the answer. It's just a question of trying to balance the interests of business

76
00:08:35,200 --> 00:08:40,400
and the interests of the wider community in our society. So there is something that the law could do.

77
00:08:40,400 --> 00:08:46,720
And I recommend that for example, to have like a trusted third party that could have access to

78
00:08:46,720 --> 00:08:51,520
that doesn't have to be spread everywhere. The more challenging, I don't want to give you an

79
00:08:51,520 --> 00:08:57,760
explanation part is there because I can't part, which is not even the person writing a code

80
00:08:57,760 --> 00:09:02,880
allegedly does fully understand what's going on. So even if they want it to, they can't. And

81
00:09:02,880 --> 00:09:07,840
that's the more challenging problem because the law doesn't really have an answer there yet

82
00:09:07,840 --> 00:09:16,880
because some things are unfortunately not explainable, not even to an full extent to the people

83
00:09:16,880 --> 00:09:26,960
writing a code. However, I don't usually take no for an answer. So I figured I tried to

84
00:09:26,960 --> 00:09:32,000
explore if that's actually fully true. And I teamed up with two other people. One of them is

85
00:09:32,000 --> 00:09:37,360
Brent Middlestedt was an ethicist and Chris Russell works in machine learning and we wrote a paper

86
00:09:37,360 --> 00:09:43,760
that is called counterfactual explanations without opening the black box. So what we try to do there

87
00:09:43,760 --> 00:09:49,680
is try to take all those concerns on board and say, okay, is there a way to understand what's

88
00:09:49,680 --> 00:09:58,320
going on inside of a black box without fully understanding the black box, right? And we came

89
00:09:58,320 --> 00:10:05,120
up with counterfactual explanations as a way to do that because we saw it from the view of the

90
00:10:05,120 --> 00:10:09,920
person who wants to have an explanation, right? I have to go to prison and I didn't get the job,

91
00:10:09,920 --> 00:10:18,480
I wasn't promoted. The thing that I want is not a full-fledged code explanation. What I want to

92
00:10:18,480 --> 00:10:23,040
know is why the hell didn't I get the promotion and what do I need to do different to get the

93
00:10:23,040 --> 00:10:28,720
promotion? That's the thing I'm actually after, right? If you fire me and you give me a piece of

94
00:10:28,720 --> 00:10:34,000
code in my hand, I'm going to be very angry at you actually and feel like you haven't listened to me.

95
00:10:34,000 --> 00:10:40,000
And that's exactly how explanation usually works in human settings. I want to know the criteria,

96
00:10:40,000 --> 00:10:45,600
the reasons why I didn't happen and what I need to do differently. And luckily, that type of

97
00:10:45,600 --> 00:10:51,600
reasoning is something that you can code. So we call it counterfactual explanations in human

98
00:10:51,600 --> 00:10:58,240
settings, but in code, you can also generate a counterfactual. So where you just have a very

99
00:10:58,240 --> 00:11:04,640
complex system, one that you might not be fully to understand why, but I can tell you why in this

100
00:11:04,640 --> 00:11:10,720
particular case the decision was made in a certain way. So if you apply for a loan, for example,

101
00:11:10,720 --> 00:11:17,120
a counterfactual will tell you you were denied the loan because your income was 30,000 pounds.

102
00:11:17,120 --> 00:11:22,720
If it had been 35,000 pounds would have given you the loan, right? So you get the most important

103
00:11:22,720 --> 00:11:28,400
criteria and it tells you something what needs to be done to change the result. It gives you

104
00:11:28,400 --> 00:11:35,040
grounds to contestation, all that good stuff, right? Without having to understand the complexity of

105
00:11:35,040 --> 00:11:41,120
the full code. And that was a way to find a middle ground there where we can do something

106
00:11:41,120 --> 00:11:47,440
in a way. And that's done something that the law could require you to do. And that was actually

107
00:11:47,440 --> 00:11:52,880
quite exciting. So we wrote that paper a couple of years back. And Google came across our work

108
00:11:53,760 --> 00:12:00,000
interestingly enough. And they implemented an intensive flow and later on they implemented it

109
00:12:00,000 --> 00:12:06,000
in Google Cloud as well. And then many other companies have followed suits such as

110
00:12:06,000 --> 00:12:12,880
Vodafone for example, which is amazing to see. So the things that we cooked up in our ivory tower

111
00:12:12,880 --> 00:12:20,400
basically were actually something that had a positive meaning for people working on a ground

112
00:12:20,400 --> 00:12:26,000
which is exciting. But anybody interested in the topic can can look at our paper and the code

113
00:12:26,000 --> 00:12:31,760
is free available. Everybody can can use the type of explanation if they wanted to. So it's

114
00:12:31,760 --> 00:12:36,960
freely accessible. But yeah, that is definitely a way to think about opening the black box

115
00:12:36,960 --> 00:12:42,560
in a meaningful way. That's awesome. Can you give us an overview of how those counterfactual

116
00:12:42,560 --> 00:12:53,520
explanations are created or generated? Yes. What you do is you try to find the closest

117
00:12:54,560 --> 00:13:00,400
and minimal changes to a current decision model that need to be taken in order to get the

118
00:13:00,400 --> 00:13:05,760
thing that you want. So you're not actually just asking how does the rationale of the algorithm

119
00:13:05,760 --> 00:13:11,120
work? What you're doing is, okay, what are the smallest possible changes that I need to do

120
00:13:11,120 --> 00:13:16,960
to get from point one to be. And the interesting thing is that I could give you multiple

121
00:13:16,960 --> 00:13:24,000
counterfactuals, right? I could tell you you didn't get admitted to law school because your reference

122
00:13:24,000 --> 00:13:30,480
letters were bad and or because your grades were too low or because you had typos in whatever,

123
00:13:30,480 --> 00:13:36,000
right? And then give you a ranking of that and then you can figure out what would be the most helpful

124
00:13:36,000 --> 00:13:41,120
for you because for some of us it's easier to change the spelling on the covered letter

125
00:13:41,120 --> 00:13:46,480
for others it would be easier to find better reference letters, right? So to give them whatever

126
00:13:46,480 --> 00:13:53,680
set of possible grounds to improve your current situation is a very, very good benefit of counterfactuals.

127
00:13:54,880 --> 00:14:00,560
So you're you have some data point that you want to

128
00:14:02,400 --> 00:14:08,080
generate these explanations for and so then you permute it in different dimensions to try to

129
00:14:08,080 --> 00:14:15,040
understand how the what different decisions the model might take and then you use that to create

130
00:14:15,040 --> 00:14:24,080
the explanations. Yes, exactly. Awesome, awesome. And then the second focus area for you is around

131
00:14:24,080 --> 00:14:31,600
data protection and it sounds like your work is exploring a fundamental question of, you know,

132
00:14:31,600 --> 00:14:42,000
is privacy still viable in this fully connected fully connected world with, you know,

133
00:14:42,000 --> 00:14:50,000
machines making predictions and accessing consumer data and the degree to which, you know,

134
00:14:50,000 --> 00:14:57,120
what our expectations should be and how we can protect data. And tell us a little bit more about

135
00:14:57,120 --> 00:15:05,280
that area and some of your work there. Yes, certainly. That is a topic very close to my heart.

136
00:15:05,280 --> 00:15:12,000
And when we when why do we need to care about I said it already a little bit, but just to give

137
00:15:12,480 --> 00:15:18,880
people a little bit of a scary landscape of what's actually out there is that I don't think we

138
00:15:18,880 --> 00:15:26,880
really fully understand how good those algorithms are to protect very, very sensitive and intimate

139
00:15:26,880 --> 00:15:32,720
information about us that we might not want to share with anybody at all. And they can do that

140
00:15:32,720 --> 00:15:40,320
from very seemingly neutral data. So if I use my search engine, for example, Bing,

141
00:15:40,320 --> 00:15:47,760
they are able to infer based on how I move my mouse what I have Alzheimer's disease. Are you aware

142
00:15:47,760 --> 00:15:53,200
of that? Are you aware that you are giving that health information away to the outside world?

143
00:15:53,200 --> 00:15:59,680
Are you aware that Twitter can infer whether you have depression just based on what you treat online?

144
00:15:59,680 --> 00:16:08,080
Are you aware that to be clear when you give these examples, are you posing them as hypotheticals

145
00:16:08,080 --> 00:16:14,160
Bing could? No, no, they have published a paper that they can do that. So that's what I'm trying

146
00:16:14,160 --> 00:16:18,000
to say. This is not a hypothetical something that I cook up in the ivory tower. That's

147
00:16:18,640 --> 00:16:28,960
fact they can do that. One of the most problematic ones is definitely what happened with Facebook

148
00:16:28,960 --> 00:16:34,640
where they're able to. And again, there's research that shows that able to infer

149
00:16:35,360 --> 00:16:45,680
sexual orientation, ethnicity, gender, ability, without any of their users identifying with any

150
00:16:45,680 --> 00:16:53,280
of those classes and groups just based on what they click on, what they like, what they post,

151
00:16:53,280 --> 00:17:00,160
and who their friends are. They can have that information and then use it in a way where they

152
00:17:00,160 --> 00:17:05,200
for example allow advertisers to exclude them from seeing certain products. And this is something

153
00:17:05,200 --> 00:17:12,400
that they did for example. So they would in the US infer that somebody was for example black

154
00:17:12,400 --> 00:17:20,000
and would allow advertisers to exclude them from seeing job offers and ads for housing

155
00:17:20,000 --> 00:17:27,040
and financial services. So it's not just a theoretical problem that you should not know those

156
00:17:27,040 --> 00:17:33,840
things about me unless I say it's okay, but it also actually has negative consequences

157
00:17:34,480 --> 00:17:39,680
because it can be used against you in a way where you don't know about it. So we've all

158
00:17:39,680 --> 00:17:44,560
of that. And again, so it's just three examples that show how powerful those algorithms are.

159
00:17:44,560 --> 00:17:48,960
The question is, well, I mean, we have the protection law. Why not just apply data protection

160
00:17:48,960 --> 00:17:56,080
law to all of those problems. And the issue here is at least the one that I, you know, I've

161
00:17:56,080 --> 00:18:00,960
been currently working on in a, or I've been working on a research project in a paper that I wrote,

162
00:18:00,960 --> 00:18:09,120
which is called a right to reasonable inferences, is that I show that the current data protection law

163
00:18:09,120 --> 00:18:19,840
was designed in a way without fully anticipating the power of AI. And therefore it was designed

164
00:18:19,840 --> 00:18:27,760
in a way in a very almost 20 century way of thinking about privacy. In the sense that, you know,

165
00:18:27,760 --> 00:18:34,320
20 century privacy is I'm in my home and I don't like my neighbor. He's very nosy and he keeps,

166
00:18:34,320 --> 00:18:38,640
you know, coming over to the fence and he's looking at me and he says, oh, Sandra is again

167
00:18:38,640 --> 00:18:48,000
eating ice cream before noon. Oh, my God, right. She has no self control. Yeah. So she has no

168
00:18:48,000 --> 00:18:54,480
self control whatsoever, which is true. But that's the idea. So what is it that the law wants to

169
00:18:54,480 --> 00:19:01,280
do is that I want to prevent the noisy, the nosy neighbor from collecting information about me.

170
00:19:01,280 --> 00:19:07,760
So I give all the power to me. So you have to ask me first, right? And that is how the protection law

171
00:19:07,760 --> 00:19:13,680
works is a lot of like it's based on consent or transparency where somebody needs to tell you,

172
00:19:13,680 --> 00:19:19,040
be aware, be aware, I'm taking this data from you. Because in a human setting, that's a dangerous

173
00:19:19,040 --> 00:19:23,680
part, you know, they are seeing something and I can anticipate what they know, right? I'm sitting

174
00:19:23,680 --> 00:19:29,040
in my yard eating ice cream. That's the information that the neighbor now has about me, right?

175
00:19:29,760 --> 00:19:36,160
But this is why all of the data protection law focuses on. With algorithm, collecting information

176
00:19:36,160 --> 00:19:42,880
about me being ice cream is the first step, right? That's not the thing that they actually after.

177
00:19:42,880 --> 00:19:48,720
They're interested in what they can infer based on my eating habits, right? And that's the question.

178
00:19:48,720 --> 00:19:53,520
That's something that my neighbor couldn't do. An algorithm now might be able to infer that I

179
00:19:53,520 --> 00:19:58,160
might have a higher chance of getting diabetes at some point, which is something my neighbor couldn't

180
00:19:58,160 --> 00:20:03,920
do, right? But an algorithm can. So the interesting or more interesting, more dangerous being actually

181
00:20:03,920 --> 00:20:09,840
happens after data is being collected. So everything that is being inferred, but the law doesn't really

182
00:20:09,840 --> 00:20:16,080
care about that so much because the law still fought the data collection part is the most dangerous

183
00:20:16,080 --> 00:20:23,520
thing. So I've wrote a very, very long paper. I don't know, 150 pages just to show that that's

184
00:20:23,520 --> 00:20:29,440
a problem because the data protection law focuses too much on the input side of things, collecting

185
00:20:29,440 --> 00:20:35,040
the data, taking data from you and not so much on the output stage, which is what can I learn about

186
00:20:35,040 --> 00:20:40,320
you and that new laws actually need to govern the outputs rather than just the inputs.

187
00:20:41,440 --> 00:20:49,040
I'm curious about the length of the paper. It sounds obvious and clear when you,

188
00:20:49,920 --> 00:20:55,360
you know, you made a very simple and clear argument for this and, you know, I'm totally

189
00:20:55,360 --> 00:21:02,640
bought in. Why do you need 150 papers to, who are you trying to convince and what is it that they

190
00:21:02,640 --> 00:21:12,160
needed that you had to build out this 150 page argument? Yes, because it's really, I don't think

191
00:21:12,160 --> 00:21:21,680
people have looked in complete detail how it's actually being regulated. And I just went through

192
00:21:21,680 --> 00:21:29,600
with a magnifying glass to show that inferential analytics, unfortunately, or inferential data

193
00:21:30,160 --> 00:21:36,000
has almost to no protection. And I showed that in the case law as well, which is something that

194
00:21:36,000 --> 00:21:40,800
wasn't really looked at. Also, I could do with the fact that I was talking about the generative

195
00:21:40,800 --> 00:21:49,520
protection regulation, which at that point was really new too, right? So there, you know, I,

196
00:21:49,520 --> 00:21:54,480
and your framework came out and explaining this new thing that no one understood.

197
00:21:54,480 --> 00:22:00,640
Yes, yes, right. So that was important because it was very, I think,

198
00:22:02,720 --> 00:22:07,120
important to get the message across how really, really

199
00:22:08,000 --> 00:22:12,400
propriomatic that is, which I think up until this point wasn't really clear.

200
00:22:14,160 --> 00:22:18,480
So yes, it's, and I just want to, that's what I usually do when I see there's a problem

201
00:22:18,480 --> 00:22:22,960
that I'm not going to give you just one example. I'm giving you like all the examples. So you

202
00:22:22,960 --> 00:22:27,440
with me and, and can understand that you really need to care at this point and something needs to

203
00:22:27,440 --> 00:22:35,680
be done. So I know GDPR expert, but my sense is that GDPR didn't really, this wasn't really one of

204
00:22:35,680 --> 00:22:42,000
the issues that GDPR was addressing or trying to address. Is that right? Yes, that's definitely

205
00:22:42,000 --> 00:22:47,520
also one of the problems. And this is why data protection law was so much designed in the way to

206
00:22:47,520 --> 00:22:57,520
just keep the noisy, the noisy neighbor out, right? The, the, what actually was capable of doing

207
00:22:58,080 --> 00:23:06,080
happened so much later, right? If we're very honest, the new GDPR that we have is to 80, 90 percent

208
00:23:06,080 --> 00:23:12,640
the same stuff that we had in the data protection directive and that framework is from the 90s,

209
00:23:12,640 --> 00:23:20,960
right? So there are updates there. Yes, but the core mechanisms and the core assumptions and

210
00:23:20,960 --> 00:23:25,280
the thing that it's supposed to be doing hasn't really changed. This is not to say that it should

211
00:23:25,280 --> 00:23:30,320
necessarily regulate AI, but it's just to say that it was never designed to regulate AI

212
00:23:31,360 --> 00:23:36,640
in a way. And therefore it's failing. And I think a lot of people had hope that this new framework

213
00:23:36,640 --> 00:23:42,000
will be visible to all the problems. And I just wanted to say, no, I think there's still a lot of work

214
00:23:42,000 --> 00:23:47,360
to be done and we need to be very, very careful to do this right because there's so much on the line.

215
00:23:49,360 --> 00:23:54,080
You mentioned regulating AI. It's not even regulating AI as much as regulating data in the

216
00:23:54,080 --> 00:23:59,520
age of AI. Yes, definitely, definitely. Yes. So even, even for it to remove from that.

217
00:23:59,520 --> 00:24:09,360
Yeah. Is there a, is there a locale or a regulatory framework that you think

218
00:24:09,360 --> 00:24:20,000
does a good job with the issues that you've described? Are we there yet? No, I don't think we're there yet

219
00:24:21,120 --> 00:24:28,720
because I think it touches so many things at the same time that if you want to come up with one

220
00:24:28,720 --> 00:24:34,720
framework, it will need to do a lot of work. I think what probably would be more helpful is that

221
00:24:34,720 --> 00:24:40,640
to rethink our regulation should work in the future anyway, that you just don't

222
00:24:40,640 --> 00:24:46,960
silo data protection in one corner without thinking about competition law and without thinking

223
00:24:46,960 --> 00:24:51,600
about non-discrimination law and without thinking about protection law because all those things

224
00:24:51,600 --> 00:24:58,560
are very much interconnected. So one law that governs everything, I'm not sure if that's useful

225
00:24:58,560 --> 00:25:05,600
or necessary, I think what's very useful and very necessary is that different types of regulators

226
00:25:06,800 --> 00:25:13,760
start to collaborate more closely because AI kind of puts them in the same room anyway,

227
00:25:13,760 --> 00:25:19,040
right? And the data flow does that anyway. So I think that's a better way of thinking about

228
00:25:19,040 --> 00:25:26,240
regulation is that almost every regulatory aspect or every sectoral law that we have will be

229
00:25:26,240 --> 00:25:33,760
touched by some type of technology at some point. So regulation is only one approach to

230
00:25:34,880 --> 00:25:40,880
ensuring good behavior. There's also kind of self-regulation or industry consortia

231
00:25:41,520 --> 00:25:56,160
or the like. Are there any examples of folks that you can point to that have taken a responsible

232
00:25:56,160 --> 00:26:03,360
approach to these issues or are particularly transparent? I'm just wondering if there's a good

233
00:26:03,360 --> 00:26:17,520
example or reference model that has emerged for say, I guess I'm envisioning an enhanced

234
00:26:19,120 --> 00:26:24,960
data use statement or something like that that talks not only about these are the organizations

235
00:26:24,960 --> 00:26:29,360
with which we share data but these are the derivative products that are created and this is how

236
00:26:29,360 --> 00:26:41,600
we use and or share that information. Is anyone doing that? Yes, I think everyone is doing that

237
00:26:41,600 --> 00:26:46,240
and I think that's a little bit also after the problem that everybody is doing that.

238
00:26:47,600 --> 00:26:51,360
Doing the creating of the derivative products and sharing them or

239
00:26:51,360 --> 00:27:00,720
not creating codes of conduct and guidelines and best practices and standards and all of that

240
00:27:00,720 --> 00:27:07,520
is being created everywhere. It's created by governments, by industry, by NGOs everywhere and

241
00:27:07,520 --> 00:27:18,080
anywhere. My colleague, Brent Smith, said he wrote a paper in nature I think which was called

242
00:27:18,080 --> 00:27:23,040
white principles alone and not enough for something along those lines and I think at the point

243
00:27:23,040 --> 00:27:26,960
when he wrote that piece and that's also I think for probably two years old at this point,

244
00:27:26,960 --> 00:27:34,640
he said that there are 150 different guidelines best practices and standards out there

245
00:27:35,680 --> 00:27:42,720
and they're all roughly the same and he goes through for them all and just points out how

246
00:27:42,720 --> 00:27:50,080
they are still lacking the thing which is being applied in practice with having a good feedback

247
00:27:50,080 --> 00:27:59,360
look how well they actually work in practice. I'm all for responsible innovation and research

248
00:27:59,360 --> 00:28:05,440
and trying to come up with best practices and I think that's absolutely needed but I think that

249
00:28:05,440 --> 00:28:11,520
part is now over. I think the interesting part is now to figure out is anybody actually deploying

250
00:28:11,520 --> 00:28:16,960
them in practice, how good are they, what kind of oversight mechanisms are there. It's great to

251
00:28:16,960 --> 00:28:22,480
have five wonderful ethical principles if you don't tell me how you actually operationalizing them

252
00:28:22,480 --> 00:28:32,800
then I don't necessarily think the job's done yet. From the perspective of concerned parties,

253
00:28:32,800 --> 00:28:41,120
academia, what does the landscape look like in trying to address these issues? There's certainly

254
00:28:41,120 --> 00:28:45,600
papers like yours where you identify the issue and show that it's not being addressed,

255
00:28:46,320 --> 00:28:54,640
what are the steps that we can build upon that to get to something that is a better place?

256
00:28:54,640 --> 00:29:03,440
Yes, I think that I mean that's obviously a biased view here but I think that academia actually

257
00:29:03,440 --> 00:29:10,560
does have a very big role to play and I'm very happy to say that academia has played a very vocal

258
00:29:10,560 --> 00:29:17,520
and important role of the last years. I think if academia hadn't been as loud and persistent,

259
00:29:17,520 --> 00:29:21,440
a lot of things would have not have changed. So the questions around a river of my

260
00:29:21,440 --> 00:29:28,640
accountability and privacy protections have been front and center on a lot of agenda of people

261
00:29:28,640 --> 00:29:33,680
in the field and have they have not been so persistently. I don't think that so much would change

262
00:29:33,680 --> 00:29:40,080
at the moment so definitely that. And in that I think one of the reasons again is why

263
00:29:40,080 --> 00:29:47,360
many of them have been so powerful and influential is because they very often work with people

264
00:29:47,360 --> 00:29:53,360
that are not from the same discipline than they are and I think that's the key thing here because

265
00:29:54,400 --> 00:29:57,200
I can just speak for myself. I think none of my papers

266
00:30:00,320 --> 00:30:06,160
I could have not done it by myself and I was very important to have an emphasis in a machine

267
00:30:06,160 --> 00:30:14,880
learning person on there to teach me and have them also endure my teachings and I think that

268
00:30:14,880 --> 00:30:20,000
made the whole work stronger and I think that's really what if you are thinking about how to

269
00:30:20,000 --> 00:30:26,880
govern things I think you owe it that you try to look at the issue from as many as perspective as

270
00:30:26,880 --> 00:30:34,320
possible. And then the third pillar of your research is focused on bias fairness and discrimination

271
00:30:34,320 --> 00:30:40,080
with regard to the law. It sounds like this is your most recent work and what you're most excited

272
00:30:40,080 --> 00:30:47,600
about right now. Tell us a little bit more about that area. Yes definitely definitely extremely

273
00:30:47,600 --> 00:30:55,200
excited about that. I think everybody will know as I said that you know bias and unfairness is

274
00:30:55,200 --> 00:31:00,320
always an issue when we think about data and algorithms because unless you collecting them in

275
00:31:00,320 --> 00:31:07,440
utopia she said fair fair chance that the data would bias in some way and that's the reality that

276
00:31:07,440 --> 00:31:14,880
we have to deal with. I think this being about kind of the fundamental premise of machine learning

277
00:31:14,880 --> 00:31:20,640
that you know we're training based on you know information we've collected about the past and

278
00:31:20,640 --> 00:31:26,480
this decisions that were made in the past and the mechanism kind of fundamentally carries

279
00:31:26,480 --> 00:31:32,640
forth biases from the past and to the future if there's not extreme care taken. Yes absolutely

280
00:31:32,640 --> 00:31:38,080
I should have said that yes I mean that's as you said that's basically how all machine learning

281
00:31:38,080 --> 00:31:43,360
works is looking at the past trying to predict the future right you feed the algorithm with a bunch

282
00:31:43,360 --> 00:31:49,840
of historical data for example who has been hired who has gotten insurance who was sent to prison

283
00:31:49,840 --> 00:31:58,320
who did reoffend who did get sick right and you train the model based on that because you think

284
00:31:58,320 --> 00:32:02,560
that you have some ground truth because you have historical data you know if somebody actually

285
00:32:02,560 --> 00:32:07,200
got sick right you know if somebody did well in law school you know if they're reoffended and if

286
00:32:07,200 --> 00:32:15,040
somebody that looks similar like the person right is now applying for the job is now applying to

287
00:32:15,040 --> 00:32:22,800
be left out on parole is now wanting to be promoted if they look like similar people that you know

288
00:32:22,800 --> 00:32:28,720
reoffended or dig well on the job then you give them the same chance because you assume

289
00:32:28,720 --> 00:32:36,320
that similar patterns will emerge right and that's all great again if we tend to make good decisions

290
00:32:36,320 --> 00:32:44,400
fair and just decisions that are accurate but if you're very honest very often that is not the case

291
00:32:44,400 --> 00:32:51,200
so unless you're very very careful you will just reinforce the biases and injustices that we had

292
00:32:51,200 --> 00:32:59,120
in human decision making but at much greater speed and much less detectable and so I got interested

293
00:32:59,120 --> 00:33:04,480
in this topic as well and the first question that I always ask myself is like well is the law

294
00:33:04,480 --> 00:33:10,960
sleeping why not just use the law to solve that problem and I got interested in the question of

295
00:33:10,960 --> 00:33:16,960
honest remediation law because that's the closest the most sensible law to look at and

296
00:33:18,160 --> 00:33:24,480
give you wrote in two papers in the past one is called why fairness cannot be automated which

297
00:33:24,480 --> 00:33:32,480
whole what he tells you how I felt about whether this is possible and and what we what we did

298
00:33:32,480 --> 00:33:38,560
there was quite similar to to previous work where we showed that the law just wasn't designed in

299
00:33:38,560 --> 00:33:44,880
a way to govern algorithms it was designed to govern people right so if you think about a

300
00:33:44,880 --> 00:33:50,560
discrimination setting a discrimination setting a traditional one is where you know somebody

301
00:33:51,280 --> 00:33:57,520
indirectly directly is not giving you the job because you're a woman or they harass you because

302
00:33:57,520 --> 00:34:03,520
of your religious beliefs or you are in a hostile environment where other things are going on that

303
00:34:03,520 --> 00:34:08,320
prevent you from succeeding the basic point is you know that something's off right so you bring

304
00:34:08,320 --> 00:34:14,160
a complaint and the discrimination law will will help you with that so with algorithms again it's

305
00:34:14,160 --> 00:34:23,040
different because they discriminate behind your back without you actually being aware so a complaint

306
00:34:23,040 --> 00:34:29,040
based system such as non discrimination law is completely powerless if the person doesn't know

307
00:34:29,040 --> 00:34:34,400
that they have been wronged and again that's not a failing of the law per se and sort of failing

308
00:34:34,400 --> 00:34:40,320
of the algorithm it's just a very unhappy mismatch of the two because again the law was designed for

309
00:34:40,320 --> 00:34:48,560
people not for for algorithms so but discrimination still occurs so what is it that I need to do

310
00:34:50,320 --> 00:34:55,840
which means we have to test and test and test because if I don't know somebody has to know

311
00:34:56,480 --> 00:35:02,960
and the problem is you can only know if you test for it because and that's the second problem very

312
00:35:02,960 --> 00:35:10,880
often you might not even know that the data that you have collected is biased or unfair towards certain

313
00:35:10,880 --> 00:35:16,880
people because again here the intuition kind of rakes down and on discrimination law was very much

314
00:35:16,880 --> 00:35:22,720
based on intuition a judge looks at the case and says oh what you bent head scars from the workplace

315
00:35:22,720 --> 00:35:28,160
that's a problem if we're religion you don't need much data to make that point because there is

316
00:35:28,160 --> 00:35:34,640
a clear understanding of the social reality or the social symbolism of head scarves and religion

317
00:35:34,640 --> 00:35:42,560
that's not much you need to do that right what but what if I you know got fired because I don't have

318
00:35:42,560 --> 00:35:52,080
a dog right that sounds maybe odd but do I know that that correlates with ethnicity or gender

319
00:35:52,080 --> 00:35:58,000
sexual orientation versus belief I really don't know anymore right so how am I supposed to bring a

320
00:35:58,000 --> 00:36:06,000
case in court if you're using data where my social gut doesn't ring alarm bells anymore right

321
00:36:07,440 --> 00:36:13,120
so those two things that I might not know about it and that even if I know about it that

322
00:36:14,720 --> 00:36:20,160
I have almost no way of proving it means that fairness cannot be optimal to my defensive

323
00:36:20,160 --> 00:36:25,760
favorite title so we thought okay then you need to test you need to test test somebody needs to

324
00:36:25,760 --> 00:36:35,440
test because otherwise you wouldn't know so we we came up with with a biased test that lets you

325
00:36:36,880 --> 00:36:44,160
do that so the test is called um demographic conditional demographic disparity CDD

326
00:36:44,160 --> 00:36:54,880
and we chose that test because it aligns the most with European nondiscrimination law in a way

327
00:36:54,880 --> 00:37:09,040
that other tests do not and yeah this year in January of February Amazon has came across our work

328
00:37:09,040 --> 00:37:15,600
um and that bias test and they found interesting and they have decided to implement it in their own

329
00:37:16,400 --> 00:37:26,080
bias toolkit um so say to make a clarify so now customers of Amazon can use that test as well

330
00:37:26,080 --> 00:37:31,600
but again as with all of our research it's publicly available so if anybody's interested in that

331
00:37:31,600 --> 00:37:36,560
having a closer look or in the court or whatever it's free and publicly available as well

332
00:37:36,560 --> 00:37:43,920
and so can you talk a little bit about what differentiates this conditional demographic disparity CDD

333
00:37:44,560 --> 00:37:53,200
tests with the tens or potentially hundreds of other statistical tests that have been used as

334
00:37:53,200 --> 00:38:00,640
metrics of bias in data sets yes certainly so um that's actually quite exciting and that's

335
00:38:00,640 --> 00:38:07,920
the the second paper that complements the the first one uh we also wrote a paper which is called

336
00:38:07,920 --> 00:38:14,080
bias preservation and machine learning and the legality of fairness metrics nondiscrimination law

337
00:38:14,080 --> 00:38:20,960
so exactly uh the question that you uh just asked me um so what we did there is we looked at

338
00:38:21,840 --> 00:38:27,360
uh 20 different fairness tests and um we came up with the classification system

339
00:38:27,360 --> 00:38:36,560
on how they how they make decisions um the one category is um called bias preserving bias tests

340
00:38:36,560 --> 00:38:43,600
and the other one is called bias preserving uh bias transforming fairness tests so what we looked

341
00:38:43,600 --> 00:38:54,400
at is that um the majority of them so 13 out of 20 uh bias preserving so what they do is they look

342
00:38:54,400 --> 00:39:00,320
at the error rates to measure fairness so they want to make sure that um whatever type of decision

343
00:39:00,320 --> 00:39:08,240
has been made and is now being made has the same uh base rate for errors the other ones the other

344
00:39:08,240 --> 00:39:15,200
seven are more looking at decision rates when they're looking at how the outcome is distributed

345
00:39:15,200 --> 00:39:24,400
across certain groups right so that's that is the main contribution coming up with that distinction

346
00:39:24,400 --> 00:39:32,160
and trying to tease out the underlying assumption of this right one bucket says um as long

347
00:39:32,160 --> 00:39:40,000
as we're not making things worse than they used to be i give my fairness check and it's okay

348
00:39:40,000 --> 00:39:47,920
that bias transforming metrics that look at the decision rates say i'm only happy if equal outcomes

349
00:39:48,560 --> 00:39:54,800
are happening across groups right so this is the underlying assumption this is fine

350
00:39:55,440 --> 00:40:03,840
unless you look at what european on the scrimination law wants to do non-scrimination law in europe

351
00:40:03,840 --> 00:40:11,440
is not just about formal equality as in do not actively treat somebody differently because

352
00:40:11,440 --> 00:40:17,440
of their race or gender or sexual orientation right which is more like a negative form as

353
00:40:17,440 --> 00:40:25,280
in passive form of discrimination law um non-scrimination line europe is much more about

354
00:40:25,280 --> 00:40:34,640
substantive equality which is more about actively dismantling inequality keeping things as they are

355
00:40:34,640 --> 00:40:40,720
is not good enough in europe so you're supposed to take an active role as much as you can both the

356
00:40:40,720 --> 00:40:48,640
pride of the public sector to actually make the world a fairer place right and the majority of

357
00:40:48,640 --> 00:40:56,400
those fairness tests don't do that because they condition on an unequal status quo and they freeze

358
00:40:56,400 --> 00:41:02,640
that and that is the main problem and with our fairness test and with other data bias transforming

359
00:41:02,640 --> 00:41:08,880
you could counterbeam that at least would give the opportunity to actually uh make it better

360
00:41:08,880 --> 00:41:22,160
got it got it and so at least for those whose problem is uh specifically covered under the regime

361
00:41:22,160 --> 00:41:28,080
of the european on discrimination tests this cdds the only test the only fairness test that

362
00:41:28,880 --> 00:41:36,400
uh is adequate not not the only one like anything that is bias transforming and there are seven

363
00:41:36,400 --> 00:41:43,200
others that are also uh bias transforming um that you can use you could use those definitely

364
00:41:43,200 --> 00:41:48,000
those are absolutely fine to use and we pointed out in the papers and we listed exactly so

365
00:41:48,560 --> 00:41:56,000
enough other tests that you could use and also it is not to say that you cannot use bias preserving

366
00:41:56,000 --> 00:42:04,400
metrics completely in europe right it just depends on what the context is if you're using bias

367
00:42:04,400 --> 00:42:11,280
if you're making decisions life changing decisions about people in europe in a sector that is

368
00:42:11,280 --> 00:42:19,680
a protected and that is be known to exhibit bias then you should use a bias transforming metrics

369
00:42:19,680 --> 00:42:25,680
right um because it at least gives you the ability to make something better than it used to be

370
00:42:25,680 --> 00:42:31,280
it doesn't have to be you don't have to make it better but you need to at least justify it in a way

371
00:42:31,280 --> 00:42:38,880
however you could still use a bias preserving metrics um and either justify that as well but I think

372
00:42:38,880 --> 00:42:44,000
it's difficult but there are legitimate areas where you can use it but you don't have a problem so

373
00:42:44,000 --> 00:42:51,360
if you use it for research only where it doesn't affect people absolutely fine to use bias preserving

374
00:42:52,080 --> 00:42:59,760
in europe as well if you are unsure of what a good outcome would actually look like it's much

375
00:42:59,760 --> 00:43:05,040
better to keep things as they are than making them worse right if you don't have a normative idea

376
00:43:05,040 --> 00:43:12,640
of how things ought to be then you can use them as well you can also use them if there are situation

377
00:43:12,640 --> 00:43:20,000
where we have you know justified bias or even desired bias if we wanted that no one to preserve

378
00:43:20,000 --> 00:43:26,160
that that is fine to use as well and then areas where you do actually have ground truth where there

379
00:43:26,160 --> 00:43:32,000
is no bias in the data set you can also use bias preserving metrics so there's a whole range

380
00:43:32,000 --> 00:43:39,600
where you can justify we use it the only problem is that if you're making life changing decisions

381
00:43:39,600 --> 00:43:46,320
about people in a protected sector and that sector is known to exhibit certain biases

382
00:43:47,280 --> 00:43:53,760
then the preference would be it be easier for you from a legal perspective to use bias transforming

383
00:43:53,760 --> 00:43:59,760
ones because at least they offer the possibility of making things better they used to be

384
00:44:00,880 --> 00:44:07,440
so the the first contribution of this paper is this drawing the distinction between bias preserving

385
00:44:07,440 --> 00:44:19,280
and bias transforming and correlating those two classes of fairness metrics or bias metrics to

386
00:44:19,280 --> 00:44:27,120
the European non discrimination law but then you're also proposing this new test how does

387
00:44:27,120 --> 00:44:34,560
how is that new test advantage relative to the class of bias transforming metrics that you know

388
00:44:34,560 --> 00:44:40,800
can work under the European law what you really need to do and what a bias test should help you do

389
00:44:40,800 --> 00:44:48,880
is reflect on what you are able to do in order to make a positive contribution the bias test is

390
00:44:48,880 --> 00:44:56,160
not supposed to tell you what's right or wrong it just tell it should just tell you that something

391
00:44:56,160 --> 00:45:01,840
might be a problem it's supposed to act as an alarm system so what you should be doing is that

392
00:45:01,840 --> 00:45:08,880
you run let's say you you you thinking about loan decisions that you run your algorithm that

393
00:45:08,880 --> 00:45:16,080
is distributing loan decisions and it will tell you or did you know that your current deployment

394
00:45:16,080 --> 00:45:22,960
doesn't give black people any loans right and then it will tell you what kind of conditions

395
00:45:23,520 --> 00:45:31,120
criteria and variables were conditioned on and what that helps to do is to have an informed

396
00:45:31,120 --> 00:45:37,440
discussion of whether or not certain biases are justified or not because again in Europe on

397
00:45:37,440 --> 00:45:44,400
the non discrimination law not everything needs to be fair what needs to happen is if something is

398
00:45:44,400 --> 00:45:51,440
unfair you need to tell me why yeah and that is the thing that the test helps you to do because

399
00:45:51,440 --> 00:45:57,280
let's go back to the bank example you could say for example what we use income to make decisions

400
00:45:57,840 --> 00:46:02,320
on what does somebody should get a loan which makes intuitive little sense right and you could

401
00:46:03,120 --> 00:46:07,840
put those criteria conditioning on input in there and it will tell you oh

402
00:46:08,880 --> 00:46:14,000
hardly any women are getting loans is that on purpose right and then you could ask the question

403
00:46:14,000 --> 00:46:19,200
okay I'm conditioning on on salary and it has a negative effect on gender is that justified

404
00:46:19,200 --> 00:46:24,480
because you could say well how can we even use income since we know about the race and gender

405
00:46:24,480 --> 00:46:29,600
pay gap right that's a horrible criteria to use in the first place right and say oh we're not

406
00:46:29,600 --> 00:46:35,840
gonna use that we're not gonna do that or you could say well yes we know about those inequalities

407
00:46:35,840 --> 00:46:41,440
but it's a very effective and defendable proxy um but are not somebody will be repairing

408
00:46:41,440 --> 00:46:48,480
the loan and actually putting people in a situation where they have to default will default

409
00:46:48,480 --> 00:46:53,680
and in depth a minute further it's also irresponsible so maybe we should be using income right

410
00:46:53,680 --> 00:47:01,360
or you could say well um okay income isn't great but how about those other criteria additional

411
00:47:01,360 --> 00:47:08,320
criteria that are also very good at predicting something but are not as disadvantaged

412
00:47:08,320 --> 00:47:14,320
um that's for example income that's the dialogue you need to have right and the test can help

413
00:47:14,320 --> 00:47:21,760
you to do that because it would lay open how your current decision system is affecting or is

414
00:47:21,760 --> 00:47:27,600
making decisions across groups where you could see does it actually make equal decisions across

415
00:47:27,600 --> 00:47:35,040
you know gender lines or lines with ethnicity or age or whatever and it tells you what kind of

416
00:47:35,040 --> 00:47:40,560
criteria we're conditioned on and then you can justify to yourself and ideally actually

417
00:47:40,560 --> 00:47:47,600
to the general public as to why those criteria are acceptable to use um even if they

418
00:47:48,640 --> 00:47:55,760
maybe end up in an unequal distribution if it is the only way to go about it right because

419
00:47:55,760 --> 00:48:05,200
that's the very inconvenient and unhappy discussion we need to have is what kind of disparity is

420
00:48:05,200 --> 00:48:12,480
acceptable and which is not and that biased test will let you do that. It sounds like

421
00:48:13,120 --> 00:48:21,520
and this is maybe a parent from the name that the the key thing that you the key innovation

422
00:48:21,520 --> 00:48:26,880
or contribution of this new test is that it makes explicit this conditioning on other factors

423
00:48:27,840 --> 00:48:33,200
something that's maybe you don't have the same degree of flexibility or the same mechanism to do

424
00:48:33,200 --> 00:48:42,320
that conditioning with some of the other tests. Yes exactly. Got it got it um and then uh you know

425
00:48:42,320 --> 00:48:51,360
so you've you've now got this test you've got this um you've identified the you know the relationship

426
00:48:51,360 --> 00:48:57,520
between these tests and and this particular set of regulations in Europe you know what are the

427
00:48:57,520 --> 00:49:07,280
next steps for you in pursuing this researcher or more broadly your interest in bias fairness

428
00:49:07,280 --> 00:49:14,240
and discrimination and law. Yes yeah fantastic question um I I'm definitely going to maintain

429
00:49:15,440 --> 00:49:22,560
an interest in in all those free areas going forward um I think one of the next topics that I

430
00:49:22,560 --> 00:49:30,640
will be um diving a bit more into is the new regulation that is looming at the horizon here in

431
00:49:30,640 --> 00:49:37,360
Europe because um there's a new draft that came out by the European Commission um the AI Act that

432
00:49:37,360 --> 00:49:46,080
is the first ever first attempt at a regulatory comprehensive regulatory framework to to govern that

433
00:49:46,080 --> 00:49:52,320
so there will be um I think a lot of work that not just me I think a lot of people will try to dive

434
00:49:52,320 --> 00:49:58,880
their teeth into to figure out um you know whether whether this is a good attempt and and what can be

435
00:49:58,880 --> 00:50:07,360
done there so I think that will remain a very active research area and the other ones that I'm

436
00:50:07,360 --> 00:50:17,760
definitely going to are areas of of health that I'm particularly interested in as well as um

437
00:50:17,760 --> 00:50:27,200
education and financial services. And those um those latter two areas health and well three health

438
00:50:27,200 --> 00:50:34,560
education and financial services uh still looking at the intersections of those with AI and the law

439
00:50:34,560 --> 00:50:43,120
or uh I think that I think those free areas will accompany definitely but um I will probably look

440
00:50:43,120 --> 00:50:49,680
at it more from like a power perspective and then and a and a broader regulatory landscape and

441
00:50:49,680 --> 00:50:55,840
oversight um landscape of that makes sense. Got it got it got it. Well Sandra thanks so much for

442
00:50:55,840 --> 00:51:03,280
taking the time to share a bit about what you are working on very interesting stuff and uh

443
00:51:03,280 --> 00:51:08,080
certainly enjoyed the conversation. Thank you so much it was great to be here thank you for the

444
00:51:08,080 --> 00:51:34,800
invitation.

