1
00:00:00,000 --> 00:00:13,400
Welcome to the Twimal AI Podcast.

2
00:00:13,400 --> 00:00:16,400
I'm your host, Sam Charrington.

3
00:00:16,400 --> 00:00:24,360
Hey, what's up everyone?

4
00:00:24,360 --> 00:00:28,640
Before we move on to today's show, I want to give you a heads up that we are back with

5
00:00:28,640 --> 00:00:34,560
another interactive podcast viewing party on Monday, May 18th.

6
00:00:34,560 --> 00:00:38,960
This time, I'll be joined by Emily Bender, Professor of Linguistics at the University

7
00:00:38,960 --> 00:00:40,680
of Washington.

8
00:00:40,680 --> 00:00:44,840
Our recent interview explored the question of whether linguistics has been missing from

9
00:00:44,840 --> 00:00:46,680
NLP research.

10
00:00:46,680 --> 00:00:52,240
We dig into questions like, would we be making more progress on more solid foundations if

11
00:00:52,240 --> 00:00:57,560
more linguists were involved in modern NLP or is the progress we're making, such as

12
00:00:57,560 --> 00:01:01,880
with deep learning models like Transformers, just fine.

13
00:01:01,880 --> 00:01:07,040
We also explore issues like fairness bias and ethics in NLP and more.

14
00:01:07,040 --> 00:01:12,040
Emily and I will be in the chat answering all of your questions about the interview.

15
00:01:12,040 --> 00:01:16,680
We'll begin the viewing party at 3 o'clock Pacific on Monday.

16
00:01:16,680 --> 00:01:22,720
You can head over to twimalai.com slash 376 viewing for more details or to add it

17
00:01:22,720 --> 00:01:24,520
to your calendar.

18
00:01:24,520 --> 00:01:27,880
And now on to the show.

19
00:01:27,880 --> 00:01:29,480
Alright, everyone.

20
00:01:29,480 --> 00:01:31,720
I am here with Nathaniel Ruiz.

21
00:01:31,720 --> 00:01:38,720
Nathaniel is a PhD student in image and video computing group at Boston University.

22
00:01:38,720 --> 00:01:41,520
Nathaniel, welcome to the Twimal AI podcast.

23
00:01:41,520 --> 00:01:42,920
Thank you very much.

24
00:01:42,920 --> 00:01:45,440
It is great to have a chance to chat with you.

25
00:01:45,440 --> 00:01:50,720
I'm looking forward to digging into our topic, which is some work you've recently done

26
00:01:50,720 --> 00:01:53,520
on disrupting deepfakes.

27
00:01:53,520 --> 00:01:58,760
But before we dive into that, tell us a little bit about your background and how you got

28
00:01:58,760 --> 00:02:01,840
started working in ML and AI.

29
00:02:01,840 --> 00:02:08,040
Yeah, so the first kind of project that I did in AI was a computer vision project when

30
00:02:08,040 --> 00:02:10,480
I was doing an internship here at MIT.

31
00:02:10,480 --> 00:02:15,760
And it was about basically detecting diseases in cassava plants and we were going to deploy

32
00:02:15,760 --> 00:02:17,760
the application in Uganda.

33
00:02:17,760 --> 00:02:21,920
So that was kind of like my first introduction to deep neural networks and image processing

34
00:02:21,920 --> 00:02:23,240
type of stuff.

35
00:02:23,240 --> 00:02:26,960
And I thought that was incredibly interesting, just like the potential of these types of

36
00:02:26,960 --> 00:02:30,800
applications and this new technology, right, or this technology that had been advancing

37
00:02:30,800 --> 00:02:32,440
like very recently.

38
00:02:32,440 --> 00:02:34,920
So I got really interested in it.

39
00:02:34,920 --> 00:02:40,320
Then I did my master's at Georgia Tech, so that was right after that internship.

40
00:02:40,320 --> 00:02:47,360
And then so at Georgia Tech, I met a mentor and my professor there who was my advisor

41
00:02:47,360 --> 00:02:49,920
for my master's, Jim Ray.

42
00:02:49,920 --> 00:02:52,680
And I joined this group and did a lot of stuff.

43
00:02:52,680 --> 00:02:57,240
So I got interested, I guess, and they do a lot of work on autism and behavioral imaging

44
00:02:57,240 --> 00:03:04,280
for autism, trying to like diagnose or model behavior and attention in toddlers or kids.

45
00:03:04,280 --> 00:03:09,400
So I thought that was an example of a model that they built.

46
00:03:09,400 --> 00:03:12,760
So we have several works on.

47
00:03:12,760 --> 00:03:19,360
So they were already doing several works on gaze estimation or attention estimation.

48
00:03:19,360 --> 00:03:22,920
And when I joined that group, I worked with Angie Chong, one of my collaborators, and

49
00:03:22,920 --> 00:03:25,920
she just graduated actually from that group.

50
00:03:25,920 --> 00:03:30,360
And we did behavior modeling and attention modeling and scenes.

51
00:03:30,360 --> 00:03:36,040
So now just more in general, like if you're looking at a person from a third person point

52
00:03:36,040 --> 00:03:41,200
of view, like an image or a video, someone, you can actually detect where they're attending

53
00:03:41,200 --> 00:03:42,640
in the scene.

54
00:03:42,640 --> 00:03:47,720
And this work kind of generalizes and you can basically detect where the person is attending

55
00:03:47,720 --> 00:03:51,440
two in the scene, and it could even be like if they turned their head, this could pick

56
00:03:51,440 --> 00:03:55,800
up that type of attention towards the back of the room, for example, or if they're looking

57
00:03:55,800 --> 00:03:59,600
at the camera, they're not looking at any object inside of the scene, they're looking

58
00:03:59,600 --> 00:04:04,840
at something, you know, beyond the frame, and it could also detect that type of attention.

59
00:04:04,840 --> 00:04:10,920
So I got really basically interested in faces, mostly, just in human beings and in faces

60
00:04:10,920 --> 00:04:12,560
in an image and video.

61
00:04:12,560 --> 00:04:18,200
So I think a lot of my work kind of like goes from that and then, yeah, that's how I got,

62
00:04:18,200 --> 00:04:22,320
you know, to recent kind of projects that I've been doing.

63
00:04:22,320 --> 00:04:24,160
And how long have you been at BU?

64
00:04:24,160 --> 00:04:31,240
This is my second year, so about a year and a half ago, so I had my PhD with my advisor

65
00:04:31,240 --> 00:04:33,080
Stan Schlerof.

66
00:04:33,080 --> 00:04:38,000
What got you started working on or looking at deepfakes?

67
00:04:38,000 --> 00:04:45,560
So actually, in the past year, I was always very interested in deepfakes, especially like

68
00:04:45,560 --> 00:04:51,400
there's a video where there's like deepfakes of Obama using a lot of computer graphics,

69
00:04:51,400 --> 00:04:52,400
right?

70
00:04:52,400 --> 00:04:56,640
They used deep neural networks and computer graphics, and this was like about like two to three

71
00:04:56,640 --> 00:05:01,080
years ago, there were like some big advancements, and you can see very realistic basically

72
00:05:01,080 --> 00:05:04,040
reanimations of Obama's face.

73
00:05:04,040 --> 00:05:07,840
That already, you know, got me very interested when I was at Georgia Tech, but I think it

74
00:05:07,840 --> 00:05:11,880
didn't have like a technical expertise like really tackled at that point.

75
00:05:11,880 --> 00:05:17,360
And more recently, I think the things that have been most impressive are the deepfake

76
00:05:17,360 --> 00:05:22,720
applications where you only grab using only one image of a person, you can basically create

77
00:05:22,720 --> 00:05:28,400
more images or video of that person with different expressions and moving their head.

78
00:05:28,400 --> 00:05:29,560
And that's been like really amazing.

79
00:05:29,560 --> 00:05:33,400
I think there's a work by Samsung, a neural talking heads or something.

80
00:05:33,400 --> 00:05:38,880
I don't actually remember the full title, but it's just amazing how just with one image,

81
00:05:38,880 --> 00:05:41,280
they can create these types of things.

82
00:05:41,280 --> 00:05:46,080
So I had been working on generative models for faces recently, and while I was working

83
00:05:46,080 --> 00:05:49,800
on that, I had a conversation with Stan, my advisor.

84
00:05:49,800 --> 00:05:52,000
There's always a privacy issue, right?

85
00:05:52,000 --> 00:05:56,160
Like when you're going to try to release a paper, you have all these issues that whose

86
00:05:56,160 --> 00:06:02,440
faces can I actually use for this work, and also kind of the reaction of the public.

87
00:06:02,440 --> 00:06:05,360
And I think there's a lot of good that could come out of these applications, but definitely

88
00:06:05,360 --> 00:06:07,920
a lot of bad that can come out of them, right?

89
00:06:07,920 --> 00:06:12,840
The good part is you could have actors and movies, you could basically edit their faces

90
00:06:12,840 --> 00:06:18,360
in real time and not ask them to shoot a scene again, maybe if they failed, in their

91
00:06:18,360 --> 00:06:21,320
expression wasn't perfectly what you wanted at that point.

92
00:06:21,320 --> 00:06:28,440
And there's a lot of different cool applications that you can do with UI-UX and sending videos

93
00:06:28,440 --> 00:06:33,960
of yourself through an iPhone or something, but there's also really bad stuff that we've

94
00:06:33,960 --> 00:06:34,960
seen, actually.

95
00:06:34,960 --> 00:06:40,920
Like the first thing that started happening is they've been using deepfakes in pornography,

96
00:06:40,920 --> 00:06:41,920
right?

97
00:06:41,920 --> 00:06:46,480
Just switching faces into pornographic scenes, and it's just completely immoral and the

98
00:06:46,480 --> 00:06:48,480
potential for damage is so big.

99
00:06:48,480 --> 00:06:53,280
So basically, there's always that effect if you're publishing something on this topic

100
00:06:53,280 --> 00:06:58,720
that you could be helping people that want to do that, you know?

101
00:06:58,720 --> 00:07:03,720
So that whole thing was in my head all that time, and during a conversation, we're talking

102
00:07:03,720 --> 00:07:05,560
about the privacy issue.

103
00:07:05,560 --> 00:07:10,040
So these networks are called image translation networks.

104
00:07:10,040 --> 00:07:16,040
So this work by Samsung is an image translation network that goes like from one image to, you

105
00:07:16,040 --> 00:07:18,880
know, a new image with a different expression imposed.

106
00:07:18,880 --> 00:07:21,240
They also can fine tune it with like several images.

107
00:07:21,240 --> 00:07:26,200
But in general, that's kind of the framework that a lot of these new works are applying.

108
00:07:26,200 --> 00:07:33,320
So StarGAN, also StyleGAN, all of these organimation also was a, I think, 2017 ECCV paper.

109
00:07:33,320 --> 00:07:39,080
I think got best paper, basically putting new expressions on a person's face, so changing

110
00:07:39,080 --> 00:07:42,040
my expression from an image to be like smiling.

111
00:07:42,040 --> 00:07:46,560
And you've seen like applications of this in apps like FaceApp, you know, that's become

112
00:07:46,560 --> 00:07:48,120
very famous.

113
00:07:48,120 --> 00:07:55,280
So this, you know, image translation network is kind of simple as like neural network that

114
00:07:55,280 --> 00:07:59,560
goes from an image to another image, and you kind of specify what the output you want

115
00:07:59,560 --> 00:08:01,200
it to be, basically.

116
00:08:01,200 --> 00:08:06,360
So, and there's this, all this other work on adversarial attacks, right?

117
00:08:06,360 --> 00:08:09,960
That everyone has been hearing because, you know, you don't want to have an adversarial

118
00:08:09,960 --> 00:08:16,120
attack is basically an imperceptible perturbation on an image that a human being doesn't notice,

119
00:08:16,120 --> 00:08:18,840
but that can completely fool a neural network.

120
00:08:18,840 --> 00:08:23,840
So you know, this thing has been explored since 2013 for classifiers.

121
00:08:23,840 --> 00:08:27,960
So if you had a neural network that tells you, hey, there's a pedestrian in the scene,

122
00:08:27,960 --> 00:08:31,160
you know, or not, then you can fool this classifier.

123
00:08:31,160 --> 00:08:35,240
So everyone is obviously freaked out about the possibility of this, you know, becoming

124
00:08:35,240 --> 00:08:40,560
a reality to attack, you know, neural networks in the wild with this type of thing.

125
00:08:40,560 --> 00:08:44,600
So both of these like kind of fashionable ideas in my head, I think, you know, it just popped

126
00:08:44,600 --> 00:08:45,600
out of nowhere.

127
00:08:45,600 --> 00:08:52,400
So maybe during that conversation, maybe if we attack, you know, an image translation

128
00:08:52,400 --> 00:08:59,200
network, you'll be able to protect your images from being converted into deepfix.

129
00:08:59,200 --> 00:09:07,120
The premise of this work is, you know, it sounds like just like in the adversarial attacks,

130
00:09:07,120 --> 00:09:17,080
you're injecting noise to an image or you're injecting noise against an image and then disrupting

131
00:09:17,080 --> 00:09:23,040
the classifier here, you're trying to inject noise on an image and disrupt the ability

132
00:09:23,040 --> 00:09:28,600
of some generative model to do whatever it's trying to do to manipulate that image.

133
00:09:28,600 --> 00:09:30,320
Is that the general idea?

134
00:09:30,320 --> 00:09:31,320
Yeah, exactly.

135
00:09:31,320 --> 00:09:36,480
So in the classifier scenario, you have, you know, an image that goes into the class,

136
00:09:36,480 --> 00:09:39,680
into the deep neural network, and then you want a class that comes out of it.

137
00:09:39,680 --> 00:09:43,440
So if it's a dog picture, you want it to classify it as a dog, right?

138
00:09:43,440 --> 00:09:49,480
And an attack in that situation would be to make it classify it as, you know, a cat,

139
00:09:49,480 --> 00:09:55,080
for example, so a wrong class or you could be a targeted class like you wanted to actually

140
00:09:55,080 --> 00:09:56,600
always classify it as a cat.

141
00:09:56,600 --> 00:10:01,840
You could do a targeted attack or an untargeted attack to like drive it away from the class

142
00:10:01,840 --> 00:10:02,840
to dog class.

143
00:10:02,840 --> 00:10:06,000
So it could be any other class like, you know, lizard, the one that's closest.

144
00:10:06,000 --> 00:10:08,880
So the closest boundary, basically.

145
00:10:08,880 --> 00:10:16,640
In this case, it's kind of like a more general, it's a harder thing to quantify, right?

146
00:10:16,640 --> 00:10:21,440
If you have like an image translation network that goes from an image of a person, you know,

147
00:10:21,440 --> 00:10:25,520
my face, you know, like with a serious face, and then someone wants me, imagine just like

148
00:10:25,520 --> 00:10:29,280
basically this is one of the types of applications for this is, imagine there's a picture of

149
00:10:29,280 --> 00:10:32,440
me in like a serious situation, right?

150
00:10:32,440 --> 00:10:35,240
And I'm like a political figure or something, right?

151
00:10:35,240 --> 00:10:39,160
And then someone grabs this image, uses animation, which is, you know, it's working right

152
00:10:39,160 --> 00:10:40,160
now.

153
00:10:40,160 --> 00:10:43,200
You could actually use this or or start again or anything or cycle again.

154
00:10:43,200 --> 00:10:46,920
And you and changes my expression into like a smiling expression, right?

155
00:10:46,920 --> 00:10:52,800
That's already directly an image to image deep fake that can have a lot of impact, basically.

156
00:10:52,800 --> 00:10:59,280
And the idea is to, you know, make this type of transformation impossible or basically

157
00:10:59,280 --> 00:11:04,680
the idea of artwork is to make it either obvious or to completely disrupt the output such

158
00:11:04,680 --> 00:11:09,520
that it's too corrupted, it's so easy to notice that it's been corrupted, basically.

159
00:11:09,520 --> 00:11:16,040
So the human observer can be like, can either doubt the source of the image or, you know,

160
00:11:16,040 --> 00:11:20,840
that the image has been manipulated or it can, you know, or it's unusable, basically.

161
00:11:20,840 --> 00:11:26,760
It's like gibberish or black, you know, that's basically the idea.

162
00:11:26,760 --> 00:11:31,720
And so you make it sound so simple, but I'm sure, you know, flipping through the paper,

163
00:11:31,720 --> 00:11:37,680
there's a lot of work that went into this where, you know, what were the challenging parts

164
00:11:37,680 --> 00:11:41,880
and how did you take this from kind of idea to a working model?

165
00:11:41,880 --> 00:11:49,360
Yeah, so I think actually the idea, so what I loved about the idea is that it was so simple

166
00:11:49,360 --> 00:11:51,440
and it was so obviously useful.

167
00:11:51,440 --> 00:11:54,480
That's what really got me excited at first, right?

168
00:11:54,480 --> 00:11:58,440
And it's actually, you know, the first couple of weeks of implementing all of this was actually

169
00:11:58,440 --> 00:12:04,480
not very hard because the main idea of trying to destroy an output, using an adversarial

170
00:12:04,480 --> 00:12:08,000
attack in an image translation network, it's actually a lot of these image translation

171
00:12:08,000 --> 00:12:11,200
networks are very susceptible to attack.

172
00:12:11,200 --> 00:12:15,400
And some of them are a little bit more protected and maybe some of our future work is going

173
00:12:15,400 --> 00:12:20,480
to be on that type of thing, like which architectures have more protection than others?

174
00:12:20,480 --> 00:12:25,520
Why are some difficult to attack and why are some easier to attack, right?

175
00:12:25,520 --> 00:12:29,600
But you know, the paper, so I think a lot of it went on, you know, a lot of the work on

176
00:12:29,600 --> 00:12:34,880
the paper went on showing that this is possible with a lot of different types of architectures,

177
00:12:34,880 --> 00:12:39,240
showing a lot of very good examples that it's like a solid technique basically.

178
00:12:39,240 --> 00:12:45,320
And you know, there are some, so basically this paper is kind of like the first or one

179
00:12:45,320 --> 00:12:51,360
of the first steps into this kind of domain because in general, so for this type of, you

180
00:12:51,360 --> 00:12:55,880
know, attack, it's called a white box attack, where you need to know all of the parameters

181
00:12:55,880 --> 00:13:00,080
of the neural network and you need to know the neural network that they're using, right?

182
00:13:00,080 --> 00:13:05,320
So that's a big, you know, kind of if in the real world this, but definitely this could

183
00:13:05,320 --> 00:13:10,280
work at this moment, you know, because a lot of attacks or a lot of deep fakes, I'm

184
00:13:10,280 --> 00:13:13,040
sorry, are very low effort.

185
00:13:13,040 --> 00:13:18,280
So a lot of things happen where someone puts an architecture online on GitHub, right?

186
00:13:18,280 --> 00:13:23,680
And then promotes it on Reddit, for example, and then a lot of people go and use this architecture

187
00:13:23,680 --> 00:13:25,560
with this like pre-trained model.

188
00:13:25,560 --> 00:13:28,840
And then, you know, create defects with this architecture, right?

189
00:13:28,840 --> 00:13:33,880
So that's kind of low effort that can already be preempted by using this technology, because

190
00:13:33,880 --> 00:13:37,800
you know, the way that script treaties, I don't know if anyone uses that term anymore.

191
00:13:37,800 --> 00:13:42,960
I think we should make a new one because it's the idea that like, you know, with hacking

192
00:13:42,960 --> 00:13:46,920
folks would just, you know, download some Pearl script or whatever and run it against

193
00:13:46,920 --> 00:13:49,880
some site to find, you know, vulnerabilities.

194
00:13:49,880 --> 00:13:50,880
Yeah, exactly.

195
00:13:50,880 --> 00:13:55,720
And you still had, it's funny because, you know, I guess SQL injections were, you know,

196
00:13:55,720 --> 00:14:01,400
were so easy to exploit in the early days, but even until like, you know, like 10, 10

197
00:14:01,400 --> 00:14:06,760
years ago or something, like people were still finding SQL injection vulnerabilities.

198
00:14:06,760 --> 00:14:12,360
And so script kitties kind of like were still able to like work in the real world.

199
00:14:12,360 --> 00:14:17,080
So I think this kind of, you know, we should make a new term maybe like DL kitties or something,

200
00:14:17,080 --> 00:14:18,080
right?

201
00:14:18,080 --> 00:14:19,080
It's just people.

202
00:14:19,080 --> 00:14:21,800
You're here first.

203
00:14:21,800 --> 00:14:26,200
People trying to, trying to use like out of the box kind of stuff on, transfer learning

204
00:14:26,200 --> 00:14:27,200
kitties.

205
00:14:27,200 --> 00:14:28,200
Yeah.

206
00:14:28,200 --> 00:14:29,200
That's a good thing.

207
00:14:29,200 --> 00:14:30,200
Pre-trained model kitties.

208
00:14:30,200 --> 00:14:31,200
There's something in there somewhere.

209
00:14:31,200 --> 00:14:37,360
But at the same time, you know, I always, like, I respect anyone that tries to use these

210
00:14:37,360 --> 00:14:38,360
things out of the box.

211
00:14:38,360 --> 00:14:39,440
That's the first step, right?

212
00:14:39,440 --> 00:14:43,960
Out of the box from, you know, wherever you can find, put your hands on any of this technology

213
00:14:43,960 --> 00:14:46,320
and then start using it and try to learn from it.

214
00:14:46,320 --> 00:14:49,480
If it's not for doing something bad, right?

215
00:14:49,480 --> 00:14:52,560
If it's just to learn, I encourage everyone, right?

216
00:14:52,560 --> 00:14:57,200
To just go on on GitHub, try a bunch of stuff and then see if they can modify it.

217
00:14:57,200 --> 00:14:59,240
Like that's the way forward, right?

218
00:14:59,240 --> 00:15:05,200
But yeah, if you are doing deep fakes for an immoral purpose, that's definitely not good.

219
00:15:05,200 --> 00:15:10,880
Yeah, but this technology, so just for this specific thing, is already usable because

220
00:15:10,880 --> 00:15:16,520
we're able to, you know, find you're probably going to have the access to the, to the weights

221
00:15:16,520 --> 00:15:18,360
and to the architecture.

222
00:15:18,360 --> 00:15:22,720
But this is a very like constrained kind of attack in some sense.

223
00:15:22,720 --> 00:15:26,000
The more advanced types of attacks are called black box attacks.

224
00:15:26,000 --> 00:15:29,760
You could be like gray box where you know the architecture, but you don't know the weights

225
00:15:29,760 --> 00:15:33,120
of the architecture or like different types, there's different types of settings for

226
00:15:33,120 --> 00:15:34,120
that.

227
00:15:34,120 --> 00:15:38,480
But the black box, which is the most powerful, is just you just have a black box where

228
00:15:38,480 --> 00:15:42,360
you send an image and then you get an image back where you get, you send an image and

229
00:15:42,360 --> 00:15:45,280
you get like a class back dog or something.

230
00:15:45,280 --> 00:15:49,400
And then you're trying to kind of attack it without knowing almost anything about it.

231
00:15:49,400 --> 00:15:53,760
I think that's really interesting and that's like where the promise lies in this because

232
00:15:53,760 --> 00:15:59,360
if someone can find very good black box attacks against a lot of these methods, then you'll

233
00:15:59,360 --> 00:16:02,560
definitely have something that that can work in the real world.

234
00:16:02,560 --> 00:16:10,640
Yeah, I mean, you almost envision a scenario where, you know, a Facebook or Twitter, Google,

235
00:16:10,640 --> 00:16:16,600
like when you upload a photo, it's applying this method to all of your photos so that there

236
00:16:16,600 --> 00:16:21,160
are not susceptible to, you know, being used for in a various purposes.

237
00:16:21,160 --> 00:16:22,160
100%.

238
00:16:22,160 --> 00:16:23,160
Yeah, yeah.

239
00:16:23,160 --> 00:16:26,360
And that's like the first kind of application that I had in mind.

240
00:16:26,360 --> 00:16:31,360
The first application that I had in mind was actually for celebrities where, you know,

241
00:16:31,360 --> 00:16:36,640
their likeness is, you know, so, you know, it actually has a big value, right?

242
00:16:36,640 --> 00:16:41,320
And a lot of people are targeting their pictures to try to modify them.

243
00:16:41,320 --> 00:16:46,360
And that type of scenario where you can, you know, maybe, you know, they can buy some

244
00:16:46,360 --> 00:16:48,640
type of service that protects their images.

245
00:16:48,640 --> 00:16:52,720
And then, yeah, if you can generalize it, then you can have any type of platform online

246
00:16:52,720 --> 00:16:54,040
that does this.

247
00:16:54,040 --> 00:16:57,080
Yeah, there's just so many interesting things, right?

248
00:16:57,080 --> 00:17:02,520
You could give permission to someone to use your picture and then to, like, modify your

249
00:17:02,520 --> 00:17:04,120
picture, right?

250
00:17:04,120 --> 00:17:08,360
And then there are some, you know, technologies that we've been thinking of in that direction

251
00:17:08,360 --> 00:17:09,360
as well.

252
00:17:09,360 --> 00:17:11,920
Lots of blockchain opportunities in there.

253
00:17:11,920 --> 00:17:17,840
I feel like, yeah, I think a lot of people kind of jump to blockchain when they're thinking

254
00:17:17,840 --> 00:17:24,280
about protecting images of people or protecting media, but that's one way.

255
00:17:24,280 --> 00:17:25,280
This is another way.

256
00:17:25,280 --> 00:17:30,880
And blockchain, when I want to make a joke about overhyped technologies, yeah, I mean,

257
00:17:30,880 --> 00:17:33,880
we all fall into overhyped sometimes.

258
00:17:33,880 --> 00:17:34,880
That's definitely true.

259
00:17:34,880 --> 00:17:40,080
But I also think, I don't know, sometimes it brings attention that maybe will fade, but

260
00:17:40,080 --> 00:17:44,160
it's still good that these kinds of domains are having a lot of attention because they

261
00:17:44,160 --> 00:17:46,280
have a lot of promise.

262
00:17:46,280 --> 00:17:50,640
And the people that stick with it are the ones that are going to make things happen.

263
00:17:50,640 --> 00:17:52,320
And then the people that leave, right?

264
00:17:52,320 --> 00:17:57,840
Okay, well, they left, and if they're not here to, like, make something happen, then

265
00:17:57,840 --> 00:18:00,760
that's, you know, their issue, basically.

266
00:18:00,760 --> 00:18:03,720
So is the noise that you're injecting?

267
00:18:03,720 --> 00:18:10,240
Is it parameterized with a single, like a single epsilon value or their characteristics

268
00:18:10,240 --> 00:18:15,920
to the noise multiple, you know, characteristics that you're manipulating to make it work with

269
00:18:15,920 --> 00:18:18,160
a particular model?

270
00:18:18,160 --> 00:18:19,160
Yeah.

271
00:18:19,160 --> 00:18:26,860
So the main attacks that we propose, and the paper is not just attacking image translation

272
00:18:26,860 --> 00:18:28,160
systems for deepfakes.

273
00:18:28,160 --> 00:18:29,160
Okay.

274
00:18:29,160 --> 00:18:31,280
It's, it has a lot of other stuff.

275
00:18:31,280 --> 00:18:34,800
For example, we have, you know, you have the image translation case, but you also have

276
00:18:34,800 --> 00:18:38,080
the conditional case where I don't want to distract of your question, but I just want

277
00:18:38,080 --> 00:18:43,720
to say that we have basically several contributions that make it a little bit more than just, oh,

278
00:18:43,720 --> 00:18:46,920
this is, you know, a neat, neat little idea, right?

279
00:18:46,920 --> 00:18:53,280
Because I think the first kind of idea is, is cool, but also the, all the contributions

280
00:18:53,280 --> 00:18:58,160
that we do make this, like, a, kind of, like, bigger kind of type of work.

281
00:18:58,160 --> 00:19:02,800
Because the main idea of attacking these image translation systems, it's actually not

282
00:19:02,800 --> 00:19:08,600
that complicated to adapt some of the techniques that we already had, like FGSM, FastGrading

283
00:19:08,600 --> 00:19:15,640
Sign Method, iterative FGSM, basically FGSM is just kind of taking a step in the direction

284
00:19:15,640 --> 00:19:21,000
of the, of the gradient for, for your image and then modifying your image so that you

285
00:19:21,000 --> 00:19:24,560
go away from the class that you want to classify this thing as.

286
00:19:24,560 --> 00:19:26,360
That's for the classifier case, right?

287
00:19:26,360 --> 00:19:32,880
In our case, you have the, this loss, which is, you can have, like, your main metric is,

288
00:19:32,880 --> 00:19:37,160
what happens if you translate the image without any attack, you have kind of, like, a ground

289
00:19:37,160 --> 00:19:38,160
truth output, right?

290
00:19:38,160 --> 00:19:42,040
Like, this is what the picture is going to look without any attack.

291
00:19:42,040 --> 00:19:47,080
And then you have a translation of your image, so this is the thing that we can modify,

292
00:19:47,080 --> 00:19:50,800
because the translation of your image within attack, right?

293
00:19:50,800 --> 00:19:52,320
That's the thing that we can modify.

294
00:19:52,320 --> 00:19:56,640
And now you want the difference between this ground truth output and this attacked output

295
00:19:56,640 --> 00:19:59,880
to be as big as possible, and with some kind of metric, right?

296
00:19:59,880 --> 00:20:04,200
And we use L2 in the formulation of our attacks, but you could use, you know, a lot of different

297
00:20:04,200 --> 00:20:05,200
type of metrics.

298
00:20:05,200 --> 00:20:11,440
And we use, like, an image, image level metric, which goes, like, pixel pixel to pixel differences

299
00:20:11,440 --> 00:20:14,400
in the, using L2, right?

300
00:20:14,400 --> 00:20:15,760
And then you want to maximize this, right?

301
00:20:15,760 --> 00:20:20,440
You want your output, the attacked output to be as different as possible as the ground

302
00:20:20,440 --> 00:20:22,520
truth output would have been.

303
00:20:22,520 --> 00:20:23,800
So you can actually just do this.

304
00:20:23,800 --> 00:20:26,440
It's just an optimization, you know, problem.

305
00:20:26,440 --> 00:20:33,680
Yeah, I guess one of the things that jumps to mind is as opposed to, like, in L2, distance,

306
00:20:33,680 --> 00:20:38,280
maximizing the ability to fool some kind of discriminator network.

307
00:20:38,280 --> 00:20:45,320
So we don't actually have to do that at all, actually, because, so to train these types

308
00:20:45,320 --> 00:20:49,360
of architectures, yeah, you have a generator that does the image translation, right?

309
00:20:49,360 --> 00:20:53,760
And then you have a discriminator and this type of game between the discriminator that's

310
00:20:53,760 --> 00:20:59,040
creating the translation and the, the generators already and the discriminator that's trying

311
00:20:59,040 --> 00:21:05,240
to detect whether it's a real or fake image is actually what makes the output so good

312
00:21:05,240 --> 00:21:10,720
and makes it, you know, approximate these distributions so well without, without too

313
00:21:10,720 --> 00:21:11,720
much blurriness, et cetera.

314
00:21:11,720 --> 00:21:15,360
You know, that's the power of Gans, but we don't even need, you know, we kind of like

315
00:21:15,360 --> 00:21:20,000
throw away the discriminator in this process, just use the generator and it just becomes

316
00:21:20,000 --> 00:21:22,880
like an optimization problem.

317
00:21:22,880 --> 00:21:33,640
Maybe the layer behind my question was, is, you know, the, your L2 distance is really

318
00:21:33,640 --> 00:21:40,520
a proxy for perceived difference from the actual face and I'm wondering like how good

319
00:21:40,520 --> 00:21:47,120
L2 is for, you know, really, you know, making the generated image far away from the face

320
00:21:47,120 --> 00:21:52,480
or, you know, perceived, the level of perceived distortion.

321
00:21:52,480 --> 00:21:56,640
And so that's where I thought maybe like some kind of discriminator train discriminator

322
00:21:56,640 --> 00:22:00,320
thing could be better than L2, but that's a really good question, actually.

323
00:22:00,320 --> 00:22:04,480
I think, you know, you could think of, of, like an L2 distance, you could, you could

324
00:22:04,480 --> 00:22:08,880
think of, you know, the attack making just the image just a little bit brighter and then

325
00:22:08,880 --> 00:22:11,520
the L2 distance would go up, right?

326
00:22:11,520 --> 00:22:17,000
But, but, so you would have a higher L2 distance between these two, but a human being would

327
00:22:17,000 --> 00:22:20,800
be like, you know, it's a little bit brighter, but it's kind of still the same picture.

328
00:22:20,800 --> 00:22:24,000
So yeah, definitely, it's not the perfect thing in the paper.

329
00:22:24,000 --> 00:22:31,040
We explain why, and on average, you know, it's, it's a good metric to use and we show examples.

330
00:22:31,040 --> 00:22:35,320
So we didn't really just go through the L1, L2 metric and be like, yeah, it's high.

331
00:22:35,320 --> 00:22:37,400
So it's, it's working, right?

332
00:22:37,400 --> 00:22:41,960
We looked at a lot of qualitative examples, but the second, you know, the second step

333
00:22:41,960 --> 00:22:46,600
that you can take after this is definitely, and I, you know, encourage anyone that wants

334
00:22:46,600 --> 00:22:50,800
to try this out, actually, to try it out because I'm trying other things.

335
00:22:50,800 --> 00:22:58,640
But if anyone wants to use like a perceptual metric, so, you know, we have like VGG, for

336
00:22:58,640 --> 00:23:04,800
example, VGG trained on faces is a pretty good kind of proxy, I mean, it's kind of weird

337
00:23:04,800 --> 00:23:09,760
to say, but it's kind of a proxy of perceptual, of a perceptual metric.

338
00:23:09,760 --> 00:23:16,120
So two faces that are similar in this like VGG 16 feature space are, you know, you

339
00:23:16,120 --> 00:23:20,840
can actually kind of cluster then better in this, in this type of sense, instead of using

340
00:23:20,840 --> 00:23:21,840
L2 metrics.

341
00:23:21,840 --> 00:23:25,680
So, yeah, having a higher distance with a VGG, which is kind of like a discriminator

342
00:23:25,680 --> 00:23:28,000
in some sense, it's a neural network, right?

343
00:23:28,000 --> 00:23:32,760
Or you could do what you're saying is you could say, is this image, you know, you could

344
00:23:32,760 --> 00:23:36,680
have like discriminator, I just had this idea right now, actually, you could have a discriminator

345
00:23:36,680 --> 00:23:40,880
that tells you, oh, this image is more or less distorted, right?

346
00:23:40,880 --> 00:23:45,320
And you want, you train your discriminator to detect distorted images and you train your

347
00:23:45,320 --> 00:23:47,360
generator to distort the images.

348
00:23:47,360 --> 00:23:51,800
And then you could have like a game in that way, maybe with a third or second discriminator

349
00:23:51,800 --> 00:23:52,800
as well.

350
00:23:52,800 --> 00:23:57,320
But definitely like that's one of the big questions of systematizing this is it's easier

351
00:23:57,320 --> 00:24:02,120
to systematize with classifiers because you know when the class is wrong, there's just

352
00:24:02,120 --> 00:24:03,120
a number, right?

353
00:24:03,120 --> 00:24:06,280
The class is four, but we wanted five, so it's wrong, right?

354
00:24:06,280 --> 00:24:11,480
In this case, you have something and that's the problem that I've been like bumping into

355
00:24:11,480 --> 00:24:17,600
all over this kind of area is you're talking about human perception and trying to kind

356
00:24:17,600 --> 00:24:23,040
of like model a reaction of a human being, right, or what the human being perceives in

357
00:24:23,040 --> 00:24:24,040
an image.

358
00:24:24,040 --> 00:24:29,320
And that's, you know, much more complex for generating faces and for disrupting defects,

359
00:24:29,320 --> 00:24:30,320
right?

360
00:24:30,320 --> 00:24:39,400
So it started a couple of questions ago asking about like the how the noise is parameterized

361
00:24:39,400 --> 00:24:48,280
and I think the question behind that question was, you know, is there, you know, some way

362
00:24:48,280 --> 00:24:53,240
to just crank the noise up as much as you can before the image starts looking distorted

363
00:24:53,240 --> 00:25:00,680
and use that as a way, you know, does that get you closer to a gray box or a black box

364
00:25:00,680 --> 00:25:07,000
type of scenario or do you really have to, are there, you know, how nuanced is the, does

365
00:25:07,000 --> 00:25:11,480
the noise have to be to defeat a particular system?

366
00:25:11,480 --> 00:25:13,600
Yeah, that's actually a great question.

367
00:25:13,600 --> 00:25:17,160
One thing that I really wanted to say that I didn't get a chance to say before we started

368
00:25:17,160 --> 00:25:19,960
is this is not just my work, right?

369
00:25:19,960 --> 00:25:25,120
Like this is work with really amazing collaborators at BU.

370
00:25:25,120 --> 00:25:31,080
So a recent, a research assistant professor, Sarah Del Bargel, who helped me so much with

371
00:25:31,080 --> 00:25:35,480
this project and then my advisor Stan Sclyroff, right, and all of these kind of ideas have

372
00:25:35,480 --> 00:25:40,000
been like discussed with them and, you know, we've all like put so much, you know, work

373
00:25:40,000 --> 00:25:41,560
into this, right?

374
00:25:41,560 --> 00:25:46,960
So, okay, moving on from that, basically, yeah, this is kind of like, that question is really

375
00:25:46,960 --> 00:25:52,200
good because one of the things that we did was try it on, so first of all, just try to

376
00:25:52,200 --> 00:25:56,520
see how sensitive any of these architectures is to just random noise, right?

377
00:25:56,520 --> 00:26:01,160
Like I'll say no to something and some of them are super sensitive.

378
00:26:01,160 --> 00:26:06,120
If you, I don't really want to say which ones, right, because I don't want to just like

379
00:26:06,120 --> 00:26:09,000
single out any any, and it's not their fault, right?

380
00:26:09,000 --> 00:26:12,760
But some of some architectures, if you inject just a little bit of random noise, then you

381
00:26:12,760 --> 00:26:16,280
can have very big perturbations in the output image.

382
00:26:16,280 --> 00:26:20,200
That was the first step and some of them are very resistant to noise.

383
00:26:20,200 --> 00:26:24,040
And then this finding holds to the adversarial attack case.

384
00:26:24,040 --> 00:26:31,960
So an adversarial attack is just a sort of structured noise that is structured using,

385
00:26:31,960 --> 00:26:39,280
you know, basically you use the gradient of the network to get the biggest type of disruption

386
00:26:39,280 --> 00:26:41,760
in the output possible, right?

387
00:26:41,760 --> 00:26:48,600
So that doing an adversarial attack of same magnitude would be way more effective than

388
00:26:48,600 --> 00:26:51,720
just being random noise of that same magnitude.

389
00:26:51,720 --> 00:26:55,000
But some architectures are just really sensitive to noise.

390
00:26:55,000 --> 00:26:57,520
That's another lesson of this, I think.

391
00:26:57,520 --> 00:27:02,960
Your initial response to that question was talking about the broader contributions to this

392
00:27:02,960 --> 00:27:08,920
paper beyond the kind of the simple, deep fake disruption mechanism.

393
00:27:08,920 --> 00:27:14,040
If that's the right way to characterize it, walk us through the, you know, what you think

394
00:27:14,040 --> 00:27:17,600
are the biggest contributions here.

395
00:27:17,600 --> 00:27:21,200
This idea of attacking image translation systems is pretty natural.

396
00:27:21,200 --> 00:27:25,960
But then there's some specific, you know, specificity of deep fake kind of image translation

397
00:27:25,960 --> 00:27:26,960
networks.

398
00:27:26,960 --> 00:27:32,560
And one of them is that you have, you're always, you almost always have a class or their,

399
00:27:32,560 --> 00:27:34,760
you know, conditional image translation networks.

400
00:27:34,760 --> 00:27:38,920
And your condition could be or your class could be, for example, in animation, you have

401
00:27:38,920 --> 00:27:43,280
the action units, which are, you know, small movements of the phase that could like correspond

402
00:27:43,280 --> 00:27:46,480
to like smiling or moving your lips upward, et cetera, right?

403
00:27:46,480 --> 00:27:49,760
And then by combining them, you create expressions.

404
00:27:49,760 --> 00:27:53,240
So these, these networks are conditional networks.

405
00:27:53,240 --> 00:27:57,080
And you want to kind of attack them irrespective of the class.

406
00:27:57,080 --> 00:28:01,200
So my attack doesn't have to, if, you know, let's just say that you don't want it to

407
00:28:01,200 --> 00:28:03,600
only work on the smiling faces or something.

408
00:28:03,600 --> 00:28:04,600
Exactly.

409
00:28:04,600 --> 00:28:10,080
Like, yeah, maybe this person is going to make everyone smile and pictures, but, you

410
00:28:10,080 --> 00:28:14,240
know, you don't even know what the other person's going to do, maybe close your eyes, right?

411
00:28:14,240 --> 00:28:18,440
So yeah, for example, if you target an attack towards, and in some of these architectures

412
00:28:18,440 --> 00:28:23,400
you're tired to attack towards one class, then it doesn't transfer to other classes as

413
00:28:23,400 --> 00:28:24,400
well.

414
00:28:24,400 --> 00:28:27,800
And in some of them, it's just an attack for one class, just transfers completely to the

415
00:28:27,800 --> 00:28:28,800
others.

416
00:28:28,800 --> 00:28:33,360
And I think actually these two kind of properties, like the fragility of an architecture

417
00:28:33,360 --> 00:28:37,240
to noise and this type of transfer are actually related.

418
00:28:37,240 --> 00:28:44,320
That sounds kind of interesting and that is it kind of saying that the fragility isn't

419
00:28:44,320 --> 00:28:52,000
necessarily an architectural trait, but more specific, like a trait of the weights of an

420
00:28:52,000 --> 00:28:53,000
architecture.

421
00:28:53,000 --> 00:28:54,000
Yeah.

422
00:28:54,000 --> 00:28:55,000
I think so.

423
00:28:55,000 --> 00:29:00,280
I think the fragility of an architecture to attack is actually, that's what I'm kind

424
00:29:00,280 --> 00:29:06,120
of discovering now, is tied not only to the type of architecture, but to the weights.

425
00:29:06,120 --> 00:29:09,720
And if you think about it, the weights is just a function of the training data and the

426
00:29:09,720 --> 00:29:12,880
training kind of process, right?

427
00:29:12,880 --> 00:29:16,360
So the optimization process and the training data.

428
00:29:16,360 --> 00:29:20,160
So these two things are pretty important, I would say.

429
00:29:20,160 --> 00:29:25,280
And it depends how important they are for each kind of architecture.

430
00:29:25,280 --> 00:29:28,960
And it's still like, this is, you know, complete.

431
00:29:28,960 --> 00:29:33,600
This is kind of like an intuition a little bit from what I've seen in my experiments, but

432
00:29:33,600 --> 00:29:38,200
I think there's a lot of like super interesting work to be done here, because I think adversarial

433
00:29:38,200 --> 00:29:43,760
attacks actually are very exciting to me, and I've just gotten into them a little bit later.

434
00:29:43,760 --> 00:29:50,480
But you know, I've always like kind of wondered like, how exactly do deep models work in some

435
00:29:50,480 --> 00:29:51,480
sense?

436
00:29:51,480 --> 00:29:54,040
Like, why do they fail in certain cases and they don't fail in others?

437
00:29:54,040 --> 00:29:58,840
You know, this type of kind of explanatory process of the failures of a network or how

438
00:29:58,840 --> 00:30:02,760
to make it better, you know, or, you know, intuitions and how to make it better.

439
00:30:02,760 --> 00:30:07,880
So I think adversarial attacks are actually like a great window into fragility of an

440
00:30:07,880 --> 00:30:10,600
explanation of these neural networks.

441
00:30:10,600 --> 00:30:14,640
So that's one cool thing about this is by attacking image translation systems, I can

442
00:30:14,640 --> 00:30:19,440
actually see in their output, like when I attack them, what they are doing.

443
00:30:19,440 --> 00:30:20,440
A lot of them.

444
00:30:20,440 --> 00:30:26,800
So, for example, you know, this is something good for a pretty specific, I think, but for

445
00:30:26,800 --> 00:30:33,560
StarGAN, if you attack it, then you have the whole image that changes all at once, and

446
00:30:33,560 --> 00:30:38,400
you have other architecture such as animation that are very targeted towards certain parts

447
00:30:38,400 --> 00:30:40,480
of the image that they're modifying.

448
00:30:40,480 --> 00:30:45,360
So one architecture has learned how to kind of like change the whole frame at once, and

449
00:30:45,360 --> 00:30:50,840
then one architecture has learned to do more like fine-grained types of changes inside

450
00:30:50,840 --> 00:30:51,840
of an image.

451
00:30:51,840 --> 00:30:54,880
And one is, you know, animations more robust than StarGAN.

452
00:30:54,880 --> 00:31:01,360
So it's almost along the lines of work like lime and other things where you're perturbing

453
00:31:01,360 --> 00:31:09,200
inputs or features and seeing how the network responds to the aim of understanding explainability

454
00:31:09,200 --> 00:31:11,040
or producing an explanation.

455
00:31:11,040 --> 00:31:17,920
You know, this is, you're kind of almost at, you know, trying to explain or understand

456
00:31:17,920 --> 00:31:22,320
these networks through the disruptions that you're injecting.

457
00:31:22,320 --> 00:31:23,800
Yeah, I think so.

458
00:31:23,800 --> 00:31:24,800
I don't know.

459
00:31:24,800 --> 00:31:29,440
Actually, this work, you know, I actually don't know about that work that you just mentioned,

460
00:31:29,440 --> 00:31:31,720
you know, going on from your explanation.

461
00:31:31,720 --> 00:31:35,160
I don't know if that has been done, maybe it's been done for like classifiers, but I don't

462
00:31:35,160 --> 00:31:37,320
know if it's been done for image translation networks.

463
00:31:37,320 --> 00:31:42,520
And I think that's like a huge frontier that this kind of opens is to try to understand

464
00:31:42,520 --> 00:31:43,520
what's going on.

465
00:31:43,520 --> 00:31:48,000
And then if you know, so for example, just, you know, example of the bad, if you know

466
00:31:48,000 --> 00:31:54,200
that your image translation network is changing the whole frame when you actually just need

467
00:31:54,200 --> 00:31:59,200
to change the hair color of the person, then you could think of this as a weakness,

468
00:31:59,200 --> 00:32:00,200
right?

469
00:32:00,200 --> 00:32:02,800
And then you could think of how do I change the architecture of the training procedure

470
00:32:02,800 --> 00:32:04,440
to correct this, right?

471
00:32:04,440 --> 00:32:07,480
And there are techniques to do it, which are, which are pretty cool, I think.

472
00:32:07,480 --> 00:32:10,760
So yeah, definitely it's delving into this type of explainability.

473
00:32:10,760 --> 00:32:16,360
I guess that's also fashionable right now, but I think it's a huge thing.

474
00:32:16,360 --> 00:32:20,680
My lab does a lot of, of, of, of explainability and deep networks.

475
00:32:20,680 --> 00:32:21,680
Okay.

476
00:32:21,680 --> 00:32:22,680
Okay.

477
00:32:22,680 --> 00:32:29,320
And so this kind of broader understanding of these architectures is another contribution

478
00:32:29,320 --> 00:32:30,320
of the paper.

479
00:32:30,320 --> 00:32:31,320
What else?

480
00:32:31,320 --> 00:32:32,320
Yeah.

481
00:32:32,320 --> 00:32:37,520
So yeah, yeah, we were on that question kind of easy to get lost.

482
00:32:37,520 --> 00:32:41,880
So that was, that was one of one of our contributions is conditional image translation

483
00:32:41,880 --> 00:32:46,320
networks and, and doing, building attacks that generalize to all of the different types

484
00:32:46,320 --> 00:32:48,960
of, of, of classes.

485
00:32:48,960 --> 00:32:52,120
So yeah, it doesn't just work if someone's trying to put a smile on your face.

486
00:32:52,120 --> 00:32:56,000
It also works when you're, when someone's trying to close your eyes in an image, right?

487
00:32:56,000 --> 00:33:01,360
Or it doesn't just work if someone's trying to make you, you know, blonde or something.

488
00:33:01,360 --> 00:33:06,640
It works also when someone tries to make your hair darker or something.

489
00:33:06,640 --> 00:33:11,000
So that's, that's one thing that the other thing is, so kind of like, okay, now we have

490
00:33:11,000 --> 00:33:14,600
an attack typical in this kind of area is you have an attack.

491
00:33:14,600 --> 00:33:16,040
What are the defenses, right?

492
00:33:16,040 --> 00:33:21,840
Like what is someone that has like an actual like beneficial image translation network?

493
00:33:21,840 --> 00:33:24,240
What can they do to defend against this type of attack?

494
00:33:24,240 --> 00:33:28,520
Because you could also think about the scenario where, you know, in this scenario, you're

495
00:33:28,520 --> 00:33:30,200
trying to, you know, obstruct deepfake.

496
00:33:30,200 --> 00:33:33,920
So you're trying to obstruct something that is done without permission of the users,

497
00:33:33,920 --> 00:33:34,920
and that can be malicious.

498
00:33:34,920 --> 00:33:39,720
But you can imagine a scenario where someone attacks, let's say, I don't know, just off

499
00:33:39,720 --> 00:33:46,280
the top of my head, like, let's say you have like an X-ray and, and you have like an image

500
00:33:46,280 --> 00:33:51,000
translation network that makes some zones more visible to a surgeon or whatever, right?

501
00:33:51,000 --> 00:33:52,920
Like it's hard to come up with an example right now.

502
00:33:52,920 --> 00:33:57,720
But going with that example, you can imagine a malicious actor introducing one of our attacks

503
00:33:57,720 --> 00:34:03,040
or something like this to this X-ray to make the output not work, right?

504
00:34:03,040 --> 00:34:07,320
So what can a person do to defend against this?

505
00:34:07,320 --> 00:34:14,640
And one of the defenses that holds up to all the scrutiny is adversarial training.

506
00:34:14,640 --> 00:34:19,720
I think it done by Madrid all and 2017 that I think that's the paper.

507
00:34:19,720 --> 00:34:24,080
So basically the idea is just it's a very, it's a very simple, very powerful idea.

508
00:34:24,080 --> 00:34:28,920
You have PGD, which is a very strong attack projected gradient descent, and you just

509
00:34:28,920 --> 00:34:33,400
augment your data set with a lot of images that have been trained, that have been attacked

510
00:34:33,400 --> 00:34:34,960
using PGD.

511
00:34:34,960 --> 00:34:42,040
And then you train your neural network with those images, and it gets, it becomes a, you

512
00:34:42,040 --> 00:34:46,880
know, more robust to these types of adversarial attacks in this type of sense that I've been

513
00:34:46,880 --> 00:34:50,040
explaining.

514
00:34:50,040 --> 00:34:55,800
And this also is, so a lot of these defense mechanisms or, you know, defenses against

515
00:34:55,800 --> 00:35:01,520
adversarial attacks are so hard to make because in security, your defense has to be valid

516
00:35:01,520 --> 00:35:05,800
even when an attacker knows the defense you're going to use.

517
00:35:05,800 --> 00:35:10,960
So a lot of defenses fall apart in this, in the scenario, and this is one that that doesn't

518
00:35:10,960 --> 00:35:12,600
or hasn't at this point.

519
00:35:12,600 --> 00:35:15,440
So one of the things is it's not foolproof.

520
00:35:15,440 --> 00:35:16,880
You can still attack the network.

521
00:35:16,880 --> 00:35:21,280
So we're able to do this adversarial training for GANs, and we have like these formulations

522
00:35:21,280 --> 00:35:26,760
because you can, you can, you can train the, the generator with adversarial noise.

523
00:35:26,760 --> 00:35:29,680
But you can also train the generator and the discriminator.

524
00:35:29,680 --> 00:35:34,640
So you attack both the real image, but you attack the real and the fake image.

525
00:35:34,640 --> 00:35:36,920
So, so there's like different ways of doing it.

526
00:35:36,920 --> 00:35:41,280
The most powerful way of doing it is, is doing the generator plus the discriminator adversarial

527
00:35:41,280 --> 00:35:45,600
training, and it does defend against some certain types of attacks.

528
00:35:45,600 --> 00:35:50,600
But so it makes it more robust, but in the end, if we have a very strong attack, it's,

529
00:35:50,600 --> 00:35:52,400
it's still pretty successful.

530
00:35:52,400 --> 00:35:54,360
That's one thing that we learned.

531
00:35:54,360 --> 00:35:59,240
And in the future, more investigation in this is kind of needed to see exactly, you know,

532
00:35:59,240 --> 00:36:03,720
how much robustness it does bring, actually.

533
00:36:03,720 --> 00:36:07,080
Oh, yeah, and the last one.

534
00:36:07,080 --> 00:36:08,080
Yes.

535
00:36:08,080 --> 00:36:13,160
Oh my god, I forgot.

536
00:36:13,160 --> 00:36:18,600
I think all of these are pretty interesting and, you know, I would like to work on all of

537
00:36:18,600 --> 00:36:19,600
them.

538
00:36:19,600 --> 00:36:20,600
It's impossible, right?

539
00:36:20,600 --> 00:36:26,800
So that this one is, the last one is in a certain scenario where you can blur the,

540
00:36:26,800 --> 00:36:30,360
so my advisor just told me, like, oh, okay, you can attack these images.

541
00:36:30,360 --> 00:36:31,360
Great.

542
00:36:31,360 --> 00:36:35,160
But what happens in the, in the real scenario, if you're like a malicious actor, you run

543
00:36:35,160 --> 00:36:39,200
into one of these images and you're like, I suspect that this one's attacked, right?

544
00:36:39,200 --> 00:36:44,200
So I'll just blur it a little bit with, you know, a Gaussian blur or an average blur

545
00:36:44,200 --> 00:36:45,200
or something.

546
00:36:45,200 --> 00:36:49,920
And then maybe since the attack is high frequency structure noise on top of the image,

547
00:36:49,920 --> 00:36:52,680
then maybe this will destroy the attack.

548
00:36:52,680 --> 00:36:56,160
And so that's one thing in this, in our scenario, it's, it's different than the classifier

549
00:36:56,160 --> 00:36:57,160
scenario.

550
00:36:57,160 --> 00:37:02,960
In our scenario, we have to kind of also be careful of this kind of like gray box scenario

551
00:37:02,960 --> 00:37:06,480
where we don't, we know maybe the architecture that they're using and the, and the weights

552
00:37:06,480 --> 00:37:10,560
that they're using, but we don't know what pre-processing they're using.

553
00:37:10,560 --> 00:37:15,280
So there are some ideas in this domain like expectation over transformation where you

554
00:37:15,280 --> 00:37:20,720
just grab kind of like the expectation of all of the losses through, with all of the

555
00:37:20,720 --> 00:37:22,560
transformations that you think of.

556
00:37:22,560 --> 00:37:26,280
And in that paper, they did a cropping and rotating, but they didn't do blurring.

557
00:37:26,280 --> 00:37:31,680
So I think our paper is one of the, you know, to the best of my knowledge, it's one that

558
00:37:31,680 --> 00:37:37,760
is, one that addresses blurring in this type of scenario of transferability across blurring.

559
00:37:37,760 --> 00:37:42,120
But also, yeah, because we noticed that blurring is actually really effective.

560
00:37:42,120 --> 00:37:47,880
You blur and naive attack just a little bit, then you can definitely translate the image.

561
00:37:47,880 --> 00:37:51,880
And there's almost no downside because the output image looks as good.

562
00:37:51,880 --> 00:37:58,840
And when we, when you say attack here, are you speaking in the sense of kind of traditional

563
00:37:58,840 --> 00:38:09,080
adversarial attack or, and or, you know, your work where you're trying to prevent manipulation,

564
00:38:09,080 --> 00:38:12,080
which the manipulation is kind of an attack in this sense.

565
00:38:12,080 --> 00:38:13,080
Yeah.

566
00:38:13,080 --> 00:38:19,880
The terminology is actually what we tried to do is, is deep fake and deep faker, right?

567
00:38:19,880 --> 00:38:24,000
That's, or manipulate, you know, and that's the person that's trying to create the deep

568
00:38:24,000 --> 00:38:30,600
fake and then attacker, an attack and disruption, right, is kind of the, the person that's trying

569
00:38:30,600 --> 00:38:35,200
to, yeah, do an adversarial attack on the image to prevent the manipulation of it.

570
00:38:35,200 --> 00:38:39,760
So in some sense, the attacker is defending against something done onto them in this scenario,

571
00:38:39,760 --> 00:38:40,760
right?

572
00:38:40,760 --> 00:38:44,000
That's kind of hard to keep the terms, yeah.

573
00:38:44,000 --> 00:38:47,840
And then the funny thing is like defense is actually the deep faker that's trying to

574
00:38:47,840 --> 00:38:50,800
defend against, you know, the attacker, right?

575
00:38:50,800 --> 00:38:51,800
Right.

576
00:38:51,800 --> 00:38:56,640
And in this case, actually, so in the, in the paper for the blurring thing, we propose

577
00:38:56,640 --> 00:39:01,160
kind of like a different kind of iterative heuristic method, which is faster than expectation

578
00:39:01,160 --> 00:39:07,480
over transformation, but as effective, for at least for our scenario and the, and that

579
00:39:07,480 --> 00:39:11,120
experiment that we did, expectation over transformation is great work and all of these,

580
00:39:11,120 --> 00:39:16,040
you know, honestly, all of the works that I saw in that in, in this paper are just, are

581
00:39:16,040 --> 00:39:20,440
just really great and just like big steps in, in this kind of field.

582
00:39:20,440 --> 00:39:23,920
And I respect all those people like immensely.

583
00:39:23,920 --> 00:39:29,080
So it's just hard to list all of the work that has kind of influenced what we do.

584
00:39:29,080 --> 00:39:35,440
So the paper kind of explores these areas and what didn't we cover yet?

585
00:39:35,440 --> 00:39:37,520
I mean, I think, I think we covered everything.

586
00:39:37,520 --> 00:39:41,960
One of the things is that maybe neglected to say is that it's, this is another really

587
00:39:41,960 --> 00:39:43,840
interesting kind of thing about the papers.

588
00:39:43,840 --> 00:39:48,640
So if you have these blurring things, so if you have different types of blur, you can

589
00:39:48,640 --> 00:39:50,920
have like different magnitudes of the blur, right?

590
00:39:50,920 --> 00:39:55,280
And every time you're attacking a different kind of blur type, you're attacking at a different

591
00:39:55,280 --> 00:40:01,920
type of kind of like scale, or if you think about it, you're adding like higher, higher

592
00:40:01,920 --> 00:40:05,880
frequency noise and then you're adding lower and lower frequency noise.

593
00:40:05,880 --> 00:40:08,960
So that's why we call, so our attack is called spread spectrum attack.

594
00:40:08,960 --> 00:40:13,800
It kind of is inspired in a spread spectrum water marking where you could put a water

595
00:40:13,800 --> 00:40:19,480
mark in a lot of different in the frequency domain, a lot of different kind of in the frequency

596
00:40:19,480 --> 00:40:25,960
band, basically, not just not just in one, but just in in a spread, you know, spread spectrum

597
00:40:25,960 --> 00:40:26,960
manner.

598
00:40:26,960 --> 00:40:31,320
This is kind of the idea of of that defense that we, though we present in the paper.

599
00:40:31,320 --> 00:40:32,320
Yeah.

600
00:40:32,320 --> 00:40:36,080
And just, I don't know, just for future work, there's so much interesting stuff there.

601
00:40:36,080 --> 00:40:40,800
Are you continuing work on this or are you working on other projects now?

602
00:40:40,800 --> 00:40:46,280
Yeah, definitely. So I tried to like double my efforts in this because I feel like this

603
00:40:46,280 --> 00:40:53,000
is one of the project that has really given me, you know, a lot of ideas and different,

604
00:40:53,000 --> 00:40:54,760
and so many different directions.

605
00:40:54,760 --> 00:40:57,880
So it's actually a little bit stressful because there's maybe a little, a lot of ground

606
00:40:57,880 --> 00:40:58,880
to cover.

607
00:40:58,880 --> 00:41:04,080
And if anyone is listening to this and wants to collaborate, yeah, just send me an email.

608
00:41:04,080 --> 00:41:08,600
You can find it on the, on the paper and then and we can set up a collaboration because

609
00:41:08,600 --> 00:41:13,040
I think, you know, there's at least like three to four directions that are very different,

610
00:41:13,040 --> 00:41:14,320
but also super interesting.

611
00:41:14,320 --> 00:41:19,400
Well, in a 10 year, thanks so much for taking the time to share a bit about what you're

612
00:41:19,400 --> 00:41:20,400
working on.

613
00:41:20,400 --> 00:41:21,400
Yeah.

614
00:41:21,400 --> 00:41:22,400
Thanks so much.

615
00:41:22,400 --> 00:41:26,920
This is a great opportunity and very, you know, best of luck with all of your next shows

616
00:41:26,920 --> 00:41:29,640
and with all of this craziness that's happening right now, right?

617
00:41:29,640 --> 00:41:30,640
Yeah.

618
00:41:30,640 --> 00:41:31,640
Yeah.

619
00:41:31,640 --> 00:41:32,640
Yeah.

620
00:41:32,640 --> 00:41:33,640
Yeah.

621
00:41:33,640 --> 00:41:34,640
How are things for you?

622
00:41:34,640 --> 00:41:35,640
You're staying, staying inside, staying safe.

623
00:41:35,640 --> 00:41:36,640
All that.

624
00:41:36,640 --> 00:41:42,240
Absolutely, like everyone should stay inside as much as possible and just go outside to

625
00:41:42,240 --> 00:41:45,240
shop or, or, you know, as much as you can, right?

626
00:41:45,240 --> 00:41:48,760
As much as, like, allows again, yeah.

627
00:41:48,760 --> 00:41:52,720
But definitely here in Boston, there's nothing going on, everything's closed and BU has

628
00:41:52,720 --> 00:41:54,320
been closed, you know, I don't know.

629
00:41:54,320 --> 00:41:58,440
Maybe we'll be like this for, uh, for around, you know, four months, three months.

630
00:41:58,440 --> 00:41:59,840
Yeah, who knows.

631
00:41:59,840 --> 00:42:00,840
Awesome.

632
00:42:00,840 --> 00:42:03,480
Well, thanks so much and, uh, again, take care.

633
00:42:03,480 --> 00:42:04,480
All right.

634
00:42:04,480 --> 00:42:05,480
Have a good one.

635
00:42:05,480 --> 00:42:06,480
Yeah.

636
00:42:06,480 --> 00:42:07,480
Yeah.

637
00:42:07,480 --> 00:42:08,480
Yeah.

638
00:42:08,480 --> 00:42:09,480
All right, everyone.

639
00:42:09,480 --> 00:42:11,400
That's our show for today.

640
00:42:11,400 --> 00:42:17,200
For more information on today's show, visit twomolai.com slash shows.

641
00:42:17,200 --> 00:42:40,120
As always, thanks so much for listening and catch you next time.

