Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting
people doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
Before I introduce this week's guest, here's a quick reminder about the upcoming Twimble
Online Meetup, which will be held tomorrow, September 12th, at 3 o'clock, specific time.
The discussion will be led by Nikola Kuchereva, who will be presenting learning long-term
dependencies with gradient descent is difficult, by Yasuo Benjio and company.
Of course, it's best if you've taken a look at the paper in advance, but even if not,
you're sure to learn something from the discussion.
For more information or to register, visit twimlai.com slash meetup.
See you online tomorrow.
If there's any one topic that I've received a bunch of requests about covering on the
show, it's the subject of this week's discussion, evolutionary algorithms and machine learning.
My guest this week is Risto Mikulainen, professor of computer science at UT Austin, and vice
president of research at sentient technologies.
During our talk, Risto and I discussed some of the things sentient is working on in the
financial services and retail fields, and we dig into the technology behind them, evolutionary
algorithms, which is also the focus of his research at University of Texas.
I really enjoyed this week's interview and learned a ton, and I'm sure you will too.
Before we get to the show, a word about this week's sponsor, Clydeira.
You probably think of Clydeira primarily as the Hadoop company, and you're not wrong
for that.
But did you know that they also offer software for data science and deep learning?
The idea here is pretty simple.
If you work for a large enterprise, you probably already have Hadoop in place, and your Hadoop
cluster is filled with lots of data that you want to use in building your models.
But you still need to easily access that data, process it using the latest open source
tools and harness bursts of compute power to train your models.
This is where Clydeira's data science workbench comes in.
With the data science workbench, Clydeira can help you get up and running with deep learning
without massive new investments by implementing an on-demand self-service deep learning platform
on your existing CDH cluster.
To learn more about the data science workbench, visit twimmaleye.com slash Clydeira.
And for a limited time, Clydeira is offering a free drone to qualify participants who register
for a demo.
Again, the URL for that is twimmaleye.com slash Clydeira.
And now on to the show.
All right, everyone.
I am on the line with Risto McGulainen, who is vice president of research at sentient
technologies and a professor of computer science at University of Texas at Austin.
Risto and I recently tried to connect at the O'Reilly AI conference in New York.
Unfortunately, we weren't able to do an in-person interview, but we were able to get on the
line together and I'm really looking forward to this conversation and I think you'll really
enjoy it because we'll be talking about something we haven't talked about yet on the podcast
and that is evolutionary algorithms.
So Risto, welcome to the podcast.
Thank you.
Pleas to be here.
It's great to have you on.
Why don't we get started by having you tell us a little bit about your background?
Okay, yes, so I got my PhD at UCLA 1990 and was working at the time of neural networks,
which I've continued doing all my career, but got also interested in evolutionary algorithms
about the same time and have gradually drifted more in that direction and especially in the
intersection of those two things.
So we've been evolving neural networks since the early 90s and you can view that as one
way of training neural networks in domains where you cannot really do other types of learning
like deep learning, supervised learning.
So that's been my research focus.
I've also originally I did a lot of work in natural language processing with neural networks
and then understanding the visual cortex with neural networks.
Most recently, this evolution-based work has focused on building robotic control game
characters and more recently solving problems in the real world using using this technology.
And that's what I'm doing at Sentiant.
I'm on Lee from UT Austin, I've been for two years trying to take the technology to
the real world and it's been a lot of fun.
So tell us a little bit about what Sentiant does and what the focus is there.
Sure, Sentiant's been around actually for 10 years, although they came out of stealth
only three years ago or two years ago.
That's a long time in stealth.
Well, it took a while to develop all the infrastructure.
So initially, Sentiant was doing stock trading and still is doing stock trading as one of
the applications.
It's a great first application because you don't need to have much customer support and
other things like that.
You just run your algorithms and they trade directly and make money, but it took a long
time to build those algorithms and also build the infrastructure, the computing infrastructure.
So evolution is one of those technologies that requires a lot of computing power to excel.
And here at Sentiant, we built that kind of a computing power distributed system across
the internet using, at that time, two million CPUs to evolve stock traders.
And that was very successful and it was a lot of fun to build that.
That was the first product and then we changed the course to added another direction rather
going to e-commerce building intelligence interfaces to e-commerce.
So there are two products there.
One of them is Sentiant Aware, which is a visual interface to catalog of products that
understands what the user wants.
It's more like a personalized assistant that you might find in Nordstrom or other fancy
department store.
You click on a choices and the system will understand what it is that you're looking
for and a couple of clicks will get you what you want.
The other one is Sentiant Sentiant, which is a way of using evolutionary computation to
design web interfaces so that they are most effective.
For instance, optimizing conversion rate on an e-commerce website is one of the applications
of that technology.
So those are the three products.
But behind all that, we develop, we are technology companies, so we develop evolution
computation methods, deep learning methods, combinations of those as well as surrogate
optimization and back by optimization in various domains.
So that's our technology pace.
Okay.
Is the, actually, I'll get back to that.
You mentioned two million CPU cores.
Is that the cloud, is that distributed or those in your own data centers?
Yeah.
That's interesting.
This is like a grid machine, sort of like folded or sitting at home.
Yeah.
Exactly.
Same idea.
Utilizing freely available compute time.
I mean, not freely available, we prefer it, but available idle compute time in around
the world.
And this is, I think, a really interesting proposition and there's so much compute
distributed now in people's laptops, PCs, cell phones.
And if he could harness that, we could do a lot of computing.
But of course, things change.
That was the CPUs were perfectly good in the first several years when we were evolving
stock traders.
More recently, we evolved deep learning neural networks.
And the need is a little different because they are trained on GPUs.
So now we have to have access to GPUs and the demographics has changed a bit.
And this is a very rapidly changing space.
It's also become very economical to buy a bunch of GPUs, thousands of them perhaps, and
build your own center.
So we are continuous looking for opportunities to harness computation, whatever form it is.
But we have expertise in both kinds of setups.
Okay.
And so is the company still using this highly distributed grid like infrastructure, or have
you shifted to an owned infrastructure?
Yeah.
We're still using it and we also exploring other ways of getting the computation.
They are, they serve a little bit different purposes.
One of them is very massively parallel and not necessarily that reliable.
And that's good for some applications.
And in others, you need more reliability.
The compute has to be accessible all the time and it has to finish.
And therefore, you need different kinds of sources for compute and we are pursuing several
sources as a result.
And you give us examples of the kinds of applications that work well and for each?
Yeah.
So for instance, a stock trading is very robust because we are evolving traders and evolution
in general.
Evolution in general is robust because you have a population of solutions and you are testing
that population in a distributed fashion across the different compute sites.
And if something goes wrong and you don't get back an evaluation of one of your candidates,
that's okay.
You have hundreds of other candidates.
It's okay to lose one candidate or have it only partially evaluated because there's
a lot of noise in this evolution research process and it actually thrives on diversity
and multiple alternatives.
So it's not depend on any single alternative or an accurate evaluation.
It's based on evaluating a lot of candidates and many times.
And that you can do on a compute source that's very unreliable.
Now if you are building a particular application based on say deep learning, you want that
deep learning network to be very well built and very well trained and reliable and sometimes
that training will take several days.
So it has to be a reliable source so that you can get back that result of the training.
Otherwise, you waste a lot of time on it.
So that's where we need more reliable sources.
Okay.
And so is the company still working on financial services or did you migrate or pivot from
financial services to the e-commerce solution?
You know, we definitely working on trading as well.
It's a more like an AI based hedge fund than financial services.
And there we have expanded to different markets.
So that part is growing and the algorithms have also evolved to some degree, but the
base is still to run a hedge fund using AI.
So the traders are automatic.
They actually are looking at the stock market and deciding what the trends are and what
should be done and then to make those trades.
Of course, in the end, there's a person also watching this with a hand on a red button
in case something goes wrong, but that almost never happens.
I don't think it actually has happened in that urgency.
But sometimes also the stock market is interesting that there might be something happening in
a stock market.
You can tell that now this is not a usual situation.
It's going haywire and we'll stop the trading and let it come back and become more normal.
So there's a human in the loop, but it is interesting that AI can do much of this on its own.
And are you trading your own book or are you working with other hedge funds and helping
them trade?
Yeah.
Currently trading our own funds.
And there's a plan to open it up in the future for investors.
Interesting.
I wonder why, if you've got that working, it's kind of this perpetual money machine, right?
Why even bother with retail?
That's a good question.
Well, in the AI world and in the startup world and in the Silicon Valley, it's always
grow scale, get more.
And this was a successful, is a successful technology, so now the question is, what else can
you do with it?
And indeed, it's nice to have several different technologies and bases, and especially then
because you start getting a cross pollination of them.
So the second one we looked at was very different from Evo's algorithms, was deep learning.
And we've found that we could use it to encode human preferences, what humans perceive
as similar and visually, and identify that this would be something that the retail industry
could really use because retail is changing rapidly and fundamentally.
From the brick and mortar stores, we are changing into internet-based commerce.
And this is a really big change, but internet commerce is quite clunky still today.
It's hard to find what you want in those web interfaces.
And the reason that was pretty obvious was that you have to know what you're looking
for.
And you have to know the keywords for those items that you're looking for.
And if you do, you may be able to find and do your satisfactory, some product that you
interested in, but if you don't, it can be very frustrating.
And as a matter of fact, we've done some tests on that, typically in an e-commerce site,
in a week, 15% of the catalog is actually seen by the users because they have to go
through these rigid categories and keywords.
And it's very hard to find different variability or diversity in a catalog.
We identified very early on that this is actually something that could be done differently.
We could use human-like perception of similarity, visual similarities.
So if you're thinking of, say, a catalog of shoes or jackets or sunglasses, then a human
can identify something that he likes.
So he likes very quickly by being presented a bunch of alternatives.
This is the one I like the most.
And then another set of candidates comes up.
And these are now based on the first click.
They are similar to the first click, but variation around that area.
And now you can click again.
And it turns out, in about seven clicks, you can find anything in the catalog, according
to our evaluation.
And in that process, the users actually see about 70% of the catalog each week.
And this is a situation where everybody wins.
As a user, you have access to more variety.
And the e-commerce vendor will sell a motor catalog, and the manufacturers get their
products out there for people to see.
So it is just fundamentally a better way to access e-commerce catalogs.
And it's based on understanding human perception and training machines, turning neural networks
to represent those similarities.
That sounds like a really interesting application.
So both...
Well, actually, you said the financial services you started with applying evolutionary algorithms
and with this e-commerce application, the focus is more on deep learning.
Exactly.
Is that the right way to think about it?
Yes.
That's exactly right.
Maybe let's dive into evolutionary algorithms now.
Tell us, you know, you gave us a little bit of a taste of it earlier, but what does that
mean?
All right.
So evolution, and you can see there's this form of similar to reinforcement learning,
where you do not get the correct answer to learn from.
I mean, in deep learning and most of the machine learning applications that are out there
are based on these large data sets, where somebody has collected the data.
This is a situation, and this is the right categorization or action at that situation.
For instance, you know, images and their classification, categorization, or speech and a transcription
of the speech.
And this is very doable, and we have systems that have come up, mechanical turk and
mighty AI and others who are now collecting and forming such data sets.
And then systems like deep learning can be involved to build models of those kinds of
tasks.
And it's very powerful, but there's a lot of tasks in the world where those actual correct
answers are not known.
So for instance, driving a car, a robot navigating, playing any kind of game.
You don't know what the actual optimal answers are.
And in such domains, the learning is based on exploration.
You try out things and see how well they work.
In a big picture, this might be called reinforcement learning.
You get reinforcement to your actions.
Now reinforcement learning, actually, as a term refers to a category or class of algorithms
that is a little different from evolution into the social or traditional sense.
They are mostly those reinforcement learning items are mostly based on value function approaches
where you list all your actions and learn how good each action is in each state.
And that's a different approach to solving a problem when you don't have gradients, when
you don't have supervised targets.
Evolution is a different approach.
The idea there is that you have a whole population of solutions and you evaluate each solution.
And as in biology, those who evaluate well get to reproduce more and generate offspring.
And the offspring is somehow generated in various ways from the parents.
You take an encoding of the parent and encoding of another parent.
And then you cross them over just like in biology.
So there's some kind of a linear encoding us like DNA.
You can take part of that DNA from one parent and another part from another parent to form
an offspring.
And that means that you are combining properties of both parents into the offspring.
And this is the fundamental idea of the evolutionary search.
You are trying to recombine components or schemas or building blocks that are sometimes
called to find better combinations of them.
And there's another component which is important as well.
That's my question.
Yeah.
Before you dive into that, I have a question about something you said about
reinforcement learning.
And that is you said that the reinforcement learning is characterized by these value functions.
And in order to evaluate these value functions, we apply gradient-based approaches.
And you said that in order to do that, we can only do that under supervision of some sort.
And elaborate on that because we're typically applying, as you said, reinforcement learning
in what we think of unsupervised problems.
Right.
So, supervised learning is different from reinforcement learning.
So that supervised learning means that we do have correct answers and we can calculate
gradients.
In reinforcement learning, we don't have those gradients.
We don't have correct targets.
So it's based on exploration.
So in reinforcement learning, the agent will try out different actions and will eventually
get an evaluation of how well those actions worked out.
And then dynamic programming is, incremental dynamic programming is typically used then to
propagate that reward back to the earlier actions and changing their value in this representation
of how good each action is.
So it is not supervised.
It thinks it's a little confused because, confusing because this, in this purest form,
this kind of value function learning applies to a table of actions in a table of states.
But that is very limiting.
So we need to be able to approximate a large number of actions and large number of states.
And that means that a lot of times we use a function approximator and then use gradient
descent to build that function approximator using the propagated values as targets.
So we use gradient descent as a way of generalizing reinforcement learning.
But the information for reinforcement learning comes from the outcomes of these exploration
episodes.
Right, right.
And one of the challenges with reinforcement learning is that you are, sorry, way to articulate
this year, kind of weighing exploration and exploitation and your exploration tends to
be focused around fixed paths.
And you don't really have a built-in mechanism to combine different exploratory directions.
And that sounds like that's one of the advantages of evolutionary algorithms.
Yes.
This is absolutely right.
So there are two main challenges for reinforcement learning.
One of them is this large search space.
Like I said, it works really well if you have a smaller space where you can enumerate
all your actions and states.
And it's difficult when those grow and become maybe continuous.
And another problem is that it works well when your state contains all the information
from the past.
It's uniquely specified or exactly specified.
If you have partially observable states, then it becomes very difficult to learn using
this kind of value function approach.
And both of those are actually addressed if you evolve, use evolution to construct neural
networks in such tasks.
So neural networks work very well when they are in continuous domains.
You don't have to enumerate all the possibilities.
They are already doing the function approximation.
And regarding the second problem, you can evolve recurrent neural networks.
And when you have recurrency, your entire sequence of actions that you've taken up to this
state is taken into account in making the next decision.
And therefore, it can disambiguate much of the state ambiguity and therefore work better
and partially observable problems as well.
So that is true.
You have to have a decision-making system such as a neural network as a base.
But then you can evolve that neural network in order to get over these two challenges
to reinforcement learning.
One challenge to reinforcement learning is the issue of reward attribution.
Is that covered under the partially observable state issue or is that a separate category
of challenge?
And how was that addressed by evolutionary algorithms?
Well, they are related.
So reward attribution, if you have a partially observable problem and you make a decision,
part of the credit for that success for that decision depends on how you got to that
state, those unobserved variables.
So if you have a system that takes into account all the actions that got you to that state,
you are doing the credit assignment better and more accurately.
So yeah, in that sense, they are related.
So maybe is there a way to, you know, granted that a podcast is limited and it's been with,
you know, not having a visual component here, but is there a way to kind of walk us through
the, you know, the setup for an evolutionary algorithm and how it's applied to training
neural networks?
Sure.
And I've been vigorously waving my hands here, like you haven't been able to see all
along.
So yeah, the big, big starting point that's different from reinforcement learning is,
is the population.
So you have a collection of individuals and typically it's 100 to 100 maybe.
In some cases, different, different variances might be smaller.
And so what is an individual?
Yes.
It's indeed individuals represents a solution, a potential solution to the problem.
Like in this case, in the simplest case, it could be a neural network.
And it means all the weights and all the nodes, weight values and all the nodes and the
structure of the neural network.
Okay.
So it's pre constrained by architecture.
You've defined an architecture for this target neural network and you're using the evolutionary
algorithm to figure out its various parameters as opposed to evolve the network architecture
itself.
Well, that's the simplest way of doing it.
But the most powerful evolutionary neural network systems actually evolve the architecture
as well.
Oh, right.
You have to have a representation that can be encoded into a, into a string typically
or a tree, but most often a string like DNA.
So you have a DNA representation of that solution and it may include the weights on the connections,
making, include the hyper parameters like the slope of the sigmoid or something like that.
And it may include the topology, like how the graph is actually connected between nodes.
It's just an issue of how you encode it.
But the, but the end result is that you will have some kind of a string representation
or a tree in a more general sense for each of the individuals, for each of the neural network,
the solutions.
And now you, and now you have a population of them.
And then on that population, you run an evolutionary algorithm and there are many flavors of those
what I've been talking about is, is the genetic algorithm flavor, John Holland's tradition.
There are many others too.
But that is the most closely associated with biology in that you take each individual,
you test them in a task, you evaluate them in a task.
And that's where the parallel computing comes in because you have a population, you can
send each individual to a different machine across the internet to be evaluated.
And sometimes most of the time the evaluation is the most computationally expensive part
of the algorithm because you may be driving a robot or you may be doing stock trading
or whatever it is that you're doing, it takes time to evaluate.
And that's really nice because evolution algorithms parallelize very well in that sense.
Each individual can be evaluated separately.
And then you get back to your values, the fitness values, how well they perform in the
task.
And now the entire population is associated, each one individual is associated with a fitness
value and you can find the best ones and you can find the worst ones.
The worst ones you throw away and the best ones you pair up so that you can do crossover,
as I mentioned earlier, and also the second component of evolution, the mutation.
Also, it gives you new combinations of building blocks or schemas, partial solutions.
Mutation creates new ones because it's not necessarily, you usually initialize the population
randomly, like random weight neural networks, but it doesn't guarantee that you have the
weight in the right place when you need it.
So mutation is a mechanism for creating that.
It's a random change in the weight value or random change in the topology that happens
on a low probability, about 2% or 4% or so, as part of that evolutionary step.
So you take in your parents and you create their offspring and that offspring then is
used to replace those poor individuals that were thrown away.
And that creates your new population, a new generation, and then this process repeats.
In this process, in essence, you are doing a parallel search in the solution space, starting
from your 100 initial random individuals, you gradually reward those that perform the
best and you perform more search in the areas where there's better solutions.
But it's in parallel.
So you're still not following just one potential kind of solution, but you are in parallel
focusing on multiple potentially good solutions.
And eventually, then you find some really good ones and the algorithm focuses on refining
that area and finding the absolute best in a smaller area around the best solution.
That's what ends up happening.
And that's a evolution algorithm in a nutshell.
As I said, there are many variations and they're also interesting to talk about, but that
principle of parallel search in a solution space, which is directed by these periodic
evaluations in the real world, is the essence of the method.
Okay.
So it sounds like, maybe perhaps not if applied to a deep neural network, but if applied
to a more simple type of machine learning model, essentially what we're doing is a search
of the solution space that could be analogous to a grid search of hyper parameters.
But in the evolutionary world, we're able to, we're able to constrain our search and
as well as parallelize it.
And so I'm imagining that, you know, from like a big O perspective, this is, you know,
log, log and time as opposed to N or something like that with a grid search.
Am I thinking about that the right way?
Yeah.
That's intuitively.
I think that's correct.
It's, of course, a different question of can we prove that's results, but that is
in essence what's happening.
So it is, it is not laying all your eggs in one basket.
It is a pursuing multiple alternatives at once and it's then gradually focusing the search
where the most promises, whereas creatures is just brute force approach, you spread your
all your individuals as wide as possible and find something in there and end up wasting
a lot of time and also not necessarily finding the very best one because you are limited
by the, by the grid.
So in that sense, that you could think of it as more intelligent version of that, much
more intelligent.
It's actually sometimes amazing how efficient it can be.
So we've done a sentient, a couple of demonstrations in a multiplexer domain, which is a very easy
domain to solve symbolically, but you can create it into a, turn it into a search problem.
You have to find the right encoding of bits so that you get the right answers to the,
to any given address from your bit string.
Now the search space is huge in this case.
It can be, and we've solved 70 bit multiplexer, which the search space is two to the two to
the seven days.
It's a very large search space and it's still evolution can find solutions quite quickly
in hundreds of thousands or millions of trials.
You'll find solution in that, that, that, that magnificently large space.
Can you take a step back and elaborate on that problem and how it's applied or how it's
used in practice?
Well, multiplexer is, it's a benchmark problem.
It's not something that you would solve using evolution algorithm.
It's only used to illustrate how, or evaluate the different algorithms and how it works,
how, how well they work.
You have a number of address bits and then you have a number of data bits.
So that is your individual.
And then what you're learning is rules, how to map those address bits to a data bit.
And that's what the multiplexer does.
Given an address, it gives you one of the data bits.
So you can evolve it to solve that in various ways, you evolve rules and the rules express
that, okay, if we have this, this and this bit in the addresses, then this is the output.
So now the space that you're searching is the set of all, all rule sets.
And that space is humongous and we can calculate how large it is.
This is worked done by John Kosa a long time ago just to demonstrate how effective learning
algorithms can be, evolution, learning algorithms can be.
At that time, I think we were considering 11 multiplexer, which has a search base of
10 to the, I think it's 616.
And you can solve it by searching this set of rules.
And in the rules, you have a certain way of identifying the input bits and then ending
and auring them and so on, performing logical operations so that in the end, you get the
data bit and you can calculate how large that search base is, how many ways there are
to combine these input bits, address bits and the logical operations on them.
And therefore you can estimate how large the search base is and how hard the problem is.
So in 11 case, it's 10 to the 616th and we expanded it to the 2 to the 7th, and it still
works.
And that is, I think to me, it's very amazing that evolution can very quickly identify
what actually works, those component solutions, those building blocks and then put them together
into a solution when search base is huge.
So that is power.
But not everything is, of course, amenable to evolutionary computation.
Before we jump into that, you mentioned provability.
Can you elaborate on the work that's been done there?
For example, you mentioned in describing evolutionary algorithm that you kind of throw
away the worst performing models or parameter sets, and then you mate, if that's the right
word, the best performing ones.
And you know, is there a formal proof that we're not susceptible to some local minimum
maxima, and you might get some better result by mating non-performers and performers?
Yeah, there's quite a bit of theory on evolutionary algorithms.
It started already in the 70s with the Schema theorem, and that says the Schema theorem
states that in this process, the shorter the Schema's are, the combinations of elements
in the solution, in that bit string, in that genetic string.
The shorter they are, and the more powerful they are, the more strongly they actually affect
the fitness of the organism.
Then more prominent, they will be in future generations, they will become more prevalent
in the population, and that is a theoretical result.
The Schema theorem shows that this happens in this mechanism, and how does a short Schema
apply to a practical problem?
What does that mean?
Well, so Schema lengths means that you have, well, so let's think of a genetic encoding,
for instance, a string of weights on a neural network.
So the Schema means that a good neural network has this weight on this connection, this
weight on this connection, and this weight on this connection.
That's a Schema of length 3.
And it may be that indeed you have these kinds of interactions between weights in a neural
network, and you have to set them right in order for the network to perform well.
And the Schema theorem just says that if they are short, small segments of weights, or segments
of the neural network that are powerful, they are easier to find.
Now if the Schema covers a large number of weights, that's more difficult to find.
Obviously, it makes sense, right?
So you have to set more numbers correctly in order to see the benefit.
And the Schema theorem just says that if you have short Schema's easy ways of gaining
the benefit, finding these three weights of the right values, that's going to be very
easy to find, and it will very quickly become prominent in the population.
And from the point of view of biology, that makes sense.
I mean, if there are some genes that are very short, I mean, just single genes instead
of interaction with multiple genes, that's going to be very prominent and very quickly
propagate to the population.
And that's the mathematics of Schema theorem just expresses that idea.
And when we're training with evolutionary algorithms, is the Schema length a constant,
or does this method kind of zoom in and zoom out to different levels of resolution?
Yes, exactly.
It will do that.
We don't know ahead of time what the Schemas are like.
So we define the encoding.
And you try to put in as much insight into that encoding as possible.
You try to define a search space where you believe, first of all, that the solutions lie
in that search space, and also that it's easy to move around in that search space.
And that is that requires human thinking and creativity, but evolution will then find
you the best combinations of those elements in that search space.
But encoding matters.
In some cases, it is obvious and it's given by the domain very much.
Like in neural networks, it's an obvious encoding to have ways put together into a string.
But then when you think about it, some more you realize that he actually is a topology
of the neural network matters as well.
And then you want to have some way of encoding topology and letting evolution discover different
topologies.
And then you discover things like, well, if in order to make it easier to search the space,
which now became much bigger because it's all the topologies in addition to just all
the weight values, a clever idea is that let's start with a very small neural network, a
small search space.
Initially, we just start with neural networks that connect inputs directly to the outputs.
No hidden nodes at all and let evolution run in that space for a while, find good, such
simple networks.
And then we'll add complexity.
We add a hidden node and another hidden node.
And then we add recurrent connections and gradually discover complexity as we go.
And this principle, instead of starting with all kinds of topologies of neural networks
as the initial population, if we start small, if we start simple and gradually complexify
evolution is much more powerful in finding these complex solutions.
So that's what I meant by being smart about how you encode it and how you let evolution
to search the space.
It makes a big difference.
And this is one principle has turned out over and over again to be very useful.
How complex can you get with this technique?
If you come up with an encoding that can represent convolution layers and rectifying layers
and the kinds of things that we do and CNNs for computer vision, can this thing come
up with a very deep model, the types of models that we're using for object recognition nowadays
or is it more limited?
Yes.
It's a good question.
So there's really been two approaches to doing this.
And the first one I just described was that you have an encoding that in principle could
encode anything, any kind of connectivity and you build up from that.
And that is actually very good on tasks like the reinforcement learning problem where you
have to define a custom kind of recurrency that retains just the right information over
time for you to disambiguate the partially observable problem.
Very recently in the last two years or so, maybe two years or so, and other approaches
emerged and that's specifically to evolve deep learning architectures.
And there's an interesting difference in that those architectures are composed of specific
components.
You mentioned different convolutional neural networks, LSTM networks, drop out as a parameter
max pool layers, all kinds of structures now exist that people compose these deep learning
architectures from.
And it now makes sense to do this a little differently because we know that there are some
components that are useful.
Well, let's give those components as a source material or raw material for evolution.
And now you can come up with a mechanism that maybe operates in a couple of different
levels, you could evolve the weights, you could evolve the components, and then you could
evolve the overall topology that's based on those components.
And this is currently where the deep learning neural evolution is, evolving deep learning
networks.
There are multiple approaches, but by and large, they do this.
They evolve the hyper parameters of those networks and maybe the topology of the networks.
But then the weights are trained using a supervised training set like image recognition,
you mentioned.
So there's still a big benefit from evolving those deep learning networks, even if you're
applying it eventually to a supervised problem, because it's very hard to construct the right
topology for your problem.
Let's evolve and do that.
And then use the training to set the weights.
Weights there are millions, billions of weights, but potentially it's very hard for evolution
to get every single one right if it needs to do a mutation to get the weight value right.
Deep learning is much better to doing that.
But the topology matters as well, and the architects of the components matters as well,
and hyper parameters matter, and that we can optimize using evolution.
So help us understand, so what you just said was the evolution is better for the architectural
identifying the architectural solution, the connectivity, and the types of layers, and
things like that.
And traditional deep learning training techniques are better for the weights.
Why exactly is that?
You mentioned because there are a lot of weights, but I thought that was an advantage of the
evolutionary approach.
Well, again, so it's actually a very simple point that if you are evolving the entire network
including the weights, then evolutionary operators need to set the weight values.
And that means crossover or mutation.
And mutation means that you are changing each weight randomly.
And if you have a million weights, that's a very slow process.
In contrast, something like deep learning of backpropagation, stochastic gradient descent,
every time you show an example, you can change every single weight.
So there's a lot more parallelism, it's a lot more efficient way of changing the weight.
So what we're saying here is, if you've got training data, use traditional training
techniques gradient descent, which uses that training data to accelerate training.
Yeah, exactly.
And where you don't have training data is kind of at this higher level.
Well, it seems like you kind of, if you have the training data, you should also be able
to use that for the architectural stuff.
But I guess we don't have techniques for doing that gradient descent.
That's where evolutionary comes in.
Exactly.
That's exactly the point.
Okay.
Now, of course, people do research and try to break free of these restrictions.
And you could, for instance, use evolution for the weights as well, even in deep learning
networks.
But there's an interesting approach that allows you to do it.
And that is that you don't evolve every single weight separately.
But you evolve, say, patterns of weights.
Color evolution.
Well, it's a different level of evolution.
It's called co-evolution, co-evolution of different levels to co-evolution of topology,
co-evolution with components and then weights.
But the trick here is that instead of having to set each value independently using mutation
crossover, you have some kind of a pattern that you are evolving.
And then you, that pattern, you use that pattern to derive the weight values.
And in extreme, that pattern could be given by a different neural network, a separate
neural network.
So you're evolving a neural network whose outputs then give you the weight values.
That technique is called compositional pattern producing neural net.
And it's been used to evolve these deep learning architectures.
And this is an example of how you can still use evolution, even if you don't have supervised
training data necessarily.
You could still use a deep learning network with millions of weights.
But you have to use this kind of an indirect encoding of its weights, perhaps through another
neural network.
And so when you're using evolutionary algorithms to evolve the architecture and the connectivity
of deep neural network or anything for that matter, are you applying evolutionary algorithms
hierarchically or that as an approach hierarchically like, you know, first evolve the connectivity
and separately evolve the layers or is it all done at once?
All of that is possible.
And all of that is under, under research right now, you know, I guess I should have anticipated
that.
Yes.
Yes.
Exactly.
And you mentioned co evolution.
That's a powerful approach in evolution.
And that means that you have two evolutionary processes, two or more going on at once and
they interact.
So in this case, it would be that you evolve the module that might be a convolutional layer
or some other, or LSTM type of a module.
And then you evolve a topology how you connect them together.
And that means that you have two populations.
One of them encodes a different LSTM node and the other one encodes how they are connected.
And then you can evolve them at the same time and evaluate them together as a single architecture
and then the individuals inherit the fitness of the entire architecture.
And that's a very interesting approach.
You could also do it differently if you have a, if you have a way of assigning a fitness
to say a single in LSTM node somehow, like maybe it's memory capacity or something, then
you could evolve that first and evolve a bunch of different maybe LSTM nodes for different
kinds of fitness functions.
Use those as raw material at a higher level evolution and do it sequentially.
That's also possible.
But yeah, go ahead.
It almost suggests that there's probably some analog for, for Gens, a generative adversarial
networks like generative adversarial evolutionary algorithms or something like that is.
Does that mean anything in this world?
Yes, exactly.
And that's actually how Gens kind of got started and motivated that.
Oh, really?
Yes.
So it was possible to evolve input examples that broke the deep learning network that had
been trained very well in a training set and, and, and then Jeff Kloon evolved these patterns
that were pretty much just noise, but the network's still confident to claim that, oh, that's
a dog.
And, and this is like the school bus giraffe kind of example, that kind of thing.
I'm not familiar with that example school bus giraffe.
What is that?
I'm, I may be mixing up my objects and animals here, but there is some set of famous examples
where, for whatever reason, if you give, you know, one of the same famous object detection
CNN's picture of a zebra or a giraffe or something like that, it confidently proclaims
it to be a school bus.
Oh, I see.
I see.
Yeah.
There are many demonstrations of this same, same problem that that sounds like it's one
of them.
Yes.
So you can mix the categories, but the most impressive demonstration to me is that there's
an image that looks pretty much just noise.
You cannot see anything in it, but still the deep learning system somehow declares that
to be, I don't know, a school bus or something else.
Right.
So, but this, the origin of that was that it was possible to evolve those images.
It wasn't just that we look at, look at bad mistakes in a training set, but evolve these
images that we had no constraints on what they had to be and they turned out to be pretty
much as noise looking images, noise images.
And evolution discovered that this is a way of getting the deep learning network to perform
very poorly.
Wow.
You've confident into something else.
Yeah.
That's really interesting background.
I didn't realize that.
And from there on, the whole field of generally a traversor and network study, like how could
we actually do this?
How could we have an adversarial training, the trainer or supervisor so that the network
could actually perform better?
Because you could use those images to train it as well as just break it.
And this is, so this is a very close relation and I think it's still being explored.
It's possible to evolve training sets and it's possible to evolve also these systems that
are learning from them so that they become more robust and also they develop internal
representations that are more representative.
So we can use evolution to bias these learning systems in a way we want and make it more
general, make them interpretable and more robust.
Interesting.
Interesting.
I interrupted you earlier when you were about to talk about classes of problems for which
evolutionary algorithms are particularly suited and not suited.
Right.
So this, of course, a lot of work in trying to expand the space of possible problems.
Now, we were comparing reinforcement learning with evolution.
I would like to point out that reinforcement learning has a little bit different perspective
and goal.
The idea there is to model a learning of an individual more or less during its lifetime.
So as it's living its life and it's performing every step counts and in a sense, it's like
an online method.
The typical application of evolution is offline engineering type of an application that you
do have, you do have a simulator for instance of the system you're trying to, or the environment
for the system trying to evolve.
And you can fail miserably in some of these candidates.
The only thing that counts is that in the end you have a very well engineered system.
And that is a different kind of a perspective.
You don't get penalized for your exploration in evolutionary algorithms.
You only get evaluated in the final result versus in reinforcement learning.
It's a continual lifelong learning system perhaps.
Now, and that makes sense.
I mean, evolution is an engineering approach can can be we can evolve.
For instance, controllers for finalist rockets is one thing that we did.
You couldn't possibly use it in a physical system because you would have to explode hundreds
of thousands of rockets.
But if you have a simulator for that system, you can do anything you want with it.
You can explore very wild solutions.
And as a matter of fact, some of those wide solutions developed into really good solutions
in the end.
And in the end, we have a controller for a finless rockets that keeps its table in a reliable
way.
And that's the kind of typical application for an evolutionary algorithm system.
Now, in on the other hand, if you have an online system that needs to learn online while
it's performing, that's not as easy to use evolution for that.
You have to build extra machinery for it.
But you can.
That's the kind of a reinforcement learning system that reinforcement learning initially
came from.
But it has also been to confuse everybody.
It has been used to do engineering design as well, just like evolution is used to do online
learning.
But that's not the opposite to the origin and that's that sort of the first application
would be engineering versus online learning.
You mentioned simulation and that reminded me of a question that I had earlier.
Is there a relationship in, you know, particularly in the context of the trading work that you've
done between Monte Carlo types of approaches, the evolutionary approaches?
Yeah.
So Monte Carlo's simulation means just that you randomize your domain and let and generate
new situations that way.
Now, when you use it as a solution mechanism, you're banking on the idea that even randomized
solutions are likely to be successful sometimes.
So you could think of evolution as a 2.0 of that that you actually trying to learn from
your mistakes.
You're trying to learn from your, those trials and that is using crossover on a good
at candidates and mutation on a good candidates and focus to search more.
And that is, I think, a good way to formulate the relationship.
You could even look at some of the evolutionary algorithm methods.
We've been talking about generic algorithms, which is crossover mutation based.
There are other evolution methods that are closer to something I want to call in that.
They will build a statistical model of the domain, which individual components, schemas,
are how reliable they are in predicting the fitness.
You form that kind of probabilistic model of your search base.
And then when you construct new individuals, when you construct offspring, you don't
do it based on crossover mutation, you do it statistically.
You sample from that model.
And it's still a population based approach in that sense falls under the evolution algorithms,
but it's closer to probabilistic reasoning and Monte Carlo methods and other statistical
methods.
And they perform quite well.
Those methods perform quite well too.
No, interesting, interesting.
Is there a simple way to kind of categorize or characterize, I should say, the various types
of evolutionary algorithms or approaches or the, you know, the various tweaks that have
evolved to the basic generic approach?
Well, we can attempt to do it.
Researches are very creative.
I mean, whenever you come up in this category, then they will cross the categories.
And that's part of how research works, too, that you will combine ideas across different
approaches and categories.
But generic algorithms is one where the close connection to biology is obvious, crossover
mutation.
And then there are these statistics based approaches, like I mentioned, sometimes called
estimation of distribution algorithms where you estimate the probabilistic model of what
works.
For example, from that, there are methods based on evolution strategy where you don't have
crossover, but you have a small population and you mostly using mutation to do the search.
But you make the mutations intelligent.
If you have a small population, you form, for instance, a covariance matrix of how your
mutations call vary.
Try to find these interactions specifically and then use that model of the interactions
to construct new individuals.
There's differential evolution, there's different buzzwords, I can give you a lot of these,
but there's a large number of them.
And they are based on sometimes letting go of the biological analogy and instead focusing
on the idea that what if you do have a population, how could you utilize the information in that
population to construct new individuals better?
And that is kind of the general umbrella of evolution algorithms.
And you could even think of an interesting direction trying to go towards the biology,
on the opposite direction, talking about abstracting it to probabilistic reasoning, probabilistic
and stochastic search.
But you could also go towards biology and there's an interesting idea, try to take some ideas
that are more biological, for instance, in directing coding.
The fact that in biology, DNA, the actual ribbon, like that, asic does not really specify
the full individual, there's a large network of interactions that come after generating
the proteins from the DNA and RNA.
So genetic regulatory networks are a huge part of biological construction from the DNA
to an individual.
And currently we are pretty much missing that in these algorithms.
So there's a lot of complexity in this kind of indirect encoding.
There are approaches that try to invoke genetic regulatory networks.
There are approaches that are trying to include a developmental phase, which is also big in biology.
So after the individual is constructed, after it's born, there's usually a period of learning
that happens.
Interaction with the environment and that then constructs the final individual.
For instance, human brain, we have 30,000 genes maybe in the genome.
There's no way the brain can be specified except at a course kind of instructive level,
pattern level.
Most of the brain structure is actually learned in an interaction with the environment.
Evolution just produces a starting point for that learning.
So combining learning and evolution that way is fundamentally biological and it's also
something that we should be looking into and we are looking into, people are looking into it.
This is really a fascinating area for folks that want to dig in a little bit deeper or
learn more, where is the best place to start?
Are there canonical papers they should start at?
Is there a resource that you'd like to recommend people take a look at?
Sure, there are some classic books that are about evolution algorithms and can find
genetic algorithm evolution computation books that are textbooks, Holland, Mitchell, Goldberg
for instance.
They tend to be a little bit old right now because the field is developing very rapidly.
It has been explosive growth, but the typical sources on Scholarpedia for instance and
then the tutorials that appear in the main conferences I think would be a great source.
There will be actually a genetic and evolution computation conference Gecko starts tomorrow
as a matter of fact in Berlin.
And the first two days are tutorials and lots of those tutorials are online, even some
of the videos about the tutorials online and I think that those are really a great way
to get started.
It is a big field and in a very diverse field, it's kind of interesting because evolution
drives on diversity, the algorithms drive on diversity, but the field is also tremendously
diverse.
So that is a bit of a challenge that you may get lost in all these terminology and all
the different approaches, and that's why I'm recommended that maybe starting from one
of the textbooks even if it's a little bit older is a good idea that gives you the perspective
and then looking at the tutorials and maybe there on it, you should have access to a
lot of literature on the internet.
Are there tools and frameworks or do the deep learning frameworks, the TensorFlow's of
the world, and the like?
Do they support, have any kind of support for evolutionary algorithms or are folks rolling
their own?
Yeah, so TensorFlow, I don't think currently has evolution component, but it is likely
that it will in the future.
It's an open source project and people are contributing to it, so it's very likely to
happen.
There's for instance, ECJ, evolution competition in Java, that's a big effort, George Mason
University, and that software includes many different evolutionary algorithms, and that
I think is currently the best source to get started with, software system to get started
with.
Great, great.
Well, to wrap things up, can you let folks know what's the best way to check in on you
or to get in touch with you?
Well, I have a name that's very easy to find, if you can figure out the spelling.
Yeah, so I have a website at UT Austin, where much of my research from my whole career is
always there.
I try to keep that up to speed, and of course, Cynthia itself has a set of blog posts and
web pages describing the technology we are developing here, which is evolution, competition
and deep learning.
So those sites are usually quite well up to date, and then we have pointers to more material
that you can use those as a starting point.
Okay, great, and we'll link to both of those in the show notes.
Very good.
Thank you.
All right, well, Risto, thank you very much.
This was amazing.
It was a lot of fun.
All right, everyone, that's our show for today.
Thanks so much for listening, and of course, for your ongoing feedback and support.
For the notes for this episode, head on over to twimlai.com slash talk slash 47.
I've got a quick favor to ask.
If you enjoy the podcast, and especially if you like this episode, please take a moment
to jump on over to iTunes and leave us a five-star review.
We'd love to read these, and it lets others know that the podcast is worth tuning into.
Another thanks to this week's sponsor, Claudeira, for more information on their data science
workbench or to schedule your demo and get a drone, visit twimlai.com slash Claudeira.
I'd also like to send a huge shout out to friend of the show Hilary Mason, whose company
Fast Forward Labs was acquired by Claudeira just last week.
For more on Hilary and Fast Forward Labs, check out my interview with her at twimlai.com
slash talk slash 11, and my interview with the former president of that company, Catherine
Hume, at twimlai.com slash talk slash 20.
Thanks again for listening, and catch you next time.
