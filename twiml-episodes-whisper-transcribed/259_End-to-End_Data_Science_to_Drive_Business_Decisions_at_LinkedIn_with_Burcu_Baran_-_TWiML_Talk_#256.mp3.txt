Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting
people, doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
I want to send a quick thanks to our friends at Cloud Era for their sponsorship of this
series of podcasts from the Stratidata conference, which they present along with O'Reilly media.
Cloud Era is long been a supporter of the podcast.
In fact, they sponsored the very first episode of Twimble Talk back in 2016.
Since that time, Cloud Era has continued to invest in and build out its platform, which
already securely hosts huge volumes of enterprise data, to provide enterprise customers with
a modern environment for machine learning and analytics that works both in the cloud
as well as in the data center.
In addition, Cloud Era Fast Forward Labs provides research and expert guidance that helps enterprises
understand the realities of building with AI technologies without needing the higher
and in-house research team.
To learn more about what the company is up to and how they can help, visit Cloud Era's
Machine Learning Resource Center at cladera.com slash ML.
I'd also like to send a huge thanks to LinkedIn for their continued support and sponsorship
of the show.
Now that I've had a chance to interview several of the folks on LinkedIn's data science
and engineering teams, it's really put into context the complexity and scale of the problems
that they get to work on in their efforts to create enhanced economic opportunities for
every member of the global workforce.
AI and machine learning are integral aspects of almost every product that LinkedIn builds
for its members and customers, and their massive, highly structured data set gives their
data scientists and researchers the ability to conduct applied research to improve member
experiences.
To learn more about the work of LinkedIn Engineering, please visit engineering.linkedin.com slash
blog.
Alright everyone, I am on the line with Burju Bottom.
Burju is a senior data scientist at LinkedIn.
Burju, welcome to this week in Machine Learning and AI.
Thank you very much.
Thanks for having me.
Absolutely.
So, you gave a presentation at Shrata, was it yesterday?
Yes, it was yesterday afternoon.
And you're actually part of a group of folks at LinkedIn talking about using the full spectrum
of data science to drive business decisions.
And so we'll dig into that topic in a lot more detail, but before we do, how did you get
started working in data science and machine learning?
Yeah.
So, yeah, so I work as a data scientist with the data mining team at LinkedIn, but actually
I am a mathematician, so I did my PhD in math in number theory and algebraic geometry,
which actually is a pure math, nothing to do with the applied math, we just prove theorems
and stuff.
And then after my PhD...
I still have nightmares of real analysis in grad school.
Oh.
And that was probably your favorite topic.
Actually, I don't like real algorithms also.
Like, what kind of linear algebra kind of stuff?
Yeah.
Okay.
So, yeah, I was still, yeah, I can understand your pain though.
Yeah.
So, I did my PhD in Europe, and then I came to US as a postdoc at Stanford Math Department.
I was there for two years, and then I went to University of Michigan.
And again, I stayed there another two years, but I was still thinking about being
mathematician.
I didn't have any idea about going in the industry, being data scientists, et cetera.
But during my time at Stanford Math Department, I had some friends in this area who are working
with me.
You make it sound like prison.
You don't have time.
And this was like five, seven years ago, I think, yeah.
So during my deadline, I had some friends, and they already know like, we're much into
this machine learning techniques, and whenever they have this math problem related to
their machine learning problem, they were bringing to me.
And that's where I started to understand like how machine learning works, but only the
math side of it, of course.
And then like during my time at Michigan, I was really very much into this machine learning.
I started thinking more about that.
I slowly, I slowly, you know, like, move toward this applied side of math, like, oh, you
know, by using math, you can also solve that kind of interesting problems.
And then I decided to, you know, like, one day actually I decided that I will quit
math, and then I will go into the machine learning.
But saying that, of course, not that easy, like having all this theoretical math background,
no coding or nothing, no sense of skill knowledge, just knowing the math, like, you cannot
just go and do a machine learning.
I came to this area again, I talked to my friends, and I asked them like, what else I should
learn them.
And I realized it will, this transition will not be that easy.
But then I was lucky that I, at that time, I heard about this inside data science fellow
program.
So this program helps people who has PhD in quantitative sciences to do the transitioning
to data science.
So I was accepted to that program, luckily, and they helped me actually to do the transition.
So they helped me to build the network and, you know, like, learn the right stuff and
then have some data science project, and then, you know, start to talk to companies about
being data scientists, going into interview and stuff.
And my first job was at a startup, which was, it's a six ounce, which is located in San
Francisco.
They are just be to be intelligence company.
So we were doing a lot of machine learning there, actually, like, I, I mean, I had learned
about the theory of machine learning, but of course, like, applying it, like coding and
dying of getting the results with the real data is completely different.
So this is like where I really learned how to do machine learning.
And then after two years working there, three years ago, I started to work at LinkedIn.
And since then, I'm working with the, with the data mining team at LinkedIn.
Okay.
Awesome.
I've had a number of folks from, they went through the insight program on the show.
I've also interviewed, I think Ross Fadley is at his name out of New York.
And I know a manual who runs the AI program or track out of the Bay area.
And I've had a bunch of really great conversations with folks affiliated with, uh, with insight
in, in one capacity or another.
It seems like a great program for folks that are kind of finishing, you know, transitioning
from academia to, you know, their data science or machine learning or they've got a bunch
of other projects now or programs now.
Yeah.
Exactly.
So actually, when I was in this inside, they were pretty new.
It was there.
I was there.
Second group, actually, like, this was four years ago.
Yeah.
And now they are pretty big actually, like, they have a lot of programs going on.
Yeah.
Yeah.
So maybe before we dive into kind of this, your session and, and data mining, I'm curious
about the, you know, this transition from kind of theoretical mathematician to data scientists.
Do you use any of the theoretical math?
Does it kind of impact at all the way you think about data science or not really?
Oh, honestly, the math that I learned at the PhD level, I don't use that at all.
Like, no, unfortunately, I wish I could do it, but no, because I really spent a lot of
time to, you know, understand those concepts.
But, but of course, like being in math, like, having math education for almost 15, 16
years, I'm being in math professionally for 10 years.
Of course, it has a lot of effect on how I do data science.
So obviously, like, I'm used to have this analytic thinking kind of thing.
So when I have the problem, I can put a structure on it and I can, I can see the big picture.
And then I can try to solve it.
And also, I have, I all the time do this reasoning thing, because no, I'm used to prove theorems
know, just combine all these logics.
If this is this, then this is that.
So this also helped me to think not only about house, but I also care about why.
So that helps me to have the understanding about the problem and also about their solution.
And also, of course, I'm very comfortable with the mathematical concept.
And actually, I have a story.
So this was my interview with this, with the startup, this is my first job as a data scientist.
So there was a lead data scientist there and he was interviewing me about machine learning
techniques.
And in the first half of the interview, he started to ask me about questions about those
machine learning algorithms.
And he was always asked me why you do this, why you do that?
And I was so comfortable to explain those.
No, I was comfortable with teaching.
I was comfortable with the math concepts.
And it was like, I was like, wow, I'm really rough in the interview.
Like, I know everything, like, and I like, okay, first half was great.
And then in the second half, he said, okay, you know, all these algorithms well.
Now let's go to the easy part, he said, now I want you to implement this, you know, algorithm
on the watch.
I was like, oops, there's not that easy for me.
And yeah, and I explained to him like, yeah, you think it's the easiest, not easy for
me.
And actually, it took a whole lot of time to implement this on the white board.
And then he explained to me that he said that, yeah, this is, I can understand that you
don't have any coding experience, because I had just told myself half the code.
So it was not really easy for me just to do it on the internet, especially on the interviews.
And but he said that the direction that you come from is completely different.
Usually people learns about these algorithms, how they work, how to code, and then maybe
they dig into the math of those.
So you are completely coming from different directions.
So you had a deeper understanding of how they work.
But now you also have to understand like, you know, like how to write the code and stuff.
So, and this is true, like, this is helping now.
I'm much more comfortable with the coding, obviously.
But this heading is deeper understanding of all these mathematical concepts behind those
algorithms is really helping me a lot, like, you know, like, because when you do the modeling,
it's, there's always some problems.
So if you know, like, where the problem comes from, it'll be, you save a lot of time,
actually.
Otherwise, it's really hard to debug everything and then try to find out what's going
on.
Yeah.
Um, so this, the, the session that you, uh, did at strata was called using the full
spectrum of data science to drive business decisions.
Yeah.
Let's just kind of pick that apart.
Let's start with full spectrum.
When you talk about full spectrum of data science, uh, what are you referring to?
And it almost implies that you, you, you think that at a lot of places, people don't use
the full spectrum of data science, maybe, um, you know, what, what are you getting at with
that?
So with the full spectrum, we are talking about the whole process.
Actually, we talk about this, how we do this production modeling, like the whole machine
learning process, not only just the algorithm part, but we start from the, uh, actually,
the problem definition part, how we clearly, when the business people bring us the problem,
how we clearly define the problem, so that we can convert into the mathematical problem
so that we can solve it.
And once we have the problem, we have the full definition, then we have to think about
the data that we have.
So for example, the label, if this will be the supervised learning, the label preparation
is very important.
So every, every business problem has different way of preparing the label.
So we should be very careful about this label preparation.
And once we had the label preparation, then we had to think about the features.
So, um, and when you had the features, how do you integrate with the labels, like, how
do you do the alignments on the timeline?
Uh, and you had the static features, dynamic features, how would you combine those static
and dynamic features?
And once you, you know, settle all those, then we start with the modeling techniques,
like training and we talk about the data partitioning and, uh, and, uh, we, we talk about,
like, the common mistakes that first we did and that, then, like, um, then, now, like,
we have a lesson, uh, so that, uh, we don't do it anymore.
And, uh, and then after the training, we talk about the model deployment.
So how do we give our results to, to our business partners?
And the most important part and actually the, um, most, um, challenging part is when you
give the results, your business, uh, business partners, uh, when you solve the problem, it's
not finished yet.
So because they use this, uh, this, uh, solution over and over again.
So you have to check the, if the, if the features are more, or the models is doing good.
So because you had the results the first day, everybody's confident about results, model
is fresh.
It's perfect.
But after 30 days, oops, something going on, the red flag.
So what's going on now?
So, so we have to monitor the features.
We have to monitor the model performance and there, we have to just run basic statistics
about, like, is everything stable or there's some degradation somewhere.
And, um, and once you do this, maybe you make, you might decide to do a model refresh.
When you do the model refresh, uh, you need to do the AB testing with the current model
and then decide which model to, to release.
But sometimes the, it's not the model that the problem, where the problem is, sometimes
it is the feature logic is different.
So for example, uh, let's say, um, your, one of the feature is the checking the traffic
of your web page, let's say, uh, and, uh, let's say your web page is just one page
web page, but after a month, it has just three pages.
But then checking the traffic of the first page is now changing the meaning.
So you should be aware of those kind of changes if it is, if it is there.
So, uh, because you need to, you need to be aware of the content, if you are giving the
right content to your machine learning model or not.
So, so we talk about whole this process and each steps, like in detail.
And then we also talk about some, uh, common pitfalls and challenges that we have during
this process.
Like, uh, for example, the model interpretation is one of the biggest challenges that
we have here, uh, because when we present our results, the business partners, they don't
only care about the results, they also care about why we get these results.
What are the key drivers?
So it's pretty challenging to, to, to find, especially, uh, when you have the more complex
models, it's, it's getting harder to explain what's going on.
So you cannot just, you know, use models as a black box, uh, for those kind of problems.
It's also data quality is very important and it's another challenge.
The more data is much harder to maintain the quality of the data.
So yeah, we talk about whole this process by giving examples and the, um, the things that
we need to care for what, but this was the whole, like, how we use the whole spectrum of
data science to, to give business decisions.
Well, that's a lot of, I think you're like, now that, now I understand why there are
like five or six people presenting this, uh, yeah, I thought it was at a full day tutorial
or a half day or something, three hours, actually, three hours.
Okay.
Yeah.
Uh, a lot will be online soon.
So, yeah.
Okay.
Oh, cool.
So, uh, let's maybe walk through some of these points, um, you know, you started, uh,
at the beginning, right, problem formulation, a lot of people, you know, we talk a lot
about algorithms, um, but that initial, you know, phase of really understanding the problem
that you're trying to solve and translating that into, you know, kind of mathematical
or data science thinking can be, uh, a challenge.
What, how do you approach that and, you know, what have you learned, uh, you know, what,
what do you think that people get wrong about that?
Um, actually, the, what I learned is that the domain knowledge is very important.
So it's not only only the business partner can, you know, make it like a good problem
to solve or it's not only data scientists who can, you know, just make it into math problem
and then start solving it.
It's the conversation is very important.
So you should, data scientists should really understand what is really the business partners
trying to solve, what is the background, where this problem is coming from.
And then, uh, once the data science understand that data science understand this, then, uh,
she needs to convert this into mathematical problem, but then she needs to explain like
how she views it, like where the solution is going is a solution that she's trying to
go, is the direction is the same as the business partners thinking about, I think the conversation
is really very important.
The, the, when data scientists try to try to convert this problem into mathematical problem,
she needs to understand the domain knowledge from the business partner.
So, uh, for example, I will give a very, very easy example.
So let's say, uh, LinkedIn talent, LinkedIn talent has this product called Curious Suscription.
So it's usually the people who are job-seeker by this products.
It's more about you can be the third degree connections profile and you can do advanced
job search, etc.
So let's say, uh, LinkedIn talent wants to make an email campaign for the product, uh,
Curious Suscription and, uh, they want to send email for this and how do they send, who,
who they should send those emails, basically, this, this is the question that they bring
up to us.
So obviously, for example, they cannot send it to all like 610 million members.
So we would have a, we would have another problem if they would do this.
So they, they bring this problem to us and then we start to ask this question, oh, do
we also have historical data about this?
Did you already sell this product before?
So if you sell this product before, can we get the data?
So are these people are the member, LinkedIn member, can we get more data about those people?
Once we are able to get the answer for this down, we slowly say, okay, we had the data
down, we can solve this problem.
We can just apply machine learning techniques and then, um, get the solution and then, uh,
it results, but then they say, okay, I don't only care about who, who, who, who will, who
I'm going to send this email, but I also care about why I send this email to these people
that I said, oops, then they want the model interpretation also.
So then, uh, the problem is different.
So I don't only care the performance of the model.
I also care about the interpretation of the model.
So when I'm doing the modeling, I need to balance between the, uh, performance and
also interpretation, because, um, without experience like, we, like models like random
forums, uh, XG booths are, uh, performance much better than the logistic regression, linear
regression, those kind of models, but, uh, the, this kind of ran, it's, it's hard to use
the, it's hard to do the model interpretation with the random forest.
Um, of course, there are these, uh, future importance coming from these machine learning
techniques, but we should be careful about the bias coming from them also.
So it's, it's hard to use them as a black box.
So we try to, we try to get use more, much simpler model, maybe sacrifice some of the
performance of the model.
So yeah, those are the things like, you know, they tell us the problem and then we think
about the mathematical solution, but then we, we, we, we drive the conversation from
their own and then try to find solution together, basically.
And is, is this a process that you usually able to, you know, you, you have an hour meeting
scheduled and they come to you and tell you their problem and you kind of, you're all
on the same page by the end of that meeting or is it a, a, a process that, you know, continues
uh, for an extended period of time?
Actually, yeah, so, um, it's usually like, my team is more like, uh, more like, uh,
help say, like horizontal team. So for example, LinkedIn turned ahead on data science team.
So they are in conversation with their data science team.
So this data science team already had them to convert this problem into mathematical problems
and this team is aware of all the data, everything that they have.
So they are in contact with them all the time, basically.
So this is a vertical data science team.
I am more horizontal data science team, we are more, um, technical.
So when they, when this, their data science team had this machine learning problem and
if it is a little bit more complicated than they expect, than they bring this problem
to our team and then we try to solve this together with their data science team.
So when we had the question, we usually contact with the data science team, but yeah, data
science team is all the time in contact with business partners.
So they are a way of what's going on on their side, yeah.
Do you often find that, um, that folks are kind of going down, you know, blind alleys
and you have to like, you know, you're, you're, you're engaged with folks that are kind
of far into the modeling process, but in order to actually make progress on what the,
the business is trying to do, you have to kind of start all the way over from the definition
of the product or, um, do, you know, just having kind of a data science team that you're
working with kind of alleviate that issue.
I, I guess I, I get finding conversations, you know, with, uh, you know, folks that are
in data science, supporting, you know, business units that often the business units will come
to them with kind of, you know, wild half, you know, thought out, you know, hey, I want
a model that does this and there's really this kind of walking back that you need to do
to get them away from thinking about a specific solution to what the problem that they're actually
trying to solve is, um, and I'm wondering how that manifests for you with the kind of two
layers that you have to get back to that, the business itself.
Yeah.
I mean, those things happen so few times it happens, for example, like, uh, the, the problem,
the way that they convert it doesn't make sense and we realize it when we start to check
the data.
So we realize it's something going something is wrong, you know, like it's not maybe these
are not the, um, these are not the results that they expect.
So and then you start to slowly debug then on the process from problem formulation to
getting the data, something must be broken there, then we slowly debug and a few times
you realize the problem preparation problem, there is a problem with the problem, the
definition, then we go back there together all together and we sit together.
We try to understand now we try to convert the problem together there.
Yeah, it happened actually a few times.
And is there a, you kind of talk about this debugging process?
Is it, uh, you know, what is that process?
Is there kind of a set way that you go about that or is it kind of systematically, you
know, checking your various assumptions into you, find something that doesn't line up?
So, uh, so there is some parts that we do manually and the other parts are done in the
platform.
So the ones that we do the manually, we really check if we are doing it correctly or not.
For example, the, the logic on the label preparation.
So is it, is it really the, the logic there is the right logic?
Um, I'm usually like, you know, the models that I'm talking about, they are done like many
times.
But when you do it for the first time, then there might be a problem.
So we are very careful about the, the, the problem, for example, uh, on B2B business or even
B2C like acquiring a new customer or empowering a new customer, uh, empowering, empowering
existing customer of like a turn model, uh, we have done those models before.
So we know how to, we know the label logic, we know how to do the problem definition.
But if you are, if the new problem comes and it's the first time that they have this problem,
there we need to be careful.
Like they're actually, uh, you don't immediately put the definition and then start the
soul.
So this is ongoing process.
You start to think about it and then you maybe try one prototype model and discuss
about the results and you go back again, like, okay, if we, if I had changed a problem
to this, then would I get the same result?
So, um, this, this is not a, in that sense, like, uh, it's not the all the time, like,
debugging issue doesn't come very often, but, uh, we know actually when it can come,
it's the problem generation was the first time that we might have this problem.
A lot of the issues that you describe, you, you mentioned kind of this label preparation,
uh, stage a couple of times, it seems like, especially when you start tackling a new business
problem, it's one of the areas where technical or process problems tend to creep in.
Can you talk a little bit about that label, uh, prep stage and the, the kinds of things
you're doing there in the way you approach that at LinkedIn?
Yeah.
So, um, so label preparation, for example, I will explain it with the example that I,
that I gave about the, the slink and talent once to, to the email campaign about the product
career subscription.
Um, for example, they say that they have data.
So the data is like they send this email before, um, and, um, how, how do you, how do you
prepare the label from that?
So this question has, uh, yes or no answer, right?
It's, it will be, it will be a binary classification, it will be either positive or negative.
So, but how do you define the positive, how do you define the negative?
For example, you send the email and then what?
What is positive there?
Just reading the email, clicking the, the, the link that we send or buying the product.
And what is the timeline that you're going to put like it, are if they buy, for example,
do you decide it?
Okay, if they buy, it will be positive for me, this will be my label.
Then are you going to wait for a day, for a week, for a month?
So what's the timeline for this?
You have to stop somewhere.
So deciding those things are, um, are important.
So again, you, you check the data, uh, when you are defining the label, uh, so this, this
is very important actually, this label preparation part.
And the example you just gave, those are labels that you can acquire, define programmatically
by, you know, presumably running some, you know, SQL queries against your data warehouse
or something like that.
Is that the, the majority of the types of problems that you tackle within the data mining
domain, or do you ever have, um, kind of these problems where you have to go back and manually
label, uh, a bunch of data?
Yeah.
So the logic is done manually, but as you say, the, after you have this logic done, then
you just write the code and then code just runs on the, on the, on the Hadoop or like
on, on HDFS and then, yeah, automatically we get the data.
But one, but we did, the, the logic is the most important part.
So once you had the logic, you discuss the logic and you get the code review on the logic,
if everybody agrees, this is the right logic, then the code just goes on, just we use the
same code over and over for the same kind of problem.
Mm-hmm.
And is the, you mentioned kind of this issue of, um, in the talent example, kind of attribution,
when do we say that someone's bought, uh, and also I frequently kind of hear about, um,
you know, issues around features with like, uh, point in time, correctness and, um, you
know, the whole time machine thing, uh, is that something that comes up in the types
of problems you're dealing with?
Yeah.
Those, uh, time alignment is really, really important.
So again, with the same example, so for example, um, let's say you will, you will also look
at the, the behavioral features of those people who you want to send the email, uh, because
it's important to check if they did the recent day job search or if they had a network,
and recently, for example, with the recruiters, so it's a really big indication that they
might buy this product, uh, but then the time is very important.
So, uh, you have to put some, some time and you are going to look at the, the, the, the,
um, action that they did before that time, right?
But why, why do you put this time?
Do you, do you put it when you send the email or, or like, when they clicked the link or,
or one day did a subscription?
Because after you send the email, till to the point that they buy, uh, if you look at
the, the, the action, their behavior is during that time, it might be biased.
It might be that they are just curious about the product and they might, I don't know,
wonder about something and go and check and do the search, it might not be that they
are planning to change their job or something.
So maybe it's a good idea to check those, you know, all these kind of behaviors before
you send the email, before they become aware of this product.
So deciding those kind of, um, time and then, uh, when you combine it with the label,
the feature and just putting, putting the, the, the, the, the timeline on the correct
place is, is also, it's also very important.
Uh, and of course, these are more for, uh, more for, uh, dynamic features like for snapshot
features like, uh, where they live, um, their job title, things like that, which doesn't,
uh, change very often, uh, these are not the big huge problem, but the dynamic features
are, yeah, this won't, won't be very careful about those.
So you've got your problem defined, you've got your, uh, labels generated, you've dealt
with alignment issues, uh, around your features, then it's time to, you know, start modeling
and training a model, you know, what are some of the, the kind of gotchas that come up
there?
Yeah.
So, um, so the first thing is, of course, like partitioning the data, like training,
evaluation and testing, um, of course, like this, the sizes depends on, on your data size
and, um, but one thing that we are very careful, um, uh, for example, if you had to duplicates
in your data, you should really need to deduct all this kind of data, because, um, let's
say you have same to same data and one goes to training set and the other goes to testing
set.
Right.
This is a problem.
Exactly.
Then this is the data leakage.
You should really, you should be careful all those kind of, uh, and, um, and after the
partition, then in the model training part, there are a few things that you have to be
careful, uh, like, um, for example, you have to check your system requirements.
So do you really need very fast algorithm or just your system is really, does, you don't
care about the speed of the algorithm, um, and, um, also, uh, I only talk about the
performance, why is the, uh, interpretation of the model, do you need the interpretation
or the, for your the performance is the most important one, because here at LinkedIn,
like, we usually sacrifice the performance of the model, like, because the interpretation
for most of the problem interpretation is really, very important, uh, but we also have
other problems where, for example, interpretation doesn't have anything to do.
So we really focus moral on the performance.
I wonder on that interpretability point.
Does that, is that static for a given problem or does it change and I'm kind of envisioning
where, you know, a problem is new to the business.
They don't really understand the, the data and the underlying behaviors.
And so they want something really interpretable to help them build up and intuition.
But then, you know, over time, they start to get a better feel for, you know, the, what
they're trying to model and maybe they're willing to, you know, sacrifice some of that
interpretability for additional performance.
Is that a common trajectory?
Exactly.
Exactly.
So this is, this is a very common actually.
So it happens a few times, for example, we propose them to send emails to some certain
customer and they say, no, we shouldn't send this to this customer.
Like, I'm pretty sure it's not the right one.
What do you say?
But the model says, really, you should send an email to this one.
So then they ask, why?
So those examples, we should be ready because of this and this.
So, and if we use algorithms as a black box, we can't answer those kind of questions.
We say, sorry, the algorithm says this.
So they say, no, I'm not going to send this email.
So we should also comies our sales representatives, like, like, these people are really important
and these are the reasons that, that, that they are important.
So actually, we have some in-house solution to that because sometimes like the, for example,
the performance of the, the performance of the models, which has good interpretations,
is really very low.
We cannot use them.
So we need to use, like, a good performer algorithm.
So we have a solution for that in-house solution.
Sorry, for example, let's say we have, let's say we have 300 features in our master model.
And we model with all these 300 features and we score, you know, the customers.
And then we say, and then we try to predict if they will buy some certain product or not.
And we want to know why they buy this product.
So what we do is that we divide the features into different buckets, where each bucket
has like semantic business meaning, like, you know, those features are like behavioral
features.
All features are social features.
All features are identity features.
And let's say I divide into three different buckets.
And then I build a model with the features within each of these buckets.
So I have like three different models, apart from the master model that I have.
And the important thing is that each of these models has their own semantic meaning.
And in this way, I, you know, like, stopped the correlation between the features, between
the groups of the features.
And I, this also reduced the noise.
And for example, when I score with my master model, each customer, I also scored them
with this, this, we called them component model.
I also scored them with these three component models, which has different meaning.
So for example, few times I realize that some, for example, certain person has a high score
with the master model, because they also have a high score coming from the behavioral feature
model.
On the other hand, somebody has another customer has the same thing for the social feature model.
So this tells us it, okay, so the first one got a high score because of the behavioral features
and the other person got high score because of the social features.
And this also helped us to send personalized emails.
So when we are trying to sell the product to this person, for example, in the email, we
talk about more like behavioral features.
We said, oh, you can do that much more search in your, with this product, actually.
And we thought that person might be more interested on search.
On the other hand, for the person who got high score because of the social features, we
more talk about what kind of networking that person can do, you know, if he buys this
product.
So this really helped us a lot, actually.
So yeah, this is our in-house solution that we use for now.
Up to now, it worked pretty well.
And so is this component model, is this, are you using it strictly for kind of interpretability
signal, or are you then like training some ensemble of these components, and then using
it to maybe replace the original model or supplement it in some way?
I mean, for now, we only use interpretive purposes.
Few times, if the master model does not do good, but we see a very good performance coming
from the component models, we try to combine.
So because I'm realize it, there is really big noise, you know, when all the features are
together.
So like the models became much better when you separate the features from each other.
But up to now, we couldn't say good performance coming from combination of those models.
So we just use them separately after now.
Okay.
Yeah.
Yeah.
I mentioned data partitioning, and, you know, there's kind of the usual, you know, test
train split kind of issues there, but I imagine that you have to do things somewhat differently
when you're working with $660 million, you know, users, and you have graphs and things
like that.
You know, beyond the de-duplication, are there, you know, ways that you deal with data
partitioning that are kind of unique to LinkedIn scale?
Yeah.
Yeah.
So, for example, a few times, since usually we collected data in the US, we should be careful
about, for example, if we are using the geolocation as a feature.
So it's usually its US, but we have also a lot of data coming from Europe or Asia.
So we should also be careful.
So we also, in order to have the good representative of each feature, we also do like certified random
sampling for each of the both train set, testing set, and validation set.
So we're also careful that each feature is representative.
Yeah.
This is also something that we are, we are very careful, you know, other than the de-duplication
of the data.
Are you typically building models against all of the data or are you, you know, sampling
and training on subsets?
We usually random, we do random sampling and training on subset.
Yeah.
Especially if we do the model on the member level.
So we are trying to, actually, we are trying to be careful about timeline also.
So, for example, let's say if you want to sell the product today, we look at the people
who bought last year and we only focus those members.
We don't go and check every member.
And now we do the model on those people who bought last year.
But of course, in terms of scoring, down we use most of our members in order to score
and then see who will get the highest score, who has the most probability to buy this product.
And is that the kind of locking in on a specific year is that because the, you know, your distribution
will have changed over time and you don't want to kind of confuse your model by pulling
in users from a broad period of time?
Yeah, exactly. We just define a certain period of time, that time, like we saw in this email,
did they buy in the next month or not?
So we make this definition very clear and then this would be our labels associated with
the members and then we start from there, then we attach the features to those members.
So, but the timeline is very clear, yeah.
And so the next thing on your list was kind of sharing the results to business partners.
Is this usually in the form of a kind of a deployed model that's like a service that
is integrated into some application or is it more, you know, like business intelligence
reports and, you know, decision criteria or what are the different ways that you, you
know, both deploy and communicate results to the business?
Yeah. So we show our results on our testing set, of course.
You know, we never touch the testing set during the training purposes or choosing hyper
parameter or like algorithm.
So like we have completely independent testing set and we, we apply this testing set, the
model that we chose, we think it's the best performance model.
And then this results, we need to, we need to show some results to our business partners.
So since we are, since this is a business problems, we also consider the business metrics.
So for example, the conversion rate is very important for them.
So we had to show them the conversion rate.
So it's also, they are interested in the ROC curve.
So it's a little bit technical, but they have a way to see the compare to models, because
when we present the model, probably there is another model coming from the last year.
So this is our baseline model.
So they, they are, they will be able to compare, oh, this model is doing good or bad than
last year.
Why is this so?
So this is all, you know, this will be all the discussion between the business people
and us.
So yeah, we, we prepare our, you know, the performance evaluation.
If it's a binary classification model, it will be probably a ROC curve and the, um, and
the conversion rate.
We also, of course, prepare the key drivers.
We showed the key drivers if they make sense to them or not.
So they, they have really a good domain knowledge.
So they can, so this feature doesn't have anything to do with the results.
Why did come up?
And now we go over, for example, they want to see the people who has the highest scores.
So we go over them and they check them.
So, uh, and if they see any red flag or not, then they let us know.
So we sit together for an hour and we discuss all those things and they go over the, the,
the companies which had the highest score.
And then we get the feedback from them.
And if the feedback is positive, then we release models to them, then scoring process
start and we can deploy our models.
Maybe we can kind of wrap things up with talking about how you ensure that these models,
uh, remain fresh.
You mentioned, uh, AB testing is something that you do.
Yeah.
Yeah.
So we use, we use AB testing in order to, if, if we need to refresh the model, then we
use the AB testing to see if the current model is better or the one that I, that we built,
uh, if the new model that we built is doing good.
But before that, we, once we had the model, we, we constantly do the future monitoring
and the model performance monitoring, we, we want to see the trends on the features.
Anything is decreasing or increasing, if there is an spike on the features, uh, it's because
of the, uh, I'm, I know, holiday season or something else going on.
So we should be aware all those kind of things.
Same thing with the model performance, there's something might be broken somewhere.
So it is really important to go and see, uh, to see if it's coming from the features or
it's coming from the model.
And then once we are aware of all those, then we might decide to refresh the model and do
the AB testing and decide to go with the current model or with a new model.
Yeah.
This AB testing is this, is it primarily offline AB testing or are you, um, doing it online
as well?
And, uh, if you're doing it online, how do you ensure I, I imagine at your scale, like ensuring
kind of statistical significance probably isn't something, you know, it happens probably
pretty quickly.
Yeah.
Yeah.
Yeah.
Maybe depending on the problem.
But like, how do you address the, those kind of practical issues associated with AB testing?
Yeah, from, from the customers, they just do the random sampling and then start, you
know, show, start to use the results from, from both, from both models, like we, they
get the results from the existing models, they get the results from the new model.
And then with this results, they, the sales scientists, they try to use both, both
models, um, and we, the way we design a sign, though, the results coming from the both models
are really random.
So sales representatives are not aware if they are using the current or the new model.
And then we just compare the results, like if, if the, if the results are coming same
or, or, or, or there is any, um, the significance level of confirmation that, that one can, one
should be able to use the new one.
Any, any parting thoughts, um, we covered a lot of ground and you certainly covered a lot
of ground in your presentation, any, did you leave the audience with kind of key takeaways
of things that they needed to be thinking about as they're implementing this in their
own companies?
Yeah.
So, okay.
So we talk about, you know, the each step of this machine learning process for the model,
for production, the model for business problems.
But actually, we can pack all those things, which will be, which will, which can serve not
only to you, but everybody in your team.
So because if one person goes all of this process once, it's okay, but if everybody in your
team does this over and over, then, um, it's, it's not really efficient.
So we should build something which is better, smarter, and which can, which can do this
process.
And it is doable.
So there are some platforms.
So does this?
So like AWS, Microsoft Azure, Google Cloud, you can consider to buy those platforms, being
aware their limitations, their data source, the algorithms that they use, or one can build
their own platform, which will do this process.
Or you can do something in the middle, like, uh, you can assemble both existing platforms
with your in-house solution.
So why should, um, consider what their problem is, their timeline, their budget.
And then, uh, instead of, you know, going over this, each of the set, the step manually one
by one, just make this a service.
And then, uh, one can use this service over and over.
But of course, um, uh, one should be careful about the, uh, uh, synthesis is the ultimate
sophistication.
So it is very important to make this platform as simple as possible, because it's important
to maintain this platform, also simplicity is important for the user experience.
But yeah, like, instead of going this step, this process one by one, just consider this
as a service is the best way to look at it, I think.
Agreed.
Awesome.
Well, Bertu, thank you so much for, uh, for walking me through all this.
Um, sounds like a really good presentation.
And, uh, I love the kind of closing imperative to, you know, think about how to make this process
more efficiently by supporting it via some platform.
Well, thank you very much.
It was a good pleasure to be here.
All right, everyone, that's our show for today.
If you like what you've heard here, please do us a favor and tell your friends about the
show.
And if you haven't already hit the subscribe button yourself, make sure to do so.
So you won't miss any of the great episodes we've got in store for you.
For more information on any of the shows in our strata data series, visit twomolei.com
slash strata sf19.
Thanks once again to Cloud Error for sponsoring this series.
Be sure to check them out at cloud error.com slash ml.
As always, thanks so much for listening and catch you next time.
