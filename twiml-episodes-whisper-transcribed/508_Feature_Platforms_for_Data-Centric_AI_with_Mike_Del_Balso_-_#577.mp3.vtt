WEBVTT

00:00.000 --> 00:10.120
Alright everyone, welcome to another episode of the Twimmel AI Podcast.

00:10.120 --> 00:16.200
I am of course your host Sam Charrington, and today I'm joined by Mike DelBalso, co-founder

00:16.200 --> 00:19.040
and CEO of Techton.

00:19.040 --> 00:22.880
Before we get going, be sure to take a moment to hit that subscribe button wherever you're

00:22.880 --> 00:24.680
listening to today's show.

00:24.680 --> 00:26.680
Mike, welcome back to the podcast.

00:26.680 --> 00:27.680
Thank you for having me.

00:27.680 --> 00:28.680
It's great to be here.

00:28.680 --> 00:29.680
I'm excited for the conversation today.

00:29.680 --> 00:30.680
I am as well.

00:30.680 --> 00:33.080
It's been a while since we spoke.

00:33.080 --> 00:36.080
I think the most recent time was about a year and a half ago.

00:36.080 --> 00:38.880
We were talking about feature stores for MLOPS.

00:38.880 --> 00:42.320
I suspect that topic will come up again.

00:42.320 --> 00:44.080
But let's dig in.

00:44.080 --> 00:49.520
I'm looking forward to talking about what you've been up to, the idea of feature platforms

00:49.520 --> 00:54.880
and of course the intersection with all that and data-centric AI.

00:54.880 --> 01:00.080
For those who haven't heard any of our interviews or don't know your background, why don't you

01:00.080 --> 01:02.520
share a little bit about how you came to work in ML?

01:02.520 --> 01:05.280
I first got involved in machine learning.

01:05.280 --> 01:09.960
I was a product manager at Google and dating myself now.

01:09.960 --> 01:17.000
But back in 2013, I joined the ads team at Google and as you know, Google uses a lot

01:17.000 --> 01:20.920
of machine learning to determine which ads to show people.

01:20.920 --> 01:23.520
This is before machine learning was super cool and stuff like that.

01:23.520 --> 01:27.480
I don't even think we use the word machine learning at the time, but we had a lot of models

01:27.480 --> 01:35.320
in production that were doing some real time decision making around who is this person,

01:35.320 --> 01:40.240
what kind of interests do they have, which had his most relevant to show to them.

01:40.240 --> 01:45.240
The team was really excellent at what we did not at a term we didn't have at the time,

01:45.240 --> 01:46.240
which was MLOPS.

01:46.240 --> 01:51.600
There's a lot of MLOPS that we were doing at that time, production machine learning.

01:51.600 --> 01:57.800
That was kind of like phase one of how I got involved in machine learning.

01:57.800 --> 02:05.760
After that, I actually joined Uber in the early data ML days of Uber at the time when there

02:05.760 --> 02:12.280
wasn't a lot of, there was very little machine learning happening in production at Uber.

02:12.280 --> 02:19.280
It was kind of the task for me to start the ML team and help us figure out what to do

02:19.280 --> 02:21.800
with all of this data, we had all this data at Uber.

02:21.800 --> 02:28.080
How do we help Uber make a lot of excellent automated and smart decisions and experiences

02:28.080 --> 02:30.760
in the product?

02:30.760 --> 02:37.000
We built the ML platform team at Uber and we made a platform called Michelangelo, which

02:37.000 --> 02:41.600
was really exciting and we can talk about that, of course.

02:41.600 --> 02:46.600
In the process of doing that, we developed a lot of cool patterns, really spent a lot

02:46.600 --> 02:52.680
of time developing a lot of MLOPS workflows and different pieces of infrastructure, one

02:52.680 --> 03:00.000
of which is the feature store and that is a really big inspiration and a really big

03:00.000 --> 03:04.760
kind of like theme of tech content that I'm working on today.

03:04.760 --> 03:10.240
You know, when I think back to Michelangelo and that blog post that got so many of us

03:10.240 --> 03:18.280
excited about what you were doing there at Uber and the state of play at the time, like

03:18.280 --> 03:24.120
compared to now, you know, we've built out a lot of the stack for MLOPS now or at least

03:24.120 --> 03:27.520
we've got contenders for various pieces.

03:27.520 --> 03:33.080
But when you left Uber to start tech time, the field was pretty wide open.

03:33.080 --> 03:34.560
Why'd you start with a feature store?

03:34.560 --> 03:36.560
I mean, you had all the pieces, right?

03:36.560 --> 03:43.360
That's a really good question. And I think we all have to think back to that time and think

03:43.360 --> 03:48.400
about a very different time where, you know, if you look today at the like, what are all

03:48.400 --> 03:53.520
the MLOPS tools in the industry, the map of the industry, there's 4,000 MLOPS companies

03:53.520 --> 03:58.360
and like, what plugs into what and how do I choose what it, you know, it's crazy.

03:58.360 --> 04:02.280
At that time, it felt like there was a lot of stuff, but there actually wasn't a lot

04:02.280 --> 04:04.120
of stuff relative to today, right?

04:04.120 --> 04:07.880
But it always feels complicated and, you know, a lot of the terms, there wasn't the same

04:07.880 --> 04:10.120
level of specialization and stuff like that.

04:10.120 --> 04:14.520
So back in the time, when we started Michelangelo, we started it in 2015.

04:14.520 --> 04:20.640
So, you know, it's a very different kind of generation of ML tooling and ML techniques

04:20.640 --> 04:22.800
and stuff like that.

04:22.800 --> 04:26.840
Back at that time, the thinking was an end to end ML platform.

04:26.840 --> 04:31.800
Let's just build a solid experience for, but it was all about democratizing ML.

04:31.800 --> 04:35.120
How do we make it possible for a data scientist to do machine learning?

04:35.120 --> 04:39.360
You know, and it was kind of like enabling them to do it for the first time.

04:39.360 --> 04:45.640
And so we built an end-to-end platform that enabled them to, you know, choose their label,

04:45.640 --> 04:53.560
choose their features in the Web UI and get their model built in a Web UI and, you know,

04:53.560 --> 04:56.280
manage it in a Web UI all the way to deploy into production.

04:56.280 --> 04:57.720
It was revolutionary at the time.

04:57.720 --> 05:03.640
It was something that really enabled people to go from zero to one.

05:03.640 --> 05:07.880
And it handled the full workflow.

05:07.880 --> 05:17.840
The industry was moving from this general idea, the dream of having the one size fits

05:17.840 --> 05:22.520
all platform that will enable all of the people within the organization really democratize

05:22.520 --> 05:26.880
everything to the kind of point where people are disillusioned with that dream and they

05:26.880 --> 05:32.240
really realize, hey, you know what we need is, you know, a bunch of reusable components

05:32.240 --> 05:35.720
because there's not a one size fits all one system.

05:35.720 --> 05:40.920
There's a bunch of reusable components that we can piece together to form the ML application

05:40.920 --> 05:43.720
that I'm trying to build, you know, so it would have the right kind of serving system

05:43.720 --> 05:48.360
for me and it will, and I can plug into the training system that's relevant for me.

05:48.360 --> 05:54.000
And so we actually did that that we brought Michelangelo through that journey as well

05:54.000 --> 05:59.520
from being a monolithic system to more of like a collection of best best practice components

05:59.520 --> 06:03.480
that are compatible and fit together quite well.

06:03.480 --> 06:09.800
And so in that going through that journey and building Michelangelo in the first place,

06:09.800 --> 06:15.480
you know, we were really focused on how do we help people get, not a model built, not

06:15.480 --> 06:19.880
like showing some results in AUC curve to their team, but how do we, you know, help the

06:19.880 --> 06:25.200
fraud team, help the ETA team at Uber, all these different teams, help them actually get

06:25.200 --> 06:28.680
into production and get value, you know, they were use case teams, they were trying to

06:28.680 --> 06:29.680
get something done.

06:29.680 --> 06:33.320
They didn't really care about like, cool, I've trained some models and they, they're

06:33.320 --> 06:35.600
not plugged into the product.

06:35.600 --> 06:38.920
And so we thought about it as they're building an ML application.

06:38.920 --> 06:45.040
And so this is super related to the concept of data centric AI, you know, what's part

06:45.040 --> 06:46.040
of an ML application?

06:46.040 --> 06:49.760
Well, of course you have, you got to build your models and manage your models.

06:49.760 --> 06:57.200
And then secondly, there's a variety of data pipelines that are also part of that ML application,

06:57.200 --> 07:01.600
the things that generate the data that your model consumes, your training system consumes

07:01.600 --> 07:08.400
to generate a model, or the data pipelines that your model in production uses to generate

07:08.400 --> 07:12.360
inference in real time, all kinds of data pipelines happening there.

07:12.360 --> 07:16.080
And so we realized, hey, in the Michelangelo system, we really built a lot of model management

07:16.080 --> 07:20.680
stuff, but what we spent most of our time building is a bunch of data management stuff.

07:20.680 --> 07:22.880
And that's actually what we called the feature store.

07:22.880 --> 07:27.560
And so it was a lot of kind of centralized and automated data engineering that we found

07:27.560 --> 07:30.520
that we were doing again and again across all these different use cases.

07:30.520 --> 07:34.440
And we brought that together into this one layer in the Michelangelo system and became

07:34.440 --> 07:36.320
our kind of its own component.

07:36.320 --> 07:38.200
We called it the feature store.

07:38.200 --> 07:42.320
We found that it was one of the most impactful systems to help someone go from zero to one

07:42.320 --> 07:48.480
to get into production quickly, and to be able to reuse machine learning across these

07:48.480 --> 07:53.660
components across different use cases to share and to help reduce the incremental cost

07:53.660 --> 07:55.960
of creating the next machine learning model.

07:55.960 --> 08:01.960
And so reflecting on that at the time we thought we glimpsed to the future.

08:01.960 --> 08:07.360
We realized it's really about the data, the feature store, the concept of the feature store,

08:07.360 --> 08:13.400
and later the feature platform is really the kind of the data layer for machine learning.

08:13.400 --> 08:17.880
And that's what got us so excited after publishing a blog post and everybody had like all kinds

08:17.880 --> 08:20.360
of extremely positive feedback around that.

08:20.360 --> 08:24.760
And I'm trying to build something like that, you know, can you guys come work for us kind

08:24.760 --> 08:25.760
of thing?

08:25.760 --> 08:27.160
There's a lot of attention there.

08:27.160 --> 08:31.440
And that made it clear to us that this is a design that's here to stay.

08:31.440 --> 08:39.920
And this is a design where this is a pattern that there should be a proper enterprise solution

08:39.920 --> 08:40.920
built around.

08:40.920 --> 08:48.320
And so that motivated us to create tech on, tech on is an enterprise feature platform.

08:48.320 --> 08:54.400
And you know, we help teams who are putting machine learning and production manage their

08:54.400 --> 08:59.360
data flows for their ML application through all stages of the ML lifecycle.

08:59.360 --> 09:09.600
You referenced the complexity of data infrastructure when you were building Michael Angelo.

09:09.600 --> 09:13.040
Speak a little bit to how you've seen that evolve over the past five years.

09:13.040 --> 09:15.520
Has it been relatively static?

09:15.520 --> 09:19.160
Has it changed significantly a little above?

09:19.160 --> 09:26.280
I think we have seen some things change quite a bit.

09:26.280 --> 09:30.040
Some things change a lot less than I would have liked.

09:30.040 --> 09:37.640
I think one of the biggest changes in our journey and the journey for many companies is

09:37.640 --> 09:43.960
frankly going from having a lot of data on-prem in a Hadoop cluster, you know, five years

09:43.960 --> 09:53.800
ago to now I'm using a cloud data platform and all my data is on a hyperscaler cloud provider.

09:53.800 --> 10:00.960
So what does that mean for a data team and an ML team?

10:00.960 --> 10:02.440
What does it mean for a data team?

10:02.440 --> 10:05.480
It means you're not managing a bunch of Hadoop clusters.

10:05.480 --> 10:09.560
You don't need a 30 person data infrastructure team to manage basic.

10:09.560 --> 10:12.600
How do I store my data and keep it up and running?

10:12.600 --> 10:16.480
Like for example, when I joined Uber, there was just outages like the data wasn't available.

10:16.480 --> 10:20.800
Like the whole Uber app would go down all the time because there was just issues with

10:20.800 --> 10:23.240
maintaining all of the data systems.

10:23.240 --> 10:29.400
Now a lot of those basic data capabilities are largely resolved.

10:29.400 --> 10:34.280
You know, you put your data on the cloud, you adopt a data lake house or a data warehouse,

10:34.280 --> 10:35.280
put your data in them.

10:35.280 --> 10:39.000
For example, snowflake or Databricks, they're great solutions and you're going to get

10:39.000 --> 10:45.360
really good performance, great reliability and it's going to be likely at a price point

10:45.360 --> 10:46.360
that works for your team.

10:46.360 --> 10:50.880
It's going to be cheaper than having maintaining your own team to build all of this stuff.

10:50.880 --> 10:56.320
And then it's interesting to reflect on what does that mean for a machine learning team?

10:56.320 --> 11:03.400
Because machine learning teams, previously in that previous world, were building custom

11:03.400 --> 11:08.760
connectors to the custom data systems that existed in their business is, how do I connect

11:08.760 --> 11:14.360
to our weird file format and the lack of reliability?

11:14.360 --> 11:22.200
Yeah, I mean, there's all kinds of, the ML teams would be constrained by the data choices

11:22.200 --> 11:26.240
of the underlying data platform team that they dependent on.

11:26.240 --> 11:30.560
And that would come with a lot of constraints at Uber, we were super fortunate because

11:30.560 --> 11:36.200
we had a super legit data platform team that did a lot of stuff.

11:36.200 --> 11:42.120
We had real time data, really high quality streaming data and an excellent kind of like

11:42.120 --> 11:45.400
a batch data system of gigantic Hadoop cluster.

11:45.400 --> 11:50.560
But that's not something that everyone was so fortunate to have.

11:50.560 --> 11:54.880
And moving to the cloud, you kind of get all of this stuff now very easily accessible,

11:54.880 --> 11:56.560
often serverless.

11:56.560 --> 12:01.680
And it makes for, and then I think that has a really big impact on the machine learning

12:01.680 --> 12:08.440
space or any consumer of data of underlying data systems because now they're standardization.

12:08.440 --> 12:14.040
I'm a machine learning team in company A, I build a connector to Snowflake and some

12:14.040 --> 12:17.080
cool thing that plugs into Snowflake and I can build models with it.

12:17.080 --> 12:21.840
Well I can share it with company B and now they can use my machine learning tool that

12:21.840 --> 12:27.520
was built for that underlying data system because we share the same underlying data system.

12:27.520 --> 12:31.560
And so there's a lot of commonality at this foundation layer that allows people to

12:31.560 --> 12:36.920
share at the ML layer, share things that they've built and have them be reusable across

12:36.920 --> 12:37.920
teams.

12:37.920 --> 12:39.840
They increase the pace of innovation there.

12:39.840 --> 12:48.680
And thinking about data lake house data warehouse, those are more about static data so to speak

12:48.680 --> 12:55.640
relative to streaming as streaming matured as much or to the same degree.

12:55.640 --> 13:02.720
Streaming has matured a bunch, but it is, there's still a long way to go, honestly, and we

13:02.720 --> 13:07.040
find that a lot of our customers are still struggling with it.

13:07.040 --> 13:14.560
And frankly part of the value proposition of the feature platform that we build that

13:14.560 --> 13:19.200
tech ton is, it makes a lot of this stuff just a lot easier.

13:19.200 --> 13:25.400
So how streaming used to be, you needed to have a whole team spin up a Kafka deployment

13:25.400 --> 13:28.280
for you and maintain a Kafka cluster.

13:28.280 --> 13:33.680
Now there's really good cloud solutions, there's confluent has confluent cloud.

13:33.680 --> 13:36.960
So it's much simpler with confluent cloud nowadays.

13:36.960 --> 13:44.120
There's a variety of other online streaming solutions that are, that make things much simpler.

13:44.120 --> 13:47.720
But we're finding that ML teams are still struggling to use them.

13:47.720 --> 13:52.760
So for example, a machine learning team is predicting fraud at a big bank.

13:52.760 --> 14:03.320
They want to find a way to say, hey, if this user has sent more than 100 transactions

14:03.320 --> 14:07.920
in the past five minutes, let's ban, let's not allow any other transactions, because it's

14:07.920 --> 14:09.880
likely that it's going to be fraud, right?

14:09.880 --> 14:14.320
Well, what do they need to do to get there, and they want to do this at scale for a lot

14:14.320 --> 14:19.080
of different users, they need to run a lot of this streaming infrastructure, and they need

14:19.080 --> 14:25.360
to be running stream processing to aggregate over all of those transaction events, turn

14:25.360 --> 14:30.640
them into feature aggregations, and then plug that into their model.

14:30.640 --> 14:37.560
And then, not just set this up once, but staff up a team to ensure that these systems don't

14:37.560 --> 14:41.080
go down, they don't go out of memory, if it goes down, that they're retried, that they're

14:41.080 --> 14:47.560
available, and high availability, and debuggable, and someone's on call for it.

14:47.560 --> 14:50.280
That's the productionization side of streaming.

14:50.280 --> 14:56.960
That's still pretty tough, and we find that a lot of teams that are trying to build interactive

14:56.960 --> 15:04.120
machine learning driven experiences, you want your recommendations on your website to

15:04.120 --> 15:06.840
react to what the customer was just clicking on.

15:06.840 --> 15:12.760
You want your fraud detection system to take into account like what the person's actions

15:12.760 --> 15:15.640
were, just were on this account.

15:15.640 --> 15:20.360
That kind of stuff productionizing it relies very frequently on streaming, it's pretty

15:20.360 --> 15:25.880
hard to do today, and it's one of the things that actually we've built a lot of IP and

15:25.880 --> 15:28.400
making a lot simpler for ML teams.

15:28.400 --> 15:34.480
All these real-time ML capabilities are an area where there's a lot of innovation happening

15:34.480 --> 15:35.480
right now.

15:35.480 --> 15:43.720
And so I think it is clear, based on that, that what you're seeing with regards to feature

15:43.720 --> 15:49.920
platforms isn't about replacing all of this infrastructure that we've only recently

15:49.920 --> 15:52.560
standardized, but rather it hooks into them.

15:52.560 --> 15:58.160
Well, let's just talk about what a feature platform is, and then we'll talk about how

15:58.160 --> 16:04.200
it can connect to your systems, and what does it do, is it replace your existing systems

16:04.200 --> 16:08.360
or not?

16:08.360 --> 16:12.560
When we work with teams who are trying to put machine learning into production, what

16:12.560 --> 16:17.000
they have to do is not just build a model and put a model into production, but they need

16:17.000 --> 16:24.760
to stand up, they need to develop, productionize, and operate data flows, feature pipelines

16:24.760 --> 16:32.280
that are constantly computing the up-to-date and very fresh feature signals that are going

16:32.280 --> 16:36.080
to be available, are going to be used for real-time inference.

16:36.080 --> 16:43.640
So we just gave the example of real-time fraud where I'm doing some aggregations over

16:43.640 --> 16:46.280
some recent web events, let's say.

16:46.280 --> 16:50.840
There may be a search use case, and so a very different type of data path, the data might

16:50.840 --> 16:56.080
come from a search box that someone typed a query into.

16:56.080 --> 17:00.760
That's more like a real-time signal that comes in from the end user application.

17:00.760 --> 17:03.320
And some signals, they're just pre-computed.

17:03.320 --> 17:05.320
Is this person in a banned country?

17:05.320 --> 17:10.880
Well, we can pre-comput that, and we can just make that ready to serve right when we

17:10.880 --> 17:12.560
want to make that prediction.

17:12.560 --> 17:16.480
There's a lot of data engineering that goes into each type of these features.

17:16.480 --> 17:26.360
People trip up with a lot of different types of data challenges that comes up with them.

17:26.360 --> 17:34.600
For example, consistency between that data at inference time compared to consistency

17:34.600 --> 17:38.440
with that data at training times, you want that data to be the same.

17:38.440 --> 17:42.400
When you generate a training data set, you need to go back in history for all of that

17:42.400 --> 17:43.400
data.

17:43.400 --> 17:47.880
You need to have all of that data log so you can generate historical training examples.

17:47.880 --> 17:52.480
You need to monitor this data to make sure it is serving at the right, operationally

17:52.480 --> 17:58.840
serving at the right speed, it's staying real-time, it's fresh, it's available.

17:58.840 --> 18:03.760
But also, you want to make sure that it's accurate, the data quality is high.

18:03.760 --> 18:10.280
And then there's a variety of data infrastructure that you end up having to spin up and support

18:10.280 --> 18:17.240
in production, for example, systems to serve this data at scale, systems to operate, to

18:17.240 --> 18:20.200
do the stream processing to generate your features.

18:20.200 --> 18:25.600
We think about it as transforming your raw data into features, storing that data both

18:25.600 --> 18:32.320
for training and for inference, serving that data for inference in real-time, monitoring

18:32.320 --> 18:38.880
the data, and building an excellent developer workflow for MLOps and for the engineers

18:38.880 --> 18:41.320
to fit it into their DevOps processes.

18:41.320 --> 18:44.840
These are all within the scope of the feature platform.

18:44.840 --> 18:50.520
And the reason why there's so many things here is because there's a lot of challenges

18:50.520 --> 18:56.920
that you need to solve before you can actually credibly claim that your model is running

18:56.920 --> 19:05.440
in production in a reliable enough way that you're comfortable depending your revenue

19:05.440 --> 19:09.600
on it, you want to actually depend your product on it, type of thing.

19:09.600 --> 19:15.320
And so we've built a platform to make all of those data challenges much simpler.

19:15.320 --> 19:17.800
And so how do we do this, though?

19:17.800 --> 19:28.800
Well, we are not delivering a completely new data and ML stack to the ML user, to our customer.

19:28.800 --> 19:34.080
We are plugging into the data sets, the data infrastructure that they have.

19:34.080 --> 19:41.280
So people come to us and they say, hey, I have Snowflake and I have Databricks and I have

19:41.280 --> 19:42.520
a Redis cluster.

19:42.520 --> 19:44.600
And we'll say, that's great.

19:44.600 --> 19:48.400
We will actually orchestrate those systems, so we'll orchestrate transformations on both

19:48.400 --> 19:49.400
of those.

19:49.400 --> 19:54.040
We'll plug into those, take data out of them, we'll load it up into Redis, we'll maintain

19:54.040 --> 19:57.760
an online serving layer for your model.

19:57.760 --> 20:05.440
So the interfaces here are, we plug into raw data and we serve that data to the model

20:05.440 --> 20:09.080
that's in production or the training system that's building the model.

20:09.080 --> 20:15.680
But under the hood, we're not implementing all of those different data processes from

20:15.680 --> 20:19.800
scratch and running all of that infrastructure in our domain.

20:19.800 --> 20:28.040
We're actually orchestrating the best in class cloud data infrastructure that your team

20:28.040 --> 20:30.040
is already running within your own stack.

20:30.040 --> 20:35.160
So we're a lot more of an orchestration layer for like a data orchestration layer for

20:35.160 --> 20:37.640
machine learning than anything else.

20:37.640 --> 20:46.920
To relate to us a bit back to this idea of data-centric AI, you mentioned before we started

20:46.920 --> 20:53.920
recording the interview as we were chatting this idea about a ML flywheel that kind of reiterated

20:53.920 --> 20:58.120
the importance of data for ML.

20:58.120 --> 21:01.120
Can you talk a little bit about that idea?

21:01.120 --> 21:02.520
For sure.

21:02.520 --> 21:09.640
So one thing that I think is really nice about data-centric ML is it's a theme that comes

21:09.640 --> 21:16.640
along with it is declarative interfaces and maybe some of the other folks that you've

21:16.640 --> 21:21.320
been speaking to in recent episodes have been talking about this as well.

21:21.320 --> 21:24.880
But this is one of the key things that we have really focused on.

21:24.880 --> 21:31.520
We don't have our users say, hey, plug this infrastructure into this infrastructure, run

21:31.520 --> 21:36.160
this operation, retry it if it doesn't work, and then plug it in here and then run it

21:36.160 --> 21:37.320
at this frequency.

21:37.320 --> 21:43.400
We just have the person say, this is my feature, this is the feature transformation, you take

21:43.400 --> 21:44.400
care of everything.

21:44.400 --> 21:51.520
To be in production and the tech-ton system handles a lot of things behind the scenes to make

21:51.520 --> 21:52.520
that happen.

21:52.520 --> 21:55.640
Earlier on in the conversation, I mentioned like at Michelangelo, one of the things that

21:55.640 --> 22:00.880
made us really successful was we were focused on the end-to-end machine learning application.

22:00.880 --> 22:06.200
It wasn't, we weren't just focused on helping someone train a model or manage models.

22:06.200 --> 22:10.400
It was, hey, what's the whole series of workflows that you have to go through to actually

22:10.400 --> 22:18.320
have an application that's both reliable and accurate and making repeated predictions.

22:18.320 --> 22:22.240
It's like a live production application that you're operating.

22:22.240 --> 22:29.760
There's a couple of steps to that that we see that the best teams are doing when they're

22:29.760 --> 22:33.320
building their production ML applications.

22:33.320 --> 22:35.480
We can walk through these steps, right?

22:35.480 --> 22:37.600
First, you got to build the training data set.

22:37.600 --> 22:40.560
What is that training data set come from?

22:40.560 --> 22:45.960
It comes from your company's underlying data sets that are model data sets that are

22:45.960 --> 22:49.720
shared across the business, wherever your company's data is.

22:49.720 --> 22:55.960
That data is typically a bunch of logs of user events that come from your product.

22:55.960 --> 22:59.920
There's just this data loop that we found our customer is building, right?

22:59.920 --> 23:05.600
You make some prediction and so maybe you're predicting like, is someone going to click

23:05.600 --> 23:08.120
on this product or not?

23:08.120 --> 23:12.120
Then based on if they click on that product or not, you log that data.

23:12.120 --> 23:16.160
That's going to become a future ground truth, a future label.

23:16.160 --> 23:22.920
You may join that data together with other labels that you've logged.

23:22.920 --> 23:29.280
You have a, like, now they're joined label data set and maybe you log some features.

23:29.280 --> 23:34.120
You have a feature data set as well, feature logs.

23:34.120 --> 23:38.880
Then you may assemble these data sets into a training data set and so now you want to

23:38.880 --> 23:41.000
take all that data and you want to build a model.

23:41.000 --> 23:44.720
Now you build a model and then you want to use that model.

23:44.720 --> 23:50.000
There's a variety of other data sets that you're building on that time and real-time inference

23:50.000 --> 23:55.520
type of workflows where you're maybe generating some candidates and then generating a feature,

23:55.520 --> 24:00.400
some features for each of those candidates and then you want to score those candidates

24:00.400 --> 24:04.080
and you ultimately are making a prediction and you're delivering that back to the

24:04.080 --> 24:08.160
product, to the user, to the customer.

24:08.160 --> 24:14.000
This is kind of data loop of, like, collect some data, organize it, learn from it and decide

24:14.000 --> 24:18.240
and then based on, you know, make a prediction and then based on that, collecting again,

24:18.240 --> 24:23.000
organizing, you know, and we want to, we want, this data goes around in this loop and

24:23.000 --> 24:30.160
what we've seen is that the best ML teams, the teams across the industry that are the

24:30.160 --> 24:36.440
most successful at machine learning, are really, really good at managing that loop and

24:36.440 --> 24:37.520
building that loop.

24:37.520 --> 24:42.720
They're really, really good at getting that flywheel to really be running.

24:42.720 --> 24:46.600
The more, every iteration of that flywheel, you're collecting more data, you're training

24:46.600 --> 24:49.960
your model, your model's getting better, you're making better predictions.

24:49.960 --> 24:56.360
And so there's a variety of things that help people become, there's a variety of, like,

24:56.360 --> 25:01.800
good things about that come out of being excellent at that flywheel.

25:01.800 --> 25:06.800
You have really clear ownership of who, you know, runs each part of that flywheel and

25:06.800 --> 25:12.600
it's easier to make changes, everything's more debuggable and more easily monitored and

25:12.600 --> 25:18.080
just small changes feel like small changes and ML just feels natural and easy.

25:18.080 --> 25:21.360
If you're ML flywheel, you don't really have it working well, maybe you don't know,

25:21.360 --> 25:28.320
hey, who's actually the person that logs data from the application into the data warehouse?

25:28.320 --> 25:31.680
Because now I want to add a new feature to my model, but I've got to go find that person

25:31.680 --> 25:34.040
so they can log some data for me.

25:34.040 --> 25:39.280
In that world, every small change you want to make, it ends up becoming like a big thing.

25:39.280 --> 25:43.880
Like, you don't know who runs this system and then it's broken, you've got to go debug

25:43.880 --> 25:46.920
it with and you've got to go find the person kind of thing.

25:46.920 --> 25:53.400
And so ML becomes really hard, you know, models don't update as often, you tend to see

25:53.400 --> 25:57.640
those teams, a symptom of those teams as they're still stuck in version one as the model

25:57.640 --> 25:59.240
that's in production.

25:59.240 --> 26:07.000
And so the machine learning, the teams that are really good at ML, they've really focused

26:07.000 --> 26:15.040
on building and managing this ML flywheel and it's my claim that great ML applications

26:15.040 --> 26:17.440
require a great ML flywheel.

26:17.440 --> 26:25.240
And so I was talking about the declarative definitions there, that fits into this whole

26:25.240 --> 26:28.440
thing because it's really tough to, there's a lot of parts, a lot of different technology

26:28.440 --> 26:30.280
all throughout the ML flywheel.

26:30.280 --> 26:34.240
And so I think it's going to be really important, you know, we're talking about data-centric

26:34.240 --> 26:44.560
AI now, the declarative definitions, patterns that are central to data-centric AI, that's

26:44.560 --> 26:45.560
going to be applied.

26:45.560 --> 26:50.840
I think, I think applying those to the whole ML flywheel is going to be a big unlock.

26:50.840 --> 26:57.520
It's going to make things a lot easier for teams to really build and maintain their whole

26:57.520 --> 27:02.040
ML flywheel because it manages and it hides away a lot of the complexity of all of the

27:02.040 --> 27:09.480
different technologies that span from the analytic world to the production world, from the

27:09.480 --> 27:13.760
forward pass of the data, you know, learning and making predictions to the bringing the

27:13.760 --> 27:18.440
data back into the analytic side of logging and organizing the data.

27:18.440 --> 27:25.160
It's a lot of stuff and teams need some much simpler interfaces to define these things.

27:25.160 --> 27:30.880
And I think these declarative interfaces are really the key to unlocking the ability to

27:30.880 --> 27:34.440
manage the ML flywheel for the average team.

27:34.440 --> 27:43.680
And a lot of ways the promise of ML ops was giving teams a platform technology for managing

27:43.680 --> 27:49.760
this flywheel or, you know, even more strongly creating a flywheel where before there was

27:49.760 --> 27:56.440
kind of bespoke one-off transitions and handoffs from one team to the next.

27:56.440 --> 28:04.320
And part of that idea was like applying some of popular ideas like DevOps and continuous

28:04.320 --> 28:12.320
delivery from software engineering to ML, how have you seen that evolve?

28:12.320 --> 28:18.800
Do you think that that has played out the way, you know, folks have wanted to or do you

28:18.800 --> 28:20.680
where do you still see gaps?

28:20.680 --> 28:26.600
So one kind of symptom that things are not as they should be is that there's, we talked

28:26.600 --> 28:30.760
about the ML ops landscape, there's like 4,000 different things there.

28:30.760 --> 28:34.120
And it's just in here, even if you know the space really well, like you do, it's still

28:34.120 --> 28:37.160
like, whoa, there's a lot of things here.

28:37.160 --> 28:43.480
And an average ML team, firstly, realistically, the average ML team is not expert at all

28:43.480 --> 28:44.960
of those different things.

28:44.960 --> 28:49.320
But secondly, they have to piece together a lot of these different items and kind of ductate

28:49.320 --> 28:54.080
them all together into like a coherent application that runs smoothly.

28:54.080 --> 29:02.320
And I think there's been a variety of ML ops efforts to make that process smoother.

29:02.320 --> 29:05.640
What we've seen is people have gotten it wrong in two different ways.

29:05.640 --> 29:08.560
One way is their scope was too small.

29:08.560 --> 29:15.480
So there's a variety of systems that focus only on a subset of that whole flywheel, right?

29:15.480 --> 29:19.920
So they may just focus on, let me make the process of training a model really good.

29:19.920 --> 29:25.680
Like I'm going to do some, you know, experiment management just in the learning phase, for example.

29:25.680 --> 29:30.520
Or let me just, you know, this is, I'm a tool to help, you know, do some prediction,

29:30.520 --> 29:35.120
inference stuff better, but it doesn't really address the whole flywheel part of it.

29:35.120 --> 29:41.760
And then the other way in which a variety of tools just have not been successful is I think

29:41.760 --> 29:44.320
they just bit off too much more than they can chew.

29:44.320 --> 29:48.040
And so they got a little bit too ambitious and they said, well, you know, we're the system

29:48.040 --> 29:49.600
that will do everything for you.

29:49.600 --> 29:53.560
And so they really set their scope on the whole ML flywheel, even if they weren't, you

29:53.560 --> 29:59.480
know, actively, consciously talking about like that bottom half of that flywheel that

29:59.480 --> 30:03.840
brings the data back into your, from production back into your training data sets.

30:03.840 --> 30:08.560
But, you know, it's just the dream of like, we handle everything, just just use our tool

30:08.560 --> 30:10.720
and everything will be so much easier for you.

30:10.720 --> 30:11.720
Yeah.

30:11.720 --> 30:15.200
I love that you bring up both those points because I've written about this previously in

30:15.200 --> 30:20.920
the, okay, definitive guide to ML platforms ebook and called it this wide versus deep paradox.

30:20.920 --> 30:25.960
Like, you know, you have just as you said, a bunch of tools that are trying to, you know,

30:25.960 --> 30:30.360
solve the end and platform, but don't have sufficient depth in any particular area.

30:30.360 --> 30:37.080
And then you have others that are, you know, they only do one thing, but then, you know,

30:37.080 --> 30:40.200
they don't necessarily integrate the pieces together.

30:40.200 --> 30:42.360
And it's a tough spot.

30:42.360 --> 30:51.040
And I think that's a part of that just state of play is, you know, what's led to the rise

30:51.040 --> 30:56.160
of platform teams that are, you know, forming to kind of have been forming to pull all the

30:56.160 --> 31:00.200
pieces together and try to ensure an end-to-end experience.

31:00.200 --> 31:05.360
If not based on end-to-end tools, that's a really good point that that does speak to the

31:05.360 --> 31:10.480
need for platform teams and that helps and platform teams justify their existence with

31:10.480 --> 31:17.680
that to finish up on like in which ways the very broad folks get it wrong.

31:17.680 --> 31:19.440
I think we're still evolving as an industry.

31:19.440 --> 31:23.320
The best patterns are still emerging here, but I think what we're seeing is those folks

31:23.320 --> 31:27.160
that promise everything, you know, it's just a really hard problem.

31:27.160 --> 31:33.280
And there's a lot of things to get right and, you know, realistically, if you're depending,

31:33.280 --> 31:37.400
if your product is, we're going to get every single thing, right?

31:37.400 --> 31:45.640
That's unrealistic and customers, you know, average ML teams, they don't want to get stuck

31:45.640 --> 31:51.160
building the, they don't want to get stuck with an inflexible ML platform that only lets

31:51.160 --> 31:53.040
them do certain things.

31:53.040 --> 31:59.600
So, you know, what we're, our approach with the ML flywheel is not to say we do everything.

31:59.600 --> 32:04.440
We, our scope is the whole ML flywheel, but we're really focused on managing the data

32:04.440 --> 32:07.840
sets and enabling the data flows through that ML flywheel.

32:07.840 --> 32:13.960
So we're focused on helping you create your feature logs, create your training data sets,

32:13.960 --> 32:19.280
create your, you know, calculate your features in real time, have a unified data model across

32:19.280 --> 32:25.000
this whole ML flywheel and generate compatible schemas and all of the different pieces of infrastructure

32:25.000 --> 32:28.640
and manage as much of the data engineering through that as possible.

32:28.640 --> 32:33.520
So we're taking a really big chunk of all of the work that needs to get done, but there's

32:33.520 --> 32:34.680
a lot of stuff that we're not doing.

32:34.680 --> 32:39.560
We don't touch models, we don't build models or not the model management system.

32:39.560 --> 32:46.440
And so we think that that's a much more manageable domain that's very valuable that we can

32:46.440 --> 32:51.720
really focus on the workflows, we can really focus on improving those workflows and making

32:51.720 --> 32:53.760
an amazing user experience there.

32:53.760 --> 32:57.840
So that's our approach and how we really see us doing something different than the whole

32:57.840 --> 33:02.080
end-to-end ML platforms that, you know, take on too much and the very specialized tools

33:02.080 --> 33:04.040
that only solve one part of the workflow.

33:04.040 --> 33:08.360
And so you brought us back to this flywheel part of the discussion, but I know you've

33:08.360 --> 33:12.200
got an interesting take on platforms teams as well.

33:12.200 --> 33:14.960
Yeah, I mean, what are you saying there?

33:14.960 --> 33:21.000
Well, on the platform teams, we were just talking about like how a lot of these challenges

33:21.000 --> 33:24.840
allow ML platform teams to, you know, justify their existence.

33:24.840 --> 33:28.160
And so if you can kind of think of both of those domains, right?

33:28.160 --> 33:31.840
If they're either piecing together a bunch of small things together and they're really

33:31.840 --> 33:36.680
running a bunch of glue code and building kind of like a brittle infrastructure within

33:36.680 --> 33:41.640
the company, or they may not want to take on that type of challenge.

33:41.640 --> 33:51.360
And then the platform team is all about evaluating different end-to-end platforms and just choosing

33:51.360 --> 33:54.360
the right one and operating that within the business.

33:54.360 --> 33:57.960
And there's different failure modes for both of those technically.

33:57.960 --> 34:02.240
So there's, you know, it's really hard to maintain your glue code and your duct tape

34:02.240 --> 34:07.160
and, you know, keep everything running in a reliable way on the first one.

34:07.160 --> 34:10.440
On the second one, you're fundamentally signing up.

34:10.440 --> 34:16.440
You're putting all your eggs in one basket and you're investing in a system that inherently

34:16.440 --> 34:20.920
is going to be, have the flexibility of just a single system.

34:20.920 --> 34:26.280
And as soon as your business needs, I'll grow that then it's going to be on you to solve

34:26.280 --> 34:27.280
that.

34:27.280 --> 34:31.320
And that's even a much harder position to be in because then you have to expand from

34:31.320 --> 34:38.800
a single platform that you have within your business to building a whole stack from scratch.

34:38.800 --> 34:44.800
So that's a little bit on the technical side, but I have to say that I've seen, I haven't

34:44.800 --> 34:49.480
seen ML platform teams be very successful in industry generally.

34:49.480 --> 34:54.960
And this is a little bit of, you know, it's not something that like ML platform teams

34:54.960 --> 34:59.320
necessarily want to hear, but, you know, we work with a lot of ML platform teams and a

34:59.320 --> 35:01.680
lot of ML use case teams.

35:01.680 --> 35:07.760
And so the use case teams are the folks who are building the recommender system to power,

35:07.760 --> 35:12.240
you know, the website or they're building the fraud detection system to block certain transactions,

35:12.240 --> 35:13.240
stuff like that.

35:13.240 --> 35:18.680
They have a very specific business impact that they're measuring their success on and they're

35:18.680 --> 35:19.680
under the gut.

35:19.680 --> 35:22.760
They're trying to get this stuff going and launched as soon as possible and they're working

35:22.760 --> 35:26.840
with business people and it's a team of data scientists, engineers, I'll mix together

35:26.840 --> 35:30.280
and the product engineers as well.

35:30.280 --> 35:34.080
The platform teams are really focused on, hey, let me make some, let me kind of like

35:34.080 --> 35:43.360
centralize a lot of our engineering investments that the use case teams are making and bring

35:43.360 --> 35:47.920
a lot of that engineering into one place and build some reusable components to better enable

35:47.920 --> 35:49.720
those use case teams.

35:49.720 --> 35:53.360
And so that's great and that's actually is a really good model.

35:53.360 --> 35:57.080
The challenge that we have and that we see a lot of like mistakes that we see a lot

35:57.080 --> 36:04.280
of companies making is it's not uncommon to see a platform team exist before the use case

36:04.280 --> 36:07.880
teams exist and that's a problem because what is the platform team trying to do if there's

36:07.880 --> 36:13.400
no use cases, you got to start with the use case, you have to have concrete business outcomes

36:13.400 --> 36:18.440
that you're trying to drive with machine learning and then only then once you understand

36:18.440 --> 36:24.800
those requirements, does it make sense to build a platform team and when you have multiple

36:24.800 --> 36:28.400
of those use case teams so you can support them.

36:28.400 --> 36:34.440
One thing that we, you know, a really good sign that a platform team is struggling and

36:34.440 --> 36:40.760
is not set up for success is when they don't have requirements.

36:40.760 --> 36:45.880
When you talk to them and you say, so what exactly are you trying to build, like what kind

36:45.880 --> 36:49.720
of, what kind of scale do you need, what kind of latencies do you need, like what do you

36:49.720 --> 36:51.800
need and they don't know.

36:51.800 --> 36:57.840
And when they don't know, it's because they don't have use cases yet and then they also

36:57.840 --> 37:02.160
don't have a clear decision making process as well because when there's not concrete

37:02.160 --> 37:09.200
business use case tied to it, it's kind of like building for the future, it's all speculative,

37:09.200 --> 37:13.600
who should make this decision and we see those teams struggle and they spend their wheels

37:13.600 --> 37:18.200
a lot because they don't really have like a concrete direction that they're going towards

37:18.200 --> 37:23.640
and so I would recommend that any ML, if you're on an ML platform team, think really carefully

37:23.640 --> 37:29.560
about who your internal customers are, what those use cases use case teams are because

37:29.560 --> 37:35.320
the ML platform is a product just like any other company is building a product and the

37:35.320 --> 37:41.280
most, like the best advice for anybody building a product is to focus on your customers.

37:41.280 --> 37:44.560
And if you don't know who your customers are as an ML platform team, it's really hard

37:44.560 --> 37:45.560
to find success.

37:45.560 --> 37:50.920
Now, that's awesome. It prompts me to plug our TwilmoCon event because we've spent a

37:50.920 --> 37:55.640
lot of time trying to help platform teams understand what the best teams in the industry

37:55.640 --> 38:02.680
are doing and you know, we've got this event coming up in the fall, but also we just revamped

38:02.680 --> 38:08.160
our website and all of the content from the past due conferences is up there and I'm

38:08.160 --> 38:11.840
thinking of a couple in particular that have talked a lot about this kind of thing.

38:11.840 --> 38:15.920
We've been doing these team tear downs when we talk about how the platform teams engage

38:15.920 --> 38:25.400
with the user teams and why those interaction modes are so important as well as thinking

38:25.400 --> 38:30.600
about an interview that I did with Fran Bell who ran a platform team at Uber that was

38:30.600 --> 38:40.200
focused on forecasting and kind of higher level platform functionality and really interesting.

38:40.200 --> 38:44.320
Their approach was to, you know, these teams were off doing what they were doing, these

38:44.320 --> 38:49.800
use case teams that you were describing and they would kind of talk to all these teams

38:49.800 --> 38:54.680
and identify the things that the wheel that everyone was reinventing and kind of build

38:54.680 --> 38:57.920
the platform around that kind of functionality.

38:57.920 --> 39:01.760
So lots of great insights there from folks that have been doing it.

39:01.760 --> 39:08.680
And you mentioned Fran Bell. I worked with Fran at Uber and that platform was very successful

39:08.680 --> 39:14.560
that she built and the approach that they had was not, hey, you know what would be cool?

39:14.560 --> 39:18.040
Let's build a time series for passing platform. It was definitely not that.

39:18.040 --> 39:24.280
It was, it was, we have a bunch of time series forecasting problems that were solving and

39:24.280 --> 39:29.360
they went and built them and then they had the three, four and then it became 10 different

39:29.360 --> 39:33.920
teams that were doing the same thing. They were like, you know, why don't we just build,

39:33.920 --> 39:38.640
you know, a thin layer that these different teams can reuse and they kind of just added

39:38.640 --> 39:43.520
more automation into this layer and centralize more, more into it over time.

39:43.520 --> 39:47.880
And as they were able to bring in the different use cases, that requirements from the different

39:47.880 --> 39:51.840
use cases, they were, they had a purview that individual use cases didn't have and they

39:51.840 --> 39:56.200
were able to think more creatively and more innovatively about how to solve this stuff.

39:56.200 --> 40:02.360
And that's a, a layer where they made some really amazing inventions. And, and that works

40:02.360 --> 40:08.280
both at the, at the kind of like platform team looking at the use case team direction.

40:08.280 --> 40:12.360
And it also works at the platform team looking upstream at other infrastructure dimension

40:12.360 --> 40:17.480
because they were able to build on top of the Michelangelo system. And so we actually

40:17.480 --> 40:22.160
integrated those systems over time and so all the time series, the time series stuff that

40:22.160 --> 40:26.480
the time series platform was building was actually, you know, compiled down to Michelangelo

40:26.480 --> 40:32.520
jobs and Michelangelo model training and model serving types of things. And, and that

40:32.520 --> 40:37.560
was like an excellent example of like a great value added by a platform team, but none

40:37.560 --> 40:43.800
of that would have been possible if the, if friends original attitude was not to just

40:43.800 --> 40:48.000
solve the problem. Like just get, you know, get your first time series thing solved and

40:48.000 --> 40:51.720
then you can figure out how to do some cool platform thing later on. Yeah. Yeah. And

40:51.720 --> 40:58.920
I'll link to that particular interview in the show notes for folks that want to, for

40:58.920 --> 41:03.560
folks who this problem of how to build a successful platform team resonates. He said something

41:03.560 --> 41:13.600
else in our kind of pre live chat that I thought was interesting around kind of what really

41:13.600 --> 41:21.600
matters when trying to, you know, when trying to do machine learning, you know, what matters

41:21.600 --> 41:27.760
ultimately is creating business value. And so it's either helping your company have more

41:27.760 --> 41:36.680
revenue, reducing the costs adding or reducing risks. And, and, and so, you know, we talked

41:36.680 --> 41:42.960
about ways in which platform teams things go wrong for them. But there's also individual

41:42.960 --> 41:50.680
ML products projects rather that, that trip up in a variety of different ways. And I think

41:50.680 --> 41:58.680
the biggest, the biggest theme that I see is focus focusing on problems that are just

41:58.680 --> 42:03.040
not that important to the business. And I saw this a bunch of times at Uber, a data science

42:03.040 --> 42:07.920
team, you know, in some corner, there's three people spending a lot of time solving this

42:07.920 --> 42:12.960
problem that's, you know, it's an interesting machine learning problem. It's really cool

42:12.960 --> 42:19.200
technology. It's fun to work on. But it takes them, of course, it's just like any project.

42:19.200 --> 42:24.800
It takes some engineering investment. It takes any type of different like cross team collaboration

42:24.800 --> 42:30.040
to actually get this thing across the finish line and into production when you want to

42:30.040 --> 42:36.080
deploy it into production. And, and, you know, that requires someone up to change some

42:36.080 --> 42:39.960
executive to say, yeah, we should spend engineers on this. This is important enough that we

42:39.960 --> 42:44.880
should, you know, staff the team against this. And often that doesn't happen. And why?

42:44.880 --> 42:48.840
Because the problem is just, you know, it's a small thing. Who cares? We didn't really

42:48.840 --> 42:52.720
need to, it wasn't that important for us that, that random problem that you chose. And

42:52.720 --> 42:59.000
so I think this is like a really important thing for like ML practitioners to think about.

42:59.000 --> 43:04.440
And I've seen it again and again have big impact on people's careers, the ability to choose

43:04.440 --> 43:08.960
the important problems. You know, you have a company, there's so many different cool

43:08.960 --> 43:13.080
things you can apply machine learning to. But how do you choose the ones that are, that

43:13.080 --> 43:17.880
are most important to the business that are going to have the biggest impact at the end

43:17.880 --> 43:22.800
of the day? I think that really requires understanding what is the, what matters to the business?

43:22.800 --> 43:28.640
What are the company's goals? What metrics are they trying to move? And then which levers,

43:28.640 --> 43:35.960
levers do I have at my, from my position to help me have some impact there? And not think

43:35.960 --> 43:40.600
about it like as a technology first, what's a cool problem? I heard about this cool technique.

43:40.600 --> 43:46.080
Let me apply it to this problem type of thing. That's, that's where a lot of teams on the

43:46.080 --> 43:50.120
use case side of things, not the platform teams get things wrong.

43:50.120 --> 43:56.520
As you're, as you're talking to, you know, use case teams and platform teams, you've identified

43:56.520 --> 44:02.720
a bunch of the pitfalls that you see them falling into, you know, how do you, you know,

44:02.720 --> 44:09.320
characterize the, the different teams that you speak in in terms of their level of maturity

44:09.320 --> 44:16.280
and how effective they are at side stepping these, these traps.

44:16.280 --> 44:26.120
I think a lot of, there are a variety of different ways to kind of plot out a like steps of,

44:26.120 --> 44:33.280
of maturity. And one of the things that's that we think about a lot is as a, as a, a

44:33.280 --> 44:39.080
sign of being at a minimal level of maturity for us to really care about working with those

44:39.080 --> 44:43.880
people or, or to feel like they're mature enough that we should work with them is do they

44:43.880 --> 44:50.000
have one model in production? That's a really good sign of the health of your ML team.

44:50.000 --> 44:57.080
Because if you can't get one model in production, what, what is it? It may not be, the problem

44:57.080 --> 45:02.440
is it may not be a machine learning thing. It may be you, I mean, you may not have executive

45:02.440 --> 45:06.120
buy-in. You may be working on the wrong problem. Maybe nobody in your company cares about

45:06.120 --> 45:09.640
that problem. And so no one's helping you get to production. Maybe you don't have the

45:09.640 --> 45:14.520
right underlying data infrastructure. Maybe the engineering team just sucks. And so you're

45:14.520 --> 45:20.240
just not, you know, there's all these possible things. And so you could be super skilled

45:20.240 --> 45:23.280
at machine learning. But then there's all of these other things that are holding your

45:23.280 --> 45:30.920
team back. And so we de-risk that by only working with people who have one model in production.

45:30.920 --> 45:34.720
And that's a sense of mature, that gives us a sense of not your ML maturity, actually,

45:34.720 --> 45:38.320
but all of the rest of the maturity. Do you have your data in the right place and stuff

45:38.320 --> 45:43.840
like that? You were at the right competence to get past the finish line once at least.

45:43.840 --> 45:51.680
And then what we can help you with is help it help you make that process a positive ROI.

45:51.680 --> 45:57.760
Make that process, you know, valuable for you, make it repeatable, make it reliable, all

45:57.760 --> 46:03.040
of that kind of stuff. But there's some kind of like core foundational things that teams

46:03.040 --> 46:11.160
need to get right first. But I think in terms of different types of ML use cases, some teams

46:11.160 --> 46:15.480
are focused on operational machine learning and the care about real-time ML. You can imagine

46:15.480 --> 46:25.760
a different maturity curve and kind of milestones for a team to get there than a similar maturity

46:25.760 --> 46:35.520
curve that would apply to a team that is doing offline, like document analysis or something

46:35.520 --> 46:36.520
like that.

46:36.520 --> 46:43.040
So I don't think there's a general maturity curve that applies to every team. But it's

46:43.040 --> 46:48.800
more like on a per-use case basis of like, what is your role in the interaction there?

46:48.800 --> 46:52.160
And then how can you find the milestones that matter to you? And then that example of

46:52.160 --> 46:56.280
getting the past to finish line once is one important one. And it strikes me that one

46:56.280 --> 47:00.800
model in production, yeah, that seems like a really low bar for 2022.

47:00.800 --> 47:08.040
Yeah, and you'd be surprised how many teams are still working on that. And it's not that

47:08.040 --> 47:14.200
wow, like machine learning is really hard there. But the types of problems are, hey, our

47:14.200 --> 47:21.400
data is still on prem. And our data science team started on AWS. And so they can kind

47:21.400 --> 47:26.840
of like pull in data into the cloud. But like, is the data productionized into the cloud?

47:26.840 --> 47:31.320
Are we going to run the ML application in the cloud? Are we going to bring the ML application

47:31.320 --> 47:36.000
back to our data center? We're still figuring out these details and there's a lot of like

47:36.000 --> 47:42.120
political things. And then those kind of things just kind of keep going. And that's not

47:42.120 --> 47:46.080
where if you're trying to help someone out with machine learning, you want to, you can't

47:46.080 --> 47:50.200
really help with the internal political stuff. You want to help them, you know, do their

47:50.200 --> 47:51.920
workflows and their processes properly?

47:51.920 --> 47:58.760
Yeah, I was going to ask if part of it is a semantic thing where, you know, folks will

47:58.760 --> 48:03.400
say my model is in production, but it's in production in the sense that it's making

48:03.400 --> 48:09.160
predictions that end up in a exosperate sheet that someone's manually analyzing versus

48:09.160 --> 48:15.720
kind of closing the loop so that a system is reacting to the decisions made by the model.

48:15.720 --> 48:20.080
That's a really good distinction. And so people say we have a bunch of models in production

48:20.080 --> 48:24.120
and we talk about operational machine learning. So I kind of break the machine learning into

48:24.120 --> 48:30.680
two buckets. Analytical machine learning, which is where it can be offline. It's mainly

48:30.680 --> 48:37.080
its main purpose is to influence a human driven decision. So let me, let me, let me score

48:37.080 --> 48:40.840
some leads and then I'll look at the scores and then maybe I'll send some emails or something

48:40.840 --> 48:45.560
like that based on them. And we'll do that on a weekly basis. That's an analytic machine

48:45.560 --> 48:51.920
learning type of use case. Operational machine learning is online. It needs to be monitored.

48:51.920 --> 48:57.360
It affects the customers directly drives automated decisions and it's often real time. And

48:57.360 --> 49:01.320
those are the types of use cases where ML is powering your product. Those are the types

49:01.320 --> 49:06.680
of use cases where being in production means something very different. Yeah. Yeah. Mike,

49:06.680 --> 49:14.680
as always, this has been a wonderful chat. You know, maybe a place to wrap up is for,

49:14.680 --> 49:19.800
you know, folks that are interested in what you've been talking at your approach to data

49:19.800 --> 49:25.920
centric AI kind of where this should they be looking. Yeah. Thanks, Sam. Two places.

49:25.920 --> 49:36.360
So and maybe one update since we last spoke. TechCon is now the main maintainer of the

49:36.360 --> 49:42.000
popular open source feature store called Feast. So that's a solution that anybody can

49:42.000 --> 49:47.600
try very lightweight. It's the kind of build your own feature store framework. And it's

49:47.600 --> 49:54.400
the most certainly the most popular feature store. You can find that at feast.dev. And

49:54.400 --> 49:59.720
if if there's any listeners from companies who are really trying to figure out like, hey,

49:59.720 --> 50:06.200
how do we build real production applications? ML applications and we need some help on

50:06.200 --> 50:11.040
the data side of things to help this stuff get into production either in real time, super

50:11.040 --> 50:17.040
reliably. That's what the TechCon feature platform is for. And so folks can check us

50:17.040 --> 50:46.040
out. So that TechCon dot AI. Awesome. Awesome. Well, thanks so much, Mike. Thank you.

