WEBVTT

00:00.000 --> 00:16.320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

00:16.320 --> 00:21.480
people, doing interesting things in machine learning and artificial intelligence.

00:21.480 --> 00:31.720
I'm your host Sam Charrington.

00:31.720 --> 00:36.240
Today we're joined by Dan Schreider, Assistant Professor in the Department of Genetics at

00:36.240 --> 00:39.840
the University of North Carolina at Chapel Hill.

00:39.840 --> 00:45.320
My discussion with Dan starts with an overview of population genomics and from there, digs

00:45.320 --> 00:50.600
into his application of machine learning in the field, allowing us to, for example, better

00:50.600 --> 00:56.480
understand population size changes and gene flow from DNA sequences.

00:56.480 --> 01:01.720
We then dig into Dan's recent paper, the unreasonable effectiveness of convolutional neural networks

01:01.720 --> 01:07.320
in population genetic inference, which was published in the molecular biology and evolution

01:07.320 --> 01:08.320
journal.

01:08.320 --> 01:13.800
The paper examines the idea that CNNs are capable of outperforming expert derived statistical

01:13.800 --> 01:17.920
methods for some key problems in the field.

01:17.920 --> 01:23.800
Before we dive in, a quick thanks to our friends at Pegasystems, sponsors of today's show.

01:23.800 --> 01:28.240
Pegaworld, the company's annual digital transformation conference, which will be held

01:28.240 --> 01:34.360
at the MGM Grand in Las Vegas from June 2nd to 5th, is just a couple of months away now.

01:34.360 --> 01:39.600
I'll be attending the event as I did last year and will once again be presenting.

01:39.600 --> 01:43.640
In addition to hearing from me, the event is a great opportunity to learn how AI has

01:43.640 --> 01:48.360
applied to the customer experience at real-pecker customers.

01:48.360 --> 01:54.360
As a Twimble listener, you can use the promo code Twimble19 for $200 off of your registration.

01:54.360 --> 01:56.760
Again, that code is Twimble19.

01:56.760 --> 01:58.320
Hope to see you there.

01:58.320 --> 02:00.240
And now on to the show.

02:00.240 --> 02:02.560
Alright, everyone.

02:02.560 --> 02:04.480
I am on the line with Dan Schreider.

02:04.480 --> 02:10.200
Dan is an assistant professor in the Department of Genetics at the University of North Carolina

02:10.200 --> 02:11.480
at Chapel Hill.

02:11.480 --> 02:14.280
Dan, welcome to this week in Machine Learning and AI.

02:14.280 --> 02:15.280
Hi, Sam.

02:15.280 --> 02:16.280
Thanks for having me.

02:16.280 --> 02:17.280
Awesome.

02:17.280 --> 02:18.760
It's great to have you on the show.

02:18.760 --> 02:25.440
So you have an undergraduate degree in computer science, but you are now an evolutionary

02:25.440 --> 02:26.440
biologist.

02:26.440 --> 02:31.560
Can you tell us about that transition and how it led you to work in machine learning?

02:31.560 --> 02:32.560
Sure.

02:32.560 --> 02:33.560
Yeah.

02:33.560 --> 02:35.160
So it's actually sort of a series of transitions.

02:35.160 --> 02:39.280
And it started when I was in undergrad, I was studying computer science.

02:39.280 --> 02:43.280
I got into that field because I liked to write code in high school.

02:43.280 --> 02:45.840
I didn't really know what I was going to do with it.

02:45.840 --> 02:53.240
And during, I believe, my sophomore year, I started going to seminars about sort of

02:53.240 --> 02:55.080
various topics in computational research.

02:55.080 --> 02:57.600
And I heard about this thing called bioinformatics.

02:57.600 --> 03:02.280
And it turned out that you could write code to do research in biology.

03:02.280 --> 03:03.680
And I thought that sounded amazing.

03:03.680 --> 03:07.760
I always sort of liked the idea of being a scientist, though I didn't know much about

03:07.760 --> 03:09.240
biology at the time.

03:09.240 --> 03:15.960
And the night after that seminar, I immediately started looking into taking biology courses

03:15.960 --> 03:17.880
and sort of shifting focus.

03:17.880 --> 03:25.680
And from that moment forward, I was training to become a biological researcher as well as

03:25.680 --> 03:29.240
a, you know, someone with computer programming skills.

03:29.240 --> 03:32.800
I was not sort of keen on the idea of being a software engineer.

03:32.800 --> 03:38.640
And yeah, when I heard that, oh, someone like me can, you know, be a biologist.

03:38.640 --> 03:39.880
I thought that was really cool.

03:39.880 --> 03:42.200
So I started doing that.

03:42.200 --> 03:48.760
And when I wrapped up my degree, I decided to stay at Indiana University and started working

03:48.760 --> 03:53.880
there with Matthew Hahn, who is my PhD advisor there.

03:53.880 --> 04:03.040
And our area of research was population genetics, which is a subfield of evolutionary biology

04:03.040 --> 04:11.360
where you're sort of looking at the evolutionary dynamics of gene sequences, especially in

04:11.360 --> 04:12.960
the recent evolutionary history.

04:12.960 --> 04:17.840
And you do this by looking at sort of the patterns of genetic variation that are present

04:17.840 --> 04:19.480
within a population.

04:19.480 --> 04:24.360
So you go out to nature, sample a bunch of individuals, sequence their genomes, and see

04:24.360 --> 04:28.600
what sense you can make of all the variation there.

04:28.600 --> 04:32.680
Started that at the beginning of grad school, fell in love with it, and haven't really looked

04:32.680 --> 04:42.120
away since now you might be gathering that this involves a lot of data analysis, a lot

04:42.120 --> 04:44.360
of sequence analysis.

04:44.360 --> 04:49.440
And we're interested in population genetics with trying to tease apart the different

04:49.440 --> 04:54.400
evolutionary forces that are shaping patterns of genetic variation, things like natural

04:54.400 --> 05:00.280
selection or demographic events such as population size changes, like population crashes

05:00.280 --> 05:05.800
or expansions, and all of these things sort of leave their footprints in patterns of genetic

05:05.800 --> 05:09.120
variation within species.

05:09.120 --> 05:13.760
So we're sort of interested in going backwards, taking these patterns of variation and making

05:13.760 --> 05:17.640
inferences about the evolutionary forces at play.

05:17.640 --> 05:21.400
And it turns out that machine learning is a great way to go about doing this because

05:21.400 --> 05:26.000
you have a lot of high dimensional data and we're trying to sort of, you know, churn out

05:26.000 --> 05:28.280
as much information from it as we can.

05:28.280 --> 05:29.280
Very cool.

05:29.280 --> 05:34.920
One question that just jumps out at me is you mentioned that your study or maybe evolutionary

05:34.920 --> 05:42.680
biology in general is focused on recent changes in genomes.

05:42.680 --> 05:44.520
What does that mean in this concept?

05:44.520 --> 05:46.600
Yeah, certainly clarify.

05:46.600 --> 05:51.880
Population genetics, my sort of subfueled within evolutionary allergies, is often more

05:51.880 --> 05:55.640
concerned with recent changes because the recent evolutionary events, because those are

05:55.640 --> 06:00.400
the things that shape present-day patterns of genetic variation.

06:00.400 --> 06:04.080
And so how recent is that in this context?

06:04.080 --> 06:09.880
Yeah, so it depends on the organism and, you know, there's sort of theoretical expectations

06:09.880 --> 06:15.280
for sort of how far back in time you can see based on present-day patterns of variation

06:15.280 --> 06:21.800
and without going into the theory, the idea is essentially that if you take one, we'll

06:21.800 --> 06:26.800
start with one human individual, so humans have two copies of each chromosome, one from

06:26.800 --> 06:35.720
mom, one from dad, and if you compare those two copies, you'll see a bunch of differences,

06:35.720 --> 06:41.600
and that is because these two chromosomes, at some point in the past, were derived from

06:41.600 --> 06:46.960
the same ancestor, but enough time has passed since then that mutations have occurred and

06:46.960 --> 06:51.800
everything, and therefore you see differences between the two.

06:51.800 --> 06:59.680
So we expect that those two chromosomes will have been separated, you know, on separate

06:59.680 --> 07:05.600
sort of evolutionary trajectories for the last two end generations, wherein is the population

07:05.600 --> 07:09.040
size, so that's sort of the expectations.

07:09.040 --> 07:14.720
For humans that turns into something like a scale of hundreds of thousands of years,

07:14.720 --> 07:20.760
and using populations, genetic approaches, you have more resolution to say something about

07:20.760 --> 07:27.360
kind of the more medium, the intermediate and recent subset of that range, so up until

07:27.360 --> 07:32.800
the last 50,000 years or so, we have a lot more power to see what's going on, and once

07:32.800 --> 07:36.800
you get back sort of half a million years ago, you're sort of running out of information

07:36.800 --> 07:40.240
there with that, that's sort of the time range that we're talking about.

07:40.240 --> 07:46.560
And to put that in some context, half a million years in humans, you know, they can compare

07:46.560 --> 07:51.840
that to the time since our split with chimpanzees, which was about five to six million years

07:51.840 --> 07:52.840
ago.

07:52.840 --> 08:00.920
Maybe walk us through some of the biological concepts that might be handy in kind of

08:00.920 --> 08:04.880
exploring what you do and how you apply machine learning.

08:04.880 --> 08:16.480
Sure, so I think sort of the key concept is that we're dealing with a sample of genomes,

08:16.480 --> 08:24.160
so it's not cost effective to sequence every genome in the population, so we draw some

08:24.160 --> 08:30.720
random subset and sample them, and then what we get is a string of letters, A, C, G, and

08:30.720 --> 08:35.040
T is for for each of the individuals that we've sequenced, and then we're sort of putting

08:35.040 --> 08:41.680
that together into this matrix where each row in the matrix is one genome sequence, and

08:41.680 --> 08:45.920
each column in the matrix is one site along that genome as we're moving along.

08:45.920 --> 08:51.120
So that the human genome, for example, is about three billion of these sites.

08:51.120 --> 08:53.800
Now not every one of those sites will exhibit variations.

08:53.800 --> 08:57.240
There's not always useful information there that we're going to look at, but you know,

08:57.240 --> 09:01.480
there's a lot of columns in this matrix.

09:01.480 --> 09:07.880
So that's sort of the data that we're dealing with, then there are a number of questions

09:07.880 --> 09:11.920
that we're interested in answering with that data, one area of research that has been

09:11.920 --> 09:17.720
a major focus of mind for the last several years is looking for the signatures of natural

09:17.720 --> 09:18.720
selection.

09:18.720 --> 09:25.320
So if a new mutation shows up in a population and is harmful, then it will be rapidly

09:25.320 --> 09:29.840
removed by natural selection because individuals bearing that mutation will be less likely

09:29.840 --> 09:31.560
to reproduce.

09:31.560 --> 09:37.800
And therefore in that region of the genome, you might expect to see a deficit of diversity.

09:37.800 --> 09:44.480
If on the other hand, a new mutation appears and it's beneficial, then it will rapidly increase

09:44.480 --> 09:50.200
in frequency because individuals harboring it are more likely to survive reproduce and

09:50.200 --> 09:57.120
leave offspring. So after some number of generations, this mutation has increased in frequency

09:57.120 --> 10:05.520
to the point where it has replaced the ancestral version of that site in the genome.

10:05.520 --> 10:13.720
And this will also create a sort of distinct characteristic signature of selection that

10:13.720 --> 10:17.480
we can try to uncover by using some of these computational techniques.

10:17.480 --> 10:24.120
So yeah, there are a whole number of other interesting areas in population genetics that

10:24.120 --> 10:28.120
are doing similar types of research.

10:28.120 --> 10:34.600
We're taking this sort of input matrix and trying to infer what is going on there.

10:34.600 --> 10:39.040
But the natural selection question has been one that's especially near and dear to my

10:39.040 --> 10:41.960
heart over the last few years.

10:41.960 --> 10:51.360
Does that depend on or assume that you are doing a complete sequence of the genome for the

10:51.360 --> 10:57.720
samples that you're working with or are you able to make inferences based on partial

10:57.720 --> 10:59.280
sequences as well?

10:59.280 --> 11:00.800
You can use partial sequences.

11:00.800 --> 11:07.520
In fact, population genetics as an empirical discipline has existed for quite some time

11:07.520 --> 11:12.480
long before we were able to sequence entire genomes, especially if organisms like humans

11:12.480 --> 11:16.320
where we have this large complex genome.

11:16.320 --> 11:20.640
Yeah, there's sort of, we refer to this as kind of a shift from population genetics to

11:20.640 --> 11:25.480
population genomics because for much of the field's history, we were interested in what's

11:25.480 --> 11:31.360
going on at say one gene rather than looking at patterns of variation across an entire genome.

11:31.360 --> 11:39.080
But now with increases and the speed and cost effectiveness of DNA sequencing technologies

11:39.080 --> 11:43.720
were now able to look at variation at genome-wide scale.

11:43.720 --> 11:51.160
So yeah, you're not limited to cases where you have whole genome data, but it's getting

11:51.160 --> 11:56.840
to the point now where anybody can sequence a gene, anybody who has a lab and modest research

11:56.840 --> 12:01.280
funding can sequence a fairly large sample of genomes.

12:01.280 --> 12:07.200
And when you're doing the type of experiments that you describe or you're trying to understand

12:07.200 --> 12:14.680
natural selection, is the implication of what you describe that you're kind of fundamentally

12:14.680 --> 12:21.160
looking at stable sections of the genome as opposed to those sections of the genome that

12:21.160 --> 12:26.680
tend to exhibit a lot of variation, or are you looking at those as well for different

12:26.680 --> 12:27.680
things?

12:27.680 --> 12:33.080
Yeah, so in my work, a lot of what I do is I'm sort of trying to walk along the genome

12:33.080 --> 12:36.880
and look at how sort of the landscape of variation changes.

12:36.880 --> 12:41.920
So you'll have some areas where there's a lot of variation, somewhere there is not.

12:41.920 --> 12:48.840
And we try to make sense of that by sort of segmenting the genome into these different

12:48.840 --> 12:50.680
classes or evolutionary models.

12:50.680 --> 12:55.800
So this chunk of the genome looks like it's being shaped by positive selection, which

12:55.800 --> 13:01.520
is that scenario where beneficial mutation has recently increased in frequency, or this

13:01.520 --> 13:06.360
region of the genome seems to be experiencing negative or purifying selection where harmful

13:06.360 --> 13:09.040
mutations are being removed.

13:09.040 --> 13:14.120
And this region of the genome seems to be evolving relatively free from selection.

13:14.120 --> 13:17.600
So mutations don't really affect fitness.

13:17.600 --> 13:21.920
They're just kind of drifting around randomly over time.

13:21.920 --> 13:26.560
So their frequencies are fluctuating, but it does nothing to do with any sort of selective

13:26.560 --> 13:30.920
benefit or harm caused by the mutation.

13:30.920 --> 13:37.880
So yeah, basically I'm trying to figure out how much of the genome is evolving under

13:37.880 --> 13:41.920
one particular model of evolution versus another, and so on and mentioned in the landscape

13:41.920 --> 13:44.680
as you move across chromosomes.

13:44.680 --> 13:50.400
And so you recently published a paper called the unreasonable effectiveness of convolutional

13:50.400 --> 13:57.360
neuron networks in population genetic inference, actually they came out earlier this month.

13:57.360 --> 14:00.800
We're speaking at middle of February.

14:00.800 --> 14:04.080
And that was published in molecular biology and evolution.

14:04.080 --> 14:09.840
Can you talk a little bit about that particular paper and what it is trying to convey?

14:09.840 --> 14:17.960
Yeah, so this paper is purely methodological and focused and that we're sort of interested

14:17.960 --> 14:23.160
in the different statistical and computational methods that population geneticists have been

14:23.160 --> 14:25.320
using over the years.

14:25.320 --> 14:32.480
And a major focus of my work over the last five years has been to try to incorporate

14:32.480 --> 14:38.000
machine learning techniques into population genetic inference.

14:38.000 --> 14:42.640
And I should probably start there before I get into this paper because this is kind of

14:42.640 --> 14:44.720
the culmination of a lot of that.

14:44.720 --> 14:50.160
I was wondering just that if it was a culmination of a trajectory of things that you tried

14:50.160 --> 14:57.920
in machine learning applied to population genetics or if you, you know, I also talk to scientists

14:57.920 --> 15:04.680
that kind of, you know, just luck into hearing about her finding out about CNNs and deep

15:04.680 --> 15:10.040
learning and apply it to their problem and damage as works and they kind of start there.

15:10.040 --> 15:12.080
So how did that evolve for you?

15:12.080 --> 15:16.360
Though, the way this evolved for me was around the time that I was finishing up in grad

15:16.360 --> 15:20.600
school and getting started with my postdoctoral research.

15:20.600 --> 15:24.840
I was becoming increasingly interested in this question of, you know, how much can we

15:24.840 --> 15:31.960
learn about natural selection from looking at genomic data and the methods for doing this

15:31.960 --> 15:38.680
in population genetics I found were, to me, they seemed a little bit antiquated in

15:38.680 --> 15:44.520
that they were often focused on taking your alignments, you know, this matrix that I'm

15:44.520 --> 15:50.360
talking about, this matrix of genome sequences and sort of boiling it down to a single number.

15:50.360 --> 15:56.120
So describing your sequence data by a single statistic, which is descriptive, it tells

15:56.120 --> 16:01.160
you something about how much variation is there in this alignment or what are the frequencies

16:01.160 --> 16:06.120
of mutations in this alignment are some of them very common within the population or

16:06.120 --> 16:10.400
they're very rare, to what extent are two mutations correlated?

16:10.400 --> 16:16.440
That is, if an individual has a mutation at site one, how does that tell you something

16:16.440 --> 16:22.360
about whether he also has a mutation at site two, there are a large number of statistics

16:22.360 --> 16:28.080
that all sort of captured different somewhat redundant but somewhat complementary patterns

16:28.080 --> 16:34.040
of a variation and there's kind of this cottage industry and population genetics of coming

16:34.040 --> 16:37.360
up with a new statistic that you think is the best one for answering the question that

16:37.360 --> 16:42.000
you're looking at and sort of describing the theoretical expectations for the statistic

16:42.000 --> 16:46.320
under various evolutionary models and, you know, applying this to some data and seeing

16:46.320 --> 16:47.320
what you can learn.

16:47.320 --> 16:53.240
Well, that's all great, but, you know, you can imagine that if you take this large matrix

16:53.240 --> 16:57.480
of genome sequence data and boil it down to a single number, you're probably throwing

16:57.480 --> 17:00.840
out a lot of useful information, right?

17:00.840 --> 17:06.360
So the work that I was doing during my postdoc and there's some other labs that are doing

17:06.360 --> 17:11.760
this too, I'm not the only one doing this, but it was sort of a small group of population

17:11.760 --> 17:17.840
geneticists doing this and we were just trying to incorporate as much of this information

17:17.840 --> 17:25.560
as we could into a method and one way to do that is to, instead of using one of these

17:25.560 --> 17:32.080
statistics, use a large vector of them, throw them all in to a vector and try something

17:32.080 --> 17:36.240
like support vector machine or random forest so you can use a support vector machine and

17:36.240 --> 17:41.800
train it to distinguish between natural selection or no natural selection.

17:41.800 --> 17:45.760
Yeah, if I can jump in just to make sure I understand what you're doing, you've got kind

17:45.760 --> 17:52.040
of this underlying set of data, which is essentially these alignments, you've got genomes or

17:52.040 --> 18:02.600
genome samples that are aligned with one another and the first thing you did was you took the

18:02.600 --> 18:08.720
traditional metrics that have been applied to these alignments and you kind of calculated

18:08.720 --> 18:17.000
all of them, put those into a vector and then use machine learning to, for example, identify

18:17.000 --> 18:24.280
clusters within the vector space that you created of these summary statistics.

18:24.280 --> 18:25.280
Is that correct?

18:25.280 --> 18:26.280
Yeah, that's right.

18:26.280 --> 18:31.560
So the idea is rather than arguing over which one of these summary statistics is best,

18:31.560 --> 18:35.480
we should see how well we can do if we use all of them at once and machine learning

18:35.480 --> 18:41.320
is one way for you to do that because you can use it for higher dimensional data.

18:41.320 --> 18:46.000
So when you say how well you do, what was the specific problem or what are the types of

18:46.000 --> 18:47.600
problems that you're trying to solve?

18:47.600 --> 18:52.880
Is it just clustering them together or are there other problems that you apply this

18:52.880 --> 18:53.880
technique to?

18:53.880 --> 19:01.160
Yeah, so a lot of population genetic inference is about discriminating between different

19:01.160 --> 19:02.960
evolutionary models.

19:02.960 --> 19:11.680
So let's go back to this question of can we find whether there has been a recent beneficial

19:11.680 --> 19:17.160
mutation that has increased in frequency and become what we call fixed or ubiquitous

19:17.160 --> 19:18.800
within the population?

19:18.800 --> 19:24.600
So a little bit of terminology, we call this a selective sweep because this mutation is

19:24.600 --> 19:27.400
selected and it sweeps through the population.

19:27.400 --> 19:35.320
So this problem of finding selective sweeps is a very difficult one, but it's one that's

19:35.320 --> 19:40.480
sort of central to pop general research because we're interested in how much recent

19:40.480 --> 19:46.760
adaptation, you know, particular species might be having whether it's humans or anything

19:46.760 --> 19:51.920
else, you know, how are we responding to the selective environment that we're in?

19:51.920 --> 19:58.160
So you can use this information to tell you something about how much adaptation is there,

19:58.160 --> 20:02.120
which parts of the genome are responsible for this adaptation, but it all boils down

20:02.120 --> 20:03.480
to this model selection thing.

20:03.480 --> 20:08.440
Can I discriminate between regions of the genome that are experiencing a sweep and those

20:08.440 --> 20:10.160
that are not?

20:10.160 --> 20:17.720
So the way that I have gone about this model selection is by treating a classifier to

20:17.720 --> 20:18.920
do it.

20:18.920 --> 20:25.760
Is the data just the sequences, at least at this point in your application of machine

20:25.760 --> 20:33.960
learning, is it just the sequences or is it the sequences, for example, and I guess

20:33.960 --> 20:39.560
I'm curious about if there's some element of time that's captured like the, you know,

20:39.560 --> 20:43.960
birth or death or eight, you know, the timestamp of the sequence or something like that.

20:43.960 --> 20:47.520
And you're looking at these sequences over a long period of time.

20:47.520 --> 20:53.400
Is it like a time series thing or are you able to infer these sweeps just by looking

20:53.400 --> 20:59.680
at t equals zero set of samples and the way that they're distributed?

20:59.680 --> 21:00.680
Sure.

21:00.680 --> 21:01.680
Yeah, that's a great question.

21:01.680 --> 21:07.120
So generally we do, you know, think about the scenario where you only have sampling from

21:07.120 --> 21:09.200
one time point.

21:09.200 --> 21:15.240
So, you know, you've randomly sampled from some population, you know, last year and

21:15.240 --> 21:20.800
you have this data set and you're trying to use it to make these inferences about, you

21:20.800 --> 21:23.280
know, recent evolutionary history.

21:23.280 --> 21:25.960
And a large reason for that, I mean there are two reasons for that.

21:25.960 --> 21:33.160
One is the practical cost of accumulating sequence data and that cost has gone down.

21:33.160 --> 21:35.200
I'll come back to that in a second.

21:35.200 --> 21:40.560
And the other is that if you want to do this sort of time series thing, you need to say

21:40.560 --> 21:43.040
you want to sample every generation.

21:43.040 --> 21:46.720
Well, if you're doing that in humans, then, you know, each generation, you know, might

21:46.720 --> 21:48.600
be 25, 30 years or so.

21:48.600 --> 21:54.760
So to sort of capture any interesting patterns or you have to be accumulating data over a long

21:54.760 --> 21:56.320
period of time.

21:56.320 --> 22:02.360
Now, some organisms have much more rapid generation times.

22:02.360 --> 22:07.720
They also work on fruit flies and mosquitoes and, you know, these things rather than having

22:07.720 --> 22:14.440
one generation every few dozen years they'll have a, you know, a dozen or so generations

22:14.440 --> 22:15.440
in one year.

22:15.440 --> 22:19.360
So over the course of a few years, you're going to accumulate a large number of generations

22:19.360 --> 22:21.200
and create time series data.

22:21.200 --> 22:26.200
And this is something that's becoming feasible now with improvements in sequencing technology.

22:26.200 --> 22:32.400
So I would say that that's kind of a small subset of the kind of landscape of pop chin research

22:32.400 --> 22:36.600
right now, but you're going to be seeing that changing very rapidly.

22:36.600 --> 22:37.600
I think so.

22:37.600 --> 22:40.880
So you'll see a lot of this time series analysis, but right now it's mostly just looking at

22:40.880 --> 22:42.560
this one snapshot.

22:42.560 --> 22:49.840
So at this stage where you're creating a classifier, you've got this data as we discussed, it tends

22:49.840 --> 22:55.960
to be from a single period of time or a period of time I'm assuming that where you can

22:55.960 --> 23:01.400
kind of, they're close enough that you can kind of ignore time as a big factor.

23:01.400 --> 23:06.880
And you've collected this data, can you talk a little bit about the data collection process?

23:06.880 --> 23:13.280
So in, this is where it gets a little bit tricky because in sort of more traditional applications

23:13.280 --> 23:18.240
of machine learning, you want to collect training data.

23:18.240 --> 23:24.560
And here we can't do that, you know, we collect data that we can apply something to, right,

23:24.560 --> 23:29.080
we can sequence a bunch of human genomes that we want to run our classifier on.

23:29.080 --> 23:33.560
But how do we get a data set where we know what the ground truth is?

23:33.560 --> 23:38.320
And you know, that can be tricky and evolutionary biology because, you know, while we are doing

23:38.320 --> 23:44.560
our best to make evolutionary inferences, it's difficult to nail down with absolute certainty,

23:44.560 --> 23:45.560
right?

23:45.560 --> 23:50.520
What is the evolutionary history of, you know, this species, this population or this gene?

23:50.520 --> 23:57.480
So what we do is we simulate.

23:57.480 --> 24:05.240
So there are these nice idealized models of evolution that allow you to simulate an

24:05.240 --> 24:10.520
evolving population and sequences within this population.

24:10.520 --> 24:15.760
So you can produce synthetic data that one can then use to train a classifier.

24:15.760 --> 24:20.360
And then the tricky part is, you know, how do we simulate this thing?

24:20.360 --> 24:25.320
How do we parameterize these simulations and, you know, you have to be careful to sort

24:25.320 --> 24:30.920
of try to make these simulations match your data as best as you can.

24:30.920 --> 24:35.800
But if you knew exactly how your data, you know, how your organism of interest was evolving,

24:35.800 --> 24:40.280
then you wouldn't be needing to do the research anyway, right, because you already know everything.

24:40.280 --> 24:46.320
So it's kind of analogous to doing some sort of Bayesian inference where you've got, you

24:46.320 --> 24:48.320
know, these priors.

24:48.320 --> 24:52.440
And, you know, here we're simulating onto those priors to generate training data.

24:52.440 --> 24:59.400
And you have to examine what is the robustness of this classifier to model a specification.

24:59.400 --> 25:03.520
What if I'm wrong about the, you know, the parameters of the simulation?

25:03.520 --> 25:08.000
What if I, you know, I thought the population size of this organism was 1 million, but

25:08.000 --> 25:14.440
actually it's half a million, how does that affect the accuracy of my downstream analysis?

25:14.440 --> 25:17.080
So you have to take all these things into account.

25:17.080 --> 25:23.000
Is there some like prototypical human genome that you use as the starting place for your

25:23.000 --> 25:31.040
simulation or some kind of standard human, how does that work?

25:31.040 --> 25:32.040
Yeah.

25:32.040 --> 25:33.040
So, yeah, there is.

25:33.040 --> 25:34.040
That's a great question.

25:34.040 --> 25:38.680
So we typically refer to these as reference genomes.

25:38.680 --> 25:46.280
So for the way that sequencing started out like this approach was that you would go and

25:46.280 --> 25:52.000
create one very high quality genome sequence for your species.

25:52.000 --> 25:57.560
So we did this in humans and it took over a decade and a billion dollars to sequence

25:57.560 --> 26:01.520
the first human genome, which we call the reference genome, but it's this very high quality

26:01.520 --> 26:10.080
sequence and you can use that by using these cheap and fast DNA sequencing technologies,

26:10.080 --> 26:17.200
which produce a bunch of very tiny chunks of DNA sequence that you then search against

26:17.200 --> 26:18.200
that reference genome.

26:18.200 --> 26:23.600
So that's this, that's how you create these alignments that I was referring to by doing

26:23.600 --> 26:27.880
sequence searches and figuring out, okay, this tiny little chunk here that goes to this

26:27.880 --> 26:32.480
part on chromosome one, I can figure that out by searching it against the reference genome.

26:32.480 --> 26:38.000
So there are sort of these two different tiers of genome sequences, are these very high

26:38.000 --> 26:45.920
quality reference genomes and then there are these genomes produced from these rapid sequencing

26:45.920 --> 26:51.040
technologies that are mapped against the reference genome in order to reveal variation

26:51.040 --> 26:55.160
because you take this tiny chunk from this individual sequence, you map it to the reference

26:55.160 --> 27:01.320
and you see, okay, this little chunk or read, we call it, it's identical to this portion

27:01.320 --> 27:04.760
of the reference genome except for this one difference.

27:04.760 --> 27:08.720
And if you accumulate enough evidence for those differences, then you know, okay, that

27:08.720 --> 27:12.120
is a mutation that's present in my population.

27:12.120 --> 27:17.960
And then you referenced needing to make sure you get the population size right.

27:17.960 --> 27:24.560
In the case of humans, for example, where does the complexity come in there and is it

27:24.560 --> 27:30.920
correct relative to this reference genome and the specifics of the population you're sampling

27:30.920 --> 27:33.520
or what kind of resolution do you need there?

27:33.520 --> 27:39.360
Yeah, I mean, that's a good question and it's still a very open area of research.

27:39.360 --> 27:47.880
So you can go get the census population size of humans today or pretty good approximation

27:47.880 --> 27:52.600
of it, but that has not been the size of our population over much of our history.

27:52.600 --> 27:57.440
There's been dramatic changes, of course, growth most recently, but in many populations

27:57.440 --> 28:04.880
there were contractions, especially with those populations that migrated out of Africa.

28:04.880 --> 28:09.480
That migration was associated with a large population bottleneck.

28:09.480 --> 28:17.840
So the population size history in humans is, it's messy, it's non-monotonic and it concludes

28:17.840 --> 28:22.440
with this very rapid, super-exponential explosion that's been going on for the last

28:22.440 --> 28:24.480
few hundred years now.

28:24.480 --> 28:33.480
So to what extent can we accurately model these population size changes, that's another

28:33.480 --> 28:41.120
area of research that actually touch upon a little bit in the CNN paper that we'll be

28:41.120 --> 28:42.120
talking about.

28:42.120 --> 28:43.120
Yeah.

28:43.120 --> 28:45.120
I don't know if I answered your question.

28:45.120 --> 28:47.120
No, I think you did.

28:47.120 --> 28:50.160
I think my takeaway was it's complicated.

28:50.160 --> 28:54.520
Yeah, okay, then I think I did a good job.

28:54.520 --> 28:56.520
Yeah.

28:56.520 --> 29:03.080
So you're doing this simulation, you're applying kind of Bayesian types of methods, you're

29:03.080 --> 29:11.080
trying to apply probability distributions at different points in the simulation process.

29:11.080 --> 29:16.040
I guess the question is, are you doing that per read or SNP or something like that?

29:16.040 --> 29:23.040
Are you doing that on a sequence level like do you, are we at the point where we're modeling,

29:23.040 --> 29:31.240
variability and distributions on a kind of subsequent level or is it kind of a more

29:31.240 --> 29:34.080
coarse-grained model today?

29:34.080 --> 29:35.080
Yeah.

29:35.080 --> 29:39.520
So for the most part, Rich, and sort of zooming in and looking at relatively small regions

29:39.520 --> 29:45.360
of the GNOME, so that's kind of what we're simulating, yeah, to sort of come up with

29:45.360 --> 29:53.200
expectations for patterns of variation within, you know, say a size of the genome that's

29:53.200 --> 29:58.600
spanning, you know, a few dozen genes, something like that or less, sometimes something on

29:58.600 --> 30:00.000
the order of a single gene.

30:00.000 --> 30:06.200
So yeah, typically we're looking at smaller regions, but for some questions, especially

30:06.200 --> 30:12.000
this question of trying to infer demographic changes like population size changes, we

30:12.000 --> 30:16.880
want to be looking at patterns of variation across the whole genome because if there's

30:16.880 --> 30:21.200
a population crash, that affects the amount of genetic variation across the entire genome.

30:21.200 --> 30:25.360
So we want to use as much of that data as possible.

30:25.360 --> 30:33.160
So yeah, another development in our field in recent years has become feasible to simulate

30:33.160 --> 30:36.040
larger and larger genomes.

30:36.040 --> 30:44.040
So we can sort of capture the expected dynamics genome-wide, not just, you know, at a gene

30:44.040 --> 30:45.040
or a few genes.

30:45.040 --> 30:47.840
So we do the whole gamut, I guess.

30:47.840 --> 30:48.840
Okay.

30:48.840 --> 30:49.840
Okay.

30:49.840 --> 31:00.000
And so this approach to training a classifier based on the sequences and simulated results

31:00.000 --> 31:05.360
to provide your ground truth was one of your first steps in the direction of applying machine

31:05.360 --> 31:11.160
learning to pop-gen, you know, the CNN, the next step, or did you have a few more steps

31:11.160 --> 31:12.160
to get there?

31:12.160 --> 31:13.160
Yeah.

31:13.160 --> 31:18.120
So I played around with this approach on, I guess, a few different problems.

31:18.120 --> 31:23.240
You know, I was mostly interested in this natural selection question, but I branched out

31:23.240 --> 31:25.360
to a few different areas.

31:25.360 --> 31:34.320
You know, I kind of got a little bit caught up in trying to push along a cultural change

31:34.320 --> 31:36.320
in our field in population genetics.

31:36.320 --> 31:42.080
I was trying to make the point to people that these machine learning methods are not scary.

31:42.080 --> 31:47.680
We should consider trying to use them for, you know, every question where it is appropriate

31:47.680 --> 31:51.680
and then see how well they compare to our more traditional methods.

31:51.680 --> 32:00.600
So another question that I was looking into was this question of finding gene flow.

32:00.600 --> 32:05.480
So this is the scenario where you have two different populations.

32:05.480 --> 32:13.040
They diverge to some time ago, you know, say African and non-African humans, you know,

32:13.040 --> 32:18.400
they split some time ago when non-Africans migrated out of the African continent and,

32:18.400 --> 32:21.160
you know, colonized your Asia.

32:21.160 --> 32:27.640
And we want to know, after that split, did these populations come back into contact at

32:27.640 --> 32:31.480
a certain time point and exchanged genetic material?

32:31.480 --> 32:36.840
And if so, can we find like the genomic regions where there has been this gene flow?

32:36.840 --> 32:41.880
So yeah, probably the African non-African question is the best example of this because

32:41.880 --> 32:49.240
that the genetic exchange is across the entire genome in cases where there has been the

32:49.240 --> 32:55.200
secondary contact like African-Americans, for example, have European ancestry across

32:55.200 --> 32:59.200
the entire genome, as well as African ancestry.

32:59.200 --> 33:06.640
A better example is probably Neanderthals, so you may have heard it's been reported in

33:06.640 --> 33:16.320
the news that we now know that Neanderthals have donated genetic material to Eurasian

33:16.320 --> 33:26.720
Asians and Europeans, humans. So a typical European individual has maybe two to four percent

33:26.720 --> 33:32.040
of their DNA tracing its ancestry back to Neanderthals.

33:32.040 --> 33:40.000
So after the split between, you know, the ancestral population that gave rise to modern humans

33:40.000 --> 33:49.400
and also Neanderthals, these two species eventually found themselves in the same location,

33:49.400 --> 33:53.680
and there was interbreeding there, and we can sort of see the genetic remnants of that

33:53.680 --> 33:58.080
in certain parts of the human genome.

33:58.080 --> 34:05.440
So I got into this question of detecting regions where there has been regions of the

34:05.440 --> 34:13.040
genome, where there has been the flow of genetic material from one population into another.

34:13.040 --> 34:20.160
And of course, here there have been a large number of statistical approaches devised

34:20.160 --> 34:28.440
over the years to detect these patterns, and I took the same approach of creating a machine

34:28.440 --> 34:36.320
learning classifier that uses a vector of these statistics to discriminate between these

34:36.320 --> 34:38.400
different models of, so here are three models where there is no gene flow between these two

34:38.400 --> 34:44.680
populations, or within this genomic window there is gene flow from population one to population

34:44.680 --> 34:52.440
two and vice versa. And of course, it works much better than any method that is using

34:52.440 --> 34:56.800
just one statistic because you're throwing out all that information, so if you incorporate

34:56.800 --> 35:03.640
as much information as you can, the feature vector you produce more accurate inferences.

35:03.640 --> 35:11.200
So I was going around talking about that work, going to conferences, trying to sell this

35:11.200 --> 35:17.920
stuff to people, say this machine learning stuff works, which here is the evidence.

35:17.920 --> 35:26.520
And somebody in the audience of that talk was interested and went and looked at my code,

35:26.520 --> 35:30.760
and thought, well, what happens if I get rid of this part that calculates all the statistics

35:30.760 --> 35:37.080
and just replace it with a convolutional neural network? So let a neural net come up with

35:37.080 --> 35:41.880
its own statistics and see how well it can answer this question. And this person was

35:41.880 --> 35:48.880
Lex Flagel who was the first author on that paper published in MBE, and Lex sent me an

35:48.880 --> 35:55.920
email one day, and yet the day I opened up this email was the most exciting day of my professional

35:55.920 --> 35:59.520
life. He opened this up and said, hey, I got this result. I was playing around with the

35:59.520 --> 36:05.840
convolutional neural net, and thought I'd see how it did. And here are the results. Is

36:05.840 --> 36:11.840
this good? So I pull up my data and I'm comparing and I'm like, huh, this is better than mine.

36:11.840 --> 36:20.800
So, yeah, so there had been a few conversations I'd had in the years leading up to this

36:20.800 --> 36:26.680
about this idea of maybe using deep learning or some approach to say, well, act directly

36:26.680 --> 36:32.920
on the alignment rather than pre-digesting it into a bunch of features or summary statistics.

36:32.920 --> 36:36.560
But I never had a chance to get around to it. And then suddenly there on my screen was

36:36.560 --> 36:42.080
the evidence that not only can you do this, but it works really well better than what

36:42.080 --> 36:47.680
I was trying at the time. So we knew right away, or I knew right away that we had to try

36:47.680 --> 36:52.960
this out on a bunch of different problems in population genetics to see if this approach

36:52.960 --> 36:59.840
would work in general. And that was the work that led to the MBE paper.

36:59.840 --> 37:05.680
In the paper you're doing just that, you're trying CNNs out on a bunch of these different

37:05.680 --> 37:13.080
problems. Do each of these problems represent one of the statistics that you were kind

37:13.080 --> 37:18.680
of aggregating together in your previous work? Or are the problems kind of at a higher level

37:18.680 --> 37:22.720
and the statistics, you know, informed approaches to the problems?

37:22.720 --> 37:27.160
Yeah, so these problems have been major areas of research and pension for quite some

37:27.160 --> 37:32.480
time. So the different problems we touch on in the paper are this problem of finding

37:32.480 --> 37:43.080
selective sweeps, this problem of finding gene flow. We try to infer population size changes

37:43.080 --> 37:51.840
and the other is inferring the rate of our combination. So our combination is when

37:51.840 --> 37:58.160
during myosis you're two different chromosome copies, exchange, genetic material, one

37:58.160 --> 38:07.720
another. So it sort of breaks up the association between mutations along a chromosome. So you

38:07.720 --> 38:11.240
would think that if you're looking at say chromosome one in the human genome, like if you take

38:11.240 --> 38:17.120
one of your copies of chromosome one as you're going back in time, that whole chromosome

38:17.120 --> 38:22.240
should have the same evolutionary history, you know, as you trace it back through ancestor

38:22.240 --> 38:28.960
to ancestor, but recombination breaks that up. Different chunks of the chromosome have different

38:28.960 --> 38:38.920
ancestors. And it's a longstanding problem in population genetics trying to infer how

38:38.920 --> 38:43.120
much of this recombination happens in different parts of the genome. So can you infer the

38:43.120 --> 38:49.280
recombination rate landscape across the genome? And you can try to do this using population

38:49.280 --> 38:56.480
genetic data. So a bunch of methods exist for doing that. All of these problems have gotten

38:56.480 --> 39:00.160
a lot of attention from researchers over the years. So there's not just one statistic for

39:00.160 --> 39:08.800
each of them. There are dozens for many of them. So we were comparing our approach of just

39:08.800 --> 39:15.440
throwing it all into a CNN and then seeing what answer comes out to many of these statistics

39:15.440 --> 39:20.480
that have been used or vectors of those statistics, you know, like the approach I had been using

39:20.480 --> 39:24.320
up to that point of throwing them all into a classifier and using something like a random

39:24.320 --> 39:30.720
forest. Can you talk a little bit about the approach to making your data fit well within

39:30.720 --> 39:34.720
the paradigm of CNNs? Did you need to do anything special there?

39:34.720 --> 39:41.920
Yeah, you certainly do. And because we wanted this to sort of be a proof of principle,

39:41.920 --> 39:47.440
we were focusing on simulated data in this paper just to show that this can work in principle,

39:47.440 --> 39:52.160
and it can work better than some of the best methods that we have at our disposal right

39:52.160 --> 39:58.240
now. We didn't want to get too deep into the rabbit hole of all the things that you have

39:58.240 --> 40:07.120
to do when you're working with real data that can be messy. So yeah, one problem with,

40:07.120 --> 40:10.960
so we kind of skipped a lot of this stuff or just pointed out that these are issues that

40:10.960 --> 40:16.880
won't have to deal with. But I think that the deep learning approach is probably better suited

40:16.880 --> 40:19.840
for dealing with a lot of these problems than some of the more traditional approaches.

40:20.480 --> 40:27.760
Because what are some of those problems? Yeah, so a common problem is that there can be

40:29.120 --> 40:33.440
piece parts of the genome where you just don't have a lot of information where, you know,

40:33.440 --> 40:37.520
you've mapped these reads, but you're not exactly sure where they go or the reads are of

40:37.520 --> 40:43.920
localities, so you're not exactly sure what nucleotide is there with the bases. So there's

40:43.920 --> 40:50.880
low confidence data or missing data, and you have to sort of mask these out. So dealing with this,

40:50.880 --> 40:55.520
I think, is actually pretty straightforward. In this machine learning framework, you take a look

40:55.520 --> 41:00.960
at your actual data and see like sort of what's the distribution of missing data along my genome,

41:00.960 --> 41:09.440
and then just adjust your simulations accordingly, synthetically mask some portion of your training

41:09.440 --> 41:17.840
data and your simulated test data, and then train. It's a lot more easy to do this when your input

41:17.840 --> 41:24.400
is just the sequence matrix rather than some statistic that has been calculated across the matrix,

41:24.400 --> 41:31.360
and it might not be designed to deal with missing data. You know, we don't really have to deal

41:31.360 --> 41:36.640
with that. We just have to supply training data that we think kind of matches what the input

41:36.640 --> 41:44.640
data look like. You know, we think of CNS, we most frequently think of image data that has kind of

41:46.640 --> 41:52.080
you know, often square or two dimensional or two three dimensional channels kind of structure.

41:52.080 --> 42:00.560
How did your sequence data map to that? Yeah, so we were using two dimensional data where each

42:00.560 --> 42:11.120
row in your matrix is a genome sequence in each column is one site. Yeah, the shape of this matrix

42:11.120 --> 42:16.880
is different from what you might find in like a typical image or something like, you know,

42:16.880 --> 42:25.200
the image database because you might have something like 50 rows, you know, 50 is a fairly

42:25.200 --> 42:29.600
small number of pixels, but you may have thousands of columns, you know, maybe 10,000. So,

42:31.200 --> 42:37.440
you know, you get these very oblong matrices that you're that you're shoving in there. So,

42:37.440 --> 42:47.760
yeah, we're actually using primarily one D convolutions within the paper kind of treating these

42:47.760 --> 42:55.120
more as time series data than as image data. And myself have also played around with applying

42:55.120 --> 43:00.800
different flavors of RNNs. And I know some other researchers that are doing that as well now.

43:02.480 --> 43:07.120
So they don't look exactly, you know, in terms of their shape, like a typical image that you'd

43:07.120 --> 43:15.520
be throwing a CNN app. This is kind of a, a net, I guess, but did you, I'm curious if you

43:16.160 --> 43:21.680
did like a one-hot encoding on your proteins or your gene, individual genes?

43:22.560 --> 43:29.280
The way that we encode the sequence data is basically zeros and ones. In a lot of population

43:29.280 --> 43:33.520
genetics, we are only concern ourselves of what we call biolilic sites. Those sites where there

43:33.520 --> 43:40.720
are two different variants. So, you know, maybe I have an A and, you know, you have a G at this site.

43:41.920 --> 43:45.920
You know, we'll just think of those two different alleles as zero and one. So, just a matrix

43:45.920 --> 43:52.960
of zeros and ones. And, you know, if we are trying to use that information to find selective sweeps,

43:52.960 --> 43:59.200
then we'll have different classes. You know, there are a few different types of selective sweeps.

43:59.200 --> 44:04.400
So, you know, like say zero for one particular type of sweep, one for another and, you know, two for

44:04.400 --> 44:09.520
a neutrally evolving region of the genome. Those are our class labels and it'll make a one-hot

44:09.520 --> 44:17.760
and coded vector of those classes. But yeah, the input matrix is just a 2D matrix of zeros and ones.

44:17.760 --> 44:24.560
Okay. For now. But I mean, there's a lot of, you know, other different ways of encoding that one

44:25.280 --> 44:29.120
could try out here. And I think it's important to stress here. And I'm sure a lot of listeners

44:29.120 --> 44:36.480
of your podcast will gather that we are very far behind the machine learning field, the deep learning

44:36.480 --> 44:41.520
field. When it comes to population genetics, we're, you know, just sort of trying things out right

44:41.520 --> 44:48.560
now and we're trying out neural nets that, you know, are at least five years or more behind the

44:48.560 --> 44:52.320
state of the art. So, there's a lot of catching up to do and a lot of experimentation with

44:52.320 --> 44:59.440
different network architectures, different input encodings and things like that that we have to try out.

44:59.440 --> 45:05.040
So, can you give us a summary of the results you saw for these four different types of problems?

45:05.040 --> 45:15.360
Yeah. Yeah. So, that's basically deep learning. If trained carefully, it can, it can at least match

45:15.360 --> 45:22.960
the current state of the art methods. If not, handle the outperform them for most of the problems

45:22.960 --> 45:28.720
that we've looked at. So, the one case where I think there's a lot more work to be done that we just

45:28.720 --> 45:37.440
sort of gave a simple first attempt at was this question of trying to infer population size histories.

45:38.560 --> 45:44.000
You know, it's the method we came up with worked fairly well there, but we want to be able to scale

45:44.000 --> 45:53.840
up to genome scale data and you know, a simple CNN approach where you have an image that encompasses

45:53.840 --> 46:00.080
an entire genome of billions of base fares is at least beyond our capabilities at the moment

46:00.080 --> 46:06.000
computationally and it's probably not the appropriate method for that anyway. Something like a

46:06.000 --> 46:11.920
recurrent neural net of some kind might be more appropriate, but for, you know, these questions of

46:11.920 --> 46:18.160
finding gene flow, finding selective sweeps, inferring or combination rates, the results were

46:18.160 --> 46:28.000
pretty stunningly amazing. And, you know, that's why we gave it the title, you know,

46:28.000 --> 46:33.120
in reference to the, you know, the popular unreasonable effectiveness meme in statistics.

46:34.240 --> 46:37.280
Yeah, I mean, we were certainly blown away about how well this stuff works and,

46:37.280 --> 46:44.720
yeah, we wanted to share that enthusiasm with the field and get people to consider these types

46:44.720 --> 46:50.080
of methods in the future for their own work. Awesome. Awesome. And so where do you go from here?

46:51.520 --> 47:00.560
Yeah, so I have started my lab here at UNC almost a year ago and we're continuing some work along

47:00.560 --> 47:09.120
these lines. So I have a postdoc right now who's very interested in trying to dive deeper into

47:09.120 --> 47:17.120
this question of population size changes and try out a variety of methods, including deep learning

47:17.120 --> 47:24.720
methods to to answer these and other demographic questions. So yeah, the deep learning is definitely

47:24.720 --> 47:32.960
still a part of it. I have another postdoc who is working on phylogenomic questions. So here,

47:32.960 --> 47:38.560
phylogenomics is different field of ocean biology where you're looking at sequences from

47:38.560 --> 47:44.720
different species rather than sequences within one population within one species and try to make

47:44.720 --> 47:49.680
inferences like just inferring the species tree that, you know, connects them. So figuring out the

47:49.680 --> 47:59.120
relationship among species, you know, trying to fill out the tree of life. And he's also trying out

47:59.120 --> 48:04.560
deep learning methods there. Yeah, it's not my plan to have a lab that's entirely based on

48:04.560 --> 48:11.360
machine learning and deep learning, but when you've had some success using these methods and,

48:12.400 --> 48:17.360
you know, they've got the cool buzzwords. So it attracts talent and, you know, I have people,

48:17.360 --> 48:22.000
the people who want to join are interested in doing deep learning. So I guess that's what I'm

48:22.000 --> 48:28.720
going to do, right? You're limited by who you can recruit. So, but no, we're enjoying it a lot,

48:29.280 --> 48:35.360
sort of trying to push the envelope to see how much we can learn by applying these methods to

48:35.360 --> 48:41.040
genomic data. Yeah, there's other stuff that I am interested in doing that it's not at all

48:41.040 --> 48:48.240
related to machine learning, but yeah, that's probably a topic for a different conversation.

48:48.240 --> 48:53.200
Well, Dan, thanks so much for taking the time to chat with us about your work. It's really

48:53.200 --> 49:00.640
exciting to see how deep learning and machine learning in general is applied to these types of

49:00.640 --> 49:06.400
problems in population genetics. Yeah, my pleasure. Thanks for having me, Sam. And I'm looking

49:06.400 --> 49:10.960
forward to seeing how this stuff evolves over the next few years because, you know, I want to be clear,

49:10.960 --> 49:16.800
there are some other labs working on this as well. And I want to see where they where they take it.

49:16.800 --> 49:17.680
Thank you. Thanks.

49:20.800 --> 49:26.480
All right, everyone. That's our show for today for more information on Dan or any of the topics

49:26.480 --> 49:33.840
covered in today's episode. Visit twimmelai.com slash talks slash 249. Be sure to register for

49:33.840 --> 49:41.120
Peggo World using the code twimmel19 for $200 off of registration. As always, thanks so much

49:41.120 --> 50:06.400
for listening and catch you next time.

