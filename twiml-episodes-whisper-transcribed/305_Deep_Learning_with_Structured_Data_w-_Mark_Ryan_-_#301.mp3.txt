As we gear up for Tumacon AI platforms, now less than two weeks away, I've been out
and about talking up the importance of automating, accelerating, and scaling machine learning
in AI in the Enterprise.
Now, I'm usually the one doing the interviewing, so being on the other side for a change
was a nice experience.
Much thanks to my friend's Alex Williams, host of the New Stack Makers podcast, Mentor
Dial, host of the popular podcast Mentor Dialogue, and James McGuire of DataMation.
I had a great time chatting with each of them, and I encourage you to check them out via
the Twumacon blog at twumacon.com slash news.
And while you're there, be sure to check out the latest speakers and agenda updates, and
if you haven't already registered, take that step to secure your ticket for what's shaping
up to be an amazing event.
And now, onto the show.
All right, everyone, I am on the line with Mark Ryan. Mark is the author of Deep Learning
with Structured Data, a book that is currently in Early Access with Manning and Do for Publication
in the spring of 2020.
Mark, welcome to the Twumacon AI podcast.
Thanks, Sam.
It's great to be here.
Awesome.
Let's get started by talking a little bit about your background, and in particular, Deep Learning
with Structured Data.
That is a topic that folks are starting to talk about, and in fact, via the Meetup group
and association with the podcast, we have a fair amount of experience exploring this
through the fast AI study groups that we do, that's a big part of one of the lessons
in that course.
But I'd love to get a sense for how you came to be interested in this particular topic
enough to write a book about it.
Sure.
Sure, Sam.
So my academic background is from Artificial Intelligence a couple of winters ago.
So I studied, studied at U of T with Graham Hurst back in the late 80s, and it was all
symbolic AI back then.
And there were some interesting use cases, but to a large extent, it didn't work.
And I went to work for IBM, had a great career there, learned a great deal, spent a lot
of time in DB2, the relational database product for my VM.
And about 2016, it became evident to me that Artificial Intelligence was starting to work.
There were things that were actually working, became general knowledge, and that reignited
the spark in me.
So I did Andrew Anges intro course, and the fast AI course.
So it had that of my introduction to deep learning.
And I was very interested in deep learning and the promise of that.
And one of the things I found a little bit frustrating is a lot of the use cases, particularly
outside of the context of the fast AI course, were to do with images or audio, they weren't
structured data.
And what I was looking for was, can I find a way to use this in my day-to-day work?
Because this looks like it'll be very useful, and I wanted to learn more about it, and
the best way to learn about it is to use data sets that you're familiar with.
So when I did the fast AI course, and this would have been version one of the course, so it's
been through a couple of iterations since then.
And there's this section on doing deep learning with structured data.
And that really sparked my curiosity, I thought, wow, this is really cool.
So we got to look around for some code to have as sort of a starter kit to get going.
And it wasn't easy to find.
But there were some Kaggle competitions where people had been working on structured data
sets and applying it to deep learning, and some very elegant little design holes that
Rothman stores and the like.
Exactly, exactly.
And I was really impressed by some of the work that I saw there.
It was very elegant, very, very straightforward, and good for me at the stage I was at then
to get started, start doing some coding.
And at the time I was responsible for the support organization for DB2.
So there were, there's tons of data there, hundreds of tickets coming in every day, lots
and lots of data.
And I thought, you know, it would be good if we could apply deep learning to this to sort
of see, can we do some predictions that are useful.
So I built up a prototype model to predict how long a ticket would take to get closed.
And that's, you know, seemed to work reasonably well.
And then did another project, sort of taking what I've learned to predict duty manager calls.
So those are cases where a client reaches a point of frustration and says, I'm done.
I'm going to pick up the phone and get something happening with this particular problem.
So applying what I had seen from some of the kernels in Kaggle and using the data that
was available, created these prototypes.
And I've learned a lot doing that.
And they were, I think they turned out fairly well.
But one of the problems with those prototypes was that the data was obviously proprietary.
It couldn't share that.
And there's a very strong ethic, you know, in machine learning and data science to share
results.
So I started to look for a more, it's like a general data set that I could use to apply
deep learning to structured data.
And I've written a few blog posts on medium about my experience with the predicting time
to resolution and predicting duty manager calls.
And Manning got in touch with me and said, would you like to write a book, kind of pull
this together?
And I thought, wow, that sounds interesting, sure.
It has been a lot of work.
And I've certainly learned a lot in the course of doing that.
And one of the things I've done is create a sort of a full end-to-end example using an
open data set, which is to do with streetcars in Toronto, Toronto is my hometown now.
And it has a very extensive streetcar network.
These are a light rail system that runs on the regular roads.
And they're great.
They're efficient.
They're relatively cheap to run.
They're cheap to create much cheaper than subways.
The problem is, because they share the roads with regular traffic, if they break down
or there's a delay, it really exacerbates gridlock.
So the city of Toronto publishes a data set that describes all of the delays that have
happened for the last five years.
And I thought, well, I'm going to roll up my sleeves and try to create a simple deep learning
model to analyze this data and see if it can come up with predictions to predict where
they're going to be streetcar delays and hopefully be able to prevent them.
So that was kind of the path I took, and that's how that was the genesis for the book.
And so should I assume that that worked, that you were able to come up with a model
that predicted the streetcar delays or predicted streetcar delays with that data set?
Yeah, it does a decent job.
It's not a huge data set.
There are about 90,000 records right now.
Okay.
So, you know, there's some limitations, but certainly for the purposes of helping somebody
who's going to be taking a trip, the accuracy is good enough to be useful.
But more importantly, in terms of as a learning exercise, I think it's useful because it's
an open data set.
It's big, but not so big, you have to deal with kind of the problems of big data.
It's very messy.
So there's a lot of work to be done to prepare the data, which I think is a good learning
experience.
And it has various different kinds of data.
There's text data, there's categorical data, there's some continuous data.
So it has a lot of the, it's big enough to be interesting, but not so big that it's overwhelming.
And I think it's, you know, it kind of makes a decent end-to-end example to go through
the topic.
Awesome.
Awesome.
Jumping back to the couple of projects that you worked on when you were at IBM, in particular
this looking at how long it took to close tickets.
When I think of a trouble ticket use case, and when I think of that trouble ticket use
case, I think of, you know, not just structure data as being useful, but also the content
of the ticket itself.
So textual data, more like the application of NLP.
Did you use only metadata about the tickets to predict the close time, or did you also
use that content?
That's a great question.
So I did use the content, there's, all the tickets had a description, the description
could sometimes it be two lines like, you know, you suck, a little bit more elaborate
than that.
And sometimes it would be a paragraph of lots of detail.
But that description was really essential, because that's kind of the initial customer's
sense of, you know, what they found.
And that was, we had a simple, the model included a simple recurrent neural network to deal
with that data.
So it, that text field was tokenized, used in beddings, and then there was a layer, an
RNN layer, that was applied in the overall model to take that text into account.
And it was interesting, the difference, because they did some experiments of including that
as a feature and then excluding it, because it was fairly expensive, it added some links
to the time it took to train the model.
And it made a reasonable difference, like it was, you know, between three and four percent
the accuracy, if this, if this field was included.
And that really exciting, I thought, that's really something.
And the other thing is that these descriptions weren't all, so the particular field is the
description field, so all of the text of the ticket.
It, no, it's just the description, the text of the ticket was, wasn't available to me
at that time.
So it wasn't full back and forth.
Because that, there could be, you know, the equivalent of 100 pages of text.
So all it was dealing with text wise was the, was the description.
So it could be up to 500, 500, 600 characters altogether.
Got it.
And so that's typically the, the textual description of the issue, either as provided by the
initial customer who's, who's submitting the ticket or whoever the support rep is that
is taking their call, it would, it would always be the customer.
And that was, it was intentional to say, and that was part of the whole idea of the model
was to only take data that was available when a ticket first hit our system.
Okay.
So, and that description would be there.
There were other things, obviously, like, whether the ticket had changed in severity,
that wouldn't be available when the ticket was first opened, because that's kind of a
data leakage problem.
You start to peak over and see, oh, that looks, you know, use data that you don't actually
have available to you when you're making the prediction.
But the textual description of the problem coming from the client was always there when
the ticket was opened, and that was the, that was one of the features that was fed into
the model.
Okay.
And so you said that the importance of this feature, you know, this, this features presence
gave you an additional 3% increase in accuracy.
That is, you know, relative to what without it, how much of an impact did it have?
So, that was in the, in terms of the absolute accuracy.
So, I think at that time, it was probably going from 73 to 76% accuracy, leaving taking
that, taking that field out or leaving it in.
Which I think says a lot about the, the power, I guess, of doing deep learning with the
structure data that adding the text, the descriptive text of the ticket only gave you an incremental
3% accuracy in terms of predicting how long it'll take to close the ticket.
Is that reasonable?
Is that your interpretation as well?
Yeah.
That's right.
I guess at the time I saw it, you know, it's, it's relative where you are.
I was, I was very happy it had that increase, but you're right that the, the portion of the
data that would have traditionally been dealt with with deep learning was only one of the
feeds that was going into the model.
And that feed by itself, well, it made a difference.
It was a relatively small proportion of the difference in terms of the accuracy.
Yeah.
Yeah.
I would have thought that without the text, it would be very hard to solve this, to solve
this problem with any, any degree of accuracy.
Interesting.
And out of curiosity, did you try training a model only on the descriptive text to see if
you were able to get any results?
I'm wondering if there's a scenario where, if you didn't have it, you know, if you
added it, it only gave you 3% incremental accuracy, but you could get a good part of the way
there if you didn't have any of the other things either.
That's a really good question.
I didn't try that, but that's, that's an interesting question.
I, I guess in this may be kind of a primitive way of thinking about it, but because there
were, I guess, you know, there were another 13 or 14 features that were available.
And sometimes this text was fairly short.
I thought, well, this will make it'll, it'll have some impact, but by itself, it probably
wouldn't be enough to actually come up with a reasonable, a reasonably accurate description.
And then just looking at some of the, at some of the descriptions, they were pretty,
pretty cryptic.
But something I just wanted to mention is that these descriptions weren't even all in
English.
So they were, they were a fair number in Japanese or Chinese.
And just given the aggregated size of the data set, it wasn't massively huge.
It was about, about a little over a million records.
It's still, it's still made a difference.
Like it still provided some, some benefit.
And I see that.
I know some people are kind of critical of the idea of using deep learning, kind of backing
up the dump truck of data and just tipping it open, seeing, seeing what comes out.
But at the same time, it was, I was, I was impressed by what could be done without doing
a whole lot of munking around with the data.
It was just sort of taking the, the structure data as it was applying a relatively simple
model to it, getting, you know, not, not bad results, I was, I was impressed, I was impressed
by that.
And particularly, because there have been so little, other than the fast AI course, so
little spoken about in terms of using deep learning with, with structured data.
Mm-hmm.
Mm-hmm.
So we, we tend to think of, rightly so, deep learning as requiring a ton of data.
And certainly exacerbated by the fact that, you know, some of the things that deep learning
is really good at are, you know, images which can be large speech, which can be large files.
Structured data can be a lot more compact, but when we think of the number of, you know,
rows or examples that you're feeding into a model is, do you require less data from
examples or rows perspective to train a deep learning model accurately on structured
data, or is it kind of about the same, but the data is just more compact.
That's a good, that's a good question.
I'd say that to get a, you know, the rule of thought that you need tens of thousands
of records to have a starting point is probably applicable, as applicable to structured data
as it would be to on structured data.
Mm-hmm.
But one of the things interesting, one of the things I heard is a critique, because certainly
there are people who said, like, don't, don't use deep learning with structured data.
And I'm thinking, well, why?
Because that's where my problem space is, that's where the job I'm trying to do is all
about structured data.
Yeah, don't use that.
You use XGBoost, use something simpler.
And I said, well, why?
And some of the answers I got back were, well, the structured data sets are too small.
If they're not big enough to actually apply deep learning to.
And I think that's a bit of a canard, because there are some huge structured data sets.
They're data sets, certainly, in the world of DB2, which is all about structured data,
structured tabular data, commonly had tables with billions of records in them.
So that objection, I think, is a little bit, it's a little bit short-sighted, and obviously
you're not going to get great results with tiny data sets, whether it's structured or
unstructured, but I think as an objection to attempting to use deep learning with structured
data, the data set size really is not material to the decision about whether it's worthwhile
or not.
Did you attempt to apply XGBoost or any other method to this kind of problem?
I did.
So I tried XGBoost on two of those problems that I described.
And I guess one thing that is true, the results were not significantly better for deep learning.
So I think that's one of the arguments people say, well, it's not a great idea to use
deep learning with structured data, is if there's a simpler way to do it that doesn't have
the complexities or the opaqueness of deep learning, then why not do that?
And I think there's something to be said for that.
But at the same time, if I'm looking down the road a little bit, the amount of
effort that it takes to get a reasonable results with XGBoost versus deep learning, particularly
the human effort required for it, I'm not convinced that XGBoost is going to, for example,
is going to be the winner there.
As there's been so much attention on the efficiency of tools related to deep learning, the
libraries are getting better, the TensorFlow 2.0 coming out, making things much more
straightforward and simpler.
I think some of the objections are a little bit out of date.
And there are certainly cases where a non-deep learning approach could produce better results
and maybe the right way to go.
I guess the argument I'm making is that people keep an open mind about applying deep learning
to structured data.
And I think particularly as the human cost is required to get results, becomes a bigger
and bigger factor, may find that deep learning has more applicability to structured data,
to keep an open mind about it.
Yeah, yeah.
Now that's a really interesting point.
I think we've got an entire conference coming up on the tooling and technology platforms
that are allowing enterprises to increasingly automate and make their ability to deliver
deep learning models into production, as well as traditional machine learning models
in a production more quickly and efficiently.
And I think that's only going to serve to reduce the barriers.
And as you mentioned, the frameworks are getting more powerful and easier to use.
And so now you've got this, yes, kind of a more opaque method, but one that is highly
automated in a sense that you don't need a lot of manual feature engineering to get
good results versus one that requires a lot of human investment to get the same level
of results or comparable results.
As that barrier to entry on the deep learning side is reduced, in addition to all the software
tools and frameworks that are improving, you've got hardware that's making it cheaper
to build these models on the compute side, that's really going to tip the balances.
That's what I'm hearing you say.
Yeah, yeah, I really believe that's the case.
And I think, and this is something in the FAST AI course, a Jeremy Howard said that this
topic, people have kind of scratched the surface of it.
And I think there's much more to be done there.
And obviously my approach, I'm trying to sort of a beaten potatoes, trying to solve problems
approach.
But I hope that there were some research as well, people looking at it from a search
point of view, you're saying what can be done.
And the other thing that I think structured data has, there's sort of by definition, structured
data has very rich metadata.
So you have a database where everything, all of the tables in the database are described
in tables.
And it's very, it's all tooled and instrumented to be able to see what everything that's there
in the database.
And I think there's some potential for work that's kind of exploratory.
I could see something like a web crawling to go through a database and see, are there
potential for a useful model here?
And you can see a situation where as the cost of compute drops, have something that's
running in the background is sort of trying all sorts of different combinations of features
in a large database sale.
And maybe some interesting results there.
And be able to do that.
And that's something the structured data gives you that you don't get from unstructured,
but you don't necessarily get from a structured data where there isn't that sort of rich metadata
describing what's there.
And we'll come back to the structure in just a moment.
But one of the, you mentioned one of the points that Jeremy makes in the fast.ai course.
And that is that in many of these examples, particularly the Kaggle ones, like I think
it was Rothman stores, competition, you know, when you look at the leaderboard, I forget
the details I'm going to watch them.
I think, you know, out of the top, you know, the top N, a good number of them, if not
the top spots were folks that, you know, applied deep learning based approaches.
And the key takeaway is, and this is, you know, them competing against data scientists
with, you know, oodles of experience in this particular retail domain.
And, you know, in many cases, folks with no domain experience, plus deep learning were
able to outperform folks that, you know, brought domain experience and, you know, handcrafted
models to bear.
Yeah.
Yeah.
That's, it's, it's a good observation use and, you know, Kaggle in a way can be, it's
some artificial aspects of it, but it's sort of, it's raw capitalism and what works best
rises to the top.
So yeah, I think there's, there's potential there.
And some of the big players are doing this and there are applications that I've heard
of Google and Amazon are doing with structured data, but it just doesn't seem to be something
that's really front and center in terms of what people, as both as they're learning
about deep learning and where the research is, it seems to be much more on the, the unstructured
that is the non, non tabular data side.
One of the other objections I've heard is, and there's this heuristic thrown out there
is that about 80% of the data in the world is unstructured.
So, you know, if that's the case, it's only like one fifth of data is structured, then,
you know, how interesting is it to apply that?
Still a lot of data.
It's a lot of data.
And I know from experience from working with DB2, I know that the, you know, the world
runs on it.
Every bank, every insurance company, every, every government depends on, on structured data,
you know, relational database may not be sexy, but it's, it underpins our modern lives.
So there's a lot of important data that's, that's structured as well, even if the volume
isn't as much as my structured data.
You were making the point that it's a, the application of deep learning to structured
data in particular is a topic that doesn't get a lot of play out there.
And it made me think of embeddings, which is one of the key techniques that are used
in applying deep learning to structured data.
When I first heard about embeddings through the Fast.ai course, I don't know, a couple
of years ago now, you didn't hear a lot about it.
It was in a couple of obscure papers.
And now everybody's doing it and talking about it.
And it's almost, you know, a pass A, right?
It's just a tool that we, we, we pull out of the toolbox to solve a great deal, a
great number of problems.
It was just an article I saw yesterday about StitchFix, the, the kind of clothing, styling,
in a box subscription company, created a model.
I was talking about some of the different models that they use internally.
And a lot of them are based on style embeddings and things like that.
And so kind of in that vein, I suspect that we'll start to hear a lot more about this.
And the Fast.ai course is just kind of ahead of its time once again.
But why don't we take a second to drill into some of the, you know, how it works and kind
of the details that you'll be talking about in the book.
It's a manning book.
So I'm assuming it's going to be kind of rolled up the sleeves and kind of dig into how
to make all this stuff work.
And I would assume that embeddings is a big, you know, one of the chapters in that book
somewhere, is that, is that the case?
Absolutely.
Absolutely.
So dealing with, I guess one of this, this may sound a bit trite, but when they're first
sort of at one hot encoding, I thought, I hate this idea.
And I know it's necessary.
I know it's something that, you know, we have to use, but I thought, wow, you know, this
is, if you've got a category with more than a dozen values in it, this just gets crazy.
Yeah.
It's huge, huge, you know, pandas, data frames, for example.
And embeddings give you a way you can, you can assign integer encodings to the values
in a categorical feature and then use embeddings to learn their relationships.
So you get away from having to use a one hot encoding and the inefficiencies that are
involved there.
So yeah.
So just in terms of what you said about embeddings completely agree that so it's a very
hot topic.
I think the idea that something that came out of NLP and has more broad applications
is really interesting.
And the idea that you kind of, you kind of get unsupervised learning, not for free, but
as some of some of the exhaust foam from doing supervised learning, solving a supervised
learning problem.
And you can use the embeddings to come up with categorizations, like, you know, movie
categorizations for Netflix, that kind of thing.
That's really interesting.
You get a bit of a two for one value.
One of the things that comes up frequently in our fast AI study groups as a point of confusion
when we're talking about the structured data topic is kind of the relationship between
embeddings from an embedding space perspective and the way it's used in NLP.
And the embeddings that we use in the structured data problems, do you have a, like, is that
real clear to you?
Do you have a great way of explaining that?
Well, I think I guess with a, with NLP, you can have, you know, tens of thousands, millions
of values and just the number of individual words that are each with their own individual
embeddings.
And then in a structured data problem where you're leaving aside textual features, just talking
about categorical features, you're going to have a smaller number of individual values.
So the embedding space may have a smaller dimension.
You may have a, if I could say a simpler embedding space than you would dealing with NLP.
But the fundamental ideas is the same.
And I guess to get back to your question before about how this sort of how things work.
So as I mentioned, the book has the Toronto Streetcar data set as the problem is being solved.
So go through the process of cleaning that up.
So getting getting rid of bad values.
One of the things that I've learned of the course of doing this was some geo coding.
So there are address values that describe where the problem occurred or where the street
car breakdown, where was there a delay.
And those address values are just completely messy.
They're totally free form, you know, say young, young and queen, their date, their misspelled,
their references to sort of known areas in Toronto.
So it's great.
It's a very interesting problem to have.
And it took advantage of Google's geo coding API to turn those values into latitude and
longitude values.
So that's probably that's the feature that required the most, the most effort to get something
coming out the other end.
And then for the categorical values, so that would be, for example, the street car route,
the vehicle number, the day of the week, some Monday, Tuesday, Wednesday, whatever.
Those get get translated to integer IDs.
And then so all the all the categorical values get get translated that way.
Text fields, individual words get translated into IDs.
And then those get prepared to be fed into a fairly simple carous model.
But the thing that is a bit of an innovation is that the model is the layers in the model
are automatically defined based on the columns that are in the table, the input table.
So if they're and all of the categorical columns get a set of layers in the model to get
an embedding layer and a number of other layers.
If there are any text columns in the input data set, they get, they get embeddings.
They also get an RNN layer and then continuous values like temperature just kind of flow
through.
So that model gets built up layer by layer automatically based on the columns that are
in the input, the input data set.
And that's in a nutshell, how the model gets put together.
So this automated generation of the model is this based on a tool that you've built?
This is just Python code.
So there are, there's part of the code is identifying which columns of the overall
data set, which columns are going to be used to train the model.
And then saying which columns are breaking them into three categories, a categorical.
So those would be things like the day of the week, a continuous, like temperature or
time duration and text.
So a description of a of a problem, for example.
This sounds like you've built this tool that you point at a data set and it will almost
like AutoML, I create a model that at least as a starting place for making some predictions
on the structure data as opposed to, you know, the book walking you through, like how
you would perform this analysis by hand.
That's right.
So the book provides, the idea of the book is to generalize a little bit.
So the, the streetcar problem is used as an example, but say more generally here's how
you deal with it.
But the intention of the code is that it could be applied to other structured data sets.
That it wouldn't be limited.
You could, that somebody could fairly quickly take a different structured data set and apply
the code and get, try it out to see how it would work with a different, a different structured
data set with different columns and different, a different mix of categorical texts and
continuous columns.
Okay.
Cool.
And how, how are the chapters structured?
Are they different features of this, you know, you building up this Python code or do you
assume it from the beginning and you're, you know, talking more theoretically about these
topics, how do you organize things?
Um, so Manning's pretty big on being practical.
So they've, they've certainly encouraged me to stay close to the code.
And this is one of the things, and I know that you've, and you've sponsored a number
of sessions going to the fast AI course.
So you've seen that ethic there of really trying stuff that, you know, you get as soon
as you can, I try to apply it in code.
And I've tried to do that in the book as well.
So introduce bits and pieces.
So talk about pandas data frames is one of the essential items, so it's been some time
on that.
Talk about the data set, talk about the different steps take, you need to take to clean up the
data set, including the geocoding problem that I talked about before.
And then a chapter on the automatic, how the, the layers of the carous model are automatically
put together.
And that's, that's one of the ones that's not that that I'm working on currently will
be released a little bit later in the year.
There'll be a, a section, as you said, talking a bit more to tail on embeddings where, what
role they play.
And then the other thing I really want to do, and this is, this has been quite a challenge
is have a, is end to end a process as possible.
So be able to talk about, here's how you deploy the model.
And here's how you get a little, a little website that would let you pop in the description
of a particular trip and get the prediction back whether or not that trip would incur
a delay.
Because that's one of the things that I found in terms of the my learning process, there
was an awful lot that kind of took you up to the point where you had, you had trained
the model and had some sense of its performance.
And then things got a little bit sketchy about how would you actually deploy it?
How would you get it into, into not even production, just get it to the point where somebody
else could use it or play with it.
So I want to, I definitely want to spend some time talking about some options for doing
that and taking the reader through a particular process for deploying the model.
Sounds awesome.
And so the, you've kind of suggested this, but the early access process or program at Manning
is one in which you are kind of incrementally posting chapters of the book.
And folks can kind of buy in early and get access to these chapters, is that right?
So they get access to the book and they get the completed book.
The other part of it is supposed to be a two way thing as well.
There's the opportunity to make comments and to frame the book either to say things
that are that need to be corrected or adjusted or make recommendations for topics that need
to be covered later on in the book.
So people who get involved early, who take advantage of this, have a chance to really
not just be passive participants, but also contribute and be part of making this book
or other books that are in this program successful and meet the needs of the people who will be
reading them.
That's awesome.
Yeah, one of the things that really jumps out at me in this conversation, we keep kind
of coming back to a fast AI, I think, because I'd seen it impact a lot of people, myself
included, and I'm seeing in this conversation how you, the echoes of that course are clear
and what you're doing here and kind of extending what you got out of that course now into
a book.
And that's exactly how the course is kind of designed that you, there's a lot of self-direction
required to go through the course and really get the most out of it.
And a lot of that is taking what's taught and applying it to things.
And so that is awesome to see.
It is a great course.
It's a fantastic opportunity, a really thing I learned so much going through it and also
seeing, it comes up in all sorts of different contexts as well, I mean, in the sessions
that you've sponsored, other people learning about deep learning, and I think it's not
just the course, it's also the approach to teaching, that's saying, get to the coding
as quickly as possible and try different stuff.
And it's pretty bold as well, and making a number of significant changes in the platform
being used, that's a lot of work and there's risk in that, but it got to tip the hat to
the team that puts that course together.
I think they've done a really, really fantastic job and had a really big impact on the industry
and on people's lives.
People have learned a lot, it's been a ladder up for a lot of people into a world that's
really exciting and really interesting and really important.
Yeah, absolutely, absolutely, and I would encourage anyone who, you know, hears us gushing
over this course and is interested in taking the course to do so and Jeremy Howard, who
teaches the course always says that taking the course is best done with other people,
the people that do that are more successful, and that's really why we, with the support
of a bunch of very dedicated volunteers have been doing, I don't even count now how many
kind of cohorts of folks we've done through the various versions of the course.
There are several versions and then a part one and a part two, but hundreds of people
now have participated in our study groups to just get support in working through these
courses.
If, you know, that's interesting to you, encourage anyone who's listening to visit
Twomlai.com slash Meetup, sign up for the Meetup and express interest in the study
groups.
And if we don't currently have one running, when you do that, you know, raise your hand
in the Slack channel in our Slack and express some interest and, you know, we tend to kick
these things off, you know, when folks are interested in them.
So with that, Mark, thanks so much for taking the time to share with us about the book.
It sounds super interesting.
Thank you, Sam and Sam.
I want to also thank you for the podcast.
I've learned so much, been a faithful listener for several years now, and it's been a great
asset.
I learned a great deal.
So thank you very much for all the work you do to get the podcast out.
Really appreciate it.
Wonderful to hear that.
Thanks so much, Mark.
Thank you.
All right, everyone, that's our show for today.
For more information about today's show, visit twomlai.com slash shows.
Remember, less than two weeks to register for Twomlai platforms.
So head over to twomlai.com now.
Thanks so much for listening, and catch you next time.
