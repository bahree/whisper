1
00:00:00,000 --> 00:00:10,400
All right, everyone. Welcome to another episode of the Twimble AI podcast. I'm your host,

2
00:00:10,400 --> 00:00:16,640
Sam Charrington. And today I'm joined by Doina Precup. Doina is a research team lead at Deep

3
00:00:16,640 --> 00:00:23,040
Mind Montreal and a professor at McGill University. Before we get going, be sure to take a moment to

4
00:00:23,040 --> 00:00:28,640
leave us a five-star rating and review wherever you're listening to today's podcast. Doina,

5
00:00:28,640 --> 00:00:33,120
this conversation has been a while in the making and I'm super excited to have you on the show

6
00:00:33,120 --> 00:00:37,760
and looking forward to digging into your research into reinforcement learning. Welcome.

7
00:00:38,720 --> 00:00:44,720
Thank you very much. I'm really excited to be on your show and yeah, big fan of your podcast.

8
00:00:44,720 --> 00:00:50,400
So looking forward to our conversation. Awesome. Awesome. Same here. Let's get started. As you know,

9
00:00:50,400 --> 00:00:54,640
we always do by having you share a little bit about your background and introduce yourself to

10
00:00:54,640 --> 00:01:01,120
our audience. Yeah, so I split my time between Deep Mind in the Montreal team as well as McGill

11
00:01:01,120 --> 00:01:07,040
University where I've been a professor since 2000. Before that, I did my PhD at University of

12
00:01:07,040 --> 00:01:14,000
Massachusetts Amherst. Working on reinforcement learning. I was actually lucky enough to take the

13
00:01:14,000 --> 00:01:18,960
first version of the reinforcement learning course taught by Andy Barton and Rich Satin out of

14
00:01:18,960 --> 00:01:27,040
their textbook in 1995 and they got me hooked to the field and I've been working on it ever since.

15
00:01:27,040 --> 00:01:34,800
Fantastic. And what prompted your interest in RL? I really found it a good way to think about

16
00:01:34,800 --> 00:01:39,920
artificial general intelligence. So agents that can learn how to do many different things in an

17
00:01:39,920 --> 00:01:46,880
open-ended environment. And it's really because we have on one hand rewards that give us a way to

18
00:01:46,880 --> 00:01:51,600
express the task that the agent should do. And on the other hand, we really have learning from

19
00:01:51,600 --> 00:01:57,600
interaction rather than let's say learning from being told what to do or not really having a clear

20
00:01:57,600 --> 00:02:03,440
goal. And so it seemed to me to be the right sort of balance between having some interesting

21
00:02:03,440 --> 00:02:09,760
signal in the reward but also having this ability to interact and explore and really learn freely

22
00:02:09,760 --> 00:02:19,920
in the environment. RL is increasingly a broad field. So let's dig a little bit into your research

23
00:02:19,920 --> 00:02:24,640
and have you share a bit about what you're most excited about. So one of the things that I've

24
00:02:24,640 --> 00:02:29,440
worked on for a long time and I'm always excited to think about it is hierarchical reinforcement

25
00:02:29,440 --> 00:02:36,160
learning. This is really about learning abstract representations, especially abstraction over time

26
00:02:36,160 --> 00:02:43,360
because oftentimes in reinforcement learning the problem is phrased at a very small time scale,

27
00:02:43,360 --> 00:02:49,440
very fine grain time scale sort of on the order of let's say muscle twitches. But really to

28
00:02:50,080 --> 00:02:54,800
self-complicated problems you need to think in terms of longer time scales and variable time scales.

29
00:02:55,360 --> 00:02:59,520
So for example, if you're cooking a meal you're not thinking about all the muscle twitches involved

30
00:02:59,520 --> 00:03:07,120
in stirring or putting a pot on the stove you're thinking in terms of larger steps. What ingredients do

31
00:03:07,120 --> 00:03:13,840
I need? Do I need to go to the store to get these ingredients? And so we reason at many levels

32
00:03:13,840 --> 00:03:19,520
of abstraction both in terms of the time scale of our actions as well as in terms of the states and

33
00:03:19,520 --> 00:03:24,800
features that we use. And I would really like reinforcement learning agents to be able to do

34
00:03:24,800 --> 00:03:30,800
the same to really leverage abstraction and learn these abstractions from their interaction stream.

35
00:03:30,800 --> 00:03:37,200
So I've been doing a lot of work in this respect in terms of for example learning a framework

36
00:03:37,200 --> 00:03:43,760
for temporal abstractions called options, learning models that look at different time scales,

37
00:03:44,640 --> 00:03:51,360
understanding how to use these models in planning algorithms and also understanding when agents

38
00:03:51,360 --> 00:03:58,960
can use what actions, what we call affordances. And so this is a long history and a long series

39
00:03:58,960 --> 00:04:04,800
of papers that is associated with it. But one of the important open questions that I still struggle

40
00:04:04,800 --> 00:04:13,600
with and think about is how do agents decide which abstractions to learn about? For us maybe it's

41
00:04:13,600 --> 00:04:18,800
easy. We think about objects for example and that somehow comes naturally but for reinforcement

42
00:04:18,800 --> 00:04:23,600
learning agents this is still something that I would like them to acquire automatically.

43
00:04:23,600 --> 00:04:30,640
Awesome and we'll return back to hierarchical RL. You also spend a lot of time working on

44
00:04:30,640 --> 00:04:35,120
reward specification for RL agents. Can you tell us a little bit about that work?

45
00:04:35,120 --> 00:04:40,720
Yeah so rewards are actually really important because they determine what the agent is really

46
00:04:40,720 --> 00:04:49,840
learning about and thinking about. And actually David Silver, Rit Sutton, Zitinder Singh and I put

47
00:04:49,840 --> 00:04:57,360
out the paper in 2020 which received quite a bit of attention both positive and negative I think

48
00:04:57,360 --> 00:05:05,360
called reward is enough. It really talks about this hypothesis that a reward signal, in fact a

49
00:05:05,360 --> 00:05:12,880
simple reward signal in a complex environment could really lead an agent to develop all the

50
00:05:12,880 --> 00:05:19,120
interesting attributes of intelligence that we might sort of intuitively think about. So

51
00:05:19,120 --> 00:05:25,040
for example in this paper we discussed squirrels. Squirrels have a perhaps a simple reward

52
00:05:25,040 --> 00:05:31,760
function. They like to eat nuts because that helps them to survive. But in that process they

53
00:05:31,760 --> 00:05:37,600
actually develop a lot of interesting abilities like the ability to remember where they put nuts

54
00:05:37,600 --> 00:05:42,480
before and the ability to predict the time of year when these are going to be available and

55
00:05:43,200 --> 00:05:48,480
the ability to deal with other squirrels and maybe you know deceive them or make sure that they

56
00:05:48,480 --> 00:05:57,120
don't get to the hidden stash of their own nuts. And so that's an interesting set of abilities

57
00:05:57,120 --> 00:06:05,200
that involves memory and planning and modeling aspects of the world that can all be seen as

58
00:06:05,200 --> 00:06:12,480
developed from this maximization of reward. And so we were asking ourselves in this hypothesis whether

59
00:06:12,480 --> 00:06:18,720
it could also be true that general age day agents would develop these kinds of abilities also

60
00:06:18,720 --> 00:06:24,560
from maximizing rewards. It's a hypothesis and so you know as any hypothesis it's interesting to

61
00:06:24,560 --> 00:06:31,520
think about how might we might you know confirm or dismiss it. And so following that we did some

62
00:06:31,520 --> 00:06:39,200
technical work trying to understand how we can take sort of intuitive task specifications and

63
00:06:39,200 --> 00:06:46,960
translate them into rewards for reinforcement learning agents. And when is this actually possible?

64
00:06:46,960 --> 00:06:54,960
So this was a paper called on the expressivity of Markov reward that was led by David Able and with

65
00:06:54,960 --> 00:07:02,080
several fantastic collaborators at DeepMind and also at Brown University. And it received the best

66
00:07:02,080 --> 00:07:10,720
paper award at Europe's last year so that was also very wise actually when we when we wrote the

67
00:07:10,720 --> 00:07:16,400
paper we weren't sure what people would make of it. But we were quite excited to see the positive

68
00:07:16,400 --> 00:07:25,360
reception. And really in the paper we aimed to make this hypothesis a bit more concrete and to

69
00:07:25,360 --> 00:07:31,200
try and understand from a mathematical point of view when do Markovian rewards which are a special

70
00:07:31,200 --> 00:07:38,960
class of reward functions capture intuitive notion of tasks. Got it. Let's maybe start with the general

71
00:07:38,960 --> 00:07:47,760
case of rewards. The hypothesis is that you know using this mechanism of reward can lead to all sorts

72
00:07:47,760 --> 00:07:57,840
of intelligence capabilities behaviors. How do you characterize that in a machine learning context?

73
00:07:57,840 --> 00:08:01,200
You know what how do you draw the analogy from the squirrel to the machine learning?

74
00:08:01,200 --> 00:08:11,200
So in the paper we thought of essentially a framework where you might imagine somebody who has a

75
00:08:11,200 --> 00:08:19,120
task in mind let's call this Alice and then you have a learner called the learner Bob and so Alice

76
00:08:19,120 --> 00:08:26,480
has has a task specification in mind and now has to translate this into a reward signal that Bob

77
00:08:26,480 --> 00:08:32,400
would get. So Bob is a usual reinforcement learning agent it inhabits a Markov decision process

78
00:08:32,400 --> 00:08:38,400
this means that on every time that Bob observes the state of the environment can do some action

79
00:08:38,400 --> 00:08:44,000
and then we'll receive as a response an immediate numerical reward signal and there will be a transition

80
00:08:44,000 --> 00:08:51,280
to an X state and there's some discount factor that the values rewards that are received too far

81
00:08:51,280 --> 00:08:57,360
in the future and Alice might have something more intuitive in mind so for example Alice might

82
00:08:57,360 --> 00:09:06,240
have a preference over certain ways of behaving so if Bob is an agent that's in a grid world

83
00:09:06,240 --> 00:09:13,760
navigation task for example Alice might prefer that Bob gets to a designated goal location quickly

84
00:09:13,760 --> 00:09:20,240
but she might also prefer that Bob doesn't step into lava and get burned and so this

85
00:09:20,240 --> 00:09:28,560
imposes now a preference over the set of policies that Bob might have the policy being the mapping

86
00:09:28,560 --> 00:09:38,000
from state to actions and now we can think of no the task specification in Alice's head as being

87
00:09:38,000 --> 00:09:44,240
this preference over the space of policies the reward function is the usual Markovian reward

88
00:09:44,240 --> 00:09:51,680
which is associated with states and actions and we want to understand can Alice efficiently compute

89
00:09:52,320 --> 00:09:58,480
a reward that captures her preferences and so now we can think a little bit about you know

90
00:09:58,480 --> 00:10:04,160
what kind of preferences she might have and how stringent they might be so one kind of preference

91
00:10:04,160 --> 00:10:10,000
which I described is over policies and we can have that be very strict or we can relax it a

92
00:10:10,000 --> 00:10:15,120
little bit we can say you know Alice certainly thinks that some actions are bad like stepping into

93
00:10:15,120 --> 00:10:22,960
lava is always bad but otherwise has no strong preferences in in other situations so whether

94
00:10:22,960 --> 00:10:28,800
you know Bob takes one route or some other route doesn't really matter so long as he doesn't get

95
00:10:28,800 --> 00:10:35,360
burned so that that leads us to ranges of policies or sort of sets of policies being acceptable or

96
00:10:35,360 --> 00:10:43,520
being superior to others and we can also think in terms of trajectories so Alice might consider

97
00:10:43,520 --> 00:10:48,800
trajectories that Bob might do in the environment and just prefer one trajectory over another right

98
00:10:48,800 --> 00:10:55,040
so if Bob is not trying to run into walls too much maybe that's that's a good thing

99
00:10:57,120 --> 00:11:02,720
so now if we have a set of preferences like this the question is can we actually always translate

100
00:11:02,720 --> 00:11:09,600
them into a reward function so can Alice compute a reward function that is Markovian

101
00:11:10,800 --> 00:11:20,960
and if so what's the complexity of of doing this so the the paper has two aspects to it right

102
00:11:20,960 --> 00:11:26,800
there's a negative result and there's a positive result you know the the negative result is in

103
00:11:26,800 --> 00:11:34,720
some in hindsight not so unexpected but it basically says you can't always find a Markovian reward

104
00:11:34,720 --> 00:11:43,120
Markovian in the state of the agent and intuitively in hindsight it's it's not so surprising because

105
00:11:44,240 --> 00:11:51,440
on one hand there may be states that are unreachable where Alice might have certain preferences

106
00:11:51,440 --> 00:11:58,640
but they're irrelevant and so if you have some disconnected state in some corner and Alice has

107
00:11:58,640 --> 00:12:04,800
some contradictory preferences there let's say in terms of the the actions that can impact

108
00:12:04,800 --> 00:12:09,440
the specification of the reward overall but it's a case that we don't really care about so that's

109
00:12:09,440 --> 00:12:15,360
sort of one mode of failure which is perhaps not interesting would it be an example like if Alice

110
00:12:15,360 --> 00:12:21,440
you know if the maze is you know nominally 20 steps to completion and Alice has a preference

111
00:12:21,440 --> 00:12:26,240
that you get there in three steps like it just can't be done that's right so this kind of thing is

112
00:12:26,880 --> 00:12:33,120
you know it's impossible and and therefore you know the math says yes yes it's impossible

113
00:12:33,120 --> 00:12:38,960
there's a more interesting case which is the case of of non-Markovianness so for example

114
00:12:38,960 --> 00:12:47,360
Alice might say I prefer that Bob always go in the same direction so if you're going up or you

115
00:12:47,360 --> 00:12:52,960
should always go up if you're going down you should always go down Markov decisions are ID so you

116
00:12:52,960 --> 00:12:58,480
can't really enforce that so you can't really enforce that if you wanted to enforce that one

117
00:12:58,480 --> 00:13:04,640
possibility would be that you modify the state space right so that we keep track of you know

118
00:13:04,640 --> 00:13:10,320
whether you've been going up and then you know we could we could do a Markovian reward in that but

119
00:13:10,320 --> 00:13:15,520
in this paper we only consider it specifying with respect to the state space that's already there

120
00:13:15,520 --> 00:13:23,200
we didn't consider this larger context of you know trying to modify the states and specify the reward

121
00:13:23,200 --> 00:13:29,680
I think it would be a very interesting avenue for future work to think about how do we go from

122
00:13:29,680 --> 00:13:37,200
observations to a specification that has you know perhaps a new state space and the reward

123
00:13:37,200 --> 00:13:42,080
function that that goes with it but that went beyond the scope of the results that we had in this

124
00:13:42,080 --> 00:13:47,920
paper however one thing that we did show in the paper which is the positive result is that

125
00:13:49,520 --> 00:13:57,920
there is a procedure which is a polynomial time procedure that Alice can use to either return a

126
00:13:57,920 --> 00:14:05,280
reward function that is consistent with her preferences if such a reward function exists

127
00:14:06,240 --> 00:14:14,800
or to determine that no such reward function is possible and the intuition actually of the

128
00:14:14,800 --> 00:14:20,560
of the algorithm is interesting it's based on a linear programming approach

129
00:14:20,560 --> 00:14:31,520
and so the the basic idea is that for any policy we can compute a stationary distribution associated

130
00:14:31,520 --> 00:14:38,080
with that policy stationary distribution over the states of the controlled Markov process

131
00:14:38,960 --> 00:14:45,920
and then we can impose constraints between the policies that our analysis acceptable set

132
00:14:45,920 --> 00:14:52,400
and the fringe policies the fringe policies are identical to those that are in the acceptable set

133
00:14:52,400 --> 00:14:59,520
except at one state they would take a different action and so we can have a set of inequality

134
00:14:59,520 --> 00:15:06,080
constraints the basically say the acceptable policies should be better in value than these

135
00:15:06,080 --> 00:15:14,080
fringe policies and with this kind of linear program either we find a solution and the size of

136
00:15:14,080 --> 00:15:19,920
the linear program is polynomial in the size of the states and actions so we can find a solution

137
00:15:19,920 --> 00:15:26,800
in polynomial time or if the linear program does not have a solution that means that no Markovian

138
00:15:26,800 --> 00:15:31,920
reward function is actually consistent with the original preferences that were expressed.

139
00:15:31,920 --> 00:15:38,400
Is there a notion of like sometimes a linear program is underspecified you've got more degrees

140
00:15:38,400 --> 00:15:46,400
of freedom than you have constraints does that come up in this analysis that's a really

141
00:15:46,400 --> 00:15:58,320
interesting question no in our case we want to find a solution if any solution is is okay

142
00:15:58,960 --> 00:16:05,120
we don't need to find a specific solution and so there will be for example potentially multiple

143
00:16:05,120 --> 00:16:12,800
reward functions that are all consistent with some optimal policy you know for example

144
00:16:12,800 --> 00:16:19,520
scaling of each other and that's that's perfectly fine so long as all of them end up

145
00:16:20,560 --> 00:16:26,400
ensuring that the preferences are satisfied so the good policies are better than than the

146
00:16:26,400 --> 00:16:32,640
others right or that the bad policies are inferior to the others then you know all of these

147
00:16:32,640 --> 00:16:39,280
solutions are are fine and acceptable for more point of view. There's an implicit assumption

148
00:16:39,280 --> 00:16:46,080
that the policy is specifiable kind of mathematically or not the policy sorry the preference

149
00:16:47,920 --> 00:16:54,800
which maybe limits the yeah maybe this is referring back to what we're talking about in terms

150
00:16:54,800 --> 00:17:02,560
of the Markovianness of it but it also kind of speaks to the gap between a preference that's

151
00:17:02,560 --> 00:17:09,200
someone might have in a real-world scenario and then trying to implement it using this method and

152
00:17:09,200 --> 00:17:14,640
how do you get to a mathematical representation of that preference like did you address that at all

153
00:17:14,640 --> 00:17:22,480
on the paper? Yeah so that that's an interesting question and we don't really talk about this in

154
00:17:22,480 --> 00:17:31,520
the paper but I can give you my my perspective on this okay on one hand what you do in the real world

155
00:17:31,520 --> 00:17:39,920
is you have some preferences but you may not have full preferences right so you might for

156
00:17:39,920 --> 00:17:46,880
example in in our grid world say well Bob should never step in lava but you know if Bob has to

157
00:17:46,880 --> 00:17:54,480
go through red squares or green squares I don't really care okay but as you observe behavior

158
00:17:55,280 --> 00:18:00,560
those preferences might actually change right you might start preferring you know the Bob maybe

159
00:18:00,560 --> 00:18:05,920
always steps on green squares because those are soft and you know Bob is like a little toddler

160
00:18:05,920 --> 00:18:14,160
and shouldn't hurt himself right so there these kinds of things may come up over time now our

161
00:18:14,160 --> 00:18:19,280
framework is very much single shot so Alice has a set of preferences they're there from the

162
00:18:19,280 --> 00:18:26,160
beginning she computes the reward and then Bob goes off and optimizes that reward function

163
00:18:26,160 --> 00:18:32,480
I think in practice there's much more of a give and take right there is much more of you know

164
00:18:32,480 --> 00:18:41,120
maybe Alice observes the behavior that might get her to have new preferences right or maybe

165
00:18:41,120 --> 00:18:48,160
revise existing preferences and so I think in in applications one would have to have an interaction

166
00:18:48,160 --> 00:18:56,480
loop where you know Alice observes gives new reward functions Bob continues to optimize over time

167
00:18:57,440 --> 00:19:05,440
and this is you know this continues for a while in order to really get to to the root of what the

168
00:19:05,440 --> 00:19:11,600
task is and you know one thing that I remember for example is when my kids were young you know we

169
00:19:11,600 --> 00:19:17,280
used to reward them for putting away their toys and at some point one of them discovered this

170
00:19:17,280 --> 00:19:21,280
interesting strategy of always taking all the toys out of the torches and putting them back

171
00:19:21,280 --> 00:19:25,920
and taking them out and putting them back you know it's the it's the behavior that gets rewarded

172
00:19:25,920 --> 00:19:32,960
right so it's the classic boat spinning in the middle of the that game yeah so basically now

173
00:19:32,960 --> 00:19:38,160
this is the stage at which one would actually revise what the reward function is by

174
00:19:38,160 --> 00:19:43,360
alerting the quirkiness and the behavior that that is being induced and I think naturally we do

175
00:19:43,360 --> 00:19:50,080
that and our framework in this paper does not does not address that um there's one other aspect

176
00:19:50,720 --> 00:19:59,200
which is also not addressed which is with how good is this reward function in order to get

177
00:19:59,840 --> 00:20:04,640
Bob to learn efficiently so this is not something that we talked about in the paper at all

178
00:20:05,360 --> 00:20:11,760
um we interestingly observed that in the examples that that we showcased in the paper

179
00:20:11,760 --> 00:20:18,800
the inferred rewards did lead to fast learning because you know the reward function is the result

180
00:20:18,800 --> 00:20:24,880
of computing an LP and so it's a dense reward function it's not sparse like what people would

181
00:20:24,880 --> 00:20:31,600
specify in these kinds of tasks and so it does lead as a side effect of that to pretty fast learning

182
00:20:31,600 --> 00:20:37,440
but it's sort of uh it's not clear that that would always happen so the linear program serves

183
00:20:37,440 --> 00:20:45,360
to constrain the search space for the reward function which yields more efficiency yeah exactly

184
00:20:45,360 --> 00:20:50,960
yeah I think my question maybe I'll comment my question a little bit differently the example

185
00:20:50,960 --> 00:20:58,160
you use in the paper is grid world yeah maybe it's the case that any preference about actions

186
00:20:58,160 --> 00:21:02,560
in grid world you know there's a straightforward mathematical representation of those

187
00:21:02,560 --> 00:21:08,960
uh in lots of other you know tasks maybe there's not a straightforward mathematical

188
00:21:08,960 --> 00:21:17,760
representation of the the preferences um you know but I'm I guess I'm supposing that there are

189
00:21:18,480 --> 00:21:25,280
marcovian uh tasks for which the preferences are not easily mathematically represented that

190
00:21:25,280 --> 00:21:35,920
may not be the case but um did you look at like what what tasks or environments this uh all of

191
00:21:35,920 --> 00:21:42,720
the supplies to that's a great question so the paper itself is more of a definitional paper we

192
00:21:42,720 --> 00:21:48,960
were trying to define the problem think about you know what it means to have a a task in mind

193
00:21:48,960 --> 00:21:58,240
and separate that from this issue of how do you then specify it as a reward um there's so it does

194
00:21:58,240 --> 00:22:06,800
assume that you have a marcovian process and the side effect of that is that we have states

195
00:22:07,760 --> 00:22:13,680
um now in the real world we have features right we don't have states but we have some observations

196
00:22:13,680 --> 00:22:20,560
we see images right those are a imperfect representation of what the state might be and that is

197
00:22:20,560 --> 00:22:25,040
not a setting that we have handled in the paper although I think it's really interesting for

198
00:22:25,040 --> 00:22:30,560
for future work so you know Alice might have preferences but might not have access to the state

199
00:22:30,560 --> 00:22:36,400
space she might only have access to observations and Bob also might not have access to the state

200
00:22:36,400 --> 00:22:42,480
space only to observations and this is where uh the framework that we have and the the specifics

201
00:22:42,480 --> 00:22:49,200
of the LP would not carry through I think there is a path to have a similar kind of solution

202
00:22:49,200 --> 00:22:56,240
in this case because generally speaking in reinforcement learning LP formulations have been

203
00:22:56,240 --> 00:23:02,640
used as a as a way to think about the problem of reinforcement learning and there are versions

204
00:23:02,640 --> 00:23:11,680
of these approximate linear LPs infinite LPs that have been proposed for other kinds of reinforcement

205
00:23:11,680 --> 00:23:16,400
learning problems so I you know my gut feeling is we could take some of that methodology maybe

206
00:23:16,400 --> 00:23:22,800
and apply it in the case where you don't have access to state um we do assume that Alice has

207
00:23:22,800 --> 00:23:29,600
preferences and so if she doesn't or if she doesn't know what they are our work doesn't

208
00:23:29,600 --> 00:23:35,760
doesn't help with that um but I think uh you know this limitation for example of not having

209
00:23:35,760 --> 00:23:42,480
necessarily an MDP having features uh these are are things that are more left for future work

210
00:23:42,480 --> 00:23:48,800
but could potentially be overcome it would also be really interesting to think about how preferences

211
00:23:48,800 --> 00:23:56,080
might be uh obtained in practice you know like what if you actually had an agent that is working

212
00:23:56,080 --> 00:24:01,760
in a look to the person the person is is stating some preferences we use this methodology to extract

213
00:24:01,760 --> 00:24:08,000
rewards and then you know maybe again the person observes what's happening and and goes back and

214
00:24:08,560 --> 00:24:13,680
you know is there a way for us to really do this in practical applications it's not something

215
00:24:13,680 --> 00:24:19,120
that we have tried but it would be fascinating to see and part of the problem is the one you

216
00:24:19,120 --> 00:24:27,280
articulated earlier is the evolution of preferences over time that you have to do that plus the

217
00:24:27,280 --> 00:24:34,560
uh trying to figure out how to how to close that loop in real time exactly okay going back to

218
00:24:34,560 --> 00:24:41,440
the work on hierarchical reinforcement learning is there a recent paper in that space that

219
00:24:43,040 --> 00:24:48,880
that comes to mind or or maybe we can talk a little bit about or start from like the

220
00:24:50,240 --> 00:24:54,400
kind of what's the research frontier there how far along are we in thinking about these kinds

221
00:24:54,400 --> 00:25:02,560
of hierarchical problems a lot of the work over time has gone into the question of discovery

222
00:25:03,360 --> 00:25:09,920
how does an agent discover abstractions we have a lot of good understanding about how to represent

223
00:25:09,920 --> 00:25:17,280
for example temporally standard actions um by having an initiation set where where such an action

224
00:25:17,280 --> 00:25:22,720
can start uh by having an internal policy and an determination condition so this is the

225
00:25:22,720 --> 00:25:27,760
a framework called options in hierarchical reinforcement learning that I worked on for a very

226
00:25:27,760 --> 00:25:35,280
long time and what would be an example of that imagine that you have a robot that's in an environment

227
00:25:35,280 --> 00:25:41,200
and uh it has a controller and the controller says you can go forward if there's nothing in front of

228
00:25:41,200 --> 00:25:47,280
you so if you're far enough from any obstacle that's your initiation set um it's almost the entire

229
00:25:47,280 --> 00:25:53,920
environment except for some spaces around your obstacles um the policy is just to go forward so

230
00:25:53,920 --> 00:25:58,880
that's already pre-specified and you stop whenever you're too close to an obstacle so that's the

231
00:25:58,880 --> 00:26:04,960
that's the termination condition and now these policies can be stochastic as well right so

232
00:26:04,960 --> 00:26:09,600
doesn't necessarily have to be that you're always going forward you might be going on any kind of path

233
00:26:10,160 --> 00:26:16,320
right until you get too close to an obstacle and such uh such controllers can be thought of not

234
00:26:16,320 --> 00:26:23,200
just in robotics but you know more intuitively for example in in planning right so if you're

235
00:26:23,760 --> 00:26:29,840
playing let's say a puzzle game like Rubik's cube you might have certain configurations that

236
00:26:29,840 --> 00:26:35,840
you want to achieve that impose sub goals and then you know the the ways to achieve those

237
00:26:35,840 --> 00:26:43,200
configurations can be thought of as as these kinds of options um so we understand how to formulate

238
00:26:43,200 --> 00:26:52,560
the problem the interesting question that is still quite open is how do we actually find these

239
00:26:52,560 --> 00:27:00,720
these sub goals um so you know for example in a in a Rubik's cube uh I don't know if you've ever

240
00:27:00,720 --> 00:27:06,640
done any of these or if you like them but you know when you learn how to do them people tell you

241
00:27:06,640 --> 00:27:13,840
oh there is you know you have to complete a phase right somebody's telling you that's that's your

242
00:27:13,840 --> 00:27:16,000
circle complete the phase that's the first thing you should do and then you know if you have this

243
00:27:16,000 --> 00:27:20,480
kind of configuration on the side here's a here's a little sequence that you should do right

244
00:27:21,760 --> 00:27:29,760
so it's told to you and yes once you know this it really greatly simplifies the problem because

245
00:27:29,760 --> 00:27:36,080
you don't search through all the space of possible moves you're now executing these sequences that

246
00:27:36,080 --> 00:27:44,800
you know are useful um but how do we discover this on our own right that's that's the key question

247
00:27:44,800 --> 00:27:52,560
and it's one that we thought about quite a bit as a community uh it's still pretty open because

248
00:27:52,560 --> 00:27:59,360
it's hard um there are some interesting answers one interesting answer that one of my PhD students

249
00:27:59,360 --> 00:28:06,800
at Miguel Pierre-Luc Bakon did a few years back it's called option critic it's basically an algorithm

250
00:28:06,800 --> 00:28:15,200
that tries to uh use the reward from the environment and and tries to find sub goals that are on the

251
00:28:15,200 --> 00:28:22,560
path to rewarding states and it uses gradient based methods very similar to uh actor critic policy

252
00:28:22,560 --> 00:28:29,520
gradient methods um and so so that works quite well on in certain environments but sometimes we

253
00:28:29,520 --> 00:28:35,760
still observe things like you know the agent using abstractions for some bit and then kind of

254
00:28:35,760 --> 00:28:43,120
collapsing them away like getting rid of them and it again in hindsight maybe one should expect

255
00:28:43,120 --> 00:28:48,640
this because if you think about how people do things oftentimes you need these kinds of hints or

256
00:28:48,640 --> 00:28:56,480
sub goals at the beginning of solving a task but if you sort of if you're only solving one task

257
00:28:56,480 --> 00:29:01,280
and you get used to it you don't have to think about that anymore you'll automatize it

258
00:29:01,280 --> 00:29:06,080
away right so people who do the Rubik's Cube in a few seconds don't really think about the

259
00:29:06,080 --> 00:29:11,120
configurations anymore like you know it's all gotten into their muscle memory and so it's a

260
00:29:11,120 --> 00:29:16,000
similar situation that we observe with our agents if they only have to solve one task they might

261
00:29:16,000 --> 00:29:21,280
use options for a while to try and help them out but then they get rid of them and they obtain

262
00:29:21,280 --> 00:29:30,720
some flat policy that is as close to optimal as possible so one way to uh to think about this

263
00:29:30,720 --> 00:29:36,080
is the agents only have to do one thing and so they're overfitting to that in some sense

264
00:29:36,080 --> 00:29:41,120
if they had a very rich very large environment complex environment with many things to do

265
00:29:41,120 --> 00:29:47,920
they may need to keep these abstractions around so if you know if you're not always having to solve

266
00:29:48,480 --> 00:29:54,640
Rubik's Cube but you have to solve different kinds of puzzles right let's say number puzzles

267
00:29:55,360 --> 00:30:01,840
then you might have certain strategies that you learn and you do keep around about you know how to

268
00:30:01,840 --> 00:30:07,840
manipulate numbers for example in these puzzles or how to do a search that's the kind of richness

269
00:30:07,840 --> 00:30:13,760
that we would need for our agents to keep these abstractions around and so part of it I think

270
00:30:13,760 --> 00:30:19,920
is is thinking about the good the interesting environments that we should use to send and part of

271
00:30:19,920 --> 00:30:25,280
it is to think about the methods themselves and you know for example our gradients enough

272
00:30:25,920 --> 00:30:32,240
should we do something else like more of a generate and test approach right where we think about

273
00:30:32,240 --> 00:30:36,880
useful sub goals and then we test them out and we have some way of curating a collection of

274
00:30:36,880 --> 00:30:45,520
sub goals for the agent I think that's all in substance quite quite open as a as a research direction

275
00:30:45,520 --> 00:30:51,040
interesting one question that's coming up for me is trying to think about the relationship

276
00:30:51,040 --> 00:30:58,480
between hierarchical RL as you've described it in curriculum learning I guess one just kind of

277
00:30:58,480 --> 00:31:06,880
riffing on you know compare contrast like curriculum learning is sequential in nature whereas

278
00:31:06,880 --> 00:31:13,680
hierarchical RL it could be more tree-like in the sense that you've got this or maybe even like an

279
00:31:13,680 --> 00:31:22,000
ensemble like the agent has this portfolio of strategies that it can employ curriculum is

280
00:31:22,000 --> 00:31:31,600
maybe more training time and hierarchical is more inference time I don't know if that's

281
00:31:31,600 --> 00:31:36,320
is that correct there is interesting relationships between curriculum and hierarchical RL so

282
00:31:36,800 --> 00:31:43,520
on one hand you could actually use curriculum learning in order to build a hierarchy so if an agent

283
00:31:43,520 --> 00:31:49,440
for example goes through a curriculum that curriculum could be targeted towards the agent learning

284
00:31:49,440 --> 00:31:57,840
options right so maybe you yeah so and that would be very helpful because it would ensure that the

285
00:31:57,840 --> 00:32:04,000
agent has has a set of options and so when you go to the next stage of the curriculum it's as if

286
00:32:04,000 --> 00:32:10,320
you've changed your action space in some sense right you don't have you know if you've learned

287
00:32:10,320 --> 00:32:17,760
how to do multiplication you don't have to learn that again now you have it as as an option and

288
00:32:17,760 --> 00:32:25,440
you can just employ it whenever it seems to be useful and so curriculum learning can definitely be

289
00:32:25,440 --> 00:32:32,880
a path towards obtaining a hierarchical representation but in hierarchical RL you can do this also

290
00:32:33,680 --> 00:32:39,280
in different ways so for example in a lot of the tasks that people have tried they learn

291
00:32:40,000 --> 00:32:47,520
the options and they learn how to choose the options at the same time and to end so an agent

292
00:32:47,520 --> 00:32:56,480
that let's say plays an Atari game is learning options for this game and using them in order to

293
00:32:56,480 --> 00:33:03,280
generate behavior in order to explore in order to represent its value function now of course it's

294
00:33:03,280 --> 00:33:11,040
harder when you learn many things and to end at the same time and so I think you know that slows the

295
00:33:11,040 --> 00:33:17,360
agent down a little bit sometimes what does harder mean here I mean there's a whole you know as you

296
00:33:17,360 --> 00:33:22,480
well know field around multitask learning that suggests that it can be more computationally

297
00:33:22,480 --> 00:33:27,680
efficient to learn or produce better results to learn multiple things at once yeah so this is

298
00:33:28,160 --> 00:33:34,000
multitask learning the multitask setup and generally speaking non stationary setups I think

299
00:33:34,000 --> 00:33:41,840
is where hierarchical RL can be the most helpful because in such setups it's worth investing

300
00:33:42,560 --> 00:33:49,040
some amount of time slow learning at the beginning so that afterwards you can do many more things

301
00:33:49,840 --> 00:33:56,160
right so you know in a multitask setup you might for example learn efficient exploration strategies

302
00:33:56,800 --> 00:34:00,960
and those will help you because the tasks that come later you'll still have to solve them and now

303
00:34:00,960 --> 00:34:06,480
you know how to walk about your environment or you know you might need to learn to use tools

304
00:34:07,680 --> 00:34:13,040
you spend some time doing that but then if you know how to use the tools then you can

305
00:34:13,040 --> 00:34:18,320
it can go and be much more efficient in in in in later tasks so I think it also depends it depends

306
00:34:18,320 --> 00:34:24,160
on how wide the task distribution is and how long the agent is going to live in this environment

307
00:34:24,160 --> 00:34:30,400
because in some sense we would expect that the benefit of hierarchical RL manifests more

308
00:34:30,400 --> 00:34:35,440
when the environment is more complicated and when the agent is going to have to live longer

309
00:34:35,440 --> 00:34:41,840
in this kind of complicated environment and be able to have it. Is there an analogy between hierarchical

310
00:34:41,840 --> 00:34:50,160
RL and you know thinking about the different layers in a CNN where the low levels are learning kind of

311
00:34:50,160 --> 00:34:56,800
more abstract things shapes textures whatever the higher layers are learning more complex shapes

312
00:34:56,800 --> 00:35:06,560
yes how far would you want to take that so the CNN is hierarchical in feature space right it's

313
00:35:06,560 --> 00:35:15,040
looking at different resolutions in the in the feature space hierarchical RL is similar in spirit

314
00:35:15,040 --> 00:35:22,000
but at the level of actions so lower layers would look at very fine resolution in terms of the

315
00:35:22,000 --> 00:35:29,440
the action actions being taken very quickly and lasting only little bits of time higher layers

316
00:35:29,440 --> 00:35:37,120
may look at actions that take a very long time to complete it's not as sort of clean as a CNN in

317
00:35:37,120 --> 00:35:43,680
the sense that you know these timescales can be variable and we don't necessarily want to separate

318
00:35:43,680 --> 00:35:51,120
them out in in very determined layers right and here's looping and it's not as clean at all

319
00:35:51,120 --> 00:35:57,600
yeah exactly so you know if you're thinking about cooking dinner uh some things take a long time

320
00:35:57,600 --> 00:36:02,640
you know like maybe you have if you're making bread you have to need the bread for a long time

321
00:36:02,640 --> 00:36:06,880
and some things are very fast and sometimes you just have to react like if the cats on the table

322
00:36:06,880 --> 00:36:12,560
you just have to do something immediately right and so the separation is not as fixed as you

323
00:36:12,560 --> 00:36:17,840
would have in in the layers of a CNN but it's the same kind of principle at the time scale we want

324
00:36:17,840 --> 00:36:22,720
to have fine timescales and longer timescales of which we make decisions and at which we also make

325
00:36:22,720 --> 00:36:29,120
predictions about those decisions and is it useful at all to compare it to like an

326
00:36:29,120 --> 00:36:35,440
unsombling type of approach where you've got a portfolio of submodels that you can choose from

327
00:36:35,440 --> 00:36:40,960
in any given time yeah so you definitely have a portfolio of submodels and you can choose between

328
00:36:40,960 --> 00:36:49,040
them um it's not like unsombling in the sense that usually so you have your portfolio make a choice

329
00:36:49,040 --> 00:36:55,200
once you've made that choice let's say to use a particular kind of model you just go ahead and use

330
00:36:55,200 --> 00:37:01,120
it rather than looking at let's say the the entire collection I think there are interesting

331
00:37:01,920 --> 00:37:07,520
things to explore in terms of planning that would be even more like ensemble methods than the

332
00:37:07,520 --> 00:37:14,000
kinds of methods that we have now and so for example right now when we think about planning with

333
00:37:14,000 --> 00:37:21,200
temporarily extended models we think about big jumps over the time scale but a model still usually

334
00:37:21,200 --> 00:37:28,240
predicts pretty much everything that will happen at the end of an option so at the same time we might

335
00:37:28,240 --> 00:37:33,360
you know if you have two hands you might have you know one hand doing one option and the other hand

336
00:37:33,360 --> 00:37:37,840
doing some other option and you might want to make sort of separate predictions and then combine them

337
00:37:38,800 --> 00:37:44,160
so I think there's a lot of room actually to think about what kind of composition operators we

338
00:37:44,160 --> 00:37:50,000
need right now when we use temporarily extended models we use the same kinds of composition operators

339
00:37:50,000 --> 00:37:55,120
as a normal mark of decision processes so think about sequences and we think about stochastic

340
00:37:55,120 --> 00:38:00,480
choice but there may be other interesting things to do like you know thinking about concurrent

341
00:38:00,480 --> 00:38:08,320
execution for example and the outcomes of that got it got it another area that you are exploring

342
00:38:08,320 --> 00:38:14,000
is continual reinforcement learning can you talk a little bit about that yeah definitely so

343
00:38:14,000 --> 00:38:22,000
continual reinforcement learning is really the natural way to do learning right it's really

344
00:38:22,000 --> 00:38:28,960
thinking about an agent that is in an environment that is much much larger than the agent it's

345
00:38:28,960 --> 00:38:36,480
enormous and the agent has a lifetime and has to continue to learn and adapt throughout its

346
00:38:36,480 --> 00:38:42,960
lifetime so in reinforcement learning often when we teach for example reinforcement learning classes

347
00:38:42,960 --> 00:38:48,720
we talk about a mark of decision process that has a finite state space it has a finite action space

348
00:38:50,080 --> 00:38:56,400
and the agent is aiming to go back to certain states understand which of these states are better

349
00:38:56,400 --> 00:39:02,320
and so on but in a continual learning setting there may not be a mark of decision process and if

350
00:39:02,320 --> 00:39:08,960
there is maybe the agent's lifetime is much shorter than the entire environment right so the agent

351
00:39:08,960 --> 00:39:15,600
can't even hope to go to each state once in its lifetime and the side effect of that is that

352
00:39:15,600 --> 00:39:21,760
there is pressure on the agent to use the data that it's seeing and to learn as much as possible

353
00:39:21,760 --> 00:39:27,440
from this data and just to continue learning over time so this is the case where for example

354
00:39:27,440 --> 00:39:32,880
hierarchical reinforcement learning should be very helpful because the agent can learn certain

355
00:39:32,880 --> 00:39:39,360
abstractions right making coffee right going for a walk and so on that would be helpful regardless

356
00:39:39,360 --> 00:39:45,360
of the situations that the agent is going to find itself and later and it's also worth investing

357
00:39:45,360 --> 00:39:53,360
the computational effort in order to to learn these kinds of representations so I sometimes think

358
00:39:53,360 --> 00:40:00,240
about what would it look like to to rewrite reinforcement learning without mark of an assumption

359
00:40:00,240 --> 00:40:05,040
right just imagine you have an agent it's receiving observations it's emitting actions there is

360
00:40:05,040 --> 00:40:10,000
a reward signal because we want the agent to have a task right so there there needs to be some

361
00:40:10,000 --> 00:40:15,520
goal specification that's the reward but otherwise we don't make assumptions about

362
00:40:16,160 --> 00:40:22,800
macovianness about stationarity right about there being a stationary distribution of states that

363
00:40:22,800 --> 00:40:30,960
the agent is visiting and one can still have algorithms that work under these settings

364
00:40:31,760 --> 00:40:38,400
um there are some really simple things that we can think about so for example doing

365
00:40:38,400 --> 00:40:44,240
usual temporal difference learning but with fixed learning rates right fixed learning rates mean

366
00:40:44,240 --> 00:40:49,840
that the agent is always paying more attention to the recent data and that's you know a very easy

367
00:40:49,840 --> 00:40:56,560
way to think about handling the the continual learning setup it's probably not sufficient right because

368
00:40:56,560 --> 00:41:01,360
only sort of encourages the agent to pay attention to the recent data but doesn't necessarily mean

369
00:41:01,360 --> 00:41:09,040
that the agent is trying to build these more abstract useful representations um so I think there's

370
00:41:09,040 --> 00:41:16,560
going to be a lot of interesting uh work in this area we have actually a survey on archive uh that

371
00:41:16,560 --> 00:41:23,280
was called out by my PhD student Kimia Ketropal um on uh continual reinforcement learning never

372
00:41:23,280 --> 00:41:29,200
ending reinforcement learning um and I'm quite excited to to explore uh this further and to think

373
00:41:29,200 --> 00:41:36,640
about problem definitions the mathematical limitations um and good algorithms obviously to handle

374
00:41:36,640 --> 00:41:43,360
these problems do we have a sense for computational computationally uh or you know in terms of

375
00:41:43,360 --> 00:41:50,800
sample complexity like how uh you expect continual RL to play out relative to classical RL

376
00:41:51,360 --> 00:41:57,360
so that's a really great question and it's an area where there is a lot of excitement and a lot of

377
00:41:57,360 --> 00:42:07,280
uh work on the theory side um so the first question to ask maybe is how do we talk about sample

378
00:42:07,280 --> 00:42:17,840
complexity in this case and um in theoretical RL regret has been the measure of um that that

379
00:42:17,840 --> 00:42:22,560
people have thought of traditionally as as a way to uh characterize sample complexity so regret

380
00:42:22,560 --> 00:42:28,320
is the difference between the value of what you're currently doing compared to the optimal value

381
00:42:28,320 --> 00:42:35,200
like how how well uh could you do possibly in an environment but if you're in a continual learning

382
00:42:35,200 --> 00:42:43,120
environment how what could you possibly do uh is up for interpretation right so maybe the environment

383
00:42:43,120 --> 00:42:49,040
is not an MDP there's not a unique optimal policy um you know what do you compare yourself against

384
00:42:49,040 --> 00:42:56,800
and uh there's been some really interesting work by Ben Van Roy's research group at Stanford

385
00:42:57,680 --> 00:43:04,880
recently looking at that question and defining essentially classes of policies

386
00:43:04,880 --> 00:43:10,880
uh and thinking of regret within those classes of policies in this kind of continual learning setting

387
00:43:10,880 --> 00:43:15,520
I think that this is something that we're gonna have to think through like what's the right measure

388
00:43:15,520 --> 00:43:21,680
of of complexity there's also uh tracking this is a different approach that people have

389
00:43:21,680 --> 00:43:26,960
thought of tracking basically says if something changes in the environment how quickly can you

390
00:43:26,960 --> 00:43:31,920
adapt to that change and I think that's a you know it's a little bit of a different perspective

391
00:43:31,920 --> 00:43:36,000
uh resetting than some of his group at University of Alberta have looked at that in the past

392
00:43:36,000 --> 00:43:42,640
what what's the idea with tracking as your environment is changing uh you want to quickly adapt

393
00:43:42,640 --> 00:43:48,480
uh to those changes so for example if you uh let's say you had the grid world and you used to go on

394
00:43:48,480 --> 00:43:53,360
the left path always but now that path is blocked you go and you discover that it's blocked

395
00:43:53,360 --> 00:44:02,400
how quickly can you find an alternative policy um and in some cases uh agents uh can do this very

396
00:44:02,400 --> 00:44:07,840
quickly right some of the agents can do this quickly um it all depends how good your exploration

397
00:44:07,840 --> 00:44:14,800
strategy is but you know an alternative to that is to say the reward signal is changing it's changing

398
00:44:14,800 --> 00:44:22,160
smoothly over time can you smoothly adapt your policy uh to uh to track uh what would be the

399
00:44:22,160 --> 00:44:27,600
the right actions right now so there's these different kinds of changes abrupt changes and

400
00:44:28,400 --> 00:44:34,240
more gradual changes and you want to uh to adapt and of course biological systems do this pretty

401
00:44:34,240 --> 00:44:41,440
well and in our algorithms if we use uh in principle if we use fixed learning rates things like

402
00:44:41,440 --> 00:44:48,800
value functions would also adapt continually um but exploration policies I think may need to be

403
00:44:48,800 --> 00:44:55,440
rethought in this context um and you know specifically a lot of exploration has been

404
00:44:55,440 --> 00:45:02,640
in this context continual or tracking in particular uh continual generally speaking

405
00:45:02,640 --> 00:45:10,320
okay tracking as a special case just because a lot of the work in in our exploration has been

406
00:45:10,320 --> 00:45:15,840
aimed at optimism in the face of uncertainty right so if you don't know something you should

407
00:45:15,840 --> 00:45:22,000
just be optimistic and go for it which is great if you're assuming that eventually you can go

408
00:45:22,000 --> 00:45:28,000
everywhere but if you have an environment that's very large and you can't go everywhere

409
00:45:28,000 --> 00:45:34,560
you might need to to do smarter things so maybe um information theoretic methods are more

410
00:45:34,560 --> 00:45:41,120
interesting in the setup right or explicit the uh keeping track of uncertainty or you know

411
00:45:41,120 --> 00:45:46,000
learning exploration strategies uh that are that are effective in the particular environment

412
00:45:46,000 --> 00:45:51,440
that the agents have so there's a lot of avenues for for future research in that setting

413
00:45:51,440 --> 00:46:04,240
does the continual RL setting lend itself more directly to a time series type of problem uh where

414
00:46:04,240 --> 00:46:09,040
you've got some agent that's uh making decisions that are presented to it presented to it over time

415
00:46:09,600 --> 00:46:15,680
relative to you know exploring environments in any way so I think there are interesting

416
00:46:15,680 --> 00:46:22,880
continual learning problems in time series prediction outside of decision making too for sure

417
00:46:24,400 --> 00:46:32,240
stock market prediction to be very concrete this one such example right uh but you know uh joking

418
00:46:32,240 --> 00:46:38,160
aside there is I think that the the time series prediction setting is really interesting to think

419
00:46:38,160 --> 00:46:47,920
about because um it avoids the problem of exploration but it still allows us to think about how fast

420
00:46:47,920 --> 00:46:53,760
an agent might adapt and to think about how do we characterize the difficulty of these problems so

421
00:46:54,800 --> 00:47:01,440
it's pretty clear intuitively that in some cases an agent may not be able to do anything so if

422
00:47:01,440 --> 00:47:08,560
you're at a time series where at every single time step there's some random bit that happens and

423
00:47:08,560 --> 00:47:13,120
the distribution that you had on the current time step has nothing to do with the distribution

424
00:47:13,120 --> 00:47:18,560
on the next time step well there's nothing the agent can do right so this is the sense that you

425
00:47:18,560 --> 00:47:25,680
know the existence of impossibility results but of course in in the real world there's structure

426
00:47:25,680 --> 00:47:31,840
right so I think they this set up allows us to think about what kinds of structure would allow

427
00:47:31,840 --> 00:47:37,040
an agent to learn successfully even if the environment is changing so of course for example if

428
00:47:37,040 --> 00:47:42,080
if there is gradual change in the environment let's say your bit is drawn from a probability

429
00:47:42,080 --> 00:47:48,080
distribution whose mean is gradually shifting over time then the agent can hope to learn

430
00:47:49,040 --> 00:47:53,600
how this shift is happening right and then perhaps even learn to anticipate what will happen

431
00:47:53,600 --> 00:48:00,880
that's but you know smoothness is only one particular kind of structure there may be others that are

432
00:48:00,880 --> 00:48:05,840
that are more interesting from practical point of view to wrap things up I'd love to have you comment

433
00:48:05,840 --> 00:48:16,000
broadly on the field of RL you know over the years you know RL has it's been hard for folks to

434
00:48:16,000 --> 00:48:20,880
do hard for folks to get up and running how do you see that part of it evolving

435
00:48:20,880 --> 00:48:27,360
what do you think are some of the big challenges going forward beyond the many that we've already

436
00:48:27,360 --> 00:48:35,120
talked about of course yeah thank you for asking about that I think RL has made tremendous progress

437
00:48:35,120 --> 00:48:42,160
and it's again from my point of view it's the closest to biological learning right it's the

438
00:48:42,160 --> 00:48:50,080
closest paradigm that we have in the field of machine learning to biological learning so it's

439
00:48:50,080 --> 00:48:58,480
very important for us to to think about it and deep RL has in fact delivered many very surprising

440
00:48:59,520 --> 00:49:09,360
successes ranging from alpha-go years back to more recently things like work by Marc Chandron

441
00:49:09,360 --> 00:49:17,120
Belmar on rooting loon balloons in the stratosphere using deep RL methods recent work by my colleague

442
00:49:17,120 --> 00:49:23,920
Martin Reed Miller at the mind on plasma control for fusion reactors these are very complicated

443
00:49:23,920 --> 00:49:28,720
control problems and reinforcement learning algorithms can handle them and and do a really great

444
00:49:28,720 --> 00:49:36,560
job so you know that makes me feel very optimistic that that we are able to scale RL and to really

445
00:49:36,560 --> 00:49:44,240
deliver some interesting practical results at the same time I think yes there are challenges and

446
00:49:44,240 --> 00:49:49,200
a lot of them have to do with these problems that we talked about you know discovery and

447
00:49:49,200 --> 00:49:55,680
efficiency how does an RL agent uses data very efficiently how does it do efficient exploration

448
00:49:56,400 --> 00:50:02,480
and how does it construct really good representations and historically reinforcement learning

449
00:50:02,480 --> 00:50:08,080
has relied a lot on other machine learning technology for example in order to build representation

450
00:50:08,080 --> 00:50:14,240
so we use deep nets deep nets are wonderful and they've they've led to a lot of these these great

451
00:50:14,240 --> 00:50:23,680
successes but in some sense that specific set of methods was developed for supervised learning

452
00:50:23,680 --> 00:50:29,360
right and supervised learning is based on a different set of assumptions than RL much more

453
00:50:29,360 --> 00:50:37,520
IID and so the way that we've made progress historically has been to kind of take RL methods and

454
00:50:37,520 --> 00:50:42,800
make them more supervised learning like using replay buffers is a classical example of this in

455
00:50:42,800 --> 00:50:49,680
order to train let's say cue learning in with deep networks I think it would be very useful for

456
00:50:49,680 --> 00:50:56,560
us to think about function approximation and optimization in the context of RL and how do we do

457
00:50:56,560 --> 00:51:02,000
that efficiently one of my students Emmanuel Bengeo who just graduated in fact looked at this

458
00:51:02,000 --> 00:51:09,840
problem last year and he discovered that looking at let's say atom optimizers in the context of RL

459
00:51:09,840 --> 00:51:14,800
doesn't give you that great results right in some cases really does not combine with

460
00:51:15,520 --> 00:51:20,560
temporal difference learning and and similar algorithm so I think one of the challenges for us

461
00:51:20,560 --> 00:51:25,200
is really to think about the function approximation and optimization problem in the context of RL

462
00:51:25,200 --> 00:51:29,920
and also to think about this continual learning setup because that's where we really want to go

463
00:51:29,920 --> 00:51:35,040
before going to build general intelligent agents they have to learn all the time they have

464
00:51:35,040 --> 00:51:41,680
to learn in these settings where you know Markovian structure doesn't hold and so that's that's

465
00:51:41,680 --> 00:51:47,120
the frontier awesome awesome well doing it thanks so much for spending some time chatting with

466
00:51:47,120 --> 00:51:52,400
us and sharing a bit about what you've been up to in the space very cool stuff you mentioned a bunch

467
00:51:52,400 --> 00:51:58,720
of research your own and others and we will try to collect that from you and be sure to include it

468
00:51:58,720 --> 00:52:03,840
on the show notes page but once again it's great chatting with you thank you for joining us

469
00:52:03,840 --> 00:52:30,560
likewise thank you for having me

