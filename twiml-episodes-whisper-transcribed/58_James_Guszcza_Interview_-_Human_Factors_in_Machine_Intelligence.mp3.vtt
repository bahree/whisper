WEBVTT

00:00.000 --> 00:15.920
Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting

00:15.920 --> 00:20.880
people doing interesting things in machine learning and artificial intelligence.

00:20.880 --> 00:23.520
I'm your host Sam Charrington.

00:23.520 --> 00:28.720
Let me start by sending a huge thanks to everyone who listened to our podcast series from the

00:28.720 --> 00:32.160
O'Reilly AI Conference in San Francisco last week.

00:32.160 --> 00:34.720
Thanks so much for your feedback and comments.

00:34.720 --> 00:36.560
We're glad you enjoyed the podcast.

00:37.600 --> 00:42.480
From now through the end of the year, I'll be attending a bunch of events and we'll be releasing

00:42.480 --> 00:46.480
a ton of great interviews, so please keep those comments coming.

00:47.360 --> 00:54.000
Reach out to us via at Twimble AI on Twitter or Facebook or via the show notes page for any

00:54.000 --> 01:01.040
episode. In the event you missed our tweets on Friday the 13th, we've got a very special announcement

01:01.040 --> 01:06.800
for you. In a few weeks, we'll be back in New York for the NYU Future Labs AI Summit.

01:07.600 --> 01:12.640
As some of you may remember, we held our very first Twimble Happy Hour in New York City

01:12.640 --> 01:18.080
just a few months ago and it was great, which inspired us to go even bigger this time.

01:18.080 --> 01:24.880
And that is just what we did. We're excited to present the AI Apocalypse and Killer Robots

01:24.880 --> 01:29.680
Halloween Social, which will be held on Monday, October 30th in New York City.

01:29.680 --> 01:35.760
This will be both a fun and informative event and I'll be doing a live podcast taping and

01:35.760 --> 01:43.040
panel discussion on the myths and realities of extreme AI featuring Dr. Seth Baum, Executive

01:43.040 --> 01:49.440
Director of the Global Catastrophic Institute, Shane Hobel, Founder of Mountain Scout Survival

01:49.440 --> 01:56.400
School, and Charlie Oliver, Founder of Tech 2025. This discussion will be interactive,

01:56.400 --> 02:01.280
so we'll get to discuss your most pressing questions about extreme AI and we couldn't

02:01.280 --> 02:06.960
be more excited about this event. For ticket info and more details about the AI Apocalypse

02:06.960 --> 02:14.160
and Killer Robots Halloween Social, visit twimbleai.com slash Halloween. And for 25% off of all

02:14.160 --> 02:21.920
ticket types for the NYU Future Labs AI Summit, use code twimble25. I'd like to take a moment

02:21.920 --> 02:27.280
to tell you about our sponsor for this episode, Nexosis, and thank them for supporting this week's

02:27.280 --> 02:33.600
show. Nexosis is a company focused on providing easy access to machine learning. The Nexosis

02:33.600 --> 02:38.800
Machine Learning API meets developers where they're at, regardless of their mastery of data

02:38.800 --> 02:44.160
science, so they can start coding up predictive applications today in their preferred programming

02:44.160 --> 02:50.000
language. It's as simple as loading your data and selecting the type of problem you want to solve.

02:50.000 --> 02:55.200
Their automated platform trains and selects the best model fit for your data and then outputs

02:55.200 --> 03:00.880
predictions. Get your free API key and discover how to start leveraging machine learning in

03:00.880 --> 03:11.120
your next project at nexosis.com slash twimble. That's n-e-x-o-s-i-s dot com slash t-w-i-m-l.

03:11.920 --> 03:15.440
Head over, check them out, and be sure to let them know who sent you.

03:16.080 --> 03:22.160
One final reminder about the upcoming twimble online meetup. Yes, just a couple of days from now

03:22.160 --> 03:28.000
on Wednesday, October 18th. At 3 p.m. Pacific time, we'll be discussing the paper,

03:28.000 --> 03:35.280
Visual Attribute Transfer Through Deep Image Analogy, by Jing Li Yao and others from Microsoft Research.

03:36.240 --> 03:41.600
The discussion will be led by Duncan Stothers. To join the meetup or to catch up on what you missed

03:41.600 --> 03:49.120
from the first two meetups, visit twimbleai.com slash meetup. Okay, about today's show. A few

03:49.120 --> 03:55.360
weeks ago, I sat down with James Goosa, US Chief Data Scientist at Deloitte Consulting,

03:55.360 --> 04:01.040
to talk about human factors in machine intelligence. James was in San Francisco to give a talk

04:01.040 --> 04:08.080
at the O'Reilly AI Conference on why AI needs human center design. James and I had an amazing

04:08.080 --> 04:14.480
chat in which we explored the many reasons why the human element is so important in ML and AI,

04:15.040 --> 04:19.280
along with useful ways to build algorithms and models that reflect this human element while

04:19.280 --> 04:26.000
avoiding problems like groupthink and bias. This was a very interesting conversation. I enjoyed it

04:26.000 --> 04:30.880
a ton and I'm sure you will too. And now on to the show.

04:38.240 --> 04:43.200
All right, everyone. I am here at the AI conference in San Francisco and I'm here with James

04:43.200 --> 04:49.680
Goosa. And James is the US Chief Data Scientist with Deloitte Consulting and he's going to be

04:49.680 --> 04:56.000
speaking later today actually on a topic that you've heard me allude to here on the podcast,

04:56.000 --> 05:01.600
the number of times, human factors in artificial intelligence. And so I'm really looking forward

05:01.600 --> 05:05.760
to diving into this conversation with you. James, welcome to the podcast. Thank you very much.

05:05.760 --> 05:10.720
I'm happy to be here. Absolutely. So why don't we get started by having you tell us a little

05:10.720 --> 05:14.400
bit about your background? You want to hear my checkered past? I want to hear your checkered

05:14.400 --> 05:21.840
past. Yeah. Well, in fact, I saw in your bio that you've got a PhD in the philosophy of science.

05:21.840 --> 05:28.080
Yeah, I know it's a cliche. Yeah, I have a PhD in philosophy from University of Chicago. I'm a

05:28.080 --> 05:32.560
very intellectually curious person. Actually, when I when I entered philosophy, what I thought

05:32.560 --> 05:37.440
was going to study was artificial intelligence. Really? Yeah, I'm a very old person. So this is

05:37.440 --> 05:42.240
back in the early 90s. And back then, artificial intelligence was, you know, talked about, you know,

05:42.240 --> 05:46.080
a lot of people are connected to this and Jerry Fodorans and so on and so on. And I almost went to

05:46.080 --> 05:49.360
University of Pittsburgh, which had a lot of types of Carnegie Mellon University. And there's a very

05:49.360 --> 05:52.640
strong, there's like one of the strongest philosophy of science programs in the country is at PIP,

05:52.640 --> 05:56.400
right down the street from CMU. And I really thought I'm going to do artificial intelligence.

05:57.040 --> 06:00.720
Long story, I changed my mind and went to the University of Chicago and I got a PhD in philosophy,

06:00.720 --> 06:04.960
but I focused more in philosophy of physics and especially the way statistics is used in physics.

06:04.960 --> 06:10.480
Okay, this is all my way of saying I studied pre-unemployment. I always joke that philosophy is

06:10.480 --> 06:14.880
the Greek word that means pre-unemployment. So, you know, I did, it was, it was wonderful. It was

06:14.880 --> 06:17.760
some of the best years of my life. I love Chicago. I love the University of Chicago. I love

06:17.760 --> 06:21.840
what I studied. It was fabulous. I needed a way to make a living. And this is back in the early

06:21.840 --> 06:26.080
early 2000s, the late 90s. I was, you know, thinking through the various options. I thought,

06:26.080 --> 06:31.360
well, I'm a humanities guy, good at law school. I'm going to do that. You know, I'm, I'm doing

06:31.360 --> 06:35.520
kind of scientific stuff. I could go to Wall Street, right. I could do the whole options thing,

06:35.520 --> 06:40.080
right, which is very sexy back then. I didn't think I, it would culturally work for me. I just,

06:40.080 --> 06:43.200
I just didn't think I, I actually landed a job and went out for the interview. I just didn't

06:43.200 --> 06:46.320
think I'd enjoy it. So, I didn't do it. So, I went, I went for the fame and fortune and glam

06:46.320 --> 06:50.640
or becoming an actuary. Okay. And I didn't know what that meant back then. But I assumed that

06:50.640 --> 06:55.520
actual science meant data science. And it didn't, but now it kind of does. And there's a weird

06:55.520 --> 06:59.600
way in which actual is where the original data scientist, there's a weird way in which my first

06:59.600 --> 07:03.520
data science program or project, which I did at the All-State Research Center in Menlo Park,

07:03.520 --> 07:07.360
California, is a weird way in which that was actually artificial intelligence. I didn't think

07:07.360 --> 07:11.360
of it at the time, but I basically, you know, credit scores, you know, you could think of credit

07:11.360 --> 07:16.560
scores as sort of an early example of AI in a sense that Chris Hammond from Narrative Science

07:16.560 --> 07:21.200
in Northwestern talks about, which is that it's not so much AI from focusing in it from a,

07:21.200 --> 07:25.440
like, through a technical lens, but functionally it's AI because we used to have this whole

07:25.440 --> 07:30.720
profession called bank loan officers. And it was like a lot of people. That was their job.

07:30.720 --> 07:35.680
It was bank loan officers. And it turns out that went out the window when we used algorithms

07:35.680 --> 07:40.640
to make loan, to make lending decisions. And I can get into this. There are a lot of reasons

07:40.640 --> 07:44.720
why that makes a lot of sense. Some of it is on the data side. And some of it is on the human

07:44.720 --> 07:49.200
cognition side. So a lot of reasons why what aspect of this makes a lot of sense. Well, there's a

07:49.200 --> 07:53.840
lot of reasons why there's a lot of reasons why that was an early case where algorithms outperformed

07:53.840 --> 07:58.320
human judgment. And you know, and the use of algorithms kind of like, you know, kind of like a

07:58.320 --> 08:04.960
shrunk a certain part of the workforce, right? And so was this contested like that feels intuitively

08:04.960 --> 08:10.160
obvious to me that, you know, this is a fundamentally database decision. And if you can accurately

08:10.160 --> 08:16.720
characterize the, you know, a person situation, you know, in data, which we are able to do now,

08:17.280 --> 08:22.800
then algorithms are going to do a pretty good job of this over, you know, golf and relationships.

08:22.800 --> 08:27.200
And some of the things that we think of as the past lives of the loan officer. Absolutely.

08:27.200 --> 08:31.040
And I would say yes, but I think that's okay. I think this kind of gets a bonus, but it's always

08:31.040 --> 08:35.600
the interesting part. Absolutely. No, exactly. Exactly. No, this, and this is sort of what it kind of

08:35.600 --> 08:39.520
gets at one aspect of what I'm going to be talking about in my talk this afternoon, which is that

08:39.520 --> 08:45.440
unated judgment is notoriously unreliable when it comes to making judgments and decisions.

08:45.440 --> 08:49.680
And you know, if the listeners have read things like Daniel Kahneman thinking fast and slow,

08:49.680 --> 08:54.880
or nudge by Taylor and Sunstein, you know, or clued by Gary Marcus, the cognitive scientist,

08:54.880 --> 08:59.760
we realized that, you know, our brains evolved. They were optimized by evolution for a certain kind

08:59.760 --> 09:04.240
of environment, you know, outrunning predators in the sedentary, whatever it was. I'm also

09:04.240 --> 09:08.400
thinking predictably irrational by the area. There's another popularization of this whole thing,

09:08.400 --> 09:12.080
exactly. But you know, the real, the real pop classic, in my opinion, is thinking fast and slow

09:12.080 --> 09:15.920
by the economy. That should, that should be in every machine learners shelf, in my opinion. This

09:15.920 --> 09:20.400
is kind of like the compliments machine learning in my opinion. So yeah, I mean, those same sort of

09:20.400 --> 09:25.200
mental heuristics that service well in kind of everyday life, they don't service so well when we

09:25.200 --> 09:29.840
put on a suit and sit around a boardroom and try to decide, should I, you know, acquire this company,

09:29.840 --> 09:34.160
should I admit the student to university? How should I treat this patient? Does this person get

09:34.160 --> 09:39.360
the loan or not? There can be all sorts of biases creeping in. Okay. And then this is the theme

09:39.360 --> 09:44.000
of predictably irrational and thinking fast and slow. So, you know, Kahneman talks about, he calls

09:44.000 --> 09:50.720
the miracles and the flaws of any judgment. It's a paraphrase. The miracle is that most of the

09:50.720 --> 09:54.400
decisions we make in day to day life are what he would call thinking fast. You know, just like,

09:54.400 --> 09:58.640
you know, effortless, they come automatically. We kind of tell a story and the story kind of works.

09:59.360 --> 10:03.040
But, you know, in these kind of mission critical cases where it's more like, you know,

10:03.040 --> 10:06.960
is this person going to, you know, commit a crime or is this person going to pay back the loan

10:06.960 --> 10:11.440
or is this person going to crash his car or does this person have the disease? Not so well,

10:11.440 --> 10:16.640
we really need help from algorithms. But that's kind of like one, one half of the story. I think

10:16.640 --> 10:20.720
another one of the topics that's really that we've all known about for decades, but it's really

10:20.720 --> 10:24.960
becoming, it's coming to the fore is the need to kind of make sure we reflect societal values

10:24.960 --> 10:29.360
in these algorithms at the same time. So, when I was, it all stayed doing this. It's kind of an

10:29.360 --> 10:32.960
interesting story. I wasn't building a credit scoring algorithm for all say because they were

10:32.960 --> 10:37.200
underwriting loans. It's because they're selling insurance contracts. Turns out the credit is

10:37.200 --> 10:40.880
hugely predictive. Who's going to crash their car or have like a water, homeowners claim something

10:40.880 --> 10:44.400
then? We can get into that. It's a very interesting story about like, why is the data so predictive

10:44.400 --> 10:49.360
in that way? But we're also very interested in the legal doctrine of disparate impacts. So,

10:49.360 --> 10:54.080
even if we didn't put like a protected class in the algorithm, there could be an unintended

10:54.080 --> 10:57.920
consequence, you know, where the algorithm could have like systematically different scores

10:57.920 --> 11:03.120
for different groups of people like income or whatever it is. Or urban rule or race or gender

11:03.120 --> 11:08.720
or whatever it is. We don't want that, right? And it's like, you know, some, you know, and it's

11:08.720 --> 11:12.240
very interesting because like, you know, these same early conversations are now reflected in a

11:12.240 --> 11:16.640
larger scale in the world of AI, right? But early on, you know, I'd get into these conversations

11:16.640 --> 11:20.480
with other quants, you know, back then there were actuaries in English. Back then we called it

11:20.480 --> 11:23.920
machine learning to answer your question, right? They were data mining. That's what we called it back

11:23.920 --> 11:28.880
then. KDD was around back then so we called it data mining. But, you know, the actuaries would say,

11:28.880 --> 11:32.000
well, this is just ridiculous. I mean, we just want to come up with the actuarly fair price.

11:32.000 --> 11:36.400
Everybody should just like be charged insurance and reflects their risk. But that might be a

11:36.400 --> 11:40.800
limited perspective. There might be other perspectives that legitimately should constrain models.

11:40.800 --> 11:45.680
So what is the, you know, what is the way to kind of optimize models? Is it, you know,

11:45.680 --> 11:48.800
is it kind of a metric of we're going to like, you know, make this prediction with the greatest

11:48.800 --> 11:53.600
out of sample accuracy? Or is it subject to this that in the other constraint? And some of those

11:53.600 --> 11:58.640
constraints are societal in nature. Some of those constraints are what we were calling human factors

11:58.640 --> 12:04.720
in nature. So there are some cases where maybe it's really complex risk. Maybe it's, maybe I'm

12:04.720 --> 12:08.880
trying to underwrite a very complex loan or a very complex insurance contractor, a very complex

12:08.880 --> 12:14.560
medical diagnosis or a judge making a parole decision. We don't necessarily want to simply turn

12:14.560 --> 12:18.960
that over to a machine, right? We don't just want to automate it away and take humans out of the

12:18.960 --> 12:22.160
loop. But, you know, rather what we want to do is we want to kind of take the best of both worlds

12:22.160 --> 12:27.520
and say, well, the machines are good in one way, you know, they can weigh together 400 factors,

12:27.520 --> 12:31.440
you know, better than we can weigh together four. And by the way, do it the same way before

12:31.440 --> 12:35.200
lunch and after lunch versus after lunch, which we don't do. But at the same time, they don't have

12:35.200 --> 12:40.000
common sense. Humans have common sense. They understand ethics. They understand the strategic goals

12:40.000 --> 12:44.720
of the organization. They understand, you know, you know, societal values, legal constraints,

12:44.720 --> 12:50.320
whatever it is. You know, public relations, whatever it is. And so figuring out how to marry

12:50.320 --> 12:55.200
the best of both worlds. That's part of what I mean by design. You know what I mean? Yeah.

12:55.200 --> 13:02.960
You know, it's like one analogy I've made for a long time is that, yeah, these algorithms

13:02.960 --> 13:08.080
do replace humans to certain tasks. So I always talk about credit scores, like in an early example

13:08.080 --> 13:14.880
of artificial intelligence. For some types of loans or risks or medical diagnosis or whatever,

13:14.880 --> 13:18.560
you just kind of like turn the algorithm on, it'll make a smart decision. And it might be wrong

13:18.560 --> 13:22.240
part of the time, but, you know, maybe the losses are acceptable for whatever reason. Yeah.

13:22.240 --> 13:28.240
In other cases, you just can't do that. And so it's more like there's this art to somehow blending

13:28.240 --> 13:32.560
the machine indication with the human judgment. And that's always fascinated me. That's been

13:32.560 --> 13:36.960
more the late motif of my work at Deloitte since leaving Allstate. You know, at Allstate, we're doing

13:36.960 --> 13:40.720
a lot of, you know, big data, personal insurance. Allstate is the second or third largest

13:40.720 --> 13:44.320
insurance carrier in the country. So they have all this big data. And you can do a lot of this kind

13:44.320 --> 13:48.000
of like, well, we'll just have a, you know, this algorithm, it'll spit out a price. Boom,

13:48.000 --> 13:52.960
there's your price. It's automation. But when I joined Deloitte, there's a lot more work for

13:52.960 --> 13:59.120
smaller medium-sized companies or organizations that want to use data to make more complex

13:59.120 --> 14:03.840
decisions. And so this idea of blending algorithmic indications with human judgment became

14:03.840 --> 14:09.120
much more of an issue. I only came to appreciate this gradually as I was working Deloitte to start.

14:09.120 --> 14:13.200
I started off as a pure quant. I was just interested in the math and I still am. I love it.

14:13.600 --> 14:16.400
So that's the geeky side of me. But there's also this kind of side where like I'm just

14:16.400 --> 14:21.040
fascinated by the way it's used in organizations. I'm just interested in like you need organizational

14:21.040 --> 14:27.680
buy-in. You need to reflect domain knowledge and institutional knowledge in the data,

14:27.680 --> 14:32.000
in the design of the algorithm. You need to think upfront about how is the algorithm going to be

14:32.000 --> 14:36.400
used in the organization? Who's going to be using it? Who are stay-coulders? And if you get all

14:36.400 --> 14:41.040
those things wrong and if you don't plan for them upfront, I won't guarantee it, but I will

14:41.040 --> 14:45.840
bet money. You'll get a negative ROI on your analytics project. So that's why I become

14:45.840 --> 14:50.160
obsessed by this. And I think the exact same issues arise in now the room age of AI.

14:50.160 --> 14:55.600
Yeah. Yeah. One of the things that struck me as interesting in hearing you tell the story about

14:55.600 --> 15:05.360
the work you did at Allstate in particular was it sounded like you were very aware at that time

15:05.360 --> 15:12.320
about you know issues that you know I think of in many ways is like only now kind of

15:12.320 --> 15:17.840
finding contemporary voice. You're right. So you know bias and algorithmic bias. I know it's very

15:17.840 --> 15:26.080
fascinating. Just last night, just last night Pedro Domingo tweeted true fact algorithms

15:26.080 --> 15:33.680
cannot discriminate. And so I replied well how do we define algorithm? What are we talking

15:33.680 --> 15:42.080
about here? Exactly. And so in that lens it's like this is the new issue like we're just

15:42.080 --> 15:46.640
gearing up to fight this fight. Yeah. But it sounds like you were grappling with this way back what?

15:46.640 --> 15:52.720
And not just not just thinking about it, but the impression I'm getting from the way you

15:52.720 --> 15:56.400
described it was that the organization had a consciousness around it. Absolutely.

15:56.400 --> 16:00.400
It's to talk more about this. Yeah, yeah. No, no, it completely is. And I'd love to talk a little

16:00.400 --> 16:04.080
bit more about pro social uses of big data. I have kind of like a little mantra on that.

16:04.640 --> 16:08.880
No, I mean this is one of those things where you know I mean Ben Franklin had decided doing

16:08.880 --> 16:13.680
well by doing good. And I don't want to make any grand claims for my employees or anything

16:13.680 --> 16:20.160
like that. But it's in organizations in light and self-interest to think in a long term.

16:20.160 --> 16:25.120
Maybe in the short term you can like just throw out whatever model you want. But you know

16:25.120 --> 16:29.200
they're smart enough to realize that if there are these unintended consequences it'll come back

16:29.200 --> 16:33.360
to the bite them later on. It doesn't make any sense, right? So since all organizations are

16:33.360 --> 16:38.320
self-interested, does that mean that some are more enlightened than others? I think some are more

16:38.320 --> 16:42.080
enlightened. But I mean it's clear. I mean you know something else we could just be a tangent.

16:42.080 --> 16:46.080
We could talk about a group thing. I mean they're they're you know think about all the organizations

16:46.080 --> 16:52.160
you know or you know both private and public sector organizations that make catastrophically

16:52.160 --> 16:58.720
bad decisions. You know when even this is another interesting thing like our organizations people

16:58.720 --> 17:03.680
will know. There are people that comprise organizations, right? Organizations can act as if

17:03.680 --> 17:07.600
they're rational or not or they're enlightened or not. And sometimes what happens is you'll

17:07.600 --> 17:11.920
meet a lot of very well-meaning people in an organization but they kind of have to self-sensor.

17:11.920 --> 17:14.560
And even if they think there's something that's not quite right. They have to kind of be

17:14.560 --> 17:19.600
their self-sensor or maybe they just get into this habit of believing their elders or their

17:19.600 --> 17:22.720
superiors because of drinking the cool aid. Drinking the cool aid. Yeah I mean this is called

17:22.720 --> 17:25.600
group thing, right? You know it's like the opposite of collective intelligence which is what

17:25.600 --> 17:30.400
the data science should be all about. No so I mean so that was a tangent but I mean I think

17:30.400 --> 17:35.920
this recognition is it was it purely internal? Was it driven by regulatory framework? Yeah

17:35.920 --> 17:40.080
sort of fear. Yeah I don't I don't want to be granteos. I do have a sense. I don't want to be

17:40.080 --> 17:43.920
grandiose. Insurance is very heavily regulated. It's regulated at the state level in fact so it's

17:43.920 --> 17:47.920
not just one agency. It's 50 agencies in the United States and it's actually it's a rare case

17:47.920 --> 17:52.160
where it's actually more heavily regulated in the US than it is in Europe and so they absolutely

17:52.160 --> 17:56.720
in part in fact part of the reason why my employer wanted to build a statue of limitations

17:56.720 --> 18:00.640
applies here. I think one of the reasons they wanted to build this thing in house is that they

18:00.640 --> 18:06.720
actually wanted to have control over the details of the model. They want to be able to make sure

18:06.720 --> 18:11.120
that no we're getting this exactly right you know it maybe it's maybe you can't use medical

18:11.120 --> 18:15.040
bankruptcies in this state. Well we're not see we can prove it because this is our algorithm.

18:15.040 --> 18:18.560
Where's if we bought some black box off the shelf thing we're not sure we've reflected that

18:18.560 --> 18:24.000
regulation in the thing and of course regulations are an attempt to reflect societal values right

18:24.000 --> 18:28.480
so the ultimate thing is you want to reflect societal values in the algorithms and regulations

18:28.480 --> 18:31.600
are kind of a halfway house. I'm speaking philosophically from my perspective but I think that's

18:31.600 --> 18:36.720
what's going on here and so that's the game right I mean the companies want to make sure they're

18:36.720 --> 18:41.760
using algorithms to you know run their their processes more efficiently in the case of all state

18:41.760 --> 18:46.480
you know we want to you know it's the oldest game in the book you want to be able to come up with

18:46.480 --> 18:52.320
a more accurate price for a risk you know the logic of credit scoring and insurance is you know

18:52.320 --> 18:57.120
we all know that six-year-old male motorcycles are bad drivers right or probably risk I should

18:57.120 --> 19:01.600
say risk you've an average drivers perhaps right risk you then perhaps a middle-aged female

19:01.600 --> 19:06.480
station wagon driver perhaps but if you can find the six-year-old male motorcycle driver who's

19:06.480 --> 19:10.800
also present in the chess club subscribe to Martha Stewart Living magazine and has a good credit

19:10.800 --> 19:16.240
score he's probably good risk and if you can collect all those good credit score six-year-old male

19:16.240 --> 19:20.720
motorcycle drivers you can kind of give them a lower rate because they are actually better risks

19:20.720 --> 19:25.600
than might appear on the surface and that means the other companies who don't have credit score

19:25.600 --> 19:28.800
have to charge more for the six-year-old male motorcycle drivers and it's this kind of

19:28.800 --> 19:35.520
adverse selection spiral so that's that's the that's the kind of like economic logic for doing

19:35.520 --> 19:41.040
and this is why insurance is a very early adopter of big data data mining analytics but that's

19:41.040 --> 19:45.920
subject to a constraint I mean if you just did kind of crowdsourcing competition you know come

19:45.920 --> 19:51.040
up with the best segmentation thing right there'll be you know unless you've prepared the data

19:51.040 --> 19:56.000
yourself and unless you're very careful about auditing that algorithm you know you're not sure that

19:56.000 --> 20:00.880
that reflects these regulatory constraints which you know our reflexes are societal values or not

20:00.880 --> 20:04.000
so you know and other things crowdsourcing would be bad in this context I'm just saying you have

20:04.000 --> 20:07.280
to kind of take that into account you're optimizing more than one thing not just out of separate

20:07.280 --> 20:12.640
accuracy but these other things too and you know and I think there are a lot of companies that it's

20:12.640 --> 20:16.800
not just regulation they just want to do the right thing you know I mean like actually Richard

20:16.800 --> 20:20.640
Thaler who's one of my heroes he's a father of behavioral economics of the University of Chicago

20:20.640 --> 20:26.800
Business School he tweeted about I won't I won't say which company it is but it's an airline

20:26.800 --> 20:32.880
that kind of fixed its fees for flights out of Miami at a fairly low rate to help people

20:32.880 --> 20:38.480
escape the storm even though they could have done surge pricing Thaler would say that that

20:38.480 --> 20:42.560
just kind of goes against the grain of human psychology you know we have these things that Adam

20:42.560 --> 20:46.880
Smith called moral sentiments you know which we call ethics now it's like that just doesn't feel

20:46.880 --> 20:53.200
right so even though from a from a technical classical economics homo economic as rational profit

20:53.200 --> 20:59.120
maximizing perspective they should charge $4,000 for a flight to Atlanta from Miami but they didn't

20:59.760 --> 21:02.320
and failure saying it's because it's because they're thinking in the longer term

21:02.960 --> 21:06.640
in that case it wasn't regulation it was just like we're playing the long game

21:06.640 --> 21:10.720
and there are other companies that did jack up the prices right and that's really interesting

21:10.720 --> 21:15.840
actually because they they may be the case I don't know I'm speculating but that may have been

21:15.840 --> 21:20.720
unbridled algorithmic thinking it may be that like a pricing algorithm you know it's quite

21:20.720 --> 21:25.120
possible that some of these competitors did do surge pricing because it's kind of like the

21:25.120 --> 21:29.440
algorithm is just kind of calling the shots and that's what the algorithms usually did absolutely

21:29.440 --> 21:33.440
right and so this is a really nice case where it's it's sort of like parallel I think to the

21:33.440 --> 21:38.160
insurance case except it wasn't due to regulation it was just more due to like customizing value

21:38.160 --> 21:41.840
you don't want to alienate people right people just are going to remember things like this so it's

21:41.840 --> 21:46.720
like our customers but does the algorithm the algorithm would have to have a pretty long

21:47.680 --> 21:53.840
life cycle to pick up on that customer lifetime value yeah that's and that's the point

21:53.840 --> 21:58.480
that suggests that you know it's more likely than not there was human in a loop there precisely

21:58.480 --> 22:03.200
no that that's exactly the point yeah it's like you know we can kind of speculate about

22:03.200 --> 22:06.800
singularities we kind of speculate and have you know fun conversations about what are we going to

22:06.800 --> 22:11.520
reach out to visual general intelligence we have like a robot that can like use common sense and

22:11.520 --> 22:15.200
price it you know both to optimize things but also that's okay but that's not having any time

22:15.200 --> 22:18.880
soon right and we've got these are machine learning algorithms are essentially like statistical

22:18.880 --> 22:22.640
models you know on steroids basically deep learning models are like well just regression

22:22.640 --> 22:26.880
models on steroids that that create their own features right so that's that's what we got

22:26.880 --> 22:30.880
and that's great it's really really powerful but but as you're suggesting I think what it implies

22:30.880 --> 22:35.440
is that we want to have humans that have common sense reasoning to keep the models in check

22:35.920 --> 22:40.880
and so that what that implies is that the people that yeah and I'm going to quote an economist here

22:40.880 --> 22:45.040
and in John Kaye the people that understand that don't do too much of that on this podcast though

22:45.040 --> 22:47.920
well I really I'm just joking sorry

22:47.920 --> 22:52.080
what are your economists yeah that's a big turn on no John Kaye was my favorite I think he's

22:52.080 --> 22:56.320
retired he was my favorite columnist in the financial times he used to be an Oxford economist I

22:56.320 --> 23:00.800
think or London Business School was something but he was asked 10 years ago to diagnose

23:01.520 --> 23:05.840
somebody asking the question in the in the aftermath of the financial crisis why is it that all

23:05.840 --> 23:14.000
these models built by Harvard, Cambridge, MIT, Quance failed so badly and Kaye was so direct

23:14.000 --> 23:17.680
into the point of so elegant he said the problem was that the people that understood the math

23:17.680 --> 23:21.600
didn't understand the world the people that understood the world didn't understand the math and

23:21.600 --> 23:26.640
you know so I think that's yeah that's that's another kind of case of where we need to kind of like

23:26.640 --> 23:30.400
or I think I think I can't remember who said this but I heard a very nice quote of the day that

23:30.400 --> 23:34.800
a really good data scientist needs a kind of communication and empathy ability to be able to kind

23:34.800 --> 23:40.160
of talk to the people that understand the world not just to reflect their knowledge in the data

23:40.160 --> 23:46.960
but also to reflect just the kind of like strategic values the societal values the long true we don't

23:46.960 --> 23:51.440
want to alienate our customers along to all those kinds of things you know what I mean so interesting

23:51.440 --> 23:57.040
so this is all kind of background for your talk like how did you had you organized your talk

23:57.040 --> 24:02.160
did you have a list of human factors that an organization needs to consider or oh no it's

24:02.160 --> 24:08.320
nothing nothing that cutting right honestly nothing about me is like that in my background's

24:08.320 --> 24:11.680
philosophy right I always kind of go back to first principles and I'm just kind of I'm just

24:11.680 --> 24:15.920
really just thinking about what are algorithms good at you know why do we have algorithms what

24:15.920 --> 24:20.800
what you know what are their limitations what are ways of overcoming those limitations and yeah

24:20.800 --> 24:26.400
and I you know I do have I do have some ideas for you know where we can need to you know inject

24:26.400 --> 24:30.880
sort of like extra statistical or extra machine learning or extra computer you know beyond computer

24:30.880 --> 24:34.480
science principles into into what we're doing and so these are all examples that I've been

24:34.480 --> 24:39.280
giving though so the way I structure the talk should we get into that now or yeah the way I

24:39.280 --> 24:44.640
structure the talk is actually I'm quoting someone I know a little bit Chris Hammond at Northwestern

24:44.640 --> 24:48.400
University of Narrative Science he's somebody greatly admire actually that he he and I overlapped

24:48.400 --> 24:51.680
the University of Chicago's I was getting my PhD in philosophy when he was a computer science

24:51.680 --> 24:56.240
professor there so he's now at Northwestern doing really innovative stuff and he's also the chief

24:56.240 --> 25:01.280
science scientist which he's science officer of narrative science the natural language generation

25:01.280 --> 25:08.640
I mean this is really a nice way to think about AI we shouldn't think of AI in terms of the

25:08.640 --> 25:12.720
underlying technology we should really think about AI in terms of like what is its function what

25:12.720 --> 25:18.480
are we trying to achieve here oh well we're trying to automate this process we're trying humans are

25:18.480 --> 25:23.040
really bad at this they fall asleep in the wheel so let's have AI that drives for them or let's

25:23.040 --> 25:27.120
just have AI that kind of recognizes their face when they're getting drowsy like the effective

25:27.120 --> 25:32.400
software right like Rana Alkalubi then kind of nudges them maybe maybe turns the radio up or

25:32.400 --> 25:37.280
something right gives them a punch just like whatever it is I'm not trying to nudge or punch

25:37.280 --> 25:41.200
that's a goal and so some of these things can be done through robotic process automation

25:41.200 --> 25:46.160
which is not even data driven it's just kind of like logic some can be done through deep learning

25:46.160 --> 25:50.080
so yeah sure if you upload my photograph into Facebook it'll say that's a picture of George

25:50.080 --> 25:55.680
Clooney which is a pretty good guess right joke so that that's automation and but there's also

25:55.680 --> 25:59.680
the augmentation side of things which I want to talk about too so that that's that's kind of

25:59.680 --> 26:03.520
like the large structures start off with Chris Hammond talk about the fact that when we talk about AI

26:03.520 --> 26:07.120
it should be kind of like a functional thing not a tech first thing so it's not just about deep

26:07.120 --> 26:11.680
learning it's not just about machine learning it's really like it's really building computer

26:11.680 --> 26:16.000
algorithms that do things that were they to be done by humans they'd be considered intelligent

26:16.000 --> 26:20.320
right that's sort of like that's kind of Chris Hammond channeling John McCarthy at Dartmouth in

26:20.320 --> 26:26.400
1956 he's one of the founding fathers of AI very smart kind of like operational definition

26:26.400 --> 26:31.040
sure and I like it because it just it's a consultant because I'm really a consultant first

26:31.040 --> 26:34.000
I'm a consultant who happens to be a data scientist or then a data scientist who happens to

26:34.000 --> 26:37.760
work in a consulting firm and so as someone who really believes that he's a consultant I think

26:37.760 --> 26:41.680
that's just a really great way of thinking about it because I've seen you know the hype cycles

26:41.680 --> 26:47.600
come and go but over and over and over again I see that the the organizations and the the leaders

26:47.600 --> 26:53.360
who kind of take a tech first view of the stuff it tends to get a lot of attention and buzz early

26:53.360 --> 26:57.680
on but it doesn't really produce the value downstream whereas if you start with kind of like a problem

26:57.680 --> 27:02.720
centric view first and kind of reverse engineer from there well what do I want you're more

27:02.720 --> 27:07.120
likely to succeed and it's likely to be a more efficient elegant and frankly cost effective

27:07.120 --> 27:11.520
solution with less risk and in many cases a lot simpler and a lot simpler would have done if you

27:11.520 --> 27:16.960
were just following shiny object yes no exactly in fact you know again I'm old so I'll you know

27:16.960 --> 27:20.880
I'll I'll quote I'll do some more own quotes one of my someone who's sort of an informal mentor

27:20.880 --> 27:26.000
of mine at the University of Chicago was a very prominent Bayesian econometrician named Arnold

27:26.000 --> 27:31.760
Zelmer and he had a concept called sophisticated simplicity sophisticatedly simple the idea is

27:31.760 --> 27:37.360
that you start up with a simple model and if it works done if it doesn't work you just gradually

27:37.360 --> 27:42.320
add structure right until it does work and then you stop yeah you don't start with most complicated

27:42.320 --> 27:47.760
things it makes you seem like most smart or impressive or macho right and I think that an

27:47.760 --> 27:51.600
analogous comment can be made about our official intelligence and Chris was kind of making this point

27:51.600 --> 27:56.480
of the day in his tutorial which I which I just loved you know he said if you if all you need is

27:56.480 --> 28:01.520
robotic process automation do it what's what's the downside just do it you don't even need big data for

28:01.520 --> 28:06.000
that you just need like you know smart consultants and programmers and you'll just save a lot of

28:06.000 --> 28:10.560
money and there's very little downside risk let me ask you about you do a lot of you well you've

28:10.560 --> 28:15.520
brought up RV a couple of times oh yeah it's it's part of the family of AI yeah I'm just curious

28:15.520 --> 28:21.520
your perspective on this I jump to the question right the question is you know can you provide for

28:21.520 --> 28:31.200
me specific proof point examples where you know people are doing RPA that's you know that suggests

28:31.200 --> 28:36.800
that RPA is more than a rebranding of BPM you know I don't I probably shouldn't comment too much

28:36.800 --> 28:41.680
on that because I'm not like one of our RPA experts and maybe it is it's just like this is this is an

28:41.680 --> 28:46.400
idea that's been around for a long time it just makes eminent common sense I don't I don't really

28:46.400 --> 28:51.600
care what you call it so much but just the idea of taking processes we're just somebody's doing

28:51.600 --> 28:57.520
something that's just like routine and wrote boring spade work and if you can just get a macro

28:57.520 --> 29:02.880
or a script to do that why not do it that that's kind of like analogous zone zone viscous simplicity

29:02.880 --> 29:07.520
like a statistics thing if it's just like looking at the difference of two means is all you need

29:07.520 --> 29:12.320
you know looking to bootstrapping do it and a business context if all you need is to automate

29:12.320 --> 29:16.400
something that's really really wrote in simple and spade work do it you don't need machine learning

29:16.400 --> 29:22.480
for that you know and you know in kind of going up up that kind of food chain of complexity you

29:22.480 --> 29:26.400
just kind of want to start I guess what I was getting is you want to start with the problem

29:26.400 --> 29:31.840
and kind of back into the either technology or the data science and the machine learning whatever

29:31.840 --> 29:37.600
is a little solve the problem and so you mentioned automation and augmentation augmentation what

29:37.600 --> 29:41.360
is that mean for you and how are you seeing folks skin value there I've seen I've seen folks

29:41.360 --> 29:46.240
gain value from augmentation in my whole career I've been in Deloitte since 2001 and it's been one

29:46.240 --> 29:50.160
of the most common themes of what I've been doing we built algorithms that will automate things

29:50.880 --> 29:56.000
but very often you know we don't always work by augmentation are we talking about augmenting

29:56.000 --> 29:59.920
human intelligence okay that's exactly right augmentation or some other yeah yeah

29:59.920 --> 30:03.520
you know I'm talking about augmenting intelligence and that that vocabulary is somewhat new to me

30:03.520 --> 30:08.000
I haven't always described what we do in those terms but I like I like the vocabulary so yeah

30:08.000 --> 30:13.760
I mean you know we very often early on when we do our projects you know like for example suppose

30:13.760 --> 30:18.880
we're working for again with so we're working for an insurance company but say it's a commercial

30:18.880 --> 30:22.640
insurance company so instead of selling auto insurance they say for example they sell workers

30:22.640 --> 30:30.240
confidence now there are fewer businesses to insurer in the world than there are cars and businesses

30:30.240 --> 30:35.200
have fewer factors in common than cars do some are florists some are hipster coffee shops some are

30:35.200 --> 30:39.600
hospitals right and so you have like means you have fewer rows in your database new fewer columns

30:40.160 --> 30:43.760
and you but you're trying to do something similar to what you know my my first job was which is

30:43.760 --> 30:46.960
you're trying to come up with like a better price for the risk or an underrated decision should I

30:46.960 --> 30:50.720
sell this this person insurance or not should I sell your hipster coffee shop insurance or not

30:51.600 --> 30:56.960
and that's the case where like what we found just empirically you know through our data and through

30:56.960 --> 31:02.400
through you know blind test validation is that it would work pretty well in certain cases and

31:02.400 --> 31:06.160
that was an empirical question and it was it was partly empirical partly strategic you know it's

31:06.160 --> 31:09.120
like we'd have to work with the client to figure out what is the cutoff here where we're going

31:09.120 --> 31:13.200
to like straight through processes decisions whereas these other decisions really is going to simply

31:13.200 --> 31:17.920
give it to an underwriter maybe like rank order some things we'll try to explain the underwriter

31:17.920 --> 31:21.760
what's going on here you know we'll try to like train the underwriter ahead of time to understand

31:21.760 --> 31:26.160
the premises of the models and you know if and if we don't do that it's just not going to work

31:26.160 --> 31:30.240
so it's like a very simple example of you know what we were calling human factors earlier I don't

31:30.240 --> 31:34.160
know human factors is quite the right word but it's some some kind of like a either a human centered

31:34.160 --> 31:40.880
or an organization centered design we you know I began to use the analogy you know Mr. underwriter

31:40.880 --> 31:45.440
or Ms. Underwriter just you know imagine that your eyes are myopic and see you go to the doctor

31:45.440 --> 31:50.800
and you get a pair of glasses you can see better well you know Daniel Connemon and Danny O'Reilly

31:50.800 --> 31:55.200
and all these behavioral economists and psychologists teach us that our brains are myopic our brains

31:55.200 --> 32:00.400
have these you know bound biased heuristics that we use to make decisions so they we have blurring

32:00.400 --> 32:06.080
mental vision and so in these in these augmentation cases the algorithms are kind of like prostheses

32:06.080 --> 32:09.760
they're kind of like eyeglasses for the mind's eye they just help de-biased our cognition

32:10.240 --> 32:15.280
so kind of getting that equation right we sort of the art to our science and what fascinates me

32:15.280 --> 32:20.000
about it is that statistics is part of it but not all of it you know so in business we've always

32:20.000 --> 32:23.680
called this kind of change management so this kind of goes into the change management rubric

32:23.680 --> 32:28.240
frankly right now it's in art but I but I like to think that it can become more of a science

32:29.120 --> 32:34.400
so I call this the last mile problem you know we don't stop with an algorithmic output

32:34.400 --> 32:39.120
we stop with the decision in the case of automation the computer makes a decision it's saying

32:39.760 --> 32:43.440
you know I'm just going to send you this ad for these pair of shoes because I think you like these shoes

32:44.240 --> 32:49.520
the augmentation is more like you know I'm going to tell the doctor there's this probability

32:49.520 --> 32:54.160
this person has this rare disease but it's really the doctor's judgment call I'm just going to

32:54.160 --> 32:58.080
and I'm going to tell the doctor why the algorithm thinks this maybe I'll use an information retrieval

32:58.080 --> 33:03.920
system our IBM Watson to give some collateral information but I'm going to give this to the doctor

33:04.560 --> 33:10.160
this is one area where behavioral economics comes back again is that behavioral economics teaches us

33:10.160 --> 33:15.520
that simply giving people information doesn't always result in the optimal decision it's also the way

33:15.520 --> 33:19.760
you present information matters that we've learned this in the last 30 or 40 years this is the

33:19.760 --> 33:23.440
whole basis of the book nut which is only 10 years old so we've really come to appreciate this a

33:23.440 --> 33:28.080
lot more so the so behavioral economics is absolutely a you know one way of thinking of this

33:28.080 --> 33:33.680
quote human factors idea or human centered design idea you know so I feel like we've been sort of

33:33.680 --> 33:39.040
like muddling through perhaps all these years and it works right it's it's it's it's it's more

33:39.040 --> 33:42.640
when I say muddling through I mean it's more an art than a science it's it's something that we've

33:42.640 --> 33:46.880
done for a long time we've gotten better at over time we do it with our with our clients but I

33:46.880 --> 33:51.520
I'm intrigued with the idea that now that AI and machine learning is becoming such a

33:52.160 --> 33:58.960
a business as a sidal trend maybe there can be a new science emerging about this idea of human

33:58.960 --> 34:03.680
computer collaboration or human computer interaction can I give you one more example that sort of like

34:03.680 --> 34:08.320
yes this is not absolutely not a gym example it's not a deloitte example but I find it incredibly

34:08.320 --> 34:12.560
thought-provoking so it's more of a metaphor but it but I find it's a very thought-provoking metaphor

34:12.560 --> 34:16.480
and it's a very nice way to think about sort of the future of work to you know people being

34:16.480 --> 34:20.640
displaced algorithms and so on and forgive me if you've heard this if you heard the sort of

34:20.640 --> 34:26.320
about freestyle chess no I don't think so good thank you I like when people say no I only learned

34:26.320 --> 34:30.160
of a few years ago myself I actually read the article and I forgot it and then I reread it

34:30.800 --> 34:36.640
so I read an article in 2011 by Gary Kasparov the chess grandmaster who's published in the New

34:36.640 --> 34:42.000
York Review of Books in 2011 and he was talking about his own experiences being put out of work

34:42.000 --> 34:47.760
by IBM Deep Blue so this is a prequel to Watson right you know there's like there's a magazine

34:47.760 --> 34:52.400
cover cover called the brain's last stand you know the machine is vanquishing man right the chess

34:52.400 --> 34:56.560
master because that's identified with human intelligence and this is way back in 97 it's like

34:56.560 --> 35:02.400
20 years ago right I mean I thought it was like I just turned 12 I think kidding and but it turned

35:02.400 --> 35:07.600
out the story is a lot more interesting than that Kasparov actually invented a new game after he

35:07.600 --> 35:13.280
lost to Deep Blue called advanced chess an advanced chess would be instead of me playing you in chess

35:13.760 --> 35:17.760
it'd be Jim equipped with a laptop playing you equipped with a laptop and it turns out that the

35:17.760 --> 35:23.680
same skills that enabled Kasparov to be good at traditional chess he wasn't quite as good

35:23.680 --> 35:30.400
as freestyle chess or this advanced chess concept right anyway that's fast forward to the year 2005

35:30.400 --> 35:35.440
and I think a German website had an open game called freestyle chess which is anybody around the

35:35.440 --> 35:41.600
world can enter it can be you know Kasparov playing another grandmaster it can be Kasparov plus

35:41.600 --> 35:44.880
teaming up with a super computer playing another grandmaster teamed up with another super computer

35:44.880 --> 35:51.520
can be anything okay and turns out there's an upset victory the team that one was two amateur

35:51.520 --> 35:56.240
chess players from New Hampshire it's two young guys working with three ordinary laptops equipped

35:56.240 --> 36:00.960
with three different chess programs they won freestyle chess they beat the grandmasters and the

36:00.960 --> 36:05.120
supercomputers and the grandmasters working with the supercomputers supercomputers and Kasparov when he

36:05.120 --> 36:11.120
wrote about this in the New York review of books he said this is a leader called Kasparov's law it's

36:11.120 --> 36:15.360
a weak human plus an ordinary computer plus a better process of working together

36:15.360 --> 36:20.480
outperforms the grandmaster or the supercomputer or both plus an inferior process

36:21.520 --> 36:24.880
and when I'm going to present this this afternoon I'm going to circle the better process

36:24.880 --> 36:30.800
that's what we need and when I when I read that for the second time I'm a slow study you know

36:30.800 --> 36:35.360
when I read this all these years later I realized oh my god that better process of the chess player

36:35.360 --> 36:39.520
working with the computer that's just like what we would do in our consulting practice when we give

36:39.520 --> 36:44.560
like a doctor an underwriter an admissions officer a public sector case worker a list of cases

36:45.280 --> 36:50.400
saying here this will be bias your judgment but it's ultimately up to you and we're going to help

36:50.400 --> 36:54.000
you do this we're going to train you to do it we're going to train you to understand the algorithm

36:54.000 --> 36:57.440
we're going to try to train you to understand you know it's premises it's assumptions the

36:57.440 --> 37:01.840
data it's based on the variables in the model and that way if you know that the the model contains

37:01.840 --> 37:07.200
variables one through forty but you know factors forty one forty two and forty three and if you judge

37:07.200 --> 37:11.600
those to be really important and the algorithm doesn't know that then you you can override the algorithm

37:11.600 --> 37:14.880
and that's okay because you're using your brain you're not just you know using kind of

37:14.880 --> 37:19.600
thinking fast you're not using biased heuristics you're making up using metacognition and using

37:19.600 --> 37:24.160
intelligence to say yeah the computer is saying this but I've also got this common sense of this

37:24.160 --> 37:28.400
other I know these contextual factors I'm going to override it and do this other thing and I

37:28.400 --> 37:33.360
could be wrong but at least it's a principal decision yeah so when I talk about freestyle chess

37:33.360 --> 37:38.640
I'm not trying to make the claim that a human computer will always win chess that's not my point

37:38.640 --> 37:45.760
but the point is that it's a very nice metaphor for this idea that the computer can do things

37:45.760 --> 37:50.320
the humans aren't good at like look through this decision tree of you know you know all these

37:50.320 --> 37:54.560
possible moves and all the implications these moves downstream better than a human can but the

37:54.560 --> 37:59.280
human has other you know kinds of capabilities right it can kind of so it can cast for off comments

37:59.280 --> 38:05.040
when about these two guys who won freestyle chess he said they're insight into looking deeply at

38:05.040 --> 38:09.760
what the computers were indicating it really enabled them to kind of outperform the grand

38:09.760 --> 38:13.200
message in a fair process so they actually had an insight into how the algorithms worked and

38:13.200 --> 38:17.120
developed kind of like an intuitive spidey sense for when should I trust this recommendation versus

38:17.120 --> 38:22.400
that recommendation so I just find it like a very nice metaphor for a real world professional

38:22.400 --> 38:26.880
making a machine critical judgment under it's called judgment under uncertainty with the help

38:26.880 --> 38:34.800
of an algorithm yeah it strikes me that the process in yeah his characterization of this is it's

38:34.800 --> 38:40.720
kind of a lot of vocabulary that has a bunch of individual things under it right there it's like

38:40.720 --> 38:44.960
user interface there's you know the things that we might traditionally think of as a process like

38:44.960 --> 38:52.880
your right steps there's mentioned a bank of experiences to fall back on on how you know

38:53.600 --> 39:00.400
how have I been able to rely on the computer's advice historically have you done or seen any kind of

39:00.400 --> 39:08.800
work to kind of characterize this more more granularly yeah I mean you mean like exactly how do you

39:08.800 --> 39:12.320
pull off this better process you may like what are the steps involved with the principles involved

39:12.320 --> 39:18.160
and so I think ultimately the goal is like as a business if I can you know if I can pick apart the

39:18.160 --> 39:23.600
pieces of what you know process means in this battle that enable you know plus computer to

39:23.600 --> 39:28.800
you know to outperform you know expert then you know that kind of provides me a roadmap for while

39:28.800 --> 39:34.000
first I need to make sure that my data is you know displayed in a way that is comprehensible for the

39:34.000 --> 39:38.880
computer example you know then I need to make sure that I've got the tools available to interact

39:38.880 --> 39:44.560
with you know the systems it's I'm just I'm curious whether you know how evolved the thinking is

39:44.560 --> 39:49.120
there like I said I would like it to be more of a science than it is but I think what ends up

39:49.120 --> 39:53.760
happening is that it's it's it's a series of kind of like what are Simon called satisfying we

39:53.760 --> 39:59.200
make it you know maybe not optimal decisions but we make we kind of like you know look at the

39:59.200 --> 40:02.160
business context maybe never will be a science maybe it's always going to be like it'll always

40:02.160 --> 40:06.960
be like a devil in the details kind of thing you know so like you know if it's a metal if it's

40:06.960 --> 40:12.560
a medical case then the case is where you let the computer just make an automatic decision versus a

40:12.560 --> 40:17.920
human might be different depending on how many doctors are around you know if you're in a poor

40:17.920 --> 40:22.800
country you know it might not be optimal to have a doctor working with a computer but you know if

40:22.800 --> 40:28.160
the village has no doctors at all and you can just like take a picture of a wound you know do deep

40:28.160 --> 40:32.480
learning on and upload it into the cloud and it comes back with like you don't need stitches versus

40:32.480 --> 40:37.840
you do need stitches that's pretty good ideally we'd have a human in the loop you know what I mean

40:37.840 --> 40:41.280
but so that it's asking me like it is modeling through kind of thing like you know what what is

40:41.280 --> 40:46.880
the cut off you know you know in some cases like jurisprudence you know I think Daniel kind of

40:46.880 --> 40:50.400
wrote about this actually a few years ago he said the public would be shocked to hear that like

40:50.400 --> 40:54.400
an algorithm was making decisions without a judge I mean is it even is that even constitutional

40:54.400 --> 40:59.280
so that might be kind of like just like ground you know unavoidable reasons why you always need

40:59.280 --> 41:03.600
to have a human in the loop but I think that you know I think that we are kind of gradually getting

41:03.600 --> 41:08.480
better at this stuff I mean people are coming up with better algorithms for explaining models

41:08.480 --> 41:12.240
like so one of the I'm probably going to get his name wrong one of the speakers early in the

41:12.240 --> 41:16.960
conference Carlos Questren from University of Austin yeah yeah he came up with the Lyme algorithm

41:16.960 --> 41:21.520
right for kind of explaining why does a deep learning model classify what it classifies well

41:21.520 --> 41:26.640
conceptually and I hated to say conceptually it's much more sophisticated but you know we've

41:26.640 --> 41:32.560
always done analogous things with our work right I mean we would we would output not just a score

41:33.520 --> 41:39.600
saying you know the answer is 42 we'd say well what does 42 mean and why does the algorithm think

41:39.600 --> 41:45.120
it's 42 you know so you know so every single score is contextualized with a set of sort of

41:45.120 --> 41:50.160
like English language you know language so again primitive natural language generation but still

41:50.160 --> 41:53.520
nevertheless natural language generation so all these kinds of things we've been doing for a long

41:53.520 --> 41:57.920
time are getting refined you know so we've got better reason algorithms we've got natural

41:57.920 --> 42:02.080
Christmas natural language generation right we've got more advanced data visualization maybe

42:02.080 --> 42:06.000
we're going to come up with better apps so that you know people the emotional aspect of this

42:06.000 --> 42:09.920
important you know John Whalen was speaking yesterday about the emotional quality of this kind of

42:09.920 --> 42:14.480
stuff and he said you know a nice comment he made was that people will choose a personal digital

42:14.480 --> 42:20.320
assistant even if it's the less accurate if it's just more emotionally pleasing to work with

42:20.320 --> 42:24.400
and so even just getting that right is something and that's something that my practice you know

42:24.400 --> 42:29.120
probably could get a little bit better at yeah so these are all different aspects of the human

42:29.120 --> 42:34.480
factors right so it's like some of it is helping us think better but there's a lot of interesting

42:35.040 --> 42:40.080
kind of neuroscience around emotions and I think you know in the last 20 years

42:40.080 --> 42:45.200
so another you know kind of like headline that it's kind of new to a lot of people including me is

42:45.200 --> 42:49.360
that emotions are not kind of like the Mr. Spock versus Captain Kirk thing that we all think of

42:49.360 --> 42:54.560
because it's not like emotions are so like the noise it closes the static rationality it's more

42:54.560 --> 42:58.400
like emotions are sort of part and parcel of the fundamental yeah yeah it's a part and parcel

42:58.400 --> 43:02.560
a big part of what thinking fast that's right thinking slow I'm just messed that up no thinking

43:02.560 --> 43:06.560
fast and slow that's exactly right yeah and also like you know the the effective computing stuff that

43:06.560 --> 43:11.840
Rana L. Kalebi was was speaking about that kind of relates I think you know that that's effective

43:11.840 --> 43:16.080
computing well there's also effective neuroscience and like one of the findings of effective neuroscience

43:16.080 --> 43:21.440
is that healthy emotions are important to rational decision making they're they're not separate you

43:21.440 --> 43:25.520
know and so that's an interesting kind of lens through which to look at this too you know and

43:25.520 --> 43:28.720
again these are these are all these are all developing now so why such an exciting time to be

43:28.720 --> 43:35.280
working in this field so I think the general idea is that I think savvy people have always realized

43:35.280 --> 43:38.960
that when it comes to more complex decisions you don't want to just turn over to an algorithm

43:38.960 --> 43:43.360
sure we're surrounded by more big data now sure our algorithms are getting better sure our

43:43.360 --> 43:48.240
computing power is is getting cheaper and cheaper so sure there will be more and more decisions

43:48.240 --> 43:53.600
that can be automated but until we come up with this kind of singularity which you know

43:53.600 --> 43:57.920
right whatever you know it's not separate podcasts not on that horizon anytime soon yeah we're

43:57.920 --> 44:01.120
you know for a lot of decisions we're going to have kind of humans in the loop we're going to need to

44:01.120 --> 44:06.640
kind of have a science of augmentation this kind of the freestyle x idea so freestyle insurance

44:06.640 --> 44:11.200
underwriting freestyle medicine freestyle jurisprudence freestyle university admissions right it's

44:11.200 --> 44:15.680
the algorithms helping be by us the humans but the humans kind of keeping the algorithms in check

44:15.680 --> 44:19.600
and so getting getting that balance right is that's what I find fascinating that that's what kind

44:19.600 --> 44:24.960
of that's fantastic yeah well I'll mention since you mentioned Carlos Gastran and Ronald

44:24.960 --> 44:29.920
Koyubi I'll note for folks that are listening that both of them have been on the podcast before

44:29.920 --> 44:39.440
great taste until that Carlos at the very first AI conference in New York and Rana at the

44:40.160 --> 44:44.880
previous one in New York this is the third one and so folks can find that find those on the

44:44.880 --> 44:51.440
website both great conversations beautiful and I really enjoy this conversation likewise thank you

44:51.440 --> 45:02.800
so much thank you real pleasure absolutely all right everyone that's our show for today thank you

45:02.800 --> 45:10.240
so much for listening and of course for your ongoing feedback and support for more information on

45:10.240 --> 45:16.640
James or any of the other topics covered in this episode head on over to twomlai.com slash talk

45:16.640 --> 45:23.600
slash 56 and please please please remember to send any questions or comments that you have

45:23.600 --> 45:31.280
either for us or our guests via Twitter at twomlai or at sam charrington or just leave a comment

45:31.280 --> 45:38.160
right on the show notes page for more information about the Halloween social visit twomlai.com slash

45:38.160 --> 45:45.840
Halloween tickets are on sale right now and we do expect a sellout so get your tickets to register

45:45.840 --> 45:53.040
for Wednesday's meetup visit twomlai.com slash meetup thanks again for listening and catch you

45:53.040 --> 46:18.240
next time

