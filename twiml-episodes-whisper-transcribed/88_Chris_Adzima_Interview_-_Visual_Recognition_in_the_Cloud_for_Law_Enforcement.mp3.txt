Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting
people doing interesting things in machine learning and artificial intelligence. I'm your
host Sam Charrington. This week on the podcast, we're featuring a series of conversations
from the AWS Reinvent Conference in Las Vegas. I had a great time at this event, getting caught
up on the new machine learning and AI products and services announced by AWS and its partners.
If you missed the news coming out of Reinvent and want to know more about what one of the
biggest AI platform providers is up to, make sure you check out Monday's show, Twimble Talk
number 83. A roundtable discussion I held with Dave McCrory and Lawrence Chung. We cover
all of AWS's most important news, including the new SageMaker, DeepLens, Recognition Video,
Transcription, Alexa for Business, Greengrass ML inference and more. This week, we're also
running a special listener appreciation contest to celebrate hitting 1 million listens here on
the podcast and to thank you all for being so awesome. Tweet to us using the hashtag Twimble 1
Mill to enter. Every entry gets a fly Twimble 1 Mill sticker, plus a chance to win a limited
run t-shirt commemorating the occasion. We'll be digging into the Magic Twimble swag bag and
giving away some other mystery prizes as well, so you definitely don't want to miss this.
If you're not on Twitter or you want more ways to enter, visit Twimbleai.com slash Twimble 1 Mill
for the full rundown. Before we dive in, I'd like to thank our good friends over at Intel
Nirvana for their sponsorship of this podcast and our reinvent series. One of the big announcements
at reinvent this year was the release of Amazon DeepLens, a fully programmable, deep learning
enabled wireless video camera designed to help developers learn and experiment with AI
both in the cloud and at the edge. DeepLens is powered by an Intel Atom X5 processor, which
delivers up to 100 gigaflops of processing power to onboard applications. To learn more about
DeepLens and the other interesting things Intel's been up to in the AI space, check out intel Nirvana.com.
Okay, in this episode, we're joined by Chris Adzima, senior information analyst for the Washington
County Sheriff's Department. Chris joined me to discuss his very interesting use case for AWS
recognition, their image object detection system, which he uses to help his agency identify criminal
suspects in the Portland area by matching photos to mug shots. We discuss how bias affects the
work he's doing and how they try to remove it from their process, as well as what his next steps
are with the recognition service. This was a pretty interesting discussion and I'm sure you'll enjoy
it. And now on to the show.
All right, everyone. I am here at the AWS Reunment Conference and I've got the pleasure of being
seated with Chris Adzima. Chris is a senior information systems analyst with the Washington
County Sheriff's Office and Washington County, if you don't know is in the Portland area. Is that
right, Chris? Chris, welcome to this week in machine learning and AI. Well, thanks for having me.
It's great to have you here. I am looking forward to learning a little bit about the talk that you
gave yesterday. What was the topic of that one? So it was called the unusual suspects and essentially,
it is about how I used AWS recognition to attempt to identify unknown suspects who committed
crimes. And what I used is previous booking photos or mug shots to upload the recognition
created collection. And now we can search surveillance footage or pictures taken by eyewitnesses
against those booking photos and attempt to identify those people based on if they had stayed
in our jail already. Oh, wow. Oh, wow. Before we get into that, why don't we spend a little bit of
time having you share with us a little bit about your background and how you got interested in
the machine learning and AI stuff to begin with? Sure. I have a pretty extensive background,
goes back about 15 years now where I started at eBay working in fraud detection. And there we
utilized machine learning. I didn't ever make any models because I'm not a data scientist,
but I utilized the models to detect things like people taking over accounts or posting fraudulent
items. Okay. So back then it was very interesting to me. And I kind of grew up with that as part of
my background. When I moved into public service, I started working at the sheriff's office. I wanted
to bring some of that interest into the sheriff's office. And so I started looking at ways we could
utilize machine learning or anything like that in order to help our deputies do their job.
And so one of the biggest areas of opportunity was the fact that we have all of these
videos, all of these pictures of people who committed crimes that we can't identify. And we also
have all of these pictures of people who have been booked into our jail. And I thought there's
got to be a way that we can marry those two situations and come up with a very interesting way to
solve that problem. And so that's kind of where how I grew up and got interested into it.
Okay. Great. Is this the first machine learning project that you've done at the sheriff's office?
Yes. This is the first one. We started looking into other machine learning things and we're
slowly getting into other things like crime detection. And by crime detection, I don't mean
somebody, one specific person committing a crime. I mean, minority report, not like minority report.
More specifically, we noticed that as crime happens in one area, it migrates to another area
over time. And we can utilize that data to predict that there's a possibility that a crime is going
to occur in an area subsequent or bordering. And therefore, we can send out patrols to that area.
Okay. Not to attempt to catch the person, but to attempt to prevent the crime from happening
in the first place. Because that's honestly what our goal is, right? We don't want to catch the
people doing the crime. We want the people to not do the crime in the first place. So that's what
we're hopes are for that. So an application like that is essentially like, you know, how you got this
you know, fleet of cars and officers and other vehicles. Like how do you deploy them?
Right. Right. How do you how do you put them in the in the areas that are going to get the most
prevention for where they are? Okay.
How long have you been at the sheriff's office? Just about two years now. In fact,
is it the 29th? It's the 29th. Yes, it'll be two years tomorrow. Oh wow. Wow.
That'd be anniversary. Yes. Did the I guess I'm curious. Did the you know, so you you used
Amazon's recognition product to do this first application to what degree did you know, having access
to this via an API as opposed to needing to build your own models contribute to your ability to
actually do it? 100%. I guess I kind of do the answer to that question, but I'm not a data scientist.
I don't know how any of the stuff works in the back end. And I'm okay with that. What I what it
really did was allow me somebody who's very big into coding. Some I know I know how to code.
Now I'm able to utilize these these machine learnings, these deep learning techniques
to help me do my job. And I didn't have to engage a data scientist. I didn't have to even
know the model. I just was able to tie into an API and utilize their model that they already
built. And it has been immensely beneficial because of that. Can you talk a little bit about your
process for developing the system? Yeah. So in my talk yesterday, I talked about how quickly
this happened. This was right around this time last year at Reinventing Announced Recognition.
And by mid-November, sorry mid-December, I had had a prototype up and running already,
basically. That's incredible. Yeah, it was really, really amazing. Once I found out
that recognition existed and I did a quick, I think it only took me about a day to go in and
read the documentation because it's not an overly complicated API. I think there's only
something like 20 calls in total. So once I read the documentation, I was able to go in and
realize that, okay, the first thing I need to do is get my mug shots available to recognition,
uploaded 300,000 mug shots into S3. I did it manually. I didn't realize there was an API I could
use. This was when I was really green with AWS. So literally I used the web for my drag and draw
300,000 increments of a thousand out into the web form. Okay. So that took me about five days.
Had I realized now that then that what I know now about the API for S3, I would have just written
the script, uploaded them that way. That's how we keep our mug shots up to date now. I have a daily
script that runs, throws them into S3 and indexes. But at least in mug shots were digital.
In the beginning, in the beginning, it was a manual process. Once I got those up into S3, though,
I did write a script because then I knew that I could wrote a script to loop through all 300,000
indexed them into recognition, which is just one simple API call. Once they were indexed,
I essentially had everything I needed to do to get up and running. Once you have that collection,
you can just do an API call to their search faces API. And you send it the binary data from the
image that you want to search from into the collection and it returns the results.
And what are the images that you want to search from? What are those input images?
So they come from multiple different ways. The web form that I created will post them to S3 and
then S3 will do the search that way. I'll then delete that one because we don't save any of the
images we search for. But then I also have a mobile app. Those will go directly into the API
that way. The source images, though, are these like, is this surveillance video or is this like
an officer on a street with your mobile app taking a picture of someone or in my talk yesterday,
I should three different examples. One of them was from a surveillance. I don't know if people
know this or not, but there are surveillance cameras on those little self-check out things at most
of those department stores. And that was the surveillance camera from that. Yeah. So that was a good
hit. Meaning like the credit card swiper thing? Well, not the self-check out kiosks.
You know, we're going to scan it yourself. So in that case, the guy was scanning the items,
but he didn't actually pay. He just put them back in the cart and walked out, made it seem like
it was legitimate. Okay. So we got a surveillance shot off of that. The second example I gave
was actually a cell phone picture. And I witnessed took a picture of somebody with their cell phone
and then called the deputies. The deputies showed up. And the deputy took a picture of the cell phone
picture. So it was like a second generation with all the glare on the phone and all.
But recognition still found the face and ran that. And then the third example, which is the example
that I am most pleased with, was an artist's rendition from an eyewitness. So a sketch.
Wow. We ran a sketch through recognition and it pulled back a legitimate result. Wow.
So when you say, well, we're running, we're running anything we can. If it has a face on it,
it doesn't matter if it's a drawing or an image of an image of an image, we're trying
because we want to identify these people. And how do you characterize the performance of
recognition for your use case? Do you have specific ways you think about that?
So as far as metrics go not yet because it's still really early. It's only been a year since I
even started. And it's only been about six months that the deputies have been using it in full force.
But I can tell you anecdotally, every single deputy, every single law enforcement officer who's
used this has said, wow, I can't believe that how good this is doing. And it's also difficult
to quantify it because I'd love to get up here and say, you know, this tool has led to
X amount of convictions, right? But it doesn't quite work like that, right? The deputies use it as
a tool. It's not their one and only thing they do. So while a deputy might put something into
the tool, get a result. It may or may not be the person in the picture, but it could lead them
to a relative. Like with facial features being similar, you might get somebody's father return
in the result. You go talk to the father, realize it was the son who was the person you were looking
for. So is that a good hit for us? I don't know how to quantify that yet. But I can tell you that all
of these situations have occurred. So it's working well. Everybody likes it. And I think that's really
all that matters when I'm putting a tool in the hands of a deputy, saving them time so that they can
go out there and keep ourselves safe. When you're thinking about it from the perspective of a developer
or a technologist using this service, how about the performance of recognition itself? Have you?
And you know, I guess you would have to like, you put in an image, I guess you would want to know
like what percent of the time, you know, is the image found when it's in the database. Do you
track that? We did some benchmarking at the very beginning about that because we had images that
were not mug shots, but we knew those people in those images had been booked into our jail at some
point. So we created, I don't want to call it a blind case study, but I didn't know which of the
group had been in our jail and which of the group had not been in our jail. And I didn't know the
identities of the people. Okay. So one of the deputies sent me over about a hundred images.
And of the images of the people who had been in our jail, I was able to correctly identify 75
percent of them. And of the people who had not been in our jail, but results were returned,
I was able to say that if 50 percent of those people, I was able to say, no, we don't have a good
idea for these people. So about 50 percent of the people who had never been in our jail, I said,
oh, I think it's this person and it actually was. But I think when it comes down to a 75 percent
accuracy saying that there's a 75 percent chance that if they had been booked in our jail,
I'm going to be able to tell you who they are. That's tremendous. So yeah, I think it's,
you know, it's like what you said previously, it's a tool, right? And if you think about it in
that context and not like recognition is going to replace policing, then it's helpful.
Yeah. And exactly. And giving the deputies as many tools as possible to go out and do their
jobs is exactly what my job is. And so I don't necessarily want to do their job for them,
although I'd love to do their job. But I don't necessarily want to do their job for them. I want
them to have a tool that they can not have to be sitting in behind a computer doing research,
looking at thousands of mug shots that possibly fit. Instead, they get a likely choice of five or
six and they can go out and take action on those follow up on leads as opposed to being tied to
a computer. Are there things that you need to think about as a developer when using these APIs
that are different from, you know, the traditional ways you might develop applications?
As far as being in law enforcement, yes. I have to concern myself with different levels
of data classifications. So for instance, I, there's laws that prevent us from sending images
of juveniles over the internet without certain security in place. And while those securities do
exist with Amazon, I have to ensure that they are in place before I send a juvenile's image. So
in that case, we just have a policy that we don't run recognition off of juvenile images.
And as far as just a regular old developer that I have been for a while now thinking about these
APIs, I have to think about them in a different way because kind of like what we were just talking
about the effectiveness of the API. Usually if you were to come with me and say this API is 75%
effective, I wouldn't even look at because that's just not beneficial. But when you think about
what it's doing and the fact that 75% beneficial is way better than 10% beneficial, which is where
we were at with scraping through mug shots using a Boolean search essentially. It's way beneficial.
And that's something that you have to wrap your head around as a developer saying that
don't just throw it out because you want to see in high 90s. But elaborate a little bit on
what you were doing before. Previous to having this recognition
product in place, we had a web form where you would go in and you would type in demographics
like I'm looking for a mail between the ages of 4050, height of 5 foot 10, black hair and
down the list of anything that you can search for. And then it would then search for all of the
inmates that meet those requirements and return thousands of mug shots that you would have to
then search through by your eyes to determine. And where those that metadata was all manually
exactly. So it's all based on when you get booked in, ask you questions, how old are you?
Sometimes we have all that information because they give us a driver's license and we can put
all that in. But in some cases, it's all based on what the inmates tell us. And I can tell you
that they foreshore give us different names. They foreshore give us different birthdays.
So yeah, it was very, while it was somewhat accurate, it definitely did have a tilt to it
because of incorrect entered data. Okay. And so you get this list of your thousand search results
and you know, for whatever reason, I guess it's TV. But I'm imagining like these big binders of
mug shots, you're not doing it like that anymore. No, not anymore. Obviously before we had a website
that the deputies could utilize, that's exactly how they did it. And there was binders full of
mug shots. I think it's still like that on TV most years. On TV. And when they when they give
what we call a six pack or a lineup card, it's still right there laid on the table where the
guy points at the picture. Yeah, it's on the computer now too. Oh, really? Yeah. So, but
while it's very different than that, it's actually very similar at the same time where yeah,
you're just slogging through picture after picture. And anybody who knows anything about like
assembly line hypnosis or anything like that, after the fifth page, you're not seeing those
faces like you should anymore. You know, you're just, you know, hoping that there's some huge
like mole on the guy's forehead or something because aside from that, you're not going to see it.
Which is where the you mentioned 10% accuracy rate before where that comes from. So, when I think
about the, you know, some of the issues associated with associated with applying machine learning
in AI to one of the things that comes up for me pretty quickly or some of the, you know, incidents
and stories we've seen around just the, you know, the bias that's injected into these types of
algorithms and how that can kind of play out in, you know, these which are, you know, you know,
really critical, you know, situations that involve human lives. Like, what kind of, how do you
think about that and what kind of experience have you had with those kinds of issues? So, luckily,
we, I'm happy to work for a sheriff who is very conscious about bias in policing.
Well, before any of the other counties around this were thinking about that, he was thinking about
it and running reports to make sure that his deputies weren't biased. I'm happy to say that
all reports look good for us. So, as far as personal or very specific to what I've done,
I don't see it, unfortunately, to say that like, this is how I would handle it or how we do him.
Fortunately, we don't see it, but I do keep that in mind because these algorithms do tend to
to have a little bit of bias. And that bias is based on when the algorithms are trained,
they tend to be trained by the developers. So, the developers are going to, you know,
whoever developed it is going to have their ethnicity, their gender more likely to be detected
than others. So, that's what's the specific effects things that you've seen in your work with
recognition? I haven't necessarily seen a lot of, I don't know, what I would call a misidentity
based on something that I would say that, oh, this is definitely because this person is one
ethnicity, but it's biased towards another ethnicity. I really haven't seen that much.
That being said, we have come across a couple of situations where I would run,
we would run an image through recognition. And it would give us a ethnicity, the results would
be an ethnicity that was contrary to what we personally thought just looking at the image.
But in that case, we still haven't had a situation where we were either proven wrong or right
based on our previous thing, because we haven't identified that person yet.
Do you find that is it equally as effective in identifying women
in, you know, out of the pictures that you've identified or equally as effective in identifying,
you know, all ethnicities or are there kind of shifts and biases in that part?
I would say that when I see results that, for instance, if a result set has multiple
genders in that result set, I will see males put in with a picture of a female, more than I will
see females returned when I give it a picture of a male. So it definitely is more,
I hate to use the word bias because, you know, I don't know how to say this except for that,
when it's all set and done, because it's just a tool that we've created.
If it gives us five results of people who are completely off, then we just kind of
sweep those results aside. But yes, I know you can bias in some ways as a kind of a loaded term,
and especially in my field. But statistically, if we kind of separate those kind of issues
that you are seeing where it is more likely to return male pictures than female pictures,
even when you give it a female picture, for example.
Right. And I find that I've almost looked at those results and saw that the results return.
So if I give recognition of female picture and it returns five results and two of them
are males, I find that those males tend to have more generic features. You know, when you look at
them, you don't see anything that stands out specifically. And so that makes me wonder if that those
people's facial architecture is just simply generic in themselves. And that's why they're getting
returned. I haven't done deep enough into looking at their specific facial analysis, which is one
thing that recognition would allow me to do is I could send that one picture through and it could
tell me what it's likelihood of it being male female, likelihood of it being one age group or
another. I haven't really done that much of a deep dive into those outliers that I've seen
to see if maybe that's what it is, maybe that recognition thinks that this picture of a male
is actually a picture of a female and that's why it's being returned. But I can say that as far as
age goes, I am a 35-year-old guy and every single time I run my picture through it, it thinks I'm 50.
So there's that. And I think that if I maybe lost a beard and maybe didn't have as much
little gray hairs, maybe it wouldn't think of me so old, but yeah, interesting. Are there kind of
continuing on the, are there ways, I'm thinking the right way to get at this question? I'm curious
about you're getting going back to the fact that recognition in a lot of ways is a black box for
you, right? Are there things that you maybe kind of, you know, blind to as a developer that you
might want to have more information about that you've run into? Well, obviously, I'm pretty much
blind to the entire recognition of how it works. I send it faces, it sends me results and I have no
idea how it gets to be, but I would like a way to a train recognition, if not, if they're just for
me, you know, not having training my own collection, but train it by some sort of feedback loop,
say that indeed this was a good hit, but indeed this wasn't a good hit, so that, you know, it can
get smarter. I'd love to see that. I'd love to see a way for me to train it in different ways,
because while I only use one slice of recognition, there are other bits that would be hugely
beneficial to us if they worked in the way we needed them. A great example of that is tattoo
recognition. We have, as well as a catalog of faces, we have a catalog of scars marks and tattoos.
I've seen this on TV also. This is on TV as well. You know, on TV, they're able to say,
oh, here's a picture of his skull tattoo, and here's him over here with his skull tattoo.
Right. That doesn't exist in reality and be in recognition. It doesn't, the API isn't detailed
enough to tell me this is a tattoo of a skull. It can say it's a tattoo, which is great,
but it would be even better if I could take all of my pictures of tattoos, feed them into
the collections in recognition, and then auto-tag those, so that we could have a more standardized
list of tattoos. So if recognition saw a skull, it would always return it as the same spelling of
skull as the same. It would generate some kind of taxonomy of tattoos. Somebody would say skull,
somebody makes a crossbone, somebody may put an eye in there somewhere. So we don't have an easy
way to textually search for if a victim comes in and says their attacker had a skull tattoo on
their chest. We could, if we had recognition already auto-tagging these tattoos, I could go
in search skull and get a list of everybody who has a skull tattoo on their chest that has been
through our jail, possibly ideing them simply by knowing that they had a tattoo, and that would be
obviously immensely, I can see how that'd be powerful, and I can tell you that I was watching a
TV show a couple of days ago, and they took a picture of a guy's side of his head, and they said,
oh, look at this guy's ear, ears are biometrically as identical to fingerprints, so we're just going
to take a picture of his ear and run it through ear recognition. I mean, obviously if they could do
that, I'd love recognition to do that. I brought that up because this is the, again, this is the
thing I come across with the public is they assume we already can do this stuff. They assume, like,
when I talk about recognition with the citizens in our area, half of them didn't realize we couldn't
do facial recognition before. They assume that when we send a picture to the news and say, can you
help me identify this person, that we've already done that part, right? Now luckily we have already
done that part, but before this year, we just didn't exist, and it's pretty funny. In my mind,
like, you know, in my mind, you're still going through these bindings of things, and the public
thinks that you have a, you know, a minority report already established. Exactly. That's interesting.
So that's another thing is I want to, I want to get us to the point where we are at where the
citizens already think we are, right? So that when they watch TV, they watch a TV show like APB,
and they see all of this stuff that doesn't exist yet, that I can at least say, well, I'm working
towards it and getting us there. And that's what, that's my goal. And so what do you,
what's next to get you there? Is it, you know, are you kind of waiting for, are you stuck waiting for
recognition to build out all these features, or is this like inspiration and justification for you
to, you know, go find a data scientist to partner with, or something like that. How do you proceed?
Many different ways. This is definitely not me sitting waiting for something to happen.
This has really energized me and energized everybody on my team to go out and innovate and find
new ways that we can assist the deputies and other law enforcement officers doing their job.
So if it means that we know we want to do something and the only way we're going to be able to do it
is finding a data scientist to partner with, then we'll do that. If it means, you know, talking with
Amazon over and over again until we get something into the product that we need, then that's what it
means. If it means finding an interesting way to use another one of their machine learning tools
that they didn't intend, because I can honestly tell you all of the talks I've had with Amazon,
when they first put out recognition, nobody thought this would be great for recognizing criminals
from mug shots. But now that I do it, it's something that they absolutely think about the law
enforcement applications for all of their future machine learning stuff. So yeah, it's just going
out there and trying to find interesting ways to use the things that we already have access to
and possibly driving that thing into what we need it to be, which is kind of what we've done
with recognition. Now, I won't take any credit for what the AWS guys do because they do great work
and they do it on their own. I'm just saying that once they see that we have a use case there,
they will take that into account and move for us.
When you think about the vast array of services that AWS offers, or when you think about the array
of machine learning and AI services that are offered by not only AWS, but Google and Microsoft,
and a host of other players, do you have a short list of things that you're excited about,
you know, tinkering with and putting the use in your service office?
Absolutely, absolutely. I mean, just this morning, the keynote announced recognition for video.
I don't even know if I'm going to wait till I get home to play with that.
We have so much video in the way of surveillance of crimes and the way of, you know,
video sent in by eyewitnesses that we could utilize to determine, you know, who that person is
and doing that thing. And then there are other use cases that just quickly going through my head,
you know, one of the things that recognition would possibly allow us to do is determine intent.
You know, is somebody intending to do harm? Is somebody intending to do somebody else?
Maybe there's a way to get notifications if a camera sees that.
So just many things I want to play around with that. There's the new, I think it's called deep lens,
which allows you to train a model based on what you show it. Obviously, you can use that
for anything from training it to determine products to training it to determine. I think they showed
record labels, things like that. I think I could train it to do better at finding
the faces of people in those surveillance cameras. Or maybe like we were just talking about,
maybe I can train it to do tattoo detection, right? So that's something I want to play around with.
Then they talked about, I can't remember the name of it now, but the new, they have a new
machine learning platform that allows you to build the models without having to know anything
about data science. And so I'm going to play around with that obviously and see if I can
get a model up and running and maybe talk, like when we were talking about
predictive policing to know where to send deputies because of, you know, maybe on the Fourth of July,
we see a lot of illegal fireworks being set off in a certain area and get a notification. Hey,
why don't you head over to that area? That kind of thing. So lots of stuff I'm excited to play with.
Awesome. Awesome. Well, Chris, thanks so much for taking the time out to chat with me. I enjoyed
learning a bit about your use case and, you know, hearing about this kind of dealing with AI
services from a developer's perspective. I enjoyed it too. Thank you very much for having me.
Thanks. All right, everyone. That's our show for today. Thanks so much for listening and for your
continued feedback and support. For more information on Chris or any of the topics covered in this
episode, head on over to twimmelaii.com slash talk slash 86. To follow along with the AWS
Reinvent series, visit twimmelaii.com slash reinvent. To enter our twimmel one mill contest,
visit twimmelaii.com slash twimmel one mill. Of course, we'd be delighted to hear from you,
either via a comment on the show notes page or via Twitter to at twimmelaii or at Sam Charrington.
Thanks again to until Nirvana for their sponsorship of this series. To learn more about their
role in deep lens and the other things they've been up to, visit intelnervana.com. And of course,
thanks once again to you for listening and catch you next time.
