1
00:00:00,000 --> 00:00:15,920
Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting

2
00:00:15,920 --> 00:00:20,880
people doing interesting things in machine learning and artificial intelligence.

3
00:00:20,880 --> 00:00:23,640
I'm your host Sam Charrington.

4
00:00:23,640 --> 00:00:27,520
This week we have a very special interview for you.

5
00:00:27,520 --> 00:00:31,860
Those of you who've been receiving my newsletter for a while might remember that while in

6
00:00:31,860 --> 00:00:36,960
Switzerland last month, I had the pleasure of interviewing Yuggen Schmiedhuber in his

7
00:00:36,960 --> 00:00:43,000
lab Idzia, which is the Dalmahl Institute for Artificial Intelligence in Lugano, Switzerland,

8
00:00:43,000 --> 00:00:45,440
where he serves as scientific director.

9
00:00:45,440 --> 00:00:51,000
In addition to his role at Idzia, Yuggen is co-founder and chief scientist at Nacense,

10
00:00:51,000 --> 00:00:57,800
a company that's using AI to build large-scale neural network solutions for superhuman perception

11
00:00:57,800 --> 00:01:00,320
and intelligent automation.

12
00:01:00,320 --> 00:01:05,520
This is absolutely the furthest I've ever traveled for an interview, but I was in the

13
00:01:05,520 --> 00:01:09,360
neighborhood, so to speak, and boy was it worth it.

14
00:01:09,360 --> 00:01:14,200
Yuggen is an interesting, accomplished, and in some circles controversial figure in the

15
00:01:14,200 --> 00:01:20,160
AI community, and we covered a ton of really interesting ground in our discussion.

16
00:01:20,160 --> 00:01:24,480
So much so that I couldn't truly unpack it all until I had a chance to sit with it after

17
00:01:24,480 --> 00:01:26,040
the fact.

18
00:01:26,040 --> 00:01:32,160
We talked a bunch about his work on neural networks, especially LSTMs, or long short-term

19
00:01:32,160 --> 00:01:37,120
memory networks, which are a key innovation behind many of the advances we've seen in

20
00:01:37,120 --> 00:01:41,360
deep learning and its application over the past few years.

21
00:01:41,360 --> 00:01:46,000
Along the way, Yuggen walks us through a deep learning history lesson that spans 50

22
00:01:46,000 --> 00:01:50,840
plus years, it was like walking back in time with the three-eyed raven.

23
00:01:50,840 --> 00:01:56,440
I know you're really going to enjoy this one, and by the way, this is definitely a nerd

24
00:01:56,440 --> 00:01:57,680
alert show.

25
00:01:57,680 --> 00:02:01,200
A few key announcements before we jump into the show though.

26
00:02:01,200 --> 00:02:05,600
First off, I want to give a big thank you to our friends at Cloudera for sponsoring this

27
00:02:05,600 --> 00:02:07,120
episode.

28
00:02:07,120 --> 00:02:11,160
You probably think of Cloudera primarily as the Hadoop company, and you're not wrong

29
00:02:11,160 --> 00:02:15,960
for that, but did you know they also offer software for data science and deep learning?

30
00:02:15,960 --> 00:02:17,400
They do.

31
00:02:17,400 --> 00:02:19,640
The idea is pretty simple.

32
00:02:19,640 --> 00:02:24,320
If you work for a large enterprise, you probably already have Hadoop in place, and your Hadoop

33
00:02:24,320 --> 00:02:29,200
cluster is filled with lots of data that you want to use in building your models.

34
00:02:29,200 --> 00:02:35,720
But you still need to easily access that data, process it using the latest open source tools,

35
00:02:35,720 --> 00:02:39,480
and harness bursts of compute power to train your models.

36
00:02:39,480 --> 00:02:43,200
This is where Cloudera's data science workbench comes in.

37
00:02:43,200 --> 00:02:46,800
With data science workbench, Cloudera can help you get up and running with deep learning

38
00:02:46,800 --> 00:02:52,760
without massive new investments by implementing an on-demand self-service deep learning platform

39
00:02:52,760 --> 00:02:55,960
right on your existing CDH clusters.

40
00:02:55,960 --> 00:02:58,840
From a tech perspective, DSW is pretty cool.

41
00:02:58,840 --> 00:03:04,720
It uses Kubernetes to transparently scale workloads across the cluster, supporting our Python

42
00:03:04,720 --> 00:03:10,560
and Scala and deep learning frameworks like TensorFlow, Keras, Cafe, and Theano.

43
00:03:10,560 --> 00:03:17,080
And as of last month's 1.1 release, GPUs on the Hadoop cluster are fully supported.

44
00:03:17,080 --> 00:03:22,120
The folks at Cloudera are so confident that you're going to like what you see, that for a limited

45
00:03:22,120 --> 00:03:27,560
time, they're offering a drone to qualified participants simply for meeting with their

46
00:03:27,560 --> 00:03:32,520
sales representative for a demonstration of the data science workbench.

47
00:03:32,520 --> 00:03:38,080
For your demo and drone, visit twimlai.com slash Cloudera.

48
00:03:38,080 --> 00:03:41,720
And C-L-O-U-D-E-R-A.

49
00:03:41,720 --> 00:03:45,920
Next up, a reminder to register for our online meetup.

50
00:03:45,920 --> 00:03:50,360
If you missed the first meetup, we expect to have the recording online later today.

51
00:03:50,360 --> 00:03:55,960
You'll be able to view it at twimlai.com slash meetup, which is also where you can register

52
00:03:55,960 --> 00:04:00,560
for the next one, which will be held on Wednesday, September 13th.

53
00:04:00,560 --> 00:04:04,600
We're currently finalizing the topic, but if you're registered, we'll notify you as

54
00:04:04,600 --> 00:04:06,200
soon as that's done.

55
00:04:06,200 --> 00:04:11,160
Finally, if you like the podcast, please go ahead and sign up for our newsletter, which

56
00:04:11,160 --> 00:04:14,960
is the best way to keep up to date with me and what's going on on the show.

57
00:04:14,960 --> 00:04:18,760
My picks and thoughts for what's new and important in machine learning and AI for the

58
00:04:18,760 --> 00:04:23,800
week and exclusive promos and giveaways just for newsletter readers.

59
00:04:23,800 --> 00:04:28,840
To subscribe, visit twimlai.com slash newsletter.

60
00:04:28,840 --> 00:04:38,200
And now on to the show.

61
00:04:38,200 --> 00:04:39,200
All right, everyone.

62
00:04:39,200 --> 00:04:45,520
I have the distinct pleasure of being here with J端rgen Schmidt-Huber, who is the scientific

63
00:04:45,520 --> 00:04:52,360
director of the Swiss AI lab Idsia, as well as a professor of Artificial Intelligence

64
00:04:52,360 --> 00:04:58,400
with Usi, the University of Lugano, and Subsi, the University of Applied Science and Art

65
00:04:58,400 --> 00:05:04,840
of Southern Switzerland, as well as being the Chief Scientist for Nacense Company that

66
00:05:04,840 --> 00:05:06,680
he co-founded.

67
00:05:06,680 --> 00:05:14,400
J端rgen has a long history in artificial intelligence and is widely recognized as being a pioneer

68
00:05:14,400 --> 00:05:21,520
in the development of recurrent neural networks and in particular LSTM neural networks, which

69
00:05:21,520 --> 00:05:24,000
we've talked about quite a bit on this podcast.

70
00:05:24,000 --> 00:05:27,840
I'm super excited to be here with you, J端rgen, welcome to this week in machine learning

71
00:05:27,840 --> 00:05:28,840
and AI.

72
00:05:28,840 --> 00:05:31,440
It's my pleasure, Sam.

73
00:05:31,440 --> 00:05:37,280
So I like to get these conversations started by just having you introduce yourself and

74
00:05:37,280 --> 00:05:43,320
tell us a little bit about how you found your way into the field of artificial intelligence.

75
00:05:43,320 --> 00:05:46,080
I can do that.

76
00:05:46,080 --> 00:05:53,080
When I was a boy, I tried to figure out how can I maximize my impact and then quickly

77
00:05:53,080 --> 00:05:59,400
it became clear to me that I have to build something that learns to become smarter than

78
00:05:59,400 --> 00:06:07,400
myself, such that I can retire and such that this smarter thing is going to further self-improve

79
00:06:07,400 --> 00:06:11,600
and solve all the problems that I cannot solve.

80
00:06:11,600 --> 00:06:17,120
And that's what I've been trying to do for the past three decades or so.

81
00:06:17,120 --> 00:06:21,680
So you've got a lot going on, I mentioned a bit of it in the intro.

82
00:06:21,680 --> 00:06:25,760
Tell us a little bit about how you spend your time and some of the research efforts that

83
00:06:25,760 --> 00:06:27,800
keep you occupied.

84
00:06:27,800 --> 00:06:35,080
In many ways, I'm still doing the same thing that I've started to publish on in 1987,

85
00:06:35,080 --> 00:06:41,480
which is both this general purpose learning machine, which learns to better and better understand

86
00:06:41,480 --> 00:06:48,800
the world and learns to exploit that knowledge about the world to become a better and better

87
00:06:48,800 --> 00:06:53,200
and more and more general problem solver.

88
00:06:53,200 --> 00:07:00,280
Over the years and decades, we have made a couple of insights related to fundamental building

89
00:07:00,280 --> 00:07:07,000
blocks that you need to build this full AGI system, but we are not yet done.

90
00:07:07,000 --> 00:07:13,840
And some of the puzzle pieces still have to fall in place in a way that we are trying

91
00:07:13,840 --> 00:07:19,360
to push both at the Institute for Artificial Intelligence, at the Swiss AI Lab.

92
00:07:19,360 --> 00:07:24,680
It's here here and in my company, Nasens, where the goal is really to build a general

93
00:07:24,680 --> 00:07:32,880
purpose AI that learns to do not only one thing in a particular domain, but then learns

94
00:07:32,880 --> 00:07:37,840
on top of that to learn a new profession in another domain.

95
00:07:37,840 --> 00:07:43,520
And then on top of that, yet another thing, such that it becomes more and more general problems

96
00:07:43,520 --> 00:07:49,840
over and even learns the learning algorithm itself, such that it's not always stuck with

97
00:07:49,840 --> 00:07:55,520
the same initial learning algorithm, but learns to improve that method itself, such that

98
00:07:55,520 --> 00:08:00,480
learns to improve the way it improves itself and so on.

99
00:08:00,480 --> 00:08:06,400
And you can imagine that on the way to that goal, there are all kinds of little subproblems

100
00:08:06,400 --> 00:08:12,880
coming up and we always focus on the most promising ones.

101
00:08:12,880 --> 00:08:18,560
Sometimes you don't have to build a full AGI to solve interesting problems.

102
00:08:18,560 --> 00:08:25,600
Sometimes just better patterned recognition will do the job in certain applications and

103
00:08:25,600 --> 00:08:31,320
so patterned recognition, which is a tiny part of the full AI thing, that is something

104
00:08:31,320 --> 00:08:35,080
that has become really commercial in recent years.

105
00:08:35,080 --> 00:08:39,800
Some of the techniques that we have developed in the past decades are now really useful

106
00:08:39,800 --> 00:08:46,080
in commercial applications, such as speech recognition and machine translation and all

107
00:08:46,080 --> 00:08:48,760
kinds of patterned recognition.

108
00:08:48,760 --> 00:08:51,040
So what is the daily procedure?

109
00:08:51,040 --> 00:08:54,040
There is no recipe for that.

110
00:08:54,040 --> 00:09:01,080
You try to make progress here and there and there and sometimes you seem to be in a

111
00:09:01,080 --> 00:09:02,080
dead end.

112
00:09:02,080 --> 00:09:06,920
In fact, most of the time you are in a dead end, but then you backtrack and after 100 dead

113
00:09:06,920 --> 00:09:13,240
ends as and again some sort of progress and that's worth it because from there, then

114
00:09:13,240 --> 00:09:21,760
new search paths are opening up and new dead ends, but also new progress is coming in.

115
00:09:21,760 --> 00:09:29,160
I get the impression that a lot of contemporary machine learning and AI researchers are not

116
00:09:29,160 --> 00:09:34,920
driving towards a vision of AGI, they're solving specific point problems.

117
00:09:34,920 --> 00:09:42,120
Do you feel that pursuing your research in the context of trying to create a generalized

118
00:09:42,120 --> 00:09:47,440
AGI gives you a different perspective or heads you down a different path or gives you

119
00:09:47,440 --> 00:09:49,280
a different approach?

120
00:09:49,280 --> 00:09:55,280
The goal of AGI certainly made a difference to us.

121
00:09:55,280 --> 00:10:01,640
First of all, if you want to build a general purpose learning machine, then you will need

122
00:10:01,640 --> 00:10:09,120
a general purpose computational architecture and in our field of neural networks, that means

123
00:10:09,120 --> 00:10:16,240
a recurrent neural network with feedback connections, which is much more powerful than what most

124
00:10:16,240 --> 00:10:22,120
of my colleagues have been studying most of the time, which are so-called feed forward

125
00:10:22,120 --> 00:10:25,600
neural networks, which don't have feedback connections.

126
00:10:25,600 --> 00:10:30,280
The difference between a recurrent network and a feed forward network is a bit like the

127
00:10:30,280 --> 00:10:36,520
difference between a general purpose computer and a mere calculator because on a general

128
00:10:36,520 --> 00:10:44,400
purpose, a recurrent network, you can run arbitrary sequential parallel programs as opposed to

129
00:10:44,400 --> 00:10:49,480
a feed forward network where you just can shift information from an input to the next

130
00:10:49,480 --> 00:10:56,960
layer and then the next and finally you have a result that is determined through a very

131
00:10:56,960 --> 00:11:01,960
limited feed forward computation where many things that you know from traditional computer

132
00:11:01,960 --> 00:11:04,760
signs are not possible.

133
00:11:04,760 --> 00:11:10,640
So to build a general AGI within your networks, you have to start with a recurrent neural

134
00:11:10,640 --> 00:11:12,240
networks.

135
00:11:12,240 --> 00:11:19,200
And the work that we have been doing in the past decades really has focused on that.

136
00:11:19,200 --> 00:11:26,000
In the late 80s, I started to work on these recurrent networks, which are more challenging

137
00:11:26,000 --> 00:11:31,600
than the feed forward networks because you have to deal with the space of our possible

138
00:11:31,600 --> 00:11:35,160
programs that you can implement on a general purpose computer.

139
00:11:35,160 --> 00:11:43,560
So basically, you have to search in the space of all programs to find solutions to problems.

140
00:11:43,560 --> 00:11:50,600
This makes it hard and soon we ran into problems with that approach and the traditional recurrent

141
00:11:50,600 --> 00:11:56,160
networks didn't really work well and so we had to come up with a couple of improvements

142
00:11:56,160 --> 00:12:04,080
such that you can really use and exploit this power, the theoretical power of recurrent

143
00:12:04,080 --> 00:12:05,840
neural networks.

144
00:12:05,840 --> 00:12:11,760
Before we dive into that, I'd like to further explore this notion of a RNN as a general

145
00:12:11,760 --> 00:12:14,320
computer, as general compute framework.

146
00:12:14,320 --> 00:12:19,040
I think that's not obvious to a lot of people and as an example, I don't know if you've

147
00:12:19,040 --> 00:12:24,200
ever heard of this, FizzBuzz with TensorFlow, does that mean anything to you?

148
00:12:24,200 --> 00:12:26,880
TensorFlow means something, but FizzBuzz does not.

149
00:12:26,880 --> 00:12:33,920
FizzBuzz is a common programmer interview question and you tell the candidate to write

150
00:12:33,920 --> 00:12:41,600
a program that basically, I forget the specifics, but it's something like Prince Fizz when it

151
00:12:41,600 --> 00:12:45,400
runs through a loop of numbers and Prince Fizz when the number is divisible by three and

152
00:12:45,400 --> 00:12:49,600
buzz when the number is divisible by five and FizzBuzz when the number is divisible

153
00:12:49,600 --> 00:12:50,600
by 15.

154
00:12:50,600 --> 00:12:56,760
And so if you know how to do a loop and use mod, it's very easy to do, and someone with

155
00:12:56,760 --> 00:13:03,080
some experience in deep learning in TensorFlow got asked this question on an interview and

156
00:13:03,080 --> 00:13:07,600
they decided to, it inspired them to try to figure out how they would accomplish this

157
00:13:07,600 --> 00:13:09,000
with deep learning.

158
00:13:09,000 --> 00:13:15,600
And in fact, I think the result of their experimentation was that this thing that's

159
00:13:15,600 --> 00:13:23,280
extremely, extremely simple to do with common general purpose programming is very, very difficult

160
00:13:23,280 --> 00:13:25,240
to do with neural networks.

161
00:13:25,240 --> 00:13:30,120
You have to come up with a lot of data, you have to train these networks and they still,

162
00:13:30,120 --> 00:13:33,120
because it's probabilistic, they still get it wrong.

163
00:13:33,120 --> 00:13:38,640
And so with that as context, I'd love to hear you further explore this idea of RNNs

164
00:13:38,640 --> 00:13:41,640
as a general purpose computer.

165
00:13:41,640 --> 00:13:42,640
Yes.

166
00:13:42,640 --> 00:13:50,040
Now, first of all, how do you see that an RNN, a recurrent neural network, is a general

167
00:13:50,040 --> 00:13:53,720
purpose computer, as general as your laptop?

168
00:13:53,720 --> 00:14:02,600
Well, you can take some parts of the recurrent network and wire them up as NAND gates, not

169
00:14:02,600 --> 00:14:07,200
AND gates, NAND gates, very simple, very tiny little subnetworks.

170
00:14:07,200 --> 00:14:11,400
And then you just have to recall that the...

171
00:14:11,400 --> 00:14:12,400
That's what your computer is.

172
00:14:12,400 --> 00:14:22,720
And the CPU of your phone or of your laptop can be emulated by a network of NAND gates.

173
00:14:22,720 --> 00:14:30,040
It essentially is a network of NAND gates, which means you can essentially emulate your

174
00:14:30,040 --> 00:14:35,640
laptop and whatever program is running on it on a vicon network.

175
00:14:35,640 --> 00:14:43,880
In a certain sense, your microchip in your phone is just a very sparsely connected recurrent

176
00:14:43,880 --> 00:14:44,880
network.

177
00:14:44,880 --> 00:14:46,400
So that's step number one.

178
00:14:46,400 --> 00:14:53,840
Let me give you an even simpler example where you see how elegant and powerful recurrent

179
00:14:53,840 --> 00:14:58,280
networks can be in comparison to feet forward networks.

180
00:14:58,280 --> 00:15:01,960
Let's look at the problem of parity.

181
00:15:01,960 --> 00:15:05,240
Suppose somebody gives you N bits of information.

182
00:15:05,240 --> 00:15:13,680
0, 111, 0, and you should classify that as to whether the number of ones in that sequence

183
00:15:13,680 --> 00:15:15,920
is even or odd.

184
00:15:15,920 --> 00:15:19,040
So that's the parity problem.

185
00:15:19,040 --> 00:15:26,240
Now you can indeed wire up a feet forward network, which has, say, 10 different input units

186
00:15:26,240 --> 00:15:34,240
to implement this mapping, which maps the input string to a decision either this is an

187
00:15:34,240 --> 00:15:38,720
odd number of ones or an even number of ones.

188
00:15:38,720 --> 00:15:42,760
And it will take you a rather big network with lots of connections.

189
00:15:42,760 --> 00:15:48,360
And it will never generalize to 11 bits or 12 or something because the size, the input

190
00:15:48,360 --> 00:15:50,240
size is so limited.

191
00:15:50,240 --> 00:15:56,600
Now, with a recurrent network, you can solve the parity problem much more elegantly.

192
00:15:56,600 --> 00:16:02,120
You just feed in the bits one by one into a recurrent network that just has one input

193
00:16:02,120 --> 00:16:08,560
unit and one internal hidden unit and one output unit and maybe an additional bias input

194
00:16:08,560 --> 00:16:10,440
unit, if you know what that is.

195
00:16:10,440 --> 00:16:16,040
And then you have just five connections in this little network and you feed in the input

196
00:16:16,040 --> 00:16:23,200
string, 1 0, 111, 1 by 1 and all the network has to learn to become a flip flop because

197
00:16:23,200 --> 00:16:31,840
whatever, whenever a new one is appearing, then the internal unit that represents what

198
00:16:31,840 --> 00:16:35,480
the network has seen so far just has to flip it state.

199
00:16:35,480 --> 00:16:40,840
And that determines then is the current number of ones, is it odd or even.

200
00:16:40,840 --> 00:16:45,520
So this program for the recurrent network is so simple.

201
00:16:45,520 --> 00:16:52,200
It fits into five connections and you can easily guess those connections.

202
00:16:52,200 --> 00:16:59,000
You just do 1,000 random guesses for the five weights, something between minus 10 and

203
00:16:59,000 --> 00:17:06,920
plus 10 and you test the result on a tiny little validation set, maybe three patterns,

204
00:17:06,920 --> 00:17:09,360
three different parity problems.

205
00:17:09,360 --> 00:17:14,640
And if it works on that, you can be almost sure that it's going to generalize to all possible

206
00:17:14,640 --> 00:17:19,760
parity problems, not only those where there are 10 bits coming in, but also those where

207
00:17:19,760 --> 00:17:25,480
there are five bits coming in, but also those where there are 5,000 bits coming in.

208
00:17:25,480 --> 00:17:33,280
So the natural and elegant sequential solution to the parity problem easily fits into a simple

209
00:17:33,280 --> 00:17:38,680
recurrent network and it needs a really awkward, complicated feed forward network to solve

210
00:17:38,680 --> 00:17:43,160
a tiny part of that for just exactly 10 bits, say, or 15 bits.

211
00:17:43,160 --> 00:17:49,520
So this is to illustrate that the difference between a recurrent neural network and the

212
00:17:49,520 --> 00:17:56,720
feed forward network is like the difference between a general purpose, computational architecture

213
00:17:56,720 --> 00:18:00,520
and a mere calculator.

214
00:18:00,520 --> 00:18:07,880
Is that to say then that the referring RNNs as a general purpose computing system is way

215
00:18:07,880 --> 00:18:12,760
more general purpose than feed forward, but we still have a ways to go in our ability

216
00:18:12,760 --> 00:18:17,960
to train these things in order for them to be truly as general as our current computing

217
00:18:17,960 --> 00:18:19,440
architectures.

218
00:18:19,440 --> 00:18:23,720
Maybe another way to ask that question is, what's the fly in the ointment here?

219
00:18:23,720 --> 00:18:30,040
What are the limitations in the way we deal with these RNNs that prevent us from using them

220
00:18:30,040 --> 00:18:34,360
as general purpose computing system as you kind of assert?

221
00:18:34,360 --> 00:18:35,360
Yeah.

222
00:18:35,360 --> 00:18:41,320
So it's one thing to have a general purpose computing architecture and it's another thing

223
00:18:41,320 --> 00:18:48,840
to run the programs running on that architecture, which are solving your favorite programs.

224
00:18:48,840 --> 00:18:54,880
So to put that into an example, so we know that these RNNs are basically functions to

225
00:18:54,880 --> 00:18:59,920
transform inputs and outputs, and so there's some set of inputs that produces the output

226
00:18:59,920 --> 00:19:06,360
of your arbitrary software program, whether that's Mac OS or what have you, but learning

227
00:19:06,360 --> 00:19:10,840
that function based on the inputs is a whole different story.

228
00:19:10,840 --> 00:19:12,760
It's a whole different story.

229
00:19:12,760 --> 00:19:17,680
The learning of the programs running on your general purpose devices on your recurrent

230
00:19:17,680 --> 00:19:21,760
neural networks, that is the really interesting part.

231
00:19:21,760 --> 00:19:28,800
And people have tried it for a long time, but only with certain tweaks to the original

232
00:19:28,800 --> 00:19:35,080
concept of recurrent networks, it became feasible in practice, and it became so powerful

233
00:19:35,080 --> 00:19:38,840
that it's now all over the place in every smartphone.

234
00:19:38,840 --> 00:19:43,760
So we're going to talk about that in one second, but I'd like you to provide a little bit

235
00:19:43,760 --> 00:19:46,640
of historical context for RNNs.

236
00:19:46,640 --> 00:19:52,000
You started working on these at a time when feed-forward networks were considered the state

237
00:19:52,000 --> 00:19:59,080
of the art, and you've seen the way, you've seen the evolution of RNNs, tells a little

238
00:19:59,080 --> 00:20:01,480
bit about that history.

239
00:20:01,480 --> 00:20:07,960
So maybe let's start with the history of deep feed-forward networks, which go back to

240
00:20:07,960 --> 00:20:08,960
1965.

241
00:20:08,960 --> 00:20:12,880
When I was a baby, I was two years old back then.

242
00:20:12,880 --> 00:20:19,400
And there was this famous mathematician, Eva Knenko, in the Ukraine, and with his student

243
00:20:19,400 --> 00:20:25,480
Lapa, they published this method for training deep feed-forward networks.

244
00:20:25,480 --> 00:20:27,200
They didn't even call them neural networks.

245
00:20:27,200 --> 00:20:31,360
They called it the group method of data handling, but that's what this stuff was.

246
00:20:31,360 --> 00:20:36,680
It was deep multilayer perceptrons, and they found a way of learning internal representations

247
00:20:36,680 --> 00:20:43,880
in these deep multilayer perceptrons, and by 1970, they already had networks with seven

248
00:20:43,880 --> 00:20:47,880
eight layers, and many people built on that later.

249
00:20:47,880 --> 00:20:52,200
And deep in there, at that time, was on the order of ten?

250
00:20:52,200 --> 00:20:53,960
On the order of ten, yes.

251
00:20:53,960 --> 00:20:59,320
And even by the standards of the new millennium, this was deep.

252
00:20:59,320 --> 00:21:04,480
Because most people in the early 2000s didn't use networks that were as deep as those

253
00:21:04,480 --> 00:21:05,480
of Eva Knenko.

254
00:21:05,480 --> 00:21:07,000
That's true.

255
00:21:07,000 --> 00:21:09,800
And he did that 1965.

256
00:21:09,800 --> 00:21:17,440
That was four years before Minskia and Papad wrote a book about the limitations of shallow

257
00:21:17,440 --> 00:21:23,720
networks with one single layer, which is called perceptron, and which is sometimes credited

258
00:21:23,720 --> 00:21:30,680
for killing neural network research in America, because people thought, oh, if these eyes

259
00:21:30,680 --> 00:21:36,720
are so limited, then we shouldn't further progress there, or we shouldn't further work

260
00:21:36,720 --> 00:21:38,040
on that.

261
00:21:38,040 --> 00:21:43,280
And maybe it was a cold war thing over there, on the other side of the Iron Curtain, I

262
00:21:43,280 --> 00:21:48,120
think this was the Soviet Union, wasn't the Ukraine, and was the Soviet Union.

263
00:21:48,120 --> 00:21:52,480
And there were these people who had already deep learning networks back then.

264
00:21:52,480 --> 00:21:59,920
However, then in the 80s, the more general attempts came where you really tried to work

265
00:21:59,920 --> 00:22:05,480
with recur networks, with general-purpose computers, not just feed-forward networks, and you want

266
00:22:05,480 --> 00:22:09,480
to have these recur networks for all kinds of interesting applications, such as, for

267
00:22:09,480 --> 00:22:15,520
example, speech recognition or video recognition, everything where there are sequences.

268
00:22:15,520 --> 00:22:19,920
For example, in the video, you don't just have one single image, which you want to classify.

269
00:22:19,920 --> 00:22:24,000
No, you have a whole stream of images, and every few milliseconds, a new image is coming

270
00:22:24,000 --> 00:22:30,720
in, which means your network somehow has to memorize what it saw before in order to make

271
00:22:30,720 --> 00:22:36,800
sense of the current input within the temporal context of before.

272
00:22:36,800 --> 00:22:43,720
So it has to learn to memorize the important stuff, and to ignore the unimportant noise.

273
00:22:43,720 --> 00:22:50,840
And this proved to be very challenging, and people then already in the 80s realized that

274
00:22:50,840 --> 00:22:56,440
the first attempts at recur networks didn't really work well.

275
00:22:56,440 --> 00:23:00,560
And what specifically didn't work well about them?

276
00:23:00,560 --> 00:23:07,560
They especially failed when there were long time lags between relevant input events.

277
00:23:07,560 --> 00:23:14,840
For example, in speech recognition, suppose somebody says 11, and another guy says 7.

278
00:23:14,840 --> 00:23:17,440
And then the end of that is always 7.

279
00:23:17,440 --> 00:23:24,440
So to see the difference, you have to memorize that he says 50 times steps ago, because

280
00:23:24,440 --> 00:23:30,240
Evan by itself already consumes 50 times steps more or less, because roughly every 10

281
00:23:30,240 --> 00:23:33,440
milliseconds, a new input vector is coming from the microphone.

282
00:23:33,440 --> 00:23:36,480
So you have to look back 50 steps.

283
00:23:36,480 --> 00:23:41,200
And in many other applications, for example, as you are listening to me now, to make sense

284
00:23:41,200 --> 00:23:44,160
of what I'm saying, you have to look much further back in time.

285
00:23:44,160 --> 00:23:49,560
You have to think back of the beginning of our discussion to make sense of what I'm

286
00:23:49,560 --> 00:23:50,720
saying now.

287
00:23:50,720 --> 00:23:58,360
So hundreds of thousands and millions and maybe billions of steps, you have to look back

288
00:23:58,360 --> 00:24:01,760
to take into account the temporal context.

289
00:24:01,760 --> 00:24:06,080
And it turned out that these original networks are the 80s.

290
00:24:06,080 --> 00:24:13,320
They could look back only for five steps, six, seven, something like that.

291
00:24:13,320 --> 00:24:19,120
So it was completely insufficient for doing interesting things.

292
00:24:19,120 --> 00:24:24,200
And this is a problem that can be called the vanishing gradient problem.

293
00:24:24,200 --> 00:24:28,040
And then we try to figure out what is the reason for that.

294
00:24:28,040 --> 00:24:33,840
And my first student in 1991 was Seb Hochreiter.

295
00:24:33,840 --> 00:24:37,640
My first student, Evan, and his task was to figure out where's the problem?

296
00:24:37,640 --> 00:24:38,640
Were you here at the time?

297
00:24:38,640 --> 00:24:39,640
Or...

298
00:24:39,640 --> 00:24:41,640
And we were back then, not in Switzerland, we were in Munich.

299
00:24:41,640 --> 00:24:42,640
Okay.

300
00:24:42,640 --> 00:24:48,400
At the Tech University Munich, T. U. M端nchen.

301
00:24:48,400 --> 00:24:55,080
And Seb in his thesis, in his diploma thesis, showed that the problem is that these errors

302
00:24:55,080 --> 00:25:01,360
signals that you are getting through backpropagation in these, we can't do our networks, as you

303
00:25:01,360 --> 00:25:07,040
are propagating them backwards from the output layer towards the previous layer and then

304
00:25:07,040 --> 00:25:10,120
the layer before the previous layer and so on and so on.

305
00:25:10,120 --> 00:25:20,440
They get smaller and smaller in a way that is catastrophic in the sense that the shrinking

306
00:25:20,440 --> 00:25:23,520
happens in an exponential fashion.

307
00:25:23,520 --> 00:25:29,080
So as these signals which tell the system how to change its connections in order to become

308
00:25:29,080 --> 00:25:34,680
a better system, as these signals are being propagated back into time, so to speak, they

309
00:25:34,680 --> 00:25:42,880
are getting smaller and smaller in an exponential way or they explode in an exponential way.

310
00:25:42,880 --> 00:25:48,400
So either the gradients, for those who know what that is, they explode or they vanish and

311
00:25:48,400 --> 00:25:53,520
in both cases, these traditional recon networks cannot learn anything.

312
00:25:53,520 --> 00:25:54,520
So just...

313
00:25:54,520 --> 00:26:01,000
At least they cannot learn to look further back in time than say just a few time steps.

314
00:26:01,000 --> 00:26:02,000
Right.

315
00:26:02,000 --> 00:26:06,400
So just to try to paraphrase that to make sure that I'm understanding.

316
00:26:06,400 --> 00:26:12,160
So the way we solve these deep neural networks is to use backpropagation basically to start

317
00:26:12,160 --> 00:26:20,960
with the output and work our way back to the input layers and we do that using gradients

318
00:26:20,960 --> 00:26:29,760
which essentially, you know, let's say modulate the error that is propagated out to the output.

319
00:26:29,760 --> 00:26:35,360
But if you go back far enough in your network or if your network is too deep, then you're

320
00:26:35,360 --> 00:26:41,200
not getting enough signal back towards the beginning of your network to make the necessary

321
00:26:41,200 --> 00:26:42,680
corrections to improve.

322
00:26:42,680 --> 00:26:43,680
Yes.

323
00:26:43,680 --> 00:26:45,080
That basically what you found.

324
00:26:45,080 --> 00:26:46,080
Yes.

325
00:26:46,080 --> 00:26:48,840
Let's maybe quickly step back a few decades.

326
00:26:48,840 --> 00:26:57,360
In 1970 there was this finished guy, Seppel-Linainmar in Helsinki and he described for the first

327
00:26:57,360 --> 00:27:05,320
time the modern version of what is now called backpropagation, which is a method for adjusting

328
00:27:05,320 --> 00:27:13,160
a deep network of nodes, of computational nodes, such that you can figure out for each

329
00:27:13,160 --> 00:27:19,840
of these nodes and for each of these connections how much did this connection contribute to the

330
00:27:19,840 --> 00:27:24,080
error that you observed at the output of the network.

331
00:27:24,080 --> 00:27:29,680
So at the output of the network, the network is computing some result which is different

332
00:27:29,680 --> 00:27:34,320
from what it should have computed and the difference is called the error.

333
00:27:34,320 --> 00:27:39,760
And now you want to figure out for each connection in your system how much did this particular

334
00:27:39,760 --> 00:27:46,440
connection contribute to this error out there and you want to change then the connection

335
00:27:46,440 --> 00:27:49,720
such that the total error gets smaller.

336
00:27:49,720 --> 00:27:55,560
And Seppel-Linainmar, this guy in Finland, who was a master student back then, he wrote

337
00:27:55,560 --> 00:28:02,720
down this technique of automatic differentiation or today it's called backpropagation.

338
00:28:02,720 --> 00:28:09,880
And then people started using that in the 80s for training neural networks.

339
00:28:09,880 --> 00:28:15,960
And then also generalized it to the case of recon neural networks where you have feedback

340
00:28:15,960 --> 00:28:24,040
connections and where you still have the same basic approach as you are trying to compute

341
00:28:24,040 --> 00:28:29,160
for each connection in a system how much did it contribute to the error at the output side

342
00:28:29,160 --> 00:28:31,320
at some later point in time.

343
00:28:31,320 --> 00:28:38,560
You are moving backwards from the output layer to watch the time step before the computation

344
00:28:38,560 --> 00:28:43,120
of what you find in the output layer and then there are certain things which are called

345
00:28:43,120 --> 00:28:49,440
error signals are being propagated back and then from there they go one step further back

346
00:28:49,440 --> 00:28:55,000
and then from there one step further back and so on until they basically traverse the

347
00:28:55,000 --> 00:29:00,680
entire network in a way that allows you to compute for each connection in the system

348
00:29:00,680 --> 00:29:04,160
how much was it responsible for the final error.

349
00:29:04,160 --> 00:29:10,560
And then through the work of Seppel-Heiter, my first student, 1991 in his diploma thesis

350
00:29:10,560 --> 00:29:18,400
it became clear that the gradients are vanishing these error signals that are being propagated

351
00:29:18,400 --> 00:29:27,920
back they are quickly vanishing which means that although the whole idea is nice in principle

352
00:29:27,920 --> 00:29:32,600
it doesn't work in practice because for a quickly you don't have any signal any longer

353
00:29:32,600 --> 00:29:38,080
which allows you to improve your network such that it becomes a better network.

354
00:29:38,080 --> 00:29:44,000
Where or when did gradient descent come into play in all this as a method for solving

355
00:29:44,000 --> 00:29:46,200
these types of networks.

356
00:29:46,200 --> 00:29:54,600
Seppel-Heiter in my 1970 formulated it as a very general method for all kinds of networks

357
00:29:54,600 --> 00:29:58,600
consisting of differentiable nodes and that paper where he talks about backpropagation

358
00:29:58,600 --> 00:30:04,360
was he also applying gradient descent and did he introduce that then as the method

359
00:30:04,360 --> 00:30:11,400
or gradient descent is a much older technique which goes back at least to Hadamard around

360
00:30:11,400 --> 00:30:13,560
1900 or something like that.

361
00:30:13,560 --> 00:30:16,840
So these are gradient descent is a very old technique.

362
00:30:16,840 --> 00:30:23,200
And then the main question is how do you efficiently compute gradients in complex systems such

363
00:30:23,200 --> 00:30:28,240
as these complex networks where one node is computing something that depends on the

364
00:30:28,240 --> 00:30:33,640
activations of other nodes and then it's broadcasting it's on result to all kinds

365
00:30:33,640 --> 00:30:38,840
of other computational nodes which again compute something you based on these broadcasts

366
00:30:38,840 --> 00:30:43,920
from all the other nodes and then send their results even further and so on.

367
00:30:43,920 --> 00:30:50,640
So once you have a very complex system like that then how do you do the credit assignment

368
00:30:50,640 --> 00:30:56,800
how do you figure out how much should you change this connection there which was active

369
00:30:56,800 --> 00:31:02,600
maybe 100 time steps ago which nevertheless had an impact on what happened 100 time

370
00:31:02,600 --> 00:31:05,360
steps later at the output side.

371
00:31:05,360 --> 00:31:10,840
So that credit assignment problem is essentially is how we find these weights or that's the

372
00:31:10,840 --> 00:31:13,920
problem of finding these the weights for your network.

373
00:31:13,920 --> 00:31:19,960
Yes gradient descent is a very general technique a super general concept from more than a

374
00:31:19,960 --> 00:31:27,800
century ago which can be used to credit a sign in complex systems like these recolonial

375
00:31:27,800 --> 00:31:33,000
networks the individual components of the networks which are the connections and the

376
00:31:33,000 --> 00:31:37,360
weights on these connections so each of these connections has a little number on it which

377
00:31:37,360 --> 00:31:42,840
says how strongly does this neuron over here influence this neuron over here at the next

378
00:31:42,840 --> 00:31:52,680
time step and the numbers maybe 0.2 or minus 0.5 or minus 6.2 and it's contributing to the

379
00:31:52,680 --> 00:31:58,520
error at the output side which was maybe observed a thousand steps later.

380
00:31:58,520 --> 00:32:04,280
And now the question is how can I trace back this influence and how can I then change

381
00:32:04,280 --> 00:32:08,880
this connection such that becomes a better connection and that my entire network becomes

382
00:32:08,880 --> 00:32:13,920
a better network such that the error gets smaller.

383
00:32:13,920 --> 00:32:19,040
And then the efficient way of computing these gradients through automatic differentiation

384
00:32:19,040 --> 00:32:25,720
or backpublication as it is called today that was introduced in 1970 by Sepulina Inma but

385
00:32:25,720 --> 00:32:30,920
then he praised that as a very general thing and he didn't apply it to neural networks

386
00:32:30,920 --> 00:32:38,040
that was then done by other people in particular Paul Werbers around 1982 when he really applied

387
00:32:38,040 --> 00:32:40,760
it to neural networks.

388
00:32:40,760 --> 00:32:47,440
And then in the 80s computers became faster and faster and by 1985 they were about let

389
00:32:47,440 --> 00:32:54,440
you see they were about 1000 times faster than when Linna Inma wrote this down and also

390
00:32:54,440 --> 00:32:59,320
many academic labs for the first time in the 80s started to have really computers to play

391
00:32:59,320 --> 00:33:05,440
around with desktop computers today everybody has that but has a smartphone instead but

392
00:33:05,440 --> 00:33:09,720
back then this was exciting new stuff and then people started playing around and for the

393
00:33:09,720 --> 00:33:17,360
first time with very small networks they learned interesting behavior on certain little

394
00:33:17,360 --> 00:33:19,680
toy problems.

395
00:33:19,680 --> 00:33:25,880
But we couldn't move past toy problems because of this vanishing grading issue which

396
00:33:25,880 --> 00:33:33,160
sub identified in his research paper which led to the creation of LSTM networks tell us

397
00:33:33,160 --> 00:33:39,160
about LSTM and how the creation of those fell out of the findings.

398
00:33:39,160 --> 00:33:40,160
Yeah.

399
00:33:40,160 --> 00:33:47,920
So the long short term memory as it is called the LSTM falls out of this finding because

400
00:33:47,920 --> 00:33:55,200
it basically overcomes the problem of vanishing gradients through a very very simple trick.

401
00:33:55,200 --> 00:34:02,080
All we have to do is have a very stupid simple unit a particular type of neuron in there

402
00:34:02,080 --> 00:34:08,720
which has the simplest activation function one can think of which is the identity function

403
00:34:08,720 --> 00:34:14,240
which is the identity function and then if that neuron is connected to itself by a connection

404
00:34:14,240 --> 00:34:24,600
of 1.0 then at the next time step it will basically replace what it had before as an activation

405
00:34:24,600 --> 00:34:31,560
number for example 0.7 by the same number 0.7 so now you can imagine that if you run

406
00:34:31,560 --> 00:34:35,320
that for a thousand time steps and nothing happens for a thousand time steps and all the

407
00:34:35,320 --> 00:34:39,160
time 0.7 will be stored in this unit.

408
00:34:39,160 --> 00:34:45,000
At the same time as you are later propagating error signals back because there was a difference

409
00:34:45,000 --> 00:34:49,000
between what the network should have done and what it really did.

410
00:34:49,000 --> 00:34:54,760
Then if you know something about gradients then you know that all the time as you are propagating

411
00:34:54,760 --> 00:35:01,320
errors backwards you have to multiply by the derivative of the activation function which

412
00:35:01,320 --> 00:35:07,000
is 1.0 because it is a very stupid activation function which is just the identity function

413
00:35:07,000 --> 00:35:10,920
and then you multiply by 1.0 again through these recurrent connection which also has a

414
00:35:10,920 --> 00:35:18,280
weight of 1.0 and then now it is obviously as long as there are no external perturbations

415
00:35:18,280 --> 00:35:23,800
in little stupid networks like that you can propagate errors back not only five steps

416
00:35:23,800 --> 00:35:28,760
or ten steps but hundreds and thousands and millions and billions of steps.

417
00:35:28,760 --> 00:35:34,440
This is not finished yet because with that simple type of network you have all the limitations

418
00:35:34,440 --> 00:35:42,320
that you get through linear systems because the identity function is just a linear function

419
00:35:42,320 --> 00:35:47,280
and there are many things you cannot learn by just linear functions.

420
00:35:47,280 --> 00:35:55,200
Now that's the reason why you have to surround the little tiny linear unit, the constant error

421
00:35:55,200 --> 00:36:01,840
carousel as we call it, you have to surround it by a cloud of very non-linear units which

422
00:36:01,840 --> 00:36:08,640
we call gates and these gates then basically learn through exploitation of the error signals

423
00:36:08,640 --> 00:36:14,080
which are being propagated back in these simple guys in the center of these LSTM cells

424
00:36:14,080 --> 00:36:20,000
they learn to adjust their own connections through gradient descent such that they open up

425
00:36:20,000 --> 00:36:24,720
at the right moment and let new stuff into these cells into these memory cells

426
00:36:24,720 --> 00:36:30,560
and they close down at other good moments such that the memory in there is protected for a while

427
00:36:30,560 --> 00:36:35,680
until they open up again and let it out such that can influence the rest of the network

428
00:36:35,680 --> 00:36:43,200
and so all of this is now being learned by these long short term memory networks, the LSTM networks

429
00:36:43,200 --> 00:36:51,040
and they are called long short term memory networks because they are basically inspired by what

430
00:36:51,040 --> 00:36:57,520
the biologists know as the short term memory memories of recent events circling around in

431
00:36:57,520 --> 00:37:04,960
form of circulating activations in your brain but it's a short term memory that lasts for a long time

432
00:37:05,760 --> 00:37:11,440
and that's why it's a long short term memory because it can last not only for five or six or

433
00:37:11,440 --> 00:37:17,040
ten steps like in the first original Recon neural networks but it can last forever basically

434
00:37:17,040 --> 00:37:26,080
so that's why it's a long short term memory and LSTM yeah now you said that this was the solution

435
00:37:26,080 --> 00:37:30,800
to everything however it's not quite true because we also had to wait for faster computers

436
00:37:31,840 --> 00:37:39,040
back then already there was an old trend which held at least since 1941 when Suzy built the first

437
00:37:39,040 --> 00:37:44,960
program controlled computer that it really were back then Suzy could do roughly one operation per

438
00:37:44,960 --> 00:37:54,160
second but since then every five years computing became roughly ten times cheaper and then by the

439
00:37:54,160 --> 00:38:01,200
time when Lina Inma for example the test thing that was thirty years later computing was already

440
00:38:01,200 --> 00:38:05,840
one million times faster but it wasn't good enough and then in the beginning of the 90s that

441
00:38:05,840 --> 00:38:11,920
was another twenty years later was roughly ten thousand times faster than during Lina Inma's time

442
00:38:11,920 --> 00:38:20,080
so it was roughly ten billion times faster than during Suzy's time is that correct 40 plus 90

443
00:38:20,720 --> 00:38:28,800
yeah seems correct but it was still not good enough to to be commercially viable but then by

444
00:38:28,800 --> 00:38:39,680
2010 roughly computers were cheap enough and the factor was high enough such that all the potential

445
00:38:39,680 --> 00:38:45,600
in the LSTM networks really could unfold itself and I think since 2015 it's

446
00:38:46,960 --> 00:38:54,240
widely used in commercial applications for example for the speech recognition on two billion

447
00:38:54,240 --> 00:39:02,160
android phones I'll maybe ask you to talk a little bit about the use cases the applications of

448
00:39:02,160 --> 00:39:09,840
LSTM's do you find most interesting but before we get to that has the fundamental technology changed

449
00:39:09,840 --> 00:39:17,120
much between 1995 and 2015 the fundamental insights were really from the previous millennium

450
00:39:17,920 --> 00:39:26,000
one has to say that however there came a couple of really nice improvements around 2000 my

451
00:39:26,000 --> 00:39:34,240
second LSTM PhD student whose name was Felix Gehrs he was the first author on a paper which

452
00:39:34,240 --> 00:39:42,080
introduced something called the Forgetgate which is a particular recurrent gate unit which turns

453
00:39:42,080 --> 00:39:50,240
out to be really useful for many applications where the network has to also learn to forget

454
00:39:50,240 --> 00:39:57,680
sometimes stuff that it has observed in the past to make room for new things and to do that in a

455
00:39:57,680 --> 00:40:05,520
very controlled fashion and so today the Vanilla LSTM as we call it is a little bit different

456
00:40:05,520 --> 00:40:14,000
from the original LSTM and there are a couple of topology evolutions that came in for example in 2009

457
00:40:14,000 --> 00:40:20,960
just in buyer another student of mine was the first author of a paper which showed that you can

458
00:40:20,960 --> 00:40:28,720
evolve through artificial evolution LSTM like architectures which have the basic concepts of LSTM

459
00:40:28,720 --> 00:40:35,760
in there but with which sometimes change a connection here and have a different topology there

460
00:40:35,760 --> 00:40:41,840
and so on that it was in 2009 and there it was possible to show that in certain applications

461
00:40:41,840 --> 00:40:47,760
this type of LSTM topology works better and faster and learns a little bit faster than this type

462
00:40:47,760 --> 00:40:55,760
of LSTM architecture so there has been a couple of improvements of this kind although the basic

463
00:40:55,760 --> 00:41:00,800
fundamental ingredients they did date back to the previous millennium so this this last thing you

464
00:41:00,800 --> 00:41:07,920
were describing the evolutionary LSTM if that's the right way to describe it the core insight

465
00:41:07,920 --> 00:41:14,880
there is basically you've got several different types of structures for LSTMs and you can

466
00:41:15,520 --> 00:41:21,200
based on the you know the problem you're solving or the objectives switch between them on the

467
00:41:21,200 --> 00:41:30,240
flyer or is it something else this is about the basic architecture of an LSTM like network

468
00:41:30,240 --> 00:41:38,880
and if you look at your own brain for example it's a product of evolution and all these little

469
00:41:38,880 --> 00:41:45,520
special types of architectures that we find in your neurons somehow they evolved over millions

470
00:41:45,520 --> 00:41:50,240
of years because some of them just work better and are better at learning certain things which

471
00:41:50,240 --> 00:41:56,720
are important for your survival than others and so the same thing happened there in our artificial

472
00:41:56,720 --> 00:42:04,480
evolution of topology is where you give the system the freedom to come up with new topologies

473
00:42:04,480 --> 00:42:10,560
which which deviate from the original LSTM architecture a little bit because we don't have a

474
00:42:10,560 --> 00:42:15,840
proof that a particular LSTM topology is optimal for all kinds of things okay there is no proof

475
00:42:15,840 --> 00:42:23,120
like that so we try to figure out maybe we can automatically find the optimum topology in a

476
00:42:23,120 --> 00:42:29,360
problem specific fashion because maybe for this problem over here you need a different topology

477
00:42:29,360 --> 00:42:36,800
is better than for this problem over here and so we were able to show them that there are indeed

478
00:42:36,800 --> 00:42:44,560
applications where you can profit from this additional optimization of the topology through evolution

479
00:42:44,560 --> 00:42:52,160
can you elaborate a little bit on the notion of topology what makes a network fundamentally LSTM

480
00:42:52,160 --> 00:42:59,680
and what are the things that can vary from one LSTM topology to another yes so the vanilla LSTM cell

481
00:43:00,640 --> 00:43:07,680
has four little neurons in there there is the central neuron which is the stupid one that I mentioned

482
00:43:07,680 --> 00:43:14,400
before which has just the identity function as an activation function then it is surrounded by

483
00:43:14,400 --> 00:43:21,680
three gates which are multiplicative so there is this thing called the input gate which is a normal

484
00:43:21,680 --> 00:43:27,040
standard neuron and non-linear standard neuron and if that one is active it can completely

485
00:43:27,920 --> 00:43:34,960
open the access to the central unit the constant error carousel or if it is shut down if it is

486
00:43:35,440 --> 00:43:43,520
zero then nothing can flow into that central cell and similar for the output gate

487
00:43:43,520 --> 00:43:53,200
and then this is recurrent gated unit the forget gate which basically can manipulate the self

488
00:43:53,200 --> 00:43:59,600
connection of the stupid cell of the cell with the linear activation function to itself and can

489
00:43:59,600 --> 00:44:05,600
make the self forget stuff that was stored there for a while and can learn to do that on

490
00:44:05,600 --> 00:44:12,160
now you can come up with all kinds of variants of that topology maybe you have a little connection

491
00:44:12,160 --> 00:44:21,040
going out directly from the central cell to these gates and these are called peephole connections

492
00:44:21,040 --> 00:44:28,400
that's what Felix Ghears also called them my second LSTM student in 2001 roughly 2000 something

493
00:44:28,400 --> 00:44:34,240
like that you can have additional modifications of the architecture and sometimes it's useful

494
00:44:34,880 --> 00:44:41,360
because as I said there is no proof that a particular LSTM topology is optimal so you might want

495
00:44:41,360 --> 00:44:49,120
to use the principles of evolution to search for good topologies automatically and to take the

496
00:44:49,120 --> 00:44:53,360
existing one which is pretty good for many applications vanilla LSTM is really good for many

497
00:44:53,360 --> 00:45:00,400
applications but even further improve it so that is the approach behind that okay you talked a

498
00:45:00,400 --> 00:45:07,360
little bit about this but if we can take a step back what was the kind of intuition for all of

499
00:45:07,360 --> 00:45:13,360
the you know for the LSTM cell architecture right at the point that that was developed that

500
00:45:14,320 --> 00:45:19,840
and correct me if I'm wrong here I think of traditional RNNs as being just a lot flatter whereas

501
00:45:19,840 --> 00:45:24,880
LSTMs have these cells that are doing all this funky stuff is that do you think about it that way I like

502
00:45:25,520 --> 00:45:31,360
yeah so in it is true that the traditional become networks are very straightforward it's just a bunch

503
00:45:31,360 --> 00:45:37,440
of nodes with non-linear activation functions and everything is connected to everything and at every

504
00:45:37,440 --> 00:45:43,840
time step each of these little nodes is computing essentially the weighted sum of all the connected

505
00:45:43,840 --> 00:45:50,400
guys at the previous time step super basic it's very simple and basic and beautiful also because

506
00:45:50,400 --> 00:45:56,560
everything that simple has a tendency to be beautiful except that it was not quite it was a little

507
00:45:56,560 --> 00:46:03,120
bit too simple so you need a little bit of additional structure to make sure that these memory

508
00:46:03,120 --> 00:46:10,880
effects hold now the LSTM is more complicated but it's also very simple because you can write it

509
00:46:10,880 --> 00:46:17,200
write it down in five lines of code five lines of pseudo code are sufficient to explain it so

510
00:46:17,200 --> 00:46:22,000
maybe a traditional recon network you can write it down in one line of pseudo code and this one needs

511
00:46:22,000 --> 00:46:27,200
five lines of pseudo code maybe something like that and still the principles are very simple

512
00:46:27,200 --> 00:46:33,600
because you have a very stupid simple linear cell at the heart of each of these LSTM cells and then

513
00:46:33,600 --> 00:46:40,320
you surround it by a cloud of non-linear gates such that these gates can learn to open access

514
00:46:40,960 --> 00:46:49,200
or close access to these memories and the network itself can learn to use these gates to

515
00:46:49,200 --> 00:46:58,080
put important stuff into short-term memory and ignore noise and so on yeah well as Einstein said

516
00:46:58,080 --> 00:47:08,240
a long time ago you should make things as simple as possible but not simpler yeah very good to what

517
00:47:08,240 --> 00:47:17,520
degree does the LSTM you know there are some related I think concepts that come up in deep learning

518
00:47:17,520 --> 00:47:23,600
like attention and things like that how does how does attention for example relate to LSTMs yeah

519
00:47:24,400 --> 00:47:30,000
so any recon your network of course already has something like internal attention

520
00:47:30,720 --> 00:47:36,800
because what it can do is essentially it can learn to direct internal spotlights of attention

521
00:47:36,800 --> 00:47:42,480
if you will to certain parts of the network we can say let's highlight this part of myself

522
00:47:42,480 --> 00:47:50,240
and let's ignore this part of myself so in a way the internal computation which is based on the

523
00:47:50,240 --> 00:47:57,680
program that is implemented on the recon network in form of its weight matrix this attention can be

524
00:47:57,680 --> 00:48:06,320
learned in a problem dependent fashion and so I think my first paper on that was in 1993 where

525
00:48:06,320 --> 00:48:14,640
basically a recon network learn to direct its internal focus of attention is is that the

526
00:48:14,640 --> 00:48:23,360
correct plural the plural of focus focus is it focus okay so the focus of attention and then could

527
00:48:23,360 --> 00:48:31,120
could use these highlighted internal patterns in hand like fashion such that it could associate

528
00:48:31,120 --> 00:48:38,560
through something called fast weights these internal attention highlighted patterns such that it

529
00:48:38,560 --> 00:48:44,320
could do certain things that you cannot do with traditional networks because it had these

530
00:48:44,320 --> 00:48:49,920
extra these extra fast weights which is actually a topic that has become really popular again

531
00:48:49,920 --> 00:48:57,760
very recently so attention is something that was implicit in many many recon networks already

532
00:48:57,760 --> 00:49:05,600
in the in the past and now it is it is waking up in very interesting commercial applications

533
00:49:05,600 --> 00:49:11,840
again speaking of commercial applications we talked about a few of the commercial applications

534
00:49:11,840 --> 00:49:20,080
of LSTMs are there others that come to mind for you that exemplify its you know power flexibility

535
00:49:20,080 --> 00:49:28,560
interest yeah so I think it's interesting to see that not only speech recognition but also

536
00:49:29,200 --> 00:49:35,680
the next higher level natural language processing can be done by the same architecture so with

537
00:49:35,680 --> 00:49:40,480
speech for example every 10 milliseconds a new vector of numbers maybe 14 numbers or something

538
00:49:40,480 --> 00:49:46,400
streaming into the system from the microphone with language it's quite different there you already

539
00:49:46,400 --> 00:49:54,080
have letters and words and so on and now on this higher level which is basically derived from this

540
00:49:54,080 --> 00:50:01,920
elementary level of speech you again can use LSTMs to for example understand text for example read

541
00:50:01,920 --> 00:50:08,480
some document and then try to make a short summary or classify that document maybe say the

542
00:50:08,480 --> 00:50:15,200
the document is a CV of a person and you want to compare to another document with which is a

543
00:50:15,200 --> 00:50:22,320
job advertisement and then you classify the document with respect to the job ad and you say okay

544
00:50:22,320 --> 00:50:27,680
this guy's a match or not machine translation maybe the most visible application is now what

545
00:50:27,680 --> 00:50:35,520
Google is doing in since 2016 since november 2016 Google translate is not based on the old

546
00:50:35,520 --> 00:50:41,440
system anymore but it has LSTM at its core and it has become much better than used to be

547
00:50:41,440 --> 00:50:50,160
and especially the most important language pair in the world which is English to Mandarin Chinese

548
00:50:50,160 --> 00:50:57,760
and back that is they're the performance is much better there was a time when the Chinese they

549
00:50:57,760 --> 00:51:05,440
laughed at the translations and they're not laughing any longer and then the same thing can be used

550
00:51:05,440 --> 00:51:12,800
for example with slight modifications to segment images which again seems like a totally different

551
00:51:12,800 --> 00:51:21,040
problem right the same thing can be used to well that seems again closer to natural language

552
00:51:21,040 --> 00:51:27,680
processing can be used to train chat parts so you have lots of chats between A and B and you

553
00:51:27,680 --> 00:51:34,240
then just train your LSTM to imitate B whenever it answers something to A but maybe you have lots

554
00:51:34,240 --> 00:51:41,280
of A's and B's and so it can learn from many many different chats and so it's only even relevant

555
00:51:41,280 --> 00:51:49,520
for this old idea of an AI test which is called the touring test which is about chatting with some

556
00:51:49,520 --> 00:51:58,160
other partner and the question is is he a human or is he a machine right and so let's see where

557
00:51:58,160 --> 00:52:04,880
that is going to end at some point then the same LSTM also can be used for all kinds of other

558
00:52:04,880 --> 00:52:12,000
sequences such as music composition one more people again interested in that Doug Eck in my lab

559
00:52:12,000 --> 00:52:18,320
about more than 10 years ago was the first who applied LSTM to music composition where the gold

560
00:52:18,320 --> 00:52:25,280
was to overcome the traditional neural composition neural music composition which existed back then

561
00:52:25,280 --> 00:52:34,480
already which was able to learn the basic harmonies but then if you listen to compositions they

562
00:52:35,120 --> 00:52:43,280
sounded like music like stuff that in principle for example when you train it on certain pieces by

563
00:52:43,280 --> 00:52:51,040
Bach which is a favorite of many of these neural network guys then even in the 80s and 90s it was

564
00:52:51,040 --> 00:52:58,080
possible by Mike Moser and Pete Todd and a couple of guys like that to learn the basic harmonies

565
00:52:58,080 --> 00:53:05,120
that you find in this type of music and then come up with randomized version there of compositions

566
00:53:05,120 --> 00:53:12,480
of the network so to speak which sound loss are like kind of this type of music except no there was

567
00:53:12,480 --> 00:53:19,280
no overarching structure so more like elevator music versions of Bach or something and then the

568
00:53:19,280 --> 00:53:25,600
gold was to also learn high level structure that you find in many songs like first section eight

569
00:53:25,600 --> 00:53:30,400
and there's another section eight and there comes B then there comes A again and then again B

570
00:53:30,400 --> 00:53:37,120
something like that yeah that was Doug Eck in in the early 2000s and it was one of your students

571
00:53:37,120 --> 00:53:42,240
yeah yeah yeah he was now at Google he is now at Google and he's running the what is called

572
00:53:42,240 --> 00:53:48,960
magenta magenta I just envied him last week okay he was a postark here actually oh wow yeah okay

573
00:53:48,960 --> 00:53:56,560
and he was the first author on these LSTM for music papers yes 2003 I think right yes wow

574
00:53:57,200 --> 00:54:06,640
great great so maybe as a way to to pull the conversation back to your broader research around general

575
00:54:06,640 --> 00:54:10,560
artificial intelligence or artificial general intelligence depending on your preference

576
00:54:10,560 --> 00:54:17,360
you did an AMA on Reddit not too long ago actually it was a couple of years ago maybe yeah

577
00:54:17,360 --> 00:54:22,400
yeah and one of the questions that I thought was pretty interesting was what are some

578
00:54:23,520 --> 00:54:28,720
beliefs that you hold that are controversial in essence that was the question and one of those

579
00:54:28,720 --> 00:54:35,840
was that intelligence is actually simple although we think of it as rather complex can you elaborate

580
00:54:35,840 --> 00:54:43,040
on that a little bit yes I think I can do that so LSTM by itself is nice for patent recognition

581
00:54:43,040 --> 00:54:50,080
and for doing really complex applications but we should see that it's just a patent recognizer

582
00:54:50,080 --> 00:54:56,240
so the full AGI thing requires more it requires reinforcement learning in an unknown

583
00:54:56,240 --> 00:55:01,520
partially observable environment where there's no teacher who tells you what to do and where you have

584
00:55:01,520 --> 00:55:08,080
to maximize your future expected reward okay and so whoever can solve that problem in a general

585
00:55:08,080 --> 00:55:14,480
fashion has solved the the grand problem of AI and that's why almost all of our research

586
00:55:14,480 --> 00:55:20,160
of the past 30 years really focused on that and so to us the LSTM which has now become popular

587
00:55:20,160 --> 00:55:24,960
is just a side effect that's just a byproduct of this more general work you can use the LSTM

588
00:55:24,960 --> 00:55:30,000
to build a model of the world to predict what's going to happen if you do that and that but you

589
00:55:30,000 --> 00:55:35,760
still need another module which is learning which is inventing new experiments and trying to

590
00:55:36,480 --> 00:55:44,160
figure out which sequences of actions lead to success and which don't and that leads to this

591
00:55:44,160 --> 00:55:48,960
field of reinforcement learning with general purpose recurrent architectures with recurrent

592
00:55:48,960 --> 00:55:55,120
works basically now if you look at the LSTM it's just five lines of pseudo code so it's very simple

593
00:55:55,120 --> 00:56:00,240
it's not the full true AI thing yet because there you have to have the full loop through the

594
00:56:00,240 --> 00:56:06,160
environment act perceive act perceive act perceive maximize future reward or reward until the end

595
00:56:06,160 --> 00:56:12,000
of your lifetime and for that I sometimes speculate we need another five lines

596
00:56:14,560 --> 00:56:21,120
and why am I so optimistic because we understand in many ways how to train the separate action

597
00:56:21,120 --> 00:56:27,040
module and how to combine that with a model of the world which can be used through the five lines

598
00:56:27,040 --> 00:56:33,280
of something like the LSTM or something that is in the same ballpark at least right and I'm

599
00:56:33,280 --> 00:56:41,040
further motivated to believe it's very simple because here in my lab in the early 2000s at least

600
00:56:41,040 --> 00:56:48,960
we already have found certain theoretically optimal journal problem solvers which are also very

601
00:56:48,960 --> 00:56:55,760
simple so there are many computer scientists actually don't or machine learning scientists don't

602
00:56:55,760 --> 00:57:02,880
even know that but there is an asymptotically optimal way of solving all kinds of computational

603
00:57:02,880 --> 00:57:12,080
problems and this goes back to Marcus Hutter in 2002 here in my lab he was a senior researcher

604
00:57:12,080 --> 00:57:18,800
now he's a professor in Australia and he wrote down a very simple method which solves any

605
00:57:19,840 --> 00:57:28,640
well defined problem as fast as the unknown fastest meta program for solving that kind of problem

606
00:57:29,520 --> 00:57:38,240
save for a constant factor or a for say for an additive constant which does not depend on the

607
00:57:38,240 --> 00:57:44,160
size of the problem so when you have for example a traveling salesman problem which is about

608
00:57:44,160 --> 00:57:50,960
finding the shortest path through n cities where each city can be visited only once then as the

609
00:57:50,960 --> 00:57:56,080
number of cities n grows the problem becomes larger and larger but it's still just a struggling

610
00:57:56,080 --> 00:58:00,800
salesman problem now we do not know the best way of solving traveling salesman problems but

611
00:58:00,800 --> 00:58:08,560
suppose you can solve it and suppose there's an unknown method which solves it in n to the 17 steps

612
00:58:08,560 --> 00:58:14,240
then this method of Marcus the fastest and shortest algorithm for all value fine problems as it is

613
00:58:14,240 --> 00:58:24,400
called will also solve it in n to the 17 steps plus a constant number of steps and overhead

614
00:58:24,400 --> 00:58:32,960
which however is constant and does not depend on n so as n grows into the 17 grows even much more

615
00:58:32,960 --> 00:58:40,960
crazily and the constant overhead vanishes which means that almost all problems are already solved

616
00:58:40,960 --> 00:58:46,400
in optimal fashion because almost all problems are large problems they're just a few problems that

617
00:58:46,400 --> 00:58:53,120
are so small that the overhead still plays a role just so I can make sure I understand what

618
00:58:53,120 --> 00:59:00,560
what he did and what you're saying is the idea that if a problem can be solved that basically

619
00:59:00,560 --> 00:59:07,440
it can be transformed into some other kind of solution modulo a constant so what I'm saying is

620
00:59:07,440 --> 00:59:14,160
you have a specification of a problem and then you want to find a program that solves the problem

621
00:59:14,160 --> 00:59:22,160
to approach that you want to build a general problem solver which finds that program automatically

622
00:59:23,440 --> 00:59:30,640
now there is a general problem solver which is the fastest and shortest way of solving all

623
00:59:31,360 --> 00:59:38,160
well defined problems by Marcus who published in 2002 here in my lab which essentially does that

624
00:59:38,160 --> 00:59:46,160
and it's optimal however only in an asymptotic sense what does that mean if you're problem for

625
00:59:46,160 --> 00:59:52,960
example you're traveling salesman problem with n cities can be solved and into the 17 steps

626
00:59:52,960 --> 01:00:00,640
then this super algorithm of Marcus is also going to solve it and into the 17 steps plus

627
01:00:00,640 --> 01:00:09,600
all of one as the computer scientists say and all of one means there is a constant overhead associated

628
01:00:09,600 --> 01:00:15,760
with it now the constant overhead is large it turns out because there's a proof search involved

629
01:00:15,760 --> 01:00:22,560
and we hide all the complexity of that in a constant which can be done no matter even if the

630
01:00:22,560 --> 01:00:28,160
constant is large then as n is getting larger and larger as the size of the problem is getting

631
01:00:28,160 --> 01:00:36,720
larger and larger the constant pales in comparison sure and that's the reason why you already have

632
01:00:36,720 --> 01:00:44,160
since the other millennium an optimal way a mathematically optimal way asymptotically optimal

633
01:00:44,160 --> 01:00:50,080
way of solving all kinds of problems especially the large problems which are so large that we

634
01:00:50,080 --> 01:00:56,800
can ignore the overhead the constant overhead does this result just say that we can asymptotically

635
01:00:56,800 --> 01:01:02,000
solve them it doesn't necessarily tell us how to do that no it says it is a constructive method

636
01:01:02,000 --> 01:01:10,480
it really tells us how to do that why I'm not doing that all the time because the small problems

637
01:01:10,480 --> 01:01:17,440
that we are looking at here on this planet in our daily lives they are so small that the the

638
01:01:17,440 --> 01:01:23,520
constant overhead does play a role and that's why we are still doing sub optimal things such as

639
01:01:23,520 --> 01:01:30,080
deep learning and all kinds of things you know but at least from a mathematical perspective

640
01:01:30,080 --> 01:01:34,560
we already know there is a mathematically optimal way of solving all kinds of problems

641
01:01:35,760 --> 01:01:41,280
in the fastest possible fashion the fastest possible in a non practical way but at least

642
01:01:41,280 --> 01:01:46,960
an asymptotically optimal way we already have that and it's very simple it's a very short thing

643
01:01:46,960 --> 01:01:55,280
so in that sense you gain additional motivation to believe that in the end the whole

644
01:01:55,280 --> 01:02:02,240
AI thing is going to be really simple and in hindsight we will look back and we'll say we can't

645
01:02:02,240 --> 01:02:07,360
believe that it took so many thousands of years to understand how to understand and how to solve

646
01:02:07,360 --> 01:02:14,800
problems automatically so this tells you that by saying that if we can get far enough along with

647
01:02:14,800 --> 01:02:22,400
compute power sophistication and approaches to overcome this constant then we're there like we

648
01:02:22,400 --> 01:02:28,800
can solve anything is that the idea not quite because we really have to do is try to find a

649
01:02:28,800 --> 01:02:36,160
similarly simple thing for the small problems too and one step in that direction is the so-called

650
01:02:36,160 --> 01:02:44,960
girdle machine which I published in 1993 in 2003 which which you can initialize with Markus's

651
01:02:44,960 --> 01:02:51,440
algorithm but also with other algorithms and which essentially learns to rewrite itself in an

652
01:02:51,440 --> 01:02:58,960
optimal fashion once it has found a proof that the rewrite is going to improve its performance

653
01:02:58,960 --> 01:03:07,520
in a way that is not only asymptotically optimal but generally optimal so however also this is

654
01:03:08,080 --> 01:03:15,280
not yet practical at the moment however getting out of that is that the very general

655
01:03:16,560 --> 01:03:21,840
problem solvers may be very simple and can be written down in a few lines of code

656
01:03:22,800 --> 01:03:28,160
we still don't have the few lines of code that we need for a practical general problem solver in

657
01:03:28,160 --> 01:03:33,840
this universe but I think we are close and the puzzle pieces are starting to fall in place

658
01:03:34,560 --> 01:03:42,160
and I hope I will see it in my lifetime awesome maybe in the next few years awesome I've really enjoyed

659
01:03:42,160 --> 01:03:47,920
the opportunity to talk with you I've certainly learned a lot for folks that want to learn more

660
01:03:47,920 --> 01:03:55,760
about what you're up to what's the best way for them to do so it's probably to look at my sprawling

661
01:03:55,760 --> 01:04:02,400
website which contains more information than you ever wanted to see about us but not only has

662
01:04:02,400 --> 01:04:09,440
the original papers but also overview pages which try to explain in rather simple terms

663
01:04:09,920 --> 01:04:18,000
where the experts might want to study in detail in the original papers great and we'll include

664
01:04:18,000 --> 01:04:24,000
a link to that in our show notes that will be so awesome great well thank you so much Erick

665
01:04:24,000 --> 01:04:28,640
it was my pleasure send great

666
01:04:30,240 --> 01:04:36,560
all right everyone that's our show for today thanks so much for listening and for your continued

667
01:04:36,560 --> 01:04:42,240
feedback and support for the notes for this episode including links to you again and the many

668
01:04:42,240 --> 01:04:50,320
resources mentioned in the show head on over to twimmalei.com slash talk slash 44 please be

669
01:04:50,320 --> 01:04:55,760
sure to comment there with your feedback or questions also please note if you share your

670
01:04:55,760 --> 01:05:02,080
favorite quote with us via a comment or via twitter we'll send you one of our fab laptop stickers

671
01:05:02,880 --> 01:05:08,480
i'll be in san francisco september 18th through 20th for the artificial intelligence conference

672
01:05:08,480 --> 01:05:14,240
and i hope to see you there too if you've already registered send me a shout on twitter and let me

673
01:05:14,240 --> 01:05:19,440
know if you're still interested in coming but haven't registered yet we've got a link

674
01:05:19,440 --> 01:05:26,000
and discount code on the show notes page good for 20% off most conference packages the following

675
01:05:26,000 --> 01:05:31,920
week i'll be at strange loop a great conference held each year right here in st louis strange loop

676
01:05:31,920 --> 01:05:37,280
is a multi disciplinary conference that brings together the developers and thinkers building

677
01:05:37,280 --> 01:05:44,240
tomorrow's technology and fields such as emerging languages alternative databases concurrency

678
01:05:44,240 --> 01:05:50,240
distributed systems security and the web will link to the conference on the show notes page as well

679
01:05:50,800 --> 01:05:58,000
finally another huge thanks to this show sponsor cloudara for more information on their data

680
01:05:58,000 --> 01:06:05,760
science workbench for the schedule your demo and receive your drone visit twimmalei.com slash cloud

681
01:06:05,760 --> 01:06:17,280
dera thanks again for listening and catch you next time

