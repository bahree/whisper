Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting
people doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
I'd like to start off this show by sending out a huge thank you to everyone listening.
We've dropped a ton of great interviews over the past few weeks and through your dedication
we continue to see a growing outpouring of feedback comments and shares with each release.
If you're a regular listener but don't normally send in feedback we'd really love to hear
from you.
So please head on over to Apple Podcasts or wherever you listen and leave us a review.
A five star review is of course appreciated but what's most important is that your voice
is heard.
It lets us know what you like or what you feel we can improve on and it also lets those
looking for a new machine learning and AI podcast know that they should join the Twimble
community.
Speaking of community, the details of our next Twimble online meetup have been posted.
On Tuesday November 14th at 3pm pacific time, we'll be joined by Kevin T who will be presenting
his paper, active preference learning for personalized portfolio construction.
If you've already registered for the meetup, you should have received an invitation
with all the details.
If you still need to register, head on over to twimbleai.com slash meetup to do so.
We hope to see you there.
Now as some of you may know, we spent a few days last week in New York City hosted by
our great friends at NYU Future Labs.
About six months ago, we covered their inaugural AI Summit and event they hosted to showcase
the startups in the first batch of their AI Nexus Lab program as well as the impressive
AI talent in the New York City ecosystem.
While we were more than excited when we found out they would be having a second summit
so soon, this time we had the pleasure of interviewing the four startups of the second
AI Nexus Lab batch.
We also interviewed a bunch of the speakers from the event and we'll be sharing those discussions
over the upcoming weeks.
In this episode, you hear from bite.ai, a startup founded by Vene Anantharaman and Michael
Walski, founders who met working at Clarify, another NYU Future Labs alumni company,
whose CEO Matt Zealer, I interviewed on twimbletalk number 22.
Data is using conversational neural networks and other machine learning to help computers
understand and reason about food.
Their product is an app called BiteSnap, which provides users with detailed nutritional
information about the food they're about to eat using just a photo and a serving size.
We dive into the details of their app and service, the machine learning models and pipeline
that enable it and how they plan to compete with other apps targeting dieters and more.
Alright everyone, I am here at NYU Future Labs in New York City and I am with the co-founders
of BiteAI.
In particular, I'm with Vene Anantharaman and Michael Walski.
Guys, welcome to this weekend machine learning and AI.
Thanks for having us.
Awesome, awesome, why don't we get started by having you tell us a little bit about
your backgrounds and we'll get to what the company is up to.
Cool, as I was introduced, my name is Vene and I kind of work on data infrastructure
and for the last 10 years I've been crawling the web trying to turn that into structured
data and currently what we're working on is building a food intelligence platform that
can understand the world's food.
Okay, so I'm Michael, actually two of us met here in this building at Clarify where the
first 10 threes and I got to work this together for 10 years before starting this company.
Before joining Clarify I was at Columbia studying the pipeline and computer science, the focus
on computer vision.
Okay, awesome, awesome.
You said understanding food, tell me a little bit more about what that means.
I'm assuming since you met at Clarify, you're doing something visual.
Yes, exactly, so to understand the world's food we need to be able to have examples of
what people are eating day to day and most ways that we communicate about food these days
is with pictures.
So we started by building an image recognition model that can understand what we're eating,
which means that we can take something that we're eating and translate into a set of
tags and also give you nutritional information.
And where we're going with that is actually we'd love to be able to take in other ways
we talk about food.
It could be through menus or through text and be able to say, hey, you know, it's XYZ,
it could be made with these ingredients and we want to build a system that can let other
people also be able to, you know, use the intelligence and understanding we have about food
to power their food application.
So it could be a nutrition tracker in a healthcare setting.
It could also be for a recipe website, they have a bunch of pictures, they want to know
what ingredients it is.
That's kind of what we're working on.
Interesting, interesting.
Now, I don't know if you've come across Hillary Mason.
She has a company called Fast Forward Labs here in New York that was actually recently
acquired, but they do kind of, they do data science experiments, if you will.
That's one of the things they do at least.
And they experimented with trying to do this, determine nutritional information based
on pictures of food.
And I think she mentioned this on the podcast I did with her.
She said it was an incredibly difficult prop walk.
Like they end up, I think they end up giving up on it and moving on to something else.
What are you doing differently that makes it tractable?
We kind of approach this from a consumer side of the product called BitesNap.
It's a nap that helps you to track what they're eating, kind of like a knife in this
palette alternative, using images as input.
Okay.
What we do is every time a user comes in and wants to log in the meal, they take a picture
of their food, we give them suggestions of what might be, users that they can correct
this, like the correct choice or kind of pick something else that we didn't predict.
Tell us the portion size.
So every time some bugs in the meal would get training data to improve the algorithm.
And what we do in the app now is, if you take a picture of the meal that we haven't
seen before, we can't recognize.
We allow you to tell us what it is and now the variance to recognize it.
So we do one shot learning.
So the next time you come around, we can recognize it.
And in doing this, we collect all this data, the users on our take about three or four
pictures a day.
Okay.
We're going to have this human loop system to help us keep improving.
Interesting.
How do you measure accuracy?
On the image recognition or on the nutrition side, like kind of the full loop.
So portion size are important and for now we have the users toss the portion size.
So we have a prior, there's this big study called inhanes where the government collected
all this data on people eat, how much they eat.
So we have kind of a distribution of, for example, a fries, how many fries some of my food
for dinner.
And using that as a prior, as like an example of what the portion size might be, okay,
we ask these just to correct that, okay, so it's still on then to kind of tell us how
much they, because it's so professional these days, I mean, just in general, to be able
to measure food and get the accurate numbers, you have to be able to tell us how much
there is.
But we're hoping that by getting this data later on, we actually start using computer
vision to predict the portion size.
Okay.
You know, I'll definitely be downloading this and playing around with this.
So I've been experimenting with a ketogenic diet, which my use fitness pal, like if you're
really into it, you're tracking your macronutrients with every meal and it's a pain in the
butt.
And so I've experimented with just taking a picture of the things that I eat and then
going back afterwards and then trying to look at it and figure out, and it's hard for
me to figure out, okay, what the portion size was, but then also, like, you know, what the
fat content was.
If you go to a restaurant, the fat content and a given meal can vary, you know, pretty
dramatically.
I mean, those aren't things that you can address just by training data, because in those
cases, the users don't even know themselves.
So we're hoping to use the data we call some users and start integrating location into
the system.
So if you know that you go to ShakeShack and you have a burger at ShakeShack, and we've
seen an example of that burger at ShakeShack and some print information, we'll be able
to say it's not just a general burger, it's the ShakeShack burger of the interesting
data that we get from there many.
Okay.
I was going to ask if you were targeting, like, chain types of meals as a, it seems like
that's easier than, so we haven't gotten to it yet, but it's kind of on our roadmap
in the next few months, it's still kind of pulling in information, starting to location.
I kind of have the other signals to improve the accuracy.
Okay.
So tell me a little bit about what some of the big challenges have been for you.
I think for us, the biggest one is just, like you said, it's a hard problem.
The coverage is an issue.
People eat all kinds of foods, there's probably millions of combinations.
And kind of the first explanation that matters.
If someone comes in, they try it, it doesn't work the first time, they might not come
back.
So for us, making sure we have good coverage of all the foods and they have high accuracy
and stuff that you had at home, that it's not at our start menu, this is important.
Right.
How about for you, Bina?
I would say that for us, because we do have a consumer app, I think marketing is pretty
difficult in the consumer space, and our background is a little more technical.
So it's kind of new for us to be marketing a consumer app, and for us, if the more consumers
we get, the more data we can get.
So we're, at least we've been lucky with the futures lab, we've been getting advisors
and help, but it's still kind of an ongoing process that we're learning, how to actually
market the app and get it out there.
Interesting.
Can you talk a little bit about the pipelines that you've created to process all the data?
Yeah, sure.
I mean, we, as we mentioned, we, you know, we scraped a bunch of images from that.
We basically built our own tools to annotate and clean data.
I guess Michael can fill in there.
We built this tool to pull in images, use active learning to help us quickly annotate
millions of images, the two of us managed to annotate three million labeled images of
the different foods.
The two of you met three million over how long it was about a week.
Wow.
So we just clustered and some classifiers to rank and be smart about how we're doing this.
I've kind of done it before, I'd clarify.
Yeah.
I have an idea of how to handle this scale of a data set.
Okay.
And we went from a model of 60 classes to now a bit of over 1500 different foods.
Now that that is out, we're getting all this data from users as well, and we're starting
to chain known that data as well.
I used to say 1500 different foods.
When I think about like how many different foods fitness palette has in it, it's multiple
of that, isn't it?
Yeah.
So there's two issues.
There's whole food state.
You might, you know, go get a grocery store.
There's homey meals.
That's where we focus on.
Right now.
Okay.
And most of these are basic ingredients.
So if you take a play of pasta, tomatoes, we might say it's pasta, but also give you
options for the noodles, the sauce, the cheese on top.
Okay.
Okay.
Okay.
Okay.
So kind of a combination between the food detection as well as like a Google look on
most.
Yeah.
I can't find the products.
Yeah.
Interesting.
And do you also, are you also allowing them?
Yeah.
I do.
I do.
I do.
I do.
I do.
I do.
I do.
I do.
I do.
I do.
I do.
I do.
I do.
I do.
Also are you also allowing them to track the nutritional information over time?
Is it kind of like take plane that role as well or just do they take the data and plug
it into something else.
It's a full tracker flagger.
Logger will give you the breakdowns for the day, for the week and you can see your history
every time.
Okay.
I need this.
Yeah.
And you should go after this keto market like there's an out of news that something
that's on your radar.
We're actually getting feedback from our users a lot of them.
We're doing keto chains diet, okay.
And this first division for that in the future is kind of build it up to start putting communities together.
Yeah.
We have users who have all this content in their pictures, and we know that a person's a key to giant diet,
you might be able to connect them to other people who are doing keto,
help them discover foods, share up their eating, start recommending other stuff to them.
Okay.
What are some of the techniques that you're using on the ML and AI side here?
So for image recognition, we're just using standard neural networks, conversion neural nets.
Stories of the I know might have to switch since it tends to fall apart by turrets.
For active learning, just simple like logistic regression, simple like logistic regression to rank the results based on the embedding.
Yeah, it's pretty much a lot of neural networks.
Okay.
Cool.
Where are you kind of in the lifecycle of product is out and on the various app stores,
both you're not going to tell me that you don't support Android.
No, we actually do support iOS and Android.
We realize this is incredibly important.
Okay. So we wear on both things because we're using React Native,
which for a small team, it really helps us because we can have an app that's available for both platforms.
In terms of the product, we actually launched an MVP earlier this year,
and we've kind of been developing that towards kind of product market fit with other people,
for with other apps like my fitness pal, people have some expectations.
So we're really, really close on kind of closing that out.
And in terms of the product, we're now actually focusing to start launching an API
so that people can use our technology and other applications.
That's interesting.
I think I look for an API for my fitness pal.
And if there was one, it was like you had to submit a form and get approved
and talk to their BD people and see what's pretty painful.
Yeah, I mean, I think that with because we're taking a little bit of different approach
in terms of our app strategy, because we actually are fine with giving people macros
and micronutrients.
We find that the information is currently valuable for people to make decisions
about what they're eating.
So we're offering that up.
And we do have plans and thoughts about opening up the API to users themselves
so that they can be creative, so that they can view and visualize their data
in their own ways.
We've experimented with that because we can let you export to CSV or JSON.
So you can pull that in and kind of build your own visualizations
and a few people from the self-quantified community are really excited about it
because they can actually then kind of make their own collages and their own charts
and integrate that into their own self-quantified solutions
because you know, that's not our focus.
But we would love to enable creativity for them around what they're eating
and that's kind of a big part of their life.
And so do you, you know, thinking about again, where you're coming from with Clarify,
do you see the bite snap product is kind of just bootstrapping an API business
or is that the product?
It's a core product for us.
But we see that it's a great way to get data to have this human look system.
It keeps improving.
We've gotten a lot of inbound requests for an API from customer research firms,
from healthcare side, from hospitals, people dealing with diabetic patients.
So we're kind of looking to open it up to other people.
There's a lot of core technology building up to other people using other ways.
Okay.
Interesting.
Maybe we can spend some time and you can kind of walk me through the annotation framework
that you built out for getting through those images.
But like I'm still that's an impressive feat.
Yeah.
The tool pretty much, we get content from the web or you have a class.
If I was to say you have a new one hour day chain before, what you do is you predict
on the other images, you get some kind of ranking for each class.
So let's say for strawberries, we'll take a classifier that's a week classifier for
strawberries, rank all of our images throughout, get a signal of what might be a strawberry.
We use that to rank.
We also do visual clustering.
So we can say, you know, for all these images, food, these are the similar looking cluster
for images of strawberries that are in the middle of these strawberries.
Let me look at that cluster and for the whole cluster and make a response of yes, this
is a strawberry.
Then we have a classifier that is that input to train and improve.
So now we can re-rank the results again.
And also get to a high enough accuracy, you can say for the rest of the image, because
the classifier is 99% accurate, or let's say it's 95%, you know, whatever the prediction
is, assume it's a cooling label.
Got it.
So when you annotated 3 million images, like that initial, you look at the cluster of
strawberries and say, yes, these are strawberries.
That might have knocked out 100,000 images for you, or some large number.
Right.
So if you take the ranking for the images, for let's say the strawberries, you kind of see
stuff on the bottom that's definitely not strawberries, so you can say, let's say the
top of the bottom 20%, it's negative data now.
Right.
You take the top 10%, if you look at it and see that, it looks like positives.
You assume that's positive, you update your classifier based on that data, and you retrain
and re-rank again, kind of moving that data away, and kind of keep diving into a clustering
we found was working literally as well, because now you can, instead of looking at single
image at a time, you can say, oh, for this cluster, so all the images look similar, they
look like strawberries.
Yeah, I'm sure you've seen like clustering on the vetting space, so now you can, instead
of responding to an image at a time for a whole cluster, say, yes, this is a strawberry
and opposite strawberry.
Okay.
And so you annotate your strawberries, and then you have all that information for kind
of the next thing you're looking at, so what extent does the annotation for strawberries
impact bananas if that's what you're doing next?
Doesn't at all.
So we've probably started again with some bananas.
Okay.
It turns out in the opposite, very good at handling noise, so if you have a classifier
you're trying on the dirty data, there's probably 60% probably that the thing that mentions
strawberry has strawberries in the image.
If you're trying to go already getting pretty good signal to recognize those items, so you
can kind of push up with that and then kind of keep improving it with the actual annotations.
The other one for us is like we'd like to be able to do segmentation or bounding boxes,
but it's really expensive to get that data at scale.
For us, we want to recognize like thousands of different foods and take us forever and
write some of these at dollars, it's actually annotated at not level.
So yeah, that's what I envision you were doing.
So right now you're identifying images that have a single thing and using that to train.
So even if you have multiple items in an image, what do you want class at a time?
Okay.
So we'll do multiple passes over it if it has multiple items.
Okay.
So most of the time we've released stuff, it's like four or five items on the plate.
Right.
Or it would be a soda, you know, a program fries.
So an example like that would do all the fries first.
We might be able to pick up on correlations, they're super good as well, but we do want
class at a time when you do sweet love.
When you say you look at the plate and you do all the fries first, are you talking about
when you're training or when you're doing inference?
Oh, no, when we're doing the cleaning annotations, the cleaning annotation phase.
Yeah.
But kind of we're hoping that if the class arrives good, the next time we go around,
the burger class five picks it up.
So we'll see that image again.
Right.
We'll see if the burger prediction is correct or not.
Okay.
And so this kind of iterative process, so what the degree is it, like have you automated
all that into some kind of pipeline or is it still, are you like manually kicking off
runs to do annotate for object X or so we kind of have a batch job to do the initial
annotations.
We have a system that's called a Persian Django, like a React front end that will take
those annotations and then have a simpler class of like an SVM or a logistic regression
to do ranking after.
So once you have the initial like suggestions for labels, it doesn't have to be a little
network.
It could be just web data.
You pick up on keywords of strawberries and images on the website that also matches
strawberry.
You kind of, it seems to weak label as a chance of it having a stripper.
And so you index that in this database, you do the skins and kind of can go class by
class.
Okay.
So that pretty much is what I mean.
Okay.
Interesting.
And how about the crawling part?
Was that where there are any challenges involved in that or was that pretty straightforward?
I mean, for the most part, it's just, it's not a challenge to download lots of stuff.
What the real challenge is actually getting decent labels and, you know, we figured out
some techniques to basically, you know, the right sites and the right places that actually
have decent labels.
And we actually even took some unlabeled data so that we could actually supplement the
weak labels we have.
So we can get more examples and that's kind of how we started.
It's just, you know, it takes a little bit of time to get it right.
Mm-hmm.
Okay.
Interesting.
What else are you doing that's cool and interesting and that we should know about?
So right now, we have an issue with package products.
We have a model that works well on whole foods, people seem pretty happy with it.
Okay.
But one, we don't have a huge database of bulk of products, of scans and distribution
data.
So now we're also working on being able to use OCR and computer vision to kind of scan
a product, be able to index it, be able to tell what brand it is, what the health claims
are, what the ingredients are just using computer vision.
Another thing we're playing with is using a ARCAD, and you say, our technologies to do
the portion size estimation.
Interesting.
You know, we have an entire working, you know, an initial idea for this, but the idea
does it will be, we take a picture, we'll get a point cloud, we have a user, pick a portion
size for now, but now we have a point cloud, we have the image, we can do some segmentation
and we have the portion size.
So over time, we'll get a more data to actually nail that part of the problem.
How does ARCAD work?
I haven't had a chance to look under the cover, sir.
We have, we have an intern environment now.
For now, you can get horizontal planes and if you only get a point cloud.
So for us, like, we have a camera open, not only to get picture, but also to get a point
cloud.
So we can get some depth information and figure out the size of the items.
Oh, interesting.
And are you like thinking about taking it the next step where you kind of project on
top of the, like use ARCAD the way it's designed and like project something on top of the
plate that says like, don't need this or, you know, put an X over the surprise.
Yeah, that would be a great, I mean, we were, in terms of ARCAD, we've
we're playing with a different user interfaces.
So one would, one is like Michael's mentioning is that we could project a cup to help you
understand what a 12-ounce cup versus a 16-ounce cup is and kind of put that next to each
other.
So you can select, oh, yeah, this is, you know, a pint glass versus this.
There's like another mode where maybe we need to also do measuring because it's not
perfectly a cup or perfectly a plate.
We can allow people to do measurement.
I don't know if you've ever played with these measurement apps are pretty easy to use.
That's kind of how we envision using AR now.
I mean, in the future, if there were AR glasses, we'd actually love to be able to project
like the information that we know about food onto the real world and that's like a friction
free environment.
A, we're recording what you're eating so you don't have to log because if you have another
passive device absorbing that information, it can now understand, hey, you're eating
pizza with burgers today, maybe tomorrow, you should eat something else, you know, to,
you know, feel better.
So I said, General, the goal is to get to this very passive mode, right now we still
give you suggestions, but for some classes, we're pretty much at a Himalayan lackier
seat.
It's kind of things like burgers fries, like when we predict it from the most part, it
is accurate.
Okay.
So we want to get to a point where this promotional user interaction, you take a picture
or you have some classes, you go by your daily and you're able to measure all the stuff
about your diet, tell if there's weaknesses, tell you how to improve, kind of have this
dashboard where you, track it, don't have to do any work.
The other thing we've noticed is people tend to eat similar stuff every day.
So soon we'll be able to predict where you ate before eating.
If you sit around at home, if you go to coffee shop and you always have a lot to have this
coffee shop, we should be able to predict that you had it and log it for you without
too many of the work.
Like using location services, you just walked in a Starbucks and I had a pumpkin spice
latte to your daily.
Yeah.
Yeah.
Yeah.
I mean, even actually for other meals like, you know, during the week you usually eat XYZ,
so why should you have to go and tap stuff?
We can just kind of fill that in so that, you know, the meals that really are different
or the ones you have to actually log.
We can actually kind of build part of that experience already now, because, you know,
we don't need necessarily AR glasses.
We can just do that based on location, post time, day, you know, what meal it is.
Hmm.
So you mentioned some of the challenges you're seeing in terms of getting this out to
market.
Like what are the top, you know, end things, top three things that you feel like you need
to be successful based on where you are now?
I would say that we need to have product market fit, like relative to the other food
logging apps.
We just can't be, you know, having features missing because people say, oh, I'm so used
to this.
Yeah.
I need it.
That's kind of a, that's a known known.
We know how to do it.
We just, it just takes us a little bit of time to get the features right and, you know,
make them look beautiful.
I mean, that's another very important thing with consumer apps is that it has to look
really good.
The bar is really high now.
Secondly, the thing is there is a lot of other apps in this space.
And for us, you know, they're incumbents.
People know them by name brands like my fitness pal.
So well, for us, it's we have to, you know, kind of become visible and we probably have
to become visible through some other, through other channels and then just pure search because
most people are searching for those main brands.
So I think those are kind of our top two challenges in terms of marketing and the app out
there.
And on the tech side, data is always, for us it's always about having data.
So there's, right now for some of the classes that we predict, we don't have the niche
for data.
So we don't need to show us suggestions.
So we might be able to predict that data, but because we don't have niche for data tied
to it.
Like, we don't have to show that prediction to users.
So did you, did you gather a database of nutrition data by searching and that kind of thing
or by just finding a database?
So the USD has a very big and detailed data set.
It's about a hundred thousand different common things people eat in America.
Oh wow.
And it's broken down by ingredients.
The common portion size is, it has the full nutritional breakdown.
So we're using that for now.
Okay.
But now we're working ways to like pull in restaurant data, to pull in product data and
kind of help us increase that database.
Okay.
Shout out to the USDA.
They do really good work.
Like, their nutrition database is quite complete.
What's interesting is that actually we learn that other nutrition databases for other
countries actually depend on the USDA database.
They actually have references to the English database and that's long term international
expansion is a little bit more tough for problem for us than most people A, we have to convert
our labels.
So the things that we call pizza is probably universal.
But you know, in the morning pastries, people call those things different things in different
languages.
We have to do all that translation, not only that with the nutritional facts, there's
regional variations with how things are prepared.
And then of course, regional dishes, which actually vary by look depending on where you
are.
And we've been getting a lot of requests from international users, hey, can you recognize
XYZ for me?
And we're like, we don't even know what that is.
Nice.
Do you have any, there's so many challenges there like all kinds of foods that are, you
know, look like one thing on the outside, but they have stuff inside.
Yeah.
All right.
The nice part about having this app that you get to control yourself is that we get to
consider correlations, we get seeding patterns, and I use other signals to improve the
predictions.
Based on the time of day, you can say this is probably a coffee, not say hot chocolate.
Kind of using other signals to improve the predictions.
We're hoping that one point will be able to say given, given that you selected these
foreign ingredients, we can figure out the portion size based on how we see it calling.
Okay.
Yeah.
I mean, just basically taking a recipe, you know, the ratios, and this is actually like,
you know, this is actually integrated into some of the USDA data, like you have, they have
some internal equations, basically from that we realize, hey, you can actually take recipes
and understand what the ratios are, and we can integrate that to things that the, you
know, other new things that we learn about from restaurant menus or from web recipes,
which makes it even easier to log.
You just take a picture of this sandwich, which is probably, you know, these things.
For most people, that accuracy is great, you know, because otherwise, they have no infertional
information in front of them.
Okay.
Awesome.
Well, Vinay and Michael, thanks so much for taking the time to chat with us, really appreciate
it, and really enjoyed longing about your company.
Cool.
Thank you for your time.
Thanks.
All right, everyone, that's our show for today.
Thanks so much for listening and for your continued feedback and support.
For more information on Michael, Vinay, bite.ai, or any of the topics covered in this episode,
head on over to twomolei.com slash talk slash 65.
To follow along with the NYU Future Labs AI Summit series, which will be piping to your
favorite pod catcher all week, visit twomolei.com slash AI Nexus Lab 2.
Of course, you can send along your feedback or questions via Twitter to add Twomolei or
at Sam Charrington or leave a comment right on the show notes page.
Thanks again to NYU Future Lab for their sponsorship of the show.
For more information on the AI Nexus Lab program, visit futurelabs.nyc.
And of course, thanks again for listening, and catch you next time.
