Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting
people, doing interesting things in machine learning and artificial intelligence.
I'm your host, Sam Charrington, hey what's up everyone, we've got an amazing Twimble
Con AI platform's event shaping up for you and I wanted to share a bit about it.
The event is going to be anchored by what we're calling keynote interviews.
These are live podcasts recorded right on the main stage and I'll be joined by some incredible
guests, here are three that I'm really excited about.
One, Andrew Eng, I could not be happier to have Andrew kicking off this event with me.
Andrew is one of my AI heroes and no one has done more to bring new practitioners into machine
learning and deep learning than he has.
Andrew is going to be sharing a bit about what he's learned helping many businesses get
productive with machine learning and AI and also speak with us about where he sees the
field going.
Number two, Hussein Mahana, Hussein is the head of AI and ML at Cruz, the self-driving
car company.
Before Cruz, Hussein helped build Google's cloud ML platform and Facebook's internal
FB learner platform before that.
Hussein is going to be sharing some of the lessons he's learned building ML platforms
from scratch at some of the most advanced companies in the space.
And number three, Fran Bell.
Fran leads a team responsible for data science platforms at Uber.
These are use case focused ML platform supporting application areas like forecasting, anomaly
detection, NLP and conversational AI segmentation and more.
Her platform sit on top of Uber's Michelangelo, putting here on a unique position to speak
with us about how low level and higher level platforms come together to support data scientists
and developer productivity.
Beyond keynote interviews like this, we've got a bunch of outstanding speakers lined up
to share their successes and failures, helping their organizations build and production
lies, ML and deep learning models.
If this lineup sounds interesting to you, visit twimmocon.com today to register.
Use the coupon code great content through July 31st for an additional 10% off of our
special early bird pricing.
Hope to see you at Twimmocon.
Alright everyone, I am on the line with Emma Strubel.
Emma is a PhD student at UMass Amherst.
Emma, welcome to this weekend machine learning and AI.
Thanks a lot.
Thanks for having me.
Excited to talk to you and I'm particularly excited to have an opportunity to talk to you
now.
Just a few days before you'll be defending your thesis.
So the best of luck on that, I'm sure by the time this, by the time this shows errors,
you will, actually, it sounds like you'll be hiking someplace and on your way to being
a visiting scientist at Facebook AI research for a year before you start as an assistant
professor in the Language Technologies Institute at Carnegie Mellon.
So congratulations on all that.
First of all.
Thank you so much.
Yeah, it's very, very, lots of exciting changes.
Yeah, absolutely, absolutely.
So how did you get to this place where you have so much opportunity, like, and focused
on AI?
Like, what tell us a little bit about your path and your story?
Yeah, so it was not at all on purpose.
And actually, I have thoughts about that, which we could talk about if you want.
Okay.
Thoughts are always a good place to start.
Yeah, I didn't even know that, like, sort of grad school or, like, getting a PhD was
really a thing that I could do until undergrad.
And I had professors who kind of, like, maybe pushed me in that direction.
I got involved in some undergraduate research.
But actually, when I applied to graduate school, I intended to study, sort of, like, computational
biology.
So I was actually interested in studying and, like, understanding, like, computational
power of biological systems.
But my relationship with my initial advisor did not work out.
And I was, like, really fortunate to just be introduced to Andrew McComb, who's my current
PhD advisor by someone else I knew in my PhD program.
And then I started working with him, and I actually just, like, found that I really enjoyed
the work.
So what department are you in the CS department, or?
Yeah, it's technically the College of Information and Computer Sciences.
When I joined, it was actually the computer science department, but we've grown a lot.
And that was also my, yeah, my undergraduate degree was in computer science and, like,
a minor in math.
And I also took some biology classes and stuff like this, because I do, I like biology
a lot.
Okay.
Cool.
Also at Amherst?
Nope.
I actually went to the University of Maine.
Okay.
So, yeah.
How do you kind of describe your, your primary research interest nowadays?
I'd say, like, the overall goal of my research is I want to build, like, I want to bring
state-of-art NLP systems to, like, actual practitioners who actually want to use them.
And I actually think there's, like, kind of, like, a large disconnect here between, you
know, people who are doing, like, state-of-art NLP research and sort of, like, chasing these,
like, high accuracies on these benchmark data sets.
And then, but if you take these models that get really high accuracy on some benchmark
data set, like, and then run them on your, like, actual data, like, your actual documents
or whatever, like, that accuracy tends to go way down.
And then similarly, I think people, like, the most accurate systems tend to be, like,
also the slowest, oftentimes, sort of, like, the biggest models that take the most computation.
So, I guess, like, two angles of my work have been, you know, developing machine learning
models that are as efficient as possible.
So trying to get that state-of-art accuracy, but requiring, like, much less computation.
And also developing models that are going to be robust so that we can train them on, like,
the annotated data that we have.
But hopefully, those same models will also perform well on, like, data from many different
sources.
And when you think about practitioners, are you thinking about any particular domain
or are you, kind of, tackling it broadly?
Yeah, I mean, it's generally, like, I hope that my research will apply to practitioners,
like, kind of, from any domain.
That's the hope.
I do.
So specifically, I've worked with collaborators in material science, which has been really
cool.
So they're interested in being able to extract, like, recipes for new materials, essentially,
from the literature.
And so I've been working with them to try to develop models that can do that.
And it's been really, really interesting.
Oh, interesting.
That use case is the literature has the recipes in it, but it's in English, if that's what
you, if that's the language you're studying in B, if that's what you can call what's written
in material science, academic, journal, particles, and you're trying to-
Yeah, exactly.
I can't understand.
Exactly.
Exactly.
And so you're trying to capture from there some representation of what the recipe is.
Yeah, exactly.
Going from some of the unstructured, it is English, yeah, research, article text, to, like,
some structured representation, that's, like, these are the material precursors, then,
you know, you heat them for this amount of time, and then, like, drive them and add this
other thing in, and then you get this result that has these properties.
So then, once you have that information, you can kind of, like, analyze it at a large
scale to, like, poss- like, our dream goal would be to, like, be able to generate new material
recipes, like, given some target material generating the recipe, because, basically, it's
done through, like, trial and error research in, like, a lab right now, so it's a very slow
and error-prone process.
Awesome.
It's a fun application, because it has, you know, this is, like, how we're going to
develop, like, sustainable energy, right?
These are, like, the materials that are used for, like, solar cells and stuff like this.
So it's a fun project.
You kind of describe this problem of, you know, having these state-of-the-art results
in academic literature and practitioners not being able to use them, and you're primarily
focused on, it sounds like, kind of, the general-generalized ability of the results, but it also
kind of speaks to, like, a whole, you know, repeatability in machine-learning research
kind of issue, as well.
Are you tackling that?
I'm not yet, but I actually have been thinking about doing something like that, like, sort
of similar to this energy and policy considerations paper, which, I guess, we haven't necessarily
talked about yet.
But we will.
Yeah.
But we will.
Yeah, yeah.
So that's something I haven't tackled yet.
But I actually was thinking about, like, sort of, like, doing an analysis of, like, I guess
I was thinking about statistical significance in particular, which is kind of, like, a related
issue.
So, like, an NLP, we don't usually measure significance of our results.
And there's kind of, like, issues with people reporting the max number out of many random
seeds.
And then it's hard to repeat these experiments.
Yeah.
But, yeah, I do think that's, like, a really interesting, interesting thing, yeah.
Okay, well, since you spilled the beans, you introduced the topic that we're going to be
talking about this.
Sorry.
No, no, no, I'm just teasing.
I'm just teasing.
So this is a paper that you, it's a pretty recent paper, at least according to the, the
most recent one that's up on archive, was this month, right?
Yeah, yeah.
I actually haven't presented it at the conference yet.
So, like, the speed of research is this.
Yep.
So the paper is called Energy and Policy Considerations for Deep Learning in NLP.
And what conference are you presenting it at?
It'll be A.C.L.
So it's the, uh, annual conference of the Association for Computational Linguistics is, like, one
of the top, like, international NLP conferences, um, it's in Florence in July.
Oh, nice.
That's a good one to go, too.
Yeah.
Well, I think it's, like, peak tourist season.
So, like, I don't know if it's, like, totally the best time to go there, but, okay.
Okay.
Yeah.
Um, so, uh, why don't you tell us a little bit about this paper?
What were your goals in writing it?
Yeah, absolutely.
Um, so I talked about how, like, one of the focuses of my research has been developing
these efficient models.
And so the motivation for a lot of that research is, um, basically that we want, you know,
that we even want machine learning models to be efficient and that we want them to be
efficient because we care about the cost in terms of money, but also in terms of, like,
the cost of the environment, because just bigger models require more energy, which has
more, like, CO2 output, basically.
So that was, it has been our, like, motivation and a few of the papers that are written.
Um, and, but then I realized that no one is really, like, uh, quantified this, especially
not an NLP.
So there's been, like, but, yeah, I don't know of any work actually really quantifying
this in terms of, like, the carbon footprint of training these NLP models.
Um, and at the same time, there's been this, um, surge recently in NLP, um, or this,
I guess, trend of training, like, bigger and bigger models on more and more data and,
um, these models are performing really well, like, getting really high accuracies, um,
but they're just, like, the computational resources required to train them are, like,
enormous, like, such that, you know, a researcher like me who's not currently at, like, a big
company, like, Facebook or Google, like, cannot afford and just, and I don't even have
access to, like, the hardware to train these models.
So we're talking about, like, language models, like GPT-2 or, okay, Google just a couple
of days ago.
Uh, I think we're in the Excel net, uh, and some folks did, uh, kind of back at the
envelope analysis that, uh, showed concluded that it costs $245,000 to train that model.
Yeah, exactly.
Yeah.
That's not accessible.
Yeah, no, not really.
I love, like, it's a pretty large portion of, like, an NSF grant, but, uh, yeah, yeah.
So basically we were inspired by, yeah, the fact that we didn't see people quantifying
this and, like, I think there's been sort of, like, an exponential growth in the actual,
like, energy and computation requirements of models recently.
Um, so, yeah, we just wanted to, to quantify that.
And then also, I guess, talk about some conclusions based on our, our results.
Yeah, and so one of the things that, uh, that you do is kind of compare the estimated
emissions for training some of these models with kind of the things that we usually associate
with, you know, environmental damage or at least emissions, like, air travel and, you
know, car travel, things like that.
What did you find there?
Yeah.
There's like a number of numbers, um, I guess one thing is training one of these models
in this particular way.
That's like very sort of computationally intensive.
Um, so this is like training, uh, a machine translation model using this new technique called
neural architecture search required basically multiple, let's see, what's it three, four?
I'm not going to do math in my head, but multiple times the entire carbon footprint
of like a car in its lifetime, including fuel and manufacturing, um, which is ridiculous.
And that's like, you know, that's even more carbon footprint than like the average American
life.
I guess.
So I should say one of the, one of the things we did is we, um, we analyzed like the total
carbon footprint of, um, basically my last paper, like the research and development required
to, you know, develop the, the last model that I published.
Okay.
It's like just like the carbon footprint of that, well, the work that I did, like easily,
like doubled or tripled, uh, my personal carbon footprint last year, which I found, like
super alarming.
Okay.
Wow.
Yeah.
Yeah.
Huh.
Were those the two models or scenarios that you were primarily, primarily looking at
that your, uh, NLP pipeline and this kind of transformer, neural architecture search model?
Yeah.
I mean, I'd say we looked at a variety of these, um, like these popular pre-trained language
models.
Um, so we looked at, uh, like LMO for GPT-2, um, and then yeah, we were interested in looking
at neural architecture search in particular because, uh, I think like the, yeah, I mean,
it's a very, uh, computation, it's a very computation-hungry approach.
And I think like the, uh, like the resulting benefit is relatively small, um, and so I
had a feeling it would be kind of like a drastic number, so I wanted to compute it and kind
of maybe encourage people to be a little bit more responsible about, uh, like sort of like
these brute force techniques.
Yeah.
I've had a number of conversations with folks and seeing like blog posts, uh, where folks
try to capture, uh, in this case, the case I'm thinking of the, like incremental cost
of training a model, you know, per incremental percentage accuracy benefit or business benefit
and like try to encourage people to think about that, you know, as they're devoting resources
to building out these models, um, yeah.
And, you know, it's not something that I hear people talking about a lot, um, and I haven't
come across any particularly like rigorous way of doing it or, you know, frameworks for
doing it or anything like that.
And I wonder in your research, did you, did, were you somehow able to like, you know, really
get into that kind of the incremental carbon emissions per incremental benefit and accuracy
or something like that?
Did you compare that across different techniques?
That's a great question.
We didn't, but, um, people should totally do that.
But I'm out of here.
I'm not really happy to do it too.
I just had to, you know, buy the time.
Yeah.
So, like, take away as from this paper that we write about is, um, basically that authors
should report numbers that make it easier to do that comparison.
And so I think there's some pretty straightforward, um, things that people could compute for like
any machine learning model that would like allow us to compare sort of like, like how
much computation is required basically to get that accuracy.
And then it would be like super easy to just, yeah, compute those numbers.
So if people could compute, um, or report just like gigaflops to convergence, that's like
a pretty easy thing.
Like, uh, there's like a couple papers where they've reported that like when they're trying
to compare and show their model is like better, um, I think if that was just like a standard
thing we reported alongside, you know, accuracy or whatever of their metric, I think that would
be like really, um, it's like not that hard and, um, it would just really allow us to make
that comparison directly, um, and similarly like something that's a little bit less straightforward
but that would be really helpful in a number of ways is reporting, um, sort of like sensitivity
to hyper parameters, um, because like a lot, yeah, a lot of this energy use actually comes
from sort of like how wasteful we are in terms of, um, you know, tuning these neural network
models that, you know, notoriously require like a lot of finessing and tuning.
And that tends to be like if you want to train out a new data set, you know, you have to
do some kind of tuning, but we don't, we do like a really bad job.
This goes back to what you mentioned about reproducibility. I think we do a really bad job of like
actually being really precise about how much of that tuning happens, um, and then how
much is required like there's like, you know, there's a lot that goes into development of a model
and develop it as a new technique. When you think about this, are you thinking about it like
from a pipeline perspective and hey, you know, this is an iterative process and we're, you know,
kind of playing with all these hyperparameters, uh, to actually get to a model or are you thinking
about it like, I don't know if this actually makes any sense, but like somehow dropout is more
expensive than, you know, something else from a, you know, technique perspective during training.
Oh, that's super interesting. No, I wasn't thinking about that, but dropout is like more,
probably more expensive than like some other technique just because it's like way less
simple efficient, like if you train with dropout versus I thought there's other like more direct
regularization approaches. I was not talking about that, but if you want to collaborate on,
something else for someone else to figure out. Yeah. That's, that's great. Yeah, people should
like listen to your podcast for like cool research ideas. Yeah, no, I was thinking more just
in terms of like if you want to take some like published model and then apply it to your data
to sort of like a new domain, I think it's really hard to tell like how much tuning is actually
going to require it and people just don't report that. Yeah. So it's both kind of like not super
reproduced. You know, you'll say, oh, we did a good search of these things, but like, I think
it's not, it's not actually that straightforward always. You know, characterizing the cost of just
inefficiencies in the way we do this stuff like, you know, imagining there's, you know, some set of
techniques that, you know, people still do all the time that are like far from, you know, best
practice or state of the art that, you know, probably just like throw, you know, tons of CO2
emissions out there for not. Like, have you given any thoughts on anything like that?
That's interesting. I mean, this is not exactly what you said, but something that came to mind
is sort of, I think people are very eager to use like deep learning when they can definitely get
away with like just like a single like logistic regression. So that comes to mind as like something
where, yeah, like at the call, yeah, people want their technology to sound cool and to use the
cool stuff. But yeah, I think for, you know, for some, yeah, for many tasks, you don't really need like a
big deep fancy model. You can just do like classification, like a, yeah, shallow classification.
It'll work fine. Right. Right. Yeah. I mean, all of these, all of these questions are kind of
interrelated. And I guess that goes back to the fundamental contribution of this work, which is,
hey, we really need to be thinking about how much it costs us to get this incremental, you know,
percentage of, you know, performance by whatever metric you're using.
Absolutely. Like something like neural architecture search, where it's kind of,
I guess it's going back to the reproducibility thing. Like, you know, take GPGP2, right? A bunch of
people have reported like trying to reproduce GPGP2 and OpenAI hasn't said everything that you
need to do to get the exact kind of results that they got. Like, do you, you know, if you're
trying to benchmark these things, do you even know like how close you are to the actual
cost to produce their model? Does that make any sense? Yeah. Yeah. That's an interesting question.
I think if I understand what you're saying correctly, it's kind of like, uh, do we actually know
what goes into training? Like, are estimates even reasonable? Like, did we, uh, yeah? Well, I guess,
you know, so part of the question, so maybe taking a step back, like to train the, the neural
architecture search based model did presumably you actually implemented that as opposed to
determine the cost analytically or no? Yeah, no. So yeah, I will describe to you our methodology.
Okay. With the exception of GPGP2 actually, we, we basically took so all the models we analyzed
had open source code. We took that code. We like just had it perform training for like up to a day
and then we sampled like the actual energy draw of the GPU or GPUs and the CPUs and the main memory.
So making the assumption that like the main energy draw from the computer is going to be from
like the, these copy, the computational hardware essentially. And then we extrapolated that
based on, uh, the amount of time and the hardware, um, as reported in the paper that they
used to train the model. Oh, okay. Yeah. So we didn't, because it would have taken, I mean,
as I said, at least two hundred and forty five thousand dollars. Yeah. Right.
So like, I'm like, I think the entire thing, but I figured this is like a reasonable, uh,
way to estimate it. And I'm happy to talk more about like how we then convert that to carbon
and stuff like this because it's a little bit like, you just have to make a lot of assumptions
along the way. But we figured it's like, we tried to make reasonable assumptions and it's like,
we think our estimates are like ballpark reasonable. Did you run it for the day on the exact
hardware that they said they used? If they said they, if they reported the hardware that they used
or did you have to extrapolate to that as well? Yeah. We had to extrapolate to that as well.
But we did make up effort to use hardware that's like fairly similar. So like,
so the GPUs that we used basically have it have the same. So like one of the metrics like
that's reported for a GPU is sort of like, it's maximum power draw. And so the maximum power draw,
the GPUs that we were using, uh, is the same as the maximum power draw, the GPUs they were using.
So yeah, I think it's like probably the, yeah, power use is similar. And they're,
they're not like incredibly different. They're like, you know, similar age GPUs and stuff like
this. But yeah, it wasn't in most cases. It was not the exact same hardware. And then in here,
you talk a little bit about the ultimate source of the energy, like whether it's renewable versus
gas, coal, etc. Like how did that, how were you able to fit that into this model? Do the various,
like data center providers, cloud providers, do they report on this stuff?
Yeah. So they're not super transparent about it. Surprise. Yeah. And so these numbers, we got
these numbers from, well, so for the cloud providers, we got the numbers from a 2017 white paper
that Greenpeace did. And it seemed like they surveyed the companies and got these numbers
somehow from them. Yeah. And so, and then so in our computation, we used basically the mapping
from like kilowatt hours of energy to carbon produced or like CO2 produced as provided by the
US EPA, like that's like the average for power consumed in the United States. Okay. And so if you
look at the breakdown of AWS's energy, and so like obviously like where the mapping from like
electricity, the carbon footprint is very dependent on like these sources. So if it's from
renewable sources, it maps to zero. And then if it's from nuclear, you know, maps to less than from
gas or coal, like much less. So yeah. So if you look at the actual breakdown of energy resources
as of 2017 of Amazon AWS, it like pretty closely mirrors the breakdown from the US. And AWS is
like the largest cloud provider. So for that reason, we thought that it was a like a pretty like
reasonable estimate for mapping. So since coming out with this work, people from some of these
companies have contacted me and let me know that these numbers are out of date and they're doing
better now. So yeah, we're considering, yeah, we're considering doing an update like in the fall
for something with more up to date numbers. And of course, and something that we don't like address
a ton in the paper is that a lot of companies are also moving more towards actually many of them.
I know like Google at least is like 100% renewable in terms of like I don't want to say renewable,
but they're yeah, basically they buy carbon offsets to make up for like energy in their cloud
that does not confirm renewable sources. So that's good. That's better than nothing, but it's also not
the same as like using actual renewable energy. Yeah, but they are relative to the other providers
given this 2017 data kicking, but on the renewable 56% for them relative to 32 for Microsoft and
2017 for AWS. Yeah, definitely. It seems like they really care about that. And also like TPUs are
actually, so all the numbers that we were able to compute were for GPU just because we don't,
we aren't able to get the power draw of TPUs. But I do think TPUs are much more, I mean because
they're better at doing this computation, they just like don't take as much time to do the same
amount of training. So also a technology developed by Google that I think, yeah, it's just like much
more energy efficient than GPUs. So that's nice. Although it's a little sad that it's like,
you know, this, I guess they're all proprietary technologies, but GPUs feel, I mean,
sorry, TPUs feel a little bit more proprietary than GPUs because I can't like buy one.
I'm like pro energy usage. Right, right, right. So you, so you could not
include Excel net in this paper. You just don't have enough information because it's so
all the data they provide it was TPU based. You'd have to do a lot more extrapolation than even
you've done here. Yeah, exactly. So for like the BERT model, for example, this was also originally
trained, you know, at Google on only TPUs. But since then, like Nvidia, like retrained it on GPUs.
So we were able to use like those numbers. But yeah, for, yeah, exactly for Excel net, it's like
unclear. Yeah, exactly what the footprint would be. I guess if I made, if I made like a follow-up
update, make like a blog post or something, I would try to get some, hopefully if it's not
too proprietary, try to get some like energy draw information for TPUs so we can do that estimate.
Right. Because I think it'd be really interesting to actually see like how they compare it
concretely. This work is very specific to NLP. Have you, do you have any specific thoughts on
like how it applies to, I mean, I guess the methodology is pretty general. And what you're
really saying here is that we should be doing this. And that's another exercise for someone
else is to apply to computer vision models and the like. Yeah, yeah. So there is some other people
have done work comparing, let's see, in computer vision in particular, you know, it's like really
a popular area. Let's see, what were they looking at? So I don't know if anyone doing the mapping
to like carbon footprint, but there are people who have compared to like the energy use, like
the energy draw required by the GPU for using sort of like different neural network layers in,
you know, like these big image classification networks and stuff like this are like, you know,
different batch sizes and stuff like this. Yeah. So there's some work they didn't do, like you could
take, I think their numbers and then do like that extra step with mapping the carbon footprint.
I do think like the NLP models, especially these like, you know, notoriously enormous
pre-trained language models are just like some of the most like energy, hungry models in machine
learning. I would say like the other other area that's probably it was just like using insane
amounts of energy would be the work at like open AI on doing like training, you know, training
reinforcement learning models to do like various games. I think that's going to be like probably,
I mean, I think the technique for training this is like kind of similar to neural architecture search
and that you're actually training like a ton of different models. And you kind of early in our
conversation, there's a hint of judgment that, you know, the incremental gains that we're seeing,
you know, going from, you know, one model to the next Elmo, the birth to GPT-2 to excel in that,
like, doesn't necessarily justify the increase in training costs. Like, did you try to quantify
that specifically? Yeah, so I think the question you asked earlier, I'm kind of would have quantified
that like we had just computed like a ratio. Yeah, yeah. Yeah, like the difference in, you know,
compute required versus like the increase in accuracy, we totally showed on that like I think
that would be nice because a lot of these models do evaluate on sort of like some of the same metrics.
Yeah, so we didn't really quantify that except, yeah, that we noted for the neural architecture
search that they get an increase of like 0.1 blue score, which is really not a lot on like English
and German machine translation. Yeah, at the cost of, we say like at least $150,000 in on-demand
compute time. Yeah, I think, I mean, I think yeah, that sort of that approach in particular,
is just a very brute force approach. I do have like a little bit of judgment about that. I mean,
I think like the gains. So I didn't make that up. Yeah, it's true. Elmo versus Bert versus
Excel now. You know, I do think that they are, like, there are actually pretty substantial differences
in accuracy between those models on, you know, these like standard NLP benchmarks. And I think
that that's that is important to do. Something with the neural architecture search, I feel like maybe
there's like like some more like well thought out research that we could be doing to like not just
do a brute force search on architectures to find like really small increase on like a single
language pair. Is there a kind of argument or devil's advocate thing here that says like really
the innovation and all these models is in their use in transfer learning. And if you think about
that, the cost to train these models is amortized across lots and lots of users. And similarly,
the incremental performance gains accrue across lots and lots of users. And so it's worth it.
Yeah, totally. I mean, I am definitely not sick. You know, I'm an NLP researcher. One of them
closely analyzed is like my model. Yeah. And I'm just gonna like stop doing research or something.
Like I'm gonna continue working on these models. Right. Right. And I do think like Elmo and
Bird. And like I guess now ex on that like, yeah, I mean, the aggregate increases on these core
NLP tasks resulting from these models is like awesome. It's like really cool to be like in this field
at this time. Yeah. So I like by no means saying we shouldn't be doing this research and we shouldn't
be using these models. And so I totally agree with you that. Yeah, that's like a great sort of
like counter argument that especially these models in particular, you know, they're sort of like
pre-trained on this large amount of like data. They don't require like explicit supervision. So
you can like kind of easily collect this large amount of data and then just train on it.
And then these models are like really applicable in a lot of different. Yeah, I can sort of
provide them as input in a lot of different tasks. Yeah, I mean, that's totally true. I still think
it's important to characterize the costs. Especially, yeah. So like another one of our conclusions is
sort of like in terms of like access to computation resources. So I do think if you want to use one
of these models in a new domain. So they tend to be trained on like web crawls and like news,
corpora. Yeah, that kind of thing. Yeah, yeah. And so I still think like the so I'm sure if you
initialize some model, let's say that you wanted to run on these like material science journal
articles or like whatever. So I'm like, yeah, research articles in a very specific domain. I think if
you, you know, if you initialize your model with one of these models, like probably it will help,
but what you really want to do is train one of these models on a corpus of that data,
which is actually something that we did in our research. And are you making a distinction between
training and tuning? Yeah, so we fine tune. So that's true. You don't need to, yeah, given the model,
it's already learned a lot of sort of stuff about, let's see if you pair about English. It's learned
a lot of stuff about, you know, English that is sort of that domain independent. I guess it
probably is domain independent. Yeah, so that's a good point. You only need to fine tune it,
but it still takes like a lot to fine tune it basically in, yeah. Yeah.
Oh, yeah, so sorry, what I was saying is, yeah, basically like in terms of like funding and stuff
like this, yeah, just developing and using these models can still be pretty expensive for academic
researchers or even, you know, just, you know, smaller companies and smaller groups who want to
use this technology, but just don't have a ton of money or ton of resources. So it would be nice
to have like public resources or something so that people have more access. Any other takeaways
or conclusions that are worth noting or exploring in this paper? Yeah, so there's one other thing
that we talked about in the paper, which I guess I maybe touched on a little bit earlier,
but like I think that we already have, so like when I analyze like the model that I trained,
a lot of the cost was just like, yeah, doing this like tuning and development and stuff,
basically doing these like just grid search as a hyper parameters, so like just like considering
like every pair of, or every like combination of like K hyper parameters, which is like a really
dumb approach. That's really like wasteful and we have techniques, you know, like Bayesian
hyper parameter search and other stuff that are that are better, but at least in my experience,
like most people are not using them. So I know Google has like an internal tool that does this,
so people do do it at Google, but the other tool is like not a good source, for example.
So I think like another thing that would be great is if these big like deep learning tool cuts
that like everyone is using like TensorFlow and PyTorch, etc, you know, made more of an effort to build
in these more energy efficient approaches that already exist, so it's like easier for people to do
more efficient modeling. That would be great. Nice. Shout out to our friends at Sigopt that does this
for Bayesian optimization, but it's I think another take on it, like they, you know, go in and are
talking to people about like performance and, you know, making better models, and this is a whole
different take on it and looking at the efficiency of doing it. Yeah, absolutely. I think, yeah, that's,
I mean, for me, that's like a big gain of, yeah, like Bayesian searches. Yeah, just like being able
to actually search the space intelligently. Right. Well, I guess yeah, in terms of energy use,
rather than just like the accuracy. I think it's often like easier to show that, at least in my
research, something I've enjoyed is like rather than trying to like make some number higher, I tend
to be just trying to like match some number, but like make the model more efficient. And I think
it's actually a lot easier. Yeah, because like everyone's trying to make the number higher. It's
at a certain point. It can be challenging. Cool. Well, Emma, thanks so much for taking the time to
share what you're working on. It's very cool stuff and an important paper for sure. Yeah, thanks
so much for inviting me. Yeah, I'm really glad because I think like, yeah, it's a really important
thing for people to think about. So I'm grateful that you invited me on the podcast and talk about
this so that we can get some more visibility and more people thinking about this. Yeah, absolutely.
Oh, I guess one more thing that I'd like to say is that there's been kind of like a lot of
discussion about this paper online and we want to like encourage that discussion and provide,
you know, like a place for that discussion to happen. So we were going to upload the paper on
OpenReview, which is like a forum where you have like a paper and then you can have like discussion
by people. So I'm playing on doing that like today or tomorrow. So maybe we can think or something.
But yeah, hopefully if people want to like discuss more, hoping to provide a forum for that to happen.
Okay. Well, yeah, shoot us the like and we'll include it in the show notes page.
Awesome. Thank you so much. Thanks Emma. Thanks.
All right, everyone. That's our show for today. For more information on today's show,
visit twomolai.com slash shows. Make sure you head over to twomolcon.com to learn more about
the twomolcon AI platform's conference. As always, thanks so much for listening and catch you next time.
