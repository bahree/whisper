Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting
people, doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
In today's episode of our AI and sport series, I'm joined by Mike Schumacher, director
of business analytics for the Portland Trailblazers and Chen Hui Hu, a data scientist at Microsoft.
In our conversation, Mike Chen Hui and I discuss how the Blazers are using machine learning
to produce better targeted sales campaigns for both single game and season ticket buyers.
Mike describes some of the early use cases the Trailblazers explored in their drive to
apply analytics and machine learning to the process of increasing ticket sales.
And Chen Hui elaborates on some of the unique challenges of the Trailblazers data set
and the modeling techniques the team apply to solve their problem.
All right, let's do it.
All right, everyone. I am on the line with a couple of guests that I've been looking
forward to speaking with.
I've got Mike Schumacher, who is director of business analytics with Portland Trailblazers,
basketball team and Chen Hui Hu, who is a data scientist at Microsoft.
Mike and Chen Hui, welcome to this weekend machine learning and AI.
Hi, Sam. Thanks for having us.
Yeah, thanks for having me.
Absolutely.
Why don't we, in the tradition of the podcast, get started by having the two of you introduce
yourselves.
Mike, how did you get involved in analytics and machine learning?
Sure. So my career started in software development.
I got my masters in enterprise software architecture and was an application developer
for 12 years before coming to the Blazers.
In those first few years before I made it to the Blazers, a lot of my work shifted from
peer development over to data warehousing, business intelligence, data analysis, data
vis, those types of things.
And my first few years with the Blazers was heavily focused on data management, getting
our data warehouse set up, marketing, automation, CRM.
As our systems and processes became more sophisticated, we started looking into machine
learning and more predictive analytics and that's kind of where we're at today.
Okay, great.
And Chen Hui.
Hi, I'm a data scientist at Microsoft AI and the research in Boston.
I joined Microsoft last year and my current interest is to solve prediction and optimization
problems in retail, marketing, and manufacturing.
Before that, I finished my PhD study, focusing on medical image data mining.
That's pretty much my respect.
Awesome.
Awesome.
So Mike, my producer who stays on top of these things tells me that a good place to start
is to congratulate you for putting quite the smack down on the few suns on an opening night.
Yeah, I did not see that coming, especially with one of our best players sitting in
the game out.
So that was unexpected, but a very pleasant surprise for sure.
Absolutely.
Absolutely.
Why don't we get started by having you tell us a little bit about some of the applications
for analytics at the Blazers and how that has grown to involve machine learning?
Sure.
Yeah, we spend a lot of time working with various departments across the organization.
We're almost like the data police at times.
There's a lot of data warehousing that we do.
We have integrations between different systems.
We help with any type of data analytics project.
The department might need help them help them use data to make better business decisions.
So that's kind of the basis of our group.
We do a lot with CRM, marketing automation, a lot of one-off projects where we're presented
with a problem and we go out and do the analysis to try to help make a decision on something.
And so that's kind of where we started.
And as our foundation has grown and we've been able to build out that foundation with
the data warehouse, we've started to reach out into these other areas like the predictive
analytics and the machine learning.
And that's where we decided to partner with Microsoft who went over and did some use
cases with them and they've turned out pretty successfully and so we've been moving in
that direction.
And how many people on the analytics team there?
We have six on our analytics team right now.
Okay.
We get involved in both front office types of use cases as well as back office player
analytics and that kind of thing.
We have two sides of the business.
Our team in particular focuses on the business side.
There is also some basketball analytics, but our team isn't as involved with those.
Okay.
All right.
Great.
So maybe we can start out by having you walk through one of the early use cases for machine
learning with the team.
Sure.
Yeah.
So when we first talked to Microsoft, we looked at how things came from in the past.
And so we used data in the past to run some of our sales campaigns.
But a lot of times we didn't have all the data we would need to set the campaign up the
way we would like to.
And so what we wanted to do was we wanted to come to Microsoft, give them access to some
of our ticketing data, some of our sales data, and ask them to help us identify patterns
that would allow us to target people in the right way.
So we have over 200,000 people that have purchased tickets in the past.
And we can't reach out to every single one of them and give them a phone call and talk
to them about if they'd be interested in coming out for another game.
And so the idea was as we wanted to use the data to determine who would be best to talk
to, who else might be best just to be reached out to be an email or some sort of marketing
and automation campaign.
And so that's kind of the use case that we went to them with.
So you do have folks that are actively outbound dialing out to pass ticket purchasers to try
to get them to buy tickets for an upcoming game via the phone.
Yes, absolutely.
We have a sales team that's that's working with new customers.
We also have a service team that that nurtures the season ticket holders and helps them maintain
relationship with those pongoing buyers.
Okay.
And so what were some of the first steps in tackling this challenge, which is ultimately,
ultimately it sounds like trying to sell more tickets?
Yeah.
The tactic that we went with was we wanted to make the data points that we had available
that we thought were predictive in understanding who would be interested in buying tickets
again, make that available to Microsoft, try to kind of stay hands off with in terms
of our bias, not necessarily bias, but our history of what we thought was the type of
person we should be targeting and give them that data, allow the machine learning to look
at all the different data points and how they interact and help us essentially score
the customers and identify who would be a potential customer again.
Okay. So Chen, when you, when this project kind of landed on your desk, how did you approach
it from the Microsoft perspective?
So I joined this project, I think in the second phase.
So initially, my colleague has been working with shareholders on two different use cases.
One targeting at predicting the seasonal ticket sales.
The other is targeting the Singletic game ticket sales prediction.
And I imported the first use case to a new environment called Azure Mission Learning
Workbench, which allows data scientists to do a lot of data preparation and divide
up models, experiments and then deploying them at cloud scale very conveniently.
So we kind of like improved our original work in this new environment and then made a
demo in the ignite conference this September.
And so was that primarily for the first use case or the second or did you have the
same couple of steps with both use cases?
For now, we only imported the first use case, but yeah, if time permitted, we can also
do the same thing for the second one.
Oh, okay.
And so what were some of the early findings with that first use case?
Did you find that there are any unique challenges associated with either the problem or the
data set that the blazers brought you?
Of course.
Yeah, I think there are two at least two unique challenges in this project.
First of all, we need to combine different data sources to profile each customer.
For example, we have data from charbracers describing the demographic information, customer
status, customer interest, purchase and patterns, attendance information and even team preferences.
And then on the other side, we have the third party data from axel, which pretty much
tells about the lifestyle interest, broader interest of each customer.
And then we need to also process those data in a Azure data SQL database.
And then we need to combine the data processing step with the model development step in the
new environment and so the so called adrup email workbench environment.
And then the second challenge is the process of develop the machine learning model.
And then to come up the best machine learning model for prediction, essentially, I think
the simplest way is just to share a bunch of models and then tune the hyperparameters,
but it turns out pretty challenging to to log the history of all the experiments.
But thanks to the new environment we have in the Azure Azure machine and workbench, we
are able to log all the hyperparameters performance metrics very easily and then visualize them
easily pick the best model.
Okay, now that's a topic that we've covered on the podcast, quite a bit of late, various
approaches to hyperparameter optimization.
It sounds like what you're describing is some kind of inherent features within the
Azure ML platform that allow you to track kind of different runs with different sets of
hyperparameters, like what exactly are you doing there?
So essentially in the Azure ML workbench, you can write like a Python script, for example,
and then chain the machine learning model with different hyperparameters.
And it allows you to also log the parameters that you are using and the performance metrics
that you end up to have in this model.
And then if you later change another set of hyperparameters, you can easily see how the
hyperparameter is impacting the performance metrics.
It will automatically generate a set of graphs telling you the impact of hyperparameters
on each of your performance metrics.
So does it end up being like a sensitivity analysis or is it more maybe more manual than
that?
So it's the original goal is like to provide you a purely automatic way to select the model.
But of course, this is a pretty challenging.
So now we have, I think it's kind of semi-automatic way because you can write another script
just to do a parameter suite, meaning that you try different combinations of parameters
and then just run a for loop.
And after running this for loop, you end up with the curve that I just described, describe
your like the impact of the hyperparameters.
And then, ideally, you can also write another script to pick the optimal model based on
certain criteria.
OK, so it sounds like it's giving you some kind of facility to implement your own grid
search of the hyperparameter space.
What types of models, like what did you learn about the models themselves for this type
of problem?
And you know, were there any surprising findings there?
And yeah, so we basically tried many standard classification models.
For example, support vector machine logistic regression and boost decision tree random
forest model.
So first of all, almost all the models perform very almost the same in terms of the accuracy.
But the boost decision tree model is slightly better.
And in terms of the accuracy and precision recall, we end up with very high number, something
around like 0.98 in our testing experiments.
So that's pretty amazing to me at the first time.
And then some other findings also are pretty interesting.
For example, if we are targeting the prediction of seasonal ticket purchase, the information
about historical ticket purchase in patent, like whether a customer is a single game ticket
purchaser, it's very important for the prediction.
And I think also the amount of money that the customer has spent in the last season is
also a strong indicator, apart from that, the income or the occupation of the customer
can also tell us a lot because the seasonal ticket is sort of expensive.
Then people who has high income may tend to keep purchasing this seasonal ticket.
Those all strike me as fairly intuitive findings.
Like was there any surprises in the results that you got back from Microsoft?
Yeah, I'd say that there were certainly ones that were intuitive.
There were ones that jumped out to us like specific games or specific sets of numbers of
games that a person has attended in the past.
It was, you know, it's different combinations of those variables that we hadn't looked
at before.
A lot of times we're looking at something very specific like this person purchased,
you know, five games in the past.
So there's someone that's very likely to want to come back to a game the following year.
There was just this combination of all these different data points.
And then in one of the other use cases we looked at the Axiom data, so like the demographic
psychographic lifestyle data that Axiom provided, data points that we'd never had before.
Some of them that are obvious like watching particular sports channels or are those
types of things actually were also predictive in the likelihood of someone purchasing.
And so it was, it was pretty cool to see, you know, all these different data points
at how they interacted.
Can you give an example of the, you know, counterintuitive combination that you saw?
Yeah, for example, there was one like there was a, and I'm not sure why, but anyone that
had been to a Minnesota Timberwolves game and had a credit card, or sorry, a loan that
wasn't a credit card was also, there was a significantly higher likelihood that they
would purchase tickets again.
Something that belonged that wasn't a credit, so there was something totally out of left
field that yeah, I'm not have expected all really interesting, yeah, definitely.
And so how, how have you used the, you know, the results of this work is this kind of
running in production, or how have you integrated into your selling and marketing workflows?
Yeah, for us, it's been kind of, we've been trying to prove out the software, I guess
you could say.
So we've just been getting into the machine learning and predictive analytics.
And so what we wanted to do is run through a couple of use cases that we could actually
measure the results to see if this is the tool we wanted to go with.
And for like the first use case we went through, normally on our sales campaigns will
convert it about 5% of the leads that we contact.
In this particular case, we converted it to 25% and they were purchasing season tickets
in general.
So, you know, very substantial.
We went through and did the second use case and looked at like what's the likelihood
of a single game buyer coming back the following season.
And in general, we get about 16% of our single game buyers back each year for various different
reasons with the models that Microsoft helped us develop.
We were able to identify them at a 30% rate versus a 10% rate of those that weren't coming
back.
And so they were definitely useful.
Can you explain that last point again, like you were taking the model and then using
that to target your outbound calls and you were able to increase the rate, the return
rate for those people that you wouldn't have otherwise or for that group of people?
Well, we weren't able to, we didn't change the rate at which we were converting them.
With our single game buyers, we'll have about 15 to 20,000 people each year that are individual
game buyers.
With the models we wanted to use, we wanted to look at and see who was likely to come
back.
And so Microsoft went through and identified, you know, we identified the top 10%.
We said, these are the people that are going to come back and of that 30% actually came
back.
Whereas the rest of the group that was identified as unlikely to come back, only 10% came
back.
So it was essentially was showing us that the model was able to identify these people, the
people that we were looking for a lot better than our other, the other ways that we were
doing this.
And so in terms of like using that for the future, we, our, our eventual goal is to get
into like a fully automated system where a customer or a fan comes to us, they opt into
us either through a previous purchase, they sign up for some sort of sweepstakes, they
join an email mailing list or something.
And essentially, we'd like to be able to pipe them through the models, have them scored
and then determine, you know, what's the best way to work with them?
Do we want to get them into some sort of email drip campaign where we, you know, send
them newsletters and we near term along that way?
Should we be sending them sales marketing, should we be putting them in a call campaign and
having a sales rep talk to them?
And so essentially, we'd like to have it to a fully automated system, but we're still
in the early stages just trying to, to make sure that the tool works way we wanted to.
And now Azure, Azure ML is kind of a developer tool.
How do you envision using that in the context of your various business processes?
Would you be developing custom applications around it or using some interface where you
can run reports against it?
Have you thought that far about it?
Yep.
Yeah.
So on my team, we have an application developer.
We have a couple of analysts.
We also have people that are focused on CRM and on the marketing and automation side.
And the nice thing is is they play really well with Microsoft.
And so essentially, we have the in house capabilities to be able to load the data into
the models.
And with Azure ML, you essentially get like a web service endpoint with the results.
And so the idea there would be we would hook up some more ETL and then pipe that data
over into our marketing automation system or over into CRM.
And then from there, use the automations within those systems to move forward with the
records.
And then where do you see this going?
What are some of the other use cases that you've got in mind for this?
Yeah.
Aside from like the fully automated scoring system, we're looking at other things.
There's some built-in text analysis that we want to get into.
We get survey data.
We'd like to be able to make that survey data more actionable.
Be able to look at the sentiment of the customer and be able to track that over time.
We'd also like to look at some things like help, help on other projects that we look
at each year.
So one in particular is what we call our drop count model.
And that's essentially how many people come to any given game for various reasons, whether
it's for staffing or for just understanding what the sales might be for that game.
Being able to model that out with this system rather than what we have been using in the
past.
And so there's a variety of different things that we want to get into.
Some automated and some just one-off situations where we want to use the algorithms to be able
to get us more efficient.
And so you ran a couple of use cases.
You got some promising early results.
You're still kind of in advance of having this fully integrated into the way you do business.
What are some of the barriers or impediments that keep you from kind of being all-in already?
Time and workload, I guess, which I'm sure you'll hear from a lot of people.
Uh-huh.
No, we got a lot of great resources, but there's a lot of other responsibilities going
on.
So it's just getting that time to be able to work with it.
System integrations have come a long way over time, but there's still development that
needs to be set up to integrate between systems.
So we're working with a couple of Microsoft products.
We're using Tableau for our data visualization, our marketing automation tools, and other
system as well.
So there's a lot of integrations that need to be set up.
And then, again, just identifying where we can get the most bang for our buck and the
time that we do set up for the development because there's a lot of projects that this
could be worked on or could be used on, but finding the one that would be most useful is
also another challenge.
Right.
For some of the future use cases, do you envision having Microsoft like similarly do some
of the data science pieces of it, the model development, or are you building capacity within
the trouble lasers to do that kind of work?
And where are you on that maturity curve?
Yeah.
Well, both.
We want to continue our partnership with Microsoft because they've been great to work
with, but we are building the capacity internally.
I have some great people on my team.
A couple people in particular are ramping up on this.
We have an application developer that is very well versed in the integrations with different
Microsoft systems.
He manages our CRM system, which is also Microsoft.
And so we have a good set up there.
I have an analyst on my team that she's been spending countless hours just going through
all the training and all the tutorials we've had.
People come out and so just building up that internally.
I'd say we're still in the early phases and that's probably probably why it's taken
us a little bit longer to get there because we don't have a full-blown data scientist on
our team.
But we're certainly working on building that out internally while we continue to work
with Microsoft.
Is there a particular skill set that you can identify that's still lacking?
No, not so much.
I mean, it's essentially just getting used to the tool, getting used to how it works, understanding
the different algorithms.
There's a lot of different ways that things can be done and just gaining that maturity
and experience in the data science type areas.
Right.
All right.
Now, Chen, we have a question for you.
I've looked at Azure ML Studio in the past and it can be, I think of the various cloud-based
approaches to data science.
I think it has an advantage in being pretty approachable because you can do a lot of gooey,
almost whizzy wig.
The type of thing that I think a lot of people expect from Microsoft.
But it's also often developers and data scientists don't really like that kind of thing.
It gets in their way and I'm wondering for you as a data scientist that has deep knowledge
of this tool, how do you approach problems?
Do you approach them within that gooey construct that is the default for Azure ML or are there
other ways that you use the tools?
We're a great question.
So actually I have been using Azure Mission Learning Studio for about a year.
So indeed, we have both pros and cons for this product.
I think it's a great tool for entry-level data scientists because as you mentioned, they
are like gooey and you can jack and jaw on the plan and then you can construct a mission
learning experiment very quickly.
And then also you can deploy it as a web service.
But in the meantime, for senior data scientists, they may want to have more flexibility to
control the machine learning model and also maybe do more complex data preparation steps.
So that's why recently Microsoft launched a new tool called Azure Mission Learning
Workbench, which is primarily for senior data scientists.
Again, to end data science and advanced analytics solution for data scientists to like
prepare data, develop experiments and deploy models at the cloud scale.
But it also allows you to do a lot of things like without using the gooey.
You can write everything basically using a Python script or develop the model, compare
the model within the Azure ML Workbench and then you can also deploy it on different
sources.
And so is the user experience there?
Does it offer kind of the notebook paradigm that's becoming pretty popular for this kind
of thing?
Or is it more at the command line and code level?
Yeah, we have both actually.
So first of all, if you are familiar with I-Python notebook, you can directly jump in to
use that in the Workbench environment.
If you are more familiar with Python script or command line environment, you can develop
a Python script and that script through the command line environment.
And more importantly, you can also deploy a machine learning model either on a local machine
or on a cloud cluster through the command line interface.
And one of the things that the ML Studio does for you is that it's kind of operating
at this higher level of abstraction where you've got these building blocks for different
types of models like you can drag and drop a random forest block model, for example.
And then kind of wire it up with your data.
Are you, do you have access to the same kinds of things in the Workbench?
Or are you rather shifting to kind of your full native Python and scikit-learn and
the kind of traditional Python ecosystem tools?
So at this moment, we are kind of shifting towards the Python, IPython notebook, scikit-learn
or kinds of tools instead of like again providing the GUI.
So we talked about some of the kind of pros and cons of that tool set in general.
Were there any challenges or places where you ran into kind of hit a wall or had to change
your strategy with regard to which tools you used in solving the specific use cases
that you worked on for the trouble users?
So for this use case, I would say because we are not trying to analyze a huge worry
about data and then also deploy it like, for example, Spark as a machine learning model
on Spark, the data size is pretty, so far it's not, it's pretty small.
So we can do the model development work both on EdgeML studio and EdgeML bench.
So the primarily goal of improving our work in the new environment is to try to demonstrate
the new capabilities of this EdgeML work bench.
Okay.
Okay, so you were able to do the things you needed to do with studio work bench is something
that you'd go to if you wanted to scale it up or in terms of the data or wanted to
deploy it out to Spark and you may deploy some use cases there just to kind of show them
this new platform and how it compares to the original.
Yeah, I would say the primary goal is to showcase, but also we want to work with chair
brazers in case they want to move towards the new environment so that we can help them
understand how to land on the new EdgeML work bench.
And this question might be a little bit inside basketball, I guess.
This is for Chen Wei, are you with the data scientist title, are you in a consulting organization
with Microsoft or are you aligned with a product organization and just working with this
particular customer because of their profile or how did you get involved in working on
this project?
Okay.
So for my organization, originally, I think we are more tied to the product team, which
is called Azure Mission Learning as you may know already.
So we are actually the Azure data science team under the Microsoft AI and the research.
So right now, this I think intersection between both the product team and the research team
in Microsoft.
So essentially, we want to bring the advanced research results to the product team so that
we can develop like machine learning products based on those algorithms.
So that's where we came.
So we will try to stand in the middle and then try to understand both the research results
and also the need for the customer about the product.
And are there any research results that you can see playing a role in the types of problems
that the trailblazers are dealing with?
So for me, I'm pretty interested in like reinforcement learning and deep learning as well.
So I was thinking about two different use cases.
For example, if we have like a really large amount data, say like millions of billions
of records, then we may try to build a model with machine with deep learning.
So now because of the the goal original was trying to prove a concept.
So we did not like use, I think a large volume of data.
But later, yeah, if that's the case, we can try deep learning.
And then another use case I was thinking about is to use reinforcement learning to learn
the behavior of the customers and then try to send them offers which may sound more
attractive to them.
Can you walk us through how that might work?
So this is again, so that's based on my intuition.
It's not like a very mature idea being discussed with trailblazers.
But for me, I think if we want to like a sale ticket to our customer, the first step
is to predict whether the customer is willing to buy it or not.
And then the second step maybe is to determine the best price, right?
So in that sense, maybe we need to dynamic change the price according to certain criteria.
For example, if there are other competitors like MBA teams offering the same discount
to your customer, and then you may offer like at least the same discount, right?
So a lot of factors may play a lure in this kind of price optimization use case.
So I think again, machine learning such as reinforcement learning can be a good tool
for solving such complex problem.
So you're thinking that your reinforcement learner would be kind of exploring this
state space of like different types of buyers and different prices and things like that
and trying to find an optimal path through that.
Yeah, yeah.
So essentially what you can control is from I think it's mainly the price or the discount
you offered to your customer, right?
And then the environment that you are sitting in is very complicated.
You have competitors, your customers may have different background.
You cannot control all of this.
So that's why we need to use reinforcement learning to dynamically update your price
and discount to check your customer.
And I'm guessing that Azure ML Studio doesn't do reinforcement learning yet.
Yeah, indeed.
We don't have reinforcement learning as maybe a toolbox, but internally we have another
tool called CNTK.
In CNTK, actually, it has a comprehensive set of functionalities.
So it also offers you the flexibility to develop reinforcement learning.
Actually, I think in the newly released version, it has some tutorial about that.
Right before we close things down, are there, if there were three things that you could
get from Microsoft in there in terms of the way they supported you or their tools, do
you have a wish list for them?
Offhand, I can't think of anything.
We've been very excited about the results.
I guess just being able to ramp up our internal knowledge, which they've been helpful with,
and just getting the ability to spend some more time on these types of projects would
be the two things that we're looking for.
In terms of the tool itself, it's been great.
There is anything at the moment that I can think of that we would add.
Okay.
Awesome.
And can we anything else that you would add or would like to share?
I would say that it's a really great experience to work with shape razors and then to come
up with the solution and trying to improve the existing solution that we have built.
So I really look forward like continuing working with you in the future.
Cool.
Awesome.
Awesome.
Well, I really enjoyed chatting with both of you and appreciate you taking the time.
Mike, I wish you and the Portland Trailblazers a great season.
Thank you.
Appreciate that.
All right.
Thanks, guys.
Thank you.
Thank you.
All right, everyone.
That's our show for today for more information on Mike, Chenhui, or any of the topics covered
in this episode, head on over to twimlai.com slash talk slash 156.
To follow along with the AI and sports series, visit twimlai.com slash AI and sports.
If you're a fan of the pod, we'd like to encourage you to head to iTunes or wherever
you listen to podcasts and leave us your five star rating and review.
We're super helpful as we push to grow the show and the community.
As always, thanks so much for listening and catch you next time.
