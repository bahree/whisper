Hey, everyone. I am here with Jonathan LaRou. Jonathan is a senior principal research scientist
and Mitsubishi Electric Research Laboratories on Merle. Jonathan, welcome to the Twimal AI
podcast. Hi, thanks for having me.
So we'll be digging into your work in the speech and audio field broadly. And to get us
cut up, I'd love to have you share a little bit about your background and how you came to
work on applying ML to those problems.
Sure, yeah. So my background actually started with mathematics. I studied math, the pure
math for a while until my master's and then did a master's with Cedric Villaini, who later
got the fields metal. And I was supposed to do my PhD with him. But I wanted to do a gap
here in China. So I need to, because I love, I studied Chinese and I like languages.
So after spending a year in China and I got back, he said, I'm going to Stanford. He
met as well as spending over here somewhere else. So I decided to go to Japan because I met
friends there in China, Japanese friends there. And I started studying Japanese and then
I realized that maybe math, pure math wasn't actually so much my thing. And I fought, I looked
around what I could do and I thought that math and language is kind of mixed well. And
I also, I was really into music and, you know, I said playing with in a band with friends
and I thought like kind of the intersection of all his interests was the speech and audio
field. And I got introduced to a professor at University of Tokyo, who was in there.
And so I was able to do my PhD with him and with a professor in Paris. So that's kind of
how I got into the field of speech and audio. And I'm really happy I did.
That's awesome. That's awesome. And a lot of your work is focused on this classical problem
in the field called the cocktail party problem. Can you tell us a little bit about the background
of that problem and, you know, some of the ways that you've kind of built your research
program around it?
Sure. So yeah, I worked and my core of my work is centered around a social separation, audio
social separation. And in the field, the kind of the holy grail is the so-called cocktail
party problem, which was kind of stated in that name by Cherry in 1953. And it's kind
of, if you imagine yourself in a cocktail party and we humans have this amazing ability
that we are able to entertain a conversation and kind of to focus on pretty much any given
sort of sound that we wanted to focus on. And despite all the background noises and the
reverberation and all these interferences that are happening in this very cluttered
acoustic scene. And so what we've been interested in in my group over the years has been to
kind of tackle this problem, to chop at it. And it's first by working on speech enhancement,
which is the separation of speech from noise. And then later on we worked on speech separation,
which is separation of speech from speech. And this was kind of a big milestone because speech
and noise have very different characteristics. And so you can kind of use this at your
advantage to separate them from each other. You can use machine learning to learn these
characteristics and then kind of classify one and one from the other. When it comes to
speech from speech, then it wasn't clear really how you would handle this. I mean,
because they have, by definition, the same kind of characteristics. So the main challenge was
how to deal with that problem. And so that's kind of where we started developing methods to do
this efficiently using deep learning. Nice. And is the ability to separate speech from noise or
speech is that a necessary prerequisite for a machine to be able to attend to a particular speech
signal strikes me that we don't necessarily separate we attend to. It's a good question. I mean,
it's actually a very good question, whether we humans actually effectively separate in the brain or
not. And I think there is some some hints that we are actually to some extent separating the
signals. Okay. And maybe we're not separating all the signals. It's more like an attention mechanism
where, you know, like the higher part of the brain kind of guide the lower parts of the brain to
decide which features which parts of the sound to focus on and to cancel other ones. So we're not
completely separating a scene as a machine maybe potentially could do. But there is some hints
that in neuroscience that's kind of kind of part of the process. It's not necessary. You're right.
Like you could totally design a speech recognition or somebody's speech recognition algorithm that
does not explicitly separate before recognizing noisy speech or even multi speaker speech. And
that's actually something that we did do. And we're not the only ones. So we,
basically, even nowadays, I would say the state of the art in like kind of single channel,
noisy speech recognition does not necessarily involve enhancement. It's sometimes surprising.
Even though it sounds much, much better, it's often a better strategy to just include more noise
in your training data and to let the neural network that does the transcription, you know,
be robust to noise inherently by the version of training instead of like, you know,
thinking that you're doing the network of favor by removing the noise when you're actually
probably introducing some artifacts that are bothering the system. So it's not always the best
strategy. But it, I mean, going forward, I think as the enhancement algorithms get better and
better, it's likely that it will become probably probably the state of the art to include some form
of enhancement. Got it. Got it. Maybe a good way to talk through your work in the field is to start
with a recent paper. And as you kind of describe what you're doing there, you can talk through some
of the work that's led up to it. And so that recent paper is a new formulation of the cocktail
party problem that you call the cocktail fork problem. What does that mean? Yeah, so that's,
we tried to be a little bit cute there. And so basically this project started with myself,
not being an English native speaker and trying to listen to movies and not always being able to
catch a dialogue with all the special effects and the music. And I was like, and I really like
to be able to just, you know, separate the dialogue and enhance it. And some some smart TVs,
you know, claim they can do this and they can do some extent, but they mostly rely on, you know,
some form of equalization. They don't actually separate the sources. And so we thought, well,
you know, we have all these separation technology, they can separate speech and speech. And now
nowadays we also can separate, you know, multiple types of sounds from each other, like for like a car,
from a, from a belt, for example. And it was kind of a natural thing to try to see, well, can we
just, you know, formulate this problem as to separating complex acoustic scene into like speech,
music and sound effects or sound events. And that's basically what we did. Like we,
it's like carefully crafted a dataset that of that uses these free types of sources and try to
replicate with as much realism as we could the mixing process of like real soundtracks, like movie
or TV show soundtracks. So in terms of loudness or in terms of amount of overlap. And to be
able to training set basically. And then we trained our kind of somewhat standard algorithms
for separation. And we treat them a little bit in order to account for the specificity of a task.
So that we called it the cocktail fork problem because there's free outputs. And cocktail
forks happen to have three outputs, free branches. So if it wasn't a nice hint at the cocktail party
problem to, to call it the cocktail fork problem because it's a, it's a tiny, it's a tiny,
easy version of the cocktail party problem. And so the, given that you started with a
kind of an inorganic training set, you, you created your training set from his component pieces,
you know, thus bypassing one of the, you know, might otherwise be the hardest part of,
of training a model here. I imagine that the, on the back end, the generalization was a big
challenge in building the model. It turned out that it actually generalizes pretty well. So we,
the demos that we put online, we're trained on, only on this synthetic data set. And, and we,
we, we show examples that we got just from taking YouTube videos like movie trailers or TV shows
and applying the network pretty much as, and exactly as it was. And it, it, it does make mistakes.
But it does generalize pretty well. And actually an interesting thing is, at first, we,
we trained the network at 16 kilohertz something rate. So we, we downsample the signals. So,
because it was lighter weight easier and faster to train and to try things. And interestingly,
we, we first used a wrong version of the speech data set, but was downsampled with a slightly
too harsh band pass filter, like low pass filter. And it was, so it was leading out a bit of,
of frequency, of the higher frequencies of speech. And when we applied it to, to these real videos,
we, there was a lot of bleed from the speed, the high frequencies of speech, a very
squeaky speech in the other sources like, and we're like, what, what's going on? And, and actually,
we realized that actually we, the version of the speech data we had was not full band. And,
and when we, we trained our models on, on a better quality data that it just went away.
So kind of interesting that like, if you, if you listened to it, you wouldn't notice,
but the network definitely was not able to learn how to separate the high frequencies for speech.
Oh, wow. It did the model. Is the model language agnostic?
So we only trained it on English, but we found that these,
these speech separation models are pretty language agnostic. A few years back in 2017,
we had a, we did a, a demo of speech separation. And that was trained on 30 hours of English,
you know, world-sweet journal, red sentences speech. And we were, this was a live demo in Japan.
And, and we were separating, you know, English from Japanese for, and Japanese from French,
right? Like, people who visited the demo tried every crazy combination. And it worked pretty well.
I mean, it's really, it's not picking up on the language content. It's really the acoustics.
And when it's, when it's separating, how clean is the separation of the language? If you,
you listen to the separated language, do you hear artifacts of whatever might be going on in
the background or... So it depends how you trained. Yeah, back in the back then, we only used
clean speech for training. And actually, it is still a challenge in the field to, to expand,
like to extend these methods to, to work well on noisy and especially, especially in reverberant
conditions. So like the, the data set that we put out in 2016, when we first came up with this
method called deep clustering, that kind of, kind of revived the field of speech separation. Because
the first one that uses deep, use deep learning, you know, way that it could separate unknown
speakers. You didn't have to train on a specific pair of speakers. It could be anybody. And
we created a data set to do this based on the Warsaw Journal. And this is still used to this day,
but it's kind of completely beaten up, like the performance on that clean data set has become
so amazing that it's, it's pointless. It's like super clean over 20 dB. And so there's no,
I think we solved that task, not we, but the field. The community has solved that task. And
the challenge now is to moving to more challenging scenarios with noise and especially with reverberation.
So we put out a couple data sets to encourage for a community to, to move on to that. We call them
WAM, the Wall Street Journal, WSJ hipster ambient mixtures. And it's like, like the band WAM.
And it's, people are now using this, this data set as well, to try, try the algorithm in a
more challenging scenario. The performance is, of course, much worse in reverberant and noisy
conditions. And when you say reverberant and noisy, you're talking about when the training,
the speech training data has noise. Both for training and the tests. So typically, like if you
don't train on data that has noise and reverberation, that could be a completely fail. But even if
you include noise and reverberation in your training data, the performance is clearly not as good
as in clean conditions, because the noise, but especially the reverberation really smears the
characteristics of the speech, everything that is normally in a spectrogram. So in a time frequency
representation of your signal, where you show the energy at each time in frequency, speech is very,
very clean. You can see the patterns. But as soon as you introduce reverberation, everything
gets smeared. And so it becomes much harder to first to, for the network to identify the structure.
And then there's more overlap between, because everything gets like mushed. So there's more overlap
between the different sound sources. So you imagine like, let's say imagine like on a spectrogram,
typically you have stripes when you have harmonic speech tends to be, there's a lot of harmonic parts
in speech and these show up as stripes on a spectrogram. So you imagine stripes and you have two
sets of stripes. If they are very clean, you can probably identify them and then make a mask that
cancel one of them and only keeps one. But if both stripes are kind of smeared and become
like kind of mushed, then there's a lot of portion but overlap and it's also harder to see the
stripes in the first place. So it makes it makes it difficult both to identify structure and it's
it's, it's, it is actually, there actually is more overlap between the structures.
So when you, so the way you've set up the, the cocktail fork problem and maybe this is general
to my question is general to other cocktail party problem applications. You've got speech and you've
got, you know, other, you know, background things. It could be other speech or it could be other,
you know, the movie soundtrack and sound effects and the like. And then you pass it through this
model and you have a speech stream that comes out on the other end. And I, I guess independent of
whether you are working with clean speech or noisy speech on the back end, I'm curious how clean
the output speeches or I'm trying to, I have a, a picture in my head of something that is doing
I don't even know how to describe it like you, some other application that's similar to this kind
of separation, but I don't remember where it's from. But like you get the speech, but there's also kind
of this warbling noise in the background that there's a, in, in, in like kind of more,
more conventional, I would say, separation methods. There's this artifact that's kind of like
that are kind of added to the speech. That's something that's typical of deep learning
based methods. We don't observe that much actually. Interestingly, yes, there's still artifacts,
I mean, but they don't sound musical mode is not such an issue in deep learning based methods.
And I'm not sure I have a good insight for why. But yeah, it's sort of like these methods like
introduce sort of a spread out noise across the structure, but doesn't really have a structure,
but then that kind of creates that kind of musical noise. I would say the artifacts in deep learning
based methods are, I mean, most of the time it's really failure to separate like, or you get,
when you're doing speech separation in an era that we very often get is that the speaker,
if you listen to each, because you have two speakers and you separate them,
the speakers actually separate that at any time instant, they're pretty well separated, but,
but halfway through, the other than makes a mistake in how it's stitched thing and then it switches
the speakers. And so if you listen with headphones, you're like, you suddenly have the speakers
to switch sides, they're separated, and that can happen multiple times. So this kind of speaker
permutation problem is one of the issues that we have typical issues that these systems have.
And so people have tried to come up with methods to alleviate that.
But yeah, that's a, invest to a lot of artifacts. It tends to work pretty well,
except when it completely fails. That's a pretty typical thing in deep learning, I would say.
And you've talked a little bit about the spectrograph representation of speech.
Do these models tend to work like in the frequency domain or a time domain, or like,
how do you think about the way they're working? So there are multiple approaches in speech
separation. Historically, a lot of them were based in the time frequency domain. So what we do
is we take, you know, short snippets of signals, we call frames. And so we just put a window
a little bit of signal and do a four-year transform to analyze a frequency content,
of that's more window. And so that gives us a vector. And then we have a bunch of these vectors,
and this is what makes the spectrogram. And historically, people only mostly treated the,
these numbers are all complex numbers, by the way. The free transform gives you a complex number
at each time frequency. And historically, people have looked mainly at the magnitude at the length
of that vector, another of that complex number. And the phase part, the reangle of the complex
number was kind of left as it is because it was thought to be pretty difficult tomorrow.
And if you look at a picture of a magnitude spectrogram, so where you don't consider afraid,
the phase looks very random, but the magnitude looks very nice with these stripes that I was
mentioning. So it feels like you can do something about it. You can do machine learning on it.
And that's what people have been doing. And still to this day, there's a lot of methods that only
handle the magnitude part, like for example, with some transform, like a frequency transform,
like a, what we call mail transform, to gather some frequencies according to some filters.
And that's what is used in, in speech recognition, for example. But, so we started with that.
And for many years, most of the methods were based in the time frequency domain,
nowadays, the state of your methods, either deal, now include the phase, they include both
the model of a full complex number in the time frequency domain, or they ditch, they do not
explicitly use a time frequency transform. They go straight from the time, the waveform,
the time domain signal to the desired time domain signal. Inplicitly, they are kind of learning
the time frequency transform. So instead of using the usual Fourier transform, they use a filter bank,
a bank of filters, and they let the network figure out what these filters should be. So it's very
similar. And it's implicitly based, still based in some form of time frequency representation,
but it's fully learned. So we see both types of methods. Can you talk a little bit about the
the model and architecture that you used in the cocktail fork problem? Sure. So that more
actually, we took from kind of state of the art model from music separation called Trust and Mix.
It was introduced by Sony. And, and it's based on a recurrent neural network,
a bi-directional long short term RNN, so BLCM. The its characteristic, what kind of makes it a
bit different is that in their model, you start from a single, start from the mixture. And then
you have multiple branches that have different, different transforms, different layers
that go through that transformers signal in ways that the network can decide. And then they
average this transform. And this goes through some BLSTM layers. And then they average again,
the outputs. And it's kind of this averaging that's between various branches that is the
special sauce. And so we took this. And the only thing that we changed is that we thought that
in the case of a cocktail fork problem, the signals that we deal with are so speech,
sound effects, and music. And they have kind of different dynamics. They, like some very,
especially with sound effects, you may have very short, impulsive sounds like bangs or claps,
or you may have long harmonic sounds like sirens, for example. And the way you do your time
frequency transform kind of dictates what characteristics of the signal you can focus on.
So if you use a very impulsive sound, you want a very short window in order to be able to
really analyze the frequency content of that very short, impulsive sound. If you have a long
sinusoidal, you want a longer window in order to get a more fine-grained, like to narrow down
the frequency, exact frequency. Otherwise, it's going to be more like a blurry representation.
So the longer the window, the more fine-grained frequency resolution you get, the worse time resolution
you get. Everything gets more blurred in the time direction. And vice versa. So depending on the
characteristic of signal, your choice of window length is going to impact what you can see in the
signal. So what we decided to do is that we already had several branches in the Sony
cross end mix algorithm. And we decided to specialize each branch to have a different window length.
So to make it able to focus on different characters, to kind of nudge it, to focus on different
characteristics of the signal. So that was the main twist on that architecture that we had. And it
did bring a small bit of significant improvement on performance.
And does that window length end up being a hyperparameter that you have to tune for each of your
sources? So yes, in the sense that we manually tried various combinations and looked at which one
performed the best on a development set. Yes. We just tried some variety within a reasonable
set of parameters. There's not many things to try. Got it. You kind of characterize this as like a
toy problem or a subset of the cocktail party problem. Going back to the broader problem,
how do you characterize where we are with regard to solving that problem with machine learning
or deep learning? And what are some of the big research directions and challenges that folks are
working on? Yeah. I would say it's a smaller version, but it's not the list. It's pretty,
I would say it's an important application that is kind of surprising to me that nobody actually
looked at this earlier, because it sounds like it should be useful for analyzing YouTube videos,
for example. If you want to know what's going on in a YouTube video and maybe sometimes somebody
speaking, but you really want to listen more to the sound events, then you would like to separate
the speech from a sound events. And most, and sometimes you have music in the background where you
don't care about. So we think of it as an important pre-processing step that we plan to use
ourselves in our projects for this kind of multimodal content analysis. But to answer your question,
I think the field is progressing very fast in the capacity that we have to separate general scenes.
I'm very excited about the progress that's been made. And you know, there's a lot of work that's
been done on general sound separation sound events. I feel like what is maybe what the challenges
are, are the first, the realism of data, like how realistic can you, can you get, can you make
your data? And because it's still very important currently for these algorithms to train in a
supervised manner. And we're only seeing, starting seeing methods that can, that can be trained
without super, like supervised data. So what I mean by this is the typical way to train these
methods is to get a bunch of sounds, mix them together, and this is your input, and then you ask
the algorithm to come up with the separated, isolated sources, which you already have,
because you mix them together. So that's easy. The downside of that is that that mixture may not be
very realistic. So, you know, we make some efforts to do it, but it's never going to be a real thing.
And so recently some, some groups have developed unsupervised methods where you only have the
mixture. And you don't have, you can't, you can't ask the algorithm to be as close as possible
to the isolated source, because they don't exist. You don't have them. And so one of the methods,
for example, was developed by my former colleague, John Hershey, and his colleagues at Google,
Scott Wisdom, is called mix it. So what they do is they take two mixtures and they mix them together.
And so they have the original mixtures. And what they have the algorithm do is very, very simple
bit of a clever is they have, they ask the algorithm to separate all the sources in this mixture
of mixture. And you have many, you have many sources. And then they judge how well this happened
by picking the best combination that comes back to each of the mixtures. And because the algorithm
doesn't know which sources mix with each other, it has to separate all of them in order to be
able to reconstruct the mixtures. So that's very simple and works pretty well.
So I think these kind of methods and another word that we also did
called weekly supervised source separation, where we trained a system where the reference
signal, the only labeling we had was wherever a type of a sound of a particular type was present
or not in the mixture. So imagine that you want separate dogs from cats from bells from cars.
And your data is mixtures of these sound classes. And in each mixture, which sources are present?
Like is there a dog? Yes, maybe somewhere as a dog and somewhere as a car. But that's it. That's
all you know. And so we devised an algorithm to train the separation under that constraint. And
the idea is that you have a separator come up with separate sources and then you use a classifier
that looks at each source and should tell you there's a dog in there when there should be a dog.
And if it detects a cat, then you did something wrong. So that's kind of one of those methods that
try to reduce the amount of labeling that is needed to train these methods. And that's very
important because if you go in the field and you record something, you may be able to ask someone
to tell you which sources are present, but it's impossible to get the actual real isolated sources.
So these methods that kind of reduce the amount of labeling that's needed to train separation
systems are pretty important to going forward. And I would say maybe yeah, I was going to suggest
another challenge in the separation. Before we move on to the next challenge, are there methods
that approach it from weak supervision from the perspective of, hey, I've got
set of training data that is, you know, it's speech and isolation. I know it's speech and
isolation. I have sound effects, you know, to you know, put in the context of this cocktail fork
problem, sound effects and isolation, music and isolation, but not providing the mixture.
Well, yeah, I think there are such like based on like auto encoders, for example,
there are methods that try to do that. Now the question is like, if you have these sounds, why not
mix them? So that's that's maybe one of a downside of this. But yeah, I mean like in a sense,
if you're able to train auto encoders on each source type, then it's pretty modular. Like if you
want to, if you have a speech autoencoder, for example, this this network is only able to reconstruct
speech. And then you have a music autoencoder, then you can combine them to kind of separate speech
from music. And so if they are these methods, typically, they don't work as well as the best,
the artets of methods that are not based on like generating the output. I think your implicit
responses that a bad mixture is going to be better than no mixture at all. I mean, I'm pretty
sure that the auto encoders are going to go further, but at this point, yes, they don't work as well,
as as this kind of more training based on mixtures.
Of course, yeah, that's and there's another type of seniority that is interesting is if you
actually have example of speech and noise, and you have some time to remember the exact
scenario. So you have example of speech and noise, and you have example of speech, but you don't
have example of noise and isolation. Then you can still kind of do some tricks to train methods
to to separate the speech from the noise. So you don't have, you have a mixture of speech and
noise, but you don't have the corresponding isolate around truth. But you're sort of what speech
sound like. So you can then come up with some tricks to to use these data to train a method,
a separation system. So you're about to mention the next challenge.
I mean, something that is kind of what I'm excited about that we started a little bit working
on still and we need to go further, I think is this idea of hierarchy in in sounds. So we published
one paper on hierarchical source separation, where we try to argue that, you know, source
separation sometimes is actually a bit of an ear pose problem. Because let's say you're in a bar
and you're talking with people and there's a band playing in the background and you say,
well, okay, separate that scene. What does it exactly mean? Like if you're saying, if your problem
is separated separate speech from noise, that's pretty well posed or speech from speech that's
pretty well posed. But as soon as you get into more complex scenarios, you ask to separate the
scene, do you want to put all the people talking together in one group? Do you want to put the band
as one coherent source or do you want each instrument in the band? You want each person separately?
So it's kind of a hierarchical structure of the sound within a scene. And this hasn't been
I could even envision trying and pose some kind of spatial hierarchy, like the people that may
be talking to one another because they're closer than more distant people. Exactly. Yeah, that's
another type of hierarchy. Exactly. So there's many different cues that can be that we can use
when we're in that scene to kind of make sense of it. But this hasn't been, I mean, the spatial
information have been used with microphone array methods, but maybe not exactly in that, in that
with that angle, if I may say so. And so we had a, we tried to argue that paper, but like
people should start trying to think about social separation as a hierarchical, we have a hierarchical
angle. And the first thing that we did in that topic was to consider a music source separation
with a hierarchy of instruments. And so a slightly different idea of hierarchy, but basically,
we were saying that if a user wants to create a system to separate a particular instrument
in a music mixture, and you allow the user to give you an example of what they want.
And so they give you a sound snippet of what they want. And so let's say you have a rock band
and they give you an acoustic guitar snippet. What do they mean? Do they want the acoustic guitar
in that recording? If there is one or nothing else, do they want any guitar electric acoustic?
Do they want any instrument that is kind of harmonic? So it's it's not exactly what they mean. So
or so we thought, well, why not consider that hierarchy? Like so when you the user
queries with guitar or acoustic guitar, you you're going to have a network estimate all these
all these levels of hierarchy. So is there an acoustic guitar if no silence? If there is there any
guitar, then you you you output the mixture of all the guitars in that recording. And then
above is all the kind of harmonic instruments and then the drums on the other side.
And we found that if you train an network to do this, it can actually help it train better with
less data at the fine grain level. So it was able. So for example, if you you don't have a lot of
tracks with acoustic guitar, you have a lot of tracks with electric guitar. The you can network
and leverage that and kind of learn the structure and can improve its performance on acoustic guitars
by learning to separate together the other guitars during train. And do you think is that an
instance of multitask learning? Is that part of why that's working? I think that's it's
some form of multitask learning. Yes. But it's in that case, it's it's a bit more specific in
that the tasks are really related to each other. So we actually impose constraint that the the
output at the upper level should include the output at the lower level. So for example, if you
have electric guitar and acoustic guitar outputs, the their parent is we impose and let's say you use
you do the separation by using a mask. So a mask is what we typically use I mean very often use
for separation. It's numbers typically between zero and one that we apply we multiply at each
time and frequency and we say to zero if we want to basically turn turn there this part of this
time frequency energy off and we put it to one if we want all of it. And so you can apply a mask to
shut off the things you don't want and keep the others. So one way to impose a hierarchy go
consternate say well I have two masks for the electric and acoustic guitar and I'm going to impose
that the apparent mask is at least as large as the max of the two. And so that kind of
imposes a hierarchy course structure and this regularization helps the network train a little bit
there. Nice. Any other challenges come to mind? Or areas that you're excited about?
Let's see I mean I'm also excited on the audiovisual aspects. So not only Sanone but the
what I mean in my team what we're really trying to do is to make sense of a complex audiovisual
scene. And so using using vision with our partners in the computer vision group at Merl and
trying to type this with sound. So we did some work on trying to use vision as an auxiliary
feature to improve sound source separation and in particular try to model how objects interact
with each other to create sound in a scene like think of a bang a stick banging on a pot.
And so this kind of visual cues can help you separate better. So yeah I'm very excited about
trying to I mean our ultimate goal in my team is what we call total transcription.
So it's like if you have a complex acoustic scene is to completely transcribe what's going on.
So if somebody speaks you transcribe the speech. You may also be interested in the emotion and how
they say it. And if there's music you you could separate the music potentially transcribe it as
well. If you have sound events you could you know try to detect them, localize them in 3D space as
well. So that's kind of that more holistic approach that I'm pretty excited about.
Nice. Well Jonathan thanks so much for sharing with us a bit about what you're working on.
Thank you for having me.
