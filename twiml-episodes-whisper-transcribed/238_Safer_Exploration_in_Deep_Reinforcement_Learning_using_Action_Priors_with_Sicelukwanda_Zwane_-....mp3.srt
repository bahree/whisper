1
00:00:00,000 --> 00:00:16,320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

2
00:00:16,320 --> 00:00:21,480
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:21,480 --> 00:00:31,720
I'm your host Sam Charrington.

4
00:00:31,720 --> 00:00:36,680
While at last year's NERP's conference, in December, I attended the second annual black

5
00:00:36,680 --> 00:00:42,720
NAI workshop, which gathered participants from all over the world to showcase their research,

6
00:00:42,720 --> 00:00:45,720
share experiences and support one another.

7
00:00:45,720 --> 00:00:51,280
Today, we conclude our black NAI series with this interview with Sikle Lukwandazwane,

8
00:00:51,280 --> 00:00:56,400
a master student at the University of Vithwatersrand, and graduate research assistant at South

9
00:00:56,400 --> 00:01:01,880
Africa's Council for Scientific and Industrial Research, or CSIR.

10
00:01:01,880 --> 00:01:06,760
At the workshop, Sikle Lukwandaz presented on safer exploration and deep reinforcement

11
00:01:06,760 --> 00:01:11,800
learning using action priors, which explores transferring action priors between robotic

12
00:01:11,800 --> 00:01:18,280
tasks to reduce the exploration space, which in turn reduces sample complexity.

13
00:01:18,280 --> 00:01:23,000
In our conversation, we discuss what exactly safer exploration means in this sense, the

14
00:01:23,000 --> 00:01:27,720
difference between this work and other techniques like imitation learning, and how this fits

15
00:01:27,720 --> 00:01:30,760
in with the broader goal of lifelong learning.

16
00:01:30,760 --> 00:01:32,760
Enjoy.

17
00:01:32,760 --> 00:01:39,680
Alright, everyone, I am on the line with Sikle Lukwandazwane.

18
00:01:39,680 --> 00:01:44,920
He's a master student at the University of Vithwatersrand, and a graduate research assistant

19
00:01:44,920 --> 00:01:50,840
at the Council for Scientific and Industrial Research, or CSIR in South Africa.

20
00:01:50,840 --> 00:01:55,680
Sikle Lukwandazwane, welcome to this week in machine learning and AI.

21
00:01:55,680 --> 00:01:57,480
Thanks a lot, Sam.

22
00:01:57,480 --> 00:01:59,240
Thank you for having me.

23
00:01:59,240 --> 00:02:06,240
So I will just let everyone know that you were very, very gracious in practicing the pronunciation

24
00:02:06,240 --> 00:02:10,760
of your name with me, and I am still butchering it, but thank you so much.

25
00:02:10,760 --> 00:02:15,880
We had a long practice session before, and I am going to work on these, Koza, was it

26
00:02:15,880 --> 00:02:16,880
Koza?

27
00:02:16,880 --> 00:02:17,880
It's a Zulu.

28
00:02:17,880 --> 00:02:18,880
Sorry, sorry.

29
00:02:18,880 --> 00:02:22,880
Yeah, it tends to be very similar to a Kosa.

30
00:02:22,880 --> 00:02:27,960
Okay, so I am going to work on this, and the next time we have you on the show, we're

31
00:02:27,960 --> 00:02:28,960
going to get it.

32
00:02:28,960 --> 00:02:29,960
We're going to get it.

33
00:02:29,960 --> 00:02:32,320
But thanks so much for your patience with that.

34
00:02:32,320 --> 00:02:36,600
Why don't we get started by having you share a little bit about your background and how

35
00:02:36,600 --> 00:02:41,480
you got started working in machine learning and artificial intelligence?

36
00:02:41,480 --> 00:02:48,000
So I got into machine learning because of the problems that I found when back when I was

37
00:02:48,000 --> 00:02:53,660
in this interest group in 2nd year, where we were at this robotics interest group at

38
00:02:53,660 --> 00:02:58,760
VITS, and we were giving these quadcopters to fly around, and my friend and I decided

39
00:02:58,760 --> 00:03:05,600
we want to fly hours in like a rectangular shape, and we found that we couldn't do that.

40
00:03:05,600 --> 00:03:08,400
Like we had issues with things like wind and momentum.

41
00:03:08,400 --> 00:03:11,680
You could never actually get the drone to be in one particular position.

42
00:03:11,680 --> 00:03:16,680
But nonetheless, even though we did fly in some type of kite shape, like I found myself

43
00:03:16,680 --> 00:03:22,720
fascinated by the fact that I basically wrote some code, and I had to sit back and relax

44
00:03:22,720 --> 00:03:26,640
and watch my code basically fly this machine around, right?

45
00:03:26,640 --> 00:03:34,520
So actually I became interested in the field of robotics and continued my studies and tried

46
00:03:34,520 --> 00:03:41,240
to pick subjects even in honors that would basically give me a good start in my career

47
00:03:41,240 --> 00:03:44,760
as a roboticist in South Africa.

48
00:03:44,760 --> 00:03:51,120
Then I think during my honors year, one of the subjects that I picked was reinforcement

49
00:03:51,120 --> 00:03:59,840
learning, and there I learned that the big challenge in trying to get machines that can

50
00:03:59,840 --> 00:04:06,200
think and act like human beings is not so much the actual machinery and trying to create

51
00:04:06,200 --> 00:04:10,600
something that is anatomically similar to a human being.

52
00:04:10,600 --> 00:04:14,920
But the bigger challenge was trying to get these machines to perform like these complex

53
00:04:14,920 --> 00:04:21,920
behaviors, things like that we take so much for granted, something as simple as picking

54
00:04:21,920 --> 00:04:27,160
a pen or picking a cup up and putting it down is something that is still a difficult

55
00:04:27,160 --> 00:04:32,880
task when you want to achieve it with a robot, or rather the bigger challenge was this

56
00:04:32,880 --> 00:04:40,000
thing, generalization, we want to have, we want to want these machines to be able to

57
00:04:40,000 --> 00:04:46,720
perform general behaviors, if I say pick something up, you shouldn't care what the object

58
00:04:46,720 --> 00:04:52,920
looks like, your code shouldn't rely on the actual position of the hand when it does

59
00:04:52,920 --> 00:04:58,120
it, it needs to generalize to all these weird scenarios, for example, if someone has a

60
00:04:58,120 --> 00:05:05,760
bigger cup, if someone has a mug, if someone has a pen, it turns out that machine learning,

61
00:05:05,760 --> 00:05:13,280
or I found out that machine learning was one of the tools that could actually help achieve

62
00:05:13,280 --> 00:05:19,560
this goal through machine learning, we can actually learn more general behaviors to

63
00:05:19,560 --> 00:05:27,160
get more interesting robotic applications, or more useful, actually useful is the word

64
00:05:27,160 --> 00:05:34,600
I was looking for there, that is literally how I got into the field, just by wanting

65
00:05:34,600 --> 00:05:40,960
to see a robot perform some simple task, and then realizing that having the robot perform

66
00:05:40,960 --> 00:05:48,640
that task is not something that is simple to say, it wasn't trivial, one alternative

67
00:05:48,640 --> 00:05:55,160
or as a third year computer science student, I think my skill set would have probably pointed

68
00:05:55,160 --> 00:06:01,320
me towards trying to hard code, okay maybe tilt the robot hand by 45 degrees and then move

69
00:06:01,320 --> 00:06:05,880
it forward a few centimeters and then go down and then open the gripper, close the gripper

70
00:06:05,880 --> 00:06:11,520
and then pick it up and then, but you realize that you just, you created like this behavior

71
00:06:11,520 --> 00:06:15,880
is only specific to this particular scenario, like you leave your robot overnight at the

72
00:06:15,880 --> 00:06:21,560
lab and someone comes and they move the cup by a few centimeters and now your whole system

73
00:06:21,560 --> 00:06:26,720
doesn't work, so the reason why I'm interested in machine learning, or the reason why I'm

74
00:06:26,720 --> 00:06:32,880
in machine learning is to be able to learn more general behaviors, you know, more useful

75
00:06:32,880 --> 00:06:40,880
behaviors for robots and other applications, because machine learning is not only useful

76
00:06:40,880 --> 00:06:45,840
for robotics, just that, that is my, yeah, interest.

77
00:06:45,840 --> 00:06:52,440
One of the things that I can really relate to is that initial inspiration, you know, I find

78
00:06:52,440 --> 00:07:01,120
I really enjoy, I don't code a lot, but I do enjoy it, but when you're writing code

79
00:07:01,120 --> 00:07:08,960
and then that is affecting the physical world through motors and other actuators, you

80
00:07:08,960 --> 00:07:13,600
know, basically robotics, whether in like small form, you know, making light splink, all

81
00:07:13,600 --> 00:07:21,240
that kind of stuff, I find it super rewarding and so I can see how that would like it you

82
00:07:21,240 --> 00:07:27,400
kind of pulled into wanting to explore more and have more, kind of be able to build these

83
00:07:27,400 --> 00:07:28,920
more robust interactions.

84
00:07:28,920 --> 00:07:36,480
Yeah, now, yeah, that's precisely the point, right, there's so much, it is actually very

85
00:07:36,480 --> 00:07:41,240
rewarding to say something that you created actually become something physical that you

86
00:07:41,240 --> 00:07:42,240
can see.

87
00:07:42,240 --> 00:07:47,600
At the Black and AI workshop, you presented on some of your research, titled safer exploration

88
00:07:47,600 --> 00:07:51,840
and deep reinforcement learning using action priors.

89
00:07:51,840 --> 00:07:55,240
Tell us a little bit about the motivation for this project.

90
00:07:55,240 --> 00:07:58,760
Okay, the motivation is not really a straightforward thing for me.

91
00:07:58,760 --> 00:08:07,840
But motivated me to go with this particular topic is my, mainly my experience with using

92
00:08:07,840 --> 00:08:14,480
deep reinforcement learning and trying to apply it to the robotics domain, right?

93
00:08:14,480 --> 00:08:23,120
So I have access to this mobile manipulator, it's basically just a powerbot, AGV platform

94
00:08:23,120 --> 00:08:29,880
with that baritwem, seven dog robot arm on top, and we would like to basically, the dream

95
00:08:29,880 --> 00:08:35,720
is to have this platform performing complicated tasks such as making someone coffee or making

96
00:08:35,720 --> 00:08:41,720
a sandwich and things like that, but it's really hard to specify rewards and it's very

97
00:08:41,720 --> 00:08:44,920
hard to produce those kind of behaviors.

98
00:08:44,920 --> 00:08:51,680
If you are trying to manually specify what is supposed to happen, ideally, would like

99
00:08:51,680 --> 00:08:58,720
the system to actually learn to perform these behaviors on its own.

100
00:08:58,720 --> 00:09:05,320
So what I'm kind of hearing here is like you kind of started out as an undergrad, like

101
00:09:05,320 --> 00:09:14,240
trying to get robots to do things and at that stage in your experience, the natural way

102
00:09:14,240 --> 00:09:18,440
to do things was to build these big rule sets.

103
00:09:18,440 --> 00:09:25,440
And then else kind of programming and you went on to explore doing this with deep reinforcement

104
00:09:25,440 --> 00:09:32,040
learning, but then you ran it to this challenge of how do we create the reward functions to

105
00:09:32,040 --> 00:09:34,280
get a robot to grip something?

106
00:09:34,280 --> 00:09:39,720
That's historically one of the hard, you know, the hard parts about using deep reinforcement

107
00:09:39,720 --> 00:09:40,720
learning.

108
00:09:40,720 --> 00:09:42,080
Yes, that is correct.

109
00:09:42,080 --> 00:09:49,640
So I found it very hard to specify a reward function for picking something up, you know.

110
00:09:49,640 --> 00:09:56,240
My first task as a master student was, okay, I want to use deep reinforcement learning

111
00:09:56,240 --> 00:10:02,320
to perform multitask learning on this particular robot platform that we have at the CSR, right?

112
00:10:02,320 --> 00:10:07,160
Then when I'm thinking about doing that, I realize, okay, this is basically one of the

113
00:10:07,160 --> 00:10:13,440
few robots that we have and reinforcement learning is known to be random in nature, right?

114
00:10:13,440 --> 00:10:21,560
You rely on maybe executing random actions to kind of discover rewarding states and stuff

115
00:10:21,560 --> 00:10:26,320
like that and you can't really afford to be performing these random actions and these

116
00:10:26,320 --> 00:10:33,640
jerky movements when you're actually working with actual machinery or the physical platform.

117
00:10:33,640 --> 00:10:40,360
So my first task was, let me just build a simulated environment and as I was doing that, I spent

118
00:10:40,360 --> 00:10:45,440
some time trying to basically find the right meshes that would kind of correspond in shape

119
00:10:45,440 --> 00:10:50,720
and size to the actual physical platform and I was using this thing called gazebo where

120
00:10:50,720 --> 00:10:55,960
you can basically have like a physical environment, you know, with all the physics and collisions

121
00:10:55,960 --> 00:11:02,080
and things and then gazebo is a gazebo is like a physics simulator that's used quite a

122
00:11:02,080 --> 00:11:04,000
bit in robotics, is that right?

123
00:11:04,000 --> 00:11:05,240
Yes, yes, yes.

124
00:11:05,240 --> 00:11:11,600
So yeah, when I say both is that I built basically some packages for this particular platform that

125
00:11:11,600 --> 00:11:19,720
would allow me to use reinforcement learning with the actual joint configurations of the robot,

126
00:11:19,720 --> 00:11:24,480
you know, so things like I need to be able to publish actions to the robot and have it perform

127
00:11:24,480 --> 00:11:28,760
those actions and then observe some change and then have something that's going to decide

128
00:11:28,760 --> 00:11:32,880
if this was good or bad, which is basically my reward function, right?

129
00:11:32,880 --> 00:11:38,480
So yeah, but the whole thing was just I want to have this particular robot platform being

130
00:11:38,480 --> 00:11:45,280
able to perform multiple tasks and on my way there towards performing multiple tasks,

131
00:11:45,280 --> 00:11:50,040
I realized that okay, there's actually a lot of challenges with getting a standard

132
00:11:50,040 --> 00:11:55,200
dipping reinforcement learning algorithm to to work properly on, you know, like any environment

133
00:11:55,200 --> 00:11:56,800
that you have, right?

134
00:11:56,800 --> 00:12:03,960
So when I think about a deep enforcement learning, I'm going to be thinking about okay, firstly

135
00:12:03,960 --> 00:12:10,280
you have this, this exploration phase where you start with random policies, right?

136
00:12:10,280 --> 00:12:16,520
And you're going to be trying to find, I guess, would be executing random actions.

137
00:12:16,520 --> 00:12:21,120
I found out that it was really hard to specify reward functions for these complicated

138
00:12:21,120 --> 00:12:25,560
behaviors, like for example, in wanting to pick something up, right?

139
00:12:25,560 --> 00:12:31,240
You need to somehow reward the agent for what's this thing re-reaching towards the object

140
00:12:31,240 --> 00:12:37,320
and then actually picking it up, you know, which is there's actually two stages you need

141
00:12:37,320 --> 00:12:41,880
something that's going to encode moving towards the object and something that's going to encode

142
00:12:41,880 --> 00:12:46,360
basically you closing your gripper around the object and then still having the object

143
00:12:46,360 --> 00:12:51,800
and moving that your hand up and having the object remain in the hand, you know, so things

144
00:12:51,800 --> 00:12:53,120
like that.

145
00:12:53,120 --> 00:12:58,640
You presented a project called safer exploration and deep reinforcement learning using action

146
00:12:58,640 --> 00:13:00,360
priors.

147
00:13:00,360 --> 00:13:04,440
First off, when you say safer exploration, you know, what does that mean and why is that

148
00:13:04,440 --> 00:13:05,440
important?

149
00:13:05,440 --> 00:13:11,000
Yeah, so when I'm talking about safer exploration, right?

150
00:13:11,000 --> 00:13:19,480
I'm trying to improve the safety of the exploration policy in a standard deep reinforcement learning

151
00:13:19,480 --> 00:13:20,640
algorithm, right?

152
00:13:20,640 --> 00:13:26,600
So being in the robotics domain, I can't very much about the physical well-being of my

153
00:13:26,600 --> 00:13:27,920
platform, right?

154
00:13:27,920 --> 00:13:33,560
So it's very important to me that I won't be, if for example, I need to learn to perform

155
00:13:33,560 --> 00:13:39,320
some task using a deep reinforcement learning algorithm, I need some form of insurance that

156
00:13:39,320 --> 00:13:43,960
my platform won't be basically performing the worst possible thing.

157
00:13:43,960 --> 00:13:45,960
It won't hurt itself in any way.

158
00:13:45,960 --> 00:13:54,360
For example, if I have like a Rumba and basically having it sweep my roof for some reason, right?

159
00:13:54,360 --> 00:13:57,920
I need to know that this thing won't drive off the roof, you know, like things like that,

160
00:13:57,920 --> 00:14:02,600
like it should be able to learn, right?

161
00:14:02,600 --> 00:14:08,400
In this scenario or in an environment where there are still states that are deemed very

162
00:14:08,400 --> 00:14:11,360
unsafe or like very undesirable, right?

163
00:14:11,360 --> 00:14:15,800
So when I talk about safer exploration, I'm just trying to

164
00:14:15,800 --> 00:14:22,000
I guess improve the safety of your standard deep reinforcement learning algorithm, I mean.

165
00:14:22,000 --> 00:14:27,400
That paradigm is based on, so you understand the standard paradigm in reinforcement learning

166
00:14:27,400 --> 00:14:33,320
in general is that you, or one of the more popular exploration techniques is this

167
00:14:33,320 --> 00:14:39,240
epsilon greedy thing where you're going to decide, okay, I'm going to execute some random

168
00:14:39,240 --> 00:14:42,120
actions or actions from random policy, right?

169
00:14:42,120 --> 00:14:48,320
Some epsilon probability of the time and then with probability one minus epsilon, you're

170
00:14:48,320 --> 00:14:53,000
going to be basically exploiting or you're going to be executing actions that come from

171
00:14:53,000 --> 00:14:55,360
your from the policy that you're training, right?

172
00:14:55,360 --> 00:15:00,560
Which is also quite random initially, but you do this so that like you can actually discover,

173
00:15:00,560 --> 00:15:06,000
you know, like potentially useful actions to perform in any state in the state that you're

174
00:15:06,000 --> 00:15:07,240
currently occupying.

175
00:15:07,240 --> 00:15:14,280
So my work is to trying to kind of relax this very random nature, right?

176
00:15:14,280 --> 00:15:19,560
When you're exploring and what I'm saying is that instead of trying to explore randomly,

177
00:15:19,560 --> 00:15:25,760
right, or executing random actions, why don't we execute actions that were performed

178
00:15:25,760 --> 00:15:33,360
by experts in these like states that were in, you know, so you'd have maybe something

179
00:15:33,360 --> 00:15:41,920
like you tried to first build some notion of account, right, to maintain, you'd maintain

180
00:15:41,920 --> 00:15:45,960
some notion of accounts to see how many times a particular action has been performed

181
00:15:45,960 --> 00:15:47,600
in a state, right?

182
00:15:47,600 --> 00:15:51,080
And then you're going to explore according to these counts, you convert these counts to

183
00:15:51,080 --> 00:15:55,680
a probability distribution and then you sample from the from from this distribution and

184
00:15:55,680 --> 00:15:58,520
that is the action that you're going to apply, right?

185
00:15:58,520 --> 00:16:04,000
Like a more intuitive example is something like if I gave you the task of opening a door,

186
00:16:04,000 --> 00:16:05,000
right?

187
00:16:05,000 --> 00:16:09,520
The first thing, if if you're an enforcement agent and you've never opened a door in

188
00:16:09,520 --> 00:16:13,360
your life, the first thing you do when you're in front of a door is that you're going to

189
00:16:13,360 --> 00:16:15,160
try every possible action, right?

190
00:16:15,160 --> 00:16:17,040
And you're going to try a very random things.

191
00:16:17,040 --> 00:16:21,960
So now in your action space, you have the action of licking the door handle, which is,

192
00:16:21,960 --> 00:16:26,360
you know, could not lead to you opening the door, you can you can try kicking the door,

193
00:16:26,360 --> 00:16:28,640
you can try, there's so many things you can try.

194
00:16:28,640 --> 00:16:33,240
But if you have the opportunity, maybe to sit on a chair next to the door and watch how

195
00:16:33,240 --> 00:16:37,400
other people do it and then try what they're trying to do, like these people may be trying

196
00:16:37,400 --> 00:16:39,320
to do other things with the door.

197
00:16:39,320 --> 00:16:46,640
But the hope is that by trying what other expert agents have tried in this particular state,

198
00:16:46,640 --> 00:16:53,720
you know, you may be doing, you may avoid this, this, this, this, this, this random nature

199
00:16:53,720 --> 00:16:58,880
like that, that may lead you to undesirable states, like it kind of grounds your, your search

200
00:16:58,880 --> 00:17:00,560
for good actions.

201
00:17:00,560 --> 00:17:06,440
And also, you know, like because these agents that you're watching expert agents, you can

202
00:17:06,440 --> 00:17:11,360
assume that they're behaving optimally or in a way that they're not trying to harm themselves

203
00:17:11,360 --> 00:17:12,360
in any way.

204
00:17:12,360 --> 00:17:18,840
So when I hear you describe it like that, the thing that comes to mind for me is imitation

205
00:17:18,840 --> 00:17:19,840
learning.

206
00:17:19,840 --> 00:17:23,600
How was that related to what you've done here?

207
00:17:23,600 --> 00:17:29,800
So how I understand imitation learning is that you're going to be observing.

208
00:17:29,800 --> 00:17:33,960
I think you're going to have access to some expert trajectories.

209
00:17:33,960 --> 00:17:38,120
And I think the task today is to try and follow those, right?

210
00:17:38,120 --> 00:17:42,000
You're basically trying to make those trajectories more general in any way.

211
00:17:42,000 --> 00:17:46,160
You want to kind of imitate the expert, right?

212
00:17:46,160 --> 00:17:51,600
Whereas here, what we're trying to do is that we're not really performing the same task

213
00:17:51,600 --> 00:17:53,960
as the expert that we're watching, right?

214
00:17:53,960 --> 00:17:59,720
So the action process framework has the flexibility that the different experts can be performing

215
00:17:59,720 --> 00:18:01,200
different tasks, right?

216
00:18:01,200 --> 00:18:05,440
And by different tasks, I mean, tasks that have different reward functions, right?

217
00:18:05,440 --> 00:18:09,800
Meaning that it doesn't matter what the action that the experts are doing, as long as the

218
00:18:09,800 --> 00:18:16,560
action spaces are the same between the agent that is learning and these experts here, right?

219
00:18:16,560 --> 00:18:24,480
You may find some benefit in basically trying what the experts tried, like if that makes

220
00:18:24,480 --> 00:18:25,480
sense.

221
00:18:25,480 --> 00:18:32,280
You're trying to act according to this body of advice, like I think advice kind of makes

222
00:18:32,280 --> 00:18:34,160
more sense, right?

223
00:18:34,160 --> 00:18:40,560
So if I can recap what you're saying to make sure I understand when an imitation learning,

224
00:18:40,560 --> 00:18:48,920
if you've got our scenario with the robot sitting in a chair, observing an expert interacting

225
00:18:48,920 --> 00:18:56,440
with the door and imitation learning as it's classically defined, what you're really trying

226
00:18:56,440 --> 00:19:05,080
to do is have the robot, you know, look at someone performing a very specific task like

227
00:19:05,080 --> 00:19:11,080
rotating the door handle thousands of times or however many times and you're trying to

228
00:19:11,080 --> 00:19:14,360
teach it that very specific action.

229
00:19:14,360 --> 00:19:18,520
And any other actions would be noise and that kind of model, whereas in what you're doing,

230
00:19:18,520 --> 00:19:25,600
you're presenting, it's kind of not random observations, but observations of a bunch

231
00:19:25,600 --> 00:19:31,480
of different things, reaching for the door, turning the knob, opening the door.

232
00:19:31,480 --> 00:19:39,360
It's more unconstrained and you're trying to use all of these observations to kind of

233
00:19:39,360 --> 00:19:43,560
condition the exploration process.

234
00:19:43,560 --> 00:19:44,560
Yes.

235
00:19:44,560 --> 00:19:45,560
Yes.

236
00:19:45,560 --> 00:19:48,640
That's exactly what I'm trying to do.

237
00:19:48,640 --> 00:19:49,640
Okay.

238
00:19:49,640 --> 00:19:56,120
So, yeah, it's just as you put it in unconstrained, I guess, version where you're trying to,

239
00:19:56,120 --> 00:20:01,080
you have all these multiple demonstrations, you're not exactly trying to follow any particular

240
00:20:01,080 --> 00:20:06,360
trajectory, but what you're interested in is what these experts are kind of doing in any

241
00:20:06,360 --> 00:20:07,360
particular state.

242
00:20:07,360 --> 00:20:13,760
So, if you visit a state, you can always ask the question, what did experts do, right?

243
00:20:13,760 --> 00:20:20,000
And maybe there'll be some arbiter or some Oracle that will tell you, hey, 63% of experts

244
00:20:20,000 --> 00:20:26,520
took this action, maybe 23% of experts took this action and then, you know, other experts

245
00:20:26,520 --> 00:20:30,560
took this particular action and no expert took this particular action, right?

246
00:20:30,560 --> 00:20:36,520
So then I want to, in that case, basically try the actions that were more popular as opposed

247
00:20:36,520 --> 00:20:41,320
to the one that the experts didn't try because you can imagine that in a model like that,

248
00:20:41,320 --> 00:20:46,960
you're kind of implicitly representing in this case, I guess, the state space in the environment

249
00:20:46,960 --> 00:20:52,200
and maybe the experts in this particular state are not moving for, are not taking the action

250
00:20:52,200 --> 00:20:54,640
for going forward because this is a cliff, right?

251
00:20:54,640 --> 00:20:58,880
So as they were training, they found that, okay, when they went forward, this led to a

252
00:20:58,880 --> 00:21:00,960
very bad reward.

253
00:21:00,960 --> 00:21:03,920
So they decided, okay, I'm not going to take that action.

254
00:21:03,920 --> 00:21:05,720
I'm just going to do this and that.

255
00:21:05,720 --> 00:21:09,560
You know, so it's basically you treat it as this party of advice that you can, like,

256
00:21:09,560 --> 00:21:13,920
you know, kind of query at every step in your exploration process.

257
00:21:13,920 --> 00:21:20,960
One question I'm curious about is how you define the state space and if you're doing

258
00:21:20,960 --> 00:21:27,600
anything to generalize it and I realize there's a lot of loaded language there.

259
00:21:27,600 --> 00:21:33,800
What I mean is kind of going back to this example of the robot observing the expert.

260
00:21:33,800 --> 00:21:42,880
If the expert is reaching for the door handle, you could learn a lot about that action.

261
00:21:42,880 --> 00:21:50,360
But if you're only able to access what that expert has done, if the robot is in the exact,

262
00:21:50,360 --> 00:21:56,960
you know, if all of the dimensions of the robot, you know, it's all of its positions

263
00:21:56,960 --> 00:22:05,920
are in the exact same point, then you kind of miss out on a lot of advice from, you

264
00:22:05,920 --> 00:22:09,960
know, experts who were, you know, their hand was doing the same thing, but it was shifted

265
00:22:09,960 --> 00:22:11,960
over a centimeter or something like that.

266
00:22:11,960 --> 00:22:12,960
Yeah, yeah.

267
00:22:12,960 --> 00:22:15,600
That's a question of portability, right?

268
00:22:15,600 --> 00:22:22,920
So, so this in the original paper, right, the way the authors basically tackle this

269
00:22:22,920 --> 00:22:29,600
problem is by introducing a second version of the action prior framework, which was conditioned

270
00:22:29,600 --> 00:22:31,880
on observations instead of the state, right?

271
00:22:31,880 --> 00:22:37,400
So obviously, if you're trying to learn this distribution over actions given a state,

272
00:22:37,400 --> 00:22:42,520
right, basically asking, okay, what action should I take at this particular state?

273
00:22:42,520 --> 00:22:47,760
You are only going to, that model is only going to be useful, should like in an environment

274
00:22:47,760 --> 00:22:52,240
where the state space doesn't change, you know, so essentially if there's an obstacle

275
00:22:52,240 --> 00:22:55,720
here, that obstacle should not move at all, right?

276
00:22:55,720 --> 00:22:58,680
And they'll also stay the same place and stuff like that.

277
00:22:58,680 --> 00:23:05,800
But as soon as you move like an obstacle or something that was not like, for example,

278
00:23:05,800 --> 00:23:11,400
if you were training in an environment way, let's say the obstacle is in one particular

279
00:23:11,400 --> 00:23:12,880
position, right?

280
00:23:12,880 --> 00:23:19,240
And then now you take demonstrations from that setting and you try to move it into a setting

281
00:23:19,240 --> 00:23:24,360
where the agent is training, where these obstacles have been moved around now.

282
00:23:24,360 --> 00:23:27,880
What's going to happen is that that information is not going to be portable, right?

283
00:23:27,880 --> 00:23:35,280
So to try and fix that problem or to fix the problem, what the authors did, this is Benjamin

284
00:23:35,280 --> 00:23:41,240
Rosman and Subramarine Ramamothi, I think, remember correctly.

285
00:23:41,240 --> 00:23:49,040
What they did is that they conditioned the action prior distribution over observations,

286
00:23:49,040 --> 00:23:55,840
meaning that I guess if you can imagine if all the obstacles were blue and your agent

287
00:23:55,840 --> 00:24:00,840
can be like, oh, your body of advice could be something that says, okay, if I see a

288
00:24:00,840 --> 00:24:05,400
blue thing, I want to do this or this, you know, so basically whenever I see an obstacle,

289
00:24:05,400 --> 00:24:09,440
I want to do one of the following deterministic things, you know, so you can increase the

290
00:24:09,440 --> 00:24:16,440
possibility of this action prior framework by just conditioning on observations instead

291
00:24:16,440 --> 00:24:17,440
of this state.

292
00:24:17,440 --> 00:24:22,680
I guess I'm trying to visualize how you represent the state for a given problem.

293
00:24:22,680 --> 00:24:28,440
How do you figure out, is it, is the representation, like, I'm, you know, I'm imagining it's a

294
00:24:28,440 --> 00:24:33,200
vector of some sort, but is it a vector of, like, if it's a robot, like all of the stepper

295
00:24:33,200 --> 00:24:36,840
motor angles and that kind of thing, or is it something else?

296
00:24:36,840 --> 00:24:37,840
Is it more abstract?

297
00:24:37,840 --> 00:24:41,960
No, no, it's exactly that.

298
00:24:41,960 --> 00:24:48,160
So if you have a robot, like a robot arm, then it will be your joint angles and stuff

299
00:24:48,160 --> 00:24:49,160
like that.

300
00:24:49,160 --> 00:24:54,200
It's literally, so how I think of reinforcement learning in general is that you're going

301
00:24:54,200 --> 00:24:58,560
to have some target environment and then you can have a state, right?

302
00:24:58,560 --> 00:25:02,640
I defined in the framework of often a Markov decision process, you can have a state and

303
00:25:02,640 --> 00:25:07,600
then you have access to actions and the transition function and then you have a reward function

304
00:25:07,600 --> 00:25:14,800
I guess in some gamma discount factor thing, but yeah, like a state can mean anything that

305
00:25:14,800 --> 00:25:19,960
you wanted to mean as far as, as long as you have formulated the problem you're trying

306
00:25:19,960 --> 00:25:25,040
to solve as a reinforcement learning problem, like that's how I think about it.

307
00:25:25,040 --> 00:25:29,880
So in my case, so then the question that I was trying to get at in this case is let's

308
00:25:29,880 --> 00:25:38,800
say that your state vector is all of your angles and you've got this.

309
00:25:38,800 --> 00:25:45,920
You're basically able to consult the Oracle, so to speak, to see if it knows anything

310
00:25:45,920 --> 00:25:48,000
about what to do in a given state.

311
00:25:48,000 --> 00:25:53,200
I guess the question that I'm asking is like, when you consult the Oracle and you say like

312
00:25:53,200 --> 00:26:00,080
these are my, you know, angles out to the second decimal point, the second decimal place,

313
00:26:00,080 --> 00:26:05,600
will it return what it knows even if it's not exactly that, but kind of close or do

314
00:26:05,600 --> 00:26:14,000
you have to kind of generalize it around it before you consult the Oracle or will you

315
00:26:14,000 --> 00:26:18,400
only know about things that happen at the very exact places?

316
00:26:18,400 --> 00:26:19,400
Does that make sense?

317
00:26:19,400 --> 00:26:20,400
Yeah, it does.

318
00:26:20,400 --> 00:26:26,960
So that's a very good question, right, so that is precisely the challenge with trying

319
00:26:26,960 --> 00:26:27,960
to move.

320
00:26:27,960 --> 00:26:33,400
Okay, let me just first say this action prior framework was defined, the thing for it to

321
00:26:33,400 --> 00:26:39,120
work very well in the discrete setting, which is true for most deep reinforcement learning

322
00:26:39,120 --> 00:26:41,480
and reinforcement learning algorithms, right.

323
00:26:41,480 --> 00:26:46,200
And moving into a continuous setting where you don't really have any guarantee that you'll

324
00:26:46,200 --> 00:26:52,240
ever see the same state again, there's, there's a need for models that can generalize,

325
00:26:52,240 --> 00:26:59,040
you know, like in some region and say, okay, for this particular region of state space,

326
00:26:59,040 --> 00:27:03,560
this is what the action distribution looks like, you know, and stuff like that.

327
00:27:03,560 --> 00:27:11,040
And I think that is what motivated my choice in what model to use to represent the action

328
00:27:11,040 --> 00:27:12,040
prior.

329
00:27:12,040 --> 00:27:17,440
That was a Gaussian process, which has a very nice, a smoothing effect, you know, like

330
00:27:17,440 --> 00:27:19,800
as far as data is concerned.

331
00:27:19,800 --> 00:27:20,800
Okay, okay.

332
00:27:20,800 --> 00:27:24,960
Yeah, now that you've said it, all of the language that I should have been using to ask

333
00:27:24,960 --> 00:27:31,280
the question is obvious, like discrete versus continuous and so just maybe to take a step

334
00:27:31,280 --> 00:27:41,200
back to make sure I understand the question I'm curious about now is how your specific research

335
00:27:41,200 --> 00:27:49,160
fits into the broader landscape of research in this area?

336
00:27:49,160 --> 00:27:56,840
Okay, so how I thought about it, right, was basically if you want to have a robot that

337
00:27:56,840 --> 00:28:04,200
can perform multiple tasks and things like that, right, you, okay, if you want a robot

338
00:28:04,200 --> 00:28:10,400
that is going to be useful in any particular scenario, then you need to think about, I think

339
00:28:10,400 --> 00:28:16,320
the field is called lifelong learning, right, and lifelong learning is basically trying

340
00:28:16,320 --> 00:28:24,040
to get these systems that can basically learn tasks as they are presented either, you know,

341
00:28:24,040 --> 00:28:29,400
in parallel or sequentially and then retain their ability to perform those tasks and then

342
00:28:29,400 --> 00:28:34,160
somehow use the knowledge that they've gained in learning the previous tasks to what's

343
00:28:34,160 --> 00:28:36,840
a thing to improve the learning of concurrent tasks.

344
00:28:36,840 --> 00:28:40,560
And I think that's almost how humans do this learning thing, right?

345
00:28:40,560 --> 00:28:47,680
So if I then think about the challenges that you get there, the first one is just trying

346
00:28:47,680 --> 00:28:54,240
to perform successful, you know, transfer learning in my case in continuous settings,

347
00:28:54,240 --> 00:29:02,400
you know, so I would, I can imagine that the action-price framework kind of fits somewhere

348
00:29:02,400 --> 00:29:11,120
in between, in between having like a very full or rather, it's one component that you

349
00:29:11,120 --> 00:29:16,640
would need to have a lifelong learning system or a lifelong learning robot, you know.

350
00:29:16,640 --> 00:29:23,320
So for example, my whole pipeline would be something like you train some expert agents

351
00:29:23,320 --> 00:29:30,000
in some target environment, right, and then these expert agents or these tasks, right,

352
00:29:30,000 --> 00:29:34,920
you're actually trying to learn can be, you know, all basically learned by the same robot,

353
00:29:34,920 --> 00:29:35,920
so to speak.

354
00:29:35,920 --> 00:29:42,040
And then this particular robot would use or sample trajectories or like keep a data set

355
00:29:42,040 --> 00:29:47,280
of trajectories from these learned tasks, right?

356
00:29:47,280 --> 00:29:53,040
And it would fit this model, this action prior distribution over those trajectories.

357
00:29:53,040 --> 00:29:58,440
And then for every task that it wants to learn, it can basically explore according to this

358
00:29:58,440 --> 00:30:03,680
action prior distribution, and after it has learned this new policy, it can then sample

359
00:30:03,680 --> 00:30:09,240
trajectories from there augmenting its data set, and then, you know, fitting the distribution

360
00:30:09,240 --> 00:30:16,000
again, you know, and then when learning a new task, it then going to use the exploration

361
00:30:16,000 --> 00:30:22,640
policy again, and then that just keeps going on until you have this machine or the system

362
00:30:22,640 --> 00:30:28,400
that can perform, you know, multiple tasks and things like that, you know, so I like

363
00:30:28,400 --> 00:30:34,200
to think of it as, I guess, a step towards lifelong learning, and one of the problems we

364
00:30:34,200 --> 00:30:42,400
have there in lifelong learning is we need to, I guess, solve the problem of transfer,

365
00:30:42,400 --> 00:30:45,960
as well as, you know, if you're talking about deep reinforcement learning, you need to

366
00:30:45,960 --> 00:30:53,320
kind of worry, or at least dedicate some time towards how to do exploration more efficiently

367
00:30:53,320 --> 00:30:55,240
and more safely.

368
00:30:55,240 --> 00:31:03,160
Before this explanation, when we were talking about experts, I was thinking in terms of

369
00:31:03,160 --> 00:31:08,800
observations, so your system would have some set of observations and those represent

370
00:31:08,800 --> 00:31:16,840
the experts, but you just introduced this concept of training expert models.

371
00:31:16,840 --> 00:31:23,120
How is that done, and kind of how is that different from the ultimate model that you're

372
00:31:23,120 --> 00:31:27,160
trying to train with reinforcement learning?

373
00:31:27,160 --> 00:31:28,600
So you please read the question.

374
00:31:28,600 --> 00:31:35,520
So it's you asking if how different is having this whole notion of an expert model compared

375
00:31:35,520 --> 00:31:38,560
to having what?

376
00:31:38,560 --> 00:31:47,160
Reinforcement learning, like the expert model is presumably the expert model is simpler and

377
00:31:47,160 --> 00:31:54,760
more readily trainable than a DRL model, is that true, or are the experts also trained

378
00:31:54,760 --> 00:31:56,760
using reinforcement learning?

379
00:31:56,760 --> 00:32:06,360
So in this case, I think my notion of expert, or how I think of expert, of what an expert

380
00:32:06,360 --> 00:32:13,360
is, it could be a deeply enforcement learning agent that was learning a task in the same

381
00:32:13,360 --> 00:32:19,280
environment, and now I just sample trajectories from the policy that I get from there, and

382
00:32:19,280 --> 00:32:24,520
then I use those samples to train the agent that I'm going to train next on a different

383
00:32:24,520 --> 00:32:25,520
task.

384
00:32:25,520 --> 00:32:31,480
It could also, expert can also mean, you know, just some, I guess in the context of video

385
00:32:31,480 --> 00:32:38,560
games, you can find that maybe a human player can kind of demonstrate what the correct actions

386
00:32:38,560 --> 00:32:44,440
or the correct policy is by playing the game and giving the reinforcement learning agent

387
00:32:44,440 --> 00:32:48,960
this trajectory that goes to the game in some sort of optimal way.

388
00:32:48,960 --> 00:32:56,560
And yeah, so it's basically either your expert is either agents that you were training in

389
00:32:56,560 --> 00:33:03,080
the same environment, or it could be just some human demonstrations and things like that,

390
00:33:03,080 --> 00:33:04,080
you know.

391
00:33:04,080 --> 00:33:12,480
In the context of getting learning from demonstration, when you're talking about robot manipulators,

392
00:33:12,480 --> 00:33:17,320
you can have someone who's trying to teach a robot how to wave by actually performing

393
00:33:17,320 --> 00:33:20,400
the action themselves and then recording that trajectory, you know.

394
00:33:20,400 --> 00:33:25,560
So my, I think the notion of expert in this case is just a bit more general.

395
00:33:25,560 --> 00:33:33,400
You know, say we're back at the door, and is the idea that we would have separate expert

396
00:33:33,400 --> 00:33:37,640
models for reaching and turning and pulling and that kind of thing?

397
00:33:37,640 --> 00:33:43,240
Or is there one expert that knows how to open a door?

398
00:33:43,240 --> 00:33:49,800
So I want to say maybe in training and expert how to open a door, right, you'd use information

399
00:33:49,800 --> 00:33:53,800
from these other experts, right.

400
00:33:53,800 --> 00:33:59,480
So reaching a door and turning the handle could all be actually useful actions.

401
00:33:59,480 --> 00:34:00,480
Okay.

402
00:34:00,480 --> 00:34:06,360
Yeah, in that case, this task is, it feels like it's arranged in a sort of like a hierarchical

403
00:34:06,360 --> 00:34:13,200
way way, turning the door could be considered as some kind of, I guess, opening the door

404
00:34:13,200 --> 00:34:18,960
could be considered as some kind of macro action that is composed of these smaller policies,

405
00:34:18,960 --> 00:34:23,320
such as reaching for the door handle, turning the handle and then pulling, you know.

406
00:34:23,320 --> 00:34:24,320
So yeah.

407
00:34:24,320 --> 00:34:32,000
And the action priors framework I'd like to think is more primitive in the sense that it's

408
00:34:32,000 --> 00:34:33,760
not hierarchical in any sense.

409
00:34:33,760 --> 00:34:39,000
It doesn't really, although you can kind of make it hierarchical by using options and

410
00:34:39,000 --> 00:34:47,200
other stuff like that, but my particular use case is using action priors or rather the

411
00:34:47,200 --> 00:34:51,840
action priors rely on actions that are kind of primitive, you know.

412
00:34:51,840 --> 00:34:57,720
So as long as my experts have the same action space and state space, then I can be able

413
00:34:57,720 --> 00:35:03,200
to use trajectories from other expert models or trained models and use them to train one

414
00:35:03,200 --> 00:35:07,120
particular target task faster and safer.

415
00:35:07,120 --> 00:35:14,000
So in other words, if you're trying to train an agent to open the door, you don't really

416
00:35:14,000 --> 00:35:15,000
care.

417
00:35:15,000 --> 00:35:21,120
You may have access to these experts that were trained on these sub tasks, but you don't

418
00:35:21,120 --> 00:35:23,280
really care about that implicit hierarchy.

419
00:35:23,280 --> 00:35:30,200
You're just looking, you're asking the question, hey, the arm is here.

420
00:35:30,200 --> 00:35:31,200
What do people use?

421
00:35:31,200 --> 00:35:33,680
What do experts usually do when the arm is here?

422
00:35:33,680 --> 00:35:40,960
And then kind of sample from a distribution of that answers that question and use that

423
00:35:40,960 --> 00:35:42,520
to inform your next step.

424
00:35:42,520 --> 00:35:49,320
I'm trying to move or rather trying to think independent of or rather I'd like my model

425
00:35:49,320 --> 00:35:56,200
to kind of not consider the reward functions or what task was being performed by any particular

426
00:35:56,200 --> 00:35:57,200
expert.

427
00:35:57,200 --> 00:35:58,200
Right.

428
00:35:58,200 --> 00:35:59,200
Right.

429
00:35:59,200 --> 00:36:00,200
Okay.

430
00:36:00,200 --> 00:36:04,600
Because it makes sense for me to use data from an expert that was performing a very similar

431
00:36:04,600 --> 00:36:05,600
task.

432
00:36:05,600 --> 00:36:10,200
And now when I say similar task, I have to in my pipeline somewhere consider, how am I

433
00:36:10,200 --> 00:36:12,360
going to compare task similarity?

434
00:36:12,360 --> 00:36:17,640
Like how do I know that this robot is performing a task that is similar to this one before deciding

435
00:36:17,640 --> 00:36:20,720
to sample trajectories from the policy that I got from this one?

436
00:36:20,720 --> 00:36:21,720
Right.

437
00:36:21,720 --> 00:36:26,520
So it's something like if I have this library of policies that do multiple things, the

438
00:36:26,520 --> 00:36:33,280
first task is to first maybe traverse through all these trained policies, checking to see

439
00:36:33,280 --> 00:36:39,760
how similar these tasks that they're performing to my tasks to the task that I'm interested

440
00:36:39,760 --> 00:36:40,760
in.

441
00:36:40,760 --> 00:36:45,400
And then after finding the one that is most similar, then I can now perform transfer.

442
00:36:45,400 --> 00:36:51,440
But I guess what I'm trying to do here is trying to perform multi task transfer, which

443
00:36:51,440 --> 00:36:59,360
is basically you just have this library of tasks, tasks and you're trying to extract some

444
00:36:59,360 --> 00:37:04,760
common knowledge from them and then use whatever body of common knowledge to kind of speed

445
00:37:04,760 --> 00:37:09,640
up or improve the learning of some tasks that you're interested in.

446
00:37:09,640 --> 00:37:12,240
You're not looking at test similarity at all.

447
00:37:12,240 --> 00:37:17,400
You're just looking at, you know, what do we know about what these experts do when we're

448
00:37:17,400 --> 00:37:18,400
in this state?

449
00:37:18,400 --> 00:37:19,400
Yes, yes.

450
00:37:19,400 --> 00:37:24,080
So I'd like to think of transfer as like falling into two categories, right?

451
00:37:24,080 --> 00:37:27,080
And the first one would be aggregate transfer, right?

452
00:37:27,080 --> 00:37:31,360
We're trying to transfer from like I guess all the tasks that I have.

453
00:37:31,360 --> 00:37:35,960
And then the other version of transfer would be kind of more selective to say, okay, in

454
00:37:35,960 --> 00:37:40,720
this library of tasks that I've learned, these maybe three tasks are very similar to the

455
00:37:40,720 --> 00:37:42,320
task that I'm trying to learn.

456
00:37:42,320 --> 00:37:45,960
And I'm going to take these three and transfer only from those, right?

457
00:37:45,960 --> 00:37:46,960
So yeah.

458
00:37:46,960 --> 00:37:51,240
And then when transferring, we also need to be careful of things like negative transfer

459
00:37:51,240 --> 00:37:55,840
because you can imagine that not all that knowledge is going to be used for some is actually

460
00:37:55,840 --> 00:38:01,880
going to be very bad for like it's going to hurt the learning of this target task if

461
00:38:01,880 --> 00:38:06,840
you're not careful, you know, and you can imagine that's one challenge that I kind of have

462
00:38:06,840 --> 00:38:09,840
to worry about here.

463
00:38:09,840 --> 00:38:17,680
So again, taking a step back to understanding the specific focus and contribution of your

464
00:38:17,680 --> 00:38:24,160
research relative to the broader landscape, it sounds like action priors is already

465
00:38:24,160 --> 00:38:32,520
out there, epsilon greedy exploration policy, we know is already out there is what you're

466
00:38:32,520 --> 00:38:40,720
adding here, the idea of this multitask transfer from multiple action priors, or is it something

467
00:38:40,720 --> 00:38:41,720
else?

468
00:38:41,720 --> 00:38:48,920
It's essentially the extension of the framework to continuous settings, you know, okay.

469
00:38:48,920 --> 00:38:57,080
So yeah, so just trying to understand how you can still maintain a notion of account

470
00:38:57,080 --> 00:39:04,160
in a continuous setting, because if you think about having a discrete space like, for

471
00:39:04,160 --> 00:39:10,840
example, and maze with with tiles as as as states, it's easy to basically keep some table

472
00:39:10,840 --> 00:39:16,720
where you're like, okay, this particular action has been performed this many times and this

473
00:39:16,720 --> 00:39:18,720
action has been performed this many times.

474
00:39:18,720 --> 00:39:23,760
And then you build a decision over that, you sample over like from that categorical distribution

475
00:39:23,760 --> 00:39:29,960
you get an action outright, but now how do you do that for for for for the case where

476
00:39:29,960 --> 00:39:37,720
you you don't really have the ability to to to even kind of maintain account, because

477
00:39:37,720 --> 00:39:42,720
you essentially have as soon as things become continuous, you kind of have infinitely many

478
00:39:42,720 --> 00:39:44,360
states, right?

479
00:39:44,360 --> 00:39:49,160
And and the other thing is that you get this effect off of multi modality where at the

480
00:39:49,160 --> 00:39:54,080
same state, you need a model that can kind of represent these different action preferences

481
00:39:54,080 --> 00:40:00,920
like these action preferences from from this this data that that you have these trajectories

482
00:40:00,920 --> 00:40:06,560
from the experts that you have like for for example, if if we're talking about maybe

483
00:40:06,560 --> 00:40:07,560
let me see.

484
00:40:07,560 --> 00:40:12,240
So you have an action space where actions can range from minus one to one, right?

485
00:40:12,240 --> 00:40:19,840
And you can imagine that maybe in this particular setting, experts, most experts prefer an action

486
00:40:19,840 --> 00:40:28,560
of 0.2 or around they prefer an action that has a mean of 0.2 and other other experts can

487
00:40:28,560 --> 00:40:35,320
have a preference around maybe off of minus 0.7 and things like that, but then there should

488
00:40:35,320 --> 00:40:39,320
be this places where if if you were to put a mean around there like that would result

489
00:40:39,320 --> 00:40:44,040
in a very, very bad kind of policy, you know, so it's kind of like you have a one to many

490
00:40:44,040 --> 00:40:46,240
mapping, right?

491
00:40:46,240 --> 00:40:49,920
At every point in the state space, right?

492
00:40:49,920 --> 00:40:53,680
And that is not really in my experience.

493
00:40:53,680 --> 00:40:57,880
It hasn't been very trivial to kind of learn, you know, cause I think neural networks are

494
00:40:57,880 --> 00:41:03,880
kind of the more for kind of one to one type of mapping, you know, where you have like

495
00:41:03,880 --> 00:41:09,840
input and, oh, no, they call this the more for many to one type of scenarios where you

496
00:41:09,840 --> 00:41:14,840
can have multiple inputs and then you have like one kind of output in the end.

497
00:41:14,840 --> 00:41:19,880
But what happens when you have like a single input, which is like your state and then you

498
00:41:19,880 --> 00:41:25,920
have all these like possible wild values that you have to get out, you know, so my experience

499
00:41:25,920 --> 00:41:30,520
with using just like a standard neural network is that you get this averaging effect and

500
00:41:30,520 --> 00:41:37,160
you have noticed that the the average of of of expert actions in a particular state is

501
00:41:37,160 --> 00:41:40,280
not an expert action or is not an optimal action.

502
00:41:40,280 --> 00:41:41,280
Yeah.

503
00:41:41,280 --> 00:41:46,480
So in the scenario of I guess a self driving car, you can imagine that you're driving into

504
00:41:46,480 --> 00:41:49,160
like a kind of fork, right?

505
00:41:49,160 --> 00:41:52,720
And then in the middle of the fork, there's something like a tree or something like that.

506
00:41:52,720 --> 00:41:57,320
And then your experts would be going either, you know, a steering have a steering angle of

507
00:41:57,320 --> 00:42:02,280
like minus 0.5 at the decision point.

508
00:42:02,280 --> 00:42:06,480
And then other other experts have have like, I guess, 0.5.

509
00:42:06,480 --> 00:42:12,520
And then what happens in that case is that if you try to learn your guess action prior

510
00:42:12,520 --> 00:42:18,120
distribution or action preference distribution at the decision point, you might just average

511
00:42:18,120 --> 00:42:22,400
over the two and you get like a steering angle of zero and that says, you know, drive straight

512
00:42:22,400 --> 00:42:23,400
into the tree.

513
00:42:23,400 --> 00:42:27,280
And then that's not useful at all, especially if you care about your cause.

514
00:42:27,280 --> 00:42:28,280
Yeah.

515
00:42:28,280 --> 00:42:31,200
And so how does this model compensate for that?

516
00:42:31,200 --> 00:42:33,000
The framework does compensate for that.

517
00:42:33,000 --> 00:42:39,000
But I have been looking at models that try to to look at how to represent a multimodal

518
00:42:39,000 --> 00:42:40,000
policy.

519
00:42:40,000 --> 00:42:45,480
Like for example, especially since I'm talking about probability distributions or it's kind

520
00:42:45,480 --> 00:42:48,880
of like I'm working with mixed Gaussian mixture models, right?

521
00:42:48,880 --> 00:42:54,560
So I need something that will basically take in a state and then give me a mixture, a

522
00:42:54,560 --> 00:42:58,600
Gaussian mixture model of sorts over the action space.

523
00:42:58,600 --> 00:43:04,200
And recently I've been playing with things like, um, I'm playing with things like mixture

524
00:43:04,200 --> 00:43:05,200
and city networks.

525
00:43:05,200 --> 00:43:08,200
And those have been giving me promising results.

526
00:43:08,200 --> 00:43:14,360
And then there's still like some ammo of models that I have, I guess I heard about, um,

527
00:43:14,360 --> 00:43:18,680
I think it's overlapping mixtures of Gaussian processes.

528
00:43:18,680 --> 00:43:25,840
And basically it's just, um, models that have, uh, support for, uh, models that can give

529
00:43:25,840 --> 00:43:30,320
you, um, multiple outputs given a single input, you know?

530
00:43:30,320 --> 00:43:33,080
So yeah, that's kind of, uh, the challenge here.

531
00:43:33,080 --> 00:43:40,360
So is the idea that kind of referring back to your self-driving car example that you,

532
00:43:40,360 --> 00:43:44,240
you know, given your state, you know, one of these mixture models would be able to tell

533
00:43:44,240 --> 00:43:52,680
you that there are two distributions that, uh, are likely one is centered at minus 0.5

534
00:43:52,680 --> 00:43:57,560
and the other centered at 0.5 and then based on, you know, based on that information, you

535
00:43:57,560 --> 00:44:02,280
would know to choose one of those as opposed to choosing the zero or averaging out to zero.

536
00:44:02,280 --> 00:44:03,280
Yes.

537
00:44:03,280 --> 00:44:04,280
Yes.

538
00:44:04,280 --> 00:44:05,280
So that's exactly what I'm trying to do.

539
00:44:05,280 --> 00:44:07,840
And, and actually you highlighted another problem there, right?

540
00:44:07,840 --> 00:44:12,120
Which is how do you choose how many modes they are, you know, like, um, no one is going

541
00:44:12,120 --> 00:44:17,640
to tell you that, um, the good kind of behaviors here, uh, like, there's only two of them,

542
00:44:17,640 --> 00:44:22,080
meaning that you have like, uh, two kernels or two, two modes, like in, in your mixture

543
00:44:22,080 --> 00:44:23,080
model, right?

544
00:44:23,080 --> 00:44:26,680
Meaning, um, you got your goal left and then go, right?

545
00:44:26,680 --> 00:44:33,280
Um, that kind of thing can, can change, um, um, at, at different points in, in, in, in

546
00:44:33,280 --> 00:44:34,280
the state space.

547
00:44:34,280 --> 00:44:38,040
Maybe some states have got like an action preference distribution way, like experts

548
00:44:38,040 --> 00:44:42,480
are doing kind of three optimal, uh, uh, things and then like in other cases, it may kind

549
00:44:42,480 --> 00:44:43,480
of change.

550
00:44:43,480 --> 00:44:48,160
So like learning this kind of model has been, uh, a bit challenging, um, when it comes

551
00:44:48,160 --> 00:44:50,160
to, like, continuous environments.

552
00:44:50,160 --> 00:44:55,800
And I feel like this is typically, typically the case when you're moving some, I guess,

553
00:44:55,800 --> 00:44:59,840
reinforcement learning algorithm from a discrete setting, uh, towards continuous settings.

554
00:44:59,840 --> 00:45:04,680
Because I'd like to think that the real world is, is made of continuous contact, continuous

555
00:45:04,680 --> 00:45:07,000
quantities and, and, and stuff like that.

556
00:45:07,000 --> 00:45:13,160
And there should be some, um, care taken as to how, or what kind of model you're going

557
00:45:13,160 --> 00:45:18,760
to choose to, to represent that, like for, for example, going from, um, standard reinforcement

558
00:45:18,760 --> 00:45:22,800
to deep reinforcement learning, we need to care about how we represent our policy.

559
00:45:22,800 --> 00:45:29,120
So, you know, um, um, going to use maybe some, uh, deep neural network and, and this

560
00:45:29,120 --> 00:45:31,280
is going to, I guess parameterize my policy.

561
00:45:31,280 --> 00:45:34,480
I also need to represent my action prior, uh, distribution.

562
00:45:34,480 --> 00:45:38,400
It's just, there's so many, you know, things that you, you need to kind of like, uh, tweak

563
00:45:38,400 --> 00:45:42,160
and, and work with for you to have a fully working, working system.

564
00:45:42,160 --> 00:45:47,280
Maybe as a way to start to wrap up, can you give me a list of kind of the top three things

565
00:45:47,280 --> 00:45:52,360
that you've learned about doing about a, I guess, about deep reinforcement learning in

566
00:45:52,360 --> 00:45:58,480
general as applied to robotics, but with a particular emphasis on taking, uh, existing

567
00:45:58,480 --> 00:46:03,120
research results from the discrete domain into continuous environments.

568
00:46:03,120 --> 00:46:10,920
Uh, firstly, just moving from discrete environments to, to, um, for, from, from discrete environments

569
00:46:10,920 --> 00:46:12,480
to continuous environments.

570
00:46:12,480 --> 00:46:18,560
I think one challenge, uh, with doing that is, uh, just representing, uh, I think your

571
00:46:18,560 --> 00:46:19,560
policy, right?

572
00:46:19,560 --> 00:46:24,080
So, for example, uh, I think, I think the first paper that I came across, um, that kind

573
00:46:24,080 --> 00:46:29,920
of did like a good job with that was, was DDPG, it made me very interested in, in trying

574
00:46:29,920 --> 00:46:37,360
to apply, uh, reinforcement learning, um, towards, um, it, it basically, like, allowed me

575
00:46:37,360 --> 00:46:40,880
to believe that, okay, reinforcement learning will work in, in, in the robotics domain,

576
00:46:40,880 --> 00:46:41,880
right?

577
00:46:41,880 --> 00:46:46,960
Um, but then the paper again, uh, I think it's, it's, it's deep deterministic, uh, policy,

578
00:46:46,960 --> 00:46:47,960
policy gradients.

579
00:46:47,960 --> 00:46:48,960
Okay.

580
00:46:48,960 --> 00:46:49,960
Right.

581
00:46:49,960 --> 00:46:53,760
There's a, a newer version, I think D4 PG, um, not sure, but yeah, but, uh, essentially,

582
00:46:53,760 --> 00:46:59,440
um, it's like an, uh, it's a critic method and that it has all these, um, okay, let me

583
00:46:59,440 --> 00:47:04,160
just say, like, it is one of the, the first papers that, that kind of worked in, in, in,

584
00:47:04,160 --> 00:47:06,400
uh, deep, in, in robotics, right?

585
00:47:06,400 --> 00:47:11,040
So to apply DDPG reinforcement learning in, like, with continuous environments, right?

586
00:47:11,040 --> 00:47:15,960
Um, I think, I think besides trying to learn, like, a, a good policy, you need to kind

587
00:47:15,960 --> 00:47:22,240
of think of, uh, exploration, like, how you're going to handle, um, exploring this very,

588
00:47:22,240 --> 00:47:25,880
like, uh, the, your, your, your, your, your, your state space, given the fact that it's

589
00:47:25,880 --> 00:47:31,000
continuous and there's like infinitely many, uh, states that you can, uh, occupy there.

590
00:47:31,000 --> 00:47:38,640
And, um, the, the other thing is data in robotics is very, like, it's, it's, it's very,

591
00:47:38,640 --> 00:47:45,200
it's very hard to come by, like, you, um, you kind of need to, or rather, the ideal case

592
00:47:45,200 --> 00:47:51,040
is to not have a lot of, um, it's, it's, it's, it's, it's, it's to not have to train

593
00:47:51,040 --> 00:47:52,880
your, your model for a long time, right?

594
00:47:52,880 --> 00:47:57,120
You'd like to maybe train, um, your model, like, for, for a very short period of time,

595
00:47:57,120 --> 00:48:02,760
but, like, in reality, if you're using, like, these model free methods, um, you find

596
00:48:02,760 --> 00:48:05,360
yourself training for a very, very long time.

597
00:48:05,360 --> 00:48:12,920
And that makes it difficult to kind of apply, um, um, existing work in the sense that

598
00:48:12,920 --> 00:48:16,720
maybe you have, like, uh, you're working on your laptop and you can't run these models

599
00:48:16,720 --> 00:48:19,360
because it's going to take a long time to converge.

600
00:48:19,360 --> 00:48:24,960
And the other thing in reinforcement learning is that, uh, I realized it very late, um, um,

601
00:48:24,960 --> 00:48:31,040
in my master's project where, like, I realized the fact that, uh, your random seed is, is

602
00:48:31,040 --> 00:48:35,400
quite a very, it's a very important thing, you know, like, so you can, you can blame the

603
00:48:35,400 --> 00:48:39,600
model all you want to, like, you get, maybe you get, like, a very good, uh, kind of learning

604
00:48:39,600 --> 00:48:42,440
curve and you feel like, okay, this thing is working and you run it again.

605
00:48:42,440 --> 00:48:46,320
Um, and what happens now, it basically gives you, like, a very flat thing and shows you

606
00:48:46,320 --> 00:48:47,320
that it doesn't work.

607
00:48:47,320 --> 00:48:50,640
So it fluctuates a lot, there's like all these variants.

608
00:48:50,640 --> 00:48:55,840
So I learned that, okay, for you to actually, for people to be able to trust your results,

609
00:48:55,840 --> 00:49:00,280
you need to be able to kind of average them over multiple runs and multiple runs means

610
00:49:00,280 --> 00:49:03,440
that you're going to be running your, you, I guess, you'll algorithm for like a long time

611
00:49:03,440 --> 00:49:06,400
and, um, that is just more compute time.

612
00:49:06,400 --> 00:49:12,160
So like, you can't, you need like, uh, I guess compute, compute is a very important thing,

613
00:49:12,160 --> 00:49:18,600
you know, so I'm fortunate enough to have, um, access to a cluster, like, um, we have,

614
00:49:18,600 --> 00:49:24,080
I think something with, with 60 blades, like, at the university, um, and I can basically

615
00:49:24,080 --> 00:49:29,200
run multiple experiments in parallel and, um, I can get my results within, uh, two

616
00:49:29,200 --> 00:49:35,360
hours or so, you know, but the one challenge is just, you know, um, you, firstly looking

617
00:49:35,360 --> 00:49:40,520
for environments, you're looking for, um, things that won't take too long, you know, um,

618
00:49:40,520 --> 00:49:46,000
and then also, yeah, exploration has been like a very, uh, a big thing, you know, so, yeah,

619
00:49:46,000 --> 00:49:51,320
I don't know if that answers your question, but, um, yeah, those were like, in my experience,

620
00:49:51,320 --> 00:49:55,680
things that I found very difficult about trying to apply, uh, deep enforcement learning

621
00:49:55,680 --> 00:50:00,800
in, in, in the robotics domain, just data and having very accurate simulations.

622
00:50:00,800 --> 00:50:05,160
Like, for example, you start out very, very ambitious, um, for example, we have a physical

623
00:50:05,160 --> 00:50:10,320
robot, uh, in house here, um, like this mobile manipulator and I'm like, I want to apply

624
00:50:10,320 --> 00:50:14,240
deep enforcement learning, maybe learn to pick up a cup, and maybe the cup will have water

625
00:50:14,240 --> 00:50:15,240
and stuff like that.

626
00:50:15,240 --> 00:50:20,160
And as you kind of traverse through the literature, you start checking things off from this

627
00:50:20,160 --> 00:50:23,760
checklist or like from these ambitions, you know, it's like, okay, maybe there shouldn't

628
00:50:23,760 --> 00:50:27,840
be water, you know, like in the cup, and then maybe the cup should be like a ball, you

629
00:50:27,840 --> 00:50:30,280
know, something that's, I don't know, easier to, to, to, to hold.

630
00:50:30,280 --> 00:50:33,560
And then you're like, okay, maybe instead of having a ball, I'll just have, I'll be moving

631
00:50:33,560 --> 00:50:37,440
the manipulator just along like this, you know, and then you keep going and then you

632
00:50:37,440 --> 00:50:41,120
find yourself, okay, maybe you have instead of having a physical robot, I'm going to

633
00:50:41,120 --> 00:50:42,840
build a simulated version.

634
00:50:42,840 --> 00:50:46,640
And then when you're there, you're like, okay, instead of building a physical simulated

635
00:50:46,640 --> 00:50:50,400
version of the robot, I'm just going to build a simple 2D thing just to see if like these

636
00:50:50,400 --> 00:50:51,400
models work.

637
00:50:51,400 --> 00:50:56,680
And then you go all the way down to like a simple navigation domain, like a 2D thing where

638
00:50:56,680 --> 00:51:01,840
you can kind of easily prototype ideas and, you know, so it's, it's a very humbling

639
00:51:01,840 --> 00:51:02,840
experience.

640
00:51:02,840 --> 00:51:08,520
You start out very, you know, big and then like you kind of, you kind of get to narrow

641
00:51:08,520 --> 00:51:12,080
down into these like particular problems and you need something that's going to allow

642
00:51:12,080 --> 00:51:16,880
you to evaluate your ideas like quickly and, and, and, and very, you know, very efficiently

643
00:51:16,880 --> 00:51:18,120
and things like that.

644
00:51:18,120 --> 00:51:23,640
So yeah, like that has been my experience, I think with, with deep, deep RL in robotics

645
00:51:23,640 --> 00:51:27,040
and I think other applications that have been trying to use it in.

646
00:51:27,040 --> 00:51:33,920
Yeah, and I think your experience is very much in line with, with the experiences of

647
00:51:33,920 --> 00:51:41,160
others and it has, you know, that kind of experience I think has led a lot of people

648
00:51:41,160 --> 00:51:50,200
to kind of, you know, leave deep RL in frustration and move on to simpler things.

649
00:51:50,200 --> 00:51:59,760
But it's, you know, certainly a, a space that I find fascinating and thank you for kind

650
00:51:59,760 --> 00:52:03,000
of walking us through the way you're approaching it.

651
00:52:03,000 --> 00:52:04,480
Yeah, thank you.

652
00:52:04,480 --> 00:52:05,480
Thank you.

653
00:52:05,480 --> 00:52:10,760
Like I really, really relate to like that, that last part, you know, I understand people's

654
00:52:10,760 --> 00:52:14,840
frustration and understand the reasons if they end up leaving deep RL, you know, like

655
00:52:14,840 --> 00:52:19,800
I think what has helped me stay here is the, the communities that have been exposed to

656
00:52:19,800 --> 00:52:26,200
like, for example, I've met other people deep learning in Daba who are using deep RL

657
00:52:26,200 --> 00:52:30,440
for the, for the unique applications and we get to kind of, you know, talk and if you're

658
00:52:30,440 --> 00:52:32,960
having a problem, you can ask this particular person.

659
00:52:32,960 --> 00:52:37,880
Same thing is happening at Black and AI and like in some cases, you know, I guess through

660
00:52:37,880 --> 00:52:42,720
Black and AI, you get to meet the leading researchers in the field of the people who wrote

661
00:52:42,720 --> 00:52:47,080
the paper that's giving you sleepless nights and headaches and you get to ask them directly

662
00:52:47,080 --> 00:52:51,880
like, yo, what is this ex like, how did you make this work and what was, what motivated

663
00:52:51,880 --> 00:52:57,040
you, your decisions like for, I guess, choosing this particular thing.

664
00:52:57,040 --> 00:53:02,880
So yeah, I really, really understand, I guess the frustration and why people would be frustrated

665
00:53:02,880 --> 00:53:07,920
with the field, but hopefully going forward with this whole movement of trying to get people

666
00:53:07,920 --> 00:53:12,760
to release their code more like this whole thing of reproducibility, problems like that

667
00:53:12,760 --> 00:53:14,280
will tend to die out.

668
00:53:14,280 --> 00:53:15,280
Awesome.

669
00:53:15,280 --> 00:53:16,280
Awesome.

670
00:53:16,280 --> 00:53:23,280
Thank you so much for taking the time to chat with me.

671
00:53:23,280 --> 00:53:24,280
Thank you, Sam.

672
00:53:24,280 --> 00:53:28,640
And I think you're having me.

673
00:53:28,640 --> 00:53:29,640
All right, everyone.

674
00:53:29,640 --> 00:53:35,480
That's our show for today for more information on secret, or any of the topics covered in

675
00:53:35,480 --> 00:53:41,040
this episode, visit twimmelai.com slash talk slash 235.

676
00:53:41,040 --> 00:53:48,200
For more information on the black and AI series, visit twimmelai.com slash black and AI 19.

677
00:53:48,200 --> 00:54:14,960
As always, thanks so much for listening and catch you next time.

