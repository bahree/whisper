All right, everyone. Welcome to another episode of the Twimal AI podcast. I am of course your host,
Sam Charrington. And today I'm super excited to be joined by Kate Sianco, associate professor at
Boston University, and a consulting professor for the MIT IBM Watson AI lab. Before we get going,
be sure to take a moment to hit that subscribe button wherever you're listening to today's show.
Kate, welcome to the podcast. Thanks so much for having me. I'm looking forward to digging into
our conversation. You are a very, very busy woman at CVPR. You've got a ton of workshops that you're
speaking at, as well as numerous papers. We will scratch the surface of all that activity
in our conversation today. But before we do that, I'd love to have you introduce yourself to
our guests and share a little bit about your background and how you came to work in the field.
Yeah, definitely. I am a professor of computer science at Boston University, as you mentioned.
I lead a research group focused on deep learning, especially applied to visual recognition.
And I'm specifically interested in many topics, some of which are things like vision and language
models, and also data set bias and adaptation to out of distribution data, also efficient models,
and AI forensics. And I would say that more broadly, one of my goals is to create AI that
can see and can understand language and interact with humans in a natural way. And also help us
solve problems in society like helping people who are visually impaired or helping the environment.
So that's briefly about my research. How I came to the field, I got interested in
AI pretty early on. I was always a fan of science fiction. So when I decided to go to graduate
school, I started as a graduate student at MIT, actually studying speech recognition.
And later on, I switched to computer vision, but I've always been really fascinated about robots
and artificial intelligence and kind of related areas. So I got my PhD from MIT,
and then I did a postdoc for a couple of years, and then I became a faculty member. So I've
basically been working in the AI field for a long time. And it's been really amazing to see,
especially in computer vision, the transformation that our field has gone through.
Because when I started as a graduate student, you know, computer vision, I think I can say that
it didn't work for most things. You know, certain things worked, face detection was working pretty
well, but it's nothing like what we're seeing here. And it's just been so exciting to see this
revolution. It hasn't really, it hasn't really been boring for me. So I'm very kind of, I think I'm
lucky that I get to do this kind of work and this kind of research. Absolutely.
You mentioned your interest in a vision and language together and how those, you know,
you shifted from one to the next and now they're kind of coming together. And one of the things
that you're doing at CVPR is speaking at the multimodal learning and applications workshop.
And I guess the first thing that jumps out at me is, you know, your reference to kind of an
exciting time for computer vision. It's also a super exciting time for this idea of multimodal
machine learning. We've been kind of, you know, doing that for a bit, but the, you know, visual
transformers and kind of the convergence of these two architectures over the past a little bit has
really opened up a big opportunity there. And I'd love to hear you kind of riff on your view of
multimodal as a field and the opportunity before us and kind of the research frontier.
Multimodal learning has been around for a long time. Actually, I mentioned that when I was
starting as a graduate student, I was actually looking into audio visual speech recognition,
which is essentially lip reading. So using both audio and the movement of someone's lips to
understand speech. So that's one example of multimodal learning. And then for my PhD, I actually
worked on trying to learn about visual categories of different objects using text that we can
scrape from the web. And this was before deep learning really took off, but kind of the ideas,
ideas have been around for a long time, right? Can we learn something from both audio and video? Can
we get free data, essentially free labeled data from the web? So people have been trying this
for a while. And I think one of the really breakthroughs that we're seeing now is the qualitative
scale in the amount of data that we can get from the web and train on, right? There was a very
recent paper that I just saw talking about emergent properties of large models. And I do agree
that when we reach a certain scale, you know, both in terms of training data and size. So,
of course, those are correlated. You typically need a lot more data to train a very large model
with lots and lots of parameters. I think that you start seeing properties that are emerging that
you're not seeing with smaller models and smaller data sets. So I think that's what we're seeing now
with, you know, things like Dali too. And kind of very cool results that combined vision and
language. And so one one thing that I'm talking about at the workshop is how we can use this
training data that is essentially free, right? So probably you've heard of Clip, which is an
OpenAI model that really I would say transformed our field over the last little while because
they were able to collect lots of training data by scraping photos and their captions from the web.
So if we have access to this kind of data and we can learn not just from images with tags that have
to be sort of manually labeled like ImageNet data. Well, we have an image and we have a category
label for each image, right? So I mean, this data is good and we have been training on this data
for all kinds of computer vision applications. Usually you pre-train on this data and then you
fine tune on your own data set. Turns out that if you can pre-train on the captioned images from
the web, you actually can do much better, especially on zero-shot learning where you have a new
category that you haven't seen, you don't have any training data for this category. It turns out
that these models, these vision language pre-train models actually generalize better than just
traditional image classification data sets for pre-training. So you were talking about the
opportunities presented by all of the information that's out on the internet that is visual
information with associated text. You know, one of the obvious questions that that begs or,
yeah, I guess it's obvious in light of the recent history of large language models is the
inherent bias in the internet information. And I recently saw a tweet that was, I think someone
with access to Dolly II or something like Dolly II typed in, you know, engineers and it was
kind of these generated images of all male engineers standing around or, you know, doctors
same kind of thing. Now, curious your thoughts on that. And, you know, really it's a question of
like having access to, you know, that kind of volume of information is, you know, clearly beneficial
in some ways, but it also has, you know, we also need to be conscious of those biases. How are you
thinking about that? Yeah, I think that the bias in data sets is in some sense inevitable.
And this is something that I've been working on also for a long time. Kind of how do we deal
with the fact that our data sets are biased simply because they're finite, right? It's very hard
to sample the entire visual world and devoid bias. And especially since, as I mentioned,
we have been really trying to go to the internet for our data source in the last, you know,
say decade or so, or a couple of decades. Because it's free, it's easy. Yeah, it's there. I mean,
it's a lot harder to, you know, we used to actually have graduate students go to the lab and take
pictures of objects. And that's, that was the training data or take pictures of people.
I remember having to collect a data set in the lab and get a signed release form from every
person that walked in that we recruited, you know, making sure that they're okay with us using
their data. But we're in a different world now, you know, you go to the internet and you have
billions of images and videos at your fingertips. And I think we're almost, it feels to me like
we're in a gold rush kind of era with respect to, you know, how data is really king right now.
And whoever can get their hands on the most data in some sense is winning the AI race, right?
So I feel like these concerns are very much important. And we should definitely be worrying about them.
That's not something that I particularly work on in, in my research. But there's great people
out there who work on these kinds of more ethical questions. Although, you know, I had a paper a
while back. The first time that I noticed really this kind of bias in image caption data was
some of the very first times that we were able to get deep learning to generate captions for
images, right? And we were working with a fairly small by today's standards data set.
And, you know, at the time was a large scale data set. And, you know, when the model started
working, it was amazing, you know, literally the year before that, the best captions that you
could generate sounded very awkward, very robotic, you know, they would say things like there is one
building and grass, you know, they were templated. And once we got the image captioning to work
with language models using neural networks, you know, because one of the major advancements,
of course, in image captioning is being able to generate fluent natural sounding language.
And once we develop these models that no longer had two separate stages where in the first stage,
you detect all the objects and the second stage of your template and you plug in the objects
you detected in the sentence template, right? So that's how you get the robotic sounding captions.
But so now, instead of that, we had an end-to-end model that you just feed it images and captions,
and it just learns a single neural network that takes the raw image and spits out the entire
sentence, the entire caption, right? And, you know, it was amazing to see, I remember this,
you know, we had a paper on this and a few other labs had a paper on this, it made the New York Times,
it was really amazing, you know, how fluent these captions sounded like. But then we started
looking at this more and digging into results more and we didn't notice that these models definitely
learn shortcuts. They learn to exploit the biases in the data. And so one of the examples
that we wrote about in our paper is when you have a data set where most of the, for example,
images of snowboarders have captions that say, you know, a man is snowboarding. Then the model
sort of learns the shortcut that if you see something that looks like a snowboarder or perhaps
even snow, you should start the sentence by a man is, you know, and then complete it according to
maybe what else you detect in the image. But it's a pretty good shortcut because it will
minimize your loss, your training loss, because you'll get it right, almost to the training
examples. And that's all really that these models care about. So we try to address this kind of bias
in the paper. But, you know, I think we still don't have a very good solution and we we definitely
need more work to improve this kind of, you know, to improve our models so that they don't have
these egregious biases. And it's only getting worse, right? Of course, now we have giant data sets
hundreds of millions or even a billion images much, much harder to both audit them and to ensure
that they don't take shortcuts like this. And the models are a lot more complex and difficult to
intuit about and understand. That's right. They're very block box, right? Yeah. So your talk at
this multimodal learning and applications workshop is more language less labeling,
vision and language pre-training for visual tasks. It immediately calls to mind something
that I've been spending a lot of time exploring recently, this whole idea of data-centric AI.
And in particular, like you start out talking about the cost of labeling and the burden that that,
you know, creates for folks that want to build applications in this base. How is that played out
for you? This is something that I've been interested in the last few years. How do we train models,
especially for new tasks or new domains without having to collect and label a lot of data? And I can
talk about sort of applications where this comes up, but I think it's a lot of applications,
actually. And so this idea of, okay, we're going to get a fixed data set and train on it and then
that's it. That's all we need. That's really not how things work in real life. In real life, you want to
build an application, you have certain things that you want your model to recognize and images.
And there are often things that you don't have a lot of training data for. You have to
pay someone to label it. It costs a lot of money. Depending on what the labels look like, you
might actually need experts. You might need to find someone who is actually good at labeling and
and the scale and it just doesn't scale very well. So I think with one of the really exciting things
that people have been doing recently is figuring out, first of all, that these very large-scale
models that are trained on this web-scale data, like Clip, for example, you can prompt them
to get them to learn very quickly. Sometimes there's not even any learning involved. You just
give them some textual input telling them what you want recognized. And then you don't need any
additional training data to get a very good improvement already on your task. Of course,
you can improve further if you had training data. But if you don't have any training data or
only have a few labels, these kinds of prompting methods seem to be working quite well.
So that's one of the things that I was going to mention in the workshop is some very recent work
where we try to essentially prompt a model like Clip, pre-trained on a large web-scale data set
of images and captions, and then prompt it a test time to classify categories in a zero-shot way
so that where you don't have any labels for those categories, you just want the model to
kind of generalize to any label that you can throw at it essentially. And it does do much better
than previous work on this task. Can you talk about the methodology that you
pursued in that work? Was a lot of the challenge figuring out the right way to prompt the model?
So we actually have two lines of work in this direction. One is exactly this challenge of how
to prompt the model correctly. For example, again, we start with a pre-trained model that already
exists, very hard to retrain these models when you're experimenting with them. So oftentimes,
you just download one that OpenAI has been available. So we start with a pre-trained model,
especially Clip is the one that we often work with. And then we want to
prompt it with essentially some textual tokens. It's already learned to take text tokens
as input and the image as input, compare them, and then predict a score that basically says yes,
these, this textual string and this image are highly related or no, they're not related.
But during pre-training, of course, these models are learning to compare captions,
right? So a caption is a more general kind of label for an image. It could say, you know,
there's a group of people playing Frisbee in the park. But what we're trying to do is now take
that model and just we just want to know, is there a Frisbee in this image? So we want to classify
the label Frisbee or we want to classify the label car or person. So it presents a problem because
the model wasn't trained for that specific task and we need to tweak it. We need to do something
to get it to do this task. And so one of the contributions in this work is to figure out a way
to prompt a model with both a positive and a negative prompt. So you think of the prompt as saying
is there a blank in this image, for example, where the blank is the word for the class you're trying
to detect like Frisbee. And what that does is it makes the task a lot more similar to what the
models learned about because it seems to the model that you're giving it a caption essentially.
But it's a fake caption, of course. You just created a fake caption and just inserted the word
for the class that you want to recognize. And so what we did is in addition to this positive
prompt where we say is there a blank in the image and we replace the blank with the with the label.
So is there Frisbee? We also give it a negative prompt which is saying essentially think of it as
the negative question. Is there no Frisbee in the image? And then we compare the score that the
model gives for the positive and the negative. And if the positive score is higher, then we predict
that yes, there is a Frisbee in the image. In a sense, it's a bit analogous to the kind of games
that you might play with a human label or with human inhuman labeling where you ask multiple
labelers to label an image and compare their responses. This is just kind of a clever way to do that.
Not to minimize it, but yeah, yeah, exactly. And then we had to do some other tricks to extract
the spatial information in a more fine grain matter. I don't want to. Yeah, it's actually not
nothing, nothing fancy. But what the image and caption model does is it takes the whole caption
and compares it to the whole image. And it makes sense, right? Because when you're training the
model, you don't know which part of the caption corresponds to which part of the image, right? So
a group of people playing Frisbee in the park, you don't know where the Frisbee is located in the
image, right? But for our task, we are only looking for the Frisbee, not the entire scene.
So we actually wanted to focus in a more fine grain way on the region where the object might be.
So what we do is we do some surgery on this network to to instead compare our prompt at each location
in the image. Okay. And then decide if the object is there instead of first kind of aggregating
over the whole image and then comparing. So that turns out that works a little bit better.
Kind of akin to a convolutional window, we're just taking slices and passing them into the model.
Yeah, actually, the model already has features that are that it's computing at each sub window in
the image. It's just that then that later on, it is pooling them and aggregating them into a
single vector. And we just kind of go in and do some surgery and compare before it does. So actually,
we don't learn, I think the cool thing about this work is that we don't learn any new parameters
except for these prompt tokens. So all the surgery we do, we don't introduce any learnable parameters,
which means there's very little overhead and learning. So we need very little data to actually
tune the model to do this task. I think in our paper, we report something like
you know, something like maybe 20,000 parameters, the extra that we're learning.
And it's it's just these token embeddings that are fed in as the prompt. And the these token
embeddings, is there a fixed set of labels that you like you're starting with a fixed set of labels,
then you create these prompts and you know, that's where the token embeddings come from.
Yeah, so the tokens are the prompt. So you can think of them as some words, but actually,
we're just learning some arbitrary words. And each word is embedded into a continuous vector.
So that's what I mean by token embeddings. So you can think of them as some words like is there a
something in the image? We're not actually specifying what words the model should use. We're just
letting it learn some words. Yeah, I think I wasn't following that part. I thought I heard you
describing something more like you identified some relatively standard patterns or something
like that, like a template for these prompts. But rather these prompts are learned and they're
they're learned for each of the classes that you're trying to be able to predict like it's you
know, if it's tree a tree, the prompt is specific to tree. Like it's the prompt that produces the
best result for tree as opposed to what might work for car. Is that am I thinking about that
correctly? Yes, you are correct. Although we also did it the other way where the prompt is generic
and you just plug in the word that you want to recognize. So this is really zero shot learning
because you're not even tuning the prompts for any specific class you you you're tuning the
prompts on one set of classes and then the user gives you some new class. You just plug it into
the prompts that you've learned. It turns out that also works and our approach also improved
performance on this kind of zero shot recognition even though the prompts are learned in a generic
way and not specific to these categories. The evaluation on the topic of evaluation. Are you
what are you comparing against? We are comparing against existing models that try to do this kind
of label classification and images. Some of which are probably not working as well because they're
not utilizing these very large vision language models that we're utilizing like Clip.
And this is something that you know often comes up now. Is it fair to compare a model that isn't
using something that was pre-trained on 400 million images with captions? And you know I
I really go back and forth. I think in some sense yes it's not fair and in fact we don't even know
these huge datasets. We don't know what's in them. OpenAI hasn't released the dataset that they
trained on. It could in fact include some of our test data because it's straight from the web.
So it doesn't seem like the best experimental protocol to be using. But on the other hand
they work so well you know it just you can't deny that you get much better performance
and people are using them. So the cat's out of the bag so to speak.
On the other hand models do tend to work well if they've seen the training day. Exactly.
And we don't know if the model has seen the training day or not. I mean we also do compare
a model to kind of an original clip that hasn't been tuned in the way that I described and we do
perform better than that. So there is an advantage to what we're doing when we compare the more
apples to apples way. But I think what's happening now is I think that researchers really are going
to have to keep up with the latest and greatest pre-trained backbones and you know you just the
the method you develop can improve performance on your dataset with respect to a backbone like
ResNet for example. But someone could come along with a backbone like visual transformer
and upperform your whole algorithm just because they have a much better backbone.
It's like I said you know the gold rush or the space race or whatever you want to call it you know
the bigger the bigger the better. And yeah it's a little bit hard I think for researchers to
keep up with this. Yeah I was just going to ask how does that how does that land for you as a
researcher like there's been a lot of talk around you know just the amount of resources that are
going into these you know quote unquote foundation foundational models you know changes what avenues
are available to what researchers just based on the resources of their organizations and
changes the research directions that are accessible to them. What's your take on that?
Yes I think that's a very real issue for many people especially in academic research labs
but also in many industrial research labs there are really only a few labs or companies
that can afford to even train these very large scale models anymore right. And it's great
when they make them publicly available because then the rest of us can kind of experiment with them
we can use them as the feature backbone and or new new algorithms that we're developing. I think
that there are some things that probably are really capabilities that as I mentioned earlier
could be emerging only when you have a very large training data set or a very large model that
you're training. And so that means that certain kinds of research is now in the realm of only
those lucky few that that have access to both the data and the compute. So the other paper is called
prefix conditioning unifies language and label supervision and it's a collaboration with google
and so here we're also looking at prefix conditioning or prompting rather but we have a slightly
different goal where as I mentioned the large vision language models are pre-trained on images
with captions right and that that gives us a lot of data that we get for free essentially from the
web but we also do have some label data sets the traditional data sets like image net which
have an image with a category label. And these two kinds of data are complementary right the
captioned images have a long tail distribution over object so they may not
cover all the categories that you want to cover or they may cover them unequally or like we mentioned
before they might have some bias like all the snowboarders or mail in the caption at least.
And so we might want to combine that data with just traditional labeled image data that is human
annotated and clean or less bias free and I'm not saying image net is completely unbiased either
so if we wanted to do that for pre-training it turns out that's what we look at in this paper
it turns out that you can do better than just combining the two kinds of data you can
so there's work from Microsoft that actually try to transform the labels in image net into
fake captions right so if you have an image an image net labeled with Persian cat you can
make a caption out of it by just plugging it into some template like a close-up of a Persian cat
so so they did this and then when you train on both the fake captions and the real captions
you do get a stronger model you get a more powerful model that combines the knowledge from
these two kinds of data it's kind of like a data augmentation type of approach yes exactly
and it generalizes even better to zero shot tasks so new data sets with novel categories
it generalizes even better to them so so one issue though is that now you have two kinds of
input data sets one with real captions and one with fake captions and these you know deep models they're
they're powerful which means they take shortcuts as we've been saying and so what
what we found is happening is the model actually knows more or less that it's learning from these
are fake captions and these are real captions and it sort of tries to learn from both kinds of captions
but then it ends up you know not really knowing at test time when you give it a new image
what to do because it basically one way to think of it is it's a little confusing for the model to
have you know real captions and fake captions and so what we ended up doing is just something super
simple which is add a token a training time to the real captions that just the token is just saying
this is a real caption and the fake captions we add a token that says this is a fake caption
and when you do that the model actually learns much better it generalizes better it
um essentially helps the model overcome this these two kinds of domains by telling it look
these are two kinds of domains so you can process them a little bit differently right so they're
there's some specialized processing in the language model that we see emerge from that where
if you give it the the caption and then you prefix it with the real caption token it will do one
thing but if you prefix that same caption with the fake caption token it will do another thing and
so for example you know if you prefix with the fake caption token the model will mostly focus on
kind of the noun because it's learned that these fake captions they're just fake so the only
information there is the noun right so and everything else is just kind of a template to
fool it into yeah it's for exactly but if you prefix it with the caption real caption token then
it will start looking at the whole caption because it knows that in real captions there is useful
semantic information in multiple words across the whole caption so so it's kind of cool
it is an example come to mind of you know the illustrates kind of the richness of a real of a
real caption relative to one of these fake captions yeah so this is an example from the paper it's
not a particularly rich caption but if you have a caption that says a sculpture of an airplane
and you prefix it with the you know this is a fake caption then the model ignores the word
sculpture and just focuses on the word airplane right and then if you prefix it with the real
caption token then the model will actually look at the sculpture as well as the airplane
in some sense this seems counterintuitive in the sense that you know that I guess the the
usual thought is hey more data different types of data that will kind of force the model to
generalize and that's going to lead to better performance and you're kind of saying that you know
more data different types of data the model is not generalizing it's just learning that the
classes and cheating right I think it is a little counterintuitive but I think we've seen this
in other work too that when you're training with different kinds of data heterogeneous data
sometimes it's better to specialize the model you know make it aware that it's being trained with
heterogeneous data and because you know I think if you throw everything together you're giving it
also less guidance right adding this prefix token is sort of just giving the model more
information look you know these two sets of images are coming from two different kinds of domains
and then the model can use that or not use that and you know in this case it seems to use it to
to its advantage kind of on the topic of domain generalization but maybe switching modalities a
little bit one of the other papers that you're speaking about at CBPR is focused on domain
generalization an unsupervised manner and this idea of kind of bridging across domains
and visual domains in particular can you talk a little bit about that paper and the motivation
there yeah definitely so this paper it's called unsupervised domain generalization by learning a
bridge across domains and we are looking at this problem of generalization to new kinds of visual
domains so an example is let's say your model was trained on driving data collected from a car
in California and then at test time you're giving it data from let's say Boston or New York in the
winter people look different because they're wearing heavy coats or hats or maybe it's raining or
maybe the trees are look different right so the visual domain has changed even though the
categories you're looking for are the same it's still people and cars and another example of this
is when you train the model on real photos but then you get a clip art or a painting or some other
type of drawing perhaps so this is a more extreme even domain shift in your input data
and you know there's been a lot of interest in robustness where we want models to be robust to
to kind of changes in the image so if you change the top left pixel you know your model shouldn't
all of a sudden flip its answer right and and there's a lot of work on adversarial robustness where
if an adversary changes the top left pixel in a certain way to make the model flip the answer
we don't want that either but I also think that a very very practical model is not some adversary
even or even degradation in the image quality but just just kind of a different viewing angle or
slightly different lighting or you know a little bit of a difference in how that object looks
like like a person with a hat on as opposed to no hat or a winter hat as opposed to summer hats
okay so all of these kinds of variations we want models to generalize to them right we don't
want them to break and they do break they they they currently you know models do not generalize
well to out of domain data data that is distributed differently from the training set so in this
paper we are trying to fix that by doing unsupervised learning where we have a bunch of images
so one example again I'll go back to paintings real images and clip art and sketches right so these
are images of the same classes but from different domains and what we want the model to do is we
wanted to learn that for example a giraffe is the same as a painting as it is in a sketch or in
a real photo right but if you train the model with self supervised losses you know this kind of
contrastive learning losses or sim clear moco all of these popular unsupervised training objectives
what the model does is it tries to learn how images are similar and it ends up actually learning
the domain similarity before category similarity in this case so we'll learn that a sketch of a
giraffe is closer to a sketch of a guitar than it is to a photo of a giraffe right because it's
picking up on those more superficial features are are you making the general statement that unsupervised
approaches have fared worse in multi domain or you know non-constant non-single domain scenarios then
then supervised approaches because of this general tendency to favor domain similarity versus
object similarity yes I think I think I will risk making that claim you know we have a couple papers
that supported within certain parameters but yes I think what happens when you have multi domain
data with class labels you know the class labels are telling the model that this is a giraffe even
though it's a sketch and this is a giraffe even though it's a photo and so focus on the
giraffeness of the thing as opposed to other you know other types of correlations exactly right
so so just that supervision of you care about the category not the domain that helps you learn a
more generalizable representation but when you have unlabeled data the only training single signal
you're giving to your model is okay if you take this sketch of a giraffe and you augment it
with some you know you add some noise or crop it or rotate it or whatever people do or even just
find its nearest neighbor it should be closer to that nearest neighbor than to some other image in
your data that's farther away and and there's no category information there so the model doesn't
know that the giraffe should be close to the painting giraffe should be close to the sketch
giraffe because in pixel space there's a huge difference between those two images
yeah so just very briefly to describe the main idea in the paper we essentially learn this bridge
domain so think of it as creating a version of each training image that tries to remove all the
domain specific information and only keep the general kind of outline and the general features of
the object right so for the giraffe example it ends up looking kind of like an edge image
but that was my impression yeah yeah but unlike an actual you know cany edge detector image or
traditional edge detector which also picks up on a lot of edges on the background seeing you know
a lot of irrelevant edges or it may on the other hand remove edges that are important like the spots
on the giraffe because you know that's just edge detection it doesn't know what's important
and what isn't in our approach this bridge domain is edge-like but it keeps the semantically
important features and edges like the outline of the giraffe and the spots in the giraffe but
removes all the irrelevant edges and the edge domain is is learned as well kind of end-to-end
as part of as part of training the whole model yes so and the important thing is that we don't just
we don't actually use that edge domain to do the final classification it's actually
use to compute which images are similar to each other and so I think one criticism of this
approach initially might be wait a minute but if you're using an edge-like domain aren't you
throwing away color information well in fact we're not the model is still learning color features
but to learn which objects are similar and which ones are not we're using this bridge domain which
is edge-like so does that mean like in the final in the loss function there's you know some factor
relating to the you know distance in the you know the edge domain space and also another factor
relating to distance in the the actual image space yes and in fact the distance in the edge domain
space guides the the model in learning it tells it which which are the similar pairs and which are
the dissimilar pairs so that it can train in the unsupervised way from the original images
right so think of it as kind of this we're faking some supervision that is a little bit stronger
hopefully than just the model by itself trying to figure out which images are similar and which
ones are not and being confused by all of these extraneous kind of textures and domain specific
features and so in creating this edge domain space like how I get how handcrafted is that in a
sense like I'm imagining if you you know found a way to create some embedding of the you know similar
classes that it's not particularly likely that you're going to come out with these cool looking
edge images like that that's more more handcrafted is what I guess the feel that I'm having here
so I think that it seems maybe handcrafted because we initialize the generator with an edge
edge image so the part of the model that learns to map the original images to the bridge domain is
initialized by computing edges of those images and so originally it learns just edge maps but
then as we keep training it in the sense supervised way to kind of learn about objects essentially
it starts to get away from the original just edge detection kind of images and starts to for
example remove some of the relevant background edges and keep the relevant ones that are useful
for computing kind of objects similarity well we'll be linking to your workshop presentations
and slides and folks can kind of dig into those for the papers that you've worked on and that you
recognize cited but I want to thank you for a great discussion once again sounds like you're
going to be super busy as CVPR yeah but it'll be fun the first in-person conference in three years
so I'm excited well Kate thanks so much for taking the time to share with us a bit about what you've
been up to thank you for having me
