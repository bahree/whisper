WEBVTT

00:00.000 --> 00:21.760
All right, everyone. I am here with Meredith Broussard. Meredith is an associate professor at NYU and research director at the NYU Alliance for Public Interest Technology.

00:21.760 --> 00:27.280
Meredith, welcome to the Twoma AI podcast. Thanks so much for having me. It's great to be here.

00:27.280 --> 00:56.640
I'm super excited to have you on the show. We last spoke. It was almost exactly a year ago in the context of the release and our screening of the coded bias documentary, which you are a part of and you participated in a panel with our community, which was a great discussion and excited to be speaking once again to get us started. I'd love to have you share a little bit about your background.

00:56.640 --> 01:17.680
Sure, so it is great to be back. So I am a data journalism professor at NYU. I started my career as a computer scientist. I quit to become a journalist and one of the things that I do is I develop AI systems for investigative reporting.

01:17.680 --> 01:32.480
However, when I started doing that, I would, you know, go to meet people at parties and I would say this is what I do and you would kind of look at me blankly and say, you know, you mean like you make robot reporters and I would say no, that sounds awesome, but that's not what I do.

01:32.480 --> 01:51.520
And so I realized that there was a need to talk more broadly about what AI is and isn't, which kind of led me into the global conversation about AI ethics and my own interests in social justice in public interest technology.

01:51.520 --> 02:10.560
I have kind of informed informed my work in that sphere. So I talk about technology, I make technology and I am excited to talk with you today about the intersection of technology and society.

02:10.560 --> 02:25.600
So the alliance that you directed NYU that's focused on public interest technology. I guess, problems to question for me when you say public interest technology, what exactly does that mean?

02:25.600 --> 02:44.640
Oh, that's such a good question. So the alliance was founded by my colleague, Charlton McElaine, who is the author of the book Black Software, which I highly recommend if you haven't read it yet. And public interest technology refers to exactly what it sounds like doing technology that is in the public interest.

02:44.640 --> 02:58.640
So there are kind of two ways you can do it. Sometimes public interest tech means making better government technology. It kind of got started as a field after the healthcare.gov debacle.

02:58.640 --> 03:21.680
After healthcare.gov launched and nobody could buy health care through it. People working in the government realized, oh, wait, we really need to modernize things. We need to upskill government workers so that we can do things like develop websites and and have effective government technology.

03:21.680 --> 03:44.720
So some people who are working in public interest tech are building better government technology. They're doing things like renovating the unemployment insurance system and making sure that healthcare.gov keeps working because interestingly, you haven't heard anything about healthcare.gov since it's initial initial struggles and that's really good.

03:44.720 --> 03:51.760
It's exactly what you want in government software, right? Like you you wanted to work. You don't want to think about it too much.

03:51.760 --> 04:07.760
So the other way you can do public interest technology is the way that journalists do it, which is that you can use technology in order to hold decision makers or algorithms accountable.

04:07.760 --> 04:23.760
So traditionally, one of the functions of the media is to hold people in power accountable. And so one of the things we do as algorithmic accountability reporters is we interrogate algorithms.

04:23.760 --> 04:37.760
And you can see a lot of really interesting algorithmic accountability work happening at the markup. The markup just published an investigation today, the day we were recording.

04:37.760 --> 04:53.760
That analyzes a huge trove of data that they found just unsecured sitting there, totally unprotected on the internet from a predictive policing software program called Pred Paul.

04:53.760 --> 05:18.760
So this has given millions of recommendations about what it thinks potential crime areas are. And the markup in gizmodo found this data, analyzed it and they discovered that the most widely used predictive policing software is systematically targeting and harassing black and brown people poor people.

05:18.760 --> 05:34.760
So the software is magnifying existing inequalities in the world. And we wouldn't know this unless we had reporters who are creating technology that is in the public interest that is monitoring.

05:34.760 --> 05:50.760
You know, kind of monitoring the software that's used by by institutions.

05:50.760 --> 06:05.760
This time of year is an invited talk at noreps that you're doing that intersects with a book that you recently completed, not yet published, but completed.

06:05.760 --> 06:23.760
I'd love to dig into those two there related. The provisional title of the book is more than a glitch what everyone needs to know about making technology anti racist accessible and otherwise useful to all.

06:23.760 --> 06:50.760
One, maybe starting point for the conversation is the book kind of speaks broadly about technology, your talk is at noreps, you know, an AI conference, you know, what do you see as the relationship between technology and AI, are you are the issues in one kind of common to the other.

06:50.760 --> 07:10.760
But something I realized recently was that I, I no longer make a distinction between regular technology and AI right so when AI was new, it felt really special and it felt like, ooh, this is exciting and different.

07:10.760 --> 07:27.760
But actually in the past couple of years, AI has become increasingly mundane, there is AI and absolutely everything now, you know, you're activating something like 250 different AI models when you do a single Google search.

07:27.760 --> 07:41.760
You know, we are, we are talking over a video link, the automated transcription software that we're going to use to generate a transcript is a kind of AI.

07:41.760 --> 08:05.760
So AI is still marketed as something that's kind of woo woo and exciting, but in in practice, it doesn't really feel like that anymore, so I, I've kind of moved to looking at the whole of technology as as kind of integrated.

08:05.760 --> 08:20.760
And it reminds me of back in the day when we thought that things on the internet were different than things in real life, right, so we had online culture and we had IRL culture.

08:20.760 --> 08:22.760
Everything was called I or E.

08:22.760 --> 08:27.760
Yeah, remember when those things were different and now they're not.

08:27.760 --> 08:35.760
So AI used to be something different and special and kind of now it's not like now it's just seamlessly integrated.

08:35.760 --> 08:47.760
So one of the things that I do in the book is I just, I talk about AI, I talk about, you know, other kinds of technology and it's kind of all, it's all of a piece.

08:47.760 --> 08:55.760
It's all about technology and society and what are the intersections and collisions there.

08:55.760 --> 09:23.760
One of the interesting questions that that raises for me is we think in the AI community a lot and talk a lot about, you know, AI ethics, responsible AI and a lot of those conversations are grounded in understanding and in an understanding of the way machine learning models are created and biases and all of that and

09:23.760 --> 09:41.760
you know, blurring the distinction between AI and technology, you know, suggests that maybe the way to think about these issues isn't from a, you know, technology up perspective, but a, you know, problem down or back perspective.

09:41.760 --> 09:45.760
Is that part of what you're encouraging us to do?

09:45.760 --> 09:57.760
I think that's a really good way of putting it. My, my perspective on how do we solve problems of AI ethics is not about starting with the technology.

09:57.760 --> 10:05.760
It's about starting with the human problem and looking at what do we already have in place.

10:05.760 --> 10:20.760
So, for example, let's take insurance, which, okay, I realized that maybe this is a boring example for some people, but I promise, I promise it's not actually going to be a boring example.

10:20.760 --> 10:39.760
So, let's take the example of insurance. So, there was big, a big scandal a few years ago when it was discovered that Optum was using models that discriminated against a particular group of people.

10:39.760 --> 10:53.760
And it was found that the models were biased. Okay, well, so how do you, how do you prove that? How do you prosecute that? How do you remedy that so that it doesn't happen again?

10:53.760 --> 11:11.760
And also, how do you figure out whether it's happening at other insurance companies? You know, our other insurance companies also using models to try and figure out who, you know, who should be allowed to access certain kinds of health care.

11:11.760 --> 11:24.760
In this case, the models were predicting that black people were going to be more expensive patients or that black people shouldn't have particular kinds of treatments.

11:24.760 --> 11:47.760
We know that the models are making, making decisions that don't make sense to human beings. And we also know that ML models machine learning models will discriminate by default. They will take all of the existing systematic inequality in the world, and they will reproduce it and they will generally not make the right decisions.

11:47.760 --> 12:03.760
So we can look at it from a social perspective. We can say, okay, what kinds of laws do we have in place that prevent discrimination? What kinds of regulatory bodies do we have in place to regulate companies?

12:03.760 --> 12:21.760
And in the case of insurance, we have state insurance regulatory boards. Right. So what do we need? Well, we need the people who work at state insurance regulatory boards to have enough technical savvy to understand that this is happening.

12:21.760 --> 12:37.760
We need them to have tools in order to identify bias. And we need the legal system to be able to intervene and say, okay, this model is being discriminatory.

12:37.760 --> 12:57.760
Okay, you know, here is the, here is the carrot, you know, we're going to, you know, we're going to start implementing sanctions or whatever. And then we've got the stick of, okay, you're still violating the law.

12:57.760 --> 13:17.760
You know, company you are in legal trouble now. So it's a, it's a systems wide approach. And it's not necessarily about building an AI to monitor the AI. It's not necessarily about, you know, defining human things in propositional logic and then building system against it.

13:17.760 --> 13:27.760
We can kind of look at what exists already and build on that instead of trying to reinvent reinvent the wheel.

13:27.760 --> 13:43.760
Part of the, the kind of learning in that story is that we've got existing frameworks, you know, frameworks around laws frameworks around auditing and governance that we can rely on to address some of these issues.

13:43.760 --> 13:53.760
What's still missing for me is what should opt in have done differently so that they didn't get into the situation that they found themselves in.

13:53.760 --> 13:57.760
Does your framework provide some guidance there?

13:57.760 --> 14:23.760
So that's a really good question. And so now we're back to public interest technology. Okay. So one of the things that I, that I talk about in my, in my nerve speech is a chapter from the upcoming book that is about public interest technology and about auditing and about how can we build software systems that do not include algorithmic bias.

14:23.760 --> 14:37.760
So my current thinking on this is that we can use all of the really amazing work that's been done recently on mathematical dimensions of fairness.

14:37.760 --> 14:52.760
And we can integrate this into our software systems and we can build monitoring systems, continuous monitoring systems to make sure that algorithmic bias is not sneaking its way in.

14:52.760 --> 15:15.760
We're working with Kathy O'Neill and her company orca O'Neill risk and algorithmic auditing risk consulting and algorithmic auditing associates and we are building a platform for monitoring algorithmic systems for algorithmic bias.

15:15.760 --> 15:30.760
And what I love about that is that it's kind of, it parallels what's already happening from a technology perspective to do continuous monitoring of machine learning models.

15:30.760 --> 15:46.760
It's just asking slightly different questions instead of focusing on accuracy and data drift and other things. This is more focused on monitoring for biased responsibility and other topics.

15:46.760 --> 16:10.760
And so for many organizations that are currently going through the process of building out this monitoring infrastructure anyway, they will be down the path and can maybe just plug in some of the or extend what they're already doing to help them address and prevent these kinds of challenges.

16:10.760 --> 16:20.760
Yeah, exactly, exactly. And looking for algorithmic bias auditing for algorithmic bias should be part of the workflow.

16:20.760 --> 16:31.760
There is a, there's a diagram that I use in the book that comes from Salesforce and it's about the software development cycle and kind of where does.

16:31.760 --> 16:41.760
You know, where should you be thinking about algorithmic bias and the quick answer is you should be thinking about it at every point in the software development cycle.

16:41.760 --> 17:00.760
And if you are monitoring and you find bias or you find a problem you discover that, you know, there are a dozen user reports that say, well, we've been having this particular kind of problem and you realize that, oh, the software is discriminating against.

17:00.760 --> 17:18.760
People of color where it's not working for people who have a particular, a few not physical phenotype, then you, you roll back, you roll back the software, you address the problem immediately.

17:18.760 --> 17:35.760
And this is just your, this is a different way of doing business, but this is how we should be approaching things. We should be acknowledging that algorithms can cause harms that this is an ethical issue that this is a marketplace issue.

17:35.760 --> 17:42.760
And this is something that companies need to keep on top of same way that they keep on top of other compliance issues.

17:42.760 --> 17:57.760
Another topic from the book and also your NURPS talk is on the intersection of gender and technology. Yeah, how does gender come into play.

17:57.760 --> 18:21.760
Sure. So what I do in my NURPS talk is I kind of read to two sections of the book. One is about public interest technology and algorithmic auditing and why I think that's exciting and the other is about how the new frontier for gender rights is inside databases.

18:21.760 --> 18:33.760
So it started a few years ago when I started actively trying to be a better ally to the trans community.

18:33.760 --> 18:45.760
And it also had to do with a problem that I was having, which is that I was running late and I needed to take the train downtown to work.

18:45.760 --> 18:59.760
And I didn't have any cash. So I was living in Philadelphia and at this point on the train, they didn't have ticket machines at every station.

18:59.760 --> 19:10.760
So if you didn't have a ticket, you needed to buy a ticket from the conductor using cash on the train, but I didn't have any cash and I was late and there was no ticket machine.

19:10.760 --> 19:22.760
And my husband had a monthly transit pass and I said, oh, can I use your monthly transit pass? And he said, well, I would give it to you, but it has a big M sticker on it for mail.

19:22.760 --> 19:34.760
And I don't think that the conductor would let you use it. And I said, well, why does it have a gender sticker that's, you know, that's ridiculous. Why can I use your trans pass? I said, I don't know, like that's just the way it is.

19:34.760 --> 19:47.760
And I didn't know about this because I had never bought a monthly transit pass before in Philadelphia. So I started thinking about, okay, well, is this a problem for other people like me?

19:47.760 --> 20:07.760
And then I, that, that question is this a problem for other people. The way it is for me is a question that reporters ask ourselves a lot and it's where a lot of trend stories get started. But then I started asking myself a different question that maybe we should ask more often, which is, is this a problem for people who are not like me.

20:07.760 --> 20:36.760
Who else might be affected by this? And I realized that if you are a member of the trans community, you're probably, you know, you're probably experiencing all kinds of microaggressions and, you know, kind of horrific situations from people selling the transit pass because the process of the transit pass was you have to go up to the

20:36.760 --> 20:57.760
you know, to the little window and you have to buy your pass and they kind of eyeball you and then put on a sticker and that did not seem like it was an interaction that was that was guaranteed to go well in 2013 when I was writing about this.

20:57.760 --> 21:11.760
I had a really, I had an interesting experience very much along these lines just recently, I was, I think I was signing up for some airline frequent flyer program or something.

21:11.760 --> 21:37.760
And I must have fat finger to form or checkbox or something like that. And I got this error message when I tried to submit it that said that the gender that I had clicked didn't match with the title that I had clicked like mister or doctor or whatever that was didn't match with, you know, the male female option.

21:37.760 --> 21:51.760
And I forget which one I did incorrectly, but I thought that's an elaborate rule right one that that is totally unnecessary.

21:51.760 --> 22:01.760
Yeah, not alone that they were, you know, they were both required fields right like, you know, I don't so that's incredibly out of date.

22:01.760 --> 22:10.760
And so it turns out that there are a lot of computer systems and a lot of human systems that are set up like this.

22:10.760 --> 22:22.760
And it comes from the fact that computer systems, especially large scale ones large scale institutional systems are designed with 1950s ideas about gender.

22:22.760 --> 22:38.760
And inside the systems gender is often stored as a binary. So you know about the gender binary. It's the idea that there are only two genders who male and female are understanding socially of gender has now expanded.

22:38.760 --> 22:50.760
They understand that there are more than two genders the gender is a spectrum and people have different gender identities gender should be an editable field for one thing.

22:50.760 --> 23:04.760
But a lot of these systems are built so that you can't edit the gender field. So there are only two options. And even further behind the scenes, you have to think about how is that data stored.

23:04.760 --> 23:11.760
So it can be stored as a string, which is a word. It could be stored as a number.

23:11.760 --> 23:19.760
Okay, 0123456789 or it can be stored as a binary as a zero or one.

23:19.760 --> 23:37.760
So when computer systems were made in the 60s 70s and like through the 90s really like until storage got really cheap programmers were extremely conservative about how much space they used in programs.

23:37.760 --> 23:53.760
So if you had a data field, you wanted to use as smallest space as possible because storage space was really expensive memory was expensive. So storing something as a binary storing a data field as a binary was a more efficient way of programming.

23:53.760 --> 24:07.760
And this ended up being the habit. And so today people still will store gender as a binary field, which prevents it from being editable prevents you from writing in a gender.

24:07.760 --> 24:22.760
So addressing this small piece of computational systems is actually really important for a for gender equality for, you know, for progress.

24:22.760 --> 24:39.760
So here have you seen downstream implications when it comes to using this data and machine learning AI, those kinds of applications, you know, I'm not too worried about that.

24:39.760 --> 24:52.760
I feel like the computers should fall in line with what is happening in society. I don't think that people should bend over backwards to, you know, to make things easier for computers.

24:52.760 --> 25:19.760
Another topic that you cover in the book is around education and subject that you've seen with in the educational domain. Can you talk a little bit about that? Sure. Something I wrote for the New York Times, a year or two ago was a story about when real students are assigned imaginary grades.

25:19.760 --> 25:25.760
So during that even mean, I know it's it's shocking. Isn't it?

25:25.760 --> 25:38.760
I've always been fascinated by, by issues of technology and education. I mean, I'm a professor. I use a lot of technology. I teach students how to make technology and also how to critique technology.

25:38.760 --> 26:03.760
And I found out that sometimes people are using algorithmic systems to predict student grades, which is really ridiculous because why are you like, why aren't you allowing the student to succeed or fail on their own?

26:03.760 --> 26:26.760
The point of education is is kind of see is letting somebody have their own journey through the educational system and there's this very American idea that you can succeed through education that you can pull yourself up by the bootstraps that we have public education that is available everybody and if you want to value yourself of it, it's there.

26:26.760 --> 26:39.760
And so these predictive systems are really terrible because all they do is predict that poor kids will fail and rich kids will succeed.

26:39.760 --> 26:44.760
And what are examples of these systems and where they're used?

26:44.760 --> 26:57.760
So what happened during the pandemic is that the international baccalaureate organization, which is an international degree granting high school degree granting organization canceled their usual exams.

26:57.760 --> 27:20.760
So IB exams happen in the spring and they are a little bit like AP exams. It's a content based exam in a different in different subjects. And if you get a high enough grade on your IB subject exam, you can get an IB diploma, which is in addition to your regular high school diploma and it's very prestigious.

27:20.760 --> 27:40.760
And you can also get college credit for high enough IB scores. So for low income students, what they can do is they can take a bunch of IB exams and get, you know, if they get good scores, then they can get college credit and graduate in fewer years.

27:40.760 --> 27:55.760
Really, it's a really crucial piece of maintaining affordability for low income students. And in fact, most of the of the students who are enrolled in IB and the US do come from low income backgrounds.

27:55.760 --> 28:19.760
So the IB obviously couldn't have their exams in person during the height of the pandemic. And so they decided to cancel the in person exams and use an algorithmic system to predict the grades that the students would have gotten had they take in the test, which they didn't do because there's a pandemic.

28:19.760 --> 28:41.760
And I mean, just so they assigned imaginary grades to real students. And the imaginary grades said, OK, we're going to predict that the poor kids and the black and brown kids are going to get bad grades. I'm going to predict that the white kids, the rich kids are going to get good grades. And of course, there was a disaster and there were mass thousands and thousands of people protested and it was just a really poor decision.

28:41.760 --> 29:01.760
So it sounds ridiculous in retrospect, but at the time, the bureaucrats were like, oh, well, let's, you know, we, we have all this data, let's just plug in into an algorithm system and let it make a prediction because, you know, it was this, this techno chauvinist idea that the computer is just going to step in and solve all of our problems.

29:01.760 --> 29:15.760
The computer doesn't really solve all of our problems. The computer is great, but the computer is not magic. And it's just going to replicate the worst of humanity.

29:15.760 --> 29:32.760
And if love to its own devices, exactly, exactly. So we can't just, we can't just build computer systems and set it and forget it and expect those computer systems to make good decisions in the social realm.

29:32.760 --> 29:45.760
The computers are machines for doing math, like they literally compute and, you know, mathematical fairness is not the same as social justice.

29:45.760 --> 30:05.760
We are past the era where we could just build computer systems to solve the easy problems. We've solved all the easy problems with computers, right? Like we are left with the hard problems. And we're not, we're not, well, we have a ways to go.

30:05.760 --> 30:28.760
And so what was some of your key takeaways for the NARPS talk? What I would love for people to take away from the NARPS talk is an increased understanding and awareness of computer systems as a sociotechnical systems.

30:28.760 --> 30:46.760
I would love for people to think harder about the way that we build systems and the way that computer systems might be interfering or preventing social progress.

30:46.760 --> 30:59.760
And so I would love if, you know, somebody hears the, here's the talk and they are building a computer system and they say, oh, well, hey, we need to make, you know, make the gender feel editable.

30:59.760 --> 31:19.760
We need to go back and, you know, look at our large scale systems inside this bank and make sure that when, when somebody transitions, we have an easy way for them to, to update their name to update their gender.

31:19.760 --> 31:42.760
I would love it if somebody would listen to the talk and adapt Google Photos so it doesn't sneak attack trans folks with, you know, with pictures of their pre-transition cells, which can be, you know, which can be really triggering and alienating.

31:42.760 --> 31:51.760
I would love if somebody would listen to the talk and say, oh, hey, I really want to get interested get involved in public interest technology.

31:51.760 --> 32:06.760
And they go to the NYU Alliance for Public Interest Technology website and find out more information or they go to the public interest technology university network website and find out more information and get involved locally.

32:06.760 --> 32:14.760
Awesome. All great next steps. Meredith, it's been wonderful catching up. Thanks so much for sharing a bit about what you're up to.

32:14.760 --> 32:43.760
Sam, thanks so much for having me. Take care.

