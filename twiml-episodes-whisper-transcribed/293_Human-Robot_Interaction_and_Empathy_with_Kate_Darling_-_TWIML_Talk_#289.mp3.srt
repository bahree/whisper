1
00:00:00,000 --> 00:00:16,320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

2
00:00:16,320 --> 00:00:21,480
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:21,480 --> 00:00:34,720
I'm your host, Sam Charrington, hey what's up everyone, before we jump into today's

4
00:00:34,720 --> 00:00:37,320
show, big news on the Twimble Con front.

5
00:00:37,320 --> 00:00:42,440
I am excited to announce that in response to strong attendee and speaker interest, we

6
00:00:42,440 --> 00:00:46,800
have supersized our agenda.

7
00:00:46,800 --> 00:00:47,800
What does that mean?

8
00:00:47,800 --> 00:00:51,640
Well we've added a second track, doubling the number of great sessions and speakers that

9
00:00:51,640 --> 00:00:53,200
we can accommodate.

10
00:00:53,200 --> 00:00:58,680
And we're unveiling part of that expanded agenda today for the first time at twimblecon.com

11
00:00:58,680 --> 00:01:00,840
slash speakers.

12
00:01:00,840 --> 00:01:05,360
Check it out and then hit that big green register today button to join us and learn from

13
00:01:05,360 --> 00:01:09,960
our experts how your organization can eliminate the barriers to building effective machine

14
00:01:09,960 --> 00:01:13,480
learning models and getting them into production.

15
00:01:13,480 --> 00:01:18,240
Hope to see you there.

16
00:01:18,240 --> 00:01:20,840
All right everyone, I am on the line with Kate Darling.

17
00:01:20,840 --> 00:01:24,360
Kate is a research specialist at the MIT Media Lab.

18
00:01:24,360 --> 00:01:27,160
Kate, welcome to this week in machine learning and AI.

19
00:01:27,160 --> 00:01:28,920
Thanks for having me.

20
00:01:28,920 --> 00:01:31,960
I'm really looking forward to this conversation.

21
00:01:31,960 --> 00:01:39,160
We met not too long back at the AWS Remarers Conference or Amazon Remarers Conference where

22
00:01:39,160 --> 00:01:46,880
you did a great presentation on some of your research into human and robot interactions.

23
00:01:46,880 --> 00:01:50,800
And I'm really looking forward to diving into that.

24
00:01:50,800 --> 00:01:54,760
But let's start with the kind of broad brush look at the field.

25
00:01:54,760 --> 00:01:58,640
You're a leading expert in robot ethics.

26
00:01:58,640 --> 00:02:02,200
What exactly is robot ethics and why is it important?

27
00:02:02,200 --> 00:02:04,320
That's a great question.

28
00:02:04,320 --> 00:02:10,760
So robot ethics sounds very science fictiony, very, you know, I robot blade runner-esque.

29
00:02:10,760 --> 00:02:18,280
But when I talk about robot ethics, what I really mean is the ethical use of robotic technologies.

30
00:02:18,280 --> 00:02:24,960
And part of that is, you know, how we integrate technology into the workforce, how we think

31
00:02:24,960 --> 00:02:29,800
about responsibility for harm in all sorts of contexts, whether that's automated weapons

32
00:02:29,800 --> 00:02:34,680
systems or automated vehicles or other types of automated technology.

33
00:02:34,680 --> 00:02:40,360
But the main thing I'm interested in in the sphere of robot ethics is the social aspect

34
00:02:40,360 --> 00:02:44,720
of integrating robots that seem very lifelike to people.

35
00:02:44,720 --> 00:02:49,200
So I'm really interested in the ways that people treat robots like they're alive, even

36
00:02:49,200 --> 00:02:51,240
though they know that they're just machines.

37
00:02:51,240 --> 00:02:55,760
And what sort of ethical issues can arise from that?

38
00:02:55,760 --> 00:03:01,400
And how did you get interested in this field, what's your background and what led you to

39
00:03:01,400 --> 00:03:04,000
this area of focus?

40
00:03:04,000 --> 00:03:12,080
So I originally studied law and social sciences and I did law and economics and intellectual

41
00:03:12,080 --> 00:03:18,600
property, but I think that my interest has always been in how systems shape behavior.

42
00:03:18,600 --> 00:03:24,160
So, you know, if you look at the laws of system or economics is a system, or now I'm really

43
00:03:24,160 --> 00:03:29,000
focused on technology as a system, and how it shapes human behavior.

44
00:03:29,000 --> 00:03:34,880
And I think what really got me interested in robots in particular was this one moment

45
00:03:34,880 --> 00:03:41,000
where I was I was in law school and I bought this baby dinosaur robot called a pleo, they

46
00:03:41,000 --> 00:03:45,640
don't make them anymore, but it was this really cool toy that had this kind of lifelike

47
00:03:45,640 --> 00:03:46,960
behavior.

48
00:03:46,960 --> 00:03:50,360
And one of the things it did was mimic pain very well.

49
00:03:50,360 --> 00:03:54,560
So like if you held it up by the tail, it had a tilt sensor and it knew that it was upside

50
00:03:54,560 --> 00:03:59,840
down, so it would start to cry and squirm around.

51
00:03:59,840 --> 00:04:06,080
It's super cute and it was really cool like for the toy that it was at the time, so I would

52
00:04:06,080 --> 00:04:10,400
show it off to people and I would be like hold it up by the tail, see what it does.

53
00:04:10,400 --> 00:04:15,680
And people would hold it up, and after a while it started to bother me when they held

54
00:04:15,680 --> 00:04:22,680
it up too long, and I would tell them to put it back down, I'd say that's enough now.

55
00:04:22,680 --> 00:04:27,800
And that was really interesting to me because I knew exactly how the toy worked, but I still

56
00:04:27,800 --> 00:04:31,160
felt this empathy for it when it was crying.

57
00:04:31,160 --> 00:04:33,680
And I was like that's weird.

58
00:04:33,680 --> 00:04:38,840
And then I started looking into this more and I discovered the whole field of human robot

59
00:04:38,840 --> 00:04:43,200
interaction that looks at how people interact with robotic technology.

60
00:04:43,200 --> 00:04:51,560
A lot of it is studies that border on psychology and people's tendency to treat these machines

61
00:04:51,560 --> 00:04:55,520
like living things and how to tweak that.

62
00:04:55,520 --> 00:05:00,520
So I got very interested in that, and I started coming at it though from the perspective

63
00:05:00,520 --> 00:05:07,440
of okay clearly we do this, but what does this mean in the broader context of a society

64
00:05:07,440 --> 00:05:11,560
where we're increasingly integrating robots into shared spaces?

65
00:05:11,560 --> 00:05:16,120
You have several of these PLEO robots to this day.

66
00:05:16,120 --> 00:05:21,440
I think you showed some photos of them in your presentation and we'll make sure to either

67
00:05:21,440 --> 00:05:26,880
include or link to some of those photos in the show notes.

68
00:05:26,880 --> 00:05:28,840
Yeah, they're very cute.

69
00:05:28,840 --> 00:05:39,280
So you kind of mentioned the increasing role of robots in our day to day lives as we're

70
00:05:39,280 --> 00:05:45,520
transitioning from world where they, you know, the primary experience that most folks

71
00:05:45,520 --> 00:05:50,280
had with robots was if they work with them like industrial types of robots and now we're

72
00:05:50,280 --> 00:05:58,560
starting to see these robots that are in stores guiding people around or your robot baristas,

73
00:05:58,560 --> 00:06:00,680
things like that.

74
00:06:00,680 --> 00:06:09,360
How is the study of human robot interaction practically applied to, you know, this new

75
00:06:09,360 --> 00:06:13,640
world that we're evolving into?

76
00:06:13,640 --> 00:06:18,320
Well yeah, so like you said, you know, we're very familiar with industrial robots.

77
00:06:18,320 --> 00:06:22,760
We've had those for a long time, but what's happening right now is that robots are coming

78
00:06:22,760 --> 00:06:29,120
into workplaces and households and public spaces, you know, stores.

79
00:06:29,120 --> 00:06:32,440
And right now the technology is still very crude.

80
00:06:32,440 --> 00:06:37,080
I know like my mom recently had an encounter with a robot in stop and shop that she was

81
00:06:37,080 --> 00:06:38,080
not happy with.

82
00:06:38,080 --> 00:06:46,200
She was like, creepy and it's beeping and I don't like it, but I think it's a matter of

83
00:06:46,200 --> 00:06:52,800
time before the design gets better and more compelling and people actually, you know, start

84
00:06:52,800 --> 00:06:54,960
to accept robots in their shared spaces.

85
00:06:54,960 --> 00:06:59,960
I think it's inevitable that this is going to happen. I also think it's inevitable that

86
00:06:59,960 --> 00:07:04,600
we're going to be working more with robotic technology because I know the media likes to

87
00:07:04,600 --> 00:07:07,600
talk about how robots are taking all the jobs.

88
00:07:07,600 --> 00:07:11,400
And that's true in some cases, jobs that are very, very easily automated.

89
00:07:11,400 --> 00:07:15,400
But in most cases, robots aren't good replacements for humans and they have a very different

90
00:07:15,400 --> 00:07:16,400
type of skill set.

91
00:07:16,400 --> 00:07:21,960
So what is actually happening is that we're going to see more technology that people have

92
00:07:21,960 --> 00:07:30,760
to work with and human robot interaction helps to study how people interact with that technology

93
00:07:30,760 --> 00:07:37,600
and how to design it in a way that they might trust it or even enjoy working with it instead

94
00:07:37,600 --> 00:07:44,280
of just saying, oh, this machine is threatening to me or it doesn't work, it made a mistake

95
00:07:44,280 --> 00:07:46,480
and I don't like it.

96
00:07:46,480 --> 00:07:51,560
Human robot interaction is oftentimes about designing the technology in a way that is

97
00:07:51,560 --> 00:07:56,640
more palatable to people and that people might even like to work with.

98
00:07:56,640 --> 00:08:04,960
And it's also about finding use cases for the technology that we might not even have.

99
00:08:04,960 --> 00:08:09,120
So it's not just about workplace integration, but there are also some applications in

100
00:08:09,120 --> 00:08:13,640
health and education that are really interesting where we're starting to see social robots being

101
00:08:13,640 --> 00:08:18,640
used as replacements for animal therapy, for example, in context where we can't use

102
00:08:18,640 --> 00:08:23,120
real animals or robots that are working with autistic children and engaging them in ways

103
00:08:23,120 --> 00:08:24,440
that we haven't seen before.

104
00:08:24,440 --> 00:08:30,720
So a lot of pretty cool things happening in human robot interaction.

105
00:08:30,720 --> 00:08:35,320
So I think it's a very useful field of study for this day and age.

106
00:08:35,320 --> 00:08:44,880
One of the aspects of human robot interaction that you study is, or at least the result of

107
00:08:44,880 --> 00:08:52,440
it is kind of this exploration of human empathy in those kind of scenarios and you've done

108
00:08:52,440 --> 00:08:58,800
a number of experiments to explore that, including I think involving these PLEO robots.

109
00:08:58,800 --> 00:09:01,920
Can you talk a little bit about some of the experiments that you've done?

110
00:09:01,920 --> 00:09:02,920
Yeah, sure.

111
00:09:02,920 --> 00:09:08,360
So the PLEO, I haven't actually done any scientific experiments with the PLEO robots because they're

112
00:09:08,360 --> 00:09:17,720
very expensive, and the experiments I've done usually involve destroying the robot.

113
00:09:17,720 --> 00:09:24,040
But what inspired the experimental work was a workshop that I did with five of these

114
00:09:24,040 --> 00:09:30,160
baby dinosaur PLEOs with my friend, Honest Gossult, where we took the baby dinosaur robots.

115
00:09:30,160 --> 00:09:32,080
We made five groups of people.

116
00:09:32,080 --> 00:09:33,080
These were all adults.

117
00:09:33,080 --> 00:09:34,520
They were at a conference.

118
00:09:34,520 --> 00:09:40,320
We had like five teams of six people each, and each team got a robot, and they named it,

119
00:09:40,320 --> 00:09:43,520
and they had to interact with it and play with it for like 45 minutes.

120
00:09:43,520 --> 00:09:49,520
And then we unveiled a hammer and a hatchet, and we told them to torture and kill the robots.

121
00:09:49,520 --> 00:09:52,640
And it was really interesting.

122
00:09:52,640 --> 00:09:53,640
Like we...

123
00:09:53,640 --> 00:09:57,720
It seems like you're getting uncomfortable just at the thought of torturing and killing these

124
00:09:57,720 --> 00:09:58,720
PLEOs.

125
00:09:58,720 --> 00:10:05,440
Well, it was really, really interesting to see that people were more uncomfortable than

126
00:10:05,440 --> 00:10:08,400
I expected them to be.

127
00:10:08,400 --> 00:10:11,840
We thought that some people would be like, yeah, sure, it's just a robot.

128
00:10:11,840 --> 00:10:16,040
I'll take this hatchet to it, and some people would be like, no, don't do it.

129
00:10:16,040 --> 00:10:21,200
And instead, in this particular group, everyone refused to even hit the robots.

130
00:10:21,200 --> 00:10:25,720
So we actually had to improvise in the workshop, and at some point we were like, okay, you

131
00:10:25,720 --> 00:10:31,800
can save your team's robot if you destroy another team's robot, and they tried to do that,

132
00:10:31,800 --> 00:10:33,280
and they couldn't do that either.

133
00:10:33,280 --> 00:10:37,200
And finally, we threatened to destroy all the robots, unless someone took a hatchet to

134
00:10:37,200 --> 00:10:38,200
one of them.

135
00:10:38,200 --> 00:10:48,600
And it was this very half-joking, half-serious discomfort that people felt when the robot

136
00:10:48,600 --> 00:10:52,120
kind of got destroyed by this hatchet.

137
00:10:52,120 --> 00:10:55,280
And there was actually a moment of silence in the room for the fallen robot.

138
00:10:55,280 --> 00:11:02,880
So it was just this very interesting, very dramatic, very not-scientific experiment day that

139
00:11:02,880 --> 00:11:03,880
we had.

140
00:11:03,880 --> 00:11:10,680
And that inspired some later research that I did at MIT with Palachnondi in Cynthia, Brazil.

141
00:11:10,680 --> 00:11:15,520
And for those experiments, we weren't using cute baby dinosaur robots in part because of

142
00:11:15,520 --> 00:11:18,560
the cost, like I mentioned, but in part also because we wanted to choose something that

143
00:11:18,560 --> 00:11:23,320
people don't immediately bond with and respond to.

144
00:11:23,320 --> 00:11:27,840
So we chose hex bugs, which are this toy.

145
00:11:27,840 --> 00:11:31,920
It's small, it moves around in a really lifelike way, like a bug.

146
00:11:31,920 --> 00:11:36,320
And we had people come into the lab and smash them with mallets, and we wanted to know two

147
00:11:36,320 --> 00:11:37,320
things.

148
00:11:37,320 --> 00:11:40,760
We wanted to know, would people hesitate more if we gave the hex bug a name and kind of

149
00:11:40,760 --> 00:11:41,760
a backstory?

150
00:11:41,760 --> 00:11:46,960
So if we said, this is Frank, and Frank's favorite color is red, and he likes to play.

151
00:11:46,960 --> 00:11:51,600
And the other thing we wanted to know was whether people's hesitation correlated in any way

152
00:11:51,600 --> 00:11:54,000
to their natural tendencies for empathy.

153
00:11:54,000 --> 00:11:59,760
So we did this psychological empathy test with them, and we found that people who scored

154
00:11:59,760 --> 00:12:05,960
low on the test for empathic concern, they would hesitate much less than the other people

155
00:12:05,960 --> 00:12:07,760
they would just hit Frank.

156
00:12:07,760 --> 00:12:11,680
And the people who scored very high on the empathic concern test would hesitate much more

157
00:12:11,680 --> 00:12:14,280
or even refuse to hit the hex bugs.

158
00:12:14,280 --> 00:12:19,720
So it was, you know, it was a little study, but it was kind of interesting because it

159
00:12:19,720 --> 00:12:24,720
indicates that, you know, we might even be able to measure people's empathy using robots,

160
00:12:24,720 --> 00:12:30,440
which is kind of like a weird turn on the VoicConf test from Blade Runner, and I know if you're

161
00:12:30,440 --> 00:12:31,920
familiar with that one.

162
00:12:31,920 --> 00:12:34,520
I don't remember the details of it.

163
00:12:34,520 --> 00:12:40,480
So in Blade Runner, you have robots that look just like humans, and so to tell whether

164
00:12:40,480 --> 00:12:45,880
someone is a robot or a human, they do this empathy test where they tell them these stories

165
00:12:45,880 --> 00:12:47,840
and see how they react to them.

166
00:12:47,840 --> 00:12:52,400
And so our version of that is, we can see how empathic you are as a human by telling

167
00:12:52,400 --> 00:12:55,240
you stories about a robot and seeing how you react to that.

168
00:12:55,240 --> 00:12:58,680
So it's kind of, it was, it's fun.

169
00:12:58,680 --> 00:13:07,680
Was it your presentation that showed a video of kids and a, like a mall security robot

170
00:13:07,680 --> 00:13:12,080
and some of the dynamics that occurred or did I see that separately?

171
00:13:12,080 --> 00:13:14,840
Or do you know the video that I'm referring to?

172
00:13:14,840 --> 00:13:15,840
I know this video.

173
00:13:15,840 --> 00:13:20,960
I did, I have not shown this video, but I have talked about the study.

174
00:13:20,960 --> 00:13:23,680
This is the one in Japan, right?

175
00:13:23,680 --> 00:13:30,000
I don't remember where it takes place, but the basic idea was they were, at least the

176
00:13:30,000 --> 00:13:36,080
thing that I remember is they were, you know, if they were multiple kids or no parents

177
00:13:36,080 --> 00:13:40,640
around, they were identifying the situations in which, you know, kids would come to abuse

178
00:13:40,640 --> 00:13:47,360
the security robot and, you know, things like multiple kids around and no parents were

179
00:13:47,360 --> 00:13:48,880
kind of key indicators.

180
00:13:48,880 --> 00:13:49,880
Yeah.

181
00:13:49,880 --> 00:13:54,600
And they ended up, it was so funny, they, because the paper is essentially about the solution

182
00:13:54,600 --> 00:13:59,880
that they found for preventing the security mall robot from getting beat up by the kids,

183
00:13:59,880 --> 00:14:05,120
which is they made it avoid people below a certain height and move towards taller people.

184
00:14:05,120 --> 00:14:10,520
They figure, okay, if there is an adult nearby, they'll intervene and stop the kids from

185
00:14:10,520 --> 00:14:16,480
like verbally and physically abusing the robot, which is kind of funny.

186
00:14:16,480 --> 00:14:22,680
It's really like, the video is funny, it's like terrifying and funny at the same time.

187
00:14:22,680 --> 00:14:27,040
And do you interpret, when you see that video, do you interpret it through the lens of kind

188
00:14:27,040 --> 00:14:31,000
of the empathy results of some of your experiments?

189
00:14:31,000 --> 00:14:38,560
Well, so my question when I look at that is, if you're a parent, do you intervene to stop

190
00:14:38,560 --> 00:14:44,760
your kid from beating up the robot for reasons other than just respecting property?

191
00:14:44,760 --> 00:14:51,280
Like, is there a reason to worry that your kid might, you know, learn that it's okay

192
00:14:51,280 --> 00:14:56,120
to treat something that responds in a life like way violently?

193
00:14:56,120 --> 00:15:01,680
And could that like translate to their behavior towards other children or animals or things

194
00:15:01,680 --> 00:15:02,880
that are alive?

195
00:15:02,880 --> 00:15:07,040
And so that's, we don't know the answer to that by any means.

196
00:15:07,040 --> 00:15:12,160
It's something that I think needs to be explored, but I also wonder about it in the context

197
00:15:12,160 --> 00:15:17,600
of adults even, you know, how muddled is it in our subconscious to treat something that's

198
00:15:17,600 --> 00:15:21,840
designed to respond in a really life like way violently?

199
00:15:21,840 --> 00:15:26,960
Is that a healthy outlet for violent behavior or is that, as my friend once said, training

200
00:15:26,960 --> 00:15:29,640
your cruelty muscles?

201
00:15:29,640 --> 00:15:30,640
We don't know.

202
00:15:30,640 --> 00:15:36,760
It's the same question as violence in video games, except that for video games, we seem

203
00:15:36,760 --> 00:15:43,720
to have landed on, you know, adults can compartmentalize when it's on a screen.

204
00:15:43,720 --> 00:15:46,600
Children were not so sure about, so we restrict it there.

205
00:15:46,600 --> 00:15:51,720
But robots bring this to a very new, very visceral level because of the physicality.

206
00:15:51,720 --> 00:15:53,200
We're very physical creatures.

207
00:15:53,200 --> 00:15:56,520
There's lots of research that shows that we respond differently to something in our

208
00:15:56,520 --> 00:15:58,400
space than to something on a screen.

209
00:15:58,400 --> 00:16:03,440
And so it seems like we might want to ask that question again.

210
00:16:03,440 --> 00:16:08,080
Well, also, with video games, the question comes up more often than not when the thing that

211
00:16:08,080 --> 00:16:14,920
we're being violent against is another human or a thing that's supposed to be a human.

212
00:16:14,920 --> 00:16:23,080
Whereas we don't often see humanoid robots kind of roaming around in real life.

213
00:16:23,080 --> 00:16:27,480
Very few of us have the opportunity to interact with things that are anywhere close to

214
00:16:27,480 --> 00:16:29,320
humanoid robots.

215
00:16:29,320 --> 00:16:33,680
That is true. It's funny how people always leap to humanoid robots.

216
00:16:33,680 --> 00:16:38,480
I think we have this tendency to constantly compare robots to humans, and I also admit

217
00:16:38,480 --> 00:16:40,920
that a lot of people are trying to build humanoid robots.

218
00:16:40,920 --> 00:16:43,760
That is definitely a fascination that is there.

219
00:16:43,760 --> 00:16:50,680
But when I think about life robots, I think about all sorts of different designs.

220
00:16:50,680 --> 00:16:55,080
I don't know if you've seen the baby seal robot that they use with dementia patients.

221
00:16:55,080 --> 00:16:56,080
No.

222
00:16:56,080 --> 00:17:01,120
It's super cute. It's been around for a long time, at least a decade.

223
00:17:01,120 --> 00:17:04,800
It's used as a therapeutic device in nursing homes.

224
00:17:04,800 --> 00:17:10,320
It's this baby harpsial that is furry and you pet it and it doesn't talk to you or do

225
00:17:10,320 --> 00:17:15,120
anything that might disappoint your expectations like a humanoid robot would because they're

226
00:17:15,120 --> 00:17:16,720
just not good yet.

227
00:17:16,720 --> 00:17:20,880
It just responds to your touch and makes these little sounds.

228
00:17:20,880 --> 00:17:22,920
It's very effective.

229
00:17:22,920 --> 00:17:27,000
I think that even though we're not in a place where we have robots that look like in

230
00:17:27,000 --> 00:17:30,720
Westworld or Blade Runner where we can't tell the difference, we are starting to see

231
00:17:30,720 --> 00:17:39,880
design that we certainly treat like a living thing, even though it's clearly not smart.

232
00:17:39,880 --> 00:17:48,160
And so part of your thesis perhaps is that our interactions with these things speak to

233
00:17:48,160 --> 00:17:54,560
empathy in the same way that our interactions with animals speak to some kind of fundamental

234
00:17:54,560 --> 00:17:57,080
empathy even though they're not human.

235
00:17:57,080 --> 00:17:58,080
Yeah.

236
00:17:58,080 --> 00:18:03,440
I actually think animals are really, really great analogy in this context.

237
00:18:03,440 --> 00:18:09,320
Obviously animals are alive and they experience things and they feel pain which robots absolutely

238
00:18:09,320 --> 00:18:10,320
do not.

239
00:18:10,320 --> 00:18:15,120
But I think one of the commonalities here is that throughout history, we've treated

240
00:18:15,120 --> 00:18:20,560
most animals like tools and products and not really cared about their inner worlds.

241
00:18:20,560 --> 00:18:25,640
And then there are just some animals that we've kind of bonded with and made our companions

242
00:18:25,640 --> 00:18:27,160
and treated with more kindness.

243
00:18:27,160 --> 00:18:31,960
And if you look at the history of the animal rights movement, this doesn't seem to really

244
00:18:31,960 --> 00:18:36,560
have anything to do with inherent biological criteria.

245
00:18:36,560 --> 00:18:40,160
It has more to do with what we relate to.

246
00:18:40,160 --> 00:18:45,760
People didn't care about whales until the moment that someone recorded them singing.

247
00:18:45,760 --> 00:18:49,360
And suddenly you have to save the whales movement.

248
00:18:49,360 --> 00:18:54,240
And this huge movement started back in the 70s because suddenly these were creatures

249
00:18:54,240 --> 00:18:56,200
that we could relate to.

250
00:18:56,200 --> 00:19:02,600
And when I look at how, yeah, I actually just met the guy who discovered whale song.

251
00:19:02,600 --> 00:19:07,320
It was very, I was very, very much fan-girling.

252
00:19:07,320 --> 00:19:10,800
It was amazing.

253
00:19:10,800 --> 00:19:11,800
That's awesome.

254
00:19:11,800 --> 00:19:16,040
But when I look at how we treat robots, I can see this going in a very similar direction

255
00:19:16,040 --> 00:19:20,080
where we treat most of them like tools and products and some of them.

256
00:19:20,080 --> 00:19:26,080
We really want to treat our pets and I think we're going to start to do that.

257
00:19:26,080 --> 00:19:36,960
You mentioned earlier trust in the human robot trust relationship and it reminded me of

258
00:19:36,960 --> 00:19:46,200
my conversation with Ayanna Howard back in February of last year, number 110 podcast.

259
00:19:46,200 --> 00:19:51,640
And we, one of the things that she talked about with some of her research in that area

260
00:19:51,640 --> 00:20:00,520
and how humans would, you know, essentially blindly follow, you know, these robots like

261
00:20:00,520 --> 00:20:06,680
for example, robots that are supposed to guide them out of a burning building, you know,

262
00:20:06,680 --> 00:20:11,280
in spite of the fact that the robots are doing, you know, they're obviously broken in some

263
00:20:11,280 --> 00:20:12,280
way.

264
00:20:12,280 --> 00:20:15,920
Like they're banging into a wall or something like that and the humans waiting around

265
00:20:15,920 --> 00:20:18,680
for them to, you know, give them guidance.

266
00:20:18,680 --> 00:20:24,880
Have you, is that research you're familiar with and how does that relate to any of your

267
00:20:24,880 --> 00:20:25,880
work?

268
00:20:25,880 --> 00:20:33,160
Yeah, so yeah, I love Ayanna's work and it's, it's very interesting.

269
00:20:33,160 --> 00:20:38,600
There is something that we call automation bias that is this trust that people place

270
00:20:38,600 --> 00:20:47,640
in robots or AI systems because in certain cases we place a lot of trust in these systems

271
00:20:47,640 --> 00:20:52,120
because we assume that they have the right answers and that they're not biased and that

272
00:20:52,120 --> 00:20:57,360
they're not going to, you know, make a mistake, the way that we trust a calculator to add

273
00:20:57,360 --> 00:20:58,360
numbers.

274
00:20:58,360 --> 00:21:05,080
So Madeline Ellish is someone who's done work on taking some of that research and looking

275
00:21:05,080 --> 00:21:11,440
at how it applies societally in the world and also on a policy level because she's, for

276
00:21:11,440 --> 00:21:13,720
example, what's that name again?

277
00:21:13,720 --> 00:21:14,720
Madeline Ellish.

278
00:21:14,720 --> 00:21:16,320
Madeline Ellish, okay.

279
00:21:16,320 --> 00:21:20,040
She's at data and society in New York, I believe.

280
00:21:20,040 --> 00:21:24,000
She loved her work, you should totally interview her.

281
00:21:24,000 --> 00:21:30,040
She, so she's looked at, for example, the fact that when there are a machine and a human

282
00:21:30,040 --> 00:21:34,960
working together, so you have a human in the loop and something goes wrong that was totally

283
00:21:34,960 --> 00:21:38,840
the machine's fault, the human usually gets blamed for it.

284
00:21:38,840 --> 00:21:46,320
So we have this over reliance on, on machines that is in some cases really unwarranted and

285
00:21:46,320 --> 00:21:50,760
it's not really clear what to do about that because technology keeps getting more and

286
00:21:50,760 --> 00:21:52,160
more complex.

287
00:21:52,160 --> 00:21:58,840
And so sometimes you can't have that education or that transparency that really lets people

288
00:21:58,840 --> 00:22:02,000
understand the limitations of the technology.

289
00:22:02,000 --> 00:22:07,600
But I also think that it's, some of it is because we currently, maybe thanks to science

290
00:22:07,600 --> 00:22:12,400
fiction and pop culture, kind of overestimate what the technology can do.

291
00:22:12,400 --> 00:22:18,040
I see that a lot that people kind of think that we are much further along in robotics

292
00:22:18,040 --> 00:22:22,680
and artificial intelligence development than we actually are.

293
00:22:22,680 --> 00:22:28,560
And I, I think that's also a problem of science communications and the media in general.

294
00:22:28,560 --> 00:22:33,880
So it's definitely an issue that needs to be addressed.

295
00:22:33,880 --> 00:22:42,360
Yeah, and this comes up in some of my conversations with folks on the business or enterprise side,

296
00:22:42,360 --> 00:22:51,080
just in thinking about how to address kind of statistical literacy and numeracy within

297
00:22:51,080 --> 00:22:58,040
organizations and trying to raise the level of understanding of, you know, as we're adopting

298
00:22:58,040 --> 00:23:01,920
more and more of these ML and AI systems within organizations, you know, what it means

299
00:23:01,920 --> 00:23:07,200
that these systems are, you know, based on statistical models and pattern matching in

300
00:23:07,200 --> 00:23:13,640
our probabilistic, and many organizations are spending a lot of energy trying to come

301
00:23:13,640 --> 00:23:20,040
up with new and innovative ways to educate folks that don't kind of think about systems

302
00:23:20,040 --> 00:23:21,560
in this way.

303
00:23:21,560 --> 00:23:26,480
It kind of sounds like we're going to need to do this on a broader societal scale as these

304
00:23:26,480 --> 00:23:30,680
systems get more and more integrated into the way things work.

305
00:23:30,680 --> 00:23:32,600
Oh, yeah, for sure.

306
00:23:32,600 --> 00:23:37,400
I mean, there's been, so there's this group called the Personal Robots Group at the

307
00:23:37,400 --> 00:23:44,400
Media Lab that I do a lot of work with and one of their students, Blakely Payne, is working

308
00:23:44,400 --> 00:23:48,600
on an AI ethics curriculum for middle schoolers.

309
00:23:48,600 --> 00:23:53,480
And it's really great like she's going to middle schools and she's doing these exercises

310
00:23:53,480 --> 00:23:58,320
with the kids where, for example, like they look at, you know, a YouTube recommendation

311
00:23:58,320 --> 00:24:03,360
algorithm and then they have to, you know, put themselves in the place of the different stakeholders

312
00:24:03,360 --> 00:24:09,280
in the system and not only understand how it works, but who it works for.

313
00:24:09,280 --> 00:24:13,320
And so they really learn to kind of question what's going on and think critically about

314
00:24:13,320 --> 00:24:19,760
it in a really good way, but they, you know, they're getting overwhelmed with requests

315
00:24:19,760 --> 00:24:27,600
from people like all over the place, you know, companies, you know, everyone is like,

316
00:24:27,600 --> 00:24:32,840
how do we adapt this to use in our organization because we need this too?

317
00:24:32,840 --> 00:24:37,720
Like this isn't just for middle schoolers, like everyone needs this education right now.

318
00:24:37,720 --> 00:24:41,400
And so they're trying to scale as fast as they can, but it is, you know, there's a massive

319
00:24:41,400 --> 00:24:45,960
amount of interest in this and I think it's, it's very important and people are starting

320
00:24:45,960 --> 00:24:48,160
to realize that.

321
00:24:48,160 --> 00:24:49,160
Interesting.

322
00:24:49,160 --> 00:24:56,600
So some of your work, maybe going back to the empathy conversation relates to the different

323
00:24:56,600 --> 00:25:03,640
ways that humans anthropomorphize robots and the different implications of that.

324
00:25:03,640 --> 00:25:05,800
And there's some policy implications that you've looked at.

325
00:25:05,800 --> 00:25:08,040
Can you talk a little bit about that sphere?

326
00:25:08,040 --> 00:25:09,040
Yeah.

327
00:25:09,040 --> 00:25:14,200
So my main interest is the anthropomorphism of robots.

328
00:25:14,200 --> 00:25:19,000
So people, you know, projecting life like qualities onto the robots.

329
00:25:19,000 --> 00:25:26,760
And there are some pundits in technology ethics who claim that this is a bad thing and that

330
00:25:26,760 --> 00:25:28,800
we need to discourage it.

331
00:25:28,800 --> 00:25:37,440
And I, I, I see some concerns that we might want to have about the fact that people treat

332
00:25:37,440 --> 00:25:38,440
robots like their lives.

333
00:25:38,440 --> 00:25:46,040
So for example, if, if, you know, the persuasive design folks get their hands on robots and,

334
00:25:46,040 --> 00:25:52,320
you know, start to try to manipulate people for, you know, corporate interest and try to

335
00:25:52,320 --> 00:25:56,640
get them to, you know, buy products and services or reveal more personal data than they would

336
00:25:56,640 --> 00:26:01,280
ever willingly enter into a database through, you know, interacting with, you know, a social

337
00:26:01,280 --> 00:26:02,800
robot in the home, for example.

338
00:26:02,800 --> 00:26:07,720
Then I think that's maybe a consumer protection issue that we might want to think about a little

339
00:26:07,720 --> 00:26:08,720
bit.

340
00:26:08,720 --> 00:26:12,960
You know, there's a lot of privacy concerns, manipulation concerns, but I don't think

341
00:26:12,960 --> 00:26:18,840
that it's inherently a bad thing because there are so many great use cases for this as well.

342
00:26:18,840 --> 00:26:24,440
And so those are, I mean, those are some questions that are popping up, but I'm also interested

343
00:26:24,440 --> 00:26:29,640
in, you know, this question that we touched on earlier of, you know, is it a bad thing

344
00:26:29,640 --> 00:26:34,400
for people to treat life like objects in a violent way?

345
00:26:34,400 --> 00:26:40,920
We don't have the answer to that, but it's already coming up in some policy questions.

346
00:26:40,920 --> 00:26:47,960
So for example, there's some discussion of whether sex robots are something that should

347
00:26:47,960 --> 00:26:52,800
be banned because they encourage certain behaviors in people or whether there's something

348
00:26:52,800 --> 00:26:58,920
that should be encouraged because they, you know, are an outlet for behaviors that we,

349
00:26:58,920 --> 00:27:02,240
you know, don't want to be levied against real people.

350
00:27:02,240 --> 00:27:08,400
So it's, there's, there are the, you know, policy questions being thrown around without

351
00:27:08,400 --> 00:27:12,960
any actual, you know, evidence behind them and a lot of moral panic, especially in the

352
00:27:12,960 --> 00:27:19,520
area of sex technology that, you know, are already becoming very relevant.

353
00:27:19,520 --> 00:27:26,520
And I, I do think that if we found evidence that, you know, it's somehow desensitizing

354
00:27:26,520 --> 00:27:33,160
to people to be violent towards robots that we might need to have some legislation that

355
00:27:33,160 --> 00:27:39,560
says, you know, you can't torture a certain kind of robot in the same way that, you know,

356
00:27:39,560 --> 00:27:43,480
we don't allow torture of animals, even though we still allow people to, you know, kill

357
00:27:43,480 --> 00:27:44,480
animals and do it.

358
00:27:44,480 --> 00:27:49,280
But you can't, you know, set a kitten on fire and throw it, you know, in a bag.

359
00:27:49,280 --> 00:27:53,560
And, you know, there are certain things that would, that just we're not comfortable with.

360
00:27:53,560 --> 00:27:54,560
And this might be a problem.

361
00:27:54,560 --> 00:27:58,000
But there are totally different reasons behind that, right?

362
00:27:58,000 --> 00:28:04,440
But the kitten, it's because we, the kitten is a creature that, you know, experiences

363
00:28:04,440 --> 00:28:09,480
pain and all of that kind of thing with the robot, the rationale would need to be much

364
00:28:09,480 --> 00:28:14,720
more about us than, you know, the target of our violence.

365
00:28:14,720 --> 00:28:15,720
Absolutely.

366
00:28:15,720 --> 00:28:18,560
And that's why I think it should be purely evidence-based.

367
00:28:18,560 --> 00:28:24,080
Like, if we have evidence that is desensitizing to people, then, you know, maybe we need

368
00:28:24,080 --> 00:28:25,800
a solution for it.

369
00:28:25,800 --> 00:28:31,000
But that said, I am also not convinced that when it comes to animals that we truly, truly

370
00:28:31,000 --> 00:28:37,480
care about their pain, because there are certain animals that we're perfectly willing to torture

371
00:28:37,480 --> 00:28:38,680
for our own benefit.

372
00:28:38,680 --> 00:28:43,240
Like it's obvious that, you know, animals like chickens feel pain, but, you know, we keep

373
00:28:43,240 --> 00:28:46,920
them in these, you know, cooped up spaces.

374
00:28:46,920 --> 00:28:50,680
We chop off their beak so that they can't peck each other's eyes out because they're

375
00:28:50,680 --> 00:28:52,320
going crazy in these cooped up spaces.

376
00:28:52,320 --> 00:28:57,400
We torture plenty of animals for our own benefit, even though we know that, you know, there's

377
00:28:57,400 --> 00:29:02,480
no biological difference between a horse and a cow that would justify us eating one of

378
00:29:02,480 --> 00:29:04,000
them in America and not the other.

379
00:29:04,000 --> 00:29:08,400
And yet, people are appalled when we suggest eating horse meat here.

380
00:29:08,400 --> 00:29:13,640
I think there are a lot of reasons that we protect animals that are actually about us.

381
00:29:13,640 --> 00:29:18,040
And as much as we don't like to hear that, I think that the way that we're interacting

382
00:29:18,040 --> 00:29:21,320
with robots is kind of making that very apparent.

383
00:29:21,320 --> 00:29:22,320
Interesting.

384
00:29:22,320 --> 00:29:31,720
And this part of the conversation reminds me of this device that I saw at CES earlier

385
00:29:31,720 --> 00:29:39,000
this year, Bot Boxer, it's like this robotic boxing training thing.

386
00:29:39,000 --> 00:29:46,800
And, you know, the device as presented was basically a punching bag that kind of, you

387
00:29:46,800 --> 00:29:50,720
know, had some behaviors in it to evade you and that kind of thing.

388
00:29:50,720 --> 00:29:56,080
But it makes me wonder, like, if the thing was, you know, one of these humanoid punching

389
00:29:56,080 --> 00:30:04,560
bags and made painful sounds, like what the implications are, it needs to be this discussion.

390
00:30:04,560 --> 00:30:08,880
It creates all kinds of interesting thought experiments.

391
00:30:08,880 --> 00:30:09,880
It does.

392
00:30:09,880 --> 00:30:10,960
Oh, yeah, that is interesting.

393
00:30:10,960 --> 00:30:17,360
But also, you know, if it's not, if we're not subconsciously treating that boxing punching

394
00:30:17,360 --> 00:30:20,880
bag like a human opponent, then it's not a good training device.

395
00:30:20,880 --> 00:30:29,520
So, you know, it, to some extent, that relies on this kind of subconscious treating it like

396
00:30:29,520 --> 00:30:30,920
a person.

397
00:30:30,920 --> 00:30:34,640
And, you know, I think that, you know, there are plenty of situations where, like, I'm,

398
00:30:34,640 --> 00:30:39,560
I would never argue that, you know, because boxing exists as a sport that people who box,

399
00:30:39,560 --> 00:30:41,440
you know, might be more likely to hit other people.

400
00:30:41,440 --> 00:30:46,760
I think we're very good at compartmentalizing in that type of way.

401
00:30:46,760 --> 00:30:51,960
It is, you know, with children, it is, there are some questions.

402
00:30:51,960 --> 00:30:53,600
You know, I have a toddler myself.

403
00:30:53,600 --> 00:30:59,520
And so, I'm starting to have to deal with things like, if he pulls the robot cat's tail,

404
00:30:59,520 --> 00:31:01,600
we have a robot cat at home.

405
00:31:01,600 --> 00:31:05,480
Do I stop him from doing that because I don't want him to learn that it's okay to pull

406
00:31:05,480 --> 00:31:07,120
a real cat's tail?

407
00:31:07,120 --> 00:31:12,160
And there are a lot of stories, even with, like, the very primitive kind of voice assistance

408
00:31:12,160 --> 00:31:17,480
we have today, you know, there are stories of older kids where parents are like, you

409
00:31:17,480 --> 00:31:23,320
know, Amazon's Alexa is turning my child into an asshole because she doesn't require

410
00:31:23,320 --> 00:31:25,960
you to say, please, and thank you, you can just bark commands at her.

411
00:31:25,960 --> 00:31:29,320
And so, my kid now thinks that's okay to do to anyone.

412
00:31:29,320 --> 00:31:33,320
And there were so many complaints to Amazon that they actually released a feature that

413
00:31:33,320 --> 00:31:37,720
you can turn on where Alexa will require, please, and thank you.

414
00:31:37,720 --> 00:31:45,920
So, we don't have scientific evidence that it has an effect on kids' behavior that translates

415
00:31:45,920 --> 00:31:52,840
to, you know, how they interact with the world, but there's some anecdotal concern at

416
00:31:52,840 --> 00:31:57,960
least that we might want to be a little bit careful because interacting with these machines

417
00:31:57,960 --> 00:32:04,800
is subconsciously like interacting with, you know, a social agent, like another person.

418
00:32:04,800 --> 00:32:11,680
Yeah, that reminds me a little bit of, you know, don't let your kids play with fake cell

419
00:32:11,680 --> 00:32:18,480
phones or your animals for that matter because then they'll just destroy your real ones.

420
00:32:18,480 --> 00:32:24,360
But, you know, it seems like that, at least in that case, like there's some age in which

421
00:32:24,360 --> 00:32:29,200
you age range, in which you have to worry about that, but at some point there's a level

422
00:32:29,200 --> 00:32:34,480
of maturity in which they realize the difference between this plastic thing that kind of looks

423
00:32:34,480 --> 00:32:38,960
like a phone and the actual real phone that you get upset about if they throw around.

424
00:32:38,960 --> 00:32:39,960
Oh, sure.

425
00:32:39,960 --> 00:32:40,960
Kids are smart.

426
00:32:40,960 --> 00:32:41,960
Right, right.

427
00:32:41,960 --> 00:32:46,120
And we have a lot of kids, like a lot of the roboticists who work in the meaty lab have kids,

428
00:32:46,120 --> 00:32:51,920
and their kids come into the lab and they totally know how the robots work and that the robots

429
00:32:51,920 --> 00:32:57,960
don't feel anything and yada yada, and yet they're still treating them like friends.

430
00:32:57,960 --> 00:33:06,440
And even the roboticists will do it, like it's really funny, like, you know, um, our

431
00:33:06,440 --> 00:33:12,800
setting of the hex bugs, our participants were mostly MIT undergrad, so people who have

432
00:33:12,800 --> 00:33:16,000
a lot of tech literacy, and we still found an effect.

433
00:33:16,000 --> 00:33:21,760
And I've seen, you know, I've seen effects in myself and others who know perfectly well

434
00:33:21,760 --> 00:33:25,440
how the robots work and will still kind of treat them.

435
00:33:25,440 --> 00:33:30,480
It's very hard to not go with that instinct to treat the robot like an agent because

436
00:33:30,480 --> 00:33:37,880
we're biologically hardwired to perceive the type of movement and interaction that these

437
00:33:37,880 --> 00:33:43,880
robots have as a social interaction, and so we like immediately slip into treating the

438
00:33:43,880 --> 00:33:51,280
robot like, you know, an agent instead of an object, and even adults who are completely

439
00:33:51,280 --> 00:33:54,120
tech literate aren't immune from that effect.

440
00:33:54,120 --> 00:34:01,160
So, you know, kids are smart, like, they know, but, you know, my workshop with the adults

441
00:34:01,160 --> 00:34:05,920
who refuse to harm the robotic baby dinosaur shows that even if you know, it might not

442
00:34:05,920 --> 00:34:06,920
matter.

443
00:34:06,920 --> 00:34:07,920
Right, right, right.

444
00:34:07,920 --> 00:34:14,840
I'm curious what's the example of the most, you know, either the most, you know, human

445
00:34:14,840 --> 00:34:20,320
life or kind of empathy producing or just interesting in general, kind of robots that

446
00:34:20,320 --> 00:34:22,440
you've, you know, seen out there.

447
00:34:22,440 --> 00:34:24,680
I'm imagining it's not Sophia.

448
00:34:24,680 --> 00:34:25,680
Yeah.

449
00:34:25,680 --> 00:34:31,680
I was just going to say, yeah, I think the humanoid ones are actually not as empathy and

450
00:34:31,680 --> 00:34:39,440
gendering because there's this thing in robotic design that some people call the uncanny valley.

451
00:34:39,440 --> 00:34:46,520
I call it expectation management where if you're comparing, like, say you take a humanoid

452
00:34:46,520 --> 00:34:51,720
robot or my shitty cat robot that I have at home, like, these are things that are designed

453
00:34:51,720 --> 00:34:55,480
to try and look as closely to a human or as closely to a cat as possible.

454
00:34:55,480 --> 00:35:00,480
So when they behave, you're expecting them to behave exactly like the thing that they're

455
00:35:00,480 --> 00:35:06,160
trying to mimic and as soon as they don't do that, they make some, like, different movement

456
00:35:06,160 --> 00:35:12,000
or, you know, utterance, it breaks the illusion and you're no longer suspending your disbelief

457
00:35:12,000 --> 00:35:14,080
and it kind of disappoints your expectations.

458
00:35:14,080 --> 00:35:21,280
And so I think that the most successful social robots actually have very different shapes

459
00:35:21,280 --> 00:35:22,280
and forms.

460
00:35:22,280 --> 00:35:25,640
I think that Baby Dinosaur works because no one's ever actually interacted with a Baby

461
00:35:25,640 --> 00:35:26,640
Dinosaur before.

462
00:35:26,640 --> 00:35:31,680
So it's kind of like, you know, I'll believe that a Baby Camerasaurist moves in this way.

463
00:35:31,680 --> 00:35:35,360
You know, I'll totally imagine that.

464
00:35:35,360 --> 00:35:40,440
But even like, have you seen Gibo at all?

465
00:35:40,440 --> 00:35:41,840
Which one is Gibo?

466
00:35:41,840 --> 00:35:42,840
To Gibo.

467
00:35:42,840 --> 00:35:49,440
Gibo, unfortunately, the company no longer exists, but they had this very successful kickstart

468
00:35:49,440 --> 00:35:56,760
or Indiegogo where it's, it's just, it's one of those home assistant robots like Amazon

469
00:35:56,760 --> 00:36:00,520
Alexa, except it had, it looks a little bit like a Pixar lamp.

470
00:36:00,520 --> 00:36:07,720
It has my body and the face and has this animated like circle that just, it's very, very simple

471
00:36:07,720 --> 00:36:12,560
and very, very compelling, like the little movements that it makes, it just makes you feel

472
00:36:12,560 --> 00:36:21,240
like you're interacting with, you know, something that has a character instead of just an object.

473
00:36:21,240 --> 00:36:28,360
And those I think are, those are the most successful robots in kind of engendering people's empathy.

474
00:36:28,360 --> 00:36:36,840
Have you explored anything related to these telepresence robots?

475
00:36:36,840 --> 00:36:44,440
Like the, the kind of iPad on a Segway, things that people can use to be remotely present

476
00:36:44,440 --> 00:36:46,320
in meetings and that kind of thing?

477
00:36:46,320 --> 00:36:52,200
Yeah, so I haven't done any work on that, but I have heard some interesting stories about

478
00:36:52,200 --> 00:36:59,640
how people, you know, people will Skype in or whatever you do to the telepresence robot.

479
00:36:59,640 --> 00:37:05,080
And if someone in that physical space comes up and like picks up the robot and moves it

480
00:37:05,080 --> 00:37:10,080
because of the way, people have reported that they feel like violated, like physically

481
00:37:10,080 --> 00:37:13,080
violated, even though they're only Skyping into it.

482
00:37:13,080 --> 00:37:14,080
Wow.

483
00:37:14,080 --> 00:37:16,080
Or if somebody's standing too close to it or whatever.

484
00:37:16,080 --> 00:37:23,400
So I think there's a lot to unpack there and explore, especially as these get more robot

485
00:37:23,400 --> 00:37:27,000
likes so far they haven't, they've been kind of like just remote controlled, you know,

486
00:37:27,000 --> 00:37:32,640
iPads on wheels, but I think as they develop more autonomous capabilities, it'll be interesting

487
00:37:32,640 --> 00:37:37,960
to see, you know, the intersection of people's feelings of autonomy while they're, you

488
00:37:37,960 --> 00:37:42,560
know, quote unquote, inside the thing and how, you know, the robot is interacting with

489
00:37:42,560 --> 00:37:43,560
the world.

490
00:37:43,560 --> 00:37:47,120
So that's definitely a very interesting area, I think.

491
00:37:47,120 --> 00:37:52,520
Yeah, as you describe that, I can almost, that it becomes almost tangible for me, that

492
00:37:52,520 --> 00:37:59,160
feeling of kind of anxiety of being, you know, someone just kind of, you know, quote unquote

493
00:37:59,160 --> 00:38:07,480
man handling my virtual self also makes me think about like, one of my thing, I guess

494
00:38:07,480 --> 00:38:13,520
the zoom is kind of the example that I'm thinking of if you like, we've got one zoom

495
00:38:13,520 --> 00:38:18,080
account that we share among a few people and like it'll kick people out of a room if

496
00:38:18,080 --> 00:38:21,080
somebody comes in and like takes over it.

497
00:38:21,080 --> 00:38:28,520
And I'm thinking about like the, you know, multiple humans, like struggling to take

498
00:38:28,520 --> 00:38:33,520
control over this virtual robot and like, you know, all kinds of issues related to like

499
00:38:33,520 --> 00:38:36,280
the hierarchy of being able to use this thing.

500
00:38:36,280 --> 00:38:40,680
Yeah, I mean, I'm telling you, the intersection of psychology and how we're using technology

501
00:38:40,680 --> 00:38:43,840
is really, I think it's so fascinating.

502
00:38:43,840 --> 00:38:45,000
Yeah, yeah, it is.

503
00:38:45,000 --> 00:38:52,720
So what are the things that are kind of on top of your list of, you know, exciting things

504
00:38:52,720 --> 00:38:55,840
that are happening, really interesting work, you know, besides the stuff that you're

505
00:38:55,840 --> 00:38:59,360
doing that folks should check out?

506
00:38:59,360 --> 00:39:03,240
I'm personally really excited, this is not my own work.

507
00:39:03,240 --> 00:39:08,320
There have been some advances in the research with autistic children.

508
00:39:08,320 --> 00:39:14,200
So a long time ago, researchers and social robots and social robotics found out that autistic

509
00:39:14,200 --> 00:39:20,040
kids will respond really well to social robots and they'll engage with them more than they

510
00:39:20,040 --> 00:39:25,560
will with, you know, an adult, a teacher, a caregiver.

511
00:39:25,560 --> 00:39:30,720
But not only that, the kids, when you bring them into a room and have them interact with

512
00:39:30,720 --> 00:39:34,960
the robot, they'll also interact more with the person who's in the room with them.

513
00:39:34,960 --> 00:39:39,840
So suddenly, they'll be making more eye contact, answering more questions than they were before.

514
00:39:39,840 --> 00:39:44,980
So it's a really great kind of facilitating device and they're not quite sure how or

515
00:39:44,980 --> 00:39:45,980
why this works.

516
00:39:45,980 --> 00:39:52,760
But what's interesting going on right now is that just last year, they published the first

517
00:39:52,760 --> 00:39:57,840
long-term study where they actually put robots in the homes of children who are on the

518
00:39:57,840 --> 00:40:03,000
spectrum and did a longer-term study where the kids were interacting with the robot for

519
00:40:03,000 --> 00:40:06,240
half an hour every day together with their caregiver.

520
00:40:06,240 --> 00:40:13,560
And then after about a month, they saw like a dramatic increase in the social skills

521
00:40:13,560 --> 00:40:17,200
that they were looking to kind of encourage in the kids.

522
00:40:17,200 --> 00:40:22,480
And so like really thousands and thousands of dollars worth of therapy just from, you

523
00:40:22,480 --> 00:40:24,400
know, interacting with this robot.

524
00:40:24,400 --> 00:40:29,560
So I think there's so much promise in that area and I'm really excited to see what other

525
00:40:29,560 --> 00:40:31,000
work comes out of there.

526
00:40:31,000 --> 00:40:33,040
That's pretty fascinating.

527
00:40:33,040 --> 00:40:38,120
You said they're not sure kind of how or why any hints or indications.

528
00:40:38,120 --> 00:40:45,280
Well, I think so the leading researcher in that space, Brian Scuzzelotti, who's at Yale,

529
00:40:45,280 --> 00:40:51,520
he thinks that it's because the kids view the robot as a social actor.

530
00:40:51,520 --> 00:40:56,360
So something that they engage with socially, but it doesn't come with the baggage of another

531
00:40:56,360 --> 00:41:01,160
child or another adult, you know, because they also understand that it's a robot.

532
00:41:01,160 --> 00:41:07,120
So that's their best guess for why this is having a pretty strong effect on these kids.

533
00:41:07,120 --> 00:41:10,960
But it's pretty cool because it's something it's a tool that we haven't, you know, had previously

534
00:41:10,960 --> 00:41:12,360
and now we do.

535
00:41:12,360 --> 00:41:13,360
Cool.

536
00:41:13,360 --> 00:41:17,440
Anything else that we should check out?

537
00:41:17,440 --> 00:41:18,440
I don't think so.

538
00:41:18,440 --> 00:41:20,640
You should check out Blakely's AI ethics curriculum.

539
00:41:20,640 --> 00:41:21,640
Okay.

540
00:41:21,640 --> 00:41:22,640
Yeah.

541
00:41:22,640 --> 00:41:23,640
Yeah.

542
00:41:23,640 --> 00:41:24,640
We'll definitely link to that.

543
00:41:24,640 --> 00:41:25,640
Very cool work.

544
00:41:25,640 --> 00:41:34,680
And then you mentioned earlier, the possible opportunities to kind of abuse the human robot

545
00:41:34,680 --> 00:41:39,880
interaction and persuasive design, like have you seen anything there that kind of, you

546
00:41:39,880 --> 00:41:46,600
know, put your, put your radar up or that folks should be aware of as kind of a negative

547
00:41:46,600 --> 00:41:52,400
example of how this kind of technology or the interactions are being taken advantage

548
00:41:52,400 --> 00:41:53,400
of?

549
00:41:53,400 --> 00:41:56,560
So I haven't seen anything.

550
00:41:56,560 --> 00:42:01,440
I, you know, I don't think that social robots right now are, you know, pervasive enough

551
00:42:01,440 --> 00:42:05,800
for this to be a problem, but I, I see it on the horizon because, you know, if you think

552
00:42:05,800 --> 00:42:10,680
about it, you know, a sexual robot that has in-app purchases, you know, is that okay or

553
00:42:10,680 --> 00:42:16,040
is that too manipulative or I can think of a million examples or even just so Sony came

554
00:42:16,040 --> 00:42:21,360
out with a new robot dog, the Ibo, which I really want when I'm probably going to get one.

555
00:42:21,360 --> 00:42:26,600
But they're not only are they very expensive, but they require a monthly subscription to

556
00:42:26,600 --> 00:42:27,880
the cloud services.

557
00:42:27,880 --> 00:42:28,880
Oh, wow.

558
00:42:28,880 --> 00:42:32,200
I don't know what functionality you lose if you stop paying for that, but it's kind of

559
00:42:32,200 --> 00:42:37,560
interesting because we know from the older Ibo is that people really treated these robots

560
00:42:37,560 --> 00:42:39,920
like a pet and like they're a part of the family.

561
00:42:39,920 --> 00:42:44,120
And so that does seem like a little bit of maybe manipulating an emotional connection

562
00:42:44,120 --> 00:42:48,360
if you're going to charge a monthly fee to keep this robot going, you know what I mean?

563
00:42:48,360 --> 00:42:55,000
Yeah, you can imagine the, the, uh, uh, loss prevention emails, you know, don't let your

564
00:42:55,000 --> 00:42:59,920
Ibo die kind of, oh my gosh, yes, they should hire you as a consultant.

565
00:42:59,920 --> 00:43:06,080
Oh, yeah, um, yeah, that doesn't, that seems to be on the other side of some line.

566
00:43:06,080 --> 00:43:12,920
Uh, well, Kate, thanks so much for taking the time to chat really, really fascinating conversation

567
00:43:12,920 --> 00:43:18,280
and, uh, I'm looking forward to following along with your work.

568
00:43:18,280 --> 00:43:19,800
Thanks so much.

569
00:43:19,800 --> 00:43:26,320
All right, everyone, that's our show for today.

570
00:43:26,320 --> 00:43:32,320
For more information on today's show, visit twomolai.com slash shows.

571
00:43:32,320 --> 00:43:38,160
Make sure you head over to twomolcan.com to learn more about the Twomolcan AI Platforms

572
00:43:38,160 --> 00:43:39,640
Conference.

573
00:43:39,640 --> 00:43:52,600
As always, thanks so much for listening and catch you next time.

