WEBVTT

00:00.000 --> 00:16.320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

00:16.320 --> 00:21.480
people, doing interesting things in machine learning and artificial intelligence.

00:21.480 --> 00:31.800
I'm your host Sam Charrington.

00:31.800 --> 00:36.900
Today we're joined by Tom Sumowski, data scientist at Urban, the parent company of Urban

00:36.900 --> 00:41.440
Outfitters, Anthropology and other consumer fashion brands.

00:41.440 --> 00:45.960
Tom and I caught up recently to discuss his project Exploring Custom Vision Services

00:45.960 --> 00:49.440
for Automated Fashion Product Attribution.

00:49.440 --> 00:53.320
We started our discussion with the definition of the product attribution problem in retail

00:53.320 --> 00:57.640
and fashion and the challenges it offers to data scientists.

00:57.640 --> 01:01.680
We then looked at the approach Tom and his team took to building custom attribution models

01:01.680 --> 01:06.720
and the results of their evaluation of various custom vision APIs for this purpose, with

01:06.720 --> 01:12.560
a focus on the Roblox and Lessons he and his team encountered along the way.

01:12.560 --> 01:16.120
Today's show is sponsored by our friends at Pegasystems.

01:16.120 --> 01:20.440
During a world, the company's annual digital transformation conference, which will be held

01:20.440 --> 01:26.440
at the MGM Grand in Las Vegas from June 2nd to 5th, is just a couple of months away now.

01:26.440 --> 01:31.280
I'll be attending the event as I did last year and will once again be presenting.

01:31.280 --> 01:36.000
In addition to hearing from me, the event is a great opportunity for you to learn how AI

01:36.000 --> 01:41.080
is being applied to the customer experience at real Pegasystems.

01:41.080 --> 01:47.280
As a Twimble listener, you can use promo code Twimble19 for $200 off of your registration.

01:47.280 --> 01:49.480
Again, that code is Twimble19.

01:49.480 --> 01:51.000
It's as easy as that.

01:51.000 --> 01:52.600
Hope to see you there.

01:52.600 --> 01:54.920
And now on to the show.

01:54.920 --> 01:58.640
All right, everyone.

01:58.640 --> 02:00.760
I am on the line with Tom Sumowski.

02:00.760 --> 02:06.560
Tom is a data scientist with Urban, the parent company of Urban Outfitters, Anthropology

02:06.560 --> 02:08.040
and other fashion brands.

02:08.040 --> 02:10.480
Tom, welcome to this week in Machine Learning and AI.

02:10.480 --> 02:11.720
Thanks for having me, Sam.

02:11.720 --> 02:18.040
So Tom was recently part of the team that worked on some work around product attribution.

02:18.040 --> 02:23.000
And just yesterday, their blog post about this project went live following them, open

02:23.000 --> 02:25.920
sourcing some code.

02:25.920 --> 02:27.160
And we'll talk all about that.

02:27.160 --> 02:31.760
But before we do, Tom, why don't you tell us a little bit about your background and

02:31.760 --> 02:34.760
how you got started working in data science and machine learning?

02:34.760 --> 02:35.760
Sure.

02:35.760 --> 02:41.040
By academic background, I have a bachelor's and master's in electrical engineering.

02:41.040 --> 02:46.080
From there, I went to Lockheed Martin, which is a defense contractor, working in their

02:46.080 --> 02:49.160
corporate research lab for many years.

02:49.160 --> 02:54.200
And in there, the focus was primarily on wireless systems signal processing.

02:54.200 --> 03:02.360
So I started off primarily with embedded systems or embedded platform development for the

03:02.360 --> 03:04.600
area of electronic warfare.

03:04.600 --> 03:08.120
So that involves anything to do with telecommunication.

03:08.120 --> 03:12.680
So these days, everything sort of talks to each other, phones, radios, you name it.

03:12.680 --> 03:19.560
Our goal was to sort of understand that communication spectrum and inferred activities that are

03:19.560 --> 03:20.560
going on.

03:20.560 --> 03:27.320
So started off in kind of building signal geolocation applications, kind of working at a very low level.

03:27.320 --> 03:35.200
Later on, the shift moved towards a theme of cognitive in the 2012 timeframe for advanced

03:35.200 --> 03:37.000
technologies in that space.

03:37.000 --> 03:42.280
So what they sort of defense meant in terms of cognitive was adding intelligence to those

03:42.280 --> 03:48.200
applications that typically were very sort of rule based and driven based on strict policies.

03:48.200 --> 03:53.800
So in other words, they were looking to apply machine learning sort of in the earlier era

03:53.800 --> 03:54.800
of that space.

03:54.800 --> 03:57.440
So this is sort of pre-deep learning.

03:57.440 --> 04:06.320
So on the job, I more or less learned how to apply machine learning for mainly pattern

04:06.320 --> 04:13.160
recognition of different signal patterns and making decisions on how you should act based

04:13.160 --> 04:14.800
on that information.

04:14.800 --> 04:20.880
So for that, it was a lot of experimentation, a lot of learning with online courses like

04:20.880 --> 04:21.880
Coursera.

04:21.880 --> 04:28.120
And since we are a research lab, it was building essentially prototypes for proof of concepts

04:28.120 --> 04:30.480
to the military.

04:30.480 --> 04:34.920
And the goal there was to get the users of these systems.

04:34.920 --> 04:38.920
So what they're called electronic warfare officers, the people that are running these

04:38.920 --> 04:44.880
systems in operations to essentially trust the prototypes enough that they can field

04:44.880 --> 04:46.120
and deploy them.

04:46.120 --> 04:53.320
So the focus across that entire time for these types of systems were trust, reliability,

04:53.320 --> 04:59.600
stability, and making sure that it's providing a lot of value to that warfare officer and

04:59.600 --> 05:02.360
not sort of inundate in them with too much information.

05:02.360 --> 05:04.760
So that was sort of the machine learning side.

05:04.760 --> 05:11.360
More recently, I guess since the 2013-2014 time frame, that's when we as an organization

05:11.360 --> 05:17.200
started getting interested in deep learning with some of the really early performance enhancements

05:17.200 --> 05:18.520
that were shown from that.

05:18.520 --> 05:24.520
So that's where I kind of dove pretty deep into trying to apply deep learning to the spectrum.

05:24.520 --> 05:29.240
So when I say spectrum, going back to communications, it's a signal.

05:29.240 --> 05:36.440
So we basically turn what we hear, say over the error in the cell phone signals into what's

05:36.440 --> 05:43.160
called a spectrogram and analyze that spectrogram and try and identify patterns for how they're

05:43.160 --> 05:48.400
behaving, how they're adapting, and kind of isolate different communication patterns.

05:48.400 --> 05:51.920
So that was pretty much my time at Lockheed Martin.

05:51.920 --> 05:57.760
About a year ago, actually for several years, I was kind of following the trends in machine

05:57.760 --> 06:00.120
learning and deep learning.

06:00.120 --> 06:06.680
And more recently, I guess what's called data science, following podcasts like this, blogs,

06:06.680 --> 06:07.680
etc.

06:07.680 --> 06:11.480
And really got an interest in how some of this machine learning technology was being

06:11.480 --> 06:15.600
fielded or deployed in other industries.

06:15.600 --> 06:18.400
So I was working more in a prototype space.

06:18.400 --> 06:21.520
We did have to build actual products and test them.

06:21.520 --> 06:26.400
But it wasn't really sort of at that level, say scaling out to hundreds of thousands of

06:26.400 --> 06:33.040
users or surfacing it across a different, unique customer base.

06:33.040 --> 06:36.440
So I started getting more of an interest in how it was being used in different industries

06:36.440 --> 06:40.640
and through a colleague, they connected me with Urban Outfitters, which recently stood

06:40.640 --> 06:44.840
up a data science and advanced analytics organization.

06:44.840 --> 06:47.160
And I've been there for about a year.

06:47.160 --> 06:54.160
Well that strikes me as a pretty significant shift from a defense contractor to kind

06:54.160 --> 06:55.400
of a hit retailer.

06:55.400 --> 06:56.400
Yeah, yeah.

06:56.400 --> 07:02.480
Well, I mean, as a natural transition to move from working with electronic warfare officers

07:02.480 --> 07:09.680
to trying to sell more dresses or match customers with the best dress that they believe that

07:09.680 --> 07:11.200
they should be wearing.

07:11.200 --> 07:17.440
Yeah, it does sound strange, but what's really funny is when I kind of started with the

07:17.440 --> 07:24.240
job and kind of really understood the landscape, the underlying technologies that are being

07:24.240 --> 07:30.400
used for both are very similar when it comes down to the approaches or the math or the deep

07:30.400 --> 07:32.640
learning models, machine learning models.

07:32.640 --> 07:34.240
It's just using it in a different way.

07:34.240 --> 07:39.240
And I kind of landed on that earlier on in my career when my originally my focus was

07:39.240 --> 07:45.520
on strict sort of wireless system signal processing and moving into more machine learning and

07:45.520 --> 07:48.600
researching some of the math behind there.

07:48.600 --> 07:54.480
Because when you kind of have to decode a message over the air coming from your phone that has

07:54.480 --> 07:58.800
an estimation that's going on in the background and the underlying math that's done there is

07:58.800 --> 08:02.360
very similar to what may be done in various machine learning domains.

08:02.360 --> 08:05.880
So there's sort of a lot of overlap when you get down to that level.

08:05.880 --> 08:14.200
And similarly for analyzing some of the spectrograms or the spectrum at Lockheed Martin, you can

08:14.200 --> 08:18.600
look at that almost as an image and you're doing image processing on it similar to how you

08:18.600 --> 08:25.560
would take a picture of a model and address and analyze the characteristics of that dress.

08:25.560 --> 08:31.240
So there's sort of very similar space, definitely a completely different domain and they both

08:31.240 --> 08:34.440
sort of have unique challenges.

08:34.440 --> 08:37.800
My background is electrical engineering as well.

08:37.800 --> 08:44.560
My graduate work was on the networking side like stochastic modeling of microcells and things

08:44.560 --> 08:45.560
like that.

08:45.560 --> 08:51.800
So I'm very, very curious about some of the work that you did in your past life and looking

08:51.800 --> 08:57.400
at applying deep learning to the frequency domain and things like that.

08:57.400 --> 09:00.040
But we're not going to talk about that now.

09:00.040 --> 09:04.760
We're going to talk about selling dresses.

09:04.760 --> 09:12.520
So you put together a couple of our two part blog series on this project in which you

09:12.520 --> 09:22.560
are using deep learning and in particular computer vision and in particular custom vision

09:22.560 --> 09:28.920
APIs to do what you called automated fashion product attribution.

09:28.920 --> 09:30.440
So what's the business problem there?

09:30.440 --> 09:36.800
What is fashion, what is product attribution and why is it important for urban outfitters?

09:36.800 --> 09:37.800
Yeah, sure.

09:37.800 --> 09:44.080
So in terms of product attribution, we'll start with like take a picture of a dress on

09:44.080 --> 09:46.960
any sort of e-commerce website.

09:46.960 --> 09:50.960
On that dress there are certain attributes about that particular product.

09:50.960 --> 09:57.840
So for dresses that may be characteristics such as the sleeve length, it could be the

09:57.840 --> 09:58.840
neckline.

09:58.840 --> 10:04.520
So is it a deep V, is it a crew neck, the color, the pattern, the length of the dress, how

10:04.520 --> 10:10.880
far it kind of goes down the leg, the fabric composition, the list sort of goes on there.

10:10.880 --> 10:19.080
And those attributes sort of provide a textual metadata or description about the essence

10:19.080 --> 10:20.760
of that product.

10:20.760 --> 10:28.120
Traditionally, the descriptions may be fairly short based on limitations of being able

10:28.120 --> 10:34.000
to code in those attributes if you're going to be doing thousands of them a week or thousands

10:34.000 --> 10:35.160
of them a month.

10:35.160 --> 10:40.280
So what we were interested in is are there ways we can sort of one automate the process

10:40.280 --> 10:45.520
of attributing our products the way we currently attribute them.

10:45.520 --> 10:49.880
And then two, are there ways that we can sort of augment the existing attributes set

10:49.880 --> 10:57.800
that our merchants, buyers and web product team use today to kind of enhance the description

10:57.800 --> 10:58.800
of those products.

10:58.800 --> 11:04.520
Because the products themselves in terms of representation, you have the image itself,

11:04.520 --> 11:12.000
you have the attributes, you have the copy or the textual description and the title.

11:12.000 --> 11:15.880
So all of those are sort of good indicators of what that particular product is and how

11:15.880 --> 11:18.360
it can be categorized.

11:18.360 --> 11:22.280
The attributes themselves are used in various different ways across the business and this

11:22.280 --> 11:27.120
is probably common across a lot of the e-commerce domain.

11:27.120 --> 11:32.600
So one way is it's used for navigation filters for the customer online when they're shopping.

11:32.600 --> 11:36.800
So how do you filter on obviously things like size and color but also some of those other

11:36.800 --> 11:38.400
attributes I described earlier.

11:38.400 --> 11:44.000
So if somebody is shopping for a cocktail party that may be different than how they shop

11:44.000 --> 11:50.200
for business attire and it'd be that's helpful to have attributes or coding that kind of

11:50.200 --> 11:54.840
tailors one to the other so that you can filter your site to give them the best sort of online

11:54.840 --> 11:57.080
experience as possible.

11:57.080 --> 12:03.680
Another way is search so you can treat those attributes as keywords and if you can identify

12:03.680 --> 12:10.760
some type of correlation or distance between those keywords that's a good way of enhancing

12:10.760 --> 12:19.520
the search beyond just strict keyword look up if it can sort of identify those associations.

12:19.520 --> 12:24.880
That's more on the front side for customer facing but the business uses it, uses the attributes

12:24.880 --> 12:31.280
in other different ways on the back end so think planning and forecasting.

12:31.280 --> 12:39.320
So how do we know as a company how many floral prints, midi dresses do we want to buy for

12:39.320 --> 12:47.000
next summer or how many even at the top level how many coats versus pants versus blouses

12:47.000 --> 12:49.520
need to be purchased season by season.

12:49.520 --> 12:55.000
So that kind of goes into the planning and forecasting side of trying to identify and

12:55.000 --> 12:59.920
kind of cluster these different products based on some of these attributes and if things

12:59.920 --> 13:06.400
are either misattributed or insufficiently attributed it can heavily sway the direction

13:06.400 --> 13:14.200
that some of those forecasts or even retrospective analysis of prior performance goes.

13:14.200 --> 13:20.080
What's the history of product attribution at a place like Urban Outfit as you just kind

13:20.080 --> 13:25.200
of relying on the data that comes from the manufacturers and you haven't done much

13:25.200 --> 13:31.000
with images previously or is there kind of historical work that's been done to try to do

13:31.000 --> 13:32.000
this.

13:32.000 --> 13:33.000
Sure.

13:33.000 --> 13:36.400
So my understanding is it's sort of done in a couple different stages.

13:36.400 --> 13:42.640
The first is during what we're calling the buying process or even the planning process.

13:42.640 --> 13:47.280
So that's when a product is purchase orders put out for it.

13:47.280 --> 13:50.760
It has attributes like you said perhaps from the manufacturer.

13:50.760 --> 13:54.720
We sometimes get samples from that product that we can sort of confirm those attributes

13:54.720 --> 13:58.320
before they get coded into the system on the front side of things.

13:58.320 --> 14:01.040
Then the order goes through and several weeks may go by.

14:01.040 --> 14:02.800
We get the product in house.

14:02.800 --> 14:05.920
We decide to list that product onto the website.

14:05.920 --> 14:10.920
That's when the web team comes in and they may sort of enhance or update some of those

14:10.920 --> 14:17.280
core attributes to Taylor to the current trends, current season or the current creative elements

14:17.280 --> 14:21.040
that are on the website to showcase some of those products.

14:21.040 --> 14:27.480
So on the front end it may be very simple just sort of colors, patterns, sizes, brands,

14:27.480 --> 14:28.480
et cetera.

14:28.480 --> 14:33.520
And then when it gets closer to customer facing website side, that's when it may be

14:33.520 --> 14:38.920
enriched with some of those examples telling you earlier like black dresses that may be

14:38.920 --> 14:41.920
good for a cocktail event.

14:41.920 --> 14:45.800
Historically that has been done very, it is a manual process.

14:45.800 --> 14:51.560
And it's important that at least at some point in that chain does get reviewed by an actual

14:51.560 --> 14:59.040
person because they're sort of so important into describing that product itself.

14:59.040 --> 15:04.760
Long term, the goal is to sort of, again automate some of that, but still have somebody

15:04.760 --> 15:11.320
always in the loop to confirm or if the system identifies errors to manually kind of override

15:11.320 --> 15:13.480
those errors.

15:13.480 --> 15:20.640
So we've also investigated in different vendors that provide this capability as well.

15:20.640 --> 15:25.720
And kind of experimented on and off and to be honest, urban outfiters as a whole does

15:25.720 --> 15:28.040
work with dozens of different vendors.

15:28.040 --> 15:33.680
So at various times we may be trying out different attributes feature sets that come from

15:33.680 --> 15:37.520
different vendors and using it in different contexts.

15:37.520 --> 15:43.640
To date though, the primary force is sort of manually driven by the merchants, the buyers,

15:43.640 --> 15:45.280
and the web product team.

15:45.280 --> 15:46.280
Okay.

15:46.280 --> 15:55.040
And so faced with this challenge of trying to interpret visual images and pull out these

15:55.040 --> 16:00.720
different attributes, there are lots of ways that you could tackle this problem with machine

16:00.720 --> 16:09.080
learning, what you did was turn to some of the cloud-based APIs that are available.

16:09.080 --> 16:15.280
Can you talk a little bit about your motivation there and the various considerations?

16:15.280 --> 16:16.280
Sure.

16:16.280 --> 16:26.840
So we actually started with building experimental in-house models using primarily like Keras built

16:26.840 --> 16:33.800
on like TensorFlow, sometimes PyTorch, and kind of tweaking existing models, retraining

16:33.800 --> 16:40.320
them, fine-tuning them, or I think what's called essentially transfer learning, to I reclassify

16:40.320 --> 16:44.200
those images based on whatever labels we have.

16:44.200 --> 16:47.240
So we did play around with that a little to start.

16:47.240 --> 16:48.560
That's of course coming in.

16:48.560 --> 16:52.160
That's the first thing that I was interested in doing, very excited to get my hand on some

16:52.160 --> 16:57.480
of the data that we had in-house, because we do have a large collection of hundreds of

16:57.480 --> 17:00.840
thousands of products with some labels on them.

17:00.840 --> 17:03.240
So why not straightening data to play with?

17:03.240 --> 17:09.880
That was one nice thing moving over to this industry, and Urban Outfitters from Lockheed

17:09.880 --> 17:16.720
Martin is Lockheed Martin in a research lab, since we kind of were working on essentially

17:16.720 --> 17:20.920
prototypes and on more of the advanced side.

17:20.920 --> 17:25.480
We were usually trying to generate our data, or it was also very expensive to acquire data,

17:25.480 --> 17:29.080
particularly setting up receivers and things to collect some of those signals.

17:29.080 --> 17:34.400
So being able to kind of walk in on day one and have the ability to tap into some of

17:34.400 --> 17:38.040
the rich product imagery and descriptions was very nice.

17:38.040 --> 17:43.480
At the same time, that was also a unique engineering exercise for me, like having to go through

17:43.480 --> 17:49.200
that amount of data, and sort of in an efficient manner to kind of get it into a point where

17:49.200 --> 17:54.560
it can train, as with many machine learning or deep learning applications, like 90% of

17:54.560 --> 17:55.880
the work.

17:55.880 --> 17:58.920
And that's where some of this kind of came in.

17:58.920 --> 18:05.480
We started seeing some of these machine auto-ML type of solutions come into market.

18:05.480 --> 18:09.600
We actually worked with Google on their alpha release of their auto-ML, so that was our

18:09.600 --> 18:16.480
first entrance into it, since they're sort of we interact with them pretty often.

18:16.480 --> 18:22.000
And from my perspective, or I should say our data science teams perspective, our team

18:22.000 --> 18:23.000
is fairly small.

18:23.000 --> 18:29.120
It's only a couple of people right now in the core data science group, and maybe a dozen

18:29.120 --> 18:35.160
total across data science, advanced analytics in the organization we work.

18:35.160 --> 18:40.400
So we have a lot of projects that we're interested in doing, and really it comes down to what's

18:40.400 --> 18:43.440
the most effective use of our time.

18:43.440 --> 18:49.000
And I was sort of attracted to at least trying out some of these auto-ML solutions to see

18:49.000 --> 18:54.240
how far can we get with sort of this automated solution?

18:54.240 --> 18:59.200
And is it good enough to use in production and satisfy the majority of our needs?

18:59.200 --> 19:05.280
It may not be 100% perfect, it may not beat the state of the art, but does it move the

19:05.280 --> 19:10.080
needle enough to warrant sort of going to a managed service versus building it in

19:10.080 --> 19:15.120
house, and that allows to focus on efforts that say use those attributes rather than having

19:15.120 --> 19:18.080
to generate those attributes, if that makes sense.

19:18.080 --> 19:24.840
When you were experimenting with the building this in-house with Keras and TensorFlow and

19:24.840 --> 19:35.320
the like, did you get far enough that as you started to explore the hosted services,

19:35.320 --> 19:42.520
you had something to compare to or did you run into, were you able to build in-house models

19:42.520 --> 19:45.240
to perform the tasks that you were trying to perform?

19:45.240 --> 19:50.640
Yeah, I'd say we actually started building the in-house models in the initiative kind

19:50.640 --> 19:56.440
of started in parallel with trying some of the managed services as well as in-house models.

19:56.440 --> 20:01.440
And in terms of like what a model like this looks like, the inputs would be the image

20:01.440 --> 20:07.680
and which in this case, the image for urban outfitters, if you go on one of our websites,

20:07.680 --> 20:13.080
is a model wearing a product, sometimes with a clear colored background, sometimes with

20:13.080 --> 20:16.920
a very rich atmospheric background.

20:16.920 --> 20:26.000
So some of our photography we shoot on site with, say, photos of some of the models wearing

20:26.000 --> 20:30.520
the products in a city or urban environment or in a desert environment.

20:30.520 --> 20:34.840
So there's some of that scenery is in there in the background as well.

20:34.840 --> 20:39.880
So we were just trying feeding the image in raw into one of these models and then the

20:39.880 --> 20:43.720
output would be to start a single attribute for that model.

20:43.720 --> 20:49.360
And that would be, say, a sleeve length and that sleeve length would have various values

20:49.360 --> 20:50.360
that you're trying to predict.

20:50.360 --> 20:51.360
Is it sleeveless?

20:51.360 --> 20:52.360
Is it short-sleeved?

20:52.360 --> 20:53.360
Is it three-quarters?

20:53.360 --> 20:56.560
Is it long-sleeved?

20:56.560 --> 20:59.400
From there we kind of built out a bank of those models.

20:59.400 --> 21:03.720
So one that would say do sleeve length, one that does neckline, one that does the dress

21:03.720 --> 21:07.240
length, one that does dress color, et cetera.

21:07.240 --> 21:12.000
And what we realized is when we're building out those models, both sort of in-house and

21:12.000 --> 21:18.320
with some of these services is that it really, that some attributes are really easy to identify

21:18.320 --> 21:20.000
like sleeve length, maybe one.

21:20.000 --> 21:26.480
Others are far more subtle, like the occasion of a dress, like, in my occasion we mean

21:26.480 --> 21:30.320
under what occasion would you want to wear a dress like this.

21:30.320 --> 21:34.360
We've also tried different types of products, so like tops and bottoms and doing the same

21:34.360 --> 21:36.120
thing with those attributes.

21:36.120 --> 21:42.280
So the challenge, I'd say the challenge across the two was sort of the same in that some

21:42.280 --> 21:48.400
attributes were far more challenging to discern the individual values between the others.

21:48.400 --> 21:56.440
And another challenge we ran into with these images is the complexity of the images required

21:56.440 --> 22:02.000
to require sometimes a level of segmentation in order to get the best results.

22:02.000 --> 22:09.600
So if we had just photo flats meaning it's just the product laid on a flat white background,

22:09.600 --> 22:14.200
then that takes out any background, it takes out any model, and it takes out any distortions

22:14.200 --> 22:17.120
of the product based on the models pose.

22:17.120 --> 22:21.000
But most of the time actually all the time we have these more creative images, which are

22:21.000 --> 22:25.920
great for providing context to the customer and how they can sort of wear this and under

22:25.920 --> 22:32.680
what occasions, but the challenge from the data side is how do you, if we're interested

22:32.680 --> 22:37.360
in just the essence of the product itself, it's hard to isolate the characteristics of

22:37.360 --> 22:38.760
the product from the background.

22:38.760 --> 22:43.760
So that's one thing that we realized really early on in both the in-house models and these

22:43.760 --> 22:48.280
managed services is the importance of isolating the product.

22:48.280 --> 22:53.640
And the same goes with if you have multiple products in a single photo.

22:53.640 --> 23:00.320
So if somebody's wearing a blouse, jeans, a hat, and some shoes, then the other products

23:00.320 --> 23:05.880
if you don't sort of isolate it can sort of mix in and bias the models that you're creating.

23:05.880 --> 23:08.200
That was probably the biggest ones that we ran into.

23:08.200 --> 23:11.920
So you started working with these custom vision services.

23:11.920 --> 23:20.040
Maybe walk us through kind of your methodology for trying out these various services.

23:20.040 --> 23:24.240
How did you even, like you didn't, did you use all of the ones that were available at

23:24.240 --> 23:26.080
the time or did you create a short list?

23:26.080 --> 23:29.640
How did you even choose which ones to look at?

23:29.640 --> 23:34.360
Yeah, we, so we isolated down, we wanted to narrow down to about five or six to keep

23:34.360 --> 23:36.000
this scope reasonable.

23:36.000 --> 23:42.720
And from there we just chose the ones that sort of showed up the most and started with

23:42.720 --> 23:44.200
some of the heavy hitters.

23:44.200 --> 23:53.040
So think like Google IBM, so to summarize a list that we tried, there was Google, AutoML,

23:53.040 --> 23:54.600
that's currently in beta.

23:54.600 --> 24:01.280
There's Salesforce Einstein Vision, which is driven by the Salesforce IBM Watson, had a

24:01.280 --> 24:03.040
vision solution.

24:03.040 --> 24:08.920
There was a company called Clarify, which is a smaller business, but they were started

24:08.920 --> 24:15.000
by a fairly prominent figure in deep learning and have been growing since 2013 issue.

24:15.000 --> 24:19.640
I want to say, yeah, that's Matt Zeeler who's been interviewed on the show before.

24:19.640 --> 24:21.040
Yes.

24:21.040 --> 24:25.400
And then there was also Microsoft Azure has their custom vision model.

24:25.400 --> 24:29.360
So we kind of stuck with the bigger ones that we found and then since Clarify had some

24:29.360 --> 24:33.760
income and see and experience in there, we added that as well.

24:33.760 --> 24:40.000
More recently after the conference presentation that I gave at rework deep learning London

24:40.000 --> 24:50.000
in September, we signed up for the Fast AI version three course and that started in October

24:50.000 --> 24:51.240
and November timeframe.

24:51.240 --> 24:56.360
And we were just curious how well would these models work just applying less than one, less

24:56.360 --> 25:02.360
than two, a Fast AI and how they would sort of compare to some of those.

25:02.360 --> 25:05.840
So that one came in just because that had a lot of momentum.

25:05.840 --> 25:09.480
They kind of had some close to out of box capabilities.

25:09.480 --> 25:15.160
But we were looking for ones that seemed fairly prominent, had various capabilities such

25:15.160 --> 25:22.760
as batch uploads, being able to automatically serve a model afterwards in some scalable

25:22.760 --> 25:27.440
manner if we wanted to run this in production.

25:27.440 --> 25:32.120
Something that had reasonable APIs or interfaces so that we can query it essentially as a

25:32.120 --> 25:34.560
service and some level of support.

25:34.560 --> 25:39.840
So there's certainly a lot of different people, a lot of players in this game.

25:39.840 --> 25:42.960
But those are just sort of the ones that we landed that we thought were representative

25:42.960 --> 25:45.680
of the current landscape.

25:45.680 --> 25:52.080
As you explore these different services, you know, so what degree did you find that the

25:52.080 --> 25:58.160
service itself is commoditized, meaning there's not a lot of difference in performance, there's

25:58.160 --> 26:07.440
not a lot of difference in features, or are there, you know, fairly significant differences

26:07.440 --> 26:12.560
from one to the next, again, performance features, usability, I think is something you

26:12.560 --> 26:13.800
looked at as well.

26:13.800 --> 26:18.400
Yeah, we broke it up in the two high level dimensions.

26:18.400 --> 26:23.280
One was performance, which includes your standard metrics like classification accuracy,

26:23.280 --> 26:29.200
AUC, and then usability, which includes all of the other factors that one may consider

26:29.200 --> 26:34.240
when using this, not from an academic perspective, but more from a business perspective.

26:34.240 --> 26:41.440
So that includes things like cost, the overall user interface in interacting with it, how

26:41.440 --> 26:48.600
easy is it to upload data, how easy is it to serve data, is it quick to serve or perform

26:48.600 --> 26:52.520
inference, how long does it take to train the models, how long does it take to tweak the

26:52.520 --> 26:58.960
models, and we tried to look at it both from a data scientist's perspective, as well as

26:58.960 --> 27:05.080
from a, anybody who is not necessarily an ML machine learning practitioner, so think

27:05.080 --> 27:10.480
like a traditional software developer or an analyst, and the reason we did that is because

27:10.480 --> 27:16.200
that's really where the target audience is for these, these at least automated machine

27:16.200 --> 27:21.560
learning products, the larger space are those people that have data, they're able to acquire

27:21.560 --> 27:25.440
labels, they're interested in building these models, but they don't necessarily want

27:25.440 --> 27:33.680
to take several different courses in machine learning and identify how to build one out.

27:33.680 --> 27:39.240
So on that front, we did have sort of an analyst slash intern kind of using these tools

27:39.240 --> 27:43.800
from that perspective, and then other more experienced data scientists used the tool

27:43.800 --> 27:47.960
too, and kind of collected our thoughts on some of the challenges we identified.

27:47.960 --> 27:53.840
We also tried to diversify the data sets that we're operating on, so we had one data

27:53.840 --> 27:59.600
set that we generated that was, we're calling it the Urban Outfitters Dress Data Set, which

27:59.600 --> 28:07.520
is just a collection of about 15 or 5500 dresses, and labeled very simply with just what's

28:07.520 --> 28:13.800
the dress pattern in terms of floral, solid stripes or not addressed, we're not addressed

28:13.800 --> 28:18.240
as something indicating it's very obviously not addressed like shoes.

28:18.240 --> 28:22.720
The goal there was just to have something that was kind of constrained and isolated.

28:22.720 --> 28:27.040
Not necessarily the actual data we'd want to use in real-time operations, but something

28:27.040 --> 28:33.160
that was controlled so that we can compare all the services against each other.

28:33.160 --> 28:38.120
So that was the one we had in-house, but we also used three public benchmark data sets.

28:38.120 --> 28:48.680
So we used C4-10 MNIST and fashion MNIST. So C4-10, I think they're 32 by 32 objects or

28:48.680 --> 28:54.560
images of different objects. MNIST is the digits, data set, and fashion MNIST is sort

28:54.560 --> 29:00.600
of the equivalent of MNIST, but using very simple looking fashion apparel, like shirts,

29:00.600 --> 29:07.200
pants, dresses, etc. So that way we sort of had a mix of different types of images going

29:07.200 --> 29:12.560
into this and we weren't sort of biasing towards the one dress data set that we had in-house.

29:12.560 --> 29:17.800
Did you get interesting information by doing C4-10 MNIST?

29:17.800 --> 29:23.920
One of the challenges I think in this industry is we talk about kind of being over-fitted

29:23.920 --> 29:29.360
on data sets like MNIST and C4-10. I'd expect all of these services to do pretty well on

29:29.360 --> 29:31.400
these. Was that the case or no?

29:31.400 --> 29:36.360
Yeah, if I recall, they all did pretty well. None of them quite got to the point of the

29:36.360 --> 29:44.840
true best scoring public benchmark. That's mainly because these algorithms that the services

29:44.840 --> 29:49.800
are using usually fall into from what I've seen one or one of two categories. The first

29:49.800 --> 29:55.240
one is transfer learning. So that's where you've taken already trained model, very deep

29:55.240 --> 30:01.400
trained model, cut off the head of it, have your own labels on the end and just retrain

30:01.400 --> 30:06.600
that sort of final layer or possibly tweak some of the layers in between. Then the other

30:06.600 --> 30:11.200
one, so that's the majority of them. Google Cloud, I think, was the one that stood out as

30:11.200 --> 30:16.160
different in that they used something called an architecture search or neural architecture

30:16.160 --> 30:21.880
search using something called NASNET that they published shortly before releasing AutoML.

30:21.880 --> 30:26.520
In that case, it's trying to find different configurations of the underlying neural network

30:26.520 --> 30:32.920
architecture that fit that data best. In the same with the in-house models, we were basically

30:32.920 --> 30:38.920
doing a in-house transfer learning on ResNet models. In all of those, you're already fit

30:38.920 --> 30:44.280
to or you're not training necessarily from scratch and training for hours with a really

30:44.280 --> 30:49.440
deep model. You're not going to hit those best performing numbers that you see in terms

30:49.440 --> 30:54.520
of the benchmarks out there, but they all get reasonably well. So if I recall, there

30:54.520 --> 31:00.280
may be fractions of a percent or a few percent often some of those data sets, and they all

31:00.280 --> 31:06.840
sort of performed very similarly. In fact, some of them, if I recall, may have even performed,

31:06.840 --> 31:13.160
we found they performed slightly better than we expected. It's mainly because that data

31:13.160 --> 31:17.760
set is, like you said, either A, already incorporated into some of those models that's already

31:17.760 --> 31:24.960
learned or B, essentially too small or too simple to be able to take a very large technique

31:24.960 --> 31:30.320
like neural architecture search and find in the best solution for. So, but across the

31:30.320 --> 31:37.760
board, for all those and even our internal dresses data set, performance as a whole was

31:37.760 --> 31:47.040
not a differentiator among any of the services that we found because most of them fell essentially

31:47.040 --> 31:53.280
within what one may deem the standard error for that data set. So like plus or minus a fraction

31:53.280 --> 31:58.480
or a couple percent. So if you're trying to eke out the absolute best performance for your

31:58.480 --> 32:04.160
use case, that may be very important for for our applications in sort of e-commerce or kind

32:04.160 --> 32:09.760
of enhancing these attributes for our internal teams. A fraction of percent is not going to make

32:09.760 --> 32:14.720
a huge difference. So it really came to that usability side that was more important for us.

32:14.720 --> 32:19.360
So that was one interesting finding is that there wasn't really any one major leader in terms

32:19.360 --> 32:26.160
of absolute performance. So it comes to all those other factors. This past summer, I had an

32:26.160 --> 32:34.480
intern. It was my daughter, actually, and she did a similar project working on or working with

32:34.480 --> 32:40.720
the speech to text API. So we had a bunch of podcasts. We had a bunch of transcripts and the

32:40.720 --> 32:49.440
idea was, could we build an automated pipeline to kind of transcribe our podcasts and using

32:49.440 --> 32:56.720
some of these similar kinds of APIs and ultimately maybe do some interesting things to help it

32:56.720 --> 33:04.080
perform better on kind of domain specific words. So this project started out with her looking

33:04.080 --> 33:11.120
at a bunch of APIs. And one of the things that we found pretty early on, like the initial plan

33:11.120 --> 33:19.680
was, we've got these APIs. We can throw a bunch of podcasts or audio samples at them and

33:19.680 --> 33:28.720
just kind of see how they do. But we found that each of the models had a bunch of like kind of,

33:28.720 --> 33:33.760
you know, they have kind of knobs, configuration parameters like, you know, Google supported three

33:33.760 --> 33:42.160
different models, telephone, video and default. They had different sample rates and things like

33:42.160 --> 33:49.760
that. All of these things are specific to audio. And the result of all that was that, you know,

33:49.760 --> 33:56.080
we certainly could not kind of compare these, you know, very easily using their default settings.

33:56.080 --> 34:02.160
But even when we tweaked the individual settings of a different model, we never really had a strong

34:03.200 --> 34:10.720
sense that we were able to compare apples to apples because, you know, we were kind of custom-fitting

34:12.320 --> 34:19.120
parameters for each of these runs. Is there a similar dynamic on the vision side? And if so,

34:19.120 --> 34:26.880
how did you address that? Yeah. So I think for on the vision side, the training and the, sort of,

34:26.880 --> 34:32.720
building the model itself is completely automated. So there's not much you can really do in terms of

34:33.360 --> 34:40.960
how to tweak knobs there. You can sort of iteratively update the model by providing new

34:42.240 --> 34:46.960
images or relabeling images as you go. But we did see that there were sort of knobs

34:46.960 --> 34:55.840
in terms of how you interact with the model after it's been trained. So not quite a knob,

34:55.840 --> 35:02.640
but a feature. One of the services offers human labeling for mislabeled or unlabeled images

35:02.640 --> 35:06.560
so that you can very rapidly kind of update your data set without having to do it from scratch.

35:07.760 --> 35:14.000
I think that one was Google. Clarify stands out as the one that was most quickly able to kind of

35:14.000 --> 35:20.400
through the interface, click on an image, relabel that image if it happened to be a false

35:21.040 --> 35:26.480
positive false negative and click retrain and get a response in a matter of like seconds.

35:27.120 --> 35:32.000
So that in that case, that's a sort of knob that's useful if you're rapidly interacting with it

35:32.000 --> 35:36.160
and you have a really messy data set that you're trying to use the service, not just to build models,

35:36.160 --> 35:42.480
but sort of to iterate you working with the model to iterate on that data set to kind of refine

35:42.480 --> 35:48.400
both the model as well as the quality of your the labels in your data. So we found more less

35:48.400 --> 35:53.760
on knobs in terms of the performance because it was a fully managed machine learning, so like

35:54.400 --> 35:59.680
machine learning as a service, but more on the usability side. And at last point that you are

35:59.680 --> 36:07.360
mentioning is that this feature that you are describing is this feature where you're able to

36:07.360 --> 36:15.520
identify mislabeled or misclassified images in a given run and kind of automatically tag those

36:15.520 --> 36:21.040
to get put into your training set for future runs. Yeah, yeah, that's that's the one there. Okay,

36:21.840 --> 36:27.680
cool. You know, and part of your analysis, you talked about this challenge that you ran into

36:27.680 --> 36:35.520
around data poisoning. Can you elaborate on that a little bit? Yes, so this goes to sort of the

36:35.520 --> 36:42.480
things that one should be careful with when using these types of services. The you don't want

36:42.480 --> 36:47.120
to look at these services as just a turnkey solution where you just give it the data. It's going

36:47.120 --> 36:51.120
to give you the perfect model and you don't really have to do much after that, particularly if you're

36:51.120 --> 36:57.280
sort of a manager that needs to plan time in deploying these type of products and allocating

36:57.280 --> 37:05.120
resources to it. So one interesting one we came into was if you what we have is for each

37:05.120 --> 37:10.320
product, there could be several different photos for it. So there could be a front-facing photo

37:10.320 --> 37:16.960
of a dress, a rear-facing side, different angles, zoomed in photos of the product. And the one

37:16.960 --> 37:24.800
example I think really resonated was a photo of a model wearing a dress and we, sorry, it was

37:24.800 --> 37:29.280
addressed with roughly four or five photos of that model wearing it with different angles and

37:29.280 --> 37:34.640
zooms. And then each one of those photos you could potentially treat as an independent sample.

37:35.520 --> 37:41.040
So each one of them is marked as a, in this case we, I think we're a classifying dress length,

37:41.040 --> 37:49.680
so is either a mini dress, mini dress or maxi dress, which is the different lengths. And if each

37:49.680 --> 37:58.720
one of those five images are treated independently, then if you use an arbitrary train validation

37:58.720 --> 38:05.360
test data split routine, then it's possible that that same exact product can end up both

38:05.360 --> 38:14.160
in your training, validation and test sets. And what that means is when you go to predict

38:14.160 --> 38:21.920
on say one of those test data, an image of that exact product was in training and then it happened

38:21.920 --> 38:27.520
to get spilled into test because of the way you split it, you could have trained to identify that

38:27.520 --> 38:34.800
that was a mini dress and then pass in a photo of that dress into the training side that is say

38:34.800 --> 38:41.520
a zoomed in picture of that dress of say just the top sides that's really accentuating say the

38:41.520 --> 38:47.280
neckline and maybe the sleeves. So you may not even see the bottom of that dress in that test photo,

38:47.840 --> 38:52.720
but the model will very accurately predict that it is a mini,

38:52.720 --> 38:58.080
light bottom length dress, which is not even in the photo. And the reason for that is because

38:58.080 --> 39:04.080
the model saw photos of that exact image under different angles in the training set.

39:04.880 --> 39:10.000
Right. So in that last statement, you said you'd pass in this view of the image into the the

39:10.000 --> 39:15.440
training side. I think you met the inference side. And so you're giving your passing your

39:15.440 --> 39:22.480
your trained model, cropped picture of the dress from which it could not reasonably predict the

39:22.480 --> 39:28.160
length of the dress and finding that it performs well, suggesting that it's picking up on it

39:28.160 --> 39:34.240
memorized basically that this dress is a mini dress. Yeah. And it may be for many other factors

39:34.240 --> 39:39.600
other than actually looking at features that indicate the length of the dress. So in that particular

39:39.600 --> 39:48.080
image, it had a nice clear blue background. The pattern of that dress was like a floral

39:48.640 --> 39:54.000
purple kind of themed dress that may have been unique to that dress out of the entire data set.

39:54.000 --> 40:01.120
So it may have just memorized that purple floral pattern is associated with an arbitrary labeled

40:01.120 --> 40:07.600
a mini length dress that we gave it. So one has to take care of that sort of if you're,

40:07.600 --> 40:14.400
if you just take, if we were to just walk in and take 50,000 photos with the labels on them and

40:14.400 --> 40:18.320
just allow it to do a random split, then they're going to kind of go all over the place. You need

40:18.320 --> 40:23.840
to kind of make sure that you cluster your products so that you're only feeding in

40:25.440 --> 40:31.280
products to training and independent products to validation or test. So that's just kind of one

40:31.280 --> 40:40.320
example we ran into. There's some other subtle ones that kind of go back to the point of segmentation.

40:41.040 --> 40:46.640
So there was one interesting example where the model was wearing a, I think it was a red and black

40:46.640 --> 40:55.840
striped dress and it kept ringing up as solid. And this was across both our in-house models and

40:55.840 --> 41:00.320
the managed service models. And we're like, why does this keep showing up as solid? It is clearly

41:00.320 --> 41:04.800
a striped dress. It looks a lot like the other striped dress that we stuck in there. So then we

41:04.800 --> 41:11.440
started poking around at it by sort of arbitrarily cropping the image in different areas and feeding

41:11.440 --> 41:18.880
in the cropped images to both the cloud services as well as our own classifier. And we realized that

41:18.880 --> 41:24.400
as we move the cropped image more and closer and closer to the hat that the model was wearing,

41:24.400 --> 41:31.680
which was a black solid hat, the more it was ringing up as solid. And then the more that we cropped

41:31.680 --> 41:38.160
that hat out and just focused just on the dress, it was stripes. So that's a case where for whatever

41:38.160 --> 41:46.240
reason it was tying that particular black solid hat. And that was the primary feature it was

41:46.240 --> 41:51.520
using to classify pattern instead of the dress itself. So it can be a little tricky to kind of like

41:51.520 --> 41:55.120
tame these models based on your input data. You have to be kind of be careful with that input

41:55.120 --> 41:59.200
data. And that goes back to what I was mentioning earlier where you can't just treat as a turnkey

41:59.200 --> 42:04.640
solution because that means in order to do that, you need to have a very clean data set. And in

42:04.640 --> 42:09.920
order to have a very well, in practice, at least I have never run into a perfectly clean data set

42:11.040 --> 42:17.840
in outside of academia. And even then they can be dirty sometimes. So that's sort of a cautionary

42:17.840 --> 42:23.120
tale to like really kind of inspect the outputs of these models. And that's actually where a lot of

42:23.120 --> 42:29.200
the effort that these managed services went into is the user interface after the model's built.

42:29.200 --> 42:34.080
How do you surface all of the false positives, false negatives? How do you categorize and cluster

42:34.080 --> 42:42.080
the different attributes that you're using your model to train on? And that way a user can

42:42.080 --> 42:45.360
without machine learning experience inspect and say, hey, something's fishy is there.

42:45.360 --> 42:52.160
So those are all great examples of things that people need to watch out for when they're working

42:52.160 --> 42:58.240
on with these products from a data perspective where there are other categories of kind of gotchas

42:58.240 --> 43:07.840
or did it all kind of boil down to data management data quality? I think it often boils down to the

43:07.840 --> 43:16.640
source of the data itself. There was an issue of sort of not necessarily an issue, but one thing

43:16.640 --> 43:22.480
to keep in mind is when do you think you're sort of getting to diminishing returns when tweaking

43:22.480 --> 43:30.080
and modifying these models? So if you hit sort of 92%, there's no real way of knowing whether or not

43:30.080 --> 43:36.560
you can get up to 98% without constantly tweaking and tweaking the model. And the same goes with

43:36.560 --> 43:42.240
sort of academic data sets. And these services allow you to, as I mentioned earlier, kind of

43:42.240 --> 43:48.560
relable mislabeled images, hit retrain and see the results again. But depending on the pricing

43:48.560 --> 43:53.280
structure, that can all come out of cost, whether it be a retraining cost or adding in additional

43:53.280 --> 43:57.360
images. So it's kind of tough to, as you're using these. And the same goes when you're kind of building

43:57.360 --> 44:06.720
these models offline or in-house in isolation. How do you know when to add more data to improve the

44:06.720 --> 44:12.400
model? How do you know when you need to improve the quality of the data? And then how do you know

44:12.400 --> 44:18.800
when it just sort of you hit? You hit the best you could do in a reasonable amount of time is one

44:19.280 --> 44:23.600
that was a little tricky. It's still data related, but it's more on the modeling side.

44:23.600 --> 44:30.160
So you've got in the blog post a couple of really interesting tables comparing the

44:31.200 --> 44:39.200
both usability and performance of the different services. And we'll link to the two blog posts

44:39.200 --> 44:46.560
in the show notes. Folks can go there to look at the detail. But the performance comparison

44:46.560 --> 44:55.200
table struck me as really interesting. In particular, the homegrown solution based on the Fast AI

44:55.200 --> 45:05.440
library performed very well state of the yard or at least let's say exhibited the best performance

45:05.440 --> 45:15.680
of all of the things that you compared on the public data sets, but not on your own urban outfit

45:15.680 --> 45:23.520
or dresses data set. That is where Google outperformed the other solutions. And now to be fair,

45:23.520 --> 45:33.840
the difference between Fast AI on your full data set and Google was very small. But I'm wondering

45:33.840 --> 45:39.680
if you have any kind of intuition as to what's happening here. Yeah, unfortunately that one's

45:39.680 --> 45:46.000
it's kind of tough to tease out exactly why Google kind of eaked out in performance over

45:46.960 --> 45:52.160
Fast AI because like you said, it was if I recall like a less than a percent or fraction of a

45:52.160 --> 46:00.320
percent. And for that data set, if I recall, there was in the test set 500 or so samples. So that could

46:00.320 --> 46:10.320
be just a few different images that happened to do slightly better in Google versus Fast AI. So

46:10.320 --> 46:16.640
factoring in the sample size of the test set, to me, that's almost sort of in the noise. The

46:16.640 --> 46:21.200
difference between those even though I think I probably highlighted them explicitly in that blog post

46:21.200 --> 46:28.800
there. So I kind of look at that as not necessarily that Google is performing better than Fast AI

46:28.800 --> 46:35.440
or that the other services are completely behind. But just it's from an from a operational

46:35.440 --> 46:40.000
standpoint, it's sort of in the noise like they all performs very similarly. Yeah, yeah.

46:41.040 --> 46:48.400
And so what was the what was kind of the the key takeaway here in terms of, you know, when you

46:48.400 --> 46:53.680
need to a tool to solve these kinds of problems for the business, where are you going to look?

46:53.680 --> 47:00.720
So we haven't actually decided to use any of the managed services. It was more of a experiment to

47:00.720 --> 47:06.960
to kind of see what was out there. Currently, our team is still kind of focused on using say

47:06.960 --> 47:13.440
Jupyter notebooks. We have been dabbling with the Fast AI solution as well. It's that's that was

47:13.440 --> 47:18.560
using like the less than one. So it was very quick to get started with it. And it also that

47:18.560 --> 47:23.760
library provides some visualization capability that's little easier than say, just manually rolling

47:24.640 --> 47:31.360
images through like MAT plotlib. But what we found, I wouldn't necessarily say we're forever staying

47:31.360 --> 47:37.520
away from these particular services. We may not even use them necessarily for a fully deployed

47:37.520 --> 47:45.840
model. But even just using it as a quick hand wavy benchmark of uploading some data, seeing how

47:45.840 --> 47:49.520
well these things, like if you have a fresh data set and you just want to see how it runs,

47:50.560 --> 47:54.640
running it through one of these services, getting in an interface so you can at least visualize

47:54.640 --> 48:01.920
some of the performance. And do it all roughly if you data set up reasonably well, roughly less than

48:01.920 --> 48:07.680
an hour. I mean, the training takes often less than five minutes surfaces you results pretty quickly.

48:08.480 --> 48:15.120
It's a nice way of very quickly getting a feel for what's in the realm of possible for these

48:15.120 --> 48:20.880
models, even for somebody who is an ML practitioner. So it kind of gives a nice sense of comfort that,

48:21.600 --> 48:28.720
yes, this is a tractable problem. You could get 80 plus percent in performance. And whether or not

48:28.720 --> 48:35.040
you kind of stick with that service to use it for deployment and productionize it. Or you just

48:35.040 --> 48:42.160
kind of use that as insight that your homegrown models may perform well. I found it much easier

48:42.160 --> 48:49.200
to kind of get up and running with that than wrangling the data in a custom in-house solution.

48:49.840 --> 48:58.880
Well, we've talked about the Fast AI library a few times. And I will add in a mention, a plug,

48:58.880 --> 49:07.440
you know, just as you found, you can do a lot going through the first couple of lessons of that

49:07.440 --> 49:13.600
Fast AI course using their library. And it is a fan favorite of folks in the Twimmel community. We've

49:13.600 --> 49:21.200
got a community that folks can find at twimmelai.com slash meetup, the Twimmel online meetup. And we've

49:21.760 --> 49:28.800
brought three cohorts of folks through the Fast AI course. The videos of our study groups are

49:28.800 --> 49:34.800
available on YouTube. And we're about to start, at least at the time of the recording of this

49:34.800 --> 49:42.400
group going through the part two course. I imagine with enough demand, we'll bring another group

49:42.400 --> 49:48.240
through the part one course again. But for anyone who's listening to this and wants to learn how to

49:48.240 --> 49:54.400
build their own state of the art vision models, the Fast AI course is a great place to start. And

49:54.400 --> 50:01.040
the Twimmel online meetup is a great place to get support in doing that. With that said, Tom,

50:01.040 --> 50:07.120
thanks so much for taking the time to share your work on this. Super interesting. And I appreciate

50:07.120 --> 50:10.880
you coming on the show. Yeah, thank you for having me.

50:14.640 --> 50:20.240
All right, everyone. That's our show for today. For more information on Tom or any of the topics

50:20.240 --> 50:28.320
covering in this episode, visit twimmelai.com slash talk slash 247. As always, thanks so much for

50:28.320 --> 50:38.320
listening and catch you next time.

