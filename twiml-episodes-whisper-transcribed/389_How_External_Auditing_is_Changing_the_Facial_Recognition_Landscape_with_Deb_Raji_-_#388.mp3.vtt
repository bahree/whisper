WEBVTT

00:00.000 --> 00:11.840
All right, everyone. I am on the line with Deb Rajee. Deb is a technology fellow at the AI

00:11.840 --> 00:17.680
now Institute at New York University. Deb, welcome to the Twimal AI podcast.

00:17.680 --> 00:23.520
Hey, thanks for having me. It is great. Yeah, absolutely. I'm really looking forward to this

00:23.520 --> 00:29.280
conversation. We've certainly got a lot to catch up on. Yeah. There's a lot been going on.

00:29.280 --> 00:33.360
It's been exciting couple of weeks. Yeah. It's been an exciting few weeks, absolutely, in the

00:34.320 --> 00:42.080
the field that you work in. But let's maybe start from the beginning and have you share a little

00:42.080 --> 00:48.560
bit about how you came into artificial intelligence and where it all started.

00:49.360 --> 00:56.560
Awesome. Yeah. So I kind of, I guess it all started in university. So I'm Canadian for reference.

00:56.560 --> 01:04.000
And I kind of entered on a whim the engineering program at the University of Toronto.

01:04.720 --> 01:08.640
And that was where I actually learned how to code. So my first semester of my first year was

01:08.640 --> 01:18.000
like my first coding class. And I hadn't encountered that before coming in. So it was just like

01:18.000 --> 01:22.960
interesting, awesome experience. And I kind of just kept doing it and kept getting involved.

01:22.960 --> 01:27.520
And my degree was in robotics engineering. So I got a lot of exposure to the computer vision

01:27.520 --> 01:32.640
space and built some skills there as well. So I spent between my third year and my fourth year,

01:32.640 --> 01:36.720
I had the opportunity to do this, to take a year off and do a year-long internship.

01:37.280 --> 01:41.600
So I did that internship at Clarify, which is this computer vision company in New York,

01:41.600 --> 01:46.400
at New York City. And I was on their applied machine learning team there. And that's really where

01:46.960 --> 01:51.840
I like learned about the machine learning research community. It was like the first time I went

01:51.840 --> 01:58.400
to Neurips, which is the big machine learning conference. And it was sort of the first time that I

01:58.400 --> 02:04.400
kind of identified the issue of bias and the ethical concerns involved in facial recognition.

02:05.360 --> 02:10.400
In the computer vision as a field in general. So while working on models there, I kind of

02:10.400 --> 02:16.800
began to notice that even the research data sets and even some of the data sets we were using as

02:16.800 --> 02:23.120
part of our engineering processes did not have, you know, faces that look like me. Like I was,

02:23.120 --> 02:29.280
I was very aware of the lack of representation that was there. And it got to a point where I kind

02:29.280 --> 02:34.640
of just started complaining about it. And people were like, we don't know what to do. And,

02:34.640 --> 02:40.160
you know, like it's already so hard to collect data at all. How do we think about bias? What do we,

02:40.160 --> 02:45.040
like, you know, we don't understand this problem. And then it became very clear that it was just kind

02:45.040 --> 02:50.160
of this understudied phenomenon. And that was when I started kind of scourging the internet,

02:50.160 --> 02:56.240
trying to identify anyone that was doing the same thing or had noticed the same problem. And that's

02:56.240 --> 03:02.320
how I kind of landed on Joy Blemwini's TED Talk, which she had given, I think probably a year or

03:02.320 --> 03:08.240
so before, I should have given this TED Talk on her experience with attempting to use open-source

03:08.240 --> 03:13.120
facial recognition software and having the technology not identify her face because she was,

03:13.120 --> 03:17.280
you know, darker skinned and having to use a white mask in order to be identified. And that was

03:17.280 --> 03:21.520
pretty much what her TED Talk was about at the time. And I was like, okay, cool. So when I was like,

03:22.400 --> 03:25.360
what does that even mean? Having to use a white mask to...

03:26.400 --> 03:32.800
So she had to use a white mask to have the facial recognition system like identify her or...

03:32.800 --> 03:35.040
Oh, meaning identify that there was a face there?

03:35.040 --> 03:39.760
Identify that there was a face there. Oh, wow. Yeah. So it was pretty, yeah. This was like,

03:39.760 --> 03:45.200
that was sort of the extent of the articulation of the problem at the time. And that was what prompted

03:45.200 --> 03:52.560
her to start the algorithmic justice league project. So I kind of reached out to her like very recently

03:52.560 --> 03:57.280
after I started at clarifying. I was like, hey, you know, I'm noticing this thing. You give a talk

03:57.280 --> 04:03.040
about this thing. Like, can we talk? And it was a very extra email. Like, I was like, already like,

04:03.040 --> 04:08.400
really deep in the woods of like, here's these data sets that we use in computer vision. And like,

04:08.400 --> 04:13.280
here's like the very tiny percentage. Like, the data sets we use are, you know, 80 to like,

04:13.280 --> 04:18.800
95% lighter skinned subjects. So it was like really bad. So I was like, these are the stats.

04:18.800 --> 04:23.120
Like, I've been trying to like get more info. Can you help? And I think she was just kind of like,

04:23.120 --> 04:30.960
wow, this person like cares. So happy to respond. And her response was something like, yeah,

04:30.960 --> 04:36.400
let's talk in a month or something like that. And I was like, okay. So yeah, like a couple months

04:36.400 --> 04:40.640
later, we actually started talking and we started, you know, collaborating and working on some

04:40.640 --> 04:46.160
stuff. So at the time, she was working on her thesis, which was around gender shades. So I helped

04:46.160 --> 04:51.440
her with that. And then, you know, as gender shades sort of, so gender shades came out February

04:51.440 --> 04:56.400
that year. So as gender shades kind of began kind of gaining steam. And like, we understood that it

04:56.400 --> 05:02.000
was a problem that other people could also recognize and empathize with. I kind of was like, okay,

05:02.000 --> 05:06.800
cool. Like, you have enough support that like, I can kind of work with you full time over a summer.

05:06.800 --> 05:11.440
Or it was ended up being a summer and a fall. But we ended up working together on this sort of

05:11.440 --> 05:17.200
follow-up study trying to identify what, what about gender shades made it an effective audit to sort

05:17.200 --> 05:22.880
of characterize and communicate these problems in a way that pressured these companies to sort of

05:22.880 --> 05:28.000
feel cornered to take action. So that was a lot of the follow-up work I did with Joy. And a lot

05:28.000 --> 05:33.440
of my subsequent work is thinking about, you know, how do we actually capture some of these,

05:33.440 --> 05:37.760
like, limitations or these failures at these models experience? And how do we communicate it

05:37.760 --> 05:43.120
to the public, but also to other researchers, to other engineers in a way that actually makes

05:43.120 --> 05:48.400
that limitation super clear. And like, raises concern in a way that prompts people to take action.

05:49.200 --> 05:53.120
So that was a lot of what my journey is. And that's a lot of the work I'm doing today. So, you know,

05:53.120 --> 05:58.720
following that, I started working with Timnett and Meg at Google and we worked on the model

05:58.720 --> 06:05.200
cards project, which was a way of sort of documenting and communicating audit results and ended

06:05.200 --> 06:08.960
up sort of becoming part of the engineering process at Google for machine learning models.

06:09.760 --> 06:13.600
And then, you know, following that, like, the National Institute of Standards and Technology is

06:13.600 --> 06:18.480
sort of taking up some of the findings in our work and the terminology in our work.

06:18.480 --> 06:25.840
So that was, it sort of become a thing, you know, and it's really sort of stem from this desire to,

06:25.840 --> 06:31.200
like, identify the problem in a consistent way and communicate in a consistent way. So that's

06:31.200 --> 06:35.280
kind of the ongoing work I'm doing today at AINow and wherever I end up in the future.

06:36.000 --> 06:42.720
That's awesome. That's awesome. And so you think of the broad area as you've referred to audits

06:42.720 --> 06:49.920
on several occasions. What all is kind of captured in that terminology?

06:50.640 --> 06:57.360
Yeah. So the reason I mentioned, I talk about sort of auditing and, you know,

06:57.360 --> 07:01.600
some of the work I've done with Google will, like, refer to it as, like, internal auditing.

07:01.600 --> 07:06.480
Is this, you know, anchor to the idea that, so especially the work that I do with Joy,

07:06.480 --> 07:12.800
we look at models that are already out there that, like, someone already decided was, like,

07:12.800 --> 07:18.000
sufficient to deploy. It had already passed whatever deployment conditions were already there,

07:18.000 --> 07:22.160
and the person had already sort of, like, thrown it over the fence. So we look at, like,

07:22.160 --> 07:26.640
models that are already built and deployed, and then we try to understand, you know,

07:26.640 --> 07:31.680
how they actually operate within society. How do they actually operate within a deployed context?

07:31.680 --> 07:38.960
So, you know, for gender shades, for example, building a test set where we identify different

07:38.960 --> 07:45.440
populations that could potentially be affected by such a product, represent that, like,

07:45.440 --> 07:49.680
each of these subgroups within a test set, and then evaluate for each of these subgroups and

07:49.680 --> 07:53.680
discuss the results for each of these subgroups. And the disparities between these subgroups

07:53.680 --> 07:58.480
is us trying to sort of simulate discussion around, you know, within society, how can we

07:58.480 --> 08:02.800
anticipate this model that's already, this product that's already out there? How can we

08:02.800 --> 08:07.200
anticipate its performance on these subgroups that we've decided that we want to look at that we

08:07.200 --> 08:12.560
care about? So that's why it's framed as an audit versus just, kind of, like, an assessment or

08:12.560 --> 08:19.600
an evaluation. It's kind of these quantitative tests to see, like, when you've deployed this thing,

08:19.600 --> 08:23.360
and it's already out there, is it actually good enough for these specific groups, these specific

08:23.360 --> 08:27.360
populations that we've decided that we care about, and we want to see, we want to observe the

08:27.360 --> 08:33.680
performance on. And this is a lot of a lot of the innovation of gender shades, too, was not just

08:33.680 --> 08:38.720
looking at subgroups along, you know, one axis of race or gender, but looking at that intersection

08:38.720 --> 08:43.360
of, for this darker female subgroup that we've decided to, to, you know, study the performance

08:43.360 --> 08:49.920
of this deployed system on, you know, how well does that, that model work for this, the subgroup

08:49.920 --> 08:54.480
that's at the intersection of different identities. So that was also kind of an interesting

08:54.480 --> 08:59.920
difference between how gender shades worked versus how other, kind of, assessments had worked

08:59.920 --> 09:04.400
in the past. And then the other sort of element of it, which I alluded to earlier, was this idea of

09:04.400 --> 09:09.920
a, of a user representative test set of, I identify all these different populations that matter,

09:10.480 --> 09:15.280
and it's not about, you know, the fact that, let's say, like, 10% of the population in

09:15.840 --> 09:21.360
Kansas is, like, darker skin. So 10% of my test set is darker skin. It's like, no, there are

09:21.360 --> 09:25.760
darker skin people. So it needs to work for them. So they're going to be equally represented in

09:25.760 --> 09:29.920
the test set so that, you know, their performance matters just as much as the performance of any

09:29.920 --> 09:35.680
other type of user that I care about. Yeah. So we kind of implemented these strategies to really

09:35.680 --> 09:41.840
look at, or allude to understanding better how these models perform, you know, in society,

09:41.840 --> 09:46.160
once they're deployed, once they're already out there. Okay. So we've kind of talked about,

09:46.160 --> 09:49.440
we've talked about gender shades, but we haven't really said what it was, it's this,

09:49.440 --> 09:55.680
this audit, but in particular, you developed this audit set or test set, and then you deployed it

09:56.400 --> 10:01.680
against some of the public facial recognition technologies that were offered by

10:02.960 --> 10:08.240
several of the cloud vendors. And it was, I think there were two different iterations of it,

10:08.240 --> 10:14.400
or releases of it with different, different vendor communities. Yeah. And that was on purpose.

10:14.400 --> 10:21.280
So the first, the first audit was IBM, Facebook Plus, and Microsoft. And it was, so the name of the

10:21.280 --> 10:26.800
test set, by the way, is the pilot, the pilot, the pilot parliament's benchmark, which is like

10:26.800 --> 10:32.800
PPB for short, and the test set where, you know, and if there's a great paper called diversity

10:32.800 --> 10:40.000
and faces where they actually just sort of PPB in comparison to all the other test sets in

10:40.000 --> 10:46.320
facial recognition at the time, or up to date, which is sort of, you can see that PPB is balanced

10:46.320 --> 10:52.640
for gender and also balanced for skin type. So you have like a set of darker images that are

10:52.640 --> 10:58.400
sort of equivalent to the number of lighter images. And in other benchmarks in this space,

10:58.400 --> 11:03.120
you can see that the proportion is, you know, highly skewed towards lighter images and highly skewed

11:03.120 --> 11:09.440
male. So it was sort of the first benchmark that encompassed sort of this balance and enabled this

11:09.440 --> 11:15.920
intersectional testing, which was really sort of the key differentiator between other, like just sort

11:15.920 --> 11:22.160
of the typical facial recognition evaluation process. And then with respect to the companies that

11:22.160 --> 11:28.080
we were looking at, we kind of picked these very specific targets. So the first iteration was

11:28.080 --> 11:33.200
Microsoft, IBM and Facebook Plus, these were, you know, huge vendors in the space. And they were selling

11:33.200 --> 11:40.000
sort of off the shelf facial recognition APIs, so like application program interfaces. So they

11:40.000 --> 11:45.040
would sell pretty much the access to the models, their facial recognition models. And if I'm an,

11:45.040 --> 11:49.600
you know, and I'm an app developer, I can just take that model and send my images to the model

11:49.600 --> 11:54.240
to get a certain set of predictions. So, you know, these models are being integrated in all kinds

11:54.240 --> 12:00.800
of applications through developer clients. So we knew it was like very impactful technology. So

12:00.800 --> 12:07.920
what we did was we evaluated the performance of these different models on our test set and

12:08.480 --> 12:14.240
observed sort of how well the models performed on these different subgroups. So darker female,

12:14.240 --> 12:19.360
lighter female, darker male and lighter male. And the result of that initial gender shake study

12:19.360 --> 12:24.400
was that there was a, you know, almost 30% disparity between the darker female and the lighter male subgroup,

12:24.400 --> 12:30.160
which was really surprising. Like you would never, you know, from my time working on a machine

12:30.160 --> 12:34.880
learning team, I know that you would never sort of deploy anything that's, you know, if it had a

12:34.880 --> 12:39.920
60% accuracy, right, you'd never deploy that system. So it was really surprising to see that

12:39.920 --> 12:44.880
on the darker female subgroup, it was performing at like 60% accuracy. So yeah, that was sort of

12:44.880 --> 12:50.960
the initial shock of the first study. And then the follow-up study was to say like, well, after

12:50.960 --> 12:55.920
that was revealed, there was a very public, it was a very public situation. So a lot of the companies

12:55.920 --> 13:01.520
released statements saying that they acknowledged the issues, a lot of them reproduced the results

13:01.520 --> 13:06.720
and committed to doing better. So, and all of them re-released. So within seven months, all of them

13:06.720 --> 13:11.360
had released new, so they had released new models, they'd retrained models and redeployed them.

13:11.360 --> 13:18.400
So we in the follow-up study sort of tested, well, how well, how much did these original

13:18.400 --> 13:23.760
audited companies actually improve their performance on the benchmark that we've, you know, designed?

13:24.640 --> 13:28.880
But also, you know, the companies that were not evaluated, that were not audited, did they,

13:28.880 --> 13:32.720
in any way, get affected by this? And the response was that, you know, the companies that were

13:32.720 --> 13:38.640
directly audited did make that improvement, but the companies that were not audited, including

13:38.640 --> 13:43.360
Amazon, which is sort of, you know, one of the big players in the space and at the time was

13:43.360 --> 13:48.800
selling their technology to facial recognition. All those companies, including Amazon, did not

13:48.800 --> 13:53.200
still have that disparity of, you know, up to 30 percent between the darker female subgroup

13:53.200 --> 13:57.680
and the lighter male subgroup. So they were still demonstrating that bias that we had initially

13:57.680 --> 14:01.840
identified, which was really alarming, especially for Amazon at the time.

14:01.840 --> 14:08.960
Alarming, but also I think it says a lot of interesting things, right? It says that, you know, with,

14:08.960 --> 14:14.560
you know, with some investment, you can change it, right? But you have to care. But it also says that

14:14.560 --> 14:19.520
external pressure is what makes you care, not just that, I mean, you've got to believe that Amazon

14:19.520 --> 14:24.560
and everyone else that's selling products in the space knew about the original data study and,

14:25.360 --> 14:31.360
you know, could have taken the steps proactively to address the issues, but based on the results of

14:31.360 --> 14:37.680
the second study, that didn't appear to be the case. Yeah, for sure. It also just reveals that if

14:37.680 --> 14:44.000
they're not audited for it, it's very easy for them to ignore. Yeah, like exactly what you said

14:44.000 --> 14:48.960
about external pressure, but also kind of like targeted, targeted pressure to very specific,

14:49.520 --> 14:55.360
to specific companies. Yeah, if they hadn't sort of been called out by name, the probability of

14:55.360 --> 15:01.920
them doing better is not is very low. And you know, in a study after that, even another paper that

15:01.920 --> 15:08.080
we've very, very recently put out called staving face, we look at the different tasks. So, you know,

15:08.080 --> 15:12.800
gender shades is looking at the gender classification tasks. So how well does it do on the binary task of

15:12.800 --> 15:16.560
identifying if it's a male or female? But there's other tasks, you know, there's like a small

15:16.560 --> 15:22.800
detection task, there's the actual face detection, you know, there's age detection, things like that.

15:22.800 --> 15:30.240
And we also evaluated for some of these other tasks using sort of a more a benchmark with a lot

15:30.240 --> 15:35.520
more metadata. And what we found was that the companies that were initially audited, not only did

15:35.520 --> 15:39.840
they, they only, they all only improved on gender classification. So they had, they still had

15:39.840 --> 15:44.720
like large disparities for age classification, for example. So even Microsoft that had been audited

15:44.720 --> 15:49.680
a couple years ago, now they have like, you know, very small disparities between their performance

15:49.680 --> 15:56.000
on darker females and lighter males for gender classification as a task. But for age classification,

15:56.000 --> 16:01.200
for example, they still have like a 30% disparity between the groups. So yeah, you have to be very

16:01.200 --> 16:06.960
specific about which populations and which subgroups you're, you're looking at your evaluating

16:06.960 --> 16:11.200
performance for, but you also have to be very specific about the task and very specific about the

16:11.200 --> 16:18.000
target. And all of this is really more or less a case for like a regulatory regime where like

16:18.000 --> 16:23.840
everyone has to sort of restrict their use of facial recognition in specific ways or get assessed

16:23.840 --> 16:29.120
kind of universally across the industry in very specific ways for very specific tasks that we are

16:29.120 --> 16:34.800
worried about as a society. And also, you know, for very specific populations that we are concerned

16:34.800 --> 16:40.480
about as a society. So yeah, it just kind of reveals how specific you have to be with respect to

16:40.480 --> 16:47.280
how you design these audits. Yeah, I was going to add you answered the question that I was going to

16:47.280 --> 16:55.840
ask, which is around like, well, I maybe you didn't, right? I think what, you know, what you saw is

16:55.840 --> 17:03.680
that folks can kind of engineer systems for the test, like engineer their system for the benchmark.

17:03.680 --> 17:13.280
And it sounds like an, to some extent, what you're saying is that that's what we need to happen

17:13.280 --> 17:21.440
is that we establish the benchmarks broadly and we, you know, through regulation or some other

17:21.440 --> 17:28.400
measure encourage the companies that are offering these technologies to engineer for these tests.

17:28.400 --> 17:32.000
And I guess there's part of me that says, you know, should it be something else? Should we show them

17:32.000 --> 17:36.320
like the error of their ways? And they say, oh, we should have a diverse team and we should kind

17:36.320 --> 17:43.760
of go off and think about all of these things. And yeah, you know, maybe the, you know, can the,

17:43.760 --> 17:49.600
can the test ever be exhaustive? Yeah, that's a great question. I don't think I answered that question.

17:52.240 --> 17:56.080
So that's a great question, because that was something that like plagued me for a long time,

17:56.080 --> 18:00.400
too, where I was like, because the other thing too is not even just with respect to the task,

18:00.400 --> 18:04.160
because I think with respect to the task, and this is what the National Institute of Standards and

18:04.160 --> 18:11.600
Technology is really into is, you know, they identify that the, that the, the, the facial sort of

18:11.600 --> 18:17.040
identification task, which is the ability for me to like identify you as Sam, you know, identify

18:17.040 --> 18:21.360
myself as Deb, or the facial verification task. If there's two images of me to be able to say

18:21.360 --> 18:26.160
that they're the same image, like they see those as the most important sort of pertinent tasks

18:26.160 --> 18:30.240
with respect to facial recognition. So they will sort of focus on that. And they're like, we don't

18:30.240 --> 18:34.560
actually care about these other things happening. So we've already identified the tasks, and they're,

18:34.560 --> 18:38.080
and then they'll say like, oh, you know, these are the groups that we care about. So we've already

18:38.080 --> 18:43.360
identified these groups. But then we, we rose the question. And this was a lot of our, our third

18:43.360 --> 18:48.000
paper, I call it sort of like like an existential crisis paper, because it's kind of, it talks about

18:48.000 --> 18:53.600
some of these like, who exists that we need to talk about. One of which is this idea of, you know,

18:54.240 --> 18:59.680
looking at the intersection of, you know, skin type and gender is just one way to look at it.

18:59.680 --> 19:04.000
There's other contexts in which it's really important to look at age. And how, you know,

19:04.000 --> 19:10.480
how do you identify the intersection of age and gender and and skin type? Like that just gives

19:10.480 --> 19:17.040
you an infinite number of permutations. And the way that we ended up sort of gaining some level

19:17.040 --> 19:23.760
of like peace is to sort of, I guess, reflect on this as limitations of the audit structure of

19:23.760 --> 19:27.760
of an audit in general to say that there are certain things that you can learn from an audit

19:27.760 --> 19:31.200
and certain things that will actually be very difficult to learn from an audit and actually

19:31.200 --> 19:36.480
require different types of evaluations such as pilots, for example, piloting the technology

19:36.480 --> 19:41.760
or even like overall restriction. So this is one of the cases for the idea of a moratorium where

19:42.480 --> 19:46.160
we understand that there's concerns with this technology and it's not just the racial bias,

19:46.160 --> 19:52.000
right? So, you know, when the technology doesn't work in the case of what we've identified

19:52.000 --> 19:54.160
through gender shades, you know, when the technology is less

19:54.160 --> 19:59.600
performing on darker skin individuals, for example, that puts them at higher risk. You know,

19:59.600 --> 20:05.280
if I get misidentified as a criminal at a higher rate than others, that's because of,

20:06.000 --> 20:10.560
you know, the technology not being as functional for me as it is for another person. But then there's

20:10.560 --> 20:16.560
also the situation of, you know, especially in cases where it's difficult to properly assess the

20:16.560 --> 20:21.520
functionality of the system, you know, maybe we should just restrict the use of the system in general.

20:21.520 --> 20:25.360
Maybe we should just take it out of the market as we figure out and we learn these things that we

20:25.360 --> 20:30.480
have these sort of more nuanced conversations and discuss the other sort of facets of concern as

20:30.480 --> 20:35.200
we discuss other issues such as privacy and transparency that exist that we need to have honest

20:35.200 --> 20:41.360
conversations about in addition to like the complexities of that bias situation. So I think that's

20:41.360 --> 20:46.880
sort of been my approach to it is that there are actually very clear limits to these kinds of

20:46.880 --> 20:51.520
audits. We need to be very aware of. There's a lot that we can learn from them, but there's also

20:51.520 --> 20:56.960
limits to what a standard can tell us and what, you know, NIST can actually do and can actually

20:56.960 --> 21:01.760
provide with respect to insight into how these technology, how the technology operates in deployment.

21:02.480 --> 21:05.840
So we need to be very careful about that. We need to do a lot more thinking about that. And while

21:05.840 --> 21:11.680
we're thinking about that, maybe this technology that's like very clearly immature in certain ways

21:11.680 --> 21:20.400
needs to be taken off the market. Well, I definitely want to dig into the kind of the broader

21:20.400 --> 21:27.120
implications of facial recognition technology and the question of moratorium or not. But before we do

21:27.120 --> 21:34.640
that, I want to continue to kind of pull on some threads around your research and the auditing

21:34.640 --> 21:43.040
and you alluded to this in your last statement. But I thought that the savings face paper was

21:43.040 --> 21:50.800
really interesting in that it was essentially saying, you know, this is our third, you know,

21:50.800 --> 21:57.520
auditing paper, you know, our third go at auditing. And it's still hard and we still mess up and

21:57.520 --> 22:03.920
it's probably not enough. And you need to be really careful even, you know, not just trying to

22:03.920 --> 22:09.200
feel the technology, but just auditing it. Yeah. Can you talk a little bit more about what you

22:09.200 --> 22:13.120
found there? Yeah. So that's saving face paper. I said it's sort of like an existential crisis

22:13.120 --> 22:17.840
paper because following gender shades and actionable auditing, we were sort of seeing these gender

22:17.840 --> 22:22.960
shades like audits appearing everywhere. Like everyone was kind of just building their own version

22:22.960 --> 22:28.880
of PPB, which is the benchmark for sort of any kind of task or any kind of situation.

22:28.880 --> 22:35.200
And people were like you were like you mentioned earlier sort of building to the test. So they

22:35.200 --> 22:42.480
were saying, okay, we're going to improve performance on on PPB or whatever PPB shadow we created

22:42.480 --> 22:46.960
for ourselves. And that is going to be sort of the bar. And once we're over that bar, then that

22:46.960 --> 22:53.360
is something sort of significant. And that whole paper was us saying like hold up something like

22:53.360 --> 22:58.960
an audit like gender shades is a demonstration of, you know, some of these clear oversights with

22:58.960 --> 23:03.680
respect to testing. So the fact that, you know, some of these really huge companies, they're deployed

23:03.680 --> 23:10.400
products, you know, failed. So these are, you know, companies that, you know, they have many people

23:10.400 --> 23:14.720
on their teams and no one on the team had sort of tested for this. Previously, that was the

23:14.720 --> 23:20.960
demonstration was to show that lack of oversight in that negligence. So an audit like gender

23:20.960 --> 23:25.680
shades is really a demonstration of that negligence and to point to the fact that there are,

23:26.320 --> 23:30.240
there are very clear gaps in the way that we currently evaluate and assess this technology

23:30.240 --> 23:37.280
before we deploy it. And it's not necessarily the bar. It's not necessarily a high bar to cross.

23:37.280 --> 23:40.720
So it's kind of like if you triple over the bar, that's embarrassing and you should be ashamed of

23:40.720 --> 23:45.360
yourself. But if you like, you know, if you pass the bar, it doesn't actually mean anything.

23:45.360 --> 23:49.200
There's still, that doesn't necessarily mean that there's no more work to do. So the saving

23:49.200 --> 23:53.920
face paper was us to say, it was us saying, you know, just because Microsoft is now doing well

23:53.920 --> 23:58.880
on PPB, it doesn't mean that it's doing well with respect to, you know, age classification. Like

23:58.880 --> 24:02.560
I mentioned, they still have huge disparities there. But that doesn't mean that, you know,

24:02.560 --> 24:06.720
just because IBM is doing okay on PPB now, it doesn't mean that they haven't thought a lot about,

24:06.720 --> 24:10.480
you know, the privacy concerns that came up with their diversity and faces paper and flicker,

24:11.200 --> 24:17.680
you know, just because we're having conversations around, you know, some of these audits being

24:17.680 --> 24:23.440
sort of conditions for use. So, you know, we're seeing a lot of policy conversations of

24:24.080 --> 24:28.000
gender-shaped-out audit being sort of the condition, you know, you have to pass gender-shades in

24:28.000 --> 24:36.640
order for your model to be able to be deployed. And we're like, there are a lot of things that

24:36.640 --> 24:41.600
need to happen before a system, like a facial recognition system gets deployed. For one thing,

24:41.600 --> 24:46.400
like community consultation, so community participation in that decision-making process,

24:46.400 --> 24:51.600
like transparency around the performance of the system, you know, there's, you know, privacy

24:51.600 --> 24:57.440
concerns being addressed. So, there's so many other things involved. It's not the barred across.

24:57.440 --> 25:02.560
So, that was the main takeaway from that work was us saying, you know, there's so many other

25:02.560 --> 25:08.960
elements to this. And, you know, those considerations can be integrated into the way that we assess

25:08.960 --> 25:15.200
our systems, right? So, you know, NIST could, you know, take on these sort of more qualitative aspects

25:15.200 --> 25:19.760
of, you know, reflecting on the assessment of these systems and facilitating some of these

25:19.760 --> 25:26.000
processes required for any kind of deployment. But, you know, we need to recognize that whole,

25:26.000 --> 25:30.880
you know, this is a whole can of worms that is much more complicated than a lot of us actually

25:30.880 --> 25:34.640
understand at the moment. And we need to, you know, while we're having these more nuanced

25:34.640 --> 25:39.680
conversations, take the product off the market or at least, you know, support that moratorium stance

25:39.680 --> 25:45.760
of pressing pause, as we all have this like deeper conversation. The other thing too, that I was

25:45.760 --> 25:49.840
at least personally realizing at the time, and I think Timnit and Joy were kind of going through

25:49.840 --> 25:56.640
this as well, where we were noticing that there were situations where the issue was not that facial

25:56.640 --> 26:02.480
recognition wasn't working. So, it wasn't that it wasn't even that like, you know, the data was

26:02.480 --> 26:07.360
an encrypted property or the privacy within, or the data was a managed property. It wasn't even that,

26:07.360 --> 26:13.920
you know, it wasn't it wasn't working for different subgroups, but it was just that it was being

26:13.920 --> 26:19.360
like actively weaponized by like this authority figure. So, when you think about facial recognition

26:19.360 --> 26:24.880
as a technology, I like to remind people that a face is the equivalent of a fingerprint. It's

26:24.880 --> 26:29.760
an identifiable biometrics. So, you know, I have your face. I can do a lot of things with that,

26:29.760 --> 26:34.400
except, you know, we are so careful with our fingerprint data. There are so many standards around,

26:34.400 --> 26:39.440
you know, how to store that information, how to how much you can centralize that data about,

26:39.440 --> 26:44.400
you know, how many people, but when it comes to faces, there's, you know, no rules. So, people

26:44.400 --> 26:50.800
have these, you know, immense repositories of people's, you know, identifiable biometrics,

26:50.800 --> 26:54.640
all in this sort of like centralized location that can be controlled by this authority figure.

26:54.640 --> 26:58.880
This is very dystopian. I apologize in the current times to like bring up this image, but

26:58.880 --> 27:06.960
I think we've seen, I think we've seen like the Clearview example come to light earlier this year

27:06.960 --> 27:11.440
where I'm sure you're familiar with that. Do you want to share a little bit about, you know,

27:11.440 --> 27:17.120
kind of what you know about that one? Yeah, so Clearview AI was, you know, a group that actually,

27:17.120 --> 27:21.600
you know, was really, it was really great reporting by the New York Times to actually identify

27:21.600 --> 27:25.600
that group and really expose them because they were, they were sort of intent on being sort of

27:25.600 --> 27:31.680
this covert under the radar stealth company for a very long time. But what they did was they looked

27:33.280 --> 27:39.040
they kind of collected social media face data. So they were, they sort of did something that I'm

27:39.040 --> 27:42.800
reflecting on as sort of digital surveillance where they would, you know, if you upload your face

27:42.800 --> 27:47.520
to Instagram, Facebook, they were collecting all of this information and using that.

27:47.520 --> 27:51.120
Like swapping all public pictures, they can get there. Yeah, yeah.

27:51.120 --> 28:00.240
I mean, it was pretty egregious what they were. Yeah, like a cartoon villain type plot. Like,

28:01.440 --> 28:05.360
really awful. Yeah. And they were, and the worst part is that they were actually cooperating

28:05.360 --> 28:08.320
with law enforcement in different ways and pitching to law enforcement and different

28:08.880 --> 28:14.640
government agencies. So they were using that information to like identify either, they were

28:14.640 --> 28:19.200
using it to identify you online or to identify you through, you know, surveillance camera footage

28:19.200 --> 28:24.640
and other sort of sort of terrifying modes of surveillance. So it was kind of a situation where,

28:24.640 --> 28:28.400
you know, they would give that power to any authority figure that could easily abuse it,

28:28.400 --> 28:34.800
easily weaponize it against you. Yeah, I would agree that the law enforcement examples were the

28:34.800 --> 28:42.400
worst in terms of kind of mass potential harm. But there was also a total lack of of governance

28:42.400 --> 28:46.560
where like from what I remember from the New York Times article, like board members would say,

28:46.560 --> 28:52.400
hey, can you find this person for me or something like that? And they would. Yeah. It was, yeah,

28:52.400 --> 28:56.240
there was also a story that I had heard of. I'm not sure how like, true this is someone like,

28:56.240 --> 29:01.440
try to identify like employees trying to identify like personal like ex-girlfriends and stuff. And

29:01.440 --> 29:07.200
it's very hard. I remember reading that as well. Yeah. Yeah. Yeah. I was going to say there are,

29:07.200 --> 29:12.320
like there's, there has been reports in the past that kind of hinted at this kind of technology

29:12.320 --> 29:16.160
where, you know, ice would just show up at people's houses and people wouldn't understand how they

29:16.160 --> 29:22.240
found them. And then it was later revealed that there they were matching sort of facial recognition

29:22.240 --> 29:26.320
data of like, you know, we know that you look like this based off of whatever mugshot or whatever

29:26.320 --> 29:32.000
information we have from your visa or whatever and identifying your Facebook profile or identifying

29:32.000 --> 29:38.000
your your trace online that way. So yeah, it really is this dangerous technology that like empowers

29:38.000 --> 29:44.000
some of these institutional bullies to kind of just barge into people's lives and like really

29:44.000 --> 29:49.760
affect them. So it has these like kind of whimsical situations, but also just like very important

29:49.760 --> 29:56.000
kind of dire consequences as well. And we keep getting pulled into kind of these broader questions.

29:56.000 --> 30:02.320
I'm still very curious about the the auditing thing. One of the questions that occurs for me is

30:02.320 --> 30:15.680
I, well, I'm curious if you if you explored, I'm not sure the kind of the taxonomy of different

30:15.680 --> 30:22.880
stand types of standards, but when I think of standards like the ISO 9001, they're like process

30:22.880 --> 30:27.680
standards as opposed to like checklist standards. I don't know what the proper names for these are,

30:27.680 --> 30:34.000
but you know, strikes me that, you know, the analogy that comes to mind is like the Volkswagen

30:34.000 --> 30:40.080
gaming the emissions standards. Like they, you know, they had the car set up so that when they

30:40.720 --> 30:46.560
learned that they were in, you know, being tested like it, it switched the engine to

30:46.560 --> 30:52.640
more environmentally friendly mode. But then the regular mode was, you know, just doing what it

30:52.640 --> 31:01.200
was doing through. Yeah. And in this case, there's, I don't think there's like, well, is there an

31:01.200 --> 31:08.720
objective state of, you know, a facial, a good facial recognition system or a facial recognition,

31:08.720 --> 31:14.400
separate from that question of the use of facial recognition and should exist and all that kind

31:14.400 --> 31:23.120
of stuff, there's not really an objective, all encompassing measure of, you know, goodness from

31:24.080 --> 31:31.040
diversity perspective, like there's all different things that you might want the system to be

31:31.040 --> 31:38.080
capable of and you have to engineer them. They're not emergent qualities. You have to engineer them

31:38.080 --> 31:46.880
in. Yeah. And so maybe the, you know, have you looked at this idea of like being around the process

31:46.880 --> 31:51.760
as opposed to, you know, checking off boxes at the end of the process? Oh, yeah. Yeah. This is

31:51.760 --> 31:58.000
like a huge part of my current work. So yeah, just to like, because you said a lot of interesting

31:58.000 --> 32:03.040
things in there, like one thing around standards is that you're, you're totally right. I'm kind of

32:03.040 --> 32:07.600
curious as to, because not a lot of people are reading standards. I'm like, I don't know. I'm curious

32:07.600 --> 32:14.800
as to, you know, which standards are you reading, Sam, but in my, yeah, I've been, I've been looking

32:14.800 --> 32:19.520
a lot at this question of standards in facial recognition mostly because it comes up so much in

32:19.520 --> 32:27.200
policy. So a lot of policy bills will sort of either reference the National Institute of Standards

32:27.200 --> 32:33.040
and Technology in the US, which is sort of the governing body around establishing some of these,

32:33.040 --> 32:39.280
like metrics or these, these bars that need to be jumped over by industry players in order for

32:39.280 --> 32:44.560
them to be considered, you know, as potential vendors within, you know, the sort of

32:45.280 --> 32:50.080
space of working with government agencies. So the National Institute of Standards and Technology

32:50.080 --> 32:57.520
is really the key kind of industry indicator of the performance of your system. If you're kind of

32:58.880 --> 33:02.560
hoping to, you know, work with different governments or different official bodies,

33:02.560 --> 33:07.040
and they very recently, like literally within the last year, you know, citing our paper too,

33:07.040 --> 33:12.240
which was very exciting, they only very recently started evaluating performance across different

33:12.240 --> 33:17.040
demographic subgroups. And that just happened literally last year for the first time. So they,

33:17.040 --> 33:23.120
before that, they hadn't incorporated that into their understanding of, you know, assessment

33:23.120 --> 33:27.280
and evaluation. And I think like with the saving face paper, we were actually challenging them to

33:27.280 --> 33:32.640
go even further and to say, there's all of these other considerations within the process of

33:32.640 --> 33:38.560
how official recognition system, quote unquote, works. You know, can you really say it works if it's

33:38.560 --> 33:42.640
violating the privacy of millions of people and there, and there's no consideration for that,

33:42.640 --> 33:48.240
there's no privacy policy included or incorporated or reflected on. Can you say it works if there's

33:48.240 --> 33:53.440
no method of, you know, transparent communication around its deployment and its use case and there's

33:53.440 --> 33:58.960
no clear evidence of like ethical consideration? Like, is that a system that makes sense to even

33:58.960 --> 34:03.600
consider to use? So yeah, some of those questions, some of those more holistic questions,

34:04.320 --> 34:09.680
they're currently like not even in that kind of space of assessment or evaluation. Like at the

34:09.680 --> 34:13.840
moment, I think as far as it goes is kind of thinking about like, how easy is it to use this and

34:13.840 --> 34:18.240
incorporate this into an application? You know, that's kind of the, they're looking at it at a very

34:18.240 --> 34:24.880
sort of product level. So that's like missed, but ISO and IEEE and other groups have actually

34:24.880 --> 34:30.640
thought about facial recognition. Wef is also sort of proposed some ideas around sort of assessing

34:30.640 --> 34:38.000
facial recognition technology. Yeah, so they actually, they sort of put together like a working

34:38.000 --> 34:42.480
group and they put it together white paper on facial recognition assessment and there's other

34:42.480 --> 34:48.320
sort of think tanks that are attempting to kind of build frameworks for the evaluation or the

34:48.320 --> 34:54.480
assessment of this kind of technology because it's kind of this contentious, controversial tool.

34:55.520 --> 35:00.080
And people are trying to identify all the axes of concern and understand what you have to think

35:00.080 --> 35:05.520
about with respect to will it ever be okay to use this tool? So that's why I think our voices are

35:05.520 --> 35:11.520
in there. Yeah. I think the distinction I was trying to get at was one is a set of standards

35:11.520 --> 35:18.640
around the output of the the process. The other is a set of standards around the process itself.

35:18.640 --> 35:25.040
Like, you know, I can envision a standard that says that, you know, a compliant facial recognition

35:25.040 --> 35:32.240
system has to be developed in a company where there's, you know, some kind of mathematical

35:32.240 --> 35:37.440
review board project, you know, process and there's, you know, some percent of diversity on the

35:37.440 --> 35:44.320
team or something, some metric of diversity on the team that's working on it and it needs to,

35:44.320 --> 35:52.320
you know, be the database, the training data set has to have some set of qualities as opposed

35:52.320 --> 35:57.280
to the output, you know, this process through which it's developed. Yeah. So this was, this was

35:57.280 --> 36:02.160
something that we brought up in our saving face papers as well as a huge where a lot of the

36:02.160 --> 36:06.560
current standards, even the ones that are focused on privacy, like the ISO standard is very,

36:07.360 --> 36:11.600
you know, into this idea of like, hey, this is an identifiable biometric. It goes under all of

36:11.600 --> 36:15.680
these things that we have where identifiable biometrics and it can only be stored in this way

36:15.680 --> 36:19.680
and it needs to be encrypted in this way and they'll check the output. They won't check any process

36:19.680 --> 36:24.080
that you, like, they won't, like, the privacy policy of data collection, like, the way that you

36:24.080 --> 36:29.200
collect data. If that was completely unethical, like, they, they just care that the data is encrypted

36:29.200 --> 36:34.240
at the end, right? So that's the definition of privacy and it's very removed from these processes

36:34.240 --> 36:40.160
like you mentioned that if they were mandated would allow for kind of richer measures to be in

36:40.160 --> 36:45.440
place and richer guardrails to be in place. I worked on a paper with colleagues at Google

36:45.440 --> 36:51.600
of called closing the AI accountability gap. I had a funer name for it, but they all refused to

36:51.600 --> 36:59.280
let me. Are you bringing that up because you want to say the name? No, I'm not going to, I'm not

36:59.280 --> 37:04.560
going to, so I wanted, you know, I wrote it with a colleague named Andrew Smart. And I wanted to

37:04.560 --> 37:09.040
name the framework, the smarter framework because his name is smart. So I thought it was really funny.

37:09.760 --> 37:14.560
No one else thought it was funny. So we ended up naming it the smacked air framework, which is kind of,

37:14.560 --> 37:20.480
you know, it doesn't ring off the tongue as well, but Andrew Smart, I think, just didn't want that

37:20.480 --> 37:25.680
attention to like smart of the smarter framework was a little bit too much for him, but effectively,

37:25.680 --> 37:30.960
yeah, with that framework, we talk about sort of these, some of these procedural considerations,

37:30.960 --> 37:36.320
like that framework is pretty much us trying to say like, you know, and us using the approach of

37:36.320 --> 37:41.360
documentation to try to really identify all of these decisions that engineers make. So for

37:41.360 --> 37:46.000
example, you know, there might be someone that would have a facial recognition system where they

37:46.000 --> 37:50.800
would say, oh, this, you know, this system has no bias because we've evaluated it on like a

37:50.800 --> 37:56.240
gender-shaped style audit. You know, we have these different subgroups and according to whatever,

37:56.240 --> 38:01.760
you know, whatever taxonomy or whatever labels that we have, you know, the performance of the

38:01.760 --> 38:08.000
model is equal for group A and group B. So there's no issue. However, you know, with, you know,

38:08.000 --> 38:12.720
some of these, you know, with some of these process-oriented audits that we did at Google,

38:12.720 --> 38:17.440
we saw that, you know, perhaps the way that the data was collected was incredibly unethical,

38:17.440 --> 38:23.040
and that's where a lot of the issues arose. Or perhaps the way that the labels were set up.

38:23.040 --> 38:28.320
So the taxonomy of the labels and the way that, you know, because when you create a computer vision

38:28.320 --> 38:32.560
system, you actually set up the targets for the system, implicitly or explicitly. So you actually

38:32.560 --> 38:38.000
define, you know, this is the objective of what I want my model to do. And here are, you know,

38:38.000 --> 38:41.840
a set, you know, if I wanted to predict between a cat and a dog, I actually give it that label of

38:41.840 --> 38:47.600
cat and dog, and I select the images that represent cat and the images that represent dog. And that

38:47.600 --> 38:52.480
process at the moment is sort of seen very callously as like, oh, this is, you know, I scrape whatever

38:52.480 --> 38:57.520
I get from the internet, and that's what I use. But there's sort of this realization that even some

38:57.520 --> 39:02.240
of these very subtle engineering decisions that we don't like to admit to ourselves are actually

39:02.240 --> 39:07.600
very important. And there's ways that you can articulate, you know, goals for the model that are

39:07.600 --> 39:12.800
implicitly discriminatory, whether or not it performs well on different subgroups. You know,

39:12.800 --> 39:17.760
so that's some of those procedural sort of elements or some of those engineering decisions,

39:17.760 --> 39:22.640
even outside of some of the governance structures that you've mentioned around, has there been an

39:22.640 --> 39:27.280
ethical review word that looked through this? Or some of the questions that you were mentioning

39:27.280 --> 39:32.960
around diversity of the people involved, right? So or even consultation with the community or with

39:32.960 --> 39:37.840
the public. So some of those even some of those governance issues separate from those in the

39:37.840 --> 39:43.200
engineering process, even there's a last there's a loss of accountability. Like one of the most

39:43.200 --> 39:51.120
sort of surprising things for me is how little we understand, especially as, you know, a machine

39:51.120 --> 39:56.000
learning engineer, sort of a typical machine learning engineer, there's not a lot of accountability

39:56.000 --> 40:02.080
currently around, you know, data providence where I get my data from. So I can create a data set

40:02.080 --> 40:07.920
coming from anywhere and there's no sort of accountability with respect to where that data is

40:07.920 --> 40:12.160
collected from and what that actually represents in which world view that's coming from and which,

40:12.160 --> 40:18.960
you know, all these politics to that data source. And there's a great project that actually

40:18.960 --> 40:23.200
happened at AI now called excavating AI. And I've been talking a lot about it because I think it

40:23.200 --> 40:29.280
does a good job discussing sort of the politics of like, you know, which labels that you pick and,

40:29.280 --> 40:34.560
you know, where your data sources are coming from and the ethics of that as part of, you know,

40:34.560 --> 40:39.520
as an integral part of the ethics of the entire development system of the model. Yeah. So that

40:39.520 --> 40:45.600
work was us trying to get people to write some of that stuff down. So at minimum, we can start talking

40:45.600 --> 40:57.040
about it. Yeah. So you've the risk of going on another. It's all good tangent divergence rabbit hole.

40:57.040 --> 41:06.160
You mentioned the decisions that machine learning engineers make, accountability in the engineering

41:06.160 --> 41:15.920
process, reminding me of the recent kind of thread with Jan LaCoon where he essentially, at

41:15.920 --> 41:23.840
least the part that I'm referring to kind of absolve research of any responsibility for bias in AI

41:23.840 --> 41:28.640
and said it's, you know, related to the things that that you just talked about, you know,

41:28.640 --> 41:35.040
engineering process and discipline and, and the like, does that mean that you agree with his take on?

41:39.360 --> 41:49.040
It was bait. I didn't realize I came to be baited like that. No. That was great. No. I think there's

41:49.040 --> 41:55.360
responsibility on all sides. So one of the reasons why I, I personally gravitate towards engineering

41:56.400 --> 42:00.720
decision making and accountability with respect to engineering decision making is because

42:01.440 --> 42:06.480
through documentation is because I understand that sometimes actually engineers do not understand

42:06.480 --> 42:11.280
their sense of responsibility and do not have the resources to support them in communicating about

42:11.280 --> 42:17.440
it. But a lot of the things that we've worked on with respect to, you know, you know, action

42:17.440 --> 42:21.920
closing the AI accountability gap, but also in earlier work, the earlier work on model cards

42:22.800 --> 42:27.040
and, you know, related work in like data sets, data sheets for data sets and other projects.

42:27.680 --> 42:32.240
All of that has been widely used by the research community before anyone else. Like the research

42:32.240 --> 42:38.480
community, especially applied machine learning work, including computer vision and natural

42:38.480 --> 42:44.240
language processing, you know, a lot of that work involves the engineering of these models.

42:44.240 --> 42:49.840
You know, and I recently like rage tweeted about this where it is so strange to me that

42:49.840 --> 42:54.720
you on the concept that because he literally works at an industry lab where they works, they,

42:54.720 --> 43:01.360
a lot of the work that fair does is, you know, a step towards productization and a lot of the

43:01.360 --> 43:05.280
models that they build, especially some of these larger models, all of these industry labs,

43:05.280 --> 43:10.320
a lot of the models that they build are models that other companies and other groups build off of.

43:10.320 --> 43:15.040
So they will sort of build these quote unquote general models, curate the data set required

43:15.040 --> 43:20.960
to train these large immense models that are then kind of fine tuned by different groups using

43:20.960 --> 43:27.200
that model for different purposes. But that idea of building this general model for whatever

43:27.200 --> 43:33.200
purpose is a lot of control. It's a huge engineering decision. It's applied science effectively.

43:33.200 --> 43:38.400
So it's so strange to me that he thinks the best that research process is separate from

43:38.400 --> 43:43.760
sort of the engineering step and the applications of that, yeah, of that work.

43:44.400 --> 43:49.600
The other thing too is, you know, there's a whole separate set of issues connected to research,

43:49.600 --> 43:55.760
I think. The problems that you choose to work on in research, the way that you test systems

43:55.760 --> 44:01.760
and research really sets the precedent for the field in a way that I'm not sure he acknowledges

44:01.760 --> 44:06.400
with his response. He kind of downplays. So, you know, with a specific example that I think

44:06.400 --> 44:11.840
we're thinking of, it was this model that depixelized faces. So, you know, take

44:11.840 --> 44:17.600
face faces and attempt to reconstruct it. And the my main issue with that work or one of the big

44:17.600 --> 44:22.720
issues with that work, there's many concerns. But one issue is that they didn't seem to test

44:22.720 --> 44:28.000
for people of color. There were multiple examples of people depixelizing the faces of people of color

44:28.000 --> 44:33.600
and then those people being reconstructed to look Caucasian. And it was just very clear that because

44:33.600 --> 44:37.520
it's not mentioned at all in the paper and it's not mentioned by the creators until it kind of

44:37.520 --> 44:43.680
blew up on Twitter. You know, the lack of testing for people of color continues to be this

44:44.640 --> 44:49.840
theme of, it's sort of procedural negligence where it's like, why how could you not,

44:50.480 --> 44:56.160
like, people of color exist, you know, how could you not evaluate or assess for this particular

44:56.160 --> 45:00.160
group that is definitely going to be part of the people, the group of people affected by this

45:00.160 --> 45:06.800
tool. So, I think him downplaying the severity of that oversight was kind of is one of the main

45:06.800 --> 45:12.400
reasons I don't agree with him. But also, I think there's, there's multiple layers of issues there.

45:16.400 --> 45:25.200
Yeah. I don't fully get the, I don't fully get why it's so important to some people to

45:25.200 --> 45:35.840
distinguish between algorithms being biased and data sets being biased. Why is that the hill

45:35.840 --> 45:44.240
that we have to die on some hill? No, but I was going to say, like, I think that it comes from a

45:44.240 --> 45:52.320
place of like really not wanting to making sure that people understand that there's not a quick fix

45:52.320 --> 45:57.600
to this. So, you know, we heard for a long time, data is all we need, you know, we just need more

45:57.600 --> 46:03.600
data, we need more diverse data. In which case, I do not have to think about, you know, diversity or

46:03.600 --> 46:08.960
ethics at this moment because I'm just creating a prototype and the next iteration, I can think

46:08.960 --> 46:13.520
about diversity and I can think about bias and I can think about data because this is just a data

46:13.520 --> 46:17.760
issue. But there's been a lot of great work. And I think you just kind of triggered the entire

46:17.760 --> 46:23.280
sort of fairness, machine learning fairness community. It was fascinating to watch.

46:26.320 --> 46:31.920
And we'll link for the, the tweet and threads. I think that would be, you know,

46:32.400 --> 46:42.160
as well as my pixelated. There's one of you. There's one of me. Yeah, Robert Ness did it for me

46:42.160 --> 46:50.480
and gave me hair. Oh, gosh. Someone said it. Someone said the output had me looking like Steven

46:50.480 --> 46:56.560
Seagal. No, actually, no, I said that. Someone says some other someone's TV that actually Steven

46:56.560 --> 47:04.320
Seagal. Oh, my gosh. Yeah. And it's just so silly to me that like, yeah, that it's sort of

47:04.320 --> 47:07.520
something that's discovered on Twitter. In the same way that it was silly to me that, you know,

47:07.520 --> 47:12.560
gender shades is something that is happening so recently. And it's this new discovery of like,

47:12.560 --> 47:16.640
you know, when you test on people of color or women of color, some of these things don't work.

47:17.120 --> 47:22.960
I think that's for me that continues to be sort of this recurring frustration is like, how can

47:22.960 --> 47:27.920
you not like see this group of people that clearly exist and not evaluate before it's on them?

47:28.800 --> 47:36.320
It's a silly example. Yeah, it sounds like you're saying that this, this, it's not the algorithm,

47:36.320 --> 47:42.480
it's the data set is essentially, you know, either, you know, saying it's someone else's problem

47:42.480 --> 47:47.120
or kind of kicking the can down the road. That's why it keeps coming up. Yeah, because people

47:47.120 --> 47:53.040
had designed algorithms. They actually do influence some of these outputs. They do influence like

47:53.040 --> 47:57.760
the fairness of some of these outcomes. And with respect to this particular work, there was

47:57.760 --> 48:02.480
an interesting thread that didn't get as much traction, but there's another thread of

48:02.480 --> 48:08.320
someone that had tried alternate approaches to the problem. And he had actually gotten sort of more

48:08.320 --> 48:14.320
faithful representations reconstructed. And it was because like so his, his represent,

48:14.320 --> 48:18.480
when he reconstructed his faces, they were not all Caucasian looking, they were all white looking.

48:19.120 --> 48:22.640
And he had taken just a different algorithmic approach to the problem.

48:22.640 --> 48:33.200
Well, they'm underlying data set the face HQ data set, but actually cared to. Yeah, or just even

48:33.200 --> 48:38.160
took a different approach and got different results, right? Like it wasn't even, it was more of that

48:38.160 --> 48:43.360
heat. It was not fixing the problem. It was just, he was approaching the problem, understanding

48:43.360 --> 48:47.520
that like black people exist. And we should test for that as well. And if it doesn't work for this

48:47.520 --> 48:52.160
group, and you know, if your black faces end up looking like white people, that is not a functional

48:52.160 --> 48:56.640
product that you can use for black people. Like no black person can use that product. And I think

48:56.640 --> 49:00.880
that for, like because that was already at the forefront of the discussion, some of these like

49:00.880 --> 49:05.360
counter proposals of these other algorithmic methods that were that kind of preserved the black

49:05.360 --> 49:10.400
faces to put it lightly or even other people are to their Asian faces as well that it wasn't

49:10.400 --> 49:17.920
working for Hispanic faces. Yeah, so people that had kind of come in with this use case in mind.

49:17.920 --> 49:21.600
We're sort of discussing and exploring alternative approaches that I kind of saw some of their

49:21.600 --> 49:26.960
preliminary results. And I was like, yeah, that like didn't completely morph the face into like,

49:26.960 --> 49:32.880
you know, Adam Sandler, whoever. So I think like, yeah, there's definitely a role of algorithmic

49:32.880 --> 49:37.280
approaches. And this is like a well study thing. You know, there's, there's so much literature on

49:37.280 --> 49:44.480
this and a lot of people bombarded y'all looking with that literature last night. It's crazy

49:44.480 --> 49:48.320
because I literally feel like this happened a week ago, but I was like, no, that all happened

49:48.320 --> 49:57.280
last night. But yeah, it was kind of good to see some of these papers come out too because so many

49:57.280 --> 50:01.760
people have so many misconceptions about how bias works. Like we often get the responses like,

50:01.760 --> 50:08.320
oh, isn't it just the data? It's like, no, there's so many layers to this. I think it was a good

50:08.320 --> 50:13.840
prompt for people to reopen the discussion of like just how complex that question of bias is

50:13.840 --> 50:20.800
and just how hard fairness is as a problem as a problem space. And how many different factors

50:20.800 --> 50:28.240
kind of like lead to it. Yeah. I said I wanted to come back to the topic of the broader topic of

50:28.240 --> 50:35.200
facial recognition and we probably should do that if we're going to get to it. You know, tell me,

50:36.560 --> 50:41.440
I'm just curious kind of your perspective on that kind of from the beginning. Like does it

50:41.440 --> 50:46.800
start with, I'm imagining it starts with the potential for harm and what those harms are. And

50:46.800 --> 50:52.880
maybe you can kind of talk through what you've seen. Yeah, so just like in terms of broader issues

50:52.880 --> 50:58.640
of facial recognition. Right. And you know, if the question that, you know, we want to put on the

50:58.640 --> 51:04.240
table or talk through right now is like, should facial recognition be on the market? Oh, I see,

51:04.240 --> 51:09.120
yeah. Right. You know, where does the answer to that question start? Yeah. So I think like a lot

51:09.120 --> 51:16.400
of the work that a lot of the work that gender shades did I think was break the myth that facial

51:16.400 --> 51:21.680
recognition worked because I think for a long time the debate was, oh, this system already works,

51:21.680 --> 51:27.520
you know, do we want it to be enabled within society as this surveillance tool as the system of

51:27.520 --> 51:32.800
control, as the system that can be weaponized, you know, debates around what does it mean to be safe,

51:32.800 --> 51:36.480
what does it mean to have a tool that promotes or discourages safety? What does it mean to have

51:36.480 --> 51:41.680
different authority figures in charge of this tool? What does it mean to allow or restrict specific

51:41.680 --> 51:46.560
use cases? So something like gender shades just broke the myth that it worked in the first place.

51:46.560 --> 51:51.920
And I think that was an important myth to break because this is very immature technology. So,

51:51.920 --> 51:57.200
you know, currently, there's a lot of hype around people, you know more than anyone. There's a lot

51:57.200 --> 52:02.000
of hype. There's a lot of hype around machine learning. There's a lot of hype. Yeah, I did a

52:02.000 --> 52:07.440
little bit, a little bit. And it's really important to have some of these audits come in to say like,

52:07.440 --> 52:11.840
wait, actually, it doesn't work for this group of people. Wait, actually, it's really biased

52:11.840 --> 52:18.400
in discriminatory in this particular way. And it opens up the conversation for future reflections

52:18.400 --> 52:24.240
of, wait, not only is it biased, but also there's these privacy issues. Oh, not only is it biased

52:24.240 --> 52:29.200
and are there privacy issues, but there's also very specifically concerning use cases that we need

52:29.200 --> 52:34.960
to pay attention to. And maybe, you know, the benefit is not actually worth the harm. So like, when

52:34.960 --> 52:38.560
we start having these, we start with this place of weight, it doesn't actually work and it's

52:38.560 --> 52:43.280
an immature technology. And we move towards a place of like, oh, wait, there's actually, even if it

52:43.280 --> 52:47.040
did work, there's all of these other concerns. Like I said, when it doesn't work, there's issues,

52:47.040 --> 52:51.680
but there's also issues when it does work. So I'm starting with that place of like, wait, this

52:51.680 --> 52:58.160
is not this magical, you know, functional thing. Just breaks like the rose colored glasses kind

52:58.160 --> 53:02.560
of come off and people are much more comfortable questioning other aspects of the technology. And I

53:02.560 --> 53:08.400
think where we are today is that, you know, Amazon, Microsoft and Facebook, or that Facebook Plus,

53:08.400 --> 53:15.280
Amazon, Microsoft and IBM have all kind of very recently come out publicly to say, there are

53:15.280 --> 53:20.240
clear limitations of this technology. We have been confronted with the facts. But also, you know,

53:20.240 --> 53:26.000
we understand this concern and we now resonate with it in the, in the current context of sort of,

53:26.800 --> 53:30.880
you know, racial injustice that we're seeing in this, in this country. So we understand that

53:30.880 --> 53:35.840
the risks are here and we understand that, you know, this technology is immature and not really

53:37.520 --> 53:42.560
ready for market. So I think that's why that idea of a moratorium of like, let's take it off of

53:42.560 --> 53:47.840
the market while we have these conversations around regulation. So like, you know, proper restriction.

53:48.560 --> 53:53.040
But also disclosure, like, you know, if any kind of agency is using facial recognition,

53:53.840 --> 53:59.920
how can we empower sort of members of society and the community to like be part of that process

53:59.920 --> 54:03.520
and to understand when facial recognition is being used on them? Because right now,

54:04.240 --> 54:11.040
we don't know the extent to which, you know, Georgetown's 2017 work looking at sort of the

54:11.040 --> 54:17.040
prevalence of facial recognition to use in the US, you know, by police, by law enforcement specifically,

54:17.040 --> 54:22.000
we don't even know about immigration and other groups. So, you know, that was so shocking to so

54:22.000 --> 54:26.240
many people that no one had any idea because it's a technology that someone can use on you without

54:26.240 --> 54:31.760
you having any clue. So, you know, can we actually enforce disclosure? Can we actually enforce,

54:31.760 --> 54:36.960
you know, some of this community participation? But also, you know, can we reconsider what it means

54:36.960 --> 54:42.640
for the technology to work? Is it just accuracy or do we actually have to understand that it has

54:42.640 --> 54:46.560
to work for different subgroups or do we actually have to start having conversations around,

54:46.560 --> 54:52.160
like you mentioned, process and privacy and all these other complex things? I think the work of

54:52.160 --> 54:57.680
exposing, you know, the bias is sort of like a good way to just expose the complexity of the

54:57.680 --> 55:03.920
technology itself and break that like myth of just like a perfectly functional kind of tool.

55:04.720 --> 55:08.400
And then once that myth is sort of broken, then people understand that like, oh, this is a

55:08.400 --> 55:13.440
Pandora's box. This is a very complex system. There's so many dimensions of concern here.

55:13.440 --> 55:18.240
We need to be way more careful than we are currently being around about it. And as a result,

55:18.240 --> 55:24.400
you know, maybe it's not ready for it's not ready to be so widely used, you know, right now it's

55:24.400 --> 55:28.880
used in a lot of places. So, maybe we should just be more careful and should just pull it off the

55:28.880 --> 55:35.040
market as we have these deeper, longer conversations about sort of the complexity of what it is.

55:35.040 --> 55:40.240
Yeah. So, that's sort of how I approach kind of the facial recognition issues in general. Yeah.

55:40.240 --> 55:48.960
Okay. Now, if I parse those recent announcements and remember them correctly, IBM's announcement

55:48.960 --> 55:53.520
was the broadest of the ones that I remember seeing. They said that they were going to stop

55:53.520 --> 55:59.120
developing facial recognition technology for, I don't know if there was a time frame associated

55:59.120 --> 56:09.520
with that or if it was indefinite. Yeah. Amazon, on the other hand, it was a fairly restricted

56:09.520 --> 56:15.120
moratorium on the sale of facial recognition to law enforcement agencies. Yeah.

56:16.000 --> 56:22.880
And I don't recall Microsoft's the scope of their announcement.

56:22.880 --> 56:27.360
Your announcement. It was very vague. It was very vague on purpose, I think.

56:29.920 --> 56:34.480
Yeah, I'm also trying to figure out what Microsoft's announcement was about.

56:34.480 --> 56:41.120
Having read it several times. Yeah. And watch the video. I'm still trying to figure out what

56:41.120 --> 56:48.960
Brad Smith was saying, but yeah, but they all kind of made broad sweeping statements in different

56:48.960 --> 56:56.080
ways. I think most shocking was Amazon because they had been so stubborn, especially with us,

56:56.080 --> 57:04.160
they had sort of, yeah, they had sort of, you know, one of our second paper came out and we had

57:04.160 --> 57:10.640
kind of called out Amazon and shattered their rose colored glasses to like, you know, how functional

57:10.640 --> 57:20.160
their situation was. We had to face a lot of, a lot of, I guess, like, aggressiveness coming from

57:20.160 --> 57:25.920
them. But they posted a blog post. A fairly flawed blog post. Yeah.

57:25.920 --> 57:33.920
Very quick kind of like off the cuff, like, yeah, like, I feel like it was just him being angry

57:33.920 --> 57:38.400
and just wrote a couple paragraphs and put it out. But also, and subsequent interviews too,

57:38.400 --> 57:43.920
you know, I read a lot of articles that coded him. And this is something that came up in

57:45.200 --> 57:50.080
the documentary that we recently, there's a recent documentary that I'll give me just this

57:50.080 --> 57:56.480
week sort of released, coded bias. And there's a scene of like me, Joanne Timnett, who's the co-author

57:56.480 --> 58:02.560
for Gender Shade. And we were sort of sitting there talking about this blog post. And I remember

58:02.560 --> 58:07.040
one of the things that struck me about that conversation that always remembers like us, we were like,

58:07.040 --> 58:12.240
we worked for like months, we wrote this paper, it passed peer review, all of these things happened

58:12.240 --> 58:16.480
in order to validate our results. We had like, you know, so many supplementaries that we like,

58:16.480 --> 58:20.800
because we wanted to make sure that our results were sort of validated and could stand up. And

58:20.800 --> 58:26.640
this guy writes like a overnight, like, blog post with zero citations. And like, you know, in the

58:26.640 --> 58:31.040
press articles, they're quoting these things as if they're like equivalent sort of rebuttals. And

58:31.040 --> 58:38.400
we're like, what? But do you have a sense for? So yeah, that was that was my frustration with Amazon.

58:38.400 --> 58:45.200
But they, I think you have a sense for what drove, you know, them to respond in that way. I mean,

58:45.200 --> 58:50.160
IBM and Microsoft and others were kind of confronted with the same realities. And I said, okay,

58:50.160 --> 58:53.920
yeah, this is probably pretty bad. Let me, you know, let's do something about that. But Amazon

58:54.720 --> 59:02.640
resisted that. Yeah, I suspect, I suspect this happened for a couple of reasons. One being that

59:02.640 --> 59:07.840
ACLU, so I'm not, you know, I'm not privita exactly what's going on internally at Amazon. But

59:07.840 --> 59:14.560
HTMLU had a couple months before the summer before our paper came out, released a couple reports

59:14.560 --> 59:19.120
that they had found of Amazon attempting to pitch at the time. They were in the process of

59:19.120 --> 59:23.680
trying to pitch their technology to ICE to, you know, different intelligence agencies to different

59:23.680 --> 59:30.080
law enforcement agencies. Also, you know, Amazon through their ring product. So ring is sort of the

59:30.080 --> 59:34.800
smart doorbell product. Sports are available in storebell where they'll sort of monitor your

59:34.800 --> 59:40.560
porch. They were trying, they were thinking of the idea of implementing sort of facial recognition

59:40.560 --> 59:45.520
and Amazon recognition to help process the footage from these ring products. And they had a lot of

59:45.520 --> 59:51.280
partnerships with, you know, thousands to the order of over 3000, I think police police departments

59:51.280 --> 59:57.600
at the time. So for them, it was sort of at the cost of this very like promising economic

59:57.600 --> 01:00:02.320
opportunity. We had kind of just like clip their wings a little bit by revealing the fact that

01:00:02.320 --> 01:00:07.680
their technology fell short for for people of color, especially, you know, darker darker skin

01:00:07.680 --> 01:00:14.160
women. So by just demonstrating and questioning the functionality of that product, we really sort

01:00:14.160 --> 01:00:20.800
of threw the whole, threw the whole product into sort of this period of like people really truly

01:00:20.800 --> 01:00:25.360
questioning and revisiting like does this thing actually work? Oh, but also privacy. Oh, but also

01:00:25.360 --> 01:00:29.440
surveillance. And like it kind of just was like the, it's always the tip of the iceberg to this

01:00:29.440 --> 01:00:33.840
larger conversation. And I think they understood that because that had, that's what had happened

01:00:33.840 --> 01:00:39.280
for Microsoft and for IBM. So I think that was why they were super defensive initially.

01:00:40.000 --> 01:00:44.720
But thankfully, you know, the research community really came out to support us. They wrote this

01:00:45.280 --> 01:00:50.080
public letter. There was a lot of press around that letter where they had kind of just refuted all

01:00:50.080 --> 01:00:55.360
the like things that Matt Woodson said in his blog post, but also, you know, other statements

01:00:55.360 --> 01:01:02.400
by Amazon later on. And it kind of ended up in a place where they conceded that we needed policy.

01:01:02.400 --> 01:01:06.880
They sort of kind of made this appeal of like, okay, so regulate us, which is a similar appeal

01:01:06.880 --> 01:01:12.080
to what Microsoft had said. And the difference between kind of those earlier appeals, you know,

01:01:12.080 --> 01:01:18.240
right at the publications of our paper, there's always this response of, oh, we support regulation

01:01:18.240 --> 01:01:21.840
in facial recognition. You know, this is what Amazon said. This is what IBM said in St.

01:01:21.840 --> 01:01:27.360
Microsoft early on. And the difference between that stance and the current stance is that I think

01:01:27.360 --> 01:01:34.240
currently, they now understand or they're now sort of pressured to understand that, you know,

01:01:34.240 --> 01:01:39.200
you can't just say that you support facial recognition policy and that you acknowledge the

01:01:39.200 --> 01:01:45.120
concerns without while also having the product on the market. So, you know, if you say that you

01:01:45.120 --> 01:01:49.840
care about the concerns that have been brought up around your technology and that you support the

01:01:49.840 --> 01:01:55.200
development of policy and regulation for facial recognition, you can't keep selling, you know,

01:01:55.200 --> 01:02:00.080
that technology at the same time. So, you know, the recent announcements around we are no longer

01:02:00.080 --> 01:02:05.680
sort of selling this technology, we're no longer allowing it to be used. It's a step forward to

01:02:05.680 --> 01:02:11.280
say that, you know, we recognize that until there is some greater understanding and that work has

01:02:11.280 --> 01:02:16.640
been done, you know, we really should not keep promoting this technology or keep deploying it.

01:02:16.640 --> 01:02:26.400
Yeah. Yeah, their response for a long time was, I don't even, I'm trying to remember if they

01:02:26.400 --> 01:02:31.920
were saying it was in their terms of service, but or, you know, in general, they were saying that,

01:02:31.920 --> 01:02:36.400
you know, no law enforcement agencies were making decisions based on this

01:02:37.360 --> 01:02:43.040
recognition law and it was, you know, people were making the decisions. And I'm curious,

01:02:43.040 --> 01:02:49.920
you know, what you've seen in terms of kind of the failure mode of that, you know, rationale

01:02:49.920 --> 01:02:56.240
and, you know, or, you know, specific examples of, you know, have there been well-publicized

01:02:56.240 --> 01:03:03.520
examples of, you know, the use of facial recognition technology and policing for harm.

01:03:04.320 --> 01:03:08.800
Oh, for harm? You know, that resulted in harm, like the current state of that.

01:03:08.800 --> 01:03:17.040
Yeah, I think most cases resulted some harm. There's a great, there's a great report from Georgetown

01:03:17.040 --> 01:03:22.000
law. I think Georgetown law has done a great job tracking the use of facial recognition in law

01:03:22.000 --> 01:03:27.200
enforcement. And honestly, just exposing some of the malpractice that happens in that space.

01:03:28.400 --> 01:03:33.520
I think one of the, you know, more striking examples is the way that law enforcement and this

01:03:33.520 --> 01:03:39.200
was reported on multiple occasions, you know, law enforcement will try to use facial recognition

01:03:39.200 --> 01:03:45.760
in order to identify as suspects in video footage, right? So they might, you know, have a crime

01:03:45.760 --> 01:03:52.400
happen. I get some footage of the video scene and then attempt to match, you know, faces in their

01:03:52.400 --> 01:03:58.000
mugshot database with, you know, whatever they can identify from the footage. Or, you know,

01:03:58.000 --> 01:04:03.760
the more kind of scary situations, they'll take, you know, sketch descriptions of a face and try to

01:04:03.760 --> 01:04:08.800
match that with mugshots in their database. You know, or other sort of iterations of, you know,

01:04:08.800 --> 01:04:14.240
insane, you know, there was one reported case from Georgetown of them photoshopping like a celebrity

01:04:14.240 --> 01:04:19.840
face kind of because they were told that the suspect looked like, you know, this particular

01:04:19.840 --> 01:04:23.760
celebrity. So they're like, okay, let's see like in our mugshot database, if we can try to match,

01:04:23.760 --> 01:04:28.320
you know, this celebrity face, like an arbitrary celebrity space, you know, to, you know,

01:04:28.320 --> 01:04:33.440
somewhat in our database. And, you know, they, they definitely, and this is a huge element of it as

01:04:33.440 --> 01:04:39.520
well, you know, the idea of how these, how the technology is actually being used is another layer

01:04:39.520 --> 01:04:45.520
of concern to another dimension to think about because Amazon in their rebuttal to us always talks

01:04:45.520 --> 01:04:51.120
about the idea of thresholds, you know, we want us, you know, we tell our clients to use a 99,

01:04:51.120 --> 01:04:55.760
so certainly with a 95% threshold. And then when the press got worse, they were like, oh,

01:04:55.760 --> 01:05:01.920
actually, we met a 99% threshold, a 99% threshold for all groups. And then if you do that,

01:05:01.920 --> 01:05:07.520
then there's no more bias. And then there was a great reporter at Gizmodo that actually went to

01:05:07.520 --> 01:05:11.440
investigate and talk to one of their police clients. And the police client was like, what is,

01:05:11.440 --> 01:05:18.160
what is a threshold? We don't know what that is. And mind you, the default is 80%. This is like

01:05:18.160 --> 01:05:23.040
the confidence threshold to make a prediction. So, yeah, there's so many ways in which, you know,

01:05:23.040 --> 01:05:28.480
this technology is being incredibly misused by, you know, different police clients in ways that

01:05:28.480 --> 01:05:34.000
are problematic that just it's not built to be used as. And then there's sort of the other case

01:05:35.200 --> 01:05:40.880
of the technology being explicitly weaponized. So one situation that I think of often is the

01:05:40.880 --> 01:05:48.800
Atlantic Tower sort of Plaza is, you know, a rent-controlled rent-stabilized building in Brooklyn. And

01:05:48.800 --> 01:05:55.520
the residents of that building recently protested the landlords installation or applications

01:05:55.520 --> 01:06:00.800
to install facial recognition in the building. And if you like sort of study the details of that case

01:06:00.800 --> 01:06:06.960
and discuss with the tenants, it becomes clear that their concern is not just around sort of the

01:06:06.960 --> 01:06:11.840
discriminatory performance of these systems or even, you know, the level of privacy. But the fact

01:06:11.840 --> 01:06:16.560
that their landlord is, they know that their landlord is trying to evict certain people because

01:06:16.560 --> 01:06:21.040
it's rent-controlled. So he can kind of raise the rent if he kind of gets rid of certain people

01:06:21.040 --> 01:06:26.480
and brings a new tenants. But also he kind of has a history with these tenants of, you know,

01:06:26.480 --> 01:06:32.720
harassing them in different ways and over-modeling them and attempting to track their movements

01:06:32.720 --> 01:06:38.800
and kind of jeopardize, kind of paradoxically jeopardize their safety by virtue of installing

01:06:38.800 --> 01:06:44.080
all of this surveillance tech. So they understood that the technology was not for their own safety,

01:06:44.080 --> 01:06:48.320
but for the sake of this landlord to be able to kind of weaponize its use to monitor them,

01:06:48.320 --> 01:06:53.920
to control their actions, to threaten them and, you know, threaten their safety and their

01:06:53.920 --> 01:07:00.320
security, their home security. So it was an interesting situation of like not even the functionality

01:07:00.320 --> 01:07:04.720
of facial recognition being part of the conversation, but just the conversation of like, wait,

01:07:04.720 --> 01:07:10.000
this is a tool where, you know, one person has a lot of identifiable biometric information about

01:07:10.000 --> 01:07:14.080
you and, you know, this authority figure can choose to use that for good or they can choose to

01:07:14.080 --> 01:07:21.360
manipulate that and really, you know, weaponize that against you. So yeah, there's sort of those

01:07:21.360 --> 01:07:25.680
situations that come up with respect to how facial recognition is used is like the client that

01:07:25.680 --> 01:07:30.640
doesn't understand how to use it and messes up in a way that hurts people. The technology not even

01:07:30.640 --> 01:07:35.440
working and being sort of mature to actually do its job and then sort of the situation of

01:07:36.800 --> 01:07:42.240
the concern being around the authority figure not being very trustworthy and we're in a current

01:07:42.240 --> 01:07:46.880
sort of state of society where not a lot of people trust the police right now. There's a lot of

01:07:46.880 --> 01:07:52.640
questioning of, you know, the police authority and sort of the validity of some of these,

01:07:52.640 --> 01:07:56.720
some of these groups that, some of these authority figures that actually currently have a lot of

01:07:56.720 --> 01:08:01.440
the access to facial recognition today. So people are really beginning to question, you know,

01:08:01.440 --> 01:08:05.760
what does it mean for us to build these tools and put it in the hands of, you know, certain authority

01:08:05.760 --> 01:08:11.120
figures that we're now questioning that we now don't trust this easily. So yeah, lots of very

01:08:11.120 --> 01:08:15.040
complicated questions with respect to, you know, how it ends up hurting people in the end.

01:08:15.040 --> 01:08:24.960
Yeah, it makes me think of some of the work that Aviva Barhan is doing around trying to shift

01:08:24.960 --> 01:08:32.320
the frame of reference from, you know, where is this possibly working to, you know, who is

01:08:32.320 --> 01:08:40.480
this possibly harming and using that as the, using that as kind of the core question that we're

01:08:40.480 --> 01:08:50.160
asking. Yeah, no, I totally, I totally agree with that shift and I actually, I've been like telling a

01:08:50.160 --> 01:08:54.960
lot of people about, you know, because there's this whole community in AI and the machine learning

01:08:54.960 --> 01:08:59.440
community about like AI for social good, right? So it's a space where there's sort of this active

01:09:00.080 --> 01:09:04.640
imagination or imagining of like positive use cases and it's kind of this exploration of

01:09:04.640 --> 01:09:11.120
different positive applications of AI. And I've been sort of pushing my idea, which, you know, it

01:09:11.120 --> 01:09:15.920
might get some pickup soon of this idea of AI for social bad, where like, like we need to actually

01:09:15.920 --> 01:09:21.680
understand, you know, how it hurts people and we need to like taxonomize, you know, the harms

01:09:21.680 --> 01:09:27.840
that come up, we need to, we need to really reflect on some of these downstream consequences and

01:09:27.840 --> 01:09:33.440
like build a vocabulary for, you know, all the ways things can go wrong and all the things impact

01:09:33.440 --> 01:09:38.000
society. Yeah. Yeah, I root that. I think, you know, part of my question earlier around the

01:09:38.000 --> 01:09:46.800
examples is trying to get at that. And, you know, when I think about, um, I guess I, you know,

01:09:46.800 --> 01:09:56.000
I haven't, I haven't seen a kind of broadly publicized like pro-publica compass version of

01:09:56.000 --> 01:10:01.280
facial recognition and law enforcement, you know, I mean, and I think that that, you know,

01:10:01.280 --> 01:10:05.440
we've seen kind of the, the implication I can't tell you how many times I've seen pro-publica

01:10:05.440 --> 01:10:10.640
compass, you know, images and slides and things like that. You know, I think we need those kinds of

01:10:10.640 --> 01:10:15.120
examples so that people understand what the, what the challenges are. Yeah, applications are,

01:10:15.120 --> 01:10:19.280
yeah. I would, I would really recommend to anyone that's interested in that to check out George

01:10:19.280 --> 01:10:25.120
Town Law's work on this because I think they're probably the closest to identifying, you know, the

01:10:25.120 --> 01:10:31.520
pro-publica, um, the pro-publica article was really compelling because they had against kind of selected

01:10:31.520 --> 01:10:38.320
a very specific target and they were able to recreate the situation using very specific examples

01:10:38.320 --> 01:10:42.880
and, and pull everything together in a beautiful story. Um, and I think the George Town Law

01:10:42.880 --> 01:10:48.800
work with respect to looking at some of these, um, real world applications raises up important

01:10:48.800 --> 01:10:53.200
questions and maybe what we need is to anchor that to a narrative that we can resonate with, like,

01:10:53.200 --> 01:10:57.680
you know, very specific example or very specific tool that's being used, a very specific police

01:10:57.680 --> 01:11:03.360
department. Um, but I would encourage anyone that's, that's curious to understand how facial recognition

01:11:03.360 --> 01:11:08.400
is being used in law enforcement to check out that work. Um, but if they're kind of looking for

01:11:08.400 --> 01:11:12.400
that story, you're right, that's something that, like, it still work to do with respect to telling

01:11:12.400 --> 01:11:17.920
that narrative of how facial recognition kind of interfaces with some of these real world, um,

01:11:17.920 --> 01:11:22.560
uh, uh, uh, institutes and structures. The other thing I'm going to say is like, it's not just law

01:11:22.560 --> 01:11:28.960
enforcement and this is something that is hard for people to understand because it's never communicated

01:11:28.960 --> 01:11:34.320
to us when facial recognition is used on us. But, um, you know, for example, higher view is a company

01:11:34.320 --> 01:11:39.200
that uses facial recognition as part of their scoring system. You know, they'll evaluate or assess

01:11:39.200 --> 01:11:43.840
for a different emotional cues to different questions and have that be part of your scoring system

01:11:43.840 --> 01:11:49.600
to get a particular job. So like in a recorded interview, such as this one, um, or, uh, you know,

01:11:49.600 --> 01:11:56.800
there's a lot of cases that I personally encounter an immigration of, you know, different, uh, you know,

01:11:56.800 --> 01:12:03.440
a lot of, a lot of the impetus for facial recognition becoming a field was because, you know, um,

01:12:04.160 --> 01:12:10.480
in like 1996, there was a push by the government in the US to, to literally funnel millions of

01:12:10.480 --> 01:12:15.920
dollars, uh, to sort of, uh, propel the, the community forward, like the research community and kind

01:12:15.920 --> 01:12:19.840
of build a research community. And they're incentive at the time, you know, a lot of the sponsors for

01:12:19.840 --> 01:12:26.080
that initial effort was Homeland Security, different intelligence agencies. So, um, it's a huge

01:12:26.080 --> 01:12:31.680
part of the kind of processes for immigration for verifying identification of, and I think

01:12:31.680 --> 01:12:36.240
probably a lot of people as part of the immigration pipeline, you know, when you're at an airport

01:12:36.240 --> 01:12:40.560
to match a passport to your face, they go through a process of verification through facial

01:12:40.560 --> 01:12:46.080
recognition. Um, and like that's already kind of been rolled out for a while now. So there's a lot

01:12:46.080 --> 01:12:52.160
of these interesting, um, use cases that like we actually experienced, but we're kind of

01:12:52.160 --> 01:12:56.240
not necessarily registering as like, oh, this is actually facial recognition happening to me at

01:12:56.240 --> 01:13:02.880
the airport to verify my identity right now. Um, but yeah, like immigration is another space where

01:13:02.880 --> 01:13:07.840
this is very prevalent. Um, there's also, you know, situations and education of like monitoring

01:13:07.840 --> 01:13:14.960
people and, um, uh, kind of student security systems. So yeah, there's definitely a lot of,

01:13:15.600 --> 01:13:21.360
uh, a lot more applications and it's a lot more prevalent than people assume. Um, and the other

01:13:21.360 --> 01:13:25.280
thing I will kind of mention, uh, and it's connected to immigration and connected to law enforcement,

01:13:25.280 --> 01:13:31.280
but the idea of sort of digital surveillance of, you know, I post my face on my Twitter profile,

01:13:31.280 --> 01:13:35.440
and I also post it on, you know, Facebook or whatever. And because of that, people can kind of

01:13:35.440 --> 01:13:40.880
connect these different accounts in different ways or find me, even if I change my name completely,

01:13:41.680 --> 01:13:45.120
even if I change my hair completely and change everything about me, because I can't change my,

01:13:45.120 --> 01:13:50.240
like, actual facial structure. Uh, yeah. So some of that digital surveillance is something that I

01:13:50.240 --> 01:13:55.200
think was very much exposed through clear view of like, oh, wait, uh, you know, when I put this

01:13:55.200 --> 01:14:02.080
information out there, um, it's actually very traceable and it can kind of be used against me. Yeah.

01:14:02.880 --> 01:14:12.240
Yeah, I think there's still this, um, you know, there's still the, the answer to this, well,

01:14:12.960 --> 01:14:18.320
you know, I don't care, you know, the government knows who I am. I haven't done anything wrong.

01:14:18.320 --> 01:14:25.920
And I think, you know, my hope is that through, you know, what's happening now in, in terms of,

01:14:25.920 --> 01:14:35.760
you know, the kind of increased awareness of the, um, you know, for example, police brutality

01:14:35.760 --> 01:14:41.760
that we're seeing in this country. And, you know, maybe we can connect that to, you know,

01:14:41.760 --> 01:14:48.960
Abibas, algorithmic injustice and relational ethics and, and, you know, kind of minimizing

01:14:48.960 --> 01:14:55.840
potential harm to undervoiced or, you know, communities without voices. And, um,

01:14:58.240 --> 01:15:02.160
I'm not sure where I'm going with that. I think I'm expressing frustration. Like, how do you,

01:15:02.160 --> 01:15:08.160
how do you, I think that there's, you know, and even, even personally, like, I think, you know,

01:15:08.160 --> 01:15:13.440
I thought about it when I got global entry, but I still did it. And I'm like, yeah, yeah, you

01:15:13.440 --> 01:15:19.680
know, it's like, it's a convenience in a lot of cases, but, you know, how do we parse through,

01:15:19.680 --> 01:15:26.240
like, what's the cost of that convenience and for who and at the, at what level, um, and there's

01:15:26.240 --> 01:15:32.880
just so many complex questions and issues here. Like, when I talk to sort of, um, my friends

01:15:32.880 --> 01:15:36.160
outside of the space about facial recognition, they'll be like, oh, like this, the thing that,

01:15:36.160 --> 01:15:41.520
like, let's me have a Snapchat filter. Like, I want that. Like, I want to be able to have,

01:15:41.520 --> 01:15:47.600
you know, facial recognition, identify, you know, my, my, my, um, my key features so that I can

01:15:47.600 --> 01:15:51.600
sort of, like, the, like, face landmarks I can put on, like, you know, the buddy ears or whatever.

01:15:51.600 --> 01:15:55.360
Like, that's important to me. And I'm like, okay, no one's threatening your Snapchat filter

01:15:55.360 --> 01:16:00.800
here. Calm down. And, you know, if anything, if you're someone that sees yourself as this, like,

01:16:00.800 --> 01:16:06.880
citizen that, like, you know, doesn't necessarily, um, that isn't guilty of anything. Like, if anything,

01:16:06.880 --> 01:16:11.600
that is sort of a reason to care more because, you know, especially if you're a person of color,

01:16:11.600 --> 01:16:17.120
I think that, um, there's definitely risk, uh, with respect to just being in these systems,

01:16:17.120 --> 01:16:22.080
which a lot of us are already, you know, a vast majority of Americans already in these systems,

01:16:22.080 --> 01:16:27.120
already embedded in these systems. Um, you know, some of the test sets, um, coming in from

01:16:27.120 --> 01:16:32.720
Homeland Security are coming from like visa images and images that you, uh, you know, and there's

01:16:32.720 --> 01:16:37.760
also, you know, recent reports of DMV, you know, driver license images sort of being shared with

01:16:37.760 --> 01:16:44.640
ISM being integrated into these mugshot databases, right? So there's no way to tell, you know,

01:16:44.640 --> 01:16:48.720
when someone takes their picture, where that picture will land, especially if it's a government

01:16:48.720 --> 01:16:53.120
agency taking that picture. And I think that that is something that we should sort of be concerned

01:16:53.120 --> 01:16:58.400
about on an individual basis, because, um, especially if you're innocent, because they can kind of

01:16:58.400 --> 01:17:04.240
duplicate you in these processes and these, um, these, uh, these, these systems that, you know,

01:17:04.240 --> 01:17:10.000
you really have no business sort of, uh, being pulled into. And it, and it encompasses your

01:17:10.000 --> 01:17:14.720
life, or could it has the potential to future for future inconveniences that could really, uh,

01:17:14.720 --> 01:17:18.800
disrupt your way of life right now. So on an individual basis, I do think there's enough

01:17:18.800 --> 01:17:23.760
reason for concern. But if you're like fully like, I have, you know, I, I have enough money for

01:17:23.760 --> 01:17:29.360
bail type person, like, I, like, even if I get like misidentified, I can protect myself on any,

01:17:29.360 --> 01:17:32.880
because there are people that are like that. Um, and then it's at that moment that you kind of

01:17:32.880 --> 01:17:37.840
appeal to, um, okay, well, think about those that do not have that privilege and people that are

01:17:37.840 --> 01:17:43.120
sort of increasingly vulnerable because of this technology. And I think that's why, um, a lot of

01:17:43.120 --> 01:17:49.600
the conversations around restriction, you know, I appreciate the companies for publicly sort of

01:17:49.600 --> 01:17:55.360
denouncing the use of the technology in a way that, um, um, you know, has been sort of well

01:17:55.360 --> 01:18:00.000
received by the public and sort of well understood by the public. But I, I don't think I'm going to

01:18:00.000 --> 01:18:04.080
depend on these companies to go all the way. Like, I don't think, I don't think they're going to

01:18:04.080 --> 01:18:08.960
shoot themselves so in the foot that, um, you know, it actually achieves some of the protections

01:18:08.960 --> 01:18:14.080
that need to happen for the sake of some of these marginalized groups and communities

01:18:14.080 --> 01:18:19.360
most at risk. Um, I think that, um, you know, at best or at worst, what we can expect from these

01:18:19.360 --> 01:18:25.360
companies is to, you know, uh, perhaps protect the majority of their, their clients or their users,

01:18:25.360 --> 01:18:28.560
because they're, their users and the concerns of their, well, that's how we got here in the first

01:18:28.560 --> 01:18:33.600
place. Yeah. Um, like, that's like a best, like, the people that would buy, you know, like,

01:18:33.600 --> 01:18:36.960
whatever product they're selling, like, maybe they'll think about them. But for them to think about

01:18:36.960 --> 01:18:42.480
people that, you know, might never have interacted with Amazon in any way, um, are completely out

01:18:42.480 --> 01:18:46.800
of their scope of concern. You know, how do we actually support and protect them? And that's when

01:18:46.800 --> 01:18:52.240
we start thinking about how important regulation is and how important it is to restrict its use, um,

01:18:52.240 --> 01:18:57.920
in very particularly sort of like, uh, predatory cases against some of these very vulnerable groups.

01:18:57.920 --> 01:19:03.200
So I think that's the impetus for that direction is like, it really shouldn't depend on companies

01:19:03.200 --> 01:19:06.960
sort of when, the other thing too, is like, a lot of the companies that are most notorious.

01:19:07.840 --> 01:19:12.640
It sounds awful, but like, you know, uh, Palantir, NEC, like, you know, some of these names,

01:19:12.640 --> 01:19:18.080
people don't even know because they're, they're not consumer-facing. Even if Palantir was to,

01:19:18.080 --> 01:19:22.160
to make a statement, I'm not sure how many, um, you know, how many people like, you know,

01:19:22.160 --> 01:19:26.080
within my family or people that aren't in this space would like recognize that name, NEC,

01:19:26.080 --> 01:19:30.880
a lot of people don't understand what that company is. Um, but they're really a large,

01:19:30.880 --> 01:19:34.080
a large part of this market. They're actually a lot of the players and a lot of these small

01:19:34.080 --> 01:19:38.400
startups too that I don't even, you know, I haven't even been able to identify as vendors because

01:19:38.400 --> 01:19:44.160
they're so, it's such an opaque system. It's such an opaque process. Um, you know, how can we actually

01:19:44.160 --> 01:19:49.680
force like, uh, government agencies to disclose and identify some of these groups so that we can

01:19:49.680 --> 01:19:54.480
begin to like question the functionality of their systems and audit them. Um, but also, you know,

01:19:55.200 --> 01:19:59.360
how can we protect the people that are currently affected by these companies that will very likely

01:19:59.360 --> 01:20:04.320
not change because facial recognition is their main product. Um, unlike with IBM, um, uh,

01:20:04.320 --> 01:20:09.040
Microsoft and Amazon, where they have other sources of revenue. Um, so, you know, how do we,

01:20:09.040 --> 01:20:13.280
you know, rather than focusing on convincing those companies to change, how can we actually

01:20:13.920 --> 01:20:18.000
push the regulation that can protect everybody that can protect everyone, especially those at risk?

01:20:18.880 --> 01:20:23.120
So yeah, I think that there's still a lot of incentive, even though there's sort of

01:20:24.000 --> 01:20:29.040
potential for personal apathy, you know, uh, there's still a lot of incentive to work on this

01:20:29.040 --> 01:20:33.840
and to think about sort of these broader implications and, uh, push for regulation specifically.

01:20:35.760 --> 01:20:43.520
Yeah. Cool. Well, Deb, thanks so much for taking the time. Yeah, I know, with you covered a lot of

01:20:43.520 --> 01:20:49.040
ground. You look like you're like, every time I talk to someone about this, you're just like,

01:20:49.040 --> 01:20:58.800
what just happened? Don't talk about facial recognition at the end of anybody. Awesome. Don't have to hear

01:20:58.800 --> 01:21:04.800
about it, but okay. I think I'm looking at the, uh, I'm looking at the kind of recording time

01:21:04.800 --> 01:21:10.240
right here. And I'm like, we could go for another hour. That's very complicated.

01:21:11.840 --> 01:21:18.800
And end up in a similar place, actually. Yeah. Unless we're rolling up sleeves and like, you know,

01:21:18.800 --> 01:21:23.760
coming up with a textonomy or something like that. And it's moving so quickly too, right? Like,

01:21:23.760 --> 01:21:27.280
things are happening every day, something happened last week, something. How many of the things

01:21:27.280 --> 01:21:31.920
that we talked about just happened this week? Like, and we didn't, yeah, I was going to ask you about

01:21:31.920 --> 01:21:38.560
the, uh, did you see the, was it one of the Springer publications? Yeah, I was trying to detect

01:21:38.560 --> 01:21:44.880
criminality out of off of facial images. Yeah. That was just a couple of days ago. I saw,

01:21:44.880 --> 01:21:48.560
at least that's when I saw the tweet. Yeah, I've been telling people like, I wonder what's going to happen

01:21:48.560 --> 01:21:55.840
tomorrow? Like, the summer that I was trying to write the saving face paper, we, we wrote a section

01:21:55.840 --> 01:22:00.720
on sort of a policy developments and facial recognition. And we were keeping track of all the state

01:22:00.720 --> 01:22:05.040
developments, all the bills and all the federal bills coming out literally every week. I would have

01:22:05.040 --> 01:22:09.680
to rewrite the section just because it was just like constant stream of like bills coming out. Yeah,

01:22:09.680 --> 01:22:15.520
it's crazy. The, the activity happening and because Amazon set this arbitrary one-year deadline,

01:22:15.520 --> 01:22:20.400
people are going into hyperdrive and things are moving even quicker. So we'll see what happens,

01:22:20.400 --> 01:22:26.400
but definitely an active space. There's a lot to talk about. Yeah. Awesome. Well, thanks so much, Deb.

01:22:26.400 --> 01:22:41.120
Yeah, for sure. Thanks for having me. Okay. All right. Take care.

