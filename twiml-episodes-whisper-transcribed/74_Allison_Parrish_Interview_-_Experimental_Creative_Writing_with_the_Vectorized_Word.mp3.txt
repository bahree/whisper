Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting
people doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
A big thanks to everyone who participated in last week's Twimble Online Meetup, and
it's Kevin T from Sigopt for presenting.
You can find the slides for his presentation in the Meetup Slack channel, as well as in
this week's show notes.
Our final Meetup of the Year will be held on Wednesday, December 13th.
Make sure to bring your thoughts on the top machine learning and AI stories for 2017
for our discussion segment.
For the main presentation, prior Twimble Talk guest Bruno Gonzalez will be discussing
the paper understanding deep learning requires rethinking generalization by Shi Huang Zhang
from MIT and Google Brain and others.
You can find more details and register at twimla.com slash Meetup.
If you receive my newsletter, you already know this, but Twimble is growing and we're
looking for an energetic and passionate community manager to help expand our programs.
This position can be remote, but if you happen to be in St. Louis, all the better.
If you're interested, please reach out to me for additional details.
I should mention that if you don't already get my newsletter, you are really missing
out and should visit twimla.com slash newsletter to sign up.
Now the show you're about to hear is part of our Strange Loop 2017 series brought to you
by our friends at Nexusos.
Nexusos is a company focused on making machine learning more easily accessible to enterprise
developers.
Their machine learning API meets developers where they're at, regardless of their mastery
of data science, so they can start cutting up predictive applications immediately and
in their preferred programming language.
It's as simple as loading your data and selecting the type of problem you want to solve.
Their automated platform trains and selects the best model fit for your data and then outputs
predictions.
To learn more about Nexusos, be sure to check out the first episode in this series at
twimla.com slash talk slash 69, where I speak with co-founders Ryan Sevy and Jason Montgomery.
Be sure to also get your free Nexusos API key and discover how to start leveraging machine
learning in your next project at nexosos.com slash twimble.
In this episode, I speak with Allison Parrish, Poet and Professor at NYU in the Interactive
Telecommunications Department.
Allison's work centers around generated poetry via artificial intelligence and machine learning.
She joins me prior to her conference talk on experimental creative writing with the
vectorized word.
In our time together, we discuss some of her research into computational poetry generation,
actually performing AI-produced poetry and some of the methods and processes she uses
for generating her work.
Alright everyone, I am here at Strange Loop in St. Louis and I am with Allison Parrish.
Allison is a teacher with the Interactive Telecommunications program at NYU and she is
actually speaking later on today on experimental creative writing with the vectorized word.
Welcome to the podcast Allison, it's great to have you.
Thank you, it's great to be here.
So why don't we get started by having you tell us a little bit about your background
and how you got interested in machine learning and artificial intelligence?
Okay, so I am a poet, sort of what I like to call myself primarily, but my educational
background originally as an undergraduate was in linguistics and I've been a computer
programmer for a really long time.
Most of the poetry that I write has to do or is produced with procedures, so I write
computer programs that produce poetry and I've been doing that for a really long time.
Mostly I've done like Twitter bots and stuff like that.
And sort of my poetic research right now is about like how do we find new ways of composition?
Like what are new means of like putting text down onto the page?
Just like me, generally I think of poetry we think of like a really artistic person
or whatever who gets super inspired by nature or something and it's like looking out
the kitchen window and sees the glint of dew off of the flower of a petal right to poem
based on that inspiration.
Oh, how do you do how you write it exactly?
I'm interested in thinking about how can we expand the possible languages and methods
that poets can use to write and how does that like you know what are the other poetic
effects that can be made with text other than just like the sort of the conventional
poetic or the conventional narrative way that the text works.
So procedure is a great way to address that is the computer, you know the computer can
choose something at random, the computer can have a system of rules for putting stuff
together and those rules don't necessarily conform with our conventional ideas about how
our text goes together.
So I primarily have been thinking about artificial intelligence and machine learning as they've
been in my practice for the past couple of years as a way to just do more of that work
of trying to create tools for poets and programmers to be expressive with language in ways that
they haven't been able to be.
Okay.
There's been a lot of work recently with folks basically training these LSTM neural
formats with, you know, tons of text and then having them produce, I got to imagine
someone's in poetry but movie scripts and all kinds of stuff.
You do any of that as well or it sounds like you're looking for more structure in the
type of things.
I don't know if it's more structure or less structure or different structure.
I mean, so I keep up with that research and I should have looked this up.
There's the Turing tests and the creative arts, I think it's called, I haven't seen that
but I imagine it's, when you look at this, do you think it was written by human?
Yeah.
That's exactly what it is.
And they have this prize and when they first started out, it was just like a couple
of different genres.
They've had a different stuff.
Now they haven't.
They've had a prize for like the best sonnet and by best what they mean is the one that
can like trick a person into thinking that it was written by a person.
Okay.
And that basically means that like there's always been a literature on computational poetry
generation, but for like the past couple of years, it's all just been like sonnet generators,
sonnet generators, sonnet generators, sonnet generators, sonnet generators.
All with like slightly different methodologies, some of which you're using like deep learning
some of which aren't.
Okay.
And I think that's super interesting research to keep up with that like those researchers
are framing their problem that they're solving in terms of verisimilitude, like they're
trying to make poems that resemble poems that are written by a person in a conventional
typical manner.
And then like in actually like in the paper in order to evaluate the work, they'll like
do like a survey or something and ask people to judge whether or not it was conventionally
created or not or what they do it.
Which for me is like that's super uninteresting.
It's like, I mean, that's a poet.
As like a researcher, it's like, well, of course, that's cool.
That's interesting.
Right.
But as a poet, it's like we already have a pretty efficient machine for writing sonnets
and that's a person.
Like if you find a person you give them $10,000 and you say, well, you write a sonnet,
you're probably going to get a pretty good sonnet.
I don't think anybody's been paid $10,000 to write a sonnet in the past, you know, a hundred
years or whatever.
So for me, I'm more interested in with that kind of stuff, I'm more interested in using
machine learning for making open new avenues for being expressive in unexpected ways.
I guess the answer is like I'm like trying to pick and choose from that research, the
stuff that can be helpful to me and the stuff that's been really helpful to me over the
past couple of years is just like this idea of a word vector and how it opens up this
ability to compose texts differently because instead of working with discrete units,
you're working with like just this kind of like blob that can be like stretched and molded
and stuff like that.
And I think that's like really useful raw material for a poet.
Interesting.
So I imagine your talk was about how you can stretch and mold the words with this blob.
Right.
So maybe walk us through kind of how you presented those ideas, what were you trying to
leave folks with?
The thing that I'm trying to leave people with is more just an idea of here's what computers
can do with language that might not occur to people individually.
I don't think that like the particular research that I'm doing is like that answer to computer
generated poetry.
But the stuff that I've been doing recently is, so if you take like these pre-trained
word vectors from an algorithm like word to vac or the the glove vectors from Stanford,
basically just get like, you know, four gigabytes of vectors that associate a word or a text
file that associates a word with the vector of numbers.
And once you have that vector of numbers, you can arrange them in sort of like a matrix
that every word in a text corresponds to a vector.
At that point, it's weirdly just like audio or image data.
Like image data only has like four dimensions, let's say like the great red, green, blue,
and alpha channel audio data usually just has the one dimension like the amplitude at
that particular sample point.
But text even though even though that data is like 300 dimensional or whatever from like
a mathematical standpoint, it's the same.
So there's like no reason.
You can basically just like the same function that you would use in like, what's the name
of the stupid Python library that has all of the.
So I can learn.
Yes.
I use this every day.
I don't know.
I'd need any function inside get learned or numpy or whatever that you could like put
an image file into and then get another image out and do that with the word vector as
well.
So this is like, this is an experiment that I did while I was doing a residency last
summer, which is basically just like, what does it look like when you blend two text
together?
You just have like, here's here's text one, which is a matrix of word vectors, here's
text two of the matrix of word vectors, what if you just like cross-fade between them?
Okay.
And then you can do things like blur the text, like apply, like algorithm to just like
average the surrounding pixels, the same way that you would do it with an image or, oh
wow, or resample it, like you can actually like take a text and then just say, instead
of being 50 word vectors along, this will be five word vectors along with you again.
And that's actually like one of the ways of doing text summarization is just like average
all of the vectors in a sentence.
And that average vector is like the thing that you use to represent the meaning of the
sentence.
There's like other more sophisticated ways to do that, but that's like surprisingly
performs well on all of these tasks.
Oh wow.
Yeah, and then so that's the kind of like thoughts that I've been bringing to this recently
in terms of the operations you can perform on the vectors.
And the idea of that, the idea behind that is just that now you can with word vectors
as the medium for text, suddenly you can think of composing a text not consisting of like
typing letters on a keyboard or selecting like words from autocomplete, but instead you
have this like very physical analog continuous way of saying like, you know, I can turn
a knob, like I could turn like a physical knob to make this text longer or shorter or
to add in these semantics from this other text, it's really interesting.
And now I'm taking that very literally, well not a literal knob, but I'm thinking of it
would be kind of cool to have some kind of goo representation of all of these vectors
and use that in a way to generate poetry or other written works like, are we there yet?
Is that something like that?
Oh, I've actually, I've been performing poetry with an interface like that with an actual
like MIDI controller with actual, and one of the things that is in the talk is I'm working
on sort of like a prototype for an interactive environment that's sort of like, do you know
like like processing the interactive programming framework?
I've heard it.
I've heard the phrase.
It basically like, it opens up a canvas object in the browser and then gives you a function
that runs 30 times a second or whatever.
And then inside of that function, you can call like a function to draw any lips or whatever
and you can have it follow the mouse and stuff like that.
So I am working on a prototype interface that's sort of like that except for word vectors.
So instead of like a pixel buffer, the main thing that you're operating on in real time
is a buffer of word vectors.
And then on every frame, it's like showing you the word vector in the database closest
to whatever that value happens to be.
So I mean, the thing about it is that like, I think a lot of the problems that I'm trying
to solve are poetic problems, right?
And I think that a lot of like artificial intelligence researchers, you know, because
this is what they're interested in are interested in solving these problems of like, you know,
let's think about how to formalize what a poem is so that we can generate better poems,
right?
Because that's sort of like the teleology of artificial intelligence research is to like
duplicate flavor, but I'm more interested in so that like the stuff that my tool makes
is not stuff that makes sense in the conventional way.
It's less, it's more cartridge-stined, mainstream contemporary poet.
I can't think of a mainstream contemporary poet that I want to like make fun of for making
poetry that makes too much sense.
So yeah, that's that's sort of my goal.
Is it like a linearity?
It's like less linear than what you typically expect or is that too simple a characterization?
What do you mean by linear?
I guess I'm imagining the poems at the tool, that the thing that you're, the poems that
you're creating, being more, I guess I'm imagining just like random words popping up.
Yeah.
Is it kind of like that?
Or is it kind of like that?
Okay.
It comes out looking like random words.
But yet there's some fundamental element of it that's still at least to use as this
is poetry.
Yeah.
I think the thing that's interesting to me is the thing that that has been interesting
to me as I've been presenting this work, I mean I, I, I present this work a lot to,
to people in, you know, machine learning, artificial intelligence research, but then
I also just like perform at regular poetry readings.
And the thing that's actually been interesting and encouraging to me is that like these metaphors
actually make sense to people like that idea of blending between two texts like averaging
two words and finding the word in the middle and then making an entire text out of those
words that are blended between the two.
As long as you can show like some delta as long as you can say like here's where it started
and here's where it's ending up, people will appreciate what's happening at each of
these of those intermediary steps.
I think this is a rule about poetry in general, the more that people understand the rules
that condition the text, the more that they understand like the aesthetics of the mechanisms,
the more they actually understand the text itself.
Okay.
So that's what I'm, the, the idea is that like if you just looked at it in isolation,
you might think oh this is nonsense, but once you understand the, the procedure behind
it, once you understand what's going into it, then suddenly it becomes this like massive
new ability of this massive new way of thinking about language and text.
That question of interpretability is like super, super important to me.
Interpretability in the sense you just described, understanding the context for a particular poem
and the receiver's ability to interpret its significance.
Right.
I have like, if you think about in, in the visual arts and with music, there are all kinds
of different ways of interpreting those artifacts.
Sure.
Like, since the invention of photography almost all visual art has turned away from depiction.
Like, people still paint things, but since, you know, as soon as photography was invented,
you had impressionism and interpreting and impressionist painting is very different
from interpreting a painting that like actually depicts something, interpreting like Jackson
Pollock painting or a Mark Rothko painting or something like that is also very different.
So there's this like whole new language for interpreting art that doesn't have to do
with that conventional way of interpreting it.
And the same thing with, with music, like we have, music with music it's even more, it's
even more striking because music is almost never, you know, intended to convey a thought,
right?
Sometimes it is, and we have languages of citizens for doing that.
I mean, like, when Bach was writing a chorale or whatever, he wasn't like, that wasn't
meaning to say, like, you know, remember to buy eggs at the grocery store or whatever,
right?
Like, it was, it was trying to, it was trying to create a particular aesthetic effect.
Okay.
And then there's some music, like, you know, music with lyrics or whatever, or like, music
with light motif from the romantic era, like those do have like specific symbolism and
they can be interpreted.
But in general, with music, like it's not, you don't listen to a song hoping to like,
immediately retrieve like a propositional sentence that the person says, right?
The song isn't something coded phrase.
Right.
Right.
Exactly.
It might include phrases, but it's not, it is not itself a way of language to fix
expression, or I should say, like, a way to communicate a single idea.
And lots of poets have been working with that same concept for a really long time.
Like, that's been part of like the modernist tradition since data, maybe even earlier.
But I think a lot of contemporary poets who are working in this area are sort of stuck
on the idea that either it has to make total sense in a conventional way, or it has to
be completely nonsense at all.
Like it's either just nonsense, unreadable nonsense, or it's this thing that is conventional
and to brag the way that it means.
And I'm trying to investigate that little middle area.
Yeah.
And that's where machine learning is so useful, because like that's sort of, for me, at
least aesthetically, that's the same place that machine learning inhabits.
It's like, well, it's not, it's not a person in society with all of that context.
It's sort of like trying to imitate and learn some of those same patterns from data.
And when machine learning is most interesting to me is when it's when it's like slightly
broken, when it gives you stuff, it's unexpected.
So you've got this apparatus that allows you to manipulate word vectors and produce poetry.
But there's also this other step that is producing the word vectors, the vectorization from
a corpus that usually influences the result.
Do you incorporate that step in as well?
Or do you take that as a given for a piece or a body of work?
With the semantic stuff, I've been taking it as a given, because I don't want to spend
the money on the easy-to-manage to do a big word-to-back thing on gigabytes and gigabytes.
I think there's something to be said about using that materiality.
A lot has been written about the bias that is in word vectors and how, you know, like
the analogies it'll show like, man is too programmer as woman is to housewife for whatever.
I kind of like how that materiality is present in the stuff that comes out of us.
Part of the reason of making stuff, of making poetry is to expose those, to expose and
help people understand the ways that language is biased.
So I don't see that as being entirely incompatible with what I'm trying to do.
And so do you use a specific pre-trained word-to-back model?
I've been using the glove vectors recently.
Glove vectors.
So there's a, and those are from Stanford and they're not, yeah, it's a different algorithm
from word-to-back but it basically ends up doing the same thing.
But it's not just the algorithm, it's a vector system that's pre-trained for you.
Yeah, you just download the text file and it's like word and then 300 numbers.
Just got it, got it.
And there's a wonderful library for Python called Spacey for doing natural language processing
stuff.
Okay.
And it comes with those vectors by default or at least it did in most recent versions.
Okay.
So those are like super easy to use.
Oh nice.
The other thing I've been working on recently, and this is the thing that I'm giving
a talk on next week, experimentally, eyeing games is phonetic similarity vectors.
So the same way that word vectors give you like two words with similar meanings will have
similar vectors.
Right.
The thing that I've been working on for like the past year and I'm an arts professor.
So I don't have a lot of resources to work on this.
I'm sure a professor in an actual computer science department would have been able to
get a grad student to do this.
But for me, it was a new adventure, was trying to come up with a system of vectors where two
words with similar pronunciations will have similar vectors so that like the word back
and pack would have vectors that are similar to each other.
The idea being that then you could come up with vector representations of sentences or
lines of poetry and then determine how similar they are in sound and then do other interesting
stuff with that.
The same way that you can like do analogies with word vectors, you'd be able to do analogies
with the way the word sounds.
Huh.
Interesting.
It also makes me think of, you know, so much of conventional poetry or rhyming words
and you could do something similar with rhyming vectors or something.
Yeah.
Yeah.
Rhyming is difficult because it's like really easy to, it's really easy to like mechanically
identify rhymes and attacks.
If you have a phonetic transcription of the text, then like you can just look at like particular
phonemes in particular places and then you know whether or not it rhymes.
Finding slant rhymes is harder because then you have to have some criterion that tells
you how similar are these two sounds and if they're similar enough that it counts as a
slant rhyme.
What did example of a slant rhyme?
So a slant rhyme would be like, it would be like in a song where somebody tries to rhyme
like, hit with pick or something like that where the sound of the end doesn't quite match.
Okay.
And like almost all contemporary lyrics use slant rhymes all the time.
Okay.
I wish I had a better example at the top of my head.
But the point is that like it doesn't have to be precise for a decount as a rhyme.
Okay.
And so a lot of the poetry papers, a lot of the poetry generation papers that you read,
now we're like sort of trying to find rule-based ways to solve that problem.
Like even in a paper that's otherwise all about deep learning, we'll have this like little
section that's like, well and then we use the sequence of if-else statements to determine
whether or not these are slant rhymes.
Okay.
One of the points of doing the phonetic similarity of actors would be able to like just say like,
okay do your surface syntactic and semantic generation of the text and then compare the
phonetic similarity, then you could even like compare the phonetic similarity across
like moving windows inside of the text and you'd be able to like detect where two lines
are most similar in the way that they sound so that you'd be able to have like sort of
like a useful tool for doing that kind of set land detection.
Okay.
But then also poetry uses sound in ways that isn't just rhymes, right?
Right.
Especially in like contemporary hip-hop like there's just like all an entire line will
consist of sounds that are similar in some way and there's like this really complicated
interplay between between lines where sounds are not quite the same but they're similar
and they produce these from like an articulatory standpoint, the way that your mouth moves,
they have this particular feeling.
And then like you know classical poets or even like romantic poets or whatever, you'll
see those patches of things like eliteration and ascendance and stuff like that, all
of which you can study on like a rule-based basis, but one of the things that I wanted
to be able to do with these phonetic similarity vectors is allow you to just write an equation
just like look at a text and then you have a number that represents what it sounds like
and then you can just you know use regular Euclidian and Neurocene distance to or
this similarity to compare those two.
And so whereas the previous work that you were describing is more about generation, this
is more about analysis?
No, it's it's the more about generation.
No, again, I'm trying to solve poetic problems, not like linguistic or research problems.
That was just like this kind of happy result of making these vectors.
I was I made the vectors because I was I was working on a book of poetry for for counter
path and I what I wanted to be able to do is just take the entire corpus and poetry in
Project Gutenberg and I basically do a random walk through the sounds of it.
So arrange every line of poetry in this database of public domain text and then just be able
to go from one line to the next based on how similar they sounded to each other.
That was the original like poetic idea because I love I love like poetic flow.
I love like that that effect that you have in a poem where it just feels like all of
the sounds are coming together.
And if you could get that from this huge repository of poems, that'd be pretty cool.
Right.
So that was like my main impetus, but then I was like, I should write a paper about this.
Papers usually have a section on whether or not it meets some criterion and their success.
I should try to make that determination.
And so then there's like this whole literature about phonetic similarity and I was able
to like pull a couple of test data sets out of there and prove that like actually the
algorithm that I was working on, you know, was performed pretty well on these tests.
Oh, wow.
And so from that, like these ideas of how you might be able to apply it in these other
contexts, just sort of valid naturally.
And I don't think, you know, because I'm an artist and I only have undergraduate degree
in linguistics, I don't think that the solution that I came up with is like the ultimate best
solution, but it definitely helped me solve my problem.
Awesome.
Awesome.
Well, thanks so much for sharing all this with us is really I don't get to talk about poetry
very often.
People don't.
And it makes me realize how little I know about it, so I need to figure out how to fix
that.
I mean, as anything about poetry, even poets don't know anything about poetry.
Is that part of the essence of the thing?
I think it partially is.
I found recently that I'd like to read about poetry more than I like poetry itself.
So maybe that's a good spring thing.
Awesome.
Well, thanks so much, Allison.
All right, everyone, that's our show for today.
Thanks so much for listening and for your continued feedback and support.
For more information on Allison or any of the topics covered in this episode, head on
over to twomlai.com slash talk slash 72.
To follow along with our strange loop 2017 series, visit twomlai.com slash ST loop.
Of course, you can send along your feedback or questions via Twitter to act twomlai or
at Sam Charrington or leave a comment right on the show notes page.
Thanks again to Nexusos for their sponsorship of the show.
Check out twomlai.com slash talk slash 69 to hear my interview with the company founders
and visit nexosis.com slash twimmel for more information and to try their API for free.
Thanks again for listening and catch you next time.
