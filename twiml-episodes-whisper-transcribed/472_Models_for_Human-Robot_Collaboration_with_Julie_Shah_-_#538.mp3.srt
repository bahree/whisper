1
00:00:00,000 --> 00:00:17,440
All right, everyone. I am here with Julie Shaw. Julie is a professor at MIT. Julie, welcome to the

2
00:00:17,440 --> 00:00:24,880
Twomo AI podcast. Thank you so much for having me. So you are a professor of robotics in particular,

3
00:00:24,880 --> 00:00:30,160
and we're going to dig into your work in that field. But to get us started, I'd love to have you

4
00:00:30,160 --> 00:00:36,080
share a little bit about your background and how you came to work in the field. Wonderful, yeah.

5
00:00:36,080 --> 00:00:44,400
So I'm a professor of aeronautics and astronautics at MIT. And I also lead a robotics lab at MIT as

6
00:00:44,400 --> 00:00:51,600
a part of the computer science and artificial intelligence laboratory. And I have a special love of

7
00:00:51,600 --> 00:00:58,960
like time, critical, safety, critical applications, which is why I'm in aerospace. And so my

8
00:00:58,960 --> 00:01:03,680
story of how I ended up in robotics has to start with how I ended up in aerospace. And I was just

9
00:01:03,680 --> 00:01:10,880
like born loving airplanes and rocket chips. I always wanted to be an astronaut. And I looked at

10
00:01:10,880 --> 00:01:18,640
there was like a I went to space camp. I was a super nerd. And space camp that gave you a book

11
00:01:18,640 --> 00:01:23,200
of all the astronauts and where they all went to school. And you know, they always stress you need

12
00:01:23,200 --> 00:01:27,040
to be really good at math and science. And I was like, Jack, that can be good at math and science.

13
00:01:27,040 --> 00:01:32,320
And a very large number of them either went to the military academies or MIT. And I was like,

14
00:01:32,320 --> 00:01:37,920
MIT sounds like a good place. So whenever anybody would ask what I wanted to do, I would say,

15
00:01:37,920 --> 00:01:42,480
I want to go to MIT and study aerospace engineering. And everybody would say, oh, that's so specific.

16
00:01:42,480 --> 00:01:48,480
What's so specific? And I'd say, okay. And then I came to MIT and I in the aerospace department.

17
00:01:48,480 --> 00:01:52,880
And then, you know, pretty, pretty soon after in the department, I'd be asked, okay,

18
00:01:52,880 --> 00:01:57,040
see your aerospace job. But what are you going to specialize in? And I was like, oh, no, I have to

19
00:01:57,040 --> 00:02:04,880
specialize further. Because it turns out that, you know, aerospace, much like robotics is a systems

20
00:02:04,880 --> 00:02:13,840
discipline. And I really loved control theory and control systems and really loved learning about,

21
00:02:13,840 --> 00:02:20,880
you know, aircraft auto pilots. And how those really had to be designed to the capabilities of

22
00:02:20,880 --> 00:02:28,240
pilots of humans and, you know, how they could affect control over a system. Having to design for

23
00:02:28,240 --> 00:02:33,840
the human machine system was really exciting to me and needing to understand both sides of that

24
00:02:33,840 --> 00:02:40,800
equation. So I actually did my master's degree in human factors engineering to kind of go deep

25
00:02:40,800 --> 00:02:46,800
on the human side. And it wasn't until my PhD, I switched gears and joined the computer science

26
00:02:46,800 --> 00:02:51,600
and artificial intelligence laboratory and did my PhD in automated planning and scheduling.

27
00:02:52,560 --> 00:02:59,440
And so, you know, this was 2010. I finished up my PhD. So before the big, the big era change

28
00:02:59,440 --> 00:03:05,600
into, you know, to machine learning. And throughout have just been really, really interested in

29
00:03:05,600 --> 00:03:11,040
how you design automation and technologies and, you know, AI and machine learning to like fit

30
00:03:11,040 --> 00:03:16,800
like a puzzle piece against human capabilities so that you can achieve some end objective.

31
00:03:17,920 --> 00:03:23,520
And that's why I, you know, started a robotics lab, the interactive robotics group and my lab

32
00:03:23,520 --> 00:03:28,400
includes human factors engineers and aerospace engineers and computer scientists and,

33
00:03:28,400 --> 00:03:34,880
um, uh, others as well. So we bring kind of across disciplinary perspective to, uh, developing the

34
00:03:34,880 --> 00:03:40,400
technology and showing its benefit. Nice, nice. And your lab, do you still have that leaning

35
00:03:40,400 --> 00:03:48,480
towards flying robots as opposed to arms or walking robots or humanoid or other types of form factors?

36
00:03:48,480 --> 00:03:53,120
Yeah, it's interesting. It's interesting. You ask that because when I, um, when I applied for

37
00:03:53,120 --> 00:03:59,840
faculty positions, it was straight out of PhD and, um, and I got the offer from MIT from my home

38
00:03:59,840 --> 00:04:04,800
department. I did all my degrees at MIT and then I had this amazing offer from MIT from my home

39
00:04:04,800 --> 00:04:10,800
department, uh, but they strongly recommended slash required that I go away for a year,

40
00:04:10,800 --> 00:04:17,440
was before starting on the faculty. And they, um, my PhD was funded under an NSF fellowship. So it

41
00:04:17,440 --> 00:04:23,920
was, uh, maybe not, not super closely tied to, you know, real applications of real systems.

42
00:04:23,920 --> 00:04:28,080
And, um, the department head at the time suggested that I go out to Boeing and he said, you know,

43
00:04:28,080 --> 00:04:32,560
they have interesting applications and robotics for manufacturing. I said, oh, it sounds great.

44
00:04:32,560 --> 00:04:39,200
It sounds like a great way to spend a year out in Seattle. So, um, and, uh, and I just, I just,

45
00:04:39,200 --> 00:04:46,080
I just caught the bug of, you know, uh, manufacturing. I just loved watching, you know, 737 or

46
00:04:46,080 --> 00:04:50,800
737 being built in front of me and then thinking about the challenges. Again, it's like an integration

47
00:04:50,800 --> 00:04:56,000
challenge. Like robots can do pieces of that work, but actually the vast majority of building a large

48
00:04:56,000 --> 00:05:01,600
commercial airplane is still done manually, like a beehive of manual work. And so they're, um,

49
00:05:01,600 --> 00:05:07,520
their challenges in how you design the technology, the robotics, the intelligent robot to integrate

50
00:05:07,520 --> 00:05:14,160
into manual workflow. Um, and so when I came back, I had us, uh, I wanted to keep doing that. I kind of

51
00:05:14,160 --> 00:05:19,520
wanted to keep working in robotics for manufacturing and collaborative robots. And I, I asked, uh,

52
00:05:19,520 --> 00:05:23,840
a different department head on the one that hired me at the time. So, is it going to be okay if I,

53
00:05:23,840 --> 00:05:28,080
you know, if I work in manufacturing in aeroastro or do I need to be flying things? And he said,

54
00:05:28,640 --> 00:05:33,760
you know, pursue, pursue the problems that you think are important. And, um, and I, and I really

55
00:05:33,760 --> 00:05:40,000
appreciated that. I work, I work on aerospace and, um, you know, applications, but I also work in,

56
00:05:40,000 --> 00:05:47,040
um, uh, yeah, in, uh, you know, automotive, you know, um, I work in decision support for fighter pilots,

57
00:05:47,040 --> 00:05:52,960
but also like nurses and doctors. Um, but the key thread is, uh, I'm just, you know, I think it's

58
00:05:52,960 --> 00:05:57,920
really exciting to develop technology that really has to be flawless to add value and has to fit

59
00:05:57,920 --> 00:06:02,720
with the human in a way that, you know, doesn't, doesn't add friction, but really eases a very

60
00:06:02,720 --> 00:06:07,760
challenging job of a dominean expert. And so- So you can kind of envision that as a, uh,

61
00:06:07,760 --> 00:06:13,040
venn diagram of, you know, mission critical in one circle and requires human interaction

62
00:06:13,040 --> 00:06:18,560
and the other and your sweet spot is in the middle. Yeah, yeah, that's a, that's a good visualization

63
00:06:18,560 --> 00:06:24,880
of, uh, of whether or not I take on a project. Yes. Okay. And so, uh, let's dig a little bit deeper

64
00:06:24,880 --> 00:06:30,480
into that. What are some of the types of projects that you work on? Or maybe even more broadly,

65
00:06:30,480 --> 00:06:38,560
how do you craft a research agenda in that, uh, that intersection? So, uh, I started on the faculty

66
00:06:38,560 --> 00:06:47,600
at MIT just a little over 10 years ago, um, or maybe 11 years ago now. And, um, and from the, um,

67
00:06:47,600 --> 00:06:53,440
sort of like the vision behind our work then and still now is to be intentional about developing

68
00:06:54,000 --> 00:06:59,200
intelligent machines or intelligent robots or even kind of computing more broadly that enhances,

69
00:06:59,200 --> 00:07:06,400
um, human capability, um, and well-being. And, um, and, you know, there are many ways to do that,

70
00:07:06,400 --> 00:07:11,760
but, you know, starting out in the lab and going back to my PhD, I was interested in, you know,

71
00:07:11,760 --> 00:07:19,280
how, how far can you get in, um, developing a robot, um, that can emulate the, the capabilities

72
00:07:19,280 --> 00:07:25,120
of an effective human team member. Um, and that's not the only model of collaboration or teamwork

73
00:07:25,120 --> 00:07:30,000
for sure. Um, and it's not even, it was always a hypothesis. It's not even a given that if you

74
00:07:30,000 --> 00:07:34,720
develop, you know, um, a system with some of the, you know, capabilities or features of an

75
00:07:34,720 --> 00:07:38,160
effective human team member, that that'll even be useful. So we've always, you know,

76
00:07:38,160 --> 00:07:43,120
conducted experiments to try to determine, you know, whether or not that's useful. But my lab

77
00:07:43,120 --> 00:07:50,800
focuses on developing novel AI models and algorithms where the AI is modeling people. Um, and so

78
00:07:50,800 --> 00:07:55,760
there are, you know, decades of studies in what makes for effective human team work, you know,

79
00:07:55,760 --> 00:08:02,320
in sports psychology and studying pilots in the cockpit or doctors and nurses in, in the operating

80
00:08:02,320 --> 00:08:06,720
room. And it's very, it's a very practical field of study because you want to be able to train

81
00:08:06,720 --> 00:08:14,000
up new people to come into this profession and work effectively in a team. Um, and, uh, but I'll just

82
00:08:14,000 --> 00:08:19,520
brush that all aside and summarize many decades of very rigorous study in human teamwork and

83
00:08:19,520 --> 00:08:25,760
human team coordination, um, in terms of, you know, like, there's, uh, three capabilities that we

84
00:08:25,760 --> 00:08:31,760
as humans bring to teams that make us really, um, make some of us exceptional team members.

85
00:08:32,640 --> 00:08:37,600
And that's the ability to know what your partner is thinking, to be able to anticipate what they'll

86
00:08:37,600 --> 00:08:43,600
do next and then be able to use that information online, um, a circumstances unfold to, um,

87
00:08:43,600 --> 00:08:50,560
uh, uh, to, to sort of change your plan. And so, um, my lab has focused on doing those three things,

88
00:08:50,560 --> 00:08:56,000
developing models that can infer human cognitive state, predictive models of human behavior and

89
00:08:56,000 --> 00:09:01,840
workflow, uh, and, um, and then also, uh, set of techniques around dynamic plan execution,

90
00:09:01,840 --> 00:09:06,400
to be able to take those predictions and use them online to have a, what kind of play the game

91
00:09:06,400 --> 00:09:12,960
with you. It's primarily in, in, in development of, of novel AI models and methods, but everything

92
00:09:12,960 --> 00:09:18,480
we do needs to be tested and evaluated with a system actually working with a person to know whether,

93
00:09:18,480 --> 00:09:22,080
you know, I'm in gelating these capabilities is, is even a value.

94
00:09:23,200 --> 00:09:30,400
Well, one of the things that you said in there that, uh, that I don't want to be as hyperbolic

95
00:09:30,400 --> 00:09:39,120
as blows my mind, but yeah, it is the idea of the robots or the AI kind of predicting what's going

96
00:09:39,120 --> 00:09:45,760
on in their human collaborators heads. That's hard enough for a human. Uh, what does that even mean

97
00:09:45,760 --> 00:09:53,520
for a robot or an AI? Yeah, that's a, that's a, that's a great question. So, um, and you have to,

98
00:09:53,520 --> 00:10:00,160
to pursue that in an academic fashion, it needs to be a well-defined problem and the, like, the,

99
00:10:00,160 --> 00:10:04,880
the human cognitive state that matters in a particular setting and, you know,

100
00:10:04,880 --> 00:10:10,000
particular setting A is very different than the state that matters in a different setting.

101
00:10:10,000 --> 00:10:14,720
Sometimes when I give talks, like, sometimes the question afterwards is, is there a unifying,

102
00:10:14,720 --> 00:10:20,800
you know, model or approach to being able to infer human cognitive state? And I've always just been like,

103
00:10:22,400 --> 00:10:26,480
I don't, you know, I don't, I don't think so, but we have to maybe ask the cognitive scientist,

104
00:10:26,480 --> 00:10:32,880
the neuroscientist. But, uh, I think that's, that's a really, like, for me, that, the fact that

105
00:10:32,880 --> 00:10:39,360
there isn't, you know, want, you know, one well-defined set of states that you're really after

106
00:10:39,360 --> 00:10:46,000
that you're aiming to infer is actually, um, one of, like, the key research questions. And I think

107
00:10:46,000 --> 00:10:53,040
it is one of the key research challenges, which is, um, like, we as humans, we want to be able to

108
00:10:53,040 --> 00:11:00,560
shape the machine learning model or the AI's model of our world or of us, um, much like a human

109
00:11:00,560 --> 00:11:04,960
brings, like, different considerations or a different mental model of interacting with another human

110
00:11:04,960 --> 00:11:12,720
or doing one task versus another task. Um, so, um, like a, a, a key, um, the direction of our

111
00:11:12,720 --> 00:11:19,120
research is, is, uh, first of all, recognizing that you will never succeed at that with an unsupervised

112
00:11:19,120 --> 00:11:23,520
learning approach. There's, I mean, you might get super, super lucky in the latent states you

113
00:11:23,520 --> 00:11:29,040
infer, happen to correspond to the human's mental model of, you know, and, and, you know, what's,

114
00:11:29,040 --> 00:11:33,200
what's particularly important in that particular context, but practically that's not going to happen.

115
00:11:33,200 --> 00:11:40,160
And we, you know, um, theoretically, you know, it's, uh, it's a challenge. So, um, so the question is,

116
00:11:40,160 --> 00:11:46,880
what inputs can you listen from a person that's easy for a person to provide, um, that can help

117
00:11:46,880 --> 00:11:52,480
lock in or, uh, sort of shape the latent space or improve the efficiency of inference for the system

118
00:11:52,480 --> 00:11:59,840
to, uh, learn those, you know, the, um, the, the latent states are their dynamics of, uh,

119
00:11:59,840 --> 00:12:05,760
of the human that, that's interacting with it. Um, and, like, providing labels, like a supervised

120
00:12:05,760 --> 00:12:09,920
approach will work, but then where did those labels come from and how practical is it in every

121
00:12:09,920 --> 00:12:14,880
setting to be able to provide labels of, of relevant human cognitive state? I mean, it's just like

122
00:12:14,880 --> 00:12:22,960
an unreasonable request. Um, so, uh, yeah, what, on the other hand, there are easy, there are

123
00:12:22,960 --> 00:12:29,920
inputs that are easy for people to provide. Like, um, I can, maybe I, maybe it's helpful to ground it

124
00:12:29,920 --> 00:12:35,760
in a, in a concrete example. So, um, say, I have the most simple system. I'm trying to, you know,

125
00:12:35,760 --> 00:12:40,240
develop my own mental model of, and it's just a subway. So just go to the end of line, it turns

126
00:12:40,240 --> 00:12:45,520
around and it comes back. Uh, I'm originally from New Jersey. So in New York, the subway goes

127
00:12:45,520 --> 00:12:50,480
uptown and downtown, and that's the mental model I hold is, uh, someone from New Jersey.

128
00:12:50,480 --> 00:12:55,360
But in Boston, um, the subway becomes exactly the same way. It goes to the end of line, it turns

129
00:12:55,360 --> 00:13:00,560
around and it comes back, but we say the subway goes inbound and outbound, and the switching point

130
00:13:00,560 --> 00:13:06,240
is some arbitrary, like, stop in the center, which is weird. So like, you're going inbound,

131
00:13:06,240 --> 00:13:09,200
as long as you're going to Park Street, and as soon as you pass through Park Street, you're going

132
00:13:09,200 --> 00:13:12,640
outbound. And then when it turns around, it comes back, you're going inbound again until you pass

133
00:13:12,640 --> 00:13:18,400
in Park Street, and then you're going outbound. So it's a, it's like, my mental model of the subway

134
00:13:18,400 --> 00:13:23,280
in New York, in Boston, it's a two-state switching model, but it's, but my mental model is different,

135
00:13:23,280 --> 00:13:30,160
depending on where I'm from. Um, and so, um, you know, we, we employed like non-parametric

136
00:13:30,160 --> 00:13:36,640
Bayesian techniques to try to infer latent states, um, of, you know, some black box systems, say,

137
00:13:36,640 --> 00:13:41,680
like, a human, um, but what are the odds you're going to learn an inbound outbound switching model

138
00:13:41,680 --> 00:13:49,280
for Boston? Um, it's like, you know, um, so, um, but here's the thing, like, you know, maybe I don't

139
00:13:49,280 --> 00:13:55,440
want to label for you inbound outbound, and then New York is uptown, downtown, um, but if I can

140
00:13:55,440 --> 00:13:59,680
just tell you the change point, the switch point. So in New York, the switch point of my mental

141
00:13:59,680 --> 00:14:03,920
model that's a way is that it switches at the end of the line, and then Boston, it switches at Park

142
00:14:03,920 --> 00:14:08,800
Street. Um, that's enough to sort of lock in, and now we're going to hold the same mental model

143
00:14:08,800 --> 00:14:14,800
of how to talk about the behavior of that, that subway. Um, so, um, looking for these ways in

144
00:14:14,800 --> 00:14:20,160
which it's very easy for a person to provide high-level input into a machine learning model,

145
00:14:20,160 --> 00:14:24,720
and then enabling the machine learning model to use those inputs within its computational framework.

146
00:14:25,360 --> 00:14:28,320
Um, is, uh, really, really exciting to me.

147
00:14:28,320 --> 00:14:36,160
There are the, the inputs that you're talking about, uh, and in particular, in this case of the

148
00:14:36,160 --> 00:14:46,720
subway, is this an interaction time input, or is this a training input? Um, so the, the particular

149
00:14:46,720 --> 00:14:53,680
work that I'm, that I'm talking about there, that's done at training time. So, um, the, the idea is,

150
00:14:53,680 --> 00:15:00,000
um, you know, you have, uh, you have your, um, time series data, but rather than work with it in

151
00:15:00,000 --> 00:15:04,480
an unsupervised fashion, if you have some labels, but it's not fully labeled, and then you take

152
00:15:04,480 --> 00:15:10,080
this sort of high-level input from the person in the form of partial policies or partial dynamics,

153
00:15:11,280 --> 00:15:15,520
and then what we do is we formulate those as constraints on a, on a variational inference

154
00:15:15,520 --> 00:15:21,440
process. In that work, it's still done at, um, training time, and I think your question is really

155
00:15:21,440 --> 00:15:26,160
exciting, because, okay, so, you know, or even you have a trained model, and now you're taking,

156
00:15:26,160 --> 00:15:33,680
you're, you, you task to do together, new environment, or, um, uh, how it is you, first of all,

157
00:15:33,680 --> 00:15:38,000
identify the differences between how you want the system to behave in the new environment,

158
00:15:38,000 --> 00:15:44,720
or the model it holds, and be able to adapt it is, um, you know, uh, online is, uh, you know,

159
00:15:44,720 --> 00:15:46,640
a really exciting research direction.

160
00:15:46,640 --> 00:15:52,800
Yeah, I think where the question was coming from was, uh, in the context of training, it,

161
00:15:52,800 --> 00:15:58,880
and, you know, me trying to construct, you know, what this trained model might, uh, be, might

162
00:15:58,880 --> 00:16:06,480
look like, it almost sounded like a feature engineering task that, uh, a, you know, subject matter

163
00:16:06,480 --> 00:16:14,640
expert might apply, um, to map, you know, some raw data of whatever the data is we're looking at

164
00:16:14,640 --> 00:16:21,200
about these trains to, uh, you know, their mental model of the world in order to get the model to,

165
00:16:21,840 --> 00:16:28,320
to kind of attach to that in the training process to use, use loose terminology. Um, you know,

166
00:16:28,320 --> 00:16:35,600
you might, if, you know, you have your, uh, if you're kind of moving away from some center,

167
00:16:35,600 --> 00:16:40,080
you might, the, your feature might be the distance from that center point, or the distance from

168
00:16:40,080 --> 00:16:47,040
your endpoint, or something like that. And I was trying it to, I was curious whether the way that

169
00:16:47,040 --> 00:16:51,920
you were building that knowledge into the system was similar to feature engineering, or was it

170
00:16:51,920 --> 00:16:56,640
a different process altogether? That's interesting. So you're highlighting that, like, um, the,

171
00:16:56,640 --> 00:17:03,760
the person's role can be in, uh, you know, identifying or synthesizing the features that are

172
00:17:03,760 --> 00:17:10,080
meaningful to, you know, their understanding of, um, uh, you know, of, of the functioning of the

173
00:17:10,080 --> 00:17:14,800
system. There's, there's definitely a role there, and also, uh, support, you know, obviously

174
00:17:14,800 --> 00:17:22,720
support there in, um, enabling a person to, um, uh, and enabling the proper elicitation of, of

175
00:17:22,720 --> 00:17:28,480
those features. Um, the, the particular work that I'm referring to, it's not in the feature

176
00:17:28,480 --> 00:17:36,880
engineering, um, space. It's, um, so we, we, um, employ, uh, like a Bayesian graphical model,

177
00:17:36,880 --> 00:17:43,520
a factored model, um, with, uh, with, with particular assumptions made on, um, you know,

178
00:17:43,520 --> 00:17:48,560
on the structure of the model to be able to support efficient inference. So you have, um,

179
00:17:48,560 --> 00:17:51,920
instead of observable states about the world, like, there's things I can see about the subway,

180
00:17:51,920 --> 00:17:57,600
right? I can see its movement, um, uh, there's actions that are, that are taken, um,

181
00:17:57,600 --> 00:18:02,240
that are observable, but then there's, like, there's, there's something in the latent space

182
00:18:02,800 --> 00:18:06,960
that clearly impacts, you know, the behavior, the observable behavior of the system as I want

183
00:18:06,960 --> 00:18:12,240
to be able to predict it and describe it. Um, and I think what's interesting is either of those

184
00:18:12,240 --> 00:18:17,280
two-state switching models can be informative, right? Especially if you want to, if you want to

185
00:18:17,280 --> 00:18:22,880
predict the behavior of the system, um, uh, you know, it's, you just kind of need a two-state

186
00:18:22,880 --> 00:18:29,520
switching model and you'll be able to predict the behavior of the system. Um, but, uh, the, um,

187
00:18:30,320 --> 00:18:36,240
the, the question of, like, which, which, which one of those, um, sometimes it can help in, uh,

188
00:18:36,240 --> 00:18:41,040
in, in predictive power of your model, but it's critically important for being able to

189
00:18:41,760 --> 00:18:47,280
support a person in inspecting and understanding and doing their own projection of the behavior

190
00:18:47,280 --> 00:18:54,960
of this learn model in a, in a different context. Um, so, um, in, in the, in, in the approach,

191
00:18:54,960 --> 00:19:04,560
it's really, uh, it's really a flat, we assume a flat, um, discrete, um, latent state space.

192
00:19:05,680 --> 00:19:11,920
But, uh, the feature engineering question is, is, you know, is, is, is equally important.

193
00:19:11,920 --> 00:19:18,080
Um, like, um, I, I worked for a while on my pilot's license, which, which was really fun,

194
00:19:18,080 --> 00:19:22,880
and then I was a grad student and I ran out of money before I could solo, which is, I thought I

195
00:19:22,880 --> 00:19:26,640
had it all planned out, but you never fly for an hour. You always fly for an hour and 15 minutes,

196
00:19:26,640 --> 00:19:31,440
and it didn't, and I'm working out. And the irony is now that I'm in the Arrow Astro Department,

197
00:19:31,440 --> 00:19:35,840
like, I think there's like, actually, like a benefit, like a professional education benefit,

198
00:19:35,840 --> 00:19:39,120
where I could do flying lessons, and now I don't have time, and I have young kids, so,

199
00:19:39,120 --> 00:19:46,160
but, you know, from my, um, from my days of learning how to fly, like, it's just like, much like a,

200
00:19:46,160 --> 00:19:50,000
like a poor machine learning model. It's like a lot of input, and, you know, like, the spurious

201
00:19:50,000 --> 00:19:54,240
correlations, like, you're like, oh, maybe I should do this when I see that over there, and it's

202
00:19:54,240 --> 00:19:59,440
like the role of the instructor to be like, here's what you attend to. These three features mean

203
00:19:59,440 --> 00:20:03,920
this, like, put your fingers up, you know, do you see the horizon above or before four fingers?

204
00:20:03,920 --> 00:20:08,000
Now, you know, if you're flying straight and level, and, you know, like that, that role of guiding

205
00:20:08,000 --> 00:20:17,680
the meaningful, the meaningful use of, of, you know, the features that the system could attend to,

206
00:20:17,680 --> 00:20:25,680
is, is enormously valuable. Without a causal model, it can be, it can be challenging to ensure

207
00:20:25,680 --> 00:20:30,880
you're learning, a model that'll produce the behavior that you hope it'll produce in a, in a novel

208
00:20:30,880 --> 00:20:43,680
environment. Is this subway example? Is this kind of a toy example for illustration, or was there

209
00:20:43,680 --> 00:20:51,440
an actual problem that you were solving an actual data set? And what exactly is that problem?

210
00:20:51,440 --> 00:20:55,680
Yeah. Yeah. Yeah. The subway, the subway example was the very, the very most simple synthetic

211
00:20:55,680 --> 00:21:03,440
domain in the paper, to like illustrate the idea of, we applied it to be able to, so in, in

212
00:21:03,440 --> 00:21:08,800
in this larger goal of being able to observe a human and develop a predictive model of their

213
00:21:08,800 --> 00:21:14,240
behavior, I think a person holds certain priorities or preferences and how they want to do their work.

214
00:21:15,040 --> 00:21:20,400
And this is something like that is practically a challenge for us into playing robots in industrial

215
00:21:20,400 --> 00:21:25,120
environments. So on an assembly line, you know, someone might say, oh, they're standard work.

216
00:21:25,120 --> 00:21:29,760
There's like a standard way of building this car. There's a standard way of building this plane.

217
00:21:29,760 --> 00:21:34,720
And I believe that, too, I went in and watched how people built things. But there's actually,

218
00:21:34,720 --> 00:21:39,680
like a whole lot of variation in how the plane is assembled or how the car is assembled.

219
00:21:41,200 --> 00:21:46,720
And, you know, different people for different reasons, different biomechanical models will have

220
00:21:46,720 --> 00:21:54,880
different ways of performing the work. And in in something like an automotive, you know, an automotive

221
00:21:54,880 --> 00:22:02,800
factory, a collaborator once told me that like half a second of efficiency could like make a break

222
00:22:02,800 --> 00:22:07,680
the business case for introducing a new technology. So once you're looking at a robot to support a

223
00:22:07,680 --> 00:22:14,160
person and being like half a second faster, like small changes in the ordering of how they install,

224
00:22:14,160 --> 00:22:19,840
you know, something on the dashboard of a car or or their motions, being able to predict where

225
00:22:19,840 --> 00:22:24,640
they'll be in a fine grain way for the robot to assist, that'll make a break the success of that

226
00:22:24,640 --> 00:22:32,720
system in in doing that. So being able to quickly kind of lock in and model an individual person's

227
00:22:32,720 --> 00:22:39,040
priorities or preferences for performing a task improves the predictive power of that model.

228
00:22:39,040 --> 00:22:45,680
And but the key is to be able to do it with relatively little data. So the concrete use case is

229
00:22:46,960 --> 00:22:51,280
the person does their assembly task in a factory and you can collect data of how they

230
00:22:51,280 --> 00:22:57,040
how they do their work. And then you want to when the robot comes in, you want to be able to model

231
00:22:57,040 --> 00:23:00,880
like their intent like where they're going to reach next on the table or where they're going to

232
00:23:00,880 --> 00:23:07,600
walk next in in the cell environment. And different people have different again priorities and

233
00:23:07,600 --> 00:23:17,120
preferences. Work load or sort of fatigue level can impact, you know, your model of your predictions

234
00:23:17,120 --> 00:23:24,640
for the person. But rather than hand specifying a threshold based on, you know, some some measures,

235
00:23:24,640 --> 00:23:31,840
it would it's better to be able to learn that and tune that threshold based on the value it gives

236
00:23:31,840 --> 00:23:36,720
you in order in order to predict where the person will be in space and time after some number of

237
00:23:36,720 --> 00:23:43,760
hours. So that's the those are the those are more concrete use cases of of inferring these latent

238
00:23:43,760 --> 00:23:53,200
states and their and especially their dynamics. Got it got it. In the case of the the shop floor

239
00:23:54,240 --> 00:24:00,000
and the the preferences, it kind of going back to that earlier question I asked about run time

240
00:24:00,000 --> 00:24:06,240
versus training time, you know, training time you've kind of built in the capability to identify

241
00:24:06,240 --> 00:24:14,000
this but the identification is happening at run time, is that right? Yeah, yeah, so um okay, so

242
00:24:16,560 --> 00:24:22,720
this is this is this is this is fun. Okay, so like what do you need for a robot to be able to

243
00:24:22,720 --> 00:24:28,080
collaborate with a person online? You needed to have a model of human behavior, which had and it's

244
00:24:28,080 --> 00:24:34,160
maintaining a belief right over the person's, you know, latent states and a belief over, you know,

245
00:24:34,160 --> 00:24:39,600
what they're going to do next. And then you need a task model and maybe you're lucky enough to have,

246
00:24:39,600 --> 00:24:44,560
you know, a clearly specified task model, but even then you have you have a partially observable

247
00:24:44,560 --> 00:24:48,720
model that the robot is reasoning about in order to be able to collaborate with the human partner.

248
00:24:49,440 --> 00:24:56,400
So that gives you as a Palm D.P. And then you want to be able to solve that Palm D.P. online.

249
00:24:58,240 --> 00:25:02,560
And that and what does it mean to solve it online? It means to give the robot the ability to

250
00:25:02,560 --> 00:25:08,960
choose its actions, whether it's like physical actions or in some cases, even communicative actions

251
00:25:10,160 --> 00:25:16,160
to be able to reason on the uncertainty of what the person will do, but also to be able to

252
00:25:16,160 --> 00:25:21,840
reduce uncertainty based on the old action. And this is quickly interject Palm D.P. is partially

253
00:25:21,840 --> 00:25:26,640
observable Markov decision process. That's it, yep, yep. And the

254
00:25:26,640 --> 00:25:34,160
going back to the beginning of your explanation, do you have one partially observable model,

255
00:25:34,160 --> 00:25:41,600
like you have this task model that is, you know, which there's some noise of what the human might,

256
00:25:41,600 --> 00:25:47,440
how the human might perform that task, or do you have two distinct models, one about the human?

257
00:25:47,440 --> 00:25:52,800
And well, it sounds like one is the human's cognitive state, which you're using to influence

258
00:25:52,800 --> 00:25:59,440
the task model. Yes, yeah. So the human's cognitive state is not directly observable.

259
00:25:59,440 --> 00:26:04,560
Therefore, the model of your human is partially observable, like you can see physically what they're

260
00:26:04,560 --> 00:26:08,880
doing, but there's some elements about what they're doing that you can't directly observe.

261
00:26:10,160 --> 00:26:17,840
And because of that, you know, the robot is, you know, is aiming to reason about, you know,

262
00:26:17,840 --> 00:26:23,200
about how it should be behave or after what actions it should take considering this partially

263
00:26:23,200 --> 00:26:32,400
observable model of human behavior. The task model, in some of our works, we assume it's fully known

264
00:26:32,400 --> 00:26:38,000
and fully observable. But, you know, just as it's really important to lower the barrier,

265
00:26:38,000 --> 00:26:45,040
to quickly and easily learning a human model to work with a person, it's really important in

266
00:26:45,040 --> 00:26:55,680
many contexts to lower the barrier of enabling like a shop floor or a line worker or a domain expert

267
00:26:55,680 --> 00:27:02,720
to teach a robot a task without an applications engineer as the intermediary, or in like in our lab

268
00:27:02,720 --> 00:27:09,920
without, you know, your AI researcher as the intermediary. And when you, when you aim to do that,

269
00:27:09,920 --> 00:27:16,320
you know, you're no longer directly specifying the task and it's often advantageous for the robot

270
00:27:16,320 --> 00:27:22,160
to maintain a belief over the true task specification as well. So it may not definitively know, you know,

271
00:27:22,160 --> 00:27:27,040
the, the specification for how to form the task, but to hedge against its uncertainty, maybe in

272
00:27:27,040 --> 00:27:30,880
some particular context, it really needs to be very, very conservative if it's a safety critical

273
00:27:30,880 --> 00:27:37,600
context. In other contexts, it might have more flexibility without there being a major consequence.

274
00:27:37,600 --> 00:27:45,200
But that's another form of, you know, partial observability that might, that would go into a

275
00:27:45,200 --> 00:27:48,240
Palm DP model that includes both an agent model and a task model.

276
00:27:49,040 --> 00:27:56,000
Yeah, I've come into contact with, well, not physically, but there are like these co-robots or

277
00:27:56,000 --> 00:28:03,600
co-bots that are in use. I think there's a backster robot like there are models for humans interacting

278
00:28:03,600 --> 00:28:11,680
with robots. But I imagine they're fairly brittle when the human does something kind of outside

279
00:28:11,680 --> 00:28:17,120
of the script. Maybe the robot just waits for human to get back in the right position or something

280
00:28:17,120 --> 00:28:23,840
like that to continue the script. And what I'm hearing is that this is a direction for

281
00:28:23,840 --> 00:28:34,640
building robots or robot AI's that can more gracefully interact with humans. Is that the big picture idea?

282
00:28:34,640 --> 00:28:39,360
Yeah, that, that, that is the big picture idea. Like the big, the big picture idea is to figure out

283
00:28:39,360 --> 00:28:47,040
how you can develop and field robots that don't require a human to be a robot to work with it.

284
00:28:47,040 --> 00:28:52,000
So, because that's, that's, that often doesn't work very well. But, you know, there are actually

285
00:28:52,000 --> 00:28:57,200
many of the applications out there today. I mean, humans are very, very adaptable, but it comes

286
00:28:57,200 --> 00:29:04,000
at a cost. So, a cost in many different dimensions. There could be one angle where you look at how you

287
00:29:04,000 --> 00:29:11,120
develop and play intelligent robots to take over or supplant elements of what's being done by a human

288
00:29:11,120 --> 00:29:20,400
today. And there is an alternate approach where you sort of take, take what can easily be done

289
00:29:20,400 --> 00:29:26,800
physically by a robot today, but then recognize it's, it's, it's little pieces of existing manual

290
00:29:26,800 --> 00:29:32,640
work that can be done by a robot. And now you have this integration challenge and then developing AI

291
00:29:32,640 --> 00:29:39,120
to help ease that those integration challenges. And a lot of those integration challenges occur

292
00:29:39,120 --> 00:29:48,000
because humans are the ultimate uncontrollable entity. So, being able to adapt or reason explicitly

293
00:29:48,000 --> 00:29:55,040
over the uncertainty and what a human will do becomes really important. And a key part of doing

294
00:29:55,040 --> 00:30:01,920
that is being able to, for the robot, specify like, what are the true constraints that underlie,

295
00:30:02,480 --> 00:30:06,960
the tasks that needs to be done, and what's acceptable for working with the person.

296
00:30:08,480 --> 00:30:11,680
And then if you have a predictive model, what the person will do, that's all the better.

297
00:30:11,680 --> 00:30:17,840
Because then you can, you know, you can, you can plan over a time price and whether it be less than

298
00:30:17,840 --> 00:30:25,760
a second or tens of seconds. And the, and the interaction can be a little, a little more fluid.

299
00:30:27,040 --> 00:30:34,480
You, the, the, like the backster, the backster is an example of like one of these collaborative robots

300
00:30:34,480 --> 00:30:39,760
that are really game changing because they're safe enough to be right alongside a person. You

301
00:30:39,760 --> 00:30:43,920
don't need a cage. You don't need to remove them physically from the same space as people.

302
00:30:43,920 --> 00:30:48,720
And there's, we've used the universal robot and some of our deployments. We really enjoy the

303
00:30:48,720 --> 00:30:57,120
Franca, Amica robot. But many of the installations of these robots are still working independently of

304
00:30:57,120 --> 00:31:02,640
people in, in production environments. They're not working interdependently. And because of that,

305
00:31:02,640 --> 00:31:07,520
they're limited in, in the value they can provide or the places they can be easily deployed. And so,

306
00:31:07,520 --> 00:31:14,240
a key motivation of our work is, is enabling that interdependence, but rather than forcing the

307
00:31:14,240 --> 00:31:20,320
person to like adhere to the fixed, you know, robot motion and a robot schedule, allowing the robot to

308
00:31:20,320 --> 00:31:27,520
accommodate the natural flexibility or uncertainty that a person brings. And, and now the scenario that

309
00:31:27,520 --> 00:31:34,720
we've talked about thus far kind of creates this picture of a human with their kind of partner robot

310
00:31:34,720 --> 00:31:41,280
working on some tasks together. But does it, to what extent do you envision it extending to

311
00:31:41,280 --> 00:31:48,480
teams of, you know, one robot embedded within a team of humans or multiple robot, one human

312
00:31:48,480 --> 00:31:55,600
with embedded within a team of robots or, you know, a more diverse mixture of humans and robots

313
00:31:55,600 --> 00:32:02,800
kind of working together on, on some kind of tasks? Yeah. The work that I've been describing so far

314
00:32:02,800 --> 00:32:08,560
around the, the techniques for the non-perject-based and techniques for inferring latent state and

315
00:32:08,560 --> 00:32:15,760
their dynamics, that, that is the PhD work of Bayblab on Holcker, who is currently an assistant

316
00:32:15,760 --> 00:32:23,120
professor at, at Rice, at Rice University. And, actually, even though, you know, many of those

317
00:32:23,120 --> 00:32:31,120
studies end up being, you know, one to one, one person, one robot, the, the motivation for a

318
00:32:31,120 --> 00:32:37,840
lot of our work is the recognition that, you know, a lot of our work is done in teams. And we do

319
00:32:37,840 --> 00:32:44,480
need to know more than just the cognitive state or mental state of, you know, one, one partner

320
00:32:44,480 --> 00:32:50,960
in order to succeed. So then that race is really interesting questions about like, what is a team

321
00:32:50,960 --> 00:32:58,160
cognitive state? And how do you model that? And how can the techniques developed for, you know,

322
00:32:58,160 --> 00:33:07,600
modeling like one black box agent, like a person, extend to modeling the sort of, like, the team

323
00:33:07,600 --> 00:33:15,920
concept, the prior work on team, team performance and, and team coordination and communication.

324
00:33:15,920 --> 00:33:20,640
You know, those settings, maybe other than the pilot, co-pilot, you know, narrow scenario, those

325
00:33:20,640 --> 00:33:27,600
are all team settings involving more than two, two people. And there is this notion that team

326
00:33:27,600 --> 00:33:33,760
cognitive state is, is something different. And a lot of thinking and study about like what,

327
00:33:33,760 --> 00:33:40,480
what, what's encompassed in a team cognitive state? And they've, and I have a really exciting

328
00:33:40,480 --> 00:33:46,480
collaboration with Harvard Medical School looking at their operating room and like cardiac

329
00:33:47,520 --> 00:33:55,200
surgeries and the very complex dynamics involved in that. And the, the researchers there, they have

330
00:33:55,200 --> 00:34:01,200
this like super futuristic, you know, surgical simulation environment that they train the residents

331
00:34:01,200 --> 00:34:07,040
and, and grants from NASA to study, you know, human teamwork and failures and human teamwork

332
00:34:07,040 --> 00:34:14,400
from a human factors perspective. And we're in, you know, in discussion currently. And they both

333
00:34:14,400 --> 00:34:22,000
unhulk her in particular has a really exciting recent working paper out of his new lab in,

334
00:34:22,000 --> 00:34:28,480
again, these synthetic scenarios, but in extending these techniques to model some aspects of

335
00:34:28,480 --> 00:34:37,520
team cognitive state, flow, workflow disruptions, sort of miscommunication or signals among a team

336
00:34:37,520 --> 00:34:44,080
that, that have adverse outcomes for, for a team's performance. And again, you have this really

337
00:34:44,080 --> 00:34:52,160
interesting question of training time versus online and what, you know, like, what, what can be predicted

338
00:34:52,160 --> 00:34:58,000
with batch versus online and with what fidelity and how does that flow in to actually training

339
00:34:58,000 --> 00:35:03,840
on you, surgeon to identify the team factors that affect performance because this is done like

340
00:35:03,840 --> 00:35:09,040
an action review or is there a way to bring these techniques actually online in an operating room

341
00:35:09,040 --> 00:35:16,800
and show some aspects of, like, current coding or rating of the team state that can help, you know,

342
00:35:16,800 --> 00:35:28,080
potentially help potentially spur sort of a repair action online that might be useful.

343
00:35:29,600 --> 00:35:35,520
Yeah, it's a great question and work that is newer and early underway.

344
00:35:35,520 --> 00:35:42,080
Yeah, it sounds like there are elements of that that are kind of fundamental psychology

345
00:35:42,080 --> 00:35:50,720
research. Like, you know, what is, you know, what is team cognition separate from the individual

346
00:35:50,720 --> 00:35:55,920
cognition, the cognition of the individuals on the team and what would a representation of that,

347
00:35:55,920 --> 00:36:02,400
you know, even start to look like. Yeah, yeah, the work on that is, so the frameworks that

348
00:36:02,400 --> 00:36:11,440
exist for it are not computational frameworks currently, but we've had a, you know, one of the

349
00:36:11,440 --> 00:36:19,360
great joys and, you know, I think from the work in our lab is talking with, you know, researchers

350
00:36:19,360 --> 00:36:27,040
and cognitive psychology and also cognitive science and sometimes those works really have like the

351
00:36:27,040 --> 00:36:33,920
sort of the basis for that sort of feature, that feature identification problem that you raised.

352
00:36:35,440 --> 00:36:40,400
We've had success in, for example, taking dialogue acts related to,

353
00:36:42,400 --> 00:36:48,480
to shared understanding developed in behavioral psychology and cognitive science literature

354
00:36:48,480 --> 00:36:55,520
and use that to develop an inference system to be able to infer a team's state of understanding

355
00:36:55,520 --> 00:37:02,400
of like a common plan that they're, they're discussing. And so there, there is a lot of opportunity

356
00:37:03,680 --> 00:37:09,520
to leverage the decades of research and insight developed in these other fields and work together

357
00:37:09,520 --> 00:37:16,080
to develop computational models that can advance the automated capability of systems to, to shore up,

358
00:37:16,080 --> 00:37:22,160
you know, weaknesses that human teams sort of naturally have in performing these challenging tasks.

359
00:37:22,160 --> 00:37:29,360
You've also done some work on in the domain of cross-training between humans and robots.

360
00:37:29,360 --> 00:37:35,200
When I hear that, I think a little bit of like imitation learning, but I think your approach is,

361
00:37:36,000 --> 00:37:40,480
you know, in a fairly in a different direction than that. Can you talk a little bit about that work?

362
00:37:40,480 --> 00:37:48,880
Yes, yes. So my lab explored this idea of cross-training or team training applied to

363
00:37:48,880 --> 00:37:55,360
human robot teams. And this goes back to the very start of the lab. And the researcher that,

364
00:37:55,360 --> 00:38:00,000
that did this research is, was my, was a master student in my lab at the time,

365
00:38:00,000 --> 00:38:06,160
Stephen O's Nikolatus. And he's now an assistant professor at USC. And the motivations of that work

366
00:38:06,160 --> 00:38:12,880
was, if you looked, you know, then a technique from, from the literature and learning from

367
00:38:12,880 --> 00:38:18,800
demonstration. It would be like a person showing a task to a robot or walking a robot, you know, through,

368
00:38:18,800 --> 00:38:25,440
through the, the steps of the task. But if your goal was interdependent action between a person

369
00:38:25,440 --> 00:38:30,320
and a robot, like, how do you demonstrate that, right? Like, it's like, you'd be like, okay, robot,

370
00:38:30,320 --> 00:38:34,000
you do this, okay, now I'm going to do this, okay, and then what you're going to do is do that,

371
00:38:34,000 --> 00:38:38,960
and then I'm going to do that. It becomes kind of awkward to think through. And when you look

372
00:38:38,960 --> 00:38:45,360
at, again, the teaming literature, we have, we have many well-honed techniques, like tried and

373
00:38:45,360 --> 00:38:51,440
true techniques for training humans to work interdependently and very challenging tasks. And the,

374
00:38:51,440 --> 00:38:57,600
it's not always possible, but the gold standard is this thing called cross-training. So I do my part

375
00:38:57,600 --> 00:39:02,720
of the task, and you do your part of the task, and we try it out together, but then we switch roles,

376
00:39:02,720 --> 00:39:06,640
then like, I do your part of the task, and you do my part of the task, and we do that for a little

377
00:39:06,640 --> 00:39:11,840
while, and then we switch roles again. And it turns out from, you know, from, from a perspective

378
00:39:11,840 --> 00:39:16,880
of optimizing human learning, for a human, humans learning how to work in teams. This works better

379
00:39:16,880 --> 00:39:21,840
than just about anything else. And you, you can only really do it with small homogenous teams,

380
00:39:21,840 --> 00:39:27,040
like a nurse really can't take a surgeon's role. But when you have, it's kind of small homogenous

381
00:39:27,040 --> 00:39:32,640
teams, like, this is the, this is the way to train your team. I imagine part of the rationals that

382
00:39:32,640 --> 00:39:37,760
you kind of develop empathy and understanding of the other role, and it helps you, like we talked

383
00:39:37,760 --> 00:39:43,280
about before, kind of subtly adapt the way you do your thing so that the needs of the partner.

384
00:39:43,280 --> 00:39:48,000
Exactly, exactly. Like, when I do your job, I suddenly realize, like, you know, the challenges

385
00:39:48,000 --> 00:39:52,640
of you doing your job when I, you know, and then you come back and you adjust the way you do things

386
00:39:52,640 --> 00:40:00,320
for the benefit of the team. Exactly. And so, one of our, one of our goals early on was to think

387
00:40:00,320 --> 00:40:07,360
about, well, how can you optimize human robot team performance in following this type of learning

388
00:40:07,360 --> 00:40:13,520
curve? And so the first, you know, the, the first try at this was to say, well, what if,

389
00:40:13,520 --> 00:40:19,840
what if a human just gives the robot inputs the way, you know, the way a human would experience

390
00:40:19,840 --> 00:40:24,000
inputs when working with another person through cross-training? So in this, in that paradigm,

391
00:40:24,000 --> 00:40:29,280
first the human did their role and the robot did its role. But then the human took the robot's role

392
00:40:29,280 --> 00:40:33,840
in a simulation environment and, like, pretended to be the robot. And then the robot, you know,

393
00:40:33,840 --> 00:40:39,280
did, you know, something that the human would do. The ability to improve the quality of the

394
00:40:39,280 --> 00:40:44,000
human's mental model of the robot was substantial. We were able to show that in experimentation.

395
00:40:45,120 --> 00:40:49,920
But there was also some evidence that the inputs the person was giving in this,

396
00:40:49,920 --> 00:40:55,200
we important this modified reinforcement model where of higher quality than you would get through

397
00:40:55,200 --> 00:41:01,360
you know, standard approaches to reinforcement learning with just positive and negative feedback

398
00:41:01,360 --> 00:41:06,880
or reward. And that's because if a robot does something and you say, like, good robot, like,

399
00:41:06,880 --> 00:41:12,000
there's, there's questions as to what you mean by good robot. Was it good robot for the thing

400
00:41:12,000 --> 00:41:16,560
the robot just did or is it good robot for the thing that you think the robot is about to do next?

401
00:41:17,680 --> 00:41:22,240
Whereas when you take the equivalent of those inputs from the person actually doing the robot's

402
00:41:22,240 --> 00:41:28,560
job, that's more like a direct demonstration of the person for the robot. Was this project

403
00:41:28,560 --> 00:41:36,400
implemented in simulation or is there a physical implementation of this? Yeah, so the cross-training

404
00:41:36,400 --> 00:41:40,960
was implemented in simulation. So the person and robot kind of played together in this game

405
00:41:40,960 --> 00:41:46,480
environment and the robot, you know, in simulation showed the movements it would make. But they weren't

406
00:41:46,480 --> 00:41:52,560
physically working together. It was in a in the 2D simulation environment. And then we had the

407
00:41:52,560 --> 00:41:58,160
person walk over to the lab to the physical robot and do the task with the physical robot after

408
00:41:58,160 --> 00:42:04,240
training under different paradigms. And when the person robot trained via cross-training and simulation,

409
00:42:04,240 --> 00:42:10,400
there were both objective and subjective measures benefits to the physical interaction between

410
00:42:10,400 --> 00:42:17,040
the person and the robot. So that leads to an exciting direction where you can imagine, you know,

411
00:42:17,040 --> 00:42:22,000
a person robot or a person teaching a robot and then learning to collaborate with a robot entirely

412
00:42:22,000 --> 00:42:27,280
in simulation. Off the assembly line without money flying by and then you just walk out onto the

413
00:42:27,280 --> 00:42:30,640
assembly line and like you're good to go, you built your mental model of how to work with this

414
00:42:30,640 --> 00:42:37,760
robot and it's learned its model of how you'll behave and when working with it. Nice. What are some

415
00:42:37,760 --> 00:42:43,680
of the things you're most excited about looking forward? That early work in the lab in cross-training,

416
00:42:43,680 --> 00:42:51,280
we still have the goal of optimizing a human and robots ability to co-learn how to work together.

417
00:42:51,280 --> 00:42:58,640
That early work in cross-training was, you know, a part of the reason we pursued it in that particular

418
00:42:58,640 --> 00:43:04,960
way was because like what does it mean to optimize a human's learning of how to work with a

419
00:43:04,960 --> 00:43:12,320
learning robot? Like I didn't know, but I knew that via certain structured interactions a human

420
00:43:12,320 --> 00:43:18,160
should learn better. And you know, so we implemented cross-training and we're able to show this,

421
00:43:18,160 --> 00:43:24,400
you know, really strong benefit. Much of our work since has been looking at trying to more

422
00:43:24,400 --> 00:43:30,960
directly support and optimize the human's learning via the approach I mentioned earlier about

423
00:43:30,960 --> 00:43:37,760
enabling a person to guide a machine learning's model ability to align its learned model with

424
00:43:37,760 --> 00:43:45,600
the human mental model. So how can we learn a machine learning model that a latent model that

425
00:43:45,600 --> 00:43:53,200
is well aligned with the human mental model directly? Not indirectly via exercising certain

426
00:43:53,200 --> 00:44:00,800
forms of interaction. And the question you raised about how you do this at runtime is really

427
00:44:00,800 --> 00:44:07,680
exciting and really interesting. On the physical side, I have a group in the lab focused on this

428
00:44:08,800 --> 00:44:16,640
because if you say you're on an automotive line which we were and you want to gather your

429
00:44:16,640 --> 00:44:20,960
model of a person doing the task so that you can introduce the robot, you have a chicken and an

430
00:44:20,960 --> 00:44:24,400
egg problem because when you collect your data of the person doing the task on their own

431
00:44:25,040 --> 00:44:28,960
and then you introduce the robot, suddenly the person does their task entirely differently

432
00:44:28,960 --> 00:44:33,200
because the robot is there. It's like out of distribution. So then we're like, okay, so we get

433
00:44:33,200 --> 00:44:37,280
these real associates come through and then we're like, okay, here's what we're going to do.

434
00:44:37,840 --> 00:44:42,640
I'm going to be the robot here. Look, I'm moving like a robot. You do your task like you would with

435
00:44:42,640 --> 00:44:46,880
me as the robot supporting you and then you try to get your data that way and learn the model and

436
00:44:46,880 --> 00:44:52,000
then you introduce the actual robot and then the person does it differently again. And so the ability

437
00:44:52,000 --> 00:45:01,360
of a system to be able to follow or even guide that that learning process of a person to work with

438
00:45:01,360 --> 00:45:08,960
the robot becomes very important, very practically important to successful deployment of the system.

439
00:45:08,960 --> 00:45:17,120
And then we have a line of research also looking at deep models, neural models and

440
00:45:17,120 --> 00:45:23,040
following a similar line of research. Like, what are the inputs a person can give to shape that

441
00:45:23,040 --> 00:45:29,040
latent space so that it better aligns with a human-held mental model that's useful for some

442
00:45:29,040 --> 00:45:37,520
specific task. So moving from the Bayesian graphical model to deep models and, you know,

443
00:45:37,520 --> 00:45:45,040
and enabling that same capability there. But all towards making these systems much more easily

444
00:45:45,040 --> 00:45:51,200
shapeable and adaptable to the needs of someone who's not a machine learning expert.

445
00:45:51,200 --> 00:45:55,840
Well, Julie, thanks so much for joining us and sharing a bit about what you're up to.

446
00:45:55,840 --> 00:46:23,840
Thank you so much for having me.

