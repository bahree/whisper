1
00:00:00,000 --> 00:00:25,000
All right, everyone. I am on the line with Claire Montellioni. Claire is an associate professor at the University of Colorado Boulder. Claire, welcome to the Twoma AI podcast. Thanks. I'm excited to be here.

2
00:00:25,000 --> 00:00:39,000
I'm super excited to have you on a show and looking forward to our conversation. We are of course going to dig into all of the amazing work you've been doing in the client climate informatics field.

3
00:00:39,000 --> 00:00:53,000
But to get us started, why don't you share a little bit about your background and how you came to work at this confluence of data science, machine learning and climate change.

4
00:00:53,000 --> 00:01:08,000
Yeah, so actually I grew up in New York City and was involved in environmental activism in high school. We organized the first environmental awareness day at our high school.

5
00:01:08,000 --> 00:01:20,000
You know, this is in the late 90s, Mayor Jenkins came and spoke. This was in New York City. And I remember we serve lunch on frisbees, you know, so that they would be reusable.

6
00:01:20,000 --> 00:01:34,000
So some fun images there. And then I came to college, you know, with science interests, there were major such as environmental science and public policy, which I dabbled in, but then I did earth and planetary sciences.

7
00:01:34,000 --> 00:01:44,000
I really wanted to understand climate change, you know, there were issues of acid rain and ozone hole and certainly global warming even back then.

8
00:01:44,000 --> 00:01:52,000
But I got more fascinated by the math of computer science. I had to take computer science to understand how these climate models worked.

9
00:01:52,000 --> 00:02:17,000
But really got hooked in by things like logic and recursion and all the theoretical topics that fascinated me. So I studied AI machine learning for my PhD and really environmentalism was on the back burner as more of an activist area and not an area of research for many years.

10
00:02:17,000 --> 00:02:36,000
But then when I was finishing my postdoc and applying for jobs, I wrote about this idea of climate informatics because during grad school, I had seen bioinformatics emerge as really revolutionizing various areas of biology by bringing in machine learning right.

11
00:02:36,000 --> 00:02:48,000
And I really wanted to do the same for the study of climate change. And I was really lucky that along the way, there were more senior people because I was a junior person who believed in this vision.

12
00:02:48,000 --> 00:03:06,000
So one was my late boss at the Columbia Center for computational learning systems, David Waltz, he hired me and he said, oh, climate informatics, this sounds great. We can hire a whole group on this if you'd like. I said, I'd like to start a conference and he said, you know, go for it.

13
00:03:06,000 --> 00:03:19,000
He also was able to find the right climate scientist with which to do the first paper with who was Gavin Schmidt, he's now the head of the NASA office in New York City.

14
00:03:19,000 --> 00:03:39,000
And he's, you know, part of the IPCC and we did a proof of concept showing that we could reduce uncertainty on an ensemble prediction and ensembles, of course, our big and AI and machine learning and something I had been working on, but that's how predictions are made to the UN panel.

15
00:03:39,000 --> 00:04:02,000
To the intergovernmental panel on climate change by people running physics driven models and then trying to interpret the high variance and spread from models from different countries. I mean, you've seen, even just for storms that sometimes the European model says the cyclone is going to hit Florida, but the American model might say that it wouldn't.

16
00:04:02,000 --> 00:04:20,000
So together we started working in this area and then we launched a conference. I gave a tutorial at NERIPS in 2014 trying to spur machine learners in this field.

17
00:04:20,000 --> 00:04:35,000
And yeah, there's been some further developments, but the environmental science interest dates back to high school and then I've been doing computer science since late college.

18
00:04:35,000 --> 00:04:46,000
Awesome, awesome. And now when you took your your break from working in the environmental area and you were doing your doctorate, what was your focus there?

19
00:04:46,000 --> 00:05:09,000
Right. Yeah. So I was actually not on applied research at all. I was more in a theoretical area. So one area that was really interesting to me, which was inspired by how humans learn was online learning in particular when your data or the model that you would like to emulate changes over time.

20
00:05:09,000 --> 00:05:29,000
So learning in a game where the rules are always changing, the analog I was thinking of at the time was the stock market. Right. But later it made a lot of sense to do this for climate change, right. We can't just, you know, regress a model and then expect that the distribution underlying the data is not changing.

21
00:05:29,000 --> 00:05:41,000
So it's really focusing on learning with non stationarity, which there was some work, but there were a lot of open issues there.

22
00:05:41,000 --> 00:05:54,000
And I also worked on active learning as sort of my my gateway to completely unsupervised learning. So what can you do when there's just really a limit on how much labeled data that you have.

23
00:05:54,000 --> 00:05:59,000
And also some work on on privacy.

24
00:05:59,000 --> 00:06:04,000
Okay. Privacy as in differential privacy and things of that nature.

25
00:06:04,000 --> 00:06:17,000
Yeah. So differential privacy had come out in the theory community. But my colleague, Kamalika, Chaudhary, and I published the first nirips paper on privacy preserving machine learning.

26
00:06:17,000 --> 00:06:30,000
Where we, you know, took the algorithms of the time logistic regression and support vector machine and tried to re you know reengineer them to match differential privacy guarantees.

27
00:06:30,000 --> 00:06:35,000
It's not an area that I've pushed on in a while, but we did.

28
00:06:35,000 --> 00:06:48,000
So I do, you know, with past students and postdocs did papers on semi supervised privacy preserving machine learning privacy preserving unsupervised learning so privacy preserving dimensionality reduction.

29
00:06:48,000 --> 00:06:55,000
And it kind of falls into my interest in doing AI for the service of social good.

30
00:06:55,000 --> 00:07:02,000
Of course, now I'm mostly focused on environmental areas that have social good.

31
00:07:02,000 --> 00:07:09,000
And so can you, you shared a bit of this in your background, but can you kind of.

32
00:07:09,000 --> 00:07:18,000
Kind of lay out the landscape of your research interests and what your group is working on now is it primarily.

33
00:07:18,000 --> 00:07:25,000
Is it all applied in the environmental area, are you also doing theoretical work.

34
00:07:25,000 --> 00:07:36,000
Good question. There are some students doing purely theoretical some purely applied and then I really like to operate at the sweet spot.

35
00:07:36,000 --> 00:07:47,000
We have some interesting stories where, you know, I told you that we had been working on online learning with non stationarity so the data is varying with time.

36
00:07:47,000 --> 00:07:57,000
The idea of tracking the best expert predictor or tracking a switching expert of the switching best expert.

37
00:07:57,000 --> 00:08:10,000
And also parameter free learning we had an early parameter free algorithm for learning the rate at which, you know, how bursty or how changing is this, this distribution.

38
00:08:10,000 --> 00:08:25,000
What did that with respect to time. And that was the first proof of concept I did with the climate scientist to show that we could robustify the ensemble prediction of the IPCC climate models using our algorithms.

39
00:08:25,000 --> 00:08:28,000
But then I had a student.

40
00:08:28,000 --> 00:08:40,000
So for his PhD, we looked at how you do this over space and time because all the data is space, your temporal, it'll vary over both latitude, longitude and time.

41
00:08:40,000 --> 00:09:00,000
And so at triple AI ways back, we published this algorithm in the context of of climate models, this algorithm that would learn the level of sort of changeability or non stationarity over both time and space while doing the learning tasks.

42
00:09:00,000 --> 00:09:08,000
So while trying to combine the climate model ensembles predictions at now a whole time space grid.

43
00:09:08,000 --> 00:09:22,000
And so this was just an algorithmic paper, there were no theorems, but if you looked at the field of online learning, it was pretty open to also have a spatial dimension or really any dimension.

44
00:09:22,000 --> 00:09:31,000
So then it turned out some finance people that I was talking about were interested in it for where the other dimension is markets and other countries.

45
00:09:31,000 --> 00:09:48,000
So some proof proof of concepts in that domain. But this really came full circle, I was on sabbatical in Europe and I visited Niko Locheza Bianchi, who has written, like, probably, or co written one of the main important textbooks on online learning.

46
00:09:48,000 --> 00:10:10,000
With with his PhD student, we talked about the theory, how would we evaluate algorithms and how would we even think about regret when you have online learners in a spatial grid with some interaction and some observation of the losses that the others are experiencing what are sort of positive and negative results that you can get there.

47
00:10:10,000 --> 00:10:20,000
And we, you know, we ended up publishing it last year in algorithmic learning theory. So I do like trajectories that hit both the theory and the practice.

48
00:10:20,000 --> 00:10:28,000
And I mean academia. So of course, that's good for the students careers as well. Right. They were trying to forge this new domain.

49
00:10:28,000 --> 00:10:44,000
And we're succeeding. There are people that have crossed disciplinary careers, someone who did a master's with me and CS and is now in a climate science lab doing a PhD. But in the meantime, while students are here, they do need to publish and machine learning venues.

50
00:10:44,000 --> 00:10:47,000
So I like to work at the sweet spot between the two.

51
00:10:47,000 --> 00:11:13,000
Nice. Nice. Yeah, I think we want to get into your recent keynote at CVPR, but continuing the thread of your your broad research interests. I'm wondering if you can speak about the, this is the opportunity and applying machine learning to climate informatics climate change environmental science in general are there.

52
00:11:13,000 --> 00:11:23,000
How do you think about the field? How do you taxonomize it? What's the research frontier? Yeah.

53
00:11:23,000 --> 00:11:34,000
Well, first, you know, if some of your listeners are considering an application area, I would strongly consider or recommend the study of climate change.

54
00:11:34,000 --> 00:11:54,000
One, I mean, there are a few unique things about it. So if we compare it, say, to biological applications, there's much more copious data here. The data has generally been collected by public missions of NASA publicly disseminated by USGS NASA and NOAA.

55
00:11:54,000 --> 00:12:10,000
And sometimes data that is simulated, so the output of physics driven models, we use as input for our machine learning because it encodes a lot of scientific knowledge and a lot of conservation laws from physics.

56
00:12:10,000 --> 00:12:36,000
And so even though we can't see the future, we have this artificial lens into the future via those climate simulations. So that's really unique. And I don't know of any other field where people have been working for 50 or 40 or, I don't know, since the late 60s to, to simulate the thing that you're trying to predict, you know, my colleagues in industry who want to predict whether a user is going to click on an ad.

57
00:12:36,000 --> 00:12:49,000
That's a really hard problem. And in climate physics does give us some predictability and top physicists and meteorologists have been simulating the climate using mathematical models for years.

58
00:12:49,000 --> 00:13:12,000
So I heard a climate scientist say that more data that is simulated from climate models has been generated and stored than all the satellite retrievals to date. So we actually have a lot of simulated data, but there hasn't been a huge amount of effort of analyzing it in an automated way.

59
00:13:12,000 --> 00:13:28,000
So we're in a very data rich scenario. And then if you think about it, it's probably the cheapest key to unlock these insights is to use data science and an AI to find underlying patterns and, you know, latent structures that we can interpret.

60
00:13:28,000 --> 00:13:42,000
So, sorry, I was just going to say you don't hear that very often from machine learning researcher that, you know, there are any data rich environment and data is not their primary challenge.

61
00:13:42,000 --> 00:13:44,000
Right.

62
00:13:44,000 --> 00:14:06,000
And then the other thing is interfacing with the domain experts. So there are a lot of areas to climate change research. And, you know, I've tried some of them, but not all of them, but I know from experience that the climate scientists themselves, many of whom are climate modelers are computer savvy.

63
00:14:06,000 --> 00:14:15,000
They're running the high performance computing, they're writing the code. So it allows analytics to be this real value add that you can just plug in.

64
00:14:15,000 --> 00:14:29,000
Whereas if you're starting from scratch, and I, you know, admire the work in other domains, but it's quite hard to start from scratch where it's hard to even speak speak the same language around computing.

65
00:14:29,000 --> 00:14:36,000
So, yeah, and, you know, it's pretty important. It's been pretty hot this summer.

66
00:14:36,000 --> 00:14:40,000
You know, you may have seen a lot of wildfires last fall.

67
00:14:40,000 --> 00:14:43,000
I think it's pretty impactful.

68
00:14:43,000 --> 00:14:55,000
Even the rich history of climate scientists in computing.

69
00:14:55,000 --> 00:15:05,000
Can you maybe talk a little bit about how the use of computing has evolved now that you're, or we're kind of introducing machine learning and analytics.

70
00:15:05,000 --> 00:15:23,000
And what were the kinds of analysis that analyses that they were doing before, and how did that, how did they differ from what you would do introducing in more analytical and ML and a methods.

71
00:15:23,000 --> 00:15:41,000
So, um, just like building any inter to the plenary field, we want to read the literature and, you know, welcome everyone into sort of a big tent. And so, um, there were, there's a whole field on statistical climatology.

72
00:15:41,000 --> 00:15:55,000
And, you know, Bayesian methods were being used. The, the special sauce that machine learning brings, um, beyond statistics or geospatial statistics is often algorithmic thinking.

73
00:15:55,000 --> 00:16:06,000
And anyone who's done a computer science degree will be aware of that and how you have an analysis algorithms class in your in CS, but you don't necessarily have that in other departments.

74
00:16:06,000 --> 00:16:23,000
Um, so making, um, things more efficient. We could be talking about running time. We could be talking about data efficient. We could be talking about efficient with respect to the amount of labeled data that you have, which can often be the bottleneck in these real domains.

75
00:16:23,000 --> 00:16:44,000
So, I think computer science did have a lot to add, but we had to find the domain experts who were open to that. Um, and then we also was very important to us to us to have a big tent and include statistics geostatistic statistical climatology.

76
00:16:44,000 --> 00:17:09,000
Um, and a range of, um, fields in computer science in, um, in data science. Actually, another just anecdotes. So, um, I got this job writing about climate informatics before ever having published about it, but hoping we could do for climate, the study of climate change what, um, bioinformatics had done for biology and, um,

77
00:17:09,000 --> 00:17:21,000
the Gavin Schmidt who I eventually collaborated with came to the CS department at Columbia, looking for something. Now he didn't know what exactly he was looking for. He's a climate scientist with lots and lots of data.

78
00:17:21,000 --> 00:17:43,000
But they, they got together this meeting of computer science professors that the chair had tapped kind of violin told you need to go attend that meeting. So everyone was sort of, you know, a little bit bored or something. And I was really excited multiple people had forwarded it to me because they knew I was looking for this.

79
00:17:43,000 --> 00:17:52,000
And I was the kind of snarky person in the back of the room anytime Gavin explained something I said, have you tried machine learning for this problem.

80
00:17:52,000 --> 00:18:11,000
And so, you know, right after the talk, we, we literally got lunch right then and there. And that's actually when we started hashing out the idea for using this online learning that you would usually use to say predict the stock market, use it to robustify the predictions of the IPCC ensemble.

81
00:18:11,000 --> 00:18:25,000
Now there's a range of, you know, systems and communication challenges. He was talking about also a challenge where say you're trying to study an extreme event that lives in time and space say a cyclone.

82
00:18:25,000 --> 00:18:29,000
So that's going to be a huge chunk of data in time and space.

83
00:18:29,000 --> 00:18:45,000
And you have these data servers at USGS or other publicly hosted sites, but even just the communications complexity of finding the boundaries in space and time of the pattern that you care about.

84
00:18:45,000 --> 00:18:54,000
He said he had a student that wrote a whole thesis on adaptive querying algorithms to just find the bounds of the data cube that you wanted.

85
00:18:54,000 --> 00:18:59,000
So there's a lot of systems challenges.

86
00:18:59,000 --> 00:19:08,000
Also mentioned Steve Easterbrook at University of Toronto, who now heads up, he's a computer science person in software engineering and he now heads up their school of environment.

87
00:19:08,000 --> 00:19:27,000
But he did a study where he embedded himself in climate modeling teams. I think he went to the UK for a few months, he went to NCAR here in Boulder and observed them because when you're studying the process of software engineering, there's a lot of sociological aspects that come in who's sharing whose code.

88
00:19:27,000 --> 00:19:37,000
You know, it turns out there's a model of sea ice that's used in multiple of the different models. So the, the climate models from different countries are correlated.

89
00:19:37,000 --> 00:19:48,000
He did a bug analysis and said that, you know, at the time, the bug rate was lower than when Windows NT, which was actually kind of a not a very high bar.

90
00:19:48,000 --> 00:19:55,000
So there's a lot of different directions and different areas of computer science that can come to bear.

91
00:19:55,000 --> 00:20:12,000
Awesome. So I suggested this before, but you recently delivered a keynote at the earth vision workshop at CVPR. Tell us about the keynote and your focus there.

92
00:20:12,000 --> 00:20:26,000
So that was an event, a workshop where it sounds like, you know, I wasn't an organizer, but it sounds like vision researchers are coming together around earth environmental and climate change problems as well.

93
00:20:26,000 --> 00:20:40,000
And of course, they're, you know, looking at image data, I also wanted to step back and say that any geospatial data can be viewed as image data, because what do you have, you have a matrix over, you know,

94
00:20:40,000 --> 00:20:50,000
longitude and latitude of some real value, it could be temperature, it could be precipitation. So there you've got an image as far as the back end math is concerned.

95
00:20:50,000 --> 00:21:03,000
If you add time, you've got essentially video, right. And so all the ideas from computer vision can apply and all the ideas that we develop for this kind of data can also apply back to computer vision.

96
00:21:03,000 --> 00:21:16,000
I wanted to empower sort of domain experts around using unsupervised learning, because often you're not going to have a lot of annotated data for your domain.

97
00:21:16,000 --> 00:21:39,000
I showed some case studies one in avalanche detection, and there the idea was that ground truth data literally was from humans trekking on the ground in very dangerous avalanche carders in the Alps to observe whether the snow had the characteristics of an avalanche having fallen on it.

98
00:21:39,000 --> 00:21:50,000
Now, if you're looking at the satellite imagery products, you're just looking at snow, and there's some subtle textural differences as to whether an avalanche has fallen.

99
00:21:50,000 --> 00:22:04,000
But our collaborators at meteor France in France, which is like Noah for France, hadn't found much skill even in humans labeling whether there had been avalanche deposits.

100
00:22:04,000 --> 00:22:13,000
So we had a situation which as a machine learner or computer vision researcher is more general, which is that you're looking for a rare event.

101
00:22:13,000 --> 00:22:21,000
What does that mean huge class imbalance is binary classification, but you have very, very few positive labeled examples.

102
00:22:21,000 --> 00:22:43,000
But in addition, there was just a limited set of label data and we weren't going to be able to get anymore. It was just what the people on the ground literally measures this ground truth survey, and we didn't have some in between product of images where humans had annotated it because they were still training the people and it was also quite hard for them to do that.

103
00:22:43,000 --> 00:23:02,000
So our thesis was ultimately you're going to do a semi supervised pipeline because in the end you want to classify avalanche. So there's some supervision as like the outer wrapper, but we wanted as much of the training as possible to be completely unsupervised.

104
00:23:02,000 --> 00:23:24,000
So you can learn an unsupervised model, say a variational auto encoder, and it would make sense intuitively that look, if I give the if I train only on images of snow without avalanches, then I'd have a model for sort of the non avalanche image or the non avalanche state.

105
00:23:24,000 --> 00:23:33,000
And it would make sense that I could look for I could try to classify images by on an unlabeled image send it through the trained VAE.

106
00:23:33,000 --> 00:23:43,000
And if the VAE kind of struggles to reconstruct it like it outputs a high reconstruction error where you're comparing the output of the network to the original input.

107
00:23:43,000 --> 00:23:52,000
Then we could say above some certain threshold that's an anomaly. And in this case, we were lucky that the anomalies were only avalanches because it was just different textures of snow.

108
00:23:52,000 --> 00:24:15,000
But if you think about where is the supervised learning needed for that pipeline, certainly tuning that hyper parameter, the threshold on reconstruction error above which you'll say that's an anomaly that, of course, will need a labeled validation set because that's a hyper parameter for an ultimately supervised task.

109
00:24:15,000 --> 00:24:29,000
But the way I explained it to you at first was, OK, let's learn a VAE just on the negative examples. We certainly did that and it definitely got a lift from our CNN supervised approaches that had to deal with the huge class imbalance.

110
00:24:29,000 --> 00:24:42,000
But it turned out that we said, you know what, we don't want there to be a bottleneck at the very beginning of our training where we have to look at the ground truth survey and compare it to images to find images and patches that don't have avalanches in them.

111
00:24:42,000 --> 00:24:48,000
Because then we're using up some of our label data, we can't use that in our validation set for hyper parameter tuning.

112
00:24:48,000 --> 00:25:01,000
So our kind of twist or creative thought here was, you know what, let's skip the supervised step at the beginning. Let's just lean on unsupervised learning.

113
00:25:01,000 --> 00:25:09,000
Let's just use all the satellite data we have other than some hold out labeled set to train the VAE.

114
00:25:09,000 --> 00:25:19,000
So we sort of argue that the supervision should only if you have a limited amount of supervision, it should only be used in tuning your hyper parameter.

115
00:25:19,000 --> 00:25:29,000
And we surprisingly got a significant lift there, which at first seems counterintuitive because the first version of the auto encoder.

116
00:25:29,000 --> 00:25:44,000
And you knew you were only training on on clean snow without avalanche. But the idea is in both cases, you know, whether it was only on the negative examples or on all the examples, you get your two VAE's.

117
00:25:44,000 --> 00:25:51,000
But in both cases, there's going to be that threshold hyper parameter for whether you're saying your reconstruction error is above a certain value.

118
00:25:51,000 --> 00:26:05,000
And in both cases, you do get to tune that on labeled data. And now the value of the thresholds might be different when you've trained on all the data that also had, you know, some noise in it because it actually had avalanches.

119
00:26:05,000 --> 00:26:21,000
But you train it to properly label a labeled validation set. And so they can both perform as well as they can. It turns out that the one with entirely unsupervised training of the VAE performed better.

120
00:26:21,000 --> 00:26:36,000
So our conjecture on that is in part, it's trained on more data, right, because, but secondly, to the extent that there's, you know, a few rare examples in there with avalanche in them, that gives you kind of more of an exploration of the feature space.

121
00:26:36,000 --> 00:26:44,000
And so we view that as really a form of regularization, which helps your ultimate model to be more generalized.

122
00:26:44,000 --> 00:26:54,000
So it was a cool trick. And I think it can apply really in any setting where you have a major class in balance, and you don't want to use a lot of labeled data.

123
00:26:54,000 --> 00:27:05,000
And so did you, you find that the, your, your margins were narrower, but the,

124
00:27:05,000 --> 00:27:18,000
the supervised training to find the threshold was, was it equally, were there any differences in the two cases and the ability to find the threshold.

125
00:27:18,000 --> 00:27:41,000
Now, the threshold values were different, but the threshold finding procedure was the same we use the same labeled validation set, just like if you report results of any two algorithms in a paper, they each get their hyper parameters tuned independently, but on the same labeled validation data set.

126
00:27:41,000 --> 00:28:07,000
And the, the conjecture of like fun that the introduction of the avalanche images, help the VAE kind of learn texture is, did you validate that in some way it's, it's conjecture, but did you, did you run any experiments to try to understand the kind of nuance there.

127
00:28:07,000 --> 00:28:17,000
Not directly, we did things like drill down and test only on,

128
00:28:17,000 --> 00:28:27,000
on even less imbalanced data sets like avalanche carders and the Alps that were known to have a lot of avalanches and that kind of actually increased the lift among the methods.

129
00:28:27,000 --> 00:28:52,000
But it would be interesting to try to derive so I mean another thing my group is really interested in is interpretability, the other paper I was going to talk about was, you know, about interpolations through latent space to bring two domains closer together, called domain alignment, but you bring up a really interesting interpretive ability question that we can maybe look at in future work.

130
00:28:52,000 --> 00:28:57,000
So is that the interpretability paper was that one that you also talked about at the.

131
00:28:57,000 --> 00:29:09,000
Yes, so this was a completely unsupervised method and understanding why it worked so well, brings in the idea of self supervision as well.

132
00:29:09,000 --> 00:29:22,000
And this is an extremely general method for the task of downscaling now that's the task coming out of, you know, climate, neurology, earth sciences, whether et cetera.

133
00:29:22,000 --> 00:29:32,000
And so down is the opposite of up so in computer vision, we would call that the task is sort of like super resolution or up sampling.

134
00:29:32,000 --> 00:29:48,000
But to be very clear then for this audience as I know your audience is AI folks downscaling means I have some gridded data say I have a map of temperature, but I'm at a course scale.

135
00:29:48,000 --> 00:29:58,000
So you know there's there's fewer grid points for a given geographical region region then at the fine scale and I'd like to infer a finer scale.

136
00:29:58,000 --> 00:30:08,000
But downscaling is called so this is super general, I mean we did proofs of concept for temperature and precipitation, but of course the this could be applied to anything.

137
00:30:08,000 --> 00:30:24,000
And we appealed actually so Brian Gronkey who's the one who came here did a machine learning masters and then you know got radicalized around climate change and is now a PhD student in Germany in climate science.

138
00:30:24,000 --> 00:30:38,000
He did spend one summer internship at Jupiter intelligence kind of a climate startup here in Boulder, just trying to use some of the super resolution techniques that were hot off the presses in computer vision.

139
00:30:38,000 --> 00:30:42,000
And that others had applied oh sorry yeah.

140
00:30:42,000 --> 00:31:05,000
I mentioned gans and techniques like that. Yeah well in the end we'll mention normalizing flows that have kind of an actor critic loss function but aren't aren't gans I do have another student passionate about evaluating how jam how gans generalize which is actually quite open.

141
00:31:05,000 --> 00:31:31,000
He found that these super resolution techniques in computer vision would would not do the best job at inferring and discovering the fine grained detail that is so important in environmental domains we all know that in painting can work and you would maybe infer from the outside of an image what's inside an image but.

142
00:31:31,000 --> 00:31:43,000
I really have a multi resolution scale in any climatological or environmental science problem and so he instead appealed to domain alignment.

143
00:31:43,000 --> 00:31:54,000
There was a paper called a line flow that was in the 2020 triple AI by Grover and others I believe it was from Stefano Airman's group at Stanford.

144
00:31:54,000 --> 00:32:11,000
So the idea so normalizing flows is the next step after VAE where a VAE does learn a distribution over your latent space your compact representation but the distribution is some variant of a Gaussian.

145
00:32:11,000 --> 00:32:36,000
Normalizing flows allow you to get much more complicated and interesting distributions over your latent space so to match reality better now you might have a prior that simple like an isotropic Gaussian but you go through a series of invertible transformations to sort of complicate your distribution over the latent space.

146
00:32:36,000 --> 00:32:55,000
And one thing for you can do downscaling you can also do upscaling there's this automatic cycle consistency that comes for free there's not a cycle consistency loss function that's being optimized but you have exact cycle consistency because the transformations that you're learning from one domain to the other are invertible.

147
00:32:55,000 --> 00:33:22,000
So the idea is you've got your course screen data say you know temperature at a course grid over the continental U.S. You also do need examples of fine grain data but you don't need pairing you don't need to know that this is the temperature map at fine scale corresponding to this input at core scale you just need to you just need independently identically distributed samples at both scales.

148
00:33:22,000 --> 00:33:44,000
They don't need to be paired so that's you know different from what the field of statistical downscaling had been doing where they did have methods but they always required paired maps for training so you just need samples from both domains so now it's the same variable temperature but the domains are core scale and fine scale.

149
00:33:44,000 --> 00:34:02,000
And then this align flow method the idea is that there's a shared latent space and you'll learn normalizing flow from one domain to the latent space and from the other domain to the latent space but since these are invertible that allows you to then go in any direction.

150
00:34:02,000 --> 00:34:23,000
You get is this really nice probabilistic unsupervised model where once trained you can do what's called conditional sampling so if the original goal was to take a course grid temperature map and feed it to the model it'll map it down to the latent space and then map out to a fine grid map.

151
00:34:23,000 --> 00:34:45,000
Also map and and because you're sampling in the latent space you can sample as many times you can take conditional samples like we showed some course grid weather map and then in our model we could sample a bunch of like fine green precipitation maps and then show it to experts do these look plausible but you can get a whole bunch of them not just one.

152
00:34:45,000 --> 00:35:14,000
And are the samples conditioned on some tweakable parameter are you just picking multiple samples and there so when I said condition we're conditioning on the whole input image so the input is the core screened you're saying what are the tweakable parameters that I mean the hyper parameters as usual trade off between your kind of fidelity which is again sort of again style lost.

153
00:35:14,000 --> 00:35:42,000
Versus your generalizability which is looking at trying to minimize it to it's it's sort of a maximum likelihood score that compares your distribution after all these complicated mappings to your prior because of course we always favor favor simple priors in order to avoid overfitting and so the trade off is between complicating your prior so much that you're basically overfitting.

154
00:35:42,000 --> 00:35:54,000
Versus getting really good fidelity maps at the fine resolution which does involve you know evolving away from a very simple prior.

155
00:35:54,000 --> 00:36:04,000
But you could also take unconditional samples so what does that mean there's no input image to the model I could just sample from this distribution I have over the latent space.

156
00:36:04,000 --> 00:36:17,000
Okay now I get some point in the latent space how do I interpret that well we don't exactly know but we can send it through the flow to get a course resolution image send it through the flow to get a fine resolution image.

157
00:36:17,000 --> 00:36:34,000
And then we can also do interpolation so Brian looked at two images at different times in one domain map them to the latent space and now there's this whole field on how should you walk through the latent space.

158
00:36:34,000 --> 00:36:51,000
Anyway this gets into what are the geodesics of the posterior distribution you've learned over the latent space and what is the efficiency of walks that are geodesic versus just you know walking in a Euclidean type path.

159
00:36:51,000 --> 00:37:02,000
But for each sample in the latent space then you can generate a course resolution image of precipitation and a fine resolution image of precipitation and so you've.

160
00:37:02,000 --> 00:37:19,000
This allows for interpretability you can say you know how does the model interpret interpolate between these two points of time at high and low resolution so it's a really flexible nice method and mostly he just extended a line flow from triple AI.

161
00:37:19,000 --> 00:37:34,000
But he decided to use glow for the normalizing flow which had shown great success on celebrity faces right which is where all the gans are trained and wanted to see if we could get some good good fidelity around weather.

162
00:37:34,000 --> 00:37:49,000
Nice that last example of pulling the samples reminding me of the interview I did with Sasha Lachioni on the her visualizing climate impact with gans.

163
00:37:49,000 --> 00:38:02,000
Awesome so the you both of those are papers that you worked on and presented at the workshop.

164
00:38:02,000 --> 00:38:15,000
So that the proceedings are in ACM because our climate informatics workshop turned into a conference last year and we actually have had archivable proceedings for the past six or so years.

165
00:38:15,000 --> 00:38:24,000
But yeah I I talked about those those collaborative works with my students and others at this this keynote Saturday.

166
00:38:24,000 --> 00:38:42,000
Awesome awesome awesome and you mentioned climate informatics and the the conference or there's a new conference that you're part of forming climate informatics can you talk a little bit about that.

167
00:38:42,000 --> 00:38:57,000
Sure so you know Gavin Schmidt and I had presented this paper. Actually, you know, Yanlequin was an early supporter I went to this invited workshop that he used to run called the learning workshop at snowbird.

168
00:38:57,000 --> 00:39:12,000
I didn't choose to either submit something as a poster or a talk and I was like, oh, this is so edgy and weird. I don't know if people are going to like it. I'm just going to ask for a poster. But he really liked it and said, would you be willing to give a talk.

169
00:39:12,000 --> 00:39:29,000
So I was presenting things in in mainstream machine learning and I even applied to run a workshop at like their nirips or ICML on AI for climate change, but the community wasn't ready. It was early days, you know, this was.

170
00:39:29,000 --> 00:39:47,000
Definitely pry it was maybe around 2014, but anyway, so we decided that we wanted a fully interdisciplinary community and so we wanted to have a standalone event. So that's why we didn't run this as a workshop at an existing climate or geosciences event or machine learning event.

171
00:39:47,000 --> 00:39:52,000
So we launched in 2011 at the New York Academy of Sciences.

172
00:39:52,000 --> 00:40:08,000
There was huge participation and very international. It was challenging because hurricane Irene was bearing down on New York. So in the middle of the technical discussions around climate change, people's hotels were getting evacuated.

173
00:40:08,000 --> 00:40:30,000
And so yeah, every year we've persisted. We were in Boulder for many years before I even moved here because I used to be at Columbia. And one year there was this huge flooding event in 2013 and we had to arrive for the conference right after that and figure out hotels and everything.

174
00:40:30,000 --> 00:40:43,000
I also learned a lot about building a community. You know, when we had it in Boulder, we always had a hike. One time when we had to avoid a rattlesnake that provided like a real bonding experience for folks.

175
00:40:43,000 --> 00:40:58,000
And of course, beer in the beautiful nature of Boulder was great for that. And then in 2019 we went international. So the workshop went to Paris in 2019 Oxford in 2020, which turned out to be virtual.

176
00:40:58,000 --> 00:41:07,000
And so since 2015 we've been running these hackathons. So a climate scientist will bring up a challenge problem and a data set.

177
00:41:07,000 --> 00:41:16,000
And then a bunch of teams will be formed with students and all sorts of people that want to participate industries really interested in coming and sponsoring and meeting our students.

178
00:41:16,000 --> 00:41:32,000
And we make sure each team is interested in plenary. So we've had hackathons on forecasting El Nino Southern oscillation on Arctic sea ice extent and storm storm intensity tracking for tropical cyclones.

179
00:41:32,000 --> 00:41:39,000
This year we're most likely doing something on wildfire. It's still in development and that event will kick off in early September.

180
00:41:39,000 --> 00:42:02,000
And it'll be in Boulder slash online. And we are transitioning the climate from a Maddox conference to be a springtime event. And it'll be held in Asheville, North Carolina at the NOAA center and some other places venues around town in spring 2022 date TBD.

181
00:42:02,000 --> 00:42:14,000
I think the big sort of validation of climate informatics, you know, partly universities, like such as University of Toronto started hiring professors in climate informatics. So that's really interesting.

182
00:42:14,000 --> 00:42:28,000
And then the world economic forum made a call to action in 2018. They put out a report on AI for the earth. And they, you know, named climate informatics as a big priority and cited us.

183
00:42:28,000 --> 00:42:47,000
And then yeah, the next big thing is that we've finally launched a journal where people in this whole spectrum, you know, all of data science and AI and all of study of climate change and the environment are welcome and encourage to submit where we feel that it has been sort of a gap.

184
00:42:47,000 --> 00:42:59,000
You know, if you're not publishing only in algorithm, you also care about the application that's not always the best place for it's not always best to publish in an ML venue.

185
00:42:59,000 --> 00:43:10,000
And since, and I'm hearing from domain experts that they can innovate on methods, but they still have to stick it in the method section, which mostly gets ignored by the reviewers in the domain journals.

186
00:43:10,000 --> 00:43:22,000
And this is open access. You can also get an extra badge for sharing your data, your notebooks, your reproducible workflows, and another extra badge for sharing your data.

187
00:43:22,000 --> 00:43:28,000
Okay, awesome. Sounds like an exciting time in the field. What's going on.

188
00:43:28,000 --> 00:43:37,000
Maybe we can close out by having you share a little bit of what you're most excited about looking forward.

189
00:43:37,000 --> 00:43:52,000
Yeah, so I'm broadening from climate change where we hope that we can, you know, make some change.

190
00:43:52,000 --> 00:44:00,000
And hopefully we can to things that communities can do to prepare and really thinking about environmental resilience.

191
00:44:00,000 --> 00:44:16,000
And this brings in infrastructures. It also brings a lot of social issues. So if we talk about a warning system for forecasting an extreme storm or how a wildfire is going to spread.

192
00:44:16,000 --> 00:44:25,000
We really want to represent all voices in and all stakeholders in thinking about how the system should work to interface with society.

193
00:44:25,000 --> 00:44:38,000
And we also want to recognize a legacy of environmental injustices where you may have communities that have been put near, you know, factories where they already have poor air quality.

194
00:44:38,000 --> 00:44:55,000
We have increased instances of asthma or other risk factors. And so if we're warning about something like wildfire, which has, you know, huge air quality degradations, we really want to prioritize vulnerable communities first.

195
00:44:55,000 --> 00:45:11,000
And really exciting. We're in a college of engineering here at CU that is the first that I'm aware of that has a funded center called rise resilient infrastructure with sustainability and equity.

196
00:45:11,000 --> 00:45:31,000
Bringing environmental equity and social justice right into an engineering school. I think that's really important for any any addressing of climate change both within our country. And then also internationally how you have countries bearing the brunt of warming that you know aren't even emitting very much.

197
00:45:31,000 --> 00:45:41,000
So I'm really excited to bring all these interests full circle around inclusion, equity and climate change.

198
00:45:41,000 --> 00:45:48,000
Awesome. Awesome. Well, Claire, thanks so much for taking some time to share a bit about what you're doing. Very cool stuff.

199
00:45:48,000 --> 00:46:03,000
Thanks for the opportunity. Thank you.

