Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting
people, doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
Alright you are in for a major treat this time.
Last week I had a chance to stop by the Googleplex in Mountain View and sit down with none other
than Jeff Dean, Google senior fellow and head of the company's deep learning research
team, Google Brain.
As you hear I was very excited for this interview because so many of Jeff's contributions since
he started a Google in 1999 have touched my life and work.
In our conversation Jeff and I dig into a bunch of the core machine learning innovations
we've seen from Google.
Of course we discuss TensorFlow and its origins and evolutions at the company.
We also explore AI acceleration hardware including the TPU, versions 1 and 2 and future directions
from Google and the broader market in this area.
We talk through the machine learning tool chain including some of the things that Googlers
might take for granted and where the recently announced cloud AutoML fits in.
And we discuss Google's process for mapping problems across a variety of domains too deep
learning and much much more.
This was definitely one of my favorite conversations and I'm pumped to be able to share it with you.
Before we jump into the show though, a few quick questions for you.
Are you an IT technology or business leader who needs to get smart on the broad spectrum
of machine learning and AI opportunities in the enterprise?
Or perhaps someone in your organization could benefit from a level up in this area?
Or maybe you could benefit from them leveling up?
If this sounds like you or someone you know, you'll probably be interested in my upcoming
AI Summit event.
Think of the event as a two-day no-fluff technology plus strategy MBA and machine learning
in AI.
You'll leave with a clear understanding of how machine learning and deep learning work,
no math required, how to identify machine learning and deep learning opportunities within
your organization, how to understand and take advantage of technologies like computer
vision and natural language processing, how to manage and label data for ML and AI,
how to build an AI first culture and operationalize AI in your business.
You'll have an informed perspective of what's going on across the machine learning and
AI landscape and we'll be able to engage confidently in discussions about machine learning
and AI with your colleagues, customers and partners.
I'm very excited about this event and the expert speakers you'll get a chance to learn
from.
For more information, visit twimmalai.com slash AI Summit and feel free to contact me
with your questions.
You can reach me at at Sam Charrington on Twitter or shoot me an email via twimmalai.com slash
contact.
We hope to see you there.
And now on to the show.
All right, everyone.
I am here with Jeff Dean.
Jeff is a Google senior fellow and head of Google Brain.
Welcome to this week in machine learning and AI, Jeff.
Thanks for having me.
I've got to say, I am quite star struck to be sitting here on the Google campus with
you.
You know, I was at the scaled machine learning conference Saturday where you had a
chance to speak and I didn't realize this until Resa mentioned it in your intro.
But there's a whole quora page of Jeff Dean facts, including things like compilers don't
warn Jeff.
Jeff warns compilers.
You know, Jeff occasionally compiles his code just to see if there are bugs in the compiler.
And my favorite, Jeff Dean doesn't actually exist.
He's really an AI that was created by Jeff Dean.
So you have to excuse me if I ramble because I am star struck.
But thanks so much for taking the time to be on the podcast.
Oh, it's my pleasure.
Why don't we get started by having you tell us a little bit about your background and
how you got involved and interested in ML and AI?
Sure.
So, you know, I've always been interested in computing and I moved around a lot as a child.
I went to 11 schools in 12 years.
Oh my goodness.
Oh my goodness.
Several different continents lived in Hawaii, Boston, Uganda, Boston, Arkansas, Hawaii,
Minnesota, Somalia, Minnesota, Atlanta, and then back to Minnesota for college.
And then was this like were you an army brat or something like that?
My dad did tropical disease research and epidemiology.
Okay.
Mom was a medical anthropologist.
Okay.
And they liked to move.
So I moved.
Nice.
Nice.
And then when I arrived at University of Minnesota as an undergrad, just before doing
its senior thesis, I took a course on parallel and distributed computing.
Okay.
And then I worked with the professor who taught me that to do a senior thesis on parallel
training of neural nets, because I sort of at that time people were very interested
in sort of neural nets as a interesting abstraction for solving problems.
And at that time, it seemed like they could do cool things on toy problems, but not much
on very large scale problems that people cared about.
Okay.
And I thought maybe if we could get like a 60X speed up on a 64 processor machine, we
could do much bigger problems.
And so I worked on some algorithms for that.
But as it turned out, we needed like a million times more compute, not 60X.
So you have to wait that one.
And so I sort of set a file neural nets away as kind of an interesting thing, but then
went off and did a lot of other work.
I eventually went on to grad school, did compiler optimization research, then came to digital
equipments, research labs in Palo Alto and worked on a variety of things, including information
retrieval on the web, profiling systems, various things like that.
And eventually came to Google in 1999, when we were a fairly small company, we were all
kind of wedged in a small office on University Avenue above what's now the demobile store.
And started working on sort of large scale distributed systems for sort of core Google
products and sort of worked first on our first advertising system and then spent many years
working on sort of Google's distributed search systems, including the ranking system and
also the crawling indexing inquiry serving systems.
Okay.
And a ton of other stuff, MapReduce.
Yes.
So then kind of towards the end of working on the search system, my colleague, Sanjay
Gamawatt and I and some other people have started working on sort of core infrastructure
systems for dealing with large amounts of data, how can you efficiently process large amounts
of data on sort of a collection of unreliable computers.
Sanjay and I came up with this MapReduce abstraction in the context of sort of rewriting
or crawling an indexing system where you want to go from having a bunch of raw pages
on disk that you've crawled to actually having all the data structures built for serving
Google queries.
And there's a whole bunch of stages there that involve processing the pages and like doing
things like extracting all the links from the pages and building a link graph and identifying
the language of each page.
And conceptually they're all pretty simple, but when you're trying to deal with making
them reliable computations across lots of machines, if you sort of individually conflate
the sort of low level mechanisms for reliability and the high level thing you're trying to do,
which is figure out what language each page is in or extract links, it becomes kind of
complicated to do.
But if you have this nice abstraction which MapReduce seemed to provide, you can actually separate
those and build an implementation of the MapReduce abstraction that allows you to sort of deal
with lots of things in there like reliability and automatic parallelization across however
many machines you want to throw at the problem, dealing with stragglers so even if a machine
doesn't die, it might be running slowly because it's doing other stuff on a shared environment.
And then you can express a lot of these fairly simple computations using fairly simple abstractions
of MapReduce.
One thing I'm curious about is a little bit of a history of machine learning and AI at
Google, leading up to TensorFlow and some of the things you're doing around the TPU, but
what's your earliest exposure, what was your earliest exposure to ML here at Google?
So we've been using collectively machine learning at Google for quite a long time.
And you can argue that the page rank, the beginning of Google's machine learning, right?
So across lots and lots of our products, we use machine learning in various ways until
maybe about six or seven years ago, typically not deep neural networks, but simpler methods
like logistic regression or sometimes simple counting based statistics methods for estimating
probabilities of various things.
And so we've done, had a long history of that and built systems around how do we actually
scale those kinds of systems to very large amounts of data.
Some of them are embedded in core products that are pretty important.
And then about seven years ago or so, we started to look into how to use deep neural nets
and see if we could make those scale to problems we cared about, you know, very large scale
problems, because there were signs of various successes in using neural nets for things
like speech and vision on a small scale that were starting to appear in the academic literature
as sort of the beginning of the revival of neural nets happening in kind of 2007, 2008.
Right.
And so in 2011, we started to really put, look at this in earnest within Google, Andrew
Eng was spending a day a week at Google sort of consulting and I happened to bump into
him in the micro kitchen and said, oh, what are you up to?
And sort of he said, oh, I'm kind of figuring out what I'm going to work on.
And but at Stanford, you know, I'm starting to look at how neural nets can solve different
kinds of problems and they're starting to be successful.
And I'm like, oh, really?
I did my undergrad thesis years ago on those.
Oh, wow.
And so we kind of got excited about trying to put together a small effort to scale neural
net training to build very big neural nets and see what we could do with them simply because,
you know, we felt like if they're having success on small scale, then building bigger ones
would be even better.
Nice.
And did the work on TensorFlow lead or follow directly from that?
Where did, how did that evolve?
Yeah, so the way this happened was we actually started building a different software system
called disc belief or distributed belief and also as a joke because people didn't think
this would really work out.
So I called it disc belief.
And basically that was a sort of programming abstraction for neural nets that wasn't in
retrospect, it wasn't as flexible as we wanted it to be.
But it would allow us to express kind of in the same way that MapReduce was a programming
API.
This was an API for neural nets.
It was sort of layer based implementations where you'd have a forward method and a backwards
method.
And so you could compose together layers of neural nets with different kinds of computations
in each layer.
And it was good for expressing things like convolutional neural nets or LSTMs or feed
forward neural nets, those kinds of things.
And we started applying them to computer visions.
We did some work on unsupervised learning with auto encoders for vision problems.
And we actually didn't have GPUs in our data center at that time.
So to get enough computation, we ended up parallelizing these computations across large numbers
of CPUs because that's what we had in our data center at that time.
And so we trained a neural net with like two billion parameters at that time, which was
quite a lot and used kind of 16,000 cores for a week to train a pretty large scale auto
encoder and found that it could do interesting things like despite being trained on completely
un-mable data, some of the neurons at the top levels of this large neural net.
It started to be responsive to things that resemble sort of high level human explainable
features, things like kind of cat-like face or the back of a person kind of thing.
And so it's clearly learning interesting high level concepts despite not having any
labeled data.
So you kind of started with disbelief.
What were the challenges or issues with disbelief that led you to say we need something
new and better?
I mean, I think we realized that this sort of forward backward computation fit some kinds
of machine learning model we wanted to be able to express, but it wasn't as general
purpose as we would like.
And so we looked at things like theano, which was an open source package from University
of Montreal that had a more of a sort of auto differentiating graph-based interpretation
of how to express these computations.
And we sort of used that as a rough programming model, but focused on making this sort of
a high-performance scalable implementation.
And also we wanted to be able to run this in a variety of different environments.
So we've always wanted the TensorFlow system to support running machine learning wherever
machine learning wants to run, which is a lot of places these days.
So you want to run it on things like mobile phones.
You wanted to run it on a desktop machine with, you know, with or without a few GPU cards.
You want to run in a data center environment where you have lots of machines each with
maybe perhaps several GPU cards.
You want to support kind of new and emerging hardware accelerators for machine learning
that are appearing both on the data center side and high power environments, but also
in sort of things like mobile phones or starting to get kind of these machine learning accelerators
you want to make sure you take advantage of those as well.
Yeah.
On the topic of machine learning accelerators, Google's now in the second version of
the TPU, the first version was focused on inference, the second version added, you know,
training capability.
I'm curious what your general thoughts are on the acceleration space.
I mean, you've noted a couple of times now at nips and this presentation, you were at
I'm sure other places that, you know, when you look into your crystal ball, you, you
see that it's foggy in terms of what we're going to need from accelerators.
And so that leads you to some specific, um, that leads you to think that we should be
a little bit more open ended in terms of where we're going with hardware accelerators.
You can do a better job of, you know, clarifying your position on these.
Yeah.
So I would say about four years ago, we realized that neural nets were going to be a really
big sort of feature in lots of our computation at Google.
Yeah.
And if you look at trying to run all the inference, we wanted on the existing data center
machines with CPUs and them, that was probably not going to cut it because we wanted to,
for example, I did some very rough back at the envelope calculations and said, okay, if
every user talks to their phone for three minutes a day and we need to do speech recognition
on that, then that alone would mean we need to double the number of data centers we had
at a very rough level.
And that sounded kind of untenable.
So we realized that inference was going to be the first important thing to tackle from
an acceleration standpoint because any services where you have a growing number of users or
a lot of users already inference is a much bigger cost than training typically.
And inference is also a much simpler problem to tackle from an acceleration standpoint
because typically you can get a single chip to do inference for a whole model.
And then if you need more capacity, you can just stamp out more copies of that chip or
more cards that you insert in a machine or a bunch of machines kind of embarrassingly parallel.
It's embarrassingly parallel.
Each chip can handle inference for the model in some modest batch size and you just kind
of go from there.
And so the first version of the TPUs that we built was an inference only accelerator, really.
It supports 8-bit quantized integer arithmetic.
There's a paper in Iska, which is a big computer architecture conference that describes
that and more detail of readers or listeners are interested in the details.
But we tackled that first because that would help us with scaling and using neural nets
in many more places in our products.
So now whenever you do a query or you're touching inference accelerators in many ways.
So for search ranking for various pieces of other aspects of dealing with forming the search
page, for Google Translate, it's used whenever you speak into your phone and that recognition
is done in the data center.
There's inference accelerators there helping with that.
Is the implication then that you're using inference accelerators beyond deep neural nets
and to, you know, for traditional machine learning as well?
No, all those are deep neural.
Yes, that's another wave that's been sweeping across many, many Google products and features
in Google products and so on is we're using deep learning.
You know, initially when we started looking at neural nets in our group in 2011, you know,
a handful of production groups started, product groups started looking at, you know, how
can we use them so we work closely with the speech team, the image search team.
And with disbelief, our first software system, that made it easier for teams to sort of
start training neural nets for their own problems they cared about.
And so, you know, eventually 50 teams or 100 teams were using deep neural nets with the
first system disbelief that we put together.
And like many things, it kind of spreads organically throughout the organization because as soon
as one team uses deep neural nets to solve a problem and another team kind of hears about
it and says, Hey, that sounds a lot like our problem.
I wonder if we could try it for our problem as well.
Then and there's, you know, mailing lists where people can post questions like, Hey, I'm
having trouble with this and people will chime in.
That's generally the way these kinds of things spread throughout Google.
And so we have this kind of nice exponential looking curve of how many different deep learning
models are used in production across lots of Google products.
You have a sense for on a percentage basis.
For example, how much of that usage is, you know, what we traditionally associate with
deep learning, you know, audio, video, image versus other types of use cases.
Um, I mean, quite a lot of it is in sort of more textual oriented applications, including
text in that.
Yeah, I guess I'm curious, um, more specifically if, if Google is, you know, in this time working
with deep learning, identified, um, and, you know, productize, I guess, thinking about
mapping, you know, non-traditional deep learning problems to the deep learning domain.
I mean, I think our experience has been that for problems where you have sufficient data
and sufficient computation, you can apply deep learning to a wide variety of problems,
both kind of more perceptual kinds of things like speech and vision, but also natural language
processing problems, kind of more structured prediction problems.
Um, and in general, you know, it doesn't work perfectly on every problem, but the vast
majority of problems we try these approaches on tend to work pretty well.
So, uh, is there ever a problem at Google for which you don't have enough data?
Sure.
I mean, we're, there are definitely some, like if you want to predict if, uh, a machine
is failing, that sometimes, you know, you don't have that many examples of failed machines.
So, uh, that's the kind of thing where you might have, you know, less data than other kinds
of problems.
Kind of the anomaly oriented cases.
One of the things that is a pretty useful general technique for what you might think of
as non-traditional, uh, things is to take kind of discrete features, uh, like maybe you
have some model that wants to use the country where a user is located.
And you can actually put, uh, discrete features into an embedding space.
So similar to like the word-to-vec model where you have discrete words and then you represent
each one of those words by say a hundred dimensional or a thousand dimensional vector.
And through the optimization process, you kind of nudge these vectors around in a super
high dimensional space.
So that similar things are kind of nearer each other in this high dimensional space.
You can actually do that with more discrete things like the country where a user is located
or whether they're on a, uh, what kind of model phone they're on, uh, because maybe some
phones have very big screens and people behave differently in how they use those than
other kinds of phones.
And so you can put an embedding vector on the model of phone and then kind of nudge, uh,
the point in this high dimensional phone or device space, uh, nearer to ones that seem
to behave similarly and, and other ones maybe form other kinds of, of, of clusters in,
in this high dimensional space.
Okay.
And that's a pretty common technique we use across lots of different problems here that
is not really a research thing.
It's kind of a more of a practitioner like if I'm trying to solve this kind of problem
with a bunch of discrete features, that's something you might try, uh, using, but it's not
like there's really a good single paper you go to to discuss this except word to veck
kind of is, or things like that, uh, is representative of that, but it's not necessarily
obvious that you can do this for non words and non language things, but for all kinds
of things.
Okay.
You know, if you had videos you're trying to recommend into my dimensional space, you
can represent each video as a high dimensional vector, right, right.
So we're talking about the TPU and, uh, hardware accelerations.
Generally, do you have a sense for, you know, when you did your back at the envelope calculation,
did you project out, you know, some number of years and, and kind of get a sense for the
percent of, you know, total, compute at Google that will be dedicated to inference.
Do you have a feeling for that?
Um, I don't have a super training feeling because we're still discovering, uh, training
is certainly a big deal as well.
So, um, our first generation TPU was for inference because that's both, uh, was a more pressing
need in 2013, 2014, but also it's a simpler problem.
You design a chip on a board and that's pretty much the extent of it for training.
That's a much sort of broader scale problem as a, at a systems level because you need
to design both accelerator chips, but also a much bigger scale system that connects those
chips together because unlike inference, it doesn't sort of scale just embarrassingly
parallel where you just plunk in a bunch more cards.
You actually need more compute than you can fit in a single chip for most training, for
large scale training problems.
And then you need, therefore to take a much more holistic systems view, how are we going
to build sort of a much larger scale system that connects together a bunch of these chips
with very high speed interconnects, how can I, you know, have a software story that's easy
to take a computation I care about and map it onto, say, a system with 64 or 256 chips
and have that be sort of easy to use for machine learning researchers and practitioners without
sort of getting into the guts of, you know, them worrying too much about how to parallelize
things across these, these different devices.
And so it's just a much bigger scale problem.
And so the second generation that we tackled was a system for both training and inference
and was much more holistic than a single chip, it's like these large scale TPU devices,
which are four chips on a board.
And then those are designed to be connected together into much larger configurations
of 64 devices, 256 chips, there's a custom 16 by 16 mesh network that, uh, or torridal
mesh, actually with wraparound links on either end on the device or on the whole broader
system.
So the whole system is like four racks in size, looks really like a machine learning
supercomputer.
Uh-huh.
And those are the kinds of things that we're now applying to much larger scale problems.
And the reason we're doing that is because we see, uh, continuing gains as you're able
to tackle larger and larger problems, um, so for example, the deep learning work that
we rolled out to sort of completely replace the old style phrase based translation system
and Google translate with a neural net, um, is, uh, impactful in many ways.
So one is that system, uh, uh, had been around for a decade, I actually helped do some of
the early low level software for some parts of that system, um, and it was a very complicated
system in software terms that had like five or six, uh, different sort of, uh, distributed
systems in it.
And then, uh, about, you know, there's like a, uh, uh, target language model that is spread
across a hundred machines where you do lookups of how often a bunch of five word phrases
occur and that language model can tell you how often every five word phrase in English
occurs.
There's a alignment model for how words in English and French align.
Okay.
And then there's like phrase tables, dictionaries and then 500,000 lines of code to glue all
this together.
Um, so the new system is literally a TensorFlow model, uh, with a neural machine translation
model that we've published a paper about and is like 500 lines of TensorFlow, and just
learn from lots and lots of data and not only is the system much simpler, but the translation
quality is much, much higher, uh, the gains we saw from rolling, moving from the old system
to this new system were equivalent or larger than the gains in quality improvements in
the previous decade of working on improving the old system.
And so that's just a sign of something could be dramatically simpler and because it's
learning from lots of observations, it can actually do a better job of the task you
care about.
And, uh, the, the way this gets back to TPUs is for some of the language pairs that we
rolled this out on, we actually have a lot of data and we can only train on one sixth
of the data we have, we can only get through one sixth of the data once in the sort of
time budget we'd set for each language pair.
Okay.
And that system supports, you know, roughly a hundred to a hundred other languages.
So there's like 10,000 language pairs you need to support.
Okay.
And so that's now on TPU 2 or was that before, uh, that's now, uh, training on, on TPUs
and it's actually, uh, running inference on TPU V1.
Okay.
So, um, but, but, uh, before we had TPU V2 rolled out, it was training on lots of GPU cards.
Okay.
Do you see the TPU effort broadly kind of replacing GPUs at Google and kind of decoupling
Google from a dependency on GPU suppliers?
Um, I wouldn't frame it quite like that.
I would say, you know, because we're designing both the TPU and we have a lot of machine learning
researchers and also our building software to map these, uh, sort of style models onto
the TPU, we can have, uh, sort of much tighter feedback loops in sort of doing vertically
integrated decision making for, for what the future of TPUs should look like.
But there's some things that TPUs don't necessarily do well, that GPUs do extremely well.
Uh, but the kinds of things that do run on TPUs, you know, we think are pretty interesting
in compelling, uh, for my, you know, performance standpoint.
One of the things we really crave is to make turnaround time for machine learning research
experiments much faster.
And so, uh, you know, there's a qualitative difference as a researcher if your fundamental
experimental turnaround time is measured in, you know, an hour rather than a week, right?
You just, it just feels very different.
You can be much more productive.
You can try more things out.
It's not a big, you know, effort where you start up an experiment and then you've like
forgotten what you've actually started by the time it finishes a week instead, you
try many more things.
Uh, and so getting that time down just improves the productivity and the rate at which we
can try out new ideas, which is really super important in this kind of new and emerging
space.
And is that an exercise in driving up tariff laps as quickly as possible or is it more
nuanced in that?
Uh, so it's a few things, right?
One is obviously you want more raw tariff laps, but also you want software layers that
make it easy to use those tariff laps, right?
You don't want to have to write complicated code in order to have a research idea you
just thought of, be mapped onto those tariff laps.
Right.
So if you can make software tools that make it easy to express ideas that then can take
advantage of say a whole TPU pod relatively quickly, um, that's just generally going to
be better.
And so it's really requires work at all these different levels of the sort of hardware
design stack, the software stack, the ease of expression for different kinds of APIs
that you might have added in using for expressing machine learning computations.
Hmm.
There are a set of things that, um, a set of tooling that Googlers take advantage or that
Googlers kind of take for granted that, um, that you think that need to evolve, I guess
more broadly and be, um, you know, someone needs to provide in order for, you know, folks
to take advantage of deep learning and be as, uh, be able to erase quickly as Google does.
Well, I think one of the things that we're, uh, trying to do is to, uh, and, and part of
the reason we decided we would open source TensorFlow.
Sure.
So we open source TensorFlow at the end of 2015 was we wanted the tools that we used to
also be sort of more broadly available to the community and then have the community collectively
work with us to improve those tools.
And I think that has actually played out reasonably well.
We now have tons and tons of people using TensorFlow.
We have, uh, you know, obviously a smaller set of people working to improve the core set
of TensorFlow APIs and algorithms and implementations, uh, but lots of contributions from across
lots of other big companies, you know, lots of, uh, smaller companies, people working on
their own as hobbyists, um, really are, are working collectively to improve those tools.
And I think that's, that's really good because it gives you a common language for expressing
machine learning computations that is really useful to have ideas spread more rapidly through
a community than they would if you relied on just publishing a paper and describing something.
And when you do that, you necessarily, by necessity, leave out a lot of details, right?
English is kind of a poor way, especially exactly what to do.
And so by allowing people to share, sort of working implementations or open source models
that they care about, uh, I think you can get that spread that was happening already within
Google of one team picking up what another team has done and trying out in their problem.
I guess I'm not a virus about, like, you know, so TensorFlow, you know, today is, you
know, in some ways, you know, look at what the way Google was thinking about this five
years ago and what was missing five years ago, right?
If you look at, you know, the way Google's thinking about it today and, and, and, but,
you know, we'll be still missing for the industry in five years.
You know, what are, you know, what's the, kind of the future of that software stack out?
This is kind of a clumsy way of asking the question, but, you know, Google had Borg, you
know, and then that turned into Kubernetes, you know, there was a disbelief, which, you
know, became kind of, uh, publicized as TensorFlow or evolved into TensorFlow in some ways.
Uh, I'm imagining there's actually in the conversation with Ryan Poplin, you know, we talked
about, um, a lot of the data augmentation pipeline that, you know, as a researcher, he
just takes for granted.
It's just there.
Um, I can imagine that being publicized or turned into open source code by Google or,
you know, someone else.
I'm just wondering if, if, um, you know, we can kind of tap into your today as a crystal
ball for, you know, what some of the gaps are in the software pipeline, uh, that need to
be figured out over the next few years to really make deep learning, uh, a lot of folks
to iterate more quickly.
Right.
So, uh, I mean, I think one of the things that's happening is people are open sourcing
of wide variety of different tools built on top of TensorFlow or that sort of run in phases
and then you sort of run TensorFlow as, as sort of the core computation engine.
And I think that's really powerful and most of them are sort of particular things, rather
than a general framework that makes something different, uh, but they all add together because
now there's this, uh, really large community of different pieces and, uh, building blocks
that you can choose from when you're solving your problems.
Um, one of the things I'm pretty excited about in terms of the research our group is doing
is this notion of automating machine learning, right?
I mean, I think even today with the explosion of interest in machine learning and more and
more people are entering the field, um, there's still an incredible shortage of people who
actually know how to take, uh, data and computation and have the expertise to then make a solution
to a machine learning problem.
Yeah.
Um, the way I like to phrase this is there's, you know, maybe 10,000 organizations in the
world who are actually really practically applying machine learning in a production way
to their problems and their, their settings, and have hired people with strong machine
learning expertise.
But there's probably 10 million organizations in the world that have data in electronic
form that could be used for machine learning and have problems that would be amenable to
machine learning.
Uh, one example is, um, every city in the world should be setting their stoplight timing
by machine learning, right?
Like right now, they probably have two lookup tables, they have rush hour and not rush
hour.
Right.
And you select among them via the time of day, yeah.
But you can imagine with a computer vision based stoplight, you could just figure out,
you know, are there cars coming in which direction and, and, uh, if not turn the light, uh, appropriately,
and maximize throughput of, uh, cars through an intersection or through the collective set
of intersections in a city, that's not something most cities are probably doing today.
Right.
Some are, but most of them are probably not.
Um, and so I think we'd like, so the idea behind auto ML or metal learning is that you
want to be able to solve new machine learning problems, um, automatically without a human
machine learning expert sitting down and saying, okay, I'm going to solve that in this particular
way.
And I think that's going to be a really promising and, and fruitful way to get machine,
more machine learning used in the world to solve problems that are, are important.
Uh, what do you project the timeline or how do you see, how do you see auto ML evolving
over time?
Yeah.
So, you know, we're continuing to push on the research aspect of it.
It's obviously an ambitious problem to be able to take any problem and solve it automatically.
Uh, I would say what we have now is we can now solve problems in a set of restricted
domains, uh, automatically, and be able to take a new problem and automatically solve it
in one of those domains and, uh, yeah, so, uh, we actually have a launched, uh, product
that we've jointly worked on with our cloud organization, cloud auto ML, that initially
can solve computer vision problems, and there's tons and tons of organizations that fall
into the category I was saying where they have, you know, a bunch of images of things
to care about, um, the, the sort of default image net model isn't exactly right for their
problem.
Okay.
Uh, like they want to be able to identify as this broken airplane part on their assembly
line or, uh, not broken one, so they have a bunch of images of that.
Some of which are labeled as, you know, broken propeller and some of which are labeled
as, you know, not broken flange of something.
And so you can take those image datasets and now automatically solve classification problems
to high level of accuracy, much more than you can get just by doing transfer learning from
say an image net model that you've already trained.
And that's already going to be useful for a pretty wide variety of, of companies and
organizations and broadening out the set of domains in which that approach works is something
reactively working out.
You'd like to do this for language problems for, you know, recommendation problems, speech
problems, uh, a wide variety of these things.
Hmm.
Do you have a, kind of a research taxonomy of auto ML?
There's, um, you know, the neural architecture search.
There's kind of meta learning stuff.
How do you think about the way the different pieces and the way they fit together?
Yeah.
I think like we started this investigation in, in our group, uh, with the neural architecture
search work, we've also been working on evolutionary approaches for, uh, those same kinds
of, of problems, just as a way of, of comparing several different approaches for tackling the
same problem.
I think that's often useful to ground work is what you're doing the right direction by,
you know, trying out several approaches.
There's a bunch of other work we've been doing on learning things that are not just
the architecture, but, uh, things like learning new optimization update rules, you know,
traditionally, you have SGD, which is a very simple update rule.
You take learning rate times the gradient and you add that to the parameters, uh, but,
uh, and then there's things like SGD with momentum and atom and add a grad that have been
developed, uh, by humans, uh, and seem to be effective for a variety of problems.
Uh, we actually, we actually published a paper in our group on, um, trying to learn optimization
update rules, symbolic optimization update rules and just gave it kind of the raw primitive
things like the things that occur in the expressions for SGD and SGD with momentum and a bunch
of other kinds of symbolic updates.
And then it learned to stitch them together and it found sort of 15 different optimization
update rules that are all better than all four of those, really, sort of things.
Interesting.
Um, which is pretty interesting.
And some of them had sort of common characteristics that you can kind of, uh, get inside from.
So that's kind of, um, and how did, how did you, how did you represent that problem so
that you could do it in a symbolic domain?
Oh, so you just, uh, uh, it's sort of similar to neural architecture search, except what
you're spitting out is not an architecture, but a symbolic update expression for optimization.
Okay.
And so you give it a bunch of primitives and say, please spit me out a, a symbolic update
rule that has, you know, no more than a depth of three and a symbolic expression for
you or something.
Okay.
And then you can search over plausible, uh, symbolic update rules, including things like,
you know, you accumulate the running average of the gradients or you, you multiply by
the learning rate or you, you know, divide by the, uh, recent parameter updates or something.
Um, and so one thing it found was, uh, an interesting rule of, I'm trying to remember the exact
rule.
It's like e to the sign of the current gradient times the sign of the recent running
average of gradients.
And so if those are the same, it's going to scale things by, by that.
And if it, they're different, then it's going to slow way down.
Um, just think basically just spits out your next 15 papers or something like that.
Something like that, although probably you take the best one and, and do that.
But, uh, you know, it is interesting that the kinds of things that are, if you have a problem
that is automatable in this sort of searchable way where you have a clear reward function and
some space to search over, I think this, this is more general than just how do you find
machine learning sort of ideas, but scientific ideas more generally.
If you can automate that process because what, what a scientist does is they run a bunch
of experiments and then they look at the results of those experiments and then they figure
out what are the next experiments to run, right?
And so integrating the results of the previous experiment back into deciding what the next
things are to run is generally a pretty slow process if there's a human in that interpretation
loop.
And so you can actually get a lot of benefit for real world problems if you have a just
sort of a clear reward signal by essentially using reinforcement learning or perhaps evolutionary
search to optimize that reward.
Are you doing much with, uh, with program learning?
Have you published on program learning?
We, we do have a, I mean, you can think of neural architecture search as a very restricted
form of program learning and we're also doing some more kind of more traditional symbolic
computer programming language synthesis work.
We've published a little bit of that, but not, it's sort of an early emerging research
area for us.
We're pretty excited about it because we think it's got a lot of potential benefit to sort
of provide tools for programmers to sort of take some of the work that is traditionally
done by programmers that could be done in a more automated way and a lot of programmers
to spend more of their time on the things that are sort of higher level, more difficult
to, to do and take away some of the drudgery work that, uh, programmers currently sometimes
do.
So we're, where do you think, uh, how would you characterize, you know, where we are
and how that work evolves over time?
Yeah.
I mean, I would say we're pretty early in that work really having real world impact,
but I think it's, it's definitely an interesting direction because obviously if you can, if
you can look at a problem and come up with an algorithmic expression of how to solve
that problem, that's a pretty important and impactful thing.
But we're very far from being able to do that in general.
Yeah.
Maybe circling back to TPUs, you know, what can you say about TPU V3?
Like what, what needs to be, you know, what, where does the work need to continue there
and, and what do you expect that the future holds in that direction?
Right.
I mean, I think one of the things about machine learning accelerator hardware is it's hard
to really read the future out far enough in this very fast moving field.
Um, you know, if you look at the, the given indication of how fast this field is moving,
uses great exponential growth chart of the number of machine learning papers on archives
since 2009 or 10 or something, it's gone from like a thousand per year, up to 20,000
papers per year.
Right.
Right.
So that's like almost a hundred papers per day being published in the machine learning
field, you know, on archive, which is sort of crazy, right?
It's like an entire conference every day, um, you know, obviously they're not all peer
reviewed and so on.
But it's a very fast moving field with lots of people with different kinds of ideas
and backgrounds coming into the field and bringing their perspective on how to solve problems
there.
Mm-hmm.
And if you develop yet the, the deep neural net to summarize all those papers off of archives
so that a researcher could actually keep up.
We are working on some words, but sadly it is not yet at the level where we can understand
all the archives papers published today and tell you which ones to look at, but that would
be cool.
Um, but I think that's an example of, you know, if you're starting to design a chip or
a system for doing machine learning acceleration, you know, currently what we focused on is trying
to make, um, low precision linear algebra super fast, right, because that seems to be a basic
building block that applies to nearly all the deep learning, uh, sort of algorithms and
systems we've looked at.
Mm-hmm.
A bunch of kind of interesting exploratory work in very low precision kinds of work that
have been sort of proven out on small scale problems in software simulation to show that
in some cases you can make, uh, four bit weights and four bit activations work.
And obviously if that worked in general across all problems you cared about, that would be
pretty cool.
Mm-hmm.
And you might really sort of, uh, bet on that in your accelerator hardware.
I think the state of this is such that it's been shown to work on some kinds of problems
on modest data set sizes, some more experiences needed before you really bet on putting it
in as the only thing in your accelerator, right?
Okay.
Because your accelerator, if you design it today, sort of pops out of the fab and into
your data centers a year and a half or two years from now and it has to live for three
years.
Right.
So wanting to bet on things that you're pretty comfortable are going to be important
things for the two to five year time frame from now.
And so that means, uh, you can have some experimental things in there, but not sort of the whole
chip being experimental.
Although I will say there's a bunch of startups now in this space that are, you know, essentially
being funded by, by the VC community to try out ideas.
And I think that's actually kind of cool because there's a lot of ideas being tried out in
the startups space, some of which are likely not to be the best ideas, but are the experiments
are being run.
Yeah.
And some of which may turn out to be influential and interesting and exciting and inform
what we should do in terms of building sort of next generations of ML accelerators.
Yeah.
I've kind of positive that in a world where we're deep learning, machine learning, enterprise
is general workloads are kind of shifting to the cloud over time.
And all of the cloud vendors are investing heavily in, you know, their own hardware to accelerate
their, uh, their workloads in part, I propose or supposed to kind of make sure they're not
dependent on any particular, uh, accelerator, uh, vendor, uh, in their supply chain.
Uh, I kind of wonder what the, you know, what the future holds for, you know, these startups
that are kind of going off on their own and building out these new accelerators, like
who are their customers going to be?
You know, what's your, what's your take on that market and kind of how it shakes out?
Well, I think, uh, you know, it's hard to say and, and I'm not in the business of picking
winners in the chip startup space, which is, which is fairly good.
Uh, but I think, you know, obviously some of them might get acquired.
Uh, some of them will sell equipment to cloud providers, probably cloud providers or to
people who want on-premise machine learning and not running their, their computer in the
cloud.
Um, so I think there, there's definitely avenues for them to be successful and, and it'll,
it's just a little unclear exactly which of the ideas that are being tried out will be
sort of good in terms of, you know, providing really high performance for general things
or maybe there's some restricted set of models or one particular hardware vendors thing
is super fast, but it doesn't really run other things very fast.
Um, I do think part of this is driven by the fact that Moore's law speeding up general
compute has basically dramatically flattened off in the last six or seven years.
And so now we're at the point where a lot of the innovation is going to be on the computer
architecture side rather than the gesture lying on the fab people to give us like smaller
features and faster transistors every two years like they had been.
And so that will mean there's now this really wide open space.
And the other thing is that now that we know machine learning computations, deep learning
models in particular are so applicable to so many problems that previously when you
were building like specialized hardware for a particular thing, you know, it applied
to relatively modest part of all the computing you want to do, right, right, you might speed
up compression algorithm by having custom compressions or goods or something, but that's
not actually that much of what you want to do, right, right.
But now you have machine learning computations where it seems plausible that might be a lot
of what we actually want to run on computers five years, 10 years from now.
And so you can now build specialized hardware that doesn't just apply to like a little bit
of your computation applies to everything a lot of it.
And so that I think is pretty exciting because now general purpose accelerators apply to
such a wide variety of things we care about, vision, language, you know, all the things
you can imagine computers really wanted to do, you know, most of them you can imagine machine
learning tackling and you can imagine machine learning acceleration hardware being an important
part of how you make that scale and be fast.
So your kind of point is then that is just that the market is going to be so massive or
is so massive that that there's potential opportunity for different approaches.
I think so and I think I would liken it to this like a big Cambrian explosion of computer
architecture again.
And now lots of ideas are being tried out and it's unclear which ones will sort of survive
this Cambrian explosion, but many of them probably will and that'll be kind of cool.
Awesome.
Awesome.
So I'm wondering if there are, you know, one, two, three things that are, you know, core
to the Google way of thinking about, you know, deep learning, machine learning AI or even
the Jeff Dean way of thinking about machine learning and AI that, you know, a lot of people
don't know, but they should, you know, even better if they're not things that you talk
about all the time.
Like, you know, what are those, you know, what are those kind of hard fought, you know,
lessons that, you know, folks need to know?
I mean, I think one of the things that has struck me a little is I was actually, I was
preparing to give a talk in the National Academy of Engineering last summer and turns out
the National Academy of Engineering put out a list of 14 sort of grand challenges, engineering
challenges for the 21st century.
And it's pretty good list, it's like, you know, make solar energy, efficient and affordable,
you know, advanced health informatics, a bunch of things like that.
And I was struck by how many of those things seem amenable to machine learning being either
key to solving or an important part of solving some of these problems.
Even things that seem kind of somewhat of a field like more like chemical understanding
or better chemistry.
And I think actually machine learning in the last few years is shown that can actually
be applicable to a lot of scientific problems, including better modeling of chemical dynamics
and these kinds of things and more rapid evaluation of materials and things like that.
So, and we've been doing a lot of work on healthcare related problems using machine learning.
And I think I'm struck by the fact that a lot of these sort of core machine learning
algorithms are going to have impact not just in kind of computer science and machine learning,
but because they're applicable to so many things across, you know, a really broad set of problems
in society, things like healthcare, self-driving cars, obviously, you know, material design.
All these things I think are pretty impacted by better machine learning and better machine
learning hardware and advances in this field.
That's pretty cool.
That's why so many people in the world are interested in this field, because I think
it's going to do amazing things for the world.
Awesome.
Well, Jeff, thank you so much.
Cool.
Thank you.
All right, everyone.
That's our show for today.
For more information on Jeff or any of the topics covered in this episode, you'll find
the show notes at twemolei.com slash talk slash one, two, four.
As you know, we love to receive your questions and feedback about the show, so don't hesitate
to comment there.
Thanks so much for listening and catch you next time.
