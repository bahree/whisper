WEBVTT

00:00.000 --> 00:15.680
Welcome to the Tumel AI Podcast. I'm your host, Sam Charrington.

00:15.680 --> 00:27.880
Alright everyone, I am on the line with Manuela Veloso. Manuela is the head of AI Research

00:27.880 --> 00:34.060
at JP Morgan Chase and a professor at Carnegie Mellon University. Manuela, welcome to the

00:34.060 --> 00:38.800
Tumel AI Podcast. Yeah, it's great to be here. Thanks for having me.

00:38.800 --> 00:45.880
Absolutely. It is wonderful to get a chance to speak with you. You are currently in New

00:45.880 --> 00:52.680
York City where you are recording this from your home on lockdown. How are things there?

00:52.680 --> 01:02.040
So again, we are on lockdown. I'm in the apartment. I've been here for three weeks but surprisingly

01:02.040 --> 01:10.080
I continue to be very productive in terms of work because with all of our virtual meetings

01:10.080 --> 01:16.640
everything seems to be going quite well. What is challenging is to exercise and to make

01:16.640 --> 01:23.100
sure that you get some fresh air and some sun. But from the work point of view, I miss my

01:23.100 --> 01:28.720
team but we have a lot of Zoom meetings. It seems to be very successful.

01:28.720 --> 01:33.760
Did you work from home quite a bit beforehand or is this a transition for you?

01:33.760 --> 01:39.560
So Sam, I came to the United States in 1984 and I've never worked from home ever.

01:39.560 --> 01:48.600
Wow. I know. I know. I know. I mean, it was like these like, you know, getting up in

01:48.600 --> 01:54.040
the morning, making your bed, getting ready and just leave every morning and then come

01:54.040 --> 02:00.440
back in the evening. So this is quite new to me to be here at home every day but somehow

02:00.440 --> 02:05.200
it says it's been working. Are you finding yourself more or less productive

02:05.200 --> 02:10.720
than like, how do you compare your level of productivity with this new way of working?

02:10.720 --> 02:17.920
I think that with the setup we created at JP Morgan in which we try to have a lot of

02:17.920 --> 02:23.600
meetings so that eventually we see each other as often as possible. I think it's working

02:23.600 --> 02:32.280
fine. I'm not 100% sure in three weeks if we are still in the same productive chain.

02:32.280 --> 02:35.560
But I think we have been quite productive. I mean, more than what I thought the first

02:35.560 --> 02:41.960
couple of days, I thought why that I would like be eating all day in the kitchen or doing

02:41.960 --> 02:48.720
my laundry or something. What is this of being home? But no, I mean, my office, I have the

02:48.720 --> 02:55.600
computer and I'm literally in my office from, you know, 18 in the morning to 16 in the afternoon.

02:55.600 --> 03:04.360
So I'm glad to hear that everything is going well on your end. Thank you. Why don't we jump

03:04.360 --> 03:12.040
into your background? You are currently on leave from current email and CMU where you

03:12.040 --> 03:18.120
were faculty in the Department of Computer Science. Tell us a little bit about your background,

03:18.120 --> 03:23.480
how you got involved and interested and involved in machine learning research and your

03:23.480 --> 03:31.200
kind of broad research interests. Yeah. Yeah. So this is like a very nice to explain. So somehow

03:31.200 --> 03:38.360
as a kid, I got very much interested in math. Math was really what I like. arithmetic

03:38.360 --> 03:42.840
from arithmetic all the way to calculus or everything. Math is what I wanted, what I

03:42.840 --> 03:49.080
like to do. In fact, I thought I would become a math teacher when I was like 14, 13, 15

03:49.080 --> 03:57.280
years old. And my father is a mechanical engineer and my mom also kept saying, no, you should

03:57.280 --> 04:03.960
be an engineer. So I became an electrical engineer and I was also very happy with the amount

04:03.960 --> 04:09.640
of math I had there. And it was very interesting. And okay, that's how I became an electrical

04:09.640 --> 04:15.680
engineer. And in fact, I am one of the people that I never read a single science fiction

04:15.680 --> 04:24.160
book. I didn't like science fiction movie. Zero, zero. So I'm not at all this typical

04:24.160 --> 04:32.560
care that loves computers and robots and aliens and start tracks and companies zero. It's

04:32.560 --> 04:38.560
true. I only cared about this math and what it could do for you in your daily life. And

04:38.560 --> 04:44.560
a lot of like interested on teaching math to people, very, very kind of stick. And then

04:44.560 --> 04:49.760
when I finished this, I'm just going to tell you what made me like a change in some sense.

04:49.760 --> 04:56.760
So when I finished my undergrad degree, I was still in Lisbon. So I was a, I am from Portugal

04:56.760 --> 05:04.760
from Lisbon. And I was involved in a master thesis in electrical and computer engineering

05:04.760 --> 05:12.760
in a project that was in the early 80s, very, very innovative in which we had to buy computers

05:12.760 --> 05:20.200
and automate the production of a company of a line. So the actual, there was a company

05:20.200 --> 05:25.440
that produced it freezers and refrigerators. It's just a manufacturing company. And all

05:25.440 --> 05:32.040
the orders in Venturi tracking, everything was then manually. And so we were supposed

05:32.040 --> 05:36.760
to work on how do we computerize, digitalize all that. That's very early.

05:36.760 --> 05:43.480
That sounds like a great project. I know. I mean, I still, I mean, I, this was fantastic.

05:43.480 --> 05:49.000
And then through this, through talking with people, so how do you represent these? How

05:49.000 --> 05:52.960
do you write that? How do you generate the list of parts? How do you do that? How do you

05:52.960 --> 05:59.520
do that? I became fascinated by what computers could actually do in terms of representing

05:59.520 --> 06:04.280
knowledge, in terms of search, in terms of learning from feedback, in terms of accumulating

06:04.280 --> 06:10.160
data in a digital way. I mean, so that was the beginning of AI for me was basically this

06:10.160 --> 06:17.720
engineering of getting like the functioning that we used to be done manually done by machines.

06:17.720 --> 06:24.240
So that's basically no really wanting to understand how does the brain work or how do

06:24.240 --> 06:31.400
I mean, it was this functional engineering goal of getting these computers to do more

06:31.400 --> 06:38.680
than just numerical computations, but to capture behavior, choices, learning, data, fascinating

06:38.680 --> 06:46.520
kind of like two years of my life in which I became in love with AI. Wow. Yeah.

06:46.520 --> 06:52.320
And so that was after undergrad. How did you then develop that interest?

06:52.320 --> 07:00.800
So then I was finished my masters and for family reasons, we came to the United States

07:00.800 --> 07:09.640
and I did another masters in computer science now at Boston University. And then I applied

07:09.640 --> 07:15.520
for a PhD in computer science and I went to Carnegie Mellon. And at Carnegie Mellon,

07:15.520 --> 07:21.160
if you think about Carnegie Mellon, that's like the, I mean, without offending anybody,

07:21.160 --> 07:26.760
but in those days, that's the mecca of AI. You know, so it was herb Simon there, Ellen

07:26.760 --> 07:32.480
Newell, Raj Reddy, Jaime Carbonell, Tom Mitchell, Jeff Finton in those days was there, Mark

07:32.480 --> 07:39.760
Raybird. I mean, it was all these people that thought nothing else than AI, literally.

07:39.760 --> 07:50.000
I remember when I came in 86, there was this meeting of the incoming students and the faculty

07:50.000 --> 07:54.560
would present themselves. And there was Raj Reddy that walked into the room and said,

07:54.560 --> 08:02.080
hey kids, imagine if all these walls, these cement walls would be displaying live the

08:02.080 --> 08:09.560
best masterpieces of the Louvre. This was like before we even had warloads. I'm telling

08:09.560 --> 08:15.840
you, this was all about taking big an Allen, you will say in 1987 or 86, what if all the

08:15.840 --> 08:22.120
people in their dorms could log into their computers and be able to know whatever is

08:22.120 --> 08:28.520
happening everywhere. And we were like, oh my god, you know, it's true. So I came here

08:28.520 --> 08:33.680
to the United States first in Boston and then I landed at Carnegie Mellon and I thought,

08:33.680 --> 08:39.400
where am I? I mean, this world of computers doing it all was fascinating at the same time

08:39.400 --> 08:45.080
that we were like trying to be chess for them, the Kasparov and all these chess playing

08:45.080 --> 08:54.000
and all sorts of, you know, anyway, Carnegie Mellon was for me and throughout the time

08:54.000 --> 08:59.280
was there, like this place where we dream big and we get amazing things accomplished. And

08:59.280 --> 09:05.720
just so you know, actually also in 1986 when I came to CMU, there were already autonomous

09:05.720 --> 09:13.200
cars there. And in fact, there was this car that was coming down the campus and in the

09:13.200 --> 09:17.320
morning, we would see by the library and then in the evening, when we would leave, it

09:17.320 --> 09:23.120
had moved like, I don't know, maybe like a hundred feet, I don't know. And it was supposed

09:23.120 --> 09:28.840
to be a big accomplishment that the thing had moved that far by itself. And this is

09:28.840 --> 09:35.000
the late 80s. So that's how I grew this, I mean, grew this passion for AI was a product

09:35.000 --> 09:40.440
of Carnegie Mellon and this dreaming big of all the things that machines could actually

09:40.440 --> 09:45.840
do. And it was fascinating thing. Well, it sounds like an amazing environment and time

09:45.840 --> 09:54.640
to be there. So fast forward to your faculty career. What, you know, it's also a little

09:54.640 --> 10:02.040
bit about your research interests. So in some sense, I build an interest always on building

10:02.040 --> 10:10.680
complete AI systems. A lot of my research has been on trying to connect the perception part

10:10.680 --> 10:16.520
with the cognition part and with the action, perception, cognition and action. I think

10:16.520 --> 10:23.840
that that first slide has been in my talk since for always. Because in some sense, also

10:23.840 --> 10:30.440
falling up on things that Alan Newell was telling us, AI has been a science of components,

10:30.440 --> 10:37.040
natural language processing, vision processing, planning, search, machine learning, speech

10:37.040 --> 10:44.920
recognition. So it has been a science of lots of interest, lots of dimensions. And I always

10:44.920 --> 10:49.320
thought that it would be good to put it all together as an agent that could do like the

10:49.320 --> 10:54.440
vision and not just the vision to classify this as a cup, but then to pick it up and not

10:54.440 --> 11:00.200
just to pick it up, but then to actually take it somewhere and not to take it somewhere,

11:00.200 --> 11:04.840
but actually to execute not just plan, but execution and all the problems of perception,

11:04.840 --> 11:09.240
cognition and action. And then that led me to spend all my life, all my life, building

11:09.240 --> 11:15.440
autonomous robots. So I've built a lot of autonomous robots, not necessarily autonomous cars.

11:15.440 --> 11:22.600
I never worked that much in outdoors robotics. I worked in indoor robotics. And my passion

11:22.600 --> 11:30.800
was not necessarily on manipulation, but on mobility. So from robot soccer to mobile robots

11:30.800 --> 11:38.600
in the environment, I've built tons of different robots that moved, interacted with people,

11:38.600 --> 11:44.840
could see the image, could see actually could do perception in real time, detect an orange

11:44.840 --> 11:52.520
ball in Robocup or other things. So I've been, I spent all my life building autonomous robots,

11:52.520 --> 12:00.520
capable of this perception, cognition, deciding where to go, and actually going. And that's

12:00.520 --> 12:08.680
what I've done in the late 90s. We started mid-90s, we started, I co-founded this Robocup initiative

12:08.680 --> 12:15.880
in which robots would play soccer in different leagues and all sorts of like different conditions,

12:15.880 --> 12:25.160
but basically the fascination was the autonomy. Even today, when I look at videos from robot soccer,

12:25.160 --> 12:32.520
the only thing I appreciate is thinking there was all done, it was all done by an algorithm.

12:32.520 --> 12:40.120
It was all automated. It's not exactly just the learning part, but it's actually the

12:41.160 --> 12:47.880
ability to frame a whole problem into something that the computer can actually do from beginning

12:47.880 --> 12:55.880
to end. I don't think we've talked about the Robocup challenge on the podcast previously. Can

12:55.880 --> 13:03.000
you frame the problem for us? I think my exposure to it has primarily been through watching

13:03.000 --> 13:08.680
blooper reels, which probably isn't representative. You know, the research accomplishment.

13:08.680 --> 13:16.760
Yeah, so Robocup started around like 94, 93, 92, and then the first competition,

13:16.760 --> 13:27.720
actual competition was in 1997 in Japan. And basically, so think about those days. In the 90s,

13:27.720 --> 13:35.080
there was like the blooming of the internet, a lot of like, you know, competitions on speech

13:35.080 --> 13:42.280
recognition, text understanding, and the physical world was not as much the focus in those days

13:42.280 --> 13:49.320
of what AI was doing. And the robot soccer came about to address this problem of

13:50.200 --> 13:57.000
of processing information, but at the physical world level. So robot soccer came about, in fact,

13:57.000 --> 14:04.120
as a challenge also for multi robot reasoning, the robots had to work in a team, they had to be able

14:04.120 --> 14:10.760
to have an algorithm that looks at the field, at the playing field, and decides where to pass the

14:10.760 --> 14:17.240
ball, and eventually had a very concrete goal, which was winning. So you had to score these goals.

14:17.240 --> 14:24.120
So that has motivated thousands and thousands of people. Robocup is even as if today

14:25.080 --> 14:30.360
happens every year, and it brings people like, it's limited. The number of teams that can come,

14:30.360 --> 14:35.320
but it's more than 3,000 people every year that come from all over the world. And the beautiful

14:35.320 --> 14:40.920
thing about Robocup is these autonomous. They are all autonomous. There has nobody joysticking

14:40.920 --> 14:47.560
these robots. There is nobody like telling anything the robots, the complete system has to be

14:47.560 --> 14:55.720
autonomous. Anyway, at the beginning, they barely moved. Yeah, yeah, it's true. They barely moved.

14:55.720 --> 15:00.520
We had a big problem with this connection of the perception and the reasoning. By the time we

15:00.520 --> 15:06.520
had the robot decide to go to the ball, the ball wasn't there anymore. So you had the camera that

15:06.520 --> 15:13.560
would process the image of the ball, that it was too slow, it was too noisy, it was everything was

15:13.560 --> 15:22.760
very hard to make the robot really act by processing its own images. Anyway, so that's a long story,

15:22.760 --> 15:28.680
and but only I had it put me always on to this mode of getting things that work.

15:28.680 --> 15:34.840
Anyway, so that's went from there to having like co-bots, which are these robots that move down,

15:34.840 --> 15:43.560
the corridors that Carnegie Mellon, they have moved thousands of kilometers in our environments,

15:43.560 --> 15:50.520
by themselves, not thousands, thousands and many, so close to 2000 maybe now. But again,

15:50.520 --> 15:56.440
like this goal of trying to put it all together, perception, cognition and action, data, reasoning

15:56.440 --> 16:03.000
and action. So the processing of the data combined with what we need these data for, and then eventually

16:03.880 --> 16:09.480
deciding and executing. So that was my life. There has been always my life. A lot of learning,

16:09.480 --> 16:15.320
but at the reinforcement learning level, learning at the action selection level, learning at the

16:16.120 --> 16:22.680
execution level, learning to improve perception with experience. As I keep calling these learning

16:22.680 --> 16:29.800
from experience, not necessarily the technique itself, whether it was neural nets or decision trees

16:29.800 --> 16:36.120
or SVNs or whatever method we use was more of these concepts of learning from experience.

16:36.680 --> 16:42.120
And so with this, with the kind of background and passion in robotics, how did you end up at

16:42.120 --> 16:49.560
JP Morgan Chase? I don't see many robots, you know, when I go to the bank. Wait, wait, wait,

16:49.560 --> 16:53.560
wait, wait, wait, wait, but I'm just saying. So that's very interesting. So it's true. I mean,

16:53.560 --> 17:00.600
I was quietly at Carnegie Mellon doing my research with a large group, many students, beautiful.

17:00.600 --> 17:07.160
And I was actually at the time in 2016, since 2017, I was the head of the machine learning department.

17:07.880 --> 17:16.520
And I got a call from JP Morgan asking, so telling us, telling me about these

17:16.520 --> 17:24.520
efforts, they were trying to start bringing basically AI reasoning, AI research, really, not

17:24.520 --> 17:35.880
just applied AI, but AI research to the firm. And what actually made me come to JP Morgan,

17:35.880 --> 17:42.360
which I'm very happy to be here at JP Morgan Chase, was in fact, the thinking like this,

17:42.360 --> 17:49.720
oh my god, this is a new area that I never thought about. And it was this fascination of thinking,

17:49.720 --> 17:54.600
oh my god, I have a chance of doing something in another area. It's almost as if I,

17:55.240 --> 18:00.840
robotics was beautiful, there was a lot to do, but I was at home in robotics. I mean, what,

18:01.720 --> 18:07.400
more image less image, more object less object, more command was kind of like, but then it was

18:07.400 --> 18:13.480
this chance. And then there was this major comment from the leadership at JP Morgan Chase that

18:13.480 --> 18:19.800
made the difference. They said, many of the companies, Google's, Amazon's, Facebook, Twitter,

18:19.800 --> 18:25.160
they were all born digital. They were all born, thinking about machine learning, thinking about

18:25.160 --> 18:30.200
data, thinking about computing, thinking about algorithms, everything was born digital. But all these

18:30.200 --> 18:37.640
other industries, like financial health, the construction, the cities, you name it existed well

18:37.640 --> 18:45.240
before the computers existed. So that was fascinating to me to think that I could come and I'm here

18:45.240 --> 18:52.840
to try to see how to bring more AI, my experience of AI, building these systems from beginning to

18:52.840 --> 18:59.640
end, can make a difference in terms of like the financial domain. So that's basically what brought

18:59.640 --> 19:06.920
me here. And you know, it was basically, it was somehow a mixture of being afraid, you know,

19:06.920 --> 19:14.840
being like, you know, apprehensive and all, an enormous excitement to be in a different thinking

19:14.840 --> 19:22.120
mode. So these are the two things. And the group that you run there, it's a research organization

19:22.120 --> 19:27.960
as opposed to, you know, building churn models for credit cards or something like that.

19:27.960 --> 19:35.800
Exactly. So we try to, we are a research group and we try to focus on, how can I say,

19:35.800 --> 19:43.880
aspirational goals? We try to focus on things that would make an impact if they were

19:43.880 --> 19:51.480
solved one day. So for example, I can, if this is okay, I can just share a few of the goals.

19:51.480 --> 19:58.120
We'll just focus on a few, but we kind of structured this AI research after a one year of being

19:58.120 --> 20:05.640
there absorbing what this financial domain was all about into seven big research goals,

20:06.440 --> 20:10.600
which we call these aspirational goals, these big research goals. And they are like this.

20:10.600 --> 20:17.720
Three of them have to do a lot with the actual domain itself and then use of the domain. And so

20:17.720 --> 20:25.320
they are. So how do you use a AI to really eradicate financial crime? Not to decrease the number of

20:25.320 --> 20:30.360
positive false positives or false negatives, the alerts, the frauds, the many laundry, but really,

20:30.360 --> 20:36.200
do we eradicate financial crime? It's like the cure cancer kind of goal. How do you do that? So

20:36.200 --> 20:41.320
that's why it's part of the research. It's like, how do you eradicate this financial crime and

20:41.320 --> 20:46.600
do a use a guy for that? Then another goal that is very important is this goal of like, how do we

20:46.600 --> 20:54.200
actually, we call it liberate data safely? How do we go about engaging with universities,

20:54.200 --> 21:01.240
making sure that people can help us and even in the firm, we can all share data and can make secure

21:01.240 --> 21:07.560
computations in a way that's safe private. I mean, another big goal. How do you go about doing

21:07.560 --> 21:14.120
this in a world full of like privacy issues and encryption issues and messiness of the data and

21:14.120 --> 21:21.720
ownership of data? How do we go about liberating these data safely? The third goal is the very

21:21.720 --> 21:26.920
interesting goal that has a lot to do with by Robotsocker when we're another, which is looking at

21:26.920 --> 21:33.800
this problem of the financial domain as a large, large multi-agent system. There are many

21:34.920 --> 21:42.520
interacting pieces. So we want to predict and affect these large complex economic systems

21:42.520 --> 21:50.040
in a way that we can actually make a difference in terms of like understanding what makes them,

21:50.040 --> 21:57.160
their dynamics and what makes them actually, how do they function in such a complex way?

21:57.160 --> 22:03.960
And so we do a lot of simulations, very simulation, very multi-agent simulations, very much like trying

22:03.960 --> 22:13.080
to understand the components of this very complex global problem with all, and it's a, I mean,

22:13.080 --> 22:18.360
again, it's a huge goal if we could one day understand how the multiple countries, the multiple

22:18.360 --> 22:26.040
parties, the modern times, virus, no virus, how can these, all these things affect the financial

22:26.040 --> 22:32.680
health of a society of individuals and what is this all about? So these three goals are very much

22:32.680 --> 22:38.440
like finance-oriented goals. And then we also have three goals that are very beautiful too,

22:38.440 --> 22:44.440
because they capture the stakeholders of these financial domain. And basically, the stakeholders

22:44.440 --> 22:51.000
are your own employees, your clients, and the regulators. And so there are these three

22:51.000 --> 22:56.840
components that you have to care about and try to bring a guy to improve the life of your

22:56.840 --> 23:03.320
own employees to help them move up in the value chain. You want to perfect the experience of our

23:03.320 --> 23:10.920
clients, and you also want to be able to agentize or to be able to automate this policy understanding,

23:10.920 --> 23:17.880
this policy compliance, this policy compliance in some sense. So you have these three other goals

23:17.880 --> 23:23.400
that have to do with people, with rules, with handling like how do we do the employees,

23:23.400 --> 23:28.360
that clients, and the regulators as well. And the seventh goal is kind of an overarching goal,

23:28.360 --> 23:37.320
which is how do we make these AI be ethical and taking into account the social good of the whole

23:37.320 --> 23:44.760
story. So we have these establishing ethical and socially good AI as the seventh kind of overarching

23:44.760 --> 23:50.520
goal. So that's what we do at JP Morgan Chase in terms of AI research is worked towards these

23:50.520 --> 23:57.160
seven goals. You spent the first year kind of studying the domain and articulating

23:57.160 --> 24:01.720
these goals. And you've been there a year and a half or so. Have you made much progress in any of

24:01.720 --> 24:07.720
these areas? Or how are you, you know, where have you made the most progress? So very good question.

24:07.720 --> 24:15.480
So as you can imagine, I started one person myself alone. So yeah, but you know, I did very

24:15.480 --> 24:22.440
different from CMU exactly. However, I was like surrounded by 250,000 people at JP Morgan Chase.

24:22.440 --> 24:29.320
So it would kind of like overwhelming also the scale. But on the other hand, the JP Morgan Chase

24:29.320 --> 24:36.200
happened to have been very, it has been a delightful journey because of the support that the

24:36.200 --> 24:42.920
leadership and everybody has given to these kind of like AI thinking. So it has been very nice.

24:42.920 --> 24:52.360
So we are currently 30 people. We are 31 or 32 or 29 with the offers we have somewhere around 30.

24:52.360 --> 24:59.080
And we kind of have worked in kind of like how we think about things is like all the projects

24:59.080 --> 25:04.920
somehow that we work on be it like us thinking about things or like the business asking us to

25:04.920 --> 25:10.840
think about things are all towards one of these goals. One way or another. Now everything is in

25:10.840 --> 25:16.360
place. I'll give you an example of two because I think maybe three. But I'll tell you one thing that

25:16.360 --> 25:25.000
is one example that is very nice. So basically we knew I'll give you this example first.

25:25.000 --> 25:30.920
We knew that the actual success currently a lot in machine learning had been like this

25:30.920 --> 25:40.120
neural net deep learning. And and we also I also know that deep learning is very successful on images,

25:40.120 --> 25:48.120
on images in particular. So when I was brought to visit the traders floor. So I was in these

25:49.000 --> 25:54.760
in London actually I was in London at JP Morgan in London and they brought me to this floor which

25:54.760 --> 26:00.520
is like well what we see in the movies right. So everybody's surrounded by screens with all these

26:00.520 --> 26:06.920
little kind of like graphs on the screens and people are making choices of sell no sell by no buy

26:06.920 --> 26:14.040
windows. And literally they I had this tour of that floor and they were explaining to me all these

26:14.040 --> 26:21.000
things and I was fascinated that I couldn't see anything else but images. Those screens that people

26:21.000 --> 26:27.480
were looking at they were not apples and orange they were not cats and dogs but they were these

26:27.480 --> 26:34.280
images of these little kind of graphs. And a lot of research has been done on applying math to

26:34.280 --> 26:41.480
predict and to this time series data but I could only see images. So I went to my lab and worked

26:41.480 --> 26:47.240
with us few of my colleagues in those those days and we said can we use these with a neural net

26:47.240 --> 26:55.000
to try to classify the images on their screens as by no buy choices. And we built this system

26:55.000 --> 27:01.640
and fascinating system we called it Mondrian. And we built this system we were able to use those

27:01.640 --> 27:09.880
images and with historical data on S and P 500 decisions we were able to reproduce 95 percent

27:09.880 --> 27:16.200
accuracy by just using images images or the by no buy choices that humans had made.

27:16.680 --> 27:23.960
That is fascinating. Surprisingly it is counterintuitive like the images are you know kind of

27:23.960 --> 27:29.320
dumbing down the data so that we as humans can interpret it why not just use the data itself.

27:29.320 --> 27:35.000
Well we could use the data itself but it doesn't do as well the data needs to be normalized like

27:35.000 --> 27:40.440
you know it's more the shape that we are capturing because the data the value 5 6 7 doesn't

27:41.160 --> 27:46.520
it's going to confuse a little bit the learning thing while the shape was remarkable at

27:46.520 --> 27:53.160
capturing it. And but on the other hand if you use also a good representation of the actual numbers

27:53.160 --> 27:59.880
it also works except that because we looked at how humans made decisions based on those numbers

27:59.880 --> 28:06.280
on the optional images images we then could do images for many other things that we have been doing

28:06.280 --> 28:16.200
that are not objects like we have traditionally applied to at least in those days to deep deep

28:16.200 --> 28:23.960
learning but they were images of other things of other processes of diet of other images of

28:23.960 --> 28:29.640
other visualizations of other decision making environments. I think about the doctor also they

28:29.640 --> 28:35.560
basically came look at your EEG data and make a decision normal normal normal and that decision

28:35.560 --> 28:41.720
comes from looking at an image and and so it's fascinating this concept was fascinating and

28:41.720 --> 28:48.840
it's this concept of associating this time series data to an image that then you can actually

28:48.840 --> 28:56.040
classify for decision making and we we it's a that was a very nice project. Is your ground truth

28:56.040 --> 29:03.560
in that example is it the decisions that the trader made or the performance no no no no no no

29:03.560 --> 29:08.920
so the decisions that traders made we are not yet like beating the whole world by using these

29:08.920 --> 29:15.640
images no we are you know I'm thinking of like all of the you know the pictures of some of these

29:15.640 --> 29:22.360
financial you know stock charts and you know how many different kind of voodoo incantations

29:22.360 --> 29:27.000
there are about this curve crossing that curve and this shape and that shape and it sounds like

29:27.000 --> 29:34.840
you're yeah in fact capturing some of that exactly exactly and I tell you I'm telling you

29:34.840 --> 29:43.240
you cannot imagine how much information about that correlates with the decisions is visually

29:43.240 --> 29:49.480
visible and in fact you just said the crossing the the the shape going up going down the other

29:49.480 --> 29:56.600
one coming down and down is very visible in an image as a feature and to the I mean to extract

29:56.600 --> 30:03.480
that from data from the actual numbers is another representation that is more that's different

30:03.480 --> 30:09.960
so I basically build upon the success we knew that images have the with machine learning

30:09.960 --> 30:16.120
to transform this problem into an image classification problem an image understanding project

30:16.120 --> 30:21.400
problem and it was very successful and it has been very successful successful and I'm telling you

30:21.400 --> 30:27.400
we are now doing prediction basically by looking at what are the what we don't know but what are

30:27.400 --> 30:34.440
the images what would the future which image would be the be the future that less disturb I mean

30:34.440 --> 30:41.400
anyway we are able to predict the images of the future and so it's a you know it's it's really

30:41.400 --> 30:46.600
beautiful so that's like to show you that you know it's a transformational way of looking at

30:46.600 --> 30:52.920
something that is very classical eventually making decisions by no buy on time series data but I

30:52.920 --> 30:58.680
looked at it as an image so that was another one that's very nice and then I will stop is just

30:58.680 --> 31:07.800
that we also have been trying to understand this problem of a client experience or fraud or all

31:07.800 --> 31:14.760
sorts of problems that we need to classify into let's think about fraud from a behavior point

31:14.760 --> 31:19.880
of view and I give you an example that is very important for us to understand so if you and I

31:19.880 --> 31:27.720
would have been asked to classify in New York City which of the cars are taxes and which ones are

31:27.720 --> 31:34.680
personal cars you know 10 years ago you and I use only the color of the car right you and I

31:34.680 --> 31:41.480
would say yellow taxi non yellow private that's it right so there was this classification based on

31:41.480 --> 31:47.960
this feature if we are asked now which cars are service cars and which are private cars the color

31:47.960 --> 31:55.240
doesn't help anymore only because basically there are all these ubers lifts v as everything

31:55.240 --> 32:01.960
so now it's a question of classifying behavior rather than just these one shot features

32:02.680 --> 32:08.120
because the behavior now is like okay this car went from here stop there then went there stop

32:08.120 --> 32:15.880
again has been goes like that oh that looks like a service car not a personal car it's the behavior

32:15.880 --> 32:23.080
in some sense that helps us classify service non service and we are trying to apply the same thing

32:23.080 --> 32:27.880
to problems that have been classified a lot by just looking at this particular transaction

32:27.880 --> 32:33.000
and deciding oh this is an amount this goes to an account that we don't know and just looking at

32:33.000 --> 32:43.240
that by building the behavior of these you know these users to try to detect what can be eventually

32:43.240 --> 32:49.880
behavior that is not normal behavior that is this this that we can distinguish from other type of

32:49.880 --> 32:56.040
behavior and so it became not a classification only of the transaction or only of this snapshot

32:56.040 --> 33:02.440
but we now in terms of research are building these representation of all these time dependencies

33:02.440 --> 33:09.800
of all these connections in terms of knowledge of all these graph based representation to try to

33:09.800 --> 33:17.640
capture much higher level the to try to classify these fraud no fraud to capture these complicated

33:18.920 --> 33:25.160
classification problems the site very similar to the taxi versus no taxi or car service versus

33:25.160 --> 33:30.840
no car service in New York the same problem and so that's another example in which we have shown

33:31.400 --> 33:39.080
that indeed by capturing how much of these nodes these these users in our network of

33:39.080 --> 33:45.720
interactions communicate with each other and these accounts transfer from one another to another

33:45.720 --> 33:53.720
you are able to detect much better behavior that is not normal so this is another example

33:53.720 --> 33:58.600
and we have done also the third example I can give you just to finish this kind of like examples

33:58.600 --> 34:08.760
is that we have been also basically automating the generation of a lot of documents either

34:08.760 --> 34:15.080
for regulators or for our clients or internally PowerPoint presentations representation

34:15.080 --> 34:22.840
as a translation from data that is in some format like numbers lots and lots and lots of

34:22.840 --> 34:30.600
information about transactions into English into another representation charts insights comments

34:30.600 --> 34:39.080
and we have been automating that kind of like a description of the data in a representation that

34:39.080 --> 34:46.520
is readable or that is formatted according to what's required to show when you say translation

34:46.520 --> 34:51.880
are you meaning that literally like using a neural machine translation type of a tool and applying it

34:51.880 --> 34:58.600
to this particular scenario very good question in fact we have not yet learned this translation we

34:58.600 --> 35:07.160
are really just writing an algorithm to make the translation computes basically all these it's

35:07.160 --> 35:15.320
like a you could think about is as a template feeling kind of like a algorithm the template is

35:15.320 --> 35:21.640
complex where do we get it we have to represent it well and then we feel and then we are already

35:21.640 --> 35:28.200
eventually at some point to do the learning of what this parameter should be and how the template

35:28.200 --> 35:34.040
should move and the personalization and who goes where but it's another level of learning it's not

35:34.040 --> 35:40.280
really learning that you know how do you say this word in a different language but so we built the

35:40.280 --> 35:47.560
translation machinery and now we have to adjust all the parameters of this machinery through feedback

35:47.560 --> 35:54.440
and through data through the use of this but we built the translation machinery and that required

35:54.440 --> 35:58.760
so the beautiful thing about building this translation machinery is that we try to

35:58.760 --> 36:05.640
built in in a way that it doesn't apply only to a specific task so it is general in terms of

36:05.640 --> 36:11.560
like different types of reports so they all require eventually the data to come into this form so

36:11.560 --> 36:18.600
you change the data and you manipulate these knowledge in a way that you can then automate so in

36:18.600 --> 36:27.160
some sense I am as you can imagine from my first days I am always a how can I say an automation

36:27.160 --> 36:34.040
person I'm more a young and hearing person and then machine learning for me is like a tool to try

36:34.040 --> 36:46.040
to get these these these automation machinery adjusted and personalized and generalized and

36:46.040 --> 36:55.080
generalized and transferred and all sorts of things but so the data plays the role for me as okay

36:55.080 --> 37:01.400
now I adjust all of these and this goes to that side this goes to this side so yeah so that's

37:01.400 --> 37:07.160
what these are examples of three projects the Mondrian project this kind of like behavior-based

37:07.160 --> 37:15.480
fraud detection and these automation of reports out of several others that we have done yep

37:15.480 --> 37:25.000
and do you see your charter at the bank in terms of a time horizon like there are lots of embedded

37:25.000 --> 37:31.960
machine learning AI folks I imagine all over the bank and creating organizations and client

37:31.960 --> 37:38.200
service marketing etc you know that are working on immediate projects or is one of the ways that

37:38.200 --> 37:47.240
you see your charter as being you're in you know five to you know 20-year realization or whatever

37:47.240 --> 37:53.720
the numbers might be it's a good question so indeed JP Morgan Chase is amazing in terms of how

37:53.720 --> 38:01.800
much machine learning is being applied throughout throughout the firm and for classification of emails

38:01.800 --> 38:07.960
for marketing decisions for decisions on credit no credit learn all these are supported by

38:07.960 --> 38:13.640
data driven algorithms I mean one way or another they are present throughout it's 50,000

38:13.640 --> 38:22.360
technologists that JP Morgan Chase has and more out of those thousands now more hundreds or AI

38:22.360 --> 38:28.440
and machine learning people very close to the business somehow we are building this understanding

38:28.440 --> 38:38.360
that the we work close to them we and they work close to us when the problems are reach a level

38:38.360 --> 38:45.640
of complexity or of dreaming that cannot they don't they don't have the immediate way of addressing

38:45.640 --> 38:50.920
them we start a much better dialogue for example this Mondrian thing or even like the generation of

38:50.920 --> 38:57.640
these documents automated or generalized and learning from the use is like kind of something we

38:57.640 --> 39:04.280
are involved it's not yet deployed so it's a while the applied machine learning the applied AI

39:04.280 --> 39:10.040
they actually are very close to the business and making sure things get immediately deployed

39:10.040 --> 39:15.800
but out in Havier our research we do these steps I mean think about the cure cancer problem

39:15.800 --> 39:20.440
you know there might be steps in which you find better chemo there might be steps that you find

39:20.440 --> 39:27.560
better devices and they end up you know getting to be in the hands of the protectors and

39:27.560 --> 39:32.200
so it's the same for us same thing with us for example these things we are doing about behavior

39:32.200 --> 39:37.320
recognition and eventually understanding the network of interactions for fraud detection

39:38.120 --> 39:46.280
will eventually be is closer to deployment than other things we are working on so we are in

39:46.280 --> 39:52.280
that spectrum and you know and we we interact a lot with the business we like a lot with our

39:52.280 --> 39:59.800
colleagues in applied machine learning and but we still kind of live in this world of being free

39:59.800 --> 40:09.400
to how do you say to transform to reinvent to to just think a little bit more without the boundaries

40:09.400 --> 40:15.960
of having something that is needed today now five 20 years I don't know but I do expect to

40:15.960 --> 40:26.200
that we expect so just so we know I expect that we produce novelty and innovation every year

40:26.200 --> 40:31.640
every year we do something when do we reach the goals who knows I mean in Robocup we used to say

40:31.640 --> 40:39.640
by 2050 we will beat the World Cup human players robots will beat the World Cup human players and

40:39.640 --> 40:47.320
we made that 2050 we are getting too close to that but anyway but now exactly when we'll eradicate

40:47.320 --> 40:55.480
financial crime but we also but these goals we built them to be like for 2030 it's like any

40:55.480 --> 41:04.120
or goal in some sense but is your your novelty and innovation metric is what is it you know direct

41:04.120 --> 41:08.200
value to the business many you've come up with something and they've implemented it and it's

41:08.200 --> 41:14.440
solving a problem or is it you know is there a publishing model for example were you expect to be

41:14.440 --> 41:19.160
you know publishing an academic literature and that's a metric for a novelty and innovation

41:19.160 --> 41:23.800
definitely the latter too we have a lot of connections with universities we publish

41:24.440 --> 41:30.040
all our results so we want to be we are involved in all the research activities and

41:31.400 --> 41:36.200
I could like tell you Sam a whole other conversation about our connections with universities

41:36.200 --> 41:42.120
the fellowships we created the research awards we created I'm sorry I did not focus on that as

41:42.120 --> 41:52.520
much but yes we are literally a research group and as such we are a research group however I do

41:52.520 --> 41:59.480
believe that we will that we make a difference also at another level of value to the company

41:59.480 --> 42:08.360
oh we bring talent you know it's a very good like we a very good benefit for our firm a lot of

42:08.360 --> 42:17.160
talent in this computer science core and AI core and machine learning AI and computer science

42:17.160 --> 42:26.520
computer engineering awesome are there any particular places that we can look for publications

42:26.520 --> 42:33.320
or some of the work that you're doing yeah yeah there is a we have an external website with all

42:33.320 --> 42:41.640
our publications and I can text to you but I believe it's just JP Morgan slash AI or JP Morgan

42:41.640 --> 42:46.920
Chase I forget now I have it the bookmarked but I will chase it down and include it in the

42:46.920 --> 42:51.880
general page perfect and it has all this description of the goals the publications we have done

42:51.880 --> 43:00.520
these different pillars everything okay how how do you see the the role in the group evolving

43:01.320 --> 43:06.840
you know again you're kind of recently having established these principles and kind of getting

43:06.840 --> 43:12.520
underway you've grown pretty significantly you know what do you expect to see over the next few

43:12.520 --> 43:23.480
years so we are a group of basically me heading and then I have three research directors Tucker

43:23.480 --> 43:29.880
Balch who came from Georgia Tech Prashantredi who came from Google and then Samina Shah came from

43:29.880 --> 43:38.920
S&P 500 and basically the three of them and me are the directors of these group we have only

43:38.920 --> 43:46.040
masters and PhDs in my group masters level and PhDs and we expect to grow by next year to be a

43:46.040 --> 43:53.080
group of probably 50 by the end of this year and then probably grow up to be 100 within the next two

43:53.080 --> 44:03.240
three years and stay at 100 150 within JP Morgan doing research AI research in the financial domain

44:03.240 --> 44:13.080
so anyone who has an interest on doing AI research in not the not only I mean or wants to consider

44:13.080 --> 44:19.640
not only the technical the tech the the giant tech the Google's the Facebook and

44:19.640 --> 44:25.960
and so forth this is a great place to be to do AI research in this particular kind of like

44:25.960 --> 44:34.040
financial domain but in great so we publish in great through AI research and AI machine learning

44:34.040 --> 44:40.440
so that's what I'm trying to build is a little AI research group up to 200 people I would say

44:40.440 --> 44:46.600
within the next five years well Manuel thanks so much for taking the time to chat with us

44:46.600 --> 44:52.120
share a bit about what you're up to it was great great meeting you and great learning about what

44:52.120 --> 44:57.240
you're doing thank you very much Sam and thanks a lot for hosting me absolutely please be safe

45:00.040 --> 45:05.480
all right everyone that's our show for today to learn more about today's guest or the topics

45:05.480 --> 45:11.880
mentioned in this interview visit twimmel ai.com of course if you like what you hear on the podcast

45:11.880 --> 45:18.680
please subscribe rate and review the show on your favorite pod catcher thanks so much for listening

45:18.680 --> 45:24.680
and catch you next time

