WEBVTT

00:00.000 --> 00:15.600
Welcome to the Tumel AI Podcast. I'm your host, Sam Charrington.

00:20.800 --> 00:24.640
Hey everyone, I am on the line with Robert Osezua Ness.

00:24.640 --> 00:30.640
Robert is a machine learning research engineer at Gamalon and an instructor at Northeastern

00:30.640 --> 00:38.720
University. Robert and I met at the last NURPS conference where he had an accepted poster session

00:38.720 --> 00:45.680
around his paper integrating Markov processes with structural causal modeling enables counterfactual

00:45.680 --> 00:51.280
inference in complex systems, which he also presented at the Black and AI workshop.

00:51.280 --> 00:57.440
This kicked off a bunch of great conversations between the two of us leading ultimately

00:57.440 --> 01:02.800
to collaboration that we'll talk a little bit about in this conversation. Robert, thanks so much

01:02.800 --> 01:09.600
for joining me on the Tumel AI Podcast. Thanks for having me, Sam. Your introduction makes me think

01:09.600 --> 01:15.760
I should have came up with more clever name for that paper. You know what, a lot of the papers

01:15.760 --> 01:22.160
we talk about on this show are quite the mouthful, so yours is no exception. Maybe someone will

01:22.160 --> 01:29.520
build a model that seeks to determine a inverse correlation or correlation between the lengthiness

01:29.520 --> 01:35.440
of a paper's title and its number of citations or something like that, but let's set that aside

01:35.440 --> 01:43.120
for now. Have you spent a few minutes introducing yourself? How did you get started in machine learning,

01:43.120 --> 01:48.560
what picture interests? You know, ultimately we'll be spending a lot of time here talking about

01:48.560 --> 01:54.080
causality. How did you come to become interested in that? You know, my path to machine learning was

01:54.080 --> 02:04.160
a bit, I'd say unconventional. I started off working in Asia and China, specifically I

02:04.160 --> 02:12.240
was at the degree at Hopkins and international studies and was planning to pursue a degree in

02:13.120 --> 02:18.960
economic, in economics, focusing on economic development. I got involved with some internet

02:18.960 --> 02:26.160
companies out in Beijing and that's kind of, that got me into coding and databases and data in general

02:26.160 --> 02:35.840
and I decided I was interested in that and went to, so I applied to four programs in statistics,

02:35.840 --> 02:42.400
particularly with a focus on computational statistics. I went back to the States, came back to the

02:42.400 --> 02:50.960
States, went to Purdue University to do my PhD in stats. My PhD work was on causal inference,

02:50.960 --> 02:59.840
graphical models, basically how to learn causal models from data, particularly in the context

02:59.840 --> 03:07.600
of systems biology and from then after I graduated I went straight into industry. Got it. Now,

03:07.600 --> 03:14.800
we hear very frequently folks refer to their path into machine learning as unconventional or

03:14.800 --> 03:22.320
indirect. In your case, you came into an interesting gaming and that led you to apply for or go into

03:22.320 --> 03:27.760
grad school for statistic. What was that particular connection? Really, it's when you're on the

03:27.760 --> 03:33.680
back end of an app and you're looking at the data and you're realizing that there's a lot of insights

03:33.680 --> 03:39.520
to be had of only we could model this data and turn it into some service on the front end.

03:39.520 --> 03:49.440
I realized, people were just starting to talk about data science and how Varian had just recently

03:49.440 --> 03:53.360
came out and said that statistics is the new sexiest. I can't remember what the exact

03:53.360 --> 03:58.720
quote was. Pick your metaphor. Yeah, pick your metaphor. Orange, New Black, statistics is the new,

04:00.080 --> 04:09.280
rock star. That's why I pivoted to stats in

04:09.280 --> 04:14.800
machine learning. I guess through stats, people might argue whether or not stats in machine learning

04:14.800 --> 04:21.600
are the same thing. The problems that I was working on at PhD were using probabilistic graphical

04:21.600 --> 04:29.360
models, which has strong roots in artificial intelligence. That was my introduction to machine

04:29.360 --> 04:38.080
learning. One of the things that's come up in our conversations about causality and the work

04:38.080 --> 04:46.160
that you're doing with your courses is the idea that historically talking about causality has

04:46.160 --> 04:56.960
been the domain of statisticians and folks like economists and that a lot of that conversation

04:56.960 --> 05:02.880
is inaccessible or isn't really tailored to the needs of developers and data scientist machine

05:02.880 --> 05:06.960
learning engineers. I didn't realize all the time we were talking about that that your background

05:06.960 --> 05:15.600
was an economics. You have some of the exposure to the way that causality has been traditionally

05:15.600 --> 05:19.840
kind of used and talked about. Maybe I guess I'll just use this as a segue to

05:21.040 --> 05:26.640
kind of opening up the floor to ask you, how do you define causality?

05:26.640 --> 05:37.120
The interesting thing about causality and maybe part of why maybe it is a challenging thing to

05:37.120 --> 05:42.480
deal with particularly for statisticians I would say is that it's very difficult to talk about

05:42.480 --> 05:49.680
it without finding yourself having a philosophical conversation. This is something that

05:49.680 --> 05:56.400
you know what is the causality. These are something that philosophers have been wrestling with

05:56.400 --> 06:01.200
through the ages, right? Hume has a counterfactual definition of causality that's

06:03.200 --> 06:06.800
you know A follows from B and had A not happen B would not have happened

06:08.320 --> 06:14.640
but you know philosophers going back to the Buddha all kind of take their stab at what is causality

06:14.640 --> 06:20.960
and so there's a there's different philosophical arguments for causality and what it means.

06:21.920 --> 06:29.360
I think from a practical standpoint what most people mean when they say cause or inference is

06:29.360 --> 06:37.760
they mean the estimation of causal effects. So if you're say for example at a tech company and

06:37.760 --> 06:46.160
you want to run some kind of experiment about whether a feature will drive a click or some other

06:46.800 --> 06:53.920
key performance indicator or metric you're asking your experiment is essentially trying to get

06:53.920 --> 06:59.280
at the question of what is the causal effect of this feature on this outcome and you'll be using

06:59.280 --> 07:08.800
assumptions and methods from statistics to estimate assuming your assumptions are valid those causal

07:08.800 --> 07:14.880
effects but when we're talking in machine learning we're now hearing you know so I had

07:14.880 --> 07:20.880
near ups like you said you know Joshua Benjillo gave this talk about having agents that can

07:20.880 --> 07:25.920
understand the causal structure of the world and that causality is essential from moving from

07:25.920 --> 07:32.080
system one to system two cognition as you day a pearl was a very preeminent causal inference

07:32.080 --> 07:39.760
researcher talks about causal reasoning in in terms of free will and you know the ability to

07:39.760 --> 07:47.120
understand intention and and so there are definitely definitely a lot of angles to tackle this

07:47.120 --> 07:52.080
question from the perspective of artificial intelligence that you know people who are running

07:52.080 --> 07:59.680
experiments in in Facebook and Netflix are not really thinking about yeah it's interesting that

07:59.680 --> 08:06.480
you you kind of you know where you started your response and ended your response when I talk

08:06.480 --> 08:14.960
to people about causality and and agreed you know at nirips and even before you know causality

08:14.960 --> 08:22.560
over the past year has really been one of those topics I think maybe peeking at nirips it was

08:22.560 --> 08:27.200
definitely a hot topic at nirps I don't think I think it's too early to say that it was peeking

08:27.200 --> 08:32.480
and I don't think that that's the case but it was definitely a hot topic at nirps but even before

08:32.480 --> 08:39.520
there was kind of this growing crescendo of interest and enthusiasm around this idea of causality

08:39.520 --> 08:47.600
when I talk to folks about it the conversations tend to be you know either the the philosophical

08:47.600 --> 08:53.760
conversations kind of very abstract conversations that you mentioned that you started your

08:53.760 --> 09:01.440
explanation with or you know talking about it like it's you know another algorithm or

09:01.440 --> 09:10.080
tool or approach on the on the shelf and that hey you know we've been doing machine learning and

09:10.080 --> 09:15.600
now we you know you know we realize we just need to pick up this you know this causality tool

09:15.600 --> 09:19.440
and kind of sprinkle it on what we've been doing all along and you know we've got a better

09:19.440 --> 09:24.560
approach to doing things do you get that sense as well in the industry and what do you think

09:24.560 --> 09:34.400
driving that yeah and I I sense a lot of cultural clash in terms of trying to articulate the problems

09:34.400 --> 09:39.840
in one field and what the ideal solutions are coming from one camp and you know what people are

09:39.840 --> 09:44.320
already doing who are working on causal inference and what they think the smart ways of what the

09:44.320 --> 09:50.480
next steps are in that line of research I think you know from the algorithmic standpoint clearly

09:50.480 --> 09:56.560
yeah that's where we want to go we want to be able to algorithmatize causal reasoning right now

09:57.200 --> 10:02.320
um and there are ways of doing that and they're connected of course so for example if we're talking

10:02.320 --> 10:09.680
about like what I said the traditional question of of causal inference is you know can we estimate

10:09.680 --> 10:16.000
a causal effect if the causal effect is again the effect of something that you care about like some

10:16.000 --> 10:21.600
kind of utility you're not really that far away from say a mark-off decision process or

10:21.600 --> 10:26.320
reinforcement learning especially for example you're you know the the thing that's your the

10:26.320 --> 10:30.080
experiment that you're running your running in sequence so so that's you know you you run the

10:30.080 --> 10:36.240
experiment you get some kind of results you you update some kind of belief structure about your

10:36.240 --> 10:42.400
motto and then run a new experiment that's getting very very close to the language of reinforcement

10:42.400 --> 10:46.480
learning but if you're in if you're working in reinforcement learning and you try and dive into

10:46.480 --> 10:52.880
that literature you're going to see a bunch of papers on public health uh experimental designs and

10:53.600 --> 10:59.920
you know and and really kind of very domain specific assumptions built up around linear models

10:59.920 --> 11:04.320
none of the stuff that you're working on if you're a typical reinforcement learning or researcher

11:04.320 --> 11:14.800
another issue would be for example if you are working on trying to if you're working on deep learning

11:14.800 --> 11:22.560
for example and you're thinking okay well how can I just apply deep learning to causal inference

11:22.560 --> 11:27.600
well one of the chief problems in causal inference is the question of whether or not something's

11:27.600 --> 11:33.760
identifiable right so this is to say that I want to I have some I have a model and I have a causal

11:33.760 --> 11:40.000
question that I want to ask my model and I have some data now given my data is this question even

11:40.000 --> 11:46.160
is it even possible for me to answer it and the answer might be no this is something that I found

11:46.160 --> 11:52.080
that people with a background in machine learning particularly a deep background and deep learning

11:52.720 --> 11:57.920
have trouble thinking about because they're they tend to think that if I get the right architecture

11:57.920 --> 12:05.280
and I get enough data and I label the the data in the correct way and I get the right loss function

12:05.840 --> 12:11.360
then I can solve this and so that that's a huge clash right there right that's a it's a fundamental

12:11.360 --> 12:17.840
clash and so um you know getting a lot getting over some of those I consider them cultural barriers

12:18.560 --> 12:26.000
is um is I think what we need to be striving for as a community the example that you brought up

12:26.000 --> 12:35.040
of you know you're an engineer working at uh you know uh add tech startup or you know any kind of

12:35.040 --> 12:43.760
web startup uh and you want to estimate the probability that the change that you're making

12:43.760 --> 12:49.120
you know has a result that you're predicting you kind of use that broadly as an example of where

12:49.120 --> 12:56.000
you might apply causality uh but we've been trying to estimate these kinds of things for a long time

12:56.000 --> 13:03.760
what's different about the causal approach to this well at a if you had to define what a fun

13:03.760 --> 13:09.280
what a causal model can do that's a non causal model can't do at a most basic level is that it

13:09.280 --> 13:16.640
can predict the effect of an intervention and so what an intervention is is some kind of action

13:16.640 --> 13:22.960
that's that you take or a modeler takes or people who are making decisions based on a model

13:22.960 --> 13:28.720
on a model are taking that affects the data generating process um and so if you are affecting

13:28.720 --> 13:34.640
the data generating process then and you're and you're you're changing it well that means that's

13:34.640 --> 13:38.240
the data that you're now getting from that new process is different from the data that you

13:38.240 --> 13:43.680
change your model on now if you have a causal model your causal model can account for that change

13:43.680 --> 13:50.000
and its predictions will still be valid but as we know with even some of the more powerful models

13:50.000 --> 13:55.120
in machine learning if we if they're just pure predictive models they don't have a causal structure

13:55.120 --> 14:01.520
to them then the second you change the uh the data generating process such that a difference

14:01.520 --> 14:05.920
from the training data you're going to have it you're going to have issues and it's not going to be

14:05.920 --> 14:11.760
very robust to new data that your model hasn't seen before and so that's that's kind of when I

14:11.760 --> 14:15.840
when I try and pitch people on like why should I care about causal inference that's really the

14:15.840 --> 14:20.880
the main thing is that so in the example I think that you're referring to like it's a simple example

14:20.880 --> 14:27.840
but I say suppose that you were you know you had some kind of sensors that were collecting

14:29.040 --> 14:33.840
data about the climate the weather in the local the micro climate where you live in the morning

14:33.840 --> 14:39.520
and using that and then in the evening you you you record whether or not it rained okay and so

14:39.520 --> 14:43.600
after a while you collect you collect enough data and then you create a predictive model that

14:43.600 --> 14:50.000
predicts whether or not it will rain and then based on this model you make a decision based on

14:50.000 --> 14:53.840
this prediction you make a decision and that decision is whether or not to carry an umbrella with

14:53.840 --> 15:01.040
you in the morning now that's fine because what happens there is carrying the umbrella is going to

15:01.040 --> 15:06.400
have no effects on the weather but we can look at a comparative example say for example in business

15:06.400 --> 15:12.640
where you look at some business indicators and then you you and then at the end of whatever

15:12.640 --> 15:18.240
period it is a quarter you know a cycle you record you know the revenue for your company

15:19.280 --> 15:24.160
and then you do that for a while and then you train a model that predicts future revenue given

15:24.160 --> 15:28.960
these business indicators and then if your future revenue looks like it's going to be low

15:28.960 --> 15:34.800
you run some advertising well that advertising is going to now start generating data that is

15:34.800 --> 15:40.400
affected by that advertising and so when you retrain your model and that new data you're getting

15:40.400 --> 15:48.240
this kind of feedback loop that where your actions are affecting the data generating process and

15:48.240 --> 15:55.520
and feeding back into the data this this is an issue for example in fraud detection so that's

15:55.520 --> 15:59.600
I don't know what happens once you start you're able to detect fraud then you start rejecting

15:59.600 --> 16:04.080
the fraudulent transactions and now all the only fraudulent transactions that make it into your

16:04.080 --> 16:08.400
future trading data are these edge cases of fraudulent transactions and so when you retrain

16:08.400 --> 16:14.400
your model it's now has this strange bias towards those edge cases and so that can confuse things

16:14.400 --> 16:21.280
and so at at a very basic the very basic use case for causal modeling and in machine learning is

16:21.280 --> 16:27.360
to be able to predict the outcomes of interventions and so and once we can bridge interventions we

16:27.360 --> 16:33.760
can do a lot of clever things on top of that so we can for example we can have models or we can

16:33.760 --> 16:39.040
have modules of our models that are more robust and can transfer across domains so that we don't

16:39.040 --> 16:43.120
have to retrain them once we get to a new data set or a new domain and considering the cost of

16:43.120 --> 16:46.880
training a model these days that can be that can lead to some significant cost savings

16:48.240 --> 16:54.480
again we can we can estimate causal effects which is very relevant and feels like reinforcement

16:54.480 --> 16:59.680
learning as I said and you know and then there's the there's the question of counterfactual reasoning

16:59.680 --> 17:07.680
so just to recap that before we move on what you're saying is that the traditional machine learning

17:07.680 --> 17:12.640
models that are kind of sitting on the shelves for us to use you know tend to make this assumption

17:12.640 --> 17:19.040
this iid assumption that the you know the data that it's trained on are all independently and

17:19.040 --> 17:26.880
identically distributed right and in many use cases the decisions that we're making based on the

17:26.880 --> 17:34.480
models that we're using actually changes the distribution and the independence of the data

17:34.480 --> 17:43.280
in future time steps and sounds like we just live with that application but it causes problems for

17:43.280 --> 17:51.120
us and one of the reasons why people are starting to get excited about causality is because it holds

17:51.120 --> 17:56.880
some promise for fixing this and thus you know making our application of machine learning more kind

17:56.880 --> 18:01.600
of applicable in the real world that we live in yeah you know so speaking of real world applications

18:02.480 --> 18:07.040
people would be interested I think you've had speakers like Tim Nidon who were talking about

18:07.680 --> 18:12.720
algorithmic fairness and you know so causal inference would have a lot to say about that for

18:12.720 --> 18:19.440
example right so like is is a policy's effect on some kind of outcome like say an adverse outcome

18:19.440 --> 18:26.160
for a specific group of people is it a direct cause of that policy is it's kind of being mediated

18:26.160 --> 18:32.960
through indirect causes or is it just correlated to that to that initial cause because of some kind

18:32.960 --> 18:40.880
of confounding factors in the outside environment that generated the data though you know so

18:40.880 --> 18:46.560
that is one reason why when we're worried about like you know algorithmic fairness and ethical

18:46.560 --> 18:52.720
way I clearly you know causal inference has a lot to say about that the example that I just gave

18:52.720 --> 18:57.840
of course and just making sure that our models if we're making decisions that affect the training

18:57.840 --> 19:04.320
data that goes into our models and making sure that's is somehow accounted for and you know those

19:04.320 --> 19:09.520
are those are some very basic applications of it although I think that when when machine learning

19:09.520 --> 19:14.480
people at machine learning conferences get excited about causality I don't think it's so much

19:14.480 --> 19:22.880
because of estimating causal effects or ethical AI or or explainable AI I think it really has to

19:22.880 --> 19:29.360
do with things like modeling like disentangle representations and and and variational autoencoders

19:29.360 --> 19:35.680
for example like if for these if we're trying to parse apart these latent variables that are

19:35.680 --> 19:39.600
that have some kind of meaning and transfer across datasets can we bring can we bring some causal

19:39.600 --> 19:45.440
reasoning to bear on that problem I think people are interested in people in a probabilistic

19:45.440 --> 19:52.240
programming community like myself are very interested in the ability to build causal models

19:52.240 --> 19:59.040
as programs and use those to reason counterfactually that that's the kind of cure that's the kind of

19:59.040 --> 20:06.080
bread and butter AI application of of causal inference although these other these other domains

20:06.080 --> 20:11.120
that I mentioned are clearly very relevant you mentioned disentangle representation and

20:11.120 --> 20:16.240
variational autoencoders when I think of that the kind of analogy that comes to mind for me

20:17.120 --> 20:23.600
is like embeddings and you know a transformation from you know the feature domain to some

20:23.600 --> 20:30.240
embedding space and typically that's a kind of opaque representation is the idea that with

20:30.240 --> 20:36.880
causality that embedding space or representation space is more meaningful causal understandable

20:36.880 --> 20:42.720
explainable or is it something different than that yeah so generally the idea is that

20:42.720 --> 20:50.240
this embedding space is somehow capturing latent causes and so there's some very there's

20:50.240 --> 20:58.320
very important question about hey okay statistically is it even possible to identify latent causes

20:58.320 --> 21:02.320
even in a multi-variant setting which some people think is possible like but there's a lot of

21:02.320 --> 21:06.080
you know you can show mathematically that you you get a lot of problems if you try doing that

21:06.080 --> 21:10.960
so people are worrying are thinking about that and they're thinking about you know there are when

21:10.960 --> 21:21.200
you have a a a a a a probability distribution say of a of a effect given a cause a conditional

21:21.200 --> 21:28.640
probability distribution of an effect given a cause is driven by some natural causal mechanism

21:28.640 --> 21:35.280
that drives that effect from the cause right and we assume that that's causal mechanism since

21:35.280 --> 21:42.720
it's a part of nature is going to be invariant across datasets right so if we can look at that

21:42.720 --> 21:49.200
type of intuition and you know what it means for these mechanisms or what what the nature of these

21:49.200 --> 21:54.160
mechanisms and how they might for example be independent of one another what that might say about

21:54.160 --> 22:00.080
the distributions that they entail you know can we now build those as assumptions inside of

22:01.200 --> 22:08.000
of a say a deep model like this right build it into the for safer example the the the loss function

22:08.000 --> 22:15.120
or the structure of the model and come up with latent representations that we can have

22:15.120 --> 22:20.400
stronger believe that these are actually good representations of causes you know and talking

22:20.400 --> 22:26.160
about causality we've talked about some of the you know the kind of philosophical origins of just

22:26.160 --> 22:34.880
defining what causality means and it's popularity within the research community how ready are the

22:34.880 --> 22:41.360
tools around causality that they can be practically used by you know folks in industry are we there

22:41.360 --> 22:49.280
yet or is there are we not quite there yet there are some excellent packages both in R and in Python

22:49.280 --> 22:57.440
that will do useful things when it comes to causal effect estimation so you know it's I there's

22:57.440 --> 23:05.200
a fellow Microsoft research name Amit Sharma who has a library called DUI which is I think when

23:05.200 --> 23:11.920
it comes to just that problem of estimating causal effects I think is has the has one of the best

23:11.920 --> 23:17.520
inner faces that I've seen for that problem it's very intuitive because you lots of options and

23:17.520 --> 23:22.720
it separates and it separates the problem of identification and estimation very well I am somebody

23:22.720 --> 23:30.960
who comes from the deep probabilistic programming community I like to think about a data generated

23:30.960 --> 23:37.360
generating function as a causal process and I like to think about how I can model that in

23:38.080 --> 23:52.720
in a program and and use tools like tensor flow or pi torch to capture the complexities and

23:52.720 --> 23:59.920
nonlinearities in that system so so for example with the workshops and the courses that I teach I

23:59.920 --> 24:08.960
use pyro which is a deep generative modeling framework from Uber AI labs that extends

24:09.600 --> 24:14.960
pi torch now that said you know some of the challenges there and there's some challenges there

24:14.960 --> 24:19.520
when it comes to inference on these models it can be I think that's an act of area of research and so

24:20.800 --> 24:27.120
if you're not you know you kind of have to turn away at the inference problem when you're working

24:27.120 --> 24:34.480
on these models but it's certainly doable well maybe refine or restate the question

24:36.000 --> 24:44.400
to you know where are people successfully using this kind of body of knowledge around

24:44.400 --> 24:50.560
causality in machine learning not in stats or economics but in machine learning you've mentioned

24:50.560 --> 24:56.560
some examples of representative problems but are these problems that are you know solvable

24:56.560 --> 25:03.680
today that people are you know using causal approaches in or are these things that we would like

25:03.680 --> 25:10.560
to apply causality to once the tools are mature you know the best applications within the AI

25:10.560 --> 25:15.120
space that I've seen and I very much hope if your listeners take issue of what I'm about to say

25:15.120 --> 25:20.560
that they they email me and prove me wrong but you know most of the the cutting edge up is

25:20.560 --> 25:30.800
happening within a research community within industry there are I would say that's I've seen

25:30.800 --> 25:37.520
people in industry working on say an analysis by synthesis approaches which is where say you

25:38.160 --> 25:46.160
have some kind of latent grammar of you know abstractions within a domain that you use to

25:46.160 --> 25:53.120
generate say images or synthetic environments for say for example training self-driving cars for

25:53.120 --> 25:59.280
example and when they design these grammars they add a kind of causal semantics to them I've

25:59.280 --> 26:04.960
seen some early success with those types of approaches the biggest application in industry that

26:04.960 --> 26:10.560
I've seen be successful that's kind of not surprising is when it comes to say actual machine learning

26:10.560 --> 26:19.280
models in production would be people who are applying causal inference techniques and experiment

26:19.280 --> 26:26.240
and experimentation to the optimizing of hyper parameters for machine learning model so it's

26:26.240 --> 26:31.760
just running experiments on models to make them to tune their parameters. So I hinted at this

26:31.760 --> 26:38.880
earlier you've got a course that you are teaching at Northeastern University as well as through

26:38.880 --> 26:46.160
your own upstart education company all deep AI and I'll have you talk a little bit about that

26:46.160 --> 26:52.880
and we're launching a collaboration around bringing a study group that you'll be leading

26:52.880 --> 27:01.280
for this course to the twimmel community and anyone who's listening to this who's interested

27:01.280 --> 27:08.000
in learning more can join us for an overview session that we'll be doing on February 1st but

27:08.000 --> 27:14.640
so let's have you share a little bit about your thinking around this course how it's different

27:14.640 --> 27:23.120
from other educational resources around causality and in particular who it's who it's for

27:23.120 --> 27:30.400
given kind of where you think the you know the market is and where causality is from a utility

27:30.400 --> 27:38.560
perspective. Okay yeah so it's it's all deep school of AI all deep AI and it helps people

27:38.560 --> 27:44.800
break into new opportunities surrounding cutting edge AI and so it assumes that you already have

27:44.800 --> 27:51.520
a background in data science and machine learning and that you can code although not a preliminary

27:51.520 --> 27:56.320
background is fine you know I started it because as a as a research engineer working in the field

27:56.320 --> 28:01.840
I was seeing that you'd have to have a case where somebody would identify some kind of pivot

28:01.840 --> 28:06.400
they would they want to make like say for example I'm a data scientist and I want to transition to

28:07.600 --> 28:12.080
an engineering role where I'm working on machine learning algorithms or that I'm a software

28:12.080 --> 28:17.360
engineer and I want to take a lead on building our company's new experimentation platform

28:18.400 --> 28:22.960
or even kind of off the wall things like I want to implement and deploy an agent step plays

28:22.960 --> 28:29.120
online poker right and so to get these types of goals to really achieve them you need to combine

28:30.000 --> 28:36.800
learning about new research in an area in AI with actually finding tools and libraries and

28:36.800 --> 28:41.680
tutorials that help you do the implementation and you know so the process of going through a bunch

28:41.680 --> 28:48.320
of research papers and medium blog posts and jubber notebooks it's slow it's arduous it's there's

28:48.320 --> 28:53.280
a lot of hype and noise that you have to turn through in order to get a little bit of signal

28:54.080 --> 28:59.680
so all deep solving this by bundling the science and the implementation and presenting in a way

28:59.680 --> 29:03.760
that delivers a transformation that people are looking for by the end of the workshop

29:04.720 --> 29:11.360
so the first sequence of courses are having to do with you know causal modeling and machine

29:11.360 --> 29:18.240
learning and so if you've been to you know like you're like you were saying before you know we've

29:18.240 --> 29:25.280
gone to these conferences we know that's the gets to common sense reasoning in AI we need to look

29:25.280 --> 29:29.840
at causal inference but it like I said if you look at the causal inference literature you're

29:29.840 --> 29:37.280
going to find weird toy problems you know language that's focused on econometrics and public health

29:37.280 --> 29:44.160
you know a lot of talk about problem specific variations of linear models and stats that's

29:44.160 --> 29:47.920
are probably a lot different from the kinds of problems that you're working on it's very hard

29:47.920 --> 29:53.920
to penetrate so the course does what the course does is explain or the courses rather they explain

29:53.920 --> 30:01.360
at a high level in the language and in the context of machine learning what these well what what

30:01.360 --> 30:06.960
causal inference is about what causal modeling is about what counterfactual inference is all about

30:06.960 --> 30:12.560
and how to algorithmize it and if you're like say for example you're familiar with things like

30:13.280 --> 30:17.680
topic models other latent variable models Bayesian non-parametric models

30:19.200 --> 30:24.960
and you're in you're comfortable working in a deep probabilistic program main language like

30:24.960 --> 30:31.760
pyro it's I think it's a great choice are those things prerequisites because I imagine there are

30:31.760 --> 30:38.640
a lot of folks out there that are interested in causality want to be able to engage in conversations

30:38.640 --> 30:44.240
about it or understand the conversations that are happening about it but don't know anything about

30:44.240 --> 30:49.520
probabilistic programming or any of the other things that you mentioned I would say that

30:50.000 --> 30:53.680
don't are not prerequisites if you have a background there you're going to be able to take off

30:53.680 --> 31:02.320
right away putting probabilistic programming aside it's it's more focused on the idea of model-based

31:02.320 --> 31:07.680
machine learning which is that which is saying that to tackle machine learning problem you should

31:07.680 --> 31:17.120
be thinking very very precisely about your data generating process and then write a create a

31:17.120 --> 31:23.520
bespoke model tailored to that process and you know and since I would argue that for a human being

31:23.520 --> 31:29.040
it's impossible to think of a process that generated the data without you thinking about it in causal

31:29.040 --> 31:37.840
terms it really just that way of thinking models sorry maps directly to the causal modeling and so

31:38.480 --> 31:43.200
probabilistic programming is just a really way really easy way of going about that if you don't

31:43.200 --> 31:50.240
have experienced probabilistic programming so none of the examples are the types of you know large

31:50.240 --> 31:55.840
deep models that take all data train and are very difficult to debug they're all very easy to

31:55.840 --> 32:01.280
sink your teeth you to while at the same time and they're all very kind of contextualized within

32:01.280 --> 32:05.360
the kinds of machine learning problems that's somebody with a machine learning background what

32:05.360 --> 32:10.560
I've used to but at the same time there it's clear how they can be generalized to

32:10.560 --> 32:18.240
more deeper and more nuanced more complex models you made a point in there about the

32:19.040 --> 32:25.120
causality I believe or I don't recall if you were precisely referring to causality or

32:25.120 --> 32:33.120
probability probabilistic programming but the idea that in thinking about a model you're thinking

32:33.120 --> 32:40.960
about the the way that the data is generated can you elaborate on that point a little bit?

32:41.600 --> 32:48.640
Yeah so it's it's a philosophy that is commonly articulated as model-based machine learning

32:49.360 --> 32:55.920
and so yeah the typical machine learning playbook is that you you get the data sets you look at

32:55.920 --> 33:00.800
the structure the features you look at the structure of the race of the target response variable

33:00.800 --> 33:06.320
you start transforming things to fit whatever modeling framework you are familiar with or whatever

33:06.320 --> 33:13.360
happens to be fashionable at the time maybe you go on to you go and pick the most fashionable model

33:13.360 --> 33:19.200
and you kind of switch out the eminist data for whatever your data is and just assume it's going

33:19.200 --> 33:24.560
to work and then you kind of iterate on it you tweak things until you can ramp up predictive

33:24.560 --> 33:31.040
accuracy this the model-based machine learning approach is is it's saying that it's taking a

33:31.040 --> 33:36.800
different philosophy for building a model you're saying that rather than being a hammer and search

33:36.800 --> 33:42.240
of a nail you should say all right let me let me think about the process that generated this data

33:43.200 --> 33:52.080
let me build a bespoke model with respect to this data and and and and try and then let me try

33:52.080 --> 33:57.760
to you know evaluate and critique this model in terms of how well how faithful I think it is to

33:58.560 --> 34:02.960
the original data generating process of course you can still have a loss function that optimizes

34:02.960 --> 34:10.560
for prediction now this relates to causality because if you're thinking about a process that generated

34:10.560 --> 34:16.480
data typically the way you think about it is as a causal process so first this happened and that

34:16.480 --> 34:22.000
that happened and then maybe some noise was added and then it was translated all of these things are

34:22.000 --> 34:29.360
you're thinking about a kind of causal narrative for a kind of causal origin story for your data

34:29.360 --> 34:35.440
and so so in this course we build on that model-based machine learning intuition that that

34:35.440 --> 34:46.080
generated modeling intuition what is it that excites you about causality is it the you know the use cases is it about the

34:46.880 --> 34:51.360
kind of the elegance of the proofs like what is it that really gets you going about this topic

34:52.240 --> 34:59.680
you know I think for me the cool thing is to be able to write really the right programs

34:59.680 --> 35:07.680
to write explicitly in terms of like a program what you think the actual processes that generated

35:07.680 --> 35:13.600
the data and and we have a lot of cool background research from all kinds of different domains

35:13.600 --> 35:19.120
that do that so we have say for example simulations in like climateology or multi-agent systems we

35:19.120 --> 35:25.120
have you know differential equations you know stochastic differential equations we have stochastic

35:25.120 --> 35:31.920
processes that we use to kind of describe how something came to be right and so it turns out

35:31.920 --> 35:37.280
that we can actually implement these as causal models and then reason about them counterfactually

35:37.280 --> 35:43.120
so what a counterfactual means is that you know I you know specifically a kind of counterfactual

35:43.120 --> 35:47.120
we call it twin world counterfactual we are thinking about parallel universes you're saying that like

35:47.840 --> 35:54.240
I did this thing and here's where I am so I ate this hamburger and now I have a stomach ache

35:54.240 --> 35:58.720
had I not eaten this hamburger I would not I would not have a stomach ache and that's something

35:58.720 --> 36:04.560
that's been very challenging for machine learning because that's that's example where

36:04.560 --> 36:09.600
that data point where you did not eat this the the hamburger and didn't get the stomach ache

36:09.600 --> 36:14.480
that doesn't exist in a training data right so you need some kind of way of simulating the

36:15.200 --> 36:19.600
alternate reality for doing that and so you know for me personally just kind of

36:19.600 --> 36:27.440
writing algorithms that that that that does that that's super fun so like I you know I was I was

36:27.440 --> 36:36.640
saying that you know it's very normal for a human being to say like oh man like had I not

36:36.640 --> 36:44.080
data that's woman in college I'd be a much happier man today that that kind of reasoning is

36:44.080 --> 36:49.440
really easy for us I think about how cool it is that you can actually write an algorithm that

36:49.440 --> 36:56.640
can reason the same way right now now I'll be honest with you I don't I've seen some some

36:56.640 --> 37:00.320
I've seen a few actual practical examples of this like in terms of

37:02.560 --> 37:07.840
within machine learning I've seen something things like a reasoning about occlusion in an image

37:09.120 --> 37:13.520
you know some of the things that we see like with deep fakes for example you can you can

37:13.520 --> 37:17.760
actually consider that a kind of counterfactual problem like what would this video look like if

37:17.760 --> 37:22.080
it were Obama talking instead of somebody else you know those are those are fun examples I don't

37:22.080 --> 37:28.320
know if they if they're really kind of the killer app I think we're still looking for that the

37:28.320 --> 37:33.040
honestly it's really fun to play with I was just getting that I think the you know what I'm

37:33.040 --> 37:40.480
hearing you say is that you know with regards to the field in general your course you're not necessarily

37:40.480 --> 37:45.840
promoting this as something that someone is going to go home and apply in their you know day job

37:45.840 --> 37:53.120
but rather it's going to position them to start to reason about the application of machine learning

37:53.120 --> 38:01.040
in some different ways that are starting to get I would say that to be clear the stuff

38:01.040 --> 38:08.080
that you need to get a job on the causal inference team at Netflix or Facebook is definitely in

38:08.080 --> 38:12.320
there that's definitely covered deeply in this course and you need to understand that stuff

38:12.320 --> 38:17.360
before you can move into some of the more cooler AI applications it's just for me is the the

38:18.240 --> 38:25.840
those cooler AI applications are kind of what drives my motivation but you know I I definitely

38:25.840 --> 38:30.080
I definitely have respect for all the people who are interested in running good experiments

38:30.640 --> 38:36.240
yeah yeah there's still a little bit of like a disconnect between you know if you if there is

38:36.240 --> 38:42.880
a causal inference job to be had at Netflix or Facebook there is some practical application or

38:42.880 --> 38:47.760
else those teams wouldn't exist right yeah yeah and those and and those teams are like I said those

38:47.760 --> 38:54.240
are typically experimentation teams kind of working on you know serving ads and trying to price

38:54.240 --> 39:00.000
correct ads correctly for an auction trying to figure out what's trailer to show you when you

39:00.000 --> 39:06.960
log into Netflix that's not it is and and and it gets kind of closer to AI when they're thinking

39:06.960 --> 39:12.240
about what they call time varying treatments what you might what a machine learning person might

39:12.240 --> 39:17.920
call a Markov decision process just trying to think of if I do these things in in sequence and

39:17.920 --> 39:23.680
have been kind of run automatically how do I do that yeah but I think there's cooler stuff

39:23.680 --> 39:32.800
than that so for those that are are listening to this and aren't aware of what it means when I say

39:32.800 --> 39:40.320
a study group one of the things we do as part of our broader twimmel meetup and community is go

39:40.320 --> 39:47.600
through online courses together we've done that for a ton of the fast AI courses as well as

39:47.600 --> 39:55.200
courses like the Stanford CS to 24 and LP course the Andrew wings deep learning that AI course

39:56.160 --> 40:03.680
and we're excited to kick off a collaboration with Robert around his causal modeling and machine

40:03.680 --> 40:10.560
learning sequence and this is particularly special because this is the first time we have an

40:10.560 --> 40:18.320
instructor of a course leading the study group with us for more information about the study group

40:19.120 --> 40:25.920
you can visit twimmelai.com slash community and join the community join the slack channel for the

40:25.920 --> 40:32.400
course and we'll post information there about the information session that again we'll hold on

40:32.400 --> 40:42.960
Saturday February 1st at 8 a.m. Robert any parting thoughts or words on we didn't get to your

40:42.960 --> 40:48.240
paper I thought we'll have to we'll have to talk about that at some other time but I think you

40:48.240 --> 40:53.120
know this you know this is certainly not the the first conversation that we'll need to have

40:53.120 --> 40:57.680
on the show about causality because again it is coming up over and over and over again

40:57.680 --> 41:04.320
and the few people that I've mentioned as I've started talking about the this collaboration

41:04.320 --> 41:08.800
you know I've gotten a lot of enthusiasm because it's one of these things that a lot of people

41:08.800 --> 41:14.560
are talking about but you know very few people really understand what it is and how to apply it

41:14.560 --> 41:20.160
so I'm definitely looking forward to getting us kicked off as are you know many other people that

41:20.160 --> 41:28.560
I've talked to again sorry any parting thoughts from your end yeah and I'd say to all of the

41:29.680 --> 41:34.400
research engineers all the research scientists data scientists out there who are maybe feeling a

41:34.400 --> 41:42.640
bit stuck on the on trying to say pursue you know certain career goals and and and and hearing about

41:42.640 --> 41:48.720
all of the hype and the trends that if you really go deep on the problems that you think are cool

41:48.720 --> 41:56.720
you really can't go wrong I I love this cosmoling stuff I love how it relates to symbolic AI I

41:56.720 --> 42:04.240
love how I replete I love the idea of creating an algorithm that can mimic how I regret dating the

42:04.240 --> 42:10.480
woman I did I dated in college I love all that stuff I love I love all that stuff about this field

42:10.480 --> 42:17.280
and you know even before it was sexy so I think I'm you know if you're interested in if you're

42:17.280 --> 42:21.600
interested in talking about it more I'd love to I'd love for you to reach out to me awesome well

42:21.600 --> 42:28.240
Robert thanks so much for taking the time to chat with us and by all means reach out to Robert

42:28.240 --> 42:35.280
R.I. about the course or Robert about his work and hopefully we'll catch you in the study group

42:35.280 --> 42:44.720
thanks Robert thanks Sam all right everyone that's our show for today for more information on

42:44.720 --> 42:52.640
today's show visit twomolai.com slash shows as always thanks so much for listening and catch you

42:52.640 --> 43:16.800
next time

