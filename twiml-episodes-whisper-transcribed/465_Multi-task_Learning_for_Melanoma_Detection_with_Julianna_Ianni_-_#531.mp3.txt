All right, everyone. I am here with Juliana Iyani.
Juliana is vice president of AI Research and Development at Prussia.
Juliana, welcome to the Twomo AI podcast. Thanks, Sam. Got to be here.
I am really looking forward to digging into this conversation.
In particular, we're going to learn a bit about some of the work that you and your team are doing
on cancer diagnosis and as well as the process of commercializing that.
Before we dig into that topic, I'd love to have you share a little bit about your background
and how you came to work in the field. Yeah, sure.
Sure. I guess I have always had an interest in being able to help people.
That's always where I wanted my career to go.
And so I pursued my undergraduate degree in biomedical engineering.
I thought that was the best way to do that.
During my undergraduate, I did several internships.
One of them was in biomedical informatics.
And I actually had kind of a bad mentorship experience during that internship.
But I ended up shaping my career positively.
So I, you know, I didn't have a lot of direction.
I ended up really kind of teaching myself machine learning principles
because of that lack of direction during that internship.
Eventually, you got assigned a fantastic new mentor and learned quite a bit during that.
But during that internship, I was learning all these things about machine learning,
which wasn't really referred to as machine learning and anything that I was reading at the time.
But, you know, all of the same ideas. And I realized that, you know, that is very powerful.
But we have kind of the most, the most data in the medical field.
It's an imaging. And so I got really, really interested in medical imaging.
And, you know, pursued that, you know, thought I need to learn, learn more about that.
And so that's how I got very interested in MRI. I took a whole bunch of classes at Vanderbilt,
my undergraduate school in MRI. And was really fascinated by everything that I went there,
wanted to pursue it. So I wanted to go to graduate school and continue research in MRI.
But actually, I haven't haven't shared this publicly with anyone yet, but I didn't get in the first time.
So I took a little bit of a detour. And I, you know, pursued kind of my second love, which was,
which was neuroscience. I did some research at a lab at Vanderbilt in
a visual attention and memory, studying the way, the way that brain works as, as you're doing
those sorts of tasks. And that curiously kind of, kind of relates a lot to what I'm doing now.
So I was really glad that I did that. Still was really interested in medical imaging. And,
went on to pursue my PhD in that the next year. I tried again, succeeded. So, yeah, so I pursued
my graduate degree in MRI. I joined Wilgerson's lab at Vanderbilt and studied kind of imagery
construction and trajectory correction and trajectory optimization for MRI for non-cartesian trajectories.
So kind of ways to make MRI faster, but still avoid having all these artifacts in your images.
And then sort of the latter half of my PhD, I was pursuing
machine learning algorithms to identify some patient tailored parameters. So for high field
imaging, so for seven to you, which is not what's usually used in hospitals, usually it's
lower field right now. But for seven to you, you can get better images, but you just have to be
a little bit more careful and tailor things to two patients. And so a lot of that...
Sorry, when you say seven to you, is that like a type of MRI?
It's like the strength of the magnetic field, sorry. Yeah, so it's a stronger magnet.
You tend to get kind of on even images, they're kind of unevenly lit, if you will,
uneven signal. And so I was I was working on designing machine learning algorithms to predict,
predict the RF-shin parameters, predict the parameters of the scanner that you need to
to tune that and make the image kind of uniformly lit. Imagine those are, you know,
those parameters were typically the domain of the radiologists, like they would have their hands on
dials trying to figure out how to get the best image for a given patient. And now you're trying
to automate that? Sort of. So it's actually it was be done by another algorithm. The issue was
that it would take a really long time. So you'd have to do a scan. So you already had the patient
in the scanner. And then you had to run this really extensive algorithm to get the parameters.
And then go plug those into the scanner. And then run the scans that you actually wanted to do.
So it's just a, you know, not a very feasible process for most patients and for bringing it kind
of to the clinic. So that was what we were working on. And then doing all this in my PhD, I
was really interested in machine learning, but seeing deep learning take off all around us,
I was I was pretty excited about the potential of that. And so really, really wanted to get in on it.
I was also kind of in my PhD, you know, realizing that what really, really drives me is seeing
something put into practice. I wasn't I wasn't getting that in academia where I was.
Some people do get that from academia, but I wasn't in the right place for that. And so I really
wanted to go into industry and be a part of a company that was that was using deep learning
from medical imaging. And in a way that it could could actually impact patient's lives in the short
term. And was Prussia already doing deep learning on images when you got there?
Yes, yes. So Prussia was very small when I joined. I think less than less than 10 people
at the time. So I got in, got in kind of early days, although the company had been around for a
few years already, I think, was founded in 2014. Now 10 people doing AI data stuff or 10 people
total the entire company. 10 people total the entire company. Okay. Yeah. Yeah.
The like the core products, you say image management like content management system for
radiology or. So pathology images are kind of different different from radiology. So usually,
you know, if you have made a suspected cancer, you'll get like an MRI or a CT scan that will be like
the first thing that happens. But later down the road, if there's still suspicion, you'll get a
biopsy. And that tissue gets sent off to a lab that processes it. They're eventually going to
slice up the tissue into really thin pieces and dead it and wax and stain it and put it on a
slide. So that, that like glass slide, like, you know, like you had in high school biology class,
like gets looked at normally under a microscope to diagnose your cancer.
But what Prosa deals with is actually digitized versions of those glass slides. So very,
very high resolution images of very, very small pieces of tissue. Tell us a bit about where deep
learning comes into the picture. So deep learning as it's kind of taken off in the field of pathology,
in the field of kind of studying this type of image. And there are whole host of potential
applications for it. What we focus on at Prosa is applications that are built in some way to make
the pathologist's lives easier to make labs more efficient. So enable more efficient assigning
of cases to different pathologists or specialists or enabling them to review their cases faster.
We also focus on deep learning systems or as we probably, probably controversially refer to them
AI systems. We also focus on systems that can improve accuracy of pathologists. So in some scenarios,
there are certain diagnoses that are a lot harder to make than others. And pathologists often
disagree on those diagnoses. And so any way that we can, that we can use AI to deliver information
to the pathologists that will improve their accuracy, that is something that we're interested
into. So that's maybe a good segue to talking about the paper that you and your team
will be presenting at the ICCV computational challenges in digital pathology workshop.
If that's not enough of a mouthful, the title of the paper is a pathology deep learning system
capable of triage, of melanoma specimens utilizing dermatopathologist consensus as ground truth.
Did I get all that? You did, you did. Sorry for that. We'll make our next title shorter.
So what's those paper all about? So this paper is about, you know, new new technology,
new AI system that we've developed that's really kind of building on our previous work. So
previously we built a system that could sort of sort and triage pathology cases before pathologists
reviewed them. It's sort of them into four different categories that were not specifically
diagnostic categories, but, you know, related to the diagnosis, kind of diagnostic groupings,
you could think of. And, you know, the reason that we wanted to do that was to kind of make
pathologists work more efficient, you know, be able to have them review certain cases earlier
in the day so they could order additional tests if they needed to earlier in the day if it was
a group of cases that could, that was more likely to need additional testing, things like that.
But that previous system didn't do something that pathologists were really, really interested in,
and that was to classify melanoma. Some melanomas is not the most common form of skin cancer,
but, you know, one of the most deadly. And so, it's also one that pathologists
disagree on a lot. So, if something, if you go in and get a biopsy of your skin and
make it sent in, and it's kind of one of the ones that looks suspicious for melanoma,
it's fairly common for pathologists to disagree on whether, whether it actually is melanoma. And
you tend to, tend to want a subspecialist to review that. So, there are, there matter pathologists is
one of the long words in our title, who are the subspecialists, who review these cases,
and not always, not always, but, you know, sometimes it's not a very obvious, not a very obvious one.
You want that subspecialist to kind of give you a second opinion, essentially, on the case.
So, it would be more efficient if we could route the cases to them first.
Okay, okay. I guess one of the virtues of a long title is that it actually says what the
paper is about. And this one, I think, does a good job of kind of raising one of the core challenges
that it sounds like you must face. If the specialists and, you know, reviewers of these pathology
images tend to disagree, that seems like that would make building datasets and, you know,
collecting ground truth particularly difficult. Yes, yes, absolutely. And I don't think,
I don't think we even realize how difficult when we first set out, set out to build this system.
I don't think the pathologists that we were working with, realized, realized exactly how much
they disagree. When we set out to build this, there's actually surprisingly not that much literature
on the matter. How have you characterized, how would you and how do you, like, have you
characterized the degree of disagreement among the pathologist looking at a given specimen?
Yeah, it's hard. So, there are, I guess, you know, a few different ways of doing that.
What we, what we did for this paper was to have three different amount of pathologists reviewing
the cases. And specifically, the ones that were melanocytic or, you know, basically the category
that, the broader category that melanoma is in. So, that ranges from benigniva, which are just kind
of your basic mole, all the way to, you know, invasive cancer. But we had pathologists review
all of those cases in three different ones and, and characterize, you know, how often,
how often did they disagree for which original diagnosis? And what we ended up doing for this paper
was to just say, okay, we're not going to use it if they didn't all agree, because very often,
they didn't all agree, especially for the ones that were kind of on the more suspicious end of
things. I think it was something like 40 to 50 percent of the time they didn't agree.
Oh, wow. So, the, so the 60 percent of the time, 50, 60 percent of the time where they do agree,
that becomes your training data set. And the outcome that they agreed on is your, your label.
Yeah. But again, from the explicit title, it sounds like you're using some kind of consensus
algorithm on, you know, real data, you know, to kind of predict the outcome.
Um, or where's consensus coming in is just that, that three out of three pathologists agree,
is basically what we're using as our ground truth. Okay. Yeah, I was, I think I was envisioning
some kind of like ensemble weird thing and like doing a consensus among your, among predictions.
Yeah. Well, you're actually, you're not far off from what, what we did end up doing with,
um, one of the models in the system. So the system, um, that actually makes these predictions,
makes these classifications is, is a hierarchy of models. Okay. And, uh, the one that is actually
classifying these, these melanocytic specimens, um, is what we ended up building for that is a
multi task model. Uh, and I think that a lot of the reason that we ended up wanting to do that
was because of this, this coordinates, because the pathologists are disagreeing so often. Um, so
we, what we wanted to do was divide these specimens into three different categories. So kind of,
you can think high intermediate and low risk and, you know, with, with melanoma being the highest
invasive melanoma being the highest and intermediate melanoma in C2 or the lesions that were
severely atypical, uh, and then low being your benign ones. Uh, but so you could do that as
like a three way classifier, right? Um, we didn't end up doing that, doing it that way because,
at least in the early days and most building this, it didn't work very well. Um,
That's a good reason we could do it that way. Yeah. Um, but I think a lot of the reason that
it didn't work while it was because of the, the disagreement. And so what we ended up doing was
building, uh, you know, a multi task classifier that just did, uh, a binary classification at a time
essentially. So it was, can you distinguish the low risk from the high risk? Can you distinguish
the low risk from the intermediate risk? And can you distinguish the intermediate risk from the
low risk? Can we chunk up this classification into smaller tasks? Um, and just do that because,
uh, our ground truth was just too noisy, uh, to do this as it's just a typical like three way
classifier. And the approach to training was kind of an end-to-end approach, you know,
they kind of multi-task trained, uh, this entire system, or did you, um, train, you know, them
independently or hierarchically or something like that, hierarchically? We did end up having to
train them independently. So it's not, not an end-to-end system. Uh, it would be cool if it was, uh,
but the system we built has, uh, has kind of several pieces. So there's, there's, there's an
and better, um, and then there's, there's kind of a hierarchy of three different models, um,
that to arrive at a final classification. Um, so it would have been, would have been a lot to build
this as an end-to-end system. Yeah. Um, probably a lot of issues that we would have run into, but not
impossible. Uh, so tell us about the, the model. What is it, you know, what are the components? I mean,
you mentioned high-level what the components are, but like, what, what are the components? And how did
you, um, how did you arrive at them? You know, were they all custom? Was there some off-the-shelf,
uh, stuff that you used? How did you ultimately arrive at an approach for this?
Yeah. So this system, like I said, was built off a, uh, a system that we, that we had previously
built. So it's a bit of an evolution, many different, many different pieces involved. Um, so one of,
one of the issues I haven't discussed yet that we've run into was that, um, some of the images have
artifacts that are kind of correlated, uh, in many cases with the, the ground truth. Um, and so we
wanted to get rid of those, um, those, namely, the major ones that we encountered were that
pathologists sometimes use penning to mark, um, mark cancer on the tissue. Yeah. Um, so that was,
that was an issue we ran into. Um, so that was kind of a whole, whole other project that we,
that we did and we had, we had a custom model built, um, to eliminate penning from our images.
And did you try to remove it or just kind of, uh, lock it out? I guess the way to, the way to talk
about this is to explain a little bit more about how our models work. Um, so first, um, first thing
we have to do because these are very, very large images, um, we have to kind of chunk them up. Um,
so we're first kind of detecting the tissue regions on the side. Um, and then we just kind of
patch those up into what we call tiles. So there's maybe like a whole bunch thousands of 128 by
128 pixel images, um, that are our tiles. Um, and so when we remove ink, we literally just drop
tiles. Yeah, I realized as I said it that kind of blocking it out wouldn't be much better than
just leaving the ink in if you fed, uh, image with blocked out ink to the downstream model.
Uh, it, yeah, probably not, but it depends. I've seen, I've seen a paper that tried to use
GANS to just, you know, generate, generate, you know, like tissue, look, yeah, in fill. Uh,
but it didn't look like it worked very well. So I don't, maybe there's a strategy there.
Yeah, to what degree was that 128 by 128 kind of a architectural hyperparameter that you played
with? Was that just where that decision come from? Sam there are so many hyperparameters and so
little time. Uh, we didn't, we didn't, we haven't played with that one yet, actually.
It may be taking a step back, you know, we've seen over the, the years kind of many approaches to
trying to detect cancer in, uh, images, um, you know, all different kinds of cancers, all different
kinds of approaches, you know, to the point where like some very, very famous AI people said,
oh, this is a solved problem. Like, you know, we're going to replace radiologists with, uh, with AI
systems. I think we've all, uh, kind of dialed back, you know, that enthusiasm a bit or rhetoric,
if you would go that far, but, um, you know, kind of give me a sense for or give us a sense for,
you know, what distinguishes this particular work from the other work out there, you know,
that is trying to do similar things. Definitely not a solved problem. I feel like, uh, we all
generally in this industry now, I can understand how complicated the, even the self-driving cars problem is,
and uh, this is at least that complicated. And that's actually, that's actually interesting,
because as much respect that I have for the complexity of this problem relative to solution,
to solving it, when I think of a self-driving car, I think of a much more complex system,
uh, with lots of moving parts. And in your mind, they're kind of on the same order in terms of
complexity, you know, elaborate, elaborate on it. Where's the complexity that's not obvious to us,
you know, when we think of problems like the one you're, you're trying to solve here?
Yeah, a great question. I mean, it wasn't obvious to me when I, when I got into this field.
Um, you know, it's like, you know, with image and that is solved, right?
Uh, what could be more complicated, but it is quite complicated. So when you think about what a
pathologist does, they're not just, not just kind of looking at an image and classifying it.
They kind of synthesize a whole bunch of things, but pathologists, when they're looking at an
image, they're not just thinking about the image, they're thinking about the patient's history
and clinical information, they're thinking about the patient's age, um, and they're thinking about,
uh, you know, the last time that they saw a patient like this or, uh, you know, all these other
inputs that they are kind of synthesizing to make a diagnosis. And then, aside from that,
the, uh, you know, the diagnosis itself can be very, very complicated for, for some of these things.
So, um, in dermatopathology, which is skin pathology, um, there are probably over 500 different
diagnoses that you can have. So there's a lot of different types of patterns that you need to be
looking for, uh, in these, in these, uh, type of images that, that we look at, um, they're very,
very high resolution. And so the pathologist is kind of first looking at them, kind of, at a high
level and just, um, you know, seeing, okay, what are the regions that I should focus my attention on?
Um, and what are the reasons I, regions I can totally ignore? Because they,
I probably, in many cases, literally can't go through a pixel by pixel. They have to decide, okay,
what can I ignore? Uh, kind of like a driver does when they're driving a car. Uh, what a,
where do I focus my attention? Uh, and then they're going in at higher power to those regions that
may be concerning, uh, based on, you know, based on all of their knowledge, um, and looking for very
particular associations between like one region of an image at another? Well, it, it sounds like the,
you know, the high level, um, summary is that the problem that you're trying to solve is not the
binary classification that we've been sold in the media. It's a much more complex and nuanced
classification problem, um, and that among other things, increases the level of complexity,
you know, on par with kind of the percept, perceptual problem that a driver, you know,
something that's trying to drive might be experiencing. But that's kind of the origin of your,
your, your comment. I think I interrupted you on a prior question and I'm trying to remember
what that was, uh, but well, what that was was like, what's different about, um, the problem that
you're trying to solve with this paper and, you know, some of the other kind of cancer detection
systems that have been publicized or that are prevalent in other fields or being worked on in
other fields. One of the big differences and other things that we're really excited about with
this work is that, um, there hasn't been, there hasn't yet been any kind of system, even like a,
you know, a different stain or a genetic test or something like that, that can identify melanoma
before a pathologist reviews a case. Uh, and the reason we're really excited about the system
being able to do that is because, uh, it means that you can send that case to the right person
very quickly rather than it having to get passed along from one person to another to, to say, like,
oh, this isn't what I should be diagnosing. Let me send it to my colleague down the hall or across
the city. Um, so we, we think we can speed up, um, the timeline for getting melanoma diagnosed with
a system, uh, which is, is pretty exciting. Uh, should bleed to, should lead to patients getting
their diagnosis faster, like, getting treatment faster ultimately.
Is it the case of other systems that are out there, like rely on some kind of
metadata or coding or something like that that comes from the pathologist in order to make
their decision? Like, good question. With, uh, skin pathology, really aren't that many AI systems
out there at all. Um, so we're, we're one of the first to really explore this and, and definitely
the first to really, uh, explore it at a, at a level where it's, um, you know, this close to the
clinic. There have been, there have been a, a number of papers kind of with a more academic that,
um, exploring the issue. Um, but the only other tests that are out there right now that can detect
melanoma are, um, our tests that are run after the pathologist has lifted the image. So it's, um,
basically, you know, they'll be looking at something that, um, looks suspicious for melanoma and
it's maybe borderline. Um, so the pathologist is not sure. Is this melanoma? Is this not, you
know, should I recommend that they get a really deep oxygen, uh, or not? Um, and so they'll send it off
to, to a lab, um, that does, like, genetic testing or does, um, an additional stain on the tissue,
um, so that they can tell a little bit more about it and, and those tests will kind of like give
you like a score, um, for the likelihood of melanoma, um, but you can't run it on just like any
skin specimen. Um, they're only intended for after the pathologist has, has looked at the case.
Talk a little bit about the, the specimen versus image issue, you know, when we read about,
you know, a system that might, you know, one of the early ones was, uh, identifying breast cancer
in a, you know, a radiological image. Um, it sounds like one of the big distinctions you're
making is that your system is looking at the specimen level. Why is that important?
So with, with pathology, uh, there are usually a couple different, um, whole-side images that belong to a,
a given specimen, um, and that specimen, by the way, might correspond to like a biopsy that has
been taken, like, of a mole. So your, your mole might, might end up being kind of a specimen,
uh, and it might end up with, uh, tissue on a couple different slides that the pathologist has to
look at. Um, and some of those, some of those slides might not even have, um, any, any cancer,
any mole on them. They might be totally clean, totally normal skin. Um, and so the pathologist,
when they're making a diagnosis, they look at, they look at all the slides that are part of this
specimen with a case. And we wanted, wanted our ASS to be able to do that too. Um, I, it was a challenge
because there's so much tissue on a single slide and not, uh, you know, not to mention multiple slides.
Um, but we're, we're excited to be able to do that, um, because it means that, uh, our, our system
is a little bit closer to, to reality, to what the pathologist is, is actually doing.
Is the model's ability to work at the specimen level kind of part of its training routine?
And I guess in contrast, what I'm imagining is that there's a simple heuristic that is,
hey, if any of these slides has cancer, then the specimen has cancer. Is that not? Yeah.
Actually true. Yeah, it's a good question. And what you just mentioned, if any of these slides
have cancer, then the specimen has cancer. That's what, that's what we've done for our previous system.
Um, so it was kind of aggregating the, the classifications at the end. Yeah. Um, but it's,
it's a, I guess it's a less accurate way of doing it. It introduces more room for, for error
in your model. And so we did, um, we did develop a way to train basically with, with all the
slides in this specimen. Um, and we, we do that basically by including all of the tiles from all
of the, all of the slides, um, in one, um, one bag under the multiple instance learning paradigm.
So what that's doing is basically, um, you know, within your network, you're kind of,
you're processing the features from, from each of the tiles, um, kind of individually. And then you
have this aggregation function. That's, um, basically kind of, um, combining, combining, um,
what the model has learned from all those tiles in it, you know, in some kind of learned weighted
fashion. Kind of continuing on this particular point is the idea that if a model makes this kind of
simple aggregated decision, the decision, um, you know, the error rate is higher, uh, because that
decision is just wrong sometimes. And, you know, the, the aggregated heuristic is just not correct,
or is it more nuanced like by doing aggregation, the training is more efficient, or the model learns
different things. And so because it learns those different things, it makes better decisions.
I think the answer is both. Um, so yeah, I think it is a more error prone process. If you have to do
that aggregation at the end, because you have the chance to like, like, you know, let's say you have
three different images in a single specimen, you, you have three different chances for your model
to make an error on one of those, and then you're going to aggregate them. Um, but the other side of
the coin is that, uh, the way we do this shouldn't prove the training process as well, um, in some
former fashion. So, um, we could do training by having a pathologist go in and label each of the
slides. So, um, I have three slides from the specimen. Um, the specimen is a melanoma specimen.
Two of the slides have melanoma and one of them, you know, is just normal skin. I could have a
pathologist label that, but it's not how, um, the medical records are, the medical records just have,
say, you know, this specimen has three slides and it's melanoma. Um, so it's like an extra
data curation step. Um, if we can't directly use that, um, you know, the other thing that we could do
is, you know, train a system with those three slides kind of independently and say they're all melanoma,
but then you're introducing noise into your system. What's the overall structure of your
model or was the architecture look like? So we talked a little bit about how, uh, you know, our first step
is kind of removing ink from the slides. Yeah. Um, after we do that, um, what we want to, what we
want to do is, um, basically create embeddings. Um, and so we actually just use an off-the-shelf
model for that. We use, um, ImageNet trained on ResNet, uh, for this, for this work. Uh, and so we,
we basically create feature embeddings for each of the tiles independently. Uh, and then we have
a hierarchy of models that actually makes the classification, um, based on those embeddings. Um,
and the first, I guess there's one model at the top of the hierarchy, um, and that is basically
distinguishing melanoma from everything else, um, or I guess melanoma and suspected melanoma
from everything else. Uh, the reason why we did that, uh, is because, uh, we wanted this system to
be highly sensitive to melanoma. And since melanoma is only about two percent of the overall cases,
um, we had quite a class of balance problem. Um, so, so that's the first step of the hierarchy.
And, and once we have kind of, we have the sort of suspected melanoma group, um, we have a subclassifier
that, um, is, uh, kind of refining that classification. Um, the, the, the, this is the multi-task model
that I talked about before that's distinguishing the high, low, and intermediate risk specimens.
Um, and then on the other side of things, um, you know, for those specimens that were, um,
basically, uh, classified as not, not at risk for melanoma, um, we, we are providing four
different classifications. So we have a separate classifier that's doing that. Um, and it's,
it's basically, uh, distinguishing, um, the two most common types of, of cancer, um,
so squamous cell carcinoma and basal cell carcinoma, uh, and then those, um, those lower risk
melanocytic specimens. So basically, you're benigny of our moles. Uh, and then we have this
wonderful group called other. And that, that is literally everything else, uh, which, um,
I was like, I think I talked about earlier is there are hundreds and hundreds of diagnosis and
in skin pathology. And so we couldn't possibly classify them all. And so we have this
catch all group. I guess it'd be, it'd be really interesting to, um, you know, understand kind of the,
uh, you know, cell images through or sent, you know, specimen images through the, the eyes of
image and that I guess is what I'm thinking of here. Yeah. Yeah. It would be really interesting.
And so you kind of put all this stuff together. You mentioned that you trained the components,
um, independently or at least the classifier independently. Is that like the features of the,
uh, that your training data is the, the feature in the embedding for, yeah. Yeah. Yeah.
Okay. The embeddings for each of the three models in the hierarchy. Yeah. Okay.
Cool. And so, you know, tell us about how it all, how it all worked and, and, you know,
what kind of results you saw and, um, you know, any challenges you ran into that kind of thing.
Yeah. Yeah. So this, this worked pretty well. We were, we were able to distinguish, uh, the melanoma
specimens from, uh, from, you know, the others pretty well. We had area under the curve of, of point
nine, three and classifying the melanocytic suspect specimens, um, and, uh, you know, reproduce that
pretty well in, in the two labs that we had for, for validation. So we were pretty happy with
this result. I'm sure I'm sure it could be improved on with, with more data even, um, the fact that we
could get, um, we could get a, a decent number of the, the melanomas kind of pulled out into this,
this, this suspect classification, uh, already means that you can prioritize those in your, in
your case lists, if you're a pathologist. So, um, you know, versus, they're just kind of like
randomly interspersed throughout your, your day. Otherwise, um, we thought this was, was, was pretty
exciting. And now you're the head of AI research and development at Procia, like, how does this
translate into something that has impacts, uh, you know, at health centers and, and with users of
your system? Yeah, yeah, that's a great question. Um, so kind of have to, have to demonstrate, uh,
that, you know, the system that we've built actually has utility in practice, um, and that you,
you kind of see similar results in practice. Um, so we've already done, uh, a couple of
deployments of this particular system at a couple, uh, academic medical centers. And you'd just kind
of, uh, a trial, uh, so our first, our first step was to see, you know, make sure that it wasn't
biasing the pathologists. So you don't want to, you don't want to impact their diagnosis in a
negative way. So we were, we were able to show that that it didn't have a, uh, any significant
impact on their diagnosis. The next step is showing that it does indeed allow us to reduce the
turnaround time for these cases. And for that, I think we need to, um, you know, we need to show
that at a, at a larger site. Um, and one that's, that's kind of actually, you know, experiencing these
challenges that, um, you know, they have, you know, multiple dermatopathologists or subspecialists
and multiple general pathologists. And, um, how do they, how do they allocate their cases in the
most ideal way that makes them the most efficient? Got it. Got it. So some, um, it beyond the research,
some promising results, kind of in practice, but, you know, work to be done to, I guess show that
it has the desired benefit at scale. Yeah, yeah, exactly. Awesome. Awesome. Well, Juliana,
thanks so much for sharing, uh, a bit about what you're up to and the, the paper. I'm sure there's
lots more interesting stuff. I'm, I'm, this last question that I asked about kind of productizing it,
um, you know, I can imagine going on for a long time, just talking about that, uh, particularly in a,
kind of mission critical, highly regulated, uh, space like healthcare. Um, yeah. But, uh, super
interesting project and, and thanks so much for sharing it. Awesome. Thanks, Sam. Got to be hearing
and glad to share with you guys. Awesome. Thank you.
