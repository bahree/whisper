Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting
people, doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
If you're a Twimble listener, you probably have an opinion about AI.
Are you looking forward to the role AI plays in your life?
Are you fearful of the changes AI will bring?
Or maybe you're just skeptical and don't think AI will make a real difference in the near
future.
Whatever the case, we want to hear your take.
So hit pause right now and jump on over to twimblei.com slash my AI to let us know what you think.
Sharing your thoughts takes two minutes and qualifies you to win some great prizes.
Before we proceed, I want to give a quick shout out to everyone who has submitted a video
so far, so here's to you, Krishnan, Amara, Keith, Shriram, Sharon, Rob, Julian and Chandana.
In today's episode, I'm joined by Roland Mimi Shavitch, co-founder, CEO and chief scientist
at 20 billion neurons.
Roland joined me at the rework deep learning summit in Montreal to discuss the work his
company is doing to train deep neural networks to understand physical actions.
In our conversation, we dig into video analysis and understanding, including how data-rich
video can help us develop what Roland calls comparative understanding or AI common sense.
We touch on the implications of AI systems having comparative understanding and how Roland
and his team are addressing problems like getting properly labeled training data.
All right.
Let's go.
All right, everyone.
I am here at the rework deep learning conference in Montreal and I am with Roland Mimi Shavitch.
Roland is a founder and chief scientist at 20 billion neurons, a company that's based
in Toronto and Berlin and is doing some pretty interesting things.
Roland, welcome to this week in machine learning and AI.
Well, thanks very much for having me.
It's great to meet you.
Why don't we get started by having you tell us a little bit about your background and
how you got interested in the field?
I was interested, I have been interested in AI throughout my career over the last, I would
say almost 20 years.
I was a student in Germany, I'm based from Germany and the neural network research there as
a master student.
And then since the space was fairly sparse in terms of supervision and so on at that
time I decided to go to Toronto to work with Jeff Hinton as an advisor to do a PhD in
2003.
Not a bad choice.
That's true.
Yes.
And it has been a fantastic time and, unsurprisingly, I learned a lot and enjoyed my time very much
in Toronto.
I also like the city incidentally very much and I'm back now in Toronto, partly for that
reason.
And so I spent a few years there doing research on neural networks with a very strong interest
in video understanding specifically.
And like in all the sub areas in deep learning and AI, in video understanding we also experienced
some nice advances and you know a cute, interesting results here and there but not the amazing
breakthrough that we all felt had to be imminent somehow but just never really happened.
And so then I moved around a little bit, I was an assistant professor in Germany for
some time and then at the University of Montreal for the last couple of years.
And then image that happened along the way in 2012 and we realized that the same thing
has to happen for video because there was just no fundamental reason why it wouldn't.
And so we started myself and some from our friends and colleagues from Germany started
to kind of think a little bit about the possibility of just launching a company that would combine
research, perspective and leadership from my side primarily and the ability to build
a company with a strong culture and so on from their side.
So all of these have been colleagues who had been in machine learning for many years
but had successfully built companies and so on.
And we felt like it was the right moment to come together, address the problems head on
by you know building the right kind of engineering environment and operation that would solve
data problems specifically and big engineering problems and see how far we can get.
And so we got together about a year and a half ago and established a lab in Toronto where
there's still a lot of talent in Canada specifically as well as a lab in Berlin where my friends
from there are based.
The company is called 20 billion neurons and we have completely focused on video analysis
and video understanding and have met quite some progress there that I'd be happy to chat
about more in a few moments.
So as part of this I stepped away from my role as an assistant professor at University
of Montreal to dedicate my work full time to this effort because I feel like we are really
really onto something and it takes basically all of your energy to push that through obviously.
Right.
Right.
So you started out with this kind of idea for a grand challenge accumulating a video
data set analogous to ImageNet.
Right.
ImageNet is a huge undertaking you know just for images I can hardly imagine something
of the same scale and impact on the video side what must be involved in doing that like
where are we.
Yeah that's a very good point and it's actually the analogy is not perfect because videos
are a different piece than images.
On various levels obviously the data amount is just ridiculously large back on various
images because there's time and you have like a factor 100 just like there that's going
to increase the amount of pixels that you have to look at basically but then also what
video is really representing and what it's what it's about videos are not about showing
an object you know and the task usually associated with video is not figuring out that there
is an orange in the field of view or a dog or a human or something but it's much more
about verbs, subtle interactions the way people behave in it, relationships between objects
over time how they change and so on and so forth.
So video is really quite a different thing in its own it's own that is though at the
same time a challenge but also an amazing opportunity.
The one of the reasons I would say that actually the reason why I've been drawn to video throughout
my career has been that video can teach you a lot more than images can and then text can
incidentally.
If you have a system that's able to understand some subtle aspects of video and can make
some subtle distinctions like for example understand the difference between putting an object
on top of another object versus next to another object versus behind another object just
simple interactions like that.
If you have a system that can do that then you essentially have a system that has comments
what you would call common sense which humans have you know you have a very natural intuitive
understanding of the world you know unlike incidentally an image net train network that
an object is not just a texture certain distribution of colors in your field of view but it is actually
a thing that has a special extent that is surrounded by air that could be moved from one
place to the other that has a weight associated with it that is subject to gravity that can
behave in certain ways but not others that can teleport for example from one place to
the other.
And all of these things are totally natural to humans humans acquire a lot of that knowledge
throughout the first years in their life and AI systems so far had none of that.
And the interesting part about video is that to the degree that we are able to get systems
to understand video concepts we enable them to get some of those concepts and that means
really equipping AI systems with a certain degree of common sense and that's what's really
exciting in what video and that is the reason why we have been kind of pushing along that
agenda and I have been interested in that for 15 years or so.
Has anyone tried to scope common sense like how much of I don't even know what the unit
would be you know it's not rules it's not necessarily layers or neurons or memory or
storage has anyone in any kind of way tried to get a handle on what the magnitude of what
a computer would need to learn to have common sense or we you know that kind of the you
know the hundred million dollar question kind of the touring test kind of scenario.
There are a few discussions obviously and there are many probably very technical discussions
in the psychology literature. There's one effort school of thought that surrounds common
sense and drives it much further that I completely subscribe to and that has been one of the
main reasons why I've been interested in getting at common sense via video.
That is a school of thought around people like Douglas Hofstadter and George Lekov who
say that our cognitive capabilities are structured around metaphors basically so whenever
you recognize an object or whenever you have some concept in your head of of something
like concept of an argument you do not understand that concept per se but you understand it
by comparison to other things and so one example is that that came from from Lekov back
then is that an argument is like a war basically when people have an argument you think of
it as like a fight between those people or something like that and so specifically people
like Douglas Hofstadter who I have always been a big fan of throughout the years has been
driving this completely to the ultimate end point and he said basically that anything
you do at any point in time is metaphors drawing analogies between different things like
I mean whenever you recognize an elevator as an elevator that you have to walk to you
basically making an analogy at that point and when you understand very subtle complex
concepts like a CEO saying this company will weather the storm you again make an analogy
and all of the cognitive processes that drive human understanding and thinking are just
made of the same stuff which is analogy making and so common sense is kind of on the lowest
level of that but there's a school of thought that says that everything that humans can
do and that makes human thinking amazing is based on the same stuff which is just expan
an expanjan away from from common sense which is focused on very very low level aspects
of the world like what objects are how they move around and so on and so forth.
So what are the implications of that school of thought on machine learning and AI?
I think the implications could be immense and there will be immense to the degree that
we will be able to make them tangible make them actual technologies that actually work
and with what we've been doing on you know understanding how objects relate to each
other what happens to them in a video it's just stretching the surface it's like basically
go getting at the lowest level but there has been a lot of drive towards concepts ideas
like grounding for example so enabling a system that does translation from one language
to another to not just associate semantic patterns with with the word embedding that it
uses but to associate actually something that's connected to a sensor and it's I think
it's quite likely that we will see more and more of that in the future such that a system
that outputs a sentence like there's a cup on the table not only has in its mind if
you want the syntactic structure in the cup the words relate to another like as it learned
from like large large text copper but also has an association of a cup you know some
wake description which is just represented in the feature activation at that moment where
the word is issued of a cup on a on a table and it can distinguish that from a cup under
the table or something just by means of like visual associations I think this is going
to happen more and more of a time are there any particular places or labs that are working
in this area that are worth taking a look at or papers I haven't seen much and I think
one of the reasons is that it is so fundamentally linked to video understanding that that
we haven't seen much of that video understanding positive this own kind of challenges because
or as I just mentioned it because of the really involved technological developments you need
like the lots of engineering you need and the data operation to get all the data and
so on because of that there is a barrier obviously and people have been hesitant to go down
that route there is some interesting work in like common sense reasoning and so on coming
from Josh Tannenbaum from MIT and there have been some efforts happening in the deep learning
community over the last couple of years which didn't fly though like a lot of us have tried
to build systems that can predict the future of a video in order to understand something
about the world that's called unsupervised learning taken and frames and you try to predict
what's going to happen in frames n plus 1 to n plus exactly and exactly and obviously
it is sufficient if you have a system that can do that you could argue that that system
really gets the world you know it really knows what options is otherwise how would it possibly
be able to render the future of a video unfortunately even though it's it's a proof it would
be a proof that a system is basically intelligent it's not feasible so far nobody has been
able to predict video sensibly for up to more than maybe a second into the future and even
then it's not really great and this is a fundamental problem that we all have been facing over
many years when I referred back to the struggles we had a few years ago before image net
and so on we thought that unsupervised learning is going to fly.
It was learning basically means that you take data and you try a system to just represent
that data and then get trained by reconstructing the data by basically inventing images and
videos and so on.
That never flew until now it's not flying.
I have lost phase in that agenda quite to some degree.
Now a lot of that work with the trying to predict the video was using like things like
variational auto encoders and all that.
Well there are many many different techniques.
We had used RNNs back in the day to try to predict synthetic neural videos and that all
worked fairly well and so on and it was nice for the first second.
Yeah for a second or actually for synthetic videos you can go further than that you can
predict like very very very long stretches of video because the structure and the synthetic
data naturally is not that complicated not the real world.
For example there was like a toy data set that showed balls bouncing around in a box.
So very very simple physical simulation without much physics and at some point we became
able to predict that very well and in the grander videos of balls bouncing around in a box
very well but it hasn't gotten us anywhere.
We haven't as a result of that build systems that really understand something about the world.
And I think ImageNet has been a great example of how far supervised learning can actually
push you because the main.
The most interesting part about everything around ImageNet is that it's able to generate
features that people then can use to solve other tasks which are not ImageNet by just
freezing the features that you learned and using like what they call penultimate layer
feature transfer learning or something like that.
And that the fact that this works so amazingly well I think gives us such strong evidence that
there is something to supervised learning and actual tasks that humans have to solve that
it was going down that route and you know pushing the unsupervised agenda to the site
for at least some time.
And so how are you taking on this problem at 20 billion rounds?
So the challenge then is obviously how to get your hands on data that is labeled data
that is video accompanied by a label.
And that is a big challenge in itself because as I just mentioned images can be labeled
by attaching nouns to what's the central object in the image and so on.
For videos it's not quite possible specifically if you want videos as a way to generate some
kind of common sense it's just not going to well you're not going to be able to do that
by just labeling nouns in a video.
What you need is videos that show very subtle concepts people doing things with objects
and interacting and so on.
And the standard way of gathering data just doesn't work in that case either because if
you want to have a video where a person puts an object on top of another object and then
on next to another object so that you can have these two classes.
Good luck if you want to try to find those videos in YouTube.
You're going to have to watch through hundreds of hours of YouTube video until you find
this exact actual happening that you can attach a label to.
So what we do at 20 billion is to have crowd workers not label videos for us but film videos
for us and we build a platform where crowd workers connect and they use their webcams
to do all kinds of stuff in their homes usually that we tell them to.
And what we tell them to do are examples that I just mentioned and many many others behaving
in a certain way do certain things like hide an object behind another cover an object
up and uncover it or push it around or push it so that it falls off the table and so
on and so forth.
And that way we have been able to gather a very, very large amount of data that we now
use to train our networks supervised.
And the name of the game of course is to use the supervised learning not just as a goal
in itself but to cash in on transfer learning there as well.
The idea being that once you have a system that understands what behind and in front
and all of these things are that it can generalize much better to new use cases for which you
have much less data.
And so you've kind of inverted the problem you're having them create the data from the
label.
Exactly.
As opposed to the opposite.
Just going back to the premise though is the idea that with I'm assuming the idea
with is that with you know just going to YouTube videos and like grabbing a video and starting
to label it the your label spaces just too large is just you have to control for the kinds
of things you want to label for any given experiment.
Exactly.
Exactly.
And as opposed to images there are just so many labels that you want to have that are
not just many even but also a compositional.
So it makes much more sense in a video context to have labels that are descriptions like
whole sentences that tell you what's happening in a video clip rather than a single noun.
So videos contain verbs not just nouns and they contain other word classes.
And the labels in our operation take the form of captions typically.
So we tell them because I act out the following concept.
Something like pushing an object over the edge of the table.
So it falls down or something like that.
And so that means the label there right there is structured and it has some similarity
to other labels such as push an object or pushing an object towards the edge of the table
but not so far that it falls down or something like that.
So labels are complex objects in themselves and we don't see any other way of getting
at label data of the type other than inverting this process.
And starting with the label generating them on our site and then just filming what they
represent.
Have there been any efforts to start with a movie for example and just label every scene
or at kind of arbitrary granularities of time label what's happening in a scene?
Yes, there have been a lot of efforts on generating data for video over many years.
As video understanding has been there, people have been trying my included.
But they were usually of that type as you just mentioned.
They're usually involved going through video and labeling video, existing video.
One of the outcomes of that has been that a lot of the video data sets that are available
are a bit funky as training material for networks.
There are data sets out there that are very common and commonly used in the community that
for example contain different sports activities from broadcast TV basically.
And then the task of the networks is to distinguish soccer from basketball or baseball
or something like that.
And so what's problematic with these kind of efforts and data sets is that in many cases
just a single image from the video just reveals what you're looking at.
I can tell you that it's soccer just because I see there's like a green...
Sunderbar, something like that, exactly.
And specifically they don't involve really getting into the nitty-gritty details of the
physics of the scene.
They don't ask a network to really attend to certain objects in the scene and look whether
they are behind another or in front or if they're falling or not falling.
If they are maybe made of liquids or solid or if they're made from cloth or all of these
really, really subtle physical aspects, that video is so great at revealing they are not
basically asked for in these kinds of tasks.
That has been a problem, my opinion, with the way that the video of understanding community
has been rolling over the last couple of years.
The other thing that kind of jumps out at me is if I don't know if like the stage directions
and things like that in a screenplay are rich enough to be used as training data as labels,
but it would be... I wonder if it would be interesting to take a movie and then use the stage
directions as kind of the labels.
But even then, those stage directions, there's so much that's kind of at the discretion
of the director and kind of evolves that's not explicitly specified in the stage direction.
Yes, very true.
And there have been efforts also to generate data sets exactly using that kind of information.
Yeah, using scripts and accompanying movies and making that the source for the labeling
basically.
Still not good enough to convince you that going the other way isn't better.
It's absolutely not because the stage directions, as you just mentioned, are really, really
high level and corporal.
The stage direction is not going to be take a cup, pour slightly next to the glass so
that you spill a little bit or something like that, which is what we want the network
to be understand.
It goes back to this issue that there's so much context that's understood by us as humans
that we don't...
Right, exactly.
Exactly.
We humans already have all that common sense understanding, so the script writer, or whoever,
is going to assume that information is already in the head of the director or something.
They don't need to spell everything out in detail.
Right.
I think to the degree that we advanced on this agenda, and networks get better at getting
this floor right now that we're looking at of basic understanding of physics and so on,
does it agree that we make direction there and it starts to get to work better?
I could see how scripted data from scripts is going to be useful, but right now the foundation
is missing.
We're asking the system to understand something really complex, highly cultural and social
and so on, without even understanding the world in which we're living, and that seems
a bit bizarre to me.
So what have you used this method to accomplish so far?
So we had a breakthrough a few weeks ago, like around three weeks ago, where we trained
a network on a very large amount of data that we've gathered so far, just for the fun
of it.
We have data across many use cases, obviously, but we just thought, okay, let's see how far
we are right now, and much sooner than expected, we saw that a lot of this subtle distinctions
that we were asking the networks to do actually already happened.
And what kinds of distinctions?
Lots of the physical things that I just mentioned, you know, a person throwing an object
in the air and catching it versus throwing an object in the air and it falls on the floor
and it's like that.
And it's still early times, we basically just have the first push through, we basically
saw, okay, there is like a non-rendered performance, the networks make like clearly non-rendered
performance, correct decisions there, which far has been very, very exciting, because
these are incredibly hard tasks.
And so now we have live demos and so on that show how, you know, you can just take the
camera, put it up, and then just do things in front of it, and it'll understand, constantly
describe what you're doing in real time, and get a lot of things that are very subtle.
And it has been very exciting, it has happened just now, basically.
Nice.
Yeah, yeah.
How do you plan to apply it?
So as I mentioned, we're gathering data driven by two considerations.
One being we want to sample the space of things that can happen in video as densely as possible
to catching on transfer learning and so on, but then we also have interactions with prospective
customers and existing customers on a very specific use cases that they have.
And so the data set fills up over time driven by these two considerations, basically.
And so the specific use cases that for which we have gathered data and for which we've
built models and so on are things like human computer interaction.
So for example, we've built one of the first accurate RGB gesture recognition systems that
have ever been around.
So people are sitting in front of the screen doing certain movements with their hands signalling
to the system that they want to turn down the volume or whatever.
You need to talk to this company, I forget the name of the company, but I was at the
Gartner conference last week, it's all a blur at this point, but one of the companies
that I talked to is the company that basically it was founded by the guy that did all of
the gesture stuff for the movie minority report.
Okay.
They commercialized that into a company and so you can have all these screens and you're
kind of dragging and flinging content around on these screens and you're doing it with
this remote and I'm like, yeah.
And they have a conference, conference room set up and there's a camera right there and
I'm like, there's a camera right there, why am I using this clumsy remote to move these
windows around?
Yes.
I can do that with a mouse on a desktop.
This makes no sense.
He's like, well, you know, there'll be a lot of training that would have to happen in
order to get users to make these gestures and like, yeah, I don't buy that.
You need to figure out how to do the gestures and it sounds like you.
Incidentally, we just figured out how to do the gesture.
We have live demos and they just work and we have agreements with customers who want
to use that and so on.
It basically just works.
Just showing once more that there is something to the image and that findings, you know,
if you gather enough data, you can make things work very nicely.
And it's true, there has been a lot of efforts around extra devices that you hold in your
hand or even, even if you get rid of those, like the connect and so on, like the devices
where you have a camera sensor, which is not an RGB sensor, but a depth sensor.
And so it sees something about the depth profile in the scene in order to do its job.
And there has been, you know, almost religious belief in the gesture recognition community
that that's what you need.
You need a funky sensor, otherwise you won't do gestures.
But, you know, if I close one, if I close one of my eyes and you do a gesture in front
of me, I can perfectly recognize it.
There's no problem whatsoever.
So why wouldn't it train neural net that just sees gestures, tons of gestures, be able
to do that as well?
We just showed that it's possible and it is perfectly possible.
So the problem is basically solved.
Yeah.
There's something definitely sticky to this idea of the camera as being kind of the universal
sensor that replaces huge swaths of other types of sensors.
We just need to be able to figure out the intelligence on the back end to allow the camera
sensor to do what we can very easily do.
Indeed.
Indeed.
That's true.
And whenever you can do something with one eye, you have a proof of concept.
It is possible.
And then you know you just have to figure out how.
Yes.
Absolutely.
Awesome.
Great.
Well, I know you did a talk yesterday.
Is there anything that you covered in your talk that we haven't managed to tease out
in our conversation so far?
I showed some demos that I can show to you via audio demos and hand gestures and white
boards are difficult for a podcast.
That is true.
Besides that, we've pretty much covered what I was talking about yesterday.
Awesome.
Great.
Well, Roland, it was great to meet you and great to chat with you.
Thank you.
Great.
Thanks so much.
All right, everyone.
That's our show for today.
For more information on Roland or any of the topics covered in this episode, head on over
to twimlai.com slash talk slash 111.
And remember to submit your thoughts for our My AI contest at twimlai.com slash My AI.
Thanks so much for listening and catch you next time.
