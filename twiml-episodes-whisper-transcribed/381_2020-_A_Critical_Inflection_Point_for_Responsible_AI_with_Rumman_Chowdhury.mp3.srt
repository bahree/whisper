1
00:00:00,000 --> 00:00:13,840
Welcome to the Twimal AI Podcast. I'm your host, Sam Charrington.

2
00:00:13,840 --> 00:00:23,600
All right, everyone. I am here with Ruman Childry. Ruman is a managing director and global

3
00:00:23,600 --> 00:00:30,560
lead of responsible AI at Accenture. Ruman, welcome to the Twimal AI Podcast. Thank you for having me,

4
00:00:30,560 --> 00:00:35,920
Sam. This has been a few years in the making and I'm glad we were able to do this. You know what?

5
00:00:35,920 --> 00:00:42,800
It only takes a global pandemic to make this conversation happen. These are the best conversations,

6
00:00:42,800 --> 00:00:49,360
I think, when I'm finally able to connect with friends and folks I know from the industry and

7
00:00:49,360 --> 00:00:54,400
you and I in particular, I think I've been trying to make this conversation happen for as you said

8
00:00:54,400 --> 00:00:58,560
a few years and it's always, oh, well, I'm going to be in Asia. I'm not in the Bay Area.

9
00:00:59,760 --> 00:01:03,920
We haven't quite been able to make it happen. So I'm super, super, super excited.

10
00:01:05,840 --> 00:01:13,440
We get this one going. Let's start out as we usually do here on the show and have you share

11
00:01:13,440 --> 00:01:20,080
a little bit about your background? You work in ethical and responsible AI, how did you come into

12
00:01:20,080 --> 00:01:27,920
that field? I do. And the answer to that is a lot of meandering. So by background, I'm a data

13
00:01:27,920 --> 00:01:32,480
scientist and a social scientist. I would officially say I make quantitative social scientists.

14
00:01:33,520 --> 00:01:40,080
I have degrees in political science, management, economics, masters and quantitative methods.

15
00:01:40,080 --> 00:01:49,440
Is that one degree or like five or six? Oh, I know folks like you.

16
00:01:54,080 --> 00:02:00,800
But I moved to Silicon Valley in 2013 to pursue a job in this like weird little field called

17
00:02:00,800 --> 00:02:06,800
data science, which I had heard about anecdotally while at my PhD program at UCSD.

18
00:02:06,800 --> 00:02:12,160
Everyone thought it was crazy. Nobody understood why I was leaving a political science PhD program

19
00:02:12,160 --> 00:02:19,360
to do some weird tech job. But here we are seven years later with data science and anything related

20
00:02:19,360 --> 00:02:26,320
to data science and AI being the only thing people talk about. So after my stint as a data scientist,

21
00:02:26,320 --> 00:02:31,200
I was actually teaching data science at a bootcamp called Metis. And that's one of such a family.

22
00:02:31,200 --> 00:02:39,120
I was doing talks on polling and the elections and in the sense of how numbers and statistics

23
00:02:39,120 --> 00:02:44,000
can be misleading because I have a background in things like survey design, polling and

24
00:02:44,000 --> 00:02:49,600
quantity of human behavior analysis. And you know, Accenture was this is about three years ago

25
00:02:49,600 --> 00:02:58,640
looking for someone to lead this weird thing called responsibility. And that's how I got this job.

26
00:02:58,640 --> 00:03:02,720
Oh, wow. And is the Metis the thing you're doing with Laura?

27
00:03:04,400 --> 00:03:07,200
No, that was something else. Okay. Okay. Cool.

28
00:03:08,720 --> 00:03:12,800
Interesting. Interesting. And so you've been at Accenture how long now?

29
00:03:13,520 --> 00:03:16,320
Three years actually hit the three year mark in early February.

30
00:03:16,800 --> 00:03:21,040
Wow. Nice. Nice. Which in the responsible air world makes me ancient?

31
00:03:21,040 --> 00:03:29,840
Absolutely ancient. And you're based in San Francisco. How have things been going for you

32
00:03:29,840 --> 00:03:34,160
with shelter in place and COVID and all that kind of stuff?

33
00:03:34,160 --> 00:03:39,360
Yeah. I mean, fortunately San Francisco had a really good response and people, you know,

34
00:03:39,360 --> 00:03:43,520
stayed at home and they more or less have been listening. I think everyone's just getting

35
00:03:43,520 --> 00:03:47,360
a little bit antsy. So I see more and more people out. Although people are still being careful.

36
00:03:47,360 --> 00:03:51,440
Fortunately, it's been pretty quiet. I live in Mission Bay, which is near the UCSF

37
00:03:51,440 --> 00:03:55,520
hospitals. And, you know, it hasn't been that bad.

38
00:03:56,320 --> 00:04:02,000
Unfortunately, I live in a really walkable neighborhood. There's parks nearby, etc. So it hasn't

39
00:04:02,000 --> 00:04:06,080
been overly unpleasant. I just think this is also the shortest, as far as the longest

40
00:04:06,080 --> 00:04:12,720
amount of time I've ever spent not flying somewhere in the last few years. So it's kind of been nice.

41
00:04:12,720 --> 00:04:19,520
Yeah, I've commented to, in fact, just earlier today, like by this time on a normal year, I'd

42
00:04:19,520 --> 00:04:26,160
have been probably to half a dozen at least conferences, you know, if not, if not a dozen.

43
00:04:26,880 --> 00:04:31,280
And you probably would have been around the world a couple of times, but I mean, it's funny

44
00:04:31,280 --> 00:04:35,600
because I have all these placeholders on my calendar and one by one, they all got dropped. But,

45
00:04:35,600 --> 00:04:45,200
you know, by now, I would have been in London twice, India and the Nordic. Actually, this week,

46
00:04:45,200 --> 00:04:49,680
I was supposed to be doing it toward different Nordic countries to visit different

47
00:04:49,680 --> 00:04:54,880
Accenture offices and client partners. And then in a month, I was supposed to be in Atlanta.

48
00:04:56,400 --> 00:05:01,280
I still have this thing in Singapore that apparently is still on the calendar for August,

49
00:05:01,280 --> 00:05:07,760
but I think they're being ambitious at this point. But, you know, it's nice to be home.

50
00:05:07,760 --> 00:05:13,760
You know, it's nice to be around my pets and organize my apartment and do all those things

51
00:05:13,760 --> 00:05:14,960
that help me talk it to do.

52
00:05:17,040 --> 00:05:27,360
Nice. So what are you up to at their Accenture, you know, in Responsible AI? What does that mean?

53
00:05:27,360 --> 00:05:32,560
What does Accenture's role in Responsible AI? And how do you help fulfill that?

54
00:05:33,200 --> 00:05:38,320
Yeah, so I have a really interesting and still unique job, although I hope that there will be

55
00:05:38,320 --> 00:05:44,640
more jobs that are like mine. My job is to actually provide and create practical client solutions

56
00:05:44,640 --> 00:05:49,360
around responsible and ethical use of artificial intelligence. And this can mean anything from

57
00:05:50,000 --> 00:05:55,520
data ethics to the unpacking of the black box. There's a lot of people call it,

58
00:05:55,520 --> 00:06:02,320
to just understandability in models, even to like the strategic organizational structure companies.

59
00:06:02,320 --> 00:06:07,840
And how do you create a government's infrastructure? And it's been really fascinating,

60
00:06:07,840 --> 00:06:13,600
because as I said, like I got this job through meandering, but what's fascinating is in this job,

61
00:06:13,600 --> 00:06:19,680
I use every degree I've ever had, every part of my brain. So, you know, while I do have to tap

62
00:06:19,680 --> 00:06:24,320
into my data science skills and think about model explainability, interpretability,

63
00:06:24,320 --> 00:06:29,360
different ways of doing data assessments, etc. I tap into my social science brain all the time.

64
00:06:29,360 --> 00:06:34,480
And I think about human behavior and human responses and how to construct something so that,

65
00:06:34,480 --> 00:06:41,120
you know, we're getting accurate data or we're creating policies that are inclusive or understandable.

66
00:06:41,120 --> 00:06:46,480
But then also I go to companies and we help them redesign their organizational infrastructure

67
00:06:46,480 --> 00:06:50,560
to create the right kind of scaffolding to enable responsible AI. And actually,

68
00:06:50,560 --> 00:06:56,480
I've been working with some folks, and we have a paper coming up quite soon. We did a workshop

69
00:06:56,480 --> 00:07:01,360
at FACT, online in this FACT store. I've been working with a researcher of mine, Bob Donna

70
00:07:01,360 --> 00:07:06,320
Marcova, Penny Wayne at Paymark, who's Spotify Labs and Jeanine Gang.

71
00:07:06,320 --> 00:07:08,880
Probably a huge fan of her yet. I haven't seen her forever.

72
00:07:09,520 --> 00:07:12,320
Because she's super busy and even pandemic.

73
00:07:13,520 --> 00:07:17,440
She's another conversation just like this. So, maybe I should reach out to her and say,

74
00:07:17,440 --> 00:07:20,960
you, you know, you're not pandemic-busier or something.

75
00:07:23,600 --> 00:07:28,720
We're sort of not pandemic-busier, but for sure, similar to me, she's on planes less. So,

76
00:07:28,720 --> 00:07:35,840
that's good. But yeah, so part of my job is the technical deployment. And there's a part of it

77
00:07:35,840 --> 00:07:40,880
that's also about the organizational structure and the strategic deployment. Because a big part

78
00:07:40,880 --> 00:07:48,480
of the public simply understanding what you're doing with AI is to improve how we communicate

79
00:07:48,480 --> 00:07:52,720
what this technology can and cannot do. Because there's a lot of hype, there's a lot of noise,

80
00:07:52,720 --> 00:07:56,400
and frankly, there's a lot of backlash to the hype. There's even ethics hype now, frankly.

81
00:07:59,440 --> 00:08:03,120
And so, I apologize. I got excited when you said, Henry, what was the paper about?

82
00:08:03,120 --> 00:08:11,120
You're like, who paper? What? Henry yet? No, it's fine. So, the paper was really interesting,

83
00:08:11,120 --> 00:08:16,400
because we're at this phase in responsible AI or ethics where there are principles that

84
00:08:16,400 --> 00:08:20,400
there are so many principles were drowning in principles, right? Algorithm Watch keeps

85
00:08:20,400 --> 00:08:27,040
this database. I think right now they have 150 plus principles of AI and ethics and organizations

86
00:08:27,040 --> 00:08:34,880
at the OUCD down to companies like Telefonica or even Accenture. And when you say 150 principles,

87
00:08:34,880 --> 00:08:42,400
are these 150 kind of published frameworks by some group or company or 150? If you want to be

88
00:08:42,400 --> 00:08:49,440
ethical, you need to do things one through 100. I got it. No, so 150 different sets of principles,

89
00:08:49,440 --> 00:08:54,400
so different organizations that have come up. But to point, interestingly, there have been a few

90
00:08:54,400 --> 00:09:00,320
papers trying to understand what are common themes across principles. So, like a meta analysis.

91
00:09:00,320 --> 00:09:05,760
So, Anna Jobin has won the channel Floridian Josh Calls of another. And there are some themes

92
00:09:05,760 --> 00:09:10,400
that are pretty common. I can't remember them all. I'll talk my head with stuff like Malmalfezan,

93
00:09:10,400 --> 00:09:18,560
Non-Malfezans, and you know, there's like five or six common themes that Floridian Calls find.

94
00:09:18,560 --> 00:09:23,200
So, it's interesting. So, to your point, everyone's talking about it, but there are common themes

95
00:09:23,200 --> 00:09:28,960
to it in general. And the big thing now is how do we drive principles into action? How do we do

96
00:09:28,960 --> 00:09:35,360
stuff with this? And our paper was about how do you enable the right sort of organizational

97
00:09:35,360 --> 00:09:39,680
structure? So, drawing from, like I said, I use every word of my brain, drawing from like

98
00:09:39,680 --> 00:09:45,200
management literature, thinking through organizational analysis, organizational structure,

99
00:09:46,800 --> 00:09:52,080
how do we draw from those principles to understand the kinds of shifts companies need to make,

100
00:09:52,080 --> 00:09:57,760
to enable the people they've hired to institute responsible AI. So, we did

101
00:09:58,800 --> 00:10:04,240
our long surveys with 24 different people across 18 different companies. And we specifically

102
00:10:04,240 --> 00:10:10,240
focused on people like myself who are there for the application, not just the research. And,

103
00:10:10,240 --> 00:10:13,200
you know, I found a lot of really, really interesting things.

104
00:10:14,080 --> 00:10:19,120
Interesting. Yeah, in fact, the last time I reached out to Henriette was an advance of

105
00:10:19,120 --> 00:10:29,680
our conference, two moles conference in the fall. And we did a panel on operationalizing AI ethics.

106
00:10:29,680 --> 00:10:34,880
So, you know, for organizations that are, you know, trying to implement AI and do it responsibly,

107
00:10:34,880 --> 00:10:41,200
you know, what are some of the things that they could do. And, you know, we got into this long

108
00:10:41,200 --> 00:10:46,880
back and forth email conversation about how it's all evolving. Yeah, it's, and that's actually what

109
00:10:46,880 --> 00:10:55,040
the paper is about. It's about how we're in this critical inflection point where there's this,

110
00:10:55,040 --> 00:10:59,760
so there's this whole organizational organizational literature about external and internal pressures

111
00:10:59,760 --> 00:11:04,240
and organizations that enable ethical change. And right now we have a significant amount of

112
00:11:04,240 --> 00:11:08,800
external pressure. And it's all just like, so it just takes time, right, and just have to do the

113
00:11:08,800 --> 00:11:14,320
things. And it took a while for the community to evolve for people to make principles, people

114
00:11:14,320 --> 00:11:19,280
to start talking about them, you know, the media engagement, public awareness. And now it's

115
00:11:19,280 --> 00:11:23,760
reached this point where companies are getting this pressure. And we also have internal champions

116
00:11:23,760 --> 00:11:28,800
and companies trying to drive this change. So, you know, how can we utilize this internal and

117
00:11:28,800 --> 00:11:34,000
external, this external pressure to enable the internal champions to do their job and what can

118
00:11:34,000 --> 00:11:39,280
companies do to help them? But yeah, absolutely, it's all, you know, so we look at the current state

119
00:11:39,280 --> 00:11:44,480
which is kind of where things are today. The prevalent state, which is, you know,

120
00:11:44,480 --> 00:11:49,840
an immediate future where things like need to go. And then this ideal future state, like where

121
00:11:49,840 --> 00:11:54,800
would people love things to be in the future? What is this ideal state of a responsible company?

122
00:11:55,440 --> 00:12:03,680
Interesting, interesting. For a while, I would hear about organizations that we're taking this

123
00:12:03,680 --> 00:12:10,320
position that, you know, we're just not going to pursue AI because it's too ethically fraught

124
00:12:10,320 --> 00:12:15,840
and we don't know what to do. Do you see that at all? Or had you seen that? Is it still something

125
00:12:15,840 --> 00:12:22,480
that you come across? Well, I have definitely seen that sentiment when it comes to individual projects.

126
00:12:22,480 --> 00:12:27,520
I think every company is really excited about the promise of AI. And I don't think anybody is

127
00:12:27,520 --> 00:12:34,480
turning down, you know, the whole concept in general. I mean, frankly, you just will be a market leader.

128
00:12:34,480 --> 00:12:39,920
Yeah, you have a chance. However, yes, I have seen hesitation when it came to implementing,

129
00:12:39,920 --> 00:12:44,480
when it comes to implementing certain projects. And it's definitely a blocker in companies

130
00:12:44,480 --> 00:12:52,080
scaling AI. One thing we find in general as a company, as Accenture, you know, there are many

131
00:12:52,080 --> 00:12:56,240
blockers to scaling artificial intelligence. Companies are drowning in proofing concepts.

132
00:12:56,240 --> 00:13:00,960
Anybody can spin up a proofing concept based on a nice database. They find some sort of online

133
00:13:00,960 --> 00:13:05,840
data, do some fancy neural net on it. You can have data science spin something up in three weeks.

134
00:13:05,840 --> 00:13:09,840
But productionizing it, scaling it is a whole other endeavor.

135
00:13:11,440 --> 00:13:17,600
Yeah. And so, part of, you've kind of talked about all these different parts of your brain that

136
00:13:17,600 --> 00:13:28,320
come into play when trying to help organizations think through this. The most organizations

137
00:13:28,320 --> 00:13:38,800
already have the kind of pieces in place to build, you know, to build ethics into the way they

138
00:13:38,800 --> 00:13:45,600
operationalize machine learning. Like, if you're in, you know, financial services, for example,

139
00:13:45,600 --> 00:13:52,400
you know, you've had to deal with, you know, making loan decisions ethically and, you know,

140
00:13:52,400 --> 00:13:57,200
things like that. So, you probably have some kind of organizational, some governance structure

141
00:13:57,200 --> 00:14:04,000
in place. To what extent did most, you know, or how prevalent is that? And what are the things,

142
00:14:04,000 --> 00:14:09,200
even if you have that, that you need to do to make it make sense in the context of ML and AI?

143
00:14:09,760 --> 00:14:14,080
Yeah, that's a really great question. And the answer is it just varies a lot. And some of it is

144
00:14:14,080 --> 00:14:18,320
the culture of the organization or the company and some of it is just the nature of the industry

145
00:14:18,320 --> 00:14:23,920
that it's in. I would say by my rule of thumb for, you know, thinking through which companies are,

146
00:14:24,560 --> 00:14:28,400
you know, would be the most successful at enabling responsibility, at least at this point.

147
00:14:29,680 --> 00:14:35,440
One would be understanding AI output as probabilistic and not deterministic. So,

148
00:14:35,440 --> 00:14:40,640
understanding the math and statistics behind it, rather than thinking of it as a magical

149
00:14:40,640 --> 00:14:47,120
computational outcome, just helps someone in my position a lot because a lot of the bias and

150
00:14:47,120 --> 00:14:51,680
unfairness from artificial intelligence or biases from the fact that, you know, if you understand

151
00:14:51,680 --> 00:14:57,920
that the that the output is a likelihood and on a certainty, then you can appreciate bias in,

152
00:14:57,920 --> 00:15:02,160
you know, in a almost like a technical sense, the way data scientists think about bias.

153
00:15:02,960 --> 00:15:08,080
And then you can appreciate how bias can enter in a system and think of, you know, how to remove

154
00:15:08,080 --> 00:15:16,720
that bias. Second would be some sort of adoption of, you know, use of AI in your organization

155
00:15:16,720 --> 00:15:21,520
already. It's difficult if you're, again, like you think this is like some sort of magical

156
00:15:21,520 --> 00:15:27,680
technology. The third, interestingly, is either develop legal functions or regulated

157
00:15:27,680 --> 00:15:35,200
industry and not just because of external pressures like regulators, but companies that are in

158
00:15:35,200 --> 00:15:39,360
industry that are highly regulated have legal functions that are already data delivery, sort of

159
00:15:39,360 --> 00:15:45,920
building that infrastructure. So, if I were to talk to a company, you know, in a less regulated

160
00:15:45,920 --> 00:15:50,000
industry, their lawyers are usually more like contract lawyers or, you know, certain types of

161
00:15:50,000 --> 00:15:54,880
maybe liquidigation lawyers, but they haven't had to actually work with ethics and compliance

162
00:15:54,880 --> 00:16:00,480
necessarily and not necessarily in that developed with the sense. Although, I really do think

163
00:16:00,480 --> 00:16:06,000
this notion of risk functions and the value of something like a chief risk officer is going to

164
00:16:06,640 --> 00:16:11,920
change quite a bit and that role will be increasingly valuable, you know, as we adopt more

165
00:16:11,920 --> 00:16:16,400
official intelligence. So, to your point about financial services, they're my favorite customers.

166
00:16:16,400 --> 00:16:22,960
In the sense that like, I mean, they're ready to, they get it also. So, marginal amount of bias

167
00:16:22,960 --> 00:16:28,800
if because I am a statistician also by background, like, you know, I think a lot of anecdotally,

168
00:16:28,800 --> 00:16:33,360
I'll say a lot of statisticians who work at these financial services organizations got a little

169
00:16:33,360 --> 00:16:37,920
bit marginalized by the rise of data science because people did not, because again, it was sort of

170
00:16:37,920 --> 00:16:43,120
sold as this magical computer thing, not a bunch of math. And the statisticians are like,

171
00:16:43,120 --> 00:16:47,440
no, we've been doing this for a long time and I wholeheartedly agree they had been doing this for a

172
00:16:47,440 --> 00:16:51,600
long time. So, they understand models, they understand how to assess them, they understand that

173
00:16:51,600 --> 00:16:56,800
accuracy isn't the only thing to look at when you look at models, you know, in this notion of

174
00:16:56,800 --> 00:17:03,040
testing, it's all like built, they get all of it. But also, the industry itself has this culture

175
00:17:03,040 --> 00:17:09,360
of ethics that comes about close to 2008 financial crisis and this really interesting document

176
00:17:09,360 --> 00:17:14,880
called SR-117 and it was written by the Federal Reserve and on that birthday in 2011.

177
00:17:14,880 --> 00:17:20,080
And if you were to read that document, you would be like, it reads as if like someone like

178
00:17:20,080 --> 00:17:27,200
you or me wrote this today about issues with models and need for ethical use, irresponsible use

179
00:17:27,200 --> 00:17:32,160
of data and appropriate use of models. It's fascinating because they were thinking about this 10

180
00:17:32,160 --> 00:17:36,720
years ago and there's a lot you can learn and certainly artificial intelligence introduces

181
00:17:36,720 --> 00:17:42,160
new challenges, but the building blocks are there. And it's probably the most robust in financial

182
00:17:42,160 --> 00:17:50,000
services and any roles. Yeah, I think when I think about ethics and one of

183
00:17:50,000 --> 00:17:58,560
the challenges of, you know, just trying to address it, you know, organizationally, it's,

184
00:17:59,760 --> 00:18:04,480
I guess I can characterize it as like you've got this one side, it's like idealistic and

185
00:18:04,480 --> 00:18:09,520
it's another side that's kind of very practical and I'm wondering, you know, do we lose something

186
00:18:09,520 --> 00:18:15,680
when we kind of hand over this concept of ethics to lawyers and risk management people?

187
00:18:15,680 --> 00:18:21,280
Um, I think that there is an evolution of the risk function that's probably going to happen.

188
00:18:21,280 --> 00:18:28,400
And you're right. When I think about this work, I try to broaden the phrase to be about risk and

189
00:18:28,400 --> 00:18:35,360
impact. And I do agree there's a cynical take on risk, which is more about, you know, how do we,

190
00:18:35,360 --> 00:18:40,800
and it gets close to the line as possible. Yeah, or also like yeah, risk is not liability.

191
00:18:40,800 --> 00:18:48,240
Yeah, but I will say working with a lot of folks who are in this field, I think there is this

192
00:18:48,240 --> 00:18:52,880
desire. So, you know, going back to financial services, a lot of this, a lot of what exists

193
00:18:52,880 --> 00:18:58,880
today as model risk management started from this conference, this gathering called Basil on

194
00:18:58,880 --> 00:19:04,240
his Basil 1, 2, 3, like these different documents. And it's not just about like here's how you

195
00:19:04,240 --> 00:19:08,640
audit a model. It's actually about how do you make an ethical company? They thought about things like

196
00:19:08,640 --> 00:19:13,840
compensation for employees being linked to performance, which would then create incentives for

197
00:19:13,840 --> 00:19:18,960
them to either misrepresented lie about what's going on, right? And not to say it's been perfect.

198
00:19:20,080 --> 00:19:26,160
There have been some notable disasters since then. But the intent is there. Maybe this is,

199
00:19:26,720 --> 00:19:33,120
you know, a good motivator for the people who are truly dedicated. But you're right. I like to

200
00:19:33,120 --> 00:19:38,720
expand the language to move beyond risk to impact. I think impact is more proactive, I think,

201
00:19:38,720 --> 00:19:45,120
impact things more broadly. And impact also isn't focused on like how do we shift the liability

202
00:19:45,120 --> 00:19:49,520
for me to someone else? Because theoretically in a pure risk function, I can just say like well,

203
00:19:50,240 --> 00:19:56,560
my organization's risk is X. And if that person's absorbing the risk, then my risk is lowered.

204
00:19:56,560 --> 00:20:00,480
It doesn't mean that people didn't get harmed, it just means I'm not being sued, right? That's

205
00:20:00,480 --> 00:20:06,640
a very, that's a more cynical thing. But then bringing the language of impact broadens it to me

206
00:20:06,640 --> 00:20:12,320
and like, no, you are actually socially responsible to, you know, the environment at large, whether

207
00:20:12,320 --> 00:20:19,840
it's the market, whether it's society or your customers. So if you're an organization that is

208
00:20:20,560 --> 00:20:25,600
kind of down the path of exploring machine learning, maybe you have, you know, one or

209
00:20:25,600 --> 00:20:34,000
any of these prototypes that you've mentioned and you're starting to operationalize ML technically.

210
00:20:34,720 --> 00:20:40,400
And you're listening to this interview and you're like, oh, ethics, I need to, I need to get

211
00:20:40,400 --> 00:20:50,240
me some of that. You know, maybe, you know, first of all, kind of what do you do there? But also,

212
00:20:50,240 --> 00:20:56,960
you know, I think you mentioned in like thinking about these problems, you kind of, you're using

213
00:20:56,960 --> 00:21:01,040
all these different kind of parts of your brain, your management part of your brain, your social

214
00:21:01,040 --> 00:21:06,560
sciences part of your brain, your technical part of your brain. You know, when I think about,

215
00:21:06,560 --> 00:21:10,560
you know, kind of me, I've got a little bit of management part of my brain from kind of, you know,

216
00:21:10,560 --> 00:21:14,880
industry, I've got a technical part of my brain. But like when I was in school, I went to an

217
00:21:14,880 --> 00:21:21,520
engineering school, RPI, go engineers. I've took like maybe two social science classes in the

218
00:21:21,520 --> 00:21:30,240
whole time. So like, I don't have that, you know, that kind of formal training to draw on. And

219
00:21:30,240 --> 00:21:38,000
there are a lot of people in industry that don't. So how do you, you know, what is the path

220
00:21:38,000 --> 00:21:46,320
look like to start to understand, you know, maybe kind of encapsulate the room on social sciences,

221
00:21:46,320 --> 00:21:53,120
you know, brain or what have you? You can really kind of, you know, if not rigorously,

222
00:21:53,120 --> 00:21:56,640
thoughtfully, think through these kinds of issues and put a structure in place so that you can

223
00:21:56,640 --> 00:22:03,760
do it repeatedly and start to scale it. So two thoughts. One is the first talk there for

224
00:22:03,760 --> 00:22:07,440
at Accenture. I actually still give it today and it's called, what do we talk about when we talk

225
00:22:07,440 --> 00:22:13,040
about bias? And what I realized is that when data scientists were communicating with non-data

226
00:22:13,040 --> 00:22:17,200
scientists, there was this like lost and translation moment about some of the most basic terms,

227
00:22:17,200 --> 00:22:23,360
like bias is one. And to your point about, you know, going to a purely technical school where you

228
00:22:23,360 --> 00:22:28,480
didn't really take social science classes, when I was teaching at Metas, I realized that my

229
00:22:28,480 --> 00:22:34,160
students that came from pure STEM backgrounds had no concept that data could be biased because,

230
00:22:34,160 --> 00:22:42,720
you know, for them, data represented an objective truth. And I had to, and it's something that

231
00:22:42,720 --> 00:22:47,920
you sort of rationalized. You never see that argument on Twitter today. No, never happens.

232
00:22:47,920 --> 00:22:52,720
No, never. But it's interesting because it's sort of anecdotally people get it, but then,

233
00:22:52,720 --> 00:22:56,640
you know, and I understand like if you're a computer scientist or a mathematician, you've always

234
00:22:56,640 --> 00:23:02,720
optimized two A dataset, right? And explaining to them that the collection of data can be flawed

235
00:23:02,720 --> 00:23:08,240
was just like this interesting aha moment. And that that's where some of my early work on this

236
00:23:08,240 --> 00:23:12,080
stuff comes in. And it's still like, you know, I told you, both at top three, over three years

237
00:23:12,080 --> 00:23:16,960
ago at this point, I still give it today. And still people are like, oh, oh yeah, actually,

238
00:23:16,960 --> 00:23:20,480
that's true. So how, also, hat size, my dog.

239
00:23:20,480 --> 00:23:27,520
Okay. Pets are unavoidable in the pen. Pets and pets and children are unavoidable in the pandemic.

240
00:23:28,640 --> 00:23:33,120
I've had too many calls with some of this child that really ran it. It's very cute. It's always very

241
00:23:33,120 --> 00:23:39,840
cute. Just earlier today, I was watching the Microsoft Bill technical keynote and Scott Hanselman,

242
00:23:39,840 --> 00:23:49,680
do you know, Scott? He was giving a, you know, long time kind of blogger inspiration, kind of champion

243
00:23:49,680 --> 00:23:57,120
of diversity and tech, all this stuff, Scott out to shout out to Scott. But he was doing his keynote

244
00:23:57,120 --> 00:24:02,800
in front of who knows how many, you know, virtual tens of thousands of people. And his kid sneaks

245
00:24:02,800 --> 00:24:06,000
into the back of his office. I love it. I love all of it.

246
00:24:06,000 --> 00:24:15,120
He's still some toy. I love it. It's so humanizing. It's like, look, life happens. It's fine.

247
00:24:15,120 --> 00:24:19,920
We all work. We're not, like, we're not automatons. Like, you know, we have pets. We have children.

248
00:24:19,920 --> 00:24:29,840
We have lives for humans. I think it's great. But back to your question, you know, I think that there,

249
00:24:29,840 --> 00:24:36,400
so if I were, if I could reconstruct the world, I think data science, much like quantitative

250
00:24:36,400 --> 00:24:40,960
finance, frankly, should actually have an arm called critical data science that there should be

251
00:24:40,960 --> 00:24:47,840
people trained in data science fields in the art of critiquing data science models. I think it

252
00:24:47,840 --> 00:24:55,040
is way too complex, frankly, for an individual to just do as an add-on to their everyday project.

253
00:24:55,040 --> 00:24:59,440
And also, it's hard to have that level of objectivity when you're auditing your own work, right?

254
00:24:59,440 --> 00:25:04,320
Because everybody likes to think they did a good job and maybe didn't, maybe didn't. And, you know,

255
00:25:04,320 --> 00:25:08,720
there are things you would miss. So a large part of like what people are saying when they say,

256
00:25:08,720 --> 00:25:12,400
how do we institute this? A lot of folks call for things like red teams,

257
00:25:12,400 --> 00:25:16,640
drawing from the way security works. Well, in order to do that, you kind of need these,

258
00:25:16,640 --> 00:25:21,120
this is a third party of people, whether it's another organization or whether it's people with

259
00:25:21,120 --> 00:25:25,920
any organization, but they need to have this like specialized skill set of being able to like

260
00:25:25,920 --> 00:25:32,880
assess models for real business and all sorts of ways, right? Whether it's how the data was collected

261
00:25:32,880 --> 00:25:37,200
to like literally the parameters of the model and even to like where you will be implementing

262
00:25:37,200 --> 00:25:42,800
and who it will be used on, whether it is well suited for those individuals, right? So that

263
00:25:42,800 --> 00:25:47,120
would be my like if I could change the world, I would add something called critical data science

264
00:25:47,120 --> 00:25:54,400
as an actual field of study to then do the science. But for today, you know, I think that people

265
00:25:54,400 --> 00:26:00,000
sometimes forget that quantitative social sciences, scientists exist and that's literally what we've

266
00:26:00,000 --> 00:26:07,200
done, like our whole lives. And it wasn't just saying when I first moved here, like I got a lot of

267
00:26:07,200 --> 00:26:12,880
slack from not being a programmer. And I just did not, like I always like to say I'm not born of tech,

268
00:26:12,880 --> 00:26:21,040
like I was not built and created here. I was 33 when I moved here, so I wasn't like this young,

269
00:26:21,040 --> 00:26:25,280
green kid like learning about how you know places should be. And I'm like, I haven't been so

270
00:26:25,280 --> 00:26:31,680
obsessed with programming. And it's obviously a great skill to have, but for me, it was one of

271
00:26:31,680 --> 00:26:37,280
many skills. And I so I think it was more valuable than other skills and the sort of weird

272
00:26:37,280 --> 00:26:44,320
tearing of what's more or less valuable is very odd to me. And then, you know, like it was interesting

273
00:26:44,320 --> 00:26:48,400
because it's, you know, you certainly get a lot of flak with being a social science scientist

274
00:26:48,400 --> 00:26:55,040
in this world. But you know, now it's interesting because all of my quantum social science

275
00:26:55,040 --> 00:27:00,240
skills come into play. So, you know, I think one is to bring in more social scientists on your

276
00:27:00,240 --> 00:27:05,280
team, you would be amazed and surprised that our level of statistical and quantitative skill

277
00:27:05,280 --> 00:27:10,640
and abilities and our programming skills, we can do all of it. But you know, to be fair,

278
00:27:10,640 --> 00:27:15,040
there's plenty of things engineers do that I can't do, right? And, but the whole goal is to make

279
00:27:15,040 --> 00:27:21,040
this an interdisciplinary group. I actually don't think anyone here for body can do this. I certainly

280
00:27:21,040 --> 00:27:25,600
personally cannot do all of it, it's supposed to be my job kind of at a high level, right?

281
00:27:25,600 --> 00:27:30,080
The more you dig into it, the more you realize how there's a role for everyone to play.

282
00:27:30,080 --> 00:27:35,120
So, thinking through, you know, the study that I did with like a Henrietta and Dreaming and Bobby,

283
00:27:35,120 --> 00:27:39,520
that's actually the answer. It's like the entire organization is responsible and everybody has

284
00:27:39,520 --> 00:27:45,440
their job to do. And I can appreciate how it's extremely daunting task if you are a data scientist

285
00:27:45,440 --> 00:27:49,520
on a project, you know, there's always this literature with data scientists as if they're gods,

286
00:27:49,520 --> 00:27:54,000
as if like, you know, and I remember my first job as a data scientist, like, you're not a god,

287
00:27:54,560 --> 00:27:59,360
you're responding to someone else's demands. You got to meet deadlines, right? I mean,

288
00:28:00,400 --> 00:28:05,040
it actually takes a pretty brave person to say, you know, we need to put this project on hold

289
00:28:05,040 --> 00:28:11,120
because this data is wrong or incomplete or to go to your project manager and say, like, you know,

290
00:28:11,120 --> 00:28:16,480
this product is not built ethically. And we need to create the right sort of incentives and

291
00:28:16,480 --> 00:28:21,840
community structures to do so. It's very, you know, like, I suppose high-level answer to your question,

292
00:28:21,840 --> 00:28:26,080
sorry, I don't have an easy answer to it. I think it's fine that, you know, people are, I think

293
00:28:26,080 --> 00:28:31,200
it's totally fine that people are offering ethics, curricula, etc. I think it's certainly needed,

294
00:28:31,200 --> 00:28:37,440
but is it in software problem? No, it's not. And I think that's a great point. And, you know,

295
00:28:37,440 --> 00:28:42,880
one of the biggest things that's changed in data science over the past, you know, three, five or so

296
00:28:42,880 --> 00:28:49,840
years is kind of this move away from thinking of the data scientists as this kind of, you know,

297
00:28:49,840 --> 00:28:55,600
lone ninja that kind of roms the night. It was never that short. It was never that. It's

298
00:28:55,600 --> 00:29:02,880
all like a weird 10% engineer thing. Like, who is that? I don't know, like, honestly, I don't know

299
00:29:02,880 --> 00:29:06,480
data scientists who are like that. I don't know who these people are. I've certainly never worked

300
00:29:06,480 --> 00:29:10,880
with one and, you know, and if they existed, nobody ever liked them and they were actually never

301
00:29:10,880 --> 00:29:16,640
very good. I remember these pictures that you that we used to see with like the data scientists

302
00:29:16,640 --> 00:29:21,680
and like all of the skills in their backpacks. We remember what I'm talking about.

303
00:29:22,880 --> 00:29:27,840
My favorite used to be these job descriptions. And you can still find them. And it'll be like,

304
00:29:27,840 --> 00:29:37,360
you know, computer science degree PhD preferred 10 years plus engineering knowledge of like

305
00:29:37,360 --> 00:29:44,480
like all of it. And I'm like, this person doesn't exist. And like up on them, they'll also say

306
00:29:44,480 --> 00:29:50,160
something like, you know, back in 2015, I would say like 10, like six to 10 years of experience

307
00:29:50,160 --> 00:29:55,920
in data science. I'm like, that term didn't exist 10 years ago. Okay. Go back to like Yahoo. And

308
00:29:55,920 --> 00:30:01,840
go find the like OG data scientists. Maybe one of them is qualified. Yeah. Yeah. But yeah,

309
00:30:01,840 --> 00:30:07,680
but to your point, actually, at my first job, I one of the their first data science hire was

310
00:30:07,680 --> 00:30:13,040
actually somebody from Yahoo, who was one of their original data scientists. And like he certainly

311
00:30:13,040 --> 00:30:17,360
wasn't that way. He was a really, he was a great teacher, a wonderful person to work with.

312
00:30:17,360 --> 00:30:21,040
He taught me a lot about how to production wise code. That was one of the things I didn't know how

313
00:30:21,040 --> 00:30:26,240
to do. I mean, I knew how to like assess models and build it in like this closed environment,

314
00:30:26,240 --> 00:30:30,160
but how do you take it and, you know, make it ready for an engineering team, though these

315
00:30:30,160 --> 00:30:34,640
were skills I didn't have. And and such it was really helpful and like a wonderful person to work

316
00:30:34,640 --> 00:30:41,440
with. I have no idea what that weird comes from. Interesting, but I think

317
00:30:43,280 --> 00:30:49,680
by and large organizations with few exceptions have kind of moved away from that towards

318
00:30:50,480 --> 00:30:56,960
more of a team approach. StitchFix comes to mind as a counter example where they are very much,

319
00:30:56,960 --> 00:31:02,240
you know, they still look for a full stack data scientist they call them. But in general,

320
00:31:02,240 --> 00:31:08,240
you know, I tend to see more of kind of a team approach that has specialists. And I think to

321
00:31:08,240 --> 00:31:14,560
your point, you know, ethics or, you know, computational social science thinking or everyone

322
00:31:14,560 --> 00:31:19,440
I call it is a, you know, it's a complimentary skill that belongs on that team as opposed to,

323
00:31:20,560 --> 00:31:26,000
you know, we have to, you know, make everyone social science ninjas in addition to being

324
00:31:26,000 --> 00:31:30,960
technical ninjas and deployment. It's actually, it's like, so I guess the, is it one of those

325
00:31:30,960 --> 00:31:36,000
things where the field just had to mature, right? And the analogy that I always think of is like

326
00:31:36,000 --> 00:31:42,880
remember the title live master from the 90s. Like, okay, like try to tell anybody born after the

327
00:31:42,880 --> 00:31:50,800
year 2000 that once upon a time, companies would hire one person to manage their website. The whole

328
00:31:50,800 --> 00:32:00,160
thing. This included graphics design. And now it's not only is it a team, it is a team of people

329
00:32:00,160 --> 00:32:06,480
who are like graphics designers, like, you know, and pure creative folks, two engineers and programmers,

330
00:32:06,480 --> 00:32:11,600
two somebody who's just developing content and really good at writing copy and messaging,

331
00:32:11,600 --> 00:32:16,400
you know, and it's a, and I think that over time, I mean, it took a while to get there and that,

332
00:32:16,400 --> 00:32:21,360
that's where it went because it was just logical. You know, our web presence became just as important,

333
00:32:21,360 --> 00:32:25,840
if not sometimes more so than a physical presence for any given company. If you wanted to do it

334
00:32:25,840 --> 00:32:29,760
right, you had to invest in people. Guess what? With interdisciplinary skill sets and you couldn't

335
00:32:29,760 --> 00:32:35,520
just write, you know, this, this job description of someone who knew HTML, CSS, and, you know,

336
00:32:35,520 --> 00:32:41,440
you know, do you know a color palette? You know, just don't make this website and windings.

337
00:32:41,440 --> 00:32:46,320
I'm perfect. Comic Sans and Winding. Oh, man.

338
00:32:48,400 --> 00:33:01,920
Nice, nice. So there's kind of beyond kind of the ethics, you know, should we shouldn't we,

339
00:33:01,920 --> 00:33:07,760
you know, not to oversimplify ethics. I guess I'm, I want to transition to kind of explainability

340
00:33:07,760 --> 00:33:12,320
and the extent to which that comes up in your work and as a concern for the people that you

341
00:33:13,360 --> 00:33:18,480
are working with and assuming so, you know, what are you seeing as kind of the ways that folks

342
00:33:18,480 --> 00:33:25,120
are addressing it? Yeah, it's interesting. So the notions of both explainability and transparency

343
00:33:25,120 --> 00:33:30,000
come about in part because of a lot of the legislation from the EU, so General Data Protection

344
00:33:30,000 --> 00:33:34,640
Regulation. And it's been interesting to see how people interpret it. So like,

345
00:33:34,640 --> 00:33:40,320
other talk that I give is about both a notion of explainability and transparency and explain

346
00:33:40,320 --> 00:33:44,560
the idea of explaining the concept of what it means to respond is really interesting, right?

347
00:33:44,560 --> 00:33:50,480
So again, like, talking into this other part of my brain. This is where I draw on like political

348
00:33:50,480 --> 00:33:55,680
philosophy and, you know, governance and democracy, right? So, you know, we want to create all

349
00:33:55,680 --> 00:34:03,760
this governments around AI. And we want to like explain things, right? But often the explainability

350
00:34:03,760 --> 00:34:08,560
is this notion that I'm just going to tell you things and somehow you're supposed to understand

351
00:34:08,560 --> 00:34:13,360
what I'm saying. So the idea would be like this panoptic, like, teacher and electoral kind of thing.

352
00:34:13,360 --> 00:34:16,960
Like, I'm standing in front of a room. I'm putting up this lecture. If you don't get it, it's your fault

353
00:34:16,960 --> 00:34:21,840
and you got to figure it out. And the, so sometimes like our notion of explainability because we

354
00:34:21,840 --> 00:34:28,320
translate from data science to, you know, other fields, whether it's a customer service representative

355
00:34:28,320 --> 00:34:34,880
or a loan officer or a judge, right? These are other people who have no interest or skills or

356
00:34:34,880 --> 00:34:39,040
abilities in our field, same way we don't have any interest colorability in their fields, right?

357
00:34:39,040 --> 00:34:43,440
So what are we at? How are we explaining things? And, you know, as it was a more concrete

358
00:34:43,440 --> 00:34:48,640
technical example, I use is that the end user license agreement. So, you know, we've all gone

359
00:34:48,640 --> 00:34:54,480
through the update our OS and then we get this like long legalized document, which none of us read.

360
00:34:54,480 --> 00:34:59,200
And even if we sat down and read it, we would not understand it because it's written by lawyers

361
00:34:59,200 --> 00:35:05,200
or lawyers. Right. So fully explained, completely explained. That is all understood. And often

362
00:35:05,200 --> 00:35:09,920
I think about explainability, I think about the notion of understanding and how, you know, if you're

363
00:35:09,920 --> 00:35:15,440
a good teacher, you're really focused on whether or not you students have understood what you've

364
00:35:15,440 --> 00:35:20,240
said, right? And even if it means explaining it again or spending it differently or, you know,

365
00:35:21,120 --> 00:35:25,680
taking more effort to understand your audience rather than kind of just saying it the way you would

366
00:35:25,680 --> 00:35:31,680
say it. So that would be sort of the government's component of it. And even this notion of transparency,

367
00:35:31,680 --> 00:35:36,880
right? So similarly, transparency assumes that if I have a pro-transparent process or a

368
00:35:36,880 --> 00:35:43,520
transparent model, that it's great. That's it. I'm done. But then there's this assumption there

369
00:35:43,520 --> 00:35:49,360
that I can do something about it. And we're not, we're, you know, all systems are inherently

370
00:35:49,360 --> 00:35:55,040
have a power dynamic. So if a massive tech corporation just says, by the way, we've changed our

371
00:35:55,040 --> 00:36:00,880
models to be like X, what are you or I or the average person's vehicle? What would you be able to do

372
00:36:00,880 --> 00:36:05,760
nothing? Right? Absolutely nothing. And this came up quite a bit thinking about like image, facial

373
00:36:05,760 --> 00:36:11,920
recognition, image detection. One pushback I would get from people, you know, on the use of facial

374
00:36:11,920 --> 00:36:17,120
recognition in stores, et cetera, to maybe that identify a shop lifter. Then that weird behavioral,

375
00:36:17,840 --> 00:36:24,080
you know, behave, what does it quite be? A effective computing stuff. I just mean like straight up,

376
00:36:24,080 --> 00:36:29,760
like identifying someone from a video. And they would say, well, what's the difference between

377
00:36:29,760 --> 00:36:34,880
that and having a security guard? And I'm like, well, the difference is if there's a security guard

378
00:36:34,880 --> 00:36:39,120
and they unfairly targeted me, I can ask who's your, like, I want to talk to your manager,

379
00:36:39,120 --> 00:36:44,880
I can call the company, I can actually take action. But if it is a image recognition system and

380
00:36:44,880 --> 00:36:50,320
it incorrectly maps my face to somebody else who is dealing, I actually have no agency,

381
00:36:50,320 --> 00:36:54,880
I have no form of redress. So I can have full transparency. I know there was facial recognition,

382
00:36:54,880 --> 00:37:00,080
I know it was used, but I actually don't have any agency. So this critical missing part of

383
00:37:00,080 --> 00:37:05,440
transparency as an agency. But from like a technical perspective, I think that's not a really

384
00:37:05,440 --> 00:37:12,080
cool stuff going on. Like I really love, you know, a lot of the, the way adversarial models are

385
00:37:12,080 --> 00:37:17,040
being used to understand model explainability. There are a lot of the, the mimic models, right,

386
00:37:17,040 --> 00:37:21,840
where you have this like student teacher model. I think there's a lot of really good stuff going

387
00:37:21,840 --> 00:37:27,120
on there. And I'm super excited to see more of it being used in production. And like again,

388
00:37:27,120 --> 00:37:31,520
going back to this notion of cooking concept versus production, it's actually really hard to move

389
00:37:31,520 --> 00:37:37,280
some of the more advanced models into a way what big corporation or even like a medium size company

390
00:37:37,280 --> 00:37:42,480
could use it. Like they're conceptually really interesting, but you know, just like how long it

391
00:37:42,480 --> 00:37:50,160
takes to run one of those assessments is sometimes a deal breaker. To run the model or an assessment

392
00:37:50,160 --> 00:37:58,320
of the model because of its like of transparency. The complexity of an explainability model.

393
00:37:58,320 --> 00:38:05,120
So some, so for example, like some of the models on kind of factual fairness would have to

394
00:38:05,120 --> 00:38:10,480
iterate across every single potential scenario that could happen with the data in order to compute

395
00:38:11,120 --> 00:38:16,160
what would happen if someone's gender was which are male to female, right? Those things take

396
00:38:16,160 --> 00:38:23,520
a while. And maybe it works if you have like 10 variables, 15 variables, it's really hard if you

397
00:38:23,520 --> 00:38:34,640
have like 300 variables. What, you know, what other interesting things are you seeing happening in

398
00:38:35,280 --> 00:38:45,040
the field that will kind of, you know, are most likely to impact your customers, your clients.

399
00:38:45,040 --> 00:38:50,560
You know, I think there's a lot of conversations happening in the field that kind of range in their

400
00:38:50,560 --> 00:38:56,720
practicality and pragmatism. And I'm curious about the more pragmatic side of that. Yeah, me too.

401
00:38:59,680 --> 00:39:04,800
What, like, so like I said, I started the shop three years ago. I used to have a slide on

402
00:39:04,800 --> 00:39:08,400
every single one of my decks, like every single one I swear because I just got so tired of it.

403
00:39:09,040 --> 00:39:13,840
I would start every talk by saying there are three things I don't talk about. And my three

404
00:39:13,840 --> 00:39:19,280
things were Terminator Hal and Silicon Valley entrepreneurs saving the world. Because at that time,

405
00:39:19,280 --> 00:39:24,800
we had a closer view of, oh, you know, but I said every, because I swear to God, if someone sucked me

406
00:39:24,800 --> 00:39:28,800
into the Hal Conversation, it was quite into lose my mind. All right.

407
00:39:32,480 --> 00:39:38,480
And so is that, is that to say that, you know, just kind of putting, putting pause on the previous

408
00:39:38,480 --> 00:39:43,520
question, is that to say that you don't get involved in like, you know, do your customers even

409
00:39:43,520 --> 00:39:50,160
care about like AI safety kinds of questions and like paperclip maximizing kinds of questions?

410
00:39:50,160 --> 00:39:56,560
Yeah, I mean, like maybe for like intellectual curiosity, sure. But, you know,

411
00:39:57,600 --> 00:40:04,560
and paperclip reference for those that are not following is a reference to Peter Bostrom. And I'll

412
00:40:04,560 --> 00:40:14,000
we'll drop an analogy that he gives and we'll drop a link to my podcast with Peter in the show notes.

413
00:40:14,960 --> 00:40:19,600
Oh, cool. Is it a podcast? And that's really awesome. Yeah. I'm pretty sure we talked about the paperclip

414
00:40:19,600 --> 00:40:24,560
thing too. I would be surprised if you didn't. But yeah, I mean, I mean, I think yes,

415
00:40:24,560 --> 00:40:30,800
our intellectual curiosity sure. But I don't think, I think we may actually slowly be moving

416
00:40:30,800 --> 00:40:36,800
into a scarier world than we were before, particularly with some of the uses of AI and human resources,

417
00:40:36,800 --> 00:40:40,800
which is interesting because it's not just like super sexy fields and oh, yeah, you know,

418
00:40:40,800 --> 00:40:44,640
autonomous vehicles, sort of that the first field where we're actually adjusting some of these

419
00:40:44,640 --> 00:40:50,240
like more existential ethical concerns has been in HR because of the rise of effective computing

420
00:40:50,240 --> 00:40:56,480
and this concept that you can somehow measure someone's potential by their face or by their

421
00:40:56,480 --> 00:41:03,760
expressions or by their mannerisms. And also, you know, and it's you know, in retrospect,

422
00:41:03,760 --> 00:41:08,240
it's actually not surprising because in when we hire somebody, it is such a

423
00:41:09,200 --> 00:41:16,400
nebulous and difficult to quantify factor that we hire people on, right? Like, sometimes it's

424
00:41:16,400 --> 00:41:22,880
like ability, frankly, right? We like to think it's based on and even if we're being really

425
00:41:22,880 --> 00:41:27,360
vigorous about it, it's often based on potential and you may have a different interpretation

426
00:41:27,360 --> 00:41:33,120
of someone's potential than I do and then and it's it's like almost an inherently a biased process.

427
00:41:33,120 --> 00:41:39,280
So once we, but yet it is a world in which there's clearly a need for some sort of automation,

428
00:41:39,280 --> 00:41:44,240
whether or just in sheer amount of volume that companies have to deal with or, you know,

429
00:41:44,240 --> 00:41:49,920
mitigating the bias that already exists. So it's interesting and kind of a difficult problem.

430
00:41:49,920 --> 00:41:56,000
And some of the answers have brought in new, weird ethical existential problems. But in general,

431
00:42:00,400 --> 00:42:04,960
thank goodness. All right, well, and I say that because not that I'm not interested in the philosophical

432
00:42:04,960 --> 00:42:10,800
conversations, but sometimes it tracks from the it is a way of avoiding the actual problems that

433
00:42:10,800 --> 00:42:14,480
exist, right? Which are, by the way, none of these are new problems. These are problems that people

434
00:42:14,480 --> 00:42:19,440
have, you know, the first thing you realize and any of these ethical conversations about AI is that

435
00:42:19,440 --> 00:42:23,920
these are the same problems I have existed in society. They're just maybe more shumped in our

436
00:42:23,920 --> 00:42:28,880
face because they can happen faster in its scale. Right, right. I think there are,

437
00:42:31,120 --> 00:42:42,320
you know, there are, I think a range of reactions to ideas like AI and machine learning

438
00:42:42,320 --> 00:42:53,680
assisted hiring and, you know, computer vision as a kind of broad class of applications. And

439
00:42:55,440 --> 00:43:03,760
you see reactions ranging from, you know, the, you know, the technology is just a hammer. The hammer

440
00:43:03,760 --> 00:43:08,160
didn't kill anyone. It was the person that used the hammer that killed someone to, you know,

441
00:43:08,160 --> 00:43:15,920
the technology is, you know, the root of the evil and we should not use, you know, technology

442
00:43:15,920 --> 00:43:25,120
X for problem Y. Right. And I'm curious how you, you know, both how you personally kind of parse

443
00:43:25,120 --> 00:43:31,360
those kinds of arguments and also how you lead folks through a process to figure out, you know,

444
00:43:31,360 --> 00:43:36,560
what makes sense for them. This is why I really like talking to my lawyer friends. Seriously,

445
00:43:36,560 --> 00:43:41,600
I know, I've actually learned a lot from legal people, whether it's like, I thought you were about

446
00:43:41,600 --> 00:43:47,600
to run into a disclosure. No, no, no, as in like, you know, these are, these are actually questions

447
00:43:47,600 --> 00:43:52,960
that, you know, they've thought of. So the big question is always who has the liability. And in

448
00:43:52,960 --> 00:43:58,640
some sense, when someone says, oh, technology is just the hammer. They're kind of staying like

449
00:43:58,640 --> 00:44:06,480
technology is liable. Right. And I'm not liable. Right. Or yes, or the, you know, or I'm just the engineer,

450
00:44:06,480 --> 00:44:15,200
like I'm not liable, et cetera. And it is an interesting conundrum, even from like a legal

451
00:44:15,200 --> 00:44:21,280
liability perspective. Like, I think we can more or less agree that we can't hold an AI or model

452
00:44:21,280 --> 00:44:26,880
liable. So fine. Is it the data scientist? Is it the company? And if so, who at the company?

453
00:44:26,880 --> 00:44:32,320
And that's not really a solved problem. I think there was a lot of discussion around the Uber

454
00:44:32,320 --> 00:44:37,200
self driving car incident, right? Whether you're a self driving car, hit this woman on the road.

455
00:44:37,200 --> 00:44:43,440
And there was a cop potato of who's liable. So it turned out that the instruments had been tuned

456
00:44:43,440 --> 00:44:47,760
in a particular way that it didn't actually see her or didn't pick her up as like a, like a,

457
00:44:47,760 --> 00:44:51,760
like an object moving that it should avoid. If it were tuned a different way, it could have,

458
00:44:51,760 --> 00:44:56,960
by the way, so I think that's a really interesting point. I know at one point, they were talking about

459
00:44:56,960 --> 00:45:01,680
how well the woman driving, who was like the test driver should have been paying better attention.

460
00:45:02,640 --> 00:45:06,240
But you know, one can make a pretty easy argument that if you're in an autonomous vehicle,

461
00:45:06,240 --> 00:45:11,920
it's really hard to be constantly paying attention. Yeah. And just to have the level of like quick

462
00:45:11,920 --> 00:45:16,880
response and reaction that you need to, if you're, you know, you can't be on alert when you're not

463
00:45:16,880 --> 00:45:21,760
doing anything for hours at a time. So, you know, and I actually don't know where that, where that

464
00:45:21,760 --> 00:45:28,640
netted. So I'd be curious. But on, you know, on, on the other end, like just, just thinking about,

465
00:45:30,320 --> 00:45:35,600
like, you know, like this, the notion of technology being neutral, like, I, it's just, it's such a,

466
00:45:35,600 --> 00:45:41,760
again, the social science, such a silly concept. Because everything we build is in view

467
00:45:41,760 --> 00:45:46,000
with our values, just literally just by creating something, because we built it in a way just to

468
00:45:46,000 --> 00:45:51,520
solve a problem. And sometimes I like to use, you know, maybe non-value slate and examples,

469
00:45:51,520 --> 00:45:56,640
I think people get very emotionally polarized one way or the other. I'll give you a really

470
00:45:56,640 --> 00:46:02,720
good example and like how I, I thought about this myself. So I was in the Nordics in January,

471
00:46:02,720 --> 00:46:08,000
and I had to go to this event and I was using like whatever maps on my phone, right? And I go

472
00:46:08,000 --> 00:46:14,400
outside and it's like negative 30 or some insane temperature, right? And like in two minutes,

473
00:46:14,400 --> 00:46:18,720
my phone completely breaks like it dies. Like I'm watching the battery go to zero and it dies,

474
00:46:18,720 --> 00:46:22,640
no, it's going on. And then I go to the event and I'm going to and I'm like, hey, it's my

475
00:46:22,640 --> 00:46:26,800
job with my phone, I'm really sorry, I'm late, I don't know, like, oh yeah, yeah, that happens.

476
00:46:26,800 --> 00:46:32,880
Apparently, at certain, at a particular temperature and below that temperature, a smart phone,

477
00:46:32,880 --> 00:46:37,600
batteries die. And all you have to do, so they all, everybody has their own solutions for it,

478
00:46:37,600 --> 00:46:42,480
like they have little, like, they stick them in their mittens or whatever. Or, and then also,

479
00:46:42,480 --> 00:46:46,800
you, you just need to recharge it for a few minutes and it brings with battery back to full power.

480
00:46:46,800 --> 00:46:49,760
And I thought that was really interesting because I'm like, you know, that, that to me doesn't sound

481
00:46:49,760 --> 00:46:56,960
like an intractable problem. But if I were developing this technology in Cupertino, California,

482
00:46:56,960 --> 00:47:01,280
where it is never below 30 degrees, I probably wouldn't see that. I probably would never have

483
00:47:01,280 --> 00:47:05,760
had that problem as, you know, something I would adjust. And I thought it was really interesting

484
00:47:05,760 --> 00:47:10,160
that like I have a watch that knows what kind of slim stroke I'm doing, whether it's a backstroke

485
00:47:10,160 --> 00:47:17,760
or butterfly or, you know, freestyle or whatever. But my phone, the phone for a significant

486
00:47:17,760 --> 00:47:23,360
population of the world completely freezes in the temperature that's actually not very unusual

487
00:47:23,360 --> 00:47:27,600
for what they are. And, you know, all that is to say, like, everything we build has values,

488
00:47:27,600 --> 00:47:32,400
we choose to prioritize certain things over others. So it's on to say that, like, it's,

489
00:47:32,400 --> 00:47:40,000
it's fundamentally, like, not values driven. And, you know, it's, and, and yes, there are panels

490
00:47:40,000 --> 00:47:46,320
to be made with like, you know, the, the creation of the nuclear energy, et cetera. A lot of people

491
00:47:46,320 --> 00:47:52,000
use the Manhattan Project when thinking about things like liability or responsibility.

492
00:47:52,720 --> 00:47:58,160
But ultimately, you know, it is a responsibility because we do make these things, right? And the

493
00:47:58,160 --> 00:48:03,280
things that we make are taken and used by people. And yes, we can't control how everyone's

494
00:48:03,280 --> 00:48:08,880
going to use what we've built. But I do think creating a culture of responsibility is absolutely

495
00:48:08,880 --> 00:48:15,920
critical and necessary. I'm trying to think through whether that answered my question at all.

496
00:48:18,800 --> 00:48:26,720
It did not. Right. Well, you know, and I think there's an argument that, you know, you live in a

497
00:48:26,720 --> 00:48:32,400
domain to which there aren't answers to, you know, answer the way, you know, that, you know,

498
00:48:32,400 --> 00:48:39,680
there may be answers to questions that, uh, you know, engineers might. Yeah. And I think sometimes

499
00:48:39,680 --> 00:48:46,400
it's fine. Like, I think the, the act of interrogation and the act of understanding is, like,

500
00:48:46,400 --> 00:48:51,360
is actually sometimes which the thing that brings you to where you want to be. Because a lot of

501
00:48:51,360 --> 00:48:56,960
these things are like personal choices, right? Personal values. And sometimes it's hard to have

502
00:48:56,960 --> 00:49:01,680
these conversations because they do end up being values-laden. So we did this survey about a

503
00:49:01,680 --> 00:49:06,560
year ago at Accenture called the gray area survey. And rather than ask these, like, really obvious

504
00:49:06,560 --> 00:49:12,640
questions, like, should the AI kill person A or B? They were kind of something that was a lot

505
00:49:12,640 --> 00:49:18,080
more difficult to answer. So one, you know, one that I think was interesting was, you know,

506
00:49:18,080 --> 00:49:23,120
our company is responsible for not having surge pricing if there is a potential threat of a disaster.

507
00:49:23,120 --> 00:49:27,680
And that actually happened to me because I was in London and uh, it was crazy. I was like walking

508
00:49:27,680 --> 00:49:31,760
on bomb street and then all of a sudden this flood of people come running at me and it turns out

509
00:49:31,760 --> 00:49:36,000
that there, like, they, there was a scare where they thought there was somebody with a gun.

510
00:49:36,000 --> 00:49:40,640
It turned out to not be. But then none of us could leave because everyone was trying to get a car

511
00:49:40,640 --> 00:49:46,400
and uh, prices for, like, Uber's et cetera, like, absolutely insane. Or 300 pounds, right?

512
00:49:46,400 --> 00:49:51,120
Um, but then that leads to the question, like, is, is it a company's responsibility to, you know,

513
00:49:51,120 --> 00:49:55,120
make their models such that if there's a threat of a, an attack that, you know, they don't have

514
00:49:55,120 --> 00:49:59,200
search, is that unsafe? Is it unfair? I don't know if the answer is to that, right?

515
00:50:00,000 --> 00:50:04,480
Another one would be, you know, uh, an assert engine, if you search for CEO, you're largely going

516
00:50:04,480 --> 00:50:10,000
to get point men, uh, who are over the age of 40, uh, is that fair or unfair? Is it ethical or

517
00:50:10,000 --> 00:50:14,800
unethical? You could say, well, it's the ground truth. That's what it is. You know, most CEOs in

518
00:50:14,800 --> 00:50:21,440
the world are old white men named John, literally. Um, but, or, and then there's certainly an argument

519
00:50:21,440 --> 00:50:25,360
to be made that, you know, just because you're searching CEO doesn't necessarily mean you have to

520
00:50:25,360 --> 00:50:29,920
be told of bias truth. And maybe it can be more as, maybe we can show images of people who are

521
00:50:29,920 --> 00:50:35,360
CEOs who don't fit that singular mold. And again, that's a values solution that that's not,

522
00:50:35,360 --> 00:50:39,360
I think there is no right answer to some of these things. And I think part of it's actually being

523
00:50:39,360 --> 00:50:42,960
comfortable with the fact that there are no single answers to a lot of these questions.

524
00:50:42,960 --> 00:50:48,560
Yeah. Yeah. Yeah. Yeah. I think the example that was maybe floating around in the back of my

525
00:50:48,560 --> 00:50:54,480
mind was the, I forget the name of the individual, but one of the developers of yellow, which is

526
00:50:54,480 --> 00:51:01,600
a object detection library announced, uh, on Twitter. Hey, this computer is being a scary. I'm out of here.

527
00:51:02,400 --> 00:51:09,040
And it was, uh, right. It was, it was really, it was that's powerful. Yeah. I mean, the, the thing

528
00:51:09,040 --> 00:51:14,080
isn't that's the thing about like somebody creating a technology absolving themselves in

529
00:51:14,080 --> 00:51:19,280
responsibility. It sends a message of indifference going back to this notion of like power structure.

530
00:51:19,280 --> 00:51:23,520
You know, it doesn't matter if I personally mean Ramon say, I don't want to use computer vision

531
00:51:23,520 --> 00:51:27,600
technologies like, okay, you know, whatever. There's the person literally created it says,

532
00:51:27,600 --> 00:51:32,640
you know what? Like, this is not something I can, like, be behind anymore and then horrified by

533
00:51:32,640 --> 00:51:39,040
how this has been used. But it's such a powerful message to send, you know, and it's very clear

534
00:51:39,040 --> 00:51:46,320
that something needs to be done. And so, you know, with something like that as kind of background or

535
00:51:46,320 --> 00:51:54,000
context, say, how do you walk through, you know, personally, where you draw the line or, you know,

536
00:51:54,000 --> 00:52:00,960
say you're an engineer at place X, you know, is there an answer for, you know, where I'm going? You've

537
00:52:00,960 --> 00:52:08,000
had this conversation. It's just like, you know, go off on a mountain and like, you know, sit in the

538
00:52:08,000 --> 00:52:14,400
position. It's tough. But then like, actually, I had like a benefit like an existential crisis

539
00:52:14,400 --> 00:52:19,280
last year and thinking through exactly these decisions, right? You know, one can arrive at the

540
00:52:19,280 --> 00:52:24,720
conclusion that inherently all capitalism is evil, right? This notion that there's no such thing

541
00:52:24,720 --> 00:52:30,560
as an ethical company. And then I had this really great book recommended to me and it's called

542
00:52:30,560 --> 00:52:35,840
against purity and has nothing to do with ethics and AI. It actually has to do with, you know,

543
00:52:35,840 --> 00:52:45,120
movements that are about, you know, sort of like a moral good and how this sort of this idea of

544
00:52:45,120 --> 00:52:50,880
the most pure or the most good is actually detrimental to the cause. And the author actually uses

545
00:52:52,320 --> 00:52:57,520
climate change and the environmental movements to talk about how it can be harmful if we're just

546
00:52:57,520 --> 00:53:01,920
trying to like our ethics each other. And then you do see it sometimes. And I think, you know,

547
00:53:01,920 --> 00:53:07,200
this idea of like who's better than someone else because person acts works at evil company. Why,

548
00:53:07,200 --> 00:53:11,920
you know, I mean, you know, I can get fucked because I work at Accenture, right? Does that mean

549
00:53:11,920 --> 00:53:17,200
that everything I do is, you know, nullified or painted? And, you know, I don't, I would like to

550
00:53:17,200 --> 00:53:22,480
think that's not the best way to approach it because frankly, you do end up in a race to the

551
00:53:22,480 --> 00:53:27,360
bottom, right? Where everybody has to be more ethical or more good to the other person or more pure

552
00:53:27,360 --> 00:53:33,200
than the other person. And it's just not a helpful, it's not not a helpful, especially how ethical is

553
00:53:33,200 --> 00:53:38,160
it to be the ethics police that, you know, thinks they're superior at everybody? Exactly, which

554
00:53:38,160 --> 00:53:42,480
actually interestingly, like when I think about AI governance, I worry about this a lot. So again,

555
00:53:42,480 --> 00:53:48,880
like, thank you about everything going about states markets and democracy. Uh, often the way we do

556
00:53:48,880 --> 00:53:54,960
AI governance, like, and everyone's doing the same thing. You get a group of quote experts together.

557
00:53:54,960 --> 00:53:59,360
These are all like folks like me, right? And we all sit in the room and we just decide

558
00:53:59,360 --> 00:54:03,760
what ethics is. It's very strange and problematic. And if someone would have created a problem

559
00:54:03,760 --> 00:54:08,800
in that way, we would call it an authoritarian regime. We would actually not call it a democracy.

560
00:54:09,760 --> 00:54:17,120
So it's interesting that I have seen very few democratic processes being built around

561
00:54:17,120 --> 00:54:22,560
governments. And interestingly, I think corporations would be the first ones to actually do that

562
00:54:22,560 --> 00:54:29,360
if they do it right. Interesting, interesting. Well, I appreciate the subtitle of this book,

563
00:54:29,360 --> 00:54:35,280
you're recommending against purity, living ethically and compromised times. I think we can all

564
00:54:35,280 --> 00:54:43,760
relate to, yeah, yeah, or another. Um, cool. What else, what else is going on? Anything else that we

565
00:54:43,760 --> 00:54:50,080
should, uh, that we should be sure to cover or any pearls of wisdom for us, other books that we

566
00:54:50,080 --> 00:54:57,920
should be. Oh, man. I was looking at my book. Like, what's going on over here? Um, I don't know.

567
00:54:57,920 --> 00:55:03,200
What am I eating? Actually, my laptop is like propped up on this book called The Essentials of

568
00:55:03,200 --> 00:55:11,040
Risk Management, 600 pages on a financial risk management. Our pass. I thought it was a really

569
00:55:11,040 --> 00:55:17,520
interesting book like making those parallels. I tried to read and go to Esher Bach, right? I just,

570
00:55:17,520 --> 00:55:25,600
it made me go to sleep. I supposed to be one of those classics of, you know, thinking through

571
00:55:26,400 --> 00:55:31,840
computing, etc. And like, I guess I'm not smart enough for it. So, I mean, I thought it was fine.

572
00:55:31,840 --> 00:55:35,760
Like the anecdotes are kind of cool, but like, I can't read like a thousand pages of like

573
00:55:35,760 --> 00:55:43,600
disjointed anecdotes. I was kind of rough. Um, I suppose like for the end time STEM people are,

574
00:55:43,600 --> 00:55:50,320
like, we are, are collapsing social sciences. I don't know if you, uh, uh, I was like, I found

575
00:55:50,320 --> 00:55:55,360
something today where it just sort of boggles my mind where, you know, entire feet like

576
00:55:55,920 --> 00:56:02,320
the soul field can just decide like, oh, wow. Behavioral sciences, we are the ones who discovered it.

577
00:56:02,320 --> 00:56:08,000
And I'm like, yeah, social sciences. Well, as an example, there was something, I don't know,

578
00:56:08,000 --> 00:56:13,200
maybe six, six months ago or something where someone wrote this article that got a lot of

579
00:56:13,200 --> 00:56:17,120
publicity. It was like, oh, we should have a field of study that does X, Y, Z, and

580
00:56:17,120 --> 00:56:21,600
and like, oh, yeah, that exists. We've been STS. We've been doing that for.

581
00:56:22,880 --> 00:56:27,920
Yeah. I mean, without naming names, because it, you know, this person is a very nice individual,

582
00:56:28,560 --> 00:56:34,240
but I was like, was talking to somebody pretty high up at a major tech company,

583
00:56:34,240 --> 00:56:41,680
leading AI. And he had never heard of the field of HCI or STS. Never heard of it, but he was

584
00:56:41,680 --> 00:56:45,600
incredibly proud of the fact that he had just hired an ethicist to advise it. And I'm like,

585
00:56:45,600 --> 00:56:51,760
so you hired a philosopher to advise you, which you've never heard of STS or HCI.

586
00:56:53,360 --> 00:56:58,240
Cool. STS being science and technology studies in HCI human computer interaction.

587
00:56:58,800 --> 00:57:02,080
Yes. And I mean, I was like, good luck with your platform.

588
00:57:04,640 --> 00:57:10,560
And the thing is like, it's not to belittle the people who are trying, right? I think that

589
00:57:10,560 --> 00:57:15,520
there is this inherent ego about tech that I just, I found like, actually like mind boggling

590
00:57:15,520 --> 00:57:20,640
when I moved here. I just never been in a field where I don't just thought they were gods.

591
00:57:20,640 --> 00:57:25,680
I thought it was very strange. But hey, I come from the lowly social sciences. I worked in

592
00:57:25,680 --> 00:57:32,080
public policy and nonprofits before, you know, I was an economist for a minute, you know, like,

593
00:57:32,080 --> 00:57:38,240
I just never worked in a field where literally people thought that they just did everything better

594
00:57:38,240 --> 00:57:45,280
than everybody. It's amazing. As a data scientist, you know, I just, like I said, I didn't understand

595
00:57:45,280 --> 00:57:50,480
and I still don't. This idea that programming is better than everything or that technology is,

596
00:57:50,480 --> 00:57:55,920
you know, this notion of technological solutionism. And I think, you know, with the,

597
00:57:56,640 --> 00:58:00,800
it's kind of being brought to the forefront, the more we build artificial intelligence

598
00:58:00,800 --> 00:58:06,800
technologies, right? That we can't automate away human behavior and human preferences. And

599
00:58:06,800 --> 00:58:13,840
actually, I'd mention Bobby. Bobby and I have a paper on something I was working on called

600
00:58:13,840 --> 00:58:19,760
technological determinism and specifically thinking through recommendation systems and how they

601
00:58:19,760 --> 00:58:25,520
might nudge you to kind of be the same person forever. And they don't really encourage you to explore

602
00:58:25,520 --> 00:58:30,240
and expand your horizons simply because of the way they're constructed, right? A recommendation

603
00:58:30,240 --> 00:58:37,120
system takes your prior behavior, maps that, you know, who you are to an assessment of people who

604
00:58:37,120 --> 00:58:42,160
are quote, like you and gives you recommendations based on things you might like given other people

605
00:58:42,160 --> 00:58:49,280
who are profiled the same as you. That's actually kind of scary. A lot of conversation around this

606
00:58:49,280 --> 00:58:58,240
in the context of, you know, it actually, this conversation gets more prevalent kind of every

607
00:58:58,240 --> 00:59:03,440
kind of political, you know, election cycle, right? As we start talking about filter bubbles and

608
00:59:03,440 --> 00:59:09,200
things like that and monocultures and... Yeah, I mean, filter bubbles are kind of a more extreme

609
00:59:09,200 --> 00:59:16,080
example. But I'm even thinking of like, I don't know, I think a lot. I fortunately went to college

610
00:59:16,080 --> 00:59:22,480
and did all my stupid things before these ways existed of tracking and chasing God forbid,

611
00:59:22,480 --> 00:59:30,320
they're really a Facebook when I was in college for a year. But I wonder about these, you know,

612
00:59:30,320 --> 00:59:36,560
but in college, you know, back in the olden times, was an interesting way to sort of reinvent yourself,

613
00:59:36,560 --> 00:59:41,280
right? To think of yourself as just get like a fresh start. And I feel like sometimes for younger

614
00:59:41,280 --> 00:59:46,400
people, like you can't ever escape who you are and where you're from. And you can't ever be someone

615
00:59:46,400 --> 00:59:50,720
else or think of the world in a different way, right? Because like you have this weird baggage,

616
00:59:50,720 --> 00:59:56,960
just technological baggage with you forever. And I find that kind of sad. And I think about like,

617
00:59:56,960 --> 01:00:01,840
how much I have changed and how much it's part of, you know, human nature to change over time.

618
01:00:01,840 --> 01:00:06,320
Like we're supposed to evolve. We're supposed to be weird and different, you know? I worry a lot

619
01:00:06,320 --> 01:00:13,360
about the homogeneity that comes with tech. And I wonder whether, you know, social scientists

620
01:00:13,360 --> 01:00:18,960
or political scientists, middle of a PhD program today could even possibly enter the field of

621
01:00:18,960 --> 01:00:23,760
data science, right? Because, you know, whether or not it's because these job ad platforms,

622
01:00:24,800 --> 01:00:31,200
or these, you know, these NLP based engines that match, uh, resumes with jobs, which is not

623
01:00:31,200 --> 01:00:35,200
sometimes qualified or didn't, you know, didn't know what I was doing. I just didn't fit someone

624
01:00:35,200 --> 01:00:41,360
paradigm. And I felt my life and not fitting other people's paradigms. And I worry very much about

625
01:00:41,360 --> 01:00:47,680
like how people can still be individuals and be curious and just learn other things. Even though

626
01:00:47,680 --> 01:00:53,600
the internet has given us basically unlimited access to things, I wonder if, you know, in a sense,

627
01:00:53,600 --> 01:00:59,200
we may actually stifle all that possibility by thinking that like everything is this repeat cycle

628
01:00:59,200 --> 01:01:04,720
of behavior that we can predict patterns and these patterns are deterministic. And I would say that

629
01:01:04,720 --> 01:01:10,160
like, you know, rule number one of quantitative social science is patterns exist in human behavior,

630
01:01:10,160 --> 01:01:14,640
right? Rule number two is just because a pattern exists doesn't mean individual follows it.

631
01:01:14,640 --> 01:01:18,400
And I think rule number two hasn't really been arrived yet in some of the fields that we're talking

632
01:01:18,400 --> 01:01:28,160
about. That seems like a good place to leave things. Holy you with the big questions.

633
01:01:32,960 --> 01:01:39,200
I mean, it is interesting stuff to think about right on a, what day is today? I'm like on a Wednesday

634
01:01:39,200 --> 01:01:45,120
on a Friday. Clearly a Friday. Clearly it's a Friday. Friday called Tuesday.

635
01:01:46,800 --> 01:01:55,120
I have no idea what day it is anymore. It's really hard. It's, I don't recall if I've talked

636
01:01:55,120 --> 01:02:03,680
about this in an interview, but I've independently kind of validated this experience for several

637
01:02:03,680 --> 01:02:09,600
people like that March was this super, super, super long month and then April kind of flew by

638
01:02:09,600 --> 01:02:17,120
really quickly. But time is just really weird right now. I think for many of us. Yeah, it's like

639
01:02:17,120 --> 01:02:21,520
an absolute reminder that time is absolutely relative. Although it must be great, kind of great

640
01:02:21,520 --> 01:02:27,040
to be a kid right now because you're like forever summer vacation. You know, like you're just like

641
01:02:27,040 --> 01:02:33,920
the longest summer ever. I don't know. You've got to be old enough to know how what you didn't have before, though, I think, right?

642
01:02:34,880 --> 01:02:39,280
Otherwise, that's true. That's true. It must be kind of cool to be like young enough that you don't

643
01:02:39,280 --> 01:02:45,680
really get how scary the situation is. Yeah. But like you're like, cool enough that you're like, oh wow,

644
01:02:45,680 --> 01:02:50,960
I just get to be home for months on end and like playing with my toys. Yeah. And all of that must be,

645
01:02:50,960 --> 01:02:55,520
it must be really interesting time. I do feel bad for all the kids graduating. And he's sort of

646
01:02:55,520 --> 01:03:06,000
for a band right now. This is worse than two thousand. Yeah. Well, Ramon, it was wonderful catching

647
01:03:06,000 --> 01:03:13,440
up with you as always. We'll have to make sure to do it more frequently. Yes, yes, absolutely.

648
01:03:15,200 --> 01:03:18,160
Thanks so much. Thank you very much. Happy Mianzen.

649
01:03:18,160 --> 01:03:28,480
All right, everyone. That's our show for today. To learn more about today's guest or the topics mentioned in this interview,

650
01:03:28,480 --> 01:03:35,040
visit twimmelai.com. Of course, if you like what you hear on the podcast, please subscribe,

651
01:03:35,040 --> 01:03:50,960
rate and review the show on your favorite pod catcher. Thanks so much for listening and catch you next time.

