WEBVTT

00:00.000 --> 00:16.320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

00:16.320 --> 00:21.480
people, doing interesting things in machine learning and artificial intelligence.

00:21.480 --> 00:31.160
I'm your host Sam Charrington.

00:31.160 --> 00:35.280
This week on the podcast we're featuring a series of shows that highlight just a few of

00:35.280 --> 00:39.800
the great innovations and innovators at the intersection of three very important and

00:39.800 --> 00:46.280
familiar topics, data science, the Python programming language and open source software.

00:46.280 --> 00:49.960
To better understand our listeners' views on the importance of open source and the projects

00:49.960 --> 00:54.920
and players in this space, I'm conducting a survey which I'd be very grateful if you took

00:54.920 --> 00:57.040
a moment to complete.

00:57.040 --> 01:01.760
To access the survey, visit Twimbleai.com slash Python survey.

01:01.760 --> 01:09.800
Please hit pause now and we'll wait for you to get back.

01:09.800 --> 01:15.880
That's twimbleai.com slash Python survey.

01:15.880 --> 01:19.840
Before we dive into the show, I'd like to send a huge thanks to our sponsor for this

01:19.840 --> 01:22.200
series IBM.

01:22.200 --> 01:27.200
Speaking of open source, IBM has a long history of engaging in and supporting open source projects

01:27.200 --> 01:33.040
that are important to enterprise data science, projects like Hadoop, Spark, Jupiter and

01:33.040 --> 01:35.680
Cubeflow to name just a few.

01:35.680 --> 01:41.640
IBM also hosts the IBM data science community, which is a place for enterprise data scientists

01:41.640 --> 01:47.320
looking to learn, share and engage with their peers and industry renowned practitioners.

01:47.320 --> 01:52.440
Here you'll find informative tutorials and case studies, Q&As with leaders in the field

01:52.440 --> 01:57.800
and a lively forum covering a variety of topics of interest to both beginning and experience

01:57.800 --> 01:59.400
data scientists.

01:59.400 --> 02:05.160
Check out and join the IBM data science community by visiting IBM.com slash community slash

02:05.160 --> 02:09.160
data science.

02:09.160 --> 02:12.200
Alright everyone, I am on the line with Brian McFee.

02:12.200 --> 02:19.000
Brian is an assistant professor of music technology and data science at NYU, as well as the

02:19.000 --> 02:24.840
creator of LibRosa, Python based library for audio and music processing that we'll be

02:24.840 --> 02:27.840
spending a bit of time chatting about in the show.

02:27.840 --> 02:30.720
Brian, welcome to this week in machine learning and AI.

02:30.720 --> 02:32.320
Thanks for having me.

02:32.320 --> 02:33.320
Absolutely.

02:33.320 --> 02:34.320
Absolutely.

02:34.320 --> 02:39.240
So it's not very often that I speak to anyone on this podcast that was a postdoctoral

02:39.240 --> 02:45.000
researcher in a center for jazz studies and ends up working in machine learning.

02:45.000 --> 02:49.160
I'd love to hear a little bit about your background and how you arrived at working at

02:49.160 --> 02:52.160
this intersection of music and machine learning.

02:52.160 --> 02:53.160
Yeah, sure.

02:53.160 --> 03:00.200
I mean, it's not too surprising because I think there's end of wanting that category.

03:00.200 --> 03:06.680
But yeah, so I kind of fell into music backwards from being more of a general purpose machine

03:06.680 --> 03:10.200
learning kind of person and then just kept going with it because I was having a good

03:10.200 --> 03:11.200
time.

03:11.200 --> 03:12.200
Oh, wow.

03:12.200 --> 03:13.200
Okay.

03:13.200 --> 03:18.440
So, yeah, so I was in grad school in UC San Diego doing my PhD and I was working on kind

03:18.440 --> 03:25.280
of, I wanted to do general machine learning stuff and had been working on some kind of applied

03:25.280 --> 03:30.920
computer vision type things and some real time embedded system type things and was really

03:30.920 --> 03:33.440
having a great time with it.

03:33.440 --> 03:41.160
And my neighbors in the office that I shared with, what, six, seven other PhD students.

03:41.160 --> 03:46.720
So two of them had started a lab of their own doing basically search engines from music.

03:46.720 --> 03:50.200
So they were building statistical models of different kinds of sounds and different kinds

03:50.200 --> 03:53.400
of music and they wanted to build like what they called it Google for music, right?

03:53.400 --> 03:59.280
So you go in, you search for high energy pop song with, I don't know, fast beat and

03:59.280 --> 04:01.440
a tuba in the background or whatever.

04:01.440 --> 04:02.440
Okay.

04:02.440 --> 04:06.240
So it would index a huge collection by the terms that you described and then figure out

04:06.240 --> 04:10.040
which tracks match your query or are likely to match your query.

04:10.040 --> 04:15.560
And off you go, you have music and this was like, oh, 2007, 2008.

04:15.560 --> 04:18.480
So that's after Pandora, right?

04:18.480 --> 04:22.560
So Pandora had been on the scene for a while.

04:22.560 --> 04:25.520
But it was well before Pandora started doing audio content analysis.

04:25.520 --> 04:26.520
Okay.

04:26.520 --> 04:30.800
So they were the way that their system worked and the way that these two other students

04:30.800 --> 04:37.440
who I should call it by name are Doug Turnbull and Luke Barrington.

04:37.440 --> 04:40.120
So they wanted to build like an automatic Pandora, right?

04:40.120 --> 04:44.640
So Pandora had their music genome project analysts listening to music, filling out a huge

04:44.640 --> 04:48.120
questionnaire, categorizing, filing and all that kind of stuff.

04:48.120 --> 04:51.080
And their thinking was, hey, computers should be good at this.

04:51.080 --> 04:53.080
Let's give that a shot.

04:53.080 --> 04:57.840
So they started doing that and I would overhear their conversations and occasionally chip

04:57.840 --> 05:02.040
in with some advice because I had some kind of web development background before going

05:02.040 --> 05:04.360
to grad school, which was useful for them.

05:04.360 --> 05:07.600
And eventually I just realized that they were having a way better time than I was and I

05:07.600 --> 05:11.000
should switch tracks.

05:11.000 --> 05:17.400
And somehow my department was cool with that and let me basically do over and change directions

05:17.400 --> 05:18.400
completely.

05:18.400 --> 05:19.400
Wow.

05:19.400 --> 05:20.400
Nice.

05:20.400 --> 05:21.400
And that's what I did.

05:21.400 --> 05:26.160
And I didn't want to do what they were doing because they had plenty to do on their

05:26.160 --> 05:27.160
own.

05:27.160 --> 05:28.680
But I'm not going to do a search engine.

05:28.680 --> 05:32.520
I'm going to do a recommender system and I'm going to do something that's kind of one

05:32.520 --> 05:33.680
or zero touch.

05:33.680 --> 05:37.080
So instead of describing what I want, I'll give you an example of what I want and then

05:37.080 --> 05:40.960
the algorithm should produce a sequence of songs to listen to.

05:40.960 --> 05:46.040
My motivation for this was spending a lot of time in the car driving up and down the coast

05:46.040 --> 05:49.760
to California, going home to visit family every now and then.

05:49.760 --> 05:55.840
It's a long drive by yourself and my iPod on shuffle was not doing it for me.

05:55.840 --> 05:57.440
I thought there should be a smarter way to do it.

05:57.440 --> 05:58.920
So that's what I wanted to do.

05:58.920 --> 06:04.920
So I started building content-based similarity models and statistical models, playlists and

06:04.920 --> 06:06.160
things of that nature.

06:06.160 --> 06:07.160
And then it was fun.

06:07.160 --> 06:08.640
So I just kept doing it.

06:08.640 --> 06:09.640
Awesome.

06:09.640 --> 06:13.680
Well, that certainly explains the title of your dissertation more like this, Machine Learning

06:13.680 --> 06:15.280
Approaches to Music similarity.

06:15.280 --> 06:16.280
Yep.

06:16.280 --> 06:18.240
That's what that's all about.

06:18.240 --> 06:23.680
As part of your dissertation, did you come up with a working system and what specific

06:23.680 --> 06:29.960
parts of the many challenges in that general area, were you able to flesh out?

06:29.960 --> 06:30.960
Yeah.

06:30.960 --> 06:34.720
So I came up with a system that more or less worked.

06:34.720 --> 06:38.960
The big challenge in any kind of music research is always data, especially if you're doing

06:38.960 --> 06:44.640
things that are centered around commercial music and popular music and trying to model

06:44.640 --> 06:49.200
the listening behavior of large populations of people, getting access to that kind of

06:49.200 --> 06:54.760
data is, well, it's easier now, it was very difficult in 2009 to 2012 when I was doing

06:54.760 --> 06:55.760
this stuff.

06:55.760 --> 06:56.760
Yeah.

06:56.760 --> 07:02.560
So access to data for supervision and also access to audio content because who's going

07:02.560 --> 07:07.040
to go out and buy like 20,000, 50,000 songs, whatever it takes to actually evaluate one

07:07.040 --> 07:08.360
of those systems meaningfully.

07:08.360 --> 07:09.360
Right.

07:09.360 --> 07:12.960
So those were the two big challenges, which are not at all technical challenges that are

07:12.960 --> 07:16.840
entirely social challenges, but that's the way it is.

07:16.840 --> 07:21.040
On the technical side, the things that I was particularly interested in were, so one

07:21.040 --> 07:25.120
integrating different views of the data, so I didn't want to do something that was purely

07:25.120 --> 07:26.120
audio based.

07:26.120 --> 07:29.360
I wanted to take in whatever kind of information I could get.

07:29.360 --> 07:34.040
So you could have the audio from the songs is one representation.

07:34.040 --> 07:39.240
You could have tags from, say, last FM or, what else?

07:39.240 --> 07:43.480
All music guide or music brains or whatever, you know, people tag music on the internet

07:43.480 --> 07:47.920
and that data is out there and it tells you something, you could have lyrics, you could

07:47.920 --> 07:51.520
have social network data, you could have artist biographies, you could have all sorts

07:51.520 --> 07:52.520
of stuff.

07:52.520 --> 07:55.640
And they all live in kind of different geometrical spaces, right?

07:55.640 --> 08:00.160
Like I compare tags to tags, I compare lyrics to lyrics, but tags to lyrics is a little bit

08:00.160 --> 08:03.240
weird and tags to audio is totally weird.

08:03.240 --> 08:07.960
So I spent a lot of time developing models that could integrate across these different

08:07.960 --> 08:13.840
modalities and put them into one similarity space that was kind of optimized to simulate

08:13.840 --> 08:15.520
trends in listening behavior.

08:15.520 --> 08:20.720
So if you have, like, I don't know, how familiar your listeners might be with collaborative

08:20.720 --> 08:23.040
filtering and recommender systems and that sort of thing.

08:23.040 --> 08:25.000
I think fairly, fairly familiar.

08:25.000 --> 08:26.000
Okay.

08:26.000 --> 08:30.640
So the idea there is, I mean, the cartoon idea is you represent items in terms of the

08:30.640 --> 08:32.400
people that consume them.

08:32.400 --> 08:35.920
So two items are similar if they have a large overlap in their populations.

08:35.920 --> 08:39.920
And two items are dissimilar if they have very little overlap.

08:39.920 --> 08:44.760
So you can induce a geometry out of that and then try to train a machine learning model

08:44.760 --> 08:50.240
to transform features so that it produces not necessarily the same similarity, but a

08:50.240 --> 08:54.000
similarity that gives you the same kinds of recommendations.

08:54.000 --> 08:56.160
So it doesn't need to get the geometry exactly right.

08:56.160 --> 09:00.400
It just needs to get, say, like the rank order of nearest neighbors.

09:00.400 --> 09:02.880
That's what you want to preserve because that's what you're going to use for recommendation

09:02.880 --> 09:03.880
anyway.

09:03.880 --> 09:09.480
And you mentioned that tags to lyrics is weird and another example.

09:09.480 --> 09:12.720
What did you mean by weird and what way?

09:12.720 --> 09:15.440
So I mean, in one sense, they're both text.

09:15.440 --> 09:17.480
So yeah, you can do that.

09:17.480 --> 09:20.120
But the statistics are very different, right?

09:20.120 --> 09:24.880
In lyrics, you'll have a lot of stop words and other things because it's natural language

09:24.880 --> 09:26.480
more or less.

09:26.480 --> 09:33.080
So just the really detailed mechanics of how you would compare lyrics to text or lyrics

09:33.080 --> 09:37.240
to tags is not as straightforward as you might expect.

09:37.240 --> 09:43.280
There's also difficulties around open versus close vocabulary or dealing with popularity

09:43.280 --> 09:47.280
or missing data or backwards or endgrams or all those kinds of things.

09:47.280 --> 09:52.960
So yeah, the example that popped up in my mind was, you know, the tag love song and love

09:52.960 --> 09:58.720
song lyrics seems like there's, you know, strong correlations there, but I can imagine

09:58.720 --> 10:03.200
they're much more challenging, much more challenging combinations.

10:03.200 --> 10:04.200
Oh, yeah.

10:04.200 --> 10:07.600
I mean, you can take a song like this is not a love song.

10:07.600 --> 10:08.600
Compare that.

10:08.600 --> 10:11.080
Depending on your endgram count, you get totally different results, right?

10:11.080 --> 10:12.080
Uh-huh.

10:12.080 --> 10:16.200
So yeah, those things are notoriously difficult to get right.

10:16.200 --> 10:24.120
And so you were, so the lab that you were that you started out switching over to initially,

10:24.120 --> 10:26.120
this was at in San Diego.

10:26.120 --> 10:27.120
Yeah.

10:27.120 --> 10:29.520
It was the computer edition lab at UC San Diego.

10:29.520 --> 10:30.520
Okay.

10:30.520 --> 10:37.040
And then you eventually affiliated with lab Rosa at Columbia and that's where the name for

10:37.040 --> 10:38.200
a liberalza came from.

10:38.200 --> 10:39.200
I'm assuming.

10:39.200 --> 10:40.200
Yeah, that's right.

10:40.200 --> 10:42.520
So I finished my PhD in 2012.

10:42.520 --> 10:43.520
Uh-huh.

10:43.520 --> 10:49.480
About the same time Dan Ellis, who is a professor at Columbia, now at Google, who was like

10:49.480 --> 10:52.160
lab Rosa was his lab.

10:52.160 --> 10:56.360
So he had just come into source of funding for a postdoc and I thought like it would

10:56.360 --> 10:59.760
be really good to work with Dan because every time I have an idea, he's already done it

10:59.760 --> 11:02.680
like five years ago, way better than I was going to do it.

11:02.680 --> 11:06.080
And published papers and has data online and code and everything.

11:06.080 --> 11:09.920
So I thought, yeah, it would just, it would save a lot of time if I just try to work with

11:09.920 --> 11:10.920
him.

11:10.920 --> 11:11.920
And that turned out to work out.

11:11.920 --> 11:12.920
Nice.

11:12.920 --> 11:14.760
And so what was your focus at that lab?

11:14.760 --> 11:20.000
So that was a joint position between lab Rosa and the Center for Jazz Studies at Columbia.

11:20.000 --> 11:24.800
And the project was to build a basically a content based jazz discography without really

11:24.800 --> 11:26.720
defining what that was up front.

11:26.720 --> 11:27.720
Uh-huh.

11:27.720 --> 11:32.680
But the idea was that the Center for Jazz Studies had all these kind of archival recordings

11:32.680 --> 11:37.040
and session recordings and all this back catalog of stuff that they wanted to index and

11:37.040 --> 11:40.840
they wanted to do it in a way that made it more directly searchable.

11:40.840 --> 11:45.000
So you could index all the metadata that tells you something, but be a lot more powerful

11:45.000 --> 11:49.280
if you could say, you know, filter down to all the Coletrain recordings and just excerpt

11:49.280 --> 11:50.280
the saxophone solos.

11:50.280 --> 11:51.280
Right?

11:51.280 --> 11:52.960
That's the sort of thing you would like to be able to do.

11:52.960 --> 11:53.960
Yeah.

11:53.960 --> 11:57.480
But nobody's going to go and mark up the start and stop points of every saxophone solo.

11:57.480 --> 11:58.480
Right.

11:58.480 --> 11:59.480
Right.

11:59.480 --> 12:03.680
But that's the sort of thing that a computer might be good at, or at least could plausibly

12:03.680 --> 12:05.400
be good at.

12:05.400 --> 12:09.280
Pretty quickly we realized that all the algorithms that we had for doing this kind of stuff were

12:09.280 --> 12:15.200
not developed on or evaluated on jazz and didn't really take into account the specifics

12:15.200 --> 12:20.040
of the idiom, so to speak, which basically means that nothing worked.

12:20.040 --> 12:25.640
So he spent a lot of time trying to generalize the methods so that they would adapt to basically

12:25.640 --> 12:29.720
things that were modern western pop music or classical, which is what everything was

12:29.720 --> 12:30.720
built on.

12:30.720 --> 12:38.880
But meaning you were able to identify pre-existing algorithms that given, you know, modern pop

12:38.880 --> 12:46.600
or classical could pick out certain instrument solos?

12:46.600 --> 12:50.560
Solo detection wasn't so much of a problem back then, it's become a little bit more of

12:50.560 --> 12:54.240
a problem now by problem on being like research area.

12:54.240 --> 12:59.080
But certainly the were algorithms for doing things like beat tracking or structural segmentation

12:59.080 --> 13:04.760
or maybe some source separation, although that's changed a lot in the last few years.

13:04.760 --> 13:07.800
But even something as simple as beat tracking, you know, just like tracking where the quarter

13:07.800 --> 13:12.720
notes are in a piece, mostly those are developed on classical or western pop music and just

13:12.720 --> 13:15.360
fell over hard when you give them anything with swing in it.

13:15.360 --> 13:21.720
It's kind of the application for beat tracking or some of the applications for beat tracking.

13:21.720 --> 13:29.320
So it's often used as a pre-processing stage in other kind of more high level analyses.

13:29.320 --> 13:34.920
So one thing that you might do is track the beats of a song and then use the estimated

13:34.920 --> 13:41.080
beat positions as kind of a nonlinear time sampling grid so that you can take a bunch

13:41.080 --> 13:45.280
of features that you extracted with the signal, re-sample them on this non-uniform grid.

13:45.280 --> 13:48.600
And now you have something that's kind of invariant to tempo.

13:48.600 --> 13:53.480
And that could be useful for doing things that are more high level structural analysis,

13:53.480 --> 13:57.600
like first chorus first or maybe one track the down beats.

13:57.600 --> 14:00.000
So went one bar changes to the next bar.

14:00.000 --> 14:03.720
That's those are the sort of thing that you don't want to be too sensitive to small

14:03.720 --> 14:09.480
variations in tempo, but they need to account for larger contextual ranges.

14:09.480 --> 14:12.840
So you set out to build this search engine.

14:12.840 --> 14:18.240
It sounds like you're a little bit of a head of a game relative to your dissertation

14:18.240 --> 14:24.040
and that you have this huge source of data via the Center for Jazz Studies.

14:24.040 --> 14:30.600
Yeah, and it's a huge source of data and it was nice and that it was kind of scoped

14:30.600 --> 14:40.440
down in terms of style and the limited recording times from like the 30s up to 1970, I think.

14:40.440 --> 14:46.200
It's basically to rule out fusion because then things get way more complicated.

14:46.200 --> 14:50.320
But yeah, it was nice because it meant that like some things that we think of as really

14:50.320 --> 14:57.160
hard in these analysis, like structure analysis, find the first chorus, verse, whatever.

14:57.160 --> 15:00.800
Those things are really difficult when you have to account for all the different forms

15:00.800 --> 15:02.480
that these can take.

15:02.480 --> 15:08.640
But if you narrow down to a much more specific genre or style, sometimes you can add additional

15:08.640 --> 15:11.640
constraints to your inference that make the problem a lot more easy.

15:11.640 --> 15:12.640
Okay.

15:12.640 --> 15:18.320
So like a typical recording in our collection would have just a standard jazz form, like play

15:18.320 --> 15:22.600
the head, soloist take turns, play the head out, done.

15:22.600 --> 15:25.560
And that made things a lot easier to analyze.

15:25.560 --> 15:31.960
And so how did this challenge lead to the development of Librosa?

15:31.960 --> 15:35.120
While we're there, explain what Librosa is.

15:35.120 --> 15:42.040
Yeah, so I'll take sort of a detour of this, but I'll get back to the question.

15:42.040 --> 15:48.760
So we have a music information retrieval, which is the name to this kind of area of interdisciplinary

15:48.760 --> 15:49.760
research.

15:49.760 --> 15:54.440
There's the annual conference called ASMR for the International Society for MIR.

15:54.440 --> 15:56.840
And it happens in the fall every year.

15:56.840 --> 16:03.360
And in 2012, it was happening like maybe a couple of weeks after I started at Columbia.

16:03.360 --> 16:08.120
And at the conference, there was a breakout session about trying to transition away from

16:08.120 --> 16:12.200
MATLAB and into Python as a research field.

16:12.200 --> 16:17.520
And it might not feel like it now, but seven years ago, it was kind of a question mark,

16:17.520 --> 16:19.680
like is Python really where we should go?

16:19.680 --> 16:20.680
Why would we do this?

16:20.680 --> 16:24.960
Because there were plenty of other frameworks that people could have adopted.

16:24.960 --> 16:25.960
Sure.

16:25.960 --> 16:28.440
And MATLAB was still very strong in academia.

16:28.440 --> 16:29.440
It was.

16:29.440 --> 16:31.760
And it still is in a lot of ways.

16:31.760 --> 16:35.840
So in signal processing, right, which is where a lot of this research comes from.

16:35.840 --> 16:39.600
And the way that research had been conducted in this field, since I don't know when,

16:39.600 --> 16:44.360
like certainly before I started it, was you inherit a pile of MATLAB scripts that do some

16:44.360 --> 16:45.360
kind of analysis.

16:45.360 --> 16:49.000
And then you run those scripts and maybe tweak them a little bit, do your analysis, and

16:49.000 --> 16:52.280
then you do another project, and you tweak the scripts a little bit more.

16:52.280 --> 16:55.720
There wasn't any sort of versioning, there wasn't any packaging, if someone else joins

16:55.720 --> 17:00.040
your lab, you just hand them whatever your scripts are in their current state at the time.

17:00.040 --> 17:03.240
And maybe they're compatible with what they need from some other place, or maybe they're

17:03.240 --> 17:04.240
not.

17:04.240 --> 17:06.240
And it was basically a big old mess.

17:06.240 --> 17:12.480
So I was kind of looking for new things to do, and being a computer scientist and not

17:12.480 --> 17:17.760
a electrical engineer or a musician, but hey, that's a problem that I can help with.

17:17.760 --> 17:22.240
So as soon as I got back from that conference, I just started writing code.

17:22.240 --> 17:26.680
And the way that it really started was taking all the code that we had from Dan Ellis's

17:26.680 --> 17:31.160
group at Columbia, that was just independent MATLAB scripts with different parameterizations

17:31.160 --> 17:36.480
and different dependencies, and just trying to consolidate them and get them a unified interface

17:36.480 --> 17:38.120
and port them over to Python.

17:38.120 --> 17:42.440
So that was the start of Librosa, which was named after Labrosa.

17:42.440 --> 17:47.520
And Labrosa actually stands for Laboratory for the Recognition and Organization of Speech

17:47.520 --> 17:48.520
and Audio.

17:48.520 --> 17:52.240
So it is actually a proper acronym.

17:52.240 --> 17:57.120
But also it was just, I think Dan Lex-Pink and everything in the lab was pink, so that

17:57.120 --> 17:58.120
nicely.

17:58.120 --> 18:02.840
I'm not sure which came first, you'd have to ask him that.

18:02.840 --> 18:09.080
So yeah, so Librosa really set out to be kind of fixing the core bottleneck in moving

18:09.080 --> 18:13.320
people away from MATLAB and into Python, or doing MIR work.

18:13.320 --> 18:16.440
Because at the time, Psychic Learn was already fairly mature.

18:16.440 --> 18:17.440
The ANO was out there.

18:17.440 --> 18:21.240
There were a few kind of deep learning frameworks that hadn't quite stabilized into what we

18:21.240 --> 18:24.320
have these days, but they were enough to do some damage.

18:24.320 --> 18:28.280
But the key part that was missing was kind of the front end, like ingesting audio and

18:28.280 --> 18:31.840
transforming into these standard representations that people know how to work with.

18:31.840 --> 18:37.000
So that's really where Librosa kind of aimed to sit and expanded a little bit from there

18:37.000 --> 18:45.640
into kind of general tools for working with music and audio, including visualization and

18:45.640 --> 18:48.120
some transformation, some effects processing.

18:48.120 --> 18:52.560
But mostly it's meant to be kind of that interface layer between what we know how to do

18:52.560 --> 18:57.120
algorithmically and what we want to feed into a statistical model if that makes any sense.

18:57.120 --> 18:58.120
Okay.

18:58.120 --> 18:59.120
Yeah.

18:59.120 --> 19:04.760
Can you walk us through maybe some typical user workflows?

19:04.760 --> 19:06.880
Yeah.

19:06.880 --> 19:14.320
So for myself, a lot of what I'll do is have an idea, like maybe I want to try out some new

19:14.320 --> 19:17.520
filtering or some new representation.

19:17.520 --> 19:19.440
And I'll typically work in a Jupyter notebook.

19:19.440 --> 19:24.680
So a lot of this is kind of developed to be easy to use in notebook environments.

19:24.680 --> 19:30.280
And I'll do a bit of hacking in a notebook, work on some small examples, make some plots,

19:30.280 --> 19:32.560
make some figures, maybe fit a small model.

19:32.560 --> 19:36.960
And then once that's kind of settled into something that I like, then I'll try and pull

19:36.960 --> 19:42.400
out whatever the core piece that I've been working on is, whether it's like a new representation

19:42.400 --> 19:46.240
or it's usually a representation kind of thing.

19:46.240 --> 19:51.320
It's a new way to do a execute transform or some different windowing function on a

19:51.320 --> 19:56.720
short time Fourier transformer, you know, something fairly low level and grungy.

19:56.720 --> 20:01.320
I'll take that out of a notebook, pull it into a script and then start building up experimentation

20:01.320 --> 20:03.840
scripts around that that'll run on a cluster.

20:03.840 --> 20:12.440
So the typical way that I use it for doing machine learning experiments is I'll have a collection

20:12.440 --> 20:17.160
of audio on disk, I'll write a script that does some transformation and then map that

20:17.160 --> 20:23.920
out over my audio collection and save the results as numpy file or an hdf file or whatever

20:23.920 --> 20:29.440
is most convenient and then have a separate downstream process that we'll consume those

20:29.440 --> 20:36.680
and run them through a TensorFlow model or a PyTorch model or something like that.

20:36.680 --> 20:40.080
And then the analysis often involves like after the model is fit and I want to look at

20:40.080 --> 20:44.720
the results, that'll typically involve loading into audio redoing some stuff on the fly.

20:44.720 --> 20:48.680
That's usually the notebook as well, but it's always helpful to have some kind of plotting

20:48.680 --> 20:53.640
and visualization routines built into the library that we can just rely on to do kind of

20:53.640 --> 20:56.480
air analysis, model inspection, that kind of thing.

20:56.480 --> 21:00.920
So that's where some feature creep comes from, but I think it's pretty long on the control.

21:00.920 --> 21:01.920
Okay, okay.

21:01.920 --> 21:10.680
So the types of the API or the types of functions that Liberoza is providing are some of the

21:10.680 --> 21:16.120
low-level things that you describe like FFTs and spectrograms and the like.

21:16.120 --> 21:20.120
Yeah, so it could be anything from spectrograms.

21:20.120 --> 21:24.560
We don't provide our own FFT, but we give wrappers for doing short time Fourier transforms,

21:24.560 --> 21:29.760
which are these days sci-fi can do that, but when we started, there was no short time

21:29.760 --> 21:35.080
Fourier transform, which is just, you know, taking a long signal, chunking up in a small

21:35.080 --> 21:38.040
pieces and then doing an FFT of each piece.

21:38.040 --> 21:45.160
And we've talked about FFTs on this podcast at least once, but for anyone it doesn't,

21:45.160 --> 21:47.440
you know, it's not familiar with that terminology.

21:47.440 --> 21:52.520
It's basically a way that you can pull out the frequency components of an audio signal.

21:52.520 --> 21:53.520
That's right.

21:53.520 --> 21:57.400
I always explain it as like running your audio through a prism and separating out the

21:57.400 --> 21:58.400
colors.

21:58.400 --> 22:00.040
Oh, that's a great analogy.

22:00.040 --> 22:01.040
Yeah.

22:01.040 --> 22:05.560
Yeah, it works, it works especially well with Pink Floyd fans, other people your mileage

22:05.560 --> 22:06.560
may vary.

22:06.560 --> 22:13.200
So yeah, it's that kind of stuff, but also ways of post-processing Fourier transforms.

22:13.200 --> 22:17.960
So there are different features that people have derived that try to bake in certain

22:17.960 --> 22:22.280
inferences for the representation that are useful for other tasks.

22:22.280 --> 22:26.040
So for example, if you want to do something like automatic chord recognition, so I want

22:26.040 --> 22:29.760
to know what chords are playing at what point, what point in the song.

22:29.760 --> 22:34.120
You don't actually care about octaves, you don't care if I'm in this register or another

22:34.120 --> 22:37.000
register, you just care about which pitch classes are active.

22:37.000 --> 22:42.120
So people design methods and representations that kind of normalize out the octave information

22:42.120 --> 22:45.880
but retain the pitch class identities, which in principle are things that you can learn

22:45.880 --> 22:50.000
and these days most people just learn that directly, but it is still useful to have kind

22:50.000 --> 22:53.920
of a simple method for doing that transformation that you can just crack open and see what it's

22:53.920 --> 22:56.080
doing exactly.

22:56.080 --> 23:02.040
So those are called chroma features, so chroma for color, and then there are other things

23:02.040 --> 23:07.080
like the male spectrum, which is kind of derived from human perceptual experiments, that's

23:07.080 --> 23:11.800
like pseudo logarithmic, and a bunch of other kind of spectral representation features.

23:11.800 --> 23:17.000
But then there's other stuff like doing sample rate conversion or separating harmonic and

23:17.000 --> 23:21.240
progressive sources or who knows what, there's a lot of stuff in there.

23:21.240 --> 23:26.200
B-tracking is in there, some basic stuff for building up structural segmentation algorithms,

23:26.200 --> 23:30.760
working on a new feature for doing a bunch of inverse problems now, so taking those representations

23:30.760 --> 23:36.200
back to the time domain, so hopefully that'll be in the next version.

23:36.200 --> 23:44.440
Are those features used for generative types of use cases or something else?

23:44.440 --> 23:50.480
Yeah, that's the thinking, because a lot of these days you're starting to see a lot

23:50.480 --> 23:56.000
of generative models of audio that work on time series features, I just like produce

23:56.000 --> 24:01.840
the raw waveform, which is great when that works, but doesn't always work.

24:01.840 --> 24:07.360
And doing generative models on complex valued spectra is challenging because doing

24:07.360 --> 24:10.440
gradient descent on complex numbers can be challenging.

24:10.440 --> 24:14.240
So people often will build generative models of magnitude spectra, so they take the Fourier

24:14.240 --> 24:20.720
transform and throw out the phase, and there are ways of trying to estimate phase and recover

24:20.720 --> 24:25.440
the original signal, or something close to the original signal given on the magnitudes.

24:25.440 --> 24:29.400
And we haven't had that in the brose of for a while, but it's always been on the to-do

24:29.400 --> 24:32.880
list, so we're finally putting that in there, and it's nothing fancy or new, but it's

24:32.880 --> 24:38.720
just there to help people kind of prototype their generative models or, you know, investigate

24:38.720 --> 24:40.240
their systems otherwise.

24:40.240 --> 24:45.920
So a lot of the features and functionality that we've talked about as far are things that

24:45.920 --> 24:54.280
you would use for analysis or for feature extraction, feature expression, are there components

24:54.280 --> 25:00.440
of librosa that are directly doing learning as well, or is it primarily for the, for

25:00.440 --> 25:03.800
kind of audio data preparation or the data science phases?

25:03.800 --> 25:07.000
Yeah, that's a great question.

25:07.000 --> 25:13.240
So there are some things that are built in that include like some very light statistical

25:13.240 --> 25:14.240
training.

25:14.240 --> 25:18.520
So those would be things like detecting node on-sets or detecting beats.

25:18.520 --> 25:22.160
Those have, properly, those are supervised learning problems, right?

25:22.160 --> 25:25.960
You're trying to build some mapping that comes from some input representation to a set

25:25.960 --> 25:32.080
of decisions, whether it's like, is this a B, or is this a new node, or whatever it

25:32.080 --> 25:37.360
happens to be, and those are in there because those are useful pre-processing stages for

25:37.360 --> 25:40.800
other more high-level algorithms, as I mentioned earlier.

25:40.800 --> 25:46.480
But we try to shy away from having too many kind of pre-trained models in librosa, and

25:46.480 --> 25:53.920
the reason for that is that data is fairly scarce still somehow, and we spent a lot of

25:53.920 --> 25:58.040
time in this field analyzing and re-analysing with the same data sets.

25:58.040 --> 26:02.040
So if we started building models in librosa that were pre-trained on some data set, it's

26:02.040 --> 26:08.480
very easy for someone down the road to pick up the library, not realize what the statistical

26:08.480 --> 26:12.040
dependencies of the model that are built in are, and then run it over the same data and

26:12.040 --> 26:16.240
report some number that sounds great, but it's totally vacuous because they're actually

26:16.240 --> 26:17.840
running on the training set.

26:17.840 --> 26:19.840
Got it.

26:19.840 --> 26:24.520
So we try and avoid that, but that would be a good thing to avoid.

26:24.520 --> 26:29.080
Yeah, and there are other libraries out there that do include pre-trained models, and that's

26:29.080 --> 26:33.800
a good thing, as far as I know, they're pretty careful about documenting where those models

26:33.800 --> 26:35.280
come from and how they're fit.

26:35.280 --> 26:39.640
It still makes me nervous to have something that's fairly low level, and likely to be

26:39.640 --> 26:46.080
at a lot of upstream pipelines to be that sensitive to statistics.

26:46.080 --> 26:51.360
But that said, there are some other things that are more unsupervised, so one of the problems

26:51.360 --> 26:55.560
that I like to work on is structure analysis, where you take a song or a time series more

26:55.560 --> 27:00.240
generally and break it into pieces that are connected and maybe recurrent later in the

27:00.240 --> 27:04.840
piece, and that, if you look at it the right way, it's a graph partitioning problem, right?

27:04.840 --> 27:07.360
So each time point becomes a vertex in a graph.

27:07.360 --> 27:12.280
You add edges between vertices that are somehow similar, whether it's similar in time or

27:12.280 --> 27:17.120
similar by repetition features, and then you partition that graph, and that's kind of

27:17.120 --> 27:20.680
an unsupervised problem that's confined to a single recording at a time.

27:20.680 --> 27:25.840
So we give you some tools to work with that sort of stuff because there isn't really a notion

27:25.840 --> 27:31.160
of statistical contamination from other datasets or inductive bias.

27:31.160 --> 27:34.920
So yeah, so we make that kind of stuff easy, but for the most part, we try to shy away

27:34.920 --> 27:39.720
from anything to statistical, but it also makes you an attest to a lot easier.

27:39.720 --> 27:40.720
Right.

27:40.720 --> 27:41.720
Right.

27:41.720 --> 27:42.720
I can see that.

27:42.720 --> 27:45.920
The structure analysis, you mentioned it a couple of times that you went into a little

27:45.920 --> 27:49.400
bit more detail just previously.

27:49.400 --> 27:53.240
Can you talk a little bit more about that?

27:53.240 --> 27:59.360
The graphical analysis element of that, and kind of how that's implemented.

27:59.360 --> 28:01.440
What is the user doing?

28:01.440 --> 28:07.520
Are they just saying go do this, or are they kind of building out a model and kind of

28:07.520 --> 28:10.280
implementing it and you're providing support?

28:10.280 --> 28:15.880
Where does the user's app and what the browser is providing start and end?

28:15.880 --> 28:18.360
Yeah, absolutely.

28:18.360 --> 28:25.840
So for that particular problem, we don't really provide an end-to-end solution in the library,

28:25.840 --> 28:31.440
but we do have an example gallery in the documentation that shows a bunch of kind of not necessarily

28:31.440 --> 28:38.600
small, but self-contained algorithms and projects that take a few different pieces of tools

28:38.600 --> 28:42.440
in LaGrosa and combine them so that you can do some sort of analysis.

28:42.440 --> 28:47.160
So structural segmentation is one of those examples, and that particular method is based

28:47.160 --> 28:52.280
on a paper I wrote four years ago with Dan Ellis, five years ago now.

28:52.280 --> 28:53.280
I don't know.

28:53.280 --> 28:54.280
Some number of years ago.

28:54.280 --> 28:57.640
Is the Laplacian segmentation method paper?

28:57.640 --> 28:58.640
Yeah, that's right.

28:58.640 --> 28:59.640
Okay.

28:59.640 --> 29:04.160
We actually have a new paper coming out that's kind of revising that and replacing sort

29:04.160 --> 29:08.520
of the under-the-hood similarity with something a little bit smarter, but the basic idea is

29:08.520 --> 29:09.520
the same.

29:09.520 --> 29:17.080
Was this paper the first time this graph-based approach to structural segmentation was proposed?

29:17.080 --> 29:19.680
I think it was the first time that it was called out as such.

29:19.680 --> 29:26.720
There were some prior work that had similar ideas and were for more of a signal processing

29:26.720 --> 29:32.480
perspective and didn't really make full use of kind of the graphical underpinnings of

29:32.480 --> 29:33.480
what was going on.

29:33.480 --> 29:39.880
I was thinking specifically of a paper by Harold Grokens and my regular, where they come

29:39.880 --> 29:44.240
up with almost the same kind of method, but in a very different way, so it was interesting

29:44.240 --> 29:46.080
to see that connection play out.

29:46.080 --> 29:52.160
There were a lot of works that kind of approached it from not necessarily a bold graph, but from

29:52.160 --> 29:56.280
a chain graph perspective, so they would have like a mark-up chain that are just trying

29:56.280 --> 29:59.360
to decide, am I in the same section or a new section?

29:59.360 --> 30:04.240
And then you have a much simpler kind of linear chain conditional random fields or

30:04.240 --> 30:07.240
a structure under the problem.

30:07.240 --> 30:10.680
But yeah, I think no one had really done spectral clustering on that before.

30:10.680 --> 30:14.800
It's kind of fiddly and finicky to get right, and we still didn't get it 100% right,

30:14.800 --> 30:17.800
but we got it closer to right in that work.

30:17.800 --> 30:24.800
I'm curious, and I don't know if this question is one that can be answered generally, but

30:24.800 --> 30:33.400
I'm curious about the, you know, if you're taking a given time series sample and making

30:33.400 --> 30:41.800
that a node in a graph and connecting it to other nodes based on this structural analysis,

30:41.800 --> 30:45.760
that's the cardinality of a given node in terms of number of connections.

30:45.760 --> 30:54.040
Is it like very dense or is it 2, 3, 4, or I'm trying to get a feel for kind of the complexity

30:54.040 --> 30:55.040
of this graph?

30:55.040 --> 30:58.920
Yeah, yeah, like what are the degree bounds on it?

30:58.920 --> 31:04.400
So the way that we did it, at least the first time around, was we actually built two graphs

31:04.400 --> 31:05.800
and then merged them.

31:05.800 --> 31:07.040
And what were the two?

31:07.040 --> 31:12.600
So the first graph is taking every point in time and connecting it just to its left and

31:12.600 --> 31:14.120
right neighbors in time.

31:14.120 --> 31:19.120
Okay, so if I'm time T, I have a like to T minus 1 and to T plus 1.

31:19.120 --> 31:23.040
And then you can refine that a little bit by adding some sort of weight on those edges

31:23.040 --> 31:27.480
that kind of measures how similar the features are between T and T plus 1.

31:27.480 --> 31:31.120
So if the sound is basically the same, it has the same kind of spectral shape, then there

31:31.120 --> 31:32.440
will be a strong edge.

31:32.440 --> 31:36.400
But if it goes from like loud, bursty noise down to absolute silence, there will be a

31:36.400 --> 31:37.840
very weak edge.

31:37.840 --> 31:41.240
So you can imagine this makes it a little bit easier to partition the graph and recover

31:41.240 --> 31:42.560
change points.

31:42.560 --> 31:43.560
Okay.

31:43.560 --> 31:46.920
So that's the first graph we call it the conveyor belt because you just move along in

31:46.920 --> 31:48.840
time and that's all you can do.

31:48.840 --> 31:54.720
The second graph was linking between repetitions of the same observations.

31:54.720 --> 31:57.280
And I say same as really main similar.

31:57.280 --> 32:02.240
So if you imagine I play some chord at time T and then I play that same chord at some

32:02.240 --> 32:06.320
other time U, then I'll add an edge between T and U.

32:06.320 --> 32:09.920
And if I can detect that those same sounds have recurred.

32:09.920 --> 32:13.600
So what you're asking about the kind of degree complexity of this graph really depends

32:13.600 --> 32:16.120
on how repetitive it's the recording.

32:16.120 --> 32:17.120
Got it.

32:17.120 --> 32:22.840
In practice, we do some adaptive bandwidth estimation based on the observer features and

32:22.840 --> 32:27.440
do some quantile estimates to say like, well, we're going to build this Gaussian kernel

32:27.440 --> 32:32.160
over the feature space and figure out some nice quantile thresholds such that we get

32:32.160 --> 32:35.520
strong connections above that threshold and very weak connections below it.

32:35.520 --> 32:36.520
Off you go.

32:36.520 --> 32:39.720
It's all the usual kind of kernel tuning literature type stuff there.

32:39.720 --> 32:48.200
So we're talking through this kind of method in general and how much you provide via the

32:48.200 --> 32:51.720
library and how much the end user has to build.

32:51.720 --> 33:00.360
And you made the point that it's not quite end-to-end, but you're providing some of the fundamental

33:00.360 --> 33:01.360
pieces.

33:01.360 --> 33:08.360
Yeah, the way I see that the users that I'm trying to, like my 80% of users are really

33:08.360 --> 33:13.360
the people that are building machine learning systems around audio analysis.

33:13.360 --> 33:17.720
It's not the people that want to do an audio analysis directly, if that's a distinction

33:17.720 --> 33:19.200
that really makes sense.

33:19.200 --> 33:21.600
Most people want to come set to it.

33:21.600 --> 33:26.880
But yeah, so basically I try to build the tools that make it easy to build the actual system.

33:26.880 --> 33:32.960
And so maybe as maybe to wrap things up, you kind of started out in this, started out

33:32.960 --> 33:41.280
down this path, you know, with this very explicit focus on Python as a, you know, direction

33:41.280 --> 33:50.120
and community, you know, versus MATLAB, which was the incumbent, if you will.

33:50.120 --> 33:54.040
And now you're a few years down that path, like how did that all?

33:54.040 --> 33:56.840
I think we know like generally how it worked out.

33:56.840 --> 34:04.880
But for you and this project, I'm curious, are there any non-obvious kind of observations

34:04.880 --> 34:11.360
that you might make around just Python as a tool or community or domain for doing this

34:11.360 --> 34:12.360
kind of work?

34:12.360 --> 34:15.480
So I think for me, timing was everything.

34:15.480 --> 34:20.120
Like I've been developing this library on and off for the last six, seven years now.

34:20.120 --> 34:25.080
And it's like really hitting the steepest point on the Python data science acceptance

34:25.080 --> 34:26.080
curve, I think.

34:26.080 --> 34:32.640
Like I don't think there's a whole lot more growth to be had in terms of number of users

34:32.640 --> 34:33.640
for Python.

34:33.640 --> 34:38.680
I think that's like, like Python is where it is and that's great.

34:38.680 --> 34:48.080
There are other languages that might tip away at some of that, say Julia or mostly Julia.

34:48.080 --> 34:52.480
But the thing that attracted me to Python in the first place was A, that's where a lot

34:52.480 --> 34:55.440
of the data analysis software was being written.

34:55.440 --> 34:58.320
So the machine learning packages were all going that direction.

34:58.320 --> 35:02.400
At the time, it was psychic learning, Theano, now it's PyTorch, Keras, TensorFlow, all

35:02.400 --> 35:03.400
that stuff.

35:03.400 --> 35:08.040
But more than that, it was all the other kind of ecosystem of packages for doing everything

35:08.040 --> 35:09.040
else, right?

35:09.040 --> 35:13.400
Text analysis and web hosting and anything else that you wanted to do, it kind of took

35:13.400 --> 35:16.520
over from Pearl in that respect, at least the way that I see it.

35:16.520 --> 35:17.520
And thank goodness for that.

35:17.520 --> 35:18.520
I know.

35:18.520 --> 35:19.520
Right?

35:19.520 --> 35:23.400
And I don't think that's going away.

35:23.400 --> 35:28.560
And I think that makes a lot of things easy and nice.

35:28.560 --> 35:35.400
But you know, Python does have some limitations, really in terms of scalability and speed.

35:35.400 --> 35:41.760
And the, you know, projects like Numba or Rapids or, you know, all desk, you know, those

35:41.760 --> 35:44.960
are all kind of chipping away at different aspects of this.

35:44.960 --> 35:49.680
Make it easier to scale up and run over, you know, 100,000 recordings, if you want.

35:49.680 --> 35:50.920
That's exciting to see.

35:50.920 --> 35:55.800
It's, you can really see how difficult it is to make that stuff work really well, though.

35:55.800 --> 35:56.800
Yeah.

35:56.800 --> 36:00.920
As an early doctor of those things, I constantly find myself trying to figure out how can

36:00.920 --> 36:07.520
I work around what I anticipate will be difficult a year from now, and that's often difficult.

36:07.520 --> 36:11.200
Yeah, I mean, those are the main things.

36:11.200 --> 36:16.680
I've recently started teaching a big data class here at NYU, which is kind of heavily focused

36:16.680 --> 36:20.160
on how deep in Spark and MapReduce and all that kind of stuff.

36:20.160 --> 36:24.200
And seeing how nice Scala is in a lot of ways and thinking like, man, there is a way to

36:24.200 --> 36:25.200
combine these two things.

36:25.200 --> 36:26.200
That would be great.

36:26.200 --> 36:29.040
But I really don't see any way to do it right now.

36:29.040 --> 36:36.040
Meaning Python and Scala or Spark and Net ecosystem and Python, I mean, there are folks

36:36.040 --> 36:39.320
that do Python stuff on Spark.

36:39.320 --> 36:43.080
It's certainly not as popular as Scala, I don't believe.

36:43.080 --> 36:44.080
Right.

36:44.080 --> 36:49.400
No, more in terms of not just Python, but so, for example, in Librosa, we have a lot of

36:49.400 --> 36:57.160
stuff that calls out to packages written in C or Numba compiled functions or things that

36:57.160 --> 37:00.240
don't necessarily integrate well with Java-based language.

37:00.240 --> 37:02.760
Got it.

37:02.760 --> 37:08.080
PySpark is great in that it hides a lot of that stuff, but it also is not so great in that

37:08.080 --> 37:12.160
the data serialization issues can really bite you and you have to move in between the

37:12.160 --> 37:14.160
JPM and the Python environment.

37:14.160 --> 37:18.760
And maybe that's not a bad thing if the data you're moving is small, but if it gets large

37:18.760 --> 37:21.440
then that's a problem.

37:21.440 --> 37:25.400
And large is a point if you're going to go through the trouble of getting a Spark cluster

37:25.400 --> 37:26.400
up and running.

37:26.400 --> 37:30.680
I mean, maybe if you have your data living in HDFS and you can just like pull in some

37:30.680 --> 37:35.040
identifier that tells you where to get the audio content and then you analyze it in Python,

37:35.040 --> 37:36.040
that's fine.

37:36.040 --> 37:40.520
But if you're pulling in blogs of decoded audio data, that's really expensive.

37:40.520 --> 37:44.960
So, there are some subtleties to making these things really integrate nicely together.

37:44.960 --> 37:48.560
I think we still haven't quite figured out as a community.

37:48.560 --> 37:49.560
I don't know.

37:49.560 --> 37:53.800
It seems like the, especially the column store stuff seems to be making a lot of those

37:53.800 --> 37:57.760
transitions easier, so projects like Feather or Arrow, things like that.

37:57.760 --> 38:00.600
It's nice to see those things kind of take it off.

38:00.600 --> 38:01.600
Cool.

38:01.600 --> 38:06.640
Well, Brian, thanks so much for taking the time to chat with us about what you're working

38:06.640 --> 38:07.640
on.

38:07.640 --> 38:12.160
Librosa sounds like a really interesting tool and having kind of not played with it,

38:12.160 --> 38:18.560
but kind of poked around a little bit in the docs and as someone who does a lot of work

38:18.560 --> 38:25.920
with audio, it looks like a really interesting environment and a way to play with kind of

38:25.920 --> 38:29.120
get up to speed with doing some machine learning stuff on audio.

38:29.120 --> 38:30.120
Yeah.

38:30.120 --> 38:31.120
Oh, thanks for having me.

38:31.120 --> 38:32.120
This is fun.

38:32.120 --> 38:33.120
Absolutely.

38:33.120 --> 38:34.120
Thank you.

38:34.120 --> 38:39.800
All right, everyone, that's our show for today.

38:39.800 --> 38:44.480
If you like what you've heard here, please do us a huge favor and tell your friends about

38:44.480 --> 38:45.640
the show.

38:45.640 --> 38:49.680
And if you haven't already hit that subscribe button yourself, make sure you do so you

38:49.680 --> 38:53.720
don't miss any of the great episodes we've gotten in store for you.

38:53.720 --> 39:00.720
As always, thanks so much for listening and catch you next time.

