1
00:00:00,000 --> 00:00:15,920
Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting

2
00:00:15,920 --> 00:00:20,920
people doing interesting things in machine learning and artificial intelligence.

3
00:00:20,920 --> 00:00:23,200
I'm your host Sam Charrington.

4
00:00:23,200 --> 00:00:27,360
This week on the podcast we're running a series of shows consisting of conversations

5
00:00:27,360 --> 00:00:33,120
with some of the impressive speakers from an event called the AI Summit in New York City.

6
00:00:33,120 --> 00:00:38,160
The theme of that event and of this series is AI in the Enterprise.

7
00:00:38,160 --> 00:00:42,240
And I think you'll find this series really interesting in that it includes a mix of both

8
00:00:42,240 --> 00:00:46,600
technical and case study oriented discussions.

9
00:00:46,600 --> 00:00:51,560
Now I won't actually be attending the AI Summit this week because I'm in Long Beach

10
00:00:51,560 --> 00:00:54,560
California attending the Nips Conference.

11
00:00:54,560 --> 00:00:59,760
There are a bunch of Twimble listeners here and I'm hoping to meet as many of you as possible.

12
00:00:59,760 --> 00:01:02,040
And yes, I have stickers.

13
00:01:02,040 --> 00:01:06,320
If you're here at Nips and you're actually listening to podcasts this week, please

14
00:01:06,320 --> 00:01:11,320
reach out to me either via the event app, the Nips event app where there's a Twimble

15
00:01:11,320 --> 00:01:16,960
listeners thread or via Twitter where my handle is at Sam Charrington.

16
00:01:16,960 --> 00:01:21,120
Before we proceed, let's quickly talk about the podcast schedule through the end of the

17
00:01:21,120 --> 00:01:22,120
year.

18
00:01:22,120 --> 00:01:26,080
Prior to my newsletter, you know that I've been on the road for a couple of weeks now.

19
00:01:26,080 --> 00:01:30,800
After this week's series, we've got two more series coming before we break for the year

20
00:01:30,800 --> 00:01:34,280
with our last show running on December 22nd.

21
00:01:34,280 --> 00:01:39,480
Now, if you're lamenting two weeks without your favorite machine learning and AI podcast,

22
00:01:39,480 --> 00:01:42,680
trust me with these two series.

23
00:01:42,680 --> 00:01:48,320
The first from the Amazon Web Services Reinvent Conference and the next one from Nips.

24
00:01:48,320 --> 00:01:53,440
You will have plenty of great content to tide you over until we get started again on January

25
00:01:53,440 --> 00:01:54,440
8th.

26
00:01:54,440 --> 00:02:01,000
Thanks to you, 2017 was a great year for the podcast and we plan to close it out strong.

27
00:02:01,000 --> 00:02:05,400
So keep your ears open the next few weeks and we hope to hear from you.

28
00:02:05,400 --> 00:02:11,440
Please note that on Wednesday, December 13th, we'll be holding our last Twimble Online

29
00:02:11,440 --> 00:02:13,760
Meetup of the Year.

30
00:02:13,760 --> 00:02:19,280
Bring your thoughts on the top machine learning and AI stories of 2017 for our discussion

31
00:02:19,280 --> 00:02:20,600
segment.

32
00:02:20,600 --> 00:02:25,680
And for our main presentation, Bruno Gensalvez, we'll be presenting the paper Understanding

33
00:02:25,680 --> 00:02:32,680
Deep Learning requires rethinking generalization by Shi Yuan Zhang from MIT and Google Brain and

34
00:02:32,680 --> 00:02:34,000
others.

35
00:02:34,000 --> 00:02:40,160
You can find more details and register for the meetup at twimlai.com slash meetup.

36
00:02:40,160 --> 00:02:45,520
This AI Summit series is brought to you by our friends at IBM Power Systems.

37
00:02:45,520 --> 00:02:50,000
IBM Power Systems offers servers designed for mission critical applications and emerging

38
00:02:50,000 --> 00:02:55,120
workloads, including artificial intelligence, machine learning, deep learning, advanced

39
00:02:55,120 --> 00:02:58,360
analytics, and high performance computing.

40
00:02:58,360 --> 00:03:03,720
IBM Power Systems benefit from a wide range of open technologies, many stemming from

41
00:03:03,720 --> 00:03:08,800
collaboration with fellow Open Power Foundation members, and they're designed to deliver

42
00:03:08,800 --> 00:03:15,520
performance efficiently, whether deployed in private, public, or hybrid clouds.

43
00:03:15,520 --> 00:03:23,120
To learn more about the IBM Power System AC-922 platform for Enterprise AI, visit twimlai.com

44
00:03:23,120 --> 00:03:25,360
slash IBM Power.

45
00:03:25,360 --> 00:03:30,920
My guess for this first show in the AI Summit series is Hillary Hunter, IBM Fellow and Director

46
00:03:30,920 --> 00:03:36,920
of the Accelerated Cognitive Infrastructure Group at IBM's TJ Watson Research Center.

47
00:03:36,920 --> 00:03:41,000
Hillary and I met a few weeks back in New York City and I am really glad we were able to

48
00:03:41,000 --> 00:03:42,800
get around the show.

49
00:03:42,800 --> 00:03:47,000
Hillary joins us to discuss our team's research into distributed deep learning, which was

50
00:03:47,000 --> 00:03:53,720
recently released as the Power AI Distributed Deep Learning Communication Library, or DDL.

51
00:03:53,720 --> 00:03:58,200
In my conversation with Hillary, we discussed the purpose and technical architecture of the

52
00:03:58,200 --> 00:03:59,200
DDL.

53
00:03:59,200 --> 00:04:04,440
It's multi-ring topology, its ability to offer synchronous distributed training of deep

54
00:04:04,440 --> 00:04:07,320
learning models and much more.

55
00:04:07,320 --> 00:04:12,680
This is for sure a nerd alert pod, especially for the performance and hardware geeks among

56
00:04:12,680 --> 00:04:13,680
us.

57
00:04:13,680 --> 00:04:17,720
Be sure to post any feedback or questions you may have to the show notes page, which

58
00:04:17,720 --> 00:04:22,880
you'll find at twimlai.com slash talk slash 77.

59
00:04:22,880 --> 00:04:25,160
And now on to the show.

60
00:04:25,160 --> 00:04:35,400
All right, everyone, I am on the line with Hillary Hunter.

61
00:04:35,400 --> 00:04:42,240
Hillary is a IBM Fellow and Director of the Accelerated Cognitive Infrastructure Group

62
00:04:42,240 --> 00:04:45,040
at IBM's TJ Watson Research Center.

63
00:04:45,040 --> 00:04:47,880
Hillary, welcome to this week in Machine Learning and AI.

64
00:04:47,880 --> 00:04:48,880
Thank you so much.

65
00:04:48,880 --> 00:04:50,200
Very excited to be here.

66
00:04:50,200 --> 00:04:52,880
I'm excited to have you on the show as well.

67
00:04:52,880 --> 00:04:56,880
Folks won't know this until I tell them, but we had an opportunity to meet just a few

68
00:04:56,880 --> 00:05:04,580
weeks ago in New York City at the NYU at a reception held in conjunction with the NYU Future

69
00:05:04,580 --> 00:05:10,560
Labs event, and it was certainly great to meet you in person and even better to have

70
00:05:10,560 --> 00:05:14,840
an opportunity to get you on the line and dig into some of the work that you've been

71
00:05:14,840 --> 00:05:15,840
up to.

72
00:05:15,840 --> 00:05:20,080
Yeah, it was a pleasure meeting you there and it was certainly a really interesting event

73
00:05:20,080 --> 00:05:24,720
and a great opportunity to see some of the exciting things going on in the New York area

74
00:05:24,720 --> 00:05:28,200
and AI and AI is just exploding everywhere.

75
00:05:28,200 --> 00:05:30,000
But it's a pleasure to be here on your podcast.

76
00:05:30,000 --> 00:05:31,720
I look forward to our discussion.

77
00:05:31,720 --> 00:05:32,720
Awesome.

78
00:05:32,720 --> 00:05:37,520
So why don't we get started by having you tell us a little bit about how you, your background

79
00:05:37,520 --> 00:05:40,520
and how you got involved in artificial intelligence?

80
00:05:40,520 --> 00:05:42,360
Yeah, it's interesting.

81
00:05:42,360 --> 00:05:46,920
I like to say that AI really has exploded for two reasons.

82
00:05:46,920 --> 00:05:52,280
One being the amount of data, I'm going to especially publicly available data sets and

83
00:05:52,280 --> 00:05:55,080
the second being the compute.

84
00:05:55,080 --> 00:05:59,720
And I come from a background technically that's really a mix of both those things.

85
00:05:59,720 --> 00:06:04,720
And AI has been an opportunity to bring together a lot of different things that I've done

86
00:06:04,720 --> 00:06:08,880
in prior technical work and a lot of different things done by members of my team in kind of

87
00:06:08,880 --> 00:06:12,160
prior technical lives before getting into AI.

88
00:06:12,160 --> 00:06:15,720
So I come from a systems perspective, from a hardware perspective.

89
00:06:15,720 --> 00:06:21,320
I was an electrical engineer by training, and so, yep, I was a doubly.

90
00:06:21,320 --> 00:06:28,040
Yeah, so that background is kind of where I'm coming from and approaching these problems

91
00:06:28,040 --> 00:06:30,400
that we're trying to tackle in AI.

92
00:06:30,400 --> 00:06:36,160
And I bring to the table background in performance, the background in data movement in systems,

93
00:06:36,160 --> 00:06:41,680
and both are proving to be really fruitful for getting some of the grand challenge kind

94
00:06:41,680 --> 00:06:44,360
of scale problems done that we're facing today in AI.

95
00:06:44,360 --> 00:06:47,600
Awesome. And have you spent most of your career at IBM?

96
00:06:47,600 --> 00:06:48,600
I have.

97
00:06:48,600 --> 00:06:49,600
Yeah.

98
00:06:49,600 --> 00:06:55,480
So I got my PhD at University of Illinois and started at IBM in 2005, and I've been with

99
00:06:55,480 --> 00:06:57,320
IBM research since then.

100
00:06:57,320 --> 00:06:58,320
Fantastic.

101
00:06:58,320 --> 00:07:04,160
And have you been working on, in the same group, working on accelerated cognitive infrastructure

102
00:07:04,160 --> 00:07:08,160
or have you done kind of evolved to that, this particular position?

103
00:07:08,160 --> 00:07:09,160
Yeah.

104
00:07:09,160 --> 00:07:10,720
This position has evolved.

105
00:07:10,720 --> 00:07:16,280
You know, we really ramped up our efforts around accelerated computing a number of years

106
00:07:16,280 --> 00:07:17,280
ago.

107
00:07:17,280 --> 00:07:22,600
Prior to that, I was working on things related to processor design and memory technologies.

108
00:07:22,600 --> 00:07:26,040
And it's interesting because, again, the memory relates to the data movement, relates

109
00:07:26,040 --> 00:07:29,720
to, you know, feeding the AI, feeding the accelerator.

110
00:07:29,720 --> 00:07:32,560
So it's all kind of come together in a really nice way.

111
00:07:32,560 --> 00:07:33,560
Great.

112
00:07:33,560 --> 00:07:40,240
And your group recently published some really interesting research on essentially scaling

113
00:07:40,240 --> 00:07:45,600
deep learning performance using distributed, distributed techniques.

114
00:07:45,600 --> 00:07:50,200
That was one of the big things that I wanted to spend some time talking about in this interview.

115
00:07:50,200 --> 00:07:53,280
Can you give us an overview of that research?

116
00:07:53,280 --> 00:07:54,280
Yeah.

117
00:07:54,280 --> 00:07:57,720
We were really excited by what we were able to publish.

118
00:07:57,720 --> 00:08:03,960
We're calling it the Power AI DDL or Distributed Deep Learning Capability.

119
00:08:03,960 --> 00:08:09,560
And basically, what we showed is that we were able to create a framework independent.

120
00:08:09,560 --> 00:08:14,920
So independent from TensorFlow or Cafe or PyTorch or your favorite way of doing deep learning.

121
00:08:14,920 --> 00:08:19,360
We were able to create a framework independent communication library that achieves close

122
00:08:19,360 --> 00:08:24,760
to optimal, very, very close to optimal, up to 95% scaling efficiency.

123
00:08:24,760 --> 00:08:31,080
So what this meant for us is that we were able to use lots of GPUs together very, very

124
00:08:31,080 --> 00:08:32,400
efficiently.

125
00:08:32,400 --> 00:08:38,760
And we were able to use those mechanisms to train a neural network, to train a ResNet

126
00:08:38,760 --> 00:08:44,280
to the highest published accuracy on a really hard problem.

127
00:08:44,280 --> 00:08:49,840
And we were also able to beat what had been shown as a record of around an hour or 66 minutes

128
00:08:49,840 --> 00:08:52,480
on a smaller neural network and a smaller problem.

129
00:08:52,480 --> 00:08:57,280
And so we really were able to show that through hardware software integration, we were able

130
00:08:57,280 --> 00:09:00,840
to have world-class AI capabilities.

131
00:09:00,840 --> 00:09:07,440
And for us, this really kind of signaled a change in the productivity curve for deep learning

132
00:09:07,440 --> 00:09:13,880
because most of the open source today just kind of handles a single node worth of performance.

133
00:09:13,880 --> 00:09:15,200
So you're kind of stuck then.

134
00:09:15,200 --> 00:09:18,640
You can have two GPUs, four GPUs, maybe eight.

135
00:09:18,640 --> 00:09:24,040
But scaling out to many, many servers has been a really big challenge.

136
00:09:24,040 --> 00:09:27,480
And the more servers and the more hardware you can use for a problem, the faster you get

137
00:09:27,480 --> 00:09:29,920
that work done and the more productive you are.

138
00:09:29,920 --> 00:09:34,080
Now, that last point might be one that's worth underscoring.

139
00:09:34,080 --> 00:09:41,520
If you look at a framework like TensorFlow, there's, as part of the open source TensorFlow,

140
00:09:41,520 --> 00:09:43,960
there's a distributed TensorFlow.

141
00:09:43,960 --> 00:09:51,440
But that's more useful for scaling across GPUs than it is across servers, is that correct?

142
00:09:51,440 --> 00:09:57,720
So there is a version of distributed TensorFlow and what we really are kind of looking at

143
00:09:57,720 --> 00:10:05,320
in that versus the DDL capabilities is the extent of scalability and ultimately the

144
00:10:05,320 --> 00:10:06,680
productivity of the servers.

145
00:10:06,680 --> 00:10:14,600
So the default distributions in TensorFlow haven't been shown to be able to use as many

146
00:10:14,600 --> 00:10:20,760
as 256 GPUs as far as I'm aware and based on our own internal studies.

147
00:10:20,760 --> 00:10:26,400
So there are X factors to be had with every extra set of servers that you can add.

148
00:10:26,400 --> 00:10:28,560
You get the work done faster.

149
00:10:28,560 --> 00:10:32,720
And also we've shown that our communication patterns and communication overheads are

150
00:10:32,720 --> 00:10:33,960
really close to optimal.

151
00:10:33,960 --> 00:10:37,920
And so the other things that are out there appear to be less efficient.

152
00:10:37,920 --> 00:10:41,720
And so that means at the end, a longer time to solution.

153
00:10:41,720 --> 00:10:42,720
Okay.

154
00:10:42,720 --> 00:10:48,040
And the 256 GPUs that you are running was across how many systems?

155
00:10:48,040 --> 00:10:49,040
Yeah.

156
00:10:49,040 --> 00:10:56,360
So we ran across 64 servers and we were using the Pascal P100 GPUs.

157
00:10:56,360 --> 00:11:00,680
The work that we did was right before, right on the cusp of Volta coming out and miss

158
00:11:00,680 --> 00:11:03,240
the very latest GPUs.

159
00:11:03,240 --> 00:11:08,160
And I like to always kind of describe why this is a hard problem because the GPUs are so

160
00:11:08,160 --> 00:11:11,680
fast that they all learn very quickly.

161
00:11:11,680 --> 00:11:16,680
And so if you think of there being 256 kind of learners in the system, the hard part

162
00:11:16,680 --> 00:11:21,320
is keeping everybody synced up in that process of keeping them synced up.

163
00:11:21,320 --> 00:11:25,600
It's really critical that that be done with as low latency of a communication as possible.

164
00:11:25,600 --> 00:11:30,560
And that's really the core of the technology that we showed is that the communication time

165
00:11:30,560 --> 00:11:31,760
is really, really low.

166
00:11:31,760 --> 00:11:35,880
And that enables you then to do fully synchronous training, meaning everyone is updating everyone

167
00:11:35,880 --> 00:11:38,920
else with all the information that's being learned.

168
00:11:38,920 --> 00:11:42,920
All the weights are being updated as they should after every batch.

169
00:11:42,920 --> 00:11:47,560
And that means at the end that you're doing a type of deep learning that is opposed to

170
00:11:47,560 --> 00:11:51,160
asynchronous where they're kind of occasionally updating each other in order to lower the

171
00:11:51,160 --> 00:11:55,760
communication overhead, when you're able to do fully synchronous, you're able to kind

172
00:11:55,760 --> 00:12:01,240
of keep everything moving forward a little bit more predictably, predictably, sorry, and

173
00:12:01,240 --> 00:12:05,240
you're able to get a higher accuracy result at the end in general.

174
00:12:05,240 --> 00:12:06,240
Okay.

175
00:12:06,240 --> 00:12:12,080
And introducing this, you said it's called Power AI DDL for those who aren't familiar

176
00:12:12,080 --> 00:12:13,480
with Power AI.

177
00:12:13,480 --> 00:12:20,760
What is Power AI and to what extent are the results that you demonstrated here tied to

178
00:12:20,760 --> 00:12:23,480
that Power AI architecture?

179
00:12:23,480 --> 00:12:24,480
Yeah.

180
00:12:24,480 --> 00:12:29,240
So for us, this is very much an effort of hardware and software co-design.

181
00:12:29,240 --> 00:12:38,240
Power AI runs on the IBM SC822LC servers, sorry, actually, S822LC servers.

182
00:12:38,240 --> 00:12:43,040
And those servers have two power processors, and they have four GPUs.

183
00:12:43,040 --> 00:12:45,880
And everything is connected by doubled up NV links.

184
00:12:45,880 --> 00:12:54,040
So Nvidia has this high bandwidth interconnect capability, and we have that high bandwidth connectivity

185
00:12:54,040 --> 00:12:58,760
not just between the GPUs, but also back to the host processor.

186
00:12:58,760 --> 00:13:03,040
And so that provides extra performance in moving data and moving weight updates and everything

187
00:13:03,040 --> 00:13:04,800
in the system.

188
00:13:04,800 --> 00:13:10,800
And our communication library also leverages all of that bandwidth to its max and to its

189
00:13:10,800 --> 00:13:12,440
full potential.

190
00:13:12,440 --> 00:13:17,480
And so when we talk about this whole space of deep learning and what we're doing from

191
00:13:17,480 --> 00:13:22,680
a systems perspective and with things like DDL, we're talking about matching the software

192
00:13:22,680 --> 00:13:26,160
to fully utilize the hardware capabilities.

193
00:13:26,160 --> 00:13:31,160
And for us, this is also a storyline around collaboration between our research division

194
00:13:31,160 --> 00:13:38,240
that I actually report in and our product division, our development division, because we were

195
00:13:38,240 --> 00:13:45,440
able to actually put this code base out for IBM customers to try and download and try

196
00:13:45,440 --> 00:13:51,640
themselves, try DDL on their servers or on the cloud at the same time that we made the

197
00:13:51,640 --> 00:13:56,880
publication and the announcement of our leadership deep learning capabilities using this framework.

198
00:13:56,880 --> 00:14:02,400
So Power AI is a, it's a download and go set of distributions of frameworks that run

199
00:14:02,400 --> 00:14:05,000
on our accelerated servers.

200
00:14:05,000 --> 00:14:09,440
And we now have the distributed deep learning capabilities available there as well for customers

201
00:14:09,440 --> 00:14:10,760
to try themselves.

202
00:14:10,760 --> 00:14:16,560
So we really, from a research perspective, we're very excited about this because it means

203
00:14:16,560 --> 00:14:21,760
that we can kind of take this rarefied skillset of distributed deep learning, this kind of

204
00:14:21,760 --> 00:14:26,320
grand challenge thing everyone's competing over and put it in the hands of our customers

205
00:14:26,320 --> 00:14:31,160
to go ahead and try out and see what they can do with it on their data with their types

206
00:14:31,160 --> 00:14:32,320
and neural networks.

207
00:14:32,320 --> 00:14:41,000
And you mentioned that the results that you saw are framework independent is, is that

208
00:14:41,000 --> 00:14:48,440
true in the strictest sense or is it rather that the software that you wrote was written

209
00:14:48,440 --> 00:14:52,560
to adapt to some fixed number of frameworks?

210
00:14:52,560 --> 00:14:57,600
So it's pretty true in a strict sense, but let me define strict to make sure we're not

211
00:14:57,600 --> 00:14:59,720
talking past each other.

212
00:14:59,720 --> 00:15:04,440
So what DDL is is it's a communication library.

213
00:15:04,440 --> 00:15:07,520
So it lets things in a system talk to each other.

214
00:15:07,520 --> 00:15:13,120
We have been able to use that communication library across many different frameworks.

215
00:15:13,120 --> 00:15:17,520
What we released is already as part of Power AI was our integration into TensorFlow and

216
00:15:17,520 --> 00:15:22,400
into Cafe, but we also published results using our integration into Torch and we have

217
00:15:22,400 --> 00:15:28,640
other integrations that we have shown work just fine as well for our internal use currently.

218
00:15:28,640 --> 00:15:33,200
And so it we have shown I think at this point integration into enough frameworks that I'm

219
00:15:33,200 --> 00:15:37,280
pretty comfortable saying that, you know, this library can be integrated into pretty much

220
00:15:37,280 --> 00:15:38,280
anything that you write.

221
00:15:38,280 --> 00:15:39,280
Right, right.

222
00:15:39,280 --> 00:15:42,480
Yeah, I think in the context of my question, the answer is yes and yes, right?

223
00:15:42,480 --> 00:15:46,720
Yes, yes and yes, there we go.

224
00:15:46,720 --> 00:15:47,720
Interesting.

225
00:15:47,720 --> 00:15:54,880
And so how does how does this compare to some of the previous and even subsequent work like

226
00:15:54,880 --> 00:16:04,320
you refer to Facebook and Microsoft work in the paper or in the blog posts since I think

227
00:16:04,320 --> 00:16:10,760
since you posted this Uber published an open source project called Haravad that seeks

228
00:16:10,760 --> 00:16:12,840
to do something similar.

229
00:16:12,840 --> 00:16:14,840
Have you are you familiar with that one?

230
00:16:14,840 --> 00:16:16,640
Yeah, I am.

231
00:16:16,640 --> 00:16:21,800
And that's obviously a great, a great result that they put out there and we love seeing

232
00:16:21,800 --> 00:16:26,840
all the different efforts that are going on in this space because it really is such an

233
00:16:26,840 --> 00:16:29,520
important area for productivity.

234
00:16:29,520 --> 00:16:34,520
The Haravad team showed a great set of scaling, scaling experiments with their integration

235
00:16:34,520 --> 00:16:36,120
essentially of an MPI.

236
00:16:36,120 --> 00:16:38,480
It was open MPI, right?

237
00:16:38,480 --> 00:16:39,480
Mm-hmm.

238
00:16:39,480 --> 00:16:43,560
MPI reduced and the nickel libraries into TensorFlow.

239
00:16:43,560 --> 00:16:50,720
We do believe that our communication topologies are a bit better than what is there.

240
00:16:50,720 --> 00:16:57,160
And so the net scaling efficiency will be better and we look forward to being able to talk

241
00:16:57,160 --> 00:17:00,560
about that in a little bit more concrete detail pretty soon here.

242
00:17:00,560 --> 00:17:05,040
But I think, you know, it's great to see these different efforts toward distributed deep

243
00:17:05,040 --> 00:17:10,280
learning happening across different types of frameworks because ultimately the community

244
00:17:10,280 --> 00:17:15,880
needs to have this type of productivity in order for deep learning really to take off.

245
00:17:15,880 --> 00:17:21,760
For us though, the positioning at Power AI is really about taking open source and taking

246
00:17:21,760 --> 00:17:27,880
the complexity of managing it, of installing it, tuning up the performance, optimizing it

247
00:17:27,880 --> 00:17:33,600
for our systems, and then ultimately providing support on it for our customers.

248
00:17:33,600 --> 00:17:38,680
And so, you know, we love to see improvements in open source and much of what's provided

249
00:17:38,680 --> 00:17:42,160
in Power AI is open source and is based on open source.

250
00:17:42,160 --> 00:17:46,520
And then we're, you know, providing improvements on top of that and optimizations on top of

251
00:17:46,520 --> 00:17:47,520
that as well.

252
00:17:47,520 --> 00:17:52,480
So, we want to both take the time to get going with open source way down, which is the

253
00:17:52,480 --> 00:17:57,560
just download and go as well as we want to then be able to provide support and optimizations

254
00:17:57,560 --> 00:18:02,280
and improvements that are really significant on top of what's going on in the space.

255
00:18:02,280 --> 00:18:03,280
Mm-hmm.

256
00:18:03,280 --> 00:18:09,240
In terms of DDL, I'm curious if you can kind of walk through the next level of detail and

257
00:18:09,240 --> 00:18:15,160
why, you know, what are some of the architectural elements that you feel given an advantage

258
00:18:15,160 --> 00:18:18,880
relative to, you know, other things that one could do in general?

259
00:18:18,880 --> 00:18:20,360
Yeah, absolutely.

260
00:18:20,360 --> 00:18:22,960
So, there's two things.

261
00:18:22,960 --> 00:18:30,320
There's one, the advantages of the hardware, which are that the GPUs are double NV linked,

262
00:18:30,320 --> 00:18:35,200
connected to one another, and also double NV linked connected back to the host processor.

263
00:18:35,200 --> 00:18:40,280
Those features provide performance if you, you know, use them with an appropriate communication

264
00:18:40,280 --> 00:18:43,320
topology, which we do do with DDL.

265
00:18:43,320 --> 00:18:48,480
And in addition, then DDL at the overall system level, when you're talking about connecting

266
00:18:48,480 --> 00:18:53,720
together a bunch of different learners, a bunch of different system nodes, uses a multirang

267
00:18:53,720 --> 00:18:59,040
topology, not a single ring, but a multirang topology, and what this results in is our ability

268
00:18:59,040 --> 00:19:03,520
to use all of the links to the greatest advantage possible.

269
00:19:03,520 --> 00:19:08,240
If you do a naive mapping onto a system, or if you have a system with, you know, PCIe

270
00:19:08,240 --> 00:19:11,240
interconnect, there are going to be bottlenecks.

271
00:19:11,240 --> 00:19:16,120
What we do is kind of maneuver around the bottlenecks, and we use all the different bandwidths

272
00:19:16,120 --> 00:19:21,520
so the bandwidth between the GPUs, the bandwidth on a node, the bandwidth getting out to the network,

273
00:19:21,520 --> 00:19:22,520
etc.

274
00:19:22,520 --> 00:19:25,920
We use those to their best possible efficiency.

275
00:19:25,920 --> 00:19:29,240
So ideally, you know, you want to use all of the hardware and all the interconnect that

276
00:19:29,240 --> 00:19:32,240
you have, and software doesn't naturally do that.

277
00:19:32,240 --> 00:19:36,080
And so we've kind of taken all that work away from the developer and said, you know, we're

278
00:19:36,080 --> 00:19:37,560
going to max out the system.

279
00:19:37,560 --> 00:19:40,760
And if you buy the hardware, it's going to work really well because of the software

280
00:19:40,760 --> 00:19:45,960
and the way the software is using all the, all the, the system bandwidths.

281
00:19:45,960 --> 00:19:46,960
Hmm.

282
00:19:46,960 --> 00:19:53,160
When you talk about these rings, where in the system architecture do they exist, are these

283
00:19:53,160 --> 00:19:57,640
at the interconnect level, are they in memory, are they someplace else?

284
00:19:57,640 --> 00:20:01,240
So these are all within the different interconnects of the system.

285
00:20:01,240 --> 00:20:02,240
Okay.

286
00:20:02,240 --> 00:20:07,600
So the, they're part of the double NV link that you mentioned.

287
00:20:07,600 --> 00:20:08,600
Yep.

288
00:20:08,600 --> 00:20:13,440
A part of the double NV link, a part of the connections out in the network, part of the

289
00:20:13,440 --> 00:20:17,920
nodes being connected to each other with network connections, that kind of layer.

290
00:20:17,920 --> 00:20:19,960
You know, there's another thing we could talk about.

291
00:20:19,960 --> 00:20:20,960
You brought up memory.

292
00:20:20,960 --> 00:20:24,920
There's another thing that we could talk about, which is the large model support that

293
00:20:24,920 --> 00:20:28,160
we do, which is also a feature in Power AI.

294
00:20:28,160 --> 00:20:31,880
And that is a situation where you really want to try to use the different pieces of memory

295
00:20:31,880 --> 00:20:32,880
in the system.

296
00:20:32,880 --> 00:20:36,680
So, if you think of it philosophically, it would distribute a deep learning.

297
00:20:36,680 --> 00:20:41,280
We're using all of the, the links in the system, all the network bandwidth, all the bandwidth

298
00:20:41,280 --> 00:20:43,440
available on a given system.

299
00:20:43,440 --> 00:20:48,480
With large memory support, we're providing an out-of-core capability, meaning capability

300
00:20:48,480 --> 00:20:53,920
of, of accessing other resources, other memory in the system greater than just what the GPU

301
00:20:53,920 --> 00:20:55,440
has all by itself.

302
00:20:55,440 --> 00:20:59,480
And the way to think about that is, you know, if I have a system, the GPU has a small

303
00:20:59,480 --> 00:21:05,440
amount of memory today about 16 gigabytes, but the host processor has a lot more memory,

304
00:21:05,440 --> 00:21:11,240
128 gigabytes or 256 or up to a terabyte these days, right?

305
00:21:11,240 --> 00:21:16,880
So we provide function that enables people to explore bigger model sizes, bigger data

306
00:21:16,880 --> 00:21:18,720
sizes, etc.

307
00:21:18,720 --> 00:21:23,720
By accessing and using actively, all of the memory and not just the GPU's memory in the

308
00:21:23,720 --> 00:21:24,720
system.

309
00:21:24,720 --> 00:21:29,640
So, generally, I would say our philosophy is to kind of use all the resources available

310
00:21:29,640 --> 00:21:35,440
in the system, you know, and let the AI developer explore things as bigly, you know, as bigly,

311
00:21:35,440 --> 00:21:41,240
as largely, as largely, as large as they'd like to go, as big as they'd like to go.

312
00:21:41,240 --> 00:21:46,520
And that includes dimensions of both memory and the number of systems in that, you know,

313
00:21:46,520 --> 00:21:49,800
that you have to tie together.

314
00:21:49,800 --> 00:21:56,360
And we've talked about the kind of framework transparency, nature of this.

315
00:21:56,360 --> 00:22:03,360
Does that mean that all of the thinking that has to go into taking advantage of these

316
00:22:03,360 --> 00:22:10,720
elements happens at the DDL, you know, and or framework layer?

317
00:22:10,720 --> 00:22:16,840
And there's no, you know, no aspects of the, you know, the problem that has to be thought

318
00:22:16,840 --> 00:22:22,560
through by the developer or there's still things that the developer has to be aware of in

319
00:22:22,560 --> 00:22:26,280
order to be able to take advantage of what you've done here.

320
00:22:26,280 --> 00:22:30,480
Yeah, it's a, it's such an important point in productivity, right, because you can run

321
00:22:30,480 --> 00:22:31,480
as fast as possible.

322
00:22:31,480 --> 00:22:34,480
But if you're going to sit there for multiple days, scratching your head, trying to figure

323
00:22:34,480 --> 00:22:38,600
out how to run fast, it doesn't do anybody any good, right?

324
00:22:38,600 --> 00:22:43,360
So, so one of the, one of the decisions that we made when we did the integration, for

325
00:22:43,360 --> 00:22:48,720
example, into TensorFlow was to leverage the slim library, which is a library that kind

326
00:22:48,720 --> 00:22:52,600
of creates the capability to run a particular neural network.

327
00:22:52,600 --> 00:22:58,800
So in that case, we have hidden the DDL capabilities completely from the developer and they just

328
00:22:58,800 --> 00:23:00,360
have to use slim.

329
00:23:00,360 --> 00:23:04,920
So we're trying to hide under in general where we can abstractions that have been created

330
00:23:04,920 --> 00:23:08,920
at higher levels so that people don't have to, you know, spend days and weeks trying

331
00:23:08,920 --> 00:23:11,880
to figure out how to, how to use this technology.

332
00:23:11,880 --> 00:23:16,280
I think we're, we're very focused on, you know, developer productivity was part of the

333
00:23:16,280 --> 00:23:20,360
reason why we, you know, went from, you know, research endeavor to very quick engagement

334
00:23:20,360 --> 00:23:24,280
with our development team and wanted to get it out there in the hands of people very

335
00:23:24,280 --> 00:23:29,160
quickly and why we're focused on speed because speed, you know, this is one of the very

336
00:23:29,160 --> 00:23:33,200
few areas of computing today where people sit around waiting for days, you know, to develop

337
00:23:33,200 --> 00:23:34,200
the capability.

338
00:23:34,200 --> 00:23:35,200
It's really kind of crazy.

339
00:23:35,200 --> 00:23:41,000
So we want to like go from days down to hours and we want to, you know, get people there

340
00:23:41,000 --> 00:23:45,520
as quickly as possible through downloading, go and then also through use of some of these

341
00:23:45,520 --> 00:23:49,440
higher level abstractions like slim, okay.

342
00:23:49,440 --> 00:23:55,000
One of the, one of the critiques, I guess, of some deep learning work is that, you know,

343
00:23:55,000 --> 00:23:59,240
you hear it described as kind of overfitting on image net and you reference image net

344
00:23:59,240 --> 00:24:01,040
in your results as well.

345
00:24:01,040 --> 00:24:06,200
Do you, you know, what gives you confidence that these results will be generally applicable

346
00:24:06,200 --> 00:24:10,280
beyond, you know, a specific, you know, data set and problem?

347
00:24:10,280 --> 00:24:11,280
Yeah.

348
00:24:11,280 --> 00:24:12,280
That's it.

349
00:24:12,280 --> 00:24:16,360
It's a, it's a great thing to talk about because it gets to why we are so passionate about

350
00:24:16,360 --> 00:24:21,880
the key thing here being that we were showing the capabilities, what you can do with the

351
00:24:21,880 --> 00:24:25,800
framework, you know, only that that's the only thing that it does, right?

352
00:24:25,800 --> 00:24:30,800
So, so as you start to look at other classes of neural networks, the kind of computation

353
00:24:30,800 --> 00:24:36,840
to communication balance changes and some classes in neural networks are known not to scale

354
00:24:36,840 --> 00:24:41,880
very far today, not, you know, known, you know, kind of they won't run it to 256 GPUs,

355
00:24:41,880 --> 00:24:44,600
they run it maybe a small number of GPUs.

356
00:24:44,600 --> 00:24:51,920
So what we see though is that if you're using the type of really close to optimal communication

357
00:24:51,920 --> 00:24:57,840
methods that we have that no matter what the kind of inherent capability of that neural

358
00:24:57,840 --> 00:25:02,160
network to scale is going to be, we're going to guarantee that you get, you know, out

359
00:25:02,160 --> 00:25:04,960
to as much scale as is possible.

360
00:25:04,960 --> 00:25:09,080
So if you're using a suboptimal communication method, you know, maybe you can only get

361
00:25:09,080 --> 00:25:14,040
to 8 or 10 GPUs for other classes of neural networks, we're going to push that number

362
00:25:14,040 --> 00:25:19,200
up by getting the communication latency to be as absolutely short as possible.

363
00:25:19,200 --> 00:25:24,200
So you know, in our view, this is really about, you know, whatever the current state is

364
00:25:24,200 --> 00:25:28,800
of a neural network and the scientific understanding of it and the ability to scale, we're going

365
00:25:28,800 --> 00:25:32,440
to push that out and get that training done, done faster.

366
00:25:32,440 --> 00:25:33,440
Awesome.

367
00:25:33,440 --> 00:25:34,440
Awesome.

368
00:25:34,440 --> 00:25:37,280
What other things are you working on in your group?

369
00:25:37,280 --> 00:25:38,280
Yeah.

370
00:25:38,280 --> 00:25:43,760
So, you know, I mentioned the large model support, which is something that, again, like I said,

371
00:25:43,760 --> 00:25:49,400
comes from the same perspective of wanting to use the available system resource and not

372
00:25:49,400 --> 00:25:55,880
just sort of, you know, offloading work to a GPU and being constrained by what that provides.

373
00:25:55,880 --> 00:26:00,800
So we want to go multi-GPU, we want to go multi-system, we also want to be able to leverage

374
00:26:00,800 --> 00:26:04,760
all of the capability of a system, the full system's memory capacity.

375
00:26:04,760 --> 00:26:09,760
You know, we are, in general, along those lines also looking at acceleration in the machine

376
00:26:09,760 --> 00:26:14,960
learning space, and we have some work that we've, you know, published and shown some really

377
00:26:14,960 --> 00:26:19,880
great results on algorithms that are a bit more chatty between the GPU and the CPU and

378
00:26:19,880 --> 00:26:25,640
how they leverage that NV link capability, that fat bandwidth pipe of communication between

379
00:26:25,640 --> 00:26:31,080
the CPU and GPU and how they kind of get in and out of memory and, you know, use the larger

380
00:26:31,080 --> 00:26:32,920
capability of the CPU memory.

381
00:26:32,920 --> 00:26:36,920
And so those are a couple of different examples, but we're really trying to kind of use the

382
00:26:36,920 --> 00:26:42,400
system to the max of its capability and tackle problems faster and enable people to explore

383
00:26:42,400 --> 00:26:44,920
problems that are larger.

384
00:26:44,920 --> 00:26:50,320
What's an example or some examples of, you know, particularly chatty machine learning

385
00:26:50,320 --> 00:26:51,640
tasks or libraries?

386
00:26:51,640 --> 00:26:58,120
Yeah, so in general, things like nearest neighbor computations, word-to-back technology

387
00:26:58,120 --> 00:27:03,560
or glove, things that are used in semantic analysis, those are a couple of different types

388
00:27:03,560 --> 00:27:09,280
of, you know, non-very deep neural network kind of things.

389
00:27:09,280 --> 00:27:16,200
And the chattyness relates to, in those network architecture is kind of the way that state

390
00:27:16,200 --> 00:27:18,080
needs to be shuffled around.

391
00:27:18,080 --> 00:27:23,880
Yeah, so it relates to how state needs to be shuffled around, but it also relates to

392
00:27:23,880 --> 00:27:28,560
how quickly the compute happens and, you know, how quickly the GPU needs to be supplied

393
00:27:28,560 --> 00:27:30,960
with more data, for example.

394
00:27:30,960 --> 00:27:33,600
So that's another big component in it.

395
00:27:33,600 --> 00:27:40,360
The faster the compute on the GPU happens, the more likely it is that you need to, you

396
00:27:40,360 --> 00:27:45,640
know, be feeding it data more quickly and getting that data fed to it more quickly can help

397
00:27:45,640 --> 00:27:50,640
it just kind of be brick-walled in its execution time and not sitting around waiting for, you

398
00:27:50,640 --> 00:27:53,280
know, the next data to get to it.

399
00:27:53,280 --> 00:28:00,440
And sort of actually profiling the execution of your, you know, your training jobs.

400
00:28:00,440 --> 00:28:05,120
How do you, you know, how do you develop an intuition for what's going to be chatty?

401
00:28:05,120 --> 00:28:09,880
Like, it's, it doesn't sound like it's just the depth of the network and the deep

402
00:28:09,880 --> 00:28:16,040
it is the, you know, the harder it is or is it, you know, or there are there, you know,

403
00:28:16,040 --> 00:28:17,040
is it, you know, is it with?

404
00:28:17,040 --> 00:28:18,040
Is it something else?

405
00:28:18,040 --> 00:28:20,520
Is it kind of, you know, memory features?

406
00:28:20,520 --> 00:28:25,600
Is it, are there things that, you know, you can look for it to get a sense for the kind

407
00:28:25,600 --> 00:28:28,520
of the chattyness of your network architecture?

408
00:28:28,520 --> 00:28:33,520
We like to talk about kind of the algebra of how deep learning happens or the pipeline

409
00:28:33,520 --> 00:28:34,920
stages.

410
00:28:34,920 --> 00:28:40,880
And we look at, you know, how much data is needed in order to start a computation on the

411
00:28:40,880 --> 00:28:41,880
GPU?

412
00:28:41,880 --> 00:28:47,320
How long does it take to move that data from, you know, the storage and then into the

413
00:28:47,320 --> 00:28:48,800
GPU?

414
00:28:48,800 --> 00:28:53,640
And then how long does the GPU take to communicate, sorry, to compute?

415
00:28:53,640 --> 00:28:55,360
How long does the GPU take to compute?

416
00:28:55,360 --> 00:28:59,200
And then how long does it take to communicate results, either back to the CPU or back to

417
00:28:59,200 --> 00:29:00,800
other GPUs?

418
00:29:00,800 --> 00:29:03,640
So those are kind of the canonical pieces.

419
00:29:03,640 --> 00:29:07,960
And so we look at, you know, how much compute is there and how long will that take?

420
00:29:07,960 --> 00:29:10,680
And then we look at how much data has to be moved.

421
00:29:10,680 --> 00:29:15,520
And we look at those as clear, you know, kind of pipeline stages or phases of the overall

422
00:29:15,520 --> 00:29:16,960
work you're trying to get done.

423
00:29:16,960 --> 00:29:24,880
But it sounds like those are, again, kind of empirical observations of a training task

424
00:29:24,880 --> 00:29:30,400
as opposed to, you know, being able to look at a picture of a network and tell by some

425
00:29:30,400 --> 00:29:35,920
characteristic of the network that, oh, this is probably going to be, you know, chatty

426
00:29:35,920 --> 00:29:38,280
and this kind of technology will apply well.

427
00:29:38,280 --> 00:29:40,080
Yeah, I mean, it's a good question.

428
00:29:40,080 --> 00:29:46,160
I mean, I think that we do have the ability to estimate pretty well based on the characteristics

429
00:29:46,160 --> 00:29:48,240
of the neural network and its data extents.

430
00:29:48,240 --> 00:29:53,800
I think it's, you know, it's definitely a mix of observation and the empirical characteristics.

431
00:29:53,800 --> 00:29:56,480
But, you know, in general, from from looking at the neural network, you should be able

432
00:29:56,480 --> 00:30:01,160
to figure out the, you know, volume of data, for example, that that needs to be moved

433
00:30:01,160 --> 00:30:06,080
to do weight updates, you know, how many nodes are in it and all that other kind of stuff.

434
00:30:06,080 --> 00:30:10,600
And there is, you know, some, you know, you have to figure out what the mini batch sizes

435
00:30:10,600 --> 00:30:12,600
are that are appropriate for that.

436
00:30:12,600 --> 00:30:17,240
So it's probably, I guess the, it's probably a mix of some things that can be determined,

437
00:30:17,240 --> 00:30:20,680
but then other things that you have to know what the training characteristics of that

438
00:30:20,680 --> 00:30:24,640
are and some of that is still somewhat experimental today.

439
00:30:24,640 --> 00:30:25,640
Okay.

440
00:30:25,640 --> 00:30:32,840
So what are you, you know, when you think about the, you know, this kind of work and all

441
00:30:32,840 --> 00:30:36,320
of the, you know, you're working on it, there are other folks working on it.

442
00:30:36,320 --> 00:30:43,400
Like, what do you see is the impact overall, you know, of it and, you know, what, you

443
00:30:43,400 --> 00:30:44,960
know, what's the, what's the time frame?

444
00:30:44,960 --> 00:30:50,120
Do you have a kind of a crystal ball vision around, you know, how this, you know, impacts

445
00:30:50,120 --> 00:30:53,160
the way folks develop deep learning models?

446
00:30:53,160 --> 00:30:59,480
Yeah, you know, it's, it's interesting because I think that there's been a lot of, you

447
00:30:59,480 --> 00:31:04,840
know, concern, as you mentioned, that, you know, folks are overfitting image net.

448
00:31:04,840 --> 00:31:09,280
And the question really is the rate and pace that deep learning will take off on other

449
00:31:09,280 --> 00:31:10,280
types of data.

450
00:31:10,280 --> 00:31:15,400
I mean, it's been proven as highly successful for image and speech, but we're seeing so

451
00:31:15,400 --> 00:31:17,280
many other use cases happen.

452
00:31:17,280 --> 00:31:21,960
You know, we're seeing use cases across, you know, risk and fraud and predictions and

453
00:31:21,960 --> 00:31:27,720
forecasting and lots of different areas that, you know, to some extent, the same thing is

454
00:31:27,720 --> 00:31:31,920
happening where there are classical machine learning techniques that are being subsumed

455
00:31:31,920 --> 00:31:36,160
by the capabilities and sort of the ease of use of deep learning.

456
00:31:36,160 --> 00:31:42,640
And the thing that excites me in this, in this capability conversation around DDL is

457
00:31:42,640 --> 00:31:49,160
the thought that with this type of speed up that we will see those fields and those use

458
00:31:49,160 --> 00:31:54,960
cases develop much more quickly because with a more productive system and a more productive

459
00:31:54,960 --> 00:32:00,360
hardware and software solution, people will be able to discover and explore their data

460
00:32:00,360 --> 00:32:04,000
and define the right models for those data sets more quickly, right?

461
00:32:04,000 --> 00:32:09,080
So if, if, you know, if you are able to get through more of your data more quickly and

462
00:32:09,080 --> 00:32:14,520
you are able to turn through more models and get to higher accuracy on these new types

463
00:32:14,520 --> 00:32:18,920
of outcomes that you're looking for around like I said, you know, risk and fraud and predictions

464
00:32:18,920 --> 00:32:23,600
and forecasting and those things, then those fields will develop and mature and those use

465
00:32:23,600 --> 00:32:26,920
cases will develop mature that much more quickly, right?

466
00:32:26,920 --> 00:32:33,280
And so, you know, this as an inflection point and kind of the rate and pace of enterprises

467
00:32:33,280 --> 00:32:38,520
ultimately, you know, to apply deep learning and increase their confidence in it and increase

468
00:32:38,520 --> 00:32:43,080
the accuracy and things that aren't just images and speech is ultimately what we're really

469
00:32:43,080 --> 00:32:44,080
excited about.

470
00:32:44,080 --> 00:32:47,800
And again, that, you know, that goes back to why we wanted to get it out into the hands

471
00:32:47,800 --> 00:32:53,080
of customers because as a productivity enabler, and we hope that it will change that rate

472
00:32:53,080 --> 00:32:58,040
and pace of, you know, adoption of these techniques and a productivity of data scientists

473
00:32:58,040 --> 00:33:00,600
teams working on their own data sets.

474
00:33:00,600 --> 00:33:07,240
Yeah, I think that's a really important point and one that I hear a lot in terms of,

475
00:33:07,240 --> 00:33:12,440
you know, deep learning becoming deeply, you know, pun intended, I guess associated with

476
00:33:12,440 --> 00:33:18,880
kind of image types of data, but there being these much broader applications and implications,

477
00:33:18,880 --> 00:33:25,120
are there any particular use cases that, you know, you think of as, you know, kind of

478
00:33:25,120 --> 00:33:28,080
the next big killer app for deep learning?

479
00:33:28,080 --> 00:33:32,920
You know, I think it's hard to necessarily forecast the next big killer app.

480
00:33:32,920 --> 00:33:38,080
I mean, I would say that, you know, we're seeing more than you might imagine use cases across

481
00:33:38,080 --> 00:33:43,640
enterprise environments of even speech and image because people want to be able to interact

482
00:33:43,640 --> 00:33:46,240
more, interact with us in a more natural way and such.

483
00:33:46,240 --> 00:33:51,480
So I would say that, you know, kind of two things, one, the speech and image, I think, is

484
00:33:51,480 --> 00:33:56,520
taking off and in other industries that have other data types, but that want to use more

485
00:33:56,520 --> 00:33:59,400
natural interaction capabilities and such.

486
00:33:59,400 --> 00:34:04,360
And so that's one thing that, you know, is great to see is the little bit more creative

487
00:34:04,360 --> 00:34:09,880
use, you know, in other contexts, especially as relates to, you know, interacting with

488
00:34:09,880 --> 00:34:10,880
people.

489
00:34:10,880 --> 00:34:15,360
The, you know, beyond just, you know, talking to your mobile phone, for example, right?

490
00:34:15,360 --> 00:34:17,040
So that's one thing.

491
00:34:17,040 --> 00:34:20,480
And then I think, you know, there's a lot of discovery and exploration right now.

492
00:34:20,480 --> 00:34:23,960
I think it's really hard to make a call is to, you know, what the next killer app and

493
00:34:23,960 --> 00:34:25,200
what the next data set is.

494
00:34:25,200 --> 00:34:30,640
But I think we are seeing a good degree of productivity on a lot of different types of data.

495
00:34:30,640 --> 00:34:34,560
And, you know, they're very interesting things happening as well in, you know, automation

496
00:34:34,560 --> 00:34:35,560
techniques.

497
00:34:35,560 --> 00:34:39,400
We have a, we have a tool set that helps to label data and other things like that.

498
00:34:39,400 --> 00:34:44,400
So as, as more folks adopt those techniques, you know, they'll be able to kind of get access

499
00:34:44,400 --> 00:34:47,120
to use of deep learning on more data sets.

500
00:34:47,120 --> 00:34:52,400
So as we use automation and, and tool sets like we have an AI vision to enable more quick

501
00:34:52,400 --> 00:34:57,760
labeling of data, then, you know, that really should also, you know, help accelerate the

502
00:34:57,760 --> 00:35:00,760
rate at which people enter into these new spaces.

503
00:35:00,760 --> 00:35:01,760
Hmm.

504
00:35:01,760 --> 00:35:04,840
The tool set you mentioned that helps with data annotation.

505
00:35:04,840 --> 00:35:06,480
Is that open source?

506
00:35:06,480 --> 00:35:11,160
It's not open source, but it's also available through the Power AI frameworks and folks can

507
00:35:11,160 --> 00:35:14,200
try it on the Nibbix cloud.

508
00:35:14,200 --> 00:35:17,480
They have the Power AI stuff available there.

509
00:35:17,480 --> 00:35:19,760
So yeah, that's, that's available for, for people to try.

510
00:35:19,760 --> 00:35:25,440
Yeah, it seems like that is also, you know, that clearly comes up constantly in these kind

511
00:35:25,440 --> 00:35:31,440
of conversations and, you know, when I think about the, you know, the full life cycle of,

512
00:35:31,440 --> 00:35:37,560
you know, these AI deep learning development projects, you know, there's certainly a lot

513
00:35:37,560 --> 00:35:43,320
of emphasis on the deep learning framework and the model development and training.

514
00:35:43,320 --> 00:35:48,520
But, you know, there's a much broader set of activities that has to take place and

515
00:35:48,520 --> 00:35:55,240
very, you know, we're just starting to see projects come on, you know, open source projects

516
00:35:55,240 --> 00:36:01,080
come online to provide, you know, a broader platform for handling all that.

517
00:36:01,080 --> 00:36:07,960
Like the data annotation is one big area, kind of model life cycle and versioning and,

518
00:36:07,960 --> 00:36:12,800
you know, production, performance management and tracking is another, you know, whole set

519
00:36:12,800 --> 00:36:19,080
of areas, you know, any thoughts on how all this stuff evolves?

520
00:36:19,080 --> 00:36:26,080
Yeah, I think that the, this is about maturity of the field of deep learning and it's about,

521
00:36:26,080 --> 00:36:30,920
you know, people moving past just fully labeled public data sets that they, you know, get

522
00:36:30,920 --> 00:36:32,320
online, right?

523
00:36:32,320 --> 00:36:39,640
And so I think, you know, the overall topic of management, of management of, you know,

524
00:36:39,640 --> 00:36:44,760
what models are being done when we're in how overall life cycle, integration with various

525
00:36:44,760 --> 00:36:50,600
data sources, all those kind of things are what we see as being necessary from an enterprise

526
00:36:50,600 --> 00:36:56,600
environment perspective because people will be, you know, wanting to improve the AI specific

527
00:36:56,600 --> 00:37:00,040
to their use cases and specific to their, to their data.

528
00:37:00,040 --> 00:37:05,200
So we have a good number of IBM tool sets out there today, again, folks can try them.

529
00:37:05,200 --> 00:37:10,080
But the data science experience is a really nice Jupyter Notebooks based environment that

530
00:37:10,080 --> 00:37:12,640
has a lot of the features we just discussed.

531
00:37:12,640 --> 00:37:17,080
You know, we also have other tool sets that are available that help with, you know, data

532
00:37:17,080 --> 00:37:21,720
integration from various sources through our spectrum family, for example.

533
00:37:21,720 --> 00:37:26,560
And then in, you know, within Power AI, you know, we have that integrated as well with,

534
00:37:26,560 --> 00:37:30,920
with the data science experience, the DSX that I just mentioned, and we have additional

535
00:37:30,920 --> 00:37:37,080
capabilities that include the, you know, sort of clicker data scientist type capabilities,

536
00:37:37,080 --> 00:37:40,520
including you're on data labeling and choosing your models and, you know, starting with some

537
00:37:40,520 --> 00:37:42,600
default models, other things like that.

538
00:37:42,600 --> 00:37:48,120
So we see these things, you know, that you mentioned as, you know, absolutely essential

539
00:37:48,120 --> 00:37:53,800
to do, you know, kind of more mature deep learning, which is, you know, going beyond,

540
00:37:53,800 --> 00:37:57,920
you know, just the, you know, the fully-lated label data sets that we have in the public

541
00:37:57,920 --> 00:37:58,920
domain.

542
00:37:58,920 --> 00:38:06,640
Right. And certainly enterprises need to, you know, need to get there in terms of maturity,

543
00:38:06,640 --> 00:38:09,880
you know, perhaps to kind of wrap things up.

544
00:38:09,880 --> 00:38:15,360
Any words for enterprises that are earlier on in the cycle and just getting started?

545
00:38:15,360 --> 00:38:22,680
Yeah, I think that one of the things that I always like to try to clarify in these discussions

546
00:38:22,680 --> 00:38:29,160
is that, you know, our intention in showing distributed deep learning capabilities at

547
00:38:29,160 --> 00:38:34,120
256GPUs was to show the capability of what you can get if you do hardware and software

548
00:38:34,120 --> 00:38:38,800
optimization, to show the flexibility across frameworks of what we have created, et cetera,

549
00:38:38,800 --> 00:38:40,880
and put it in people's hands.

550
00:38:40,880 --> 00:38:46,760
But everything that we did was done with kind of that enterprise consumer in mind, knowing

551
00:38:46,760 --> 00:38:53,080
that many, you know, may also be early in their journey. And so the, the DDL environment

552
00:38:53,080 --> 00:38:57,880
supports any number of notes, any number of systems that someone would like to use it

553
00:38:57,880 --> 00:39:02,200
on. It supports different types of networks. You don't have to use what we published our

554
00:39:02,200 --> 00:39:07,000
results on. And so, you know, what we really want to do is to kind of start with where people

555
00:39:07,000 --> 00:39:11,080
are at, start with the data volume that they have, start with the, you know, neural network

556
00:39:11,080 --> 00:39:15,680
capabilities that they have, and provide that ability to grow over time, you know, as they

557
00:39:15,680 --> 00:39:19,800
hit that stride of productivity of really figuring out how they want to do their deep learning

558
00:39:19,800 --> 00:39:24,480
and they want to apply it to the rest of their data sets, as they discover new areas within

559
00:39:24,480 --> 00:39:28,160
their enterprise that they want to apply deep learning and replace classical techniques.

560
00:39:28,160 --> 00:39:33,000
Perhaps they can grow out, they can scale out their deep learning environment, add more

561
00:39:33,000 --> 00:39:38,280
systems to it, and still get that, you know, kind of productivity. So I think, you know,

562
00:39:38,280 --> 00:39:41,800
that's, that's always kind of important to point out because it's not just the taking

563
00:39:41,800 --> 00:39:46,120
the really long day-as-long jobs down to hours for use of a huge system. It's that no

564
00:39:46,120 --> 00:39:50,760
matter what the system size is, you know, no matter what the data set size is, we want

565
00:39:50,760 --> 00:39:55,160
to try to meet people where they're at and provide, you know, this flexibility and elasticity

566
00:39:55,160 --> 00:40:00,920
and capability. Great, great. Well, hello, thank you so much. I really appreciate you taking

567
00:40:00,920 --> 00:40:06,520
the time to chat with us. I learned a ton about what you're doing with DDL and I'm looking

568
00:40:06,520 --> 00:40:12,440
forward to following the effort. It was a pleasure talking to you. Thanks so much for the opportunity.

569
00:40:14,280 --> 00:40:20,840
Alright, everyone, that's our show for today. Thanks so much for listening and for your continued

570
00:40:20,840 --> 00:40:26,920
feedback and support. For more information on Hillary or any of the topics covered in this

571
00:40:26,920 --> 00:40:35,240
episode, head on over to twomlai.com slash talk slash 77. To follow along with this AI Summit

572
00:40:35,240 --> 00:40:41,960
series, visit twomlai.com slash AI Summit. Of course, you're encouraged to send along your

573
00:40:41,960 --> 00:40:48,040
feedback or questions to us by leaving a comment right on the show notes page or via Twitter

574
00:40:48,040 --> 00:40:55,560
to at twomlai or at Sam Charrington. Thanks again to IBM Power for their support of this series.

575
00:40:55,560 --> 00:41:02,760
For more about the IBM Power Systems platform for Enterprise AI, visit twomlai.com slash IBM Power.

576
00:41:02,760 --> 00:41:12,760
Thanks once again for listening and catch you next time.

