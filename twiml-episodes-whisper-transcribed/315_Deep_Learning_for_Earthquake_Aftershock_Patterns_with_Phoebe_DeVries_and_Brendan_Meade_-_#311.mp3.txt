Welcome to the Twimal AI Podcast.
I'm your host Sam Charrington.
Have you been enjoying our Twamokon coverage, but want more?
Twamokon video packages are now available for advanced purchase over at twamokon.com
slash videos.
The package features over 13 hours of content, including all the awesome keynotes and panels
you've heard on the podcast, all of the breakout sessions, including 9K study and 8 tech track
sessions, as well as the highly regarded team tear down panels with Airbnb and SurveyMonkey.
Again, visit twamokon.com slash videos for more info.
And now on to the show.
Alright everyone, I am on the line with Phoebe DeVries and Brendan Mead.
Phoebe is a postdoctorate fellow with the Department of Earth and Planetary Sciences
at Harvard, as well as an assistant faculty at the University of Connecticut.
And Brendan is professor of Earth and Planetary Sciences at Harvard, and an affiliate faculty
in computer science as well.
Phoebe and Brendan, welcome to this week in machine learning and AI.
Thanks so much for having us Sam.
Thank you.
Let's get started by having the two of you introduce yourselves.
Brendan wanted to go first.
Thanks Sam.
Yeah, I'm a professor of Earth and Planetary Sciences at Harvard, where I've been for the
past 13 years prior to that.
I got undergraduate degree in history of science from Johns Hopkins University, and I got
my PhD in Earth and Planetary Sciences from MIT.
And here at Harvard, we do a lot of work on computational science with a particular
emphasis on understanding earthquakes.
Fantastic.
And Phoebe?
Yeah.
Well, I did my undergrad at Harvard and applied math.
And then I spent a year at Cambridge doing a master's degree in glaciology, and then came
back to work in Brendan's research group for my PhD.
And now I'm a postdoc with him.
And for most of my PhD, we worked on sort of large-scale forward modeling problems of
time-dependent stress changes after large earthquakes, but now we're really focusing
on machine learning for earthquake science.
So Brendan, the description of your lab is that you are focusing on deconvolving, tectonic
and earthquake cycle signals across Japanese islands to identify the coupled, subduction
zone interface that ruptured during the Great to Hoku-Oki earthquake of 2011.
Yeah, that's true.
That's certainly something that we worked on.
Let me tell you how we got into this problem.
Okay.
And our lab, what we've been really interested in, is trying to say as much as we can about
earthquakes before they happen.
And one of the great revolutions of the past 20 to 25 years now has been the ability to
measure using very high precision GPS measurements how the earth's surface moves at scales where
we can actually see the strain accumulation in the Earth's crust, the elastic strain that
will be released in a future-grade earthquake.
And so for a long time, our goal has been to try and map this type of behavior around
the globe, and try and use this information to say something about the future sizes and
locations of large earthquakes.
To do this, we have employed a lot of HPC approaches ranging from visco-elastic modeling to boundary
element modeling, and more recently, our interests have drifted more towards the more direct
applications of ML and AI to these problems where there are vast data sets waiting to be
explored.
And that's what got us to the paper we wrote recently.
Awesome.
And Phoebe, you were the lead author on this paper, which was called Deep Learning of
Aftershock Patterns Following Large Earthquakes.
Tell us about the paper.
What was the main thrust of the work?
Well, we were interested in Aftershock's from a machine learning perspective, or Aftershock
locations specifically, because there are these very well-established empirical relationships
that can describe the time decay of Aftershock frequency, and also the likely maximum magnitude
of Aftershocks as well.
But the locations of Aftershocks are a lot more difficult to explain.
So that's really what got us interested in this problem from a machine learning perspective.
There are a lot of data about Aftershocks.
So it's really a sort of good, you know, it's a very good application for machine learning
just off the bat.
What are some of the available data sources?
Is it primarily the GPS data that Brendan was describing?
Yes, there are a lot of earthquake catalogs, just records of where and when earthquakes
have occurred, just thousands and thousands of them.
And what we did in this study was we combined data from two separate catalogs.
One was a catalog of very large earthquakes, most of them larger than six or so.
And the other was a catalog of Aftershock's following those two.
And it was the combination of those two catalogs that enabled us to analyze the relationship
between the main shocks themselves and the Aftershocks that followed.
And is there, is it a challenge to differentiate earthquakes from Aftershocks?
Is that an issue or not really?
That is discussed a lot in the literature.
We decided to take a very sort of broad or simple approach to it.
So we just defined Aftershocks as the earthquakes that take place sort of within a year after
these large main shocks and within 100 kilometers horizontally and down to 50 kilometers depth,
just to keep things simple for this kind of first approach, first try.
Yeah, I think Phoebe is exactly right.
In the literature, you can find arguments that Aftershocks are extremely well-defined phenomena.
And you can find arguments that Aftershocks are nothing more than another part of a generic
earthquake sequence that has been ongoing for hundreds of not thousands of years.
And I absolutely agree with Phoebe that we took a very simplified approach to classifying
them on this study.
You mentioned that your lab has previously spent a lot of time on high-performance computing
based analysis of these earthquakes and Aftershocks.
I'm curious if you can describe that a little bit more as a segue to talking about what's
been new and working with machine learning based approaches.
Phoebe, do you want to talk about your thesis?
Well, I could talk about the visco-elastic modeling piece of what Brendan's group has been
doing.
So we implemented a three-dimensional code to calculate time-dependent stress changes
in the crust after large earthquakes due to visco-elastic relaxation.
And we were using these models to try to get at questions of whether or not or look at
questions of possible delayed earthquake triggering.
One of the applications we are one of the examples that we looked at was the North Anatolian
fault in Turkey where there's been this remarkable sequence of large earthquakes that have
just been marching to the west over time since 1939.
And so we used these visco-elastic models to try to look at what effects visco-elastic
relaxation may have had in terms of loading the hypersenters of the subsequent earthquakes
in the sequence.
So not to get too deep into the science, but this visco-elastic study is essentially modeling
the Earth or maybe the surface of the Earth as kind of using fluid dynamics types of
approaches to try to anticipate earthquakes and aftershocks?
Yeah, so it's a quasi-static model.
So we used Berger's realities to model the behavior of the lower crust and upper mantle.
And it was sort of purely phenomenological, it just seems to explain the data really
well and that's what motivated the models.
The previous approach to calculating these models or doing this analysis was based on
high-performance computing.
What did that entail?
In terms of the high-performance computing behind the visco-elastic models?
Or I guess I'm trying to get at ways that the modeling process may have differed between
what you did before in machine learning and was it this specific problem that said,
hey, this needs to be machine learning and not high-performance computing or for the
class of problems that you tend to see in this space, could you choose either?
And machine learning was kind of the new fancy thing, so we try that.
How do they qualitatively differ in terms of solutions to these kinds of problems?
Those are the kinds of things I'm curious about.
Yeah, Sam, that's a great question.
It turns out one led to the other.
So when Phoebe did the work using this HPC code to try to see if we could explain the
delayed triggering of earthquakes along the North and Italian fault, that ended up taking
about 2.5 million CPU hours.
Wow.
And that was how long it took to try and explain the triggering of about seven or eight
earthquakes.
And we said, we'd like to do this elsewhere, but that's taking a long time.
And so together we came up with the idea to see if we could train a neural network to emulate
our HPC code.
And so what we did was with the HPC code, we were able to generate lots of training data.
And then we trained a neural network to simply map inputs to outputs, all the nonlinear
ones.
And then once we had done all the nonlinear ones, we could just combine the linear part
easily.
And so when we did that, we were able to replace this HPC code with an incredibly compact
neural network, which is able to give almost exactly the same answer and run in 500 times
faster.
Oh, that's awesome.
And that just one of the reasons why the description of your lab that I saw on your Harvard
page caught my interest was it mentioned deconvolving.
I'm assuming that that is referring to motion of tectonic plates and not convolutional
neural nets.
And there's an overlap there.
I would have been way ahead of my time if I had been not clever.
Yeah.
So when we got into the work of interpreting these GPS data, the world was kind of divided
into two halves.
There were the people who looked at the data and thought it told you about strain accumulation
in the earthquake cycle.
And people who thought it told you about long-term tectonic motions.
Neither group was wrong.
In fact, both groups were right.
And it was the ability to integrate and deconvolve those two signals together that really
allowed for progress to be made.
And that involved developing a class of large scale models.
And that's what allowed us to really isolate the part we care about, which is the
strain accumulation signal.
So let's talk a little bit about that modeling process.
How did you go about that?
Yeah.
So the core idea was to think about earthquakes, not as random phenomena, not as things that
popped up in the middle of nowhere, but rather to think about earthquakes as a byproduct
of plate tectonics.
So most earthquakes occur simply because large pieces of the earth's rocky crust are
moving past each other, and they're temporarily stuck together, and they're stuck together
along these localized surfaces called faults that occasionally fail.
And when they do fail, because the frictional resistance to motion is overcome by the stresses
that are accumulated, they rupture in large earthquakes.
So to make progress, what we found we had to do was we had to integrate both the physics
of what was going on at large scales, that is, the motion of tectonic plates, with the
physics of what was going on around faults, that is earthquake cycle physics.
And we were able to do that and link them together, and by linking them together, we finally
had a computational tool that allowed us to tease apart the competing effects due to
do the both sets of physics.
And with that, we were able to isolate the strain accumulation signature that we were
looking for.
Okay.
So you started that by saying, something to get left me with the impression that the
idea that earthquakes are caused by these tectonic shifts or plates was, you know, contentious
in some way, or not, let's start with contentious in some way in the field.
And yeah, it sounds like there's, so one model is this tectonic plate model, and the
other is more this local phenomenon?
I would say it was more just a case of what people were interested in.
They were interested in the tectonic problem, or they were interested in the earthquake
problem.
And I think the realization that a lot of people had, and this started back in the 80s,
and the late 80s and through the 90s and early 2000s, was that those two scales had to
be linked.
The small scale and the large scale had to be linked to really make sense of either.
And that was a computational challenge, in particular, due to the complex geometry of
fault systems of plate boundaries, that that was where the real challenge was.
How did you represent those geometries and integrate them into a computational model
that included the physics for both?
And the specifics of that model, how do you go about developing that?
Well, that takes us back a long time, that takes us back to my PhD, and so the ideas have
been in the air that these scales had to be linked for more than a decade.
And people had done a very nice local scale work on it.
And what I attempted to do was to generalize that work so that we could put in high fidelity,
very geometrically complex representations of fault systems, like those in Southern California,
which are unbelievably complex.
And yet at the same time, have all the motions on the faults in Southern California be consistent
with what's going on between the two large plates, the North American and the Pacific
plate, on either side of the Southern California fault system?
And the hardest part of that was actually not any of the physics part, but the most interesting
part was thinking about the problem very algorithmically, and figuring out how to efficiently
specify how the fault system geometry would be related to the motions of the plates on
either side of these faults.
It was an extremely geometrically complex problem involved mapping back and forth between
four different reference frames.
And I'm really glad I finished it so that I don't have to work on that again.
That's the macro scale piece.
How is that then linked to the micro scale piece, which is if I caught you correctly, that's
more looking at these earthquakes as a time series?
Is that right?
What do you think about it?
Yeah, I think Phoebe's work is what really linked that in, because she added time-dependent
evolving motions due to the coupling between the Earth's crust and the mantle.
And so I think Phoebe can speak to the time-dependent part of that story.
Awesome.
Well, yeah.
Well, we haven't actually totally linked Brendan's block models.
Well, yeah, so we built in some time dependence to the framework that Brendan built over his
PhD.
And since then, using these visco-elastic models.
And that just allowed us to sort of add a time perturbation throughout the earthquake cycle
to sort of incorporate information about when the most recent earthquake had occurred on
that fault.
All right, so the picture of starting to emerge from me, you've got this model that kind
of depends on two time scales, this physical geometric perspective, and this more time-oriented
perspective or local perspective.
And one big challenge is linking these two and kind of driving some consistency between
them.
And then, once you've got this model, you identified the computational complexity of actually
using it and set out to build a deep learning-based approach that would essentially model your
model.
Is that a decent summary?
Sam, that's like the best abstract of what we've done for the past 10 or 15 years I've
ever heard.
That's fantastic.
I really like that way of describing it.
And that was our entry into the machine learning and the machine learning world.
And from that, we eventually found these datasets that we could start making slightly U.S.
model-dependent predictions with, and that's what led us to the paper.
Nice, nice.
So Phoebe, as you set out to explore this machine learning, deep learning world, what were
the things that you found that you were able to apply to your particular problem?
We were just, well, so after we worked on sort of accelerating this computationally intensive
code using a train neural network, we, well, at least from my perspective, I just started
to think of all the different problems in Earth science that involve sort of inputs and
outputs and figuring out what the relationship is between those inputs and outputs.
And so that's really what got us thinking about this aftershock problem because we could
calculate stress changes in the crust and upper mantle after large earthquakes and then
see if we could use a neural network to map those stress changes to aftershock locations.
So it just got us thinking about all these different problems.
What was the nature of the deep learning model that you used for these types of problems?
Did you, was it kind of an off-the-shelf type of a model or was it something that you
crafted from scratch based on the specifics of your problem?
No, it was very, very simple.
We implemented it in Keras.
It was a fully connected network about as simple as you can get.
And it just seemed to work really well.
And do you have any intuition about the fact that it works so well, so quickly give you
any intuition about the problem, like does it say anything about latent characteristics
of the problem or the relationships between the data that you had and the actual physical
phenomena or is it, it just happens to work well because these things are good at picking
out patterns and data.
It turns out for this problem there's a remarkable amount of physical insight that we could
gain from this.
So let me set the stage a little bit.
When we modeled the aftershocks in the study, we didn't just take the location of the
main shock and say go predict the aftershocks.
Instead what we did was we took the location of the main shock and we took information
about large earthquakes in particular how much they slipped and where they slipped.
And we put that information through a forward model which predicted stresses, the changing
stress everywhere in the earth's crust as a result of the large earthquake.
Those were the raw inputs that we actually put into that served as essentially the features
for a machine learning model.
And then the labels in our machine learning model were the locations of the earthquakes.
And so what this did was we provided a sort of physics-based regularization of the features
in a way that ensures that conservation of mass is satisfied, conservation of linear
momentum is satisfied.
And because of this, when we were able to look at the neural network and do inference
with it after we had already trained in everything, we were able to look at synthetic examples
and we were able to interpret those synthetic examples in the context of some of the basic
physics that we had put in.
And what it led us to discover was that in fact the neural network had not learned some
absolutely out there pattern.
The neural network had actually come very close to finding a physical quantity that we had
heard of before.
And that was essentially something called the Von Mises yield criterion.
And this is a metric that exists in the literature and is very commonly used to explain the transition
from elastic to plastic behavior in particular.
And so that's one of the things that was really interesting about this study.
We didn't just get a neural network, we didn't just develop a neural network that had greater
predictive power.
And it turned out we learned that the physics that might be controlling the triggering
of aftershocks was different from what we thought it was before.
That's really interesting.
Well how did you go from the predictions that you were seeing, the inferences that you
were seeing, to backing that into this Von Mises yield criterion?
How did you see that that was there?
Yeah, go ahead Phoebe.
Oh yeah.
So we looked at synthetic examples because it was kind of daunting to think about interpreting
the predictions of the neural network for these very complex slip distributions that
we got from these catalogs.
So we looked at just for idealized earthquakes, what the neural network was predicting.
And then we just simply compared that spatial pattern to the spatial patterns of a big suite
of different stress metrics to see sort of which ones it was most highly correlated with.
And it turned out that something like 98% of the variance in the neural network output
could be explained by the Von Mises yield criterion.
Okay, so you already had this criterion in mind as a contributor or a way to model these
aftershocks going into this problem.
I would say that there were a huge number of candidate functions that we would have
considered.
And my bet, I don't know about Phoebe, but my bet was that it was going to be some really
complicated combination of four or five things that we're going to have to be combined
in some weird nonlinear way to mimic what the neural network was doing.
And as Phoebe mentioned to our surprise, what we found was that there was essentially
one quantity that the neural network was coming close to discovering on its own.
And in that sense, it's really interesting because a neural network is obviously incredibly
good at finding very complicated nonlinear functions.
And what it told us in this case was that to the extent that we trained it, it couldn't
find anything that did too, too, too much better than a physical, a single physical quantity
that actually exists.
So Phoebe, what was the most challenging aspect of applying neural networks to this
problem?
The neural network really wasn't that challenging, it was the sort of data assembly, which was
really, really a lot.
And I think the generation of that, the generation of the data set was really the heavy lift
of this because we had to read in all these complex slip distributions from this online
catalog that many different authors had contributed to.
So there were lots of sort of slight inconsistencies in the way that these data files were written.
And so it had to be sort of very robust to those little inconsistencies.
And then for each slip distribution, we had to calculate the stress changes in the vicinity
and within 100 kilometers of each of these large main shocks and then incorporate another
catalog of the aftershocks.
So it was just a very large scale data assembly problem.
The aftershocks that you were predicting, you were predicting those in two dimensional
space, as opposed to kind of radio distance from the earthquake, presumably.
And were you also trying to predict the magnitude of the aftershocks as well?
No, that's what we're working on now.
But we were doing it in the volume around the fault.
So we actually, we decided to make this problem very easily tractable to start out with.
We decided to frame this problem of explaining aftershock locations as a binary classification
problem.
And the way that we did that was we discretized the volume around each of these main shocks.
And then framed to the problem is trying to classify each grid cell as either containing
or not containing aftershocks.
So that is the sort of way that we approached it, but we did do it not in two dimensions.
We did it in the whole volume around the fault.
Was that binary classification approach?
Was that the first thing you decided to try and it worked or did you kind of iterate
or evolve to that?
That's the first thing we tried.
And the results were just so interesting that we went with it.
But we're certainly now trying lots of different other approaches to see what we can learn.
And what are some of those?
Well, we're looking at aftershock magnitudes right now.
And it's all very preliminary, but it's really interesting to look at the kind of patterns
that the neural network outputs and sort of use these approaches as kind of pattern synthesizers
almost.
And then I think going forward, also looking at aftershock density and eventually aftershock
timing as well.
I'm so curious about this, about how you incorporated the Von Mise's yield criterion.
Was that incorporated into the neural network itself or the machine learning aspect of
this project or was that secondary analysis that you just apply to the results?
Option two.
Got it.
Okay.
And that's why it's interesting because it's not a quantity that we gave it.
And one could compute that quantity from the inputs, you essentially have to solve an
eigenvalue problem, but it's that's why it was such a surprise to us.
Yeah, it reminded me a little bit of a conversation with I'm forgetting his name.
I believe at the University of Washington, where he was basically the outputs of this network
were coefficients of a bunch of factors, so kind of linear kind of what you thought you
would see like a linear combination of a bunch of different factors.
And he used that to basically to derive mathematical equations for physical input parameter.
Input parameters.
Yeah.
It was being used for laser tuning.
That's great work.
It's coming out of the University of Washington, Brunton is one of the people who's doing it.
And that offers the prospect of using machine learning based techniques to constrain the
physics of systems, even when we don't know the physics of a priori.
I think that's a really promising technique.
Not maybe for earthquake problems, but in particular for a lot of problems involving movement
on the Earth's surface, whether it's groundwater or ice or things like that that are very complex
media problems.
I think the techniques, those sorts of techniques are going to be deeply powerful.
Yeah.
Yeah.
It was Nathan Coots at a Coots at University of Washington for that particular one.
Where do you see this going?
Phoebe, you're continuing this looking at, you mentioned the magnitude of the aftershocks.
You mentioned as well the time dependency of the aftershocks.
Do you have a sense for how the modeling approach will need to change as you take on these
new problems?
I mean, we're very much in exploration mode right now.
So I think we'll go to more complex network architectures if we need to.
But yeah, it's just exciting to think about.
And aftershock forecasting as a problem is sort of exciting to think about from a machine
learning perspective because we've taken into account only static elastic stress changes.
These sort of features of the network are only these static stress changes due to the
main shocks.
But there are a lot of other phenomena that may affect aftershock behavior, everything
from the locations of existing geological structures in the region to poor or elastic
stress changes, visco-elastic stress changes.
So I think, I don't know if I'm going to agree with this, but I feel like the most important
implication of this paper from my perspective is kind of the approach.
Yeah, I think I agree with that entirely.
I think I even zoom out a little bit and I think the approach is really important in
so much as it provides a window into how we can rebuild a lot of earth science and potentially
a lot of other sciences of complex systems in a way that doesn't require us to know everything
about those systems beforehand.
And what I mean is we spend a lot of time doing bottom-up modeling of all sorts of phenomena.
And that's an excellent approach, but it requires that we know what's going on from the bottom
up.
And I think for a lot of problems that we face in earth science and in the world, what
we really want to do first is try to make predictions.
And that's where we can make a lot of progress here.
We can try to make predictions, improve predictions of weather systems, climate systems, the
earthquake system, environmental systems.
And if we can exploit the ability to make predictions even in the absence of knowing what the
first principles are, I think that deeply motivates us to go try and find out what those
first principles are.
That's one of the jobs of the scientist.
And what we now have are tools that enable us to do that, not just by sitting around
and thinking about what the first principles ought to be, but by giving us these networks
that we can probe into and try to understand why they're predicting what they are.
One of the questions that remains for me is in using this neural network approach, you've
been able to pretty significantly reduce the time it takes to get new predictions to make
new results by training a network to emulate the output of this traditional model that
was hard to build and expensive to run.
So you're using neural networks to model a model, so you still have to have the model.
And I guess the question is, do you still need to develop these traditional models for
each of the new problems that you want to try and solve?
And does that remain a limiting factor in your ability to accelerate innovation as you
were describing?
I think that's a great question, and we've done it both ways.
So when we accelerated this big HPC code, that did require Ford model.
And in that sense, its primary utility is not insight, but simply rather speed.
And it's nice too, because anyone can download this model and run it kind of on a laptop,
instead of needing a data center to run it in, which is nice.
But for the aftershock study that we did more recently, in that case, we didn't have
to develop anything other than a kind of a classical machine learning model.
And so I think insights are going to be gained in both ways, and it's going to help more
people do a lot of science in both ways.
Okay, and so just so that I understand that last point, the inputs to the aftershock model,
I thought those features came from the traditional HPC model.
Did I misunderstand that?
Yeah, those are two separate studies by large.
So got it, okay.
One could have put in that HPC model, that's a really good idea, Sam.
We should probably do that study.
Hold on, I got to write that down.
But we used a simpler version of it for this particular study, which was not time dependent.
It was just a step function change in stress due to the earthquake itself.
I'm sure I'll make even more sense when I listen to it again.
But this has been really great. I appreciate you taking the time to share with me a bit
of what you're working on.
Any final words or thoughts?
Not for me.
Thank you so much for having us.
Oh, Sam, this has been great.
Thanks for having us.
And I hope for a lot of earth and environmental science problems, there can be continued and
greater dialogue between the ML community and the earth science community, because we
have a lot of data from complex systems that we don't understand, and we really want
to predict those to live in a better world.
Awesome.
Thanks so much, Phoebe and Brendan.
Thanks, Sam.
Thanks.
All right, that's our show for today.
To learn more about today's show, visit twomolai.com slash shows.
Make sure you visit twomolcon.com slash videos to secure your access to Twomolcon video content
now.
Peace.
