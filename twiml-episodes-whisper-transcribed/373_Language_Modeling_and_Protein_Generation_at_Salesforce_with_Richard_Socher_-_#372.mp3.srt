1
00:00:00,000 --> 00:00:13,400
Welcome to the Twimal AI Podcast.

2
00:00:13,400 --> 00:00:16,400
I'm your host Sam Charrington.

3
00:00:16,400 --> 00:00:24,800
Hey, what's going on everyone?

4
00:00:24,800 --> 00:00:30,560
A few weeks ago we held our very first interactive podcast screening with Google's Kwaklay.

5
00:00:30,560 --> 00:00:34,840
Attendees got to watch the video interview together and chime in with their own questions

6
00:00:34,840 --> 00:00:38,520
which Kwak and I addressed live during the session.

7
00:00:38,520 --> 00:00:44,000
We had a bunch of fun with this and got a lot of great feedback and so while we still haven't

8
00:00:44,000 --> 00:00:48,760
quite figured out what to call this thing, we have scheduled our next one and it's coming

9
00:00:48,760 --> 00:00:54,640
up this Thursday, May 7th at 12 noon Pacific time.

10
00:00:54,640 --> 00:00:59,700
This time around I'll be hosting Lucas B.Wald, founder of Weights and Biosys for a live

11
00:00:59,700 --> 00:01:03,520
interview screening and an interactive Q&A session.

12
00:01:03,520 --> 00:01:09,520
We'll be discussing data versioning, managing machine learning pipelines, managing ML experiments,

13
00:01:09,520 --> 00:01:11,360
and much more.

14
00:01:11,360 --> 00:01:12,720
You don't want to miss this one.

15
00:01:12,720 --> 00:01:18,640
To join us, visit youtube.com slash Twimal AI, subscribe to the channel and then hit the

16
00:01:18,640 --> 00:01:24,320
bell to turn on notifications so that YouTube will let you know when we're about to get started.

17
00:01:24,320 --> 00:01:30,000
In the meantime, Weights and Biosys is extending a special offer to Twimal listeners, including

18
00:01:30,000 --> 00:01:33,800
unlimited private projects and priority support.

19
00:01:33,800 --> 00:01:43,680
For more details, visit WNB.com slash Twimal, that's w-a-n-d-b.com slash Twimal.

20
00:01:43,680 --> 00:01:48,320
That said, May the 4th be with you.

21
00:01:48,320 --> 00:01:51,040
Hey everyone, I am on the line with Richard Socher.

22
00:01:51,040 --> 00:01:55,640
Richard is the Chief Scientist and EVP at Salesforce.

23
00:01:55,640 --> 00:01:58,120
Richard, welcome to the Twimal AI podcast.

24
00:01:58,120 --> 00:02:01,200
Aloha, great to be here, wants to chat with you soon.

25
00:02:01,200 --> 00:02:04,600
You said Aloha and I was surprised that you're actually in the Bay Area.

26
00:02:04,600 --> 00:02:10,720
I always see these wonderful photos of you all over the place and I only get to ever

27
00:02:10,720 --> 00:02:17,600
see you in person at NURPS nowadays and the Black and AI events and stuff like that.

28
00:02:17,600 --> 00:02:23,040
So it's great to get a chance to connect with you mid-year, how is everything?

29
00:02:23,040 --> 00:02:26,680
Life is pretty good, I'm very grateful.

30
00:02:26,680 --> 00:02:32,480
Our research can continue working on some research during this crisis now, food that is specific

31
00:02:32,480 --> 00:02:40,200
to COVID-19 but by and large, I sometimes joke that the PhD prepared me for several years

32
00:02:40,200 --> 00:02:46,160
for staying at home, meeting pasta every day and working on a computer all day.

33
00:02:46,160 --> 00:02:52,160
So I'm in pretty good spirits trying to have a little bit of positive impact and still

34
00:02:52,160 --> 00:02:58,920
go about my work and make sure my team is doing well throughout this crisis and it's a tough

35
00:02:58,920 --> 00:03:05,680
time but I'm very grateful for the line of research we can work remotely and do quite

36
00:03:05,680 --> 00:03:06,680
well.

37
00:03:06,680 --> 00:03:07,680
Awesome, awesome.

38
00:03:07,680 --> 00:03:08,680
I'm glad to hear that.

39
00:03:08,680 --> 00:03:14,840
It is, don't usually do this but it's April 10th that we're recording this.

40
00:03:14,840 --> 00:03:19,440
What is it week four for you for lockdown shelter in place thereabouts?

41
00:03:19,440 --> 00:03:20,440
That's right, yeah.

42
00:03:20,440 --> 00:03:25,480
Yeah, it's crazy to time, weirdly it's slow and fast at the same time, days more

43
00:03:25,480 --> 00:03:30,720
than to each other, you know, the home office is just the home and the office and everything.

44
00:03:30,720 --> 00:03:35,760
So yeah, time definitely does not seem linear going through this.

45
00:03:35,760 --> 00:03:42,240
It is very strange but hey, before we jump into some of the main topics that we want to

46
00:03:42,240 --> 00:03:47,280
cover and particular language models and some of the recent work you've been doing applying

47
00:03:47,280 --> 00:03:52,880
that to the bio space, share with us a little bit about your background and how you came

48
00:03:52,880 --> 00:03:54,600
to work in AI.

49
00:03:54,600 --> 00:03:55,600
Sure.

50
00:03:55,600 --> 00:04:02,800
Boy, it almost starts in high school when I re-liked math and languages and when you think

51
00:04:02,800 --> 00:04:09,880
about those two fields, one you would hope is true even if you go light years in some

52
00:04:09,880 --> 00:04:18,400
other direction and language is this constantly morphing system where every teenager could

53
00:04:18,400 --> 00:04:23,080
just say, yo-lo and boom, you have a new word and now the signs are going to mess the

54
00:04:23,080 --> 00:04:29,920
deal with that and so they marry when you try to use computers to use math to try to understand

55
00:04:29,920 --> 00:04:30,920
language.

56
00:04:30,920 --> 00:04:37,760
So I studied linguistic computer science back at 2000 and early 2000s and that at the

57
00:04:37,760 --> 00:04:44,920
time seemed kind of like an orchid, kind of cute, cruxotic, niche topic to my parents.

58
00:04:44,920 --> 00:04:48,960
But I thought, man, if we can get computers to understand language, that would be just

59
00:04:48,960 --> 00:04:49,960
incredible.

60
00:04:49,960 --> 00:04:53,960
All the things they could do, especially if you're lazy and you want to re-repetitive

61
00:04:53,960 --> 00:04:57,960
tasks to be done by a computer would be quite amazing.

62
00:04:57,960 --> 00:05:04,800
And so that kind of morphed into a couple of other interests and trying to use eventually,

63
00:05:04,800 --> 00:05:10,000
initially just statistical machine learning and sort of machine learning by itself and

64
00:05:10,000 --> 00:05:15,480
then AI more broadly applied to computer vision problems.

65
00:05:15,480 --> 00:05:19,480
But I really do think in the end language is the most interesting manifestation of human

66
00:05:19,480 --> 00:05:20,480
intelligence.

67
00:05:20,480 --> 00:05:25,960
There's some quite incredible visual systems and apparatuses in the animal kingdom, like

68
00:05:25,960 --> 00:05:31,960
the mantis shrimp with all kinds of focal vision and so on and each eye and all of that

69
00:05:31,960 --> 00:05:36,760
and a lot of animals have quite sophisticated visual systems, but it's really language

70
00:05:36,760 --> 00:05:41,360
that is connected with thought and culture and society and information.

71
00:05:41,360 --> 00:05:48,680
And so I got really excited about language and then in 2010 I saw a handful of people

72
00:05:48,680 --> 00:05:55,800
apply neural network techniques and extend them to computer vision.

73
00:05:55,800 --> 00:06:00,120
And at the time, I had also just become a little bit disillusioned myself around how

74
00:06:00,120 --> 00:06:06,200
much time natural language processing folks spend on feature engineering.

75
00:06:06,200 --> 00:06:13,200
And so I thought, couldn't we use some of these ideas from computer vision and neural networks

76
00:06:13,200 --> 00:06:16,560
for natural language processing and it was not easy.

77
00:06:16,560 --> 00:06:23,000
And the beginning early days had a lot of rejected papers, reviewers just ignoring reasonably

78
00:06:23,000 --> 00:06:26,640
good experimental results saying, oh, why are you submitting neural network stuff to

79
00:06:26,640 --> 00:06:32,080
this conference, this is not the 90s anymore, this stuff doesn't work and so on.

80
00:06:32,080 --> 00:06:37,720
But eventually, more and more people kind of joined the small core that initially was

81
00:06:37,720 --> 00:06:45,640
read just Yoshiro Benjo and Jeff Hinton's labs and Andrew Hings lab at Stanford and it expanded

82
00:06:45,640 --> 00:06:50,480
more and more and now it's kind of the default way for doing things as to use neural networks

83
00:06:50,480 --> 00:06:57,320
of course, they've developed more and more novel architectures too and it's just been

84
00:06:57,320 --> 00:06:58,320
super exciting.

85
00:06:58,320 --> 00:07:05,200
So now I work not just on the research side anymore, but also on a lot of applied problems.

86
00:07:05,200 --> 00:07:10,280
You know, in the end, I often think about trying to have impact and in the end, when you

87
00:07:10,280 --> 00:07:15,400
do research, you hope that people will pick up that research, extend it and actually apply

88
00:07:15,400 --> 00:07:19,840
it to some real world problems, but if you have the opportunity to both do research and

89
00:07:19,840 --> 00:07:23,920
apply it to real problems, you kind of reduce the variance of the impact that you have and

90
00:07:23,920 --> 00:07:31,840
so I work on a lot of NLP problems with chatbots and service and sales and marketing applications

91
00:07:31,840 --> 00:07:37,920
trying to, for instance, automatically reply to emails or to phone conversations or having

92
00:07:37,920 --> 00:07:43,440
chat conversations is a really great one also during a lot of stuff in computer vision trying

93
00:07:43,440 --> 00:07:49,560
to identify different objects and super market shelves and doing complex OCR for forms

94
00:07:49,560 --> 00:07:55,520
and a lot of interesting things, recommendation engines, voice, machine translation.

95
00:07:55,520 --> 00:07:59,800
Now the group is pretty large and so we get to work on a lot of different things.

96
00:07:59,800 --> 00:08:05,840
You know, it's a research organization, but it's part of Salesforce which we, you know,

97
00:08:05,840 --> 00:08:10,800
is kind of a, you know, do you still, does Salesforce still think of itself as a CRM company

98
00:08:10,800 --> 00:08:13,360
or is it much harder than that now?

99
00:08:13,360 --> 00:08:19,080
Yes, the term CRM has kind of expanded and now includes everything that you might do

100
00:08:19,080 --> 00:08:20,400
with a customer, right?

101
00:08:20,400 --> 00:08:24,440
So we're one of the largest e-commerce platforms because customers buy stuff online.

102
00:08:24,440 --> 00:08:29,240
Yeah, we're obviously the largest sales service and marketing organization, but we also

103
00:08:29,240 --> 00:08:32,240
have help companies integrate all their different data.

104
00:08:32,240 --> 00:08:37,640
Now with Tableau, we help people understand their customer data and do a lot of analytics

105
00:08:37,640 --> 00:08:43,400
behind it and then we look at, you know, where are the customers and we help governments

106
00:08:43,400 --> 00:08:48,880
see their citizens as their customers and help them, especially now also in this crisis,

107
00:08:48,880 --> 00:08:53,160
build software really quickly, build chatbots so they can answer questions.

108
00:08:53,160 --> 00:08:57,200
You know, the, in the end, if you go to the DMV Department of Motor Vehicles and you

109
00:08:57,200 --> 00:09:03,160
have a question, chatbots that give you answers there are also, you know, your customer

110
00:09:03,160 --> 00:09:04,920
there of the DMV.

111
00:09:04,920 --> 00:09:07,720
We work with healthcare providers where the patients are customers.

112
00:09:07,720 --> 00:09:13,360
So the definition of what a customer is is getting broader and broader and we're in all

113
00:09:13,360 --> 00:09:14,680
those areas.

114
00:09:14,680 --> 00:09:18,720
And so with the company being in all those areas and being a very product focused company

115
00:09:18,720 --> 00:09:25,120
as opposed to, you know, an academic institution, you know, how does the company balance investments

116
00:09:25,120 --> 00:09:31,560
in, you know, research with product requirements and how does that specifically impact you

117
00:09:31,560 --> 00:09:37,520
and your team and what you decide to, or, you know, end up working on?

118
00:09:37,520 --> 00:09:41,840
It's a great question and it's definitely something that I think every company struggles

119
00:09:41,840 --> 00:09:47,960
with as they can sort of think past their next quarter and think about, you know, the next

120
00:09:47,960 --> 00:09:53,600
couple of years, we, I wear really a couple of different hats.

121
00:09:53,600 --> 00:09:59,680
On the pure research side, we really can work on and have a lot of freedom and we can work

122
00:09:59,680 --> 00:10:04,720
on a lot of different things and we basically look for what's, what's the, what could have

123
00:10:04,720 --> 00:10:09,960
the biggest impact on both the field of AI research as well as on application fields

124
00:10:09,960 --> 00:10:16,720
like economics or a natural language processing or medicine even in other areas.

125
00:10:16,720 --> 00:10:22,560
So there, we have a lot of freedom, but then the, another large part of my group is really

126
00:10:22,560 --> 00:10:28,240
just part of engineering and we're building real products like chatbots or case classification

127
00:10:28,240 --> 00:10:34,560
of emails or opportunity and lead scoring where, you know, a salesperson might have 5,000

128
00:10:34,560 --> 00:10:40,240
people they could email or call in any given day and you can use systems to rank and say,

129
00:10:40,240 --> 00:10:44,400
these customers are the most likely to actually want to buy your product today and then help

130
00:10:44,400 --> 00:10:51,480
them kind of get through and triage that we work with marketing where we can classify

131
00:10:51,480 --> 00:10:57,600
the sentiment of different tweets, for instance, or we can identify company logos on tweets

132
00:10:57,600 --> 00:11:04,280
and surface them even if you don't say at or hashtag this company name.

133
00:11:04,280 --> 00:11:10,680
We work with industries to understand super market shelves and do visual things.

134
00:11:10,680 --> 00:11:16,200
So there's a lot of product development and as long as that product development feels

135
00:11:16,200 --> 00:11:21,400
like and we feel like we're having a lot of positive impact on the business through AI,

136
00:11:21,400 --> 00:11:25,200
we get the carve out a niche where we just think about all the stakeholders, not just

137
00:11:25,200 --> 00:11:31,520
to share all this but all the stakeholders of the ecosystem and there we can work on things

138
00:11:31,520 --> 00:11:38,880
like even protein sequence models and medical computer vision, classifying different types

139
00:11:38,880 --> 00:11:47,360
of breast cancer and a whole host of fundamental research programs and optimization of neural

140
00:11:47,360 --> 00:11:53,520
networks which is pretty theoretical as well as on some really applied NLP projects that

141
00:11:53,520 --> 00:11:58,320
are actually very nicely at the intersection of pure research and applied research.

142
00:11:58,320 --> 00:12:03,360
So for instance, one paper was published a while back and have extended for a while now

143
00:12:03,360 --> 00:12:08,800
on a sequence of SQL where we translate natural language English questions into SQL queries.

144
00:12:08,800 --> 00:12:16,000
And that is a fundamental problem you tried to disambiguate language that sort of touches

145
00:12:16,000 --> 00:12:22,840
upon what is the meaning of a question which is pretty hard and one way to define the

146
00:12:22,840 --> 00:12:28,880
meaning of a question is well it's a program that you once you execute it over the right

147
00:12:28,880 --> 00:12:34,360
set of knowledge will give you the correct answer to the question that the person asking

148
00:12:34,360 --> 00:12:38,920
the question had in mind and one way of doing that is to translate it into code or into

149
00:12:38,920 --> 00:12:45,040
SQL because a lot of data in the world is stored in databases and so that is a very fundamental

150
00:12:45,040 --> 00:12:49,360
question that we actually started with pure research but now with Tableau and other

151
00:12:49,360 --> 00:12:54,080
analytics and so on it is actually quite relevant and business users might also want to ask

152
00:12:54,080 --> 00:12:59,720
questions like how many customers bought this particular product and this particular zip

153
00:12:59,720 --> 00:13:05,400
code between this time range and you just want to be able to answer that question in normal

154
00:13:05,400 --> 00:13:11,720
English language and get a real answer for it and so there are some more and more connections

155
00:13:11,720 --> 00:13:15,320
between the fundamental research group and the applied research group and then all the

156
00:13:15,320 --> 00:13:17,160
product groups and engineering groups.

157
00:13:17,160 --> 00:13:23,000
That provides really interesting context for talking a little bit about one of your recent

158
00:13:23,000 --> 00:13:31,320
papers which is and you mentioned this on protein generation even with all of that in mind

159
00:13:31,320 --> 00:13:34,440
what was how did you end up working on protein generation?

160
00:13:34,440 --> 00:13:40,920
Yeah it's a good question originally we're inspired by language modeling so we've worked

161
00:13:40,920 --> 00:13:47,720
on language modeling for almost four years since we since met a might my startup got acquired

162
00:13:47,720 --> 00:13:53,800
by Salesforce and for a while had the best language models there are still LSTMs or quasi-recurrent

163
00:13:53,800 --> 00:13:59,600
neural networks and variants of these with pointer mechanisms that could copy over words

164
00:13:59,600 --> 00:14:05,800
as they try to predict the next word and it's a very interesting fundamental task in

165
00:14:05,800 --> 00:14:10,040
natural language processing right in some ways it's NLP complete in the sense that if you

166
00:14:10,040 --> 00:14:15,040
can always predict correctly what the next word is then that means you can also do all of

167
00:14:15,040 --> 00:14:21,680
question answering right I can ask you know what's the capital of California and then the next

168
00:14:21,680 --> 00:14:26,320
word should be the capital of California and if you do that correctly it means you have knowledge

169
00:14:26,320 --> 00:14:33,440
about the world you can answer questions you can ask what is the translation of ithibadi from

170
00:14:33,440 --> 00:14:38,960
German into English and then you should get the translation as the next couple words so as you

171
00:14:38,960 --> 00:14:44,160
can get better and better at predicting the next word you might be able to solve a lot of different

172
00:14:44,160 --> 00:14:49,520
natural language processing problems now at the same time that is kind of the long term future

173
00:14:49,520 --> 00:14:53,840
nobody actually thinks that you're you could be good enough especially not a couple of years ago

174
00:14:53,840 --> 00:14:59,840
to do all of these different NLP problems as question as language modeling but originally language

175
00:14:59,840 --> 00:15:04,880
modeling was just used to disambiguate words for instance if I ask you what's the price of wood

176
00:15:04,880 --> 00:15:14,080
is this the wood W-O-U-L-D auxiliary verb or is it the W-O-O-D the noun from a tree wood and so

177
00:15:14,080 --> 00:15:19,360
but with the price of it's much more likely to have a noun after and so you can disambiguate you

178
00:15:19,360 --> 00:15:26,480
know either a translation or speech recognition models or you can come up with better selection

179
00:15:26,480 --> 00:15:32,720
of sentences that were generated by a translation model to say this is more fluent of an English

180
00:15:32,720 --> 00:15:37,760
sentence than another alternative and so that was originally what language modeling was used for

181
00:15:37,760 --> 00:15:44,080
but then we were able deep learning and our group and in other groups and now all these

182
00:15:44,080 --> 00:15:49,360
transformer models and bird models and so on really get better at predicting not just the next word

183
00:15:49,360 --> 00:15:53,280
but you can also predict works in the middle of the sentence and you train really good

184
00:15:53,920 --> 00:15:59,680
general representations of language that capture general knowledge we actually use these

185
00:15:59,680 --> 00:16:06,560
kinds of language models to give explanations for why you make choose a certain classification

186
00:16:06,560 --> 00:16:12,240
output and it turns out that in several cases they actually capture common sense even which is

187
00:16:12,240 --> 00:16:18,240
really hard to capture an illogical or database kind of structure. Can you give some examples of

188
00:16:18,240 --> 00:16:25,680
what you're describing there? Yeah sure so the paper was done by Nadine in our group

189
00:16:25,680 --> 00:16:35,840
at Salesforce Research and basically she created natural language explanations for common sense

190
00:16:35,840 --> 00:16:44,080
reasoning questions and so the paper was we have actually a couple but one was explain yourself

191
00:16:44,080 --> 00:16:49,360
leveraging language models for common sense reasoning and so the idea here is we had a couple of

192
00:16:49,360 --> 00:16:56,880
different multiple choice questions and in with you want to give the answer you also ideally

193
00:16:56,880 --> 00:17:04,640
will say why you gave that particular answer so for instance if you push a glass off a table

194
00:17:04,640 --> 00:17:11,840
what will happen? A it will float, be it will fall down or in break and things like that and then

195
00:17:11,840 --> 00:17:18,240
you will basically say oh why will it fall down and it's not because of gravity and so this seems

196
00:17:18,240 --> 00:17:25,440
like a simple thing and for any particular small scenario you could possibly generate all the logical

197
00:17:25,440 --> 00:17:30,160
consequences but if you think about anything like I was going to say it seems like a simple thing

198
00:17:30,160 --> 00:17:37,280
for anyone who's not involved in AI and say oh the person blows the lease from a grass area using

199
00:17:37,280 --> 00:17:43,040
the blower the blower A puts the trimming product over her face in another section B is seen

200
00:17:43,040 --> 00:17:49,600
up close with different attachments C continues to blow mulch all over the yard several times

201
00:17:49,600 --> 00:17:54,800
and so on so you know these kinds of questions and knowing how to answer that it's just requires a

202
00:17:54,800 --> 00:18:01,600
lot of common sense and common sense is incredibly hard to really formulate in a knowledge representation

203
00:18:01,600 --> 00:18:06,800
such that AI models can really make use of it so long story short I got a little off of a

204
00:18:06,800 --> 00:18:11,680
tangent there language modeling super fascinating lots of other applications that people hadn't thought

205
00:18:11,680 --> 00:18:17,600
about that that came out in the last couple of years and in our case we also wanted to actually

206
00:18:17,600 --> 00:18:23,120
make it more controllable we had these interesting language modeling results from open AI that could

207
00:18:23,120 --> 00:18:28,320
actually generate long texts and it was quite surprising to a lot of people that these models were

208
00:18:28,320 --> 00:18:34,000
so good at doing that at the time but all you could do is you primate with the beginning of a sentence

209
00:18:34,000 --> 00:18:39,600
you say like a knife and then we're just kind of ramble on and on and we thought couldn't we make

210
00:18:39,600 --> 00:18:46,240
this more controllable so we edit control codes based on different training data and based on

211
00:18:46,240 --> 00:18:51,360
basically the whole internet so you can make a control code and say this is a horror story

212
00:18:51,360 --> 00:18:58,160
now say a knife and then you know sort of peek through the keyhole of the door and the door slightly

213
00:18:58,160 --> 00:19:02,480
started creaking and opening and you say oh my god what's going on but you can also say a knife

214
00:19:02,480 --> 00:19:07,360
and then say give me a review and so I just say oh knife is really great cutting and fits well

215
00:19:07,360 --> 00:19:11,840
into our kitchen and so on you could basically control the language modeling a little bit better

216
00:19:11,840 --> 00:19:18,000
and we thought that would make it more useful and now people can kind of collaborate with these

217
00:19:18,000 --> 00:19:23,600
models to generate texts and it almost helps you if you're not trying to do creative writing to

218
00:19:24,480 --> 00:19:29,200
to give you sort of spitball with you and just generate stuff and then you can modify it

219
00:19:29,200 --> 00:19:34,400
you can create marketing messages and have control codes based on these were successful marketing

220
00:19:34,400 --> 00:19:38,960
campaigns and of course you know you have certain things in mind so you want to kind of play

221
00:19:38,960 --> 00:19:44,880
with the model and its output so that was control and that paper just or folks we'll link to it

222
00:19:44,880 --> 00:19:50,320
in the show notes but that's conditional transformer language model for controllable generation

223
00:19:50,320 --> 00:19:57,440
CTRL that's exactly right and that is the largest model is 1.6 billion parameter language model

224
00:19:57,440 --> 00:20:03,120
you train it on a very large corpus you can create any URL as a control token that will

225
00:20:03,120 --> 00:20:09,440
talk in the language of that URL in like cnn.com and so on it's quite fascinating and for a while

226
00:20:09,440 --> 00:20:16,320
we're actually worried also that it might have ethical issues but really I don't think that is the

227
00:20:16,320 --> 00:20:23,280
case people unlike GANs that generate images where people thought images were you know despite

228
00:20:23,280 --> 00:20:30,160
Photoshop people still thought of images as a good proof for something you can forever in human

229
00:20:30,160 --> 00:20:34,640
history since we've had language just misattribute text to somebody else you can just write an

230
00:20:34,640 --> 00:20:38,960
email and say oh this other person sent the email and like well we have to actually show metadata

231
00:20:38,960 --> 00:20:45,120
to really verify that that is really something that somebody said so don't think it helps with

232
00:20:45,120 --> 00:20:50,480
sort of creating misinformation also because you while it's more controllable in terms of the

233
00:20:50,480 --> 00:20:56,880
style now with CTRL or control you still can't force it to say the things that are in your head

234
00:20:56,880 --> 00:21:00,800
like you want to usually when you want to create misinformation campaigns you have something

235
00:21:00,800 --> 00:21:07,200
specific in mind that you would like the models to say but long story short quick question on that

236
00:21:07,200 --> 00:21:13,440
the control code you mentioned you can generate them from any URL when you have something like a

237
00:21:13,440 --> 00:21:21,520
review or a movie or horror I think was the example used it does that particular code map to

238
00:21:21,520 --> 00:21:27,600
a specific URL that you've selected or is there something more to that mechanism so the

239
00:21:27,600 --> 00:21:33,760
control codes are very broad you can say like this is Wikipedia or like Wikipedia style or coming

240
00:21:33,760 --> 00:21:40,480
from Wikipedia like training data it can be just a URL it could be a style like a kid story or a

241
00:21:40,480 --> 00:21:45,760
horror story like you can you can have any set of control codes and one set of control codes are

242
00:21:45,760 --> 00:21:50,880
URL so if you can say create me a Kickstarter campaign for like some new gadget that you come up

243
00:21:50,880 --> 00:21:56,320
with and it'll literally write you a Kickstarter campaign with that URL it's it's actually pretty

244
00:21:56,320 --> 00:22:03,840
hilarious what people have been able to generate with it in terms of the text and the control codes

245
00:22:03,840 --> 00:22:11,280
those are you're providing those to the model at inference time they're not baked in during the

246
00:22:11,280 --> 00:22:16,080
training process is that correct so they are in that we use the whole internet as the training data

247
00:22:16,080 --> 00:22:23,200
and so every URL hat was in some way a control code sure but you don't have to specify a subset of

248
00:22:23,200 --> 00:22:27,840
the internet that will serve as your control codes later why you don't have to yeah you don't have

249
00:22:27,840 --> 00:22:32,800
to but you can but you can yeah the control codes are very general and some of them where we knew

250
00:22:33,760 --> 00:22:40,960
for instance where they came from like Wikipedia we can kind of stylize them and we had certain

251
00:22:40,960 --> 00:22:46,960
groups of documents where we said all right these are all Wikipedia articles so let's just you know

252
00:22:46,960 --> 00:22:51,760
say this is the style of Wikipedia it's very factual you don't just ramble on you don't use

253
00:22:51,760 --> 00:23:00,000
colloquial language and things like that and so can you kind of describe the the general mechanism

254
00:23:00,000 --> 00:23:09,520
that the model is using to base output on these control codes sure um so essentially in a language

255
00:23:09,520 --> 00:23:16,160
model you have a sequence of words and at at the end you have a classifier and that classifier

256
00:23:16,160 --> 00:23:21,280
goes over the whole vocabulary sometimes that vocabulary can be just the characters of the English

257
00:23:21,280 --> 00:23:26,880
language for instance sometimes the vocabulary can be full words and in some cases they're so

258
00:23:26,880 --> 00:23:30,880
called bite-pair encodings and don't want to get too technical but it's basically like character

259
00:23:30,880 --> 00:23:38,000
sequences that are very frequent and you can basically at each time step classify what's the next

260
00:23:38,000 --> 00:23:43,440
most likely token that you want to generate and the control tokens are just another input in the

261
00:23:43,440 --> 00:23:51,760
beginning of these language models and say this is the current input and then after that you say

262
00:23:51,760 --> 00:23:57,280
now start you know you can also have a sequence of words to prime the model further and then it

263
00:23:57,280 --> 00:24:02,560
just continues to generate so it's essentially just another input also in a vector representation

264
00:24:02,560 --> 00:24:08,960
uh based on on these control tokens i'm trying to get at what's special like if i took gpt2 for

265
00:24:08,960 --> 00:24:14,560
example and you know started my prompt with Wikipedia i'm imagining that wouldn't produce the same

266
00:24:14,560 --> 00:24:22,320
effect as your model conditioned on Wikipedia as you're token that's right yeah because the original

267
00:24:22,320 --> 00:24:28,240
language models just took raw text they didn't think about metadata of that text so it's not

268
00:24:28,240 --> 00:24:34,800
hard to say like a knife and then control like in which direction you want that to go it'll

269
00:24:34,800 --> 00:24:40,480
generate whatever is the most likely based on all of the text it's seen okay right so you have no

270
00:24:40,480 --> 00:24:44,480
no way of controlling it in any which direction you want it to go you want to have a colloquial

271
00:24:44,480 --> 00:24:51,920
story you want to have a tweet you want to have a very factual story that sounds very you know

272
00:24:51,920 --> 00:24:58,080
standard uh and so on so this is kind of the metadata associated with with language and you can use

273
00:24:58,080 --> 00:25:02,640
that to control where the output uh is headed in in which general direction it should

274
00:25:02,640 --> 00:25:09,840
so then we trained these language models uh it's quite exciting uh and uh we we released a whole

275
00:25:09,840 --> 00:25:16,160
model and it's it's basically the largest model that would still fit on on fit on a general gpu

276
00:25:16,160 --> 00:25:22,960
that you couldn't get uh AWS like a p100 um and and once it's larger there are some larger language

277
00:25:22,960 --> 00:25:27,680
models that people have trained but they become impossible for anybody else to run like and we're

278
00:25:27,680 --> 00:25:34,240
up to like 17 billion parameters now that's Microsoft that's right and so you know in Nvidia I had

279
00:25:34,240 --> 00:25:39,360
some other ones but they're just like okay if you have a gigantic you know a million dollar cluster

280
00:25:39,360 --> 00:25:46,720
then and you can do this or you want to spend a lot of money per hour of even just loading it up

281
00:25:46,720 --> 00:25:52,800
and and and so on so we felt like this would marketize a little bit what these methods can do uh

282
00:25:52,800 --> 00:25:58,080
and allow basically anybody who can spawn a reasonably cheap AWS instance to play around with

283
00:25:58,080 --> 00:26:03,040
these models and and fine tune them too and and make them work for their uh their particular use

284
00:26:03,040 --> 00:26:08,160
cases and then we thought about what what would other use cases be where else do you have data and

285
00:26:08,160 --> 00:26:15,600
you want to generate useful sequences and uh that uh in in our group we have uh Ali Madani uh

286
00:26:15,600 --> 00:26:21,680
who's a great researcher in our group he's working some medical applications around computer vision

287
00:26:21,680 --> 00:26:30,000
two uh and he basically wanted to study the language of biology uh and basically tried to generate

288
00:26:30,000 --> 00:26:36,160
proteins in a controllable fashion and proteins uh and I don't want to get to technical in the

289
00:26:36,160 --> 00:26:41,360
biocide also I'm not a biologist but basically proteins govern everything uh in human biology

290
00:26:41,360 --> 00:26:48,240
in our bodies and viruses everything and so this AI system which we called progen uh for protein

291
00:26:48,240 --> 00:26:54,000
generation uh with this uh controllable language model is this really high capacity language model

292
00:26:54,000 --> 00:27:01,520
that was trained on the largest protein database that's available so we had 280 million proteine samples

293
00:27:01,520 --> 00:27:08,080
and now we can actually do something as really challenging for science and basically unlock the

294
00:27:08,080 --> 00:27:13,920
potential of protein engineering for synthetic biology material science and human health and we

295
00:27:13,920 --> 00:27:20,320
started this project uh almost a year ago uh and at the time we thought about you know really

296
00:27:20,320 --> 00:27:26,240
pressing uh diseases like cancer and and others uh or material science where you can try to generate

297
00:27:26,240 --> 00:27:31,280
bacteria that might eat plastic and things like that so it was pretty good uh and it's a super

298
00:27:31,280 --> 00:27:37,760
fascinating area of research that I think can have a lot of uh positive potential for the world

299
00:27:37,760 --> 00:27:44,400
and the environment and people but now of course uh it's COVID-19 is is on everybody's mind and

300
00:27:45,120 --> 00:27:51,760
it turns out that even there uh like two years ago the Nobel price in medicine uh was started to

301
00:27:51,760 --> 00:27:58,720
uh or was given to a team that built uh was able to create synthesize new proteins that didn't

302
00:27:58,720 --> 00:28:07,120
exist before to that had a specific function and in the case of these control tokens now the

303
00:28:07,120 --> 00:28:15,040
functions can be a specific type of control token like make the the cell glow or uh make this

304
00:28:15,040 --> 00:28:23,520
uh particular cell uh be a spike protein for a specific receptor and things like that and that

305
00:28:23,520 --> 00:28:32,800
is exactly uh what we can now do for for trying to work uh on on viruses and and antibodies and

306
00:28:32,800 --> 00:28:37,840
things like that to try to generate the right antibody and things like that so it is it just me or

307
00:28:37,840 --> 00:28:43,920
is it pretty amazing that you know we can now build AI models that we're solving you know that

308
00:28:43,920 --> 00:28:49,680
can be applied to what a couple of years ago won a Nobel Prize. Yeah well so yes and no I mean

309
00:28:49,680 --> 00:28:54,560
I don't want to push that too far but that's right that's right and so we we have to we have to be

310
00:28:54,560 --> 00:28:59,040
careful uh it's still very ongoing research and I don't want to hype it up too much until we have

311
00:28:59,040 --> 00:29:04,480
more real results but basically the previous line of research that won the Nobel price they basically

312
00:29:04,480 --> 00:29:11,760
randomly permuted these proteins and then tested out many many iterations until they found something

313
00:29:11,760 --> 00:29:18,080
that would have the actual biology and functions that they wanted and in our case we're able to

314
00:29:18,080 --> 00:29:25,440
reduce the search space because you know there there are a lot of potential uh protein sequences

315
00:29:25,440 --> 00:29:30,240
that you might get with these random permutations but if you had a sense from reading 280

316
00:29:30,240 --> 00:29:35,520
million of these proteins before that a sense of what's a reasonable structure what's most likely

317
00:29:35,520 --> 00:29:41,600
to come after this sequence of uh characters and amino acids and so on uh then you might create

318
00:29:41,600 --> 00:29:49,200
much more uh guided and directionally correct proteins and we actually see you know the energy

319
00:29:49,200 --> 00:29:54,560
how stable they would be and there are a couple of metrics uh that we show uh in the progen paper

320
00:29:55,280 --> 00:30:00,800
that the proteins that this model generates are much more likely to be viable to be synthesized

321
00:30:00,800 --> 00:30:06,240
and to have the functions that you want them to have then randomly permutated ones and that's obviously

322
00:30:06,240 --> 00:30:12,880
a relatively low baseline right random permutation is not that okay better than random permutation

323
00:30:12,880 --> 00:30:19,920
but it's really hard because the human AI the human intelligent system has evolved uh over many

324
00:30:19,920 --> 00:30:28,240
many years to do a deal with human language and the statistics and correlations that human language

325
00:30:28,240 --> 00:30:35,120
has and so we're not very good at looking at you know a b like so the contribution here is not that

326
00:30:35,120 --> 00:30:40,640
you're making a dent in this actual problem but rather that you're applying this language model

327
00:30:40,640 --> 00:30:47,520
that's built on human language to a totally different type of language that kind of underlies

328
00:30:47,520 --> 00:30:53,360
biology that's correct and now and then we show that this new tool can be useful for a broad range

329
00:30:53,360 --> 00:31:00,880
uh of of applications and now we're actually collaborating with people to actually go and and

330
00:31:00,880 --> 00:31:05,840
generate and synthesize these proteins and we're partnering uh of a lot of different universities

331
00:31:05,840 --> 00:31:10,160
and now there's a lot of exciting space a lot of exciting work in this space but it's all a little

332
00:31:10,160 --> 00:31:16,000
bit too early to to tell we just uh released uh progen and so now uh sort of follow up work as

333
00:31:16,000 --> 00:31:20,400
is in the pipeline and so can you go into that follow up a little in a little bit more detail

334
00:31:20,400 --> 00:31:29,760
in the sense of to you know validate the results of this model what has to happen it sounds like it's

335
00:31:29,760 --> 00:31:37,200
what it's ultimately producing is candidate proteins that have to then be validated uh

336
00:31:37,200 --> 00:31:42,320
experimentally is that the idea or that basically it yeah you first want to generate them

337
00:31:42,320 --> 00:31:47,760
computationally then you want to synthesize synthesize the actual proteins in real biology

338
00:31:47,760 --> 00:31:54,080
create the molecules then you want to see if uh on a very simple chemistry level they would actually

339
00:31:54,080 --> 00:31:59,920
exhibit uh the functions that you want if you then think you have some successful candidates

340
00:31:59,920 --> 00:32:06,320
then you would want to run some animal studies and eventually you hope to create new antibodies

341
00:32:06,320 --> 00:32:12,960
new uh ways to deal with uh and uh protect people from certain diseases long line there's

342
00:32:12,960 --> 00:32:17,040
there are many many steps and then obviously we don't have a wet lab and things like that so we're

343
00:32:17,040 --> 00:32:24,240
going to depart people on that we we only go so far uh-huh uh-huh is there it's your knowledge

344
00:32:24,240 --> 00:32:31,040
is there role for simulation or do we are we not there in terms of simulating protein structure

345
00:32:31,040 --> 00:32:38,480
before you go and actually synthesize you could and I guess uh some people work work in that space

346
00:32:38,480 --> 00:32:43,760
it might also be helpful it turns out it's not actually that expensive to synthesize a protein

347
00:32:43,760 --> 00:32:51,920
it's about ten bucks okay uh and so uh depending on the time cost computation tradeoffs and so on

348
00:32:51,920 --> 00:32:56,720
uh you and if you really reasonably certain that you were able to give good candidates to be

349
00:32:56,720 --> 00:33:02,400
actually synthesized then maybe yeah but yeah it's like I said it's very early work synthetic

350
00:33:02,400 --> 00:33:06,800
biology as a field has been around for a while but it's still quite nascent in comparison to a lot

351
00:33:06,800 --> 00:33:11,600
of other fields so I don't want to go into too many details until we have some more more concrete

352
00:33:11,600 --> 00:33:19,840
results yeah yeah and is the you know the idea in this line of work around language models to

353
00:33:20,560 --> 00:33:25,760
hey we've got some interesting thing here with proteins let's you know push that a lot further

354
00:33:25,760 --> 00:33:33,600
or hey we you know demonstrated that we can apply this to you know this biological code what other

355
00:33:33,600 --> 00:33:39,600
types of codes or languages are that we can apply this to like where where you headed with

356
00:33:39,600 --> 00:33:46,960
this general uh you know line of research so I think there are a couple of really exciting areas

357
00:33:47,520 --> 00:33:53,600
of research uh several which have connections to language models in some ways I think of

358
00:33:53,600 --> 00:33:59,120
three equivalent super tasks of NLP actually language modeling is one question answering is another

359
00:33:59,120 --> 00:34:05,440
and dialogue systems is the third and they're equivalent in terms of in the sense that every NLP

360
00:34:05,440 --> 00:34:13,440
problem can be cast as one of those three tasks and you know you can sentiment analysis people

361
00:34:13,440 --> 00:34:18,080
think of it as just classification but really if you ask here's a sentence yes what is the sentiment

362
00:34:18,080 --> 00:34:25,760
of the sentence then the next word that you predict should be uh that you know the classification

363
00:34:25,760 --> 00:34:31,280
uh and the label that that sentence has so even sentiment analysis can be seen as question

364
00:34:31,280 --> 00:34:35,200
answering and then everything that's question answering is also language modeling and in some ways

365
00:34:36,000 --> 00:34:41,520
with insights were kind of irrelevant because some people said that yeah but so what but now we

366
00:34:41,520 --> 00:34:46,480
actually have these models with billions of parameters and we may actually be able to build a single

367
00:34:46,480 --> 00:34:51,440
model for all of NLP and that's been kind of my dream ever since I started in 2010 training

368
00:34:51,440 --> 00:34:57,040
these neural nets uh on on language and because I realized the underlying substrate matters less and

369
00:34:57,040 --> 00:35:01,600
less it becomes more and more important I think in the future to be able to train these large

370
00:35:01,600 --> 00:35:06,800
neural network architectures for multiple tasks so you can have transfer learning between them

371
00:35:06,800 --> 00:35:11,520
you can get to zero shot learning tasks like I want to be able to have NLP systems that answer

372
00:35:11,520 --> 00:35:16,480
questions that they haven't seen before in the training data and I think better that they're

373
00:35:16,480 --> 00:35:22,000
examples like that in squad the Stanford question answering data set uh but it mostly extracts phrases

374
00:35:22,000 --> 00:35:27,680
from a from a document and it doesn't really have to capture complex knowledge uh in and of itself

375
00:35:28,320 --> 00:35:35,280
but language models are very tied to this uh also for I think one of the most exciting and least

376
00:35:35,280 --> 00:35:40,880
soft tasks of our time and that is summarization uh we've had a bunch of summary papers in the group

377
00:35:40,880 --> 00:35:49,040
two uh white check and in our group has just released one on factual uh correctness and consistency

378
00:35:49,040 --> 00:35:56,000
of uh summaries it turns out the whole field of summarization worked with pretty bad datasets

379
00:35:56,000 --> 00:36:02,080
and including ourselves some of the models that were very we're called them abstractive uh versus

380
00:36:02,080 --> 00:36:07,920
extractive so the difference there is extractive summarization just takes chunks and phrases

381
00:36:07,920 --> 00:36:12,000
and sentences from the original document you want to summarize just says this is an important

382
00:36:12,000 --> 00:36:17,680
sentence and then you have it uh whereas abstractive you have the hope that you would understand

383
00:36:17,680 --> 00:36:23,760
what's going on and then you can rephrase it in a different or shorter form uh and some of these

384
00:36:23,760 --> 00:36:28,720
language models get us closer to being able to do that that's exactly right but it turns out they

385
00:36:28,720 --> 00:36:35,360
also often learn to copy phrases from the input uh in just a really clever way and so long

386
00:36:35,360 --> 00:36:43,280
story short the biggest problem uh for NLP and summarization is for instance that uh first names

387
00:36:43,280 --> 00:36:50,640
proper nouns are often have similar vector representations so Jason, Jeremy, John uh they all

388
00:36:50,640 --> 00:36:57,600
look like similar to to these neural network models they are a list of 500 roughly similar numbers

389
00:36:58,560 --> 00:37:03,600
and the problem with that is that in a summary it's really crucial like when you say like

390
00:37:03,600 --> 00:37:08,640
there's converter or you know an accident or something you mix up the names it's really

391
00:37:08,640 --> 00:37:13,200
really different right but to a model this isn't really that different into all the evaluation

392
00:37:13,200 --> 00:37:19,920
metrics of the field of summarization you don't get really dinged hard uh and reduce your metrics

393
00:37:19,920 --> 00:37:25,840
by mixing up one name with with another but for people it's very crucial and so uh looking at

394
00:37:25,840 --> 00:37:31,600
factual uh consistency and accuracy of summarization is something that we care about a lot

395
00:37:32,240 --> 00:37:36,400
and interactively working on and there's again a connection to the language models too.

396
00:37:36,400 --> 00:37:44,320
Huge uh commercial implications of this like we're all inundated with textual data

397
00:37:44,960 --> 00:37:53,040
and you know if we could easily and cost effectively and accurately summarize that that would be

398
00:37:53,040 --> 00:37:57,280
huge. I'm just speaking from personal experience and 500 open browser tabs.

399
00:37:58,320 --> 00:38:06,160
That's exactly right I think it's a super impactful uh technology for our time where ignorance is

400
00:38:06,160 --> 00:38:12,480
almost a choice uh for for most people right but the fact is that there's too much information

401
00:38:12,480 --> 00:38:18,320
and sorry to really get through it and that's what AI is really good good at right uh take

402
00:38:18,320 --> 00:38:23,040
laborious tasks that just don't scale with people and automate them for us.

403
00:38:23,840 --> 00:38:29,840
And going you know for a full circle or at least circling back to progen I've talked to

404
00:38:29,840 --> 00:38:38,480
the folks that are not necessarily trying to apply summarization to scientific literature but

405
00:38:38,960 --> 00:38:46,160
to otherwise use AI to mine insights from scientific literature and kind of project into

406
00:38:46,800 --> 00:38:53,840
you know where future innovation might come from and you know summarization that is you know

407
00:38:53,840 --> 00:39:00,240
the preserve kind of the scientific essence the facts of a you know research paper could be a step

408
00:39:00,240 --> 00:39:06,400
in that direction as well. Absolutely yeah like uh researchers like everybody else are inundated

409
00:39:06,400 --> 00:39:12,480
with too much information right if you try to follow the archive uh and and see all the new papers

410
00:39:12,480 --> 00:39:18,240
that come around AI in just specific fields uh it's it's almost impossible to keep up with the

411
00:39:18,240 --> 00:39:24,160
literature and still do your own work and not just be reading on top uh and so I think summarization

412
00:39:24,160 --> 00:39:30,560
is going to become more exciting and now that we have solved other tasks that can be a little less

413
00:39:30,560 --> 00:39:37,440
ambiguous uh the field of NLP can kind of move towards these these tougher more ambiguous kinds of

414
00:39:37,440 --> 00:39:43,760
tasks. What do I mean by this? So for instance machine translation uh you have a pretty small set

415
00:39:43,760 --> 00:39:49,600
of uh outputs that make sense given an input sentence from German that you want to translate into

416
00:39:49,600 --> 00:39:55,360
English short there may be like two or three different variants like thank you it can be translated

417
00:39:55,360 --> 00:40:00,880
as fiendank or dankershörn. I know two two options but it doesn't make sense to just say like

418
00:40:00,880 --> 00:40:06,560
katsun to it to just cadendok like it so you have a pretty small variant in the outputs that

419
00:40:06,560 --> 00:40:12,640
makes sense for that input to be generated. On the other hand in summarization boy there's so many

420
00:40:12,640 --> 00:40:20,080
possible variants and uh when you think about it in the end summarization is also highly contextual

421
00:40:20,080 --> 00:40:26,000
and needs to in the limit be highly personalized. For instance when Elmo came out the first sort

422
00:40:26,000 --> 00:40:32,160
of language model uh that had really really good representations uh personally to me a good

423
00:40:32,160 --> 00:40:36,560
summary of that paper would have been oh it's like cove contextual vectors but instead of

424
00:40:36,560 --> 00:40:41,280
trained with machine translation it's trained as a language modeling objective and it works really

425
00:40:41,280 --> 00:40:46,320
well on a lot of different tasks. But if you don't know what Elmo or language models or word

426
00:40:46,320 --> 00:40:51,680
vectors are that is a completely useless summary and really the good summary of that paper introduces

427
00:40:51,680 --> 00:40:57,280
you and maybe it pulls in even text from elsewhere and helps understand what is going on in the first

428
00:40:57,280 --> 00:41:03,520
place with this field and then gives you sort of the incremental novelty of that uh you know new

429
00:41:03,520 --> 00:41:08,400
research result that comes out so and the same is true for for every sort of conflict you might

430
00:41:08,400 --> 00:41:14,320
follow on earth or if you read you know COVID-19 news every day uh you don't need to know

431
00:41:14,320 --> 00:41:19,360
there's a virus but if you just came from that arctic uh and you know for last month and all

432
00:41:19,360 --> 00:41:24,880
the time there's a lot of new stuff uh use the summaries would be very very different and obviously

433
00:41:24,880 --> 00:41:30,480
there's different sort of reading levels if you're doing summarization for kids versus adults and

434
00:41:30,480 --> 00:41:34,880
and so on. So I think summarization there's a lot of interesting things that we can still do in

435
00:41:34,880 --> 00:41:41,200
that thing. Well Richard thanks so much for taking the time to catch up with me and share with all

436
00:41:41,200 --> 00:41:47,280
of us what you're up to there. That sounds like really cool stuff and looking forward to checking

437
00:41:47,280 --> 00:41:52,400
in more frequently uh with you and keeping up with what you're doing there. Yeah, love your

438
00:41:52,400 --> 00:41:58,240
podcast and uh always always enjoyed talking about AI. So thanks for having me. Take care, thanks

439
00:41:58,240 --> 00:42:07,760
so much. All right everyone that's our show for today. For more information on today's show

440
00:42:07,760 --> 00:42:28,800
visit twomolai.com slash shows. As always thanks so much for listening and catch you next time.

