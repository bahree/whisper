1
00:00:00,000 --> 00:00:29,560
All right, it looks like it is live, and let me just get the turn of volume off there.

2
00:00:29,560 --> 00:00:36,560
There is a delay if you, I don't recommend necessarily that you pull it up, but we'll

3
00:00:36,560 --> 00:00:40,640
be checking here to make sure everything's hunky-dory.

4
00:00:40,640 --> 00:00:44,560
But there is a delay, and it's very confusing if you have the current and the 22nd delay

5
00:00:44,560 --> 00:00:45,560
audio going.

6
00:00:45,560 --> 00:00:50,080
So if you do want to see it in 22nd delay, be sure to mute it.

7
00:00:50,080 --> 00:00:51,920
But we are live.

8
00:00:51,920 --> 00:00:58,920
Everyone, welcome to the first Twinmo Live, where we'll be talking about OpenAI's recent

9
00:00:58,920 --> 00:01:08,600
GPT-2 language model release and announcement and controversy and exploring the host of

10
00:01:08,600 --> 00:01:10,680
issues that it raises.

11
00:01:10,680 --> 00:01:18,880
I am Sam Charrington, your host for the discussion, and for those of you who may have stumbled

12
00:01:18,880 --> 00:01:25,200
across this and are not familiar with this week a machine learning in AI, it is a podcast

13
00:01:25,200 --> 00:01:31,560
that I launched coming up on three years ago that's really dedicated to informing and

14
00:01:31,560 --> 00:01:35,520
educating people about machine learning and artificial intelligence, and I've been

15
00:01:35,520 --> 00:01:43,080
fortunate to have very many wonderful guests, including several of our panelists today.

16
00:01:43,080 --> 00:01:49,080
You can find the podcast easily at twinmolei.com.

17
00:01:49,080 --> 00:01:55,680
So before we dive into our discussion, I'd like to give our panelists an opportunity to

18
00:01:55,680 --> 00:01:57,600
introduce themselves.

19
00:01:57,600 --> 00:02:01,360
Anima, why don't you get us started?

20
00:02:01,360 --> 00:02:07,560
Hi, I'm Anima Anand Kumar, Director of Machine Learning Research at NVIDIA, as well as

21
00:02:07,560 --> 00:02:10,920
a professor at Caltech.

22
00:02:10,920 --> 00:02:17,840
So thank you, Sam, for doing this, I think you're always looking at the pulse of the community

23
00:02:17,840 --> 00:02:26,480
and this is a topic that has garnered a lot of recent interest and thought process.

24
00:02:26,480 --> 00:02:31,160
So thanks for doing this and I'm happy to be part of this.

25
00:02:31,160 --> 00:02:37,920
I do want to make a clarification that these are my personal comments.

26
00:02:37,920 --> 00:02:42,800
I'm a very well coincide with my employers, but I'm not assigned to be speaking on their

27
00:02:42,800 --> 00:02:46,240
behalf in this particular instance.

28
00:02:46,240 --> 00:02:48,120
Awesome, thank you.

29
00:02:48,120 --> 00:02:49,120
Amanda?

30
00:02:49,120 --> 00:02:51,280
Hi, I'm Amanda Askel.

31
00:02:51,280 --> 00:02:55,080
I'm a policy research scientist at OpenAI.

32
00:02:55,080 --> 00:03:03,400
My background is actually in ethics and since working on all areas really still like policy

33
00:03:03,400 --> 00:03:04,400
here.

34
00:03:04,400 --> 00:03:06,400
Awesome.

35
00:03:06,400 --> 00:03:08,400
Rob?

36
00:03:08,400 --> 00:03:12,160
Yeah, my name is Miles Rendage and I'm also on the policy team at OpenAI.

37
00:03:12,160 --> 00:03:16,840
My background is more in social science and tech policy and I have a particular interest

38
00:03:16,840 --> 00:03:20,840
in malicious uses of AI and was involved in a report last year on the topic, so it's

39
00:03:20,840 --> 00:03:23,640
part of my interest here.

40
00:03:23,640 --> 00:03:24,640
Awesome.

41
00:03:24,640 --> 00:03:25,640
Rob?

42
00:03:25,640 --> 00:03:27,120
Hey everyone, I'm Rob Monroe.

43
00:03:27,120 --> 00:03:29,480
I'm a VP of Product at Lilt.

44
00:03:29,480 --> 00:03:33,880
Lilt makes technology that combines human and machine translation.

45
00:03:33,880 --> 00:03:39,320
My background is mixed, I've been a founder and executive of a number of AI startups.

46
00:03:39,320 --> 00:03:44,800
In larger companies, I ran product for AWS's first natural language processing and translation

47
00:03:44,800 --> 00:03:45,800
services.

48
00:03:45,800 --> 00:03:51,640
And before I moved here to the US to get a PhD and then I'll be at Stanford, I was working

49
00:03:51,640 --> 00:03:56,720
in post-conflict development in Sierra Leone and Liberia for the United Nations and I've

50
00:03:56,720 --> 00:04:02,160
continued to work in disaster response, both for Man and Made and Natural Disaster System.

51
00:04:02,160 --> 00:04:03,160
Great.

52
00:04:03,160 --> 00:04:04,160
Great.

53
00:04:04,160 --> 00:04:05,160
And Stephen.

54
00:04:05,160 --> 00:04:12,200
Hi, I'm Stephen Marady, most commonly known on the internet as Smarity and I'm an independent

55
00:04:12,200 --> 00:04:18,360
AI researcher but the primary reason I'm interesting here is that I focus on language modeling

56
00:04:18,360 --> 00:04:22,960
as my research area and I've held steadily out results on some of those same results

57
00:04:22,960 --> 00:04:28,320
as OpenAI's model and two of the data sets that they get steadily out results on are

58
00:04:28,320 --> 00:04:29,320
mine.

59
00:04:29,320 --> 00:04:30,320
Awesome.

60
00:04:30,320 --> 00:04:31,320
Awesome.

61
00:04:31,320 --> 00:04:38,160
So let's dive right in and given your focus on this area, Stephen, you would be a great

62
00:04:38,160 --> 00:04:41,480
person to kind of provide some context for us.

63
00:04:41,480 --> 00:04:46,200
What is a language model and how are they used?

64
00:04:46,200 --> 00:04:51,560
Why are they important and what's the kind of context, the technical context in which

65
00:04:51,560 --> 00:04:53,400
this announcement was made?

66
00:04:53,400 --> 00:04:54,400
Right.

67
00:04:54,400 --> 00:04:58,680
So language modeling for anyone who hasn't run across it yet.

68
00:04:58,680 --> 00:05:03,280
If you have your phone out and you're doing your normal typing, the predictive keyboard

69
00:05:03,280 --> 00:05:08,040
is essentially the time you'll run into your language models immersed.

70
00:05:08,040 --> 00:05:13,200
So the aim is just to guess the next word in your sequence but then you can of course

71
00:05:13,200 --> 00:05:17,200
go for some more complicated steps along rather than guessing words.

72
00:05:17,200 --> 00:05:21,920
You could guess characters or turquins but yeah, the underlying technology is literally

73
00:05:21,920 --> 00:05:24,200
just guess the next turquin in the sequence.

74
00:05:24,200 --> 00:05:26,440
What of the sequence might be?

75
00:05:26,440 --> 00:05:32,000
And so you've most likely run into it with your phone but it's also used in speech recognition

76
00:05:32,000 --> 00:05:38,600
to disambiguate some words so the word recognition itself could be ref the ignition of a cough

77
00:05:38,600 --> 00:05:42,200
theoretically or it could be speech recognition.

78
00:05:42,200 --> 00:05:43,200
So it's used there.

79
00:05:43,200 --> 00:05:49,880
It's also used in a number of other situations in similar contexts or abstract summarization

80
00:05:49,880 --> 00:05:51,480
and so on.

81
00:05:51,480 --> 00:05:56,840
But most recently the kind of really interesting step has been that these incredibly complex

82
00:05:56,840 --> 00:06:02,800
language models if you run them over enough data can basically capture a bunch of sub-tasks

83
00:06:02,800 --> 00:06:07,440
that you don't ask it to capture by just guessing the next word but it might end up learning

84
00:06:07,440 --> 00:06:08,440
that anyway.

85
00:06:08,440 --> 00:06:13,520
So things like counting in some models we never tell them how a model is supposed to count

86
00:06:13,520 --> 00:06:18,120
but it ends up doing that to be able to guess text and you can then take this model and

87
00:06:18,120 --> 00:06:24,000
slot it into a more complex system where it kind of this knowledge that it's already gotten

88
00:06:24,000 --> 00:06:27,960
just to be able to guess the next word in the sequence well ends up transferring to these

89
00:06:27,960 --> 00:06:28,960
other tasks.

90
00:06:28,960 --> 00:06:35,400
So things like sentiment analysis or question answering or translation.

91
00:06:35,400 --> 00:06:41,760
Awesome and so how are there some standard tests it sounds like that are used to kind

92
00:06:41,760 --> 00:06:44,480
of assess the performance of language models.

93
00:06:44,480 --> 00:06:46,560
What are some of these tests?

94
00:06:46,560 --> 00:06:54,760
Yeah so the absolute standard is something called complexity which is basically how confused

95
00:06:54,760 --> 00:06:59,040
is a language model when you tell it what the next token is.

96
00:06:59,040 --> 00:07:02,560
So if you're guessing and you say New York and you're going to guess the next word and

97
00:07:02,560 --> 00:07:07,520
you say city if I said New York state you know maybe I'm not that confused by it because

98
00:07:07,520 --> 00:07:14,640
I was thinking that as well but usually it's city versus if you came up with New York static

99
00:07:14,640 --> 00:07:19,680
or something like that some completely unexpected word then you'll see the complexities spike.

100
00:07:19,680 --> 00:07:24,560
So basically the only aim for language modeling is to minimize how confused the model is

101
00:07:24,560 --> 00:07:31,000
it having seen like a given sequence and so that's kind of the metric that open AI focus

102
00:07:31,000 --> 00:07:34,040
on and kind of all these previous papers have as well.

103
00:07:34,040 --> 00:07:35,040
Okay.

104
00:07:35,040 --> 00:07:43,480
And so Miles and Amanda this work with GPT2 is as the two indicates the second in a series

105
00:07:43,480 --> 00:07:50,440
of research into language models can you talk a little bit about the background of this

106
00:07:50,440 --> 00:07:58,320
project as well as the type of model specifically that it represents namely transformer models.

107
00:07:58,320 --> 00:08:05,440
Yeah so I mean we've been interested in sort of unsupervised learning of useful representations

108
00:08:05,440 --> 00:08:10,720
of text for a while in the sentiment neuron paper or blog post I believe a year or two

109
00:08:10,720 --> 00:08:18,960
ago was an example of sort of early interest at open AI in the first GPT paper as well.

110
00:08:18,960 --> 00:08:25,960
The main difference in terms of you know what GPT2 versus previous transformer based language

111
00:08:25,960 --> 00:08:32,520
models is scale so it's not the biggest language model that's ever been produced but as far

112
00:08:32,520 --> 00:08:36,800
as we know it's the best performing model along various metrics including the quantitative

113
00:08:36,800 --> 00:08:43,880
ones as well as sort of qualitative assessment of the quality of long text production.

114
00:08:43,880 --> 00:08:48,720
And specifically in terms of like size you know the range is you know from millions to

115
00:08:48,720 --> 00:08:53,400
billions of parameters and the one that we have chosen not to release is 1.5 billion.

116
00:08:53,400 --> 00:08:57,600
So there have been bigger language models previously but what's interesting here is that

117
00:08:57,600 --> 00:09:01,640
there's a very diverse data set being used to produce it and make use of this larger capacity

118
00:09:01,640 --> 00:09:08,360
of the model and the GPT sort of model is easier to sample from than say birch or other

119
00:09:08,360 --> 00:09:13,440
sort of recent efforts to sort of push language modeling beyond this scale.

120
00:09:13,440 --> 00:09:21,320
Okay and the specific type of models a transformer model what is that represent?

121
00:09:21,320 --> 00:09:26,280
I think probably Samarity would be a better person to answer that.

122
00:09:26,280 --> 00:09:30,920
Okay so the transformer model many of the audience might have heard of like recurrent neural

123
00:09:30,920 --> 00:09:34,400
network so RNNs and LSTM is that type of thing.

124
00:09:34,400 --> 00:09:38,560
The idea with that would be imagine you could only see one word on a page at a time and

125
00:09:38,560 --> 00:09:41,920
you only had one button which was to go to the next word.

126
00:09:41,920 --> 00:09:45,840
So that's the way the LSTM or recurrent neural network ends up looking at text and trying

127
00:09:45,840 --> 00:09:47,720
to guess the next word.

128
00:09:47,720 --> 00:09:51,120
And the problem with that is I don't know about the rest of the panelists but my memory

129
00:09:51,120 --> 00:09:56,400
is terrible so about 10 words in I'll have forgotten what everything was behind that.

130
00:09:56,400 --> 00:09:59,960
The idea with the transformer network is instead of having this you know step along one at

131
00:09:59,960 --> 00:10:00,960
a time.

132
00:10:00,960 --> 00:10:05,760
You say okay I have 100 words and I'm trying to guess the next word.

133
00:10:05,760 --> 00:10:10,040
The word at the very end can basically talk to all of the words previous to it and try

134
00:10:10,040 --> 00:10:14,440
and pull in some of their knowledge based on whether or not you know I should be essentially

135
00:10:14,440 --> 00:10:15,440
talking to you.

136
00:10:15,440 --> 00:10:20,640
So if I'm about to say if I the last word is president I might look back the last 100

137
00:10:20,640 --> 00:10:26,040
characters 100 words and I might find some other exact instances of president so a good

138
00:10:26,040 --> 00:10:30,880
idea there would be just to grab the next word along or you might do something more complicated

139
00:10:30,880 --> 00:10:33,560
where multiple words kind of have to chat to each other.

140
00:10:33,560 --> 00:10:39,080
And so that's the idea of this attention phase where words can look around at other words

141
00:10:39,080 --> 00:10:43,280
based on how you know interesting they are to the back of the word and you go through

142
00:10:43,280 --> 00:10:48,000
multiple stages of this and hopefully at the end all of kind of the relevant knowledge

143
00:10:48,000 --> 00:10:52,440
from this sequence will be captured in the very word at the end and that word can go

144
00:10:52,440 --> 00:10:53,440
through.

145
00:10:53,440 --> 00:10:54,440
Cool.

146
00:10:54,440 --> 00:10:58,440
I was going to say in Obama or Trump or Nixon or something like that.

147
00:10:58,440 --> 00:10:59,440
Awesome.

148
00:10:59,440 --> 00:11:00,440
Awesome.

149
00:11:00,440 --> 00:11:09,680
So a big part of the controversy I suppose with the release of this model was you know

150
00:11:09,680 --> 00:11:15,760
not so much the research itself and the technical details but kind of the way the model was

151
00:11:15,760 --> 00:11:16,760
released.

152
00:11:16,760 --> 00:11:21,720
Robin wondering if you can maybe provide some context from your perspective just reflecting

153
00:11:21,720 --> 00:11:25,920
on the release and the firestorm that it created at least in Twitter.

154
00:11:25,920 --> 00:11:28,280
I don't want to over amplify it.

155
00:11:28,280 --> 00:11:31,840
Twitter can be a bit of an echo chamber as we all know.

156
00:11:31,840 --> 00:11:37,880
But what's your take on you know some of the things that were maybe controversial about

157
00:11:37,880 --> 00:11:38,880
the announcement?

158
00:11:38,880 --> 00:11:39,880
Yeah.

159
00:11:39,880 --> 00:11:40,880
Happy to.

160
00:11:40,880 --> 00:11:44,880
And Maul's Amanda correct me if I'm characterizing up an AI wrong.

161
00:11:44,880 --> 00:11:50,360
So I believe that open AI decided not to make this model public which is something that's

162
00:11:50,360 --> 00:11:53,280
been standard recently in the research community.

163
00:11:53,280 --> 00:11:59,720
And the reason behind this was because the potential negative use cases outweigh the positive

164
00:11:59,720 --> 00:12:00,720
ones.

165
00:12:00,720 --> 00:12:06,320
So you could get bad actors who could use a model like this to for example generate fake

166
00:12:06,320 --> 00:12:11,200
news which stylistically sounded very much like a real person.

167
00:12:11,200 --> 00:12:19,560
And so it could be used for things like election meddling or generally creating discontent

168
00:12:19,560 --> 00:12:24,400
on the internet both by individual trolls or potentially state sponsored.

169
00:12:24,400 --> 00:12:28,160
So is that is that is that correct of an AI folk?

170
00:12:28,160 --> 00:12:29,160
Yeah.

171
00:12:29,160 --> 00:12:33,640
I mean one thing I'd clarify is that you know we did not claim nor are we confident that

172
00:12:33,640 --> 00:12:39,280
the out that the negative uses of GPT-2 would outweigh the risks but rather that we're

173
00:12:39,280 --> 00:12:44,280
not confident that they wouldn't and that you know this is sort of what seems to us like

174
00:12:44,280 --> 00:12:50,080
a sort of precautionary approach in this context of given the sort of you know your reversibility

175
00:12:50,080 --> 00:12:51,080
of release.

176
00:12:51,080 --> 00:12:52,560
You want to add anything to that?

177
00:12:52,560 --> 00:12:53,560
Yeah.

178
00:12:53,560 --> 00:12:54,560
Now this sounds correct.

179
00:12:54,560 --> 00:12:56,760
So I think it's easy to think that you have to have like really high confidence that

180
00:12:56,760 --> 00:13:02,320
what you're releasing is going to have negative consequences before you decide to at

181
00:13:02,320 --> 00:13:04,240
least do a partial release.

182
00:13:04,240 --> 00:13:10,280
I think our thought was that caution early is a good plan and then to try and get feedback

183
00:13:10,280 --> 00:13:15,520
on this approach so it might be that you know one criticism might be that this is kind

184
00:13:15,520 --> 00:13:21,600
of like too preemptive or too early and I think it's just that the costs of starting

185
00:13:21,600 --> 00:13:26,280
to think about these things early are generally lower than the costs of thinking about them

186
00:13:26,280 --> 00:13:31,280
too late when you are fairly confident that the misuse risk is high and so this was like

187
00:13:31,280 --> 00:13:36,080
some of the kind of reasoning that went behind this and then as Miles said you know deciding

188
00:13:36,080 --> 00:13:40,560
to do a partial release is reversible whereas deciding to do a full release is not reversible

189
00:13:40,560 --> 00:13:45,640
so exercising caution can mean initially doing a partial release and that was what you decided

190
00:13:45,640 --> 00:13:46,640
to do.

191
00:13:46,640 --> 00:13:50,440
Because yeah I guess that's a very thorough distinction not yet knowing rather than being

192
00:13:50,440 --> 00:13:52,880
confident that it was necessarily bad.

193
00:13:52,880 --> 00:13:56,840
Yeah and so this is like certainly a decision process that I've had to be go through many

194
00:13:56,840 --> 00:13:58,560
times in the past.

195
00:13:58,560 --> 00:14:08,320
So in disaster response in Chinabaspring in particular thinking about what kinds of data

196
00:14:08,320 --> 00:14:13,720
were being collected and how for example if you take a tweet of someone reporting a blocked

197
00:14:13,720 --> 00:14:19,560
road they don't know why it's blocked but then all of a sudden you recontextualize that

198
00:14:19,560 --> 00:14:24,800
and your reporting that there are rebel movements in an area and now that the rebels know

199
00:14:24,800 --> 00:14:27,440
that you've reported them.

200
00:14:27,440 --> 00:14:32,440
So if you're very careful about taking other people's data and releasing that and certainly

201
00:14:32,440 --> 00:14:37,360
you know build in a model on open data like overnight I did and then we release in that

202
00:14:37,360 --> 00:14:42,000
accounts as recontextualized as in.

203
00:14:42,000 --> 00:14:49,000
Similarly I've seen very full more propaganda from a cost radio through to social media

204
00:14:49,000 --> 00:14:53,440
used in in Sierra Leone Liberia country studies to live in and I've worked in a lecture

205
00:14:53,440 --> 00:14:57,960
monitoring in during their elections there and even when there weren't state sponsored

206
00:14:57,960 --> 00:15:03,800
actors made decisions like this as well as responding to earthquake in Haiti in 2010

207
00:15:03,800 --> 00:15:07,880
and a centralized data which machine learning scientists can build on.

208
00:15:07,880 --> 00:15:13,080
We deliberately admitted all the data which were reported on a company minus children

209
00:15:13,080 --> 00:15:18,840
who are alone and for this very reason we believed that the potential negative use cases

210
00:15:18,840 --> 00:15:23,040
was something we couldn't protect well for and so this wasn't something that we wanted

211
00:15:23,040 --> 00:15:24,640
to make available.

212
00:15:24,640 --> 00:15:28,240
So I appreciate that decision process.

213
00:15:28,240 --> 00:15:36,040
To characterize that the alchrist I think that there's two aspects to it one at least

214
00:15:36,040 --> 00:15:40,560
me at least I didn't see the context in how this decision was made it felt a little bit

215
00:15:40,560 --> 00:15:44,720
buried in the paper and it probably deserved more space in the article.

216
00:15:44,720 --> 00:15:48,520
Hope I'm preaching to the converted because we've got open AI's ethics people here not

217
00:15:48,520 --> 00:15:54,280
there other machine learning people and I think the other which is less than me to speak

218
00:15:54,280 --> 00:16:01,240
about I call myself a practicing researcher now is that the paper very proudly reported

219
00:16:01,240 --> 00:16:06,800
new state of doubt results for a model that wasn't then immediately available to the research

220
00:16:06,800 --> 00:16:09,600
community to replicate.

221
00:16:09,600 --> 00:16:16,640
Yeah, I can add one point on sort of the norm that you mentioned around openness I think

222
00:16:16,640 --> 00:16:21,680
it's important to sort of be clear that it was not the case that before this everyone

223
00:16:21,680 --> 00:16:25,840
always release all their state of the art models all the time it's rather that it was

224
00:16:25,840 --> 00:16:31,520
rarely or you know it was rarely the case that people would explicitly use sort of misuse

225
00:16:31,520 --> 00:16:36,680
of the model of the sorts and of the source that we're worried about as a justification

226
00:16:36,680 --> 00:16:42,040
as opposed to profit or sort of you know keeping things underrapener to have a big announcement

227
00:16:42,040 --> 00:16:46,040
or something like that so it's the motivation for non-release that I think is distinctive

228
00:16:46,040 --> 00:16:49,360
as opposed to that we didn't publish everything.

229
00:16:49,360 --> 00:16:53,800
Was there a big recent paper in language modeling where they didn't release?

230
00:16:53,800 --> 00:16:58,400
So I'm thinking of the big recent ones kind of an international order being laser from

231
00:16:58,400 --> 00:17:04,760
Facebook, Bert from Google and Elmo from Alan Institute.

232
00:17:04,760 --> 00:17:09,240
I believe they all release their models where other others are not aware of.

233
00:17:09,240 --> 00:17:13,080
Yeah, so I was speaking more about AI generally it might be the case that there's more of a

234
00:17:13,080 --> 00:17:17,400
tendency towards publication in the language model literature specifically.

235
00:17:17,400 --> 00:17:20,920
Right, right.

236
00:17:20,920 --> 00:17:29,600
And so part of the issue that this raises is around reproducibility and I'm sure we'll

237
00:17:29,600 --> 00:17:32,720
come back to that in the conversation.

238
00:17:32,720 --> 00:17:38,640
Anima you've also raised some issues around the kind of the way it's been handled from

239
00:17:38,640 --> 00:17:41,800
just a reporting and media relations perspective.

240
00:17:41,800 --> 00:17:43,640
Can you elaborate on those?

241
00:17:43,640 --> 00:17:50,000
Yeah, I mean, you know, I certainly understand that it's important to think about malicious

242
00:17:50,000 --> 00:17:55,400
use cases and all the impact of releasing a certain technology, right.

243
00:17:55,400 --> 00:18:02,080
So I'm appreciative of the thought process that goes into the risk analysis but as Miles

244
00:18:02,080 --> 00:18:08,320
pointed out that's never been the reason for not releasing a language model before.

245
00:18:08,320 --> 00:18:14,720
So that's, you know, suddenly like where the new risk is coming up that didn't come up

246
00:18:14,720 --> 00:18:19,640
with the earlier models which are nearly as good if not as good.

247
00:18:19,640 --> 00:18:23,920
And also the claim that this is much better than the previous models is not even something

248
00:18:23,920 --> 00:18:30,120
the research community has verified or can easily verify without access to the models.

249
00:18:30,120 --> 00:18:36,080
So there is the issue of like truly understanding what the capabilities of the most recent

250
00:18:36,080 --> 00:18:43,240
model are and having independent researchers evaluate that effectiveness.

251
00:18:43,240 --> 00:18:49,200
And before all that's done, it felt like the media was at the center stage of all this,

252
00:18:49,200 --> 00:18:50,200
right.

253
00:18:50,200 --> 00:18:56,520
The access was only given to journalists and in a very limited way to write articles for

254
00:18:56,520 --> 00:18:57,520
the public.

255
00:18:57,520 --> 00:19:03,520
So before it reached the research community before there was any chance to evaluate its

256
00:19:03,520 --> 00:19:13,000
technical capabilities, you know, there was these huge like kind of media blitz on, you

257
00:19:13,000 --> 00:19:18,320
know, the terminator coming, it's gotten so dangerous that AI needs to be locked up in

258
00:19:18,320 --> 00:19:25,240
a wall to these kind of articles, you know, promoted a lot of fear mongering that's already

259
00:19:25,240 --> 00:19:28,960
been present in some of these general media articles.

260
00:19:28,960 --> 00:19:35,960
And that's where this distortion of the scientific facts and the current capabilities that

261
00:19:35,960 --> 00:19:38,520
I have big issues with.

262
00:19:38,520 --> 00:19:39,520
Okay.

263
00:19:39,520 --> 00:19:47,640
And another issue that's been raised along the lines of the reproducibility concern has

264
00:19:47,640 --> 00:19:55,400
been one about open source and the model's source code being open.

265
00:19:55,400 --> 00:19:59,720
Steven, is that one that you have a daughter?

266
00:19:59,720 --> 00:20:07,440
Yeah, so the idea that the model being open, that's been a pretty popular idea for many

267
00:20:07,440 --> 00:20:09,640
of the AI research labs.

268
00:20:09,640 --> 00:20:11,880
As Miles mentioned, it isn't the default.

269
00:20:11,880 --> 00:20:16,200
There are many kind of papers that don't publish their models or don't publish that occurred.

270
00:20:16,200 --> 00:20:22,480
But you know, one of the main ideas behind kind of this entire field is the open nature

271
00:20:22,480 --> 00:20:23,480
of our work.

272
00:20:23,480 --> 00:20:25,600
We've published the papers on the archive.

273
00:20:25,600 --> 00:20:27,680
There's no payment to get the paper.

274
00:20:27,680 --> 00:20:34,160
The techniques are generally very well learned by everyone involved and there are free toolkits

275
00:20:34,160 --> 00:20:36,000
or frameworks.

276
00:20:36,000 --> 00:20:40,880
And to this stage, you know, Google's collaboratory or what have you, they can give you free

277
00:20:40,880 --> 00:20:42,040
GPUs.

278
00:20:42,040 --> 00:20:45,880
This is a great idea because, you know, basically anyone can get involved.

279
00:20:45,880 --> 00:20:49,760
If someone would like to, they could take open AI's language model or, you know, one

280
00:20:49,760 --> 00:20:55,520
from Google and video and test and see how it works, potentially use it for the application.

281
00:20:55,520 --> 00:21:01,000
So open, reproducible research, it kind of hits all those lines down, whether or not

282
00:21:01,000 --> 00:21:03,880
someone can take the model and improve it, whether they can take it to use it for an

283
00:21:03,880 --> 00:21:09,520
interesting application, whether they can just explore the current capabilities as, you

284
00:21:09,520 --> 00:21:13,080
know, maybe a researcher trying to understand the latest advances.

285
00:21:13,080 --> 00:21:15,080
Yeah.

286
00:21:15,080 --> 00:21:19,480
And at some point, I'll come back to kind of my also perspective on, well, basically

287
00:21:19,480 --> 00:21:24,880
like I love the kind of discussion and the open AI model itself.

288
00:21:24,880 --> 00:21:28,760
I'm, you know, always interested in language modeling research, but I feel like one issue

289
00:21:28,760 --> 00:21:30,640
was that everything kind of came together.

290
00:21:30,640 --> 00:21:32,520
It was a new language model.

291
00:21:32,520 --> 00:21:37,200
It was discussions about responsible disclosure, how journalists react to AI research and

292
00:21:37,200 --> 00:21:40,400
publication and then how the general media consumes it.

293
00:21:40,400 --> 00:21:44,720
I think that was one of the main things kind of powering this confused firestorm on Twitter

294
00:21:44,720 --> 00:21:46,160
and potentially in media.

295
00:21:46,160 --> 00:21:49,760
Yeah, Amanda and Miles, you're shaking your heads.

296
00:21:49,760 --> 00:21:52,800
We agree, we agree.

297
00:21:52,800 --> 00:21:56,160
Thoughts from, from you.

298
00:21:56,160 --> 00:21:57,160
You're muted.

299
00:21:57,160 --> 00:21:59,440
That was my fault.

300
00:21:59,440 --> 00:22:00,440
Yeah.

301
00:22:00,440 --> 00:22:02,880
Like, there's a lot of points being raised.

302
00:22:02,880 --> 00:22:08,600
I think one thing, I just want to go back to a point that was made earlier, where there's

303
00:22:08,600 --> 00:22:12,400
this kind of question of like, well, it's like, you know, we've seen lots of really impressive

304
00:22:12,400 --> 00:22:17,120
language models before, the way that this was presented was like, as if this is some

305
00:22:17,120 --> 00:22:21,120
kind of like changing kind, you know, what's this new risk that's arising that wasn't

306
00:22:21,120 --> 00:22:22,960
arising before?

307
00:22:22,960 --> 00:22:29,600
And I think on this, one thing that's worth noting is just that if machine learning is

308
00:22:29,600 --> 00:22:34,400
an area where you see incremental progress, one concern you might have is the point at

309
00:22:34,400 --> 00:22:40,960
which it makes sense to do something like partial disclosure of research is always going

310
00:22:40,960 --> 00:22:44,480
to kind of look like the wrong point, because it's going to be this sudden shift from

311
00:22:44,480 --> 00:22:49,840
full disclosure of models in the case of like language models to like a partial release,

312
00:22:49,840 --> 00:22:52,960
like what we did here.

313
00:22:52,960 --> 00:22:56,880
But if it's the case that you always just have these like incremental improvements, and

314
00:22:56,880 --> 00:23:01,440
this is like an example of it, there might not be some huge shift.

315
00:23:01,440 --> 00:23:05,960
It might not be that you saw some like completely new potential misuse.

316
00:23:05,960 --> 00:23:09,040
It's just that at some point you have to make the decision.

317
00:23:09,040 --> 00:23:14,640
So I guess I want to say like it doesn't, we weren't intending to imply that it was like

318
00:23:14,640 --> 00:23:15,640
something that it wasn't.

319
00:23:15,640 --> 00:23:19,040
I think we were very explicit in the blog post and in the paper about like the nature

320
00:23:19,040 --> 00:23:23,600
of the research and where it sat in relation to other research, but it's hard to, you

321
00:23:23,600 --> 00:23:27,040
know, convey that well also deciding that you're going to do a slightly different

322
00:23:27,040 --> 00:23:29,040
release type from what's happened before.

323
00:23:29,040 --> 00:23:32,800
Yeah, I mean, we did try and, you know, be clear that we were talking both about, you

324
00:23:32,800 --> 00:23:37,120
know, GPT-2 as well as language models in general, but I think we could have been a lot

325
00:23:37,120 --> 00:23:42,960
clear about sort of what are the threats of GPT-2 raw versus GPT-2 fine-tune versus,

326
00:23:42,960 --> 00:23:45,920
you know, GPT-3 or, you know, someone reproducing it.

327
00:23:45,920 --> 00:23:50,640
So, and like what are the sort of, you know, domains or skill levels required for these

328
00:23:50,640 --> 00:23:51,640
different things?

329
00:23:51,640 --> 00:23:54,960
So this is something that we plan to be a lot more transparent about in the future about

330
00:23:54,960 --> 00:24:00,240
like why would we do this and what are the trade-offs involved and sort of, you know, what

331
00:24:00,240 --> 00:24:03,920
was, what were the options we considered and why did we not do the things that people

332
00:24:03,920 --> 00:24:06,400
are saying, yeah, we should do now.

333
00:24:06,400 --> 00:24:08,880
Nima, you're reacting to this.

334
00:24:08,880 --> 00:24:16,360
I think I'm kind of worried when it's, you know, when Amanda said that, you know, just

335
00:24:16,360 --> 00:24:21,000
because it's incremental, doesn't mean we'll, you know, that at some point we should stop

336
00:24:21,000 --> 00:24:24,080
releasing or only do this partial release, right?

337
00:24:24,080 --> 00:24:30,360
And that's what I'm worried about if the community is moving towards, away from openness

338
00:24:30,360 --> 00:24:38,200
and to close setting just because one day we suddenly feel there is a threat and even

339
00:24:38,200 --> 00:24:41,400
if there is, it's not going to help, right?

340
00:24:41,400 --> 00:24:46,720
Because there's already so much available in the open and it's so easy to, you know,

341
00:24:46,720 --> 00:24:52,760
go look at these ideas and including the blog post and the paper from OpenAI and reproduce

342
00:24:52,760 --> 00:24:53,760
this.

343
00:24:53,760 --> 00:24:59,560
I think it was Stephen who commented on Twitter about the kind of resources it takes for

344
00:24:59,560 --> 00:25:04,280
a bad actor to truly reproduce if they wanted to and it's not a lot, right?

345
00:25:04,280 --> 00:25:10,880
So, you know, so it's not really stopping the bad actors and these malicious use cases

346
00:25:10,880 --> 00:25:18,920
because of this, you know, partial release and holding back this full scale model, but

347
00:25:18,920 --> 00:25:23,560
what it's, who it's truly hurting are the academic researchers.

348
00:25:23,560 --> 00:25:28,360
You know, the students, the junior researchers with the least access to the resources, you

349
00:25:28,360 --> 00:25:32,880
know, the marginalized communities, maybe, you know, people across the world where there

350
00:25:32,880 --> 00:25:34,880
is less compute infrastructure.

351
00:25:34,880 --> 00:25:38,920
I mean, they cannot easily reproduce the resources.

352
00:25:38,920 --> 00:25:45,000
That will, it'll take them a lot more to go and reproduce and then do for the research.

353
00:25:45,000 --> 00:25:50,120
So it's hurting the research community a lot more and almost doing nothing to stop the

354
00:25:50,120 --> 00:25:52,840
malicious use cases, in my view.

355
00:25:52,840 --> 00:26:00,120
I'd like to interject with a question from a user on YouTube, G23, who asks, it would

356
00:26:00,120 --> 00:26:04,160
be possible and this is asking maybe a bit of a theoretical, but would it be possible

357
00:26:04,160 --> 00:26:09,680
to establish some kind of partnership program so that researchers kind of vetted researchers

358
00:26:09,680 --> 00:26:15,560
could get access to this to this work without fully making it open.

359
00:26:15,560 --> 00:26:18,920
Is that something that OpenAI has considered?

360
00:26:18,920 --> 00:26:24,800
Yeah, so absolutely, and you know, there's an email address on the blog, the original

361
00:26:24,800 --> 00:26:29,680
blog post where you can suggest both, you know, specific use cases that you're interested

362
00:26:29,680 --> 00:26:34,480
in as well as ideas around, you know, alleviating these trade-offs in terms of access versus

363
00:26:34,480 --> 00:26:36,240
limiting this use.

364
00:26:36,240 --> 00:26:41,600
To comment briefly on Anima's point around openness, I totally agree there are tons of benefits

365
00:26:41,600 --> 00:26:45,840
of openness and it's been, you know, the benefits of openness, if anything, have become

366
00:26:45,840 --> 00:26:50,000
more acute to us through this process because it's frustrating, you know, making claims

367
00:26:50,000 --> 00:26:55,320
that people are saying, you're unsubstantiated and trying to sort of, you know, persuade

368
00:26:55,320 --> 00:26:59,120
people that, you know, we're not making this up that there are actually these capabilities.

369
00:26:59,120 --> 00:27:03,320
So it's super, you know, frustrating to have to deal with that trade-off and, you know,

370
00:27:03,320 --> 00:27:08,080
that we, it's quite possible that we made the wrong choice that, you know, we should

371
00:27:08,080 --> 00:27:11,200
have been thinking more, I mean, we did think, to some extent, we should have been thinking

372
00:27:11,200 --> 00:27:17,240
more about sort of the low, you know, low compute, you know, asserts of actors, you know,

373
00:27:17,240 --> 00:27:21,220
people in developing countries and so forth who, you know, could only get access to this

374
00:27:21,220 --> 00:27:22,220
through a pre-train model.

375
00:27:22,220 --> 00:27:26,320
But it's, I think, it's also quite plausible and I hope it's the case we might the right

376
00:27:26,320 --> 00:27:30,640
call and having a sort of reading period to have this conversation and start thinking

377
00:27:30,640 --> 00:27:35,800
more critically about defenses and coordination around these topics will actually be an

378
00:27:35,800 --> 00:27:36,800
up benefit.

379
00:27:36,800 --> 00:27:47,440
I mean, I think we're thinking a lot about these considerations, so things like are there

380
00:27:47,440 --> 00:27:52,400
ways that we can give access to this kind of work to academics who want to work on it,

381
00:27:52,400 --> 00:27:58,480
for example, also are there ways of interacting like across industry or like, you know, people

382
00:27:58,480 --> 00:28:03,080
suggested a kind of partnership? We're interested in exploring all of those ideas, and I think

383
00:28:03,080 --> 00:28:09,320
one of the purposes of sort of starting a conversation here was to get a lot of that out on the table

384
00:28:09,320 --> 00:28:13,480
and not to say something like, oh, we want to just take action on our own and decide to close things

385
00:28:13,480 --> 00:28:20,040
off. The goal was really to start a conversation around this and get feedback. Not to say something

386
00:28:20,040 --> 00:28:25,400
like, yes, we're just like, we think that we can like prevent misuse by simply closing up research

387
00:28:25,400 --> 00:28:29,560
or something like that. That's like not the intention. So just like earlier points that people

388
00:28:29,560 --> 00:28:35,480
read. Yeah, one of the things that it confused me a little bit about the conversation that as I

389
00:28:35,480 --> 00:28:40,680
was following it on Twitter was there seemed to be, and it came up in this conversation as well,

390
00:28:40,680 --> 00:28:47,080
some suggestion that the models themselves weren't particularly novel, and I guess part of the

391
00:28:47,080 --> 00:28:55,240
issues that we can't really know, but Jeremy Howard, for example, seemed to suggest that the models

392
00:28:55,240 --> 00:29:00,280
were, you know, the code is out there to do what OpenAI did. They just did it at a scale that

393
00:29:00,280 --> 00:29:07,800
no one has done before. I wonder if anyone has a take on that. I suddenly do it if I can jump in.

394
00:29:09,320 --> 00:29:15,800
Much of the model is really quite the same as OpenAI's previous release of GBT,

395
00:29:16,760 --> 00:29:23,240
and the main thing I kind of refer to is scale it till it breaks models, where you just take

396
00:29:23,240 --> 00:29:27,000
an existing model and you ask that, you know, interesting question, because this is something

397
00:29:27,000 --> 00:29:30,680
you can ask with machine learning, if you just keep scaling the model up and keep throwing in more

398
00:29:30,680 --> 00:29:36,360
data, does the behavior of the model change substantially? And so that's really the question that

399
00:29:36,360 --> 00:29:41,800
the OpenAI team were asking, not necessarily like, you know, we have a new WhizBang model underneath

400
00:29:41,800 --> 00:29:47,480
the surface. But because of that, that also does, you know, raise some interesting questions,

401
00:29:47,480 --> 00:29:52,840
along with the fact that OpenAI released the code immediately, because in terms of kind of

402
00:29:52,840 --> 00:29:59,320
responsible disclosure for this, anyone can kind of reproduce the research, either with their

403
00:29:59,320 --> 00:30:05,240
existing code or with previously released code. And I think Anima mentioned, I kind of crunched

404
00:30:05,240 --> 00:30:10,440
numbers. It's about $43,000, but it's suddenly not cheap, but for, you know, a state actor or

405
00:30:10,440 --> 00:30:15,240
someone else like that, or a substantially large company, it's suddenly within, you know, their

406
00:30:15,240 --> 00:30:20,120
ability to do so. But the model itself hasn't really strongly changed. And so it's more a question

407
00:30:20,120 --> 00:30:27,160
of, I guess, capabilities when you scale models up to this size. Rob, how about you, any thoughts

408
00:30:27,160 --> 00:30:32,520
on that? Yeah, I mean, I actually like to go back to the question of limited release as well.

409
00:30:32,520 --> 00:30:39,080
So most research institutions have some form of IRB, all in the U.S. too, so ethical review

410
00:30:39,080 --> 00:30:44,840
boards. Who do exactly this? And so when I was doing my PhD, some of the data I looked at

411
00:30:44,840 --> 00:30:50,680
was health care messages in the general language of Malawi. And because they contained PII,

412
00:30:50,680 --> 00:30:56,120
I had to go through a review process, both here in the USA and then also in Malawi to get approval

413
00:30:56,120 --> 00:31:00,200
to use this data and, you know, promise to treat it carefully, delete it when I know long

414
00:31:00,200 --> 00:31:05,640
it needed it. So a lot of these processes are already in place. People in other scientific

415
00:31:05,640 --> 00:31:10,680
disciplines, especially biological and medical ones, routinely have to go through this process.

416
00:31:10,680 --> 00:31:17,960
And yeah, I think that already exists for a lot of AI researchers. And that kind of takes me

417
00:31:17,960 --> 00:31:23,400
into the point that I wanted to make because, yeah, to Steven's point, these models continually get

418
00:31:23,400 --> 00:31:28,840
better with more and more data. But we don't have more and more data for most of the world's languages.

419
00:31:28,840 --> 00:31:33,960
So I think the way that OpenAI differ from all the other language models that have been released

420
00:31:33,960 --> 00:31:39,080
recently is that it really only looked at English. It did some look at novel translation

421
00:31:39,080 --> 00:31:46,200
between English and French. But when you look at Bert, you know, a month ago, I went for that

422
00:31:46,200 --> 00:31:52,200
Facebook. It's a few weeks ago. Bert and laser from Facebook had a hundred different languages.

423
00:31:53,080 --> 00:31:59,480
So English, you know, it's only constitutes 5% of the world's conversations daily. It's the

424
00:31:59,480 --> 00:32:04,840
most privileged language in the world. And it's the language for which it's most easy for us to

425
00:32:04,840 --> 00:32:10,120
fight fake news right now because we have AI that can identify fake versus real news. We have

426
00:32:10,120 --> 00:32:17,960
teams of people at the different social media companies doing this. And so for me, rather than fake

427
00:32:17,960 --> 00:32:25,480
news or killer robots or other things that your employer might be worried about, OpenAI, it's

428
00:32:25,480 --> 00:32:30,360
inclusion in AI, which I think is the biggest ethical problem that we're facing right now.

429
00:32:30,360 --> 00:32:37,400
And if these models are any working at a scale that we have for English, then even the software

430
00:32:37,400 --> 00:32:44,120
component, the algorithms don't matter. They're not going to be able to be used for 99% of the world's

431
00:32:44,120 --> 00:32:53,320
languages. So I'm really curious that, you know, if this model kind of model won't even work for

432
00:32:53,320 --> 00:32:57,000
the majority of the world's languages, where if I didn't fake news is the hardest right now,

433
00:32:57,000 --> 00:33:01,080
because that data simply doesn't exist. Why, why, why are we particularly concerned then about

434
00:33:01,080 --> 00:33:09,720
the OpenAI English-only model compared to others? Miles, you have a thought on that?

435
00:33:10,520 --> 00:33:15,800
Yeah, I mean, yeah, so in terms of the sort of biased question and

436
00:33:15,800 --> 00:33:19,320
representiveness around the language, I think that it's definitely something we've considered

437
00:33:19,320 --> 00:33:24,760
in addition to other sort of more subtle or non-obvious risks. And you know, certainly we

438
00:33:24,760 --> 00:33:29,320
forward grounded the malicious use risks of sort of people deliberately using this, but that's

439
00:33:29,320 --> 00:33:33,720
also something we need to consider in terms of, you know, what is the consequence of releasing

440
00:33:33,720 --> 00:33:41,080
these models and, you know, sort of bias around sex and race is another thing that we've considered

441
00:33:41,080 --> 00:33:46,840
as a reason for caution. So, you know, that's not to say that we wouldn't, if we had, if we were,

442
00:33:46,840 --> 00:33:53,000
if we had no other concerns besides, you know, the English, you know, bias, would we so release it?

443
00:33:53,000 --> 00:33:57,160
I don't know, that's an interesting question. I think one of these, I think one of these

444
00:33:57,160 --> 00:34:02,920
correlates, too. So, obviously, like, language and race correlate strongly, but in some cases,

445
00:34:02,920 --> 00:34:10,440
the more closely intertwined. So, your race through a lot of Latin America is determined more

446
00:34:10,440 --> 00:34:16,280
by the language, you speak, than your actual biological ethnicity. And there is a huge

447
00:34:16,280 --> 00:34:21,400
gender bias there, too. So, in a lot of the world, you're more likely to have more education

448
00:34:21,400 --> 00:34:28,360
and be taught a privileged language if you're raised male than you are female. So, these really

449
00:34:28,360 --> 00:34:35,160
do correlate strongly with each other. And also, not to single you out. Even though these other

450
00:34:35,160 --> 00:34:39,560
models have been released in more languages, that's missing from their valuations as well.

451
00:34:40,360 --> 00:34:43,560
So, I think their, the first model to really get a lot of publicity in the machine learning

452
00:34:43,560 --> 00:34:49,960
community was the alumni model out of the Allen Institute. So, they wanted best paper award,

453
00:34:49,960 --> 00:34:54,360
I think two years ago, maybe last year, at one of the big competition linguistics conferences.

454
00:34:55,080 --> 00:35:00,360
They've evaluated NDD recognition, so identifying the names of people, places, and locations.

455
00:35:00,360 --> 00:35:03,880
They evaluated a multilingual data set, which was both English and German,

456
00:35:03,880 --> 00:35:07,720
but you feel they're paper, then you have one set of results. And then you say this,

457
00:35:07,720 --> 00:35:11,560
you just have to infer that they know all the German data and only evaluated the English.

458
00:35:12,440 --> 00:35:15,880
And then the birth paper at a Google did exactly the same. They reported a new set of

459
00:35:15,880 --> 00:35:21,640
the art on this data set, which is called a multilingual data set, but reported only to English

460
00:35:21,640 --> 00:35:27,000
results. And English and German are basically as close to related as any two languages can be.

461
00:35:27,000 --> 00:35:30,280
They're like from the same language family, they're a lot of cognates.

462
00:35:31,080 --> 00:35:34,440
Enough played around with the Elmo model, and it doesn't get into a near state of the art

463
00:35:35,480 --> 00:35:40,280
for this German data set. Bird gets a little bit better, but again, not state of the art.

464
00:35:40,280 --> 00:35:46,920
And so I worry a little bit then, you know, to what extent have those researchers, or ones at OpenAI,

465
00:35:48,200 --> 00:35:53,320
given that the imperative of always having state of the art, have they tried this in other languages,

466
00:35:53,320 --> 00:35:57,800
maybe something as close related to English as German, they didn't get state of the art results,

467
00:35:57,800 --> 00:36:03,960
and as a result, they brushed it under the carpet rather than sharing a really important negative result.

468
00:36:06,360 --> 00:36:09,000
Steven, what do you have some thoughts on the language issue?

469
00:36:09,000 --> 00:36:16,200
Yeah, so one of the kind of proof of concepts that's in the OpenAI paper, but we've also seen

470
00:36:16,200 --> 00:36:22,200
similar strands of research across the community, is kind of twofold. One is that unsupervised

471
00:36:22,200 --> 00:36:28,200
language models substantially help translation. That's kind of an obvious. But in this situation,

472
00:36:29,080 --> 00:36:33,560
the OpenAI team were actually purposely stripping out than just retaining just English.

473
00:36:33,560 --> 00:36:38,360
And for one of those reasons is that the data sets they were comparing against primarily English.

474
00:36:39,560 --> 00:36:43,640
But they ended up kind of accidentally leaving, I think it was 10 megabytes or so of

475
00:36:43,640 --> 00:36:49,560
French in there. And these were kind of like, I wish I knew more French, but like Bonjour,

476
00:36:49,560 --> 00:36:55,160
means hello in English, like as a sentence. And the language model, it does,

477
00:36:55,160 --> 00:36:59,400
a reason I mentioned this as a proof of concept, they obviously tried to strip out as much

478
00:36:59,400 --> 00:37:03,560
of the languages they could, but it ended up with some remaining in there. But even from that small

479
00:37:03,560 --> 00:37:08,680
amount, translating from French to English did reasonably well. And there's reasons for that,

480
00:37:08,680 --> 00:37:12,600
you know, the language model itself has just been learning what English looks like. And so from

481
00:37:12,600 --> 00:37:18,440
even a few examples of French, I can say, well, frequently these go across. But the next stage up,

482
00:37:18,440 --> 00:37:25,800
which is kind of the broader community, there are many efforts to have unsupervised translation

483
00:37:25,800 --> 00:37:33,880
between languages. And I think you made reference Rob to laser beforehand. And the beautiful thing

484
00:37:33,880 --> 00:37:38,760
about this is that by helping, say, translate from English to German, which are very similar,

485
00:37:38,760 --> 00:37:43,080
but have, you know, at least a few, I guess, rules in terms of changing around the orders of things,

486
00:37:43,080 --> 00:37:49,080
or, you know, different ways in which words combine. You can take those same kind of learnings

487
00:37:49,080 --> 00:37:56,760
for this model and transfer it to a very resource-low language, and still have that transition across.

488
00:37:56,760 --> 00:38:02,120
Now, it is a completely fair point that it hasn't worked. The opening I team for their language model,

489
00:38:02,120 --> 00:38:07,880
here hasn't applied it to, you know, further languages. But one thing which I kind of personally

490
00:38:07,880 --> 00:38:13,560
have some, this is almost unrelated experience to me, I released a language model sometime back

491
00:38:13,560 --> 00:38:21,480
called the AWDLSTM. The fast.ai team took it and then have it as a kind of underlying basis,

492
00:38:21,480 --> 00:38:27,320
now it's been immensely modified, the underlying basis originally for the language model ULM fit.

493
00:38:27,320 --> 00:38:33,160
And the fast.ai community have then ported this to dozens of different languages. And kind of

494
00:38:33,160 --> 00:38:38,760
the really fun thing for me is I was mainly focused on English. I should probably expand my language

495
00:38:38,760 --> 00:38:44,040
modeling vocabulary, even if I don't know the languages myself. But the code that I wrote because

496
00:38:44,040 --> 00:38:48,680
it was general and your machine learning does this, transfer very well across these other languages.

497
00:38:48,680 --> 00:38:53,160
And I'm aware of at least seeing that the transform model has been able to do this quite successfully

498
00:38:53,160 --> 00:39:00,360
in the past. So I'd expect, naively, the opening I model to have the same sorts of advantages.

499
00:39:01,880 --> 00:39:08,200
That past, definitely true. So the transform model has been a lot more successful across

500
00:39:08,200 --> 00:39:16,200
languages than the R&N LSTM based methods. And then it actually comes down to the reason that

501
00:39:16,200 --> 00:39:22,120
Stephen introduced initially. And that R&N LSTM based model is really only looking at one word at

502
00:39:22,120 --> 00:39:26,200
a time, enough to pass that memory all the way through, rather than being smarter about finding

503
00:39:26,200 --> 00:39:31,720
long distance relationships. And so English is a complete outlier in terms of how important word

504
00:39:31,720 --> 00:39:36,520
order is. It's more common in most languages, that the subjective verb in the object,

505
00:39:36,520 --> 00:39:40,280
the subjective verb by the suffixes or the prefixes. They go in those words and the words can be

506
00:39:40,280 --> 00:39:45,160
in any order. And so this is, one of the things that is a little bit problematic about a lot of

507
00:39:45,160 --> 00:39:50,920
these results is that testing only on English, which is it's not in the middle, it's an outlier,

508
00:39:52,040 --> 00:39:56,440
how important word order is and then standardized bellings are and then lack of suffixes.

509
00:39:57,160 --> 00:40:00,440
It doesn't really tell you about how it's going to do more broadly.

510
00:40:01,000 --> 00:40:03,960
It's certainly the machine translation community and that's what I'm addressing my company right

511
00:40:03,960 --> 00:40:08,440
now. The transformer based methods are really blowing the R&N based methods out of the water.

512
00:40:09,480 --> 00:40:14,680
But that's not so clear in a lot of the language models recently, even if they've been released

513
00:40:14,680 --> 00:40:21,320
in multiple languages, only been evaluated in English. And so I think it's representing a real gap

514
00:40:21,320 --> 00:40:26,280
in the knowledge that people like me and industry can take from the research community.

515
00:40:27,800 --> 00:40:33,560
So a question I've got for Anima is really about kind of the true capabilities of these

516
00:40:33,560 --> 00:40:40,360
types of models. I think looking at the sample that was released in the blog post,

517
00:40:42,920 --> 00:40:47,880
with this model, you can provide a prompt and the output is conditioned on this prompt. And so

518
00:40:47,880 --> 00:40:54,360
the prompt was something about scientists discover unicorns and there's a rather long and

519
00:40:54,360 --> 00:41:00,920
rather coherent text about the backstory of the scientific discovery. It was rather impressive to

520
00:41:00,920 --> 00:41:10,360
me. Are you equally impressed? Where do you think this fits in the broad scheme of

521
00:41:10,360 --> 00:41:14,760
capability of these types of models? I think that particular example,

522
00:41:17,720 --> 00:41:23,480
I think you could argue that it furthers the whole AI boogie man terminator thing. It's

523
00:41:23,480 --> 00:41:31,400
particularly unexpected or at least I found it particularly unexpected. You know, do you think

524
00:41:31,400 --> 00:41:36,840
and we can we'll ask open AI as well like and I think actually to be fair in the blog post,

525
00:41:36,840 --> 00:41:42,600
I said this was I think they said this was an example that was selected out of 10 or something

526
00:41:42,600 --> 00:41:50,920
with that prompt. But to what extent are these types of examples cherry picked? You know,

527
00:41:50,920 --> 00:41:57,880
what does it say about kind of where we are in this, you know, the path towards, you know,

528
00:41:57,880 --> 00:42:01,720
some AI outcome that we don't fear? The thing that we're talking about here that we fear and that

529
00:42:01,720 --> 00:42:08,440
we're kind of not disclosing because we fear, you know, how close are we? And you know, and a

530
00:42:08,440 --> 00:42:13,880
short answer is, right, I can't really tell without knowing all the details about the model,

531
00:42:13,880 --> 00:42:21,800
right, and having to the model. I mean, any researcher would I think comment that as like a

532
00:42:21,800 --> 00:42:28,680
one-line answer, right? But more, I guess importantly, the issue is like, as you said, not one

533
00:42:28,680 --> 00:42:34,120
example. Like, you know, the question of like not just like how well it's doing on some cherry

534
00:42:34,120 --> 00:42:38,760
picked examples, but also what the failure modes were. Like, what did the others look like?

535
00:42:38,760 --> 00:42:44,920
Or was it completely incoherent or was it like diverse enough? Was it doing the same thing

536
00:42:44,920 --> 00:42:51,080
over and over again? I mean, these are all questions we ask for when we try to evaluate the models,

537
00:42:51,080 --> 00:42:55,720
right? I mean, we can look at quantitative measures like for complexity, but that's, you know,

538
00:42:55,720 --> 00:43:00,280
like not enough by itself, right? Like, you know, so that's, I mean, there is no easy way to

539
00:43:00,280 --> 00:43:06,440
evaluate unsupervised learning, right? That's a general philosophical question. Like, what does it mean

540
00:43:06,440 --> 00:43:14,280
to have done unsupervised learning well? Because with supervised learning, we have a notion of accuracy,

541
00:43:14,280 --> 00:43:20,600
okay, you get 100% accuracy on your unseen test dataset, then you're, you know, really amazing,

542
00:43:20,600 --> 00:43:26,360
right? But even there are the limitations because you may want to go beyond the test dataset and

543
00:43:26,360 --> 00:43:33,720
their further issues. Whereas with unsupervised learning, the question is, what is a good model?

544
00:43:33,720 --> 00:43:39,400
Like, you know, when, when do we say that this, you know, the answer that you obtained that you're

545
00:43:39,400 --> 00:43:45,080
happy with? Was it because that it was coherent enough? Or did you want it to have certain

546
00:43:45,080 --> 00:43:50,520
factual reasoning? Did you want it to go through a certain logical set of steps?

547
00:43:50,520 --> 00:43:55,960
Right? What would mean it to be impressive? Right? One from a human evaluation perspective,

548
00:43:55,960 --> 00:44:01,800
that other from a more quantitative perspective, it is hard to tell and that's why this is

549
00:44:01,800 --> 00:44:08,920
an open research topic that the community thinks a lot about on how to evaluate language models.

550
00:44:10,760 --> 00:44:16,520
And that's the reason we need the research community to be very much embedded in discussing

551
00:44:16,520 --> 00:44:24,440
this model and getting access to it. Miles, a reaction of that? Yeah, I mean, I totally agree that

552
00:44:24,440 --> 00:44:29,080
it's much easier to evaluate their capabilities with more access and it's a very acute trade-off

553
00:44:29,080 --> 00:44:34,680
that we're trying to navigate. I mean, one point that Anima raised was that, and you know,

554
00:44:35,480 --> 00:44:39,960
very much on point, is that it was not obvious what the specific sort of threats we were concerned

555
00:44:39,960 --> 00:44:44,840
about were and, you know, and more generally where to draw those lines. So, and you know, this is

556
00:44:44,840 --> 00:44:48,600
still something we're thinking through and planning to share more about our process around sort of

557
00:44:48,600 --> 00:44:54,760
threat modeling and evaluating these capabilities, but just to get some intuition. You know, first of all,

558
00:44:54,760 --> 00:45:00,440
you know, this is not just about GPT-2, but also language models in general. So, all of this should

559
00:45:00,440 --> 00:45:04,680
be sort of taken with the grain of salt that we don't know exactly where the biggest threats are

560
00:45:04,680 --> 00:45:09,080
and how quickly things will develop from here. But roughly, we have, you know, a few sort of

561
00:45:09,080 --> 00:45:14,360
tiers of, you know, sources of information that we draw on. We look at how things are being used

562
00:45:14,360 --> 00:45:19,240
in the wild, like what is actually the situation with fake news and, you know, where the bottlenecks in

563
00:45:19,240 --> 00:45:23,320
terms of text reduction and so forth. So, that's one set of perspectives. It's like, what is the

564
00:45:23,320 --> 00:45:28,680
role of, you know, text in society and what are the defenses against mass produced or misleading

565
00:45:28,680 --> 00:45:33,560
text. And then there's sort of in-house analysis, you know, through, you know, both sort of

566
00:45:33,560 --> 00:45:38,680
formally doing science as well as informally, you know, allowing people access to the model,

567
00:45:38,680 --> 00:45:43,560
including non-experts within the opening eye organization. So, that's where some of the

568
00:45:43,560 --> 00:45:48,600
samples came from for the blog posts were sort of non-expert users playing out with an interactive

569
00:45:48,600 --> 00:45:52,920
version of the system as opposed to, you know, like Alec Radford and Jeff, we were trying to come up with

570
00:45:52,920 --> 00:45:58,120
the most impressive possible example. So, but it's, I, we agree that, you know, from the text,

571
00:45:58,120 --> 00:46:03,240
it's not obvious that that's the case. Yeah. So, in terms of threat modeling, you know, it's

572
00:46:03,240 --> 00:46:08,520
important to think about, you know, what can we do in-house? What can we do with a given level of skill

573
00:46:08,520 --> 00:46:13,560
as well as, you know, what would fine-tune variations of the system? You know, we gave the example

574
00:46:13,560 --> 00:46:19,000
of Amazon Reviews as one example where we've looked at fine-tuning and we're able to realize that

575
00:46:19,000 --> 00:46:24,200
it was quite tractable, but we're still thinking through, you know, what is the sort of suite of,

576
00:46:25,080 --> 00:46:28,520
you know, questions you should ask about powerful language models.

577
00:46:30,760 --> 00:46:37,400
Awesome. So, Anima, a question for you from Connor on YouTube, maybe pushing back a little bit,

578
00:46:37,400 --> 00:46:43,000
do you see any limits with respect to the types of models or with respect to releasing models?

579
00:46:43,000 --> 00:46:48,680
Are there any societal considerations that an AI scientist should make in creating

580
00:46:49,560 --> 00:46:54,520
or responsibilities that they bear in releasing their models? Where would you draw the line?

581
00:46:55,640 --> 00:47:01,480
I mean, certainly, I think every scientist should think about societal impacts, right?

582
00:47:02,200 --> 00:47:08,120
In, you know, in any discipline, I think we should all be mindful of the impact we have

583
00:47:08,120 --> 00:47:14,840
through the deployment of technologies we release. But at the same time, we need to ask ourselves,

584
00:47:14,840 --> 00:47:21,640
if I'm limiting a certain technology, what are the trade-offs? Like, you know, there's both the

585
00:47:21,640 --> 00:47:29,560
benefits and the risks in releasing a technology and we need to do that trade-off. And in the machine

586
00:47:29,560 --> 00:47:36,600
learning field, which has been very open until now, most of it is in the open, right? And even if

587
00:47:36,600 --> 00:47:43,800
it's not, it doesn't take a whole lot of resources to get to those capabilities. And so locking

588
00:47:43,800 --> 00:47:49,480
it up seems counterproductive to me at this stage, especially in the context of language models

589
00:47:49,480 --> 00:47:56,120
and similar research where so much of it, very similar frameworks, you know, the open source code,

590
00:47:56,120 --> 00:48:02,040
they're all available. It doesn't take a lot to reproduce that by bad actors. On the other hand,

591
00:48:02,040 --> 00:48:09,560
it can limit access to people in the marginalized communities, people with low, you know,

592
00:48:09,560 --> 00:48:16,760
with limited access to resources. So that's why I see in this setting the equation to be more

593
00:48:16,760 --> 00:48:29,480
tilted towards release. Okay. So a question or response from Miles or Amanda, which of you is that?

594
00:48:29,480 --> 00:48:37,480
Oh, yeah. In this case, I think one thing that's just worth noting is like, we are like very

595
00:48:37,480 --> 00:48:43,320
sensitive to these issues. Like, I agree that it's important that there's like equity among researchers

596
00:48:43,320 --> 00:48:47,960
and that one of the downsides of like not releasing anything, even just doing a partial release,

597
00:48:47,960 --> 00:48:53,640
is that researchers don't have access to it. You have issues like, you know, it makes it harder

598
00:48:53,640 --> 00:49:00,440
to replicate or at least there's a delay in replication. I think it would be interesting to me,

599
00:49:00,440 --> 00:49:05,160
I guess I have two thoughts. One is that we might not even disagree about roughly the weight

600
00:49:05,160 --> 00:49:10,440
of all of the considerations here because I think our position was one of kind of caution

601
00:49:10,440 --> 00:49:14,600
where it's just like, the question isn't something like, do you think this is the exact right moment

602
00:49:14,600 --> 00:49:21,000
to do a partial release? But rather something like, are you like basically how certain are you that

603
00:49:21,000 --> 00:49:25,320
isn't or how certain are you or how confident are you that you're actually on the right side of

604
00:49:25,320 --> 00:49:31,640
this scale? And I think that's like the kind of questions that we're asking. And in part,

605
00:49:31,640 --> 00:49:36,120
also I think the thing that this really highlights, you know, when we start bringing up these pros

606
00:49:36,120 --> 00:49:41,640
and cons is the need for like greater discussion of this in the ML community. So in some ways, when

607
00:49:41,640 --> 00:49:45,400
there isn't a kind of framework or there isn't a kind of agreed upon set of norms, so there isn't

608
00:49:45,400 --> 00:49:50,280
a partnership on this. Each actor is having to kind of think about these issues themselves.

609
00:49:50,280 --> 00:49:54,440
And for our part, we relate, well, that means that you have to take on a lot of caution. And it

610
00:49:54,440 --> 00:49:59,960
is important to be cautious, even if you're considering all of the negative consequences of that.

611
00:50:00,840 --> 00:50:04,600
So I think it's great that this discussion is happening, but it's also worth noting that this could

612
00:50:04,600 --> 00:50:10,200
be made easier if we did potentially have some of these mechanisms to like really help people think

613
00:50:10,200 --> 00:50:16,440
through when to release watch release and the pros and cons. Yeah, and how to raise risks in a way

614
00:50:16,440 --> 00:50:22,200
that isn't seen or isn't actually, you know, alarmist. I mean, you know, so I think one perspective

615
00:50:22,200 --> 00:50:27,880
that I think, you know, is worth considering is that the AI community does not have all the answers,

616
00:50:27,880 --> 00:50:32,120
and it's not the only one that needs to know what the technology's coming down in the pipeline are.

617
00:50:32,920 --> 00:50:36,040
And you know, that was sort of where we were coming from with the outreach to journalists,

618
00:50:36,040 --> 00:50:41,400
prior to the launch, and it's possible that, you know, we could have done more, you know,

619
00:50:41,400 --> 00:50:46,920
increase the ratio of researcher, you know, outreach to, you know, non-researcher outreach,

620
00:50:46,920 --> 00:50:52,520
but, you know, the basic ideas that this is not, you know, just an open AI thing or not an AI

621
00:50:52,520 --> 00:50:57,160
community thing, but, you know, a more general question of sort of how do we handle these powerful

622
00:50:57,160 --> 00:51:01,080
technologies that seem to be coming, even if they're not, you know, totally there. Yeah.

623
00:51:02,120 --> 00:51:10,680
Can you can you comment on the the approach you took to evaluating the kind of the ethical field

624
00:51:10,680 --> 00:51:16,920
in the release, the level of kind of rigor or detail, did you identify, you know, did you have

625
00:51:16,920 --> 00:51:23,560
a specific, you know, kind of persona or threat that you are most concerned about, whether it's

626
00:51:23,560 --> 00:51:29,960
one that's been stated or unstated, or where you kind of reacting, responding, or anticipating

627
00:51:29,960 --> 00:51:34,360
just the broad threat. How, you know, how did you kind of pursue this?

628
00:51:36,120 --> 00:51:40,040
Yeah, so I think, you know, there are a couple of different lenses, and we're still not

629
00:51:40,040 --> 00:51:44,600
sure what the right lens is. There's sort of, you know, GPT-2 itself, and, you know, there's

630
00:51:44,600 --> 00:51:49,400
a fine-tuned version. We, you know, part of where we were coming from was seeing what the zero-shot

631
00:51:49,400 --> 00:51:54,520
version of GPT-2 was, and that sort of gave us some heuristic evidence that even more powerful

632
00:51:54,520 --> 00:52:00,440
capabilities would be possible with fine-tuning or larger models. So, you know, some of it was sort

633
00:52:00,440 --> 00:52:04,760
of, you know, anecdotal experiences of people interacting with the model, like Alec and Jeff

634
00:52:04,760 --> 00:52:11,000
just sort of seeing impressive things, and sort of, you know, a growing number of people interacting

635
00:52:11,000 --> 00:52:17,000
with the model, seeing how easy it was to get, you know, human-ish-looking outputs, not necessarily,

636
00:52:17,000 --> 00:52:22,120
you know, semantically accurate or factually correct to, you know, one of the points that Anima

637
00:52:22,120 --> 00:52:26,760
raised earlier, you know, what is the relevant threshold? We thought that it was, you know,

638
00:52:26,760 --> 00:52:31,560
we still don't know exactly what the right threshold is, but something about sort of human passable,

639
00:52:31,560 --> 00:52:37,000
human-ish-text across a very wide range of demands for very little human-input.

640
00:52:37,000 --> 00:52:42,280
This is, this is testable. You know, there's obviously a lot of fake news out there right now,

641
00:52:42,280 --> 00:52:46,360
and typical bad actor does this in a really simple way. It's, you know, like templates,

642
00:52:46,360 --> 00:52:50,600
we get to drop in certain words and it generates variations of those sentences. Just, you know,

643
00:52:50,600 --> 00:52:54,440
like a hundred lines of code, but it's powerful. You have like ten sentence variants and ten

644
00:52:54,440 --> 00:53:00,360
sentences. You can create billions of different unique paragraphs. And so, you know, it's testable

645
00:53:00,360 --> 00:53:05,160
to create a system like that today, and then have humans say, you know, which is, you know,

646
00:53:05,160 --> 00:53:13,080
which is the more likely to be real. And so that's, and that's standard, right? Like how am I

647
00:53:13,080 --> 00:53:17,720
compared to the state of the R? And the paper had this. So the paper had a bunch of benchmarks

648
00:53:17,720 --> 00:53:23,960
against existing technologies to show they're better than researchers. But the ethical component

649
00:53:23,960 --> 00:53:29,800
or at least everything I've seen so far has been purely qualitative. And so as the ethics people

650
00:53:29,800 --> 00:53:36,440
in open AI, do you think you could convince that the scientists to drop one academic benchmark

651
00:53:36,440 --> 00:53:42,120
and run some new studies which would really demonstrate what the negative impact might be compared

652
00:53:42,120 --> 00:53:46,600
to what's already out there? I mean, I think we're kind of heading in that direction in terms of

653
00:53:46,600 --> 00:53:52,680
quantifying the risks of models and things like bias, you know, in the context of language is

654
00:53:52,680 --> 00:53:56,200
something that we're starting to be more aware of. And I could, you know, I could imagine, you know,

655
00:53:56,200 --> 00:54:00,920
sort of like misuse potential, you know, label for, you know, different sizes of models or something.

656
00:54:00,920 --> 00:54:06,360
But I think we're still in a more sort of pre-conceptual framework phase in the sense that like we,

657
00:54:06,360 --> 00:54:11,160
we know a few considerations, we know a few sort of specific threats, we have some ideas for

658
00:54:11,160 --> 00:54:16,440
how to evaluate them, but ultimately, you know, we don't, we don't have in-house experts on,

659
00:54:16,440 --> 00:54:21,000
you know, everything related to fake news, everything related to cybersecurity, et cetera. So this

660
00:54:21,000 --> 00:54:25,560
is very much, you know, a conversation that we welcome input on. So specific ideas for testing the

661
00:54:25,560 --> 00:54:28,840
model, specific ideas for sort of threat modeling are super welcome.

662
00:54:31,720 --> 00:54:38,120
I think it's a little interesting that Westine have this discussion about text, which at least

663
00:54:38,120 --> 00:54:42,200
I might have an optimistic view, but I feel like the technology is still getting there in terms

664
00:54:42,200 --> 00:54:47,800
of the potential, you know, strongest misuse possibilities. And then, you know, a lot of the time

665
00:54:47,800 --> 00:54:52,520
misinformation online isn't going to be about writing a long-form article. It seems reasonable.

666
00:54:52,520 --> 00:54:57,400
It's about writing a very short snippet that is terrible in a bunch of ways and spreading it

667
00:54:57,400 --> 00:55:03,000
everywhere. But this type of discussion, you know, we as a community should have really started

668
00:55:03,560 --> 00:55:08,120
properly having like, we shouldn't be caught in the dark by this as even a question,

669
00:55:08,120 --> 00:55:14,120
because, you know, deepfakes has been substantially, you know, to many people quite destructive

670
00:55:14,120 --> 00:55:19,640
to their lives. And for the same sort of things that powered that, the code that we released,

671
00:55:19,640 --> 00:55:22,520
the pre-trained models, which have been released, which were then built upon.

672
00:55:23,880 --> 00:55:28,040
Like, I feel like we as a community should have had a better response to that. Like, that should

673
00:55:28,040 --> 00:55:34,280
have been a awakening moment. And this should be a potential like later, okay, you know, now we can

674
00:55:34,280 --> 00:55:39,640
stop considering text modeling through this lens before it's time. I don't know if anyone else

675
00:55:39,640 --> 00:55:45,480
has strong thoughts on that. Do we know where the right forum is for that conversation? I'm not

676
00:55:45,480 --> 00:55:52,600
sure that it's Twitter. I mean, Twitter is a part of it, but, you know, is it standard bodies?

677
00:55:52,600 --> 00:56:01,400
Is it kind of for at the conferences like Nourips and others? Or is it some, you know, structure that,

678
00:56:01,400 --> 00:56:06,040
is it, you know, regulatory? Is it some structure that hasn't been created? Are there things that we can

679
00:56:06,040 --> 00:56:15,000
learn from other, you know, technologies that have both beneficial uses and potential for weaponization

680
00:56:15,000 --> 00:56:22,520
that, you know, what have they done? I mean, I think I would be careful in using terms like

681
00:56:22,520 --> 00:56:27,640
weaponization, right? Because a lot of, you know, like discussions on, especially on Twitter,

682
00:56:27,640 --> 00:56:33,400
as you said, it's not the best medium, like tends to like devolve all the way into a nuclear weapons.

683
00:56:33,400 --> 00:56:42,040
And, you know, those, you know, it's cases of malicious use, right? And this is where there is a

684
00:56:42,040 --> 00:56:48,200
lot of, you know, what we saw that I was most disappointed about the whole episode was the media

685
00:56:48,200 --> 00:56:55,880
distortion in, you know, wide reaching to the public in so many different countries. The message was,

686
00:56:55,880 --> 00:57:00,600
this is so dangerous that this is now locked up, right? That's what the public took away.

687
00:57:01,160 --> 00:57:07,560
And that's disappointing because I think that's a severe distortion from these much more nuanced

688
00:57:07,560 --> 00:57:12,600
conversation we are having today, right? This is what we need to be doing. We need to have the

689
00:57:12,600 --> 00:57:18,520
dialogue within the community and also with the public. I think there is still a big gap of what

690
00:57:18,520 --> 00:57:24,600
the think of as AI or even intelligence, you know, they are not able to truly evaluate

691
00:57:24,600 --> 00:57:32,760
how intelligent or how capable the current AI systems are. And I think that's a severe deficiency

692
00:57:32,760 --> 00:57:39,160
if we cannot close that gap between the research community and the general public. And that's

693
00:57:39,160 --> 00:57:45,000
what worries me the most that this conversation, you know, we need to present it in a much more

694
00:57:45,000 --> 00:57:52,600
balanced form to the public and to the media. I also want to mention like one thing that I

695
00:57:52,600 --> 00:57:57,720
always think in my mind, if we think way back when Facebook released a paper that was essentially

696
00:57:57,720 --> 00:58:03,960
about two robots bothering. And somehow like I was in Australia and I heard on like the nightly

697
00:58:03,960 --> 00:58:09,160
news, Facebook AI has like, you know, taken over something they had to shut down the experiment.

698
00:58:09,160 --> 00:58:14,440
It's so dangerous they had to shut it down. Exactly. But that's a thing, right? Like we have very

699
00:58:14,440 --> 00:58:19,160
few chances of reaching this global audience and we need to be very careful about what ends up stuck

700
00:58:19,160 --> 00:58:23,560
in their heads. Because we have the potential to put the right piece of information in someone's

701
00:58:23,560 --> 00:58:28,200
head. So I know, you know, maybe to be wary about receiving in the future, receiving that email

702
00:58:28,200 --> 00:58:33,160
from grandma, which seems entirely syntactically correct, but she's asking about Bitcoin and has

703
00:58:33,160 --> 00:58:38,600
an account already to feed it, right? Like maybe that's the one piece of information we can

704
00:58:38,600 --> 00:58:44,200
somehow impose a worry about in the future. But you know, in the case of the Facebook AI story,

705
00:58:44,200 --> 00:58:49,160
you know, I feel like the information that has been locked in people's head is really quite wrong.

706
00:58:49,160 --> 00:58:53,800
And, you know, Facebook's, they took the, you know, they took it on themselves. They read a new

707
00:58:53,800 --> 00:58:58,520
blog purse that explained just how it got distorted within the media. But I can, you know, imagine

708
00:58:58,520 --> 00:59:05,480
very strongly that people who saw that television station news segment in Australia are almost never

709
00:59:05,480 --> 00:59:11,320
going to read that blog purse. And that's I think what I'm more worried about fake news about AI

710
00:59:11,320 --> 00:59:21,560
than AI generating the fake news. So maybe to start to wrap things up, miles in Amanda with the

711
00:59:21,560 --> 00:59:28,840
benefit of hindsight, you know, how much you approach this differently? Well, yeah, I'll give Amanda

712
00:59:28,840 --> 00:59:33,640
a second to think about that. And while I give you a comment, which is that in terms of venues for

713
00:59:33,640 --> 00:59:39,800
keeping the conversation going, we'll be hosting and dinner at I clear for where both policy people

714
00:59:39,800 --> 00:59:45,160
and technical people from opening I will be happy to discuss them talk about the future of language

715
00:59:45,160 --> 00:59:52,680
models, generally, not just to PT too. Yeah, so I think that like ideally being able to have a kind

716
00:59:52,680 --> 01:00:01,320
of wide range of inputs from the ML community prior to like making a decision like this is going

717
01:00:01,320 --> 01:00:07,240
to be very helpful. In the case of like media attention, there's a sense in which it's like a

718
01:00:07,240 --> 01:00:13,960
little bit harder to kind of navigate or control because like the way that something is going to be

719
01:00:13,960 --> 01:00:21,080
told like as we've kind of heard is like like a little bit more delicate, although I really sympathize

720
01:00:21,080 --> 01:00:26,440
with this, you know, like if we could do the not necessarily just how you do things differently,

721
01:00:26,440 --> 01:00:31,880
but like like how you wish things would go, you know, we were quite positive in the blog post as

722
01:00:31,880 --> 01:00:38,520
well, like we talked about all of the positive ways that this could be used. And obviously like in

723
01:00:38,520 --> 01:00:42,440
general, like the thing that gets reported is quite negative. And I think that's like really

724
01:00:42,440 --> 01:00:46,440
unfortunate because like, you know, people who are doing ML research are doing it because they want

725
01:00:46,440 --> 01:00:53,240
to see, you know, excellent uses of it in the world. And you know, I think that it wasn't our intention

726
01:00:53,240 --> 01:00:58,200
to say something like this research is all bad and you should be afraid of it. Rather just like,

727
01:00:58,200 --> 01:01:02,760
hey, we need to start having a conversation about this. So I think yeah, I would like to see more

728
01:01:02,760 --> 01:01:07,320
of that. And I think that it's correct that essentially what I would like to see more of is the

729
01:01:07,320 --> 01:01:10,920
kind of nuance discussion that we're having here, where we're sensitive to all of the pros and

730
01:01:10,920 --> 01:01:17,400
cons both of like doing open research and like the potential misuse of that research. So yeah,

731
01:01:17,400 --> 01:01:21,720
I think like creating forums where people can do that is like the thing that I would really like

732
01:01:21,720 --> 01:01:29,400
to see going forward. Yeah, I mean, I agree with all that. And just in general, both open AI and

733
01:01:29,400 --> 01:01:33,720
the rest of the AI community needs to find ways to smooth this conversation out over time. So it

734
01:01:33,720 --> 01:01:39,080
doesn't happen all of it, you know, in one, you know, Twitter storm and sort of, you know, find

735
01:01:39,080 --> 01:01:43,880
whether it's sort of, you know, recurring workshops at conferences or whatever to sort of institutional

736
01:01:43,880 --> 01:01:51,640
conferences. And how about the rest of you, Steven, what, what would you like to see done differently

737
01:01:52,440 --> 01:02:01,320
and what do you hope to see grow out of this this scenario? Yeah, well, I mean, you know, I love

738
01:02:01,320 --> 01:02:06,600
language modeling. It's always exciting for me when I'm about. So it's a good thing for me. What

739
01:02:06,600 --> 01:02:09,880
I'm going to be interested in is, you know, at some point when I can play with this model,

740
01:02:09,880 --> 01:02:15,880
play with other models, you almost like one of the biggest issues we haven't filled is, you know,

741
01:02:15,880 --> 01:02:19,800
urban fitting. You find ways that these machine learning models learn to cheat in subtle and

742
01:02:19,800 --> 01:02:25,320
strange ways. And one of the, you know, craziest examples of that is in visual question answering.

743
01:02:25,320 --> 01:02:29,560
So you give the machine learning model an image and then you ask it a question to answer.

744
01:02:30,280 --> 01:02:36,760
For some time, these visual question answering systems did worse than just looking at the question

745
01:02:36,760 --> 01:02:41,640
without ever looking at the image. And the field didn't quite realize it just because they didn't

746
01:02:41,640 --> 01:02:46,360
run the right experiments. So that's the thing that I think I would enjoy playing with. There's

747
01:02:46,360 --> 01:02:50,760
the modern version of that for text, which is for a question answering data sets called squad.

748
01:02:51,560 --> 01:02:55,560
You know, many people are really excited about how intelligent it was, how many of the questions

749
01:02:55,560 --> 01:03:00,360
it got correct. But then a group went through and kind of methodically looked at each instance and

750
01:03:00,360 --> 01:03:05,960
was like, oh, with a, you know, a few dozen lines of code and red jerks' regular expressions

751
01:03:05,960 --> 01:03:10,600
basically ways to capture some patterns. You can answer all of these. So it's a question of,

752
01:03:10,600 --> 01:03:15,000
okay, this language model is obviously quite good. But exactly how good is it? And, you know,

753
01:03:15,000 --> 01:03:19,480
what interesting, strange methods of cheating might it be using that's able to trick all of us

754
01:03:19,480 --> 01:03:29,000
at a glance? Rob? Yeah. So I wrote what I said earlier. And then to speak to Anima's point about

755
01:03:29,000 --> 01:03:36,840
making sure the right message gets out there, I felt like for me, the release wasn't most offensive

756
01:03:36,840 --> 01:03:42,680
in terms of what wasn't and what wasn't public. I think the biggest shortcoming was that it was

757
01:03:42,680 --> 01:03:49,000
only an image that it was not diverse, that it was the most privileged language, which correlate

758
01:03:49,000 --> 01:03:56,200
to almost every other privileged demographic. And that to me was the bigger ethical concern than

759
01:03:56,200 --> 01:04:04,120
anything to do with what actually got released in terms of the language model. Anima? Yeah. I mean,

760
01:04:04,120 --> 01:04:11,480
I think, you know, putting more thought process into, you know, collaborating with academia and

761
01:04:11,480 --> 01:04:18,920
research community in general, right? And making sure that especially the, you know, researchers with

762
01:04:18,920 --> 01:04:24,680
less compute resources are not at this advantage. That's something, you know, non-profit like

763
01:04:24,680 --> 01:04:31,640
OpenAI would have a very big role to play, right? To remove the barriers and to truly democratize AI.

764
01:04:32,520 --> 01:04:39,880
I think thinking also on that angle while quantifying the risks and coming up with a more

765
01:04:40,520 --> 01:04:46,360
quantitative analysis of risks and also maybe incentive mechanisms like how to better

766
01:04:46,360 --> 01:04:52,520
deploy AI and better release it to the community. These are all things we can, you know,

767
01:04:52,520 --> 01:04:58,280
for the research and come up with some best practices for the community and also best practices

768
01:04:58,280 --> 01:05:05,560
in terms of how we talk about this to the general media and how this gets reported. That's something

769
01:05:05,560 --> 01:05:13,400
we as a community need to work more about. Yeah. And, you know, I love the that you're hosting the

770
01:05:13,400 --> 01:05:20,120
dinner at. I clear, but I'd also love to see, you know, OpenAI kind of roll up its leaves and

771
01:05:20,120 --> 01:05:25,560
help figure out where the right place is to have this conversation more formally. And, you know,

772
01:05:25,560 --> 01:05:29,640
who the right people are to bring to the table a number of the folks that I've commented on YouTube

773
01:05:29,640 --> 01:05:34,440
and Twitter about, you know, security researchers have dealt with this kind of issue for a long time.

774
01:05:34,440 --> 01:05:38,840
How do we get them into the fold? You know, folks have been doing threat modeling. How do we get them

775
01:05:38,840 --> 01:05:46,360
into the folds? There's a lot of work that has to happen to, you know, create, you know, that space

776
01:05:46,360 --> 01:05:54,200
and use it to further the conversation. And, yeah, I'd love to see more happening there. But for

777
01:05:54,200 --> 01:05:59,080
tonight, I'm glad to be part of the, you know, getting this beyond 280 characters a pop.

778
01:06:00,120 --> 01:06:06,840
And thank all of you for taking the time to jump on and talk about this really important issue.

779
01:06:07,720 --> 01:06:11,960
Thank you. Thanks a lot for doing this. All right. Good night, everyone.

780
01:06:11,960 --> 01:06:15,880
Good night, good night. Thanks, everyone, for joining via YouTube live.

781
01:06:15,880 --> 01:06:22,200
Yeah. And before we go, actually, we'll put it in the description when we post the video,

782
01:06:22,200 --> 01:06:26,600
but for folks who aren't following all of you on Twitter, why don't we do a quick

783
01:06:27,480 --> 01:06:29,880
Twitter handle a roll call?

784
01:06:29,880 --> 01:06:45,320
Um, I'm sorry. Rob, uh, WWE Rob World Wide Rob. Smarity is Smarity. Smarity.

785
01:06:48,360 --> 01:06:56,600
Miles underscore Brundage. Yeah, Miles underscore Brundage. I'm just Amanda Askell.

786
01:06:56,600 --> 01:07:00,520
Oh, one word. Awesome. Awesome. Well, thanks. Thanks again, everyone. Good night.

787
01:07:00,520 --> 01:07:27,240
All right. Bye. Thanks.

