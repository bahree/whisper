Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting
people, doing interesting things in machine learning and artificial intelligence.
I'm your host, Sam Charrington, before we dive in I'd like to quickly share a few updates
about our ongoing giveaways.
If you haven't done so already you've got until 11.59pm Pacific time tonight to enter
into our AI conference New York Bronze Pass giveaway.
Visit twimlai.com slash AI and Y giveaway to get your entry in.
Also the hackers among us should be sure to visit twimlai.com slash TF giveaway to enter
our TensorFlow Edge Kit giveaway.
Three lucky winners will receive a coral edge TPU device and the Spark Fund Edge Development
Board amongst other goodies.
The contest ends on April 8th.
Today we're joined by Peter Whittick, assistant professor at the University of Toronto who
works on quantum enhanced machine learning and the application of high performance learning
algorithms and quantum physics.
Peter and I caught up back in November to discuss the presentation he gave at the AWS
Reinvent Conference, pragmatic quantum machine learning today.
In our conversation we start with a bit of background including the current state of
quantum computing, a look ahead to what the next 20 years of quantum computing might
hold and how current quantum computers are flawed.
We then dive into our discussion on quantum machine learning and Peter's new course on
the topic which debuted in February.
I'll link to that in the show notes.
Finally, we briefly discuss the work of E1 Tang, a PhD student from the University of
Washington whose undergrad thesis a quantum inspired classical algorithm for recommendation
systems made quite a stir last summer.
As a special treat for those interested, I'm also sharing my interview with E1 as a bonus
episode alongside this one.
I'd love to hear your thoughts on how you think quantum computing will impact machine
learning in the next 20 years.
Send me a tweet or leave a comment on the show notes page.
I'd like to thank Pegasystems, this episode sponsor, and join them in inviting you to
Pegaworld, the company's annual digital transformation conference to be held at the MGM Grand
in Las Vegas from June 2nd to 5th.
Pegasystems puts AI at the center of its customer engagement software so that every customer
touchpoint on every channel is optimized in real time, so that customers find each interaction
relevant and timely, whether a sales call, a marketing campaign, or a customer service
chat.
At Pegaworld, you'll hear great stories of AI applied to the customer experience at
real Pegacustomers.
The event is a great way to learn from a who's who of the Fortune 500, and of course,
I'll be there, and we'll be speaking as well.
To register, visit Pegaworld.com and use the promo code Twimble19 when you sign up for
$200 off.
Again, that's Twimble19, it's as easy as that.
Hope to see you there.
And now onto the show.
All right, everyone, I am on the line with Rob Walker. Rob is the vice president of
decision management and analytics with Pegasystems.
You may remember Rob's name from TwimbleTalk number 127 on hyperpersonalizing the customer
experience with AI.
Rob, welcome back to this week in machine learning and AI.
Yeah, thanks, Sam.
Glad to be back.
Absolutely.
Glad to have you back.
I'm looking forward to what I believe will be a really interesting conversation on the
role of empathy in AI.
For folks that want to learn more about you, I'll refer them back to the previous episode,
but give us a quick overview of your focus at Pegasystems.
Yes, I'm happy to do that.
So within Pegas, I'm responsible for our AI space, but we really try, I mean, there's
so much hype around AI and we don't do AI just for AI sake.
We really try to focus on making AI work for typically pretty large enterprises and
typically in the area of customer engagement, right?
So in the previous episode, we talked about hyperpersonalization and hyperpersonalization
is really trying to be one-to-one conversations with customers for companies to do that.
And that requires a lot of AI.
It also requires lots of other things, but AI is an important aspect of that.
And that's what I mostly worry about.
And also areas around AI.
It's not just, hey, AI is cool.
Let's use that in customer engagement to make customer engagement better, but it's
stuff like, can we trust it?
Who in the organization should be responsible for it?
If it makes weird decisions, if there is a bias, those kind of things.
So that's typically what I worry about.
The concept that kept coming up in our last conversation, I think this is pretty central
to the way you think about applying AI to optimizing customer experiences.
This idea of helping your customers figure out the next best action to take with their
customers.
Is that right?
That is correct.
Yes.
Yes.
So the companies we work with typically implement like this customer decision hub, right?
And it's a centralized decision authority that across all the different channels that
companies may have, figures out the next best action during conversations, right?
So what should we say?
What should we not say?
What price should we mention if it's commercial, basically trying to have very reasonable
conversations, but at the same time, because most of the companies we work with are not
just charities, right?
So at the same time, you need to sort of improve customer value, right?
So the next best action, the best in next best action, is typically some metric about customer
value and trying to improve that over time by doing the best thing possible to optimize
that.
So this concept of empathy in AI is something that you'll be speaking about at the next
Pegaworld, an event that your company hosts annually.
I attended the last one and I'll be attending the next one.
How did this idea of empathy, introducing empathy into these kinds of transactions or customer
experiences?
Where did that come from?
Well, so I've always been interested.
So before I joined Pegaw at some point before that I was a scientist, right, in AI.
I did my PhD in that area and I've always been interested in not just all the cool things
AI can do around predicting customer behavior and things like that, but also potentially
the not so cool things, right?
So can you trust it?
Is it transparent?
Is it opaque?
Is there a bias?
Can it go rogue?
You know, those kind of things.
So over the last years, we've really tried to sort of guard the moral high ground if you
will around AI and I'm just look at what it can, the value it can bring, but also mitigate
the risk that you can have with AI.
And following sort of that path, empathy was a very natural thing.
I mean, you know, the bigger thing is morality, you know, the morality of AI and AI decisions,
which is, you know, that's a big beast and sort of more ethical behavior and empathy
seem to be something that was just about tangible enough to try to really put it into the
product and into the vision of best practice around using AI.
So we've been spending quite some time thinking about that and how you can operationalize
that kind of thing.
Now, when you start talking about the morality of AI, certainly, and even to large degree
empathy, I start to, you know, the picture in my head starts to form around, you know,
what some of us will call AGI, artificial general intelligence, you know, what we talk
about more commonly is like sci-fi AI, right?
Is that what we're talking about here?
Or are we talking about something that, you know, how can you make this more concrete
for us?
Yeah, because because I'm definitely not talking about that kind of thing.
I mean, that's really, well, it's, it's, it's interesting.
Right?
I mean, I think everybody in AI is thinking about how that, how that, how that works.
But I think just as a human species, if I just, you know, just even reading the news, you
know, today, I think I'm, I'm not sure we have morality, you know, very, very, very,
very clear for, for, for everyone, at least there is a lot of discussion about what would
be moral judgment and not a moral judgment to even expect that of AI.
I think it's already a, a tough act, but I'm certainly not talking about it in that kind
of realm quite yet, although it's a very interesting topic, right?
I was just thinking about, you know, self-driving cars, right?
And, and, and sort of the moral judgments, they may, one day have to make, right?
In extreme circumstances.
But this is very much sort of a smaller subset of those challenges where we're talking about
customer engagement and those kinds of things.
And I think in that area, empathy really shows in, in, in, in, in, in, in pretty clear dimensions.
Like stuff like, is this, if we are talking to me as a company, and it's an AI-driven
conversation, is that, is that a relevant?
Right?
Or are you wasting my time?
Stuff like, is it, is it appropriate?
Right?
So you're talking to me about something and I, it might be interesting, it may be interesting
to me but it may not still be appropriate. Maybe you shouldn't be selling me a gun or a car
or a credit card that I actually can't pay back when I'm in debt. So it's those kind of thing
and is there mutual value? Are you talking to me for something that can we have a transaction that
has a mutual value or is it just about the company? And I think if companies implement those
kind of considerations well, I think that will do pretty well on an empathy scorecard for starters.
Now that's interesting and actually somewhat different from the, I don't know if it's different
from the direction that I thought we were going to go here or that you know the picture that formed
in my mind when we were talking earlier about empathy or if one is part of the other. But I'll tell
you I'll kind of recount the picture that I have in my money. Let me know where it fits into
this world. You know I was envisioning primarily the kinds of interactions that you might have
via a chat bot or you know a chat kind of interface and you often have or even you know to the
extent to which AI is driving a call a call center agent and their responses because that's some
of that or IVR. You know some of that is starting to happen. But I was envisioning kind of this
set of capability where you know maybe the you know whether the chat bot or the IVR you know IVR
is a great example right it's like you can IVR should be able to tell from my voice or could
be could tell from my voice that I'm getting frustrated navigating the 50 million you know menus
and maybe escalate me you know a little bit more quickly to someone or you know you can imagine
the same kind of thing happening in a chat interaction where you know I'm interacting with this
with this virtual agent it's not getting what I need to do or it's you know I'm needing asking
me to repeat myself you know multiple times you know there's a there's a degree of empathy and
all that where it's understanding my I guess I'm kind of simplifying that is understanding my
emotional state and using that as part of the the decisioning around what the next best action
to take is yeah but it sounds like maybe that's a piece of what you're saying but you're also
talking about you know maybe about the broader AI ethics conversation your example around you
know should we offer the the credit card to the person who can't afford it is one that kind of
you know resonates and kind of drives me in that direction yeah now I think so so I think the
example that you gave right like is is somebody getting frustrated and those kind of things I think
are a very important part of of empathy but I think it's part of the delivery mechanism so I think
what we're trying to do is sort of take that in in in in in in two different layers if you will
right one is when you're talking to someone and somebody is getting frustrated or if you do voice
detection and or inflection and you you sort of notices that somebody is getting upset
you may want to change that may influence the next best action as part of the context so I would
call that although it's called more of the sort of the superficial way of empathy right it's trying
to feel somebody's mood and use it as a context but that can become from a lot of different senses
it could be the you know as I said the inflection of a voice it could be when this is face to face
or you're in front of a camera right can be that you know people can sort of read their face or
the system can read the face of the of the customer and see that's not going well but that's not
the same as the underlying level of making sure that the next best action that you are contemplating
is one that is empathetic or even moral right so I see that as two different different thing I think
people think about empathy a lot like you were just describing it like hey I see this is
making you upset so I need to you know hurry this along or ask you wrong and that's all
cool very human stuff but it's under the livery of a particular action but determining what you're
going to do that also requires empathy and that's more along the line of is this is this
relevant is this appropriate is this suitable does kind of things and do you specifically use
the word empathy to distinguish it from ethics or are those ideas different in your mind or
are they you know it's just a different word in this case yeah now I think I think the if I think
about ethics that sort of you know ethical behavior I think empathy is basically the step where
humans for now maybe AI at some point will basically be able to you know please themself in someone
else's place and say hey if this is happening to me you know is this going to make me you know
happier and things like that so I think they are in that sense very very related
hmm so in that sense ethics is kind of relevant to some broader set of you know societal norms
whereas empathy we don't have to figure all that out it's just about you know this particular
customer and I am I doing the right thing by this customer yes am I doing the right thing and I
think there are few dimensions to that right am I doing the right thing in terms of am I forcing
you to take like a risk that I actually think you know is too high I already know that you won't
be able to pay back that mortgage or that credit card or that thing or you don't really need this
kind of thing right so that's that's that's that's part of that decision but also is it relevant
in the first place right it's it's an empathetic thing to not try and waste somebody's time right
I mean if you don't I don't know about you Sam but if you like all these ads all the stuff that you
get if you you know you know browse the internet and and look at all of these pages it's it's
we're used to it it doesn't show a lot of empathy right everybody's trying to get your attention
to do things that you know maybe 1% of the time you're vaguely interested in right so that's
that's that's that's part of it is it relevance is it not wasting my time is it do you remember the
context do you remember that I just spoke to you in another channel right I just walked into the
branch I just visited your website and now I'm going into the IPR do you even remember that or
do are you forcing me to basically repeat everything I did I did you know in the previous channel
that's empathy right empathy with customers that are trying to solve a problem or that want to
get value out of their interaction with the company this is clearly an issue that is much bigger
than AI right we don't have to you know look very far to recognize that the in many ways the
previous financial crisis with the the mortgage bubble grew out of giving loans to people that
you know weren't qualified for them and there are many many more examples where organizations you
know fail to exhibit the kind of empathy that you are describing that have nothing to do with
artificial intelligence or machine learning you know why why take this on from an AI perspective
well I think that's a good question and I think the the answer to that is that if the way we look
at customer interaction in general is to always do this next best action kind of thing and the next
best action is actually collaboration between humans you know inside the company deciding on you
know rules or thresholds or policies working together with AI where AI is maybe determining the
risk it's determining the the level of likely interest from from the customer and it's that combination
that creates the metric for this is the best thing to do right now right so you're you're quite
right it's the and actually that mortgage example or the bubble that you just described is a great
combination right there are analytic models that should have said listen um for this group of
people um the risk is not really acceptable and um you shouldn't be pushing them on this level
of of of of mortgage right suitability for instance is not taking into account right so it's
that combination of AI and rules um that I you know we we call that decisioning or decision
management um that basically needs to represent empathetic behavior right so it's not just the AI
it's also the the rules but one of the reasons I think the AI aspect is so important is because
the AI is learning right so it can you know have evolved um a particular bias right it may be a
very opaque algorithm that may have evolved that bias and you don't even know right so there are
a lot of aspects of AI that I think really touch on ethics and empathy as well when you're talking
about this at Pega World are you you know are you raising this as an issue that customer should
start thinking about this are you talking about new capabilities that you're unveiling at Pega
uh uh with you know with the product that will help them address these issues is it um you know
or is there something else well I don't want to feel all my own center but uh we'll we'll definitely
so two years ago one of the things that I was talking about in the keynote was about this
single the t switch and the t was you know stands for transparency but also for trust and it's
basically the ability that once you have this centralized next best action capability inside of
your company um that you can have full control over where you allow um sort of opaque algorithms
like deep learning or genetic algorithms or that's kind of fancy stuff or where you insist on
more transparent algorithms that you can actually explain to a customer and that you really
explain or that you really understand yourself right so that was one aspect um next up is the
is that the thing around empathy I won't I think it's a really good thing if companies are aware
at all times how empathetic their behavior is so um think about sort of a dashboard where you would
see of all the actions my company is taking and these are you know the decide companies we work with
these are hundreds of millions a day right hundreds of millions of interactions a day and then being
able to see okay these are actions that we are taken automatically is a combination of AI and rules
that are not empathetic and that means that they are going against the relevance or they are not
appropriate like not suitable so we we're we're talking about this credit card but it's not really
suitable um or it doesn't really create value for the customer right so imagine that while all of
this is running this combination of AI and rules and it's making hundreds of millions of you know
decisions and having all of these conversations with customers that you can just see that as a real
time thing and say hey really we're getting you know less empathetic um let's see in our
strategies in our customers strategies that we have in the algorithms that we use where we're losing
that right are we pushing products that we shouldn't be pushing don't we have the rules that are
determining suitability it's it's it's it's it's those kind of things and then in addition to that
I think we can also determine sort of the cost of not being empathetic right so if you for instance
if you if you if you if you are going against somebody's interests and I don't mean I mean interest
in in in terms of of relevance right so so what a lot of marketing is currently doing right
they're they're they're spamming you with stuff they're wasting your time on things that are
actually not that relevant to you it would be good to not only know the percentage of of of of of
events where that happens it would also be really good if you had a sense of the money um
you're actually losing and and you can calculate that mathematically by for instance saying hey
this is what we actually talked about to this customer we talked about this mortgage
and the customer said no to it because it wasn't relevant um what we could have been talking about
is this particular issue that we spotted um you know in in in in in a in a different channel or
last them uh last week or an hour ago or maybe a much more relevant offer that we didn't think
we wanted to do because the margin was you know not as big as on the mortgage right having making
that a transparent thing having people own the kind of um you know empathy level of the brand
I think is um is a really important thing going going forward it strikes me that that latter point
around quantifying the cost of these non empathetic actions is really a big part of where the problem
lies it's it's easy to it's easy to know the cost of the you know they expected
revenue or profit from offering something but a lot harder to know the cost of just wasting the
the customer's time or you know reducing the the brand goodwill because of some series of you
know less relevant or poor experiences how do you overcome that gap yeah well I think some of the
math actually works out quite nicely right so remember that when we do this next best action and
I said before the best is a function of you know the value that is created in the in the in the
relationship so um the math works out that you actually can know that if you go against the
propensity because for instance you're selling let's use this mortgage example that works really
well um if you um if you are thinking this is not particularly relevant but if the customer says
yes this is the margin this is the money the i as a bank in this case will will will will make
right if you calculate all the times a customer said no because you're basically offering stuff
that's not particularly relevant just you know you're you're hoping that the customer will say yes
um what how could we have used that moment how could we have used that interaction with the
customer in a better way that would have created more value if you just multiply that with a hundred
of millions of of decisions you make you get to a monetary value and you find in examples like
this that you know there's some explicit decision that where the customer is saying hey let's offer
this more profitable thing even though we know it's not as relevant or um does that does that happen
in more subtle ways no the i don't think these ways are particularly subtle right so the way
the way because the way we work um is like so to do this next best action right we would calculate
um every single thing this is also part of this hyper personalization
vision right to be completely one to one it means that of all the possible conversations you could
have right you're going to rate them in real time based on the context and you're going to say
this is the thing we are going to talk about as a combination of what the AI thinks is
particularly appropriate and relevant as well as my rules that I have around profitability
and inventory uh inventory and and all of those kind of things right so it's that combination
but we calculate them all in parallel so it's relatively easy to see okay this is what we chose
to actually talk about but this is what we could have talked about if we had weighted suitability higher
or if we didn't overrule this very low um or this very high propensity and said well even though
that's relevant it's not what we want to talk about right so you can see how you get a drop off
of typical uh or or of specific conversation topics that you decide not to pursue for other reasons
and that's what you can um calculate is the task then starting that to you know build awareness
on the part of customers or users or kind of the the industry uh as as a whole to incorporate
these types of empathy metrics if we call them that into um you know their systems their rules their
algorithms um and start to is it as I don't know if simple is the right word but is it as simple
as you know just you know starting to try to put numbers around uh these you know suitability
context to where relevance you know risk and then feeding them into your uh uh your automation
tooling with uh kind of appropriate weights or is it does it go beyond that I think i think the
way you describe it so what we're trying to do is first of all we will talk about that's like an
easy task no no no no no no no no like that but yeah yeah no that's not easy in itself but what we
we are trying to do is, first of all, make it very explicit. When we talk about next-best
action, there are patterns that we've seen that are repeatedly successful and they include
things like relevance, suitability, mutual value, risk mitigation. The tooling and the methodology
already encourage companies to at least think about it. They may think, okay, well, for suitability,
we really don't care about it, or not as much, or we let profitability, trump, suitability,
anytime, but at least the product, if you follow the product guidance, you will have to take all
of these considerations into account. There is an ethical framework built into the software,
into the strategies that it will generate. That's one aspect of it, and then the other aspect of it
are like the dashboard that you will show. It's basically shaming companies a little bit if they
want, having them self-shame them into appropriate behavior, where they would say, hey, listen,
we cranked up profitability, but it's at the expense of suitability or customer interest.
At least, I think the awareness and the transparency around these things will be leading to better
behavior. How does the company begin to put tangible numbers and costs around things like mutual
value and suitability in contexts, awareness and relevance? Relevance is maybe easier because it
impacts propensity to buy. Risk is something that's fundamentally numerical, but some of these
others are a little bit squishier, maybe. You're right. From what we've seen,
is that the less squishy things, once you are aware that you need to put them in,
and that the AI or the decision in general, touching 100 million, making 100 million decisions
in all the different channels with all of your customers, that that is part of your brand,
and you need to protect that. I think that's a very important thing. I think for the squishier
things, I think what we also encourage and also make possible is continuous experimentation.
There is always control groups, there is all sorts of things where you can, for a small
percentage, a small sample of the customers, you can actually measure if you're having an effect,
if they have a positive response to the brand, and you can see if what kind of strategy changes
would improve that, and that is a best practice kind of thing to do.
Do you have any examples of folks that you've worked with that you can kind of walk us through
how this all plays out and how they went about making, kind of incorporating these ideas
into the way they make decisions? I think even before we invested in all this
around empathy and also before the transparency and opacity, it's not like these big brands are
not aware of these issues. I think 10 years ago, maybe longer ago, I had long conversation
with banks that were sued for miscelling that I think we've seen more recent examples,
where obviously these companies want to sort of control that even if just for their own sake,
to not be part of some class action. And in the next best action methodology,
stuff like relevance, appropriateness, failure, and risk have always been sort of first class
citizens. What we're now trying to do is to make sure that it's much harder to break those
patterns, or if you don't want to be compliant with these kinds of ethics practices, you at least
have to make the effort. I think to your question, I think especially the banks, I think we see
that in other industries as well are getting very worried about their brand image in that regard,
and they are putting suitability criteria, for instance, it's a pretty hot topic right now,
they're putting that as part of their next best action strategy. We just want to help them
by showing the cost of that and the benefits of that. You mentioned compliance in there.
Do you envision a time where an enterprise will have a formal empathy compliance regime?
Will it be called that? Will it be called something else? Does it already exist under some other
guys? I think that definitely will happen in some cases already happens. I don't think it's
called empathy. It's probably more on the ethics board. It's not only about the company itself,
it's also about basically a compliance issue out of self-interest, especially now. This is
where the AI is so important, where there's so much self-learning going on on this incredible scale
that there could be a bias and there could be all sorts of things happening that may not be so
easy to control or even spot. I know that some of the companies I talk to, and these are larger
companies, but they have a board already for all the algorithms that are involved in customer
interaction. I think that's a sensible thing to do. It's part of what inspired this transparency
or trust switch in the software to make sure that all AI is, at least, you can control
the level of transparency that you require in certain circumstances, talking to customers.
Do you envision a chief empathy officer? It sounds like no, it's probably going to be if anything,
it'll be a chief ethics officer or some other role. Where do you see this all sitting?
Yeah, well, I think this is a very interesting topic, because I think this will become
very, very important if it isn't already. I think you will get into a situation where you have
at the first level AI trying to check other AI for biases or an ethical behavior, because
it's just a lot, and it would only escalate if such a bias or other irregularities is detected.
But it's certainly, and again, I'm talking about the larger companies with tens to hundreds of
millions of customers that are very worried about, especially with the level of automation that's
now available, and then AI that is dynamically learning new things or evolving new things
to have an ethics board like that. We try from a product perspective, we try to make sure
that like you have QA tests on quality assurance tests, on performance, and other things that,
as a matter of course, you would do the same thing around biased detection or other irregularities
before you release the next version of your corporate brain, so to speak, to make that easier.
I think the ideas of making these more empathetic types of qualities of your various offers,
as you've suggested throughout this conversation, it's very much connected to this idea of
transparency. There are these dollars and sense things that we build into decision-making algorithms
all the time, but there's all this other stuff that goes into the customer experience, and what we're
doing here, we're calling empathy is really the idea of making a lot of those non-premofacy
financial aspects A, more transparent, and then B, like putting trying to make them more financial
or putting numbers against them, and then incorporating them into decision-making,
dashboarding them so that there's some awareness of them, and so that the organization can
manage against them. It's a really interesting set of ideas around how to make this idea of AI
ethics a lot more tangible. Yeah, I think yeah, yeah, tangible I think is the right word, so can
we are there straightforward ways to make sure that in our customer's strategies, empathy is well
represented, and we can choose to ignore it, but then there are these, as you say, these
desporting, these gauges, these dials that show you, that shame you into some compliance.
And also, let's not forget that, I think the reason we as humans have empathy,
you know, there can be lots of different theories around that, but personally, I think that
evolved, right? It evolved out of a desire to collaborate, right? So, empathy is not like a cost
to the company, empathy is actually establishing your, you know, better relationship and a longer term
relationship with your customers. Well, it would be interesting to kind of follow along with
this work and see, you know, I'd love to hear a case study of, you know, how a customer kind of
implements this end and once you've got this out in the market and have folks working with it.
Yeah, I mentioned PegaWorld earlier, any besides from your own keynote, other things that
you're looking forward to at the conference? Yeah, well, I mean, this will be the biggest effort,
so it's always just very exciting about, you know, to show people, you know, where we are at,
and it's not just about empathy, right? It's about, it's about also making decisions in general
at a huge scale, you know, with this real-time AI on the one hand, and then on the other hand,
and that's also part of empathy, although we didn't talk about it right now, but then
following up on it, right, with the processes, right? So we are really, and you will hear a lot
about that at PegaWorld, we're trying to, you know, have that combination very strongly. So it's
we have the AI and a decision to decide what to do, right? And then we have sort of the end-to-end
automation that will tell the company how to do it and to do it fast and efficiently, which also
plays into empathy. So I really love that interplay between sort of decisions and processes,
so I'm expecting a lot of really good inter discussions and presentations from our customers.
Awesome, awesome. Well, Rob, I'm looking forward to seeing you once again at the event.
Thanks so much for taking the time to jump on and talk this through with us.
Okay, well, you're very welcome. Thank you. Thanks, Rob.
All right, everyone, that's our show for today. For more information on Peter or any of the topics
covered in the show, visit twimmelai.com slash talk slash 245. As always, thanks so much for listening
and catch you next time.
