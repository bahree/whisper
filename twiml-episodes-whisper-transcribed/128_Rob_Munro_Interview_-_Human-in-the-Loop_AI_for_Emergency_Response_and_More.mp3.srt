1
00:00:00,000 --> 00:00:16,320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

2
00:00:16,320 --> 00:00:21,480
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:21,480 --> 00:00:31,920
I'm your host, Sam Charrington.

4
00:00:31,920 --> 00:00:37,960
In this episode, I chat with Rob Monroe, CTO of the newly rebranded Figure 8, formerly

5
00:00:37,960 --> 00:00:40,560
known as Crowdflower.

6
00:00:40,560 --> 00:00:45,640
Figure 8's human and aloof AI platform supports data science and machine learning teams, working

7
00:00:45,640 --> 00:00:51,960
on autonomous vehicles, consumer product identification, natural language processing, search relevance,

8
00:00:51,960 --> 00:00:54,680
intelligent chat bots, and more.

9
00:00:54,680 --> 00:00:59,000
Rob and I had a really interesting discussion, covering some of the work he's previously

10
00:00:59,000 --> 00:01:04,280
done applying machine learning to disaster response and epidemiology, including a use case

11
00:01:04,280 --> 00:01:10,000
involving text translation in the wake of the catastrophic 2010 Haiti earthquake.

12
00:01:10,000 --> 00:01:14,040
We also dig into some of the technical challenges that he's encountered in trying to scale

13
00:01:14,040 --> 00:01:18,720
the human and aloof side of machine learning, since joining Figure 8, including identifying

14
00:01:18,720 --> 00:01:23,480
more efficient approaches to image annotation, as well as the use of zero-shot machine learning

15
00:01:23,480 --> 00:01:26,160
to minimize training data requirements.

16
00:01:26,160 --> 00:01:31,680
Finally, we briefly discuss Figure 8's upcoming train AI conference, which takes place

17
00:01:31,680 --> 00:01:34,440
on May 9th and 10th in San Francisco.

18
00:01:34,440 --> 00:01:39,840
At train AI, you can join me and Rob, along with a host of amazing speakers like Gary

19
00:01:39,840 --> 00:01:46,840
Casparov, Andre Carpathi, Marty Hurst, and many more, and receive hands-on AI, machine

20
00:01:46,840 --> 00:01:51,480
learning, and deep learning training through real world case studies on practical machine

21
00:01:51,480 --> 00:01:53,600
learning applications.

22
00:01:53,600 --> 00:02:01,600
For more information on train AI, head over to figure-8.com slash train-AI, and be sure

23
00:02:01,600 --> 00:02:09,600
to use the code TwimmelAI, that's TWIMLAI, for 30% off of your registration.

24
00:02:09,600 --> 00:02:14,560
For those of you listening to this on or before Friday, April 6th, Figure 8 is offering an

25
00:02:14,560 --> 00:02:17,400
even better deal on event registration.

26
00:02:17,400 --> 00:02:22,720
Use the code Figure-8 to register for only $88.

27
00:02:22,720 --> 00:02:27,040
A huge thanks to Figure 8 for sponsoring this episode of the podcast.

28
00:02:27,040 --> 00:02:40,120
And now on to the show.

29
00:02:40,120 --> 00:02:41,120
All right, everyone.

30
00:02:41,120 --> 00:02:43,440
I am on the line with Rob and Rob.

31
00:02:43,440 --> 00:02:49,200
Rob is CTO of Figure 8, the company that you may know as Crowdflower.

32
00:02:49,200 --> 00:02:51,400
They recently changed their name.

33
00:02:51,400 --> 00:02:54,400
Rob, welcome to this weekend machine learning and AI.

34
00:02:54,400 --> 00:02:55,400
Thank you.

35
00:02:55,400 --> 00:02:56,400
It's great to be here.

36
00:02:56,400 --> 00:02:58,480
It's great to have you on the show.

37
00:02:58,480 --> 00:03:03,120
We usually start by having our guests give us a little bit of an introduction to their

38
00:03:03,120 --> 00:03:07,680
backgrounds and how they got interested and involved in machine learning and AI.

39
00:03:07,680 --> 00:03:10,480
Why don't you introduce yourself to the audience?

40
00:03:10,480 --> 00:03:11,480
I'd be delighted to.

41
00:03:11,480 --> 00:03:15,360
My introduction to AI was a little bit circular.

42
00:03:15,360 --> 00:03:21,480
So I was working as a software developer for a number of years, not really focused on

43
00:03:21,480 --> 00:03:23,480
artificial intelligence.

44
00:03:23,480 --> 00:03:28,080
And that had taken me to work for the United Nations Eye Commission for Refugees.

45
00:03:28,080 --> 00:03:35,280
I was working in refugee camps in Liberia while I was living in Sierra Leone.

46
00:03:35,280 --> 00:03:40,960
And there was one particular moment when I was in rural Liberia in a refugee camp there.

47
00:03:40,960 --> 00:03:46,080
And we were installing solar power systems at a clinic supporting the camp.

48
00:03:46,080 --> 00:03:51,440
And we heard all these rumors about refugees coming over the border from Cottovoire.

49
00:03:51,440 --> 00:03:54,840
But we didn't know whether there were 10 refugees or 10,000.

50
00:03:54,840 --> 00:03:59,000
They're in just the next valley, but we couldn't reach them.

51
00:03:59,000 --> 00:04:05,360
And ultimately, after a day of doing work at this clinic, we had to move on without establishing

52
00:04:05,360 --> 00:04:06,520
work condition.

53
00:04:06,520 --> 00:04:09,040
These refugees were in and how many they were.

54
00:04:09,040 --> 00:04:14,520
And what really stood out for me was that I had five bars of cell phone reception at

55
00:04:14,520 --> 00:04:15,520
that time.

56
00:04:15,520 --> 00:04:20,360
And I'm lucky to get a full reception here in Silicon Valley.

57
00:04:20,360 --> 00:04:27,640
And so there's no doubt that this refugee community also had cell phones with them.

58
00:04:27,640 --> 00:04:31,560
Those cell phones were probably bouncing signals off the same tower as mine, but I'd

59
00:04:31,560 --> 00:04:33,360
know where to connect with them.

60
00:04:33,360 --> 00:04:38,640
And even if we did, we probably didn't share any languages.

61
00:04:38,640 --> 00:04:43,600
And the languages for which they did speak, even basic things that we took for granted

62
00:04:43,600 --> 00:04:49,320
and AI 10 years ago, like spam filtering or search engines, wouldn't have worked well

63
00:04:49,320 --> 00:04:51,880
or at all for their languages.

64
00:04:51,880 --> 00:04:54,000
So there really just wasn't anything out there.

65
00:04:54,000 --> 00:04:59,960
And in terms of the supporting technology, which would allow us to understand or translate

66
00:04:59,960 --> 00:05:00,960
between the languages.

67
00:05:00,960 --> 00:05:05,360
And so it was clear to me at that time that we really solved the problem of connect in

68
00:05:05,360 --> 00:05:06,360
the world.

69
00:05:06,360 --> 00:05:10,880
Now, but we had an imbalance in how we're bringing services.

70
00:05:10,880 --> 00:05:15,920
And so I thought, well, anyone can be out drilling solar panels into the roof of a clinic

71
00:05:15,920 --> 00:05:18,760
that doesn't really require specialized skills.

72
00:05:18,760 --> 00:05:22,400
I studied artificial intelligence as an undergraduate or something that I'd remain interested

73
00:05:22,400 --> 00:05:23,400
in.

74
00:05:23,400 --> 00:05:26,840
And so it was at that point that I decided that I really wanted to pursue a career in

75
00:05:26,840 --> 00:05:33,240
artificial intelligence and make sure that we could bring it to everybody in the world.

76
00:05:33,240 --> 00:05:36,880
And so that's what ultimately took me back to graduate school.

77
00:05:36,880 --> 00:05:44,160
So I completed my PhD at Stanford focused on how we can apply natural language processing

78
00:05:44,160 --> 00:05:50,000
to low resource languages, both in health and disaster response context.

79
00:05:50,000 --> 00:05:57,360
And since then, I've worked in a combination of social impact companies and also, you know,

80
00:05:57,360 --> 00:06:02,280
right through to very large, you know, 4 to 500 tech companies like Amazon.

81
00:06:02,280 --> 00:06:08,320
And that brought me to working at Figure 8 just six months ago where I'm really excited

82
00:06:08,320 --> 00:06:11,360
to be the new CTO here.

83
00:06:11,360 --> 00:06:12,360
Awesome.

84
00:06:12,360 --> 00:06:15,040
What did you do your PhD on?

85
00:06:15,040 --> 00:06:16,040
Sure.

86
00:06:16,040 --> 00:06:23,560
So I was looking at ways where we could go from very little initial training data to adapted

87
00:06:23,560 --> 00:06:30,080
models in health and disaster response contexts, especially looking at text messages.

88
00:06:30,080 --> 00:06:35,280
So when people were sending text messages to a hospital clinic in Malawi in a language

89
00:06:35,280 --> 00:06:42,400
district with Chichoa, and then also looking at two disaster response communication data

90
00:06:42,400 --> 00:06:50,840
sets in Haiti and Pakistan from an earthquake in 2010 and floods later that year in Pakistan.

91
00:06:50,840 --> 00:06:55,520
These are actual disasters that as a disaster response professional I worked in.

92
00:06:55,520 --> 00:07:00,320
And so I was very much answering that question for myself, okay, if we don't have any initial

93
00:07:00,320 --> 00:07:08,280
training data in these languages, what's the minimum amount of human interaction and labeling

94
00:07:08,280 --> 00:07:14,160
and categorization and mapping of these messages that is required until we can start building

95
00:07:14,160 --> 00:07:19,920
out automated services in these languages in a way that's language independent.

96
00:07:19,920 --> 00:07:20,920
Oh, interesting.

97
00:07:20,920 --> 00:07:21,960
And how far did you get?

98
00:07:21,960 --> 00:07:27,880
Did you clearly not have completely solved problem, but what did you, what conclusion did

99
00:07:27,880 --> 00:07:29,400
you arrive at?

100
00:07:29,400 --> 00:07:34,240
The broad conclusion was that you could do a lot of so word modeling, which would mean

101
00:07:34,240 --> 00:07:40,080
that you could take one technique and get very similar accuracies across these different

102
00:07:40,080 --> 00:07:41,080
languages.

103
00:07:41,080 --> 00:07:45,920
But what an important observation was that the techniques that worked in English would

104
00:07:45,920 --> 00:07:48,760
not necessarily carry over to other languages.

105
00:07:48,760 --> 00:07:54,640
So linguistically, English tends to be an outlier language in that the spelling is extremely

106
00:07:54,640 --> 00:08:00,200
standardized, even with like the US and the UK variants.

107
00:08:00,200 --> 00:08:06,240
People are highly literate and we have very few affixes in English, very few prefixes

108
00:08:06,240 --> 00:08:09,600
and suffixes compared to most languages.

109
00:08:09,600 --> 00:08:15,920
And what that means is that a lot of approaches to data language processing that views a single

110
00:08:15,920 --> 00:08:23,120
word as a fundamental unit won't apply, won't be very accurate in other languages.

111
00:08:23,120 --> 00:08:30,280
So for example, in the text messages between health workers in Chichoa, in 600 messages,

112
00:08:30,280 --> 00:08:36,480
there were more than 50 different spellings for the word patient due to variations in literacy,

113
00:08:36,480 --> 00:08:40,920
but also just due to the large number of different suffixes and combinations of suffixes and

114
00:08:40,920 --> 00:08:44,600
prefixes that you get in that language.

115
00:08:44,600 --> 00:08:50,920
And so it was positive in terms of being able to find techniques that we've now deployed

116
00:08:50,920 --> 00:08:58,480
in disaster response situations globally, but it also raised some questions about a lot

117
00:08:58,480 --> 00:09:03,520
of research in natural language process, and there's just focused on English as to how

118
00:09:03,520 --> 00:09:10,560
much it really could make an impact if you were taking it to the 7000 other languages

119
00:09:10,560 --> 00:09:11,560
in the world.

120
00:09:11,560 --> 00:09:12,560
Right, right.

121
00:09:12,560 --> 00:09:20,640
Because your sense that the techniques broadly apply but the tooling and the dictionaries

122
00:09:20,640 --> 00:09:27,920
and all of that, you know, haven't, you know, need to be kind of ported over to these

123
00:09:27,920 --> 00:09:34,160
other languages or that there's, you know, a whole set of different research that has

124
00:09:34,160 --> 00:09:38,240
to happen to support languages with different structures.

125
00:09:38,240 --> 00:09:45,400
And part of the question is prompted by at least on the, you know, with deep learning

126
00:09:45,400 --> 00:09:53,120
and neural networks, a lot of what I hear in the conversations that I'm thinking back

127
00:09:53,120 --> 00:10:01,320
to a specific interview where the, I think this was Shubus and Gupta at Baidu at the time

128
00:10:01,320 --> 00:10:05,360
was talking about how they did their, you know, English to Mandarin translation without

129
00:10:05,360 --> 00:10:11,600
any Mandarin speakers, you know, on staff at the time because the deep neural networks

130
00:10:11,600 --> 00:10:17,800
were able to figure out the meaning and the ability to translate without having specific

131
00:10:17,800 --> 00:10:23,440
knowledge, structural knowledge of really either of the languages.

132
00:10:23,440 --> 00:10:27,920
So how do you think that that kind of translates to, you know, this issue that you're pointing

133
00:10:27,920 --> 00:10:32,360
out the disparity in kind of tools and research across languages?

134
00:10:32,360 --> 00:10:38,040
I think a lot of that research is being really exciting in terms of being able to get up

135
00:10:38,040 --> 00:10:44,880
to speed a lot faster without resources like dictionaries or the sources or other resources

136
00:10:44,880 --> 00:10:46,640
in those languages.

137
00:10:46,640 --> 00:10:52,080
So for most languages in the world, we don't have a dictionary, so we're not an online

138
00:10:52,080 --> 00:10:57,000
dictionary that is easily available for, for natural language tools, for the majority

139
00:10:57,000 --> 00:11:03,360
of the world's languages, the only real scientific resource we have is probably a PhD written

140
00:11:03,360 --> 00:11:08,800
by a hippy linguist 40 years ago, and so that doesn't, doesn't give you much of a, much

141
00:11:08,800 --> 00:11:10,760
of a starting point.

142
00:11:10,760 --> 00:11:16,640
And so a lot of the techniques that I was looking at in my PhD and then I've seen taken

143
00:11:16,640 --> 00:11:21,680
much further in exciting ways in more recent deep learning approaches has really focused

144
00:11:21,680 --> 00:11:26,400
on making no assumptions about the structure going in and still being able to get these

145
00:11:26,400 --> 00:11:33,200
really accurate results, and I think a lot of work in machine translation has shown

146
00:11:33,200 --> 00:11:34,200
that.

147
00:11:34,200 --> 00:11:40,880
The only caveat is that a lot of the neural-based techniques typically require a lot more

148
00:11:40,880 --> 00:11:48,280
in the way of labeled data, and so there are particular use cases where the first time

149
00:11:48,280 --> 00:11:52,920
you work in a language is following a sudden onset disaster, then you're coming up against

150
00:11:52,920 --> 00:11:57,960
other problems of, you need some kind of model to start working with a very small amount

151
00:11:57,960 --> 00:12:03,320
of data, and certainly you can't be waiting days, which is typical for training some

152
00:12:03,320 --> 00:12:06,400
of these large machine-limited models.

153
00:12:06,400 --> 00:12:12,560
So it's certainly been net positive, moved towards language independence, but it has been

154
00:12:12,560 --> 00:12:15,520
bringing up some new problems associated with it.

155
00:12:15,520 --> 00:12:21,360
Yeah, so this experience on the research side and running into the need to have these

156
00:12:21,360 --> 00:12:26,520
labeled data sets in place in order to really make progress and create the kind of tools

157
00:12:26,520 --> 00:12:32,840
that you're looking to create really set you up for, I guess, a deep and personal experience

158
00:12:32,840 --> 00:12:41,200
for the need for organizations to be able to amass these labeled training data sets, and

159
00:12:41,200 --> 00:12:50,040
you spent quite a bit of time working on that from the time you spent at the UNHCR and

160
00:12:50,040 --> 00:12:52,800
then in grad school.

161
00:12:52,800 --> 00:12:53,800
That's right.

162
00:12:53,800 --> 00:13:00,800
Yeah, I was a client of figure eights, maybe five or six different times before joining

163
00:13:00,800 --> 00:13:06,680
the company, and fully appreciated the value in being able to build up training data

164
00:13:06,680 --> 00:13:09,240
sets for a number of different use cases.

165
00:13:09,240 --> 00:13:16,280
The first time I used figure eight was during grad school in 2010, so at that point I was

166
00:13:16,280 --> 00:13:21,880
tasked with running their first step in a 911 service, following earthquake in Haiti in

167
00:13:21,880 --> 00:13:24,200
January 2010.

168
00:13:24,200 --> 00:13:29,200
And at the time, more than 100,000 people killed immediately, and while a lot of local

169
00:13:29,200 --> 00:13:34,480
services collapsed, as in buildings, physically collapsed, housing, and to respond services,

170
00:13:34,480 --> 00:13:37,440
most of the cell phone towers remained active.

171
00:13:37,440 --> 00:13:41,560
And so working with a number of people, and including the US State Department, we set

172
00:13:41,560 --> 00:13:45,760
up a free phone number that anyone in Haiti gets in a text message to.

173
00:13:45,760 --> 00:13:50,880
So the lines were overloaded for phone calls, but text messages were still getting through.

174
00:13:50,880 --> 00:13:56,520
But it showed this problem where everyone in Haiti, almost everyone in Haiti, only spoke

175
00:13:56,520 --> 00:14:00,760
Haitian Creole, a language that is not widely spoken.

176
00:14:00,760 --> 00:14:04,960
But everyone come in into Haiti, only had English as a common language.

177
00:14:04,960 --> 00:14:12,520
So I was tasked with finding people who could translate a text message sent in Haitian

178
00:14:12,520 --> 00:14:17,760
Creole, categorize that, plot the location on a map based on the written location and

179
00:14:17,760 --> 00:14:18,840
the message.

180
00:14:18,840 --> 00:14:23,280
And so then you would have a structured English report with a longitude and latitude streamed

181
00:14:23,280 --> 00:14:25,000
back to the men's seat responders.

182
00:14:25,000 --> 00:14:31,040
So people like the US Coast Guard, who could go in and respond to those actual messages.

183
00:14:31,040 --> 00:14:36,520
And that was my first experience with figure eight is something we launched in just 48 hours

184
00:14:36,520 --> 00:14:43,120
and then ultimately found about 2,000 members of the Haitian diaspora worldwide from

185
00:14:43,120 --> 00:14:46,400
across 49 different countries.

186
00:14:46,400 --> 00:14:47,880
And they were the workforce.

187
00:14:47,880 --> 00:14:52,480
So they were the ones who were reading the messages, translating them, categorizing.

188
00:14:52,480 --> 00:14:57,800
Using their local knowledge to know where villages were, which villages would not appear

189
00:14:57,800 --> 00:15:02,520
on any map, but they could at least know from the satellite view where they were.

190
00:15:02,520 --> 00:15:07,200
And then they were able to do this for 80,000 messages.

191
00:15:07,200 --> 00:15:12,960
So an incredibly large amount of data, like several novels with a data in more or less

192
00:15:12,960 --> 00:15:16,040
real time in about four and a half minutes.

193
00:15:16,040 --> 00:15:22,520
So that was a really important moment for me in realizing with distributed human computing

194
00:15:22,520 --> 00:15:29,360
how quickly you can structure data and how you can do this across a really large number

195
00:15:29,360 --> 00:15:30,880
of people.

196
00:15:30,880 --> 00:15:35,680
And so it was certainly a positive experience for as much as it could be for the members

197
00:15:35,680 --> 00:15:40,720
of the Haitian diaspora to be able to help their country from so far away.

198
00:15:40,720 --> 00:15:46,400
Obviously, I really appreciated being able to get that scale of information to the disaster

199
00:15:46,400 --> 00:15:49,200
responders on the ground as well.

200
00:15:49,200 --> 00:15:52,600
And that's ended up also being something that I, like I mentioned, I then studied from

201
00:15:52,600 --> 00:15:58,360
my PhD, looking at ways that we could extend that process with natural language processing

202
00:15:58,360 --> 00:16:03,720
so that we could scale it to even large data sets, larger than even very large numbers

203
00:16:03,720 --> 00:16:05,880
of humans I could process.

204
00:16:05,880 --> 00:16:08,600
And that's where a lot of my work has been used inside the time.

205
00:16:08,600 --> 00:16:15,960
It sounds like you have been involved in a number of, or were involved in a number of

206
00:16:15,960 --> 00:16:24,400
different uses of figure eight and as you, as you described it, distributed human computing,

207
00:16:24,400 --> 00:16:27,680
prior to actually joining the company, what were some of the others?

208
00:16:27,680 --> 00:16:33,520
Yeah, so I use case that extends from from that one very closely is in epidemic tracking.

209
00:16:33,520 --> 00:16:39,920
So just these outbreaks are still the largest large cause of mortality in the world.

210
00:16:39,920 --> 00:16:41,800
And no one is really tracking them all.

211
00:16:41,800 --> 00:16:46,560
So you see a lot of movies where there's a wall room and a big map of the world and

212
00:16:46,560 --> 00:16:51,040
like a heat map it flares up every time there's an outbreak that only exists in the movies

213
00:16:51,040 --> 00:16:52,840
unfortunately.

214
00:16:52,840 --> 00:16:58,040
And I think that the budget for those movies might be bigger than the actual budget for tracking

215
00:16:58,040 --> 00:17:00,040
epidemic globally.

216
00:17:00,040 --> 00:17:04,560
Maybe the closest thing is the Google trends or Google predict.

217
00:17:04,560 --> 00:17:09,160
I forget the specific thing but Google and blue trends, yeah, flu trends, right?

218
00:17:09,160 --> 00:17:14,880
Is it still only flu or is it, do they, are they able to predict a broader set of things

219
00:17:14,880 --> 00:17:15,880
now?

220
00:17:15,880 --> 00:17:20,160
They were able to predict a broader set of things that, that group which was in Google

221
00:17:20,160 --> 00:17:24,720
or at the time were actually one of the funders of the company that I was working in doing

222
00:17:24,720 --> 00:17:25,720
epidemic tracking.

223
00:17:25,720 --> 00:17:27,640
So we're within closely with them.

224
00:17:27,640 --> 00:17:32,560
And this was also a problem which was very linguistic.

225
00:17:32,560 --> 00:17:40,040
So like I mentioned, English only makes up about 5% of the world's conversations daily.

226
00:17:40,040 --> 00:17:47,440
And most of the world's diseases come in the thin band of the tropics where you just

227
00:17:47,440 --> 00:17:50,280
correlates with ecological diversity.

228
00:17:50,280 --> 00:17:55,280
So most of the world's ecological diversity is in the tropics therefore most of the pathogens

229
00:17:55,280 --> 00:17:57,280
come from there too.

230
00:17:57,280 --> 00:18:02,600
That also happens to correlate with more than 90% of the world's linguistic diversity.

231
00:18:02,600 --> 00:18:10,320
So the first time that a language is mentioned, it's really, really unlikely to be in English

232
00:18:10,320 --> 00:18:13,840
or even any other dominant language.

233
00:18:13,840 --> 00:18:20,280
And so we can go back in time and find examples of disease outbreaks well before they were

234
00:18:20,280 --> 00:18:26,000
finally identified by virologists and epidemiologists as being new strains of a potential disease

235
00:18:26,000 --> 00:18:27,000
outbreak.

236
00:18:27,000 --> 00:18:28,000
Okay.

237
00:18:28,000 --> 00:18:32,680
So for swine flu, there are open reports in a local spanish language newspaper written

238
00:18:32,680 --> 00:18:36,880
in Mexico about months before it was identified as new virus.

239
00:18:36,880 --> 00:18:42,880
In the case of bird flu coming out of areas just outside of Hong Kong, there were reports

240
00:18:42,880 --> 00:18:47,960
weeks before it was identified as being a stranger of the flu.

241
00:18:47,960 --> 00:18:53,840
And so this is another example using crowd flower to collect information about potential

242
00:18:53,840 --> 00:18:58,520
disease outbreaks worldwide, you know, make sure they're real feverers, not Justin Bieber

243
00:18:58,520 --> 00:19:05,440
feverers, and then filter from millions of reports across 15 different languages, use

244
00:19:05,440 --> 00:19:10,840
a machine learning and a small number of human experts as well.

245
00:19:10,840 --> 00:19:16,040
So that the turnout breaks each day that mattered, whether only reports seen by the virologists

246
00:19:16,040 --> 00:19:20,360
out of the millies of potential reports out there.

247
00:19:20,360 --> 00:19:25,600
I'm wondering if you're aware of any efforts to kind of bring together a network of, you

248
00:19:25,600 --> 00:19:35,240
know, data scientists and machine learning AI experts to be able to help respond to emergencies

249
00:19:35,240 --> 00:19:36,240
as they happen.

250
00:19:36,240 --> 00:19:40,800
Like the, I'm not sure the specifics of how you got pulled into the, well, you got pulled

251
00:19:40,800 --> 00:19:48,120
into the Haiti situation based on the company you were at after, yeah, I forget how you

252
00:19:48,120 --> 00:19:54,720
say you got pulled into the, it was a connection at the U.S. State Department.

253
00:19:54,720 --> 00:20:00,600
So at that time, I was doing work on a text messages between health workers in Africa

254
00:20:00,600 --> 00:20:03,920
in the general language of Malawi.

255
00:20:03,920 --> 00:20:09,680
And when it became clear that that text messages were the only formal communication, those

256
00:20:09,680 --> 00:20:16,120
widely available in Haiti, I got dragged into it as really just as the only person working

257
00:20:16,120 --> 00:20:17,120
in this field.

258
00:20:17,120 --> 00:20:22,800
So unfortunately, no one has, as far as I know, tried to complete a PhD in text messages

259
00:20:22,800 --> 00:20:26,920
and low resource languages and health and disaster response context since.

260
00:20:26,920 --> 00:20:33,080
So I was, I remain the expert on that area, unfortunately, by being the only person who

261
00:20:33,080 --> 00:20:36,440
has looked into that kind of research.

262
00:20:36,440 --> 00:20:42,760
What's been very encouraging is that since that time, more than 10 years later, a lot of

263
00:20:42,760 --> 00:20:48,240
large companies have been moving in the direction of providing better tooling and support.

264
00:20:48,240 --> 00:20:54,200
So immediately before I joined, figure eight, I was running product for natural language

265
00:20:54,200 --> 00:21:00,760
processing and translation at AWS, so emeralds, web services, Amazon AI.

266
00:21:00,760 --> 00:21:07,080
And so as the product lead there, I was able to have a lot of influence and found a lot

267
00:21:07,080 --> 00:21:14,240
of internal support for making sure that Amazon's first suite of native NLP technology on

268
00:21:14,240 --> 00:21:21,600
AWS is language independent where they a clear roadmap to supporting as wide a variety

269
00:21:21,600 --> 00:21:23,360
of languages as possible.

270
00:21:23,360 --> 00:21:28,080
And I think that this is where we can see a lot of the biggest impact while I appreciated

271
00:21:28,080 --> 00:21:33,800
my time working for the UN and with charity organizations.

272
00:21:33,800 --> 00:21:39,560
I found that it's in many ways more important to make sure that you get this diversity in

273
00:21:39,560 --> 00:21:41,480
people's everyday tools.

274
00:21:41,480 --> 00:21:46,640
So the analogy I like to use is that if you go into an area after a disaster, the disaster

275
00:21:46,640 --> 00:21:52,640
response community are all driving turtle land rivers, right, because a turtle is a well

276
00:21:52,640 --> 00:21:53,640
known car.

277
00:21:53,640 --> 00:21:57,840
It's been tested for millions of miles before you took it into a critical situation.

278
00:21:57,840 --> 00:22:02,360
And there's plenty of people who know how to operate one or to repair one.

279
00:22:02,360 --> 00:22:05,520
And I take a similar view with software.

280
00:22:05,520 --> 00:22:13,520
And so software, which is specifically made for low resource health and disaster response

281
00:22:13,520 --> 00:22:17,600
environments, I can tend to be buggy, not well supported.

282
00:22:17,600 --> 00:22:24,440
And so while it's not as easy to immediately measure the impact, I think some of the greatest

283
00:22:24,440 --> 00:22:29,480
ways to improve the world have been in making sure that the most popular cloud platform

284
00:22:29,480 --> 00:22:34,680
AWS as a view to be language independent so that people can build out these tools in

285
00:22:34,680 --> 00:22:40,560
a well known environment, along with other work that I've done both here at Figure 8 and

286
00:22:40,560 --> 00:22:41,560
previously.

287
00:22:41,560 --> 00:22:49,640
So for example, working with a lot of manufacturers of cell phones to ensure that they give

288
00:22:49,640 --> 00:22:54,560
as much linguistic diversity as possible in their speech recognition systems.

289
00:22:54,560 --> 00:23:04,160
I think a lot of people associate Figure 8 slash crowd flower with this idea of human and

290
00:23:04,160 --> 00:23:05,160
loop.

291
00:23:05,160 --> 00:23:14,640
And I think one of the themes that has been part of our conversation around machine learning

292
00:23:14,640 --> 00:23:22,280
and AI is this idea that often people will put it as AI versus humans.

293
00:23:22,280 --> 00:23:31,200
And there has been, I think a consistent and growing set of voices that it's not AI versus

294
00:23:31,200 --> 00:23:34,880
humans or AI or humans is AI and humans.

295
00:23:34,880 --> 00:23:40,200
How is your perspective on that evolve since doing this early work?

296
00:23:40,200 --> 00:23:48,200
I imagine you get involved in a lot of customer activity at Figure 8.

297
00:23:48,200 --> 00:23:56,800
What's your take on the importance of humans in delivering machine learning and AI products

298
00:23:56,800 --> 00:24:01,200
and solutions and how do you see that evolving over time?

299
00:24:01,200 --> 00:24:06,360
Yeah, I think one of the most important problems we're solving in technology right now is

300
00:24:06,360 --> 00:24:11,560
working out what it looks like for humans and AI to work together on problems.

301
00:24:11,560 --> 00:24:17,200
This is something that we're seeing across our client base, whether it's a self-driving

302
00:24:17,200 --> 00:24:25,200
car company looking to get the right human feedback on the hours and hours of videos taken

303
00:24:25,200 --> 00:24:31,920
from their cars collecting data right through to medical imaging, what's the right way for

304
00:24:31,920 --> 00:24:41,040
a predictive service to identify, for example, breast cancer cells to either replace or advise

305
00:24:41,040 --> 00:24:46,880
the doctor right through to the use cases that I've been looking at in text where your

306
00:24:46,880 --> 00:24:52,920
letting humans make a decision based on large volumes of reports and the AI component

307
00:24:52,920 --> 00:24:55,520
is prioritizing that for a person.

308
00:24:55,520 --> 00:25:00,560
And so I think we're really just scratching the surface right now in the ways in which

309
00:25:00,560 --> 00:25:05,160
AI and humans can work together, and there's a lot more excited work.

310
00:25:05,160 --> 00:25:11,640
When you say more exciting work is that work to be identified or work that is being done

311
00:25:11,640 --> 00:25:15,120
in different places that you're specifically aware of?

312
00:25:15,120 --> 00:25:19,120
Yeah, I mean, we're doing a lot of exciting work right now with companies.

313
00:25:19,120 --> 00:25:24,440
So for example, I imagine you're a self-driving car company and you're looking to identify

314
00:25:24,440 --> 00:25:28,400
pedestrians on the street.

315
00:25:28,400 --> 00:25:33,360
If you don't have any AI systems in an incredible training data, there's going to be a pretty

316
00:25:33,360 --> 00:25:34,880
tedious experience.

317
00:25:34,880 --> 00:25:39,400
So a person might have to draw a manually draw a bounding box around every single pedestrian

318
00:25:39,400 --> 00:25:42,640
and every frame in hours and hours of video.

319
00:25:42,640 --> 00:25:44,920
And that's incredibly tedious.

320
00:25:44,920 --> 00:25:50,280
And most of that video is going to be empty, it's going to be people driving down freeways

321
00:25:50,280 --> 00:25:51,720
and then highways.

322
00:25:51,720 --> 00:25:58,600
And so there's at least four different ways where we're commonly introducing AI into

323
00:25:58,600 --> 00:26:01,360
that human annotation process at the moment.

324
00:26:01,360 --> 00:26:06,920
So the first is selecting what's interesting, so what's the car at the intersection and

325
00:26:06,920 --> 00:26:12,400
having some highway driving, but not having that be 90% of what the car is learning.

326
00:26:12,400 --> 00:26:18,880
And then with those bounding boxes around the pedestrians, to what extent can we pre-populate

327
00:26:18,880 --> 00:26:25,520
using the machine learning model and have the humans' edits accept or reject those boxes.

328
00:26:25,520 --> 00:26:32,520
And then automatically track those objects between frames against allowing humans to edit

329
00:26:32,520 --> 00:26:34,280
except then reject.

330
00:26:34,280 --> 00:26:39,200
And then finally, taking advantage of a lot of craft class quality control methods, how

331
00:26:39,200 --> 00:26:42,560
can we give the same task to multiple people to make sure that they agree with each other

332
00:26:42,560 --> 00:26:50,040
and errors don't propagate and then combine those different, maybe slightly overlapping

333
00:26:50,040 --> 00:26:52,560
boxes in the most optimal way.

334
00:26:52,560 --> 00:26:57,280
And so every piece of that step, selecting the right data, using predictive bounding

335
00:26:57,280 --> 00:27:01,800
boxes rather than having somebody manually draw them, semi-automating the tracking in

336
00:27:01,800 --> 00:27:06,520
videos and then combining in different human judgments into one final judgment for the

337
00:27:06,520 --> 00:27:11,400
training data or four of those steps using machine learning in different ways.

338
00:27:11,400 --> 00:27:17,320
So I think that's already incredibly exciting in terms of the ways that the humans and

339
00:27:17,320 --> 00:27:22,520
machines are collaborating, but at the end of the day, that's just putting a box around

340
00:27:22,520 --> 00:27:23,520
objects.

341
00:27:23,520 --> 00:27:30,520
I think the kinds of interfaces that we can develop, not even at R&D stage yet.

342
00:27:30,520 --> 00:27:35,200
I think this is going to be some of the most exciting advances at the intersection of human

343
00:27:35,200 --> 00:27:38,600
computer interaction in AI in the coming decades.

344
00:27:38,600 --> 00:27:44,880
And do you have any ideas around what those might look like or do you know places where

345
00:27:44,880 --> 00:27:47,760
folks are working on that kind of thing that you can point us to?

346
00:27:47,760 --> 00:27:53,400
You know what, there's no one really working on this anymore than we are right now.

347
00:27:53,400 --> 00:28:00,640
We're going to this out with a lot of our customers and just starting to see some research

348
00:28:00,640 --> 00:28:08,000
papers coming out of AI labs and human computer interaction labs, but it's really new.

349
00:28:08,000 --> 00:28:14,800
There's not even conferences in academia dedicated to the intersection of human computer

350
00:28:14,800 --> 00:28:17,440
interaction and artificial intelligence at the moment.

351
00:28:17,440 --> 00:28:20,960
So this is really nice and I think incredibly important.

352
00:28:20,960 --> 00:28:21,960
Yeah, I agree.

353
00:28:21,960 --> 00:28:28,320
It's something that I've been kind of mentioning and asking about and for on the podcast

354
00:28:28,320 --> 00:28:30,200
probably for over a year now.

355
00:28:30,200 --> 00:28:37,200
I think there's a book that's kind of classic in the design, the design field, the design

356
00:28:37,200 --> 00:28:38,200
of everyday things.

357
00:28:38,200 --> 00:28:41,160
I forget the name of the author.

358
00:28:41,160 --> 00:28:50,080
But we've built up a huge body of expertise and approaches, methodologies, senses for

359
00:28:50,080 --> 00:28:57,280
what works and what doesn't around design minus intelligence, just kind of design of

360
00:28:57,280 --> 00:28:59,280
stuff.

361
00:28:59,280 --> 00:29:09,200
And it strikes me that designing with intelligence, designing for devices and systems that have

362
00:29:09,200 --> 00:29:15,240
intelligence is its own field and we need to have people that are thinking about this

363
00:29:15,240 --> 00:29:23,440
stuff and researching it and kind of pushing the frontiers of our knowledge about it.

364
00:29:23,440 --> 00:29:29,640
I think so too and when you add in the complexity of people approaching technology on very different

365
00:29:29,640 --> 00:29:34,840
devices in very different cultures, speaking very different languages, there is so much

366
00:29:34,840 --> 00:29:42,800
out there that we are yet to even look at from an academic viewpoint that I can't wait

367
00:29:42,800 --> 00:29:45,280
to see more people get involved with.

368
00:29:45,280 --> 00:29:53,640
Yes, when you look at these, the four ways that you're looking to evolve the kind of

369
00:29:53,640 --> 00:30:00,440
the human AI interface from a training perspective, can you talk a little bit about what some

370
00:30:00,440 --> 00:30:05,720
of the most interesting technical challenges have been and how you've approached them?

371
00:30:05,720 --> 00:30:08,080
Yeah, yeah, absolutely.

372
00:30:08,080 --> 00:30:12,400
Some of them are things which we haven't seen at all before, which is somewhat surprising.

373
00:30:12,400 --> 00:30:20,960
So for example, combining the different people's judgments, so taking those say three different

374
00:30:20,960 --> 00:30:24,280
bounding boxes around a pedestrian which don't quite overlap with each other from three

375
00:30:24,280 --> 00:30:28,520
different workers and making sure that that final bounding box which would then become

376
00:30:28,520 --> 00:30:30,960
the training data is the correct one.

377
00:30:30,960 --> 00:30:33,240
So it turns out there's no simple heuristic.

378
00:30:33,240 --> 00:30:35,800
You can't take the average of those boxes.

379
00:30:35,800 --> 00:30:40,600
You can't take the weighted average given the past accuracy of each of those workers on

380
00:30:40,600 --> 00:30:48,680
the task, but you can make this a machine learning task in itself, where you have the

381
00:30:48,680 --> 00:30:53,600
past accuracy of those people, you have their three boxes and you have the image itself

382
00:30:53,600 --> 00:30:57,720
and you can give all of that to a machine learning algorithm.

383
00:30:57,720 --> 00:31:06,440
And so a fun way that one of our scientists set up here was to make this its own layer.

384
00:31:06,440 --> 00:31:11,560
So in a typical image, you have RGB, red, green and blue layers, because that's how the

385
00:31:11,560 --> 00:31:13,520
image is encoded.

386
00:31:13,520 --> 00:31:20,120
And then we treat the human, the various human identifications of boxes as additional layers.

387
00:31:20,120 --> 00:31:27,360
And so this was I think a really neat solution to think about new channels of information.

388
00:31:27,360 --> 00:31:33,120
Some human generated, some computer generated, and then allow in the machine learning algorithm

389
00:31:33,120 --> 00:31:38,800
to find the right combination of information to produce the most accurate result.

390
00:31:38,800 --> 00:31:45,680
Does it work at all to not think of these boxes as as bounding boxes per se, but more

391
00:31:45,680 --> 00:31:47,120
like probability densities?

392
00:31:47,120 --> 00:31:49,920
That's exactly what we do.

393
00:31:49,920 --> 00:31:54,360
That's a good suggestion.

394
00:31:54,360 --> 00:31:59,840
Yeah, we treat them as probability densities where those probabilities are taken from

395
00:31:59,840 --> 00:32:01,680
their their past accuracy.

396
00:32:01,680 --> 00:32:07,800
So with the figure eight platform, we have a lot of quality control measures such as

397
00:32:07,800 --> 00:32:14,040
making sure that people pass quizzes before they can take a task, plus embed in the known

398
00:32:14,040 --> 00:32:19,280
answers into tasks so that we can track someone's accuracy over time and remove them from

399
00:32:19,280 --> 00:32:21,480
a task if they're not accurate.

400
00:32:21,480 --> 00:32:27,640
And what this means is that we have fairly accurate probabilities given every single

401
00:32:27,640 --> 00:32:33,240
person's past work, so someone might be 70 percent accurate, someone might be 95.

402
00:32:33,240 --> 00:32:38,320
And so then each of those probably distributions can be represented.

403
00:32:38,320 --> 00:32:45,000
And then the machine learning can figure out exactly what the right densities or thresholds

404
00:32:45,000 --> 00:32:49,800
are to combine with the image data to produce that final result.

405
00:32:49,800 --> 00:32:50,800
Interesting, interesting.

406
00:32:50,800 --> 00:32:54,960
Anything else come to mind in terms of interesting challenges in this area?

407
00:32:54,960 --> 00:32:58,720
Well, like I said, I think we're just scratching the surface.

408
00:32:58,720 --> 00:33:05,080
And one of the things we lean on really heavily is transfer learning.

409
00:33:05,080 --> 00:33:10,720
So when we're helping someone do predictive bounding boxes or create their final model,

410
00:33:10,720 --> 00:33:15,800
we were able to do this by extending a model that's built on millions of previous images

411
00:33:15,800 --> 00:33:18,800
that that figure it has access to.

412
00:33:18,800 --> 00:33:23,560
And that gets a very high degree of accuracy, even when someone has relatively little training

413
00:33:23,560 --> 00:33:25,240
data to begin with.

414
00:33:25,240 --> 00:33:29,400
I think what's really interesting at the moment is that transfer learning has been incredibly

415
00:33:29,400 --> 00:33:33,880
effective with images, but not so much with language.

416
00:33:33,880 --> 00:33:38,200
So you can take a model learnt on everyday objects.

417
00:33:38,200 --> 00:33:43,360
And then if you start applying that with transfer learning to medical imaging, you get a really

418
00:33:43,360 --> 00:33:48,240
large head start in terms of accuracy from small amounts of training data.

419
00:33:48,240 --> 00:33:53,960
And if you take things which seem almost identical, so you take sentiment analysis on Twitter

420
00:33:53,960 --> 00:33:59,200
content in English, and then you try to apply that to sentiment analysis on your reviews

421
00:33:59,200 --> 00:34:03,160
in English, then that transfer learning is giving you a couple of percent increase

422
00:34:03,160 --> 00:34:09,200
in accuracy only, certainly not enough to make a big difference from a business standpoint.

423
00:34:09,200 --> 00:34:12,600
And so what do you think that is?

424
00:34:12,600 --> 00:34:18,920
Well, I think linguistically, it's just because language is so complex and exciting, the

425
00:34:18,920 --> 00:34:23,600
ways in which we will talk about food for your review, which is really different to the

426
00:34:23,600 --> 00:34:28,240
way we will talk about whatever it is people complaining about on Twitter.

427
00:34:28,240 --> 00:34:35,520
And those stylistics are enough that it's not really the same signal at all.

428
00:34:35,520 --> 00:34:43,600
Whereas with whether it's medical imaging or everyday objects all about satellite imagery,

429
00:34:43,600 --> 00:34:49,760
a 2D object with just three layers of colours has more similarities.

430
00:34:49,760 --> 00:34:55,000
There are 3D objects represented in two dimensions, there are colours, there are clear boundaries

431
00:34:55,000 --> 00:34:57,280
between objects.

432
00:34:57,280 --> 00:35:04,240
And so as a linguist, I find it fascinating, as a machine learning practitioner, I find

433
00:35:04,240 --> 00:35:05,240
it frustrating.

434
00:35:05,240 --> 00:35:09,600
I know a lot of smart people are working on this problem.

435
00:35:09,600 --> 00:35:16,800
I think some of the early results in what's been called zero-shot machine learning, machine

436
00:35:16,800 --> 00:35:21,720
translation, good early examples of the supplemental language.

437
00:35:21,720 --> 00:35:29,520
So I'm hopeful that we can get to a point where we have these models of different language

438
00:35:29,520 --> 00:35:35,600
tasks, which can make adapted new tasks much, much more efficient.

439
00:35:35,600 --> 00:35:42,200
Yeah, this keeps bringing me back to the, I guess the earlier comment or a related comment

440
00:35:42,200 --> 00:35:46,880
to the earlier one I made that it's, I think it's easy to look at some of the results we

441
00:35:46,880 --> 00:35:56,240
see around, you know, machine translation and deep neural net, language modeling.

442
00:35:56,240 --> 00:36:03,120
And I forget the person who made the quote, but there's a famous quote about, you know,

443
00:36:03,120 --> 00:36:10,080
for every linguist I fire, every linguist I get rid of, my accuracy of my models increases,

444
00:36:10,080 --> 00:36:15,000
talking about, you know, the advances we've made in statistical language modeling.

445
00:36:15,000 --> 00:36:20,240
It's interesting, in this conversation here, you, you know, reinforce the importance of

446
00:36:20,240 --> 00:36:27,320
the underlying, you know, linguistics and the language modeling and even within English,

447
00:36:27,320 --> 00:36:33,320
you know, to the extent that even within English, you know, examples from, you know, one

448
00:36:33,320 --> 00:36:37,520
type of communication, Twitter and another type of communication, Yelp reviews, you

449
00:36:37,520 --> 00:36:41,440
know, produce such different results that you can't really transfer easily between the

450
00:36:41,440 --> 00:36:42,440
two.

451
00:36:42,440 --> 00:36:49,280
Yeah, and I think we're at Westside and increasing and the importance of linguistics.

452
00:36:49,280 --> 00:36:52,320
I remember that adage, I think it was every time a linguist quits.

453
00:36:52,320 --> 00:36:58,840
I'm not sure if it was them being fired, the original quote, or maybe it was.

454
00:36:58,840 --> 00:37:05,840
And that's been an amazing change in terms of we really don't as much need someone with

455
00:37:05,840 --> 00:37:09,960
that linguistic knowledge to manually say what the features are, like this is a word

456
00:37:09,960 --> 00:37:13,120
boundary, you know, this is a proper noun, etc.

457
00:37:13,120 --> 00:37:18,360
The deep learning models can take care of a lot of that for us.

458
00:37:18,360 --> 00:37:22,080
And that's certainly also been a research focus for a while.

459
00:37:22,080 --> 00:37:26,120
So in my own research, I was trying to abstract away from specifically linguistic knowledge

460
00:37:26,120 --> 00:37:28,280
and resources so we could scale.

461
00:37:28,280 --> 00:37:33,960
But I'm thinking about some use cases here at the moment where while we don't need the

462
00:37:33,960 --> 00:37:39,240
linguists to extract the features, we get deep learning to that for us.

463
00:37:39,240 --> 00:37:43,800
Linguistic intuitions about how to set up a task can really, really help.

464
00:37:43,800 --> 00:37:51,120
So for one current use case, we're working with a large online retailer that also has

465
00:37:51,120 --> 00:37:53,320
a in-home device.

466
00:37:53,320 --> 00:37:58,000
And they have this problem where on their online store, there are titles, or often like

467
00:37:58,000 --> 00:38:02,520
40 or 50 words long, because the people putting products up there assume that a 50 word

468
00:38:02,520 --> 00:38:05,280
title is going to have better search engine optimization.

469
00:38:05,280 --> 00:38:09,400
However, that's a terrible experience if you're speaking to a device in your home and

470
00:38:09,400 --> 00:38:13,880
you have to listen to all 40 words and then that title to do audio shopping.

471
00:38:13,880 --> 00:38:16,360
And so they need to need to shorten those titles.

472
00:38:16,360 --> 00:38:22,680
And so making this purely a summarization task with sequence sequence models and deep

473
00:38:22,680 --> 00:38:26,440
learning really isn't producing very good results.

474
00:38:26,440 --> 00:38:30,680
The shortened titles and very natural that it look very good at all.

475
00:38:30,680 --> 00:38:33,040
But a bunch is a little bit of linguistic knowledge.

476
00:38:33,040 --> 00:38:36,560
We can see which are the important entities within the title.

477
00:38:36,560 --> 00:38:42,080
We have the name of the brand, the size, the color, the function.

478
00:38:42,080 --> 00:38:47,000
And treat this as a task where we identify those entities and then we can pile that short

479
00:38:47,000 --> 00:38:48,800
title from those entities.

480
00:38:48,800 --> 00:38:53,520
And so we were going from something about 50% of human level accuracy to now something

481
00:38:53,520 --> 00:39:00,000
at 90% of human level accuracy, just because we had a smart linguist analyze the problem

482
00:39:00,000 --> 00:39:06,240
and not have to hard code any features, but they just knew how to cast this problem as

483
00:39:06,240 --> 00:39:10,800
a sequence sequence problem for identifying entities rather than a sequence sequence problem

484
00:39:10,800 --> 00:39:13,560
for summarizing text.

485
00:39:13,560 --> 00:39:22,680
So I think this is one example of where domain expertise is coming back into machine learning.

486
00:39:22,680 --> 00:39:27,600
So a quick note, because I'm sure at least someone out there listening is going to be

487
00:39:27,600 --> 00:39:30,560
wondering who about this quote.

488
00:39:30,560 --> 00:39:36,680
The quote is from Frederick Jalenic, it's every time I fire a linguist, the performance

489
00:39:36,680 --> 00:39:40,720
of the speech recognizer goes up.

490
00:39:40,720 --> 00:39:47,440
So you are much more gracious than Frederick Jalenic in your treatment of the linguists.

491
00:39:47,440 --> 00:39:55,040
But I think what's interesting here is the kind of going back to this broader question

492
00:39:55,040 --> 00:40:04,840
of humans and AI, humans or AI, and human and the loop, there are multiple layers where

493
00:40:04,840 --> 00:40:19,680
AI and humans interface, there's the user, and the AI is getting, in many cases, training

494
00:40:19,680 --> 00:40:26,080
data from the user, the user has to use the AI in some cases, the user is configuring

495
00:40:26,080 --> 00:40:32,880
things that have AI in them and those have peculiarities, and so there's kind of work

496
00:40:32,880 --> 00:40:39,680
that needs to happen there about that user experience element.

497
00:40:39,680 --> 00:40:45,080
There's the folks that are creating training data, and you've talked about some of the

498
00:40:45,080 --> 00:40:56,000
work that you're doing in applying machine learning to that interface between the organization

499
00:40:56,000 --> 00:41:01,720
or entity that's collecting training data and humans that are helping to produce that

500
00:41:01,720 --> 00:41:05,040
training data, helping to label.

501
00:41:05,040 --> 00:41:11,400
And then we've talked about just now, and certainly plenty of times on this podcast, the

502
00:41:11,400 --> 00:41:19,400
role of domain expertise in, you know, broadly speaking data science teams that are building

503
00:41:19,400 --> 00:41:23,520
out machine learning.

504
00:41:23,520 --> 00:41:27,840
So those are kind of three places that humans are kind of in this loop.

505
00:41:27,840 --> 00:41:32,720
Are there others that don't fall into those three?

506
00:41:32,720 --> 00:41:36,920
So this, I'm numerating those three, so that would be the exit.

507
00:41:36,920 --> 00:41:42,680
The end uses the experts and pretzels work is doing labeling of those of three.

508
00:41:42,680 --> 00:41:43,680
Yeah, yeah.

509
00:41:43,680 --> 00:41:48,640
I'm just wondering if you guys have a model for thinking about, or if you have a model

510
00:41:48,640 --> 00:41:55,520
for thinking about kind of the roles, the varying roles that humans play in working with

511
00:41:55,520 --> 00:42:00,480
AI, and does that model, you know, tell you anything interesting?

512
00:42:00,480 --> 00:42:06,800
Yeah, I think we're seeing more and more tiered model of humans creating training data.

513
00:42:06,800 --> 00:42:12,680
So historically, when Figuert was called Crowdflower, it was because initially most of the work

514
00:42:12,680 --> 00:42:15,840
down the platform was CrowdSource.

515
00:42:15,840 --> 00:42:20,640
And Crowdflower still runs at the largest marketplace for CrowdSource workers for training

516
00:42:20,640 --> 00:42:21,640
data.

517
00:42:21,640 --> 00:42:25,440
So we have hundreds of thousands of people and more than 150 countries working on the

518
00:42:25,440 --> 00:42:26,440
platform.

519
00:42:26,440 --> 00:42:29,720
However, that's becoming a smaller and smaller part.

520
00:42:29,720 --> 00:42:36,120
So about a third of people using the Figuert software today are using the running

521
00:42:36,120 --> 00:42:37,120
tunnel experts.

522
00:42:37,120 --> 00:42:42,480
So they have domain expertise, whether that's in medical or financial domain, or simply

523
00:42:42,480 --> 00:42:45,160
sensitive data that I don't want to farm out.

524
00:42:45,160 --> 00:42:48,840
And while they're recognizing the importance of creating training data, they don't see

525
00:42:48,840 --> 00:42:54,480
this as a problem that they want to outsource, at least not for the human component, certainly

526
00:42:54,480 --> 00:42:56,960
they'll listen to our software still.

527
00:42:56,960 --> 00:43:02,920
And then we also have a very large number of, but maybe another third of workforce who

528
00:43:02,920 --> 00:43:06,320
are what we call an NDA crowd.

529
00:43:06,320 --> 00:43:09,440
So these are people who work in a center.

530
00:43:09,440 --> 00:43:15,000
They're often guaranteed an hourly salary, so they have job security.

531
00:43:15,000 --> 00:43:16,000
And they're everywhere.

532
00:43:16,000 --> 00:43:22,640
So that could be in New Orleans, or the Philippines, or in India, we just started to work with

533
00:43:22,640 --> 00:43:30,200
the UN org, hoping to provide employment for refugees in Europe as well.

534
00:43:30,200 --> 00:43:35,000
And the advantage of having groups of people in a room means that one, you know their identity,

535
00:43:35,000 --> 00:43:38,960
they can sign an NDA and see more sensitive data, and also they can be trained up a lot

536
00:43:38,960 --> 00:43:39,960
more.

537
00:43:39,960 --> 00:43:46,520
So they have the ability to understand more complicated data concepts.

538
00:43:46,520 --> 00:43:48,560
And so I find this to be really, really interesting.

539
00:43:48,560 --> 00:43:56,880
So for example, there is a center employing only women in areas of India where people normally

540
00:43:56,880 --> 00:44:00,560
don't get to take part in the information economy.

541
00:44:00,560 --> 00:44:05,640
And they're doing a lot of the work for us for self-driving car companies, which means

542
00:44:05,640 --> 00:44:13,040
that people whose other job opportunities would really only be in agriculture or in roadwork

543
00:44:13,040 --> 00:44:18,800
have learned what every single street sign means across North America, Japan, Europe,

544
00:44:18,800 --> 00:44:22,480
and can very quickly annotate all of those.

545
00:44:22,480 --> 00:44:27,720
So I find it to be fascinating because I don't know how to do these tasks, but they're

546
00:44:27,720 --> 00:44:34,360
able to take on this work, become domain experts, and then contribute to the training data

547
00:44:34,360 --> 00:44:35,360
worldwide.

548
00:44:35,360 --> 00:44:37,440
I think this is often very overlooked.

549
00:44:37,440 --> 00:44:43,360
So I read recently that someone made an estimate that there were 10,000 AI professionals

550
00:44:43,360 --> 00:44:44,960
worldwide.

551
00:44:44,960 --> 00:44:50,200
And so last year we had 60,000 people from Venezuela were alone creating training data

552
00:44:50,200 --> 00:44:54,320
on our platform and became very, really, really proficient at it.

553
00:44:54,320 --> 00:45:00,560
And so I think elevating people who are creating training data, whether that's a crowdsourced

554
00:45:00,560 --> 00:45:06,600
worker or an expert analyst within a company, is going to become something that we'll see

555
00:45:06,600 --> 00:45:11,120
more in the future as people recognize the human input component.

556
00:45:11,120 --> 00:45:20,240
And do you have a perspective on the kind of jobs and broader economic implications of

557
00:45:20,240 --> 00:45:21,240
that?

558
00:45:21,240 --> 00:45:25,480
Yeah, I think ultimately it'll find its way to the end users.

559
00:45:25,480 --> 00:45:31,120
And a lot of the software will be making sure that they can get that implicit feedback

560
00:45:31,120 --> 00:45:33,600
for the training data from the end user.

561
00:45:33,600 --> 00:45:36,600
And so we've seen this already in search engines.

562
00:45:36,600 --> 00:45:41,680
We do power a lot of search engines for various companies of crowdflower, but there are some

563
00:45:41,680 --> 00:45:45,680
like very large commercial search engines that really don't need a lot of extra training

564
00:45:45,680 --> 00:45:49,400
data because they get explicitly when you type something in a Google, whichever link

565
00:45:49,400 --> 00:45:54,960
you click on gives them that feedback as to how they should optimize their search engine

566
00:45:54,960 --> 00:45:59,000
for future similar search terms.

567
00:45:59,000 --> 00:46:03,880
And so that's the piece that's more missing at the moment.

568
00:46:03,880 --> 00:46:08,760
And this is where I see especially for areas like the medical domain.

569
00:46:08,760 --> 00:46:16,400
So there's huge debates at the moment about whether AI are going to replace physicians.

570
00:46:16,400 --> 00:46:21,400
And I can understand people worried about their job, but whenever I hear these arguments,

571
00:46:21,400 --> 00:46:26,240
I'm like, well, before I moved here to the US, the village I lived in in Sierra Leone

572
00:46:26,240 --> 00:46:30,320
had one doctor for every 100,000 people.

573
00:46:30,320 --> 00:46:35,040
And a little more than a decade later, it's still as one doctor for every 100,000 people.

574
00:46:35,040 --> 00:46:37,360
And that's going to be true in 10 years time.

575
00:46:37,360 --> 00:46:39,360
That's not really going to change.

576
00:46:39,360 --> 00:46:43,000
So it's economics of that nation I'm going to change that quickly.

577
00:46:43,000 --> 00:46:48,120
So I would love to be in a world where physicians were a seeded in idle because AI had solved

578
00:46:48,120 --> 00:46:49,120
healthcare.

579
00:46:49,120 --> 00:46:55,040
But I really don't think that's going to happen.

580
00:46:55,040 --> 00:47:01,960
What will happen is that that one physician will be able to scale what they are capable

581
00:47:01,960 --> 00:47:05,800
of working on as they are assisted by AI.

582
00:47:05,800 --> 00:47:10,720
And because they are going to encounter diseases or combinations of infections, which are

583
00:47:10,720 --> 00:47:16,760
very specific to their environment, we want to make sure that we're capturing the data

584
00:47:16,760 --> 00:47:22,720
in a safe but meaningful way at that point of care.

585
00:47:22,720 --> 00:47:27,160
So that AI that they're using can become optimized to the local population that they're

586
00:47:27,160 --> 00:47:28,160
serving.

587
00:47:28,160 --> 00:47:29,160
Awesome.

588
00:47:29,160 --> 00:47:31,320
I think we are about out of time.

589
00:47:31,320 --> 00:47:35,200
Is there anything that you'd like to add to close us out?

590
00:47:35,200 --> 00:47:36,200
Yeah.

591
00:47:36,200 --> 00:47:43,160
So I would love to invite anyone who is in the Bay Area to our train AI conference, which

592
00:47:43,160 --> 00:47:44,840
is in about one month.

593
00:47:44,840 --> 00:47:47,000
We'll be willing to it.

594
00:47:47,000 --> 00:47:49,840
And we have a fun keynote speaker there.

595
00:47:49,840 --> 00:47:58,520
So Gary Casparov, probably the most famous person to fight AI in lose, we'll be talking

596
00:47:58,520 --> 00:48:00,400
about his experience.

597
00:48:00,400 --> 00:48:04,520
And I love how positive he's become.

598
00:48:04,520 --> 00:48:10,600
So he is now one of the biggest advocates for chess that combines AI in humans, showing

599
00:48:10,600 --> 00:48:16,640
that the best chess players are combinations of humans and machines, which will consistently

600
00:48:16,640 --> 00:48:20,240
be both the best machines and the best humans even today.

601
00:48:20,240 --> 00:48:27,800
So we're looking forward to hearing about his experience and then having a large number

602
00:48:27,800 --> 00:48:34,880
of industry practices of machine learning, also sharing their experiences in combining

603
00:48:34,880 --> 00:48:36,680
human and machine intelligence.

604
00:48:36,680 --> 00:48:37,680
Awesome.

605
00:48:37,680 --> 00:48:38,680
And I will be there as well.

606
00:48:38,680 --> 00:48:44,680
And any listeners that are interested in attending can use the code TWIMAL to receive

607
00:48:44,680 --> 00:48:46,920
30% off of registration.

608
00:48:46,920 --> 00:48:47,920
All right.

609
00:48:47,920 --> 00:48:50,520
Well, Rob, it was great chatting with you.

610
00:48:50,520 --> 00:48:52,640
Thanks so much for taking the time.

611
00:48:52,640 --> 00:48:53,640
Same was my pleasure.

612
00:48:53,640 --> 00:48:57,640
Thanks for speaking to me today.

613
00:48:57,640 --> 00:48:58,640
All right, everyone.

614
00:48:58,640 --> 00:49:00,640
That's our show for today.

615
00:49:00,640 --> 00:49:05,760
For more information on Rob or any of the topics covered in this episode, you'll find

616
00:49:05,760 --> 00:49:11,520
the show notes at twimalai.com slash talk slash 125.

617
00:49:11,520 --> 00:49:16,080
If you're new to the podcast and like what you hear or you're a veteran listener and haven't

618
00:49:16,080 --> 00:49:21,440
already done so, please head on over to your podcast app of choice and leave us your most

619
00:49:21,440 --> 00:49:23,720
gracious rating and review.

620
00:49:23,720 --> 00:49:26,920
It helps new listeners find us, which helps us grow.

621
00:49:26,920 --> 00:49:29,000
Thanks in advance.

622
00:49:29,000 --> 00:49:32,040
Thanks again to Rob and to figure eight for sponsoring this show.

623
00:49:32,040 --> 00:49:37,960
And of course, make sure you head on over to figure-8 dot com slash train dash AI to learn

624
00:49:37,960 --> 00:49:43,560
more about the train AI conference and be sure to use code twimalai for 30% off of your

625
00:49:43,560 --> 00:49:45,400
registration.

626
00:49:45,400 --> 00:50:11,160
Thanks for listening and catch you next time.

