1
00:00:00,000 --> 00:00:04,800
Hi, it's Sam, a quick story before we get going.

2
00:00:04,800 --> 00:00:10,480
A few weeks ago at the Twomel AI Summit, I met a listener, Nicholas, who works for a leading

3
00:00:10,480 --> 00:00:12,200
energy company.

4
00:00:12,200 --> 00:00:16,160
We spent a bit of time chatting, and he mentioned a few of the things he'd learned about on

5
00:00:16,160 --> 00:00:20,360
the podcast that he went ahead and implemented at his company.

6
00:00:20,360 --> 00:00:24,720
He was particularly excited about the semantic folding technology he learned about on Twomel

7
00:00:24,720 --> 00:00:26,280
Talk Number 10.

8
00:00:26,280 --> 00:00:32,600
Then, a couple of weeks later, I spoke with Alex, who works at a leading German career

9
00:00:32,600 --> 00:00:33,600
site.

10
00:00:33,600 --> 00:00:38,200
Alex mentioned that he'd recently kicked off a project there to explore applying zero-shop

11
00:00:38,200 --> 00:00:39,600
machine learning.

12
00:00:39,600 --> 00:00:42,880
Once again, an idea he learned about on the podcast.

13
00:00:42,880 --> 00:00:45,840
I'd really like to hear more stories like this.

14
00:00:45,840 --> 00:00:50,640
If the podcast has helped you at work or school, if it's educated or connected to you

15
00:00:50,640 --> 00:00:55,320
to resources or techniques you've found useful or valuable, please take a moment to share

16
00:00:55,320 --> 00:01:02,000
them over at twomelai.com slash 2av, the number 2av.

17
00:01:02,000 --> 00:01:05,800
The occasion, of course, is the podcast's second birthday.

18
00:01:05,800 --> 00:01:08,640
So go ahead, give it this birthday present.

19
00:01:08,640 --> 00:01:13,160
Okay, Nick, hit it.

20
00:01:13,160 --> 00:01:24,720
Hello, and welcome to another episode of Twomel Talk.

21
00:01:24,720 --> 00:01:29,480
The podcast why interview interesting people, doing interesting things in machine learning

22
00:01:29,480 --> 00:01:31,800
and artificial intelligence.

23
00:01:31,800 --> 00:01:42,200
I'm your host, Sam Charrington.

24
00:01:42,200 --> 00:01:47,560
In this episode of our train AI series, I sit down with Anima Anon Kumar, brand professor

25
00:01:47,560 --> 00:01:52,920
at Caltech and principal scientist with Amazon Web Services to discuss the research she's

26
00:01:52,920 --> 00:01:56,000
doing out of her tensor lab.

27
00:01:56,000 --> 00:02:00,680
In our conversation, we review the application of tensor operations to machine learning and

28
00:02:00,680 --> 00:02:06,280
discuss how an example problem, document categorization, might be approached using three-dimensional

29
00:02:06,280 --> 00:02:10,400
tensors to discover topics and relationships between them.

30
00:02:10,400 --> 00:02:15,240
We touch on multi-dimensionality, expectation maximization, and the Amazon products Sage

31
00:02:15,240 --> 00:02:17,240
Maker and Comprehend.

32
00:02:17,240 --> 00:02:22,080
Anima also goes into how to tensorize neural networks and apply our understanding of tensor

33
00:02:22,080 --> 00:02:27,240
algebra to perform better architecture searches.

34
00:02:27,240 --> 00:02:31,040
Before we start, though, a shout out to our friends over at Figure 8 for their sponsorship

35
00:02:31,040 --> 00:02:33,160
of this week's series.

36
00:02:33,160 --> 00:02:37,720
Figure 8 is the essential human and aloop AI platform for data science and machine learning

37
00:02:37,720 --> 00:02:39,000
teams.

38
00:02:39,000 --> 00:02:43,600
The Figure 8 software platform trains, tests, and tunes machine learning models to make

39
00:02:43,600 --> 00:02:45,920
AI work in the real world.

40
00:02:45,920 --> 00:02:51,280
Learn more at www.figure-8.com.

41
00:02:51,280 --> 00:02:57,240
Just a reminder, this episode was recorded on site at Train AI, so there is some unavoidable

42
00:02:57,240 --> 00:02:59,040
background noise.

43
00:02:59,040 --> 00:03:01,920
And now on to the show.

44
00:03:01,920 --> 00:03:04,880
All right, everyone.

45
00:03:04,880 --> 00:03:08,680
I am pleased to be seated here with Anima Anankumar.

46
00:03:08,680 --> 00:03:15,320
Anima is Bren Professor and Microsoft and Sloan Fellow in the Department of Computing

47
00:03:15,320 --> 00:03:20,760
and Mathematical Sciences at Caltech, as well as a principal scientist with Amazon Web

48
00:03:20,760 --> 00:03:21,760
Services.

49
00:03:21,760 --> 00:03:25,080
Anima, welcome to this weekend machine learning and AI.

50
00:03:25,080 --> 00:03:26,080
Thank you, Sam.

51
00:03:26,080 --> 00:03:27,600
It's a pleasure to be here.

52
00:03:27,600 --> 00:03:28,600
Yeah.

53
00:03:28,600 --> 00:03:31,360
It's absolutely a pleasure to finally get an interview going.

54
00:03:31,360 --> 00:03:35,480
It's not been that long since I saw you last, scaled ML, I think.

55
00:03:35,480 --> 00:03:36,480
Stanford.

56
00:03:36,480 --> 00:03:37,480
That's right.

57
00:03:37,480 --> 00:03:43,320
I mean, the scaled ML was a small and an intimate event, but just so many amazing speakers

58
00:03:43,320 --> 00:03:47,600
and the audience was also really cool to interact with.

59
00:03:47,600 --> 00:03:48,600
Absolutely.

60
00:03:48,600 --> 00:03:49,600
Absolutely.

61
00:03:49,600 --> 00:03:55,080
Can you tell us a little bit about your background and how you got interested and started down the

62
00:03:55,080 --> 00:03:57,040
path of ML and AI?

63
00:03:57,040 --> 00:03:58,040
Yeah.

64
00:03:58,040 --> 00:04:03,400
I guess you could say, like, even as a child, I was always interested in math, you know,

65
00:04:03,400 --> 00:04:09,080
like to me, like kind of just the aspect of understanding a lot of structure and explaining

66
00:04:09,080 --> 00:04:13,960
the world through math seem magical.

67
00:04:13,960 --> 00:04:18,600
And then through the years, you know, I did engineering and in the beginning of my PhD,

68
00:04:18,600 --> 00:04:27,240
I wanted to ask questions on how we can use mathematical techniques to understand data

69
00:04:27,240 --> 00:04:33,280
better, to process them better, scale that up and run them in real systems.

70
00:04:33,280 --> 00:04:38,160
Of course, my beginning was humble, you know, I was looking at purely the theoretical aspects

71
00:04:38,160 --> 00:04:41,600
I was asking questions and sensor networks.

72
00:04:41,600 --> 00:04:43,800
Now you might call them Internet of Things.

73
00:04:43,800 --> 00:04:44,800
IoT.

74
00:04:44,800 --> 00:04:45,800
Right.

75
00:04:45,800 --> 00:04:49,400
It didn't exist, which dates me.

76
00:04:49,400 --> 00:04:53,000
But the questions were still very relevant today.

77
00:04:53,000 --> 00:04:56,960
How much of learning do you do on the edge, on the devices?

78
00:04:56,960 --> 00:04:58,360
How do you transmit this?

79
00:04:58,360 --> 00:05:00,440
What about bandwidth constraints?

80
00:05:00,440 --> 00:05:01,520
How do you route them?

81
00:05:01,520 --> 00:05:06,040
How do you exploit the correlations of measurements between different sensors?

82
00:05:06,040 --> 00:05:09,400
So these were the questions that I started tackling.

83
00:05:09,400 --> 00:05:13,520
And to me, like solving these math problems was very fascinating.

84
00:05:13,520 --> 00:05:17,720
But I wanted to also get them working on real systems, right?

85
00:05:17,720 --> 00:05:22,240
So because back then, we didn't have the IoT revolution going at, I decided to switch

86
00:05:22,240 --> 00:05:26,160
to settings where I could just purely work with data.

87
00:05:26,160 --> 00:05:28,560
So I suppose we have all the data in one place.

88
00:05:28,560 --> 00:05:33,960
Can we do something interesting and can I try this out in different scenarios?

89
00:05:33,960 --> 00:05:38,920
And that's when I started working on probabilistic graphical models, understanding relationships

90
00:05:38,920 --> 00:05:44,680
between variables, how we can discover those graphical relationships at scale.

91
00:05:44,680 --> 00:05:49,160
And what techniques would allow us to provide guarantees?

92
00:05:49,160 --> 00:05:53,560
Like, when do we successfully discover them?

93
00:05:53,560 --> 00:05:59,920
And understanding really, when we can discover information about hidden variables.

94
00:05:59,920 --> 00:06:02,720
So as you can imagine, every model is wrong, right?

95
00:06:02,720 --> 00:06:08,880
So there's only better models which can approximate the real world.

96
00:06:08,880 --> 00:06:12,040
In some ways, you know, better than others.

97
00:06:12,040 --> 00:06:18,960
But the question is whether having hidden variables will allow us to have a more realistic model.

98
00:06:18,960 --> 00:06:23,160
Because so many times we cannot measure everything about the real world.

99
00:06:23,160 --> 00:06:28,120
So the ones that are hidden from the measurements can be incorporated them into the model

100
00:06:28,120 --> 00:06:30,040
and how do we discover them.

101
00:06:30,040 --> 00:06:35,520
As an example, you know, think about categorizing documents, right?

102
00:06:35,520 --> 00:06:37,960
So there's a whole bunch of news articles.

103
00:06:37,960 --> 00:06:42,920
But suppose there's no human annotation of what the documents are talking about,

104
00:06:42,920 --> 00:06:44,280
then that's a hidden variable.

105
00:06:44,280 --> 00:06:47,880
We don't know those categories for the documents.

106
00:06:47,880 --> 00:06:51,320
So suppose we can incorporate them as hidden variables

107
00:06:51,320 --> 00:06:54,920
and use the text as absurd variables.

108
00:06:54,920 --> 00:06:58,520
Can we now learn about these relationships from data?

109
00:06:58,520 --> 00:07:01,560
Even when the variables are not directly absurd?

110
00:07:01,560 --> 00:07:02,480
Okay, great.

111
00:07:02,480 --> 00:07:06,280
And so your lab at Caltech is called the tensor lab.

112
00:07:06,280 --> 00:07:11,000
And you spend a lot of your time focusing on researching tensors

113
00:07:11,000 --> 00:07:14,920
and tensor implementations of machine learning algorithms and things like that.

114
00:07:14,920 --> 00:07:20,240
Can you maybe walk us through that line of research and what it's all about

115
00:07:20,240 --> 00:07:25,240
and tie it back to this problem that you started with?

116
00:07:25,240 --> 00:07:26,240
Absolutely.

117
00:07:26,240 --> 00:07:28,400
I mean, it's been a very interesting journey.

118
00:07:28,400 --> 00:07:35,160
It's taken me about six years now to see the tensor field mature more, right?

119
00:07:35,160 --> 00:07:37,640
So I do want to add a caveat.

120
00:07:37,640 --> 00:07:41,800
You know, the research on tensor algebra is very different from tensor flow.

121
00:07:41,800 --> 00:07:45,640
You know, there is always this one person in the audience

122
00:07:45,640 --> 00:07:49,120
who will ask me about tensor flow when I give the talk on tensor.

123
00:07:49,120 --> 00:07:52,800
The common point there is the phrase tensor, which means what?

124
00:07:52,800 --> 00:07:56,720
Yeah, so tensor, you know, as a naive definition, right,

125
00:07:56,720 --> 00:07:58,720
is a multi-dimensional array.

126
00:07:58,720 --> 00:08:01,800
So now you don't just have two dimensions.

127
00:08:01,800 --> 00:08:06,760
It's not just rows and columns, you have many more dimensions in your array

128
00:08:06,760 --> 00:08:08,760
that can be thought of a tensor.

129
00:08:08,760 --> 00:08:13,800
But then there is a deeper algebraic meaning that I think many people

130
00:08:13,800 --> 00:08:16,160
who are not in the field may not be aware, right?

131
00:08:16,160 --> 00:08:19,880
So just as we treat the matrix not just as rows or columns,

132
00:08:19,880 --> 00:08:22,960
there's a whole host of linear algebraic techniques.

133
00:08:22,960 --> 00:08:25,760
So matrix really can be thought of as an operator.

134
00:08:25,760 --> 00:08:29,080
You're operating matrix on a field, you know,

135
00:08:29,080 --> 00:08:32,320
you're using that to change your vector, right,

136
00:08:32,320 --> 00:08:33,320
when you multiply a vector.

137
00:08:33,320 --> 00:08:34,600
The transformation of some sort.

138
00:08:34,600 --> 00:08:36,280
Exactly, exactly.

139
00:08:36,280 --> 00:08:40,960
And so tensors could be thought of as a richer class of such transformations.

140
00:08:40,960 --> 00:08:43,840
And so in that sense, the tensor flow term is apt

141
00:08:43,840 --> 00:08:47,880
because you have an input data, which can be thought of as a tensor.

142
00:08:47,880 --> 00:08:50,920
And there's an output that's also a tensor, right?

143
00:08:50,920 --> 00:08:54,200
I mean, if it's just a vector, that's a special case of tensor.

144
00:08:54,200 --> 00:08:57,120
So tensor flow is one that kind of flows through the network

145
00:08:57,120 --> 00:09:00,360
as a series of tensor transformations.

146
00:09:00,360 --> 00:09:03,000
But the shortcoming in most of the current networks

147
00:09:03,000 --> 00:09:06,480
is we are still using linear algebraic computations

148
00:09:06,480 --> 00:09:08,040
in different layers.

149
00:09:08,040 --> 00:09:09,840
Whether it's a fully connected layer

150
00:09:09,840 --> 00:09:12,440
where we are doing a matrix multiplication,

151
00:09:12,440 --> 00:09:14,160
it could be a convolutional layer

152
00:09:14,160 --> 00:09:16,440
where we have these set of filters.

153
00:09:16,440 --> 00:09:21,560
So can we now expand these set of operations to more dimensions?

154
00:09:21,560 --> 00:09:25,480
To give you an example, you know, can we now in a layer

155
00:09:25,480 --> 00:09:28,200
instead of doing just a matrix multiplication?

156
00:09:28,200 --> 00:09:31,640
Can we directly operate on higher dimensional tensors?

157
00:09:31,640 --> 00:09:35,960
If the input to a layer is now a three-dimensional set of activations

158
00:09:35,960 --> 00:09:41,880
from the previous layer, can we now set up a computation

159
00:09:41,880 --> 00:09:45,360
that doesn't just turn into a matrix multiplication?

160
00:09:45,360 --> 00:09:48,280
That directly operates on all the three dimensions.

161
00:09:48,280 --> 00:09:53,400
And so can exploit the information of different dimensions more effectively.

162
00:09:53,400 --> 00:09:56,800
Is it just me or as part of the challenge with this,

163
00:09:56,800 --> 00:10:04,880
the general trouble we have visualizing higher dimensions than three?

164
00:10:04,880 --> 00:10:07,880
Yeah, no, it's interesting that you ask.

165
00:10:07,880 --> 00:10:10,560
Actually, people, you know, this is not new, right?

166
00:10:10,560 --> 00:10:12,840
Like, thinking in high dimensions.

167
00:10:12,840 --> 00:10:17,360
There is a book I discovered a few years ago called Flatland from...

168
00:10:17,360 --> 00:10:18,240
Edwin Abbott.

169
00:10:18,240 --> 00:10:20,040
Yeah, exactly.

170
00:10:20,040 --> 00:10:21,720
Right? So it's been in arts.

171
00:10:21,720 --> 00:10:24,840
It's been in literature, the general theory of relativity

172
00:10:24,840 --> 00:10:27,120
thinks about many dimensions, right?

173
00:10:27,120 --> 00:10:34,000
So to tell the audience about what this book is about,

174
00:10:34,000 --> 00:10:38,960
the part that I found it fascinating is this is set up in a two-dimensional world.

175
00:10:38,960 --> 00:10:43,520
Like every person is a two-dimensional object like a square or a polygon.

176
00:10:43,520 --> 00:10:48,200
And then there's a three-dimensional monster that visits this world, right?

177
00:10:48,200 --> 00:10:52,400
So as the three-dimensional object is moving through the two-dimensional world,

178
00:10:52,400 --> 00:10:54,560
it's rapidly changing shapes.

179
00:10:54,560 --> 00:10:58,200
And so that's very scary to the two-dimensional people.

180
00:10:58,200 --> 00:11:00,800
So I would use this analogy that many people,

181
00:11:00,800 --> 00:11:02,560
when they cannot visualize something,

182
00:11:02,560 --> 00:11:07,320
they feel that's way too abstract or that's confusing.

183
00:11:07,320 --> 00:11:11,440
But then math is much richer than just the three dimensions that we live in.

184
00:11:11,440 --> 00:11:13,920
Or the four-dimension, if you use time, right?

185
00:11:13,920 --> 00:11:20,160
So they can formulate and solve problems in infinite dimensions.

186
00:11:20,160 --> 00:11:23,120
And that form of thinking and that form of algorithms

187
00:11:23,120 --> 00:11:27,600
will help us process all the multimodal data we are obtaining today.

188
00:11:27,600 --> 00:11:30,800
So it's not just visual data, it could be textual data.

189
00:11:30,800 --> 00:11:33,000
There's, again, to other domain knowledge.

190
00:11:33,000 --> 00:11:37,200
So how do we use all this in an integrated approach?

191
00:11:37,200 --> 00:11:40,200
And that's where tensors can have a big impact.

192
00:11:40,200 --> 00:11:44,160
And so maybe give us some concrete examples.

193
00:11:44,160 --> 00:11:49,760
I mean, one of the things that strikes me is that when we're talking about sensor data,

194
00:11:49,760 --> 00:11:54,640
we're talking about sensor data that, and this is perhaps maybe naive,

195
00:11:54,640 --> 00:11:59,840
but we're talking about sensor data that comes from an inherently three-dimensional world.

196
00:11:59,840 --> 00:12:04,640
I guess if you start layering on time and maybe channels and things like that,

197
00:12:04,640 --> 00:12:06,760
it gets more complex.

198
00:12:06,760 --> 00:12:10,120
Is that where the multi-dimensionality comes from?

199
00:12:10,120 --> 00:12:12,480
You know, when we're extracting fundamentally,

200
00:12:12,480 --> 00:12:15,040
we're extracting things from our real world.

201
00:12:15,040 --> 00:12:21,200
It's interesting that you ask, because indeed, one direct way of using tensors

202
00:12:21,200 --> 00:12:24,240
is when the data itself has many dimensions, right?

203
00:12:24,240 --> 00:12:30,480
But another more nuanced way of using it is to model the relationships between data.

204
00:12:30,480 --> 00:12:35,000
So let me go back to the example of document categorization.

205
00:12:35,000 --> 00:12:39,200
So I talked about how do we automatically discover

206
00:12:39,200 --> 00:12:43,400
the categories or topics in documents when there are no labels available, right?

207
00:12:43,400 --> 00:12:46,440
So unsupervised learning problems tend to be really hard,

208
00:12:46,440 --> 00:12:50,640
because we don't have a human label saying, example saying,

209
00:12:50,640 --> 00:12:53,600
where, what are the topics?

210
00:12:53,600 --> 00:12:57,880
So in this scenario, what you can do is intuitively look at frequency of words,

211
00:12:57,880 --> 00:13:01,280
right? This bag of words is the simplest technique that's out there.

212
00:13:01,280 --> 00:13:02,840
So you can just count words.

213
00:13:02,840 --> 00:13:06,360
So let's say the word apple, it appears a lot in the document.

214
00:13:06,360 --> 00:13:09,880
I mean, this by itself is not informative because it could be the fruit

215
00:13:09,880 --> 00:13:12,000
or it could be the company, right?

216
00:13:12,000 --> 00:13:14,520
So now you can say, okay, let me not just count one word,

217
00:13:14,520 --> 00:13:18,920
let me count occurrences, pairwise occurrences of words.

218
00:13:18,920 --> 00:13:22,680
So what if the word apple and orange occurs together, right?

219
00:13:22,680 --> 00:13:25,480
So even in this case, maybe it's likely to be fruit,

220
00:13:25,480 --> 00:13:27,840
but orange is also a company, right?

221
00:13:27,840 --> 00:13:29,400
So now what about three words?

222
00:13:29,400 --> 00:13:32,400
If I count like occurrences of triplets of words,

223
00:13:32,400 --> 00:13:36,760
if it's apple, orange, banana, then likely it's a fruit, right?

224
00:13:36,760 --> 00:13:40,440
But it's humanly impossible to go label every triplets of categories

225
00:13:40,440 --> 00:13:43,200
and say what topics they should belong to, right?

226
00:13:43,200 --> 00:13:47,160
So can there be algorithms that can automatically discover at scale

227
00:13:47,160 --> 00:13:49,840
these higher order relationships?

228
00:13:49,840 --> 00:13:53,680
Right, so if you think about pairwise relationships, they are correlations.

229
00:13:53,680 --> 00:13:55,840
So you can represent that as a matrix.

230
00:13:55,840 --> 00:13:58,800
So you list all the words as rows and columns,

231
00:13:58,800 --> 00:14:02,840
which count the core occurrences that can be represented as a matrix.

232
00:14:02,840 --> 00:14:03,840
But if you have to do that-

233
00:14:03,840 --> 00:14:08,440
So N-Y's relationships correlates to end-dimensional tensors.

234
00:14:08,440 --> 00:14:09,600
Precisely.

235
00:14:09,600 --> 00:14:13,640
And now the question is, what class of operations can you do on them

236
00:14:13,640 --> 00:14:18,240
to discover these hidden variables to discover the topics in documents?

237
00:14:18,240 --> 00:14:23,320
And what I showed in one of my first papers is how we can decompose

238
00:14:23,320 --> 00:14:26,280
a certain tensor, a three-dimensional tensor,

239
00:14:26,280 --> 00:14:29,720
and discover the topics and discover the relationship

240
00:14:29,720 --> 00:14:33,360
between those topics and the words in the documents.

241
00:14:33,360 --> 00:14:33,960
Interesting.

242
00:14:33,960 --> 00:14:42,720
And so prior to this work or prior to a tensor-oriented approach,

243
00:14:42,720 --> 00:14:45,440
we would first go through the step of mapping this all

244
00:14:45,440 --> 00:14:50,640
to linear algebra and matrix operations and things like that.

245
00:14:50,640 --> 00:14:54,600
And it sounds like the argument is that that's inefficient.

246
00:14:54,600 --> 00:15:01,120
Is it an inefficiency or are we missing out on key tools in doing that?

247
00:15:01,120 --> 00:15:03,000
So there is a bit of everything.

248
00:15:03,000 --> 00:15:07,640
So for this specific, as always, the problem is complicated.

249
00:15:07,640 --> 00:15:14,800
For this specific topic of modeling and other latent variable estimation,

250
00:15:14,800 --> 00:15:16,440
there were two sets of approaches.

251
00:15:16,440 --> 00:15:18,960
One is expectation maximization.

252
00:15:18,960 --> 00:15:21,040
So you have the likelihood function.

253
00:15:21,040 --> 00:15:25,160
So you do a sort of local search method and hope

254
00:15:25,160 --> 00:15:27,720
to reach globally optimal solution.

255
00:15:27,720 --> 00:15:30,160
But many times, it gets stuck, especially

256
00:15:30,160 --> 00:15:32,000
for high-dimensional problems.

257
00:15:32,000 --> 00:15:34,360
And it's also slow.

258
00:15:34,360 --> 00:15:36,960
And the other approach, these matrix approaches,

259
00:15:36,960 --> 00:15:41,000
tend to do simple linear dimensionality reduction techniques.

260
00:15:41,000 --> 00:15:44,480
The principle component analysis is the most common one.

261
00:15:44,480 --> 00:15:48,760
So if you try to do a matrix approach or a linear algebra approach

262
00:15:48,760 --> 00:15:53,160
for topic modeling, you would only discover the subspace

263
00:15:53,160 --> 00:15:56,360
of the topic vectors.

264
00:15:56,360 --> 00:15:59,120
So you won't discover the actual modeling.

265
00:15:59,120 --> 00:16:04,400
And also, there is the class of spectral clustering

266
00:16:04,400 --> 00:16:07,280
where if the documents have separated topics,

267
00:16:07,280 --> 00:16:09,600
you could try to do a clustering approach.

268
00:16:09,600 --> 00:16:11,600
But in many cases, they are not.

269
00:16:11,600 --> 00:16:15,760
There are news articles talking both about science and politics and so on.

270
00:16:15,760 --> 00:16:21,240
So there is no separation of documents into distinct categories.

271
00:16:21,240 --> 00:16:24,280
And in those problems, there is really

272
00:16:24,280 --> 00:16:29,000
no other approach other than expectation maximization

273
00:16:29,000 --> 00:16:31,920
that people have used in practice before.

274
00:16:31,920 --> 00:16:34,760
And what we found is in our experiments

275
00:16:34,760 --> 00:16:38,320
that these tensor methods can be scaled up very efficiently

276
00:16:38,320 --> 00:16:41,800
because they can build on current linear algebra techniques

277
00:16:41,800 --> 00:16:46,880
and be parallelized in an embarrassingly parallel way.

278
00:16:46,880 --> 00:16:49,640
And we have that actually now available

279
00:16:49,640 --> 00:16:53,760
with Amazon SageMaker and the Comprehend Service.

280
00:16:53,760 --> 00:16:57,040
So with that, we have benchmarks that we've released

281
00:16:57,040 --> 00:17:00,200
comparing it to open source topic modeling frameworks

282
00:17:00,200 --> 00:17:02,560
that use expectation maximization.

283
00:17:02,560 --> 00:17:04,880
And we see SageMaker and what?

284
00:17:04,880 --> 00:17:08,520
So SageMaker is the machine learning platform to run experiments.

285
00:17:08,520 --> 00:17:11,440
And the comprehend is the natural language processing service.

286
00:17:11,440 --> 00:17:12,320
OK.

287
00:17:12,320 --> 00:17:14,280
So with Comprehend, you can go and directly

288
00:17:14,280 --> 00:17:18,360
access the results of topic modeling with SageMaker.

289
00:17:18,360 --> 00:17:23,440
You can even play around with the algorithm and run benchmarks and so on.

290
00:17:23,440 --> 00:17:24,600
Tell me if this is right.

291
00:17:24,600 --> 00:17:29,080
It strikes me that your work is kind of operating in two dimensions.

292
00:17:29,080 --> 00:17:31,280
Well, now I'm overusing the term dimensions.

293
00:17:31,280 --> 00:17:36,120
But there's this one angle where there are different approaches.

294
00:17:36,120 --> 00:17:37,800
There's expectation maximization.

295
00:17:37,800 --> 00:17:41,200
There's PCA and others.

296
00:17:41,200 --> 00:17:48,240
And your work is not just that, or is it, the tenserization

297
00:17:48,240 --> 00:17:50,160
of expectation maximization.

298
00:17:50,160 --> 00:17:52,960
It's another approach, but it's an approach

299
00:17:52,960 --> 00:17:57,920
that is made possible because you can operate in this tensor domain.

300
00:17:57,920 --> 00:17:58,640
Is that correct?

301
00:17:58,640 --> 00:18:01,280
Or it really is another approach.

302
00:18:01,280 --> 00:18:03,680
It's looking at a different objective.

303
00:18:03,680 --> 00:18:06,160
So if you have a probabilistic model,

304
00:18:06,160 --> 00:18:09,280
the standard approach is maximized likelihood.

305
00:18:09,280 --> 00:18:11,360
But classically, there was another approach which

306
00:18:11,360 --> 00:18:13,680
was to match the moments.

307
00:18:13,680 --> 00:18:14,880
It goes back to spearmen.

308
00:18:14,880 --> 00:18:18,400
In fact, it goes back to even Gauss, if you go.

309
00:18:18,400 --> 00:18:20,280
If you only match the first two moments,

310
00:18:20,280 --> 00:18:24,160
that's when you get a Gaussian distribution

311
00:18:24,160 --> 00:18:27,200
as an approximation to your true distribution.

312
00:18:27,200 --> 00:18:30,040
And what this is doing is now expanding

313
00:18:30,040 --> 00:18:32,880
those two higher order moments and also

314
00:18:32,880 --> 00:18:37,160
to many coordinates, to multivariate distributions.

315
00:18:37,160 --> 00:18:42,160
Because the intuition is now, if you have a mixture of Gaussians,

316
00:18:42,160 --> 00:18:45,320
you can all just do with two second order moments.

317
00:18:45,320 --> 00:18:47,240
You need to go to higher order moments

318
00:18:47,240 --> 00:18:50,040
to try to separate the mixture components.

319
00:18:50,040 --> 00:18:52,840
And that applies to a broad class of models.

320
00:18:52,840 --> 00:18:55,360
Is tenser and tensor operations a tool

321
00:18:55,360 --> 00:18:58,480
that we can kind of naively go back and apply

322
00:18:58,480 --> 00:19:02,480
to these other things like expectation maximization PCA?

323
00:19:02,480 --> 00:19:07,040
And is it a magic wand that we can wave at other results

324
00:19:07,040 --> 00:19:12,560
and make them better or do we need a whole new set of approaches

325
00:19:12,560 --> 00:19:15,960
that are designed with this set of operations in mind?

326
00:19:15,960 --> 00:19:18,280
So I would say there's this whole spectrum of problems.

327
00:19:18,280 --> 00:19:21,640
Some are more low-hanging fruit than the other ones.

328
00:19:21,640 --> 00:19:24,480
So there are indeed some straightforward use of tensors

329
00:19:24,480 --> 00:19:26,560
where you say, like, oh, PCA.

330
00:19:26,560 --> 00:19:28,800
Now I can do PCA in more dimensions.

331
00:19:28,800 --> 00:19:31,520
Let me tenserize it.

332
00:19:31,520 --> 00:19:34,160
On the other hand, there are more sophisticated approaches

333
00:19:34,160 --> 00:19:37,640
where I can ask, can I now look at the problem

334
00:19:37,640 --> 00:19:41,120
in a new light because I can use tensor methods?

335
00:19:41,120 --> 00:19:44,640
Is this really the right approach there?

336
00:19:44,640 --> 00:19:46,680
And so yeah, so there is a whole spectrum.

337
00:19:46,680 --> 00:19:50,840
And the one that I'm recently been exploring a lot more

338
00:19:50,840 --> 00:19:54,000
on has been on tenserizing neural networks,

339
00:19:54,000 --> 00:19:57,760
but in interesting ways, because the question is,

340
00:19:57,760 --> 00:20:00,640
when can we win over simple linear algebraic techniques

341
00:20:00,640 --> 00:20:02,680
over different layers?

342
00:20:02,680 --> 00:20:06,640
Or can we even think of the overall deep layers

343
00:20:06,640 --> 00:20:10,080
as some kind of hierarchical tensor transformation?

344
00:20:10,080 --> 00:20:12,160
Because that's what it really is.

345
00:20:12,160 --> 00:20:18,160
And one of the works that some of the researchers

346
00:20:18,160 --> 00:20:21,000
have looked at, Nadok Cohen as the primary author,

347
00:20:21,000 --> 00:20:22,720
has been precisely to ask that.

348
00:20:22,720 --> 00:20:26,440
Can we look at algebraic techniques, the group theory,

349
00:20:26,440 --> 00:20:29,000
to understand neural networks better?

350
00:20:29,000 --> 00:20:31,480
So walk us through that because I think you presented that

351
00:20:31,480 --> 00:20:33,840
at scale.ml, and one of the examples

352
00:20:33,840 --> 00:20:35,560
you use if I'm remembering this correctly

353
00:20:35,560 --> 00:20:41,240
was looking at a computer vision task

354
00:20:41,240 --> 00:20:44,760
and convolutional neural nets and kind of mapping that

355
00:20:44,760 --> 00:20:48,080
to what that might look like in a tensor world.

356
00:20:48,080 --> 00:20:49,800
Am I remembering that correctly?

357
00:20:49,800 --> 00:20:51,160
A little bit.

358
00:20:51,160 --> 00:20:55,920
I mean, yes, I think so the basic problem there

359
00:20:55,920 --> 00:20:58,080
is we have these convolutional layers,

360
00:20:58,080 --> 00:21:01,000
and then you feed the output of the convolutional layers

361
00:21:01,000 --> 00:21:03,320
to the fully connected layers.

362
00:21:03,320 --> 00:21:05,280
So the output of the convolutional layers

363
00:21:05,280 --> 00:21:07,600
is actually a three-dimensional structure

364
00:21:07,600 --> 00:21:09,680
because there is the spatial dimensions

365
00:21:09,680 --> 00:21:11,840
and then there's the number of channels.

366
00:21:11,840 --> 00:21:14,560
But then when we are feeding them to the fully connected layers,

367
00:21:14,560 --> 00:21:16,680
we are flattening them and just turning it

368
00:21:16,680 --> 00:21:18,640
to a matrix multiplication.

369
00:21:18,640 --> 00:21:22,480
So we are all losing all the three-dimensional information.

370
00:21:22,480 --> 00:21:24,560
And the question we asked was a very simple one.

371
00:21:24,560 --> 00:21:27,720
What if we try to preserve this three-dimensional information

372
00:21:27,720 --> 00:21:29,440
all the way to the end?

373
00:21:29,440 --> 00:21:32,600
And the way to do it is instead of doing matrix multiplication,

374
00:21:32,600 --> 00:21:35,600
turn that into a three-dimensional tensor contraction.

375
00:21:35,600 --> 00:21:38,040
So multiply with weights on each of the dimensions

376
00:21:38,040 --> 00:21:40,080
of the three-dimensional tensor.

377
00:21:40,080 --> 00:21:42,840
And in the last layer, do a tensor regression

378
00:21:42,840 --> 00:21:46,960
and use a low-rank tensor weights as a way

379
00:21:46,960 --> 00:21:49,760
to reduce the number of parameters.

380
00:21:49,760 --> 00:21:50,800
And that's what we found.

381
00:21:50,800 --> 00:21:53,000
We found a huge amount of space savings

382
00:21:53,000 --> 00:21:56,040
compared to the standard neural networks

383
00:21:56,040 --> 00:21:59,200
up to 65% in these fully connected layers.

384
00:21:59,200 --> 00:22:00,640
Space savings in what sense?

385
00:22:00,640 --> 00:22:03,000
In terms of the number of parameters.

386
00:22:03,000 --> 00:22:05,720
So the idea is by exploiting all the dimensionality

387
00:22:05,720 --> 00:22:09,200
information, we can have more compact networks.

388
00:22:09,200 --> 00:22:10,240
OK.

389
00:22:10,240 --> 00:22:13,480
How else are you looking at applying this work?

390
00:22:13,480 --> 00:22:14,200
Yeah.

391
00:22:14,200 --> 00:22:15,920
And so that was one example.

392
00:22:15,920 --> 00:22:19,480
The other set of examples is, as I said,

393
00:22:19,480 --> 00:22:21,840
to investigate the ability of tensors

394
00:22:21,840 --> 00:22:26,960
to capture higher-order correlations better.

395
00:22:26,960 --> 00:22:29,280
And so we looked at time series.

396
00:22:29,280 --> 00:22:31,680
And so here, this can be very challenging

397
00:22:31,680 --> 00:22:35,080
because there can be all kinds of higher-order correlations

398
00:22:35,080 --> 00:22:38,760
that affect forecasting outcomes.

399
00:22:38,760 --> 00:22:40,280
And especially if you want to forecast

400
00:22:40,280 --> 00:22:43,920
way into the future, there can be much more important.

401
00:22:43,920 --> 00:22:48,080
And so what we did was we took RNNs and LSTMs

402
00:22:48,080 --> 00:22:49,800
and used a window of measurements

403
00:22:49,800 --> 00:22:52,560
and looked at tensorizations of them.

404
00:22:52,560 --> 00:22:55,600
And of course, if you naively tensorize that blows up

405
00:22:55,600 --> 00:22:58,600
the number of parameters, because that's a high-order

406
00:22:58,600 --> 00:22:59,480
tensor.

407
00:22:59,480 --> 00:23:00,960
But then you can do efficient tensor

408
00:23:00,960 --> 00:23:04,960
train approximations, a low-rank approximation.

409
00:23:04,960 --> 00:23:07,600
So what that yields is a succinct representation

410
00:23:07,600 --> 00:23:09,680
of higher-order correlations.

411
00:23:09,680 --> 00:23:12,280
And we found a lot of benefit compared

412
00:23:12,280 --> 00:23:15,880
to baseline LSTMs on a range of data sets,

413
00:23:15,880 --> 00:23:20,320
like forecasting, traffic, hour-saheads, forecasting,

414
00:23:20,320 --> 00:23:24,240
weather, hundreds of days into the future.

415
00:23:24,240 --> 00:23:29,520
So there is a much better performance in those settings.

416
00:23:29,520 --> 00:23:32,040
So I can foresee a lot of such cases

417
00:23:32,040 --> 00:23:34,440
where we have a diverse set of measurements.

418
00:23:34,440 --> 00:23:36,960
So in tensors become a natural framework

419
00:23:36,960 --> 00:23:38,400
to encode the input.

420
00:23:38,400 --> 00:23:40,600
But then even during their processing

421
00:23:40,600 --> 00:23:43,320
through all the layers, you kind of treat them

422
00:23:43,320 --> 00:23:45,040
as tensor operations.

423
00:23:45,040 --> 00:23:47,160
So you can exploit the information

424
00:23:47,160 --> 00:23:50,440
in different dimensions more effectively.

425
00:23:50,440 --> 00:23:53,720
Do you see this tensorization as it's applied

426
00:23:53,720 --> 00:23:54,760
to neural networks?

427
00:23:54,760 --> 00:23:57,440
Is this something that is automatable

428
00:23:57,440 --> 00:24:01,120
or is it akin to network architecture

429
00:24:01,120 --> 00:24:03,360
in its level of complexity?

430
00:24:03,360 --> 00:24:05,760
Did you do this via graduate student descent,

431
00:24:05,760 --> 00:24:08,960
or did you want it through some process?

432
00:24:08,960 --> 00:24:12,000
I mean, so I would say as a great starting point,

433
00:24:12,000 --> 00:24:15,480
would be to have a software framework that makes it easy

434
00:24:15,480 --> 00:24:17,520
to define these layers.

435
00:24:17,520 --> 00:24:20,840
So that's where John Kosofi, he

436
00:24:20,840 --> 00:24:24,880
interned with our group last year, and he's been working

437
00:24:24,880 --> 00:24:28,240
with me and also his advisors back in London

438
00:24:28,240 --> 00:24:31,200
on developing tensorly framework.

439
00:24:31,200 --> 00:24:34,560
So the tensorly software, you can think of it as Keras.

440
00:24:34,560 --> 00:24:36,640
It has a front end that makes it very easy

441
00:24:36,640 --> 00:24:39,760
to define these tensor operations.

442
00:24:39,760 --> 00:24:42,760
But you can now connect it to multiple different backends.

443
00:24:42,760 --> 00:24:47,040
TensorFlow, PyTor, TirmixNet, and the baseline NAMPI

444
00:24:47,040 --> 00:24:47,560
as well.

445
00:24:47,560 --> 00:24:49,360
So if you don't want to bother about deep learning,

446
00:24:49,360 --> 00:24:51,440
you can even just do basic NAMPI

447
00:24:51,440 --> 00:24:54,600
and do the operations on CPU.

448
00:24:54,600 --> 00:24:58,480
But the benefit with this is now different researchers

449
00:24:58,480 --> 00:25:02,600
and developers can go try out different architectures,

450
00:25:02,600 --> 00:25:06,320
see what works well in their domain for their data sets.

451
00:25:06,320 --> 00:25:08,760
And so I would say that's a good starting point.

452
00:25:08,760 --> 00:25:10,320
But indeed, the architecture design

453
00:25:10,320 --> 00:25:11,960
would be more complex because we

454
00:25:11,960 --> 00:25:15,400
have a broader set of operations.

455
00:25:15,400 --> 00:25:17,240
But on the other hand, I do think

456
00:25:17,240 --> 00:25:19,600
there's some nice research to be done

457
00:25:19,600 --> 00:25:21,800
in terms of how we can use group theories,

458
00:25:21,800 --> 00:25:24,920
how we can use our understanding of tensor algebra

459
00:25:24,920 --> 00:25:26,960
to do better architecture search.

460
00:25:26,960 --> 00:25:29,200
elaborate on that, what are you thinking there?

461
00:25:29,200 --> 00:25:31,560
I mean, this kind of, you know, has also

462
00:25:31,560 --> 00:25:36,000
been seen in quantum systems and other areas of physics

463
00:25:36,000 --> 00:25:41,040
on asking different forms of tensor representations,

464
00:25:41,040 --> 00:25:44,440
what kind of correlation structures to the end use,

465
00:25:44,440 --> 00:25:48,360
and what would make sense for a particular domain?

466
00:25:48,360 --> 00:25:52,320
And we're just basically touching the surface of this,

467
00:25:52,320 --> 00:25:55,760
so it's not still an algorithm that we can automate.

468
00:25:55,760 --> 00:25:57,520
But I think those kinds of understanding

469
00:25:57,520 --> 00:26:00,760
could help us design better architecture search.

470
00:26:00,760 --> 00:26:03,280
So again, I would say like, you know,

471
00:26:03,280 --> 00:26:05,760
we're talking about architecture search, right?

472
00:26:05,760 --> 00:26:07,720
And that is indeed a hard problem.

473
00:26:07,720 --> 00:26:10,680
I mean, right now, even simple, greedy search

474
00:26:10,680 --> 00:26:13,800
kind of approaches take up a lot of resources.

475
00:26:13,800 --> 00:26:16,400
So if we want to solve it at scale very efficiently,

476
00:26:16,400 --> 00:26:17,440
that's out there.

477
00:26:17,440 --> 00:26:18,280
OK.

478
00:26:18,280 --> 00:26:20,560
But on the other hand, there are some immediate things

479
00:26:20,560 --> 00:26:23,760
we can do to improve the current state of art.

480
00:26:23,760 --> 00:26:25,800
What are the things that you're working on?

481
00:26:25,800 --> 00:26:29,080
Yeah, it's been really a broad set of problems.

482
00:26:29,080 --> 00:26:33,000
I recently, I think I shared it online on slide share

483
00:26:33,000 --> 00:26:35,920
that talks about how to, you know,

484
00:26:35,920 --> 00:26:39,800
bring our thinking of all aspects of AI together, right?

485
00:26:39,800 --> 00:26:42,880
So I see three facets.

486
00:26:42,880 --> 00:26:45,240
There is data, there are algorithms,

487
00:26:45,240 --> 00:26:47,240
and then there's the infrastructure.

488
00:26:47,240 --> 00:26:50,880
And until now, we've been kind of thinking of each separately,

489
00:26:50,880 --> 00:26:52,840
but there's a need to bring them together

490
00:26:52,840 --> 00:26:57,440
to have the best efficiency, the best impact.

491
00:26:57,440 --> 00:26:59,560
To give an example, you know, typically,

492
00:26:59,560 --> 00:27:03,240
we first collect data, feed them into machine learning algorithms.

493
00:27:03,240 --> 00:27:05,080
But in most practical applications,

494
00:27:05,080 --> 00:27:07,440
data is such a crucial aspect.

495
00:27:07,440 --> 00:27:09,880
And what we found in a series of studies

496
00:27:09,880 --> 00:27:13,200
is you can drastically improve data collection, aggregation,

497
00:27:13,200 --> 00:27:16,200
and augment our current data better.

498
00:27:16,200 --> 00:27:21,520
To give an example, we looked at the name-density recognition

499
00:27:21,520 --> 00:27:26,280
task, and the on-to-notes is the biggest public benchmark there.

500
00:27:26,280 --> 00:27:27,600
The one is what's it called?

501
00:27:27,600 --> 00:27:28,480
On-to-notes.

502
00:27:28,480 --> 00:27:28,960
OK.

503
00:27:28,960 --> 00:27:29,640
On-to-notes.

504
00:27:29,640 --> 00:27:30,480
Yeah.

505
00:27:30,480 --> 00:27:36,520
And so then we ran a simple active learning heuristic

506
00:27:36,520 --> 00:27:40,440
which says, you know, feed the current model

507
00:27:40,440 --> 00:27:42,360
a set of labeled examples.

508
00:27:42,360 --> 00:27:45,880
And after it updates, look for the examples

509
00:27:45,880 --> 00:27:48,680
that have a high uncertainty in the current model.

510
00:27:48,680 --> 00:27:51,320
So this is the basic active learning framework.

511
00:27:51,320 --> 00:27:54,160
And we found that we could reach the state of the art

512
00:27:54,160 --> 00:27:57,960
with just 25% of the dataset.

513
00:27:57,960 --> 00:27:59,680
So there are two aspects to it.

514
00:27:59,680 --> 00:28:02,840
One is that active learning can drastically reduce

515
00:28:02,840 --> 00:28:04,440
our data requirements.

516
00:28:04,440 --> 00:28:07,240
And the other is, most of the time, we are collecting data

517
00:28:07,240 --> 00:28:08,560
that's really relevant.

518
00:28:08,560 --> 00:28:10,520
We don't need huge datasets.

519
00:28:10,520 --> 00:28:14,280
Many times, you can even do deep learning on small data.

520
00:28:14,280 --> 00:28:17,200
And that, to me, presents a lot of promise

521
00:28:17,200 --> 00:28:18,920
because in so many domains, we don't

522
00:28:18,920 --> 00:28:21,680
have the luxury of large datasets.

523
00:28:21,680 --> 00:28:26,040
So how do you map this to this idea of data algorithms

524
00:28:26,040 --> 00:28:28,720
and infrastructure?

525
00:28:28,720 --> 00:28:31,240
Because in this case, what we are doing

526
00:28:31,240 --> 00:28:33,240
is we're planning our data collection

527
00:28:33,240 --> 00:28:36,200
in the loop with our model training.

528
00:28:36,200 --> 00:28:40,320
So we can ask the same in terms of even aggregating data.

529
00:28:40,320 --> 00:28:43,080
You have a whole crowd of workers.

530
00:28:43,080 --> 00:28:47,920
How do you reconcile differences in their answers?

531
00:28:47,920 --> 00:28:50,640
The standard approaches, we do majority voting.

532
00:28:50,640 --> 00:28:52,400
We have a whole bunch of annotators.

533
00:28:52,400 --> 00:28:56,080
We try to clean up the input data very carefully.

534
00:28:56,080 --> 00:28:58,920
Then we feed it to the machine learning model.

535
00:28:58,920 --> 00:29:00,880
But what we show is you can, in fact,

536
00:29:00,880 --> 00:29:02,680
do them both in the loop.

537
00:29:02,680 --> 00:29:06,080
You can try to keep learning about the annotator quality

538
00:29:06,080 --> 00:29:09,160
as you go along to train the machine learning model.

539
00:29:09,160 --> 00:29:11,360
And the machine learning model can help you

540
00:29:11,360 --> 00:29:14,480
come up with better estimates of annotator quality.

541
00:29:14,480 --> 00:29:18,840
And with this, in fact, with the Microsoft Coco data

542
00:29:18,840 --> 00:29:21,520
sets, we took the raw annotations that were available

543
00:29:21,520 --> 00:29:22,880
for this data set.

544
00:29:22,880 --> 00:29:25,400
And we found that the optimal thing to do

545
00:29:25,400 --> 00:29:28,280
was to have just a single annotator per sample.

546
00:29:28,280 --> 00:29:30,880
So if you had a fixed budget of annotations,

547
00:29:30,880 --> 00:29:33,520
you're better off labeling more samples

548
00:29:33,520 --> 00:29:36,920
than to be very careful about labeling every sample

549
00:29:36,920 --> 00:29:39,240
and having a whole lot of annotators

550
00:29:39,240 --> 00:29:41,440
to reduce the noise on a single sample.

551
00:29:41,440 --> 00:29:42,280
Oh, interesting.

552
00:29:42,280 --> 00:29:43,880
Interesting.

553
00:29:43,880 --> 00:29:47,280
Of the three, you haven't mentioned infrastructure.

554
00:29:47,280 --> 00:29:48,880
How does that tie into the loop?

555
00:29:48,880 --> 00:29:50,560
Or how do we tie that more closely into the loop?

556
00:29:50,560 --> 00:29:51,560
Right.

557
00:29:51,560 --> 00:29:54,320
So this ties in with the work I've been doing

558
00:29:54,320 --> 00:29:56,280
at Amazon Web Services.

559
00:29:56,280 --> 00:30:00,160
So naturally, I think the answer will be the cloud

560
00:30:00,160 --> 00:30:04,720
because they're not just saying because I'm

561
00:30:04,720 --> 00:30:06,640
in the cloud division.

562
00:30:06,640 --> 00:30:09,240
I mean, as we scale up these models,

563
00:30:09,240 --> 00:30:12,400
as we have data requirements grow,

564
00:30:12,400 --> 00:30:18,560
the only answer will be to allow for elastic scaling

565
00:30:18,560 --> 00:30:24,400
and put computing at the hands of every individual.

566
00:30:24,400 --> 00:30:28,200
Anybody can go access large amounts of infrastructure.

567
00:30:28,200 --> 00:30:30,040
So that's the basic aspect, like making

568
00:30:30,040 --> 00:30:33,080
GPUs available to everyone is the starting point.

569
00:30:33,080 --> 00:30:35,920
But where I find a lot of interesting challenges

570
00:30:35,920 --> 00:30:39,160
is when we try to build machine learning services

571
00:30:39,160 --> 00:30:41,080
across all levels of the stack.

572
00:30:41,080 --> 00:30:44,720
So there's an expert developer who likes to maybe tweak

573
00:30:44,720 --> 00:30:47,480
with the backend to enterprises that just want

574
00:30:47,480 --> 00:30:51,920
to run application services and not even worry

575
00:30:51,920 --> 00:30:55,080
about any machine learning that's going on underneath.

576
00:30:55,080 --> 00:30:59,480
How do we satisfy the needs of these diverse customers?

577
00:30:59,480 --> 00:31:02,760
And I've been involved in building SageMaker.

578
00:31:02,760 --> 00:31:04,400
That's the machine learning platform

579
00:31:04,400 --> 00:31:07,160
that removes a lot of heavy lifting associated

580
00:31:07,160 --> 00:31:10,800
with DevOps work when it comes to productionizing AI.

581
00:31:10,800 --> 00:31:13,520
So how do we go from prototyping

582
00:31:13,520 --> 00:31:16,600
to a full-scale production machine learning service

583
00:31:16,600 --> 00:31:18,240
very easily?

584
00:31:18,240 --> 00:31:22,160
And the idea is we have built all these algorithms

585
00:31:22,160 --> 00:31:24,880
where there's a backend that automatically scales

586
00:31:24,880 --> 00:31:28,280
to multiple machines and gives high efficiency.

587
00:31:28,280 --> 00:31:31,760
And there's also the model serving aspect that helps you

588
00:31:31,760 --> 00:31:35,800
to manage different models, do A, B testing very easily,

589
00:31:35,800 --> 00:31:40,680
and deploy the models in a very scalable way.

590
00:31:40,680 --> 00:31:43,800
So I think all these aspects, so we are just

591
00:31:43,800 --> 00:31:46,360
beginning to think in an integrated way

592
00:31:46,360 --> 00:31:48,280
and there'll be more of it in the future.

593
00:31:48,280 --> 00:31:50,080
Awesome, awesome.

594
00:31:50,080 --> 00:31:52,360
You were just on a panel here at Train AI.

595
00:31:52,360 --> 00:31:54,120
What was the panel about?

596
00:31:54,120 --> 00:31:57,960
Yeah, so Rob Monroe, the CDO figure eight,

597
00:31:57,960 --> 00:32:00,640
you know, formerly crowd flower,

598
00:32:00,640 --> 00:32:03,680
was earlier in our group in Amazon AI.

599
00:32:03,680 --> 00:32:05,360
So we've interacted a lot.

600
00:32:05,360 --> 00:32:07,680
We've had a lot of discussions about data,

601
00:32:07,680 --> 00:32:09,680
about the state of machine learning,

602
00:32:09,680 --> 00:32:13,160
about his adventures around the world.

603
00:32:13,160 --> 00:32:16,760
He's, I don't know if you know, he's cycle large parts

604
00:32:16,760 --> 00:32:19,880
of India, Africa, like, you know, there is, you know,

605
00:32:19,880 --> 00:32:21,160
so Rob is great.

606
00:32:21,160 --> 00:32:25,120
And so it's very happy to be on a panel when he invited me

607
00:32:25,120 --> 00:32:30,960
that asks about taking somebody from PhD to products, right?

608
00:32:30,960 --> 00:32:34,720
So academia versus industry, and for me,

609
00:32:34,720 --> 00:32:37,920
being in both the world, I felt like, you know,

610
00:32:37,920 --> 00:32:39,960
this was a great fit.

611
00:32:39,960 --> 00:32:42,280
How would you summarize the panel,

612
00:32:42,280 --> 00:32:45,880
like, what were the main insights offered?

613
00:32:45,880 --> 00:32:49,320
Yeah, I think the main one is, like, you know,

614
00:32:49,320 --> 00:32:52,080
the question of like, why PhD, right?

615
00:32:52,080 --> 00:32:53,920
Like, it looks like, oh, there's so much happening

616
00:32:53,920 --> 00:32:55,840
in industry, why PhD?

617
00:32:55,840 --> 00:32:58,280
I think there is a lot of relevance for PhD,

618
00:32:58,280 --> 00:33:01,400
even today, in fact, more than before,

619
00:33:01,400 --> 00:33:05,400
because first of all, there's a whole set of unsolved problems

620
00:33:05,400 --> 00:33:08,480
and industry is looking at only a sliver of them.

621
00:33:08,480 --> 00:33:12,400
And in industry, sometimes things are so fast moving,

622
00:33:12,400 --> 00:33:15,000
you have to kind of get ship products

623
00:33:15,000 --> 00:33:18,480
and you don't have that opportunity for a deeper thinking.

624
00:33:18,480 --> 00:33:21,320
Right? So if you're kind of in a position to say,

625
00:33:21,320 --> 00:33:22,720
I want this adventure.

626
00:33:22,720 --> 00:33:24,480
I want to, like, explore on my own.

627
00:33:24,480 --> 00:33:26,480
I want to ask my own questions, right?

628
00:33:26,480 --> 00:33:28,320
Then PhD is the right fit.

629
00:33:28,320 --> 00:33:30,040
But it's not for everybody.

630
00:33:30,040 --> 00:33:32,840
So it's like, I would never advise somebody to do a PhD

631
00:33:32,840 --> 00:33:35,000
just to get a machine learning job, right?

632
00:33:35,000 --> 00:33:39,040
So that's like the novelism, yeah, exactly.

633
00:33:39,040 --> 00:33:41,000
And so, yeah, they were useful

634
00:33:41,000 --> 00:33:43,080
and very interesting discussions like that.

635
00:33:43,080 --> 00:33:44,440
Fantastic.

636
00:33:44,440 --> 00:33:48,080
Well, Nima, before we wind down any other thoughts

637
00:33:48,080 --> 00:33:49,840
that you'd like to share?

638
00:33:49,840 --> 00:33:53,240
No, I think I really appreciate all the great work

639
00:33:53,240 --> 00:33:54,080
that you're doing.

640
00:33:54,080 --> 00:33:58,200
I think it's very important to publicize all the efforts

641
00:33:58,200 --> 00:34:01,040
that are happening in AI and machine learning.

642
00:34:01,040 --> 00:34:04,040
And more importantly, like, kind of give a realistic view

643
00:34:04,040 --> 00:34:05,400
of the field, right?

644
00:34:05,400 --> 00:34:08,840
So I do think there is a lot of hype in the media on one hand

645
00:34:08,840 --> 00:34:11,560
and at the same time, a lot of fear mongering

646
00:34:11,560 --> 00:34:15,040
and sometimes by big and powerful names that I'm sure of.

647
00:34:15,040 --> 00:34:16,040
So we may name us.

648
00:34:16,040 --> 00:34:18,320
Yeah, but it's very easy to infer.

649
00:34:18,320 --> 00:34:20,240
And I completely disagree.

650
00:34:20,240 --> 00:34:23,040
I think it's a, I would call it an abuse of power

651
00:34:23,040 --> 00:34:26,960
to kind of use their positions to do this kind of a fear

652
00:34:26,960 --> 00:34:31,360
mongering because there's a lot of real world impact

653
00:34:31,360 --> 00:34:35,120
to be had, especially a lot of social impact to be had

654
00:34:35,120 --> 00:34:38,040
through the use of AI.

655
00:34:38,040 --> 00:34:40,920
And we are still very much at the beginning.

656
00:34:40,920 --> 00:34:43,040
There is so much development to be done.

657
00:34:43,040 --> 00:34:46,680
And investing in AI, investing in AI education

658
00:34:46,680 --> 00:34:51,840
should be the priorities of governments and organizations.

659
00:34:51,840 --> 00:34:55,160
So I'm so happy that, you know, you have this podcast

660
00:34:55,160 --> 00:34:59,680
and you're making efforts to provide a balanced view

661
00:34:59,680 --> 00:35:00,600
of the topic.

662
00:35:00,600 --> 00:35:00,880
Great.

663
00:35:00,880 --> 00:35:02,400
Well, thank you so much.

664
00:35:02,400 --> 00:35:03,800
It's great to chat with you.

665
00:35:03,800 --> 00:35:04,800
Thank you.

666
00:35:07,040 --> 00:35:08,200
All right, everyone.

667
00:35:08,200 --> 00:35:10,320
That's our show for today.

668
00:35:10,320 --> 00:35:13,320
For more information on Anima or any of the topics

669
00:35:13,320 --> 00:35:17,560
covered in this episode, head on over to twimlai.com slash

670
00:35:17,560 --> 00:35:20,640
talk slash 142.

671
00:35:20,640 --> 00:35:22,760
Thanks again to figure eight for their sponsorship

672
00:35:22,760 --> 00:35:23,840
of this show.

673
00:35:23,840 --> 00:35:26,400
To follow along with the train AI series,

674
00:35:26,400 --> 00:35:31,680
visit twimlai.com slash train AI 2018.

675
00:35:31,680 --> 00:35:35,720
Finally, show us some love for the podcast second anniversary

676
00:35:35,720 --> 00:35:37,880
and share how it's been helpful to you.

677
00:35:37,880 --> 00:35:42,640
You can do that over at twimlai.com slash 2AV.

678
00:35:42,640 --> 00:35:46,000
Thanks so much for listening and catch you next time.

