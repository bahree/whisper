WEBVTT

00:00.000 --> 00:04.880
The conversation you're about to hear was recorded live at Twomokon AI

00:04.880 --> 00:11.680
platforms. For more coverage of Twomokon, visit Twomokon.com-news or follow us

00:11.680 --> 00:20.040
on Twitter at Twomokon AI. But first a word from our sponsor. Big thanks to our

00:20.040 --> 00:26.880
friends at IBM for being a founding sponsor of Twomokon AI platforms. IBM

00:26.880 --> 00:31.080
Watson is the company's comprehensive suite of AI tools for the enterprise

00:31.080 --> 00:37.400
which includes Watson Studio, Watson Machine Learning, and Watson Open Scale. The

00:37.400 --> 00:41.960
IBM Watson Suite allows enterprises to build, deploy, and manage AI models in

00:41.960 --> 00:47.560
any environment including on-premises and in private and public clouds. We

00:47.560 --> 00:51.360
encourage you to check out the IBM Data Science and AI community by visiting

00:51.360 --> 00:57.040
Twomokon AI.com slash IBM. And if you join, you'll get a complimentary month of

00:57.040 --> 01:01.560
select IBM programs on Coursera.

01:03.600 --> 01:11.880
All right, so super excited to invite up our next guests. Fran Bell, Fran, runs a

01:11.880 --> 01:19.440
Data Science platform's team at Uber. She's got over 100 data scientists

01:19.440 --> 01:25.280
working on building tools that are a platform that's at a higher level of

01:25.280 --> 01:35.560
abstraction than Uber's already famous Michelangelo platform. Fran. Welcome to

01:35.560 --> 01:40.440
Twomokon. So I kind of paraphrased your role a little bit. Why don't you tell

01:40.440 --> 01:44.160
us a little bit about your team and your charter? Yeah, absolutely. Thank you so

01:44.160 --> 01:48.200
much for having me. It's really a pleasure to be here at the inaugural Twomokon.

01:48.200 --> 01:54.480
About the charter of the team, the vision really is to provide cutting-edge

01:54.480 --> 01:59.000
data science at the push of a button to anyone within the company. So that

01:59.000 --> 02:04.120
basically means that we're aiming to transform anyone within Uber into a

02:04.120 --> 02:08.680
data scientist. An example of this is forecasting. So forecasting obviously

02:08.680 --> 02:13.680
underpins a large number of use cases within Uber. And so the vision here is to

02:13.680 --> 02:18.800
provide the latest and greatest cutting-edge forecasts to folks at a push of a

02:18.800 --> 02:24.600
button via UI, for example, as integrated into a BI stack or programmatically

02:24.600 --> 02:30.160
accessible through our API. And so the only thing that our end users within Uber

02:30.160 --> 02:34.960
need to provide is historic data, whether it's for example in the form of a

02:34.960 --> 02:40.480
CSV file or query, and the forecast horizon. So how far you want to forecast

02:40.480 --> 02:45.320
out. And we do everything else automatically in the background. We scan over

02:45.320 --> 02:50.280
whole suite of forecasting algorithms, either those that we have integrated

02:50.280 --> 02:56.160
off the shelf into the platform, or also those that we have developed in-house

02:56.160 --> 03:01.760
proprietary. And we've gone way beyond forecasting in terms of data science

03:01.760 --> 03:05.920
areas, other areas that we're investing heavily in in platformization or

03:05.920 --> 03:11.320
anomaly detection, experimentation, more recently also conversational AI

03:11.320 --> 03:15.840
and natural language. And then personally I'm very excited about our proof of

03:15.840 --> 03:20.680
concepts of also platformizing and semi-automating intelligent insights

03:20.680 --> 03:26.640
generation and data exploration. So of those conversational AI seems like the

03:26.640 --> 03:31.640
I'd man out so to speak. How did you end up working on that one? Yeah so we see a

03:31.640 --> 03:37.160
lot of really great benefits. We have a lot of conversational AI data at Uber.

03:37.160 --> 03:41.640
One example is our customer obsession ticket assistant, which was one of our

03:41.640 --> 03:48.920
first use cases in this space. So here for example we wanted to aid customer

03:48.920 --> 03:53.800
service representatives in solving customer support tickets that are coming

03:53.800 --> 03:58.240
in. And as you can imagine given the size of our platform we get quite a few

03:58.240 --> 04:04.080
of those. And so using natural language and deep learning approaches we were

04:04.080 --> 04:09.200
able to build recommendations for our customer service representatives of what

04:09.200 --> 04:14.160
is the topic that folks are writing in about. What are potential actions that

04:14.160 --> 04:18.800
the customer service representative might want to take. And then also providing

04:18.800 --> 04:24.040
guard lines or guard rails basically around what could be the best starting

04:24.040 --> 04:29.800
point to actually address the response to the end consumer. Of course the

04:29.800 --> 04:34.480
customer service representative always has the final call and say on this. But we

04:34.480 --> 04:39.800
saw really great improvements in our customer care experience as a result of

04:39.800 --> 04:44.080
having this AI basically assisting our customer service representatives.

04:44.080 --> 04:49.600
Okay cool. So you've got this portfolio of platforms essentially that you're

04:49.600 --> 04:55.560
building to support these different use cases. How do you know when it's time

04:55.560 --> 05:00.720
to platform something? Yeah that's a really great question. And we're looking at

05:00.720 --> 05:04.440
multiple different dimensions here and very deliberately see which of the

05:04.440 --> 05:07.360
data science areas we want to platformize. It's obviously a heavily

05:07.360 --> 05:13.680
investment. And so we look at three items. The first one is can the

05:13.680 --> 05:17.600
platformization of this area really create step function improvements to our

05:17.600 --> 05:21.280
user experience and the business. And so sticking with the example of

05:21.280 --> 05:27.480
forecasting if we can forecast tightly accurately demand in a particular space

05:27.480 --> 05:32.080
and time we can create more magical user experiences. The second thing is

05:32.080 --> 05:37.560
really the wealth of use cases that exist across the company. Of course building

05:37.560 --> 05:42.080
a platform you want to be able to tackle many different use cases. And so with

05:42.080 --> 05:46.400
forecasting for example it does spend the entire enterprise ranging from

05:46.400 --> 05:50.880
marketing to obviously marketplace supply and demand financial aspects

05:50.880 --> 05:55.280
operations as well as our hardware. We still have a lot of hardware on premise

05:55.280 --> 05:59.080
and so accurately forecasting the hardware needs especially on high

05:59.080 --> 06:03.640
demand days and special days such as Halloween or New Year's Eve is really

06:03.640 --> 06:07.840
important. So that's the second aspect. And then thirdly. All we need is a

06:07.840 --> 06:15.960
special day at Uber. Yes that's it is. See lots of demand on that front. And then

06:15.960 --> 06:20.560
the third dimension really is the reusability of the models and

06:20.560 --> 06:25.640
methodologies that we apply. And again with forecasting you know a common

06:25.640 --> 06:29.560
framework that is needed to build forecasting algorithms is a backtesting

06:29.560 --> 06:34.040
framework. So understanding the accuracy of your forecasts and that really is

06:34.040 --> 06:40.560
needed for any step along the forecasting journey. And so having a common

06:40.560 --> 06:45.560
central paralyzed language extensible backtesting framework is something

06:45.560 --> 06:49.800
that's really important. So as your team out evangelizing the

06:49.800 --> 06:55.160
opportunity to platformize and looking for customers that are already working

06:55.160 --> 07:00.680
on things that meet these criteria or our folks coming to you saying hey we've

07:00.680 --> 07:04.760
got these problems help us all them. How does the relationship with your

07:04.760 --> 07:10.040
ultimate customer evolve? Yeah absolutely. So we have a lot of the product teams

07:10.040 --> 07:14.840
coming to us with use cases at the same time because we are this horizontal

07:14.840 --> 07:19.040
team that spans across the entire company across all lines of business we have

07:19.040 --> 07:23.720
a very unique vantage point as well. And so we can also gently nudge some of

07:23.720 --> 07:29.000
the product teams to come and join us in this journey as well. Okay and so when

07:29.000 --> 07:34.640
you identify a problem space that it makes sense to platformize how do you

07:34.640 --> 07:38.200
approach that you just jump in and start building, start coding or what is

07:38.200 --> 07:43.360
the methodology look like? Yeah that's a great question. So the way we build

07:43.360 --> 07:48.040
platforms is in a use case driven manner. So that basically means that with

07:48.040 --> 07:52.440
every use case that is strategically chosen we augment the platform and we

07:52.440 --> 07:57.200
reuse as much capabilities as possible from the platform. And that really

07:57.200 --> 08:02.040
allows us to have wins very early on. And learning from this we actually now

08:02.040 --> 08:06.240
have a three-faced approach to platformization. So step one is really

08:06.240 --> 08:11.120
consulting. So we have these deep domain experts in particular areas of data

08:11.120 --> 08:15.560
science on the team. And so we embed them with particular areas of the

08:15.560 --> 08:21.920
business where we see opportunities of having use cases in these areas. And so

08:21.920 --> 08:26.040
that has a couple of advantages. Firstly the domain experts learn more about the

08:26.040 --> 08:30.120
business, about the opportunities, the pain points, and really can bring back

08:30.120 --> 08:35.800
these learnings to then drive the best design for these platforms. It also

08:35.800 --> 08:40.920
allows us to tackle these use cases early on and really show wins and gain the

08:40.920 --> 08:45.880
trust of our partners and leadership on that front. That of course is not a

08:45.880 --> 08:50.400
scalable approach. This is why we set out to do platforms in the first place. But

08:50.400 --> 08:54.120
it's a very good starting point. And so the second thing that we usually do is

08:54.120 --> 08:58.960
templatization. So what I mean by this is we build recipes, whether it's form of

08:58.960 --> 09:04.480
documentation, example ipython notebooks, providing talks and educational

09:04.480 --> 09:08.920
aspects. And this really allows us now to have a one-to-many multiplicative

09:08.920 --> 09:14.760
effect throughout the organization, mostly to other data scientists that are

09:14.760 --> 09:20.160
dedicated to these business areas. And then over time as we're taking on more and

09:20.160 --> 09:24.160
more of these use cases, we really expand our platform to become more and

09:24.160 --> 09:28.960
more self-service and work towards that vision of really providing it at the

09:28.960 --> 09:33.200
push of a button without domain expertise required. Of course, including

09:33.200 --> 09:38.040
best practices and guardrails in the process. So in introducing you I mentioned

09:38.040 --> 09:43.560
Michelangelo, Uber's low-level machine learning infrastructure platform. Uber

09:43.560 --> 09:47.160
was one of the first companies to publish about what they were doing to

09:47.160 --> 09:53.520
automate machine learning. If I interpret your LinkedIn profile correctly, you

09:53.520 --> 09:58.400
were at Uber doing applied machine learning platforms before at least before

09:58.400 --> 10:03.840
that article hit possibly before the Michelangelo effort even started. What's

10:03.840 --> 10:09.040
the relationship between these two teams? Yeah, we have a fantastic working

10:09.040 --> 10:13.120
relationship with Michelangelo as well as the AI organization engineering branch

10:13.120 --> 10:18.120
that we also work with very closely to platformize. And we have three modes

10:18.120 --> 10:23.360
of interaction here. The first one is as the head of platform data science, I get

10:23.360 --> 10:28.000
pulled in into the strategic and vision setting when it comes to Michelangelo

10:28.000 --> 10:31.640
working closely with their engineering and product lead. So right from the

10:31.640 --> 10:35.880
start, there's a really great collaborative relationship that we can build on.

10:35.880 --> 10:41.360
And then we have two other modes that have evolved over time. The first one is

10:41.360 --> 10:47.440
Michelangelo was more decent. We deeply embedded folks from our teams into the

10:47.440 --> 10:51.840
Michelangelo group. So for example, with the customer obsession ticket assistant

10:51.840 --> 10:55.840
example that I mentioned earlier, this was actually the first deep learning

10:55.840 --> 11:00.240
algorithm that ran on Michelangelo. And so as you can imagine, a lot of the

11:00.240 --> 11:04.840
features required for doing deep learning were in a very nascent state at the

11:04.840 --> 11:08.280
time. And so having data scientists who were working on this particular

11:08.280 --> 11:12.680
problem deeply embedded in the Michelangelo group and working together with the

11:12.680 --> 11:17.800
engineers and product managers there to build all capabilities not only to solve

11:17.800 --> 11:22.600
the customer obsession ticket assistant case but also more generic aspects that

11:22.600 --> 11:28.320
then really benefited the community more at large to build deep learning, you

11:28.320 --> 11:33.640
know algorithms and frameworks was really important here. Of course, as

11:33.640 --> 11:38.560
Michelangelo has evolved over time and became more mature, we are becoming more

11:38.560 --> 11:43.720
of an end consumer of the platform and it becomes more of a self-service

11:43.720 --> 11:50.200
component especially with the onset of PyML. PyML has really provided step

11:50.200 --> 11:55.560
function improvements to what's PyML. So PyML basically allows us to write

11:55.560 --> 12:01.480
Python code and bring our own models that then basically via Michelangelo get

12:01.480 --> 12:06.760
deployed in a sandbox environment at scale. And so that really reduces the

12:06.760 --> 12:11.320
barrier to entry for data scientists to be less reliant on, you know, the

12:11.320 --> 12:16.360
native approaches that are already integrated in Michelangelo or software

12:16.360 --> 12:20.920
engineers that would help with productionization for example. And so this

12:20.920 --> 12:26.240
approach has become really prominent across Uber and has really opened up new

12:26.240 --> 12:34.000
avenues for self-service on Michelangelo. And so with your team pre-existing some

12:34.000 --> 12:41.640
of that effort, you've got platforms and use cases that you've stood up before

12:41.640 --> 12:46.000
they were mature and ready now that they're more mature and ready. Is it a

12:46.000 --> 12:51.280
dynamic relationship in the sense that, you know, there's a heavy migrated in any

12:51.280 --> 12:58.760
of those legacy models over to Michelangelo or, you know, if it's there and is

12:58.760 --> 13:03.600
working, you're going to leave it alone. Yeah, that's a really great question. So

13:03.600 --> 13:08.720
we have a couple of aspects here. One is platforms that are more recent. So for

13:08.720 --> 13:13.000
example, a conversational AI platform here from the get go we build on top of

13:13.000 --> 13:17.000
the Michelangelo capabilities and are actively utilizing this. But as you

13:17.000 --> 13:20.760
correctly pointed out, you know, some of the platforms were built well beyond

13:20.760 --> 13:25.640
before Michelangelo existed or was very recent. And so we have our own

13:25.640 --> 13:29.760
independent stacks on this front. But here it's really important to see the

13:29.760 --> 13:34.440
opportunities for integration and to see a timeline where some of these

13:34.440 --> 13:40.240
platforms are already or may be merging in the future. And then the third part

13:40.240 --> 13:44.560
is platforms that are currently standalone, but likely will never merge with

13:44.560 --> 13:48.800
Michelangelo. So for example, our experimentation platform, which has very

13:48.800 --> 13:52.960
different types of methodologies and workflows, I wouldn't imagine would be

13:52.960 --> 13:57.520
combined with Michelangelo. Can you elaborate on that? What about the methodologies

13:57.520 --> 14:04.360
and workflows makes them? You're not good fits. Yeah, absolutely. So here we use

14:04.360 --> 14:09.560
more statistical approaches. So hypothesis testing, multi-arm bandits, etc.

14:09.560 --> 14:14.600
versus the traditional machine learning approaches. And so for that reason we

14:14.600 --> 14:18.320
keep them separate. What are some of the key technical challenges that you

14:18.320 --> 14:24.840
face for this portfolio of use cases? Yeah, that's a great question. So each of

14:24.840 --> 14:31.240
the platforms is different. We have different users, different use cases, and so

14:31.240 --> 14:36.120
therefore also different requirements on the technology side. But I can go into

14:36.120 --> 14:40.800
two concrete examples here. The first one is for real-time anomaly detection.

14:40.800 --> 14:45.240
This was actually the first platform that I've built at Uber. And so here the

14:45.240 --> 14:49.680
idea is that we wanted to detect system outages as quickly as possible. So

14:49.680 --> 14:56.200
people not being able to sign in or sign up or perhaps trips being degraded,

14:56.200 --> 15:03.720
etc. And here we basically saw that this was still an open research problem.

15:03.720 --> 15:10.000
And we set out to build a new platform around it and also advance the space.

15:10.000 --> 15:14.880
We have a couple of patents now in that space as well. But here the key kind of

15:14.880 --> 15:19.640
requirement beyond the innovation component was that we needed to have

15:19.640 --> 15:24.320
extremely low latencies. So as you can imagine, because there's a real-time

15:24.320 --> 15:30.640
problem, we had basically those considerations to take care of on and

15:30.640 --> 15:35.800
extremely high QPS because we have hundreds of millions of signals

15:35.800 --> 15:40.840
basically back end as well as aggregate mobile signals that we're tracking in

15:40.840 --> 15:46.200
order to understand whether there's a system outage going on. Another one

15:46.200 --> 15:51.240
example is forecasting. As I mentioned earlier, we integrated our forecasting

15:51.240 --> 15:57.760
algorithms also into our BI stack for easy access through UIs. And so there's

15:57.760 --> 16:01.760
now a really great path where you can query a metric, you can visualize this

16:01.760 --> 16:06.160
metric using DashBuilder, an internal tool that we've built. And then you'll

16:06.160 --> 16:10.560
also have this little button that says, I want to forecast this metric. And so

16:10.560 --> 16:13.880
obviously we want to make sure that we have a good user experience here and that

16:13.880 --> 16:16.360
people don't have to wait, you know, minutes or hours.

16:16.360 --> 16:21.400
You're hearing theme here. Exactly, right. And so having low latency for some of

16:21.400 --> 16:24.760
these forecasting algorithms is really important. And so are some of these

16:24.760 --> 16:28.560
technical challenges ever a reason why you might build something from the

16:28.560 --> 16:33.640
ground up yourself, so to speak, as opposed to rely on what the Michelangelo

16:33.640 --> 16:39.440
team offers or is that not a consideration typically? Yes, it does flow into kind

16:39.440 --> 16:44.480
of our decision making process here in terms of what is already available, how

16:44.480 --> 16:49.240
easily it is extensible. And often the timing kind of component comes in as

16:49.240 --> 16:53.560
we discussed earlier in terms of, you know, where was Michelangelo when we

16:53.560 --> 16:58.560
started to build? And how can we evolve that in the future? Can you talk a little

16:58.560 --> 17:05.920
bit about the technology stack that your platforms tend to rely on? Do you have

17:05.920 --> 17:10.680
your own kind of not Michelangelo, but you know, you've got these higher level

17:10.680 --> 17:13.840
platforms that are very application or use case focused. You have your own

17:13.840 --> 17:18.600
kind of intermediate level of abstraction, or are you building kind of use

17:18.600 --> 17:26.720
cases more independently? Yeah, so when we don't build on top of Michelangelo, which

17:26.720 --> 17:32.160
is quite a few of our platforms, we build microservice architectures, we build

17:32.160 --> 17:38.520
them and go in Java actually for performance reasons. Databases are typically

17:38.520 --> 17:44.480
in my SQL. Then we run our instances typically on prem for efficiency

17:44.480 --> 17:50.400
reasons. And then several of our use cases are batch and offline, especially on

17:50.400 --> 17:55.120
the training side. But for those that we discussed earlier, where latency is

17:55.120 --> 18:00.200
something that we want to focus on, we use caching for optimization. And do

18:00.200 --> 18:04.480
you rely heavily on open source in this area or publish open source in this

18:04.480 --> 18:09.760
area? Yeah, both. So we're definitely building on the shoulder of giants and

18:09.760 --> 18:14.040
using open source wherever possible. I think we wouldn't have evolved as

18:14.040 --> 18:21.040
quickly if it wasn't for open source. And so utilizing the methodologies that

18:21.040 --> 18:25.360
have been developed by the communities is very essential. But we also are

18:25.360 --> 18:29.400
heavily invested in open sourcing ourselves as well. And we have quite a few

18:29.400 --> 18:34.680
open source projects. If we look at the data science domain, there are quite a

18:34.680 --> 18:40.200
few examples here as well. We have a pyro that was developed by the AI

18:40.200 --> 18:44.440
organization, which is a probabilistic programming language. We have horror

18:44.440 --> 18:50.000
vod that is distributed deep learning framework on TensorFlow that has gained a

18:50.000 --> 18:55.520
lot of popularity in the community. There is Ludwig that was also built by the

18:55.520 --> 19:00.480
AI organization, which allows for a deep learning framework where you actually

19:00.480 --> 19:06.480
don't have to write code anymore to deploy and train models. And then more

19:06.480 --> 19:11.440
recently, our org has worked together with the AI organization to develop

19:11.440 --> 19:18.280
Plato. This is a very flexible and use case rich platform that allows for

19:18.280 --> 19:24.480
conversational AI, especially in the research and prototyping phase. And then

19:24.480 --> 19:30.360
also our causal ML package is a Python package that we recently launched in

19:30.360 --> 19:36.960
collaboration with our marketing team that basically covers uplift modeling

19:36.960 --> 19:42.880
use cases as well as causal inference in combination with machine learning.

19:42.880 --> 19:47.280
So yeah, quite a lot of efforts in that direction. And we're considering to do

19:47.280 --> 19:52.960
more. With so much out and available in open source that you're incorporating

19:52.960 --> 19:58.440
into these systems that you're building, reminds me a little bit of podcasts

19:58.440 --> 20:03.480
we published not too long ago. Quote that the guest mentioned that became the

20:03.480 --> 20:08.560
title of the podcast was machine learning or AI was is a systems engineering

20:08.560 --> 20:13.360
problem. I wonder how much of what you're doing is systems engineering

20:13.360 --> 20:18.800
connecting pieces that you know in many cases exist already versus kind of

20:18.800 --> 20:23.640
pure innovation and building new stuff. Yeah, it's definitely a combination of

20:23.640 --> 20:27.760
both, especially because we want to be fast to market with a lot of these

20:27.760 --> 20:33.160
things. So we leverage whatever is already there. But often more often than not

20:33.160 --> 20:37.320
actually, we need to not only deploy the cutting edge but actually be and

20:37.320 --> 20:42.400
define the cutting edge as well. And as I was hinting a little bit earlier, one

20:42.400 --> 20:46.600
example is the real-time and non-intection platform that I built when I joined

20:46.600 --> 20:52.240
Uber about five years ago. And so here it became very quickly clear that with

20:52.240 --> 20:56.360
the scale that we're operating at, the real-time nature, the signal to noise

20:56.360 --> 21:00.840
ratio because we actually would be sending pager to the alerts that would wake

21:00.840 --> 21:04.600
people up potentially in the middle of the night. If our algorithm thought there

21:04.600 --> 21:09.800
was a system outage going on. And so we were able to break new ground very

21:09.800 --> 21:14.880
quickly in the space. And one other thing that really helped develop these

21:14.880 --> 21:20.360
algorithms to the precision recall that we needed was I put myself on call. I

21:20.360 --> 21:24.760
was very customer obsessed. We're in the pager. Sorry. Wearing the pager. Yes,

21:24.760 --> 21:29.440
wearing the pager for six teams, multiple consecutive weeks. I can tell you I

21:29.440 --> 21:35.000
did not sleep much. I was working up a lot during this time. But that also helped

21:35.000 --> 21:38.880
to improve the algorithms really quickly. May she want to get it right? Yes,

21:38.880 --> 21:45.840
exactly. Nice, nice. Your team is kind of building very use-case specific

21:45.840 --> 21:50.960
things, you know, very close to the end user, doing low-level kind of

21:50.960 --> 21:59.000
infrastructure as well to support all of this. How do you organize? How do you

21:59.000 --> 22:04.840
build an organization to support all of this? Yeah, so we are organized by

22:04.840 --> 22:09.480
data science expertise. So we have a forecasting team, a non-detection team,

22:09.480 --> 22:14.800
experimentation team, convi team, computer vision team, etc. basically. And then

22:14.800 --> 22:19.120
as many other companies we are cross-functionally organized. So we have

22:19.120 --> 22:23.960
data scientists and engineers, product managers, designing, working

22:23.960 --> 22:28.560
together, basically building all of these platforms. When we zoom into the

22:28.560 --> 22:33.960
data scientists, we tend to hire full stack data scientists for these roles

22:33.960 --> 22:38.640
exactly for the reason that you described. We see a lot of advantages to

22:38.640 --> 22:42.720
having folks who can write production-level code in addition to having this

22:42.720 --> 22:48.360
deep domain expertise in a particular data science area. And in some of the

22:48.360 --> 22:52.880
advantages we see here already start in the design phase. So having somebody

22:52.880 --> 22:58.680
has deep understanding about the constraints of the infrastructure stack. As

22:58.680 --> 23:03.120
we're developing a lot of these things at scale with latency constraints can

23:03.120 --> 23:07.920
be highly advantageous. Because things that might look good on paper might

23:07.920 --> 23:12.240
actually not work in the real world once we deploy it in these ecosystems.

23:12.240 --> 23:17.320
Do you have examples of that? Things that look good on paper that didn't

23:17.320 --> 23:20.760
actually work out? Yeah. So coming back again to the real-time

23:20.760 --> 23:26.320
anomaly detection framework. So at the beginning we were developing algorithms

23:26.320 --> 23:31.000
that would have extremely fine granular data points. So we obviously want to

23:31.000 --> 23:36.320
have near real-time signal. And so we wanted to have an understanding minute by

23:36.320 --> 23:41.600
minute or even in finer granularity what was going on. And so originally we

23:41.600 --> 23:47.080
designed an algorithm that would require multiple weeks of data to train one

23:47.080 --> 23:51.840
way of how we did this is to frame it as a forecasting problem. And then half

23:51.840 --> 23:55.880
that minute granularity. And that obviously would be an extremely large overhead

23:55.880 --> 24:02.480
onto our database systems on that front. And so we basically designed the

24:02.480 --> 24:07.760
algorithms such a way that we have lower course of granularity in more

24:07.760 --> 24:11.520
historic kind of aspects. And then very fine granularity is overlaid as a

24:11.520 --> 24:16.360
secondary step in order to still capture the variance on for example a minute

24:16.360 --> 24:21.400
by minute level. So just having that kind of constraint in mind made sure that

24:21.400 --> 24:27.600
we didn't require unnecessarily high kind of overheads on our databases and

24:27.600 --> 24:32.760
unnecessary infrastructure cost as a result. So that's one example on that

24:32.760 --> 24:38.400
front. Example another example of where we see a lot of advantages for having

24:38.400 --> 24:44.280
full stack data scientists is in the productionization step. So here we're

24:44.280 --> 24:49.160
trying to avoid handoffs in terms of a data scientist writing a script or a

24:49.160 --> 24:54.080
white paper and then you know providing it to a software engineer. We see a

24:54.080 --> 24:59.040
lot of opportunities for errors on that front logical errors in particular. And

24:59.040 --> 25:04.520
so having data scientists to also write the production level code without such a

25:04.520 --> 25:09.520
hand of step is really important to us. And then finally also developer

25:09.520 --> 25:16.760
velocity. So as we all know there is this prototyping step involved and we're

25:16.760 --> 25:20.640
trying to exceed some threshold criterion that we've set out in the

25:20.640 --> 25:24.640
beginning of the design phase. And it's of course not known when are we gonna

25:24.640 --> 25:30.080
exceed basically this threshold criterion. And so that can lead them to

25:30.080 --> 25:33.760
lag times once you have actually found an algorithm you want to productionize

25:33.760 --> 25:38.480
that software engineer might be busy with other things during that time. And so

25:38.480 --> 25:43.200
again having data scientists to can write production level code can really

25:43.200 --> 25:48.640
also help speed up that innovation cycle as well. You mentioned developer

25:48.640 --> 25:52.960
velocity and that makes me think of kind of velocity in a sense of agile

25:52.960 --> 25:58.040
methodologies. Do you mean it that concretely? And is there a methodology that

25:58.040 --> 26:03.240
you've kind of evolved to or developed that works well in the context of

26:03.240 --> 26:08.880
these types of problems? So the way we work is very closely in software

26:08.880 --> 26:13.480
engineering kind of principles. In both in terms of best practices in terms of

26:13.480 --> 26:18.040
our working structure we work on a daily basis hand-in-hand with software

26:18.040 --> 26:22.200
engineers. So we have developed a lot of this. But I think you bring up a

26:22.200 --> 26:25.960
really good point in terms of you know developer velocity and productivity.

26:25.960 --> 26:31.760
And so a big goal of the platform teams more holistically is to really speed

26:31.760 --> 26:35.800
up that innovation cycle while increasing accuracy of the various

26:35.800 --> 26:40.040
different methodologies employed right. And so the way we see it is we have

26:40.040 --> 26:44.840
these four major steps within the development cycle of a machine learning or

26:44.840 --> 26:49.160
broadly speaking data science problem. You have exploratory data analytics

26:49.160 --> 26:54.000
than this iterative prototyping phase productionization and then roll out

26:54.000 --> 26:58.840
and monitoring and that closes the loop for kind of a new cycle to start off.

26:58.840 --> 27:04.680
If there's you know a V2 that we want to progress in. And so building abstraction

27:04.680 --> 27:08.360
layers higher level abstraction layers and this is where you know a lot of the

27:08.360 --> 27:12.840
work that my team and collaborators are coming in to play really helps to

27:12.840 --> 27:17.400
facilitate not us only for us ourselves kind of this cycle but really for the

27:17.400 --> 27:23.600
entirety of the company to really go faster and faster around that loop. So you're

27:23.600 --> 27:27.880
primarily doing this by hiring full-stack engineer full-stack data scientists

27:27.880 --> 27:34.720
they're not easy to find. And we throw that time around like it defines

27:34.720 --> 27:41.800
concretely a specific set of skills but not you not every full-stack data

27:41.800 --> 27:47.320
scientist is going to have the same strengths. How do you grow your data

27:47.320 --> 27:52.640
scientists or if you find folks that need support in one or more areas or how do

27:52.640 --> 27:59.080
you manage the kind of learning cycle for your team? Yeah absolutely so when

27:59.080 --> 28:04.360
we hire for these roles we also hire complimentary of course right so there

28:04.360 --> 28:08.200
would be some folks on the team who lean more towards the research side and

28:08.200 --> 28:12.600
others who lean more towards the engineering and you know software development

28:12.600 --> 28:17.080
side and folks that sit in between. So that's one aspect and then there's a very

28:17.080 --> 28:21.880
strong learning and teaching culture at Uber really continuously striving to

28:21.880 --> 28:29.360
improve oneself and so we have a lot of programs even within Uber educational

28:29.360 --> 28:34.480
programs everything from introductory courses to machine learning all the way

28:34.480 --> 28:40.600
to domain experts basically who then give workshops and training sessions hands

28:40.600 --> 28:45.720
on basically courses. I've invested a lot in mentorship programs at Uber as

28:45.720 --> 28:50.720
well building out a community across all of the data science and analytics where

28:50.720 --> 28:55.960
we then partner folks and in sometimes we also do 20% projects similar to

28:55.960 --> 29:00.840
what Google for example does where we have people then immersed into various

29:00.840 --> 29:05.800
different teams to get hand on a Hans experience basically in some of these

29:05.800 --> 29:09.320
domains so yeah I think continuous learning and teaching is something that's

29:09.320 --> 29:14.200
really really important. Okay and what are you excited for going forward what's

29:14.200 --> 29:20.320
the future of data science platforms that Uber look like? Yeah absolutely so I

29:20.320 --> 29:26.360
see both at Uber as well as in the industry a big push towards these higher and

29:26.360 --> 29:31.640
higher level abstractions for platforms to really kind of commoditize it to

29:31.640 --> 29:36.640
make it available to a broader audience beyond data science machine learning

29:36.640 --> 29:41.320
engineers and engineers more broadly speaking and so coming back to that

29:41.320 --> 29:46.600
four-step data science workflow or machine learning workflow that it

29:46.600 --> 29:51.720
described earlier you know one of the gaps that we saw is that we didn't have

29:51.720 --> 29:56.320
any semi-automation or automation around a data exploration or insights

29:56.320 --> 30:00.360
generation as a whole and that's something that actually goes well beyond

30:00.360 --> 30:04.040
data science or machine learning workflows right? It's that first step of the

30:04.040 --> 30:09.560
four-step cycle exactly the first step right where there is still a lot of human

30:09.560 --> 30:14.720
hours that need to go into that to dig into the data to understand and explore the

30:14.720 --> 30:19.040
data and also for business analysts right a lot of the questions that they

30:19.040 --> 30:24.920
would be getting is I have an important business KPI and it moved up or down

30:24.920 --> 30:28.520
right to investigate you know all the various different slices and

30:28.520 --> 30:32.960
dices of the data on what might have happened right? And so what we have

30:32.960 --> 30:37.280
been starting to work on is a proof of concept to actually have an algorithm

30:37.280 --> 30:42.360
automatically scan through our data and to surface a potentially interesting

30:42.360 --> 30:47.920
insights to folks whether it's machine learning experts or business

30:47.920 --> 30:52.720
analysts for example and they obviously would go and dig into some of these

30:52.720 --> 30:58.120
suggestions and understand them more deeply but here we see a really large

30:58.120 --> 31:04.320
opportunity not only to save people a lot of time but also to really open up

31:04.320 --> 31:08.960
new insights that might not have been discovered previously and this is some of

31:08.960 --> 31:13.800
the feedback that we're getting from our early adopters in this field that the

31:13.800 --> 31:18.440
machine was able to come up with interesting suggestions that they say

31:18.440 --> 31:24.080
wouldn't have come up themselves and so I think that really will if successful

31:24.080 --> 31:28.000
will revolutionize how we do data analytics at Uber and I think more

31:28.000 --> 31:32.360
broadling the industry awesome awesome well friend thanks so much for joining us

31:32.360 --> 31:44.440
here at Twomalcon thanks for having me great speaking with you all right

31:44.440 --> 31:48.960
everyone I hope you enjoyed our show straight from the main stage at Twomalcon

31:48.960 --> 31:55.320
AI platforms for more information about today's show visit Twomalai.com and

31:55.320 --> 32:02.080
for more Twomalcon coverage visit Twomalcon.com slash news thanks so much

32:02.080 --> 32:05.680
for listening and catch you next time

