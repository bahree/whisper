WEBVTT

00:00.000 --> 00:16.320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

00:16.320 --> 00:21.480
people, doing interesting things in machine learning and artificial intelligence.

00:21.480 --> 00:34.800
I'm your host, Sam Charrington, before we dive in I'd like to quickly share a few updates

00:34.800 --> 00:37.160
about our ongoing giveaways.

00:37.160 --> 00:42.600
If you haven't done so already you've got until 11.59pm Pacific time tonight to enter

00:42.600 --> 00:46.600
into our AI conference New York Bronze Pass giveaway.

00:46.600 --> 00:52.240
Visit twimlai.com slash AI and Y giveaway to get your entry in.

00:52.240 --> 00:58.760
Also the hackers among us should be sure to visit twimlai.com slash TF giveaway to enter

00:58.760 --> 01:01.520
our TensorFlow Edge Kit giveaway.

01:01.520 --> 01:06.640
Three lucky winners will receive a coral edge TPU device and the Spark Fund Edge Development

01:06.640 --> 01:09.400
Board amongst other goodies.

01:09.400 --> 01:13.320
The contest ends on April 8th.

01:13.320 --> 01:17.640
Today we're joined by Peter Whittick, assistant professor at the University of Toronto who

01:17.640 --> 01:21.880
works on quantum enhanced machine learning and the application of high performance learning

01:21.880 --> 01:24.840
algorithms and quantum physics.

01:24.840 --> 01:28.880
Peter and I caught up back in November to discuss the presentation he gave at the AWS

01:28.880 --> 01:33.880
Reinvent Conference, pragmatic quantum machine learning today.

01:33.880 --> 01:37.160
In our conversation we start with a bit of background including the current state of

01:37.160 --> 01:41.560
quantum computing, a look ahead to what the next 20 years of quantum computing might

01:41.560 --> 01:45.400
hold and how current quantum computers are flawed.

01:45.400 --> 01:49.400
We then dive into our discussion on quantum machine learning and Peter's new course on

01:49.400 --> 01:52.200
the topic which debuted in February.

01:52.200 --> 01:54.200
I'll link to that in the show notes.

01:54.200 --> 01:59.240
Finally, we briefly discuss the work of E1 Tang, a PhD student from the University of

01:59.240 --> 02:04.440
Washington whose undergrad thesis a quantum inspired classical algorithm for recommendation

02:04.440 --> 02:07.640
systems made quite a stir last summer.

02:07.640 --> 02:12.200
As a special treat for those interested, I'm also sharing my interview with E1 as a bonus

02:12.200 --> 02:14.560
episode alongside this one.

02:14.560 --> 02:18.520
I'd love to hear your thoughts on how you think quantum computing will impact machine

02:18.520 --> 02:21.080
learning in the next 20 years.

02:21.080 --> 02:24.520
Send me a tweet or leave a comment on the show notes page.

02:24.520 --> 02:29.680
I'd like to thank Pegasystems, this episode sponsor, and join them in inviting you to

02:29.680 --> 02:35.200
Pegaworld, the company's annual digital transformation conference to be held at the MGM Grand

02:35.200 --> 02:38.840
in Las Vegas from June 2nd to 5th.

02:38.840 --> 02:43.400
Pegasystems puts AI at the center of its customer engagement software so that every customer

02:43.400 --> 02:49.480
touchpoint on every channel is optimized in real time, so that customers find each interaction

02:49.480 --> 02:54.560
relevant and timely, whether a sales call, a marketing campaign, or a customer service

02:54.560 --> 02:55.880
chat.

02:55.880 --> 03:00.160
At Pegaworld, you'll hear great stories of AI applied to the customer experience at

03:00.160 --> 03:02.240
real Pegacustomers.

03:02.240 --> 03:06.600
The event is a great way to learn from a who's who of the Fortune 500, and of course,

03:06.600 --> 03:10.080
I'll be there, and we'll be speaking as well.

03:10.080 --> 03:15.720
To register, visit Pegaworld.com and use the promo code Twimble19 when you sign up for

03:15.720 --> 03:16.720
$200 off.

03:16.720 --> 03:20.760
Again, that's Twimble19, it's as easy as that.

03:20.760 --> 03:23.000
Hope to see you there.

03:23.000 --> 03:25.920
And now onto the show.

03:25.920 --> 03:33.280
All right, everyone, I am on the line with Rob Walker. Rob is the vice president of

03:33.280 --> 03:36.600
decision management and analytics with Pegasystems.

03:36.600 --> 03:44.080
You may remember Rob's name from TwimbleTalk number 127 on hyperpersonalizing the customer

03:44.080 --> 03:46.360
experience with AI.

03:46.360 --> 03:49.000
Rob, welcome back to this week in machine learning and AI.

03:49.000 --> 03:50.000
Yeah, thanks, Sam.

03:50.000 --> 03:52.000
Glad to be back.

03:52.000 --> 03:53.000
Absolutely.

03:53.000 --> 03:54.000
Glad to have you back.

03:54.000 --> 03:59.760
I'm looking forward to what I believe will be a really interesting conversation on the

03:59.760 --> 04:03.240
role of empathy in AI.

04:03.240 --> 04:08.760
For folks that want to learn more about you, I'll refer them back to the previous episode,

04:08.760 --> 04:12.480
but give us a quick overview of your focus at Pegasystems.

04:12.480 --> 04:14.680
Yes, I'm happy to do that.

04:14.680 --> 04:23.160
So within Pegas, I'm responsible for our AI space, but we really try, I mean, there's

04:23.160 --> 04:28.240
so much hype around AI and we don't do AI just for AI sake.

04:28.240 --> 04:35.880
We really try to focus on making AI work for typically pretty large enterprises and

04:35.880 --> 04:39.240
typically in the area of customer engagement, right?

04:39.240 --> 04:44.800
So in the previous episode, we talked about hyperpersonalization and hyperpersonalization

04:44.800 --> 04:52.760
is really trying to be one-to-one conversations with customers for companies to do that.

04:52.760 --> 04:56.840
And that requires a lot of AI.

04:56.840 --> 05:01.920
It also requires lots of other things, but AI is an important aspect of that.

05:01.920 --> 05:04.640
And that's what I mostly worry about.

05:04.640 --> 05:07.800
And also areas around AI.

05:07.800 --> 05:09.520
It's not just, hey, AI is cool.

05:09.520 --> 05:13.000
Let's use that in customer engagement to make customer engagement better, but it's

05:13.000 --> 05:16.800
stuff like, can we trust it?

05:16.800 --> 05:20.040
Who in the organization should be responsible for it?

05:20.040 --> 05:24.840
If it makes weird decisions, if there is a bias, those kind of things.

05:24.840 --> 05:28.120
So that's typically what I worry about.

05:28.120 --> 05:33.720
The concept that kept coming up in our last conversation, I think this is pretty central

05:33.720 --> 05:40.720
to the way you think about applying AI to optimizing customer experiences.

05:40.720 --> 05:47.000
This idea of helping your customers figure out the next best action to take with their

05:47.000 --> 05:48.000
customers.

05:48.000 --> 05:49.000
Is that right?

05:49.000 --> 05:50.000
That is correct.

05:50.000 --> 05:51.000
Yes.

05:51.000 --> 05:52.000
Yes.

05:52.000 --> 05:58.520
So the companies we work with typically implement like this customer decision hub, right?

05:58.520 --> 06:04.920
And it's a centralized decision authority that across all the different channels that

06:04.920 --> 06:13.160
companies may have, figures out the next best action during conversations, right?

06:13.160 --> 06:14.680
So what should we say?

06:14.680 --> 06:16.840
What should we not say?

06:16.840 --> 06:24.600
What price should we mention if it's commercial, basically trying to have very reasonable

06:24.600 --> 06:30.720
conversations, but at the same time, because most of the companies we work with are not

06:30.720 --> 06:32.680
just charities, right?

06:32.680 --> 06:37.600
So at the same time, you need to sort of improve customer value, right?

06:37.600 --> 06:43.240
So the next best action, the best in next best action, is typically some metric about customer

06:43.240 --> 06:49.800
value and trying to improve that over time by doing the best thing possible to optimize

06:49.800 --> 06:50.800
that.

06:50.800 --> 06:58.600
So this concept of empathy in AI is something that you'll be speaking about at the next

06:58.600 --> 07:04.000
Pegaworld, an event that your company hosts annually.

07:04.000 --> 07:07.920
I attended the last one and I'll be attending the next one.

07:07.920 --> 07:15.400
How did this idea of empathy, introducing empathy into these kinds of transactions or customer

07:15.400 --> 07:17.200
experiences?

07:17.200 --> 07:18.880
Where did that come from?

07:18.880 --> 07:21.440
Well, so I've always been interested.

07:21.440 --> 07:27.480
So before I joined Pegaw at some point before that I was a scientist, right, in AI.

07:27.480 --> 07:35.160
I did my PhD in that area and I've always been interested in not just all the cool things

07:35.160 --> 07:41.440
AI can do around predicting customer behavior and things like that, but also potentially

07:41.440 --> 07:43.080
the not so cool things, right?

07:43.080 --> 07:45.440
So can you trust it?

07:45.440 --> 07:46.640
Is it transparent?

07:46.640 --> 07:48.400
Is it opaque?

07:48.400 --> 07:50.240
Is there a bias?

07:50.240 --> 07:51.240
Can it go rogue?

07:51.240 --> 07:53.680
You know, those kind of things.

07:53.680 --> 07:59.800
So over the last years, we've really tried to sort of guard the moral high ground if you

07:59.800 --> 08:06.360
will around AI and I'm just look at what it can, the value it can bring, but also mitigate

08:06.360 --> 08:11.320
the risk that you can have with AI.

08:11.320 --> 08:15.600
And following sort of that path, empathy was a very natural thing.

08:15.600 --> 08:22.320
I mean, you know, the bigger thing is morality, you know, the morality of AI and AI decisions,

08:22.320 --> 08:29.560
which is, you know, that's a big beast and sort of more ethical behavior and empathy

08:29.560 --> 08:36.800
seem to be something that was just about tangible enough to try to really put it into the

08:36.800 --> 08:41.440
product and into the vision of best practice around using AI.

08:41.440 --> 08:47.240
So we've been spending quite some time thinking about that and how you can operationalize

08:47.240 --> 08:48.320
that kind of thing.

08:48.320 --> 08:56.760
Now, when you start talking about the morality of AI, certainly, and even to large degree

08:56.760 --> 09:02.240
empathy, I start to, you know, the picture in my head starts to form around, you know,

09:02.240 --> 09:07.640
what some of us will call AGI, artificial general intelligence, you know, what we talk

09:07.640 --> 09:11.240
about more commonly is like sci-fi AI, right?

09:11.240 --> 09:12.880
Is that what we're talking about here?

09:12.880 --> 09:18.200
Or are we talking about something that, you know, how can you make this more concrete

09:18.200 --> 09:19.200
for us?

09:19.200 --> 09:23.080
Yeah, because because I'm definitely not talking about that kind of thing.

09:23.080 --> 09:26.720
I mean, that's really, well, it's, it's, it's interesting.

09:26.720 --> 09:27.720
Right?

09:27.720 --> 09:33.760
I mean, I think everybody in AI is thinking about how that, how that, how that works.

09:33.760 --> 09:38.440
But I think just as a human species, if I just, you know, just even reading the news, you

09:38.440 --> 09:44.440
know, today, I think I'm, I'm not sure we have morality, you know, very, very, very,

09:44.440 --> 09:48.400
very clear for, for, for everyone, at least there is a lot of discussion about what would

09:48.400 --> 09:52.920
be moral judgment and not a moral judgment to even expect that of AI.

09:52.920 --> 09:59.920
I think it's already a, a tough act, but I'm certainly not talking about it in that kind

09:59.920 --> 10:03.720
of realm quite yet, although it's a very interesting topic, right?

10:03.720 --> 10:06.680
I was just thinking about, you know, self-driving cars, right?

10:06.680 --> 10:12.720
And, and, and sort of the moral judgments, they may, one day have to make, right?

10:12.720 --> 10:14.880
In extreme circumstances.

10:14.880 --> 10:21.920
But this is very much sort of a smaller subset of those challenges where we're talking about

10:21.920 --> 10:25.220
customer engagement and those kinds of things.

10:25.220 --> 10:34.000
And I think in that area, empathy really shows in, in, in, in, in, in, in pretty clear dimensions.

10:34.000 --> 10:38.960
Like stuff like, is this, if we are talking to me as a company, and it's an AI-driven

10:38.960 --> 10:41.760
conversation, is that, is that a relevant?

10:41.760 --> 10:42.760
Right?

10:42.760 --> 10:44.680
Or are you wasting my time?

10:44.680 --> 10:46.600
Stuff like, is it, is it appropriate?

10:46.600 --> 10:47.600
Right?

10:47.600 --> 10:50.640
So you're talking to me about something and I, it might be interesting, it may be interesting

10:50.640 --> 10:59.840
to me but it may not still be appropriate. Maybe you shouldn't be selling me a gun or a car

10:59.840 --> 11:07.520
or a credit card that I actually can't pay back when I'm in debt. So it's those kind of thing

11:07.520 --> 11:12.880
and is there mutual value? Are you talking to me for something that can we have a transaction that

11:12.880 --> 11:19.840
has a mutual value or is it just about the company? And I think if companies implement those

11:19.840 --> 11:29.120
kind of considerations well, I think that will do pretty well on an empathy scorecard for starters.

11:30.000 --> 11:35.840
Now that's interesting and actually somewhat different from the, I don't know if it's different

11:35.840 --> 11:41.920
from the direction that I thought we were going to go here or that you know the picture that formed

11:41.920 --> 11:51.440
in my mind when we were talking earlier about empathy or if one is part of the other. But I'll tell

11:51.440 --> 11:56.480
you I'll kind of recount the picture that I have in my money. Let me know where it fits into

11:56.480 --> 12:04.480
this world. You know I was envisioning primarily the kinds of interactions that you might have

12:04.480 --> 12:12.800
via a chat bot or you know a chat kind of interface and you often have or even you know to the

12:12.800 --> 12:21.520
extent to which AI is driving a call a call center agent and their responses because that's some

12:21.520 --> 12:30.000
of that or IVR. You know some of that is starting to happen. But I was envisioning kind of this

12:30.000 --> 12:38.160
set of capability where you know maybe the you know whether the chat bot or the IVR you know IVR

12:38.160 --> 12:44.480
is a great example right it's like you can IVR should be able to tell from my voice or could

12:44.480 --> 12:49.840
be could tell from my voice that I'm getting frustrated navigating the 50 million you know menus

12:49.840 --> 12:55.280
and maybe escalate me you know a little bit more quickly to someone or you know you can imagine

12:55.280 --> 13:02.000
the same kind of thing happening in a chat interaction where you know I'm interacting with this

13:02.800 --> 13:09.360
with this virtual agent it's not getting what I need to do or it's you know I'm needing asking

13:09.360 --> 13:15.440
me to repeat myself you know multiple times you know there's a there's a degree of empathy and

13:15.440 --> 13:21.680
all that where it's understanding my I guess I'm kind of simplifying that is understanding my

13:21.680 --> 13:28.080
emotional state and using that as part of the the decisioning around what the next best action

13:28.080 --> 13:33.760
to take is yeah but it sounds like maybe that's a piece of what you're saying but you're also

13:33.760 --> 13:41.360
talking about you know maybe about the broader AI ethics conversation your example around you

13:41.360 --> 13:48.640
know should we offer the the credit card to the person who can't afford it is one that kind of

13:48.640 --> 13:54.960
you know resonates and kind of drives me in that direction yeah now I think so so I think the

13:54.960 --> 14:01.600
example that you gave right like is is somebody getting frustrated and those kind of things I think

14:01.600 --> 14:07.840
are a very important part of of empathy but I think it's part of the delivery mechanism so I think

14:07.840 --> 14:13.200
what we're trying to do is sort of take that in in in in in in two different layers if you will

14:13.200 --> 14:17.840
right one is when you're talking to someone and somebody is getting frustrated or if you do voice

14:17.840 --> 14:24.160
detection and or inflection and you you sort of notices that somebody is getting upset

14:25.920 --> 14:32.240
you may want to change that may influence the next best action as part of the context so I would

14:32.240 --> 14:38.480
call that although it's called more of the sort of the superficial way of empathy right it's trying

14:38.480 --> 14:45.120
to feel somebody's mood and use it as a context but that can become from a lot of different senses

14:45.120 --> 14:50.640
it could be the you know as I said the inflection of a voice it could be when this is face to face

14:50.640 --> 14:55.600
or you're in front of a camera right can be that you know people can sort of read their face or

14:55.600 --> 15:02.080
the system can read the face of the of the customer and see that's not going well but that's not

15:02.080 --> 15:09.280
the same as the underlying level of making sure that the next best action that you are contemplating

15:09.280 --> 15:17.360
is one that is empathetic or even moral right so I see that as two different different thing I think

15:17.360 --> 15:21.680
people think about empathy a lot like you were just describing it like hey I see this is

15:21.680 --> 15:27.440
making you upset so I need to you know hurry this along or ask you wrong and that's all

15:27.440 --> 15:34.480
cool very human stuff but it's under the livery of a particular action but determining what you're

15:34.480 --> 15:40.800
going to do that also requires empathy and that's more along the line of is this is this

15:40.800 --> 15:47.920
relevant is this appropriate is this suitable does kind of things and do you specifically use

15:47.920 --> 15:56.720
the word empathy to distinguish it from ethics or are those ideas different in your mind or

15:56.720 --> 16:03.840
are they you know it's just a different word in this case yeah now I think I think the if I think

16:03.840 --> 16:10.560
about ethics that sort of you know ethical behavior I think empathy is basically the step where

16:10.560 --> 16:17.600
humans for now maybe AI at some point will basically be able to you know please themself in someone

16:17.600 --> 16:25.280
else's place and say hey if this is happening to me you know is this going to make me you know

16:25.280 --> 16:30.960
happier and things like that so I think they are in that sense very very related

16:30.960 --> 16:39.040
hmm so in that sense ethics is kind of relevant to some broader set of you know societal norms

16:39.040 --> 16:44.640
whereas empathy we don't have to figure all that out it's just about you know this particular

16:44.640 --> 16:50.880
customer and I am I doing the right thing by this customer yes am I doing the right thing and I

16:50.880 --> 16:56.640
think there are few dimensions to that right am I doing the right thing in terms of am I forcing

16:56.640 --> 17:02.560
you to take like a risk that I actually think you know is too high I already know that you won't

17:02.560 --> 17:07.280
be able to pay back that mortgage or that credit card or that thing or you don't really need this

17:07.280 --> 17:13.200
kind of thing right so that's that's that's that's part of that decision but also is it relevant

17:13.200 --> 17:18.800
in the first place right it's it's an empathetic thing to not try and waste somebody's time right

17:18.800 --> 17:23.280
I mean if you don't I don't know about you Sam but if you like all these ads all the stuff that you

17:23.280 --> 17:29.440
get if you you know you know browse the internet and and look at all of these pages it's it's

17:29.440 --> 17:34.400
we're used to it it doesn't show a lot of empathy right everybody's trying to get your attention

17:34.400 --> 17:41.200
to do things that you know maybe 1% of the time you're vaguely interested in right so that's

17:41.200 --> 17:48.160
that's that's that's part of it is it relevance is it not wasting my time is it do you remember the

17:48.160 --> 17:53.680
context do you remember that I just spoke to you in another channel right I just walked into the

17:53.680 --> 17:59.920
branch I just visited your website and now I'm going into the IPR do you even remember that or

17:59.920 --> 18:06.560
do are you forcing me to basically repeat everything I did I did you know in the previous channel

18:06.560 --> 18:14.400
that's empathy right empathy with customers that are trying to solve a problem or that want to

18:14.400 --> 18:21.280
get value out of their interaction with the company this is clearly an issue that is much bigger

18:21.280 --> 18:30.160
than AI right we don't have to you know look very far to recognize that the in many ways the

18:30.800 --> 18:37.760
previous financial crisis with the the mortgage bubble grew out of giving loans to people that

18:37.760 --> 18:44.880
you know weren't qualified for them and there are many many more examples where organizations you

18:44.880 --> 18:49.680
know fail to exhibit the kind of empathy that you are describing that have nothing to do with

18:49.680 --> 18:56.880
artificial intelligence or machine learning you know why why take this on from an AI perspective

18:57.840 --> 19:04.320
well I think that's a good question and I think the the answer to that is that if the way we look

19:04.320 --> 19:09.920
at customer interaction in general is to always do this next best action kind of thing and the next

19:09.920 --> 19:17.840
best action is actually collaboration between humans you know inside the company deciding on you

19:17.840 --> 19:26.720
know rules or thresholds or policies working together with AI where AI is maybe determining the

19:26.720 --> 19:35.440
risk it's determining the the level of likely interest from from the customer and it's that combination

19:35.440 --> 19:41.280
that creates the metric for this is the best thing to do right now right so you're you're quite

19:41.280 --> 19:45.760
right it's the and actually that mortgage example or the bubble that you just described is a great

19:45.760 --> 19:52.080
combination right there are analytic models that should have said listen um for this group of

19:52.080 --> 20:00.160
people um the risk is not really acceptable and um you shouldn't be pushing them on this level

20:00.160 --> 20:07.760
of of of of mortgage right suitability for instance is not taking into account right so it's

20:07.760 --> 20:16.480
that combination of AI and rules um that I you know we we call that decisioning or decision

20:16.480 --> 20:25.360
management um that basically needs to represent empathetic behavior right so it's not just the AI

20:25.920 --> 20:33.120
it's also the the rules but one of the reasons I think the AI aspect is so important is because

20:33.120 --> 20:42.720
the AI is learning right so it can you know have evolved um a particular bias right it may be a

20:42.720 --> 20:48.400
very opaque algorithm that may have evolved that bias and you don't even know right so there are

20:48.400 --> 20:55.040
a lot of aspects of AI that I think really touch on ethics and empathy as well when you're talking

20:55.040 --> 21:02.240
about this at Pega World are you you know are you raising this as an issue that customer should

21:02.240 --> 21:09.440
start thinking about this are you talking about new capabilities that you're unveiling at Pega

21:09.440 --> 21:17.840
uh uh with you know with the product that will help them address these issues is it um you know

21:17.840 --> 21:24.080
or is there something else well I don't want to feel all my own center but uh we'll we'll definitely

21:24.080 --> 21:29.200
so two years ago one of the things that I was talking about in the keynote was about this

21:29.200 --> 21:35.120
single the t switch and the t was you know stands for transparency but also for trust and it's

21:35.120 --> 21:41.440
basically the ability that once you have this centralized next best action capability inside of

21:41.440 --> 21:50.000
your company um that you can have full control over where you allow um sort of opaque algorithms

21:50.000 --> 21:55.600
like deep learning or genetic algorithms or that's kind of fancy stuff or where you insist on

21:55.600 --> 22:00.720
more transparent algorithms that you can actually explain to a customer and that you really

22:00.720 --> 22:07.760
explain or that you really understand yourself right so that was one aspect um next up is the

22:07.760 --> 22:14.720
is that the thing around empathy I won't I think it's a really good thing if companies are aware

22:14.720 --> 22:24.800
at all times how empathetic their behavior is so um think about sort of a dashboard where you would

22:24.800 --> 22:30.960
see of all the actions my company is taking and these are you know the decide companies we work with

22:30.960 --> 22:36.720
these are hundreds of millions a day right hundreds of millions of interactions a day and then being

22:36.720 --> 22:44.480
able to see okay these are actions that we are taken automatically is a combination of AI and rules

22:45.120 --> 22:51.760
that are not empathetic and that means that they are going against the relevance or they are not

22:51.760 --> 22:57.200
appropriate like not suitable so we we're we're talking about this credit card but it's not really

22:57.200 --> 23:04.880
suitable um or it doesn't really create value for the customer right so imagine that while all of

23:04.880 --> 23:10.080
this is running this combination of AI and rules and it's making hundreds of millions of you know

23:10.080 --> 23:15.760
decisions and having all of these conversations with customers that you can just see that as a real

23:15.760 --> 23:23.360
time thing and say hey really we're getting you know less empathetic um let's see in our

23:23.360 --> 23:29.600
strategies in our customers strategies that we have in the algorithms that we use where we're losing

23:29.600 --> 23:35.280
that right are we pushing products that we shouldn't be pushing don't we have the rules that are

23:35.280 --> 23:42.000
determining suitability it's it's it's it's it's those kind of things and then in addition to that

23:42.000 --> 23:49.520
I think we can also determine sort of the cost of not being empathetic right so if you for instance

23:49.520 --> 23:55.440
if you if you if you if you are going against somebody's interests and I don't mean I mean interest

23:55.440 --> 24:01.360
in in in terms of of relevance right so so what a lot of marketing is currently doing right

24:01.360 --> 24:05.840
they're they're they're spamming you with stuff they're wasting your time on things that are

24:05.840 --> 24:12.640
actually not that relevant to you it would be good to not only know the percentage of of of of of

24:12.640 --> 24:19.200
events where that happens it would also be really good if you had a sense of the money um

24:19.200 --> 24:24.960
you're actually losing and and you can calculate that mathematically by for instance saying hey

24:24.960 --> 24:28.400
this is what we actually talked about to this customer we talked about this mortgage

24:28.960 --> 24:34.480
and the customer said no to it because it wasn't relevant um what we could have been talking about

24:34.480 --> 24:40.720
is this particular issue that we spotted um you know in in in in in a in a different channel or

24:40.720 --> 24:47.840
last them uh last week or an hour ago or maybe a much more relevant offer that we didn't think

24:47.840 --> 24:54.400
we wanted to do because the margin was you know not as big as on the mortgage right having making

24:54.400 --> 25:03.120
that a transparent thing having people own the kind of um you know empathy level of the brand

25:03.120 --> 25:11.040
I think is um is a really important thing going going forward it strikes me that that latter point

25:11.040 --> 25:22.480
around quantifying the cost of these non empathetic actions is really a big part of where the problem

25:22.480 --> 25:29.920
lies it's it's easy to it's easy to know the cost of the you know they expected

25:29.920 --> 25:38.640
revenue or profit from offering something but a lot harder to know the cost of just wasting the

25:38.640 --> 25:46.480
the customer's time or you know reducing the the brand goodwill because of some series of you

25:46.480 --> 25:55.120
know less relevant or poor experiences how do you overcome that gap yeah well I think some of the

25:55.120 --> 26:00.560
math actually works out quite nicely right so remember that when we do this next best action and

26:00.560 --> 26:06.880
I said before the best is a function of you know the value that is created in the in the in the

26:06.880 --> 26:14.400
relationship so um the math works out that you actually can know that if you go against the

26:14.400 --> 26:18.240
propensity because for instance you're selling let's use this mortgage example that works really

26:18.240 --> 26:24.720
well um if you um if you are thinking this is not particularly relevant but if the customer says

26:24.720 --> 26:31.280
yes this is the margin this is the money the i as a bank in this case will will will will make

26:31.760 --> 26:38.000
right if you calculate all the times a customer said no because you're basically offering stuff

26:38.000 --> 26:42.080
that's not particularly relevant just you know you're you're hoping that the customer will say yes

26:42.960 --> 26:48.800
um what how could we have used that moment how could we have used that interaction with the

26:48.800 --> 26:54.880
customer in a better way that would have created more value if you just multiply that with a hundred

26:54.880 --> 27:00.720
of millions of of decisions you make you get to a monetary value and you find in examples like

27:00.720 --> 27:08.480
this that you know there's some explicit decision that where the customer is saying hey let's offer

27:08.480 --> 27:15.760
this more profitable thing even though we know it's not as relevant or um does that does that happen

27:15.760 --> 27:23.200
in more subtle ways no the i don't think these ways are particularly subtle right so the way

27:23.200 --> 27:30.160
the way because the way we work um is like so to do this next best action right we would calculate

27:30.720 --> 27:35.120
um every single thing this is also part of this hyper personalization

27:35.120 --> 27:40.960
vision right to be completely one to one it means that of all the possible conversations you could

27:40.960 --> 27:46.880
have right you're going to rate them in real time based on the context and you're going to say

27:46.880 --> 27:52.160
this is the thing we are going to talk about as a combination of what the AI thinks is

27:52.160 --> 27:57.920
particularly appropriate and relevant as well as my rules that I have around profitability

27:57.920 --> 28:03.680
and inventory uh inventory and and all of those kind of things right so it's that combination

28:03.680 --> 28:10.240
but we calculate them all in parallel so it's relatively easy to see okay this is what we chose

28:10.240 --> 28:19.520
to actually talk about but this is what we could have talked about if we had weighted suitability higher

28:19.520 --> 28:25.920
or if we didn't overrule this very low um or this very high propensity and said well even though

28:25.920 --> 28:30.800
that's relevant it's not what we want to talk about right so you can see how you get a drop off

28:30.800 --> 28:38.480
of typical uh or or of specific conversation topics that you decide not to pursue for other reasons

28:38.480 --> 28:46.720
and that's what you can um calculate is the task then starting that to you know build awareness

28:46.720 --> 28:56.400
on the part of customers or users or kind of the the industry uh as as a whole to incorporate

28:56.400 --> 29:07.520
these types of empathy metrics if we call them that into um you know their systems their rules their

29:08.880 --> 29:15.920
algorithms um and start to is it as I don't know if simple is the right word but is it as simple

29:15.920 --> 29:22.480
as you know just you know starting to try to put numbers around uh these you know suitability

29:22.480 --> 29:30.320
context to where relevance you know risk and then feeding them into your uh uh your automation

29:30.320 --> 29:37.280
tooling with uh kind of appropriate weights or is it does it go beyond that I think i think the

29:37.280 --> 29:41.760
way you describe it so what we're trying to do is first of all we will talk about that's like an

29:41.760 --> 29:50.000
easy task no no no no no no no no like that but yeah yeah no that's not easy in itself but what we

29:50.000 --> 29:55.760
we are trying to do is, first of all, make it very explicit. When we talk about next-best

29:55.760 --> 30:05.600
action, there are patterns that we've seen that are repeatedly successful and they include

30:05.600 --> 30:16.080
things like relevance, suitability, mutual value, risk mitigation. The tooling and the methodology

30:16.080 --> 30:21.920
already encourage companies to at least think about it. They may think, okay, well, for suitability,

30:21.920 --> 30:29.520
we really don't care about it, or not as much, or we let profitability, trump, suitability,

30:29.520 --> 30:36.240
anytime, but at least the product, if you follow the product guidance, you will have to take all

30:36.240 --> 30:44.720
of these considerations into account. There is an ethical framework built into the software,

30:44.720 --> 30:50.640
into the strategies that it will generate. That's one aspect of it, and then the other aspect of it

30:50.640 --> 30:57.200
are like the dashboard that you will show. It's basically shaming companies a little bit if they

30:57.200 --> 31:04.960
want, having them self-shame them into appropriate behavior, where they would say, hey, listen,

31:04.960 --> 31:11.120
we cranked up profitability, but it's at the expense of suitability or customer interest.

31:11.120 --> 31:21.200
At least, I think the awareness and the transparency around these things will be leading to better

31:21.200 --> 31:32.000
behavior. How does the company begin to put tangible numbers and costs around things like mutual

31:32.000 --> 31:38.560
value and suitability in contexts, awareness and relevance? Relevance is maybe easier because it

31:38.560 --> 31:46.400
impacts propensity to buy. Risk is something that's fundamentally numerical, but some of these

31:46.400 --> 31:55.760
others are a little bit squishier, maybe. You're right. From what we've seen,

31:55.760 --> 32:05.760
is that the less squishy things, once you are aware that you need to put them in,

32:05.760 --> 32:14.160
and that the AI or the decision in general, touching 100 million, making 100 million decisions

32:14.880 --> 32:18.960
in all the different channels with all of your customers, that that is part of your brand,

32:18.960 --> 32:26.720
and you need to protect that. I think that's a very important thing. I think for the squishier

32:26.720 --> 32:34.240
things, I think what we also encourage and also make possible is continuous experimentation.

32:34.240 --> 32:40.240
There is always control groups, there is all sorts of things where you can, for a small

32:40.240 --> 32:44.800
percentage, a small sample of the customers, you can actually measure if you're having an effect,

32:44.800 --> 32:53.920
if they have a positive response to the brand, and you can see if what kind of strategy changes

32:53.920 --> 32:58.320
would improve that, and that is a best practice kind of thing to do.

32:58.320 --> 33:07.040
Do you have any examples of folks that you've worked with that you can kind of walk us through

33:07.760 --> 33:16.480
how this all plays out and how they went about making, kind of incorporating these ideas

33:16.480 --> 33:24.320
into the way they make decisions? I think even before we invested in all this

33:24.320 --> 33:33.840
around empathy and also before the transparency and opacity, it's not like these big brands are

33:33.840 --> 33:42.880
not aware of these issues. I think 10 years ago, maybe longer ago, I had long conversation

33:42.880 --> 33:51.200
with banks that were sued for miscelling that I think we've seen more recent examples,

33:51.200 --> 34:01.440
where obviously these companies want to sort of control that even if just for their own sake,

34:01.440 --> 34:07.440
to not be part of some class action. And in the next best action methodology,

34:08.080 --> 34:13.520
stuff like relevance, appropriateness, failure, and risk have always been sort of first class

34:13.520 --> 34:22.560
citizens. What we're now trying to do is to make sure that it's much harder to break those

34:22.560 --> 34:32.160
patterns, or if you don't want to be compliant with these kinds of ethics practices, you at least

34:32.160 --> 34:38.880
have to make the effort. I think to your question, I think especially the banks, I think we see

34:38.880 --> 34:45.360
that in other industries as well are getting very worried about their brand image in that regard,

34:45.360 --> 34:50.640
and they are putting suitability criteria, for instance, it's a pretty hot topic right now,

34:50.640 --> 34:56.640
they're putting that as part of their next best action strategy. We just want to help them

34:56.640 --> 35:04.560
by showing the cost of that and the benefits of that. You mentioned compliance in there.

35:04.560 --> 35:11.760
Do you envision a time where an enterprise will have a formal empathy compliance regime?

35:11.760 --> 35:16.080
Will it be called that? Will it be called something else? Does it already exist under some other

35:16.080 --> 35:23.680
guys? I think that definitely will happen in some cases already happens. I don't think it's

35:23.680 --> 35:35.680
called empathy. It's probably more on the ethics board. It's not only about the company itself,

35:35.680 --> 35:42.240
it's also about basically a compliance issue out of self-interest, especially now. This is

35:42.240 --> 35:49.040
where the AI is so important, where there's so much self-learning going on on this incredible scale

35:49.040 --> 35:57.360
that there could be a bias and there could be all sorts of things happening that may not be so

35:57.360 --> 36:06.240
easy to control or even spot. I know that some of the companies I talk to, and these are larger

36:06.240 --> 36:15.760
companies, but they have a board already for all the algorithms that are involved in customer

36:15.760 --> 36:22.960
interaction. I think that's a sensible thing to do. It's part of what inspired this transparency

36:22.960 --> 36:29.440
or trust switch in the software to make sure that all AI is, at least, you can control

36:30.480 --> 36:38.720
the level of transparency that you require in certain circumstances, talking to customers.

36:38.720 --> 36:43.200
Do you envision a chief empathy officer? It sounds like no, it's probably going to be if anything,

36:43.200 --> 36:50.480
it'll be a chief ethics officer or some other role. Where do you see this all sitting?

36:51.200 --> 36:55.440
Yeah, well, I think this is a very interesting topic, because I think this will become

36:56.000 --> 37:03.280
very, very important if it isn't already. I think you will get into a situation where you have

37:03.280 --> 37:15.680
at the first level AI trying to check other AI for biases or an ethical behavior, because

37:15.680 --> 37:25.760
it's just a lot, and it would only escalate if such a bias or other irregularities is detected.

37:25.760 --> 37:32.800
But it's certainly, and again, I'm talking about the larger companies with tens to hundreds of

37:32.800 --> 37:39.520
millions of customers that are very worried about, especially with the level of automation that's

37:39.520 --> 37:46.000
now available, and then AI that is dynamically learning new things or evolving new things

37:47.040 --> 37:54.000
to have an ethics board like that. We try from a product perspective, we try to make sure

37:54.000 --> 38:00.480
that like you have QA tests on quality assurance tests, on performance, and other things that,

38:00.480 --> 38:09.280
as a matter of course, you would do the same thing around biased detection or other irregularities

38:09.280 --> 38:18.560
before you release the next version of your corporate brain, so to speak, to make that easier.

38:18.560 --> 38:31.600
I think the ideas of making these more empathetic types of qualities of your various offers,

38:32.640 --> 38:39.040
as you've suggested throughout this conversation, it's very much connected to this idea of

38:39.040 --> 38:46.080
transparency. There are these dollars and sense things that we build into decision-making algorithms

38:46.080 --> 38:51.680
all the time, but there's all this other stuff that goes into the customer experience, and what we're

38:51.680 --> 39:02.240
doing here, we're calling empathy is really the idea of making a lot of those non-premofacy

39:02.240 --> 39:11.680
financial aspects A, more transparent, and then B, like putting trying to make them more financial

39:11.680 --> 39:17.920
or putting numbers against them, and then incorporating them into decision-making,

39:17.920 --> 39:22.640
dashboarding them so that there's some awareness of them, and so that the organization can

39:23.360 --> 39:33.920
manage against them. It's a really interesting set of ideas around how to make this idea of AI

39:33.920 --> 39:40.000
ethics a lot more tangible. Yeah, I think yeah, yeah, tangible I think is the right word, so can

39:40.000 --> 39:47.440
we are there straightforward ways to make sure that in our customer's strategies, empathy is well

39:47.440 --> 39:51.840
represented, and we can choose to ignore it, but then there are these, as you say, these

39:51.840 --> 40:00.320
desporting, these gauges, these dials that show you, that shame you into some compliance.

40:00.320 --> 40:05.280
And also, let's not forget that, I think the reason we as humans have empathy,

40:05.280 --> 40:11.040
you know, there can be lots of different theories around that, but personally, I think that

40:11.040 --> 40:18.000
evolved, right? It evolved out of a desire to collaborate, right? So, empathy is not like a cost

40:18.000 --> 40:25.840
to the company, empathy is actually establishing your, you know, better relationship and a longer term

40:25.840 --> 40:32.800
relationship with your customers. Well, it would be interesting to kind of follow along with

40:32.800 --> 40:40.080
this work and see, you know, I'd love to hear a case study of, you know, how a customer kind of

40:40.800 --> 40:46.960
implements this end and once you've got this out in the market and have folks working with it.

40:47.760 --> 40:53.360
Yeah, I mentioned PegaWorld earlier, any besides from your own keynote, other things that

40:53.360 --> 40:58.800
you're looking forward to at the conference? Yeah, well, I mean, this will be the biggest effort,

40:58.800 --> 41:08.160
so it's always just very exciting about, you know, to show people, you know, where we are at,

41:08.160 --> 41:14.160
and it's not just about empathy, right? It's about, it's about also making decisions in general

41:14.160 --> 41:19.600
at a huge scale, you know, with this real-time AI on the one hand, and then on the other hand,

41:19.600 --> 41:23.520
and that's also part of empathy, although we didn't talk about it right now, but then

41:23.520 --> 41:30.800
following up on it, right, with the processes, right? So we are really, and you will hear a lot

41:30.800 --> 41:36.480
about that at PegaWorld, we're trying to, you know, have that combination very strongly. So it's

41:37.440 --> 41:43.520
we have the AI and a decision to decide what to do, right? And then we have sort of the end-to-end

41:43.520 --> 41:51.360
automation that will tell the company how to do it and to do it fast and efficiently, which also

41:51.360 --> 41:57.520
plays into empathy. So I really love that interplay between sort of decisions and processes,

41:58.160 --> 42:05.680
so I'm expecting a lot of really good inter discussions and presentations from our customers.

42:05.680 --> 42:10.880
Awesome, awesome. Well, Rob, I'm looking forward to seeing you once again at the event.

42:10.880 --> 42:16.000
Thanks so much for taking the time to jump on and talk this through with us.

42:16.000 --> 42:22.160
Okay, well, you're very welcome. Thank you. Thanks, Rob.

42:22.160 --> 42:27.840
All right, everyone, that's our show for today. For more information on Peter or any of the topics

42:27.840 --> 42:36.480
covered in the show, visit twimmelai.com slash talk slash 245. As always, thanks so much for listening

42:36.480 --> 42:46.480
and catch you next time.

