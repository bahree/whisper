WEBVTT

00:00.000 --> 00:29.560
All right, it looks like it is live, and let me just get the turn of volume off there.

00:29.560 --> 00:36.560
There is a delay if you, I don't recommend necessarily that you pull it up, but we'll

00:36.560 --> 00:40.640
be checking here to make sure everything's hunky-dory.

00:40.640 --> 00:44.560
But there is a delay, and it's very confusing if you have the current and the 22nd delay

00:44.560 --> 00:45.560
audio going.

00:45.560 --> 00:50.080
So if you do want to see it in 22nd delay, be sure to mute it.

00:50.080 --> 00:51.920
But we are live.

00:51.920 --> 00:58.920
Everyone, welcome to the first Twinmo Live, where we'll be talking about OpenAI's recent

00:58.920 --> 01:08.600
GPT-2 language model release and announcement and controversy and exploring the host of

01:08.600 --> 01:10.680
issues that it raises.

01:10.680 --> 01:18.880
I am Sam Charrington, your host for the discussion, and for those of you who may have stumbled

01:18.880 --> 01:25.200
across this and are not familiar with this week a machine learning in AI, it is a podcast

01:25.200 --> 01:31.560
that I launched coming up on three years ago that's really dedicated to informing and

01:31.560 --> 01:35.520
educating people about machine learning and artificial intelligence, and I've been

01:35.520 --> 01:43.080
fortunate to have very many wonderful guests, including several of our panelists today.

01:43.080 --> 01:49.080
You can find the podcast easily at twinmolei.com.

01:49.080 --> 01:55.680
So before we dive into our discussion, I'd like to give our panelists an opportunity to

01:55.680 --> 01:57.600
introduce themselves.

01:57.600 --> 02:01.360
Anima, why don't you get us started?

02:01.360 --> 02:07.560
Hi, I'm Anima Anand Kumar, Director of Machine Learning Research at NVIDIA, as well as

02:07.560 --> 02:10.920
a professor at Caltech.

02:10.920 --> 02:17.840
So thank you, Sam, for doing this, I think you're always looking at the pulse of the community

02:17.840 --> 02:26.480
and this is a topic that has garnered a lot of recent interest and thought process.

02:26.480 --> 02:31.160
So thanks for doing this and I'm happy to be part of this.

02:31.160 --> 02:37.920
I do want to make a clarification that these are my personal comments.

02:37.920 --> 02:42.800
I'm a very well coincide with my employers, but I'm not assigned to be speaking on their

02:42.800 --> 02:46.240
behalf in this particular instance.

02:46.240 --> 02:48.120
Awesome, thank you.

02:48.120 --> 02:49.120
Amanda?

02:49.120 --> 02:51.280
Hi, I'm Amanda Askel.

02:51.280 --> 02:55.080
I'm a policy research scientist at OpenAI.

02:55.080 --> 03:03.400
My background is actually in ethics and since working on all areas really still like policy

03:03.400 --> 03:04.400
here.

03:04.400 --> 03:06.400
Awesome.

03:06.400 --> 03:08.400
Rob?

03:08.400 --> 03:12.160
Yeah, my name is Miles Rendage and I'm also on the policy team at OpenAI.

03:12.160 --> 03:16.840
My background is more in social science and tech policy and I have a particular interest

03:16.840 --> 03:20.840
in malicious uses of AI and was involved in a report last year on the topic, so it's

03:20.840 --> 03:23.640
part of my interest here.

03:23.640 --> 03:24.640
Awesome.

03:24.640 --> 03:25.640
Rob?

03:25.640 --> 03:27.120
Hey everyone, I'm Rob Monroe.

03:27.120 --> 03:29.480
I'm a VP of Product at Lilt.

03:29.480 --> 03:33.880
Lilt makes technology that combines human and machine translation.

03:33.880 --> 03:39.320
My background is mixed, I've been a founder and executive of a number of AI startups.

03:39.320 --> 03:44.800
In larger companies, I ran product for AWS's first natural language processing and translation

03:44.800 --> 03:45.800
services.

03:45.800 --> 03:51.640
And before I moved here to the US to get a PhD and then I'll be at Stanford, I was working

03:51.640 --> 03:56.720
in post-conflict development in Sierra Leone and Liberia for the United Nations and I've

03:56.720 --> 04:02.160
continued to work in disaster response, both for Man and Made and Natural Disaster System.

04:02.160 --> 04:03.160
Great.

04:03.160 --> 04:04.160
Great.

04:04.160 --> 04:05.160
And Stephen.

04:05.160 --> 04:12.200
Hi, I'm Stephen Marady, most commonly known on the internet as Smarity and I'm an independent

04:12.200 --> 04:18.360
AI researcher but the primary reason I'm interesting here is that I focus on language modeling

04:18.360 --> 04:22.960
as my research area and I've held steadily out results on some of those same results

04:22.960 --> 04:28.320
as OpenAI's model and two of the data sets that they get steadily out results on are

04:28.320 --> 04:29.320
mine.

04:29.320 --> 04:30.320
Awesome.

04:30.320 --> 04:31.320
Awesome.

04:31.320 --> 04:38.160
So let's dive right in and given your focus on this area, Stephen, you would be a great

04:38.160 --> 04:41.480
person to kind of provide some context for us.

04:41.480 --> 04:46.200
What is a language model and how are they used?

04:46.200 --> 04:51.560
Why are they important and what's the kind of context, the technical context in which

04:51.560 --> 04:53.400
this announcement was made?

04:53.400 --> 04:54.400
Right.

04:54.400 --> 04:58.680
So language modeling for anyone who hasn't run across it yet.

04:58.680 --> 05:03.280
If you have your phone out and you're doing your normal typing, the predictive keyboard

05:03.280 --> 05:08.040
is essentially the time you'll run into your language models immersed.

05:08.040 --> 05:13.200
So the aim is just to guess the next word in your sequence but then you can of course

05:13.200 --> 05:17.200
go for some more complicated steps along rather than guessing words.

05:17.200 --> 05:21.920
You could guess characters or turquins but yeah, the underlying technology is literally

05:21.920 --> 05:24.200
just guess the next turquin in the sequence.

05:24.200 --> 05:26.440
What of the sequence might be?

05:26.440 --> 05:32.000
And so you've most likely run into it with your phone but it's also used in speech recognition

05:32.000 --> 05:38.600
to disambiguate some words so the word recognition itself could be ref the ignition of a cough

05:38.600 --> 05:42.200
theoretically or it could be speech recognition.

05:42.200 --> 05:43.200
So it's used there.

05:43.200 --> 05:49.880
It's also used in a number of other situations in similar contexts or abstract summarization

05:49.880 --> 05:51.480
and so on.

05:51.480 --> 05:56.840
But most recently the kind of really interesting step has been that these incredibly complex

05:56.840 --> 06:02.800
language models if you run them over enough data can basically capture a bunch of sub-tasks

06:02.800 --> 06:07.440
that you don't ask it to capture by just guessing the next word but it might end up learning

06:07.440 --> 06:08.440
that anyway.

06:08.440 --> 06:13.520
So things like counting in some models we never tell them how a model is supposed to count

06:13.520 --> 06:18.120
but it ends up doing that to be able to guess text and you can then take this model and

06:18.120 --> 06:24.000
slot it into a more complex system where it kind of this knowledge that it's already gotten

06:24.000 --> 06:27.960
just to be able to guess the next word in the sequence well ends up transferring to these

06:27.960 --> 06:28.960
other tasks.

06:28.960 --> 06:35.400
So things like sentiment analysis or question answering or translation.

06:35.400 --> 06:41.760
Awesome and so how are there some standard tests it sounds like that are used to kind

06:41.760 --> 06:44.480
of assess the performance of language models.

06:44.480 --> 06:46.560
What are some of these tests?

06:46.560 --> 06:54.760
Yeah so the absolute standard is something called complexity which is basically how confused

06:54.760 --> 06:59.040
is a language model when you tell it what the next token is.

06:59.040 --> 07:02.560
So if you're guessing and you say New York and you're going to guess the next word and

07:02.560 --> 07:07.520
you say city if I said New York state you know maybe I'm not that confused by it because

07:07.520 --> 07:14.640
I was thinking that as well but usually it's city versus if you came up with New York static

07:14.640 --> 07:19.680
or something like that some completely unexpected word then you'll see the complexities spike.

07:19.680 --> 07:24.560
So basically the only aim for language modeling is to minimize how confused the model is

07:24.560 --> 07:31.000
it having seen like a given sequence and so that's kind of the metric that open AI focus

07:31.000 --> 07:34.040
on and kind of all these previous papers have as well.

07:34.040 --> 07:35.040
Okay.

07:35.040 --> 07:43.480
And so Miles and Amanda this work with GPT2 is as the two indicates the second in a series

07:43.480 --> 07:50.440
of research into language models can you talk a little bit about the background of this

07:50.440 --> 07:58.320
project as well as the type of model specifically that it represents namely transformer models.

07:58.320 --> 08:05.440
Yeah so I mean we've been interested in sort of unsupervised learning of useful representations

08:05.440 --> 08:10.720
of text for a while in the sentiment neuron paper or blog post I believe a year or two

08:10.720 --> 08:18.960
ago was an example of sort of early interest at open AI in the first GPT paper as well.

08:18.960 --> 08:25.960
The main difference in terms of you know what GPT2 versus previous transformer based language

08:25.960 --> 08:32.520
models is scale so it's not the biggest language model that's ever been produced but as far

08:32.520 --> 08:36.800
as we know it's the best performing model along various metrics including the quantitative

08:36.800 --> 08:43.880
ones as well as sort of qualitative assessment of the quality of long text production.

08:43.880 --> 08:48.720
And specifically in terms of like size you know the range is you know from millions to

08:48.720 --> 08:53.400
billions of parameters and the one that we have chosen not to release is 1.5 billion.

08:53.400 --> 08:57.600
So there have been bigger language models previously but what's interesting here is that

08:57.600 --> 09:01.640
there's a very diverse data set being used to produce it and make use of this larger capacity

09:01.640 --> 09:08.360
of the model and the GPT sort of model is easier to sample from than say birch or other

09:08.360 --> 09:13.440
sort of recent efforts to sort of push language modeling beyond this scale.

09:13.440 --> 09:21.320
Okay and the specific type of models a transformer model what is that represent?

09:21.320 --> 09:26.280
I think probably Samarity would be a better person to answer that.

09:26.280 --> 09:30.920
Okay so the transformer model many of the audience might have heard of like recurrent neural

09:30.920 --> 09:34.400
network so RNNs and LSTM is that type of thing.

09:34.400 --> 09:38.560
The idea with that would be imagine you could only see one word on a page at a time and

09:38.560 --> 09:41.920
you only had one button which was to go to the next word.

09:41.920 --> 09:45.840
So that's the way the LSTM or recurrent neural network ends up looking at text and trying

09:45.840 --> 09:47.720
to guess the next word.

09:47.720 --> 09:51.120
And the problem with that is I don't know about the rest of the panelists but my memory

09:51.120 --> 09:56.400
is terrible so about 10 words in I'll have forgotten what everything was behind that.

09:56.400 --> 09:59.960
The idea with the transformer network is instead of having this you know step along one at

09:59.960 --> 10:00.960
a time.

10:00.960 --> 10:05.760
You say okay I have 100 words and I'm trying to guess the next word.

10:05.760 --> 10:10.040
The word at the very end can basically talk to all of the words previous to it and try

10:10.040 --> 10:14.440
and pull in some of their knowledge based on whether or not you know I should be essentially

10:14.440 --> 10:15.440
talking to you.

10:15.440 --> 10:20.640
So if I'm about to say if I the last word is president I might look back the last 100

10:20.640 --> 10:26.040
characters 100 words and I might find some other exact instances of president so a good

10:26.040 --> 10:30.880
idea there would be just to grab the next word along or you might do something more complicated

10:30.880 --> 10:33.560
where multiple words kind of have to chat to each other.

10:33.560 --> 10:39.080
And so that's the idea of this attention phase where words can look around at other words

10:39.080 --> 10:43.280
based on how you know interesting they are to the back of the word and you go through

10:43.280 --> 10:48.000
multiple stages of this and hopefully at the end all of kind of the relevant knowledge

10:48.000 --> 10:52.440
from this sequence will be captured in the very word at the end and that word can go

10:52.440 --> 10:53.440
through.

10:53.440 --> 10:54.440
Cool.

10:54.440 --> 10:58.440
I was going to say in Obama or Trump or Nixon or something like that.

10:58.440 --> 10:59.440
Awesome.

10:59.440 --> 11:00.440
Awesome.

11:00.440 --> 11:09.680
So a big part of the controversy I suppose with the release of this model was you know

11:09.680 --> 11:15.760
not so much the research itself and the technical details but kind of the way the model was

11:15.760 --> 11:16.760
released.

11:16.760 --> 11:21.720
Robin wondering if you can maybe provide some context from your perspective just reflecting

11:21.720 --> 11:25.920
on the release and the firestorm that it created at least in Twitter.

11:25.920 --> 11:28.280
I don't want to over amplify it.

11:28.280 --> 11:31.840
Twitter can be a bit of an echo chamber as we all know.

11:31.840 --> 11:37.880
But what's your take on you know some of the things that were maybe controversial about

11:37.880 --> 11:38.880
the announcement?

11:38.880 --> 11:39.880
Yeah.

11:39.880 --> 11:40.880
Happy to.

11:40.880 --> 11:44.880
And Maul's Amanda correct me if I'm characterizing up an AI wrong.

11:44.880 --> 11:50.360
So I believe that open AI decided not to make this model public which is something that's

11:50.360 --> 11:53.280
been standard recently in the research community.

11:53.280 --> 11:59.720
And the reason behind this was because the potential negative use cases outweigh the positive

11:59.720 --> 12:00.720
ones.

12:00.720 --> 12:06.320
So you could get bad actors who could use a model like this to for example generate fake

12:06.320 --> 12:11.200
news which stylistically sounded very much like a real person.

12:11.200 --> 12:19.560
And so it could be used for things like election meddling or generally creating discontent

12:19.560 --> 12:24.400
on the internet both by individual trolls or potentially state sponsored.

12:24.400 --> 12:28.160
So is that is that is that correct of an AI folk?

12:28.160 --> 12:29.160
Yeah.

12:29.160 --> 12:33.640
I mean one thing I'd clarify is that you know we did not claim nor are we confident that

12:33.640 --> 12:39.280
the out that the negative uses of GPT-2 would outweigh the risks but rather that we're

12:39.280 --> 12:44.280
not confident that they wouldn't and that you know this is sort of what seems to us like

12:44.280 --> 12:50.080
a sort of precautionary approach in this context of given the sort of you know your reversibility

12:50.080 --> 12:51.080
of release.

12:51.080 --> 12:52.560
You want to add anything to that?

12:52.560 --> 12:53.560
Yeah.

12:53.560 --> 12:54.560
Now this sounds correct.

12:54.560 --> 12:56.760
So I think it's easy to think that you have to have like really high confidence that

12:56.760 --> 13:02.320
what you're releasing is going to have negative consequences before you decide to at

13:02.320 --> 13:04.240
least do a partial release.

13:04.240 --> 13:10.280
I think our thought was that caution early is a good plan and then to try and get feedback

13:10.280 --> 13:15.520
on this approach so it might be that you know one criticism might be that this is kind

13:15.520 --> 13:21.600
of like too preemptive or too early and I think it's just that the costs of starting

13:21.600 --> 13:26.280
to think about these things early are generally lower than the costs of thinking about them

13:26.280 --> 13:31.280
too late when you are fairly confident that the misuse risk is high and so this was like

13:31.280 --> 13:36.080
some of the kind of reasoning that went behind this and then as Miles said you know deciding

13:36.080 --> 13:40.560
to do a partial release is reversible whereas deciding to do a full release is not reversible

13:40.560 --> 13:45.640
so exercising caution can mean initially doing a partial release and that was what you decided

13:45.640 --> 13:46.640
to do.

13:46.640 --> 13:50.440
Because yeah I guess that's a very thorough distinction not yet knowing rather than being

13:50.440 --> 13:52.880
confident that it was necessarily bad.

13:52.880 --> 13:56.840
Yeah and so this is like certainly a decision process that I've had to be go through many

13:56.840 --> 13:58.560
times in the past.

13:58.560 --> 14:08.320
So in disaster response in Chinabaspring in particular thinking about what kinds of data

14:08.320 --> 14:13.720
were being collected and how for example if you take a tweet of someone reporting a blocked

14:13.720 --> 14:19.560
road they don't know why it's blocked but then all of a sudden you recontextualize that

14:19.560 --> 14:24.800
and your reporting that there are rebel movements in an area and now that the rebels know

14:24.800 --> 14:27.440
that you've reported them.

14:27.440 --> 14:32.440
So if you're very careful about taking other people's data and releasing that and certainly

14:32.440 --> 14:37.360
you know build in a model on open data like overnight I did and then we release in that

14:37.360 --> 14:42.000
accounts as recontextualized as in.

14:42.000 --> 14:49.000
Similarly I've seen very full more propaganda from a cost radio through to social media

14:49.000 --> 14:53.440
used in in Sierra Leone Liberia country studies to live in and I've worked in a lecture

14:53.440 --> 14:57.960
monitoring in during their elections there and even when there weren't state sponsored

14:57.960 --> 15:03.800
actors made decisions like this as well as responding to earthquake in Haiti in 2010

15:03.800 --> 15:07.880
and a centralized data which machine learning scientists can build on.

15:07.880 --> 15:13.080
We deliberately admitted all the data which were reported on a company minus children

15:13.080 --> 15:18.840
who are alone and for this very reason we believed that the potential negative use cases

15:18.840 --> 15:23.040
was something we couldn't protect well for and so this wasn't something that we wanted

15:23.040 --> 15:24.640
to make available.

15:24.640 --> 15:28.240
So I appreciate that decision process.

15:28.240 --> 15:36.040
To characterize that the alchrist I think that there's two aspects to it one at least

15:36.040 --> 15:40.560
me at least I didn't see the context in how this decision was made it felt a little bit

15:40.560 --> 15:44.720
buried in the paper and it probably deserved more space in the article.

15:44.720 --> 15:48.520
Hope I'm preaching to the converted because we've got open AI's ethics people here not

15:48.520 --> 15:54.280
there other machine learning people and I think the other which is less than me to speak

15:54.280 --> 16:01.240
about I call myself a practicing researcher now is that the paper very proudly reported

16:01.240 --> 16:06.800
new state of doubt results for a model that wasn't then immediately available to the research

16:06.800 --> 16:09.600
community to replicate.

16:09.600 --> 16:16.640
Yeah, I can add one point on sort of the norm that you mentioned around openness I think

16:16.640 --> 16:21.680
it's important to sort of be clear that it was not the case that before this everyone

16:21.680 --> 16:25.840
always release all their state of the art models all the time it's rather that it was

16:25.840 --> 16:31.520
rarely or you know it was rarely the case that people would explicitly use sort of misuse

16:31.520 --> 16:36.680
of the model of the sorts and of the source that we're worried about as a justification

16:36.680 --> 16:42.040
as opposed to profit or sort of you know keeping things underrapener to have a big announcement

16:42.040 --> 16:46.040
or something like that so it's the motivation for non-release that I think is distinctive

16:46.040 --> 16:49.360
as opposed to that we didn't publish everything.

16:49.360 --> 16:53.800
Was there a big recent paper in language modeling where they didn't release?

16:53.800 --> 16:58.400
So I'm thinking of the big recent ones kind of an international order being laser from

16:58.400 --> 17:04.760
Facebook, Bert from Google and Elmo from Alan Institute.

17:04.760 --> 17:09.240
I believe they all release their models where other others are not aware of.

17:09.240 --> 17:13.080
Yeah, so I was speaking more about AI generally it might be the case that there's more of a

17:13.080 --> 17:17.400
tendency towards publication in the language model literature specifically.

17:17.400 --> 17:20.920
Right, right.

17:20.920 --> 17:29.600
And so part of the issue that this raises is around reproducibility and I'm sure we'll

17:29.600 --> 17:32.720
come back to that in the conversation.

17:32.720 --> 17:38.640
Anima you've also raised some issues around the kind of the way it's been handled from

17:38.640 --> 17:41.800
just a reporting and media relations perspective.

17:41.800 --> 17:43.640
Can you elaborate on those?

17:43.640 --> 17:50.000
Yeah, I mean, you know, I certainly understand that it's important to think about malicious

17:50.000 --> 17:55.400
use cases and all the impact of releasing a certain technology, right.

17:55.400 --> 18:02.080
So I'm appreciative of the thought process that goes into the risk analysis but as Miles

18:02.080 --> 18:08.320
pointed out that's never been the reason for not releasing a language model before.

18:08.320 --> 18:14.720
So that's, you know, suddenly like where the new risk is coming up that didn't come up

18:14.720 --> 18:19.640
with the earlier models which are nearly as good if not as good.

18:19.640 --> 18:23.920
And also the claim that this is much better than the previous models is not even something

18:23.920 --> 18:30.120
the research community has verified or can easily verify without access to the models.

18:30.120 --> 18:36.080
So there is the issue of like truly understanding what the capabilities of the most recent

18:36.080 --> 18:43.240
model are and having independent researchers evaluate that effectiveness.

18:43.240 --> 18:49.200
And before all that's done, it felt like the media was at the center stage of all this,

18:49.200 --> 18:50.200
right.

18:50.200 --> 18:56.520
The access was only given to journalists and in a very limited way to write articles for

18:56.520 --> 18:57.520
the public.

18:57.520 --> 19:03.520
So before it reached the research community before there was any chance to evaluate its

19:03.520 --> 19:13.000
technical capabilities, you know, there was these huge like kind of media blitz on, you

19:13.000 --> 19:18.320
know, the terminator coming, it's gotten so dangerous that AI needs to be locked up in

19:18.320 --> 19:25.240
a wall to these kind of articles, you know, promoted a lot of fear mongering that's already

19:25.240 --> 19:28.960
been present in some of these general media articles.

19:28.960 --> 19:35.960
And that's where this distortion of the scientific facts and the current capabilities that

19:35.960 --> 19:38.520
I have big issues with.

19:38.520 --> 19:39.520
Okay.

19:39.520 --> 19:47.640
And another issue that's been raised along the lines of the reproducibility concern has

19:47.640 --> 19:55.400
been one about open source and the model's source code being open.

19:55.400 --> 19:59.720
Steven, is that one that you have a daughter?

19:59.720 --> 20:07.440
Yeah, so the idea that the model being open, that's been a pretty popular idea for many

20:07.440 --> 20:09.640
of the AI research labs.

20:09.640 --> 20:11.880
As Miles mentioned, it isn't the default.

20:11.880 --> 20:16.200
There are many kind of papers that don't publish their models or don't publish that occurred.

20:16.200 --> 20:22.480
But you know, one of the main ideas behind kind of this entire field is the open nature

20:22.480 --> 20:23.480
of our work.

20:23.480 --> 20:25.600
We've published the papers on the archive.

20:25.600 --> 20:27.680
There's no payment to get the paper.

20:27.680 --> 20:34.160
The techniques are generally very well learned by everyone involved and there are free toolkits

20:34.160 --> 20:36.000
or frameworks.

20:36.000 --> 20:40.880
And to this stage, you know, Google's collaboratory or what have you, they can give you free

20:40.880 --> 20:42.040
GPUs.

20:42.040 --> 20:45.880
This is a great idea because, you know, basically anyone can get involved.

20:45.880 --> 20:49.760
If someone would like to, they could take open AI's language model or, you know, one

20:49.760 --> 20:55.520
from Google and video and test and see how it works, potentially use it for the application.

20:55.520 --> 21:01.000
So open, reproducible research, it kind of hits all those lines down, whether or not

21:01.000 --> 21:03.880
someone can take the model and improve it, whether they can take it to use it for an

21:03.880 --> 21:09.520
interesting application, whether they can just explore the current capabilities as, you

21:09.520 --> 21:13.080
know, maybe a researcher trying to understand the latest advances.

21:13.080 --> 21:15.080
Yeah.

21:15.080 --> 21:19.480
And at some point, I'll come back to kind of my also perspective on, well, basically

21:19.480 --> 21:24.880
like I love the kind of discussion and the open AI model itself.

21:24.880 --> 21:28.760
I'm, you know, always interested in language modeling research, but I feel like one issue

21:28.760 --> 21:30.640
was that everything kind of came together.

21:30.640 --> 21:32.520
It was a new language model.

21:32.520 --> 21:37.200
It was discussions about responsible disclosure, how journalists react to AI research and

21:37.200 --> 21:40.400
publication and then how the general media consumes it.

21:40.400 --> 21:44.720
I think that was one of the main things kind of powering this confused firestorm on Twitter

21:44.720 --> 21:46.160
and potentially in media.

21:46.160 --> 21:49.760
Yeah, Amanda and Miles, you're shaking your heads.

21:49.760 --> 21:52.800
We agree, we agree.

21:52.800 --> 21:56.160
Thoughts from, from you.

21:56.160 --> 21:57.160
You're muted.

21:57.160 --> 21:59.440
That was my fault.

21:59.440 --> 22:00.440
Yeah.

22:00.440 --> 22:02.880
Like, there's a lot of points being raised.

22:02.880 --> 22:08.600
I think one thing, I just want to go back to a point that was made earlier, where there's

22:08.600 --> 22:12.400
this kind of question of like, well, it's like, you know, we've seen lots of really impressive

22:12.400 --> 22:17.120
language models before, the way that this was presented was like, as if this is some

22:17.120 --> 22:21.120
kind of like changing kind, you know, what's this new risk that's arising that wasn't

22:21.120 --> 22:22.960
arising before?

22:22.960 --> 22:29.600
And I think on this, one thing that's worth noting is just that if machine learning is

22:29.600 --> 22:34.400
an area where you see incremental progress, one concern you might have is the point at

22:34.400 --> 22:40.960
which it makes sense to do something like partial disclosure of research is always going

22:40.960 --> 22:44.480
to kind of look like the wrong point, because it's going to be this sudden shift from

22:44.480 --> 22:49.840
full disclosure of models in the case of like language models to like a partial release,

22:49.840 --> 22:52.960
like what we did here.

22:52.960 --> 22:56.880
But if it's the case that you always just have these like incremental improvements, and

22:56.880 --> 23:01.440
this is like an example of it, there might not be some huge shift.

23:01.440 --> 23:05.960
It might not be that you saw some like completely new potential misuse.

23:05.960 --> 23:09.040
It's just that at some point you have to make the decision.

23:09.040 --> 23:14.640
So I guess I want to say like it doesn't, we weren't intending to imply that it was like

23:14.640 --> 23:15.640
something that it wasn't.

23:15.640 --> 23:19.040
I think we were very explicit in the blog post and in the paper about like the nature

23:19.040 --> 23:23.600
of the research and where it sat in relation to other research, but it's hard to, you

23:23.600 --> 23:27.040
know, convey that well also deciding that you're going to do a slightly different

23:27.040 --> 23:29.040
release type from what's happened before.

23:29.040 --> 23:32.800
Yeah, I mean, we did try and, you know, be clear that we were talking both about, you

23:32.800 --> 23:37.120
know, GPT-2 as well as language models in general, but I think we could have been a lot

23:37.120 --> 23:42.960
clear about sort of what are the threats of GPT-2 raw versus GPT-2 fine-tune versus,

23:42.960 --> 23:45.920
you know, GPT-3 or, you know, someone reproducing it.

23:45.920 --> 23:50.640
So, and like what are the sort of, you know, domains or skill levels required for these

23:50.640 --> 23:51.640
different things?

23:51.640 --> 23:54.960
So this is something that we plan to be a lot more transparent about in the future about

23:54.960 --> 24:00.240
like why would we do this and what are the trade-offs involved and sort of, you know, what

24:00.240 --> 24:03.920
was, what were the options we considered and why did we not do the things that people

24:03.920 --> 24:06.400
are saying, yeah, we should do now.

24:06.400 --> 24:08.880
Nima, you're reacting to this.

24:08.880 --> 24:16.360
I think I'm kind of worried when it's, you know, when Amanda said that, you know, just

24:16.360 --> 24:21.000
because it's incremental, doesn't mean we'll, you know, that at some point we should stop

24:21.000 --> 24:24.080
releasing or only do this partial release, right?

24:24.080 --> 24:30.360
And that's what I'm worried about if the community is moving towards, away from openness

24:30.360 --> 24:38.200
and to close setting just because one day we suddenly feel there is a threat and even

24:38.200 --> 24:41.400
if there is, it's not going to help, right?

24:41.400 --> 24:46.720
Because there's already so much available in the open and it's so easy to, you know,

24:46.720 --> 24:52.760
go look at these ideas and including the blog post and the paper from OpenAI and reproduce

24:52.760 --> 24:53.760
this.

24:53.760 --> 24:59.560
I think it was Stephen who commented on Twitter about the kind of resources it takes for

24:59.560 --> 25:04.280
a bad actor to truly reproduce if they wanted to and it's not a lot, right?

25:04.280 --> 25:10.880
So, you know, so it's not really stopping the bad actors and these malicious use cases

25:10.880 --> 25:18.920
because of this, you know, partial release and holding back this full scale model, but

25:18.920 --> 25:23.560
what it's, who it's truly hurting are the academic researchers.

25:23.560 --> 25:28.360
You know, the students, the junior researchers with the least access to the resources, you

25:28.360 --> 25:32.880
know, the marginalized communities, maybe, you know, people across the world where there

25:32.880 --> 25:34.880
is less compute infrastructure.

25:34.880 --> 25:38.920
I mean, they cannot easily reproduce the resources.

25:38.920 --> 25:45.000
That will, it'll take them a lot more to go and reproduce and then do for the research.

25:45.000 --> 25:50.120
So it's hurting the research community a lot more and almost doing nothing to stop the

25:50.120 --> 25:52.840
malicious use cases, in my view.

25:52.840 --> 26:00.120
I'd like to interject with a question from a user on YouTube, G23, who asks, it would

26:00.120 --> 26:04.160
be possible and this is asking maybe a bit of a theoretical, but would it be possible

26:04.160 --> 26:09.680
to establish some kind of partnership program so that researchers kind of vetted researchers

26:09.680 --> 26:15.560
could get access to this to this work without fully making it open.

26:15.560 --> 26:18.920
Is that something that OpenAI has considered?

26:18.920 --> 26:24.800
Yeah, so absolutely, and you know, there's an email address on the blog, the original

26:24.800 --> 26:29.680
blog post where you can suggest both, you know, specific use cases that you're interested

26:29.680 --> 26:34.480
in as well as ideas around, you know, alleviating these trade-offs in terms of access versus

26:34.480 --> 26:36.240
limiting this use.

26:36.240 --> 26:41.600
To comment briefly on Anima's point around openness, I totally agree there are tons of benefits

26:41.600 --> 26:45.840
of openness and it's been, you know, the benefits of openness, if anything, have become

26:45.840 --> 26:50.000
more acute to us through this process because it's frustrating, you know, making claims

26:50.000 --> 26:55.320
that people are saying, you're unsubstantiated and trying to sort of, you know, persuade

26:55.320 --> 26:59.120
people that, you know, we're not making this up that there are actually these capabilities.

26:59.120 --> 27:03.320
So it's super, you know, frustrating to have to deal with that trade-off and, you know,

27:03.320 --> 27:08.080
that we, it's quite possible that we made the wrong choice that, you know, we should

27:08.080 --> 27:11.200
have been thinking more, I mean, we did think, to some extent, we should have been thinking

27:11.200 --> 27:17.240
more about sort of the low, you know, low compute, you know, asserts of actors, you know,

27:17.240 --> 27:21.220
people in developing countries and so forth who, you know, could only get access to this

27:21.220 --> 27:22.220
through a pre-train model.

27:22.220 --> 27:26.320
But it's, I think, it's also quite plausible and I hope it's the case we might the right

27:26.320 --> 27:30.640
call and having a sort of reading period to have this conversation and start thinking

27:30.640 --> 27:35.800
more critically about defenses and coordination around these topics will actually be an

27:35.800 --> 27:36.800
up benefit.

27:36.800 --> 27:47.440
I mean, I think we're thinking a lot about these considerations, so things like are there

27:47.440 --> 27:52.400
ways that we can give access to this kind of work to academics who want to work on it,

27:52.400 --> 27:58.480
for example, also are there ways of interacting like across industry or like, you know, people

27:58.480 --> 28:03.080
suggested a kind of partnership? We're interested in exploring all of those ideas, and I think

28:03.080 --> 28:09.320
one of the purposes of sort of starting a conversation here was to get a lot of that out on the table

28:09.320 --> 28:13.480
and not to say something like, oh, we want to just take action on our own and decide to close things

28:13.480 --> 28:20.040
off. The goal was really to start a conversation around this and get feedback. Not to say something

28:20.040 --> 28:25.400
like, yes, we're just like, we think that we can like prevent misuse by simply closing up research

28:25.400 --> 28:29.560
or something like that. That's like not the intention. So just like earlier points that people

28:29.560 --> 28:35.480
read. Yeah, one of the things that it confused me a little bit about the conversation that as I

28:35.480 --> 28:40.680
was following it on Twitter was there seemed to be, and it came up in this conversation as well,

28:40.680 --> 28:47.080
some suggestion that the models themselves weren't particularly novel, and I guess part of the

28:47.080 --> 28:55.240
issues that we can't really know, but Jeremy Howard, for example, seemed to suggest that the models

28:55.240 --> 29:00.280
were, you know, the code is out there to do what OpenAI did. They just did it at a scale that

29:00.280 --> 29:07.800
no one has done before. I wonder if anyone has a take on that. I suddenly do it if I can jump in.

29:09.320 --> 29:15.800
Much of the model is really quite the same as OpenAI's previous release of GBT,

29:16.760 --> 29:23.240
and the main thing I kind of refer to is scale it till it breaks models, where you just take

29:23.240 --> 29:27.000
an existing model and you ask that, you know, interesting question, because this is something

29:27.000 --> 29:30.680
you can ask with machine learning, if you just keep scaling the model up and keep throwing in more

29:30.680 --> 29:36.360
data, does the behavior of the model change substantially? And so that's really the question that

29:36.360 --> 29:41.800
the OpenAI team were asking, not necessarily like, you know, we have a new WhizBang model underneath

29:41.800 --> 29:47.480
the surface. But because of that, that also does, you know, raise some interesting questions,

29:47.480 --> 29:52.840
along with the fact that OpenAI released the code immediately, because in terms of kind of

29:52.840 --> 29:59.320
responsible disclosure for this, anyone can kind of reproduce the research, either with their

29:59.320 --> 30:05.240
existing code or with previously released code. And I think Anima mentioned, I kind of crunched

30:05.240 --> 30:10.440
numbers. It's about $43,000, but it's suddenly not cheap, but for, you know, a state actor or

30:10.440 --> 30:15.240
someone else like that, or a substantially large company, it's suddenly within, you know, their

30:15.240 --> 30:20.120
ability to do so. But the model itself hasn't really strongly changed. And so it's more a question

30:20.120 --> 30:27.160
of, I guess, capabilities when you scale models up to this size. Rob, how about you, any thoughts

30:27.160 --> 30:32.520
on that? Yeah, I mean, I actually like to go back to the question of limited release as well.

30:32.520 --> 30:39.080
So most research institutions have some form of IRB, all in the U.S. too, so ethical review

30:39.080 --> 30:44.840
boards. Who do exactly this? And so when I was doing my PhD, some of the data I looked at

30:44.840 --> 30:50.680
was health care messages in the general language of Malawi. And because they contained PII,

30:50.680 --> 30:56.120
I had to go through a review process, both here in the USA and then also in Malawi to get approval

30:56.120 --> 31:00.200
to use this data and, you know, promise to treat it carefully, delete it when I know long

31:00.200 --> 31:05.640
it needed it. So a lot of these processes are already in place. People in other scientific

31:05.640 --> 31:10.680
disciplines, especially biological and medical ones, routinely have to go through this process.

31:10.680 --> 31:17.960
And yeah, I think that already exists for a lot of AI researchers. And that kind of takes me

31:17.960 --> 31:23.400
into the point that I wanted to make because, yeah, to Steven's point, these models continually get

31:23.400 --> 31:28.840
better with more and more data. But we don't have more and more data for most of the world's languages.

31:28.840 --> 31:33.960
So I think the way that OpenAI differ from all the other language models that have been released

31:33.960 --> 31:39.080
recently is that it really only looked at English. It did some look at novel translation

31:39.080 --> 31:46.200
between English and French. But when you look at Bert, you know, a month ago, I went for that

31:46.200 --> 31:52.200
Facebook. It's a few weeks ago. Bert and laser from Facebook had a hundred different languages.

31:53.080 --> 31:59.480
So English, you know, it's only constitutes 5% of the world's conversations daily. It's the

31:59.480 --> 32:04.840
most privileged language in the world. And it's the language for which it's most easy for us to

32:04.840 --> 32:10.120
fight fake news right now because we have AI that can identify fake versus real news. We have

32:10.120 --> 32:17.960
teams of people at the different social media companies doing this. And so for me, rather than fake

32:17.960 --> 32:25.480
news or killer robots or other things that your employer might be worried about, OpenAI, it's

32:25.480 --> 32:30.360
inclusion in AI, which I think is the biggest ethical problem that we're facing right now.

32:30.360 --> 32:37.400
And if these models are any working at a scale that we have for English, then even the software

32:37.400 --> 32:44.120
component, the algorithms don't matter. They're not going to be able to be used for 99% of the world's

32:44.120 --> 32:53.320
languages. So I'm really curious that, you know, if this model kind of model won't even work for

32:53.320 --> 32:57.000
the majority of the world's languages, where if I didn't fake news is the hardest right now,

32:57.000 --> 33:01.080
because that data simply doesn't exist. Why, why, why are we particularly concerned then about

33:01.080 --> 33:09.720
the OpenAI English-only model compared to others? Miles, you have a thought on that?

33:10.520 --> 33:15.800
Yeah, I mean, yeah, so in terms of the sort of biased question and

33:15.800 --> 33:19.320
representiveness around the language, I think that it's definitely something we've considered

33:19.320 --> 33:24.760
in addition to other sort of more subtle or non-obvious risks. And you know, certainly we

33:24.760 --> 33:29.320
forward grounded the malicious use risks of sort of people deliberately using this, but that's

33:29.320 --> 33:33.720
also something we need to consider in terms of, you know, what is the consequence of releasing

33:33.720 --> 33:41.080
these models and, you know, sort of bias around sex and race is another thing that we've considered

33:41.080 --> 33:46.840
as a reason for caution. So, you know, that's not to say that we wouldn't, if we had, if we were,

33:46.840 --> 33:53.000
if we had no other concerns besides, you know, the English, you know, bias, would we so release it?

33:53.000 --> 33:57.160
I don't know, that's an interesting question. I think one of these, I think one of these

33:57.160 --> 34:02.920
correlates, too. So, obviously, like, language and race correlate strongly, but in some cases,

34:02.920 --> 34:10.440
the more closely intertwined. So, your race through a lot of Latin America is determined more

34:10.440 --> 34:16.280
by the language, you speak, than your actual biological ethnicity. And there is a huge

34:16.280 --> 34:21.400
gender bias there, too. So, in a lot of the world, you're more likely to have more education

34:21.400 --> 34:28.360
and be taught a privileged language if you're raised male than you are female. So, these really

34:28.360 --> 34:35.160
do correlate strongly with each other. And also, not to single you out. Even though these other

34:35.160 --> 34:39.560
models have been released in more languages, that's missing from their valuations as well.

34:40.360 --> 34:43.560
So, I think their, the first model to really get a lot of publicity in the machine learning

34:43.560 --> 34:49.960
community was the alumni model out of the Allen Institute. So, they wanted best paper award,

34:49.960 --> 34:54.360
I think two years ago, maybe last year, at one of the big competition linguistics conferences.

34:55.080 --> 35:00.360
They've evaluated NDD recognition, so identifying the names of people, places, and locations.

35:00.360 --> 35:03.880
They evaluated a multilingual data set, which was both English and German,

35:03.880 --> 35:07.720
but you feel they're paper, then you have one set of results. And then you say this,

35:07.720 --> 35:11.560
you just have to infer that they know all the German data and only evaluated the English.

35:12.440 --> 35:15.880
And then the birth paper at a Google did exactly the same. They reported a new set of

35:15.880 --> 35:21.640
the art on this data set, which is called a multilingual data set, but reported only to English

35:21.640 --> 35:27.000
results. And English and German are basically as close to related as any two languages can be.

35:27.000 --> 35:30.280
They're like from the same language family, they're a lot of cognates.

35:31.080 --> 35:34.440
Enough played around with the Elmo model, and it doesn't get into a near state of the art

35:35.480 --> 35:40.280
for this German data set. Bird gets a little bit better, but again, not state of the art.

35:40.280 --> 35:46.920
And so I worry a little bit then, you know, to what extent have those researchers, or ones at OpenAI,

35:48.200 --> 35:53.320
given that the imperative of always having state of the art, have they tried this in other languages,

35:53.320 --> 35:57.800
maybe something as close related to English as German, they didn't get state of the art results,

35:57.800 --> 36:03.960
and as a result, they brushed it under the carpet rather than sharing a really important negative result.

36:06.360 --> 36:09.000
Steven, what do you have some thoughts on the language issue?

36:09.000 --> 36:16.200
Yeah, so one of the kind of proof of concepts that's in the OpenAI paper, but we've also seen

36:16.200 --> 36:22.200
similar strands of research across the community, is kind of twofold. One is that unsupervised

36:22.200 --> 36:28.200
language models substantially help translation. That's kind of an obvious. But in this situation,

36:29.080 --> 36:33.560
the OpenAI team were actually purposely stripping out than just retaining just English.

36:33.560 --> 36:38.360
And for one of those reasons is that the data sets they were comparing against primarily English.

36:39.560 --> 36:43.640
But they ended up kind of accidentally leaving, I think it was 10 megabytes or so of

36:43.640 --> 36:49.560
French in there. And these were kind of like, I wish I knew more French, but like Bonjour,

36:49.560 --> 36:55.160
means hello in English, like as a sentence. And the language model, it does,

36:55.160 --> 36:59.400
a reason I mentioned this as a proof of concept, they obviously tried to strip out as much

36:59.400 --> 37:03.560
of the languages they could, but it ended up with some remaining in there. But even from that small

37:03.560 --> 37:08.680
amount, translating from French to English did reasonably well. And there's reasons for that,

37:08.680 --> 37:12.600
you know, the language model itself has just been learning what English looks like. And so from

37:12.600 --> 37:18.440
even a few examples of French, I can say, well, frequently these go across. But the next stage up,

37:18.440 --> 37:25.800
which is kind of the broader community, there are many efforts to have unsupervised translation

37:25.800 --> 37:33.880
between languages. And I think you made reference Rob to laser beforehand. And the beautiful thing

37:33.880 --> 37:38.760
about this is that by helping, say, translate from English to German, which are very similar,

37:38.760 --> 37:43.080
but have, you know, at least a few, I guess, rules in terms of changing around the orders of things,

37:43.080 --> 37:49.080
or, you know, different ways in which words combine. You can take those same kind of learnings

37:49.080 --> 37:56.760
for this model and transfer it to a very resource-low language, and still have that transition across.

37:56.760 --> 38:02.120
Now, it is a completely fair point that it hasn't worked. The opening I team for their language model,

38:02.120 --> 38:07.880
here hasn't applied it to, you know, further languages. But one thing which I kind of personally

38:07.880 --> 38:13.560
have some, this is almost unrelated experience to me, I released a language model sometime back

38:13.560 --> 38:21.480
called the AWDLSTM. The fast.ai team took it and then have it as a kind of underlying basis,

38:21.480 --> 38:27.320
now it's been immensely modified, the underlying basis originally for the language model ULM fit.

38:27.320 --> 38:33.160
And the fast.ai community have then ported this to dozens of different languages. And kind of

38:33.160 --> 38:38.760
the really fun thing for me is I was mainly focused on English. I should probably expand my language

38:38.760 --> 38:44.040
modeling vocabulary, even if I don't know the languages myself. But the code that I wrote because

38:44.040 --> 38:48.680
it was general and your machine learning does this, transfer very well across these other languages.

38:48.680 --> 38:53.160
And I'm aware of at least seeing that the transform model has been able to do this quite successfully

38:53.160 --> 39:00.360
in the past. So I'd expect, naively, the opening I model to have the same sorts of advantages.

39:01.880 --> 39:08.200
That past, definitely true. So the transform model has been a lot more successful across

39:08.200 --> 39:16.200
languages than the R&N LSTM based methods. And then it actually comes down to the reason that

39:16.200 --> 39:22.120
Stephen introduced initially. And that R&N LSTM based model is really only looking at one word at

39:22.120 --> 39:26.200
a time, enough to pass that memory all the way through, rather than being smarter about finding

39:26.200 --> 39:31.720
long distance relationships. And so English is a complete outlier in terms of how important word

39:31.720 --> 39:36.520
order is. It's more common in most languages, that the subjective verb in the object,

39:36.520 --> 39:40.280
the subjective verb by the suffixes or the prefixes. They go in those words and the words can be

39:40.280 --> 39:45.160
in any order. And so this is, one of the things that is a little bit problematic about a lot of

39:45.160 --> 39:50.920
these results is that testing only on English, which is it's not in the middle, it's an outlier,

39:52.040 --> 39:56.440
how important word order is and then standardized bellings are and then lack of suffixes.

39:57.160 --> 40:00.440
It doesn't really tell you about how it's going to do more broadly.

40:01.000 --> 40:03.960
It's certainly the machine translation community and that's what I'm addressing my company right

40:03.960 --> 40:08.440
now. The transformer based methods are really blowing the R&N based methods out of the water.

40:09.480 --> 40:14.680
But that's not so clear in a lot of the language models recently, even if they've been released

40:14.680 --> 40:21.320
in multiple languages, only been evaluated in English. And so I think it's representing a real gap

40:21.320 --> 40:26.280
in the knowledge that people like me and industry can take from the research community.

40:27.800 --> 40:33.560
So a question I've got for Anima is really about kind of the true capabilities of these

40:33.560 --> 40:40.360
types of models. I think looking at the sample that was released in the blog post,

40:42.920 --> 40:47.880
with this model, you can provide a prompt and the output is conditioned on this prompt. And so

40:47.880 --> 40:54.360
the prompt was something about scientists discover unicorns and there's a rather long and

40:54.360 --> 41:00.920
rather coherent text about the backstory of the scientific discovery. It was rather impressive to

41:00.920 --> 41:10.360
me. Are you equally impressed? Where do you think this fits in the broad scheme of

41:10.360 --> 41:14.760
capability of these types of models? I think that particular example,

41:17.720 --> 41:23.480
I think you could argue that it furthers the whole AI boogie man terminator thing. It's

41:23.480 --> 41:31.400
particularly unexpected or at least I found it particularly unexpected. You know, do you think

41:31.400 --> 41:36.840
and we can we'll ask open AI as well like and I think actually to be fair in the blog post,

41:36.840 --> 41:42.600
I said this was I think they said this was an example that was selected out of 10 or something

41:42.600 --> 41:50.920
with that prompt. But to what extent are these types of examples cherry picked? You know,

41:50.920 --> 41:57.880
what does it say about kind of where we are in this, you know, the path towards, you know,

41:57.880 --> 42:01.720
some AI outcome that we don't fear? The thing that we're talking about here that we fear and that

42:01.720 --> 42:08.440
we're kind of not disclosing because we fear, you know, how close are we? And you know, and a

42:08.440 --> 42:13.880
short answer is, right, I can't really tell without knowing all the details about the model,

42:13.880 --> 42:21.800
right, and having to the model. I mean, any researcher would I think comment that as like a

42:21.800 --> 42:28.680
one-line answer, right? But more, I guess importantly, the issue is like, as you said, not one

42:28.680 --> 42:34.120
example. Like, you know, the question of like not just like how well it's doing on some cherry

42:34.120 --> 42:38.760
picked examples, but also what the failure modes were. Like, what did the others look like?

42:38.760 --> 42:44.920
Or was it completely incoherent or was it like diverse enough? Was it doing the same thing

42:44.920 --> 42:51.080
over and over again? I mean, these are all questions we ask for when we try to evaluate the models,

42:51.080 --> 42:55.720
right? I mean, we can look at quantitative measures like for complexity, but that's, you know,

42:55.720 --> 43:00.280
like not enough by itself, right? Like, you know, so that's, I mean, there is no easy way to

43:00.280 --> 43:06.440
evaluate unsupervised learning, right? That's a general philosophical question. Like, what does it mean

43:06.440 --> 43:14.280
to have done unsupervised learning well? Because with supervised learning, we have a notion of accuracy,

43:14.280 --> 43:20.600
okay, you get 100% accuracy on your unseen test dataset, then you're, you know, really amazing,

43:20.600 --> 43:26.360
right? But even there are the limitations because you may want to go beyond the test dataset and

43:26.360 --> 43:33.720
their further issues. Whereas with unsupervised learning, the question is, what is a good model?

43:33.720 --> 43:39.400
Like, you know, when, when do we say that this, you know, the answer that you obtained that you're

43:39.400 --> 43:45.080
happy with? Was it because that it was coherent enough? Or did you want it to have certain

43:45.080 --> 43:50.520
factual reasoning? Did you want it to go through a certain logical set of steps?

43:50.520 --> 43:55.960
Right? What would mean it to be impressive? Right? One from a human evaluation perspective,

43:55.960 --> 44:01.800
that other from a more quantitative perspective, it is hard to tell and that's why this is

44:01.800 --> 44:08.920
an open research topic that the community thinks a lot about on how to evaluate language models.

44:10.760 --> 44:16.520
And that's the reason we need the research community to be very much embedded in discussing

44:16.520 --> 44:24.440
this model and getting access to it. Miles, a reaction of that? Yeah, I mean, I totally agree that

44:24.440 --> 44:29.080
it's much easier to evaluate their capabilities with more access and it's a very acute trade-off

44:29.080 --> 44:34.680
that we're trying to navigate. I mean, one point that Anima raised was that, and you know,

44:35.480 --> 44:39.960
very much on point, is that it was not obvious what the specific sort of threats we were concerned

44:39.960 --> 44:44.840
about were and, you know, and more generally where to draw those lines. So, and you know, this is

44:44.840 --> 44:48.600
still something we're thinking through and planning to share more about our process around sort of

44:48.600 --> 44:54.760
threat modeling and evaluating these capabilities, but just to get some intuition. You know, first of all,

44:54.760 --> 45:00.440
you know, this is not just about GPT-2, but also language models in general. So, all of this should

45:00.440 --> 45:04.680
be sort of taken with the grain of salt that we don't know exactly where the biggest threats are

45:04.680 --> 45:09.080
and how quickly things will develop from here. But roughly, we have, you know, a few sort of

45:09.080 --> 45:14.360
tiers of, you know, sources of information that we draw on. We look at how things are being used

45:14.360 --> 45:19.240
in the wild, like what is actually the situation with fake news and, you know, where the bottlenecks in

45:19.240 --> 45:23.320
terms of text reduction and so forth. So, that's one set of perspectives. It's like, what is the

45:23.320 --> 45:28.680
role of, you know, text in society and what are the defenses against mass produced or misleading

45:28.680 --> 45:33.560
text. And then there's sort of in-house analysis, you know, through, you know, both sort of

45:33.560 --> 45:38.680
formally doing science as well as informally, you know, allowing people access to the model,

45:38.680 --> 45:43.560
including non-experts within the opening eye organization. So, that's where some of the

45:43.560 --> 45:48.600
samples came from for the blog posts were sort of non-expert users playing out with an interactive

45:48.600 --> 45:52.920
version of the system as opposed to, you know, like Alec Radford and Jeff, we were trying to come up with

45:52.920 --> 45:58.120
the most impressive possible example. So, but it's, I, we agree that, you know, from the text,

45:58.120 --> 46:03.240
it's not obvious that that's the case. Yeah. So, in terms of threat modeling, you know, it's

46:03.240 --> 46:08.520
important to think about, you know, what can we do in-house? What can we do with a given level of skill

46:08.520 --> 46:13.560
as well as, you know, what would fine-tune variations of the system? You know, we gave the example

46:13.560 --> 46:19.000
of Amazon Reviews as one example where we've looked at fine-tuning and we're able to realize that

46:19.000 --> 46:24.200
it was quite tractable, but we're still thinking through, you know, what is the sort of suite of,

46:25.080 --> 46:28.520
you know, questions you should ask about powerful language models.

46:30.760 --> 46:37.400
Awesome. So, Anima, a question for you from Connor on YouTube, maybe pushing back a little bit,

46:37.400 --> 46:43.000
do you see any limits with respect to the types of models or with respect to releasing models?

46:43.000 --> 46:48.680
Are there any societal considerations that an AI scientist should make in creating

46:49.560 --> 46:54.520
or responsibilities that they bear in releasing their models? Where would you draw the line?

46:55.640 --> 47:01.480
I mean, certainly, I think every scientist should think about societal impacts, right?

47:02.200 --> 47:08.120
In, you know, in any discipline, I think we should all be mindful of the impact we have

47:08.120 --> 47:14.840
through the deployment of technologies we release. But at the same time, we need to ask ourselves,

47:14.840 --> 47:21.640
if I'm limiting a certain technology, what are the trade-offs? Like, you know, there's both the

47:21.640 --> 47:29.560
benefits and the risks in releasing a technology and we need to do that trade-off. And in the machine

47:29.560 --> 47:36.600
learning field, which has been very open until now, most of it is in the open, right? And even if

47:36.600 --> 47:43.800
it's not, it doesn't take a whole lot of resources to get to those capabilities. And so locking

47:43.800 --> 47:49.480
it up seems counterproductive to me at this stage, especially in the context of language models

47:49.480 --> 47:56.120
and similar research where so much of it, very similar frameworks, you know, the open source code,

47:56.120 --> 48:02.040
they're all available. It doesn't take a lot to reproduce that by bad actors. On the other hand,

48:02.040 --> 48:09.560
it can limit access to people in the marginalized communities, people with low, you know,

48:09.560 --> 48:16.760
with limited access to resources. So that's why I see in this setting the equation to be more

48:16.760 --> 48:29.480
tilted towards release. Okay. So a question or response from Miles or Amanda, which of you is that?

48:29.480 --> 48:37.480
Oh, yeah. In this case, I think one thing that's just worth noting is like, we are like very

48:37.480 --> 48:43.320
sensitive to these issues. Like, I agree that it's important that there's like equity among researchers

48:43.320 --> 48:47.960
and that one of the downsides of like not releasing anything, even just doing a partial release,

48:47.960 --> 48:53.640
is that researchers don't have access to it. You have issues like, you know, it makes it harder

48:53.640 --> 49:00.440
to replicate or at least there's a delay in replication. I think it would be interesting to me,

49:00.440 --> 49:05.160
I guess I have two thoughts. One is that we might not even disagree about roughly the weight

49:05.160 --> 49:10.440
of all of the considerations here because I think our position was one of kind of caution

49:10.440 --> 49:14.600
where it's just like, the question isn't something like, do you think this is the exact right moment

49:14.600 --> 49:21.000
to do a partial release? But rather something like, are you like basically how certain are you that

49:21.000 --> 49:25.320
isn't or how certain are you or how confident are you that you're actually on the right side of

49:25.320 --> 49:31.640
this scale? And I think that's like the kind of questions that we're asking. And in part,

49:31.640 --> 49:36.120
also I think the thing that this really highlights, you know, when we start bringing up these pros

49:36.120 --> 49:41.640
and cons is the need for like greater discussion of this in the ML community. So in some ways, when

49:41.640 --> 49:45.400
there isn't a kind of framework or there isn't a kind of agreed upon set of norms, so there isn't

49:45.400 --> 49:50.280
a partnership on this. Each actor is having to kind of think about these issues themselves.

49:50.280 --> 49:54.440
And for our part, we relate, well, that means that you have to take on a lot of caution. And it

49:54.440 --> 49:59.960
is important to be cautious, even if you're considering all of the negative consequences of that.

50:00.840 --> 50:04.600
So I think it's great that this discussion is happening, but it's also worth noting that this could

50:04.600 --> 50:10.200
be made easier if we did potentially have some of these mechanisms to like really help people think

50:10.200 --> 50:16.440
through when to release watch release and the pros and cons. Yeah, and how to raise risks in a way

50:16.440 --> 50:22.200
that isn't seen or isn't actually, you know, alarmist. I mean, you know, so I think one perspective

50:22.200 --> 50:27.880
that I think, you know, is worth considering is that the AI community does not have all the answers,

50:27.880 --> 50:32.120
and it's not the only one that needs to know what the technology's coming down in the pipeline are.

50:32.920 --> 50:36.040
And you know, that was sort of where we were coming from with the outreach to journalists,

50:36.040 --> 50:41.400
prior to the launch, and it's possible that, you know, we could have done more, you know,

50:41.400 --> 50:46.920
increase the ratio of researcher, you know, outreach to, you know, non-researcher outreach,

50:46.920 --> 50:52.520
but, you know, the basic ideas that this is not, you know, just an open AI thing or not an AI

50:52.520 --> 50:57.160
community thing, but, you know, a more general question of sort of how do we handle these powerful

50:57.160 --> 51:01.080
technologies that seem to be coming, even if they're not, you know, totally there. Yeah.

51:02.120 --> 51:10.680
Can you can you comment on the the approach you took to evaluating the kind of the ethical field

51:10.680 --> 51:16.920
in the release, the level of kind of rigor or detail, did you identify, you know, did you have

51:16.920 --> 51:23.560
a specific, you know, kind of persona or threat that you are most concerned about, whether it's

51:23.560 --> 51:29.960
one that's been stated or unstated, or where you kind of reacting, responding, or anticipating

51:29.960 --> 51:34.360
just the broad threat. How, you know, how did you kind of pursue this?

51:36.120 --> 51:40.040
Yeah, so I think, you know, there are a couple of different lenses, and we're still not

51:40.040 --> 51:44.600
sure what the right lens is. There's sort of, you know, GPT-2 itself, and, you know, there's

51:44.600 --> 51:49.400
a fine-tuned version. We, you know, part of where we were coming from was seeing what the zero-shot

51:49.400 --> 51:54.520
version of GPT-2 was, and that sort of gave us some heuristic evidence that even more powerful

51:54.520 --> 52:00.440
capabilities would be possible with fine-tuning or larger models. So, you know, some of it was sort

52:00.440 --> 52:04.760
of, you know, anecdotal experiences of people interacting with the model, like Alec and Jeff

52:04.760 --> 52:11.000
just sort of seeing impressive things, and sort of, you know, a growing number of people interacting

52:11.000 --> 52:17.000
with the model, seeing how easy it was to get, you know, human-ish-looking outputs, not necessarily,

52:17.000 --> 52:22.120
you know, semantically accurate or factually correct to, you know, one of the points that Anima

52:22.120 --> 52:26.760
raised earlier, you know, what is the relevant threshold? We thought that it was, you know,

52:26.760 --> 52:31.560
we still don't know exactly what the right threshold is, but something about sort of human passable,

52:31.560 --> 52:37.000
human-ish-text across a very wide range of demands for very little human-input.

52:37.000 --> 52:42.280
This is, this is testable. You know, there's obviously a lot of fake news out there right now,

52:42.280 --> 52:46.360
and typical bad actor does this in a really simple way. It's, you know, like templates,

52:46.360 --> 52:50.600
we get to drop in certain words and it generates variations of those sentences. Just, you know,

52:50.600 --> 52:54.440
like a hundred lines of code, but it's powerful. You have like ten sentence variants and ten

52:54.440 --> 53:00.360
sentences. You can create billions of different unique paragraphs. And so, you know, it's testable

53:00.360 --> 53:05.160
to create a system like that today, and then have humans say, you know, which is, you know,

53:05.160 --> 53:13.080
which is the more likely to be real. And so that's, and that's standard, right? Like how am I

53:13.080 --> 53:17.720
compared to the state of the R? And the paper had this. So the paper had a bunch of benchmarks

53:17.720 --> 53:23.960
against existing technologies to show they're better than researchers. But the ethical component

53:23.960 --> 53:29.800
or at least everything I've seen so far has been purely qualitative. And so as the ethics people

53:29.800 --> 53:36.440
in open AI, do you think you could convince that the scientists to drop one academic benchmark

53:36.440 --> 53:42.120
and run some new studies which would really demonstrate what the negative impact might be compared

53:42.120 --> 53:46.600
to what's already out there? I mean, I think we're kind of heading in that direction in terms of

53:46.600 --> 53:52.680
quantifying the risks of models and things like bias, you know, in the context of language is

53:52.680 --> 53:56.200
something that we're starting to be more aware of. And I could, you know, I could imagine, you know,

53:56.200 --> 54:00.920
sort of like misuse potential, you know, label for, you know, different sizes of models or something.

54:00.920 --> 54:06.360
But I think we're still in a more sort of pre-conceptual framework phase in the sense that like we,

54:06.360 --> 54:11.160
we know a few considerations, we know a few sort of specific threats, we have some ideas for

54:11.160 --> 54:16.440
how to evaluate them, but ultimately, you know, we don't, we don't have in-house experts on,

54:16.440 --> 54:21.000
you know, everything related to fake news, everything related to cybersecurity, et cetera. So this

54:21.000 --> 54:25.560
is very much, you know, a conversation that we welcome input on. So specific ideas for testing the

54:25.560 --> 54:28.840
model, specific ideas for sort of threat modeling are super welcome.

54:31.720 --> 54:38.120
I think it's a little interesting that Westine have this discussion about text, which at least

54:38.120 --> 54:42.200
I might have an optimistic view, but I feel like the technology is still getting there in terms

54:42.200 --> 54:47.800
of the potential, you know, strongest misuse possibilities. And then, you know, a lot of the time

54:47.800 --> 54:52.520
misinformation online isn't going to be about writing a long-form article. It seems reasonable.

54:52.520 --> 54:57.400
It's about writing a very short snippet that is terrible in a bunch of ways and spreading it

54:57.400 --> 55:03.000
everywhere. But this type of discussion, you know, we as a community should have really started

55:03.560 --> 55:08.120
properly having like, we shouldn't be caught in the dark by this as even a question,

55:08.120 --> 55:14.120
because, you know, deepfakes has been substantially, you know, to many people quite destructive

55:14.120 --> 55:19.640
to their lives. And for the same sort of things that powered that, the code that we released,

55:19.640 --> 55:22.520
the pre-trained models, which have been released, which were then built upon.

55:23.880 --> 55:28.040
Like, I feel like we as a community should have had a better response to that. Like, that should

55:28.040 --> 55:34.280
have been a awakening moment. And this should be a potential like later, okay, you know, now we can

55:34.280 --> 55:39.640
stop considering text modeling through this lens before it's time. I don't know if anyone else

55:39.640 --> 55:45.480
has strong thoughts on that. Do we know where the right forum is for that conversation? I'm not

55:45.480 --> 55:52.600
sure that it's Twitter. I mean, Twitter is a part of it, but, you know, is it standard bodies?

55:52.600 --> 56:01.400
Is it kind of for at the conferences like Nourips and others? Or is it some, you know, structure that,

56:01.400 --> 56:06.040
is it, you know, regulatory? Is it some structure that hasn't been created? Are there things that we can

56:06.040 --> 56:15.000
learn from other, you know, technologies that have both beneficial uses and potential for weaponization

56:15.000 --> 56:22.520
that, you know, what have they done? I mean, I think I would be careful in using terms like

56:22.520 --> 56:27.640
weaponization, right? Because a lot of, you know, like discussions on, especially on Twitter,

56:27.640 --> 56:33.400
as you said, it's not the best medium, like tends to like devolve all the way into a nuclear weapons.

56:33.400 --> 56:42.040
And, you know, those, you know, it's cases of malicious use, right? And this is where there is a

56:42.040 --> 56:48.200
lot of, you know, what we saw that I was most disappointed about the whole episode was the media

56:48.200 --> 56:55.880
distortion in, you know, wide reaching to the public in so many different countries. The message was,

56:55.880 --> 57:00.600
this is so dangerous that this is now locked up, right? That's what the public took away.

57:01.160 --> 57:07.560
And that's disappointing because I think that's a severe distortion from these much more nuanced

57:07.560 --> 57:12.600
conversation we are having today, right? This is what we need to be doing. We need to have the

57:12.600 --> 57:18.520
dialogue within the community and also with the public. I think there is still a big gap of what

57:18.520 --> 57:24.600
the think of as AI or even intelligence, you know, they are not able to truly evaluate

57:24.600 --> 57:32.760
how intelligent or how capable the current AI systems are. And I think that's a severe deficiency

57:32.760 --> 57:39.160
if we cannot close that gap between the research community and the general public. And that's

57:39.160 --> 57:45.000
what worries me the most that this conversation, you know, we need to present it in a much more

57:45.000 --> 57:52.600
balanced form to the public and to the media. I also want to mention like one thing that I

57:52.600 --> 57:57.720
always think in my mind, if we think way back when Facebook released a paper that was essentially

57:57.720 --> 58:03.960
about two robots bothering. And somehow like I was in Australia and I heard on like the nightly

58:03.960 --> 58:09.160
news, Facebook AI has like, you know, taken over something they had to shut down the experiment.

58:09.160 --> 58:14.440
It's so dangerous they had to shut it down. Exactly. But that's a thing, right? Like we have very

58:14.440 --> 58:19.160
few chances of reaching this global audience and we need to be very careful about what ends up stuck

58:19.160 --> 58:23.560
in their heads. Because we have the potential to put the right piece of information in someone's

58:23.560 --> 58:28.200
head. So I know, you know, maybe to be wary about receiving in the future, receiving that email

58:28.200 --> 58:33.160
from grandma, which seems entirely syntactically correct, but she's asking about Bitcoin and has

58:33.160 --> 58:38.600
an account already to feed it, right? Like maybe that's the one piece of information we can

58:38.600 --> 58:44.200
somehow impose a worry about in the future. But you know, in the case of the Facebook AI story,

58:44.200 --> 58:49.160
you know, I feel like the information that has been locked in people's head is really quite wrong.

58:49.160 --> 58:53.800
And, you know, Facebook's, they took the, you know, they took it on themselves. They read a new

58:53.800 --> 58:58.520
blog purse that explained just how it got distorted within the media. But I can, you know, imagine

58:58.520 --> 59:05.480
very strongly that people who saw that television station news segment in Australia are almost never

59:05.480 --> 59:11.320
going to read that blog purse. And that's I think what I'm more worried about fake news about AI

59:11.320 --> 59:21.560
than AI generating the fake news. So maybe to start to wrap things up, miles in Amanda with the

59:21.560 --> 59:28.840
benefit of hindsight, you know, how much you approach this differently? Well, yeah, I'll give Amanda

59:28.840 --> 59:33.640
a second to think about that. And while I give you a comment, which is that in terms of venues for

59:33.640 --> 59:39.800
keeping the conversation going, we'll be hosting and dinner at I clear for where both policy people

59:39.800 --> 59:45.160
and technical people from opening I will be happy to discuss them talk about the future of language

59:45.160 --> 59:52.680
models, generally, not just to PT too. Yeah, so I think that like ideally being able to have a kind

59:52.680 --> 01:00:01.320
of wide range of inputs from the ML community prior to like making a decision like this is going

01:00:01.320 --> 01:00:07.240
to be very helpful. In the case of like media attention, there's a sense in which it's like a

01:00:07.240 --> 01:00:13.960
little bit harder to kind of navigate or control because like the way that something is going to be

01:00:13.960 --> 01:00:21.080
told like as we've kind of heard is like like a little bit more delicate, although I really sympathize

01:00:21.080 --> 01:00:26.440
with this, you know, like if we could do the not necessarily just how you do things differently,

01:00:26.440 --> 01:00:31.880
but like like how you wish things would go, you know, we were quite positive in the blog post as

01:00:31.880 --> 01:00:38.520
well, like we talked about all of the positive ways that this could be used. And obviously like in

01:00:38.520 --> 01:00:42.440
general, like the thing that gets reported is quite negative. And I think that's like really

01:00:42.440 --> 01:00:46.440
unfortunate because like, you know, people who are doing ML research are doing it because they want

01:00:46.440 --> 01:00:53.240
to see, you know, excellent uses of it in the world. And you know, I think that it wasn't our intention

01:00:53.240 --> 01:00:58.200
to say something like this research is all bad and you should be afraid of it. Rather just like,

01:00:58.200 --> 01:01:02.760
hey, we need to start having a conversation about this. So I think yeah, I would like to see more

01:01:02.760 --> 01:01:07.320
of that. And I think that it's correct that essentially what I would like to see more of is the

01:01:07.320 --> 01:01:10.920
kind of nuance discussion that we're having here, where we're sensitive to all of the pros and

01:01:10.920 --> 01:01:17.400
cons both of like doing open research and like the potential misuse of that research. So yeah,

01:01:17.400 --> 01:01:21.720
I think like creating forums where people can do that is like the thing that I would really like

01:01:21.720 --> 01:01:29.400
to see going forward. Yeah, I mean, I agree with all that. And just in general, both open AI and

01:01:29.400 --> 01:01:33.720
the rest of the AI community needs to find ways to smooth this conversation out over time. So it

01:01:33.720 --> 01:01:39.080
doesn't happen all of it, you know, in one, you know, Twitter storm and sort of, you know, find

01:01:39.080 --> 01:01:43.880
whether it's sort of, you know, recurring workshops at conferences or whatever to sort of institutional

01:01:43.880 --> 01:01:51.640
conferences. And how about the rest of you, Steven, what, what would you like to see done differently

01:01:52.440 --> 01:02:01.320
and what do you hope to see grow out of this this scenario? Yeah, well, I mean, you know, I love

01:02:01.320 --> 01:02:06.600
language modeling. It's always exciting for me when I'm about. So it's a good thing for me. What

01:02:06.600 --> 01:02:09.880
I'm going to be interested in is, you know, at some point when I can play with this model,

01:02:09.880 --> 01:02:15.880
play with other models, you almost like one of the biggest issues we haven't filled is, you know,

01:02:15.880 --> 01:02:19.800
urban fitting. You find ways that these machine learning models learn to cheat in subtle and

01:02:19.800 --> 01:02:25.320
strange ways. And one of the, you know, craziest examples of that is in visual question answering.

01:02:25.320 --> 01:02:29.560
So you give the machine learning model an image and then you ask it a question to answer.

01:02:30.280 --> 01:02:36.760
For some time, these visual question answering systems did worse than just looking at the question

01:02:36.760 --> 01:02:41.640
without ever looking at the image. And the field didn't quite realize it just because they didn't

01:02:41.640 --> 01:02:46.360
run the right experiments. So that's the thing that I think I would enjoy playing with. There's

01:02:46.360 --> 01:02:50.760
the modern version of that for text, which is for a question answering data sets called squad.

01:02:51.560 --> 01:02:55.560
You know, many people are really excited about how intelligent it was, how many of the questions

01:02:55.560 --> 01:03:00.360
it got correct. But then a group went through and kind of methodically looked at each instance and

01:03:00.360 --> 01:03:05.960
was like, oh, with a, you know, a few dozen lines of code and red jerks' regular expressions

01:03:05.960 --> 01:03:10.600
basically ways to capture some patterns. You can answer all of these. So it's a question of,

01:03:10.600 --> 01:03:15.000
okay, this language model is obviously quite good. But exactly how good is it? And, you know,

01:03:15.000 --> 01:03:19.480
what interesting, strange methods of cheating might it be using that's able to trick all of us

01:03:19.480 --> 01:03:29.000
at a glance? Rob? Yeah. So I wrote what I said earlier. And then to speak to Anima's point about

01:03:29.000 --> 01:03:36.840
making sure the right message gets out there, I felt like for me, the release wasn't most offensive

01:03:36.840 --> 01:03:42.680
in terms of what wasn't and what wasn't public. I think the biggest shortcoming was that it was

01:03:42.680 --> 01:03:49.000
only an image that it was not diverse, that it was the most privileged language, which correlate

01:03:49.000 --> 01:03:56.200
to almost every other privileged demographic. And that to me was the bigger ethical concern than

01:03:56.200 --> 01:04:04.120
anything to do with what actually got released in terms of the language model. Anima? Yeah. I mean,

01:04:04.120 --> 01:04:11.480
I think, you know, putting more thought process into, you know, collaborating with academia and

01:04:11.480 --> 01:04:18.920
research community in general, right? And making sure that especially the, you know, researchers with

01:04:18.920 --> 01:04:24.680
less compute resources are not at this advantage. That's something, you know, non-profit like

01:04:24.680 --> 01:04:31.640
OpenAI would have a very big role to play, right? To remove the barriers and to truly democratize AI.

01:04:32.520 --> 01:04:39.880
I think thinking also on that angle while quantifying the risks and coming up with a more

01:04:40.520 --> 01:04:46.360
quantitative analysis of risks and also maybe incentive mechanisms like how to better

01:04:46.360 --> 01:04:52.520
deploy AI and better release it to the community. These are all things we can, you know,

01:04:52.520 --> 01:04:58.280
for the research and come up with some best practices for the community and also best practices

01:04:58.280 --> 01:05:05.560
in terms of how we talk about this to the general media and how this gets reported. That's something

01:05:05.560 --> 01:05:13.400
we as a community need to work more about. Yeah. And, you know, I love the that you're hosting the

01:05:13.400 --> 01:05:20.120
dinner at. I clear, but I'd also love to see, you know, OpenAI kind of roll up its leaves and

01:05:20.120 --> 01:05:25.560
help figure out where the right place is to have this conversation more formally. And, you know,

01:05:25.560 --> 01:05:29.640
who the right people are to bring to the table a number of the folks that I've commented on YouTube

01:05:29.640 --> 01:05:34.440
and Twitter about, you know, security researchers have dealt with this kind of issue for a long time.

01:05:34.440 --> 01:05:38.840
How do we get them into the fold? You know, folks have been doing threat modeling. How do we get them

01:05:38.840 --> 01:05:46.360
into the folds? There's a lot of work that has to happen to, you know, create, you know, that space

01:05:46.360 --> 01:05:54.200
and use it to further the conversation. And, yeah, I'd love to see more happening there. But for

01:05:54.200 --> 01:05:59.080
tonight, I'm glad to be part of the, you know, getting this beyond 280 characters a pop.

01:06:00.120 --> 01:06:06.840
And thank all of you for taking the time to jump on and talk about this really important issue.

01:06:07.720 --> 01:06:11.960
Thank you. Thanks a lot for doing this. All right. Good night, everyone.

01:06:11.960 --> 01:06:15.880
Good night, good night. Thanks, everyone, for joining via YouTube live.

01:06:15.880 --> 01:06:22.200
Yeah. And before we go, actually, we'll put it in the description when we post the video,

01:06:22.200 --> 01:06:26.600
but for folks who aren't following all of you on Twitter, why don't we do a quick

01:06:27.480 --> 01:06:29.880
Twitter handle a roll call?

01:06:29.880 --> 01:06:45.320
Um, I'm sorry. Rob, uh, WWE Rob World Wide Rob. Smarity is Smarity. Smarity.

01:06:48.360 --> 01:06:56.600
Miles underscore Brundage. Yeah, Miles underscore Brundage. I'm just Amanda Askell.

01:06:56.600 --> 01:07:00.520
Oh, one word. Awesome. Awesome. Well, thanks. Thanks again, everyone. Good night.

01:07:00.520 --> 01:07:27.240
All right. Bye. Thanks.

