WEBVTT

00:00.000 --> 00:16.320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

00:16.320 --> 00:21.480
people, doing interesting things in machine learning and artificial intelligence.

00:21.480 --> 00:32.040
I'm your host Sam Charrington.

00:32.040 --> 00:37.280
In this episode of our Strata Data Conference series, we're joined by Justin Norman, director

00:37.280 --> 00:42.520
of research and data science services at Cloud RF Fast Forward Labs.

00:42.520 --> 00:46.880
Fast Forward Labs was an applied AI research firm and consultancy founded by Hillary

00:46.880 --> 00:52.880
Mason, whose Twimble Talk episode remains an all-time fan favorite.

00:52.880 --> 00:57.600
My chat with Justin took place on the one-year anniversary of Fast Forward Labs' acquisition

00:57.600 --> 00:59.200
by Cloudera.

00:59.200 --> 01:03.240
So we start with a bit of an update before diving into a look at some of their recent

01:03.240 --> 01:05.480
and upcoming research projects.

01:05.480 --> 01:10.880
Specifically, we discussed their recent report on multitask learning and their upcoming

01:10.880 --> 01:13.760
research into federated machine learning.

01:13.760 --> 01:19.600
To learn more about Cloudera and CFFL, visit Cloudera's machine learning resource center

01:19.600 --> 01:22.520
at cladera.com slash ML.

01:22.520 --> 01:26.560
A huge thanks to them for sponsoring this series.

01:26.560 --> 01:29.520
Thanks also to our second sponsor, Capital One.

01:29.520 --> 01:34.320
At the Nip's conference in Montreal this December, they'll be co-hosting a workshop focused

01:34.320 --> 01:39.880
on challenges and opportunities for AI and financial services and the impact of fairness,

01:39.880 --> 01:43.040
explainability, accuracy and privacy.

01:43.040 --> 01:47.040
The call for papers is open now through October 25th.

01:47.040 --> 01:54.400
For more information or submissions, visit twimlai.com slash C1 Nips.

01:54.400 --> 01:58.880
If you love this show, you've got to love our sponsors because they help make it possible.

01:58.880 --> 02:03.160
So please take a look at what they're up to and tell them Twimble sent you.

02:03.160 --> 02:08.880
And now on to the show.

02:08.880 --> 02:13.720
All right, everyone, I am here in New York City at the Stratoc conference and I'm seated

02:13.720 --> 02:15.400
with Justin Norman.

02:15.400 --> 02:19.960
Justin is the director of research and data science services at cladera fast forward

02:19.960 --> 02:20.960
labs.

02:20.960 --> 02:23.280
Justin, welcome to this weekend machine learning and AI.

02:23.280 --> 02:24.280
Thank you very much.

02:24.280 --> 02:25.280
Glad to be here.

02:25.280 --> 02:26.280
Awesome.

02:26.280 --> 02:32.040
So Justin, you started your career in data as an officer with the US Marine Corps.

02:32.040 --> 02:33.560
Tell us a little bit about your background.

02:33.560 --> 02:35.000
Yeah, that's absolutely right.

02:35.000 --> 02:39.360
So I actually got a chance to study computer science and focus on mathematical optimization

02:39.360 --> 02:41.000
at the Naval Academy.

02:41.000 --> 02:45.680
So naturally, when I made the decision to join the Marine Corps, I was asked to go into

02:45.680 --> 02:50.240
a technical role, I'm pretty much straight off the bat, which was just fine by me.

02:50.240 --> 02:56.160
But over the course of about seven years, I got a chance to both locally and also overseas

02:56.160 --> 03:01.560
participate in a lot of large scale network enterprises, which had massive data challenges.

03:01.560 --> 03:06.760
Not the least of which were that time for over 30,000 connected devices under management.

03:06.760 --> 03:11.160
And just understanding what traffic flows looked like normal, what traffic looks like that

03:11.160 --> 03:12.160
were anomalous.

03:12.160 --> 03:16.200
And being able to react dynamically to that was a large portion of my job.

03:16.200 --> 03:22.440
So machine learning was embedded quite frankly in the sort of a soul of what we were doing.

03:22.440 --> 03:25.880
But at the time, we didn't really have terminology for data science and machine learning.

03:25.880 --> 03:28.160
We just sort of did what we needed to do.

03:28.160 --> 03:32.960
But naturally that would help me to sort of progress into industry as the field emerged

03:32.960 --> 03:36.600
a bit further later in my lifetime.

03:36.600 --> 03:39.960
You went on to found a startup in machine learning, is that right?

03:39.960 --> 03:40.960
I did.

03:40.960 --> 03:48.160
So I worked with a few people that I knew quite well to try to work on the problem of business,

03:48.160 --> 03:50.400
predictive analytics within sports.

03:50.400 --> 03:55.480
And so we weren't so focused on the saber matrix that you see from the sort of traditional

03:55.480 --> 04:00.120
sports analytics, but we're a lot more interested in trying to find ways to solve some of the

04:00.120 --> 04:04.840
corporate challenges that we saw in a lot of other industries, but applying them to industries

04:04.840 --> 04:09.920
which really didn't have that kind of background in skill set and helping them to accelerate

04:09.920 --> 04:11.240
from a financial perspective.

04:11.240 --> 04:19.400
So a lot of our focus was actually in the beautiful game in football, but the right football.

04:19.400 --> 04:23.040
And we did a lot of work in Central and South America to help some of those smaller clubs

04:23.040 --> 04:28.080
actually be able to take advantage of the business value of their players.

04:28.080 --> 04:33.240
For example, transferring or trading a player that had a higher social media impact than

04:33.240 --> 04:38.040
perhaps another player that was equally skilled to try to impact things like ticket sales

04:38.040 --> 04:41.360
and retention of a season ticket holders, things like that.

04:41.360 --> 04:42.360
Interesting.

04:42.360 --> 04:50.000
I did an interview with a guy named Minoa gift that was doing some very similar things

04:50.000 --> 04:55.760
in terms of looking at how to score player, social media impact and the ultimate impact

04:55.760 --> 04:58.440
on the game.

04:58.440 --> 05:06.920
So you went on to, among other things, do data science at Cisco and now at Cloud or a

05:06.920 --> 05:11.480
Fast Forward Labs, tell us a little bit about your role and your focus at CFFL.

05:11.480 --> 05:12.480
Great.

05:12.480 --> 05:13.480
Yeah.

05:13.480 --> 05:18.560
So as people may or may not be aware, we're actually exactly at one year of the acquisition

05:18.560 --> 05:20.360
of Fast Forward Labs at Cloud Air.

05:20.360 --> 05:25.560
So in a nutshell, my role is to take Fast Forward Labs kind of from where it is, which

05:25.560 --> 05:32.040
is actually very well integrated and quite successful with its existing clients and some additional

05:32.040 --> 05:34.200
quite our clients and really scale that globally.

05:34.200 --> 05:36.680
And I mean that from not just a physical standpoint.

05:36.680 --> 05:42.080
So yes, we want to expand to media and into Asia, but actually also from a data perspective.

05:42.080 --> 05:46.400
So can we take the research that we've done, the products that we've developed and start

05:46.400 --> 05:52.160
to do those at scale and the enterprise and be really known as not just leaders in the

05:52.160 --> 05:56.960
research field, but leaders in how to apply that research across both corporate organizational

05:56.960 --> 05:59.280
governmental entities.

05:59.280 --> 06:04.600
And that is from a technical perspective, a challenge, but also from a human and strategic

06:04.600 --> 06:09.200
perspective, a challenge, which is why I think Fast Forward Labs is really uniquely positioned.

06:09.200 --> 06:13.400
Because as you know, our research not just doesn't just focus on the technical and technique

06:13.400 --> 06:17.760
aspect of things that we have quite a bit of research in that area, but also in how to

06:17.760 --> 06:24.840
layer the people in process and skills, conversations into the application of that technology so that

06:24.840 --> 06:28.440
you can get the outcome when you actually expect to have it.

06:28.440 --> 06:36.200
And so for those that don't know, the research that you're referring to is a series of reports,

06:36.200 --> 06:40.640
can you talk a little bit about some of the topics that you've covered in the report

06:40.640 --> 06:41.640
just as context?

06:41.640 --> 06:42.640
Sure.

06:42.640 --> 06:46.600
The best way I think to talk about the reports is to start with a process that we go through

06:46.600 --> 06:49.600
to actually decide what we're going to do.

06:49.600 --> 06:51.480
And I think it's really powerful.

06:51.480 --> 06:58.640
So the entire team gets together and really calls out, I'd say, with the intent to find

06:58.640 --> 07:07.360
some bad ideas of types of topics that might be relevant within the next 12 to 24 months

07:07.360 --> 07:08.360
within machine learning.

07:08.360 --> 07:13.520
And the intent is that we're going to look at topics that are not just papers or research

07:13.520 --> 07:17.800
that is out there in scholarly journals, but actually something that is starting to be

07:17.800 --> 07:22.480
applied in industry, or maybe we have the ability to bring that last step.

07:22.480 --> 07:27.080
And so topics in the past, like natural language generation, symmetric recommendations,

07:27.080 --> 07:32.520
image analysis with deep learning, and then most recently multi-task learning, have sort

07:32.520 --> 07:36.720
of become of the topic that has gone that we've narrowed down in that process and really

07:36.720 --> 07:43.720
have felt that is most relevant to the widest group of consumers that exist within industry,

07:43.720 --> 07:45.840
academia, and beyond.

07:45.840 --> 07:51.800
So we've actually just launched a report as I just referred to called multi-task learning,

07:51.800 --> 07:56.760
which focuses on actually executing multiple machine learning tasks at the same time.

07:56.760 --> 07:59.160
And so we're really excited about that.

07:59.160 --> 08:02.200
And there's actually a number of people this week at Strata who are going to be speaking

08:02.200 --> 08:03.680
about that in greater depth.

08:03.680 --> 08:07.120
And so very excited to hear what they have to say about that.

08:07.120 --> 08:10.640
But we're not stopping and actually have two more reports in the queue.

08:10.640 --> 08:13.560
One of them we actually know the research, but the research is going to be focused on

08:13.560 --> 08:15.640
and that's on federated learning as well.

08:15.640 --> 08:21.600
So we're really excited to be able to share this because we think that now we're beginning

08:21.600 --> 08:25.920
to come in influence not just by the community that we had through Fast Forward Labs, which

08:25.920 --> 08:31.600
was extensive, but also because we're at Clavera, we now have a focus along the enterprise

08:31.600 --> 08:33.640
with a much, much larger scale.

08:33.640 --> 08:37.600
It's helping us to understand what's relevant to that community as well.

08:37.600 --> 08:42.800
And that definitely has directly influenced what we've been able to perform in the research

08:42.800 --> 08:44.920
space as well.

08:44.920 --> 08:49.200
On that multi-task learning paper, I haven't had a chance to take a look at that one,

08:49.200 --> 08:55.160
but around the time it was being that initiative was being kicked off someone at CFFL reached

08:55.160 --> 08:59.320
out to me to CFI knew anyone who was working in that space.

08:59.320 --> 09:02.920
And I had recently done an interview with Ryan Poplin at Google.

09:02.920 --> 09:09.600
I'm pretty sure that's who it was who had made an interesting comment that they were looking

09:09.600 --> 09:16.720
at these retinal, fundus images and using those to determine a variety of demographic

09:16.720 --> 09:20.480
information about the people who the images came from.

09:20.480 --> 09:27.240
And they noticed that when they asked their model to do to determine a single thing and

09:27.240 --> 09:32.120
compare the performance relative to asking the model to do multiple things, the model

09:32.120 --> 09:35.240
performed better when it had multiple things to do.

09:35.240 --> 09:45.520
And his theory was that that kind of forced the model to generalize better.

09:45.520 --> 09:46.760
And so I kind of made the connection.

09:46.760 --> 09:48.160
I hope that connections are good connection.

09:48.160 --> 09:50.160
That's a very good connection.

09:50.160 --> 09:54.680
And I think you'll see that in the report even that we recreated results on that specific

09:54.680 --> 09:57.480
use case, but on other use cases that was very similar.

09:57.480 --> 10:01.360
And in some areas, it's very well explainable to your point why that happens.

10:01.360 --> 10:02.680
And in some areas, it's not.

10:02.680 --> 10:06.080
And I think that's one of the most powerful parts of the research component of what that

10:06.080 --> 10:08.160
report covers.

10:08.160 --> 10:12.920
So you certainly can recreate that experience where you do have model performance that's

10:12.920 --> 10:18.360
a bit better on one or multiple, excuse me, multiple or even three or four or even

10:18.360 --> 10:20.920
more learning tasks at the same time.

10:20.920 --> 10:24.480
But the type of data is highly relevant to whether or not you're going to see that performance.

10:24.480 --> 10:27.080
And I'm one of the things that we spend a lot of time talking about.

10:27.080 --> 10:34.200
And so is the, what's the kind of core thesis of that, of that paper?

10:34.200 --> 10:39.960
Well, so that for, like I said, for certain types of data sets and for certain learning

10:39.960 --> 10:43.480
tasks, you should definitely do it.

10:43.480 --> 10:44.880
So it is the core thesis.

10:44.880 --> 10:49.440
But I think the most important part is that it is possible to actually execute these

10:49.440 --> 10:56.360
multiple learning tasks and actually get them at scale, utilizing some of the more now

10:56.360 --> 10:58.680
commoditizable learning functions.

10:58.680 --> 11:02.360
So things like deep learning models are highly relevant to this as well.

11:02.360 --> 11:07.480
So the focus is on really making sure that people understand which techniques exist and

11:07.480 --> 11:08.920
then when to apply them.

11:08.920 --> 11:13.760
And are there a specific set of techniques for multitask learning or is it just the same

11:13.760 --> 11:18.720
stuff but with multiple objectives there, but a little bit of both.

11:18.720 --> 11:21.880
So there are definitely certain sets of techniques, but it depends on the objective.

11:21.880 --> 11:25.000
Can you give me an overview of the specific types of techniques that were you would need

11:25.000 --> 11:26.000
Frederica for?

11:26.000 --> 11:27.000
Okay.

11:27.000 --> 11:30.920
That was the report that recently came out going forward.

11:30.920 --> 11:36.480
You are looking at, you mentioned that the next one is going to be on federated learning.

11:36.480 --> 11:37.480
That's right.

11:37.480 --> 11:39.240
What's the, what's the thinking around that?

11:39.240 --> 11:43.520
So one of the things that was always a theme at fast-forward labs.

11:43.520 --> 11:47.880
And I think again, back to my earlier point is something that's coming in front of mine

11:47.880 --> 11:51.920
because of the types of customers that we've been working with at Clara is the idea of

11:51.920 --> 11:55.600
really trying to execute and scale machine learning at the edge.

11:55.600 --> 12:00.800
So by the edge, I mean, at devices that are, you know, the last device before a human

12:00.800 --> 12:07.840
or last device before a sensor, you know, executes its function to read data from an environment.

12:07.840 --> 12:11.280
And there's a number of reasons to want to you machine learning at the edge.

12:11.280 --> 12:16.880
I think one of the biggest ones is if the device, and now they do, have enough computational

12:16.880 --> 12:20.960
power to execute that, you get returns that results back that you can, you know, influence

12:20.960 --> 12:24.560
outcomes with directly without having to report back to some kind of central model or

12:24.560 --> 12:27.000
sometimes central computing store.

12:27.000 --> 12:29.640
So this is most relevant in IoT applications.

12:29.640 --> 12:34.800
I think you can even think of it as a human with your cell phone.

12:34.800 --> 12:38.240
There's obviously multiple machine learning models that are powering very important functions

12:38.240 --> 12:39.760
on your phone.

12:39.760 --> 12:46.600
But one of the challenges that comes into play right away with that is that ultimately,

12:46.600 --> 12:49.800
there's a lot of people who want to have the power of machine learning and AI enabled

12:49.800 --> 12:54.440
capabilities, but without necessarily exposing the underlying data structure.

12:54.440 --> 12:59.040
And the data that, that data structure that might be influenced by a person's personal

12:59.040 --> 13:05.280
choices, by, for example, on a commercial application, competitive IP applications that

13:05.280 --> 13:08.960
don't want to be shared with the industry, but you still want the benefit of being able

13:08.960 --> 13:16.400
to train and to share, for example, parameters, hyperparameters that settings that are relevant

13:16.400 --> 13:21.640
across the scope of problems without necessarily exposing how you got there.

13:21.640 --> 13:27.880
And so federated learning is a set of techniques for an approach to be able to do just that,

13:27.880 --> 13:33.720
where you potentially have a centralized model or a model that serves as a baseline for

13:33.720 --> 13:37.520
one of multiple devices that are deployed and over multiple workflows.

13:37.520 --> 13:42.160
But you still get a chance to benefit from that without exposing each one of the individual

13:42.160 --> 13:46.120
data underlying data structures that are there.

13:46.120 --> 13:49.760
And that's very important from an ethical perspective, very important from a commercial

13:49.760 --> 13:51.160
perspective.

13:51.160 --> 13:56.720
And also it does have a performance impact, a positive performance impact, as you might

13:56.720 --> 14:00.360
know from starting from, you're essentially starting from, you know, a little bit farther

14:00.360 --> 14:03.720
down the races that are that that's starting line.

14:03.720 --> 14:13.440
So is the issue that you're kind of framing around not wanting to disclose model parameters

14:13.440 --> 14:18.960
is the issue that you're looking to address the idea that if you push a model out to

14:18.960 --> 14:25.880
the edge, you're exposing, you know, that model, the parameters of that model are IP and

14:25.880 --> 14:26.880
you're exposing that IP.

14:26.880 --> 14:27.880
Kind of the other way around.

14:27.880 --> 14:30.040
It's the data we don't want to expose.

14:30.040 --> 14:36.680
So, for example, if you're using a phone, the types of text messages or images that

14:36.680 --> 14:40.000
you have on your phone are very private, you don't want that to be shared.

14:40.000 --> 14:46.080
But being able to do natural language processing or analysis of your text messages in order

14:46.080 --> 14:50.120
to respond better, provide applications on top of that is something of course you want

14:50.120 --> 14:51.120
to have that.

14:51.120 --> 14:56.560
So if you are a provider of software, you want to be able to train models on the widest

14:56.560 --> 15:01.360
set of information you possibly can, but you may not have the benefit of reading everybody's

15:01.360 --> 15:03.360
text messages nor should you.

15:03.360 --> 15:08.240
So is there a way that potentially you can get the benefit of a more precise model that

15:08.240 --> 15:13.240
fits better to a larger set of data, generalizes to a larger set of data without actually having

15:13.240 --> 15:14.600
access to that data?

15:14.600 --> 15:20.840
And so being able to perform these multiple learning functions over time and a very broad

15:20.840 --> 15:27.040
set of workers or workflows that are existing in near real time and then giving that information

15:27.040 --> 15:33.560
back the potential parameter settings of a model or the sort of things that shape a fit

15:33.560 --> 15:39.440
to have a model be more generalizable without exposing the underlying data is what we're

15:39.440 --> 15:40.440
looking for.

15:40.440 --> 15:45.680
And so the idea is that I've got access to some set of data at the edge.

15:45.680 --> 15:50.800
I don't want to transcend, send all of that data back to some central location, for privacy

15:50.800 --> 15:54.120
reasons, bandwidth reasons, whatever, all of the above.

15:54.120 --> 16:00.560
So what can I do on the edge to presumably, we talk a lot about the edge for inference,

16:00.560 --> 16:06.760
but what we're talking about here is to train or partially train on the edge and then use

16:06.760 --> 16:12.720
that to, it sounds like the idea of federated is use that to enhance a centralized model

16:12.720 --> 16:17.560
that can maybe even be like distributed or some aspect of that model or what that model

16:17.560 --> 16:20.040
is learned, distribute out to the other edge devices.

16:20.040 --> 16:22.400
How do you do that?

16:22.400 --> 16:26.240
Well, I mean, you'll have to wait for the report.

16:26.240 --> 16:32.920
But the longest short of it is, it's now possible from a computational perspective to execute

16:32.920 --> 16:38.000
these multiple workers when you're actually looking at thinking about a model that's

16:38.000 --> 16:40.080
being deployed anywhere.

16:40.080 --> 16:45.280
It's being done by some automation workflow, just like any other software engineering component.

16:45.280 --> 16:51.080
And so now that we can do this, not just like with one or two on a laptop, but with 1,000

16:51.080 --> 16:56.760
or 1,000, 100,000, and actually have not just a bit like, we don't have been going straight

16:56.760 --> 17:00.080
but also about computational constraints.

17:00.080 --> 17:05.200
That's a large part of what we'll be exposing is how to actually execute this.

17:05.200 --> 17:08.360
And then what are some of the constraints of that from a technology perspective?

17:08.360 --> 17:09.800
That's one aspect of it.

17:09.800 --> 17:19.000
And then from a sharing of model parameter information or model fit information, how actually

17:19.000 --> 17:25.760
to bring that back to a centralized location, to average and generalize that across the

17:25.760 --> 17:30.120
full schemes that it's useful to be to your point, distribute it back into that matrix

17:30.120 --> 17:32.840
of learners is also focused on.

17:32.840 --> 17:36.960
So there are a couple of techniques that we've developed and a few that we've adopted

17:36.960 --> 17:40.120
and written about that you'll hear about very soon.

17:40.120 --> 17:48.320
And the past, my impression of the reports has been that the focus is on kind of going

17:48.320 --> 17:53.600
out, exploring what's out there in important areas and writing about those.

17:53.600 --> 18:01.080
One of my favorites was the natural language summarization, and it did kind of a survey

18:01.080 --> 18:06.360
of all the different techniques that have been used to do summarization, LDA, lots of

18:06.360 --> 18:08.280
other things.

18:08.280 --> 18:14.640
It sounds like here you're starting a tiptoe into developing some technology or algorithms.

18:14.640 --> 18:20.080
Yeah, and I think that happened largely because we recognize that this is a real and really

18:20.080 --> 18:23.400
important need for industry and for government.

18:23.400 --> 18:28.280
But the techniques that we did research on weren't as mature as they might have again

18:28.280 --> 18:30.680
for other areas of research that we've explored.

18:30.680 --> 18:33.960
So if they were out there, we'd certainly tell you that these are the ones you should

18:33.960 --> 18:34.960
use.

18:34.960 --> 18:39.160
I think we've taken a good survey of what we've found for the multi, for excuse me, for

18:39.160 --> 18:40.400
a federated learning.

18:40.400 --> 18:44.400
But this time around, it became necessary for us to bridge the gap a little bit.

18:44.400 --> 18:47.720
And so, of course, the team has that capability and it's more than happy to do that when we

18:47.720 --> 18:48.960
think it's relevant.

18:48.960 --> 18:56.920
I mean, in a lot of ways that is, it kind of pokes at the thesis of the research, which

18:56.920 --> 19:03.480
is, I've always interpreted it as, this is stuff that's right at the edge and hate this

19:03.480 --> 19:08.280
cool thing happens someplace which kind of flipped it and people don't really know yet.

19:08.280 --> 19:09.280
Right.

19:09.280 --> 19:15.040
I'm trying to remember what the thing was in the summarization paper, but there was, there

19:15.040 --> 19:18.120
was some new, I think it was, it was word vectors.

19:18.120 --> 19:19.120
Right.

19:19.120 --> 19:24.520
So there was a bunch of LDA stuff and language modeling that people were using for this and

19:24.520 --> 19:27.720
it was kind of worked a little bit, but it wasn't great.

19:27.720 --> 19:32.640
But hey, word vectors came around and now we can really do interesting summarization stuff.

19:32.640 --> 19:38.680
The thesis was, hey, we're kind of scanning the world looking for these opportunities

19:38.680 --> 19:45.440
where external change has enabled some new way of some new capability.

19:45.440 --> 19:50.160
You know, here, you're kind of more being that external change.

19:50.160 --> 19:51.160
Yeah.

19:51.160 --> 19:53.480
It's a pretty, it strikes me as a pretty significant shift.

19:53.480 --> 19:58.120
It is a significant shift and I think it goes back to, you know, how we started the

19:58.120 --> 20:04.440
podcast, which is, you know, Cladera gives us a platform now to have, I think, a perspective

20:04.440 --> 20:07.480
on what's happening in industry from the top layer, right?

20:07.480 --> 20:11.080
So I think one of the things that's very powerful that we've maintained about fast forward

20:11.080 --> 20:15.760
lab is that we're talking to academia, we're talking to startups, we're talking to, you

20:15.760 --> 20:19.120
know, organizations which are focused on machine learning and AI.

20:19.120 --> 20:22.720
And that's a particularly powerful perspective that we've always had, but we didn't have

20:22.720 --> 20:29.400
as much exposure to the, you know, industry players, but the people who are doing machine

20:29.400 --> 20:34.280
learning and AI at scale in the edge and the petabyte scale, right?

20:34.280 --> 20:39.920
So I think what we're finding now is we're being asked by that community to solve a different

20:39.920 --> 20:44.320
set of problems and those problems may not have been commoditized yet.

20:44.320 --> 20:49.440
So we have the ability to connect what they're looking at strategically and from a business

20:49.440 --> 20:55.160
or a mission relevance perspective and then apply that cutting edge research, which was

20:55.160 --> 21:02.160
always sort of there with a layer of, you know, of interface to use it to use a word.

21:02.160 --> 21:06.440
So I think that's what what's occurring as a result of that one year being at the company.

21:06.440 --> 21:07.440
Okay.

21:07.440 --> 21:08.440
Okay.

21:08.440 --> 21:09.440
Interesting.

21:09.440 --> 21:15.840
And now this whole area of devices and the edge is not new to you.

21:15.840 --> 21:16.840
No.

21:16.840 --> 21:19.960
And we didn't talk about it in your background, but you spent some time at Fitbit.

21:19.960 --> 21:20.960
I did.

21:20.960 --> 21:24.040
How has that informed your thinking about this whole space?

21:24.040 --> 21:29.040
It definitely has been shaped by thinking about this space and made it quite frankly

21:29.040 --> 21:35.760
top of mind for me anytime that I recommend building a model factory, AI system.

21:35.760 --> 21:40.840
And so just to get a little bit deeper about that at companies that have an edge device

21:40.840 --> 21:46.000
that's being delivered to consumer or is being used to gather information from the environment

21:46.000 --> 21:51.040
like a sensor, you end up having this really poignant real time challenge.

21:51.040 --> 21:55.240
And so you need to and you want to do machine learning right at that device or near that

21:55.240 --> 22:01.400
device because it's important to be able to serve back either to your human or to the

22:01.400 --> 22:02.400
environment.

22:02.400 --> 22:08.520
The results of a model or the score of a model in some very, some very poignant way.

22:08.520 --> 22:13.600
And you know, the example that you get on a device like a fitness tracker is that a person

22:13.600 --> 22:17.960
might go through some fitness activity and want to see, you know, projections of where

22:17.960 --> 22:22.480
they might be, had certain variables changed, had they run faster, you know, what their

22:22.480 --> 22:28.280
heart rate might be, if they were to continue the next set of their exercise.

22:28.280 --> 22:31.800
And for example, you know, something very famously, you know, the number of steps that

22:31.800 --> 22:37.080
you're doing, that's not a static number for any human being and their performance upwards

22:37.080 --> 22:40.560
or downwards on that, you know, depends on a variety of factors, a variety of features

22:40.560 --> 22:42.840
to use that machine learning language.

22:42.840 --> 22:47.680
So of course, you want to be able to provide that back as quickly as possible and you know,

22:47.680 --> 22:53.160
latency and incurred by going back to a more robust off-mind data store.

22:53.160 --> 22:57.000
Sometimes it's just not, it's just not for a moment enough to provide that.

22:57.000 --> 23:03.960
And so you end up wanting to do this sort of challenging activity, which is to do, you

23:03.960 --> 23:09.040
know, training of a model and batch scoring of a model within your data store, where all

23:09.040 --> 23:13.240
the data actually lives on a larger, on a larger platform, like, for example, the

23:13.240 --> 23:20.120
chip, and you want to be able to somehow take the results of that train model and put it

23:20.120 --> 23:24.960
somewhere that's much closer to the user itself, which is probably connected near to some

23:24.960 --> 23:26.840
connected device at the edge.

23:26.840 --> 23:32.160
And it actually means that you need a couple of different environments to be able to do

23:32.160 --> 23:36.200
a scoring of machine learning models or AI models.

23:36.200 --> 23:41.000
And then your production environment doesn't become something that's running, you know,

23:41.000 --> 23:44.680
within a 24 hours of latency, but something that may be running several times a minute

23:44.680 --> 23:45.680
or even a second.

23:45.680 --> 23:49.080
And if you say a couple of different environments, what do you mean?

23:49.080 --> 23:52.160
I mean, I mean, deployment environments from machine learning models.

23:52.160 --> 23:57.840
So for example, historically deployment might look something like, I have a machine learning

23:57.840 --> 24:02.920
model that's written in Python and I'm going to use a cron job or something like that to

24:02.920 --> 24:04.920
schedule a run of the model.

24:04.920 --> 24:09.680
It'll just pull a representative sample from, you know, from some interface layer on top

24:09.680 --> 24:13.760
of the doop and then I'll return the results of that and dump it in a table somewhere

24:13.760 --> 24:14.760
else also to do.

24:14.760 --> 24:19.320
And then people can query that with whatever they want, you know, let's say SQL, right?

24:19.320 --> 24:22.880
So that's a, that's a, that's a, a point of workflow.

24:22.880 --> 24:28.400
And then if you want from an application perspective to return results, you could put some kind

24:28.400 --> 24:32.240
of restful API layer on top of that and then you listen to that endpoint or hit that endpoint

24:32.240 --> 24:35.400
whenever you, you have a request, that's, that's an idea.

24:35.400 --> 24:37.000
Not super performant, right?

24:37.000 --> 24:38.000
Yeah.

24:38.000 --> 24:42.560
So if I was a, you know, a cell phone or a fitness tracker and there were 17 million

24:42.560 --> 24:43.560
of me, right?

24:43.560 --> 24:48.000
And I wanted to do that, that, that same function that I was querying that API directly

24:48.000 --> 24:49.000
from there.

24:49.000 --> 24:50.640
It's pretty challenging, right?

24:50.640 --> 24:54.600
So you actually end up needing a different environment, actually completely different

24:54.600 --> 24:59.120
soft software engineering workloads to deliver that come from performance, but you still

24:59.120 --> 25:03.600
want access to the underlying data because it's still saving the doop and that's how

25:03.600 --> 25:04.600
you train, right?

25:04.600 --> 25:08.920
So this is, you know, sort of an extension of the problem that we were talking about

25:08.920 --> 25:13.920
before, where you actually do, you know, want to have access to the data.

25:13.920 --> 25:19.240
And so having a separate environment that's closer to the closer to the consumer, whether

25:19.240 --> 25:24.440
that be a device or a person, allows you to actually do some pre-processing and treat

25:24.440 --> 25:28.320
pre-printing of features actually at the edge or near the edge.

25:28.320 --> 25:32.800
And so for that, you know, common set of information that are features that I like power these

25:32.800 --> 25:33.800
models.

25:33.800 --> 25:40.160
And wanting to, you know, to do that pre-processing almost as the data streamed itself, right?

25:40.160 --> 25:44.920
And then allow the model that's sitting on that device to pull from that source of data

25:44.920 --> 25:48.520
where it needs to, and then return results much more quickly.

25:48.520 --> 25:53.680
So that's one of the things that I learned while working at a device company is that sometimes

25:53.680 --> 25:58.000
the machine learning challenges are not necessarily from a technical algorithm perspective, but

25:58.000 --> 26:02.480
actually how you apply it and how you apply it to get the outcome that you're looking

26:02.480 --> 26:08.520
for from, you know, from a consumer perspective and sometimes near real time or step second

26:08.520 --> 26:09.520
time.

26:09.520 --> 26:15.800
It sounds like when you describe this architecture, it's in the different environments, you

26:15.800 --> 26:19.560
know, we've talked about kind of the central capability we've talked about the edge, but

26:19.560 --> 26:22.960
it almost sounds like you're laying out a hierarchical environment where you've got,

26:22.960 --> 26:27.080
you know, maybe you've got regions and something that's, you know, immediately behind the

26:27.080 --> 26:33.200
edge, like this is talks about often in an IoT environment, you've got your edge devices

26:33.200 --> 26:37.560
and then you've got your plant device that's kind of a, you know, it's, I don't think

26:37.560 --> 26:40.920
it, I don't know that it's clear what the exact function of that is today.

26:40.920 --> 26:46.520
It's kind of doing some training and intermediate model development or at least that's the people

26:46.520 --> 26:47.520
aspire to that.

26:47.520 --> 26:48.520
Sure.

26:48.520 --> 26:53.640
Today, it's more doing data collection and centralizing the data transfer to, you know,

26:53.640 --> 26:57.800
some mothership or some kind of intermediate mothership.

26:57.800 --> 26:59.080
How do you see all that evolving?

26:59.080 --> 27:00.080
Yeah.

27:00.080 --> 27:05.040
So what I think is starting to happen is because there's a much clearer definition of what

27:05.040 --> 27:09.000
actually you're going to need from a, especially for commoditized models.

27:09.000 --> 27:12.720
So if we're talking about like ensemble models, like random forests are actually boost.

27:12.720 --> 27:16.000
We know it's, you know, we're very, very good at understanding what these models are

27:16.000 --> 27:19.080
going to need from an input layer perspective to be successful, right?

27:19.080 --> 27:24.360
So when you begin to pre-process or pre-trained features or pre-trained data and put that

27:24.360 --> 27:28.520
post to the edge, the interviewer that you're talking about actually can be responsible

27:28.520 --> 27:35.120
for promoting these feature sets and, you know, accomplishing this migration of data with

27:35.120 --> 27:41.520
the very precise scale as opposed to just dumping or translating everything when we think

27:41.520 --> 27:42.520
we might need it.

27:42.520 --> 27:48.440
And I think that's where MLOPS, AIOPS is actually going to start to take a bigger role.

27:48.440 --> 27:54.040
And, you know, really even applying machine learning to, you know, the workflow itself

27:54.040 --> 27:57.760
and what necessarily needs to be accomplished during that phase.

27:57.760 --> 28:03.920
And you start to see the, you know, technologies like airflow and Jenkins, you know, be the control

28:03.920 --> 28:08.720
layer for it, but underlying it, there's actually a machine learning functions that are

28:08.720 --> 28:13.960
powering what we translate into the, you know, the online feature store.

28:13.960 --> 28:16.440
Which I think is something that's really exciting.

28:16.440 --> 28:23.400
Now, airflow is kind of a data workflow open-source project developed by Airbnb.

28:23.400 --> 28:24.400
Airbnb?

28:24.400 --> 28:28.600
I think the original developer came from Facebook and then worked at Airbnb, but yeah,

28:28.600 --> 28:33.800
so airflow is an example of what I'd call an AIOPS or AIOPS tool.

28:33.800 --> 28:39.840
It definitely does scheduling and more flamaticant, just like any other tool like in that category

28:39.840 --> 28:40.840
would do.

28:40.840 --> 28:44.840
But I think what makes it specific is that it's really tuned for the types of techniques

28:44.840 --> 28:48.960
that you'll need to perform in a machine learning workflow specifically.

28:48.960 --> 28:55.360
And so I think that category of, you know, tools is starting to become not just needed,

28:55.360 --> 29:01.920
but also up here in multiple, you know, software options, which shows that it's validated

29:01.920 --> 29:02.920
in the market.

29:02.920 --> 29:05.720
Airflow is an open-source approach to it, but there are certainly a lot of others who

29:05.720 --> 29:07.760
are taking a more commercial approach to it.

29:07.760 --> 29:12.240
And I think it's going to be necessary and is a lot of these companies and, you know,

29:12.240 --> 29:13.240
project scale.

29:13.240 --> 29:17.680
And you've mentioned Jenkins, which, you know, has been around forever, kind of in the

29:17.680 --> 29:19.680
Java community for those bills and stuff.

29:19.680 --> 29:20.680
Right.

29:20.680 --> 29:21.680
How does that fit into this whole?

29:21.680 --> 29:26.280
Well, I think we're, so now we're, like, getting into the idea of how we're going to,

29:26.280 --> 29:28.480
or what is a model, or what is a deployed model.

29:28.480 --> 29:29.480
Yeah.

29:29.480 --> 29:35.320
And so, you know, a deployed model, as we mentioned before, could be simply a table somewhere

29:35.320 --> 29:41.120
that we've scored or we use a function to score, you know, some fitted, some model to

29:41.120 --> 29:42.120
score.

29:42.120 --> 29:44.120
So, you know, the data, the results of data, and put it into a table.

29:44.120 --> 29:46.200
That could be, you know, what we consider deployed.

29:46.200 --> 29:51.240
But I think more and more we're starting to see deployed, meaning it is the model, the

29:51.240 --> 29:56.760
model architecture itself, and the ability to query that, that model architecture for

29:56.760 --> 30:01.280
the result that we're looking for in some kind of performant or way that we can fit into

30:01.280 --> 30:02.280
an SLI.

30:02.280 --> 30:03.440
I think that's what we're starting to see.

30:03.440 --> 30:08.960
And so, when containerization and microservices, you know, are layered together to try to

30:08.960 --> 30:12.640
produce this result holistically, so, like, you know, a model, you know, becomes the

30:12.640 --> 30:17.720
container itself, and the services that it takes to be able to interface with it, tools

30:17.720 --> 30:21.320
like that, you know, automation, workflow management, tools become critical.

30:21.320 --> 30:26.080
Because when you're actually interfacing with a model, you actually might be spending

30:26.080 --> 30:33.200
the whole workflow of a machine, the data, workflow that the machine needs to interact

30:33.200 --> 30:36.880
with, and then performing the scoring, and then actually returning that result all

30:36.880 --> 30:37.880
of us.

30:37.880 --> 30:44.660
And so, it's no longer about just having the skill set to be able to produce a nicely

30:44.660 --> 30:47.240
fitted model against some set of data.

30:47.240 --> 30:51.800
It's actually being able to put, to return that result, and the way that it needs to

30:51.800 --> 30:56.120
be consumed, either by an API or directly to an application.

30:56.120 --> 30:59.480
And so, now we're in this space where we're thinking about things a bit more holistically

30:59.480 --> 31:00.480
than we have to.

31:00.480 --> 31:06.440
Maybe going back a little bit to this kind of federated model, federated idea, and

31:06.440 --> 31:09.960
just kind of in the back of my head thinking through like how this might work.

31:09.960 --> 31:17.240
And at, you know, one thought is at a low level, you know, people are doing things around

31:17.240 --> 31:22.600
distributed training, and that's all centered, or frequently centered around this idea of

31:22.600 --> 31:28.760
like exchanging gradient updates from one machine to another, or, you know, having something

31:28.760 --> 31:35.520
that's tracking all of these gradient updates that the distributed workers involved in training

31:35.520 --> 31:36.680
can access.

31:36.680 --> 31:40.720
Like is that involved in, is that part of what we're talking about, or what we're talking

31:40.720 --> 31:45.960
about, you know, when you talk about federated training and federated ML, is it something

31:45.960 --> 31:47.120
at a higher level?

31:47.120 --> 31:51.080
Well, so I think it's certainly a component of what we're talking about, but I think

31:51.080 --> 31:57.080
we have to be really careful there not to slip into the space of, of training the model

31:57.080 --> 31:59.440
of leaking, essentially, across.

31:59.440 --> 32:02.840
And that's exactly what we don't want to accomplish with federated.

32:02.840 --> 32:05.480
So I think the, and a library on what you mean by that.

32:05.480 --> 32:11.800
So the idea of, you know, even though individual workers or individual models that are, that

32:11.800 --> 32:16.800
are being deployed by workers, don't have access directly to other, to other data stores.

32:16.800 --> 32:19.960
The fact that they could learn the underlying patterns or structures and perhaps in for

32:19.960 --> 32:23.200
what's there, we really have to be careful about that.

32:23.200 --> 32:28.800
And so the idea of a federated sort of abstracts, I think a little bit more, you know, how

32:28.800 --> 32:33.880
to accomplish that, though we're sharing, you know, not just graded information, but really

32:33.880 --> 32:37.600
a larger set of parameters that are, that are, you know, available depending on the

32:37.600 --> 32:38.600
type of model.

32:38.600 --> 32:43.040
And actually, because it's a, it's much more oriented around the techniques, the specific

32:43.040 --> 32:46.080
algorithms that we're using underneath that shouldn't matter as much.

32:46.080 --> 32:50.440
But rather, you know, it is the mechanism for which these distributed workers are able

32:50.440 --> 32:56.320
to share two centralized repository and then that centralized repository performs better,

32:56.320 --> 33:00.640
you know, then in each individual one could do that we're really focused on.

33:00.640 --> 33:06.240
So then it sounds like it is, it is operating at a higher level and the, the goal is less

33:06.240 --> 33:12.640
about, you know, sharing these gradient updates or whatever is a low level kind of information

33:12.640 --> 33:18.920
sharing for the model in question to allow this distributed model to converge and more

33:18.920 --> 33:25.240
like training a model individually at different places and sending key information about that

33:25.240 --> 33:30.440
model, sharing key information about that model to make everyone's models better.

33:30.440 --> 33:31.440
Right.

33:31.440 --> 33:37.560
And so, again, the sort of central idea, if you were asked to summarize that would be

33:37.560 --> 33:42.080
that, you know, if you have, you know, the ability to take a random of these techniques,

33:42.080 --> 33:47.000
which might be, you know, ultimately computationally expensive, it's in to do across such a large

33:47.000 --> 33:52.800
set of independent workers, you would achieve an outcome which would be impossible without

33:52.800 --> 33:57.280
that centralized sharing information, without that federated sharing information.

33:57.280 --> 34:05.240
So the other thing that this makes me think a little bit about is differential privacy.

34:05.240 --> 34:11.320
Is that something that you've looked at previously in other roles or that you are looking

34:11.320 --> 34:16.320
at in the context of, of this federated learning?

34:16.320 --> 34:17.320
Yeah, definitely.

34:17.320 --> 34:21.000
I think it's going to, we're going to definitely try to write about that, but the research

34:21.000 --> 34:22.000
is ongoing.

34:22.000 --> 34:25.440
So I wouldn't be able to tell you what we've uncovered yet because we're literally looking

34:25.440 --> 34:27.440
at some of these things right now.

34:27.440 --> 34:28.440
Okay.

34:28.440 --> 34:34.400
You talked a little bit about online versus offline learning often when the whole online

34:34.400 --> 34:40.520
learning comes up, it comes up in the context of spark and streaming and pipelines.

34:40.520 --> 34:42.720
How do you see that fitting into this?

34:42.720 --> 34:43.720
Yeah.

34:43.720 --> 34:50.520
So when I was describing this area earlier where, you know, where you might want to respond

34:50.520 --> 34:55.880
to, you know, to an activity or something that has occurred, like, for example, with

34:55.880 --> 35:02.080
a fitness tracker, those exchanges of data are happening with real-time streams.

35:02.080 --> 35:06.640
So, you know, I don't think it's a secret to anyone that's, you know, that's spark streaming

35:06.640 --> 35:14.400
and really, you know, things like Kafka, you know, interfaces between these devices and

35:14.400 --> 35:17.120
our key component of how this works.

35:17.120 --> 35:21.200
So what I would say is that's the piping behind this process is that we've been talking

35:21.200 --> 35:23.200
about.

35:23.200 --> 35:29.840
What makes it a little bit more challenging is the sort of potentially asynchronous nature

35:29.840 --> 35:35.200
of how data works in that area, and this is back to the sort of Cisco days, and the, you

35:35.200 --> 35:40.440
know, for lack of a better term, the UDPness of it, where you may receive some or all the

35:40.440 --> 35:46.960
data, some of it may be at order, probably will be, and, you know, you still have to find

35:46.960 --> 35:49.560
a way to coherently respond to that.

35:49.560 --> 35:53.400
And I think that component of it becomes a data engineering challenge, which is at the

35:53.400 --> 35:57.000
edge, combined with a machine learning challenge, which is at the edge, combined with a software

35:57.000 --> 35:58.360
engineering challenge at that the edge.

35:58.360 --> 36:01.680
And so we have to solve all these problems very, very quickly.

36:01.680 --> 36:06.160
And so what I've seen in industry so far is I've worked on it is that everyone's architecting

36:06.160 --> 36:12.360
slightly different workflows are on this, and it's probably time that we and others begin

36:12.360 --> 36:16.800
to standardize that a bit to make it more accessible for the community.

36:16.800 --> 36:19.920
And so we're specifically talking about workflows.

36:19.920 --> 36:26.480
Are you talking about the whole, the federated concepts specifically, or we're not, we're

36:26.480 --> 36:31.640
talking about the workforce for managing these edge devices and software engineering for

36:31.640 --> 36:32.640
edge devices.

36:32.640 --> 36:33.640
Yeah.

36:33.640 --> 36:38.000
I mean, you asked about the, about, you know, how a premium instrument plays into it.

36:38.000 --> 36:41.800
And I think it, like I said before, it's the piping that regardless of what application

36:41.800 --> 36:45.480
you're trying, whether it's a federated learning mechanism, or simply you're just trying

36:45.480 --> 36:49.800
to deliver online services, machine learning services, you're going to have to interact

36:49.800 --> 36:51.040
with that environment.

36:51.040 --> 36:53.280
And the ecosystem right now is pretty complex.

36:53.280 --> 36:56.960
And I think we've got to do some work to, you know, to at least abstract some of it so

36:56.960 --> 37:00.800
that we're more focused on solving the problem than dealing with engineering workflows, which

37:00.800 --> 37:02.760
is where we currently are.

37:02.760 --> 37:06.160
And how close do you think we are to being able to do that?

37:06.160 --> 37:11.400
We've advanced engineering practices, you know, with DevOps, for example, significantly

37:11.400 --> 37:18.640
over the past few years, and we've got a growing base of experience with mobile.

37:18.640 --> 37:26.400
Although for, you know, from the device perspective, the rate of change of, you know, what's

37:26.400 --> 37:32.640
happening in the device tends to be a lot slower than what we'd want in a kind of an IoT

37:32.640 --> 37:35.160
enterprise, more agile environment.

37:35.160 --> 37:41.320
But with, you know, with those as kind of background, you know, then we've got these two, you

37:41.320 --> 37:44.920
know, IoT, which is changing quickly, and we're just kind of wrapping our heads around

37:44.920 --> 37:49.720
and trying to figure out men to layer onto that, the machine learning and AI stuff.

37:49.720 --> 37:54.120
Like how do you think we as an industry like put together?

37:54.120 --> 37:55.120
You mentioned standards.

37:55.120 --> 38:00.280
It seems like we're so far from standards, you know, just a way that works that consistently

38:00.280 --> 38:01.600
to manage all of this stuff.

38:01.600 --> 38:02.600
Yeah.

38:02.600 --> 38:11.440
I don't think that would be a smart thing to do, but I will say that it might be useful

38:11.440 --> 38:18.000
for us to maybe go a little bit away from segregating ourselves by community.

38:18.000 --> 38:23.520
There are really well, very well understood, very powerful practices and networks that

38:23.520 --> 38:26.640
have existed for years that might be really helpful for us as some of these challenges.

38:26.640 --> 38:31.560
They've been dealing with real-time streaming since the beginning of computers sharing

38:31.560 --> 38:33.120
information with each other, right?

38:33.120 --> 38:36.280
I mean, I spent a lot of time at Cisco, and let me tell you, there's some people there

38:36.280 --> 38:40.080
who have gotten down to the algorithm level and digester shortest path and can tell you

38:40.080 --> 38:43.960
exactly how messages respond to that, you know, from device to device when you're talking

38:43.960 --> 38:45.800
about the context of a router.

38:45.800 --> 38:47.040
And those standards exist.

38:47.040 --> 38:51.160
They're actually really, they're very well documented, they're published, they're public.

38:51.160 --> 38:56.640
So I think that we have examples of how to accomplish this, you know, sort of standards-based

38:56.640 --> 39:01.320
practice out there, but it may not come from the community we're familiar with, right?

39:01.320 --> 39:05.600
So as a machine learning practitioner or a data scientist, you're probably a lot more

39:05.600 --> 39:08.920
familiar with the software engineering community, and you're probably a lot more familiar with

39:08.920 --> 39:14.680
people who are publishing standards by way of, you know, GitHub or standards by way, you

39:14.680 --> 39:21.840
know, of library, but we may not have access to or may not have thought about, you know,

39:21.840 --> 39:26.920
applications for handling messaging or handling workflows, you know, even a lower layer

39:26.920 --> 39:31.440
of the stack and the computing and computing, and I think it's going to be this interface

39:31.440 --> 39:36.000
of communities that are happening that helps us to build out these standards and practices

39:36.000 --> 39:41.000
that make sense because we are not talking about one community, we're actually talking,

39:41.000 --> 39:45.360
we just talked in this conversation about multiple different architectures converging.

39:45.360 --> 39:49.560
So I think we need to expand our view a little bit and start to look for places and people

39:49.560 --> 39:53.960
that have experiences, you know, potentially a different part of the stack that can help

39:53.960 --> 39:59.080
us be successful where we are here in the real time of online and mental conversation.

39:59.080 --> 40:03.600
You know, you talked about Spark as being core to this streaming environment.

40:03.600 --> 40:11.000
Is there an analogous environment or platform on the Python side that's gained any traction?

40:11.000 --> 40:18.280
Yeah, not that I'm aware of, and I think largely that's happening because of the way that

40:18.280 --> 40:21.040
Python tends to interact with data in general.

40:21.040 --> 40:24.520
I think data scientists, data scientists, we've tended, and this might just be a community

40:24.520 --> 40:25.520
issue.

40:25.520 --> 40:29.800
We've tended to use it on the, you know, the research and dev side.

40:29.800 --> 40:33.040
And so interacting with data, you know, that the more abstracted, I mean, the less abstracted

40:33.040 --> 40:37.800
layer really at that point has not been something I've seen a lot of research poured into,

40:37.800 --> 40:38.800
but I could be wrong about that.

40:38.800 --> 40:42.280
So, you know, I think I haven't seen it, but, you know, that's not to say it doesn't

40:42.280 --> 40:43.280
exist.

40:43.280 --> 40:44.280
All right.

40:44.280 --> 40:50.520
So you, you mentioned a term AI ops, or you may have said ML ops a couple of times I've

40:50.520 --> 41:00.400
done some writing about that term, but as applied to AI enabled IT operations, so like managing

41:00.400 --> 41:07.320
the data center with AI, are you starting to see, or are you starting to evangelize the

41:07.320 --> 41:16.720
use of that term to specifically refer to ops as it relates to AI, meaning managing the

41:16.720 --> 41:19.320
AI infrastructure of an organization?

41:19.320 --> 41:20.320
Yeah.

41:20.320 --> 41:23.160
So, I think that's where I would hope we would go.

41:23.160 --> 41:27.160
I mean, you know, who knows what these terminologies are going to end up, but I mean, we're

41:27.160 --> 41:31.480
talking about AI when, you know, 10 years ago we've been talking about, you know, applied

41:31.480 --> 41:32.480
statistics.

41:32.480 --> 41:36.800
So, you could go anywhere, but I do think that that's a really powerful way to use that

41:36.800 --> 41:42.360
term, because I think in a lot of ways, I actually saw a tweet the other day that was really

41:42.360 --> 41:43.360
great.

41:43.360 --> 41:47.800
You know, that was talking about, you know, asking, you know, software engineers and IT

41:47.800 --> 41:54.800
people to deploy models, being just like, you know, trying to have, see world, take care

41:54.800 --> 41:58.560
of your drafts, just because they take care of other large scale mammals, right?

41:58.560 --> 42:02.480
Like, it's really completely different set of skills, yeah.

42:02.480 --> 42:07.520
And so, when we think about software engineering, we have a term, we have a terminology, we

42:07.520 --> 42:12.760
have a framework, we actually have tools and entire category of software that's devoted

42:12.760 --> 42:16.920
to taking care of the operations necessary to manage the software infrastructure that runs

42:16.920 --> 42:17.920
the company, right?

42:17.920 --> 42:20.960
It's very, it's a huge business and it's actually really important.

42:20.960 --> 42:25.400
But from an AI perspective, historically, and I've experienced this personally, you're

42:25.400 --> 42:32.520
usually either asked to do it yourself or you're, you're translating or giving, you know,

42:32.520 --> 42:37.600
your baby, so to speak, to IT or engineering organization and telling them to scale that.

42:37.600 --> 42:42.880
And what almost universally happens, they have no idea why you made those decisions.

42:42.880 --> 42:47.280
They disagree with them, sometimes for really good reason, about the way that you approach

42:47.280 --> 42:48.680
it from a software engineering perspective.

42:48.680 --> 42:52.280
And so, they bring engineer the whole thing and miss a lot of the core components of

42:52.280 --> 42:53.280
what you were trying to accomplish.

42:53.280 --> 42:55.040
Do you have a specific example of that?

42:55.040 --> 42:59.880
Yeah, so I can tell you that one of the first models that I actually developed myself

42:59.880 --> 43:07.680
in industry, you know, was written in NAR, I think like a lot of us were doing it.

43:07.680 --> 43:14.320
It was a simple logistic progression, or turned a binary result and I felt like it was quite

43:14.320 --> 43:18.680
generalizable for the problems that we're looking at, so I delivered it to my IT organization

43:18.680 --> 43:23.080
to be applied back into one of the products that was actually already online for one of our

43:23.080 --> 43:27.680
internal tools and they wrote the entire thing in Java.

43:27.680 --> 43:36.080
And so the challenge with that was, you know, Java has a lot of, it's very verbose.

43:36.080 --> 43:44.600
And so the performance that I had sort of expected out of my small, like, you know, seven-line

43:44.600 --> 43:48.960
Python code, now had a whole bunch of other dependencies that I'd never planned for.

43:48.960 --> 43:55.960
And so the application, the AI application, so to speak, that returned, you know, sub-second

43:55.960 --> 44:00.440
or very close to second results for the data set size I was looking at now took, you

44:00.440 --> 44:02.280
know, five to ten minutes to run.

44:02.280 --> 44:08.440
Probably we think of going from, you know, our Python to Java is a step forward in performance,

44:08.440 --> 44:09.440
right?

44:09.440 --> 44:10.440
Totally.

44:10.440 --> 44:13.960
If only that's all you're doing, but in the context of it being run in a much more robust

44:13.960 --> 44:19.480
application, which had not quite a few other dependencies that I never cared about, you

44:19.480 --> 44:21.480
know, that that's isn't necessarily the case.

44:21.480 --> 44:27.800
And so, you know, I think if we'd had a much more robust AI workflow in place, I would

44:27.800 --> 44:35.680
be deploying my model, so to speak, you know, and the workflow or even the AI tool kit

44:35.680 --> 44:40.440
itself would decide the best way to implement that or to return that rather than, you know,

44:40.440 --> 44:44.800
the IT organization myself hadn't had the negotiation for every single product that we

44:44.800 --> 44:46.120
developed, right?

44:46.120 --> 44:51.600
And so that, you see that all the time when someone checks in code, you know, to, you

44:51.600 --> 44:57.040
know, to a DevOps workflow, they have no idea how it might be integrated at the end.

44:57.040 --> 44:58.040
It doesn't matter.

44:58.040 --> 45:02.800
It's that the core component of what the application that they've developed gets integrated

45:02.800 --> 45:03.800
into the overall.

45:03.800 --> 45:06.960
And so we just got to get there from an AI perspective.

45:06.960 --> 45:10.760
And I think we're starting to make a lot of progress on that.

45:10.760 --> 45:16.920
Years ago, maybe five years ago, let's say, you know, we would talk about data scientists

45:16.920 --> 45:20.440
as this unicorn, you know, did everything, right?

45:20.440 --> 45:28.680
And now more often than not, we're starting to see, you know, specifically in internet

45:28.680 --> 45:29.680
companies, right?

45:29.680 --> 45:35.160
You've got this very well-defined role now of machine learning engineer that works, you

45:35.160 --> 45:37.800
know, tends to work hand in hand with data science.

45:37.800 --> 45:38.800
Sure.

45:38.800 --> 45:42.280
People are even changing, you know, data scientists is becoming a bit passe and there's

45:42.280 --> 45:46.080
like applied research scientists and these other roles, but they're, it's much more

45:46.080 --> 45:54.200
of a teaming kind of relationship between these two and the ML engineer than inherits, you

45:54.200 --> 45:58.440
know, because they are an engineer, they kind of inherit everything that's been developed,

45:58.440 --> 46:00.840
you know, in terms of DevOps processes.

46:00.840 --> 46:01.840
Yep.

46:01.840 --> 46:07.760
And it seems to be that a lot of that is as much smoother, you know, for organizations

46:07.760 --> 46:09.280
that kind of have that worked out.

46:09.280 --> 46:10.280
Sure.

46:10.280 --> 46:13.640
This tends to be internet companies, not enterprises, there's still a lot of work that

46:13.640 --> 46:16.800
needs to be done, I think, to kind of translate these practices.

46:16.800 --> 46:17.800
I think that's right.

46:17.800 --> 46:18.800
But we're getting there.

46:18.800 --> 46:20.000
And I think you're right about that.

46:20.000 --> 46:21.000
This is a good story.

46:21.000 --> 46:24.920
I'm not, you hear that, like, to sell, you know, doom and gloom, I think we're absolutely

46:24.920 --> 46:25.920
getting there.

46:25.920 --> 46:28.320
I would expand your list to data first companies.

46:28.320 --> 46:33.120
Companies then, like, use, you know, information that they either generate or derive from their

46:33.120 --> 46:37.520
customers as the main product that they then, you know, present back to their customers.

46:37.520 --> 46:39.280
That's a great distinction, yeah.

46:39.280 --> 46:43.840
And so, you know, there are many there in that category, but the ride sharing, you know,

46:43.840 --> 46:49.680
companies, you know, anyone that's doing consumer pricing, you know, like hotel companies,

46:49.680 --> 46:52.680
come to mind is the ones that are really innovating in this area.

46:52.680 --> 46:55.720
It's a little slower for utility companies.

46:55.720 --> 46:59.840
It's a little slower for telecoms and it's a little bit slower, you know, for governments.

46:59.840 --> 47:03.520
And I think these are some of the organizations which actually have the data scale that we

47:03.520 --> 47:05.680
really want to interact with.

47:05.680 --> 47:08.520
And I haven't seen as much of that there, but we're on the way.

47:08.520 --> 47:14.480
And to your point, if we can get the tooling, you know, ready or closer to being, you know,

47:14.480 --> 47:18.920
being mature, as we build those roles in those organizations, I think that's a really

47:18.920 --> 47:19.920
good marriage.

47:19.920 --> 47:20.920
Yeah.

47:20.920 --> 47:21.920
Yeah.

47:21.920 --> 47:22.920
Awesome.

47:22.920 --> 47:24.920
Well, Justin, thanks so much for taking the time to chat with us.

47:24.920 --> 47:27.920
It's been great to meet you.

47:27.920 --> 47:30.920
Welcome to CFFL here to like 90 days.

47:30.920 --> 47:32.920
This is day 90.

47:32.920 --> 47:33.920
Awesome.

47:33.920 --> 47:34.920
Awesome.

47:34.920 --> 47:37.920
Well, I enjoyed the rest of the conference and thank you.

47:37.920 --> 47:45.240
All right, everyone, that's our show for today.

47:45.240 --> 47:49.920
For more information on Justin or any of the topics we covered in this show, visit

47:49.920 --> 47:54.640
twomolei.com slash talk slash 185.

47:54.640 --> 48:00.680
For more information on the entire strata data series, visit twomolei.com slash strata

48:00.680 --> 48:03.520
and why 2018.

48:03.520 --> 48:07.480
Once again, a big thanks to our sponsors, Cladera and Capital One for their sponsorship

48:07.480 --> 48:09.080
of this series.

48:09.080 --> 48:37.600
As always, thanks so much for listening and catch you next time.

