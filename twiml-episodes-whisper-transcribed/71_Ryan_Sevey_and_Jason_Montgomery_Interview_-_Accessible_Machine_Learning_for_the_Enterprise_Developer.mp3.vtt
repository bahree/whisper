WEBVTT

00:00.000 --> 00:15.920
Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting

00:15.920 --> 00:20.880
people doing interesting things in machine learning and artificial intelligence.

00:20.880 --> 00:23.120
I'm your host Sam Charrington.

00:23.120 --> 00:28.600
Thanks so much to those of you who participated in the Twimble Online Meetup last week, and

00:28.600 --> 00:31.680
it's Kevin T from Sigopt for presenting.

00:31.680 --> 00:36.040
You can find the slides for Kevin's presentation in the Meetup Slack channel as well as in

00:36.040 --> 00:37.840
this week's show notes.

00:37.840 --> 00:42.400
Our final Meetup of the Year will be held on Wednesday, December 13th.

00:42.400 --> 00:48.160
Bring your thoughts on the top machine learning and AI stories of 2017 for our discussion segment.

00:48.160 --> 00:53.560
For our main presentation, prior Twimble Talk guest Bruno GonzÃ¡lez will be discussing

00:53.560 --> 00:59.160
the paper, Understanding Deep Learning requires rethinking generalization.

00:59.160 --> 01:03.720
By Xiu Juan Zhang from MIT and Google Brain and others.

01:03.720 --> 01:09.440
You can find more details and register at twimlai.com slash Meetup.

01:09.440 --> 01:13.600
If you receive my newsletter, you already know this, but Twimble is growing and we're

01:13.600 --> 01:19.440
looking for an energetic and passionate community manager to help expand our programs.

01:19.440 --> 01:23.760
This full-time position can be remote, but if you happen to live in St. Louis, all the

01:23.760 --> 01:24.760
better.

01:24.760 --> 01:28.120
If you're interested, please reach out to me for additional details.

01:28.120 --> 01:32.040
I should mention that if you don't already get my newsletter, you are really missing

01:32.040 --> 01:37.120
out and should visit twimlai.com slash newsletter to sign up.

01:37.120 --> 01:42.000
This week we'll be featuring a series of shows recorded from Strange Loop, a great developer

01:42.000 --> 01:46.680
focus conference that takes place every year right in my backyard.

01:46.680 --> 01:49.880
Not literally in my backyard, but here in St. Louis.

01:49.880 --> 01:54.520
The conference is a multidisciplinary melting pot of developers and thinkers across a variety

01:54.520 --> 01:58.960
of fields and we're happy to be able to bring a bit of it to those of you who couldn't

01:58.960 --> 02:00.680
make it in person.

02:00.680 --> 02:06.000
Later this week, you'll hear from Sumit Shintala, a research engineer at Facebook AI Research

02:06.000 --> 02:12.680
Lab, Matt Taylor, open source community manager at Nementa, Alison Parrish, professor in the

02:12.680 --> 02:19.480
interactive telecommunications program at NYU and Sam Richie, software engineer at Stripe.

02:19.480 --> 02:25.360
We'd like to send a huge shout out to Nexosis, who helped make this series possible.

02:25.360 --> 02:30.000
Nexosis is a company focused on making machine learning more easily accessible to enterprise

02:30.000 --> 02:31.000
developers.

02:31.000 --> 02:34.800
In fact, you'll learn a bunch more about the company and what they're up to in this

02:34.800 --> 02:40.960
show, which features my interview with Nexosis Founders, Ryan C.V. and Jason Montgomery.

02:40.960 --> 02:45.920
Ryan and Jason and I discuss how they got their start by applying ML to identifying cheaters

02:45.920 --> 02:51.360
in video games, the application of machine learning for time series data analysis, and

02:51.360 --> 02:54.520
of course, the Nexosis machine learning API.

02:54.520 --> 02:59.880
If you like what you hear, then invite you to get your free Nexosis API key and discover

02:59.880 --> 03:02.640
what they can bring to your next project.

03:02.640 --> 03:11.480
You can do that at nexosis.com slash twimmel, that's n-e-x-o-s-i-s dot com slash twimmel.

03:11.480 --> 03:14.480
And now on to the show.

03:14.480 --> 03:25.600
Hey everyone, I am on the line with Ryan C.V. and Jason Montgomery.

03:25.600 --> 03:33.720
Ryan is the CEO and co-founder of Nexosis and Jason is the C.T.O. and also a co-founder.

03:33.720 --> 03:36.120
Welcome to this week in machine learning and AI, guys.

03:36.120 --> 03:37.120
Thanks for having us.

03:37.120 --> 03:38.120
Yeah, awesome to be here.

03:38.120 --> 03:40.160
Thanks so much for hosting us.

03:40.160 --> 03:41.160
Fantastic.

03:41.160 --> 03:42.160
Fantastic.

03:42.160 --> 03:46.760
So, as is the tradition here on the show, why don't we get started by having each of

03:46.760 --> 03:51.960
you introduce yourselves and tell us a little bit about your journey to machine learning

03:51.960 --> 03:52.960
and AI?

03:52.960 --> 03:59.560
Yeah, sure, so happy to, as you mentioned, I'm the CEO and co-founder of Nexosis, Jason

03:59.560 --> 04:05.040
here is our C.T.O. and kind of just going back to maybe the beginning of how all this

04:05.040 --> 04:06.040
came to be.

04:06.040 --> 04:11.040
Jason and I actually met each other at a company called American Electric Power, which is headquartered

04:11.040 --> 04:12.240
here in Kamba, Ohio.

04:12.240 --> 04:17.160
Him and I were both on the cybersecurity engineering team.

04:17.160 --> 04:24.640
And AP, I believe is the largest generator of electricity in the United States.

04:24.640 --> 04:27.880
So when you think about that, you can think about all the different assets that they have

04:27.880 --> 04:34.240
to put in the field, which kind of directly correlates to how much data is being collected.

04:34.240 --> 04:39.560
And basically our task bear was to ensure that all their assets were secure from both

04:39.560 --> 04:42.400
internal and external attackers.

04:42.400 --> 04:47.640
We quickly realized that the amount of data that was being generated would be pretty much

04:47.640 --> 04:53.000
impossible for the analyst team to go through and identify any kind of anomalies things of

04:53.000 --> 04:54.320
that nature.

04:54.320 --> 04:58.200
So Jason and I then started thinking and hearing more about machine learning and there were

04:58.200 --> 05:02.400
classes offered online, I believe through Stanford University.

05:02.400 --> 05:05.640
And him and I both signed up to do these online courses.

05:05.640 --> 05:10.800
And we went and did all the homework, did all the, you know, kind of bonus material, if

05:10.800 --> 05:14.280
you will, and we got to a point where the instructor said, okay, you now know more than

05:14.280 --> 05:15.520
everyone in the valley.

05:15.520 --> 05:19.680
And we're like, all right, that's a good stopping point.

05:19.680 --> 05:26.520
So it was becoming very, very, just becoming very mathematical and that's not necessarily

05:26.520 --> 05:27.520
that thing.

05:27.520 --> 05:33.400
But, you know, we are both very technical individuals, lots of development experience, things of that

05:33.400 --> 05:37.320
in our background and just quite frankly, like learning six players of mathematics isn't

05:37.320 --> 05:38.320
that appealing.

05:38.320 --> 05:43.720
But it's not, you know, like cool, I guess it's neat to understand the math behind this

05:43.720 --> 05:47.160
one algorithm, but quite frankly, that wasn't where we're taking it, right?

05:47.160 --> 05:49.840
We were, we were taking a class early, I understand how we could use machine learning

05:49.840 --> 05:52.240
to solve a very particular problem.

05:52.240 --> 05:58.600
So out of that, we just kind of long story short, Jason ended up leaving American Electric

05:58.600 --> 05:59.600
Power.

05:59.600 --> 06:00.600
He went to varicode.

06:00.600 --> 06:05.000
He was a dot net, principal dot net researcher for them, varicode is an information security

06:05.000 --> 06:08.680
company that does software.

06:08.680 --> 06:12.960
Yeah, we do a binary static analysis, you submit your binaries and we find flaws in the

06:12.960 --> 06:17.720
code and give you sort of a report that says, you need to fix code in these places or

06:17.720 --> 06:18.720
what not.

06:18.720 --> 06:22.480
So I did, I was a researcher there for about three years.

06:22.480 --> 06:27.920
Reason we bring that up is when he left AP to go to varicode, I shortly thereafter left

06:27.920 --> 06:34.000
AP to go to healer Packard, but Jason, I kept doing joint research projects together

06:34.000 --> 06:38.040
just mostly out of fun, right, in our spare free time.

06:38.040 --> 06:42.200
So that means that we lost access to the AP data, which was, that's fine.

06:42.200 --> 06:45.880
And we were kind of still enthralled with this notion of what machine learning could

06:45.880 --> 06:46.880
do.

06:46.880 --> 06:49.080
And we started thinking about other use cases.

06:49.080 --> 06:53.760
And I forget exactly how it happened, but I asked Jason one day why he didn't enjoy

06:53.760 --> 07:00.040
online gaming and his response was basically, there's a lot of cheaters in it and I want

07:00.040 --> 07:04.240
to know if I'm getting, you know, under nominated by someone that video gave that it's because

07:04.240 --> 07:07.240
they're actually better than me and not because they're using cheats.

07:07.240 --> 07:11.080
So I thought, well, that's kind of an interesting feedback.

07:11.080 --> 07:16.080
I said, I wonder if we could use machine learning to identify patterns and, you know, public

07:16.080 --> 07:20.880
data sets available via the steam API that would identify a cheater.

07:20.880 --> 07:25.120
And I think really the whole notion back then was if you're cheating, you're trying

07:25.120 --> 07:31.560
to do something that a normal human wouldn't do, so it should show up in the stats, right?

07:31.560 --> 07:37.400
Like if you see this ridiculously high, like headshot percentage, you're probably cheating

07:37.400 --> 07:39.240
unless you're a professional, right?

07:39.240 --> 07:43.800
Or into the stats show, like for the last two years, you have like an accuracy rating

07:43.800 --> 07:47.680
of like 15%, and then suddenly overnight your accuracy jumps to 50%.

07:47.680 --> 07:50.280
Well, that's suspicious, right?

07:50.280 --> 07:55.480
So steam makes it pretty easy to get all that data that we needed via their API.

07:55.480 --> 08:01.160
So we spent about six months and we created a proof of concept that basically had a pretty

08:01.160 --> 08:02.160
good accuracy rating.

08:02.160 --> 08:08.200
It was like 88% accurate on identifying whether or not a player should be banned by

08:08.200 --> 08:09.400
valving a Thai cheat.

08:09.400 --> 08:15.000
Well, let me, if I can interrupt and ask a question or a couple of questions, I'm assuming

08:15.000 --> 08:21.120
then that you were using supervised learning for this task, and if that's the case, what

08:21.120 --> 08:22.600
did you use for labels?

08:22.600 --> 08:27.600
Did you manually label some number of users that you thought were cheaters?

08:27.600 --> 08:33.600
And then how did you validate this to determine your 88% accuracy rate did?

08:33.600 --> 08:38.800
Did steam also publish, you know, whether they thought folks were cheating or did you just

08:38.800 --> 08:41.280
look and see if they were eventually banned or something like that?

08:41.280 --> 08:42.480
Yeah, that's a good question.

08:42.480 --> 08:47.720
So what we did was we took the professional gaming league websites.

08:47.720 --> 08:53.080
We took data from there and our theory behind using them to train not cheaters was that

08:53.080 --> 08:57.480
people tend to be watching the professional gamers more, there's more eyes on them.

08:57.480 --> 08:59.520
So cheating would be more obvious.

08:59.520 --> 09:02.320
And then we went to the, there was a website called Backband.

09:02.320 --> 09:06.600
Its whole purpose was to take steam IDs and categorize those that got banned based on

09:06.600 --> 09:07.920
certain dates.

09:07.920 --> 09:12.600
We were able to correlate those with the steam API and pull down their game stats.

09:12.600 --> 09:14.000
We did some cleaning of the data.

09:14.000 --> 09:18.720
We looked at, you know, when they purchased CSGO if they had the game because we, the data

09:18.720 --> 09:22.080
set wasn't perfect like all data has lots of crap in it.

09:22.080 --> 09:28.040
But we spent a lot of time sort of analyzing it and making sure we had a pretty, pretty

09:28.040 --> 09:32.880
confident set of cheaters and a pretty confident set of those we felt weren't cheating.

09:32.880 --> 09:34.720
And that was sort of the process we went.

09:34.720 --> 09:38.400
Awesome. And so what do you, what do you do with this proof of concept?

09:38.400 --> 09:39.400
Yeah.

09:39.400 --> 09:44.480
So at this point in time, we released the research to the public at a security conference

09:44.480 --> 09:48.240
called Derby Con and I think 2014.

09:48.240 --> 09:52.240
And there were some Reddit posts and it got a lot of traction actually valve reach out

09:52.240 --> 09:57.400
to me about the research and kind of was asking how we did it and whatnot.

09:57.400 --> 10:01.520
And I told them because at the time, that was like, well, fix the issue that be more

10:01.520 --> 10:05.120
than enough for us.

10:05.120 --> 10:12.160
So that kind of naturally led to the formation of Nexosis because as we were going through

10:12.160 --> 10:19.840
the model building, we tried using different services like Microsoft's ML Studio, we tried

10:19.840 --> 10:24.000
using Amazon's API for machine learning.

10:24.000 --> 10:27.000
We tried using Google's product API.

10:27.000 --> 10:32.960
And at the time, that toe was still a company which became Tory, which then sold the Apple.

10:32.960 --> 10:37.160
So we tried all these different things on the market and the conclusion that we reached

10:37.160 --> 10:40.720
from all this was nothing's really out there for the developer.

10:40.720 --> 10:45.240
Everything is really catered and aimed at the data scientists.

10:45.240 --> 10:50.920
You so often understand how to train and build a model and then you just get into this

10:50.920 --> 10:53.600
question of how do I make the model better.

10:53.600 --> 10:57.160
And like fundamentally, you have to know the algorithm you want to use and you have to

10:57.160 --> 11:01.800
know why you would want to use algorithm A over algorithm B. So there's just lots of issues

11:01.800 --> 11:06.800
that from a developer point of view, i.e., we want to just get something out there that's

11:06.800 --> 11:11.320
doing a good job, a very, very painful experience.

11:11.320 --> 11:15.240
So we then set out with that knowledge and tact and we said, look, why don't we look at

11:15.240 --> 11:21.080
machine learning under the lens of a developer and how would a developer want to consume machine

11:21.080 --> 11:22.800
learning.

11:22.800 --> 11:27.400
And this has been done in other industries for a while, right?

11:27.400 --> 11:32.800
So the one that I can point to just off off my head is Twilio.

11:32.800 --> 11:38.200
And you think about what Twilio did, kind of very similar when you look at the data science

11:38.200 --> 11:40.000
machine learning landscape.

11:40.000 --> 11:44.560
Prior to Twilio, if you wanted to create a communications application, you would have

11:44.560 --> 11:51.200
to really have someone on staff that understood the telcal infrastructure, understood voice

11:51.200 --> 11:55.200
over IP protocols and how to configure it.

11:55.200 --> 11:58.640
We actually have a slide here internally that kind of compares, here's what they'll look

11:58.640 --> 12:01.160
like before Twilio and here's what they'll look like after.

12:01.160 --> 12:04.920
I think the world today looks very similar on the data science point of view, right?

12:04.920 --> 12:11.000
You have to know all the different just how do you do VTL, how do you deal with missing

12:11.000 --> 12:12.000
variables?

12:12.000 --> 12:13.240
There's just so much to it.

12:13.240 --> 12:18.000
And then again, how do you know what algorithm you want to use and when blah, blah, blah,

12:18.000 --> 12:19.000
right?

12:19.000 --> 12:20.600
It becomes very complicated.

12:20.600 --> 12:25.440
So the whole notion of exosys is saying, look, developers, you use the same language that

12:25.440 --> 12:26.520
you're used to using.

12:26.520 --> 12:32.360
If you're a.NET developer, come to us, you know,.NET, C-Sharve, if you're a Java developer,

12:32.360 --> 12:33.360
then go use Java.

12:33.360 --> 12:38.280
You don't have to learn a new language to incorporate machine learning to your project.

12:38.280 --> 12:41.440
And that's really the whole basis of exosys.

12:41.440 --> 12:42.440
Okay.

12:42.440 --> 12:49.040
When I think about what, let's take Azure ML for instance, when I think about what they're

12:49.040 --> 12:50.040
trying to do.

12:50.040 --> 12:53.000
It sounds exactly like what you're describing, right?

12:53.000 --> 12:59.200
They're trying to create an API that lets a developer deliver, you know, machine learning

12:59.200 --> 13:00.600
types of applications.

13:00.600 --> 13:05.920
You know, they've got the studio, you can even do it drag and drop if you're so interested.

13:05.920 --> 13:10.680
If you're interested in doing so, maybe we can dig a little deeper and you can, when

13:10.680 --> 13:16.000
you're faced with skeptics that say, you know, how can you enable a developer to do machine

13:16.000 --> 13:20.360
learning without knowing anything about data science, like how do you address that?

13:20.360 --> 13:21.960
It's a good question.

13:21.960 --> 13:27.680
When we looked at ML Studio, for instance, a lot of the work, as you know, is data preparation,

13:27.680 --> 13:30.760
data cleaning, ETL, it's like 80% of the work.

13:30.760 --> 13:35.560
And so, and then they're scaling, there's how do you do imputation strategies, how do

13:35.560 --> 13:39.240
you aggregate data, all these sort of questions.

13:39.240 --> 13:42.880
And developers are very used to using working with data and data types.

13:42.880 --> 13:47.200
And you can typically take a data type, identify, and you kind of know what step you need to

13:47.200 --> 13:50.040
do to impute or aggregate at that point.

13:50.040 --> 13:55.480
And so, giving the developer the tools in their hands to sort of define maybe the data types

13:55.480 --> 13:59.600
with metadata, but not really having to worry about what needs to happen before it

13:59.600 --> 14:03.200
can go into the algorithm, before we convert it all into numbers, whether we need to one

14:03.200 --> 14:08.120
hot and code, categorical data, things like that, you know, how do we pick features.

14:08.120 --> 14:11.880
All of that stuff is, there's a lot of automation that can be done to simplify that.

14:11.880 --> 14:15.680
Now, you still have to know your data, you have to know a good question you want to ask,

14:15.680 --> 14:19.640
you have to validate that, you know, you are submitting features that make sense.

14:19.640 --> 14:23.280
We're not going to know your data for you, but in a lot of ways, we can automate some

14:23.280 --> 14:28.760
of that heavy lifting at scale, and then we can build lots and lots of models, looking

14:28.760 --> 14:32.320
at different combinations of those things, and then sort of finding what falls out of

14:32.320 --> 14:33.320
that.

14:33.320 --> 14:36.840
And just to add to it, the other thing with all the different platforms is that you

14:36.840 --> 14:41.840
also have to know how to go in and manually tune the models to make them better, right?

14:41.840 --> 14:46.840
So if you're in Azure Studio, and I don't know, you pick an algorithm, which I think

14:46.840 --> 14:48.600
is the first hurdle, right?

14:48.600 --> 14:54.000
So your developer, you're in Azure, and now you have, you know, 15 different algorithms

14:54.000 --> 14:55.400
underneath regression.

14:55.400 --> 14:57.760
So which one are you going to pick, right?

14:57.760 --> 15:00.320
Like your developer, you're sitting there, you're seeing 15 algorithms, how do you know

15:00.320 --> 15:01.880
which one to use?

15:01.880 --> 15:07.360
And how do you then go about making that model better, right, be it tuning, or maybe

15:07.360 --> 15:10.520
you have new features, with Nexus, we do all that for you.

15:10.520 --> 15:18.840
So we have probably close to like 150 different algorithms at this point that are in the platform.

15:18.840 --> 15:24.720
And basically just kind of high level how it works is we hold these tournaments and the

15:24.720 --> 15:29.760
algorithm that's performing the best based on scoring metrics is the one that's used.

15:29.760 --> 15:34.080
So you as a developer don't even have to come to us and say, hey, I'm not, like I'm going

15:34.080 --> 15:38.760
to use, who's the decision trees, you don't even have to know that you just have to say,

15:38.760 --> 15:43.320
hey, here's the problem that I have, here's what I want to solve for you guys build all

15:43.320 --> 15:47.080
the models and then you guys tell me they want us doing the best, right?

15:47.080 --> 15:50.400
They're sort of like a universal pipeline we use, depending on the type of that problem

15:50.400 --> 15:51.400
you want to solve.

15:51.400 --> 15:54.840
And it sort of makes a lot of those decisions based on metadata that they provide as

15:54.840 --> 15:55.840
well.

15:55.840 --> 16:00.120
Are there specific types of problems that that this works for?

16:00.120 --> 16:05.200
It sounds a little bit too good to be true to be kind of applicable to everything or anything.

16:05.200 --> 16:09.680
Like I can give it any kind of data and it could do any kind of algorithm.

16:09.680 --> 16:15.200
Yeah, so right now we're not going super deep into what is traditionally viewed as the

16:15.200 --> 16:20.360
deep learning space, which that's pretty well solved, right?

16:20.360 --> 16:26.200
Like if you want to go predict, is this a hot dog or not, like, there's something up there

16:26.200 --> 16:28.080
or do that for you already.

16:28.080 --> 16:30.720
So looking at Valley reference, right?

16:30.720 --> 16:35.400
And like so we're not really super focused on deep learning, we are starting to incorporate

16:35.400 --> 16:40.400
more things around voice and speech, so you know, P and not exactly sure how deep

16:40.400 --> 16:46.480
I'm going in that vein, but we're really more focused on the true machine learning and

16:46.480 --> 16:49.240
kind of the layer that's above deep learning.

16:49.240 --> 16:54.920
So you know, we're not going to probably release any time soon, kind of like an image recognition

16:54.920 --> 16:56.720
element into our API.

16:56.720 --> 17:01.120
Again, I think that's been solved and it's been solved pretty well, but where you don't

17:01.120 --> 17:07.560
see a lot of stuff is underneath like regression classification, clustering, and again, kind

17:07.560 --> 17:11.440
of all the real machine learning types of elements.

17:11.440 --> 17:17.360
So today we launched the API doing time series, which I think is also a little bit unique.

17:17.360 --> 17:23.560
We don't see many platforms out there that have true time series capabilities in it.

17:23.560 --> 17:29.120
And time series is just naturally kind of very hard thing to create models in and really

17:29.120 --> 17:31.000
apply machine learning to them.

17:31.000 --> 17:35.040
So we launched with any kind of time series problems that today, if you have a time series

17:35.040 --> 17:38.920
like question like demand forecasting, API, we do that.

17:38.920 --> 17:44.560
We just release regression, so any type of regression problem can be solved with the API.

17:44.560 --> 17:48.280
And later this quarter will be releasing the classification endpoint, and then we'll

17:48.280 --> 17:53.480
just continue going down that vein of machine learning if that makes sense.

17:53.480 --> 17:54.480
Okay.

17:54.480 --> 18:01.000
And now it sounds like, you know, when I think about the the tournaments that you described

18:01.000 --> 18:05.880
as kind of conceptually happening on the back end, you know, it strikes me that for a

18:05.880 --> 18:12.480
given problem, you know, let's say I've got a bunch of time series data and I am trying

18:12.480 --> 18:17.240
to do a predictive maintenance type of application.

18:17.240 --> 18:19.240
Is that the kind of thing that you might do?

18:19.240 --> 18:20.240
Yeah.

18:20.240 --> 18:26.920
That's one of the most just because of where we were and we did the XR's retail accelerator,

18:26.920 --> 18:32.240
a lot of the early use case with the API are more about, hey, I have this store is in

18:32.240 --> 18:38.480
Columbus, Ohio, and I need to know of the 100,000 products that I have, how many I need

18:38.480 --> 18:43.120
to have on the shelf for next week, right, so when I place my reorder, how many new products

18:43.120 --> 18:45.320
I need to have in my warehouse.

18:45.320 --> 18:49.160
So more than the main forecast and type of element, but yeah, predictive maintenance would

18:49.160 --> 18:50.160
work as well.

18:50.160 --> 18:51.160
Okay.

18:51.160 --> 18:58.560
And so the time series in that that lot of cases in the retail case is transactions, transaction

18:58.560 --> 18:59.560
history.

18:59.560 --> 19:00.560
Right.

19:00.560 --> 19:01.560
Yep.

19:01.560 --> 19:02.560
Okay.

19:02.560 --> 19:08.040
And either of these cases, like I'm thinking about the, you know, you've got some set

19:08.040 --> 19:16.720
of models then that you are training the, that you're training against this data.

19:16.720 --> 19:21.000
So there's kind of a fan out there, but and then for each of these models, you've also

19:21.000 --> 19:25.680
got to, you know, do the hyper parameter tuning and all that.

19:25.680 --> 19:27.240
So there, there's a fan out there.

19:27.240 --> 19:33.320
It strikes me that I guess I'm trying to put my hands on like the, the scale aspects of

19:33.320 --> 19:34.720
the problem.

19:34.720 --> 19:39.720
It seems like for any given individual problem, you end up doing a ton of different training

19:39.720 --> 19:40.720
runs.

19:40.720 --> 19:44.640
And I'm wondering, you know, if there's some way you can kind of characterize or help me

19:44.640 --> 19:50.880
understand the, you know, the, the way that that looks from an underlying resource perspective.

19:50.880 --> 19:51.880
Yeah.

19:51.880 --> 19:55.760
We have a very dynamic workflow engine that starts, it's cute driven.

19:55.760 --> 20:00.400
So we can scale in and out of number of CPUs we want to use.

20:00.400 --> 20:03.840
So it, it scales up and down automatically based on demand.

20:03.840 --> 20:08.280
So it's, we built, we spent a lot of time building automation to sort of handle that.

20:08.280 --> 20:11.920
So we're not, you know, we have the cloud to work with so we can do scale sets.

20:11.920 --> 20:15.560
We can do a lot of different things in parallel.

20:15.560 --> 20:20.760
So you know, we could build a hundred or a thousand models all simultaneously and compare

20:20.760 --> 20:25.120
different results, try different hyper parameters, try different feature combinations and things

20:25.120 --> 20:26.120
like that.

20:26.120 --> 20:29.920
And then once we get sort of down to the, to a solution that works well that they're

20:29.920 --> 20:35.280
happy with, they can sort of tune more parameters around that or go with that model and use it

20:35.280 --> 20:36.280
to predict.

20:36.280 --> 20:37.280
Okay.

20:37.280 --> 20:42.840
Is there any particular method that you're using to do the hyper parameter optimization?

20:42.840 --> 20:47.080
I would have to ask our data science team to get into details of that.

20:47.080 --> 20:52.720
We do not have PhDs and, you know, 10 years of experience in the industry.

20:52.720 --> 20:55.920
While we do have some understanding of machine learning, we thought it important to hire

20:55.920 --> 21:00.000
a research team to solve some of the more complicated issues that we're not qualified to

21:00.000 --> 21:01.000
solve.

21:01.000 --> 21:06.120
And so in just as we, he means him and I, yes, we do have PhDs at Nexosis.

21:06.120 --> 21:07.120
Yes, we do.

21:07.120 --> 21:08.120
Right.

21:08.120 --> 21:09.120
Yeah.

21:09.120 --> 21:10.120
Got it.

21:10.120 --> 21:11.120
Got it.

21:11.120 --> 21:15.880
And so when we talk about the kind of the time series, I guess it kind of makes sense

21:15.880 --> 21:22.240
that you, that that was a, uh, uh, initial place to start in that that was a little bit

21:22.240 --> 21:27.040
of the kind of data problem that you ran into it at the power company, is that right?

21:27.040 --> 21:28.040
Yep.

21:28.040 --> 21:33.200
Can you talk it all to kind of a unique, you know, anything unique challenges or things

21:33.200 --> 21:36.080
that you do with regards to time series?

21:36.080 --> 21:42.040
One thing that I don't think people think about is using the product skew example that

21:42.040 --> 21:48.200
we were discussing earlier, if you have a hundred thousand skews, that means ideally you

21:48.200 --> 21:52.480
have a hundred thousand models, right, so every skew has its own model.

21:52.480 --> 21:58.200
And the real power of what we're doing is that each individual skew could have a completely

21:58.200 --> 22:00.880
different type of algorithm winning, right?

22:00.880 --> 22:06.560
So if you're selling snow shovels has one skew item, a different type of fundamental

22:06.560 --> 22:09.960
algorithm might be winning based on location too, right?

22:09.960 --> 22:15.040
So if you have a store in Columbus, Ohio, then you have a store in like Atlanta, Georgia,

22:15.040 --> 22:20.160
very, very different results, even though that's theoretically the exact same item, right?

22:20.160 --> 22:25.200
So that's really the other powers that we're able to take into account, okay, you're selling

22:25.200 --> 22:30.000
a snow shovel in Columbus, Ohio, and it's probably going to be snowier here, so you're going

22:30.000 --> 22:31.000
to sell a lot more.

22:31.000 --> 22:32.000
Right.

22:32.000 --> 22:35.000
And again, just a different algorithm might be winning here on Columbus, even though it's

22:35.000 --> 22:39.640
the same exact item, and then just thinking that even farther out when you think about

22:39.640 --> 22:44.560
things like bottled water as an example, you know, an algorithm that's going to predict

22:44.560 --> 22:48.120
how much water you're going to sell might look a lot different than an algorithm that's

22:48.120 --> 22:52.440
going to predict, you know, how many white t-shirts you're going to sell, right?

22:52.440 --> 22:57.840
The feature importance in both of those might be dramatically different and they are different,

22:57.840 --> 22:58.840
right?

22:58.840 --> 23:03.760
So that's the other thing when you start thinking about scale, one model or one algorithm

23:03.760 --> 23:07.800
doesn't fit all, right?

23:07.800 --> 23:14.360
And so you're able to like, so if as a developer, in this maybe dig into this retail

23:14.360 --> 23:19.040
case, as a developer, how am I giving you my data?

23:19.040 --> 23:21.120
I'm assuming that's the starting place.

23:21.120 --> 23:22.120
Yeah.

23:22.120 --> 23:26.160
So what we do is we have a couple different ways you can import data through the API right

23:26.160 --> 23:30.840
now, and it's through JSON or CSV, and then we have some S3 endpoints where you can put

23:30.840 --> 23:34.440
larger data sets, and then we'll ingest them from there.

23:34.440 --> 23:36.120
Okay.

23:36.120 --> 23:41.120
And so I'm giving you this data via the API.

23:41.120 --> 23:49.120
And how am I doing anything then to kind of describe this data or are you like figuring

23:49.120 --> 23:50.440
it all out somehow?

23:50.440 --> 23:52.760
So we try to have what we call sane defaults.

23:52.760 --> 23:57.160
So if you didn't give us any metadata and you uploaded a data set, we would do some basic

23:57.160 --> 24:01.760
analysis of that data set and try to create appropriate data types for each one.

24:01.760 --> 24:04.320
Now is that going to be, is that going to work great?

24:04.320 --> 24:08.520
Probably not, but will it work well enough to sort of get some results?

24:08.520 --> 24:09.520
That's our hope.

24:09.520 --> 24:14.480
As people sort of learn more about what they need to do with the data, we have metadata

24:14.480 --> 24:20.840
that you can use to sort of describe this is a string, this is categorical data, or

24:20.840 --> 24:26.920
I need an imputation strategy on this numeric field that does mean mode or, you know, that's

24:26.920 --> 24:27.920
sort of thing.

24:27.920 --> 24:32.520
So you can start to describe and we'll try to make some of those decisions for you,

24:32.520 --> 24:36.480
but it's better certainly once the developer gets in and sort of gets their hands around

24:36.480 --> 24:39.480
their own data and understand what they need to do with it.

24:39.480 --> 24:40.480
Okay.

24:40.480 --> 24:42.600
And you've mentioned imputation strategy a couple of times.

24:42.600 --> 24:43.600
Tell us what that means.

24:43.600 --> 24:49.080
The idea there is if you want to look at aggregating data over time, the question is around

24:49.080 --> 24:50.080
do you add it up?

24:50.080 --> 24:52.280
Do you summon it or do you take an average, right?

24:52.280 --> 24:57.040
If you're looking at the weather, adding them, temperatures doesn't make any sense, right?

24:57.040 --> 24:58.560
You want to average it over time.

24:58.560 --> 25:03.640
So it's that general idea there that you can sort of indicate what type of data it is

25:03.640 --> 25:08.840
and then we'll take that step of how you want to handle that aggregation or imputation

25:08.840 --> 25:09.840
around that.

25:09.840 --> 25:10.840
So fields are missing, right?

25:10.840 --> 25:15.680
We might put in something else, or if you want to roll up your daily or hourly forecasts

25:15.680 --> 25:19.680
up to a monthly forecast, we're going to use different strategies to sort of roll that

25:19.680 --> 25:24.120
data up as well as fill in empty values.

25:24.120 --> 25:33.440
So I give you this time series of transactional data for this retail use case that has date

25:33.440 --> 25:39.160
time, skew purchase price, and maybe some other stuff.

25:39.160 --> 25:43.480
Like how do I then, how do I tell you what problem I'm trying to solve?

25:43.480 --> 25:48.760
You're basically going to tell us the column that you're wanting to predict off of, right?

25:48.760 --> 25:49.760
Yep.

25:49.760 --> 25:53.320
So in that same metadata, you just say, these are features and this is a target.

25:53.320 --> 25:55.880
And you can turn those on and off for each run as well.

25:55.880 --> 26:00.000
So if you want to turn off a feature, you could then predict on McCollum.

26:00.000 --> 26:01.000
You could turn it back on.

26:01.000 --> 26:02.640
You could do what if scenarios that way, too?

26:02.640 --> 26:07.560
Just sort of say what if we did this or that with those features, too.

26:07.560 --> 26:13.240
And then are you able to do anything with around like artificial features, like so features

26:13.240 --> 26:18.600
that aren't in the data and the, you know, predicting home price example, you might want

26:18.600 --> 26:23.520
to look at the number of rooms times the number of bathrooms and that, you know, particular

26:23.520 --> 26:28.560
and artificial feature might have some predictive value, you know, that either of those features

26:28.560 --> 26:30.680
by themselves doesn't have.

26:30.680 --> 26:34.520
I mean, it depends, I would say in those cases, no, we're not going to just sort of brute

26:34.520 --> 26:38.040
force through all your columns and try multiplying them and see if something happens or dividing

26:38.040 --> 26:39.040
by something, you know what I mean?

26:39.040 --> 26:40.480
Like, that's a hard problem.

26:40.480 --> 26:44.640
So I think in that sense, you need to know a little bit of your data and what the indicators

26:44.640 --> 26:45.640
are.

26:45.640 --> 26:49.880
But in other cases, we can sort of do some interesting things with maybe holiday calendars

26:49.880 --> 26:53.160
and we can automatically overlay with your time series data.

26:53.160 --> 26:57.280
We've done some work sort of with, you know, Latin launch locations, we can incorporate

26:57.280 --> 27:00.840
whether automatically certain weather features that might help.

27:00.840 --> 27:05.440
So it depends on what it is and we have some interesting ideas and plans in the future

27:05.440 --> 27:06.520
for that as well.

27:06.520 --> 27:13.000
But no, I mean, at some point you have to sort of understand what the indicators are that

27:13.000 --> 27:14.480
may help you predict, right?

27:14.480 --> 27:18.160
Like, we're not, we're not a crystal ball.

27:18.160 --> 27:23.840
So I tell you what, what columns I want to predict on and how does the, does, you mentioned

27:23.840 --> 27:32.520
earlier that the, the platform will, you know, build individual models for each of the,

27:32.520 --> 27:33.520
the excuses.

27:33.520 --> 27:38.680
Is that something that I need to tell it to do or is that, is it always doing that?

27:38.680 --> 27:43.280
It always does that with a caveat, you know, once you have a model, though, you can reuse

27:43.280 --> 27:44.280
it.

27:44.280 --> 27:47.320
The problem with time series data is, of course, as often yesterday as a better predictor

27:47.320 --> 27:50.240
of today or tomorrow than a week ago.

27:50.240 --> 27:55.200
So there's sort of the notion of how, how long my algorithm is good for, right?

27:55.200 --> 27:58.840
And so on the time series, you end up rebuilding models a lot more frequently than you might

27:58.840 --> 28:03.480
with like a regression model that's just, you know, not as concerned about those sort

28:03.480 --> 28:08.280
of yesterday as the future or two days ago, three days ago, last week.

28:08.280 --> 28:09.280
Okay.

28:09.280 --> 28:13.720
Maybe tell me a little bit about some of the kind of technical challenges that you had

28:13.720 --> 28:16.920
to overcome to put all this together.

28:16.920 --> 28:20.760
Boy, yeah, that's, that's a big question.

28:20.760 --> 28:21.760
That's the word start.

28:21.760 --> 28:23.760
That could be a second podcast.

28:23.760 --> 28:29.160
Yeah, I think really, you know, when you try to build a generalized platform, it's, there's

28:29.160 --> 28:34.120
so many challenges, you know, around handling all sorts of data, not, you know, and we can't

28:34.120 --> 28:35.120
boil the ocean.

28:35.120 --> 28:39.520
So we have to make a lot of trade-offs and choices around what is the MVP going to look

28:39.520 --> 28:40.520
like?

28:40.520 --> 28:42.160
What is the next version going to look like?

28:42.160 --> 28:43.160
What can wait?

28:43.160 --> 28:44.160
What do we have to have now?

28:44.160 --> 28:45.160
How do we get something to market?

28:45.160 --> 28:50.360
And so from the product side, it's been a big challenge to sort of make those trade-offs

28:50.360 --> 28:54.760
and then get the work done in a way that we feel good about the results.

28:54.760 --> 29:02.120
So yeah, I think building sort of that ETL pipeline in a general sense, sort of that pipeline

29:02.120 --> 29:05.760
around, you know, do you scale the data before you run the algorithms?

29:05.760 --> 29:06.760
When do you have to do that?

29:06.760 --> 29:12.440
And sort of building in sort of all that core capability around, how do we get any sort

29:12.440 --> 29:16.640
of data into a common sort of matrix to do ML on?

29:16.640 --> 29:18.960
And there's lots of different paths to get there.

29:18.960 --> 29:23.640
And I think one of the big challenges was trying to define the best use cases for us

29:23.640 --> 29:26.080
to sort of get the broadest hit.

29:26.080 --> 29:29.520
Now, you know, we have to trade-off some accuracy for that and that's okay.

29:29.520 --> 29:34.640
As we go on, I think we feel like we can get better and go deeper in a lot of areas.

29:34.640 --> 29:35.640
Mm-hmm.

29:35.640 --> 29:39.560
Yeah, that was a question that I had earlier and I didn't ask it.

29:39.560 --> 29:47.680
You know, often when, you know, I've talked to folks working on generalized machine learning

29:47.680 --> 29:55.360
platforms, the idea tends to be, you know, you're trying to get the developer, the organization

29:55.360 --> 29:59.760
from, you know, zero to 80 percent.

29:59.760 --> 30:04.560
And then maybe once they're, you know, at 80 percent have some maturity, they may find

30:04.560 --> 30:09.920
it they, you know, that they need to invest further to get to, you know, 90 percent.

30:09.920 --> 30:13.160
Is that the general way you think about the problem as well?

30:13.160 --> 30:19.840
Yeah, to a certain degree, it's always, I just think where we are today, getting people

30:19.840 --> 30:23.000
familiar with what machine learning can do is kind of the first hurdle.

30:23.000 --> 30:26.600
And then as you were saying, how do we get better performance after we actually have

30:26.600 --> 30:31.760
something in the environment or some kind of application deploy that's using machine

30:31.760 --> 30:32.760
learning?

30:32.760 --> 30:36.400
So I think there's this natural evolution and that's one of the things that we've kind

30:36.400 --> 30:41.280
of built into our own platform is let's talk about like a regression problem.

30:41.280 --> 30:46.080
You were talking about house prices, I think earlier, well, let's say your initial data

30:46.080 --> 30:51.560
set has like 15 features and it's like room size and how many bedrooms and whatever, right?

30:51.560 --> 30:54.720
And you build your model and it's pretty good.

30:54.720 --> 31:00.000
But then as time goes on, you might think, well, hey, you know what, maybe this new thing

31:00.000 --> 31:03.200
that I just thought about is going to have a big impact and maybe that new thing in this

31:03.200 --> 31:05.960
scenario is the school rating in your area.

31:05.960 --> 31:10.040
So now you're going to bring in the whole new feature of school ratings for the house

31:10.040 --> 31:11.680
that you're looking at.

31:11.680 --> 31:14.360
And then with us, it's just naturally going to build a new model.

31:14.360 --> 31:15.960
So you don't even have to think about it.

31:15.960 --> 31:19.280
All right, well, now I have the same feature to idea into algorithm or not.

31:19.280 --> 31:22.960
If you do need a new algorithm, our platform is just going to figure that out and it's going

31:22.960 --> 31:27.040
to say, all right, well, you know, maybe you're using like a classio regression or something

31:27.040 --> 31:32.520
before and now you're going to use whatever else because you added all these new features.

31:32.520 --> 31:35.520
I think that's really the other big power of this, right?

31:35.520 --> 31:40.520
And as people start looking at the accuracy and the results, I think the natural question

31:40.520 --> 31:43.240
is always going to be, well, how can we do better, right?

31:43.240 --> 31:50.160
And we are trying to spend a lot of time and energy on that educational component, which

31:50.160 --> 31:53.680
is kind of answering that question, okay, we have this today.

31:53.680 --> 31:54.680
How do we get a better, right?

31:54.680 --> 31:57.600
Is it going to get better if we add in school ratings?

31:57.600 --> 32:02.920
Maybe probably, but let's figure out what happens when we actually do it and then they'll

32:02.920 --> 32:08.280
figure out, okay, well, this had no impact or had a big impact and then that should lead

32:08.280 --> 32:11.920
to the next question of, all right, well, what else could we add, maybe to make it even

32:11.920 --> 32:12.920
better, right?

32:12.920 --> 32:18.240
And you could just keep going down that old trend and naturally just our platform is going

32:18.240 --> 32:23.040
to figure out, okay, I should mature and add new features, we're going to pick maybe

32:23.040 --> 32:24.040
a new algorithm.

32:24.040 --> 32:26.040
It's going to perform even better.

32:26.040 --> 32:27.360
Yeah, it's funny.

32:27.360 --> 32:35.880
A lot of those things, those activities that you're describing on the spectrum of improving

32:35.880 --> 32:39.400
your algorithms are things that I think of as data science.

32:39.400 --> 32:48.720
Like if you separate out the knowledge of the underlying math from the process and the

32:48.720 --> 32:56.400
way of thinking about data and features that have some predictive value and using those

32:56.400 --> 33:04.040
to create predictions, like that to me, a lot of that is what data science is really

33:04.040 --> 33:05.040
about.

33:05.040 --> 33:10.640
And I'm wondering if you, you know, is it that you end up teaching developers, those parts

33:10.640 --> 33:15.440
of data science in order to get them productive on this platform or are you finding that there

33:15.440 --> 33:20.840
are, you know, maybe folks with different roles that understand that stuff but don't understand

33:20.840 --> 33:22.160
the math.

33:22.160 --> 33:28.360
How do you kind of see the audience and for what you're doing and is it evolving at all?

33:28.360 --> 33:31.000
I think the audience is absolutely evolving.

33:31.000 --> 33:36.520
I think as we look at the future, we're releasing some additional features that should really

33:36.520 --> 33:41.120
marry this notion of developers teaming up with data scientists.

33:41.120 --> 33:44.360
We want to enable more collaboration in that space.

33:44.360 --> 33:50.640
We think one of the main things as we look at more mature organizations is assorting

33:50.640 --> 33:53.040
the time from R&D to production.

33:53.040 --> 33:58.000
So if you're a data scientist and now you're collaborating real time over, you know, this

33:58.000 --> 34:02.840
maybe beta application that the developer has made that kind of really speeds things up

34:02.840 --> 34:07.560
and then as the data scientist is looking at what the developer may be made as an initial

34:07.560 --> 34:12.280
group of concept, you know, yes, the data scientist might really be honed in on how to make

34:12.280 --> 34:13.680
this better.

34:13.680 --> 34:18.720
So that's one approach and then I think again, you just have so many, from an innovation

34:18.720 --> 34:25.040
perspective, enabling developers to quickly prototype things as a tremendous value, right?

34:25.040 --> 34:28.880
And then just being able to show if you're a developer, you know, maybe you have a data

34:28.880 --> 34:32.840
science friend or someone at your company is a data scientist, you know, being able to

34:32.840 --> 34:39.120
say, hey, look, I made this prototype application and it's doing X, you know, what do you think

34:39.120 --> 34:40.720
what else maybe could I do to it?

34:40.720 --> 34:44.320
So just fostering that collaboration is obviously really important to us.

34:44.320 --> 34:49.240
As we look at especially 2018, you should see more and more things start to be released.

34:49.240 --> 34:54.880
Just to add on to that, there's really not enough of data scientists to go around unfortunately.

34:54.880 --> 35:00.680
So we also enabling, I think, people to get some capability up is better than, you know,

35:00.680 --> 35:02.800
them just having to go, I guess, go hungry.

35:02.800 --> 35:03.800
So to speak.

35:03.800 --> 35:04.800
Yeah.

35:04.800 --> 35:05.800
Yeah.

35:05.800 --> 35:12.760
And where it's industry or vertical focused, where you're able to, you know, you've got

35:12.760 --> 35:16.560
data scientists that are thinking about the problems in those verticals and what the

35:16.560 --> 35:21.280
data needs to look like and what the features are so that you can, you know, guide or offer

35:21.280 --> 35:25.920
a special, you know, features for specific verticals or is that not a focus for you right

35:25.920 --> 35:26.920
now?

35:26.920 --> 35:27.920
Yeah.

35:27.920 --> 35:32.320
So shortly, we should be releasing this notion of kind of data templates.

35:32.320 --> 35:37.280
The idea behind that is that the bare minimum here are kind of the features that you would

35:37.280 --> 35:41.760
need in order to be successful with this type of problem, right?

35:41.760 --> 35:48.800
So skew level forecasting as an example, you know, the bare minimum features would be

35:48.800 --> 35:49.800
like the time, right?

35:49.800 --> 35:56.160
So ideally, you want a year or data, it can work with less than that, but best performances

35:56.160 --> 35:57.160
a year.

35:57.160 --> 36:01.840
And then you're going to want, you know, probably daily sales activity.

36:01.840 --> 36:05.680
And then beyond that, like those are just the bare minimum, you can create a forecast

36:05.680 --> 36:11.280
off just those two things because the platform automatically extracts the database on the

36:11.280 --> 36:15.080
timestamp, you know, if it's Monday to say Wednesday and I'll find the weekly and daily

36:15.080 --> 36:16.800
trends things of that nature.

36:16.800 --> 36:22.440
But then you could start adding into that template additional things that maybe aren't necessarily

36:22.440 --> 36:24.120
required with a nice to have, right?

36:24.120 --> 36:28.720
So a nice to have would be, do you have a promotion going on for that particular item,

36:28.720 --> 36:29.720
right?

36:29.720 --> 36:31.800
And that could be a binary one or zero.

36:31.800 --> 36:35.680
You could extractulate out like what kind of motion it was or it isn't like a buy one

36:35.680 --> 36:36.680
get one.

36:36.680 --> 36:37.680
Is it just a percent off?

36:37.680 --> 36:39.080
Things of that nature.

36:39.080 --> 36:41.240
But yeah, we have plans to kind of put out there.

36:41.240 --> 36:45.320
Here's a bare minimum that you need and here's kind of like the nice to have and kind

36:45.320 --> 36:46.320
of go from there.

36:46.320 --> 36:47.560
Oh, interesting.

36:47.560 --> 36:52.520
So in that example, where you're, you've got a developer, they've gone out and collected

36:52.520 --> 36:58.240
some sales and marketing, you know, historical data and they're doing a forecast.

36:58.240 --> 37:05.520
How do you articulate to this developer that may not be, you know, statistically sophisticated,

37:05.520 --> 37:09.440
the extent to which they should rely on this result that your platforms put out for

37:09.440 --> 37:10.440
them?

37:10.440 --> 37:13.400
Yeah, currently we put out some metrics on that.

37:13.400 --> 37:18.120
And what we've determined is we need to get a little more friendlier on those metrics.

37:18.120 --> 37:23.040
So we have some sort of education around what the metrics mean and sort of our, some

37:23.040 --> 37:25.400
of our learning on our documentation site.

37:25.400 --> 37:28.920
But we want to really go to the next level I feel like and really sort of hold their

37:28.920 --> 37:33.600
hand a little more on what the metric is saying about the model based on what the data

37:33.600 --> 37:34.600
they have.

37:34.600 --> 37:39.640
It's currently, I think we return mate scores if it's a time series site forecast.

37:39.640 --> 37:42.840
And yeah, what we plan to make it even more friendly than that, right?

37:42.840 --> 37:47.680
Because developer might not understand what mate scores may mean absolute percentage

37:47.680 --> 37:48.680
error.

37:48.680 --> 37:49.680
Uh-huh.

37:49.680 --> 37:50.680
Okay.

37:50.680 --> 37:56.600
So this is like these interfaces between kind of what you might expect the data scientists

37:56.600 --> 38:01.800
to know and what the developer might not know or some of the key challenges that you

38:01.800 --> 38:06.000
face as you evolve this and bring more people onto the platform.

38:06.000 --> 38:10.600
Yeah, honestly, I think that's a, you're just quite frankly, I think that's a minor challenge.

38:10.600 --> 38:15.360
I think the biggest challenge that we have and I think just the industry has in general

38:15.360 --> 38:17.040
is access to data.

38:17.040 --> 38:22.240
If I want to build, and obviously I was thinking about this over the weekend, I wanted to build

38:22.240 --> 38:29.280
a couple of different applications and the hurdle is getting the data sets, right?

38:29.280 --> 38:36.000
So if I wanted to build an application off of predicting cancer, there's certain cancer

38:36.000 --> 38:37.000
data sets already out there.

38:37.000 --> 38:41.600
It's a very famous, the breast cancer data set that kind of has the size of the tumor

38:41.600 --> 38:42.600
and things of that nature.

38:42.600 --> 38:47.040
But maybe a big important feature there is where the people live, right?

38:47.040 --> 38:52.400
If you live next to, I don't know, a waste site, you know, that's a little bit extreme,

38:52.400 --> 38:54.200
but let's say that you live there.

38:54.200 --> 38:55.680
Maybe that's why you have cancer, right?

38:55.680 --> 39:00.000
So maybe where people live might actually be a huge indicator whether or not you're going

39:00.000 --> 39:04.880
to develop cancer or that lump that you discovered is the minor eminigment, right?

39:04.880 --> 39:08.000
But it's very hard to get that data set, right?

39:08.000 --> 39:13.120
For one, you have the HIPAA issue, so there's that hurdle to get over.

39:13.120 --> 39:16.720
But then more than that, it's just, are people going to want to share the data?

39:16.720 --> 39:21.960
The other idea I had was predicting whether or not a startup would be successful.

39:21.960 --> 39:25.960
And one of the things that I think you would need to do that would be kind of the financial

39:25.960 --> 39:29.080
information on the startup, right, and how much money they're spending, where and the

39:29.080 --> 39:30.080
return.

39:30.080 --> 39:36.360
But again, trying to get reliable data that's going to help me build that model is kind

39:36.360 --> 39:37.360
of what I think right now.

39:37.360 --> 39:41.160
It's the biggest obstacle in the field.

39:41.160 --> 39:46.160
Any other thoughts on that particular point, any other obstacles that you see?

39:46.160 --> 39:48.680
Yeah, I mean, there's plenty of them.

39:48.680 --> 39:53.120
I think really walking them through what their data might be telling them is going to be

39:53.120 --> 39:54.120
helpful.

39:54.120 --> 39:59.040
So like one of the big challenges is you submit a data set and I think there needs to be

39:59.040 --> 40:03.920
more of an indicator of, you really can't do anything with this potentially in some

40:03.920 --> 40:04.920
situations.

40:04.920 --> 40:08.440
It's like, it's okay, and then like this is really good data.

40:08.440 --> 40:13.280
So I think sort of kind of defining and helping them along without just just raw metrics,

40:13.280 --> 40:18.640
you know, like give them a little more, I guess, safety feeling about their models is still

40:18.640 --> 40:19.640
a challenge.

40:19.640 --> 40:21.440
And then we have some other ideas around that.

40:21.440 --> 40:22.440
Yeah.

40:22.440 --> 40:26.800
I think just also the add to that one of the challenges is how machine learning AI, whatever

40:26.800 --> 40:31.920
you want to call it, today has been talked about in the media, it's put into the minds

40:31.920 --> 40:35.280
of a lot of people that machine learning is this magic bullet.

40:35.280 --> 40:39.520
And to give kind of a real example, we have some people today that have signed up for

40:39.520 --> 40:43.640
the API that want to use it to protect things like Bitcoin price.

40:43.640 --> 40:47.840
And the data set is just the price point on a day.

40:47.840 --> 40:51.040
And they think that machine learning is just going to like magically figure out what

40:51.040 --> 40:52.840
the price is going to be.

40:52.840 --> 40:56.800
And you know, sometimes we have to talk to the people who sign up on that use case or

40:56.800 --> 41:01.560
like, well, if that's all the features you have, do you really think that to say is the

41:01.560 --> 41:06.600
reason why Bitcoin jumped up or do you think there's this other feature out there that

41:06.600 --> 41:09.160
is really impacting and influencing it, right?

41:09.160 --> 41:14.840
So again, it's just people really have to, I think, get beyond that machine learning isn't

41:14.840 --> 41:16.520
a magical bullet.

41:16.520 --> 41:20.640
It's only going to find the patterns in the correlation in your data set.

41:20.640 --> 41:24.040
If it's not there in the data, you know, it's never going to find a pattern.

41:24.040 --> 41:25.040
Yeah.

41:25.040 --> 41:26.040
That's awesome.

41:26.040 --> 41:32.160
Anything else that you'd like to share with the audience as we come no close?

41:32.160 --> 41:35.560
You know, there's just the other only thing that I was thinking about is as we're talking

41:35.560 --> 41:38.280
about kind of our overall goal.

41:38.280 --> 41:43.160
I think really most of 2018 is going to be this theme of how do we enhance collaboration

41:43.160 --> 41:45.720
between developers and data scientists.

41:45.720 --> 41:49.680
One of the newer features that we'll be releasing in 2018 is something called bring your

41:49.680 --> 41:51.000
own algorithm.

41:51.000 --> 41:57.040
So what this is going to do is it will allow a data scientist to create maybe a very specific

41:57.040 --> 42:02.480
algorithm that is really good at solving for a particular thing.

42:02.480 --> 42:03.640
So let me give an example.

42:03.640 --> 42:10.800
Let's say that you are a data scientist at Best Buy or some big box retailer and let's

42:10.800 --> 42:16.840
say the retailer really cares about the price or the sales for like, I don't know, LED

42:16.840 --> 42:17.840
TVs.

42:17.840 --> 42:23.960
Well, the notion is that they might create a very specific algorithm that's really good

42:23.960 --> 42:30.160
at figuring out, hey, how many flat screen, LED 55 inch TVs that we're going to sell.

42:30.160 --> 42:33.600
And all that they would have to do is take that algorithm that they build in house.

42:33.600 --> 42:39.320
They could just plop it into our API and their algorithm will live just in their instance.

42:39.320 --> 42:40.840
It'll live by itself.

42:40.840 --> 42:45.520
Well, it'll live just in their instance by itself, but it will live side by side our 150

42:45.520 --> 42:46.920
plus algorithms.

42:46.920 --> 42:48.520
So they'll be able to see your real time.

42:48.520 --> 42:52.360
Hey, here's where our algorithm is winning and here's where it's not winning.

42:52.360 --> 42:58.960
I guess a related question that I had from earlier, as you're starting to get into collaboration

42:58.960 --> 43:04.480
and data scientists working with developers and developers exploring their data and creating

43:04.480 --> 43:12.120
new features and things like that, it opens up this whole set of issues around model management

43:12.120 --> 43:18.320
and model governance and model providence and stuff like that, are you the company thinking

43:18.320 --> 43:24.320
about any of those things or do you offer support for those kinds of challenges?

43:24.320 --> 43:27.280
Yeah, that's certainly come up in the past.

43:27.280 --> 43:33.440
We've had it come up primarily in the insurance industry, you know, the regulations around

43:33.440 --> 43:34.440
the model building.

43:34.440 --> 43:38.920
You really have to understand how the decision is coming about right because you can't

43:38.920 --> 43:44.200
discriminate in things of that nature, so yeah, for kind of the more enterprise customers

43:44.200 --> 43:49.680
will allow them to actually download that model and algorithm and kind of have that supporting

43:49.680 --> 43:55.280
documentation they need to have to prove that, you know, we're not using for lack of

43:55.280 --> 44:00.480
better word illegal types of data sets to create, you know, the prediction result.

44:00.480 --> 44:04.480
So I think as time goes on, we're going to see more and more of that.

44:04.480 --> 44:09.680
And again, right now, one way that we can solve it is just by giving a dock rise container

44:09.680 --> 44:13.600
that contains the algorithm and explain how it works and things of that nature.

44:13.600 --> 44:18.520
So yeah, I mean, as time goes on, I think we're going to see more and more of that.

44:18.520 --> 44:19.520
Okay.

44:19.520 --> 44:20.520
All right.

44:20.520 --> 44:21.520
Great.

44:21.520 --> 44:26.560
And then did you have an offer for like free access to the platform or free API key or

44:26.560 --> 44:28.920
something like that available to listeners?

44:28.920 --> 44:29.920
Yeah.

44:29.920 --> 44:35.360
One can sign up for the API for free, that's part of our philosophy as a company and being

44:35.360 --> 44:39.400
developer first is that we want people to use it.

44:39.400 --> 44:42.200
So we do have a community edition, it's 100% for free.

44:42.200 --> 44:45.760
It will always be free that community tier.

44:45.760 --> 44:52.200
So yeah, anyone can go to nexos.com, sign up for an API key and get started in under five

44:52.200 --> 44:53.200
minutes.

44:53.200 --> 44:54.200
Okay.

44:54.200 --> 44:55.200
Awesome.

44:55.200 --> 45:00.560
Well, guys, thank you so much for taking the time to chat with us.

45:00.560 --> 45:06.040
I think what you're doing is really interesting and I will certainly be looking forward to

45:06.040 --> 45:10.520
keeping up with you and I'd love for you to keep in touch and, you know, let me know about

45:10.520 --> 45:13.560
you know, as the platform evolves, your continued success, et cetera.

45:13.560 --> 45:18.280
Yeah, definitely was pleasure being on here and thanks so much for having us.

45:18.280 --> 45:19.280
Okay.

45:19.280 --> 45:23.760
So before we go, why don't you take a second and tell me a little bit about what's attraction

45:23.760 --> 45:24.760
been to date?

45:24.760 --> 45:29.080
How many users do you have or how many companies are using it?

45:29.080 --> 45:30.080
Yeah.

45:30.080 --> 45:35.800
So currently we released the act of the public on July, no, on 2017, today we have over

45:35.800 --> 45:42.520
4,000 developers that have signed up and we have close to 500 applications that have

45:42.520 --> 45:43.520
been deployed.

45:43.520 --> 45:44.520
Oh, wow.

45:44.520 --> 45:45.520
All right.

45:45.520 --> 45:50.160
Well, once again, thank you so much for taking the time to chat with me.

45:50.160 --> 45:51.160
I appreciate it.

45:51.160 --> 45:52.160
Yeah.

45:52.160 --> 45:53.160
Likewise, pleasure.

45:53.160 --> 45:54.160
It was fun.

45:54.160 --> 45:55.160
Thanks, guys.

45:55.160 --> 46:03.440
All right, everyone, that's our show for today.

46:03.440 --> 46:08.680
Thanks so much for listening and for your continued feedback and support.

46:08.680 --> 46:14.080
For more information on Ryan, Jason, or any of the topics covered in this episode, head

46:14.080 --> 46:18.880
on over to twimlai.com slash talk slash 69.

46:18.880 --> 46:22.680
This interview kicks off our Strange Loop 2017 series.

46:22.680 --> 46:28.400
To follow along with the series, visit twimlai.com slash ST Loop.

46:28.400 --> 46:34.320
Of course, you can send along your feedback and questions via Twitter to at twimlai or

46:34.320 --> 46:39.360
at Sam Charrington or leave a comment right on the show notes page.

46:39.360 --> 46:44.520
Thanks once again to Nexosis for their sponsorship of the show and this series.

46:44.520 --> 46:51.600
For more info on them and to get your free API key, visit nexosis.com slash twimlai.

46:51.600 --> 46:56.080
And of course, thanks once again to you for listening and catch you next time.

