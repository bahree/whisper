1
00:00:00,000 --> 00:00:08,240
All right, everyone. Welcome to another episode of the Twimmel AI podcast. I am your host Sam Charrington and today

2
00:00:08,240 --> 00:00:13,680
I'm joined by Demetrius Zermas. Demetrius is a principal scientist at Centera.

3
00:00:14,320 --> 00:00:21,760
Before we get into today's conversation, please take a moment and head over to Apple Podcasts, Spotify, or your listening platform of choice,

4
00:00:21,760 --> 00:00:28,960
and subscribe, review, and leave us a five-star rating if you enjoy the show. Demetrius, welcome to the podcast.

5
00:00:28,960 --> 00:00:35,760
Hey Sam, thank you for having me. I'm really looking forward to this conversation. We'll be chatting a bit about

6
00:00:35,760 --> 00:00:42,160
data center AI and in particular how it applies to your work in precision agriculture.

7
00:00:42,960 --> 00:00:48,960
But before we get there, let's start like we usually do with a little bit of background.

8
00:00:49,760 --> 00:00:56,480
How'd you get started in ML and AI? I'm originally from Greece and when it was growing up,

9
00:00:56,480 --> 00:01:07,280
I really wanted to work with robotics, and then as I managed to get accepted for a PSD program in the United States,

10
00:01:07,280 --> 00:01:16,880
I was really hoping to do robotics. But the lab I joined was doing both robotics and AI, and I was drawn to AI

11
00:01:16,880 --> 00:01:27,440
more, so then that sort of changed my plans or how I was viewing myself or my future self.

12
00:01:28,800 --> 00:01:35,280
And that essentially started a trajectory that led me to where I'm here today.

13
00:01:35,280 --> 00:01:42,320
Awesome, tell us a little bit about Centera. Centera is a software and hardware company,

14
00:01:42,320 --> 00:01:50,800
specializing in precision agriculture. We are building tools to assist agronomists and

15
00:01:50,800 --> 00:02:01,600
assist enterprises that work in the business of precision agriculture or business of agriculture,

16
00:02:01,600 --> 00:02:07,760
with making decisions for their crops during the growing season.

17
00:02:07,760 --> 00:02:14,240
Awesome. And what kind of tools in particular, what do those tools look like? Starting from the hardware,

18
00:02:15,040 --> 00:02:24,160
we are building cameras that can be mounted on small drones. We also built a drone that has

19
00:02:25,040 --> 00:02:34,800
a fixed wing airplane that you can launch by hand. And then these are paired nicely with a suite

20
00:02:34,800 --> 00:02:41,680
of software and analytics solutions. So we have a platform where you can upload your data and then

21
00:02:41,680 --> 00:02:48,480
view the results after the analysis is done. We also create all the analysis on the images we collect,

22
00:02:48,480 --> 00:02:55,280
and this can be counting how many plants have emerged. And that way you can, or the agronomists can

23
00:02:55,280 --> 00:03:04,080
decide whether the seed planted was doing well or not, whether he need to re-plant or not, as well as

24
00:03:05,120 --> 00:03:12,480
figuring out the general vigor and health of the crop they're growing throughout the year.

25
00:03:12,480 --> 00:03:19,120
Why build all of these components? Like why build a separate drone and a separate camera

26
00:03:19,120 --> 00:03:28,560
as opposed to the software part that's kind of directly impacting the agronomists?

27
00:03:28,560 --> 00:03:36,800
There is a continuous hunger for improvement of productivity in the precision agriculture and

28
00:03:36,800 --> 00:03:44,880
the agriculture industry in general. They want to be flying more acres or capturing data for more

29
00:03:44,880 --> 00:03:52,960
acres in a smaller amount of time. We figured that by combining a fixed wing drone that can fly

30
00:03:52,960 --> 00:04:00,800
for about an hour, we can cover agronomists and cover much more ground than whatever was available

31
00:04:00,800 --> 00:04:09,440
in the market. We are of course supporting some well-known drone companies. I'm not going to

32
00:04:09,440 --> 00:04:21,440
mention any names because as long as they capture images that are up to some specifications,

33
00:04:21,440 --> 00:04:27,520
then of course the algorithm can use them. How long have you been in the company? What stage

34
00:04:28,080 --> 00:04:32,480
was the company at with regard to using ML and AI when you joined?

35
00:04:32,480 --> 00:04:40,480
I was informed that I recently concluded the fourth year of joining the drones in

36
00:04:40,480 --> 00:04:49,200
there. Time flies for sure. When we started or when I started, the company that was barely any

37
00:04:50,480 --> 00:04:59,760
analytics available. We had some simple plant counting algorithm that was sort of

38
00:04:59,760 --> 00:05:09,920
struggling to expand and able to be used for different types of plants. That was something

39
00:05:09,920 --> 00:05:16,240
that the company leadership wanted to fix. We wanted to invest majorly at the analysis of

40
00:05:16,240 --> 00:05:22,640
the images we're capturing because that was a unique strength for the company. The ability to

41
00:05:22,640 --> 00:05:31,760
be able to both capture the data and process them. That was the beginning. I think the first

42
00:05:31,760 --> 00:05:41,120
employee of a data analytics group. Is plant counting still one of the major challenges that you're

43
00:05:41,120 --> 00:05:47,920
working on? More broadly, what are some of the various ways that you do use machine learning there?

44
00:05:47,920 --> 00:05:54,400
Plant counting is still something important and something that we're focusing on and I know

45
00:05:54,400 --> 00:06:02,400
means it has been completely solved. There are always new challenges, you know corner cases

46
00:06:02,400 --> 00:06:08,400
that people are asking us who can support this and that. Apart from plant counting, we are

47
00:06:08,400 --> 00:06:13,840
doing other types of counting. We're counting tassels of corn at the end of the

48
00:06:13,840 --> 00:06:19,600
towards the end of the growing season where counting value crops are some permanent crops like

49
00:06:19,600 --> 00:06:25,840
trees, value crops like lettuce for example. The way I like to separate the different problems

50
00:06:25,840 --> 00:06:32,560
we're addressing is we like to count things. We like to detect areas of images that show some

51
00:06:32,560 --> 00:06:41,360
particular textures, some particular pattern. We're trying to classify images into different

52
00:06:41,360 --> 00:06:51,280
classes that show some specific characteristics. We also want to be able to detect anomalies,

53
00:06:51,280 --> 00:07:00,000
things that should not be there like weeds for example or a plant that is of the correct species

54
00:07:00,000 --> 00:07:08,160
but is their own hybrid. For example, a corn seed that was left there from a previous year and

55
00:07:08,160 --> 00:07:17,360
didn't germinate previous years just germinated by accident when it shouldn't. So these types of

56
00:07:19,040 --> 00:07:26,880
problems, it's a broad spectrum of what we're seeing. Yeah, you mentioned that when you joined

57
00:07:26,880 --> 00:07:35,040
and early on in the companies evolution of using ML and analytics, one of these first problems was

58
00:07:35,040 --> 00:07:43,200
the counting problem and forget the exact words you used but it sounded like they were some

59
00:07:43,200 --> 00:07:52,000
barely working algorithms at first. Let's dig into that evolution. What were you using at first

60
00:07:53,840 --> 00:07:59,280
and what were some of the challenges that you were running into? That was the standard

61
00:07:59,280 --> 00:08:06,400
tale of how we are moving our computer vision technology moves from the classical computer vision

62
00:08:06,400 --> 00:08:13,760
to deep learning and the merits of deep learning and how it overtakes everything. The challenges as

63
00:08:14,320 --> 00:08:20,800
everybody who's familiar with the old computer vision can tell you that you cannot create

64
00:08:20,800 --> 00:08:30,320
an algorithm that can be completely autonomous in an environment that you cannot control. Lightning

65
00:08:30,320 --> 00:08:36,400
the light conditions change. A cloud passes over the field when you're capturing an immense

66
00:08:37,600 --> 00:08:42,800
different soil types and different regions of the country or different regions of the world.

67
00:08:42,800 --> 00:08:52,240
Everything lays their own into trying to set some parameters, some thresholds in achieving your

68
00:08:52,240 --> 00:09:00,640
result, your final result and that essentially forced us to start creating very complicated

69
00:09:01,840 --> 00:09:10,480
algorithmic pipelines. At some point it was almost unsustainable to be able to update them or

70
00:09:10,480 --> 00:09:17,520
verify that they're working tests and etc. Then the next four years ago deep learning was

71
00:09:18,640 --> 00:09:24,480
the hype. We decided to start looking some ways of utilizing deep learning to do counting

72
00:09:25,600 --> 00:09:34,160
to solve that problem and even from the very few initial experiments the results were

73
00:09:34,160 --> 00:09:41,520
substantial. We were looking at seeing much better results than computer vision and

74
00:09:42,640 --> 00:09:50,640
that was essentially the sign telling us that as much as you love classical computer vision

75
00:09:50,640 --> 00:09:58,960
because that was what we were taught. It's time to change, time to update and embrace

76
00:09:58,960 --> 00:10:08,480
machine learning, deep learning. That's essentially what we are doing since then.

77
00:10:09,280 --> 00:10:18,240
Thinking about this move into a deep learning type of an approach, just given the company's business

78
00:10:18,240 --> 00:10:22,800
which you described earlier, you have these drones, you have these camera, your drones can fly

79
00:10:22,800 --> 00:10:31,120
longer than any other drones, should be easy. You've got lots and lots of images. Is that the case?

80
00:10:34,320 --> 00:10:40,880
Yes and no. That's a trick question for the audience because that's what we're talking about.

81
00:10:40,880 --> 00:10:52,240
Yes, yes. Having embracing the deep learning approach for sure, solve some of the problems of

82
00:10:52,240 --> 00:11:00,480
having variability in the images we capture for sure. But then again, on the other hand,

83
00:11:00,480 --> 00:11:12,640
one of the main problems of the successful deep learning product is that 80% of the solution is

84
00:11:12,640 --> 00:11:20,080
having data, having good annotations, having a variety of annotations from different scenes.

85
00:11:20,080 --> 00:11:32,640
Which essentially means that you have to take a step back before you send any images to be annotated

86
00:11:32,640 --> 00:11:40,000
and make a decision on whether this image is going to add new information to your training,

87
00:11:40,000 --> 00:11:47,280
or if it's going to saturate a sensory or overtrain on things that you already know,

88
00:11:47,280 --> 00:11:54,240
that you already have captured in your data set. The problem of selecting which images are the ones

89
00:11:54,240 --> 00:12:07,520
that need to be annotated and trained, arose, and that's another significant hard rule we need to

90
00:12:07,520 --> 00:12:16,960
overcome. You mentioned that in one of the challenges with classical computer vision,

91
00:12:16,960 --> 00:12:25,040
techniques was you had to create this really complicated algorithmic pipeline to deal with all

92
00:12:25,040 --> 00:12:33,200
of the variability that you were seeing in your images and in the cornfields and deep learning

93
00:12:35,200 --> 00:12:43,680
did help with that. But there was still an aspect of variability that was problematic for you.

94
00:12:43,680 --> 00:12:55,680
Yes, and that had mostly to do with corner cases. After you go through the initial excitement

95
00:12:55,680 --> 00:13:02,000
of having something that works much better, you start actually testing and looking at corner

96
00:13:02,000 --> 00:13:08,400
cases and making sure that it works for you on every occasion. And what we found was that

97
00:13:08,400 --> 00:13:18,960
there were, of course, areas where we could improve. We always need to detect smaller plants,

98
00:13:18,960 --> 00:13:28,640
for example, but as well as being able to detect different and isolate different plants when they

99
00:13:28,640 --> 00:13:34,800
are overlapping. I don't know if you have ever visited the cornfield after the first month,

100
00:13:34,800 --> 00:13:43,920
month and a half, depending on the weather, it is very hard to even walk between the rows.

101
00:13:44,480 --> 00:13:51,520
I can imagine trying to identify which plant is wheat and separate and between the leaves.

102
00:13:53,520 --> 00:14:01,520
But at the same time, the ability to be able to do this separation is important as it can reveal

103
00:14:01,520 --> 00:14:09,520
a lot of information to both people that are growing corn in a production setting as well as

104
00:14:09,520 --> 00:14:16,720
the people that are growing for seed production. So the people that are growing the corn that will

105
00:14:16,720 --> 00:14:23,600
be planted next year. So you've got all these factors that are introducing variability into your

106
00:14:23,600 --> 00:14:32,720
data set. And you mentioned you've got these corner cases that you're trying to be able to account

107
00:14:32,720 --> 00:14:38,000
for. You've got this real world problem that you're trying to solve and you can't just solve it

108
00:14:38,000 --> 00:14:43,120
for the perfect case. What's the relationship between the variability and the corner cases,

109
00:14:43,120 --> 00:14:52,400
I guess, in your situation? The first thing that comes to mind is the size of the database.

110
00:14:52,400 --> 00:15:00,320
This is a weird problem in a sense that we have millions of images.

111
00:15:02,240 --> 00:15:11,520
So in that sense, we're lucky. But at the same time, there's an imbalance in the data.

112
00:15:12,960 --> 00:15:20,240
And the corner case is something that happens that does not happen frequently.

113
00:15:20,240 --> 00:15:27,840
So although we have a sea of data, identifying images that have good examples of the corner

114
00:15:27,840 --> 00:15:36,880
cases is stuff. So I think that's the challenge. How do you search through your data space,

115
00:15:38,960 --> 00:15:44,320
identifying corner cases, identifying images that are outliers,

116
00:15:44,320 --> 00:15:55,280
and then include them in a somewhat meaningful way in your main training data set.

117
00:15:57,040 --> 00:16:06,960
Because there's always the fear of the danger of skewing your data one direction or the other.

118
00:16:06,960 --> 00:16:15,680
And of course, it requires a lot of testing to figure out if you're doing the right thing.

119
00:16:16,800 --> 00:16:23,600
But starting, but you need to start somewhere and just trying to solve the problem of,

120
00:16:24,400 --> 00:16:29,040
I need somewhat a made way to tell me which images are unique or interesting.

121
00:16:29,040 --> 00:16:36,160
I think the first step. So you have the millions of images. These are

122
00:16:37,920 --> 00:16:46,640
kind of raw images, so to speak, off of the camera. And your first step in curating this

123
00:16:46,640 --> 00:16:52,560
data set is, well, probably first you just took a bunch of images and kind of proved to yourself

124
00:16:52,560 --> 00:16:59,760
that deep learning would work. But then the next step is, okay, how do we curate a data set that really

125
00:16:59,760 --> 00:17:11,280
works for all the cases that you need to consider? And in order to do that, you couldn't manually go

126
00:17:11,280 --> 00:17:17,040
through millions of images to identify. You essentially have to label all of your images to identify

127
00:17:17,040 --> 00:17:24,400
the ones that were useful. And that is costly and expensive. And so you did what?

128
00:17:24,960 --> 00:17:35,120
We tried to approach it in a couple of ways. One is the first one and more simpler in my mind is,

129
00:17:35,120 --> 00:17:42,480
let's try and train a network and randomly pick images from different data sets.

130
00:17:42,480 --> 00:17:51,280
And see what the performance is. So essentially, down sizing your

131
00:17:53,280 --> 00:18:01,040
your these millions of images to a few thousands of images by randomly sampling.

132
00:18:02,240 --> 00:18:08,080
So before we even annotate, presentation is costly. Let's do inference and see, do we get good

133
00:18:08,080 --> 00:18:15,600
results or not? If we don't get good results, then the network is balanced. So we should probably

134
00:18:17,040 --> 00:18:20,400
use these images, these challenging images in the data set.

135
00:18:20,400 --> 00:18:27,920
So and just to make sure I understand that you've got these different data sets that represent

136
00:18:27,920 --> 00:18:35,360
kind of in a discrete way some of the variability that you see in the wild. So what if we sample

137
00:18:35,360 --> 00:18:42,320
kind of randomly from across these data sets that in aggregate, we're still representing

138
00:18:42,320 --> 00:18:47,760
the variability that we see in the wild. And so let's train a network on that and see if that works.

139
00:18:47,760 --> 00:18:56,240
Yes, but not necessarily before you even train, let's see how the current network performs

140
00:18:56,240 --> 00:19:06,160
on these images across the spectrum. The sort of nice thing is that there is variability in this

141
00:19:06,160 --> 00:19:12,240
problem, but there is not unlimited variability. I mean, at some point you have exhausted all of the

142
00:19:12,240 --> 00:19:20,240
different soil types you would encounter. You have exhausted flying in the morning at the middle of

143
00:19:20,240 --> 00:19:30,960
the day or in the evening or later in the day. So there are a few components that create,

144
00:19:31,760 --> 00:19:39,120
if you combine them, create something unique, but they are finite. So it's through this sort of

145
00:19:39,120 --> 00:19:45,840
brute force method you can get a good sense of what you're dealing with. After doing this to

146
00:19:45,840 --> 00:19:52,320
the manual labor for a few images, then next year comes we're getting two times more images

147
00:19:52,320 --> 00:20:02,080
than the previous one. And then you start asking yourself if you have actually captured all the

148
00:20:02,080 --> 00:20:09,120
changes that you could actually see in the field. And if that statement holds, then you're right,

149
00:20:09,120 --> 00:20:16,960
we're done, but if it doesn't, and how do I know if it doesn't? So then the next step was to do

150
00:20:16,960 --> 00:20:25,760
something a little bit more sophisticated, let's say. And that's when I started reading about

151
00:20:25,760 --> 00:20:36,640
zero-shot learning and clustering without a supervision. I thought that this is probably a good

152
00:20:36,640 --> 00:20:44,640
area to try and implement. That would probably be an adequate solution or at least the most

153
00:20:44,640 --> 00:20:51,680
sophisticated solution that would ease my anxiety of whether I captured enough data or not.

154
00:20:51,680 --> 00:20:58,400
And so what does a zero-shot learning-based approach look like in your scenario?

155
00:20:58,400 --> 00:21:05,280
In my case, essentially, we're talking about finding visual similarities between images.

156
00:21:05,280 --> 00:21:14,160
And essentially, this happens by downsizing significantly the images we capture.

157
00:21:14,160 --> 00:21:26,400
Yeah, I should have mentioned this before, but one of the unique properties of the problem is

158
00:21:26,400 --> 00:21:31,680
that the images we capture are at least 3,000 by 4,000 pixels. So we're doing what I'm about

159
00:21:31,680 --> 00:21:38,800
a very high definition. Images, some of them are also a wide field of view. We're going to

160
00:21:38,800 --> 00:21:51,280
feed an image like that in a normal network. So downsizing significantly to something I can

161
00:21:51,280 --> 00:22:01,360
be handled is step number one. And then after this downsizing, we're passing it through a network that

162
00:22:01,360 --> 00:22:12,480
acts as sort of a not only code there. Essentially takes the image to the dimensional image and finds

163
00:22:12,480 --> 00:22:24,000
a representation in a higher dimensional space. And by doing so, it allows the algorithm itself to

164
00:22:24,000 --> 00:22:32,800
find which images are close by in that higher dimensional space and then clusters them together.

165
00:22:32,800 --> 00:22:44,480
And then through this by looking at this embedding space, essentially, which is this higher dimensional

166
00:22:44,480 --> 00:22:52,960
one, you can sort of find similarities between images. And what you will find may surprise you.

167
00:22:52,960 --> 00:23:05,040
Because what is what are similar images for the algorithm? For us, maybe some uniformity in the

168
00:23:05,040 --> 00:23:14,240
soil. The first time we tried this, the algorithm just grouped the images based on the direction

169
00:23:14,240 --> 00:23:23,360
of the row. Not very useful. How do you adjust for that? How do you iterate towards an algorithm

170
00:23:23,360 --> 00:23:35,040
that works? So this happens with augmentations. So what the algorithms do is that they do not have

171
00:23:35,040 --> 00:23:42,560
explicit constraints between images that should look alike. It's completely unsupervised. So what

172
00:23:42,560 --> 00:23:49,200
they do is that they take an image, they apply some augmentations and then they treat the augmented

173
00:23:49,200 --> 00:23:56,640
image and the original image as similar. So the network is being trained through this process of

174
00:23:58,640 --> 00:24:04,880
taking an image, augmenting it and then projecting it, projecting both of them in that embedding

175
00:24:04,880 --> 00:24:11,440
space. And these two should be very close in the embedding space because they come from the

176
00:24:11,440 --> 00:24:18,880
same image. So essentially, in order to overcome the issues of unwanted clustering or unwanted

177
00:24:18,880 --> 00:24:25,280
similarity, essentially, you are augmenting so that the network learns that it doesn't matter

178
00:24:25,280 --> 00:24:32,000
what direction the rows are with random rotations, for example. So the implementation you can

179
00:24:32,000 --> 00:24:38,640
use is the random rotation. This means that the network learns now that the image where the rows

180
00:24:38,640 --> 00:24:47,520
are perpendicular is very similar with the image where the rows are horizontal. So this misses

181
00:24:47,520 --> 00:24:52,560
this characteristic of the image and looks for something else. And I'm imagining that

182
00:24:53,680 --> 00:25:00,960
you alluded to this earlier, the only way you really figure out what is happening is you have to

183
00:25:00,960 --> 00:25:08,000
look at the results and spend a lot of time and just kind of intuit what the network is trying to

184
00:25:08,000 --> 00:25:20,800
do. By the way, this is one of the reasons why I relate algorithmic development in computer vision

185
00:25:20,800 --> 00:25:27,360
because you can't see the result. So you know if you're doing something wrong. Yes, it requires

186
00:25:27,360 --> 00:25:39,840
a lot of verification and reviewing. But then it is possible to get the images from the embedding

187
00:25:39,840 --> 00:25:46,960
space down into 2D or you have the connection. Essentially, you know that this image is that one

188
00:25:46,960 --> 00:25:55,120
in the embedding space. So you can see you can, you know, through a key means or a clustering,

189
00:25:55,120 --> 00:26:03,040
you can find which are the closest neighbors. And you know, through that essentially move forward

190
00:26:03,040 --> 00:26:07,200
with making informed decisions or whether what you're trying to do works or not.

191
00:26:08,560 --> 00:26:17,040
Do you start with images that, you know, some smaller set of images that you know how they relate

192
00:26:17,040 --> 00:26:24,320
and then look and see if they relate in the space or... Yes, definitely. I mean, you know,

193
00:26:24,320 --> 00:26:32,640
it's like this unit testing sort of attitude towards the data. Yeah, and again, as I mentioned,

194
00:26:32,640 --> 00:26:40,960
with having millions of images, it doesn't allow you to literally just start processing

195
00:26:42,320 --> 00:26:48,000
gigabytes of data. And since most of the processing, if not all of the processing is done on the cloud,

196
00:26:48,000 --> 00:26:55,600
you can imagine, you know, the hefty price will have to pay. So talk a little bit about the

197
00:26:55,600 --> 00:27:01,840
result you saw with the zero shot approach. How did it work out for you? So we're still the

198
00:27:02,480 --> 00:27:11,520
testing phase. So we're working with smaller data. Crop season started sometime in May. And

199
00:27:11,520 --> 00:27:19,440
that means for us, stop whatever you do and make sure that the product works. So, yeah, so

200
00:27:20,000 --> 00:27:28,160
through some testing, it seems to be working well. Some networks work better than others, of course.

201
00:27:29,680 --> 00:27:40,160
But we have been able to identify what we are looking for, which is that we can decrease the number

202
00:27:40,160 --> 00:27:49,200
of images that express some unique characteristics by about 40%. Which is great, because this means that,

203
00:27:49,200 --> 00:27:55,040
you know, from, instead of annotating 100 images, now we can annotate 60. Since we're going through

204
00:27:55,040 --> 00:28:00,800
a process of somewhat cumbersome and elaborate process of annotation, where 18 images seen by

205
00:28:01,600 --> 00:28:04,720
two people at least, and then we need to run some

206
00:28:04,720 --> 00:28:13,280
comparisons and figure out if there's an agreement scoring high enough or not, and then correct the

207
00:28:13,280 --> 00:28:22,560
annotations. That's a huge time saver. And maybe should have raised this as a point more directly

208
00:28:22,560 --> 00:28:29,520
earlier on, but the, you know, we, in the context of deep learning, we talk, you know, we throw

209
00:28:29,520 --> 00:28:34,880
around this idea, collect more data, collect more data, collect more data, but really your ultimate

210
00:28:34,880 --> 00:28:40,880
goal here, well, your ultimate goal is to produce a better product, but your intermediate goal

211
00:28:42,080 --> 00:28:50,160
was not to collect more data. It was to, you know, identify less data or the right data to annotate,

212
00:28:50,160 --> 00:28:59,920
so that you wouldn't get eaten alive by annotation costs. That is very true, yes. And I think that I, we

213
00:28:59,920 --> 00:29:09,120
probably see that this is a direction most people are heading, even in recent research that's,

214
00:29:09,920 --> 00:29:17,440
that's probably a narrative that people are trying to push. And I think it actually makes sense.

215
00:29:17,440 --> 00:29:27,440
The fortunate thing about this particular problem, the counting, the plan counting is that we are

216
00:29:28,080 --> 00:29:34,320
receiving the data, whether we want it or not, because people are flying, collect, collect data

217
00:29:34,320 --> 00:29:40,800
for their own purposes, so they're there, so why not take advantage of it? But if you think of

218
00:29:40,800 --> 00:29:51,360
how we, we will be approaching new products, then that's a game changer, because

219
00:29:53,840 --> 00:29:59,600
you know, a new customer comes in and they want to count potatoes. The first thing people are asking

220
00:29:59,600 --> 00:30:07,920
me is, how many, how many images do you want? And I think after asking me so many times already,

221
00:30:07,920 --> 00:30:14,400
we have created some common understanding where they know they know not to ask me this question.

222
00:30:16,400 --> 00:30:22,160
Because my answer is always, I don't want more images, I want a variety of images, so it's better if

223
00:30:22,160 --> 00:30:28,880
you go to 10 different fields in different states and capture two images from it than giving me

224
00:30:28,880 --> 00:30:37,920
1000 images from two neighboring fields, for example. And are those specific numbers to, you know, kind of

225
00:30:39,120 --> 00:30:44,560
arbitrary numbers to prove your point, or is that kind of the order of magnitude that you actually

226
00:30:44,560 --> 00:30:51,920
require? I would take the actual order, mind you, would be around 100. That's the typical, because

227
00:30:51,920 --> 00:30:58,000
again, since they are quite large, the images are quite large, you may imagine to train the network,

228
00:30:58,000 --> 00:31:07,360
you have to crop them down. So out of one image, you can generate 40, smaller ones, but we'll

229
00:31:07,360 --> 00:31:13,280
actually go into training. So I think it's a substantial number, and it always depends on the

230
00:31:13,280 --> 00:31:20,720
problem we're trying to solve. But I think it's an indicative number. Yeah, just to kind of

231
00:31:20,720 --> 00:31:28,400
to play back, the, you know, it's a, when we think about, you know, deep learning, like that's an

232
00:31:28,400 --> 00:31:35,840
astonishingly small number, what we're doing with that 100, you know, that's a 100 images across

233
00:31:35,840 --> 00:31:45,360
some representative number of fields, and what you're creating with that is this embedding space

234
00:31:45,360 --> 00:31:55,600
that you can then use to create automatically more, you know, a more balanced data set,

235
00:31:55,600 --> 00:32:00,640
which will probably consist of a lot more images. Is that right?

236
00:32:00,640 --> 00:32:06,800
The reason why this helps a lot with the creation of new products is that I can, I can quickly

237
00:32:06,800 --> 00:32:16,960
identify how many more images, or in which locations or what type of images we would like to

238
00:32:17,520 --> 00:32:27,440
collect more of. So, let me get these 100 images, and then through this zero-short learning,

239
00:32:27,440 --> 00:32:35,840
we'll find some, we have already, we have identified a few different clusters that we expect,

240
00:32:36,880 --> 00:32:41,040
you know, different light conditions, different sort of different sort of types, whether the

241
00:32:41,040 --> 00:32:48,560
images are the edge of the field or not. So, these are things that we expect. So, essentially,

242
00:32:49,200 --> 00:32:55,840
by doing this clustering, I know where I am short, what type of image I am short of.

243
00:32:55,840 --> 00:33:04,400
So, this is sort of the first leap iteration. I get 100 images, identify what type of images I want

244
00:33:04,400 --> 00:33:08,320
more, and then I send them back out to collect more of that type.

245
00:33:08,320 --> 00:33:15,440
Oh, okay. So, it's not even just reducing labeling costs. Here, we're talking about,

246
00:33:16,400 --> 00:33:22,320
you know, the cost of sending someone to a field and flying a plane or a drone, and, you know,

247
00:33:22,320 --> 00:33:28,720
it's the whole end-to-end data collection costs, which I'm imagining is significantly more

248
00:33:28,720 --> 00:33:36,000
even. Yes, it can be, especially people are busy during the growing season. So, it is, you know,

249
00:33:36,000 --> 00:33:42,320
I have to be very respectful of their time when I'm asking you data. And in previous years, it was more

250
00:33:42,320 --> 00:33:49,200
of a gut feeling. You know, we have seen so many images, we have seen what works or what and what

251
00:33:49,200 --> 00:33:57,440
doesn't work. But now it's more specific, more targeted. I want this, you know, fly at this time,

252
00:33:58,080 --> 00:34:06,000
try to get images of where, you know, the day is not cloudy. So, it gives you a lot of space

253
00:34:06,000 --> 00:34:12,560
to plan. So, this last example you talk through is still a counting type of a problem. Do you see the

254
00:34:12,560 --> 00:34:23,920
approach applying to a broader set of projects? Yes, for sure. The next place I would like to

255
00:34:23,920 --> 00:34:31,040
spend more time is detect the weed detection. And that is also something that is significant for

256
00:34:31,040 --> 00:34:41,280
the industry. And it's also a problem where the annotation is much more costly. You can imagine

257
00:34:41,280 --> 00:34:46,960
what, when you're counting things, you are requesting a human annotator to click on the object you

258
00:34:46,960 --> 00:34:54,320
want to count for weeds, especially when you have patches of weed. We're talking about drawing

259
00:34:54,320 --> 00:35:03,680
polygons. And the polygons are a series of clicks. And that requires, you know, more concentration

260
00:35:03,680 --> 00:35:07,120
from the annotator because there may be small weeds. We don't want to miss those.

261
00:35:07,120 --> 00:35:14,800
Polygons are generally more time consuming than just clicking and then going to the next one.

262
00:35:15,760 --> 00:35:20,640
So, this is, I think it will be a time saver for annotations there too.

263
00:35:20,640 --> 00:35:32,080
And maybe one last question. More broadly, has what you've learned in, you know, this project and

264
00:35:32,080 --> 00:35:40,080
this approach change how you think about deep learning problems and applying deep learning within

265
00:35:40,080 --> 00:35:45,920
your space? Yeah. I mean, previously I thought that 80% of the solution is data. Now I think 90%

266
00:35:45,920 --> 00:35:56,560
of the solution is data. This has, in use, some changes or demanded some changes in how we architect

267
00:35:56,560 --> 00:36:06,960
the deep learning framework we're building. These are more technical changes, essentially.

268
00:36:06,960 --> 00:36:28,080
So, we have a data pipeline that goes from fetching data from our main database to reviewing the

269
00:36:28,080 --> 00:36:33,120
selection of images we're doing, to sending them to a couple of human annotators and then

270
00:36:33,120 --> 00:36:39,920
compare their annotations and do some quality control. So, initially, the way we were doing the

271
00:36:40,480 --> 00:36:49,840
fetch data from the main database was either random or I'm going to select a few images and then say

272
00:36:49,840 --> 00:36:56,880
this image has weeds in it, this image has different type of soil, this image is cloudy, etc.

273
00:36:56,880 --> 00:37:05,920
Now things are more automated, which means that we can process, we can make this type of decision

274
00:37:05,920 --> 00:37:15,920
in season since it's automated. We can essentially assess the images we collect Monday to Friday,

275
00:37:16,720 --> 00:37:21,680
we can make the assessment over the weekend and then on Monday we can start training a new

276
00:37:21,680 --> 00:37:30,480
network if there are any new data we want to include. As opposed to, before that, since it was a

277
00:37:30,480 --> 00:37:37,680
human intense process, we had to wait until the end of the growing season where everybody was

278
00:37:37,680 --> 00:37:48,320
relaxed and free and then we could do this type of data discovery. So, yeah, there are some small

279
00:37:48,320 --> 00:37:55,440
changes, essentially, with additional features that we couldn't do before. Awesome, awesome.

280
00:37:55,440 --> 00:38:02,320
Well, it sounds like a very exciting project and journey and I'm super appreciative of you jumping

281
00:38:02,320 --> 00:38:07,120
on and sharing a bit about what you've accomplished to me, Tris. Thank you very much, Sam. Thank you

282
00:38:07,120 --> 00:38:18,400
for having me, for reaching out and I hope this, my experience has helped. I'll see if there goes.

283
00:38:18,400 --> 00:38:45,840
Awesome. Thanks so much. Bye-bye.

