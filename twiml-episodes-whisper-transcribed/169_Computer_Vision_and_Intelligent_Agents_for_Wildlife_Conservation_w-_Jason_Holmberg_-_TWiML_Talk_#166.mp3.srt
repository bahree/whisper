1
00:00:00,000 --> 00:00:06,080
Hey everybody, Sam here. We've got some great news to share, and also a favorite

2
00:00:06,080 --> 00:00:12,360
to ask. We're in the running for this year's People's Choice Podcast Awards, in both the

3
00:00:12,360 --> 00:00:17,960
People's Choice and Technology categories, and we would really appreciate your support.

4
00:00:17,960 --> 00:00:24,000
To nominate us, you'll just head over to twimlai.com slash nominate, where we've linked to and

5
00:00:24,000 --> 00:00:29,720
embedded the nomination form from the award site. There, you'll need to input your information

6
00:00:29,720 --> 00:00:35,480
and create a listener nomination account. Once you get to the ballot, just find and select

7
00:00:35,480 --> 00:00:41,560
this week in machine learning and AI on the nomination list for both the Adam Curry People's

8
00:00:41,560 --> 00:00:49,400
Choice Award and the this week in tech technology category. As you know, we really, really appreciate

9
00:00:49,400 --> 00:00:55,320
each listener and would love to share in this accomplishment with you. Remember that URL

10
00:00:55,320 --> 00:01:02,520
is twimlai.com slash nominate. Feel free to hit pause and take a moment to nominate us now.

11
00:01:14,360 --> 00:01:20,520
Hello and welcome to another episode of Twimla Talk, the podcast why interview interesting people,

12
00:01:20,520 --> 00:01:26,440
doing interesting things in machine learning and artificial intelligence. I'm your host Sam

13
00:01:26,440 --> 00:01:40,440
Charrington. In this episode, I'm joined by Jason Holmberg, Executive Director and Director

14
00:01:40,440 --> 00:01:47,240
of Engineering at Wild Me. Wild Me's Wild Book and Wild Book for Whale Sharks are both open source

15
00:01:47,240 --> 00:01:52,840
computer vision based conservation projects that have been compared to a Facebook for wildlife.

16
00:01:53,720 --> 00:01:59,160
Jason kicks us off with the really interesting story of how Wild Book came to be and its eventual

17
00:01:59,160 --> 00:02:06,440
expansion from a focus on whale sharks to include giant manta rays, humpback whales, zebras and giraffes.

18
00:02:07,560 --> 00:02:12,360
Jason and I explore the evolution of these projects use of computer vision and deep learning,

19
00:02:12,360 --> 00:02:17,400
the unique characteristics of the models they're building and how they're ultimately enabling a

20
00:02:17,400 --> 00:02:22,920
new kind of citizen science. Finally, we take a look at a cool new intelligent agent project that

21
00:02:22,920 --> 00:02:28,360
Jason is working on which mines YouTube for wildlife sightings and automatically engages with

22
00:02:28,360 --> 00:02:34,120
the relevant individuals and scientists on Wild Book's behalf. And now onto the show.

23
00:02:34,120 --> 00:02:44,280
All right, everyone. I am on the line with Jason Holmberg. Jason is the Director of Engineering

24
00:02:44,280 --> 00:02:49,480
for Wild Me. Jason, welcome to this week in machine learning and AI. Hey, thanks for having me on.

25
00:02:49,480 --> 00:02:53,480
Jason, why don't you start out by telling us a little bit about your background and how you got

26
00:02:53,480 --> 00:03:02,280
involved in ML and AI. So I have a very unusual background and journey into running an organization

27
00:03:02,280 --> 00:03:07,320
that is focused on applying machine learning to wildlife conservation. I am a scuba diver.

28
00:03:07,320 --> 00:03:14,040
My background is in chemical engineering and in Arab studies. I did a master's degree at Georgetown.

29
00:03:14,040 --> 00:03:20,280
So neither of these would be suggestive of a journey to machine learning except maybe the

30
00:03:20,280 --> 00:03:26,840
scuba diving because wildlife is an area where we see a lot of application potential for machine

31
00:03:26,840 --> 00:03:31,560
learning. But the journey is a little bit interesting in that as I got this Arab studies degree,

32
00:03:31,560 --> 00:03:39,000
I found myself living at various times in the Middle East, in Egypt, in Tunisia, in Lebanon.

33
00:03:39,000 --> 00:03:44,440
And on one of those opportunities, I, one of those trips, I was able to travel down to Djibouti

34
00:03:44,440 --> 00:03:49,800
there in East Africa, which is an amazing melting pot of cultures just sitting there.

35
00:03:50,440 --> 00:03:55,720
And I was able to go scuba diving and I was excited to see the world's biggest fish, the whale shark.

36
00:03:55,720 --> 00:04:01,800
There was no guarantee I would see one and we did this live aboard sailboat going out to sail

37
00:04:01,800 --> 00:04:07,240
and dive amongst the set frayers islands, which means seven brothers. There are coincidentally

38
00:04:07,240 --> 00:04:13,880
only six islands, so I'm not sure who did the math, but there I was. It embedded sort of in a group

39
00:04:13,880 --> 00:04:19,240
of French tourists on a sailboat and in fact I went scuba diving and as we were coming up,

40
00:04:19,240 --> 00:04:26,680
there was a whale shark. No, whale sharks grow up to 60 feet in length and this was an eight foot

41
00:04:26,680 --> 00:04:31,480
juvenile. So quite small, but actually that's extremely rare. Most of the whale sharks we see in

42
00:04:31,480 --> 00:04:36,840
the wild are about 15 feet and larger. So this was a very rare sighting of this juvenile and I was

43
00:04:36,840 --> 00:04:42,200
just enamored. And the interesting thing about a small whale shark is that I was proportional and

44
00:04:42,200 --> 00:04:48,760
sized to it. So this animal looked back at me, this giant fish and we had a moment of just swimming

45
00:04:48,760 --> 00:04:54,680
together underwater. It was curious about me. Later in life, I was able to swim with some 40-foot

46
00:04:54,680 --> 00:04:59,800
whale sharks in the Galapagos Islands and interacting with a giant, a Leviathan like that is a very

47
00:04:59,800 --> 00:05:06,440
different experience. I'm nothing to a Leviathan. I'm just a passing speck of dust in the water

48
00:05:06,440 --> 00:05:11,480
column. But this young juvenile sort of gave me the time of day if you will and we swam together

49
00:05:11,480 --> 00:05:17,960
for a bit and then it disappeared and that just sparked something. This was, this was in the spring

50
00:05:17,960 --> 00:05:25,000
of 2002. So flash forward, fall of 2002, I decided to go on a whale shark research expedition and

51
00:05:25,000 --> 00:05:30,600
I spent a week down in Baja California on this small boat as this micro light plane circles overhead,

52
00:05:30,600 --> 00:05:35,240
you know, trying to spot the whale shark and call down to us. And the sad story is that we saw

53
00:05:35,240 --> 00:05:40,040
absolutely no whale sharks while we were baking in the Sun for an entire week. But I had the second

54
00:05:40,040 --> 00:05:46,200
spark, which is as I sat there in the boat next to this field biologist who's got a spear and a

55
00:05:46,200 --> 00:05:53,160
plastic tag that's almost like, you know, the somebody's title on their desk, you know,

56
00:05:53,160 --> 00:05:57,480
those sliding plastic tags that they might have saying their name and their title. It's about

57
00:05:57,480 --> 00:06:02,680
that size and often it will have letters and numbers on it indicating the individual that they

58
00:06:02,680 --> 00:06:07,320
tagged. So I'm sitting next to this biologist for a full week, nothing to do. And he's got this

59
00:06:07,320 --> 00:06:12,120
spear in this tag and I ask him, you know, what percentage of the time do you, you know,

60
00:06:12,120 --> 00:06:17,080
recite this tag? You're about to spear on this side of a whale shark and he said, oh, you know,

61
00:06:17,080 --> 00:06:22,280
about 1% of the time. And I said, wait a second, 1% because I have an engineering background and

62
00:06:22,280 --> 00:06:27,480
I guarantee you anytime you have a process that's 1% efficient, I can get you to two, right? Just

63
00:06:27,480 --> 00:06:34,840
just out of rock curiosity and labor. And that was the second kick in the pants. I happened to

64
00:06:34,840 --> 00:06:40,520
have graduated Georgetown with my Arab studies degree was actually working at a company called

65
00:06:40,520 --> 00:06:47,720
Softworks EMC, then EMC, then now it's called Dell EMC. And I had, I was learning to code and I had

66
00:06:47,720 --> 00:06:52,920
all this extra time in my hands. I was young and single and I started thinking about the pattern

67
00:06:52,920 --> 00:06:58,520
of spots on the whale shark and whale shark can have blue or brownish skin, but it has all these

68
00:06:58,520 --> 00:07:03,720
white spots like constellations just rippling across both sides and the top of the animal.

69
00:07:04,360 --> 00:07:08,520
And I started thinking about using it as fingerprints and and I wasn't the first person to think

70
00:07:08,520 --> 00:07:14,040
of that. It had been theorized, but nobody had actually sort of implemented a pattern recognition

71
00:07:14,040 --> 00:07:20,280
system for whale sharks. And here you see the hints of of computer vision emerging, right?

72
00:07:20,920 --> 00:07:27,960
So this is this is fall 2002 and I began creating my own, you know, basic trigonometry type

73
00:07:27,960 --> 00:07:32,520
spot pattern recognition system where I would take a photograph and I would map the spots and I

74
00:07:32,520 --> 00:07:37,560
would take those sets of coordinates and have, you know, 10 comparison sets in my list and then

75
00:07:37,560 --> 00:07:42,600
my N plus one set of coordinates. And I would try to come up with some complex trigonometry.

76
00:07:43,480 --> 00:07:48,120
And and really wasn't getting anywhere, you know, by about the 11th or the 12th pattern,

77
00:07:48,120 --> 00:07:53,480
my accuracy would just start falling apart. And so I remember working late one night, not

78
00:07:53,480 --> 00:07:58,120
not on work, but actually on this. It was a Friday night and I had no plans and a friend of mine,

79
00:07:58,840 --> 00:08:03,960
who we now have a really long term friendship. He is Dr. Zauvin Erzemanian at NASA Greenbelt.

80
00:08:03,960 --> 00:08:11,320
He called me out for a beer and as some discoveries go, they happen over a beer. And so I sort of

81
00:08:11,320 --> 00:08:16,440
dejectedly say, well, yeah, sure, why not? I'm not making any progress here on Friday night.

82
00:08:16,440 --> 00:08:22,840
So I go downtown Washington, DC and I sit down with Zauvin and order a beer and he has another

83
00:08:22,840 --> 00:08:26,920
astronomer with him there coming back from a convention. This is an optical astronomer.

84
00:08:26,920 --> 00:08:31,960
And Zauvin says, hey, why don't you tell my friend about what you're trying to do with with the sharks?

85
00:08:31,960 --> 00:08:37,000
I said, well, you know, I'm trying to map the spots on the side of a whale shark and compare the

86
00:08:37,000 --> 00:08:42,760
patterns across photographs. And casual as can be this optical astronomer whose name I've forgotten

87
00:08:42,760 --> 00:08:48,120
at the moment just turns to me and goes, oh, yeah, we do that. What are you talking about? Hold on a

88
00:08:48,120 --> 00:08:52,120
second. What are you talking about? And he said, well, in optical astronomy, we take multiple pictures

89
00:08:52,120 --> 00:08:57,640
of the night sky and we want to create large composite images. And sure enough, what we need to do

90
00:08:57,640 --> 00:09:02,040
is triangulate on different constellations. And that allows us to get key points and create master

91
00:09:02,040 --> 00:09:07,880
image sets. And I was just blown away. And so, you know, he references me to the right papers.

92
00:09:07,880 --> 00:09:12,120
And then Zauvin really sees that I'm going to put in the effort. And so he sits down as an astronomer

93
00:09:12,120 --> 00:09:16,120
to help me, even though he's a pulsar astronomer, he, you know, moon lighting and whale shark research

94
00:09:16,120 --> 00:09:20,680
is just something you might do at that level, I guess. I'm not sure I saw the connection coming

95
00:09:20,680 --> 00:09:26,920
when you referred to the spot patterns as constellations. Yeah, exactly. And this just sort of all

96
00:09:26,920 --> 00:09:33,480
unfolds in this, you know, amazing way. And so Zauvin and I sit down and translate this 1984

97
00:09:33,480 --> 00:09:39,880
arcane paper by, I believe it's Edward growth, who it turns out also as, you know, as these things

98
00:09:39,880 --> 00:09:45,080
do, sometimes that Zauvin used to play baseball with. But nonetheless, this Edward growth is a

99
00:09:45,080 --> 00:09:49,960
professor who's funded under the Hubble Space Telescope. He develops this algorithm for mapping

100
00:09:49,960 --> 00:09:56,440
and comparing star patterns across photographs. And it works. We modify it a little bit. You know,

101
00:09:56,440 --> 00:10:00,280
the photographs of the night sky can have, you know, need to be rotationally independent,

102
00:10:00,280 --> 00:10:03,720
depending on, depending on where the observer is, but whale sharks have a top and a bottom. So

103
00:10:03,720 --> 00:10:10,440
we make these changes. And voila, we have this highly accurate whale shark spot pattern comparison

104
00:10:10,440 --> 00:10:15,320
system. And we can use the patterns on the left and the right side as a pair of tags, like a left

105
00:10:15,320 --> 00:10:21,960
and a right thumbprint. So now we have the inklings of a, you know, an early 2000s computer vision

106
00:10:21,960 --> 00:10:27,400
matching system and one that's actually fairly well-defined and proven. Here's the other problem,

107
00:10:27,400 --> 00:10:32,280
though. Where do I get all these photographs from? So at this point, I reach out to a biologist

108
00:10:32,280 --> 00:10:39,240
named Brad Norman in Western Australia. He had worked for years swimming with the whale sharks

109
00:10:39,240 --> 00:10:45,080
that congregate off X-Muth and Coral Bay up in the northwest part of Australia. And it's a

110
00:10:45,080 --> 00:10:51,000
really good tourism destination, very reliable place to go and swim with these whale sharks from about,

111
00:10:51,000 --> 00:10:57,160
let's say, late March to late July every year. And he'd been collecting these photographs on the

112
00:10:57,160 --> 00:11:02,920
off chance that somebody came up with this algorithm. So I blindly reach out to Brad Norman via

113
00:11:02,920 --> 00:11:07,720
email and he responds very skeptically and I say, hey, you know, I think I might have this and you

114
00:11:07,720 --> 00:11:13,960
might have the other piece, which is the data. And so we begin this long-distance collaboration

115
00:11:13,960 --> 00:11:20,040
showing that this computer vision system actually worked. And we could reliably across years,

116
00:11:20,040 --> 00:11:25,640
across data sets, across photographs, compare individual whale sharks and determine without

117
00:11:25,640 --> 00:11:29,800
physically tagging the animal. That in fact, years later, this was the same whale shark we might

118
00:11:29,800 --> 00:11:35,320
have seen years before. So then we hit what we sort of thought that was the big problem, like

119
00:11:35,320 --> 00:11:39,240
creating the pattern recognition system. But what we actually found is there was simply no

120
00:11:39,880 --> 00:11:45,400
database content management system for wildlife at all, not even a data model that really could

121
00:11:45,400 --> 00:11:51,080
be used out of the box. And so it turns out that 90% of the work was creating the precursors to what

122
00:11:51,080 --> 00:11:58,200
we now call wild book, which is an open source platform to marry good data management,

123
00:11:58,840 --> 00:12:06,680
computer vision, and citizen science altogether to create a platform for wildlife biologists to

124
00:12:06,680 --> 00:12:13,000
collect more data, process it faster, especially in the context of what's called marker capture.

125
00:12:13,000 --> 00:12:20,840
So if you've ever seen the tag on a deer's ear hanging off, or if you've ever seen the band

126
00:12:20,840 --> 00:12:26,360
on a bird's foot with a number on it, those are the tags. And for whale sharks, it was the spear

127
00:12:26,360 --> 00:12:31,880
and the plastic tag. But those are used to repeatedly identify individuals in a population

128
00:12:31,880 --> 00:12:36,760
in what's called mark and recapture studies. And out the other end, you can either get an abundance

129
00:12:36,760 --> 00:12:41,480
estimate how many of these critically endangered animals do we have, or at least a population

130
00:12:41,480 --> 00:12:45,960
trajectory? Are there numbers relatively getting bigger or smaller from when we started sampling?

131
00:12:46,680 --> 00:12:53,640
And so it turns out that photographic data is now so ubiquitous that harnessing it means that

132
00:12:53,640 --> 00:12:59,400
wildlife biologists who tend to still use Microsoft access and excel on a daily basis, you know,

133
00:12:59,400 --> 00:13:04,840
never mind cloud computing, which is not even a new concept anymore, that they could suddenly

134
00:13:04,840 --> 00:13:11,000
have a platform to utilize this much richer source of data. And it turns out it's much better for

135
00:13:11,000 --> 00:13:15,880
the animals. We're not physically harming them or getting in the way of their daily life.

136
00:13:17,320 --> 00:13:22,440
And so our success applying this to whale sharks, building a cloud platform that allowed

137
00:13:22,440 --> 00:13:27,000
citizen scientists, divers, and snarklers to submit their photographs, the computer vision,

138
00:13:27,000 --> 00:13:32,680
which was the carrot that researchers needed to drop their competitive boundaries and begin

139
00:13:32,680 --> 00:13:37,960
collaborating for what is a massively migratory fish. All of this just sort of melded nicely

140
00:13:37,960 --> 00:13:43,960
into what is now whaleshark.org, the wild book for whale sharks. And whaleshark.org has over 150

141
00:13:43,960 --> 00:13:49,320
researchers and volunteers all over the globe who log in. And about close to 6,000 people from

142
00:13:49,320 --> 00:13:55,960
the public have contributed photographs. That model has proved so successful and so cost-effective

143
00:13:55,960 --> 00:14:03,560
that we've started replicating it for giant manta rays for humpback whales and for zebras and

144
00:14:03,560 --> 00:14:10,440
giraffes now. And as we started achieving success, that's when a new cadre of researchers showed up.

145
00:14:11,320 --> 00:14:15,160
Those researchers are now my copy eyes on the wild book project, their

146
00:14:15,800 --> 00:14:21,080
Professor Tony Burger Wolf, University of Illinois Chicago. She's the overall project leader.

147
00:14:21,080 --> 00:14:26,440
She does data science. We work with Professor Chuck Stewart, Rensselaer Polytechnic Institute.

148
00:14:26,440 --> 00:14:31,400
He has this amazing lab almost exclusively dedicated to wildlife computer vision.

149
00:14:31,400 --> 00:14:36,200
And I actually was able to hire one of his students earlier this year. That's another story related

150
00:14:36,200 --> 00:14:41,480
to an anonymous Bitcoin millionaire. If you've ever heard of the Pineapple Fund, we can talk about

151
00:14:41,480 --> 00:14:48,920
that in a bit. And I had no. It's an interesting story. And then Dr. Dan Rubenstein, professor of

152
00:14:48,920 --> 00:14:57,240
ecology at Princeton University. And somehow, and I feel as I've told you this story that how much

153
00:14:57,240 --> 00:15:02,360
luck has played, I lucked into a group of four collaborative PIs. And at one point,

154
00:15:02,360 --> 00:15:07,000
we all just looked at each other via Skype or whatever mechanism it was at the time. And we

155
00:15:07,000 --> 00:15:10,920
said, you know, this is the project of a lifetime. We all sort of nodded. And there's this

156
00:15:10,920 --> 00:15:17,640
general sense that funded or not funded somehow, we're just going to get this done. And amazingly,

157
00:15:17,640 --> 00:15:22,440
we are now a group of specialists working together before it was me sort of trying to hack

158
00:15:22,440 --> 00:15:27,160
through a computer vision system and apply the data management. Now we have this wonderful pathway

159
00:15:27,160 --> 00:15:31,640
of original research for computer vision done at Rensselaer Polytechnic advised by Tanya at

160
00:15:32,200 --> 00:15:38,040
UIC. And then we transitioned that over to software engineering team at Wild Me. So Wild Me is

161
00:15:38,040 --> 00:15:43,160
executive director. I manage four staff, one machine learning expert, Jason Parham. And then

162
00:15:43,800 --> 00:15:50,760
three software developers, Zhu Blunt, John Vandost, and Colin Kingan, all of whom are nine to five

163
00:15:50,760 --> 00:15:55,800
professional software developers working on wildlife conservation. And just the fact that for

164
00:15:55,800 --> 00:16:01,160
however long we can afford them, we can afford this dedicated of a team is it is a giant coup in

165
00:16:01,160 --> 00:16:06,920
conservation. I'm I'm just excited at where we are. And if you've ever heard of Microsoft's AI

166
00:16:06,920 --> 00:16:13,320
for Earth program, they're investing about 50 million over the next five years into applying AI

167
00:16:13,320 --> 00:16:18,200
for Earth conservation. We just signed to deal with them and they are our biggest supporters at

168
00:16:18,200 --> 00:16:23,480
the moment. And we're excited about working with them and Azure cognitive services and a growing

169
00:16:23,480 --> 00:16:30,040
relationship with them as well. What a great story. What a great story. You mentioned in there

170
00:16:30,040 --> 00:16:38,200
that computer vision started to affect this shift in the I guess the academic culture almost

171
00:16:38,920 --> 00:16:45,240
within this community to make researchers more collaborative. Can you elaborate on that a little

172
00:16:45,240 --> 00:16:53,080
bit? Sure. So for some animals that don't move very far, it's it's very cost effective for

173
00:16:53,080 --> 00:16:57,640
a researcher to go into the field regularly to observe them, especially if they're located

174
00:16:57,640 --> 00:17:03,560
near the animal and to collect as much data as possible about a population. These in these

175
00:17:03,560 --> 00:17:09,960
researchers can almost do a complete census. Many animals are not like that. They are migratory,

176
00:17:10,520 --> 00:17:15,000
especially in the marine environment. They can dive deep and have many unobservable states.

177
00:17:15,000 --> 00:17:20,120
And this is where mark or capture modeling showing up at a field site observing who's there and

178
00:17:20,120 --> 00:17:25,880
who isn't repeatedly comes into play. But in the marine environment or in remote environments,

179
00:17:25,880 --> 00:17:30,600
it's just not cost effective to have a researcher co-located there all the time. Sometimes the

180
00:17:30,600 --> 00:17:35,880
animals have migrated away. Sometimes they've simply moved to an area that's remote and difficult

181
00:17:35,880 --> 00:17:42,040
to get to. But all of that is extremely expensive both to support the researcher and to just keep

182
00:17:42,040 --> 00:17:48,680
them in the field. With with marker capture, the idea is you show up and sub sample the population

183
00:17:48,680 --> 00:17:54,440
at certain times. And really a lot of the marker capture models that are used to estimate these

184
00:17:54,440 --> 00:17:58,840
critically endangered and endangered populations are sort of based around human limitations.

185
00:17:58,840 --> 00:18:03,720
There's this assumption inside the models as you as you model capture probability as you mount

186
00:18:03,720 --> 00:18:09,240
model survival rates to ultimately arrive at abundance or population trajectory that the amount

187
00:18:09,240 --> 00:18:13,720
of time that is passed between your sampling sessions far outweighs the size of your sampling

188
00:18:13,720 --> 00:18:19,400
sessions. So they're not meant for continuous monitoring. And also research collaborations

189
00:18:19,400 --> 00:18:27,720
themselves and the competitiveness in academia really create these silos inside what could be

190
00:18:27,720 --> 00:18:33,080
thought of as a master data set. You know, a zebra does not know where the geographic border is

191
00:18:33,080 --> 00:18:38,440
where one research team begins and another ends saying for a whale shark or a humpback whale is

192
00:18:38,440 --> 00:18:45,160
they migrate. We're sub sampling data based on really human limitations of funding and time

193
00:18:45,160 --> 00:18:50,600
and competitiveness rather than sort of getting the superset of data, which is what are these animals

194
00:18:50,600 --> 00:18:56,040
doing all the time? Where are they? When are they there? Who are they interacting with? You know,

195
00:18:56,040 --> 00:19:02,120
what behaviors are they undertaking in these locations? So the idea is how do we how do we push out

196
00:19:02,120 --> 00:19:07,800
this human limited data collection into something much more scalable? And that's where citizen

197
00:19:07,800 --> 00:19:15,000
science comes in. You know, can we get a minimum high quality volume of data from citizen scientists

198
00:19:15,000 --> 00:19:19,560
or a minimum quality of good data from citizen scientists from the public from people on whale

199
00:19:19,560 --> 00:19:27,640
watching boats from people doing safaris in Kenya? Can their photos be used as data? And that

200
00:19:27,640 --> 00:19:32,200
amplifies the data collection of a researcher. Now we start at that point also getting into things

201
00:19:32,200 --> 00:19:39,480
like bias, you know, inconsistent effort in the data collection and the sampling. But importantly,

202
00:19:39,480 --> 00:19:44,440
the largest volume of data that a researcher can get is visual at this point. Everyone's out now

203
00:19:44,440 --> 00:19:50,040
in this modern world where we all have cameras, multiple cameras attached to us, especially when we

204
00:19:50,040 --> 00:19:56,280
go traveling. That volume of data is sort of yet unharnessed and especially for animals that are

205
00:19:56,280 --> 00:20:02,680
individually identifiable. Getting those photos is really getting a data point for researchers.

206
00:20:02,680 --> 00:20:08,680
The problem is is if that if you're still in access and excel on a desktop and have your photos

207
00:20:08,680 --> 00:20:15,240
sort of sitting in folder drives, the flood of data that you can get is far outpaces the ability

208
00:20:15,240 --> 00:20:21,160
of a research team to curate it. Let me give you an example. We work with Sarah Soda Dolphin

209
00:20:21,160 --> 00:20:26,600
Research Project and they identify individual dolphins, bottled nose dolphins by taking pictures

210
00:20:26,600 --> 00:20:32,680
of their dorsal fin as it pops up out of the surface. So when they have a huge collection of dolphins,

211
00:20:32,680 --> 00:20:37,480
thousands of individuals, when they get that N plus one photo, that new photo and they decide,

212
00:20:37,480 --> 00:20:44,680
okay, what, which dolphin is this? It takes them approximately nine hours to match that one photo.

213
00:20:44,680 --> 00:20:51,480
Similarly, if you go to Cascadia Research Collective in Olympia, Washington and they maintain this

214
00:20:51,480 --> 00:20:57,880
master set of humpback whale imagery, if you stand in the middle of their offices and you look

215
00:20:57,880 --> 00:21:04,920
around you in 360 degree view, you see interns looking at images side by side. And so there's a lot

216
00:21:04,920 --> 00:21:10,760
of inefficiency in manually reviewing all this visual data that the world can provide and that

217
00:21:10,760 --> 00:21:15,800
as a sort of long-winded answer to your question is why computer vision is just so critical.

218
00:21:15,800 --> 00:21:21,560
It is a literal time and cost savings to these researchers whose funding just doesn't scale very well.

219
00:21:22,440 --> 00:21:30,680
I guess I'm thinking of the way that computer vision is kind of evolved in this application since

220
00:21:30,680 --> 00:21:38,200
your initial models, clearly deep learning and convolutional neural nets. Well, I'm imagining

221
00:21:38,200 --> 00:21:44,440
or playing a huge role here as they are in other places. I'm wondering particularly if there are

222
00:21:44,440 --> 00:21:52,280
any unique characteristics of this type of data that change what you can or what you can't do or

223
00:21:52,280 --> 00:21:59,000
the way the types of models that you use to identify these specimens. Sure. And you're absolutely

224
00:21:59,000 --> 00:22:04,760
right. The convolutional neural networks are changing this and that's why our partnership with

225
00:22:04,760 --> 00:22:11,480
Rensselord Polytechnic is so important. The crude algorithm we still use for whale sharks has

226
00:22:11,480 --> 00:22:16,120
been replaced for other species. For humpback whales, we use deep convolutional neural networks

227
00:22:16,840 --> 00:22:23,560
to identify humpback flukes that the backside, the underside of their tail, if you will, in two

228
00:22:23,560 --> 00:22:31,640
different ways. These tools are also proving much more scalable, but let me take a step back here.

229
00:22:31,640 --> 00:22:37,320
A lot of times the challenges that I read about in the literature in computer vision are about

230
00:22:37,320 --> 00:22:44,680
categorization. Maybe identify the species. Maybe there's tens of thousands. Or more

231
00:22:44,680 --> 00:22:49,400
ideally, you're looking at categorizing things. It might be a binary, you know, is this a failed part

232
00:22:49,400 --> 00:22:54,680
or a past part? It might be, you know, put these images into 10 different buckets. The interesting

233
00:22:54,680 --> 00:23:01,240
thing about computer vision for wildlife, especially for identifying individuals, is that

234
00:23:01,240 --> 00:23:05,880
by nature, we're searching for an individual, we're searching for a needle in a haystack.

235
00:23:05,880 --> 00:23:10,360
Let me give you an example of what the problem. So my first attempt at humpback whales and doing

236
00:23:10,360 --> 00:23:17,240
a computer vision system, I sat down and started playing with a fairly basic computer vision algorithm.

237
00:23:17,240 --> 00:23:24,760
And my first competent pass, I was 99.8% successful in categorizing my photos. And I thought,

238
00:23:24,760 --> 00:23:28,920
wait a second, I must be a genius. I must be good at this. This is my calling in life.

239
00:23:28,920 --> 00:23:37,240
And then I looked at what the algorithm had produced. And I had created a machine learning agent

240
00:23:37,240 --> 00:23:42,440
that predicted false 100% of the time. Because when you have, yeah, it cheated. And you know,

241
00:23:42,440 --> 00:23:47,400
what? It was accurate 99.8% of the time. Because when you have one photograph and you're looking

242
00:23:47,400 --> 00:23:53,560
through 50,000 photographs for an individual you may have only seen once before, your probability

243
00:23:53,560 --> 00:24:00,760
that it's false is almost always 100%. It's so close to 100%. So, you know, applying computer

244
00:24:00,760 --> 00:24:07,240
vision to identify individual animals in a field of many thousands of humpback whales, over 9,000,

245
00:24:07,240 --> 00:24:13,160
you know, among 50, 60,000 photographs, it's a needle in a haystack problem. And that's why

246
00:24:13,160 --> 00:24:19,160
deep learning is so incredibly important here. I'll give you an example of some of the research

247
00:24:19,160 --> 00:24:25,240
that's coming out of RPI that we're adapting. My team is adapting a wild me. There's a PhD candidate

248
00:24:25,240 --> 00:24:30,120
Hendrick Weedeman at Rensselaer Polytechnic. He's creating an algorithm called curve rank.

249
00:24:30,120 --> 00:24:36,280
And sort of tying in with that Sarasota dolphin research project. The idea of curve rank is that

250
00:24:36,280 --> 00:24:43,480
that the edge of a dolphin fin is individually identified. Now, previous attempts for edge

251
00:24:43,480 --> 00:24:48,920
identification have absolutely occurred. They've used, you know, for example, dynamic time warping as

252
00:24:48,920 --> 00:24:55,400
a basic, you know, point pattern matching or line matching algorithm. The problem is is that

253
00:24:55,400 --> 00:25:01,480
not every bit of that fin is actually indicative of individually identifiable information. In fact,

254
00:25:01,480 --> 00:25:06,520
most of it isn't. Most of it's fairly flat and generic. So, what's important about Hendrick's work

255
00:25:06,520 --> 00:25:12,920
is given this catalog of multiple photos per marked individual in the dolphin population

256
00:25:12,920 --> 00:25:18,120
that the Sarasota dolphin research project provided. He learns which section of the fin

257
00:25:18,120 --> 00:25:23,960
contains that individually identifiable information. And that then allows us to only compare those

258
00:25:23,960 --> 00:25:29,800
small sections of the fin to give us the ability to, you know, go through that haystack and find

259
00:25:29,800 --> 00:25:36,840
that needle. I guess that there have been research efforts around what's called fine grain

260
00:25:36,840 --> 00:25:42,600
classification problems. Are you familiar with that work and does that apply? Here is it a

261
00:25:42,600 --> 00:25:49,800
different formulation of the problem when you're trying to identify the individual. Do you even

262
00:25:49,800 --> 00:25:56,120
think of it as a classification problem or something different? And this may be that I'm somewhat

263
00:25:56,120 --> 00:26:00,520
of an outsider coming into the machine learning community. I generally don't think of it as a

264
00:26:00,520 --> 00:26:06,840
classification problem. I do have intelligent agent classification problems that I'd like to go

265
00:26:06,840 --> 00:26:13,000
into, but in terms of the individual identification, I generally don't think of it as a classification

266
00:26:13,000 --> 00:26:18,520
problem. Okay. And I don't think we, you know, for each individual, I don't think we train up an

267
00:26:18,520 --> 00:26:27,000
individual classifier, if you will, although I'm aware of others. Let's see. There's Ben Hughes

268
00:26:27,000 --> 00:26:31,720
out of the UK. He's been working on white shark fin identification, and he absolutely builds a

269
00:26:31,720 --> 00:26:36,760
classifier per marked individual. So I believe that is more of a classification, traditional classification

270
00:26:36,760 --> 00:26:45,000
problem. Okay. So you alluded to some new work that you're doing to build intelligent agents around

271
00:26:45,000 --> 00:26:51,000
Wild Book. Can you talk a little bit about that? Sure. So I'm building what is I believe a combination

272
00:26:51,000 --> 00:26:59,320
of a model based reflex agent and a learning agent to data mine YouTube for wildlife sightings that

273
00:26:59,320 --> 00:27:05,480
get missed. And so there's a short story that I'll get into here that describes why I'm going

274
00:27:05,480 --> 00:27:10,120
about that. And this is my individual work at Wild Me when being executive director gives me a

275
00:27:10,120 --> 00:27:16,840
little bit of breathing room. This is what I like to work on. So flashback to 2014 and 2015,

276
00:27:16,840 --> 00:27:22,520
I'm running whale shark dot org, published a couple of papers on marker capture populations

277
00:27:22,520 --> 00:27:27,640
off Mingulu reef in Western Australia. I've just had children and I'm most of the time being

278
00:27:27,640 --> 00:27:33,960
a good parent sitting at home in front and Saturday nights, feeling like my contribution to wildlife

279
00:27:33,960 --> 00:27:39,160
conservation is sort of falling away to just programming and maintenance. So I really felt like,

280
00:27:40,040 --> 00:27:43,320
I can't be in the field right now. My family needs me. I can't be scuba diving, but

281
00:27:44,680 --> 00:27:49,480
isn't there some data that I could collect? Something that other people generally don't have access

282
00:27:49,480 --> 00:27:55,960
to other researchers in the community. So I started while watching TV just surfing YouTube for

283
00:27:55,960 --> 00:28:02,040
whale shark videos and started downloading them, capturing key frames, mapping the spots on the

284
00:28:02,040 --> 00:28:07,400
whale shark, submitting it in a wild book, running identification. And it turns out that in 2014

285
00:28:07,400 --> 00:28:13,880
in 2015, just doing that, I collected more data than any individual researcher in the field and

286
00:28:13,880 --> 00:28:18,520
identified more marked individuals. And that really got me thinking, okay, that's great. Except

287
00:28:18,520 --> 00:28:23,240
I don't want to keep doing this. This is actually kind of boring, but I'd like to automate this.

288
00:28:23,240 --> 00:28:29,320
And so piece by piece, as this wild book collaboration evolved, and as Rensseler Polytechnic

289
00:28:29,320 --> 00:28:36,760
trained up a convolutional neural network to detect whale sharks in imagery, then I was able to

290
00:28:36,760 --> 00:28:40,840
bring that in and say, okay, if I download all the videos, can you tell me, are there clusters of

291
00:28:40,840 --> 00:28:45,240
key frames with a high enough confidence that we can say there's a whale shark in it? And then as I

292
00:28:45,240 --> 00:28:49,160
started canvassing the literature for other things, like, you know, how would I determine based on

293
00:28:49,160 --> 00:28:54,280
freeform text in a YouTube video when this video occurred and where it occurred? I started seeing

294
00:28:54,280 --> 00:28:59,960
the different machine learning components that were needed, both computer vision and in the natural

295
00:28:59,960 --> 00:29:05,640
language processing sphere, that would be needed to replicate my actions. Essentially, I was trying

296
00:29:05,640 --> 00:29:09,960
to put myself out of a job with machine learning. Now, it wasn't a paying job. It was just a volunteer

297
00:29:09,960 --> 00:29:15,800
gig, but the idea is, you know, is potentially this the new way of working, of building yourself

298
00:29:15,800 --> 00:29:21,880
a set of intelligent agents that do work for you as part of your job. And in this case,

299
00:29:22,680 --> 00:29:27,320
if we considered my job trying to find whale shark data, then we've absolutely built that.

300
00:29:27,320 --> 00:29:34,120
And the way the system works is every night at 10 p.m. on our server, a little agent wakes up,

301
00:29:34,120 --> 00:29:41,000
and it asks YouTube, you know, tell me everything that's been, or give me a list of videos that have

302
00:29:41,000 --> 00:29:45,560
been uploaded in the past 24 hours that are tagged or titled or described with the word whale shark,

303
00:29:45,560 --> 00:29:50,280
or in Spanish, Tiber on Bayana. And we want to expand this to other languages. We then get a list

304
00:29:50,280 --> 00:29:56,600
from YouTube back of, you know, in JSON, of the titles, the tags, and the descriptions. And

305
00:29:56,600 --> 00:30:04,360
based on how I historically curated that data from 2014 and 2015, I'm serving as the critic

306
00:30:04,360 --> 00:30:11,240
in the learning-based model, it then will make a prediction on true or false. Does this,

307
00:30:11,240 --> 00:30:16,440
the based on the way the video is described, does it contain a wildlife sighting? And that's

308
00:30:16,440 --> 00:30:22,040
important because it's really acting as a very powerful filter. And I retrain it periodically,

309
00:30:22,040 --> 00:30:28,280
so as I make decisions, ultimate decisions on what it collects, it then learns on what I said,

310
00:30:28,280 --> 00:30:32,200
yeah, that was right, or know that was wrong, and read is able to make better predictions.

311
00:30:32,200 --> 00:30:36,200
So it first makes this true false, is this a wildlife sighting whale shark. And that's

312
00:30:36,200 --> 00:30:41,320
important because the word whale shark will show up in grand theft auto videos and all kinds of

313
00:30:41,960 --> 00:30:46,280
random places you wouldn't expect, right? And also, you know, just things like the Georgia Aquarium

314
00:30:46,280 --> 00:30:51,320
as a whale shark. So it sounds like though that you had the foresight to collect all of that

315
00:30:51,320 --> 00:30:57,640
descriptive information and your ultimate decision when you first started classifying these

316
00:30:57,640 --> 00:31:02,120
these videos. Absolutely. And it's made me really think about, you know, what are the other things

317
00:31:02,120 --> 00:31:07,960
in my job that I could start logging now, what decisions I made that all eventually I could train

318
00:31:07,960 --> 00:31:13,160
machine learning on. And because making a prediction, you know, it's a very human thing that

319
00:31:13,160 --> 00:31:18,680
really cross applies to machine learning quite well. So this agent makes that initial prediction.

320
00:31:18,680 --> 00:31:23,240
And it says, all right, based on how this is tag titled and described, I think this is a whale

321
00:31:23,240 --> 00:31:29,080
shark sighting in the wild. And then it goes on to the next step, which is it will then download

322
00:31:29,080 --> 00:31:34,840
the video, sample the video every two seconds, and then take those key frames and look for high

323
00:31:34,840 --> 00:31:42,360
confidence clusters using a trained convolutional neural network. And we fed data from whale shark.org

324
00:31:42,360 --> 00:31:47,160
in. We did a mechanical Turk process to find the whale shark in the imagery, trained up a detector,

325
00:31:47,160 --> 00:31:52,760
and now that detector gives us confidence scores. And if across the video, at least one of the

326
00:31:52,760 --> 00:31:59,880
frames rises above a confidence threshold, we then go on. If not, then we leave. And the interaction

327
00:31:59,880 --> 00:32:05,160
of the predictor and the computer vision piece is important because, for example, in the vision

328
00:32:05,160 --> 00:32:09,480
piece, there's a whale shark somewhere in the world painted on the side of a Chinese airplane.

329
00:32:09,480 --> 00:32:13,640
And that video periodically shows up. So we need the vision to find whale sharks, but then we need

330
00:32:13,640 --> 00:32:19,320
the oftentimes that predictor to throw it out and say, okay, no, no, no, we're not describing,

331
00:32:19,320 --> 00:32:23,000
you know, a whale shark off the Philippines. We're talking about an airplane, so ignore that. So

332
00:32:23,000 --> 00:32:27,800
those two interplay quite well and serve as a very strong filter. At this point, once we've

333
00:32:27,800 --> 00:32:33,800
gotten past the predictor and computer vision, we're on to thinking, okay, this is probably a whale

334
00:32:33,800 --> 00:32:40,280
shark in a YouTube video. And it's at this point that I will take the title tags in description and

335
00:32:40,280 --> 00:32:46,920
send them up to Azure Cognitive Services and say, all right, do language detection, pretty common

336
00:32:46,920 --> 00:32:52,760
natural language processing task. If any of them are not in English, I will then re-ask Cognitive

337
00:32:52,760 --> 00:32:57,640
Services to use neural machine translation, translate back to English. And then we'll take that

338
00:32:57,640 --> 00:33:03,800
master string that's, I'll concatenate all of it. And then I will run a Stanford package called

339
00:33:03,800 --> 00:33:10,840
SU Time, which uses named entity recognition to tell me when this occurred. And importantly,

340
00:33:10,840 --> 00:33:15,880
I feed in the date that the video was posted, and that contextualizes the description. So if

341
00:33:15,880 --> 00:33:20,280
somebody says, you know, I saw this whale shark last week, or if they gave a formal date,

342
00:33:20,280 --> 00:33:28,040
2018, dash 06, dash 24, what have you? It's able to resolve that and give me back an ISO 8601 date.

343
00:33:28,040 --> 00:33:32,760
Now I know when. So I've got visual confirmation of a whale shark or high confidence visual

344
00:33:32,760 --> 00:33:37,960
confirmation of a whale shark. I know when based on using natural language processing. And then I do

345
00:33:37,960 --> 00:33:44,120
some just simple string mining data mining to figure out the where. Eventually, I want to train

346
00:33:44,120 --> 00:33:47,560
up machine learning to also detect where. But right now, we just look for keywords.

347
00:33:48,440 --> 00:33:54,040
On that, when part to what degree is that a probabilistic determination? If you've got multiple

348
00:33:54,040 --> 00:34:00,040
possible dates, or if it's, you know, about a week and a half ago, does it just, does it pick

349
00:34:00,040 --> 00:34:05,400
something? Or do you get some weighted set of dates? I'm wondering, I guess, where in the

350
00:34:06,200 --> 00:34:11,080
in the process does the final determination of a candidate date happen?

351
00:34:11,080 --> 00:34:17,960
That's a really good point. And definitely an area for improvement. So what we get back out of

352
00:34:17,960 --> 00:34:24,520
SU time is a ray of, is an array of all of the dates. And then that are present. And it's not

353
00:34:24,520 --> 00:34:29,560
probabilistic. It's anything that it, it determines it finds. Now, there may be some probability threshold

354
00:34:29,560 --> 00:34:34,600
built in internally that I'm not aware of. But it will, it will simply, simply give me a list

355
00:34:34,600 --> 00:34:41,080
back of everything it finds. I will then, because it's giving me a standardized format, I will then

356
00:34:41,080 --> 00:34:47,960
do my best to take the most detailed date. So it might find last year and say, okay, 2017,

357
00:34:47,960 --> 00:34:54,600
it might find, you know, last month. And then it might actually find June 25th, 2008.

358
00:34:54,600 --> 00:35:00,440
I will go for the date that contains the most amount of information, including starting with year

359
00:35:00,440 --> 00:35:07,960
then month, then day. But learning which date should be used is actually something that we could

360
00:35:07,960 --> 00:35:14,520
train machine learning on. Absolutely. And give a probabilistic ranking on that. So it's a great

361
00:35:14,520 --> 00:35:20,440
really great point. We don't do that. But importantly, if we don't have the where or the when

362
00:35:20,440 --> 00:35:26,200
determined, we actually will have the agent post back to YouTube, again, trying to post back

363
00:35:26,200 --> 00:35:29,960
in the, the poster's original language. So we might engage in neural machine translation again

364
00:35:29,960 --> 00:35:35,240
if we detected Spanish, ask them the question, when did you see this well chart or where or where

365
00:35:35,240 --> 00:35:41,480
and when? Meaning in the comments? Yeah. That's pretty cool. It is. And then we just added recently

366
00:35:41,480 --> 00:35:46,680
the listener that will then asynchronously just pop in occasionally and look for the response.

367
00:35:46,680 --> 00:35:51,320
So it's going to look through all the YouTube videos that we have curated, look for those missing

368
00:35:51,320 --> 00:35:58,040
date and location, go out and then look for response, feeding the replies to our comments back

369
00:35:58,040 --> 00:36:03,560
through the machine learning to do location and date detection. And then ultimately in this vein

370
00:36:03,560 --> 00:36:09,080
of that we like to call rewarding the gift of data for the gift of knowledge, we win a researcher

371
00:36:09,080 --> 00:36:14,840
finally approves and says, okay, this is whale shark A001 stumpy often in glue reef. When we make

372
00:36:14,840 --> 00:36:21,240
that ID of the YouTube derived data, we post back the link to the wild book that says, did you know

373
00:36:21,720 --> 00:36:27,080
that we found whale shark A001 in your video? And here's a link to everything we know about it.

374
00:36:27,080 --> 00:36:31,880
And we have to our questions, we have about a 45% response rate, which I think in the world of

375
00:36:31,880 --> 00:36:37,960
marketing is awesome. But when we ask questions, people do reply, the agent listens for the reply

376
00:36:37,960 --> 00:36:44,600
and then tries to inform them of the conclusions we've made based on their data. And the great

377
00:36:44,600 --> 00:36:49,160
thing about this is if we think about citizen science, public participation, there's an upper

378
00:36:49,160 --> 00:36:54,120
limit to the number of people who are participate that's related to the amount of outreach. How much

379
00:36:54,120 --> 00:36:57,640
have we gone out to the public and told them that they can even, you know, contribute data about

380
00:36:57,640 --> 00:37:02,520
zeroes or giraffes or whale sharks. And then there's a subset of those people who are informed that

381
00:37:02,520 --> 00:37:07,080
we'll choose to participate. And so we're always up against this. Well, not enough people know

382
00:37:07,080 --> 00:37:11,960
about this. How do we get the word out, et cetera? The interesting thing about running an intelligent

383
00:37:11,960 --> 00:37:16,840
agent this way is it flips that model. We go to where people are already choosing to participate.

384
00:37:16,840 --> 00:37:23,640
We show up randomly, curate their data under fair use and then inform them what we found.

385
00:37:23,640 --> 00:37:30,040
And all of that is automated. And so, you know, retraining that agent and proving its quality is

386
00:37:30,040 --> 00:37:35,320
always an ongoing mission. But now we're looking to cross apply that to other species. You know,

387
00:37:35,320 --> 00:37:38,600
how do we start doing this from homeback whales and giant mantas, et cetera?

388
00:37:39,480 --> 00:37:46,120
Have you also thought at all about other domains more broadly that you might be able to apply

389
00:37:46,120 --> 00:37:52,520
this approach, meaning beyond wildlife conservation? I've thought about data mining for marketing.

390
00:37:52,520 --> 00:37:57,480
I would be surprised if there wasn't a company already doing this whereby you could monitor

391
00:37:57,480 --> 00:38:03,400
social media using a complex interaction of machine learning and intelligent agent. And, you know,

392
00:38:03,400 --> 00:38:08,840
figure out who is talking about your product, who's having a bad time with it, who's having

393
00:38:08,840 --> 00:38:14,040
a good time with it, who are your influencers, et cetera. But, you know, the combination of computer

394
00:38:14,040 --> 00:38:18,520
vision and natural language processing to really find the product in the imagery is my product

395
00:38:18,520 --> 00:38:23,880
appearing. And then, you know, look for sentiment analysis. What are people saying? Is it generally

396
00:38:23,880 --> 00:38:29,080
good or bad? Look at the comments. Is are those good or bad? And then get a sense of, you know,

397
00:38:29,080 --> 00:38:35,480
how a product is doing in the marketplace. That struck me as a sort of for-profit cross application.

398
00:38:36,280 --> 00:38:42,280
This is really interesting. It just, when I think about the traditional AI, a big part of

399
00:38:42,280 --> 00:38:47,880
traditional AI, maybe in the 80s or something, was research around this whole idea of intelligent

400
00:38:47,880 --> 00:38:53,240
agents. And we'd all have these avatars that, you know, while we sleeper out doing our bidding,

401
00:38:53,240 --> 00:38:58,520
this is one of the best examples I've heard thus far of something along those lines. I mean,

402
00:38:58,520 --> 00:39:03,080
they're probably our others and maybe folks will, you know, write in and tell me about

403
00:39:03,080 --> 00:39:06,520
all the other great examples of this. But it seems like a really interesting

404
00:39:07,320 --> 00:39:14,200
approach to, as you say, curate content, but also engage with folks and, you know, peak

405
00:39:14,200 --> 00:39:22,120
their curiosity, allow them to contribute to this broad effort without, you know, them needed,

406
00:39:22,120 --> 00:39:28,600
needing actively seek it out. It's very cool. Yeah, and it's a little bit mind bending too.

407
00:39:28,600 --> 00:39:33,800
You know, I think about whale shark.org and how we have something like 152 researchers.

408
00:39:33,800 --> 00:39:39,400
And then this one inhuman agent participating in this, you know, global study of the world's

409
00:39:39,400 --> 00:39:45,800
biggest fish. You know, what does that mean in terms of having autonomous agents collect data

410
00:39:45,800 --> 00:39:50,760
and participate? One of the things I want to do is start having the agent not just participate or

411
00:39:50,760 --> 00:39:55,400
interact with the public, but interact with the researchers who are logging into wild book and

412
00:39:55,400 --> 00:40:00,840
curating data. The agent absolutely should show up and tell them it found new data from their study

413
00:40:00,840 --> 00:40:08,360
sites. And then, you know, what can it analyze? What can it inform them of as part of their experience?

414
00:40:08,360 --> 00:40:13,240
You know, we're pretty far from Jarvis and Iron Man, but, you know, we are at least a baby step

415
00:40:13,240 --> 00:40:20,280
in that direction. Right. Right. Very cool. One of the things that you mentioned in the process here

416
00:40:20,280 --> 00:40:27,160
is active learning. So kind of correcting this agent periodically and allowing it to update its

417
00:40:28,360 --> 00:40:34,440
its model, its view of the world. How have you built that into the process? Is it? Do you kind of

418
00:40:34,440 --> 00:40:38,280
just retrain periodically? Or do you have something more sophisticated happening than there?

419
00:40:39,000 --> 00:40:45,640
I retrain periodically. It's basically a dance I do with the agent. So the agent collects data.

420
00:40:45,640 --> 00:40:51,240
I will do the finishing curation on it. Basically deciding, did you get it right or not?

421
00:40:52,200 --> 00:40:59,400
And then that will be a part of the next training set. And in the early stages, you know,

422
00:40:59,400 --> 00:41:04,520
we're talking about thousands of data points. So I'm using a simple random forest algorithm

423
00:41:05,240 --> 00:41:12,520
using trigrams, not especially sophisticated. We're using Weka, the open source package in Java.

424
00:41:13,400 --> 00:41:18,600
But, you know, there's a lot of room for improvement there. And then especially as the agent,

425
00:41:19,560 --> 00:41:25,560
you know, as we get more data and get a little more mature in using this agent, I might

426
00:41:25,560 --> 00:41:31,320
allow it to periodically just retrain itself based on my curation. But then ultimately,

427
00:41:31,320 --> 00:41:35,800
I want to expand it to the other, you know, human researchers participating in the system and

428
00:41:35,800 --> 00:41:43,000
their decisions around the data that it collected. Did you choose Weka based on a preference

429
00:41:43,000 --> 00:41:51,080
for Java or was there some other driver? My background is through Java. I'm rapidly converting

430
00:41:51,080 --> 00:41:57,480
to Python. But, you know, as you've heard from my story, my route into machine learning is so

431
00:41:57,480 --> 00:42:04,920
non-traditional that Weka was a very accessible machine learning tool. It's, you know, the algorithms

432
00:42:04,920 --> 00:42:11,400
inside of it. And these are older techniques, obviously. And for newer techniques, I really value

433
00:42:11,400 --> 00:42:16,600
my staff member, Jason Parham, who is just finishing up his PhD, as well as, you know, our

434
00:42:16,600 --> 00:42:22,360
collaboration with Chuck Stewart, Hendrick Weedeman, over at Rensseler Polytechnic. They're pushing

435
00:42:22,360 --> 00:42:28,280
the boundaries of pattern recognition for wildlife. And then, you know, it's my professional

436
00:42:28,280 --> 00:42:36,360
engineering team's job to implement those paper-worthy algorithms as user-worthy software products,

437
00:42:36,360 --> 00:42:40,360
essentially. Wildbook is open source, but we very much think of it as a product because

438
00:42:40,360 --> 00:42:45,400
ultimately, these different wildlife biologists who are in the field coming back with photographs

439
00:42:45,400 --> 00:42:52,200
of their study populations need to use computer vision successfully. You know, one challenge we

440
00:42:52,200 --> 00:42:56,680
have right now, for example, is humpback whales. We actually have two computer vision algorithms

441
00:42:56,680 --> 00:43:03,080
that operate in the 75 to 80 percent success range, identifying the correct whale in the top one.

442
00:43:03,080 --> 00:43:07,960
But they have different failure modes. So, teaching a field biologist who has no computer vision

443
00:43:07,960 --> 00:43:13,880
experience, how to interpret two different computer vision ranking results, and potentially even

444
00:43:13,880 --> 00:43:19,960
boosting those or using an SVM to create a metascore that is interpretable to them is one of the

445
00:43:19,960 --> 00:43:24,440
usability challenges we still have ahead of us. Right now, we literally show two lists. You know,

446
00:43:24,440 --> 00:43:29,480
computer vision found this and the other computer vision algorithm curve rank found this,

447
00:43:29,480 --> 00:43:33,880
and it's up to them to sort of interpret that. How do we give them more confidence and reduce the

448
00:43:33,880 --> 00:43:39,560
amount of interpretation they have to do? And then, if you push it forward, I'm really excited about

449
00:43:39,560 --> 00:43:47,960
the marriage of statistics with computer vision. You know, a PhD now professor, former PhD candidate

450
00:43:47,960 --> 00:43:53,080
now professor at Eastern Kentucky University named Amanda Ellis, took some of our whale shark work

451
00:43:53,080 --> 00:43:58,520
and said, all right, a human aided by computer vision ultimately decided the IDs of these

452
00:43:58,520 --> 00:44:03,800
different whale sharks and these images. What if we took the cloud of photographs? We use the

453
00:44:03,800 --> 00:44:09,240
computer vision algorithm to create pairwise scores between every image in the cloud. And what if

454
00:44:09,240 --> 00:44:14,680
the whole concept of animal identity was not left to a human being, but it was left to the computer

455
00:44:14,680 --> 00:44:20,840
to find its pathway through the different pairwise relationships and create animal identity

456
00:44:20,840 --> 00:44:26,040
within this cloud of photographs and ultimately provide a population estimate with no human

457
00:44:26,040 --> 00:44:30,360
curation. And interestingly enough, some of the work that Chuck Stewart's lab has been doing at

458
00:44:30,360 --> 00:44:35,160
RPI is related to building the similar data constructs. So I'm trying to put these two groups

459
00:44:35,160 --> 00:44:40,920
together because if we have a cloud of photographs and all the pairwise computer vision relationships

460
00:44:40,920 --> 00:44:45,800
that have been mined among them and were able to pass that over to a statistician, what it means

461
00:44:45,800 --> 00:44:52,600
is we could go from years between population estimates for critically endangered species to weeks,

462
00:44:52,600 --> 00:44:58,280
which means that we can evaluate conservation strategies for these animals faster, right?

463
00:44:58,280 --> 00:45:03,000
Most population papers that come out for a particular population at a study site are generally,

464
00:45:03,000 --> 00:45:08,040
you know, five, six, ten years between population estimates. That's just too slow for these animals

465
00:45:08,040 --> 00:45:13,480
that are critically endangered and declining rapidly. With machine learning, we're able to rapidly

466
00:45:13,480 --> 00:45:19,160
reduce that. It's going to be a lot more research, a lot more development effort, but you know,

467
00:45:19,160 --> 00:45:24,520
it's just crazy to me how many applications for computer vision and machine learning there are

468
00:45:24,520 --> 00:45:30,360
in wildlife research and how much of that skill set is out there untapped for conservation.

469
00:45:30,360 --> 00:45:39,320
I'm curious. You've mentioned on several occasions citizen science as regards the engagement of,

470
00:45:39,320 --> 00:45:44,920
you know, ordinary folks taking pictures, posting them, or maybe labeling things through

471
00:45:45,720 --> 00:45:55,400
wild book. I'm wondering if you have, if you see a role for the analog among data scientists,

472
00:45:55,400 --> 00:46:00,600
you know, as data science and machine learning technologies become more accessible as the data

473
00:46:00,600 --> 00:46:07,560
science community grows and wants to engage in the areas around which they've, they have personal

474
00:46:07,560 --> 00:46:12,840
passions. Do you engage with those communities? Do you see roles for them to support projects

475
00:46:12,840 --> 00:46:19,640
like this? Oh, absolutely. I'm really fascinated about new data science techniques for managing and

476
00:46:19,640 --> 00:46:26,360
increasing volume of data for wildlife conservation and Tanya Burger Wolf, my collaborator at UIC

477
00:46:26,360 --> 00:46:33,480
is investigating this as well. You know, what are the biases in crowdsourced visual data?

478
00:46:33,480 --> 00:46:38,200
Can we create population estimates on that that are as accurate or better than traditionally

479
00:46:38,200 --> 00:46:45,160
collected research data? You know, what are, how big are some of these populations? How can somebody

480
00:46:45,160 --> 00:46:52,360
with data science skills help a researcher parse the statistics? How can they, can they help them

481
00:46:52,360 --> 00:46:57,400
curate their data in such a way that it's easy to get them into the population modeling packages?

482
00:46:57,400 --> 00:47:04,920
Can they come out with the next population model? I really feel like the next generation of

483
00:47:04,920 --> 00:47:12,120
conservation tools needs to be driven by data scientists and machine learning experts. It's the

484
00:47:12,120 --> 00:47:18,520
only way that wildlife conservation, which is still, you know, so unfortunately done on the desktop

485
00:47:18,520 --> 00:47:25,640
can leap 20 years ahead and catch up to some of the other fields that are using machine learning.

486
00:47:26,600 --> 00:47:30,440
It really needs to make that evolutionary leap and it's going to take data scientists,

487
00:47:30,440 --> 00:47:34,280
machine learning experts to help people leap 20 years ahead.

488
00:47:34,280 --> 00:47:39,240
And so if someone who's listening is interested in this area and like to get involved,

489
00:47:39,240 --> 00:47:44,840
how would you recommend they get started? So, you know, the first place to look is locally.

490
00:47:44,840 --> 00:47:49,720
What are some of the wildlife conservation efforts going on locally? And is there a local

491
00:47:49,720 --> 00:47:55,480
researcher you can reach out to? I have had, and no others who have had the same experience,

492
00:47:55,480 --> 00:48:00,920
which is wildlife conservationists, biologists actually tend to really appreciate

493
00:48:01,720 --> 00:48:06,440
IT and data experience. I've gotten nothing but warm welcomes when I've reached out to them

494
00:48:06,440 --> 00:48:10,200
and said, hey, can I help with your data? Can we try to solve a problem for you?

495
00:48:10,920 --> 00:48:15,320
And so, you know, look locally for that. Otherwise, you know, look at Wild Book,

496
00:48:15,320 --> 00:48:20,360
we are an open source package and would love for contribution. And we can absolutely put

497
00:48:20,360 --> 00:48:25,720
you in touch with wildlife biologists who we might not currently have time to do work with,

498
00:48:25,720 --> 00:48:31,160
but who could use Wild Book and might need a new machine learning algorithm for their data.

499
00:48:31,160 --> 00:48:37,560
So, we'd absolutely love to talk to you. Awesome. Awesome. Well, we'll link to Wild Book and to

500
00:48:38,600 --> 00:48:44,840
Wild Me on the show notes page before we wrap up. Is there anything else that you'd like to share?

501
00:48:45,640 --> 00:48:51,160
No, thanks for the opportunity to talk about the Wild Book project, our mission at Wild Me,

502
00:48:51,160 --> 00:48:56,600
and I really encourage your listeners to apply their skills to help with wildlife conservation.

503
00:48:56,600 --> 00:49:02,520
We really need machine learning and data science skills to help out. Fantastic. Thanks so much,

504
00:49:02,520 --> 00:49:12,040
Jason. Thanks, Jim. All right, everyone. That's our show for today. For more information on Jason

505
00:49:12,040 --> 00:49:19,080
or any of the topics covered in this episode, head over to twimlai.com slash talk slash 166.

506
00:49:19,080 --> 00:49:26,760
Don't forget to visit twimlai.com slash nominate to cast your vote for us in the People's Choice

507
00:49:26,760 --> 00:49:56,600
Podcast Awards. And as always, thanks so much for listening and catch you next time.

