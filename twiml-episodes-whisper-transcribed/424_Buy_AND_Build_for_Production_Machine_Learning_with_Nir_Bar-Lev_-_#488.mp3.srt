1
00:00:00,000 --> 00:00:20,400
All right, everyone, I am here with near BarLev near is the CEO and a co-founder of ClearML near welcome to the Tumul AI podcast.

2
00:00:20,400 --> 00:00:50,400
Hey, Sam. Thank you so much. Really happy to be here. Hey, I'm super excited for our chat. We've known each other for a bit now. I think we first met at like a GTC event or exactly the last GTC that was not online, I think. Yeah. Yeah. And we've had some really interesting conversations about the MLOPS space since then. And so I'm looking forward to chatting with you here. Before we do that, I'd love to give our audience a big

3
00:00:50,400 --> 00:01:18,400
bit of an opportunity to learn about you and your background and how you came to co-found ClearML. Yeah, definitely. So started actually one of the IDF Elite Technology Units and worked my way through there, became an engineer, worked as an engineer for about eight, nine years in financial systems, ERP systems, those kinds of systems, mostly larger companies.

4
00:01:18,400 --> 00:01:34,400
Then actually made a transition to more of the product business roles, business development, product development sales went on to get an MBA award and then I joined Google.

5
00:01:34,400 --> 00:01:54,400
And slightly after Google had acquired Android, and so that was a super cool time. It was one of the founding members of the mobile team at Google worked on the strategy with Andy Rubin, who's obviously, you know, an amazing guy built Google's voice recognition platform, which is kind of interesting.

6
00:01:54,400 --> 00:02:16,400
That, you know, all technology and now it's completely revamped based on AI. And then moved to Israel to help build Google R&D Center, which is about a 1600 person strong R&D Center, one of the biggest for Google in the world outside of the US.

7
00:02:16,400 --> 00:02:31,400
And spent my time mostly roles in Europe and the least in Africa, led Google Analytics in Europe, led search advertising in Europe, which was a business of about $10 billion annually at the time.

8
00:02:31,400 --> 00:02:41,400
And then ended up as a general manager for mobile payments for cash-based economies. That was my last role Google.

9
00:02:41,400 --> 00:02:46,400
And then, you know, looked on to what to do next. That was about actually was a decade ago.

10
00:02:46,400 --> 00:03:02,400
And at the time I was at a leadership, get away with the founders of DeepMind, which Google had acquired about a year prior to that, and had some really interesting discussions with the founders there.

11
00:03:02,400 --> 00:03:23,400
And, you know, about the potential AI and about the challenges. And I don't know if you recall there was a really interesting blog post that Google had came up with at around that time about how they improved electrical consumption in their data center.

12
00:03:23,400 --> 00:03:49,400
Did you send it about 40% or so? Exactly. And they did that with AI. Now, you know, that was, you know, that got a lot of attention. But what wasn't known outside was that building that initial model took three weeks, building a prototype to check it out, just validated in one data center, not a working product, but a prototype took three months, pulling it out as a product over a year.

13
00:03:49,400 --> 00:04:03,400
Wow. And that's exactly, you know, that was Google. And obviously, you know, that really set the stage for, you know, AI, you may have the best data, you know, data scientists in the world and building the model is, you know, could have amazing results.

14
00:04:03,400 --> 00:04:13,400
But if you don't know how to actually put it into production, you know, it's not going to provide any value. And really that's Emma Lops, right. And then I didn't, it wasn't called that way in the past.

15
00:04:13,400 --> 00:04:21,400
It has been for about a year now, right. And I felt like, you know, this is, you know, this is something I want to get into.

16
00:04:21,400 --> 00:04:38,400
And so I joined two of two partners, my co funders, amazing, amazing technologists. One of them was really one of the first people who started deep learning and computer vision in the academia in Israel.

17
00:04:38,400 --> 00:04:46,400
And we started on a journey. We didn't call it any lots at the time. But that's really for some sort of weird thing.

18
00:04:46,400 --> 00:04:53,400
Yeah. Yeah. Wow. There's a ton in there.

19
00:04:53,400 --> 00:05:03,400
Do you remember the year I'm trying to keep myself from going down the Andy Rubin, I don't think that's what we're here to talk about, but he's quite a controversial figure.

20
00:05:03,400 --> 00:05:10,400
It's the year for what, for that blog post, the year for the blog post.

21
00:05:10,400 --> 00:05:15,400
I'd say circa 2015.

22
00:05:15,400 --> 00:05:19,400
And maybe a year earlier.

23
00:05:19,400 --> 00:05:20,400
Yeah.

24
00:05:20,400 --> 00:05:28,400
Would you, would you guess that that timeline? Yeah, I think we're all working on that timeline in the ML ops.

25
00:05:28,400 --> 00:05:36,400
But, you know, Ram, I'm wondering if you'd venture against us to what it would look like at Google today.

26
00:05:36,400 --> 00:05:45,400
So, you know, I have a bunch of friends, obviously, Google, you know, it looks very different. I mean, obviously, Google is one of the world leaders in the space.

27
00:05:45,400 --> 00:05:55,400
And I think that Google was uniquely positioned to understand both, you know, the power and opportunity inherent in AI.

28
00:05:55,400 --> 00:06:03,400
Also, the challenges that would, you know, would be needed to surpass to be able to put it in production.

29
00:06:03,400 --> 00:06:10,400
And they also had the capabilities, right? They had data scientists and they had engineers.

30
00:06:10,400 --> 00:06:22,400
And they call it Google Brain, which was the team there and they had a lot of a lot of manpower and a lot of brain power going at this for many, many directions.

31
00:06:22,400 --> 00:06:27,400
And they had the, you know, frankly, the resources to build this.

32
00:06:27,400 --> 00:06:31,400
And probably makes sense if you think about the skill that they're working at.

33
00:06:31,400 --> 00:06:43,400
But for 99.9% of the other companies, I'd venture to say it's not, you know, it's not within their core competency to build the scaffolding for AI.

34
00:06:43,400 --> 00:06:56,400
A lot of them, you probably want to have data scientists because, you know, if you want to be competitive in virtually every industry today, you have to integrate AI into your business.

35
00:06:56,400 --> 00:07:00,400
And so, you know, that you don't want to outsource that.

36
00:07:00,400 --> 00:07:12,400
But building ML ops, that requires a lot of expertise, specifically on, you know, that, you know, that frontier between the science and the engineering.

37
00:07:12,400 --> 00:07:15,400
And, you know, being able to take the science and make it engineering.

38
00:07:15,400 --> 00:07:17,400
And that's what we're doing.

39
00:07:17,400 --> 00:07:30,400
Yeah. Yeah. I think what I was looking for in that answer was, you know, kind of seeing Google as like the,

40
00:07:30,400 --> 00:07:44,400
you know, Google's going to be the best that, you know, Google's going to do this as good as anybody else. And so, if, you know, they're able to cut the process down, you know, 90%.

41
00:07:44,400 --> 00:07:53,400
That's probably the best, you know, that kind of gives us some asymptotic bounds or how good, how, how good good ML ops allows you to be.

42
00:07:53,400 --> 00:08:03,400
That's not only impacted them 40%. Then that says that there are fundamental challenges in the ML lifecycle that ML ops.

43
00:08:03,400 --> 00:08:07,400
You know, isn't able to facilitate.

44
00:08:07,400 --> 00:08:19,400
And, you know, we've got lots of anecdotal figures. I'm sure you do as well from your customer engagement. So I was curious if you had, you know, from Google.

45
00:08:19,400 --> 00:08:30,400
Yeah. Yeah. Unfortunately, I don't have those numbers from Google in a while. And that's, you know, that's kind of like a sensitive numbers to share with an old friend.

46
00:08:30,400 --> 00:08:32,400
Yeah. Absolutely.

47
00:08:32,400 --> 00:08:33,400
I don't have.

48
00:08:33,400 --> 00:08:42,400
Well, you know, that's probably a good segue for us to dig deeper into the numbers and experiences that you do have and can share.

49
00:08:42,400 --> 00:09:01,400
Maybe good context for that is for you to share a bit about what Claire Mell is up to and kind of how you approach helping folks with ML ops and some of the ways that you differentiate different your cells from other folks.

50
00:09:01,400 --> 00:09:04,400
Right.

51
00:09:04,400 --> 00:09:15,400
Well, when we think of ML ops, you know, there's a lot of things that go underneath, right. And, you know, the way we think about it.

52
00:09:15,400 --> 00:09:17,400
And again, it's not.

53
00:09:17,400 --> 00:09:25,400
I mean, there may be other small pieces, but generally speaking, you've got, you know, everything that has to do with getting the data into your system.

54
00:09:25,400 --> 00:09:49,400
And then you have what we call experiment management or experiment tracking and logging, which is, you know, how we think about the day to day data scientists, how they manage their training experiments so they can, you know, compare log experiments for reproducibility track results and be able to, I mean, that's basically their development environment.

55
00:09:49,400 --> 00:10:03,400
Then you have everything around orchestration because, you know, literally from day one, if you're developing something, once you get beyond something you can run on your own computer, you need to run big workloads and lots of data.

56
00:10:03,400 --> 00:10:06,400
And you want to be able to do that effectively on cluster.

57
00:10:06,400 --> 00:10:12,400
And so you have the whole orchestration piece. How do you actually get that to work really well.

58
00:10:12,400 --> 00:10:23,400
Because, you know, AI workloads are very different from from regular software workloads. We can get into that later if we want, but that's another piece.

59
00:10:23,400 --> 00:10:37,400
Then you've got everything around data management, versioning tracking, but recently being called feature stores, which is part of it.

60
00:10:37,400 --> 00:10:57,400
High planning. And then you even have beyond that, something that we also are, you know, providing is what we call hyper data sets, which is not only do you want to be able to, you know, have data lineage and versioning and able to manage your features for training and for deployment.

61
00:10:57,400 --> 00:11:10,400
You also want to be able to have in-depth analysis of the data at hand. Where are my data biases? Where are my skews? How do I manage those? You know, think of it as like BI for AI data.

62
00:11:10,400 --> 00:11:14,400
And so that's another piece in ML ops.

63
00:11:14,400 --> 00:11:22,400
Then you've got the whole piece around deployment and then monitor. Right. So how do you deploy effectively deploy models.

64
00:11:22,400 --> 00:11:32,400
And it changes because there's deployment on the cloud, which is oftentimes machine learning lots of models, concurrently, but then you also have deployment on the edge.

65
00:11:32,400 --> 00:11:43,400
Think of all the use cases around deep learning, especially whether it's computer vision or sensor analysis, speech recognition, et cetera, that really live on the edge.

66
00:11:43,400 --> 00:11:52,400
And the edge could be, you know, data center. It could be a robot. It could be obviously a Thomas vehicle or it could be, you know, a small camera.

67
00:11:52,400 --> 00:12:05,400
So managing that and then monitoring the models in production in inference to identify model drift and concept drift, which is something that we have to live with in AI.

68
00:12:05,400 --> 00:12:26,400
And then that feedback loop, right. How do you actually connect that back together to create a situation where you could connect data coming in from the field with the original data set that use balance that retrain that and then send up, you know, optimized models, ideally automatically.

69
00:12:26,400 --> 00:12:40,400
So that's, you know, that's pretty much what ML ops is about. And what we're aspiring to do is to provide a very comprehensive solution.

70
00:12:40,400 --> 00:12:48,400
And, you know, that's different from many of the companies out there that try to focus on a very specific piece.

71
00:12:48,400 --> 00:13:06,400
I remember one of our, remember one of our first conversations was digging into this, you know, what I call the wide versus deep paradox in the ebook that we published the definitive guide to machine learning platforms.

72
00:13:06,400 --> 00:13:24,400
And the way I characterized it at the time was you've got, you've got a set of vendors that are trying to quote unquote own the ML ops workflow and support data scientists and ML engineers end to end.

73
00:13:24,400 --> 00:13:52,400
And then you've got another set of vendors that are trying to go deep in specific feature areas, like I'm going to go super deep in experiment management. And what I thought was this interesting paradox was, you know, the folks that were advanced enough to need an end to end platform had often invested heavily in some deep feature, either they built it or they bought it.

74
00:13:52,400 --> 00:14:05,400
The folks that knew enough about the problem that they knew they needed a deep solution, you know, often had gaps, you know, in their end and pipeline so that they.

75
00:14:05,400 --> 00:14:32,400
So the end result of this was that the deep vendors were kind of pushed to go broader and the broad vendors were pushed to go deeper and it just created a ton of confusion in the marketplace and you see, you know, lots of these kind of point solution vendors gradually expand the offering over the time and from our early conversations, it sounds like you've gone deep into specific areas over time.

76
00:14:32,400 --> 00:14:44,400
You know, I'm curious how you how you think about that, you know, why versus deep now, many months since the last time we talked about it.

77
00:14:44,400 --> 00:15:05,400
Yeah, I don't think there's one size fits all. I think that and I also think that what you alluded to is is really right. Look, ML ops is ultimately, you know, we and all the companies in the space are providing tools for people to build stuff on top of.

78
00:15:05,400 --> 00:15:21,400
We're not providing applications and we're not even providing, you know, tools for business analysts, sometimes called data scientists, but, you know, doing business analysts kind of work, we're building tools so they can build services and products on top of it.

79
00:15:21,400 --> 00:15:35,400
And when you do that, you know, one of your kind of competitors is your customer themselves, right, because you're putting this to data scientists and then engineers who love to build stuff.

80
00:15:35,400 --> 00:15:42,400
And so I think the trick is to how to provide them something that makes sense.

81
00:15:42,400 --> 00:15:51,400
The team is going to have each team is going to have their own different opinions on what they want to see.

82
00:15:51,400 --> 00:15:57,400
You know, from our perspective, you know, how we think about it. Let's look at that way. And again, I don't think it'll fit everyone's way.

83
00:15:57,400 --> 00:16:07,400
So the first point is we think that in AI, you've got three moving pieces. You've got models. You've got the data and you've got the code.

84
00:16:07,400 --> 00:16:19,400
It's that integration is going to be really, really hard. Integration is hard in computer science period. But when you have three moving pieces rather than just code, that's going to be even tougher.

85
00:16:19,400 --> 00:16:28,400
And so if you have to deal with multiple vendors, it's not just integration on the technical level. It's integration of a procurement level, financial level, operational level, etc.

86
00:16:28,400 --> 00:16:42,400
And so we think that if as a vendor, right, you can provide a solution that let's assume isn't the best in every single one of those pieces, but it provides 90% of the value.

87
00:16:42,400 --> 00:16:53,400
You know, you're probably, you know, that versus the cost and the burden that you're saving for the customer on all those integrations, we think is worth it.

88
00:16:53,400 --> 00:17:00,400
But we didn't stop at that. We also built something that's very modular.

89
00:17:00,400 --> 00:17:05,400
Because for two reasons, one, because we believe in what we call buy and build.

90
00:17:05,400 --> 00:17:13,400
And so you as a customer, we leave, we want to buy the scaffolding, the platform, because you're not an expert in that.

91
00:17:13,400 --> 00:17:19,400
You want to make sure you buy it from a vendor that's going to keep making sure they're got the best technology at all times.

92
00:17:19,400 --> 00:17:23,400
But you're building because you're building your specific solution top of it.

93
00:17:23,400 --> 00:17:28,400
And so a paradigm like that requires, you know, really robust APIs, etc.

94
00:17:28,400 --> 00:17:38,400
But if you build, if you do computer science engineering correctly, you're going to build modular robust pieces, even between our modules.

95
00:17:38,400 --> 00:17:45,400
And so that fact creates a situation where, at least for our platform, it's not an all or one thing.

96
00:17:45,400 --> 00:17:54,400
You don't have to use everything or nothing. You can actually use pieces of it and pieces from other vendors or pieces that were built in house.

97
00:17:54,400 --> 00:17:59,400
And because our competition or alternative could be something that they've built in the house.

98
00:17:59,400 --> 00:18:06,400
And we see that a lot. We are in multiple customers where, you know, we're not providing the full platform.

99
00:18:06,400 --> 00:18:14,400
They're using us for certain pieces. They're using in build solutions. And then some of them are using, you know, some of the other companies and vendors out there.

100
00:18:14,400 --> 00:18:20,400
And the idea is that I found quite interesting in your TwilmoCon presentations.

101
00:18:20,400 --> 00:18:28,400
We, you know, we think of it as mutually exclusive set of options by versus bill, bill versus by.

102
00:18:28,400 --> 00:18:35,400
But a lot of the way you discuss the way you deliver your solution.

103
00:18:35,400 --> 00:18:42,400
And in the case study joint presentation with your customer or theodore.

104
00:18:42,400 --> 00:19:02,400
And was focused on how actually buy and build coexist in their world and what the different pieces were, you know, where there were other open source and commercial products that, you know, altogether came to create a, you know, we're put into play to create a solution for them.

105
00:19:02,400 --> 00:19:17,400
I guess one question that that that comes to mind then is if you're taking that approach, do you, how does that avoid these integration costs that, you know, are the big problem?

106
00:19:17,400 --> 00:19:32,400
Well, so first of all, one point, I think that it's worth mentioning if we didn't, when we, when I wouldn't say buy and build were open source. So the buy is just using us rather than building it on your own.

107
00:19:32,400 --> 00:19:47,400
But that's a, that's a great question. There is no single answer, but what we do find is that significant portion of our customers tend to use more than one module of ours.

108
00:19:47,400 --> 00:19:53,400
And the more that they use us over time, the more they tend to use more modules.

109
00:19:53,400 --> 00:20:07,400
So you're right, we may not be saving all the integrations, but we're saving some. So rather than using, let's say, on the extreme five vendors or five different solutions, whether they're in bill, you know, they're going to end up using two, right.

110
00:20:07,400 --> 00:20:16,400
And so there is saving there, but you write that we don't save everything, you know, all the integration all the time.

111
00:20:16,400 --> 00:20:37,400
I imagine that there are ideas like data structures or even kind of API philosophies around which there's some efficiencies like you learn it for a system and even though the modules are different.

112
00:20:37,400 --> 00:20:52,400
You know, you're learning a kind of a way of thinking about things and there's some efficiency in using modules in the same out of the same product or project, even if you have to do some integration between them.

113
00:20:52,400 --> 00:21:07,400
Yeah, yeah, that. And also, you know, we're also working, you know, we're part of the AI Alliance. And for example, we published a blog post with Packaderm on an integration.

114
00:21:07,400 --> 00:21:13,400
And, you know, there's actually a GitHub library on integration between clear ML and Packaderm, so.

115
00:21:13,400 --> 00:21:26,400
So the extent that we can help support lowering the bar in terms of integration, even with with the ecosystem, large ecosystem, even if it's not, you know, our modules will do that.

116
00:21:26,400 --> 00:21:46,400
You bring a correct point. I don't think that there is an answer, right, at the end of answer to this. It's always going to be, you know, we're playing in the gray area. Some are going to use just us. Some are going to use, you know, not just to use us and choose, you know, other vendors who are internally build solutions.

117
00:21:46,400 --> 00:22:07,400
And significant percentage are going to use, you know, us for a bunch of things. And so lower significant part of their integration cost and still decide that for a certain piece for their specific use case, it's worth using some other vendor or some internal solution, and then they will be willing to pay that price for that specific integration.

118
00:22:07,400 --> 00:22:17,400
And within that, we obviously try to lower the cost of integration as much as we care.

119
00:22:17,400 --> 00:22:43,400
Along the modules you have or the, you know, even more broadly, the elements of the end workflow, where you're seeing folks, you know, buy versus build, use what you have versus use their own versus use another, another product.

120
00:22:43,400 --> 00:23:08,400
So, expert management today is pretty much a buy, right, again, buy could be using open source could be using us as an open source could be using ML flow or, you know, with biases or the other solutions, but pretty much very, very rare these days to see a company building their own solution around expert management.

121
00:23:08,400 --> 00:23:34,400
That is what I think that expert management for number of reasons, one, it's the first thing that data scientists actually interact with and before you actually even get to the ML engineers and data scientists are not engineers and so they would actually prefer to use something rather than build it becomes difficult for them.

122
00:23:34,400 --> 00:23:44,400
And so only in cases where you have an ML engineer say no, no, no, I want to build this, it becomes like, you know, why would I want to build this I'm a data scientist, I want to focus on the science.

123
00:23:44,400 --> 00:23:57,400
And so, you know, hitting that with data scientists as the first problem is one thing, I think that it's the easiest to get started with.

124
00:23:57,400 --> 00:24:10,400
Again, if you think about the lifecycle of a team or product, right, you know, pipelining deployment and data management, although we think it's the most important piece by far.

125
00:24:10,400 --> 00:24:31,400
For many reasons, it gets less attention, expert management just becomes the easiest thing to default to, and you know, there's, there's a, at some point, right, if you think about the maturity of the industry, you get to a point where if you go online to start looking for things to build or how to build it.

126
00:24:31,400 --> 00:24:41,400
You're very sincere, you know, why are you doing this right for management, here's a list of tools, just use it right, and so you realize, you know, I'm not going to do this, not going to make that mistake.

127
00:24:41,400 --> 00:24:55,400
Then you got to other things that, you know, today, I'd say you still see companies tooling together stuff, but they do less and less of what they've built.

128
00:24:55,400 --> 00:25:09,400
Although we're seeing a lot of companies that, you know, started three years ago, four years ago, where there were very, very few solutions if any out there, and so they had to build something.

129
00:25:09,400 --> 00:25:22,400
And so now they're faced with the dilemma, do I continue to support what I've built, right, some cost or whatever you want to call it, or do I, do I switch because they're switching costs, they're real switching costs, right, they have something that's working for them.

130
00:25:22,400 --> 00:25:27,400
And so that's the dilemma that they're that they're in.

131
00:25:27,400 --> 00:25:40,400
I think that, you know, over the next two, three, four years, it's going to be a situation where very, very few companies are going to build things on their own, it's really just going to be a decision.

132
00:25:40,400 --> 00:25:49,400
Do I take an open source, or you know, lots of open source pieces and combine them together, or, you know, combination with other vendors, and then build my specific thing on top.

133
00:25:49,400 --> 00:26:01,400
But I don't think we're going to see anyone building scaffolding the real scaffolding of Emma Lops, unless they're a Google or Facebook, which for that makes sense.

134
00:26:01,400 --> 00:26:22,400
Do you, how do you see things shaking out in terms of folks using a software based approach versus just taking whatever the cloud vendor of choice has to offer in terms of a platform.

135
00:26:22,400 --> 00:26:32,400
What we're seeing today still is that the solutions provided by the cloud vendors have two big disadvantages.

136
00:26:32,400 --> 00:26:39,400
One, which I think is going to be harder for them to shake off, which is that they tie you into their cloud.

137
00:26:39,400 --> 00:26:56,400
And, you know, we, and I think that certainly a lot of other vendors are making sure that at least, you know, we're agnostic, right? Clear enough, for example, works on any cloud, like any one of the big three, but also many other clouds on prem or any kind of deployment that you like.

138
00:26:56,400 --> 00:26:59,400
And so that's one big thing.

139
00:26:59,400 --> 00:27:15,400
It's important because, you know, companies don't want to be tied to a cloud vendor at times. And at least for a lot of use cases again, mostly on deep learning, you're talking about hybrid setups.

140
00:27:15,400 --> 00:27:20,400
We've got data coming in from the edge, you've got to do some processing at the edge.

141
00:27:20,400 --> 00:27:30,400
So making 100% cloud is a problem. And they're in those situations, even if you decide that you want, you're going to go for with Azure, for example, right?

142
00:27:30,400 --> 00:27:36,400
For the next 10 years, you still have a problem, how do you deal with ML ops on the edge? That's one problem.

143
00:27:36,400 --> 00:27:58,400
The second thing is, at least to date, the ML ops solutions that are out there by cloud vendors, and this is, you know, I'm going to quote my customers who come to us is that these solutions tend to be very shallow and very opinionated.

144
00:27:58,400 --> 00:28:13,400
And so they will work really, really well. If it's, if you're using it exactly the way that the engineers at, you know, X cloud vendor company designed them and the great engineers, but they designed in a very specific way and it's very shallow and you can do that really well.

145
00:28:13,400 --> 00:28:17,400
And it integrates incredibly well with their underlying other pieces in the cloud.

146
00:28:17,400 --> 00:28:30,400
And then if you want to do something slightly different for your specific use case, then a lot of times they're going to hit walls and, you know, we have customers who've come to us exactly for that reason.

147
00:28:30,400 --> 00:28:40,400
Now this is something that, you know, cloud vendors are going to probably work at. And so we're going to have more challenging competitive landscape later on.

148
00:28:40,400 --> 00:28:47,400
And that's, you know, that's a natural progression of, of, you know, of markets.

149
00:28:47,400 --> 00:29:00,400
You mentioned some of the work your customers are doing. We did have the opportunity here from Ariel at clear mail. And again, I mentioned theodore dotin from theodore.

150
00:29:00,400 --> 00:29:11,400
And I want to, for you to kind of give us a snapshot of either what they're up to or, you know, another customer that comes to mind is doing some interesting things.

151
00:29:11,400 --> 00:29:13,400
Who's pushing the envelope out there?

152
00:29:13,400 --> 00:29:17,400
Who's pushing the envelope?

153
00:29:17,400 --> 00:29:35,400
We, so we have one customer, another startup that has built a system for analysis of, you know, extra machines at, you know, airports and, you know, for security.

154
00:29:35,400 --> 00:29:49,400
They're, you know, they're a very small team relatively speaking. But in a matter of, you know, a couple of years, they've built a solution that is by far better than anything else out there.

155
00:29:49,400 --> 00:30:12,400
And they're, you know, they're pushing the envelope on how to use ML ops. We're very, really proud that they're using us and to end. But they've, they've built a system where they can literally deliver models that are tailored for each extra machine, specifically, because, you know, no extra machine is exactly the same as the other.

156
00:30:12,400 --> 00:30:26,400
And if you look at some extra images, you know, if the gun is really nicely put flat out against the screen, you'll see, but what if it's an an angle where you see the side, you know, it may look like any, any one of a hundred different things.

157
00:30:26,400 --> 00:30:33,400
And so accuracy becomes really super important. And even, you know, the slightest deviations between machines can affect that.

158
00:30:33,400 --> 00:30:48,400
And they've been able to use us to deploy a system where they automatically can optimize models for each specific customer and each specific machines based on the performance of the different models.

159
00:30:48,400 --> 00:30:57,400
You know, obviously, they look at the edge cases where, for example, the model is absolutely sure about what they found. And they use those to retrain models.

160
00:30:57,400 --> 00:31:12,400
But it's, it's 95 to 99% automated. And, and that's, you know, we think that's amazing. And we think that's where everyone's going to get to get at some point where you can deliver, you know, what we call, you know, overfitted model.

161
00:31:12,400 --> 00:31:23,400
But in a good way for each single sensor in the space or for each single model for, you know, doing a very specific, I don't know, identifying the right ad for a very specific person.

162
00:31:23,400 --> 00:31:28,400
Right, obviously private features aside. So that's one customer that's really pushing the envelope.

163
00:31:28,400 --> 00:31:40,400
It's an interesting way to think about that idea overfitting the, just to replay that, you know, overfitting the problem with overfitting is that, you know, it doesn't generalize.

164
00:31:40,400 --> 00:31:49,400
But if you are overfitting to any particular sensor or device or something, you don't need it to generalize because you've built a model for that thing.

165
00:31:49,400 --> 00:31:58,400
You want to generalize, I guess you wanted to generalize in one dimension, like in the dimension of the data, the distribution, but not.

166
00:31:58,400 --> 00:32:05,400
You're also able to have it very well fitted to the specifics of your, to your center.

167
00:32:05,400 --> 00:32:13,400
Yeah, exactly. So usually what happens is you want to build the generalized model initially. And then you want to overfitted specifically on some dimensions.

168
00:32:13,400 --> 00:32:19,400
Now for each one of those, you know, cases, again, it could be by sensor, it could actually be for different things.

169
00:32:19,400 --> 00:32:28,400
You can even think of a cloud situation where, you know, you're delivering a model to identify, you know, do prediction on, I don't know, whatever weather, right.

170
00:32:28,400 --> 00:32:35,400
And you can overfit the model for San Francisco, which has a very interesting weather pattern and overfit that for every different city, right.

171
00:32:35,400 --> 00:32:46,400
Similarly for any use case. And, and by the way, you can also overfit for the data. We had a, we have a customer doing security cameras.

172
00:32:46,400 --> 00:32:59,400
And, you know, they gave us a test case initially where we looked at how to build this, you know, basically build a system that supports multiple models, right for each, each camera.

173
00:32:59,400 --> 00:33:11,400
And there was a situation where there was a camera pointed at an entrance to large train station. And the entrance had these glass windows and there were fresh reflections there.

174
00:33:11,400 --> 00:33:21,400
You know, that has a very particular set of specifics in terms of the data. You're always going to get images that may have reflections in them.

175
00:33:21,400 --> 00:33:27,400
And so you can actually even overfit for that on that dimension as well.

176
00:33:27,400 --> 00:33:30,400
Interesting, interesting.

177
00:33:30,400 --> 00:33:46,400
We've talked quite a bit about, you know, devices and distributed sensors and in our recent kind of pre conversation, the whole topic of federated ML came up.

178
00:33:46,400 --> 00:33:52,400
And it's an area that you're doing some interesting stuff. And can you elaborate a bit on what you're doing there.

179
00:33:52,400 --> 00:34:00,400
Yeah, so we're doing two big things in federated learning.

180
00:34:00,400 --> 00:34:15,400
And so if you think about federated learning, you know, a series is basically saying, how do I train a single model using data that's located in multiple locations, which I cannot gather together to one location.

181
00:34:15,400 --> 00:34:25,400
And so I have to solve the, obviously, the compute problem and because I need to train that on the edge, but then I have to somehow connect the data to single model.

182
00:34:25,400 --> 00:34:28,400
In order to do that.

183
00:34:28,400 --> 00:34:45,400
So, theoretically, what you would need to do is, you know, you basically run one epoch, take all the, you know, all the feature scolars, send them back to one location, combine them together, send them to, you know, then redistribute them, do that again.

184
00:34:45,400 --> 00:34:49,400
If you do that every epoch, obviously, it's going to take forever.

185
00:34:49,400 --> 00:34:58,400
When we train models, we do this billions of times, right. And so think of the network latency and, et cetera, that completely doesn't work.

186
00:34:58,400 --> 00:35:08,400
So one thing that you need to solve is, how do you build a system that knows how to do that effectively, given the network constraints, right.

187
00:35:08,400 --> 00:35:21,400
Combine feature vectors every epoch, but every X epochs and how do you do that? That's not too much and not too little. And then how do you actually also work on effectively doing that over a network.

188
00:35:21,400 --> 00:35:31,400
And then obviously do that from a security perspective, because again, you can't access the data. You have to make sure that when it passes to network again, it's, it's, you can't get access to it.

189
00:35:31,400 --> 00:35:35,400
That's, that's an engineering problem combined with a science problem, right.

190
00:35:35,400 --> 00:35:47,400
And so that's one aspect of what we do. And really figuring out and providing a solution that's very effective in terms of being able to deliver.

191
00:35:47,400 --> 00:35:59,400
Almost or the same results in terms of if I had trained the model with the data all in more location and doing that in terms of an effective time and cost.

192
00:35:59,400 --> 00:36:26,400
The other piece of what we do is, and before we stray too far from that, how much of the results that you've seen there, would you describe as ML ops and, you know, the process of moving data around versus, I'm wondering if there's like an algorithmic component to this as well.

193
00:36:26,400 --> 00:36:45,400
There is, you know, there is a data data science component to this, right, in terms of you need to figure out how you combine those fact, you know, the, the, the feature of actors effectively, how you actually send, you know, when you send that that's a data science aspect of it.

194
00:36:45,400 --> 00:36:59,400
The engineering around sending this data securely effectively fast is a pure engineering problem. Okay. I don't even know if you want to call it ML ops.

195
00:36:59,400 --> 00:37:09,400
But you're getting it. You hit the nail on the head. There's aspects of both data science and engineering required to address this issue.

196
00:37:09,400 --> 00:37:16,400
You can't do it without without. And that's one piece. The second piece that we help.

197
00:37:16,400 --> 00:37:29,400
Or that's fundamental to our federated learning offering is the data aspect. This is what I call the hyper data sets before this is the underlying aspect of.

198
00:37:29,400 --> 00:37:43,400
So let's take a use case with, let's say hospitals, right, federated learning is a problem that a lot of health care is looking at a lot because you've got data, you've got patient security.

199
00:37:43,400 --> 00:37:55,400
And in many, many aspects, the health care institutions don't want to physically have the data go out, even if they're allowed to.

200
00:37:55,400 --> 00:38:02,400
They're safe and sorry, right. And so they would rather be able to have a solution where they data physically stays in their locations.

201
00:38:02,400 --> 00:38:15,400
So, but let's say you have a hospital in, you know, in location that has a large Hispanic population and then a hospital that is in location that has a large Asian American population, for example, right.

202
00:38:15,400 --> 00:38:23,400
They may, you know, they tend to potentially have different, you know, diseases that take genetic disease, for example, again.

203
00:38:23,400 --> 00:38:31,400
And so, or maybe one location where you have more women than the men or where the population tends to be younger and older.

204
00:38:31,400 --> 00:38:48,400
And so if you want to build a model, you have to take that into consideration when you're taking data from those different locations, one hospital, which may be skewed towards, you know, more elderly population may have more data than another one.

205
00:38:48,400 --> 00:38:53,400
And so, do you use that iteration of one to one or not.

206
00:38:53,400 --> 00:39:03,400
And so, for a successful model that's built off of federated learning model, you have to take that into consideration.

207
00:39:03,400 --> 00:39:14,400
And so you have to have a system that enables you to look at your data at the makeup of your data and at the skews and biases and be able to rebounce that for training.

208
00:39:14,400 --> 00:39:22,400
And you need to be able to do that without seeing the actual physical data because you don't have access to that, which is essentially what we provide with hyper data sets.

209
00:39:22,400 --> 00:39:30,400
We provide an ability for people to actually bounce the data based on those aspects without ever having access to the data.

210
00:39:30,400 --> 00:39:42,400
And then the system itself, when it trains, it knows how to balance the data to make sure that you're not getting a model that's skewed because maybe one hospital has too much data about elderly people.

211
00:39:42,400 --> 00:40:02,400
The picture that I'm creating is like, you know, database or or data warehouse views and and the views in this case are doing data science, things like, you know, stratification and waiting and stuff like that.

212
00:40:02,400 --> 00:40:13,400
So the data scientist isn't doing that as part of their model development process. They're just kind of accessing a data set that's provided via this view that does that stuff for them. Is that the.

213
00:40:13,400 --> 00:40:16,400
Absolutely, we call the data view.

214
00:40:16,400 --> 00:40:28,400
You hit on something really super interesting, Sam, you know, when when we were talking to our marketing guy to work on, you know, how do they explain hyper data sets and feature stores and he's like, well, it's a data warehouse.

215
00:40:28,400 --> 00:40:39,400
Yeah, you're right. But for whatever reason, you know, you tell us the data center. No, no, we don't work on data warehouse. We need to take the data out of the data warehouse and work on a feature store.

216
00:40:39,400 --> 00:40:49,400
It's a data warehouse. It's a database. You're absolutely right. A lot of the features and capabilities we provide the hyper data sets are really essentially.

217
00:40:49,400 --> 00:41:16,400
It's essentially a database that was built and optimized for AI use cases. And is that the, is that the feature store in your solution or is that specific to this federated ML federated learning problem and you do something else for, you know, generic feature store if we can call it that.

218
00:41:16,400 --> 00:41:39,400
We've, so we've divided, you can think of it as actually three layers. We have, we have a feature store, which is, you know, basically a feature store how everyone discusses it, data lineage, you know, snapshot and time being able to serve it for production or for training, being able to look at all the features.

219
00:41:39,400 --> 00:42:00,400
That's that's one solution that we provide. Then we provide an additional module that we call hyper data sets, which is the ability to actually create those data views and identify the skews and have search capabilities within the data and then rebalancing it.

220
00:42:00,400 --> 00:42:15,400
And then the third layer is the federated learning system that takes advantage of that in order to deliver federated learning application or, or, you know, platform got it, got it.

221
00:42:15,400 --> 00:42:32,400
And on the feature store side, are you, I've got to imagine you come across the real time versus batch and, you know, folks wanted to do both on a converge infrastructure like do you have a take on that.

222
00:42:32,400 --> 00:42:48,400
We, most of our, you know, most of our customers tend to focus more on deep learning, where that's less of an issue.

223
00:42:48,400 --> 00:43:07,400
Yeah. The folks, a lot of our customers in the machine learning space tend to basically use the feature store for development. And then when they're deploying it, they're just using the deployment server.

224
00:43:07,400 --> 00:43:22,400
They're not connected to connecting back to the feature store. There's, there's, by the way, there's overlap between feature store from that perspective, being able to deliver to into deployment and a lot of the deployment servers, even open source deployment servers out there that provide, you know, some of those capabilities as well.

225
00:43:22,400 --> 00:43:25,400
Can you elaborate on that a little bit?

226
00:43:25,400 --> 00:43:40,400
For example, Triton from Nvidia, provides a lot of the capabilities to be able to deliver data for inference optimally. And, you know, you don't need to go back and rely on a feature store for that.

227
00:43:40,400 --> 00:43:42,400
Got it. Got it.

228
00:43:42,400 --> 00:43:43,400
Cool.

229
00:43:43,400 --> 00:43:53,400
I think we also wanted to talk a little bit about transfer learning. You're doing some cool stuff there. Maybe we'll spend a few minutes on that and finish up.

230
00:43:53,400 --> 00:44:03,400
So one of the concepts that we've built around the external management piece.

231
00:44:03,400 --> 00:44:14,400
And essentially the whole platform is really a concept around, you can think about from regular computer science object oriented computing.

232
00:44:14,400 --> 00:44:29,400
Basically, you want to create, you know, or dolphins, right, you want to create, create containerized solutions, but that are also within them also have, you know, encapsulated solutions, right, where they have a very clearly defined interface between them.

233
00:44:29,400 --> 00:44:38,400
So you can connect and, and remove pieces and connect something else without changing anything or being, or needing to understand what's going on inside.

234
00:44:38,400 --> 00:44:50,400
This is how we think of an experiment. When you think of a task, we think of, okay, this is, it's an object that includes the code as an object includes the data as an object includes.

235
00:44:50,400 --> 00:44:58,400
You know, the model is an object, etc, etc, and then you have the artifacts, the logs, the results, all those are objects.

236
00:44:58,400 --> 00:45:10,400
And then you go into concept of what we will be basically called clone where you can with a click of a button basically clone that object with all the ancillary, you know, related objects connected to it.

237
00:45:10,400 --> 00:45:17,400
And what you can do now is start switching objects without having to go back to your underlying code.

238
00:45:17,400 --> 00:45:30,400
This means that you can very, very, very easily, you basically have the underlying structure for transferring, right, I want to build a model to identify X or predict X.

239
00:45:30,400 --> 00:45:38,400
Now I want to use, you know, I have a model that predict X why don't I use the same thing to predict why I have to do is train it on different data.

240
00:45:38,400 --> 00:45:53,400
If you didn't have this concept, this architectural concept in design within the system, you would need to go back to your code and build that model you have to now connected to that specific database and that specific data set.

241
00:45:53,400 --> 00:46:13,400
And it's aspects and there's a lot of work related to it with clear mail, it's a click of a button, you clone, you clone that task, you clone experiment, you basically, you know, change switch the data set from one to another, you don't have to go back to the code and voila, you can now run it, you have transfer learning.

242
00:46:13,400 --> 00:46:32,400
What we've gone even further is Nvidia has a transfer learning kit, it's called a TLT transfer learning toolkit that they have on their NGC cloud that also provides a lot of pre build models that you can transfer from.

243
00:46:32,400 --> 00:46:50,400
And you know, TLT is now integrated into clear mail and comes out of the box, so not only can you use TLT objects or these TLT models are already built in your object format and you can just plug your data in and that's it got it.

244
00:46:50,400 --> 00:47:07,400
Very cool. Any thoughts on kind of how ML ops evolves over the next few years, how are you thinking about the market and where it's headed.

245
00:47:07,400 --> 00:47:24,400
I think, you know, first of all, I think that, you know, we're going to see more and more of ML ops, you know, the dilemma of do I build or buy, et cetera, that's all going to appear it's all going to be, you know, buy and build or, you know, open source use and build.

246
00:47:24,400 --> 00:47:30,400
And I think that, you know, pretty much no one is going to to think otherwise.

247
00:47:30,400 --> 00:47:40,400
We have to, you know, there's a lot of maturity that needs to happen with a lot of tools, you know, I can think of so many features that we have on the list that we want to build.

248
00:47:40,400 --> 00:47:54,400
And I think that one thing that we're going to see with ML ops is that we're going to get through the maturity of the tools and the complexity on one hand underneath, but also being able to deliver it in a simple way.

249
00:47:54,400 --> 00:48:02,400
We're going to be able to get people to build stuff that aren't data scientists.

250
00:48:02,400 --> 00:48:10,400
And so today you have a lot of, I don't want to call necessarily toy applications for people who are, you know, have zero capabilities or building models.

251
00:48:10,400 --> 00:48:39,400
They work for very simple models, right, but I'm talking about the ability for a very specific, you know, industrial application, right, in whatever industry that you have to build something but not need to have, you know, an army of data scientists and be able to have the business owner of that problem, be much more integrated into the product development.

252
00:48:39,400 --> 00:48:47,400
And that's going to cut costs significantly, that's going to increase the quality of the work that companies are going to come out with.

253
00:48:47,400 --> 00:49:00,400
And I think that's all going to be delivered by more sophisticated ML ops tools on one hand, and on the other hand, being able to deliver those with, you know, simple interfaces.

254
00:49:00,400 --> 00:49:15,400
Or easier to use intuitive to use. And we're seeing that, you know, the really beginning of that. Also, you know, you're going back to your original question on the timeline of when Google did the blog post and what happened.

255
00:49:15,400 --> 00:49:32,400
We always underestimate as, you know, as startups and technologists at the cutting edge of things, you know, we, we tend to forget that most of the world is behind what we think of, you know, what we think really is granted for granted, we take for granted most of what isn't there yet.

256
00:49:32,400 --> 00:49:45,400
I was just talking to you to speak. I was just talking to a customer earlier today, they're building solutions for the, you know, for defense, right. So they're going to use our platform and they build the specific, you know, solutions for defense.

257
00:49:45,400 --> 00:49:51,400
And you know, he said that the one thing, you know, the CEO there told me when I go to these defense contractors.

258
00:49:51,400 --> 00:50:00,400
You know, the one thing I can say that's common across all of them is when I go from one side of the door, the other, you know, through that magnetometer and they're going to check everything and they're going to take.

259
00:50:00,400 --> 00:50:07,400
You know, it's like I passed, you know, through a time total 20 years back. Right.

260
00:50:07,400 --> 00:50:17,400
So, you know, there's so much to do, you know, you know, there's a whole huge industry behind that needs to get up to speed on that.

261
00:50:17,400 --> 00:50:26,400
AI is really just scratching the surface and ML ops is going to be able to deliver that to all those industries and we're going to see that happening, you know, just that.

262
00:50:26,400 --> 00:50:32,400
And the next five to 10 years is going to proliferate across all the industries is what we're going to see.

263
00:50:32,400 --> 00:50:34,400
Awesome. Awesome.

264
00:50:34,400 --> 00:50:44,400
Well, Nier, thanks so much for taking the time to chat and catch up and share a bit about what you're working on with everyone in the audience.

265
00:50:44,400 --> 00:50:55,400
Thank you, Sam, for the opportunity. It was a pleasure. It's always great to talk to you. You always have very insightful questions. It's a pleasure.

266
00:50:55,400 --> 00:50:57,400
Thanks, Nier.

