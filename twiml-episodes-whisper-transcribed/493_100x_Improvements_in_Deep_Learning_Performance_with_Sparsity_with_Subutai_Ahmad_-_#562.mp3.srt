1
00:00:00,000 --> 00:00:05,600
We think with the right hardware, you know, optimizations, 90%

2
00:00:05,600 --> 00:00:10,080
sparse should give you more than a 10x gain. So it's not, we're not talking

3
00:00:10,080 --> 00:00:14,160
about 50%, 20%, 30%, but we're talking about orders of magnitude here.

4
00:00:19,360 --> 00:00:24,640
All right, everyone. I am here with Subatai Ahmed. Subatai is VP of research at

5
00:00:24,640 --> 00:00:30,560
Numenta. Subatai, welcome to the Twoma AI podcast. Thank you so much for having me, Sam.

6
00:00:30,560 --> 00:00:34,560
Let's get started like we do here on the show by having you share a little bit about your

7
00:00:34,560 --> 00:00:41,840
background and your journey into the world of AI. Sure. So I've actually been in deep learning

8
00:00:41,840 --> 00:00:47,120
and something called computational neuroscience since the late 80s, long before it was called deep

9
00:00:47,120 --> 00:00:54,080
learning and I did research in, you know, understanding models of the brain and, you know,

10
00:00:54,080 --> 00:00:58,480
how that impacts machine learning in the late 80s and early 90s. I couldn't really see how to

11
00:00:59,280 --> 00:01:03,920
do anything practical with that at that time. And so I sort of switched gears and did research

12
00:01:03,920 --> 00:01:09,120
in more traditional machine learning and then did a couple of startups. About 17 years ago,

13
00:01:09,840 --> 00:01:15,920
I ran into Jeff Hawkins and Donna Dabinsky when they were founding Numenta. And it's sort of all

14
00:01:15,920 --> 00:01:22,080
my worlds kind of came together at that point. You know, Numenta as a company is really trying to

15
00:01:22,080 --> 00:01:27,440
show how we can take the neuroscience and really translate it eventually into practical systems.

16
00:01:28,400 --> 00:01:32,720
And so I've been at Numenta doing research for the last 17 years on that topic,

17
00:01:32,720 --> 00:01:40,000
very fortunate to be able to do that. Oh, that's fantastic. When we throw around this idea of deep

18
00:01:40,000 --> 00:01:46,640
learning being biologically inspired all the time, but I think Numenta is one of several places

19
00:01:46,640 --> 00:01:52,480
that's trying to push that idea of biological inspiration even further. Can you talk

20
00:01:52,480 --> 00:01:58,640
about the company and kind of the core ideas that you're researching?

21
00:01:58,640 --> 00:02:04,400
Yeah, I think deep learning has been somewhat biologically inspired and they're definitely,

22
00:02:04,400 --> 00:02:08,160
you know, the idea of modeling a neuron and, you know, some of the ideas have been convolutional

23
00:02:08,160 --> 00:02:13,920
networks and so on are directly taken from the neuroscience. But by and large, you know,

24
00:02:13,920 --> 00:02:18,800
if you look at neuroscience, it's pretty far from, you know, a lot of the details. It's very,

25
00:02:18,800 --> 00:02:25,280
very abstracted out. And so the core idea behind Numenta is, you know, deep learning is great.

26
00:02:25,280 --> 00:02:30,000
If you can really solve a lot of practical problems, but if we really want to understand intelligence

27
00:02:30,000 --> 00:02:35,600
and build truly intelligent systems, we need to go back and see what, you know, what have

28
00:02:35,600 --> 00:02:40,480
neuroscientists learned and how can we, you know, take those properties and create sort of

29
00:02:40,480 --> 00:02:46,160
algorithms, understand it from the algorithmic standpoint and then incorporate those ideas into

30
00:02:46,160 --> 00:02:51,440
deep learning. You know, we feel just, you know, building bigger computers and faster computers

31
00:02:51,440 --> 00:02:55,200
and throwing more data at it is all great, but that's not, though, that's not going to lead us

32
00:02:55,200 --> 00:03:01,520
to intelligent systems. So we think it's, you know, we have an existence proof. There's an

33
00:03:01,520 --> 00:03:06,720
amazing thing, the brain that is super intelligent, it's far more intelligent than any deep learning

34
00:03:06,720 --> 00:03:12,080
system out there. Why not try to understand, you know, what's going on there and see if we can

35
00:03:12,080 --> 00:03:18,400
apply that to practical systems? When you think about some of those core things that neuroscientists

36
00:03:18,400 --> 00:03:24,480
have learned or that neuroscience teaches us that we can put to use in building learning systems,

37
00:03:25,120 --> 00:03:29,360
what are those things? What does that landscape look like? Yeah, there's, there's quite a lot.

38
00:03:29,360 --> 00:03:34,880
But neuroscientists have field has really exploded in the last 20 or 30 years. You know, many,

39
00:03:34,880 --> 00:03:39,760
many people may not realize this, but the experimental techniques have really gotten extremely

40
00:03:39,760 --> 00:03:46,160
sophisticated. And so there's many, many things in the neuroscience now that we can actually

41
00:03:46,160 --> 00:03:52,080
apply to practical systems. At the same time, there's a ton of detail in neuroscience that is not

42
00:03:52,080 --> 00:03:57,200
applicable to practical systems. So it's, it's a bit of a challenge to figure out exactly where to

43
00:03:57,200 --> 00:04:02,240
draw that line. You know, what aspects are really important and what aspects are just sort of more

44
00:04:02,240 --> 00:04:08,720
biological detail that don't impact practical systems. So kind of the, you know, some of the big

45
00:04:08,720 --> 00:04:14,320
items that hopefully we can get into more detail on just ideas like a cortical column idea that

46
00:04:14,320 --> 00:04:21,120
there's a common micro circuit that's underlying all of intelligent function. The idea of the neuron

47
00:04:21,120 --> 00:04:27,120
itself, the neuron neurons in biological brains are much more sophisticated and powerful than the

48
00:04:27,120 --> 00:04:34,080
neurons we use in deep learning. You know, even core representational ideas like sparsity, the

49
00:04:34,080 --> 00:04:39,920
brain is extremely sparse and leads to a lot of efficiencies and other properties. Yeah,

50
00:04:39,920 --> 00:04:47,120
can we extract some of those ideas and incorporate it in? Even some ideas like the, you know, the brain

51
00:04:47,120 --> 00:04:52,720
is inherently a sensory motor system or constantly moving around the world and we learn by movement

52
00:04:52,720 --> 00:04:57,600
and we learn the structure of the world through movement and this ties into how cortical columns

53
00:04:57,600 --> 00:05:03,680
are set up. So all of those ideas are somewhat different than what's in deep learning today and

54
00:05:03,680 --> 00:05:08,880
those are concepts that we think is a lot that's understood now and it behooves us to kind of

55
00:05:08,880 --> 00:05:14,560
incorporate that into deep learning systems today. So this idea of a cortical column as this

56
00:05:14,560 --> 00:05:21,280
fundamental micro circuit is a compelling one is not something that I've heard of previously. Can

57
00:05:21,280 --> 00:05:26,160
you dig into what that means, what this micro circuit looks like and what we know about it?

58
00:05:26,880 --> 00:05:32,320
Sure. It's actually a fantastic idea. It's an amazing idea and this was, I think first

59
00:05:33,520 --> 00:05:39,440
proposed by a neuroscientist called Vernon Mount Castle in the 70s and what he noticed is that

60
00:05:39,440 --> 00:05:44,560
wherever you look in the neocortex. So first of all, the neocortex is kind of the largest

61
00:05:44,560 --> 00:05:49,600
structure in our brain. It's really where most of intelligent function actually happens and,

62
00:05:49,600 --> 00:05:53,920
you know, whether it's visual processing, auditory processing, language, you know, high level thought

63
00:05:53,920 --> 00:05:58,960
that all occurs in the neocortex. So what he saw is that no matter where you look in the

64
00:05:58,960 --> 00:06:07,440
neocortex, you see a very, very similar micro circuit in a very similar connections between the

65
00:06:07,440 --> 00:06:13,120
layers and between neuron similar neuron types and a sort of a prototypical architecture.

66
00:06:14,320 --> 00:06:21,200
And there are entire neocortex has somewhere around 150,000 cortical columns. So each cortical

67
00:06:21,200 --> 00:06:27,360
column is about the size of a grain of rice and it sort of sprinkled throughout our neocortex.

68
00:06:27,360 --> 00:06:34,080
And like I said, they all have this common architecture. And so do you say that when you say

69
00:06:34,080 --> 00:06:40,960
common architecture and connections, the picture that pops up in my mind is kind of the neuron

70
00:06:40,960 --> 00:06:44,960
and the dendrites and these connections. But it sounds like you're talking about a higher level

71
00:06:44,960 --> 00:06:52,720
structure. Is that right? Yeah. So the new cortex, the way it's structured is it's like a

72
00:06:52,720 --> 00:06:59,200
big flat 2D sheet that sort of scrunched up and stuck in your brain. But if you were to flatten it

73
00:06:59,200 --> 00:07:06,080
out, it's about a couple of millimeters thick. And there's several layers, you know, down that

74
00:07:06,080 --> 00:07:12,800
thickness in that dimension. And so there's about, you know, scientists say roughly six layers.

75
00:07:13,920 --> 00:07:19,680
And these layers have a, you know, intricate connectivity between them. So there's different neuron

76
00:07:19,680 --> 00:07:26,880
types that are in different layers and they connect in a recurrent circuit. And that architecture

77
00:07:26,880 --> 00:07:32,400
is what what we mean when we say that's repeated throughout the, you know, cortex. So it is,

78
00:07:32,400 --> 00:07:39,680
it is a little bit complicated to go through. But this, I'd say probably 50,000 or 100,000 neurons

79
00:07:39,680 --> 00:07:45,680
inside a cortical column and a fairly sort of prototypical connectivity structure between them.

80
00:07:45,680 --> 00:07:52,400
And what have we learned about the columns themselves and their behavior? And I'm imagining this

81
00:07:52,400 --> 00:07:57,920
is something that you, you know, might try to model computationally as well. Yeah, there's a lot

82
00:07:57,920 --> 00:08:03,360
we can do computationally. And what neuroscientists have found, you know, there's this common structure

83
00:08:03,360 --> 00:08:10,560
and the proposal by Mount Castle is that, you know, the reason of visual area as a visual area is

84
00:08:10,560 --> 00:08:15,520
not because there's anything really fundamentally different. It's just that the inputs are different.

85
00:08:16,560 --> 00:08:21,920
The learning algorithms and the architecture is largely similar to the auditory cortex or to

86
00:08:21,920 --> 00:08:26,880
the language areas. And neuroscientists have even done this experiment where experiments where you

87
00:08:26,880 --> 00:08:34,240
sort of swap modalities and you take an auditory auditory cortex and you feed it visual information.

88
00:08:34,240 --> 00:08:38,960
And what you see is you actually see visual feature detectors show up in the auditory cortex.

89
00:08:39,920 --> 00:08:45,680
And, you know, so that tells you there's a very similar kind of algorithm. It's it's not about

90
00:08:45,680 --> 00:08:54,320
the sensory modality. It's about the common algorithm. So just to interject the meaning that this,

91
00:08:54,320 --> 00:09:01,360
you know, we talk about the convolution being neurologically inspired. Is the suggestion that

92
00:09:01,360 --> 00:09:07,040
that's a software feature as opposed to a hardware feature? Exactly. Yeah, so to speak. Yeah,

93
00:09:07,040 --> 00:09:11,920
so convolution sort of says basically that you have sort of similar feature detectors, you know,

94
00:09:11,920 --> 00:09:17,200
throughout your visual field. So that's and you do see that in the in the visual cortex. You see

95
00:09:17,200 --> 00:09:24,000
very similar features across what this says is even more it takes that sort of one notch off. It's

96
00:09:24,000 --> 00:09:30,160
like the entire architecture. It's not just one little feature as sort of the entire architecture is

97
00:09:30,160 --> 00:09:35,840
really preserved across sensory modalities. And this is an amazing thing. If this were true and we

98
00:09:35,840 --> 00:09:41,680
believe this is largely true is in order to really understand how to implement intelligent systems

99
00:09:41,680 --> 00:09:46,880
in a new or cortical way, you sort of have to understand how one cortical column works and how

100
00:09:46,880 --> 00:09:51,520
multiple cortical columns interact. And then it's just essentially a scaling problem, you know,

101
00:09:51,520 --> 00:09:57,520
you just build more and more of them. So that really simplifies the process and and from a

102
00:09:57,520 --> 00:10:02,160
computer scientist point of view, that's great. It's like there's one thing we need to understand.

103
00:10:02,160 --> 00:10:06,960
It may be complex, but it's not that complex. You know, we can understand it. And once we understand

104
00:10:06,960 --> 00:10:11,920
it, it's a scaling issue. And so how far along are we in this journey to understand the cortical

105
00:10:11,920 --> 00:10:16,960
column? Yeah, I think we've made a lot of progress in understanding it. There's still a lot of work

106
00:10:16,960 --> 00:10:24,160
to do. You know, one of the so, you know, if you look at convolutional neural networks, they

107
00:10:24,160 --> 00:10:30,080
may make maybe one or two layers of this six layer structure. So they, you know, so they may make

108
00:10:30,080 --> 00:10:36,560
a small piece of that. Some of the things we've learned is that every cortical column is actually

109
00:10:36,560 --> 00:10:42,560
inherently a sensory motor system. And what I mean by that is every cortical column gets sensory

110
00:10:42,560 --> 00:10:49,200
inputs and every cortical column actually sends out motor commands. So when you look at the visual

111
00:10:49,200 --> 00:10:56,080
area, it's not just a feed, it's not just an area which gets visual input. It actually sends out

112
00:10:56,080 --> 00:11:00,800
signals, you know, to your eyes, for example, and your head to move your, you know, head and eyes

113
00:11:00,800 --> 00:11:06,880
around. If you look at the so-called motor areas, they actually get sensory input as well. So

114
00:11:06,880 --> 00:11:11,520
there's no such thing as a sensory area or a motor area. Everything's an inherently sensory motor

115
00:11:11,520 --> 00:11:19,760
area. And what we know is that mammals in particular and humans, we really learn by moving around

116
00:11:19,760 --> 00:11:26,880
the world. So this algorithm is inherently one that's constantly driving action, constantly making

117
00:11:26,880 --> 00:11:32,080
predictions. We see when our predictions are correct and when they're not correct and we learn

118
00:11:32,080 --> 00:11:37,040
from those mistakes. And that's really how we learn about the world. We're not just passive

119
00:11:37,040 --> 00:11:42,560
systems that, you know, getting billions of images as input and then suddenly we can recognize

120
00:11:42,560 --> 00:11:47,440
cats and dogs. You know, we inherently understand the world and the structure of the world by moving

121
00:11:47,440 --> 00:11:54,000
around and making actions and stuff. So that's a really big, you know, component of it.

122
00:11:54,000 --> 00:12:01,600
Another sort of really interesting thing is that, you know, if the cortical column is the same

123
00:12:01,600 --> 00:12:07,280
everywhere, the implication of that is that if some area of the brain is doing something,

124
00:12:08,080 --> 00:12:14,080
it must be doing that algorithm everywhere. So if you look at high-level thought and what's

125
00:12:14,080 --> 00:12:19,280
required for high-level thought, well, the same processes must be occurring also in low-level

126
00:12:19,280 --> 00:12:28,160
visual areas and other areas. And what our theory says is that every cortical column is actually

127
00:12:28,160 --> 00:12:34,720
its own independent modeling system. So every little cortical column is building a structured

128
00:12:34,720 --> 00:12:40,640
model of the world through movement by understanding how movement shapes perception and builds up

129
00:12:41,360 --> 00:12:46,960
a model of the world. And all of these cortical columns, the way they work is they interact

130
00:12:46,960 --> 00:12:52,720
and they're with each other and they come to a consensus about what is the current percept that's

131
00:12:52,720 --> 00:13:00,400
coming in. And through this sort of voting algorithm, that's essentially what we are consciously

132
00:13:00,400 --> 00:13:06,240
aware of. You know, every with thousands and thousands of these cortical columns, each working

133
00:13:06,240 --> 00:13:12,960
independently. And then they come to a consensus about what actually we are seeing and I think.

134
00:13:12,960 --> 00:13:19,600
So it's an extremely distributed system of lots of independent modeling. Again,

135
00:13:19,600 --> 00:13:22,400
this is very different from the way deep learning systems are struggling today.

136
00:13:22,400 --> 00:13:26,480
Right. It raises a lot of really interesting questions for me. Like,

137
00:13:28,080 --> 00:13:34,000
you know, we talk about these distributed system with lots of common components. You know,

138
00:13:34,000 --> 00:13:39,120
one view is like a swarming kind of thing where everything's the same and you have these voting

139
00:13:39,120 --> 00:13:46,320
mechanisms, but you've also described in talking about the visual versus auditory function,

140
00:13:46,960 --> 00:13:53,840
a degree of specialization. And I'm still trying to wrap my head around like, is that hardware

141
00:13:53,840 --> 00:14:00,640
or software specialization, meaning does the, you know, almost like kind of is there like a bone

142
00:14:00,640 --> 00:14:06,560
marrow kind of is it kind of like a bone marrow and it like physically changes to specialize or

143
00:14:06,560 --> 00:14:13,920
is it just changing thresholds or software things? Yeah. Yeah. That's the answers to questions

144
00:14:13,920 --> 00:14:18,640
like these. I mean, we know some of it. You know, that's it's a great question and it's something

145
00:14:18,640 --> 00:14:22,800
that's puzzled nor scientists for a long time. If you look at the auditory area, you see auditory

146
00:14:22,800 --> 00:14:27,040
neurons. And if you look at the visual area, you see visual neurons. So what's, you know, what's

147
00:14:27,040 --> 00:14:33,680
common there? Well, what's so, you know, it's similar to a deep learning system in the sense that

148
00:14:33,680 --> 00:14:37,920
you can take a convolutional network. If you feed at visual information, it's going to learn edges

149
00:14:37,920 --> 00:14:42,640
and, you know, corners and so on. If you feed at auditory information, it's going to learn auditory

150
00:14:42,640 --> 00:14:48,240
features. So it's, it's the same thing. It's these are, there's a learning algorithm that's going

151
00:14:48,240 --> 00:14:55,600
on. And because the input is different, it's just going to learn different things. And you can't

152
00:14:55,600 --> 00:14:59,600
just look at the end of the day and just probe it and see what it's learned. You have to understand

153
00:14:59,600 --> 00:15:06,160
the learning process itself. So it is, you know, the hardware is essentially very, very common.

154
00:15:07,040 --> 00:15:13,040
But the sort of emergent functionality is very different because the inputs against is very

155
00:15:13,040 --> 00:15:19,600
different throughout throughout its life. I mean, there are some differences between auditory areas

156
00:15:19,600 --> 00:15:24,880
and visual areas. So there's some specialized color detectors and things like that in the

157
00:15:24,880 --> 00:15:29,200
visual areas that you don't see in the auditory areas. And so, but those are really in the, in the

158
00:15:29,200 --> 00:15:34,400
details. If you step back and look at the large scale architecture, there's huge commonalities

159
00:15:35,280 --> 00:15:40,080
across them. Yeah, it is, it is interesting. It's fascinating to think through.

160
00:15:42,080 --> 00:15:48,720
Absolutely. So the cortical column is kind of this higher level architecture. You also mentioned

161
00:15:48,720 --> 00:15:56,000
the neuron itself and the way we've been modeling that for deep learning versus the way we think

162
00:15:56,000 --> 00:16:00,960
about that from a neuroscience perspective. Can you elaborate a bit on that? Yeah. So if you look

163
00:16:00,960 --> 00:16:07,920
at a deep learning system, you know, the basic idea of what a neuron does is it takes a very simple

164
00:16:07,920 --> 00:16:13,680
function of the input and computes a very simple outputs and basically takes linear weighted sum

165
00:16:13,680 --> 00:16:19,360
off its inputs and passes it through non-linearity, like a sigmoid or a relu and then outputs and

166
00:16:19,360 --> 00:16:25,840
there's a very simple idea. That idea has been around for more than a hundred years. It's called

167
00:16:25,840 --> 00:16:31,120
the point neuron model. And that basic idea in our mathematical system and practicalism really

168
00:16:31,120 --> 00:16:37,920
hasn't changed. But if you look at the biology, real neurons are nothing like that. Real,

169
00:16:37,920 --> 00:16:42,560
real neurons, you know, you mentioned these dendrites. The neurons have really complicated

170
00:16:42,560 --> 00:16:49,200
dendritic structures and these dendritic structures as where neurons get input. And the neuron actually

171
00:16:49,200 --> 00:16:55,200
does very sophisticated processing of its input through these dendritic structures before actually

172
00:16:55,200 --> 00:17:02,560
computing an output. And so there are many, many layers of this. But you know, neuron is a pretty

173
00:17:02,560 --> 00:17:09,120
sophisticated computing device in the biological brains. And so, you know, we've been sort of thinking,

174
00:17:09,120 --> 00:17:14,800
you know, what, what aspects of that do we actually need to model in deep learning systems?

175
00:17:14,800 --> 00:17:20,480
Are there advantages to these dendritic structures? And when we think about things like

176
00:17:20,480 --> 00:17:25,360
continual learning, you know, the fact that, you know, humans can constantly learn new things

177
00:17:25,360 --> 00:17:31,200
without forgetting stuff, you know, deep learning systems are not good at that. And we think the

178
00:17:31,200 --> 00:17:36,480
dendritic structure and the non-linear processing that goes on in these dendrites and neurons is

179
00:17:36,480 --> 00:17:42,240
actually critical to how we learn new things without forgetting stuff. So that's an example of,

180
00:17:43,040 --> 00:17:47,600
you know, some functionality that that's difficult today in deep learning that could be improved

181
00:17:47,600 --> 00:17:54,320
if we, if we could incorporate some of these properties. I'm curious what the neuroscience says

182
00:17:54,320 --> 00:18:01,840
around like the, are these cortical columns and the neurons? Are they fundamental for

183
00:18:01,840 --> 00:18:10,480
memory as well as kind of the processing? Like, is it uniform across these very different

184
00:18:10,480 --> 00:18:19,200
modes as well? Some of the more recent deep learning research directions are like incorporating

185
00:18:19,200 --> 00:18:24,720
memory into deep learning systems as a way to, you know, get closer to the kind of intelligence

186
00:18:24,720 --> 00:18:31,520
that we exhibit. But I'm wondering if you're saying that, well, there's a single thing underneath

187
00:18:31,520 --> 00:18:36,160
from a biological perspective. Yeah, I mean, it all has to be done by neurons, right? Some are

188
00:18:36,160 --> 00:18:44,560
on. Yeah, exactly. It's nothing else. So it's, you know, in the brain, you know, memory, learning,

189
00:18:45,440 --> 00:18:51,760
language, all of those functions, a large part of it happens in the neocortex and a lot of it

190
00:18:51,760 --> 00:18:57,360
happens in what's called the hippocampus and hippocampus structures as well. And all of those both,

191
00:18:57,360 --> 00:19:02,800
you know, both of those structures have exact same type of neurons that I was talking about,

192
00:19:03,360 --> 00:19:08,560
what's called pyramidal neurons. So they have the same complicated, you know, nonlinear

193
00:19:08,560 --> 00:19:15,760
integration. So those properties of neurons are common for all function, you know, all intelligent

194
00:19:15,760 --> 00:19:21,280
function, you know, memory, language, speech, you know, as you and I are talking and listening to

195
00:19:21,280 --> 00:19:27,120
each other, that's what we're doing. Our dendrites are processing away and creating these representations.

196
00:19:27,120 --> 00:19:34,560
To what degree are the notions that notions around like spiking something that you're focused on

197
00:19:34,560 --> 00:19:41,360
in your research? Yeah, so neurons send signals to each other by initiating a spike. So it's an

198
00:19:41,360 --> 00:19:47,360
electrical signal that, you know, goes from one end to the other. We think we're not sure that

199
00:19:47,360 --> 00:19:52,640
that's really critical to a model or not. You know, we do need to communicate from one neuron to

200
00:19:52,640 --> 00:19:57,200
another, but it may be okay to just, you know, put a number in a memory location. That may be,

201
00:19:57,200 --> 00:20:03,440
that's what happens in deep learning. And that may be just fine. You know, it is true that spiking,

202
00:20:03,440 --> 00:20:08,320
you know, is primarily binary, like it either a neuron, either spikes or it doesn't,

203
00:20:08,320 --> 00:20:12,320
whereas in deep learning system, we tend to transmit really high precision numbers.

204
00:20:13,200 --> 00:20:19,360
And that's part of the reason deep learning systems are so expensive and energy inefficient.

205
00:20:19,360 --> 00:20:24,000
So if you could go to the point where we can transmit essentially binary information, there's a

206
00:20:24,000 --> 00:20:29,680
chance we could make deep learning systems really, really efficient. And so that's one area where

207
00:20:29,680 --> 00:20:35,760
the spiking, it may be important to model spiking, but by and large, you know, in our work,

208
00:20:35,760 --> 00:20:42,800
we haven't found it really necessary to model kind of the details of all the details of spiking as

209
00:20:42,800 --> 00:20:48,560
it is in biology. So, you know, again, with neuroscience, it's always a question of where do you draw

210
00:20:48,560 --> 00:20:52,880
the line? You know, what level of detail do you incorporate and what level you've done? And so

211
00:20:52,880 --> 00:20:58,320
this is where we're kind of on the border of. And I imagine that that is a kind of a combination

212
00:20:58,320 --> 00:21:05,120
of, you know, intuition and experimentation and seeing what works. Exactly. Yeah. Yeah. We try a

213
00:21:05,120 --> 00:21:11,760
lot of stuff. Our bias is that if something is really prevalent in the neuroscience, if it's really

214
00:21:11,760 --> 00:21:17,920
common and a big feature, it probably has some views. And so we do look at all of this stuff pretty

215
00:21:17,920 --> 00:21:23,120
carefully, but we don't incorporate it into our models until we can actually come up with a

216
00:21:23,120 --> 00:21:28,880
functional reason for it. Like there's some, you know, some benefit to be gained, but we do look at

217
00:21:28,880 --> 00:21:38,160
a lot of these details pretty closely and talk to neuroscientists constantly. So we've talked about

218
00:21:38,160 --> 00:21:46,960
cortical columns, the neuron model, kind of from the biological neuroscience perspective,

219
00:21:46,960 --> 00:21:55,680
how do these concepts translate into things that, you know, learning machines for to say broadly?

220
00:21:56,240 --> 00:22:00,880
You know, from a cortical column perspective, you know, if we can, you know, understanding that

221
00:22:00,880 --> 00:22:08,560
architecture helps us, what should help us design better kind of modules and layers and deep

222
00:22:08,560 --> 00:22:13,520
learning system. In the deep learning system today, it's very, very simple, you know, you have a

223
00:22:13,520 --> 00:22:18,960
convolutional layer or maybe even a transformer linear layer or something like that. They're very

224
00:22:18,960 --> 00:22:24,240
simple layers. What the neuroscience tells us is that each layer is a lot more complicated,

225
00:22:24,960 --> 00:22:31,040
has a recurrent structure and it gets sensory input and motor output. So we can actually take the

226
00:22:31,040 --> 00:22:37,280
notion of what a layer is in deep learning and incorporate some of these other elements into it and

227
00:22:37,280 --> 00:22:45,360
make it a lot more, you know, complex. And that will provide a lot of benefits as I mentioned,

228
00:22:45,360 --> 00:22:49,680
the idea of continual learning, for example, you know, the ability to learn new things without

229
00:22:50,800 --> 00:22:57,120
forgetting stuff. But more importantly, if we can make these layers inherently sensory motor,

230
00:22:57,120 --> 00:23:02,800
what we can build in are layers that really understand the 3D structure of the world and the

231
00:23:02,800 --> 00:23:08,960
physical structure of the world. They inherently understand it. They build 3D models at every level

232
00:23:08,960 --> 00:23:15,040
of the hierarchy and this will allow you to have extremely robust neural networks that don't get

233
00:23:15,040 --> 00:23:21,040
fooled very easily with, you know, input that's slightly different from what it's seen before.

234
00:23:22,240 --> 00:23:29,520
It should lead to neural networks that are much more invariant to distortions and things like

235
00:23:29,520 --> 00:23:34,000
that. Today's neural networks, you really have to show it, you know, let's say you're doing a

236
00:23:34,000 --> 00:23:38,800
visual system, you have to show it images of every object in every possible pose and every possible

237
00:23:38,800 --> 00:23:43,600
lighting condition and all of that stuff. Whereas if you can really inherently understand the

238
00:23:43,600 --> 00:23:49,840
structure of stuff, the amount of training data you will need will be dramatically smaller.

239
00:23:50,400 --> 00:23:56,960
And the representations you build will be much more robust and invariant. So these are the

240
00:23:56,960 --> 00:24:02,560
kinds of properties. I think we can really build in at a very fundamental level into deep learning

241
00:24:02,560 --> 00:24:10,240
systems by taking clues from biology. What does it mean for a model to have an inherent 3D

242
00:24:11,200 --> 00:24:17,600
understanding? I mean, it's a number, right? Are we talking about like changing the coordinate

243
00:24:17,600 --> 00:24:26,720
system, the something polar? It's a fantastic question. It's something we've worked on a lot

244
00:24:26,720 --> 00:24:32,560
exactly that question. We published this theory called the 1000 brain theory. So this idea that

245
00:24:32,560 --> 00:24:36,800
cortical column, the thousands of these cortical columns and each cortical column is kind of this

246
00:24:36,800 --> 00:24:43,440
independent modeling system. So what the 1000 brain theory says, and this is a lot of its derived

247
00:24:43,440 --> 00:24:50,400
from neuroscience data, is in each cortical column, you have, you are building up models that are

248
00:24:50,400 --> 00:24:55,440
based on coordinate systems, just like you mentioned. So there's something essentially reference

249
00:24:55,440 --> 00:25:02,720
frames in each cortical column. And you can think of us, think of cortical columns as building maps

250
00:25:02,720 --> 00:25:10,240
of the world. So just like you have a physical map that might show a city and shows how you can

251
00:25:10,240 --> 00:25:17,120
move around a city and what's located at different GPS coordinates along the map. In the same way,

252
00:25:17,120 --> 00:25:27,040
when we inherently build up a structured model of an object, we create a 3D map. And in that map,

253
00:25:27,040 --> 00:25:34,720
we associate locations with features and properties of the object. And we know how you can manipulate

254
00:25:34,720 --> 00:25:39,680
that object, how you can make predictions about what will happen if you go from one part of the

255
00:25:39,680 --> 00:25:46,720
object to the next. And that map is in what we call in the reference frame of the object itself.

256
00:25:46,720 --> 00:25:52,000
It's not in the observer's reference frame. It's in the coordinate system of the object.

257
00:25:52,640 --> 00:25:58,000
And so that's essentially what we mean by inherently understanding the object. You would build up

258
00:25:58,000 --> 00:26:04,480
this really detailed map life structure of objects in its own reference frame. And we may be viewing

259
00:26:04,480 --> 00:26:09,680
it from very different angles in many different ways. But all we have to do is translate that new

260
00:26:09,680 --> 00:26:18,080
viewpoint into this map life structure. And now we can navigate and understand how that object

261
00:26:19,040 --> 00:26:25,280
works. So that was quite a mouthful. There was a lot of stuff in there. But inherently, we're

262
00:26:25,280 --> 00:26:32,800
building up these map life structures that contain the full physical and geometric structure of

263
00:26:32,800 --> 00:26:39,920
objects and concepts and so on. Yeah, it's reminding me in some ways to the, I'm blanking on the

264
00:26:41,040 --> 00:26:46,560
the model, but like Jeff Hinton's, the name of the model, but Jeff Hinton has been talking about

265
00:26:46,560 --> 00:26:54,960
this post convolutional model that has these properties of being kind of more spatial and

266
00:26:54,960 --> 00:27:00,480
translation invariant and things like that. Yeah, so he came up with this idea capsule,

267
00:27:00,480 --> 00:27:06,720
maybe they're the same as capsules. Yeah, so Jeff Hinton has actually been thinking about these

268
00:27:06,720 --> 00:27:12,000
ideas since the 70s. We found papers of him writing in the 70s about how we need to build up

269
00:27:12,000 --> 00:27:16,800
object-centered reference representations that are an object-centric reference frame. So he's

270
00:27:16,800 --> 00:27:22,880
been thinking about this a long time. And so everything I mentioned is definitely very much

271
00:27:24,160 --> 00:27:30,240
in the same kind of line of thinking. I would say what we've learned from the neuroscience is that

272
00:27:30,240 --> 00:27:36,000
these particle columns are much more powerful than we thought. It's much more than what a capsule

273
00:27:36,000 --> 00:27:42,080
does. They're really independent modeling systems and they're inherently sensory motor.

274
00:27:42,800 --> 00:27:50,240
So motor actions and predictions are an inherent piece of this puzzle. And so these are all

275
00:27:50,240 --> 00:27:56,320
aspects that have to be incorporated in as well. But definitely very, I think there's some

276
00:27:56,320 --> 00:28:00,560
truism in deep learning that no matter what idea you think of, Jeff Hinton has probably thought

277
00:28:00,560 --> 00:28:11,440
of it 20 years ago. He's great. A similar question to the last one. What does it mean for

278
00:28:12,480 --> 00:28:20,240
these computational models to be inherently sensory motor? Is that implying like a system level

279
00:28:20,240 --> 00:28:28,880
connection between input sensors and representations or something else? Yeah. So basically

280
00:28:28,880 --> 00:28:34,880
cortical columns send out motor commands. And so there are 150,000 of these cortical columns. They're

281
00:28:34,880 --> 00:28:42,000
all sending out signals to your motor systems. So whether it's manipulating your hands, your

282
00:28:42,000 --> 00:28:48,880
eyes, your head, your speech systems, all of that stuff is getting those what we call sub-cortical

283
00:28:48,880 --> 00:28:57,600
structures. Motor systems are getting sort of input from the neocortex. And there's some sort

284
00:28:57,600 --> 00:29:03,440
of a reconciliation that has to happen. And again, sort of like a voting process. And then the

285
00:29:03,440 --> 00:29:07,920
motor system decide, okay, these are the muscles I have to move in this way to achieve the

286
00:29:09,040 --> 00:29:15,600
the goal that the neocortex is telling me. So basically there's some sort of to use reinforcement

287
00:29:15,600 --> 00:29:21,120
learning terms as an action policy. There's a lot of possible actions that could happen. And there's

288
00:29:21,120 --> 00:29:25,840
some arbiter that's figuring out, okay, what is the best thing I should do at this point in time?

289
00:29:26,880 --> 00:29:34,960
It's still not clear to me how that necessarily what that looks like from a systems perspective,

290
00:29:34,960 --> 00:29:41,200
although it does prompt this really interesting thought that kind of echoes RL or even like an

291
00:29:41,200 --> 00:29:47,200
active learning where, you know, today we collect a bunch of data, throw it at some model and train

292
00:29:47,200 --> 00:29:51,760
it. And the model doesn't really have anything to say about the data that it receives. And this is

293
00:29:51,760 --> 00:29:57,760
kind of suggesting an active learning-esque kind of inherent capability to the model where it is

294
00:29:59,520 --> 00:30:06,960
telling some downstream system what it needs to perform. Is that that sounds aspirational as

295
00:30:06,960 --> 00:30:13,760
opposed to what we're doing today? Yeah, yeah, I mean, it is, it's exactly that. It's an active

296
00:30:13,760 --> 00:30:19,840
system. And it's what brains are doing constantly. You and I are doing this right now. And so we

297
00:30:19,840 --> 00:30:28,160
are not passive systems. And so we are inherently active. And that is a large part of why we are

298
00:30:28,160 --> 00:30:34,480
so able to learn, you know, fundamentally what how our world works. And so, you know, it's a

299
00:30:34,480 --> 00:30:40,000
predictive system. So when we make, when we send out motor commands and send out actions,

300
00:30:40,000 --> 00:30:45,520
we make predictions about what we're going to see. And based on what we actually end up

301
00:30:45,520 --> 00:30:52,080
sensing, we can update our internal models using the error and the predictions. And this is an

302
00:30:52,080 --> 00:31:00,720
extremely efficient way of learning. And so, you know, and we're going to take actions that

303
00:31:00,720 --> 00:31:06,080
are going to tell us most about the world. You know, if I've seen, you know, something over and

304
00:31:06,080 --> 00:31:10,480
over again, I'm just, I'm mostly going to ignore it. I'm going to go for the novel stuff. And,

305
00:31:10,480 --> 00:31:15,120
you know, that's how I'm going to learn most quickly. So it's, you know, as opposed to seeing the

306
00:31:15,120 --> 00:31:20,240
same data, you know, over and over again, you'll, you'll really become efficient at how you learn,

307
00:31:20,240 --> 00:31:31,360
learn stuff. Yeah. Yeah. I think part of the way I asked the last question was trying to get at. Where

308
00:31:31,360 --> 00:31:38,960
are we today with this line of research? You know, granted that is research and it's trying to

309
00:31:38,960 --> 00:31:46,800
to do, you know, big things that are modeled on biology. But what indications you have that it

310
00:31:46,800 --> 00:31:52,640
is a promising direction to go to build the kind of systems that, you know, we want to build whether

311
00:31:52,640 --> 00:31:57,440
that's what today's deep learning is doing or, you know, what, you know, we want it to do in 10

312
00:31:57,440 --> 00:32:02,080
years. Yeah, it's definitely still research. I think we're making really good progress on it.

313
00:32:02,720 --> 00:32:09,840
We are focused on building sort of initial machine learning based models of these cortical columns,

314
00:32:10,800 --> 00:32:15,920
trying to figure out exactly how these reference range transformations should happen. How do you

315
00:32:15,920 --> 00:32:21,200
build up these object centric models? And then, you know, some of the issues we discussed about how do

316
00:32:21,200 --> 00:32:28,160
you, you know, take the motor signal or the action output and then translate it into movements of

317
00:32:28,160 --> 00:32:34,960
your sensor and so on. So we're making a really good, you know, progress on that, but we definitely

318
00:32:34,960 --> 00:32:40,880
don't have, you know, the full system working at. But it, you know, I don't want to put timelines on

319
00:32:40,880 --> 00:32:45,440
it, but it's something we're really excited about and hopefully we'll have stuff to announce on that

320
00:32:45,440 --> 00:32:54,080
soon. Yeah, nice. You mentioned sparsity as kind of this foundational property of the way that

321
00:32:54,080 --> 00:32:59,360
you build out these models. Can you elaborate on that a bit? Yeah, yeah, I can talk quite a bit

322
00:32:59,360 --> 00:33:04,720
about that, you know, in contrast to the cortical column stuff, which is there's still quite a bit

323
00:33:04,720 --> 00:33:09,280
to figure out and we're making good progress on it. I would say the sparsity stuff is something

324
00:33:09,280 --> 00:33:15,680
that's really practical and we've shown use, you know, a lot of use cases today on large scale

325
00:33:15,680 --> 00:33:22,960
models and small scale models and deep learning. So sparsity is basically when you have weights that

326
00:33:22,960 --> 00:33:29,440
are weight matrices or weights that are mostly zero, meaning that there's a very sparse set of

327
00:33:29,440 --> 00:33:35,600
connections between layers. In the brain, you see another type of sparsity, something we call

328
00:33:35,600 --> 00:33:42,880
activation sparsity, meaning very few neurons are firing at a time. So, you know, only about 1%

329
00:33:42,880 --> 00:33:50,400
of neurons in your cortex are about firing at a time. And so there's sort of connectivity,

330
00:33:50,400 --> 00:33:56,800
sparsity in connections and sparsity in activations. And both of those factors can actually lead to

331
00:33:56,800 --> 00:34:04,320
tremendous efficiencies in processing deep learning systems. The brain uses only about 20 or 30 watts

332
00:34:04,320 --> 00:34:10,000
of power, which is pretty amazing when you consider, you know, the billions, tens of billions of

333
00:34:10,000 --> 00:34:14,960
neurons in there. And you can compare that with today's GPU based systems, which are power,

334
00:34:14,960 --> 00:34:20,000
you can probably power a small village with some of the clusters that are out there.

335
00:34:21,600 --> 00:34:26,400
And the way that this works is basically, you know, if you have zeros in your weight matrix,

336
00:34:26,400 --> 00:34:32,320
you can skip that multiplication because you know already that multiplication is going to be zero.

337
00:34:32,320 --> 00:34:38,720
There's no point doing that multiplication. And if you have both activations that are sparse

338
00:34:38,720 --> 00:34:46,800
and weights that are sparse, you get this multiplicative effect where a fractional

339
00:34:47,680 --> 00:34:53,520
percentage of the multiplications actually have to be performed. So one way to think about this

340
00:34:53,520 --> 00:34:59,360
is suppose you have 90% weight sparsity. So only 10% of the weights are non-zero. So you could

341
00:34:59,360 --> 00:35:07,600
imagine like a 10x gain, you know, you can skip, you know, 910s of the multiplications. But if you

342
00:35:07,600 --> 00:35:14,960
also have 90% activations sparsity, only 1% of the products are going to have non-zero on both

343
00:35:14,960 --> 00:35:21,680
sides. So you can skip 100 times the computation. There's this sort of multiplicative effect that

344
00:35:21,680 --> 00:35:28,160
happens. And what we've focused a lot on is how do you translate that into deep learning systems

345
00:35:28,160 --> 00:35:33,760
that are accurate at the same time? How can you change that? That into hardware architectures

346
00:35:33,760 --> 00:35:39,440
and actual implementations that can actually exploit that efficiency? And those are both areas

347
00:35:39,440 --> 00:35:43,280
that we've started to make tremendous progress on and learn quite a bit about.

348
00:35:43,920 --> 00:35:50,240
And how do you go about approaching that? It strikes me as kind of the classical 10 cent for the

349
00:35:50,240 --> 00:35:58,800
not-and, you know, $1,000 for knowing where to put it. Yeah, exactly. That's actually a great way

350
00:35:58,800 --> 00:36:02,720
to say it because, you know, what we've been doing in deep learning is throwing more and more

351
00:36:02,720 --> 00:36:08,080
compute at the problem. And it's great for some companies that are making GPUs. It's fantastic

352
00:36:08,080 --> 00:36:12,000
for them. They just want to throw more compute at it. But it would be even better if you just

353
00:36:12,000 --> 00:36:17,840
didn't have to do the compute, right? At all. And that's what sparsity allows you to do is sort of

354
00:36:17,840 --> 00:36:26,320
knowing where to skip the compute. And so there are two challenging aspects of that. One is like,

355
00:36:26,320 --> 00:36:31,680
how do you even train networks that are accurate and but have all of these zeros flying around

356
00:36:31,680 --> 00:36:38,320
all over the place? And the second piece is how do you actually translate into hardware architectures?

357
00:36:38,880 --> 00:36:44,960
And so with the first part in training, what we have found is that there are a number of a bunch

358
00:36:44,960 --> 00:36:51,040
of different ways you can go about it. One sort of critical aspect is that the way you train

359
00:36:51,040 --> 00:36:56,800
sparse networks is different from the way you train dense networks. And in particular,

360
00:36:57,360 --> 00:37:02,080
you know, with deep learning you have to do really be careful about your parameters and how you set

361
00:37:02,080 --> 00:37:07,440
up the network and how you set up the experiments and really explore, you know, the hyperparameters

362
00:37:07,440 --> 00:37:13,680
and so on. And with sparse networks, what we found is that you need to do your own hyperparameter

363
00:37:13,680 --> 00:37:19,120
exploration in a way that's quite different from the way you do dense networks. And so we've,

364
00:37:20,640 --> 00:37:24,160
you know, that's one of the big things that we've learned coming out of this.

365
00:37:24,640 --> 00:37:32,400
What is it about sparsity versus density denseness that drives this different way that you

366
00:37:32,400 --> 00:37:36,720
need to approach it? So, you know, a lot of deep learning systems are trained using back

367
00:37:36,720 --> 00:37:43,360
propagation and back propagation inherently assumes you're in this dense and dimensional space

368
00:37:43,360 --> 00:37:48,000
and it's trying to move around this end dimensional space, whereas if you have a sparse system,

369
00:37:48,400 --> 00:37:54,240
you know, an exponentially large percentage of that space is just out of bounds. And so you just

370
00:37:54,240 --> 00:37:58,480
can't go there. And so you're really fighting against what back propagation wants to do.

371
00:37:59,040 --> 00:38:06,640
And so we've had to use a lot of tricks and hyperparameter optimization techniques. We use a technology

372
00:38:06,640 --> 00:38:13,760
from a company called Sigopt that has this sort of Bayesian hyperparameter optimization techniques

373
00:38:13,760 --> 00:38:19,600
that's worked really, really well for us. And we've used that to figure out exactly what

374
00:38:19,600 --> 00:38:25,120
combination of hyperparameters really allow back propagation based networks to effectively

375
00:38:25,120 --> 00:38:30,400
kind of navigate that space and find global optimal or something close to global optimal.

376
00:38:30,400 --> 00:38:36,960
Have you kind of measured the, like, or how do you measure even the difference between trying

377
00:38:37,680 --> 00:38:43,760
kind of standard back prop versus more of an optimization type of an approach?

378
00:38:43,760 --> 00:38:51,280
Yeah. So, so back prop, you know, you're typically computing some sort of a loss function or

379
00:38:51,280 --> 00:38:58,720
error function and you kind of measure that. What's important for us is not just that error function,

380
00:38:58,720 --> 00:39:04,320
but also other things like sparsity, like activation sparsity and stuff. So we have to measure

381
00:39:04,320 --> 00:39:11,280
multiple things at once. So it becomes a more complicated optimization process. It's not as simple

382
00:39:11,280 --> 00:39:17,920
as just finding the lowest error. You need to find the lowest error in conjunction with networks that

383
00:39:17,920 --> 00:39:22,720
are as sparse as possible from a connection standpoint and as sparse as possible from an activation

384
00:39:22,720 --> 00:39:29,440
standpoint. And so doing this sort of multimetric optimization is quite tricky and this is where

385
00:39:29,440 --> 00:39:35,440
sort of the sigop technology that I mentioned really helped us and really shines in that respect.

386
00:39:35,440 --> 00:39:40,240
So yeah, it is tricky to figure out, yeah, how do you measure it and how do you go about doing it?

387
00:39:40,240 --> 00:39:46,560
That's, there was a lot of learning that was involved in that. And is the, is the,

388
00:39:46,560 --> 00:39:55,920
this optimization process, is it telling you, is it telling you broadly, like, is it giving you

389
00:39:55,920 --> 00:40:01,680
some broad parameter around sparsity, like, you know, level of sparsity, or is it telling you

390
00:40:01,680 --> 00:40:08,960
specifically, you know, at any given time step in a training loop, like what neurons you don't

391
00:40:08,960 --> 00:40:17,520
need to worry about? Yeah, it can tell us both actually. So yeah, and so in some cases, we know

392
00:40:17,520 --> 00:40:22,240
what level of sparsity we might want in the weights, but we have a lot of freedom in how, how much

393
00:40:22,240 --> 00:40:28,160
activation sparsity can have as one example. And so you want to be able to balance accuracy versus

394
00:40:29,200 --> 00:40:37,520
sparsity in that case, and it can help guide in both areas. I think what I'm trying to reconcile is if

395
00:40:37,520 --> 00:40:46,480
if, if you're, if each of the, each of the weights, or if each of the weights is a, you know,

396
00:40:46,480 --> 00:40:53,520
a metric that you're trying to optimize the, the spaces, the dimensionality of the space is

397
00:40:53,520 --> 00:41:01,360
ridiculous. Right, right. That, is that what you're doing? Yeah, it's something we call,

398
00:41:01,360 --> 00:41:07,760
well, it's not, it's something we're doing. It's also something the brain is doing. So what's,

399
00:41:08,320 --> 00:41:13,040
what people may not realize is that the connectivity in our brain is actually not fixed.

400
00:41:13,920 --> 00:41:18,720
The neurons in our brain are constantly adding and dropping connections. So it's something we

401
00:41:18,720 --> 00:41:25,440
call dynamic sparsity. The connectivity itself is being learned, which is kind of mind boggling

402
00:41:25,440 --> 00:41:32,800
to think about. Something like, I saw a study that in the adult brain, something like 30% of the

403
00:41:32,800 --> 00:41:39,760
connections are different every few days, which is just a staggering number to think about.

404
00:41:40,800 --> 00:41:45,360
It says your brain is going to be quite different a few days from now. And what's going on is that

405
00:41:45,360 --> 00:41:50,800
neurons are constantly trying to learn new things and forget about the stuff that's no longer relevant.

406
00:41:50,800 --> 00:41:56,320
So you have your core memories that are, that are going to be stable. And then you're constantly trying

407
00:41:56,320 --> 00:42:00,560
to learn new things. And this gets back to the continuing learning thing I alluded to earlier.

408
00:42:00,560 --> 00:42:04,240
We're constantly trying to learn new things. And the brain does that by growing new connections

409
00:42:04,240 --> 00:42:10,000
really quickly. And then if something sticks, those connections will become stronger. But most of the

410
00:42:10,000 --> 00:42:14,560
stuff we see day to day are just random connections and curious connections. So those will,

411
00:42:14,560 --> 00:42:19,120
we'll drop off. So that's kind of what's going on. Now we have to translate that to deep learning

412
00:42:19,120 --> 00:42:24,560
where you have to actually learn the mask over the weights, like which weights are going to be

413
00:42:24,560 --> 00:42:31,520
on or off. And yeah, that makes the whole process quite interesting. And I'm still like, is the,

414
00:42:32,560 --> 00:42:38,560
are you learning the mask at the individual weight level or is that parameterized in some way?

415
00:42:39,600 --> 00:42:46,480
There are some smaller dimensionality of like mask patterns that you're optimizing over.

416
00:42:46,480 --> 00:42:51,920
Yeah, it's a great question actually. We just published a paper called two sparsities are

417
00:42:51,920 --> 00:42:56,480
better than one where we showed that when you think about hardware architectures, you actually

418
00:42:56,480 --> 00:43:01,200
have to be careful about the sparsity patterns because some sparsity patterns will map really

419
00:43:01,200 --> 00:43:06,480
well to the hardware and others won't. And so we've come up with some techniques called complementary

420
00:43:06,480 --> 00:43:11,040
sparsity where there's a set of patterns that map really, really efficiently to the hardware.

421
00:43:11,040 --> 00:43:17,840
And so you want your training algorithms to understand the constraints of the hardware

422
00:43:18,720 --> 00:43:23,200
in this optimization process. And if you can do all of that stuff well and balance everything

423
00:43:23,200 --> 00:43:27,600
well and still get really accurate networks, we've shown you can get actually two orders of

424
00:43:27,600 --> 00:43:32,320
magnitude improvement in performance. So you can get networks that a hundred times faster

425
00:43:32,320 --> 00:43:40,160
on the same hardware, then a corresponding dense network would be on that architecture.

426
00:43:40,160 --> 00:43:49,440
By hardware here, we're talking about GPUs or FPGAs or exactly. Yeah, so the paper we published

427
00:43:51,200 --> 00:43:56,640
was a proof of concept on FPGAs and the nice thing about FPGAs is we can really design the

428
00:43:56,640 --> 00:44:02,720
circuits to be exactly what we want. We actually think you can take the same ideas and apply them

429
00:44:02,720 --> 00:44:09,280
to CPUs and GPUs as well. There's some additional tricks that are involved, but we're making good progress

430
00:44:09,280 --> 00:44:14,640
on that. So we think, you know, sort of stepping back a little bit, we think if you can be really

431
00:44:14,640 --> 00:44:20,240
smart about where you place the zeros and what computations devoid, we think there's a potential

432
00:44:20,240 --> 00:44:25,440
of making deep learning orders of magnitude more power efficient and more compute efficient

433
00:44:25,440 --> 00:44:31,920
than it is today. And if you think about kind of the carbon impact of deep learning today,

434
00:44:31,920 --> 00:44:40,160
which is just insane, you know, it sort of behooves us as an industry to really pay attention to

435
00:44:40,160 --> 00:44:46,400
this and really improve the energy usage of our deep learning systems. It's kind of completely

436
00:44:46,400 --> 00:44:52,160
out of control today. And you're saying deep learning and just to be absolutely clear, you're talking

437
00:44:52,160 --> 00:44:59,680
about conventional deep learning for lack of a better term, as opposed to what we were talking

438
00:44:59,680 --> 00:45:05,120
about earlier, cortical columns, different neuron models, things that we're working on in the

439
00:45:05,120 --> 00:45:12,160
research. This is applying sparsity and optimization to, you know, drive greater efficiency and

440
00:45:12,160 --> 00:45:17,600
something that's roughly akin to what we're doing today. Is that fair? Yes, yes. The sparsity

441
00:45:17,600 --> 00:45:23,280
stuff could be applicable in today's deep learning networks. It seems it's becoming more and more

442
00:45:23,280 --> 00:45:27,920
clear that these sparse techniques can be applied to basically all of the network architectures

443
00:45:27,920 --> 00:45:32,960
that are out there today, whether it's, you know, convolutional systems, transformers,

444
00:45:32,960 --> 00:45:38,960
conformers, you know, ResNet, you know, all the different architectures that are common today

445
00:45:39,680 --> 00:45:44,560
can benefit from sparsity. Having said that, the cortical column

446
00:45:45,520 --> 00:45:50,720
implementations when we get there will also be sparse because the brain is sparse and then

447
00:45:50,720 --> 00:45:55,920
sparsity gives you, you know, tremendous benefits. But the good thing about the sparsity work is

448
00:45:55,920 --> 00:46:01,360
it can be actually applied today to today's deep learning systems. And is sparsity,

449
00:46:02,880 --> 00:46:08,320
is it an emergent property of the architecture or is it, you know, the use case or the data?

450
00:46:09,120 --> 00:46:15,280
Is it something that you can, you know, always apply and it has some benefit because it's

451
00:46:15,280 --> 00:46:21,200
broader is it only if your data or your problem looks a certain way? You can apply to almost any

452
00:46:21,200 --> 00:46:27,360
problem domain. Where the thing you do need is that you need networks that are larger.

453
00:46:28,080 --> 00:46:34,160
It's sort of a little bit paradoxical thing about, but you need larger networks in order to

454
00:46:34,160 --> 00:46:41,440
allow really sparse processing. And what happens mathematically is as you get larger networks,

455
00:46:41,440 --> 00:46:46,320
you get these exponentially larger spaces that you're working with. And it's so it's easier and

456
00:46:46,320 --> 00:46:54,640
easier to find solutions that can be extremely sparse in there. So even if you look at the total

457
00:46:54,640 --> 00:46:58,960
number of non-zero weights, if you have a small network, maybe you can get to, let's say,

458
00:47:00,640 --> 00:47:06,400
you know, a thousand non-zero weights at a layer, just to pick a number. If you have a larger

459
00:47:06,400 --> 00:47:13,200
network, you might actually be able to get to 500 or 100 non-zero weights. So it's it's it's

460
00:47:13,200 --> 00:47:20,000
counterintuitive, but by making the dimensionality's bigger, you can actually get by with smaller

461
00:47:20,000 --> 00:47:25,360
absolute number of weights. Yeah. Is that a property of Bayesian optimization in particular or

462
00:47:26,160 --> 00:47:31,360
something else that this is just a space? Yeah, this is just in the mathematics of sparsity.

463
00:47:31,360 --> 00:47:39,520
The way it works is that as the spaces get larger, you can get the same amount of information with

464
00:47:39,520 --> 00:47:46,240
a smaller number of weights. So it's an information theoretic result there. Now from a, I mean,

465
00:47:46,240 --> 00:47:51,200
obviously from a Bayesian parameter optimization standpoint, it just makes it even harder to

466
00:47:51,200 --> 00:47:56,400
find all these hyperparameters and create networks that can train and that's so.

467
00:47:58,000 --> 00:48:04,320
You mentioned in the application of the Bayesian optimization stuff that you were doing,

468
00:48:04,320 --> 00:48:11,200
you know, we talked about the multimetric nature of it. You also says some things that

469
00:48:12,320 --> 00:48:19,680
sounded like kind of constraint, you know, constrained optimization, which I know SIGUP does as

470
00:48:19,680 --> 00:48:26,000
well. Is that something that you're using as part of the formulation? Yeah, yeah. So with the

471
00:48:26,000 --> 00:48:31,600
multimetric, we can optimize multiple quantities simultaneously, like sparsity and error and so on,

472
00:48:31,600 --> 00:48:38,080
with the constraints, we are using constraints. So with SIGUP, you can put all of these constraints

473
00:48:38,080 --> 00:48:43,280
in your parameters. So we make sure that we stay within ranges that we know will work well.

474
00:48:45,920 --> 00:48:50,080
So for example, constraints that we know are imposed by the hardware.

475
00:48:51,920 --> 00:48:55,440
Or there's combinations of hyperparameters that just don't make sense.

476
00:48:56,320 --> 00:49:01,120
And so we want to put those constraints so that the hyperparameter optimization doesn't

477
00:49:01,120 --> 00:49:07,840
visit parts of the hyperparameter space. That's a mouthful. That just don't make sense.

478
00:49:09,200 --> 00:49:14,000
Yeah, I mean, with hyperparameter optimization, every point in the space is an entire training run.

479
00:49:14,000 --> 00:49:18,560
And so you have to be judicious in how you, you know, picking those points efficiently.

480
00:49:19,120 --> 00:49:24,480
And so the constraints, any constraints you can put in that you know will just reduce the

481
00:49:25,200 --> 00:49:28,720
space of possibilities and make the entire process a lot more efficient.

482
00:49:28,720 --> 00:49:35,360
And you mentioned transformers a couple of times. Have you applied any of the sparsity techniques

483
00:49:35,360 --> 00:49:40,720
that we're talking about to transformers and language models and, you know, some of the things

484
00:49:40,720 --> 00:49:45,760
that are potentially causing, you know, the large environmental impact that you alluded to?

485
00:49:46,320 --> 00:49:50,480
Yeah, yeah. It's exactly because of that that we are actually spending quite a bit of time

486
00:49:50,480 --> 00:49:56,800
on transformers. They're becoming very popular. But at the same time, these are just massive

487
00:49:56,800 --> 00:50:03,280
models that are taking up consuming huge amounts of power and, you know, creating a pretty big

488
00:50:03,280 --> 00:50:09,680
negative environmental impact today. We've been, we've having quite a bit of success in sparsifying

489
00:50:09,680 --> 00:50:16,320
transformers. Because these models are larger, it's, it's, you know, it's possible to get

490
00:50:16,320 --> 00:50:20,560
quite sparse with that. So we've been able to get transformer models. We've worked with the

491
00:50:20,560 --> 00:50:28,720
BERT model, which is kind of the canonical transformer model that everyone uses as kind of a template.

492
00:50:28,720 --> 00:50:36,400
We've been able to get to 90% sparse and even higher without losing accuracy in these kind of

493
00:50:36,400 --> 00:50:42,160
models. So there's a potential of really accelerating these transformer models and making them much,

494
00:50:42,160 --> 00:50:47,040
much more power efficient. So we published a blog post on that recently and we should be

495
00:50:47,040 --> 00:50:52,560
having, you know, a lot more on that in the, in the few coming few months. We'll definitely

496
00:50:52,560 --> 00:50:59,600
include a link to that in the show notes pages. Is there a rule of thumb that 90% sparse

497
00:51:00,560 --> 00:51:09,040
translates to, you know, 50% power or something like that? We think with the right hardware,

498
00:51:09,920 --> 00:51:16,560
you know, optimizations, 90% sparse should give you more than a 10x gain. So it's not, we're not

499
00:51:16,560 --> 00:51:24,000
talking about 50%, 20%, 30%, but we're talking about orders of magnitude here. And so with 90%,

500
00:51:24,000 --> 00:51:30,880
you can skip 9 out of 10 computations, but your model also gets really smaller. So you get additional

501
00:51:30,880 --> 00:51:36,560
advantages over that. So if you, in hardware, you know, you have, you know, memory hierarchies,

502
00:51:36,560 --> 00:51:41,600
the some memories are faster than others. And a smaller model means most of your model can fit

503
00:51:41,600 --> 00:51:48,000
in these faster memory areas. And, you know, memory contention gets much smaller. So there's

504
00:51:48,000 --> 00:51:51,920
a bunch of other practical things that come in. So, you know, we eventually think that we can get

505
00:51:51,920 --> 00:52:00,640
more than a 10x for that. The brain is 95 to 98% sparse. So if we can get anywhere close to the

506
00:52:00,640 --> 00:52:05,440
levels of the brain, we're talking, and again, remember, there's multiple types of sparsities

507
00:52:05,440 --> 00:52:11,680
that, that interact and have multiplicative benefits. So we're talking multiple orders of magnitude

508
00:52:11,680 --> 00:52:18,240
efficiency gains if we can really get close to how it is in the brain. And, and, you know, I think

509
00:52:18,240 --> 00:52:23,760
a lot of deep learning researchers used to think that was impossible, but I think the brain shows

510
00:52:23,760 --> 00:52:28,480
that it's not only is it possible, it's actually the best example we have today.

511
00:52:28,480 --> 00:52:39,120
All right. I'm curious about you. We've been talking about training slash learning. What about the

512
00:52:39,120 --> 00:52:46,400
inference side of things with regards to sparsity? Yeah. So, but both inference and training will

513
00:52:46,400 --> 00:52:51,600
speed up with sparsity. We focused actually a little more on the inference side because that's the

514
00:52:51,600 --> 00:52:57,600
more kind of common use case, but training will speed up too. But the fundamental mathematical

515
00:52:57,600 --> 00:53:04,640
operations that occur during, during inference and training, they're very similar. And so they

516
00:53:04,640 --> 00:53:10,880
should both benefit from this. Awesome. Awesome. But we've covered a ton of ground. What are you

517
00:53:10,880 --> 00:53:17,440
most excited about over, you know, say the next 10 months or some other arbitrary time horizon?

518
00:53:18,960 --> 00:53:23,520
Yeah. What's, you know, there's this big question in the machine learning community, like

519
00:53:23,520 --> 00:53:31,920
can brain inspired understandings actually impact machine learning positively. And I think I'm

520
00:53:31,920 --> 00:53:39,200
really excited that the stuff that we've mentioned is now coming into real practical domain. And

521
00:53:39,200 --> 00:53:45,680
with sparsity, I think 10 months from now or year from now, we'll see really dramatic improvements in

522
00:53:45,680 --> 00:53:51,280
what we can do from an efficiency standpoint. And I'm really hopeful in that time frame that we

523
00:53:51,280 --> 00:53:55,440
will be able to take many more of the ideas of the cortical column and start showing really

524
00:53:55,440 --> 00:54:04,000
concrete hard core benefits to machine learning systems. And to really showcase that we can take,

525
00:54:05,600 --> 00:54:11,440
understanding the brain is not just an academic scientific exercise. It can actually have practical

526
00:54:11,440 --> 00:54:16,400
engineering benefits. And to me, that's what I'm really passionate about as a computer scientist.

527
00:54:16,400 --> 00:54:21,440
And hopefully, you know, year from now will be in a dramatically better space with respect to all

528
00:54:21,440 --> 00:54:27,920
that than we are now. Fantastic. Fantastic. Well, Subatai, thanks so much for taking the time to

529
00:54:27,920 --> 00:54:31,840
share a bit about what you're working on. Very cool stuff. Yeah, thank you so much, Sam. It was

530
00:54:31,840 --> 00:54:36,560
a pleasure talking in there. There's a great question. And I'm glad we got it. Thank you

531
00:54:36,560 --> 00:54:46,960
going to a lot of the detail on that. Absolutely. Thank you.

