Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting
people, doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
I want to send a quick thanks to our friends at Cloud Era for their sponsorship of this
series of podcasts from the Stratidata conference, which they present along with O'Reilly media.
Cloud Era is long been a supporter of the podcast.
In fact, they sponsored the very first episode of Twimble Talk back in 2016.
Since that time, Cloud Era has continued to invest in and build out its platform, which
already securely hosts huge volumes of enterprise data, to provide enterprise customers with
a modern environment for machine learning and analytics that works both in the cloud
as well as in the data center.
In addition, Cloud Era Fast Forward Labs provides research and expert guidance that helps enterprises
understand the realities of building with AI technologies without needing the higher
and in-house research team.
To learn more about what the company is up to and how they can help, visit Cloud Era's
machine learning resource center at cloudera.com slash ml.
All right, everyone.
I am on the line with Eric Colson.
Eric is the chief algorithms officer at StitchFix.
And Eric and I are here today to talk a little bit about some of the work the company's
done around algorithms and organizing for success in data science.
And I suspect some other stuff will come up as well.
Eric, welcome to this week in machine learning and AI.
It's fun to be here with you and thanks for having me on board.
Absolutely.
Absolutely.
Why don't we start by learning a little bit about your background.
You spent what, five or six years at Netflix before joining StitchFix.
How did you kind of get started in data science and machine learning and kind of work your
way to Netflix and StitchFix?
I don't know.
Okay.
You want to go way back.
Well, let's see.
To be honest, data science is kind of what I wanted to do from the very beginning, even
back 20 years ago, undergrad days, I'm not the closer to 25 now, that we didn't have
the word data science back then.
But when I majored in my undergrad was economics, and particularly micro economics was a passion
of mine.
I thought this is something that I wanted to do for companies, right?
Go in there and it seems like such a, what I call a noble way to compete, to be used
data and statistics and things to operate a business better more efficiently.
And that's a win-win because it would be better for the consumers better for the company
hence a noble way to compete.
And I thought that'd be great.
And so I learned as much as I could in my micro economics, I maybe took a few courses
in statistics, but that was about it.
I graduated and I got into the industry, into companies, I got a gig consulting, helping
companies do this.
And it turns out that micro economic stuff was pretty much all theory.
Nobody was doing this stuff yet.
You talk about being born at the right time, however, you know, right when I got out
of undergrad was right when things were starting to turn.
The stuff was still highly theoretical, but data was becoming available and the cost
of the process it was becoming affordable.
And so now you could actually start to do these things.
Again, these were represented as graphs in economics textbooks.
Nobody actually really did them.
They were just kind of conceptual, but for the first time ever it was becoming possible.
So it was kind of fun to be, you know, entering the workforce at exactly that moment where
I got to do something that the time was very novel.
But I learned sort of the hard way that economics did not properly prepare me for this.
The required skills were sort of distributed into several fields.
I mentioned statistics, there's also computer science and then more of a business domain
expertise type of thing.
And I found out that economics alone was just completely impractical.
You needed, you know, the statistics to actually quantify the exact curves and the shapes
of those curves and where points intersect, so we find the top of a curve that type of
thing.
And then to do anything, you need a computer science to process that data to allow those statistics
to compute, et cetera.
And then of course you needed domain expertise in the business, which I got through my consulting
path.
But my whole career has been that quest to kind of combine those three things all in the
quest to do these micro economic type of models to put them into production and make them
a reality.
So that's my background.
It goes back, you know, 25 years ago consulting and then, you know, going back to school to
get better computer science skills and better statistic skills, et cetera.
And sort of meandered across business domains to consulting later into the world of big
data when I time at Yahoo or learning how to process petabytes of data.
And then later when I got to Netflix actually putting things into production through algorithms
actually having, you know, systems that would take automated action on things to actually
do things in response to these micro economic trends and so forth.
And so that was kind of my amalgam of stuff that I had to come together with.
And then my latest thing is figuring out how to organize for this.
So it's been a long road with a lot of things coming together nicely and again completely
unplanned by my part.
This was all just a silly quest to do these micro economic things.
But along the path I got meds into different directions to acquire different skills and
the experiences that kind of have all now culminated into where I am today.
But you talk about micro economics and algorithms, at least the kinds of algorithms that we tend
to see at companies like Yahoo and Netflix and StitchFix as almost one in the same.
But I never hear that characterization as you know what we're trying to do is put micro
economics into practice.
You suppose there's any kind of unique perspective that your passion for micro economics and
background in that field that you bring to data science?
Yeah, because you're corrected.
I believe the things like recommendation systems really were born out of schools of computer
science as opposed to economics.
But if you think about what they're doing is they're making an organization more efficient
right, which is really micro economics is the classic micro economics which are the things
that are in my textbooks where things like pricing efficiency.
Figure out your base price elasticity and setting prices appropriately or more from the
school of operations research how to run your operations more efficiently.
And they all use the same concepts from math and statistics to and leverage data to materialize
those concepts and then you take action them through automated means or algorithms.
So all those things kind of I put under this kind of superheading of micro economics.
And if those textbooks were written now, I think they could include things like recommendation
systems because they are doing a similar thing.
They're leveraging a company's internal asset data in order to be more efficient figuring
out what to show different customers and say.
You've been at Citrix for how long now?
Coming up on seven years now.
Yeah, it's a little I you know I first started actually as an advisor to the company and
that was a little over seven years ago.
I was at Netflix prior to this.
So it was the VP of data science and engineering at Netflix for six or six and a half years.
No interest in actually leaving Netflix, but I got a call and you get calls a lot, but
Netflix, you're pretty busy.
You work a lot of hours is hard and the company is a fast pace.
And so you don't really participate in a lot of things outside of Netflix, so things
like advising.
So I would pretty quickly turn these things down and wouldn't even listen to most of them.
But I do remember receiving a particular call from a venture capitalist was in the parking
of Netflix trying to leave to go home.
And I took the call and he told me about Citrix and its founder Katrina Lake, who's trying
to do this interesting thing with clothing.
And instantly it struck a chord.
It was sort of a fortuitous timing that very day, some of the meetings I walked out of
earlier that day, we were stumbling on some problems, things like more opportunities
I should call them, you know, we talked about our recommendations as the meta Netflix.
And you know, at the time most of the recommendations were there on the website as well as in the
mobile app.
But what we would do is we'd cast a wide net with the recommendations, because we're
not that confident in what somebody wants to watch a meta predict for a moment.
So on the website, you might be met with a hundred recommendations that want a hundred
box shots of different movies and might be interested in.
We used to joke.
There's a product manager still at Netflix today that would joke, you know, if we are
more confident in our recommendations, we wouldn't show so many a hundred.
We would show maybe like five or even one if we're super confident in our recommendations,
we could just play the very thing the person wants to see.
And you can imagine that you open up the app and the very thing you wanted to see just
starts playing.
Now that would be bold.
All right, so this was a conversation we had just had and I get the call about Citrix,
which is doing this very thing, but with clothing, right?
They're going to be so bold is not to actually just recommend the clothes to you, but they're
rather going to ship them to your door side unseen.
So I found that was interesting and, you know, hearing about their business model was
very interesting.
And you know, again, they caught me at the right timing that, you know, this sounds like
something that I'd like to participate in, not as an employee.
That sounds crazy.
I'm not going to leave Netflix for this, you know, risky startup, but I'll be an advisor.
And so I, well, even that didn't happen immediately.
I had the first research Katrina, the founder, our CEO, which is very impressive on paper,
but she hadn't run a company before.
This was her first startup.
And so, you know, I was actually even reluctant to even take the meeting with her because
of time, I just never had time.
But we finally ended up meeting up in San Francisco and we hit it off and once I met her
in person, I realized there's a lot more thought put into this than I had first believed.
And, you know, I was very inspired by her.
And anyways, we made it happen.
I became an advisor to the company.
I still wasn't going to joke.
And I last said, merely four or five months as a big advisor, one of the conditions of
my advising is I needed access to the data.
I wanted to be able to see what's going on in there and how, you know, the quality of
the data, how predictive it can be, and what it could be useful with.
And she complied and was able to access the data and I looked at things.
And I never seen data like this before.
It was like straight out of the textbook, those old, like those microeconomic textbooks,
things that were supposed to be theory, where it's clear as day, the curves looked like
the curves they're supposed to be had gouches, you had, you know, power laws, you had all
the things right there staring at the face, you didn't even have to do much to reveal
that we just kind of query the data in there, it was presented to you and a lot of this
is fun.
There's a lot of predictive power once you have that data that's so well behaved and I
loved explaining the novel and so I get hooked on this data pretty quickly over my four
months of the advisor.
I'm supposed to be spending like an hour a month with the company.
That's my contract.
I was spending more like 25 hours a month, just tinkering and again, I'm a busy guy.
I had things going on at Netflix, but I found the time to tinker with this stuff and I
got hooked.
The other thing that happened over this four or five months as an advisor is I had my
wife try the service.
It was women only at the time, so I had my wife try it and a few of the ladies in my neighborhood
try.
Of course, they try it the first time because I asked them to, but I watched them and I
watched how the anticipation when they're waiting to receive their fix, their shipment
and I watched the joy as they open these things and I watched how they talked about how
it made them feel and then I watched most importantly how it changed their behaviors.
All of a sudden they stop going to stores.
This becomes their primary means of shopping and this is something Katrina tried to tell
me from the very beginning to say, we're going to change the way people shop and I thought
that was just a founder aspiration and I go, that's great, you should believe that because
you're the founder.
I don't want to believe it because I'm just an advisor, but I found that she was not
just one smoke, she meant it and it took me five months and staring at data to be convinced
of it and seeing some anecdotal examples of my wife and neighborhood ladies to really
find it compelling.
I said, wow, she's onto something and that's when I approached Katrina, who's August of 2012
is, hey, Katrina, if you'll have me in as an employee, I'd love to do this.
We figured out how to make that work and it's been a fun ride ever since.
Oh, what a great story.
So since then, you've done quite a bit with algorithms and data science at StitchFix.
Can you kind of give us an overview of the different ways that data science plays out
within StitchFix?
Yeah, so StitchFix, the fun thing is, is we found these applications of data science
throughout the company.
So what had originally started is the most ostensible thing we could do is a recommendation
system, right?
We're going to be picking out clothes for people and we're going to be paying, you know,
this is the start contrast to Netflix.
Netflix, we recommend things just digitally, bits and bytes, we recommend things through
a web page or a map, but with StitchFix, we're actually going to send it to you, right?
So we need to pay the shipping to get it to your house and that is expensive, right?
So we got the shipping in both directions we're paying for, plus that cost of the physical
inventory being unavailable to other clients for a period of time, right?
So this is getting expensive, plus we have a much bigger investment on the part of our
client, right?
So our client is expecting a very relevant box of clothes.
She's going to be very upset if we get this wrong.
Contrast to Netflix, people, you know, there's going to be recommendations that happen, but
it's sort of a shoulder shrug.
You move on, you know, as a consumer or a client at Netflix, you know, that's weird.
Why are you recommending that?
And you move on.
And on Netflix, I didn't really cost them much on that, at least on the margin for an
incremental bad recommendation, not a big deal.
Of course, an aggregate, it could be pretty bad, but on the margin, not a big deal.
So Citrix has this interesting nuance that, oh, we're actually, we have bigger penalties
for getting it wrong.
And it turns out this is a good thing.
This makes you pay attention as an algorithm developer.
You want a penalty so that you really have to, you have the screen incentive to get it
right.
You don't want the penalties to be too high because then you won't take any risk.
And that's why you get them like financial loans, right?
They don't want a single default that ever happened because while the defaults will
probably evoke great learnings, it's still very costly, it could be $50,000.
So Citrix is right in a sweet spot with our recommendations, the penalties are severe.
You don't want to have many of them, but not so severe they're not going to take any
risk.
You want to learn, they'll do those things.
So okay, this is interesting now.
And so we started, you know, again, this was the most obvious thing we can do is start
to build a recommendation engine.
And we've done that.
We did that from the very beginning and it was great.
We started out with very simple methods and started getting more and more leverage.
Of course, we have a new ones with our humans in the loop.
We also have stylists because there's things that machines can't do, things like empathizing
and relating to other humans, right?
So we still need that, but we need machines to do all the things that are more quantitative
and paracol.
And so there's this whole challenge of combining those two different process.
There's machine hardware with human hardware.
And so we've figured out how to do that over the years.
But that was just our initial foot in the water on algorithms.
We quickly learned that there's a whole bunch of other things we can do.
And things like inventory management algorithms, things like demand forecasting algorithms,
things like escalation algorithms.
Or even, we even have algorithms that are designing the closed now.
So the algorithms proliferated and we can talk about how that happens in a pit.
But we now have several dozen algorithmic capabilities that are running various parts
of the company.
And not much of it was planned for.
They all sort of emerged kind of naturally.
And so I can't take credit for planning this all out.
This was having the right people around and the right motivations for them to figure
this stuff out, not eating without being asked to.
So a lot of these things that you'll see on that algorithms tour, if you go check that
out, they just emerged on their own, not driven by top down processes.
And that's the conversation that we'll get into in a bit.
But I want the main thing to convey now is that it's not just the recommendation system.
We have over a hundred algorithm developers at StitchFix and we only have about seven
that are working on the recommendation system.
And the other 93 are doing other types of algorithms that are equally important.
They're just a little bit more behind the scenes.
They're not ostensible to the outside world as much as our recommendation system is.
And you mentioned the algorithms tour.
That's a post that you put up on the StitchFix blog a couple of years back.
And we will link to that in the show notes so folks can check it out.
It's definitely pretty interesting.
But when you think about these hundreds of algorithms, how do you think about them from
a portfolio and value perspective?
Is the recommendation system kind of the big value driver and then you've got a bunch
of low hanging fruit smaller systems or are there other kind of big value drivers in there?
How do you think about that algorithmic landscape, if you will?
Well, you can see.
So there are ones that are equally valuable to even a recommendation system.
And there are smaller ones, bigger ones, etc.
So they're kind of all of them out.
Now they are nicely, most of the time, nicely quantifiable.
You can do things like A, B, Testum to find out what would happen in the absence of their
presence and you can find out what they're worth to the company.
That's how most of them emerge as we can say things like, we develop this new algorithm.
And if we use it, it will generate X.
So it becomes what we call no-brainer.
There's no cost or the cost has already been born and we know the value.
That's what you call no-brainer.
Of course, we're going to push that to production and get the benefits.
To give you a little more context of some of the value it's driving, so you have things
like inventory algorithms that manage the inventory or buying algorithms.
So we do hold inventory.
So what happens is we buy things at wholesale, sell them at retail, right?
Same business model that's existed for hundreds of years, but we do it more efficiently.
So when we buy things at wholesale, we need to decide what to go buy.
And we crudely break it into two classes of purchases.
We'll be called rebuys, things that we've had some experience before, such as dresses
and jeans and things that we've actually tried out in the past, and we've got data on
it now.
So those things can be managed algorithmically.
And so we have an algorithm that says what to go buy.
And that is tremendously valuable.
I hadn't had an appreciation of this before working in retail.
I've heard from others that hadn't worked in retail about what a challenge it says,
but I didn't have the appreciation until I got to live and breathe it.
Buying is tricky.
Buying your merchandise is one of the most critical things you can do for a company, right?
This is your big capital outlay.
You're going to buy this inventory and then you're going to sell it.
And if it doesn't sell, you're stocked with a lot of inventory.
And if you buy the wrong sizes of stuff, it could be way off, right?
So most companies, most retailers buy this kind of standard distribution of sizes, you
know, you have things like extra small, small, medium, large, and extra large.
And you get like a bell shaped curve over that, you know, the quantity you're going to
buy.
But it turns out that that's not necessarily what's going to get sold.
And so rather than buy in a traditional bell shaped curve, we'll buy in the distributions
that are clients exhibit.
So in some cases, we may choose to get no extra large at all in some particular bios
because we've learned that it doesn't fit well.
Or we'll buy twice as many smalls as mediums because we know that our clients will demonstrate
that behavior.
And so this is really a lot more efficient when you can buy the right things that you know
we're going to sell in the right amount of time.
And so again, we use a myriad of algorithms and some of them were borrowed from the area
of Operation Research to figure out how frequently to buy things, what quantities to buy in,
and what overall distributions of what we call an assortment to buy.
And this makes a humongous difference in providing value to our clients and to our economic
sense, the checks.
Now as you describe these hundreds of algorithms or 800 algorithms constantly at work at Stitch
Fix, I'm thinking of, it calls to mind for me, you know, these hundreds of individual
applications and is there a common operating system that all of these applications run
on?
Can you speak a little bit to the way you support the different algorithmic efforts from
a technology perspective?
Right.
So that is a critical point of success for any algorithms team is to have a great algorithms
platform team.
And so we have that, they're also part of the department, right, so they're not a separate
team, they're within us and they call them data platform or algorithms platform.
And these folks are a little bit more computer science oriented.
They're great generalizers, they'll build things that run that algorithm or that algorithm
or that one, right.
So they know how to generalize things.
They also build the infrastructure that runs all the algorithms as well as other tooling
like, you know, job schedules and job sequencers, things that will handle data distribution.
What they're doing is encapsulating all the things that are more computer sciencey in
nature so that the data scientists don't need to worry about them as much, right.
They can go focus most of their time on science and statistics and math and they don't
need to know the innards of containerization, for example, or parallel processing.
So that can be encapsulated for them so they can get back to working on science.
And so that has been a wonderful compliment.
And again, having them as part of the same team is key.
That way they can adopt the same priorities and values.
And we come up with a much better solution, sort of a mantra we use at Cishfix, an algorithms
team is no handoffs.
So we don't want to hand off things between teams and so we want to build and design the
rules for autonomy.
And so when you think about our algorithms platform, a lot of people mistakenly think, oh,
you know, I get it.
They're the data engineers that build the pipelines for the data scientists.
Like, no, that's not true.
They build the data platform.
The data scientists have to build their own data pipelines, but they can run them on
the platform.
So it is a platform of a mature sense.
And it's all homegrown.
We're, of course, in the cloud and AWS and we've, you know, we've found as much as
we can promote this community spark and all the usual players are in there.
And then we've augmented with our own stuff as we've needed to.
And so that was maybe a side door segue and they're talking about organizations and the
way you organize data science, a part of that being have a team that supports them from
a platform perspective.
What are some of the other things that you've learned from an organizational perspective
that have contributed to the success of the data science team there?
Yeah, there's a few things that we've learned over the years.
Roughly, I'll put them into three big buckets.
They are have your own department, reporting to the CEO.
Second, when it comes down to individual roles of the data scientist, we tend to leverage
what we call data science generalist or full stack data scientist.
And then finally is more of a statement about how process we prefer a more bottoms of
approach versus central planning.
So we can go into each of those.
Let's start with our own department, reporting to the CEO.
Now this isn't something I would say I would endorse for every company, but when data
science is part of your competitive differentiation, then you want to take it seriously and give
it its own department, reporting to the CEO, whatever your differentiation is, you want
to make it its own department, reporting to the CEO.
Because in that way, you get to develop your own tooling and workflows, ways of working
right.
Data science is not like any other field, right?
It's not like engineering.
It's not like marketing or product.
It has its own way of working.
The biggest distinction is that most of its capabilities when you design a data product
or an algorithmic product, it usually can't be designed.
Up front, it needs to be learned as you go.
And this warrants a very different work style, and it's one that just doesn't fit into
a lot of engineering workflows or marketing or others.
And so if it's tucked, if you have your data science team tucked under a different department
like that, it'll be forced to inherit the work style of the parent organization.
And so that won't work for data science.
I think it needs to have its own ethos and ways of working.
And so you've got to make it its own.
Now, to be clear, when you say these projects need to be learned, we're not talking strictly
about machine learning.
We're talking about learning in the broader sense.
Is that right?
That's correct.
I mean, even inherent to machine learning, there is uncertainty, right?
You don't know if this thing's going to work or not.
You may think, well, gosh, it'd be great if we can develop an algorithm to do whatever
and you may find that at least what the data you have, there's no predicted power.
So it's not going to help, right?
So that does happen.
But even when you have success, you found predictive capabilities within your data and
you start to build your algorithm.
Oftentimes, each individual parameter or even the type of model or even the hyper parameters
that all need to be learned, right?
You can't design them up front.
You get any to try things in the data and see what it tells you and then iterate and
go back.
Often, there's a lot of feature engineering that is done through trial and error.
Like you make your best attempt in your first pass and then you learn something by looking
at the results of trying that feature in a model and you're saying, wow, it must be
something different than I had originally thought.
I'm going to try it this way now and you keep going and iterating.
And then you usually find a bunch that doesn't know the things that you didn't anticipate
trying at all in the process, right?
So it's a heavy iterative process to develop an algorithm.
It's not something that you spec out and hand off to somebody else.
It's a, here you go.
Here's your algorithm.
I'll just go implement it.
Now, you have to kind of learn it as you go.
And so this is something that when you have a property like that where it can't be designed
up front, then you're going to want to organize very differently and that's where
we get into that full stack data scientist.
The full stack data science thing is a, you know, it's sort of a, it's a generous role
as opposed to way a lot of other companies are doing things specializing their data science
teams.
We got like data engineering or ML engineering or inference engineer or research scientist
and each plays a smaller part and a big collection of capability where all those things need
to come together as a single capability.
But you got the different pieces farmed out to different specialists and it, we do that
because it makes a lot of sense to us.
We learned from Adam Smith and we were taught that, oh, you know, we get these process efficiencies
due to the division of labor and it sounds really good.
But how many works when you know exactly what it is you're building, right?
And you have specs that are crystal clear down to the millimeter of precision, then you
can divide and conquer like that.
But when you need to learn as you go, you have to rely on iteration and the challenge
with specialists is they have a high coordination cost.
It's a lot more expensive to coordinate in multiple people than it is just one.
Even worse, even more nefarious than those coordination costs are a wait times.
This is the time in between work.
So let's suppose you have a data engineer that builds a new data pipeline and, you know,
maybe a research scientist is now going to construct a model from that data.
And the research scientist says, well, she discovers that, oh, she needs a few more features
that are not manifest in the data.
So she goes back to the data engineers and you add these things in.
Data engineer says, yes, I can, but I'm busy on a different project right now.
I'll get back to you next week.
And so a week goes by and it may have only been a few hours of work to add the new features,
but a week goes by.
So that's a week of wait time for what is just a few hours of work.
And that is expensive.
And that's the cost of specialization is you're coordinating these disparate resources that
all work on other projects because they're specialists and you end up with long gaps in
between the work.
And it's very costly, especially when you benefit so much from learning and iteration.
So that's the reason we don't do the data science specialist roles instead.
We do the full stack data scientists that can take the initiative from conception, you
know, coming up with the idea and framing the problem to doing the modeling, to doing
the data engineering, to even putting it in production.
One person to or one are very few people to go through all those steps.
We find is a lot more effective than dealing with other coordination costs than wait times.
One of the perhaps most prominent trends in organizing data science efforts, and you spoke
to this, but is that aspect of specialization?
I feel like, you know, when we first started talking about this, whatever, 10 years ago,
someone was looking for kind of this unicorn data scientist, and maybe you can talk about
if that's the same or different than a full stack data scientist, but everyone was looking
for this unicorn that knew the business, knew the stats, knew how to code.
And in a lot of ways, I think part of the progress that we've made is splitting out these skill
sets and allowing for some of that specialization so that, you know, we don't have to find these
kind of Swiss Army knife ninja, whatever's that can do everything.
And as the most recent element of that, you know, we've seen the, you know, for the past
two years, this role of machine learning engineer has really kind of taken off.
And that is someone that kind of knows the machine learning stuff, you know, well not
as much as maybe a research scientist or someone, or not as deeply as a research scientist
or someone that has, you know, the statistical background, but knows how to build systems
with it and can also code, you know, in a production ready way.
And you know, that seems to have driven a lot of the scale at some of the companies that
are really heavily invested in machine learning and data science.
And I'm wondering are there things that you, in particular that you kind of attribute
your different way of seeing things?
Is it an issue of scale or your team smaller than, you know, the teams at some of the companies
that are taking a more specialized approach or, you know, is it just a fundamentally different
way of looking at kind of the efficiencies of an organization?
Yeah.
Well, first of all, yes, your unicorn analogy is correct.
Those are the type of people we look for.
They can, you know, frame the problems, have enough business context and speak business
enough to be credible, but also do the math and the statistics to build and select models
and train models, but also put them into production, right?
We don't like handoffs between those rules, because it slows us down.
So we do hire unicorns, now we've been adding that unicorns do exist.
It's just that they're a little horn.
It's not always visible in early stages development, you can train people, right?
So we can hire folks, very smart folks, that are willing to learn these things.
And it's sort of a trait that we look for.
It's a tough one to really identify in interviews, but you can pull out of them in clever ways,
asking them about a time where they've had to go way outside of their, you know, purview
to get something done.
And we look for that.
We've hired a lot of physicists, for example, that have gone way outside of their purview
to get something done.
You know, they might have to go their job is to, you know, get satellite imagery.
And because they didn't have any, you know, support to process the data, they took it upon
themselves to build structures and packages to make the processing more efficient.
Or because, you know, they needed to more machines than their organization was providing
them with.
They took it upon themselves to go get into the cloud and provision their own machines,
right?
So these are examples of folks that will do whatever it takes to get that thing up and
running.
And that's what we look for, and that's what we provide them with all the tools that
they need to be effective at doing an end-to-end solution.
Now we're not crazy here.
We do know that there are boundary conditions to this, and we're well aware of them.
We do benefit from a few things at StitchFix.
Number one, our data is not particularly among us.
We deal with terabytes of data, not pediments.
And this makes it much more feasible to have these unicorn types do, you know, code well
enough to, you know, do their own data pipelines, as well as implement them in a production
framework, et cetera.
Now if there is a point where the data gets so big that you really need to get highly
specialized, you know, you can no longer do this stuff in Python, you have to resort to
C++ or heavily typed languages where the processing could be a lot more efficient.
We're not in that place.
We deal with terabytes, not pedabytes, and so we can get away with this for that reason.
That's the data volume effect.
The other thing we benefit is availability requirements.
So we do have the gamut.
We do have some algorithms that are extremely highly available, or need to be extremely
highly available, very high SLAs.
Others are low availability, meaning they're, like I described earlier, a buying algorithm
on our middle, go and purchase, or, you know, give out a buy sheet of what to go buy.
And that algorithm does not need to be nearly as highly available, because it's only used
internally by about 30 merchants, right?
So it spits out their buy sheets for them, and that one, you know, it could break, and
our algorithm developer can send out a note to the 30 people and says, hey, I'm on it,
it'll be back up and running in an hour, and that's fine.
So that, in that scenario, it's a much lower availability requirement, and it allows us
to be, to make judgment calls on, you know, the level of support we need, it needs to provide.
So those are two examples of boundary conditions.
Okay, our data is not that big, that you need specialization, nor is it, in some cases,
need to be so highly available that it can never fail, right?
You have different engineering requirements based on those parameters.
And so it allows us to do things different.
We also, oftentimes, don't have the same consequences as other companies were, I mentioned
our styling, our rhythm is pretty sensitive to failures, we don't want to get that one
wrong.
But failures are not so bad.
You know, in fact, I would say, we would do things differently if we were in manufacturing
or medical.
In those two domains, you want to be far more buttoned up, you don't want to do the
amount of risk taking that we do with our algorithms.
In those areas, you need to be ironclad because the cost of getting something wrong, it
could be devastating to the company.
So we're aware of these boundary conditions, and I think that's a key message to get
out there is you want to do what's appropriate for your environment.
And for us, we found that, given those different requirements, we don't need to be as highly
available in some cases, and we don't have as big a data in some cases, and we don't
have the big consequences in other cases.
And so that means we can get by with doing things and a more generalist model, which allows
us to innovate much faster.
The good side of generalist roles is you can innovate much faster, you can try things
much quicker, you can tail quicker, and also lead to successes quicker.
And it also leads to very fulfilling roles.
There's nothing more satisfying than owning something from end to end.
It gives you the three properties of job satisfaction that Dan Ping talks about in his book
drive.
You get autonomy because you are no longer depending on something else for success.
You get mastery because you know this thing from end to end and you get purpose or impact.
You get to impact the company in a very measurable way.
And all those three things combine to make it one sweet role in data science and statistics.
And you mentioned you've got this platform's team that is part of the algorithms organization.
Is that team staff with the same type of generalist could they swap out and take the
place of one of the algorithms folks that's working on a modeling problem in vice versa?
Less so why there is that is a more clear distinction and skill set.
I would say that our algorithms platform team it tends to be much better computer scientist
than our data science side of the house which tend to be in you know some of the sciences
either statistics or math or even neuroscience or some of the physics domains.
But they usually don't come from a computer science background.
And that's why they're very complimentary as well you know one team is building all the
tooling to enable data scientists to not have to worry about the internet.
And so there is a bit of a distinction there.
Now happily we've had some migration between the groups and to our surprise it's more
or so from the data science side to the data platform or our own platform side.
We thought that there would be the other way.
It seems like everybody wants to be a data scientist these days and less an out a platform
developer.
But happily we found the office that we've had more migration the other way.
Now not everyone can transition back and forth I think there has to be you know a little
bit of skills that people either have picked up or have been classically trained you know
computer science is one of those things where I think it matters that you get some real
training real academic training not just hack your way into things it can really matter
and the way you design and write code.
But happily it's we've had some malleability across those two pieces of the team.
But I guess the key distinction is that typically the data scientist that's working on a problem
isn't waiting for platform features or capabilities there.
The platform team is outside of that innovation loop for getting projects done.
That's right.
There's no handoffs between the teams right you're not going to data scientists isn't going
to go to a data platform engineer and say here's what I need from you right that data
rather than the short term hopefully there's some long term communication there.
Long term what happens actually is it's the data platform that figured out what needs
to be built.
They build all the things that aren't asked for.
So nobody asked for you know a containerization package they just realized you know that's
probably hard for the data scientists to do that we should do that for them.
Well nobody asked for you know a distributed processing engine you know the data platform
folks just observed the yeah the running of some issues let's take care of that for
them right.
They do all these wonderful things by just being good listeners good observers and build
what's not asked for but it's desperately going to be needed.
And so that's how things get done is we just keep people very closely working together.
They do have some different seal sets and they watch for where they can add value.
And so you talked about three kind of characteristics that the way you organize and we've touched
on two of them the third is around kind of supporting emergent behaviors in the organization
what does that mean.
Yeah so you know I mentioned we have several dozen algorithmic capabilities and I can only
think of one of them that was actually asked for all the others were emergent meaning
they came from data science tinkering.
So what that means is we have data scientists and we hire them to do something they have
some stated priorities and they'll be working on that stuff.
And along the way they'll come across some data that was curious or interesting to them
you know not what they're supposed to be doing but they stumble and say I wonder why
that turned out to be as big as it was.
And they might follow that path and go explore it and then so doing they might trip across
some unexplained phenomenon and it leads to the development of a new capability you know
just at nobody asked them to do it just curiosity ensued.
And the next thing you know they've discovered you know the next capability that we're going
to be leveraging.
In example being we have somebody named Dara he was working on these purchasing algorithms
and as a side project he sort of tinkered and he said I wonder if I could use an algorithm
to design clothing.
And so then he started tinkering and figured out that this looks pretty good that if he
used this form of genetic algorithm he can recombine old styles together in new ways
that nobody's ever seen before to create something new.
And you know we're all kind of skeptical of it but the nice thing with data is it comes
with evidence.
These aren't just opinionated ideas they come with evidence right.
So by the time he's coming to talk to us about it he's already proven it out to you
know for the most part you know we have measures and statistics like AUC and RMSC
that kind of tell us we're on the right path this has predictive value and then the next
that might be to try it out in real life you know try it out on the AV test on real clients
and that'll you know either validate or reject your earlier evidence and then from that
point it becomes really easy back to that part where I call it a no-brainer it's already
been built so there's no cost to build it and you have evidence of the impact it'll have
once you launch it live and so usually it just kind of goes right into production from
that.
What I described there was an example of a success of course there's failures we have
lots of failures in fact they they outnumber the successes by some magnitude I don't know
if it's three to one or five one we've never really kept track but the thing is they're
very low cost they fail and he probably not even think to tell anybody about it because
you just tried some curiosity thing anytime I didn't go anywhere you shut it down you move
on and that's the beauty I've learned of these data products is they're extremely low
cost to explore and yet they come with this evidence that show when you're on the right
path and then you can easily even build and productionize them for barely you know cheap
they don't take a lot of upfront cost it's just somebody's tinkering and then you can
try them out you know an AV test that really get the true measure and then you know push
it live to the rest of the company if it really holds up and so low cost exploration
with evidence and then the last little bit that I think is an important thing is these asymmetric
outcomes right to cost the failures small the costs are the benefits of success are big
and so you can have that sort of losing ratio you might have you know three to one failures
to success rate and the one success will pay for all the failures and it's a way to really
kind of endorse innovation and foster rich in your organization is to you know those three
properties of low cost exploration evidence of their efficacy and asymmetric outcomes.
And so to maybe wrap things up you know much of the way you've talked about these organizational
principles is unconventional in the well I don't yeah it's even unconventional the right word
here in the sense of all the stuff is new and I don't know that there's necessarily a convention
but it's it certainly goes against the direction that things are headed you know if someone
out there has heard what you are describing and is in or runs an organization that is not organized
like this at all but is kind of interested in these ideas any advice for them. Yeah you really
got to be thoughtful about your environment you know no way of working is necessarily better than
any other it has to be a good fit for your environment and so you know even what we've done here
it's districts if we didn't do this from the beginning I'm not sure it would have been
the company would have been as amenable to it right we were you know here from the very beginning
we established some of these ways of working and then we had a lot of successes to build on and I
think that's what allowed us to pave the way to learn and foster the parts that worked and get rid
of the parts that didn't work so well so we have to be very reflective on that like you have to
figure out well why is there friction when we try to do this and why is there you know resistance
when we try to do this other way and figure out what it is is going on you know it's kind of like
the study of incentives within an organization figure out what works so you need to do the right
thing for your environment is one thing and then be aware of those boundary conditions right that
I mentioned you know we can are able to do this for you know various reasons of our data is not
big we don't have in most cases the availability requirements etc so you have to really be thoughtful
about this and figure out you know we've spent significant time on this debating and a
entire day off sites just to talk about org structure and process and the main credit I will give
us is we didn't do the default thing we didn't just say well let's do it we did our last companies
in fact I explicitly came to stitch fix partially for that reason I mentioned all that all my
rational for joining but there was one other nice thing and that was that I'd made a lot of
mistakes in my press career at Netflix and Yahoo or you know they weren't really mistakes until
hindsight gave me the clarity I needed I looked back and said gosh if I could start over I would
do things differently at all these companies and sometimes it's hard to start over at your existing
place because you just have a lot of legacy code you need to support and so forth but I remember
that when I went to stitch fix I said wow I have a clean slate right now I'm going to leverage
everything I learned in my career and do this very intentionally from the beginning and set up
all the stuff that I thought would always be a good idea that I learned only learned you know
three years of experience and so that thoughtfulness I think is what brought on a lot of this and
again not I couldn't claim to have foreseen everything that we've done it was just a matter of
keeping our eyes open and keep you know a good group of people that I was fortunate enough to
hire and have them to debate things with and you know collaborate with on setting up the right
structures and incentives to do this you know way there was a hundred percent appropriate for our
environment and I think that paid off pretty well so don't ever underestimate the amount of
thought and work it takes it takes to put something like this together and don't accept the defaults
don't just do it like you did at your last company do it intentionally correct for your environment
well Eric thanks so much for taking the time to chat with me about what you've been up to
I think there are really some interesting learnings in here for folks that are on their own
processes of or their own paths of organizing for data science sounds good well thanks for having
me on it's been fun all right everyone that's our show for today if you like what you've heard
here please do us a favor and tell your friends about the show and if you haven't already
hit the subscribe button yourself make sure to do so so you won't miss any of the great episodes
we've got in store for you for more information on any of the shows in our strata data series
visit twomolei.com slash strata sf19 thanks once again to cloud error for sponsoring this series
be sure to check them out at cloud error.com slash ml as always thanks so much for listening
and catch you next time
