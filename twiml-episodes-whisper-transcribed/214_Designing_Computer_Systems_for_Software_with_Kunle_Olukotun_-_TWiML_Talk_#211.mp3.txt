Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting
people, doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington. A couple of weeks ago, I had the pleasure of attending
the NURPS conference in Montreal. I had a wonderful time there and of course, the highlight
was the chance to meet a ton of Tummel listeners. I held two fun meetups at the conference. The
first was focused on AI and production and AI platforms and infrastructure and it attracted
a pretty nice crowd. We basically took over a dumpling restaurant in Chinatown right by the
convention center. I also held a listener meetup one evening and got to hang out with listeners
from all over the country and world. Of course, I took advantage of the opportunity to sit down
with a few of the many great researchers attending and presenting the conference and this week on
the show, we're excited to share our 2018 NURPS series. In this, the first episode of the series,
we're joined by Coonlay Olucatune, professor in the department of electrical and computer science
at Stanford University and chief technologist at San Bonova Systems. Coonlay was an invited speaker
at the conference this year, presenting on designing computer systems for software 2.0.
In our conversation, we discussed various aspects of designing hardware systems for machine and
deep learning, touching on multi-core processor design, domain specific languages, and graph-based
hardware. We covered the limitations of the current crop of machine learning hardware,
such as GPUs, and peer a bit into the future as well. This was a fun one that I know you'll enjoy.
And now on to the show. All right, everyone. I am here with Coonlay Olucatune. Coonlay is a
professor of electrical engineering and computer science at Stanford University as well as the
chief technologist at San Bonova Systems. Coonlay, welcome to this week in machine learning and AI.
Thank you. So you just came from giving a talk on designing computer systems for software 2.0.
Here at NURPS in Montreal, and I am really excited about digging into the topic of your talk. But
before we do that, I'd love to learn a little bit more about your background. How did you get
started in kind of this intersection of computer systems, architecture, and machine learning?
Yeah, so I've been in computer architecture for a long time. I've been at Stanford for almost
27 years. And I had done a lot of work in computer hardware. In fact, many of the ideas that
underlie multi-core microprocessors were developed in my lab in the mid-90s before they became
mainstream. And I did a company that started up. It was called a Farrow Web Systems. And we did some
high-throughput processes for the data center. And it got acquired by a Sun Microsystems.
Subsequently, Sun was acquired by Oracle. And all the big spark servers were basically based
on the technology that was acquired back in 2002. So I have a long history of kind of doing
hardware. About 10 years ago, after I came back from doing the first startup, I realized that the
issue going forward wasn't so much building new hardware. It was actually getting programs to run
efficiently on that hardware and getting programs at all. And so we've gone into the right ones
run anywhere at Sun, right? Big cloud emission there. Yeah, of course the joke was right once
debug everywhere. But yeah, I mean, I think Java didn't help the cause really.
Many things worse from an efficiency point of view, right? So certainly, you could imagine that
maybe it may develop as more productive, but that's arguable too. We're still garbage collecting
to this day, right? Exactly. So anyway, about 12 years ago now, we started thinking about,
how could we kind of make a difference to the application developer? How could we make their
life easier? And you know, even go you go back to sort of 2006, 2007. And it was clear that
the world of high performance hardware, which is the world that I have spent most of my life in,
wasn't just parallel cores, right? It wasn't just multiple CPUs. It was also these new
up and coming things called GPUs. And then, of course, it wasn't just shared memory, it was also
clusters, right? And people wanted to program these things. And yet every programming,
everything, everything, every program you needed to write needed to be different for the
particular platform that you wanted to run on, right? And that, to me, seemed like a real problem
from the point of view of the software developers. And so what we decided the right approach
was to look at using domain-specific languages, right? So domain-specific languages had been around,
and then you're probably familiar with them, but let me just define them, just to make sure that
everybody understands what they are. So they, the domain-specific languages, as the name suggests,
are designed for a particular domain, right? That was a particular problem domain that you
want to solve. And so if you can give the programmer both the operators and the data types that
match that domain, then they can, you make their life a lot easier, right? Really good example
since we're talking about machine learning would be MATLAB, right? So matrix and linear algebra,
you give yourself a bunch of matrix and linear algebra operators and data types, matrices,
and vectors, and so on. And then now you can write your algorithms with linear algebra much
more easily. So, you know, so it sounds like if I can interrupt, it sounds like you consider MATLAB
to be a DSL, which suggests to me that there's a spectrum of domain specificity in the DSL world.
Right, right, right, right. Yes, they're absolutely. So, I mean, there are other examples that are
you know, different domains, sequels, and other examples, right? Different domain, but again,
MATLAB may be a bad example since you could probably use it to program one more than anything,
but the question is how efficient would, if you wanted to write an operating system using MATLAB?
A good thing to do. So there's this notion that yeah, it really should fit the problem that you're
trying to solve. But you know, the thing about MATLAB that is a good example of the kind of initial
approach of domains with the languages is that they weren't really focused on performance,
right? They focused on productivity and not performance, right? And it was almost a sense that hey,
you couldn't, you had to give up one to get the other. And so the research approach that we
took was to say, look, there's some real value by using the abstractions provided by the domain
specific languages. They're higher level. They're more declarative. You're saying what you want
rather than, you know, you know, how to get it, right? So that the example would be, you know,
MATLAB says, hey, this is matrix multiply. If you do that and see you write a bunch of loops,
right? I don't know what to do with a bunch of loops, right? But I do know what to do if you tell
me it's matrix multiply. So that raising of the abstraction level was something that could be a
driver for high performance if you knew what, if you had the right sort of compiler technology.
So I don't want to rat hole on the compiler technology that we developed, but it was really
cool and it did all sorts of things, including enable you to take a high level MATLAB like program
and run it without modification on the whole range of computing devices you might see in the data
center. So multi-core GPU cluster or any combination of the like. The running on them isn't usually the
problem. The problem is taking advantage of them. Presumably it's taking, yeah, yeah, it's taking
cool, but yeah, no, no, no, no, no, no, no, no, no, no, yeah, running, yeah, yeah, right, you can
get that one good thread and it would be much good. Yeah, no, yeah, I'm just taking on, you know,
all the apps that are running on my laptop and I just see one of the core, the core is, you know,
no, no, by running. Yes, actually getting good performance, yeah. And hopefully close to what
you would have gotten if you had written in this low-level thing. So, you know, if the low-level
programming languages were CUDA for a GPU, could you take this high level representation and get to
the performance level that you would have gotten had you written the CUDA, right? So that's what we're
that we're talking about, right?
Yeah.
Has book, thank you for clarifying.
Right.
Right.
Right.
And then, but what that got us into
was this whole realm of data analytics.
And so we started looking at how to do SQL and Spark
and machine learning.
And we fact defined a new DSL for the machine learning
well that we called optimal, you know,
this is a play on, but it was spelled O-N-O-P-T-I-M-L, right?
Of course, of course.
And it was, you know, it was 2008 when we kind of defined it.
It was so M-L hadn't really taken off then.
But it was, it was so we like to think
we were ahead of the curve, absolutely.
It sounds like it.
Yeah.
So that was kind of the impetus for kind of our playing
in this whole machine learning space, right?
OK.
There's an ocean that, hey, this, we can define DSLs.
We can do machine learning algorithms.
We can write them easily.
We could run them efficiently on a whole range of architectures.
I should note that the other piece of technology
that we kind of relied on was all these DSLs
were not what are called standalone DSLs.
So an example of a standalone DSL
would be the examples I gave before, map lab,
SQL, they're standalone, right?
An example of an embedded DSL would be something like TensorFlow.
And so all our DSLs were embedded in Scala.
And one thing that I mentioned in the talk and, you know,
part, you know, I'm an academic.
So it's all about sort of putting your stake in the ground
for a few of an idea and then seeing, you know,
what, what, you know, who came after, right?
And did they cite you?
So we did optimize this in 2008 and TensorFlow's later.
But TensorFlow, you know, if you look
at all of these data allocation machine learning,
you'll see that, of course, essentially,
you can describe them at some level of abstraction,
as a set of operators with data flow
ox between them, right?
They all look like that way.
And you define that the operators may look different,
but they fundamentally all look that way.
And so yeah, what we wanted at the end of the day
was a data flow graph.
And the question is how you get it and how you describe it.
TensorFlow takes a very explicit way of describing
your flow graph, right?
But that's not the most natural thing to do.
The most natural thing is just describe your program
and then have the underlying infrastructure extract
that graph.
And so that's what we did, right?
And so one of the slides in the talk, I said, you know,
I say, here's Kving's clustering, right?
Here's the four lines of code it takes in my DSL.
And here's TensorFlow.
It takes 20 because it's one of the four
and optimal.
What are the kinds of trying to get at the level?
So you have a group by group, you know, group the samples
by their distance to the means.
OK.
Great.
You've done it.
All of a sudden you've got clusters.
Nice.
And then the other one is OK, now go find the centroid
of the clusters and you've got your new means.
So I could describe it now.
And anybody who kind of knows the group by operator,
like you do, can drop it instantly, right?
As opposed to constructing some graph.
Right.
So that's the difference in the extract.
Yeah.
And I think that's the idea that I was getting at with these,
like the variations of domain specificity.
Like you're operating at a way higher level of abstraction.
Yeah.
And so this DSL, you said, 2008, has this continued
to be kind of central to some of the things
you've been working on since?
Or yeah.
So I mean, I think it was an early, so let me kind of just
trace the arc of what we did.
And then I'll become clear where it sits.
So yeah, that was kind of OK.
We had these DSL's embedded in Scarlet.
But our thesis was, if you want to develop a new application,
one DSL wasn't going to do the job with the reason
that you just described is they were specific to you know.
So you say, OK, now I want a SQL piece.
That means I need a SQL DSL.
I want a machine learning piece.
That means I need a machine learning DSL.
I want to graph an analog piece.
It means I need a graph DSL.
And maybe I come up with something else, right?
That you might need.
And then you say, well, I've got this application
that is going to use all of them in some way.
We're going to argue whether they're
going to use them sequentially.
And the data's going to pass between whether the things
are going to be more intertwined.
But let's suppose that you have multiple DSL's.
What you'd like to do is to be able to take
a single application, compose the multiple DSL's,
and do global optimization, right?
That was, yeah.
Yeah, OK.
So that's what you want to do.
And so in order to do that, the solution that comes to mind
is to say, OK, why don't we capture the,
since we said they're all up like graphs anyway, right?
Could we figure out some underlying representation
that's graph thankful like that?
All of these DSL's could happen, right?
So then you have dependencies, and you can parallelize.
And then you could confuse across the boundaries
of the DSL and get rid of intermediate data and data
movement, which is the scourge of any high-performance
implementation, right?
And so that was the goal, right?
To say, can we create, that was one of the goals.
The other goal was this notion that, hey,
you come up with a new domain area,
but actually developing a high-performance DSL
is a difficult problem.
So what if we could kind of remove the burden
of the high-performance apart from you
by creating a framework that allows you
to develop new DSL's?
And then leverage all of this high-performance compiler
optimization infrastructure on top of that.
So that was the goal.
And so our view was, so we developed this infrastructure.
We developed the framework.
The DSL optimal was kind of the poster child DSL wasn't
the most work into.
And at the end of the day, we figured the real value was
in the optimization framework, not in the DSL itself.
And so going forward, we said, so Scala has its issue.
I think it has peaked and gone down in terms of popularity.
It has all sorts of issues associated with the complexity
of the language and the number of developers
who are conversant with it.
And so our view was it was too difficult
to push a Scala-based DSL.
But we could imagine pushing a Scala-based compilation
infrastructure if the DSLs that you created came from someone
else.
So you can imagine to kind of fast forward to today
and you want to optimize TensorFlow, right?
So TensorFlow will give you a graph.
You take that graph.
You give it to our framework.
And then we can optimize it.
And is that theoretical idea?
Is that actually what you're doing?
That's actually what we're doing, yeah.
OK, which is a whole huge part of the system
that you don't have to worry about anymore.
You just need to take these graphs and figure out how to do it.
Right, so again, it's all about actually
getting application developers to use any particular language
as its own set of issues associated with it.
And Google's a much bigger entity than we could ever be
in pushing that sort of thing.
So we're going to wind up right on top of that
and provide the added advantage that, hey,
not only can you take TensorFlow, but you
can take PyTorch and you can take SQL and you can take whatever.
And you can change it to this representation.
I should say a little bit about the representation just
because before you do that, a quick question.
So the part of the vision here that was these very targeted
domain-specific languages, TensorFlow doesn't really get you
there.
There are some things in some areas that
might be built on top of TensorFlow that
will kind of get you there.
Or do you think it does?
So I would view TensorFlow as a domain-specific language,
where the domain again is matrix and then
around it, but it's one of the DSLs
that you might want to have a full system.
Have you kind of moved away from the idea
that folks will develop very specific DSLs for specific problem
domains or do you think that that will happen
on top of TensorFlow?
It might happen on top of TensorFlow.
It might be other languages to get developed.
You ask sort of what the arc of the research is.
I mean, when we decided that issue of what kinds of DSLs
people want to develop, that is a very domain-specific
question, right?
And the question wrapped up in what languages people want
to use and what abstractions really make sense.
And we wanted to not have to develop new languages
ourselves and get traction on those languages.
And so rather, we thought, OK, if there
is existing languages, how can we accelerate them?
And how can we provide a platform that
might cut across a bunch of existing languages?
OK.
So then, guessing that this platform serves
as both a center point for your research group at Stanford,
but also ties to what you're doing at Sambanova?
Yeah, to some of the ideas.
Definitely, Sambanova has their own platform, which
is different from other things.
But certainly, some of the core ideas.
OK.
So then we can maybe come back to more
on what Sambanova is doing.
So please continue the thread on the system.
We started talking about the connection
between the software and hardware.
And we kind of left off at optimizing the software.
So let me fast forward to more recently at about ML.
And let's talk about.
So about six years ago, Chris Ray showed up at Stanford.
And he is a database machine learning expert.
And we got chatting.
And we started working together with a bunch of joint students.
And he is a math whiz.
And he knows all about theory, which I don't do.
I was more of a hands-on builder kind of guy.
And we started working together.
And we started thinking about what one could do
with machine learning algorithms to optimize them
for modern hardware.
And he came from Wisconsin.
And one of the things he did was an idea
for training machine learning algorithms
using Tocastic Grand Accent SGD that allows them
to be paralyzed much more efficiently.
But the interesting thing about his innovation
is an innovation that nobody who didn't know anything
about the algorithm would make.
Because they would look to that and said,
if I do this, the algorithm's going to be incorrect.
Not something we want, right?
So I think to tell a little story.
So I teach at course in parallel software at Stanford.
And there's one rule that I tell the students about parallel
programming in a what's called shared memory.
Is this something your listeners would understand?
Some of them.
OK.
All right.
All right.
So programming with shared memory, first rule.
If you touch shared data, you should put synchronization
around.
You should put locks around it, right?
There's a whole set of concurrency issues.
You do, yeah.
But basically locks me.
It's like going to slow mode, right?
Essentially, that means that when you've got a lock,
there's only one problem.
What you'd like is as much parallelism as possible.
Whenever you take a lock on a piece of data,
only one processor can actually be touching the data at the time.
So a multiple processes want to touch it,
the others have to wait, right?
And that means you're slowing things down.
And just a bit of size, if we talk about locking,
what you would like to do is not use too much locking.
Because if you do too much locking,
you'll create too much overhead.
But if you don't do enough locking,
you'll create what are called data races, right?
Where you touch the data without locking.
Well, so the general conventional wisdom
is, if you touch shared data, you should take locks.
But if you actually parallelize SGD by using locking,
you do so little work based on when you take a lock
that your turn, find out the all your time
will be spent locking.
That's all you'll ever do all day long.
And that will just basically mean your program
won't run very fast in power.
So Chris Ray and his student came up
with an idea they called hog wild with an exclamation mark.
And the idea is pretty simple.
It says, throw out the locks, okay?
And you say, that can't be correct.
What you mean, I'm just going to touch the data,
willy nilly, I don't know.
So sequentially, right?
You're going to touch the data one iteration
after the next to your SGD.
Everything looks very reasonable.
Now you've got a bunch of parallel processes
which are updating the model in any fashion whatsoever.
They're getting stale data, right?
There is no sequential order to the updates that you could match.
Maybe there is, but it certainly isn't the case
that I read something in the previous iteration,
and I'm updating it that, this iteration could be,
I read it, and iterations before.
The day is already be updated,
and now I'm going to change the model based on this, right?
So all kinds of mayhem.
In other words, an idea that goes against the fiber
of any distributed computing parallel program.
Exactly, exactly, exactly, you got it.
It's anathema to anybody who knows about programming.
And you're saying, what the hell?
What is this?
But it works.
And why does it work?
You can prove it works as long as you don't delay updates too much,
and it's back to this noise argument that we were having before.
Fundamentally, Stochastic Green descent
is a stochastic algorithm, and it has a bunch of noise
associated with things, but the model's solution bounces around.
So this is just a quick note.
We were having that talk before we started recording,
so you should walk us through that again,
because it comes up, not just here,
but also when we talk about quantization.
Yeah, okay, all right, all right.
So there's no notion about these algorithms.
Stochastic Green is sent in, in particular,
which is the workhorse of machine learning training.
And, you know, the basic idea, as you all know,
is maybe some of you know, is you've got a big data set.
You take one of the elements of data set,
you estimate the gradient and you move in the opposite direction,
and you update the model based on that.
Now, if you have lots of processes,
lots of threads that are doing this,
then what you'd like to do is you'd like to lock each piece
of the model as you update it,
so you make sure that only one processor,
the processes all see a consistent view of the model.
So, as I said, fundamentally, this update product,
sometimes you compute a gradient,
because you're only estimating the gradient,
and the gradient sends you in the wrong direction, right?
So you don't actually get, you know,
if you were doing, if you weren't doing stochastic gradient descent,
then you would always go down until you get got to the optimum,
the optimum, which would be a global optimal function with convex.
But if you're doing stochastic gradient descent,
you can sometimes go uphill and you bounce around.
So there's fundamentally noise associated with stochastic gradient descent,
and you can prove, if you are so inclined,
about how much noise you will see in a convex optimization problem,
not deep learning, deep learning's non-convex.
So the question then is,
if you do anything to perturb the process of getting to the optimum,
you will add noise.
And the question is, how much noise do you add,
and can you bound that noise to be below the inherent noise
associated with stochastic gradient descent?
If you can prove that the noise you add is less than the noise
that is already there, then you'll say you're not affecting the solution.
So that's the nature of the proof,
all about reasoning about noise.
Okay, so with that proof,
you can think about how much noise you add based on how stale the updates are,
and then you can prove that things will converge to the right answer
at roughly the same rate as it would be if you had the locks in.
Even if you go hog-wise.
Even if you go hog-wise.
So interesting.
So that's, and everybody uses it these days.
Google uses it, Microsoft uses it.
Everybody goes hog-wise because doing anything else will slow you down dramatically.
Okay, where were we?
We were getting to hardware, I think.
Oh, getting to hardware.
Okay, that's right.
So again, so we were talking about,
so that was the nature of a bunch of work we did together with Chris Ray,
this notion of, sort of, what can you do with stochastic-grang descent
to improve its behavior on modern hardware?
Okay, so modern hardware likes to be very parallel.
It likes to not have to idea you'd like to not use too much memory.
Modern hardware likes to do things on small data types,
think eight-bit integers, rather than 64-bit floating point, right?
So that is the key thing about modern hardware.
The other thing that was interesting about the work with Chris Ray was
related to the things that I talked about at designing my talk,
designing computer systems for software talk 2.0.
I was just in this notion that there are two big trends in computing today.
One is exemplified by the thousands of people at NURPS, right?
Which is the interest in machine learning and the broad applicability
of the approaches and the dramatic improvements in applications,
especially kind of high-end applications that have to do with doing things
that humans traditionally have been good at.
And that, of course, is causing everybody to get excited.
And one of the things that is true of building complex machine learning models
is that they take lots of computation.
And if you look at why machine learning has been successful,
it's because the computation has been available, right,
to actually train these large networks, right?
So if you, the ideas are all, maybe the data wasn't there,
but even if the data was there, the computing computation certainly wasn't there, right?
So you've got this situation where machine learning
needs lots of computation. At the same time, Moore's Law is basically slowing down, right?
So Moore's Law, as most people know, talks about the doubling of transistors
every 18 to two months to two years.
But what people don't know is that Moore's Law may be slowing down,
but that's not the real problem.
The real problem is this other related law or scaling factor called denod scaling.
So denod scaling basically says,
if I double the number of transistors on a chip,
if I scale things the right way, I can keep the power the same.
So I can double the number of transistors and keep the power the same.
Now, if I take denod scaling away, I double the number of transistors
and my power doubles too.
That's, that was screwed, right?
And that's the position we've been in, right?
Which is why processes haven't been getting faster,
because they're not the ways that you speed up conventional processes
and not power efficient.
And we're already at the limit of our power dissipation, right?
Especially, you know, in almost anything you talk about, right?
Whether it be your desktop or your laptop or your mobile device.
And so we're tapped out in terms of sort of what we can do with general purpose processes,
given that we're power limited.
So the question then becomes, what do you do instead, right?
And I think that is why it's such an interesting time in computer systems,
because we've got this convergence of these two big trends,
this need for insatiable amounts of computation to build machine learning models
at the same time, the conventional ideas for improvement performance are basically stored.
And so this is kind of motivating all kinds of exploration into alternatives
for general purpose processes. GPUs was an early-entrant,
but there are lots of companies investing, or lots of investment going in.
So one of the questions after the talk was, how much investment do I think has gone into new hardware for AI?
And I estimate billions.
I can say that.
Yeah.
I mean, hardware companies typically take in much larger amounts than software
and I can think of it doesn't easily.
Yeah.
Yeah, exactly.
Right.
You know.
So, yeah.
Sabanova was well-funded too.
So I just need to follow that.
You quickly get to building this.
So I'm working with Chris Ray has been all about playing these games,
with efficiency and noise and so on, to get higher.
So when doing SGD, it's all about how many iterations does it take to a certain level of accuracy?
And then what you really care about is not just how many iterations,
but how long does each of those iterations take?
So the definition of the number of iterations is what we call statistical efficiency.
And then the amount of time each iteration takes, we call hardware efficiency.
So no kinds of games about as you play with the noise, you affect statistical efficiency.
But you're also potentially improving hardware efficiency.
If you make hardware efficiency worse and statistical efficiency worse, then of course you screwed up.
But what typically you can do is make hardware efficiency much better without affecting statistical efficiency too much.
So hog wild would be an example, right?
You threw away the locks and iterations got faster.
And now you didn't have to do too many extra and you got to the same accurate result.
It strikes me that there's also economic efficiency in there that is not always perfectly correlated with either of those other two.
What efficiency would that be?
In terms of meaning, the cost of getting the result that you ultimately want.
Right. Right. Right. Right. Right. Right. Right. Right. Right. Yeah. Yeah. Yeah.
So that kind of led you down to, you know, into the hardware and kind of designing.
Yeah. So yeah, instead of always been an hardware guy, but it's always been about.
So I just have set up this problem that, hey, CPUs are kind of not going to improve anymore.
CPUs are better, but they still fundamentally have issues.
You know, the question is, you know, how can you design something that is both very efficient, especially in terms of power efficient.
And also very flexible. Right.
Because the most power efficient thing you could design would be exactly what you want. Right.
So you say, oh, here's my algorithm. I'm going to cost it directly into hardware. And I'm going to go fab a chip based on that.
As long as that algorithm never changes. Now that's the problem. Right. Right. Right.
You know, there's a chart that I showed. So Moore's Law says a number of transistors doubles every every year. And if you look at the machine learning papers on archive, that's exceeding Moore's Law.
So how many of those ideas are any good? Who knows? Right. But this is probably some good ideas in that. How many of those make that way to software let alone hardware.
Exactly. Exactly. Good point.
So what you really do need is a way to get both high efficiency and flexibility at the same time, because hey, you need to be able to come up with new ideas and be able to implement them quickly.
You know, implement them both quickly in terms of performance and quickly in terms of how much time it took you to implement.
You mentioned that the GPUs aren't perfect. And I happened to be overhearing a conversation here at NURPS yesterday the day before.
It was kind of like, well, you know, CPUs didn't work so well for this. But then we have GPUs and they solve all the problems. And it was kind of this always well.
So maybe it's worth talking about what are some of the challenges of GPUs? Well, GPUs, of course, were designed for graphics.
And they still, of course, have some of the baggage associated with graphics, but GPUs, such as, well, they've got extra hardware for doing graphics that you care less about.
So you think about Silicon areas as finite resource that you want to use. And you care about machine learning if I use 20% of it to make graphics go fast.
You say, hey, why are you taking my resources to do things I don't care about, right?
So they still have those specific things, but more fundamentally, they are these architecture is designed to execute matrix multiply very efficiently.
Okay, so it sounds like a good thing. We need to do that. We need to do that. So the question then becomes is matrix multiply what you want ultimately ultimately what you might want is some variant of matrix multiply that isn't quite what GPUs are good at.
You might also want something that does sparse computation, right? They're doing dense matrix multiply very efficiently. Maybe you want something that does sparse.
Maybe you want to be able to fuse lots of operators together to create this very weird function.
And you would like to be able to do that without having to write some custom code of action function.
If I can delve into some of the intricacies, I mean, so when talking about efficiency, you know, it has to do with how much of the silicon area actually goes into doing real look, right?
So how much of the silicon area actually does multiply add? How much of the silicon area provides the memory resources for those those multiply add units? And how much of the silicon area goes into what we call overhead, right?
Managing threads, managing register, register contacts, doing things that are not really required to move the computation forward, but are necessary to support the programming model that came with GPUs, right?
And then, you know, what are some of the things that GPUs are lacking, right? So going back to this model of data analytics is being a set of data flow operators, right?
So ideally, what you'd like to be able to do is cast that data flow graph directly into hardware.
GPUs actually make that difficult because of the way that the memory is organized. So but if you could do that efficiently, then there's all sorts of things that you could do that to make your computation be faster and more importantly to make it be much more efficient, right?
Everybody talks about peterroflops, but that's not the story really. It really is about how optimally you can map any particular application to your hardware and what efficiency do you get?
On a bunch of applications, the efficiencies of which GPUs gets are maybe, you know, less than 10%, right? So that means less than 10% of the time you're actually kind of using the full capabilities of the GPU.
So what that means is I could potentially build a machine that has a quarter of the capability, but if I could use it 90% of the time, I might be better off. I'm not saying I said, well, I will build that with that kind of machine.
So, you know, what you say, what's the matter with GPUs? It's a fairly technical argument, but one that has real ramifications about what performance you get out of the end of the day. So you kind of combine these issues of sort of, you know, overhead for doing things that are not machine learning.
So, overhead for supporting a threading mix, a thread of three graphics, overhead for kind of supporting this programming model associated with CUDA, not very efficient mapping of a bunch of these different networks.
And then you see why there's room for other players to come in.
So I think I pulled you down to depression. It's what I'm glad to talk about, but it's one that requires some level of understanding of what GPUs are.
Presumably, a big part of what you're working on are things that fix all of the above.
Yes, exactly. How do you give them? Yeah. And so it sounds like then one of the maybe interesting bits here is what it means to build a computing architecture that's kind of more natively graph aware.
Yeah, yeah, yeah. Can you talk a little bit about that?
Yeah, yes. So I think some of the things that we found out was that it is about graphs, but it's about hierarchical graphs.
So what you typically see when you kind of look below the colors of these graphs is that what you see is these operators.
And the operators that you're probably familiar with from maybe the distributed execution world is map and reduce, right?
So these are pretty basic operators and depending on what functions you are operating over, they can be made fairly general.
And so we take operators like that, producing a few others and we nest them and that gives you a lot more capability. And then once you have that graph, right, then you can think about how could you optimize that graph so that uses memory very efficiently.
So the communication is string lines so you can pipeline operations very efficiently. And then you think about how can I make that graph work very efficiently in hardware? What do I need to do?
So that's what what it's about. And we at Stanford, we worked on an architecture that we call plasticine.
It's named after a children's modeling clay that is popular in Europe. I grew up in London and so I played with it as a little boy.
We're in Canada now. I'm sure they've got it in Canada.
The closest thing to it in the States is called Play-Doh. Everybody played with Play-Doh. The key difference, and I think it's an important one from the naming point of view, is that if you leave Play-Doh out, it becomes a rock.
Well, it turns out the plasticine's oil base, so it never, and it's like a puddy stuff. It's like a puddy eggs and stuff. It's never hard. But it's not silly. It doesn't maintain your shape.
No, it's about gradations. It's not a new one. It's not a new one. So if you're going to see the movie Wilson and Grovitt, or the Clay Nation sort of thing, gummy. Yeah, gummy is probably plasticine.
Keep it in shape. Take a picture of it. Yeah, exactly. So yeah, so plasticine is before the record. Gumby was before my time, but SNL...
Oh, what a cracker!
It wasn't before mine. I can't say.
So yeah, so plasticine's an architecture that was designed to execute these hierarchical data flow graphs, very efficiently. So it's kind of native execution mode. It's a hierarchical data flow graph.
But there are a whole bunch of things that you want to do to do efficient machine learning execution. You want to do data flow graphs. You want to deal with sposity, right?
It's a quick way to like rattle off what it means to do graphs and hardware. Like are there kind of key principles?
So I mean, I think you can think of execution units as these nodes, right? And the ways of communicating between them so that you can map these graphs directly onto these execution units, right?
And then the way that you think about moving data through these is in terms of a pipeline of data moves through it, right? So if you're only going to do things once, then you'd be wasting your time, right?
But think of it as a multi-part intersecting assembly line, right? That's the way to think about how to set things up.
Really all kind of thinking about the cause being the data and the kind of moves through this assembly line and kind of at the end, the cause is complete, right?
But maybe they're doing, you know, is there some notion of like having if you've got some large graph and you're kind of mapping it to this more static substrate that you're kind of swapping in parts of the graph?
Yeah, yeah. So that's a very good question, right? So, you know, there's some machine that your TensorFlow initially had no way. I mean, you've got a static graph right? You've done.
And from a hardware point of view, you say, array, that's what I want. I can optimize the hell out of it. I map it to my substrate and I'm not going to change it.
But then you come along and say, ah, I don't sometimes I want to do this part of graph. Sometimes I want to do that part of the graph. So you have to have some way of reprogramming things, right?
And so a critical element of any architecture like this is how long does it take you to reprogram it, right?
So that is a consideration, right? Because you're talking about a reprogrammable thing as opposed to a general purpose. Yeah, yeah. So think of it, you know, as like, you know, the assembly line metaphor is pretty good, right?
On the factory floor, you set up the assembly line and you're going to make the set of cars and it's going to be this way for months, right?
So you set it up and then you let it go, right? But if every of you, you know, well, back when I used to be a graduate student at Michigan, people were talking about these flexible manufacturing lines, right?
Because, you know, Michigan's course Michigan, right? So, and so then maybe, maybe you're doing different things where we come, right? So now you have to have some way of adding some flexibility, having some reprogrammability.
And so yes, now it's more flexible. But there's a truism in hardware design and I get goes back to something I said before about the fact that, you know, you could be more efficient if you only had to do one thing.
And anytime you add flexibility, you are losing efficiency, right? And so as always, this game was okay. How much extra flexibility am I going to add and how what's it going to do to my efficiency?
Yeah, at some point you go too far, you say, you know, screw it. And CPUs, of course, are at the extreme of that of that spectrum, right?
Very flexible on the flexibility side with huge amounts of overhead.
And the other side would be, oh, this is the model that I want to bake in the silicon.
And silicon, I can't change it. Yeah, probably efficient, but then. So you get the spectrum. And you're saying, you're saying, you know, what, you know, what's the ingenuity about what I do about what I, what, what, what things I can change and what things are fixed, right?
And so that's the game. And then what, what's my compiler tool chain to target that, right?
Right. Compile tool chains for CPUs are very well established, right? There's, you know, C, L of the M, you know, what have you?
You can, CUDA is pretty, pretty well established too. And some of these other architectures that you could imagine, if you didn't have, you know, A, they have many more degrees of freedom.
So actually coming up with, with the right thing is, is more difficult and B, they're just weird, you know, in fact, we're in a good way in that.
And then one of the things that they don't have is explicit instructions, right?
So I mean, well, it means that, so an instruction would say, like explicit instruction that, you know, shift and move and allow level stuff.
Right. Well, they have them, but they're done spatially, right? So you say, you say, okay, I'm going to, instead of this, this particular clock cycle, I'm going to do an ad.
And I'm going to do a shift and then I've got an executing instruction. Then you say, no, you send it up. You're always going to do a shift at the end of time.
It's more like configuration. That's what you're going to do. And I'm just just what I put in.
Yeah, I'm going to feed you data and you're going to do what you do, right?
And you're not going to change. And then the next step is going to do something else, right?
And so, and so this is how you get these things. So why, how do you make the data flow so that everything gets to the right place so that the things that do their thing, you need a network.
Right. Right. So now you understand how you're building your building up kind of hardware, which has some flexibility, but, but, but, but, but not too much, right?
The key thing is, is where you put the flexibility and how you actually generate code. Okay. Interesting. Interesting. And so, but I should say it's all driven by the relatively static nature of at least the early machine learning.
Right. Right. It's become more flexible, but it's still relatively static compared to say, pick your favorite software in space. Right. Right. Right. Yeah.
And so, how does this tie into what you're doing at some an over? So at seven over, we are doing a bunch of things. We are figuring out how to create a new platform for AI. Right.
And what is platform can mean a lot of things. So, so how low do you go? How high do you go?
Well, let me do low those easy because we're going all the way to silicon. Exactly. Hi. You know, we're going to come up all the way to to the frameworks.
Okay. Yeah. Okay. And be very broad. My understanding is still early. So I can't say what we're actually doing.
But, you know, if I can, I can talk about the founding team and sort of what capable is they bring in that kind of, but we don't need to go into the team.
Yeah. Well, I think it's kind of interesting about this interview is that I'm not sure we ever transition from your like background to, you know, to your, to your talk.
We kind of at all wove together and chronologically in time. We end up it ended up at what you're doing now. Yeah. And so I, I have covered what I, what I talked in my.
I did it. Yeah. We, we, we, we, we, we, we, we, we covered the whole space. And so, and, and maybe I should recap. And then there'll be clear. So the recap is, okay.
I started out by saying, hey, we're in it. We're in this, this era where machine learning is ascended and more is Laura's.
Yeah, it's gone. Right. We need to do things differently. We need new algorithms based around these ideas of, of, of, of trading off this is called efficiency for hardware efficiency.
We need to make specific languages for encoding them. We need optimizing compilers for generating code for a variety of different architectures, including new hardware accelerators, which are defined on top of these data flow operators.
And then plastic seat being an example. So we're gonna go build that. Yeah. Yeah. Yeah. Yeah. Nice. Nice. Also, well, cool. Thank you so much for taking the time to chat with me and we'll thank you for reaching out to speed of pleasure. Fantastic. Yeah.
All right, everyone. That's our show for today. For more information on Kuhnle or any of the topics covering in this episode, visit twimmelai.com slash talk slash 211.
You can also follow along with our NURP series at twimmelai.com slash NURP's 2018. As always, thanks so much for listening and catch you next time.
