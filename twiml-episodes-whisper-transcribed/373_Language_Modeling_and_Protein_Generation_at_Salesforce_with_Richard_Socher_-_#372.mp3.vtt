WEBVTT

00:00.000 --> 00:13.400
Welcome to the Twimal AI Podcast.

00:13.400 --> 00:16.400
I'm your host Sam Charrington.

00:16.400 --> 00:24.800
Hey, what's going on everyone?

00:24.800 --> 00:30.560
A few weeks ago we held our very first interactive podcast screening with Google's Kwaklay.

00:30.560 --> 00:34.840
Attendees got to watch the video interview together and chime in with their own questions

00:34.840 --> 00:38.520
which Kwak and I addressed live during the session.

00:38.520 --> 00:44.000
We had a bunch of fun with this and got a lot of great feedback and so while we still haven't

00:44.000 --> 00:48.760
quite figured out what to call this thing, we have scheduled our next one and it's coming

00:48.760 --> 00:54.640
up this Thursday, May 7th at 12 noon Pacific time.

00:54.640 --> 00:59.700
This time around I'll be hosting Lucas B.Wald, founder of Weights and Biosys for a live

00:59.700 --> 01:03.520
interview screening and an interactive Q&A session.

01:03.520 --> 01:09.520
We'll be discussing data versioning, managing machine learning pipelines, managing ML experiments,

01:09.520 --> 01:11.360
and much more.

01:11.360 --> 01:12.720
You don't want to miss this one.

01:12.720 --> 01:18.640
To join us, visit youtube.com slash Twimal AI, subscribe to the channel and then hit the

01:18.640 --> 01:24.320
bell to turn on notifications so that YouTube will let you know when we're about to get started.

01:24.320 --> 01:30.000
In the meantime, Weights and Biosys is extending a special offer to Twimal listeners, including

01:30.000 --> 01:33.800
unlimited private projects and priority support.

01:33.800 --> 01:43.680
For more details, visit WNB.com slash Twimal, that's w-a-n-d-b.com slash Twimal.

01:43.680 --> 01:48.320
That said, May the 4th be with you.

01:48.320 --> 01:51.040
Hey everyone, I am on the line with Richard Socher.

01:51.040 --> 01:55.640
Richard is the Chief Scientist and EVP at Salesforce.

01:55.640 --> 01:58.120
Richard, welcome to the Twimal AI podcast.

01:58.120 --> 02:01.200
Aloha, great to be here, wants to chat with you soon.

02:01.200 --> 02:04.600
You said Aloha and I was surprised that you're actually in the Bay Area.

02:04.600 --> 02:10.720
I always see these wonderful photos of you all over the place and I only get to ever

02:10.720 --> 02:17.600
see you in person at NURPS nowadays and the Black and AI events and stuff like that.

02:17.600 --> 02:23.040
So it's great to get a chance to connect with you mid-year, how is everything?

02:23.040 --> 02:26.680
Life is pretty good, I'm very grateful.

02:26.680 --> 02:32.480
Our research can continue working on some research during this crisis now, food that is specific

02:32.480 --> 02:40.200
to COVID-19 but by and large, I sometimes joke that the PhD prepared me for several years

02:40.200 --> 02:46.160
for staying at home, meeting pasta every day and working on a computer all day.

02:46.160 --> 02:52.160
So I'm in pretty good spirits trying to have a little bit of positive impact and still

02:52.160 --> 02:58.920
go about my work and make sure my team is doing well throughout this crisis and it's a tough

02:58.920 --> 03:05.680
time but I'm very grateful for the line of research we can work remotely and do quite

03:05.680 --> 03:06.680
well.

03:06.680 --> 03:07.680
Awesome, awesome.

03:07.680 --> 03:08.680
I'm glad to hear that.

03:08.680 --> 03:14.840
It is, don't usually do this but it's April 10th that we're recording this.

03:14.840 --> 03:19.440
What is it week four for you for lockdown shelter in place thereabouts?

03:19.440 --> 03:20.440
That's right, yeah.

03:20.440 --> 03:25.480
Yeah, it's crazy to time, weirdly it's slow and fast at the same time, days more

03:25.480 --> 03:30.720
than to each other, you know, the home office is just the home and the office and everything.

03:30.720 --> 03:35.760
So yeah, time definitely does not seem linear going through this.

03:35.760 --> 03:42.240
It is very strange but hey, before we jump into some of the main topics that we want to

03:42.240 --> 03:47.280
cover and particular language models and some of the recent work you've been doing applying

03:47.280 --> 03:52.880
that to the bio space, share with us a little bit about your background and how you came

03:52.880 --> 03:54.600
to work in AI.

03:54.600 --> 03:55.600
Sure.

03:55.600 --> 04:02.800
Boy, it almost starts in high school when I re-liked math and languages and when you think

04:02.800 --> 04:09.880
about those two fields, one you would hope is true even if you go light years in some

04:09.880 --> 04:18.400
other direction and language is this constantly morphing system where every teenager could

04:18.400 --> 04:23.080
just say, yo-lo and boom, you have a new word and now the signs are going to mess the

04:23.080 --> 04:29.920
deal with that and so they marry when you try to use computers to use math to try to understand

04:29.920 --> 04:30.920
language.

04:30.920 --> 04:37.760
So I studied linguistic computer science back at 2000 and early 2000s and that at the

04:37.760 --> 04:44.920
time seemed kind of like an orchid, kind of cute, cruxotic, niche topic to my parents.

04:44.920 --> 04:48.960
But I thought, man, if we can get computers to understand language, that would be just

04:48.960 --> 04:49.960
incredible.

04:49.960 --> 04:53.960
All the things they could do, especially if you're lazy and you want to re-repetitive

04:53.960 --> 04:57.960
tasks to be done by a computer would be quite amazing.

04:57.960 --> 05:04.800
And so that kind of morphed into a couple of other interests and trying to use eventually,

05:04.800 --> 05:10.000
initially just statistical machine learning and sort of machine learning by itself and

05:10.000 --> 05:15.480
then AI more broadly applied to computer vision problems.

05:15.480 --> 05:19.480
But I really do think in the end language is the most interesting manifestation of human

05:19.480 --> 05:20.480
intelligence.

05:20.480 --> 05:25.960
There's some quite incredible visual systems and apparatuses in the animal kingdom, like

05:25.960 --> 05:31.960
the mantis shrimp with all kinds of focal vision and so on and each eye and all of that

05:31.960 --> 05:36.760
and a lot of animals have quite sophisticated visual systems, but it's really language

05:36.760 --> 05:41.360
that is connected with thought and culture and society and information.

05:41.360 --> 05:48.680
And so I got really excited about language and then in 2010 I saw a handful of people

05:48.680 --> 05:55.800
apply neural network techniques and extend them to computer vision.

05:55.800 --> 06:00.120
And at the time, I had also just become a little bit disillusioned myself around how

06:00.120 --> 06:06.200
much time natural language processing folks spend on feature engineering.

06:06.200 --> 06:13.200
And so I thought, couldn't we use some of these ideas from computer vision and neural networks

06:13.200 --> 06:16.560
for natural language processing and it was not easy.

06:16.560 --> 06:23.000
And the beginning early days had a lot of rejected papers, reviewers just ignoring reasonably

06:23.000 --> 06:26.640
good experimental results saying, oh, why are you submitting neural network stuff to

06:26.640 --> 06:32.080
this conference, this is not the 90s anymore, this stuff doesn't work and so on.

06:32.080 --> 06:37.720
But eventually, more and more people kind of joined the small core that initially was

06:37.720 --> 06:45.640
read just Yoshiro Benjo and Jeff Hinton's labs and Andrew Hings lab at Stanford and it expanded

06:45.640 --> 06:50.480
more and more and now it's kind of the default way for doing things as to use neural networks

06:50.480 --> 06:57.320
of course, they've developed more and more novel architectures too and it's just been

06:57.320 --> 06:58.320
super exciting.

06:58.320 --> 07:05.200
So now I work not just on the research side anymore, but also on a lot of applied problems.

07:05.200 --> 07:10.280
You know, in the end, I often think about trying to have impact and in the end, when you

07:10.280 --> 07:15.400
do research, you hope that people will pick up that research, extend it and actually apply

07:15.400 --> 07:19.840
it to some real world problems, but if you have the opportunity to both do research and

07:19.840 --> 07:23.920
apply it to real problems, you kind of reduce the variance of the impact that you have and

07:23.920 --> 07:31.840
so I work on a lot of NLP problems with chatbots and service and sales and marketing applications

07:31.840 --> 07:37.920
trying to, for instance, automatically reply to emails or to phone conversations or having

07:37.920 --> 07:43.440
chat conversations is a really great one also during a lot of stuff in computer vision trying

07:43.440 --> 07:49.560
to identify different objects and super market shelves and doing complex OCR for forms

07:49.560 --> 07:55.520
and a lot of interesting things, recommendation engines, voice, machine translation.

07:55.520 --> 07:59.800
Now the group is pretty large and so we get to work on a lot of different things.

07:59.800 --> 08:05.840
You know, it's a research organization, but it's part of Salesforce which we, you know,

08:05.840 --> 08:10.800
is kind of a, you know, do you still, does Salesforce still think of itself as a CRM company

08:10.800 --> 08:13.360
or is it much harder than that now?

08:13.360 --> 08:19.080
Yes, the term CRM has kind of expanded and now includes everything that you might do

08:19.080 --> 08:20.400
with a customer, right?

08:20.400 --> 08:24.440
So we're one of the largest e-commerce platforms because customers buy stuff online.

08:24.440 --> 08:29.240
Yeah, we're obviously the largest sales service and marketing organization, but we also

08:29.240 --> 08:32.240
have help companies integrate all their different data.

08:32.240 --> 08:37.640
Now with Tableau, we help people understand their customer data and do a lot of analytics

08:37.640 --> 08:43.400
behind it and then we look at, you know, where are the customers and we help governments

08:43.400 --> 08:48.880
see their citizens as their customers and help them, especially now also in this crisis,

08:48.880 --> 08:53.160
build software really quickly, build chatbots so they can answer questions.

08:53.160 --> 08:57.200
You know, the, in the end, if you go to the DMV Department of Motor Vehicles and you

08:57.200 --> 09:03.160
have a question, chatbots that give you answers there are also, you know, your customer

09:03.160 --> 09:04.920
there of the DMV.

09:04.920 --> 09:07.720
We work with healthcare providers where the patients are customers.

09:07.720 --> 09:13.360
So the definition of what a customer is is getting broader and broader and we're in all

09:13.360 --> 09:14.680
those areas.

09:14.680 --> 09:18.720
And so with the company being in all those areas and being a very product focused company

09:18.720 --> 09:25.120
as opposed to, you know, an academic institution, you know, how does the company balance investments

09:25.120 --> 09:31.560
in, you know, research with product requirements and how does that specifically impact you

09:31.560 --> 09:37.520
and your team and what you decide to, or, you know, end up working on?

09:37.520 --> 09:41.840
It's a great question and it's definitely something that I think every company struggles

09:41.840 --> 09:47.960
with as they can sort of think past their next quarter and think about, you know, the next

09:47.960 --> 09:53.600
couple of years, we, I wear really a couple of different hats.

09:53.600 --> 09:59.680
On the pure research side, we really can work on and have a lot of freedom and we can work

09:59.680 --> 10:04.720
on a lot of different things and we basically look for what's, what's the, what could have

10:04.720 --> 10:09.960
the biggest impact on both the field of AI research as well as on application fields

10:09.960 --> 10:16.720
like economics or a natural language processing or medicine even in other areas.

10:16.720 --> 10:22.560
So there, we have a lot of freedom, but then the, another large part of my group is really

10:22.560 --> 10:28.240
just part of engineering and we're building real products like chatbots or case classification

10:28.240 --> 10:34.560
of emails or opportunity and lead scoring where, you know, a salesperson might have 5,000

10:34.560 --> 10:40.240
people they could email or call in any given day and you can use systems to rank and say,

10:40.240 --> 10:44.400
these customers are the most likely to actually want to buy your product today and then help

10:44.400 --> 10:51.480
them kind of get through and triage that we work with marketing where we can classify

10:51.480 --> 10:57.600
the sentiment of different tweets, for instance, or we can identify company logos on tweets

10:57.600 --> 11:04.280
and surface them even if you don't say at or hashtag this company name.

11:04.280 --> 11:10.680
We work with industries to understand super market shelves and do visual things.

11:10.680 --> 11:16.200
So there's a lot of product development and as long as that product development feels

11:16.200 --> 11:21.400
like and we feel like we're having a lot of positive impact on the business through AI,

11:21.400 --> 11:25.200
we get the carve out a niche where we just think about all the stakeholders, not just

11:25.200 --> 11:31.520
to share all this but all the stakeholders of the ecosystem and there we can work on things

11:31.520 --> 11:38.880
like even protein sequence models and medical computer vision, classifying different types

11:38.880 --> 11:47.360
of breast cancer and a whole host of fundamental research programs and optimization of neural

11:47.360 --> 11:53.520
networks which is pretty theoretical as well as on some really applied NLP projects that

11:53.520 --> 11:58.320
are actually very nicely at the intersection of pure research and applied research.

11:58.320 --> 12:03.360
So for instance, one paper was published a while back and have extended for a while now

12:03.360 --> 12:08.800
on a sequence of SQL where we translate natural language English questions into SQL queries.

12:08.800 --> 12:16.000
And that is a fundamental problem you tried to disambiguate language that sort of touches

12:16.000 --> 12:22.840
upon what is the meaning of a question which is pretty hard and one way to define the

12:22.840 --> 12:28.880
meaning of a question is well it's a program that you once you execute it over the right

12:28.880 --> 12:34.360
set of knowledge will give you the correct answer to the question that the person asking

12:34.360 --> 12:38.920
the question had in mind and one way of doing that is to translate it into code or into

12:38.920 --> 12:45.040
SQL because a lot of data in the world is stored in databases and so that is a very fundamental

12:45.040 --> 12:49.360
question that we actually started with pure research but now with Tableau and other

12:49.360 --> 12:54.080
analytics and so on it is actually quite relevant and business users might also want to ask

12:54.080 --> 12:59.720
questions like how many customers bought this particular product and this particular zip

12:59.720 --> 13:05.400
code between this time range and you just want to be able to answer that question in normal

13:05.400 --> 13:11.720
English language and get a real answer for it and so there are some more and more connections

13:11.720 --> 13:15.320
between the fundamental research group and the applied research group and then all the

13:15.320 --> 13:17.160
product groups and engineering groups.

13:17.160 --> 13:23.000
That provides really interesting context for talking a little bit about one of your recent

13:23.000 --> 13:31.320
papers which is and you mentioned this on protein generation even with all of that in mind

13:31.320 --> 13:34.440
what was how did you end up working on protein generation?

13:34.440 --> 13:40.920
Yeah it's a good question originally we're inspired by language modeling so we've worked

13:40.920 --> 13:47.720
on language modeling for almost four years since we since met a might my startup got acquired

13:47.720 --> 13:53.800
by Salesforce and for a while had the best language models there are still LSTMs or quasi-recurrent

13:53.800 --> 13:59.600
neural networks and variants of these with pointer mechanisms that could copy over words

13:59.600 --> 14:05.800
as they try to predict the next word and it's a very interesting fundamental task in

14:05.800 --> 14:10.040
natural language processing right in some ways it's NLP complete in the sense that if you

14:10.040 --> 14:15.040
can always predict correctly what the next word is then that means you can also do all of

14:15.040 --> 14:21.680
question answering right I can ask you know what's the capital of California and then the next

14:21.680 --> 14:26.320
word should be the capital of California and if you do that correctly it means you have knowledge

14:26.320 --> 14:33.440
about the world you can answer questions you can ask what is the translation of ithibadi from

14:33.440 --> 14:38.960
German into English and then you should get the translation as the next couple words so as you

14:38.960 --> 14:44.160
can get better and better at predicting the next word you might be able to solve a lot of different

14:44.160 --> 14:49.520
natural language processing problems now at the same time that is kind of the long term future

14:49.520 --> 14:53.840
nobody actually thinks that you're you could be good enough especially not a couple of years ago

14:53.840 --> 14:59.840
to do all of these different NLP problems as question as language modeling but originally language

14:59.840 --> 15:04.880
modeling was just used to disambiguate words for instance if I ask you what's the price of wood

15:04.880 --> 15:14.080
is this the wood W-O-U-L-D auxiliary verb or is it the W-O-O-D the noun from a tree wood and so

15:14.080 --> 15:19.360
but with the price of it's much more likely to have a noun after and so you can disambiguate you

15:19.360 --> 15:26.480
know either a translation or speech recognition models or you can come up with better selection

15:26.480 --> 15:32.720
of sentences that were generated by a translation model to say this is more fluent of an English

15:32.720 --> 15:37.760
sentence than another alternative and so that was originally what language modeling was used for

15:37.760 --> 15:44.080
but then we were able deep learning and our group and in other groups and now all these

15:44.080 --> 15:49.360
transformer models and bird models and so on really get better at predicting not just the next word

15:49.360 --> 15:53.280
but you can also predict works in the middle of the sentence and you train really good

15:53.920 --> 15:59.680
general representations of language that capture general knowledge we actually use these

15:59.680 --> 16:06.560
kinds of language models to give explanations for why you make choose a certain classification

16:06.560 --> 16:12.240
output and it turns out that in several cases they actually capture common sense even which is

16:12.240 --> 16:18.240
really hard to capture an illogical or database kind of structure. Can you give some examples of

16:18.240 --> 16:25.680
what you're describing there? Yeah sure so the paper was done by Nadine in our group

16:25.680 --> 16:35.840
at Salesforce Research and basically she created natural language explanations for common sense

16:35.840 --> 16:44.080
reasoning questions and so the paper was we have actually a couple but one was explain yourself

16:44.080 --> 16:49.360
leveraging language models for common sense reasoning and so the idea here is we had a couple of

16:49.360 --> 16:56.880
different multiple choice questions and in with you want to give the answer you also ideally

16:56.880 --> 17:04.640
will say why you gave that particular answer so for instance if you push a glass off a table

17:04.640 --> 17:11.840
what will happen? A it will float, be it will fall down or in break and things like that and then

17:11.840 --> 17:18.240
you will basically say oh why will it fall down and it's not because of gravity and so this seems

17:18.240 --> 17:25.440
like a simple thing and for any particular small scenario you could possibly generate all the logical

17:25.440 --> 17:30.160
consequences but if you think about anything like I was going to say it seems like a simple thing

17:30.160 --> 17:37.280
for anyone who's not involved in AI and say oh the person blows the lease from a grass area using

17:37.280 --> 17:43.040
the blower the blower A puts the trimming product over her face in another section B is seen

17:43.040 --> 17:49.600
up close with different attachments C continues to blow mulch all over the yard several times

17:49.600 --> 17:54.800
and so on so you know these kinds of questions and knowing how to answer that it's just requires a

17:54.800 --> 18:01.600
lot of common sense and common sense is incredibly hard to really formulate in a knowledge representation

18:01.600 --> 18:06.800
such that AI models can really make use of it so long story short I got a little off of a

18:06.800 --> 18:11.680
tangent there language modeling super fascinating lots of other applications that people hadn't thought

18:11.680 --> 18:17.600
about that that came out in the last couple of years and in our case we also wanted to actually

18:17.600 --> 18:23.120
make it more controllable we had these interesting language modeling results from open AI that could

18:23.120 --> 18:28.320
actually generate long texts and it was quite surprising to a lot of people that these models were

18:28.320 --> 18:34.000
so good at doing that at the time but all you could do is you primate with the beginning of a sentence

18:34.000 --> 18:39.600
you say like a knife and then we're just kind of ramble on and on and we thought couldn't we make

18:39.600 --> 18:46.240
this more controllable so we edit control codes based on different training data and based on

18:46.240 --> 18:51.360
basically the whole internet so you can make a control code and say this is a horror story

18:51.360 --> 18:58.160
now say a knife and then you know sort of peek through the keyhole of the door and the door slightly

18:58.160 --> 19:02.480
started creaking and opening and you say oh my god what's going on but you can also say a knife

19:02.480 --> 19:07.360
and then say give me a review and so I just say oh knife is really great cutting and fits well

19:07.360 --> 19:11.840
into our kitchen and so on you could basically control the language modeling a little bit better

19:11.840 --> 19:18.000
and we thought that would make it more useful and now people can kind of collaborate with these

19:18.000 --> 19:23.600
models to generate texts and it almost helps you if you're not trying to do creative writing to

19:24.480 --> 19:29.200
to give you sort of spitball with you and just generate stuff and then you can modify it

19:29.200 --> 19:34.400
you can create marketing messages and have control codes based on these were successful marketing

19:34.400 --> 19:38.960
campaigns and of course you know you have certain things in mind so you want to kind of play

19:38.960 --> 19:44.880
with the model and its output so that was control and that paper just or folks we'll link to it

19:44.880 --> 19:50.320
in the show notes but that's conditional transformer language model for controllable generation

19:50.320 --> 19:57.440
CTRL that's exactly right and that is the largest model is 1.6 billion parameter language model

19:57.440 --> 20:03.120
you train it on a very large corpus you can create any URL as a control token that will

20:03.120 --> 20:09.440
talk in the language of that URL in like cnn.com and so on it's quite fascinating and for a while

20:09.440 --> 20:16.320
we're actually worried also that it might have ethical issues but really I don't think that is the

20:16.320 --> 20:23.280
case people unlike GANs that generate images where people thought images were you know despite

20:23.280 --> 20:30.160
Photoshop people still thought of images as a good proof for something you can forever in human

20:30.160 --> 20:34.640
history since we've had language just misattribute text to somebody else you can just write an

20:34.640 --> 20:38.960
email and say oh this other person sent the email and like well we have to actually show metadata

20:38.960 --> 20:45.120
to really verify that that is really something that somebody said so don't think it helps with

20:45.120 --> 20:50.480
sort of creating misinformation also because you while it's more controllable in terms of the

20:50.480 --> 20:56.880
style now with CTRL or control you still can't force it to say the things that are in your head

20:56.880 --> 21:00.800
like you want to usually when you want to create misinformation campaigns you have something

21:00.800 --> 21:07.200
specific in mind that you would like the models to say but long story short quick question on that

21:07.200 --> 21:13.440
the control code you mentioned you can generate them from any URL when you have something like a

21:13.440 --> 21:21.520
review or a movie or horror I think was the example used it does that particular code map to

21:21.520 --> 21:27.600
a specific URL that you've selected or is there something more to that mechanism so the

21:27.600 --> 21:33.760
control codes are very broad you can say like this is Wikipedia or like Wikipedia style or coming

21:33.760 --> 21:40.480
from Wikipedia like training data it can be just a URL it could be a style like a kid story or a

21:40.480 --> 21:45.760
horror story like you can you can have any set of control codes and one set of control codes are

21:45.760 --> 21:50.880
URL so if you can say create me a Kickstarter campaign for like some new gadget that you come up

21:50.880 --> 21:56.320
with and it'll literally write you a Kickstarter campaign with that URL it's it's actually pretty

21:56.320 --> 22:03.840
hilarious what people have been able to generate with it in terms of the text and the control codes

22:03.840 --> 22:11.280
those are you're providing those to the model at inference time they're not baked in during the

22:11.280 --> 22:16.080
training process is that correct so they are in that we use the whole internet as the training data

22:16.080 --> 22:23.200
and so every URL hat was in some way a control code sure but you don't have to specify a subset of

22:23.200 --> 22:27.840
the internet that will serve as your control codes later why you don't have to yeah you don't have

22:27.840 --> 22:32.800
to but you can but you can yeah the control codes are very general and some of them where we knew

22:33.760 --> 22:40.960
for instance where they came from like Wikipedia we can kind of stylize them and we had certain

22:40.960 --> 22:46.960
groups of documents where we said all right these are all Wikipedia articles so let's just you know

22:46.960 --> 22:51.760
say this is the style of Wikipedia it's very factual you don't just ramble on you don't use

22:51.760 --> 23:00.000
colloquial language and things like that and so can you kind of describe the the general mechanism

23:00.000 --> 23:09.520
that the model is using to base output on these control codes sure um so essentially in a language

23:09.520 --> 23:16.160
model you have a sequence of words and at at the end you have a classifier and that classifier

23:16.160 --> 23:21.280
goes over the whole vocabulary sometimes that vocabulary can be just the characters of the English

23:21.280 --> 23:26.880
language for instance sometimes the vocabulary can be full words and in some cases they're so

23:26.880 --> 23:30.880
called bite-pair encodings and don't want to get too technical but it's basically like character

23:30.880 --> 23:38.000
sequences that are very frequent and you can basically at each time step classify what's the next

23:38.000 --> 23:43.440
most likely token that you want to generate and the control tokens are just another input in the

23:43.440 --> 23:51.760
beginning of these language models and say this is the current input and then after that you say

23:51.760 --> 23:57.280
now start you know you can also have a sequence of words to prime the model further and then it

23:57.280 --> 24:02.560
just continues to generate so it's essentially just another input also in a vector representation

24:02.560 --> 24:08.960
uh based on on these control tokens i'm trying to get at what's special like if i took gpt2 for

24:08.960 --> 24:14.560
example and you know started my prompt with Wikipedia i'm imagining that wouldn't produce the same

24:14.560 --> 24:22.320
effect as your model conditioned on Wikipedia as you're token that's right yeah because the original

24:22.320 --> 24:28.240
language models just took raw text they didn't think about metadata of that text so it's not

24:28.240 --> 24:34.800
hard to say like a knife and then control like in which direction you want that to go it'll

24:34.800 --> 24:40.480
generate whatever is the most likely based on all of the text it's seen okay right so you have no

24:40.480 --> 24:44.480
no way of controlling it in any which direction you want it to go you want to have a colloquial

24:44.480 --> 24:51.920
story you want to have a tweet you want to have a very factual story that sounds very you know

24:51.920 --> 24:58.080
standard uh and so on so this is kind of the metadata associated with with language and you can use

24:58.080 --> 25:02.640
that to control where the output uh is headed in in which general direction it should

25:02.640 --> 25:09.840
so then we trained these language models uh it's quite exciting uh and uh we we released a whole

25:09.840 --> 25:16.160
model and it's it's basically the largest model that would still fit on on fit on a general gpu

25:16.160 --> 25:22.960
that you couldn't get uh AWS like a p100 um and and once it's larger there are some larger language

25:22.960 --> 25:27.680
models that people have trained but they become impossible for anybody else to run like and we're

25:27.680 --> 25:34.240
up to like 17 billion parameters now that's Microsoft that's right and so you know in Nvidia I had

25:34.240 --> 25:39.360
some other ones but they're just like okay if you have a gigantic you know a million dollar cluster

25:39.360 --> 25:46.720
then and you can do this or you want to spend a lot of money per hour of even just loading it up

25:46.720 --> 25:52.800
and and and so on so we felt like this would marketize a little bit what these methods can do uh

25:52.800 --> 25:58.080
and allow basically anybody who can spawn a reasonably cheap AWS instance to play around with

25:58.080 --> 26:03.040
these models and and fine tune them too and and make them work for their uh their particular use

26:03.040 --> 26:08.160
cases and then we thought about what what would other use cases be where else do you have data and

26:08.160 --> 26:15.600
you want to generate useful sequences and uh that uh in in our group we have uh Ali Madani uh

26:15.600 --> 26:21.680
who's a great researcher in our group he's working some medical applications around computer vision

26:21.680 --> 26:30.000
two uh and he basically wanted to study the language of biology uh and basically tried to generate

26:30.000 --> 26:36.160
proteins in a controllable fashion and proteins uh and I don't want to get to technical in the

26:36.160 --> 26:41.360
biocide also I'm not a biologist but basically proteins govern everything uh in human biology

26:41.360 --> 26:48.240
in our bodies and viruses everything and so this AI system which we called progen uh for protein

26:48.240 --> 26:54.000
generation uh with this uh controllable language model is this really high capacity language model

26:54.000 --> 27:01.520
that was trained on the largest protein database that's available so we had 280 million proteine samples

27:01.520 --> 27:08.080
and now we can actually do something as really challenging for science and basically unlock the

27:08.080 --> 27:13.920
potential of protein engineering for synthetic biology material science and human health and we

27:13.920 --> 27:20.320
started this project uh almost a year ago uh and at the time we thought about you know really

27:20.320 --> 27:26.240
pressing uh diseases like cancer and and others uh or material science where you can try to generate

27:26.240 --> 27:31.280
bacteria that might eat plastic and things like that so it was pretty good uh and it's a super

27:31.280 --> 27:37.760
fascinating area of research that I think can have a lot of uh positive potential for the world

27:37.760 --> 27:44.400
and the environment and people but now of course uh it's COVID-19 is is on everybody's mind and

27:45.120 --> 27:51.760
it turns out that even there uh like two years ago the Nobel price in medicine uh was started to

27:51.760 --> 27:58.720
uh or was given to a team that built uh was able to create synthesize new proteins that didn't

27:58.720 --> 28:07.120
exist before to that had a specific function and in the case of these control tokens now the

28:07.120 --> 28:15.040
functions can be a specific type of control token like make the the cell glow or uh make this

28:15.040 --> 28:23.520
uh particular cell uh be a spike protein for a specific receptor and things like that and that

28:23.520 --> 28:32.800
is exactly uh what we can now do for for trying to work uh on on viruses and and antibodies and

28:32.800 --> 28:37.840
things like that to try to generate the right antibody and things like that so it is it just me or

28:37.840 --> 28:43.920
is it pretty amazing that you know we can now build AI models that we're solving you know that

28:43.920 --> 28:49.680
can be applied to what a couple of years ago won a Nobel Prize. Yeah well so yes and no I mean

28:49.680 --> 28:54.560
I don't want to push that too far but that's right that's right and so we we have to we have to be

28:54.560 --> 28:59.040
careful uh it's still very ongoing research and I don't want to hype it up too much until we have

28:59.040 --> 29:04.480
more real results but basically the previous line of research that won the Nobel price they basically

29:04.480 --> 29:11.760
randomly permuted these proteins and then tested out many many iterations until they found something

29:11.760 --> 29:18.080
that would have the actual biology and functions that they wanted and in our case we're able to

29:18.080 --> 29:25.440
reduce the search space because you know there there are a lot of potential uh protein sequences

29:25.440 --> 29:30.240
that you might get with these random permutations but if you had a sense from reading 280

29:30.240 --> 29:35.520
million of these proteins before that a sense of what's a reasonable structure what's most likely

29:35.520 --> 29:41.600
to come after this sequence of uh characters and amino acids and so on uh then you might create

29:41.600 --> 29:49.200
much more uh guided and directionally correct proteins and we actually see you know the energy

29:49.200 --> 29:54.560
how stable they would be and there are a couple of metrics uh that we show uh in the progen paper

29:55.280 --> 30:00.800
that the proteins that this model generates are much more likely to be viable to be synthesized

30:00.800 --> 30:06.240
and to have the functions that you want them to have then randomly permutated ones and that's obviously

30:06.240 --> 30:12.880
a relatively low baseline right random permutation is not that okay better than random permutation

30:12.880 --> 30:19.920
but it's really hard because the human AI the human intelligent system has evolved uh over many

30:19.920 --> 30:28.240
many years to do a deal with human language and the statistics and correlations that human language

30:28.240 --> 30:35.120
has and so we're not very good at looking at you know a b like so the contribution here is not that

30:35.120 --> 30:40.640
you're making a dent in this actual problem but rather that you're applying this language model

30:40.640 --> 30:47.520
that's built on human language to a totally different type of language that kind of underlies

30:47.520 --> 30:53.360
biology that's correct and now and then we show that this new tool can be useful for a broad range

30:53.360 --> 31:00.880
uh of of applications and now we're actually collaborating with people to actually go and and

31:00.880 --> 31:05.840
generate and synthesize these proteins and we're partnering uh of a lot of different universities

31:05.840 --> 31:10.160
and now there's a lot of exciting space a lot of exciting work in this space but it's all a little

31:10.160 --> 31:16.000
bit too early to to tell we just uh released uh progen and so now uh sort of follow up work as

31:16.000 --> 31:20.400
is in the pipeline and so can you go into that follow up a little in a little bit more detail

31:20.400 --> 31:29.760
in the sense of to you know validate the results of this model what has to happen it sounds like it's

31:29.760 --> 31:37.200
what it's ultimately producing is candidate proteins that have to then be validated uh

31:37.200 --> 31:42.320
experimentally is that the idea or that basically it yeah you first want to generate them

31:42.320 --> 31:47.760
computationally then you want to synthesize synthesize the actual proteins in real biology

31:47.760 --> 31:54.080
create the molecules then you want to see if uh on a very simple chemistry level they would actually

31:54.080 --> 31:59.920
exhibit uh the functions that you want if you then think you have some successful candidates

31:59.920 --> 32:06.320
then you would want to run some animal studies and eventually you hope to create new antibodies

32:06.320 --> 32:12.960
new uh ways to deal with uh and uh protect people from certain diseases long line there's

32:12.960 --> 32:17.040
there are many many steps and then obviously we don't have a wet lab and things like that so we're

32:17.040 --> 32:24.240
going to depart people on that we we only go so far uh-huh uh-huh is there it's your knowledge

32:24.240 --> 32:31.040
is there role for simulation or do we are we not there in terms of simulating protein structure

32:31.040 --> 32:38.480
before you go and actually synthesize you could and I guess uh some people work work in that space

32:38.480 --> 32:43.760
it might also be helpful it turns out it's not actually that expensive to synthesize a protein

32:43.760 --> 32:51.920
it's about ten bucks okay uh and so uh depending on the time cost computation tradeoffs and so on

32:51.920 --> 32:56.720
uh you and if you really reasonably certain that you were able to give good candidates to be

32:56.720 --> 33:02.400
actually synthesized then maybe yeah but yeah it's like I said it's very early work synthetic

33:02.400 --> 33:06.800
biology as a field has been around for a while but it's still quite nascent in comparison to a lot

33:06.800 --> 33:11.600
of other fields so I don't want to go into too many details until we have some more more concrete

33:11.600 --> 33:19.840
results yeah yeah and is the you know the idea in this line of work around language models to

33:20.560 --> 33:25.760
hey we've got some interesting thing here with proteins let's you know push that a lot further

33:25.760 --> 33:33.600
or hey we you know demonstrated that we can apply this to you know this biological code what other

33:33.600 --> 33:39.600
types of codes or languages are that we can apply this to like where where you headed with

33:39.600 --> 33:46.960
this general uh you know line of research so I think there are a couple of really exciting areas

33:47.520 --> 33:53.600
of research uh several which have connections to language models in some ways I think of

33:53.600 --> 33:59.120
three equivalent super tasks of NLP actually language modeling is one question answering is another

33:59.120 --> 34:05.440
and dialogue systems is the third and they're equivalent in terms of in the sense that every NLP

34:05.440 --> 34:13.440
problem can be cast as one of those three tasks and you know you can sentiment analysis people

34:13.440 --> 34:18.080
think of it as just classification but really if you ask here's a sentence yes what is the sentiment

34:18.080 --> 34:25.760
of the sentence then the next word that you predict should be uh that you know the classification

34:25.760 --> 34:31.280
uh and the label that that sentence has so even sentiment analysis can be seen as question

34:31.280 --> 34:35.200
answering and then everything that's question answering is also language modeling and in some ways

34:36.000 --> 34:41.520
with insights were kind of irrelevant because some people said that yeah but so what but now we

34:41.520 --> 34:46.480
actually have these models with billions of parameters and we may actually be able to build a single

34:46.480 --> 34:51.440
model for all of NLP and that's been kind of my dream ever since I started in 2010 training

34:51.440 --> 34:57.040
these neural nets uh on on language and because I realized the underlying substrate matters less and

34:57.040 --> 35:01.600
less it becomes more and more important I think in the future to be able to train these large

35:01.600 --> 35:06.800
neural network architectures for multiple tasks so you can have transfer learning between them

35:06.800 --> 35:11.520
you can get to zero shot learning tasks like I want to be able to have NLP systems that answer

35:11.520 --> 35:16.480
questions that they haven't seen before in the training data and I think better that they're

35:16.480 --> 35:22.000
examples like that in squad the Stanford question answering data set uh but it mostly extracts phrases

35:22.000 --> 35:27.680
from a from a document and it doesn't really have to capture complex knowledge uh in and of itself

35:28.320 --> 35:35.280
but language models are very tied to this uh also for I think one of the most exciting and least

35:35.280 --> 35:40.880
soft tasks of our time and that is summarization uh we've had a bunch of summary papers in the group

35:40.880 --> 35:49.040
two uh white check and in our group has just released one on factual uh correctness and consistency

35:49.040 --> 35:56.000
of uh summaries it turns out the whole field of summarization worked with pretty bad datasets

35:56.000 --> 36:02.080
and including ourselves some of the models that were very we're called them abstractive uh versus

36:02.080 --> 36:07.920
extractive so the difference there is extractive summarization just takes chunks and phrases

36:07.920 --> 36:12.000
and sentences from the original document you want to summarize just says this is an important

36:12.000 --> 36:17.680
sentence and then you have it uh whereas abstractive you have the hope that you would understand

36:17.680 --> 36:23.760
what's going on and then you can rephrase it in a different or shorter form uh and some of these

36:23.760 --> 36:28.720
language models get us closer to being able to do that that's exactly right but it turns out they

36:28.720 --> 36:35.360
also often learn to copy phrases from the input uh in just a really clever way and so long

36:35.360 --> 36:43.280
story short the biggest problem uh for NLP and summarization is for instance that uh first names

36:43.280 --> 36:50.640
proper nouns are often have similar vector representations so Jason, Jeremy, John uh they all

36:50.640 --> 36:57.600
look like similar to to these neural network models they are a list of 500 roughly similar numbers

36:58.560 --> 37:03.600
and the problem with that is that in a summary it's really crucial like when you say like

37:03.600 --> 37:08.640
there's converter or you know an accident or something you mix up the names it's really

37:08.640 --> 37:13.200
really different right but to a model this isn't really that different into all the evaluation

37:13.200 --> 37:19.920
metrics of the field of summarization you don't get really dinged hard uh and reduce your metrics

37:19.920 --> 37:25.840
by mixing up one name with with another but for people it's very crucial and so uh looking at

37:25.840 --> 37:31.600
factual uh consistency and accuracy of summarization is something that we care about a lot

37:32.240 --> 37:36.400
and interactively working on and there's again a connection to the language models too.

37:36.400 --> 37:44.320
Huge uh commercial implications of this like we're all inundated with textual data

37:44.960 --> 37:53.040
and you know if we could easily and cost effectively and accurately summarize that that would be

37:53.040 --> 37:57.280
huge. I'm just speaking from personal experience and 500 open browser tabs.

37:58.320 --> 38:06.160
That's exactly right I think it's a super impactful uh technology for our time where ignorance is

38:06.160 --> 38:12.480
almost a choice uh for for most people right but the fact is that there's too much information

38:12.480 --> 38:18.320
and sorry to really get through it and that's what AI is really good good at right uh take

38:18.320 --> 38:23.040
laborious tasks that just don't scale with people and automate them for us.

38:23.840 --> 38:29.840
And going you know for a full circle or at least circling back to progen I've talked to

38:29.840 --> 38:38.480
the folks that are not necessarily trying to apply summarization to scientific literature but

38:38.960 --> 38:46.160
to otherwise use AI to mine insights from scientific literature and kind of project into

38:46.800 --> 38:53.840
you know where future innovation might come from and you know summarization that is you know

38:53.840 --> 39:00.240
the preserve kind of the scientific essence the facts of a you know research paper could be a step

39:00.240 --> 39:06.400
in that direction as well. Absolutely yeah like uh researchers like everybody else are inundated

39:06.400 --> 39:12.480
with too much information right if you try to follow the archive uh and and see all the new papers

39:12.480 --> 39:18.240
that come around AI in just specific fields uh it's it's almost impossible to keep up with the

39:18.240 --> 39:24.160
literature and still do your own work and not just be reading on top uh and so I think summarization

39:24.160 --> 39:30.560
is going to become more exciting and now that we have solved other tasks that can be a little less

39:30.560 --> 39:37.440
ambiguous uh the field of NLP can kind of move towards these these tougher more ambiguous kinds of

39:37.440 --> 39:43.760
tasks. What do I mean by this? So for instance machine translation uh you have a pretty small set

39:43.760 --> 39:49.600
of uh outputs that make sense given an input sentence from German that you want to translate into

39:49.600 --> 39:55.360
English short there may be like two or three different variants like thank you it can be translated

39:55.360 --> 40:00.880
as fiendank or dankershÃ¶rn. I know two two options but it doesn't make sense to just say like

40:00.880 --> 40:06.560
katsun to it to just cadendok like it so you have a pretty small variant in the outputs that

40:06.560 --> 40:12.640
makes sense for that input to be generated. On the other hand in summarization boy there's so many

40:12.640 --> 40:20.080
possible variants and uh when you think about it in the end summarization is also highly contextual

40:20.080 --> 40:26.000
and needs to in the limit be highly personalized. For instance when Elmo came out the first sort

40:26.000 --> 40:32.160
of language model uh that had really really good representations uh personally to me a good

40:32.160 --> 40:36.560
summary of that paper would have been oh it's like cove contextual vectors but instead of

40:36.560 --> 40:41.280
trained with machine translation it's trained as a language modeling objective and it works really

40:41.280 --> 40:46.320
well on a lot of different tasks. But if you don't know what Elmo or language models or word

40:46.320 --> 40:51.680
vectors are that is a completely useless summary and really the good summary of that paper introduces

40:51.680 --> 40:57.280
you and maybe it pulls in even text from elsewhere and helps understand what is going on in the first

40:57.280 --> 41:03.520
place with this field and then gives you sort of the incremental novelty of that uh you know new

41:03.520 --> 41:08.400
research result that comes out so and the same is true for for every sort of conflict you might

41:08.400 --> 41:14.320
follow on earth or if you read you know COVID-19 news every day uh you don't need to know

41:14.320 --> 41:19.360
there's a virus but if you just came from that arctic uh and you know for last month and all

41:19.360 --> 41:24.880
the time there's a lot of new stuff uh use the summaries would be very very different and obviously

41:24.880 --> 41:30.480
there's different sort of reading levels if you're doing summarization for kids versus adults and

41:30.480 --> 41:34.880
and so on. So I think summarization there's a lot of interesting things that we can still do in

41:34.880 --> 41:41.200
that thing. Well Richard thanks so much for taking the time to catch up with me and share with all

41:41.200 --> 41:47.280
of us what you're up to there. That sounds like really cool stuff and looking forward to checking

41:47.280 --> 41:52.400
in more frequently uh with you and keeping up with what you're doing there. Yeah, love your

41:52.400 --> 41:58.240
podcast and uh always always enjoyed talking about AI. So thanks for having me. Take care, thanks

41:58.240 --> 42:07.760
so much. All right everyone that's our show for today. For more information on today's show

42:07.760 --> 42:28.800
visit twomolai.com slash shows. As always thanks so much for listening and catch you next time.

