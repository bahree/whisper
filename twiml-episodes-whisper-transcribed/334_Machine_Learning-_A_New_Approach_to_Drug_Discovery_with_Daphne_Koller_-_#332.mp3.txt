Welcome to the Twimmel AI Podcast.
I'm your host Sam Charrington.
This week on the podcast, I'm happy to share just a few of the nearly 20 interviews I recorded
earlier this month at the 33rd annual NURRIPS conference.
If you've been waiting for the Twimmel pendulum to swing from workflow and deployment back
over to AI and ML research, this is your time.
We've got some great interviews and store for you over the upcoming weeks.
Before we move on, I want to send a huge thanks to our friends at Shell for their support
of the podcast and their sponsorship of this NURRIPS series.
Shell has been an early adopter of a wide variety of AI technologies to support use cases
across retail, trading, new energies, refineries, exploration, and many more, and is doing
some really interesting things, but don't take it from me.
Microsoft CEO Satya Nadella recently noted that what's happening at Shell is pretty amazing.
They have a very deliberate strategy of using AI right across their operation from the drilling
operations to safety.
Last year, the company established the Shell.ai Residency Program, a two-year full-time program
which allows data scientists and AI engineers to gain experience working on a variety of
AI projects across all Shell businesses.
If you're in a position to take advantage of an opportunity like this, I'd encourage
you to hit pause now and head over to Shell.ai to learn more, once again that Shell.ai.
And now on to the show.
Alright everyone, I am here in not-so-sunny Vancouver, continuing my conversations from the
33rd NURRIPS conference and I've got the pleasure of being seated with Daphne Kohler, founder
and CEO of Ensitro.
Daphne, welcome to the Twomo AI podcast.
Glad to be here.
Thank you.
Well, I historically or typically start these off by having my guests share a little bit
about their backgrounds and I will certainly allow you to do that, but I feel like we could
spend the entire time on your background, you've done so much in this space.
We were talking before we pressed record about your history even with this conference.
But without further ado, please introduce yourself to our audience.
Thank you.
I do feel like an old timer at this point.
I was doing machine learning long, long before it became popular.
So I really got into this field around 93 or 94, maybe even a little earlier than that,
have been a long time participant in this conference.
In fact was the program chair in 2007 and the general chair in 2008 and I very much remember
how in 2007 it was the first time the conference actually hit 1,000 attendees and 1,000 papers
submitted and we all thought that was really, really big and now we have...
12,000, 13 this year.
This is 13,000 attendees plus five on the waiting list who couldn't get in.
But I don't even know how many thousands of papers were submitted.
Well, the graphs I see are exponential, they are ridiculous.
And so I remember the conference and what we old timers feel is the good old days when
you could actually go and run into people that you know and I can't even find them because
there's just such a crowd that you can't even move.
So it's an interesting transition that we've seen in this space in the last three to five
years and in some ways it's been an amazing resurgence for this field and I'm super excited
and proud of what we as a community have accomplished but at the same time you sort of somewhat
miss the intimacy of this of the community as it used to be.
But anyway, coming back to the other parts of my background so I started to work in as
I said machine learning in the early 90s and at that point the data sets that we as machine
learning people had to work with were honestly kind of boring and lame.
So I remember the 20 news group data set which were a bunch of articles from 20 news
groups and one of the quote unquote challenge problems was could you figure out which news
group an article came from and it was rather not super interesting also from an aspirational
perspective not only from a technical perspective because you can't get a lot of enthusiasm
or please I couldn't get a lot of enthusiasm for classifying articles into news groups.
And so in the late 90s I started to look around for data sets that I felt would have more
of an aspirational nature to them and decided to work in the space of biology because at
that point biologists were actually starting to accumulate data sets that seemed more interesting.
Things like the first one I worked on was actually tuberculosis infections so there is
an interesting social network graph of who might have infected who with tuberculosis as
well as some clinical data for each of them.
I worked on some of the earliest data sets where people measured gene expression gene
activity profiles for cancer patients and what could you extract from that about the
types of cancer that might exist worked on some of the earliest human genetics data sets
as the human genome project came about.
So really at that time machine learning on biological data is actually much more interesting
on a lot of other kinds of machine learning both technically and certainly from the perspective
you feel like you're doing something potentially make a difference.
And so that was around the as I said the late 90s early 2000s and I worked in that area
for a long time.
Was it all at Stanford?
Yeah, this was all at Stanford with the exception of a short sabbatical at UC San Francisco
where I actually spent time in a real biology lab which was great.
And I think it was a really exciting trajectory at that point to see how more and more technology
developments were allowing biology to be measured in a quantitative way and not just biology
but also medicine.
So I worked on some really I think inspiring medical problems like one of them was on how
can you take the measurements that are already taken from the premature babies in a NICU?
These are teeny little babies that are sometimes 28 weeks gestational age 1500 grams or about
as big as the palm of your hand.
And by a combination of non-invasive measurements that machine learning could extract from the
bedside monitors that measure their heart rate, respiratory rate and pulse ox.
Can you predict much earlier which babies are going to need more tension and are going
to have more significant medical difficulties than some of the others?
It was actually really exciting to be able to discover new science if you will as a
byproduct of what the machine learning was able to predict.
So that was a lot of fun.
And so that was sort of where I thought my career would continue to evolve.
I fully expected to retire as an academic.
And then sort of this unexpected transition happened that where work that I'd been doing
at Stanford that had really nothing to do with my research.
It was a side interest in technology assisted education by a long process of a couple years
in collaboration with several others of my Stanford colleagues emerged in the launch
of the first three Stanford massive open online courses back in the fall of 2011.
And I don't think any of us had any expectation that this would turn out the way it did, but
when we looked at those courses where each of the three had a hundred thousand people
or more, it was kind of I was among that first batch in Andrew's course.
Okay.
Well, thank you for being our first or second batch.
Well, thank you for being one of our earliest users.
We are grateful.
And it was one of those moments in time when you look at your life and there's a huge
fork in the road.
And you could say, well, I can continue on my current path, which is a great path.
And likely if I do that, then this thing that I accidentally helped create would just
likely die away because no one else was going to like take it up and run with it.
And or I can, what I expected to was put my career and hold temporarily and go through
this other thing for a couple years and really get it off the ground.
And so at that point, Andrew and I both decided to take a leave of absence from Stanford
and go and find a course here together.
And so that was something that we did kind of got started in the fall of 2011 and then
our leave of absence became official in the beginning of 2012.
And then I ended up doing that for about five years from the, as I said, from about the
fall of 2011 to the fall of 2016.
And at some point in the middle of Stanford said, well, you know, your two years leave of
absence are up and so are you coming back?
And I said, well, not quite yet.
And I said, well, if you're not coming back, then you have to resign and I said, okay,
fine.
So I did.
And I don't regret that at all.
I mean, I guess at that point, it wasn't that standard to leave a chaired professorship
at a top university.
I think a lot more people are doing that now.
But I think it was the right thing to do because I don't believe the company would have survived
if I'd gone back to Stanford in 2014.
So I left Stanford in 2014, continue the course Sarah for another couple years.
And then in 2016, the company was on a great trajectory still is, by the way.
But it wasn't still as primarily a content company.
I mean, one can try and sprinkle machine learning here and there and try and make it better.
But it's not where the core of the business really lies.
And I realized that the company would do fine if I left, but whereas around me, when
I looked, machine learning was changing the world.
All of a sudden, all that vague promise that had wrong man to the field in the first place,
we were actually in a position to make that happen.
But one place that it hadn't had much of an impact was on the life sciences.
And that's really what brought me back because I felt like there was an incredible opportunity
to take machine learning and apply it in an area where I was one of the very few people
who could actually bridge those two worlds.
Because I've been doing it for a really long time and I think on one of the few people,
at least at the certain level of seniority, who was truly bilingual at this point.
And I think that's what you critically need in this space is a few people who can really
see both sides of it and bring them together in new ways rather than just kind of, here's
a problem that someone has already kind of cut and dried and defined clearly now, go
solve it.
Yeah.
Yeah.
And so there are a ton of ways that you can apply machine learning in the broad domain
of life sciences, healthcare, medicine.
The particular one that you are involved in is in drug discovery.
Right.
I've seen you talk about, and others talk about some of the context there, some of the
numbers there that kind of define, you know, how drug discovery is working for us.
Maybe you can share some of those.
Yeah.
So I think it's known to pretty much everyone.
You can't open a newspaper without seeing some discussion of drug pricing.
Yeah.
You know it's broken.
We know it's badly broken, but I don't think people fully realize where some of those
difficulties emerge from.
So I think there is a narrative out there that all of it is you because pharma companies
are kind of trying to gouge the consumer and the insurance companies in the government.
And I think it's certainly true that there has been some bad acting out there and it
in ways that are inappropriate.
But I think people also don't fully appreciate just how prone to failure this field is and
how much investment one needs to make in time and money to find even one successful
drug.
So if you look at the number similar to like startup investing, worse, I think.
Okay.
I mean, and because the costs are larger, I mean, the cost of making a single, getting
a single drug approved irrespective of whether it's a blockbuster drug that's going to make
a ton of money or just something for an orphan indication is $2.5 billion and rising.
We don't pump that much money into most startups.
No, we do not.
And now admittedly, this is when you account for the cost of all of the failures for all
of the things that didn't quite make it.
But even so, if you look at, well, how much amortized across the entire industry does
this cost to make one successful startup, it's not nearly $2.5 billion.
So I think there's, my analogy for this is that drug discovery is like a really long road.
It's, you know, 15 years is a not unreasonable estimate that has multiple forks in it.
One of those paths is going to get you to success.
Maybe if you're lucky, 99 will not.
You have no idea which of them is going to be more successful.
So oftentimes it's a bit of a gut instinct or a guess.
And when you take the wrong path, it's not that you find out in a matter of six months.
There's not such thing as a, okay, I'm going to do a product market fit like you do on
a consumer.
You don't fast fail.
It's three years and hundreds of millions of dollars before you slow fail.
And so it makes it very challenging to do this type of process.
And one of the things that we really are hoping to do is to use machine learning to build
where you might think it was a compass.
Something that when you get to these forks in the road, you have a predictive model that
is machine learning trained that says, you know, here's my probability distribution on
the success of each of those paths.
So here's the one that I recommend that you follow.
And that's something that we hope will help avoid a lot of the wrong paths that are currently
being taken and allow us to get to a successful drug much faster because you don't end up
taking all that time to follow the wrong forks and with a much, much lower cost because
if you're not spending all that effort on things that are not going to succeed, then hopefully
that will really help bend that ridiculous cost curve that has come to be called e-rooms
law, which is an interesting e-rooms law.
e-rooms law.
If you think about e-room, e-r-o-o-m, it's the inverse of Moore's law.
Wow.
Okay.
And Moore's law is the exponential increase in productivity on the tech side, e-rooms
laws, the exponential decrease in productivity on drug discovery in that the cost, the number
of drugs approved per billion U.S. dollars has been decreasing exponentially consistently
for the past 70 years.
Wow.
And in thinking about that law, is it, you know, how much of that is friction in the
discovery process regulation, that kind of thing versus some kind of fundamental, you
know, we just running out of things to try or, you know, we losing ground to disease
and generally, like, what's the way to think about all the components of that?
I think that's a really excellent question.
And people have written entire papers trying to tease apart the different factors here.
Partly, I think there's a legitimate case to be made that there's an increased regulatory
burden, some justified trying to be more careful about the lives and health of patients in
clinical trials, some less justified, just lots of bureaucracy and paperwork, doesn't
actually add value, but it'll cost money.
I think that's part of it.
I think another part of it is what Jack's channel was one of the bigger experts in the
space calls the better than the Beatles problem, which is that when you're in the
business of movies or books and you're looking for the next blockbuster, next song, next
movie, the movies from the past, the books from the past, people are looking for new stuff
because they may have already consumed that old stuff and they want a new piece of content.
That's not true for drugs.
I mean, you actually have to be better than all the previously approved drugs for this
to make sense.
Doctors are not looking for the next thing.
If you actually want to be better, if you want to actually have a market, you have to
actually be better than everything that's been come before and has been approved.
The floor keeps rising, if you will, the bar that you have to overcome keeps rising.
I think when you look at it in that light, there's a couple of different, you can think
of it as almost like a dichotomy, if you will.
There's things for which we've already done a pretty decent job of making drugs, cardiovascular
disease, diabetes, infectious disease.
There are some pretty good drugs out there.
The next drug really has to be better and to prove that it's better, you actually, in
many cases, need a very large and very expensive clinical trial.
Then there is the classes of diseases for which we really don't have any good drugs.
I think of those CNS disease of the neural, of the central nervous system is probably
the most obvious category.
That's because we have, it's such a complicated system.
We do not understand it, probably won't for a very long time in the level of mechanistic
understanding.
The model systems that we've been using developed drugs to disease in general, which are typically
animal models, are a very far cry from the human central nervous system.
It turns out that it just doesn't translate in the sense that we find drugs that make
mice smarter, have less, more empathic, have less neuronal death, and it just doesn't
translate into human disease, largely because the mice don't get the disease in the first
place.
Therefore, you are artificially creating a disease in the mouse, and when you cure the artificial
disease, it turns out to have very little to do with curing the real disease.
Interesting.
Interesting.
In Cetra, when you're thinking about this kind of compass analogy of what you do and whoever
the actor is in this case is at the fork in the road, are you using machine learning to
evaluate the efficacy potentially of some compound as a treatment, or are you applying machine
learning to the broader system, like was the probability distribution that you're creating
one that is incorporating market factors, and all these other things, are you really focused
on the biology at this point?
At this point, focus on the biology, and down the line, we might focus on the chemistry,
and then maybe on things like the selection of patients for the clinical trial to identify
the ones that are more likely to be responsive to the drug, and then downstream from that
is, can we use machine learning to have better biomarkers of efficacy so that you can actually
tell whether a patient is responding to your drug in ways other than literally a questionnaire
that's filled out on sheets of paper that a nurse then transcribes into the computer,
which is how this is often done, and even downstream in the manufacturing.
I think there's a lot of opportunities before we get to market factors and things like
that.
Right now, our primary focus is really on the biology and making a prediction on if I make
this intervention in a human, or even in this particular human, how likely is this human
to respond?
What is the clinical outcome going to be of that intervention?
That's a very challenging problem to make a prediction on, but I think there's some
of your tools out there, both on the machine learning and on the biology side, that if you
put them together, give us a chance of making human predictions.
That sounds to me like a personalized medicine type of application, which seems much further
down than the, I'm at a compass trying to develop a drug.
Yeah, no, the personalization is, I think, not really the key focus of what we're doing,
but rather the recognition that a lot of the drugs that people have tried haven't succeeded
because you're treating multiple diseases with one drug.
So if you look at the big success in the field of precision oncology in the last decade
or so, a lot of the successes have come from the realization that breast cancer, for
instance, is not one disease.
The patients who are hurt too positive are very different from the patients who have
a block of one mutation, and you treat them with completely different therapies.
Colleagues of mine who worked at Genenteic at the time that herceptin, which is the drug
that targets her two positive breast cancers, was developed, say that if you had applied,
if you tried out herceptin in an all-comers breast cancer population, you'd need a clinical
trial of about 10,000 women in order to demonstrate even the tiny effect size because you're
averaging out on a whole bunch of people who are not going to respond, but nevertheless
have side effects.
So in your case, it's less about personalization than targeting, really.
Exactly.
Targeting to the right patient population.
You mentioned part of the opportunity here is kind of the application of new machine learning
techniques to this round.
What's kind of the landscape of techniques that you're able to apply?
One of the things that we are doing is relying both on new developments in machine learning,
but at the same time, on new developments in high throughput biology and bioengineering.
That's a space where maybe less familiar to folks who are more expert in the technology
side of things, but there's been as much progress on that side as there's been on machine
learning.
So, for instance, at this point, there is the capability for us to take a small sample
of your skin or a small sample of your blood, then transform that some of those cells into
what are called stem cells, and these are those cells that can then turn into any lineage
in your body.
I can basically create Daphne neurons, or Daphne cardiomyocytes, or Daphne hepatocytes,
and all of them have the genetics that I have.
So, if I have a certain propensity to disease that manifests in that particular cell type,
you could potentially see it there, but it's the right cell lineage.
And so, I have the ability at this point to basically do population diversity, but at
the cellular level.
And I have the further ability to use amazing technologies like CRISPR that allow us to
modify the genome to even create mutations that we know are disease-causing.
So, for instance, if I want to really see what a very high penetrant version of the disease
looks like, I can introduce that mutation into a normal genome and see what the difference
is between the width of mutation without the mutation.
So, think of it as the ability to artificially create training sets on what disease looks
like at the cellular level.
Interesting.
And so, now, you think about, well, that gives me a ton of data because you have potentially
hundreds of genetic backgrounds, maybe even more, with tens of thousands of readouts from
each of those cells, like the kind you might get from super-resolution microscopy, or
transcriptional measurements of all of the genes in the cell.
And now, you ask yourself, okay, with all of those measurements, if this is what healthy
looks like, and this is all the population of unhealthy, what differentiates them?
What does healthy cell look like relative to an unhealthy cell?
And are the unhealthy cells, or are they all one big homogeneous cluster, or are there
subclusters that are very distinctive of the molecular level?
And then, with that, you have an understanding of what the disease looks like at the level
of cellular phenotypes, as they're called.
And because it's an intervenable system, it's not a human.
We're doing an experiment and a human is really hard.
You can ask yourself, if I put this compound into a bunch of cells that are from this cluster,
does that revert the phenotype back to something that looks healthier?
And if it does, maybe that's a drug that will also revert the phenotype at the clinical
outcome level, which is what we ultimately care about, because you want to hear people
not cells.
So basically, you can think about the machine learning as helping us to distinguish between
different cellular phenotypes in a way that aligns with human clinical outcome.
Interesting.
And so, from that perspective, is it primarily kind of unsupervised clustering types of
approaches that you find most useful in your work, or is it a broad set of things?
It's actually a broad set of things, because we do have some supervision signal, not as
much as you would like, because there is, for instance, some understanding that if you
have this mutation, then your chances of disease are really high.
So you can think of it as kind of like a bit of a supervision signal, or you have supervision
signal in the sense of, here is a hundred of these what's called induced pluripotent stem
cells, IPS.
Induced pluripotent stem cells.
So pluripotent means they can go into multiple lineages, and induced means I created them
from a skin sample rather than it's a fetal stem cell.
So you have these IPS cells that you got from patients, that's the positives, and you
have the IPS cells that you got from controls, that's the negatives.
So there is a little bit of supervision signal, but certainly not as much as you want, and
so I think if you had to put a label on it, it falls largely in the category of weekly
supervised learning, not completely unsupervised, not completely supervised, but somewhere in
between.
Got it, got it.
And as you describe this, you can almost envision a kind of closed loop process where you're
able to, you know, I've seen some of the like the bio robots that can, you know, take
an array and do all kinds of experiments at, you know, high throughput.
Are you there yet?
Absolutely.
Okay.
Well, we, sorry.
Let me, let me.
For those who can't see you, she's getting very excited.
Yes.
Now, that's definitely in our roadmap, and it's what we're working towards.
The robots are already there.
Yeah.
They're doing a lot of the high throughput, more menial work automatically at this point.
We don't have the full-blown closed loop system yet, because we only just got the lab
up and running.
But the goal is exactly that we would have the ability to take data off the instruments,
process it automatically, and then use what we see to guide the next round of experiments.
And I think that's going to be incredibly powerful, both at the high level of, you know,
the high, just the experiments in terms of what makes for sick versus healthy cells.
But we actually embed machine learning in every single part of what we do.
So for instance, imaging a plate with imaging plate at 40X resolution across multiple
fluorescent channels can take as long as 30 days.
So very long experiment, because you just have to do tile by tile by tile.
Do you really need to image every well and every tile in every well at 40X resolution,
or can you imagine using machine learning to say, I'm going to do a very quick pass at
10X.
See where they're sitting.
See where the interesting things are.
And then dig down a higher resolution into the cells that are more likely to be interesting.
You can think about machine learning in all sorts of different places, or here's another
example.
Do you really need to image every single one of those fluorescent channels, or can you
infer from some channels what the other ones are going to be, and then image fewer channels?
It sounds easy.
But then I'm envisioning these, you know, you're not building the robots from scratch, you're
getting them from roast or whoever or life sciences, and, you know, they're proprietary
about the way that they control their robots.
Like, can you easily insert your machine learning models into these off-the-shelf tools?
So first of all, is that even an issue?
Am I picking at the right thing?
No, you're here.
You're absolutely right.
It is definitely an issue.
Some manufacturers are more open than others in terms of making their APIs available so
that you can control the robot, you can control the microscope, for instance, from the outside.
And sometimes we have to actually kind of hack into this a little bit, but sometimes
they're being more flexible.
And in some cases, we actually build custom hardware because we have to do that, because
it provides us the flexibility that we need both on the hardware side as well as the software
side.
Yeah.
So we've talked through some examples of the kinds of problems that you're solving this
way.
I think there's still a hole for me in like the where do you start?
What's the first step?
What's the next step like?
So I think the first step is really to build a team that is truly cross-functional and
is able to communicate across what is usually a chasm between these two disciplines.
Most machine learning people took biology, maybe back in high school sometime, and have
a vague recollection of what they learned, but not much beyond that.
I mean, there's so much there in what you're doing is biology, it's chemistry, it's genetics.
And on the other side, most biologists don't really know much about computer science or
machine learning.
They may have done some whatever data analysis and an Excel spreadsheet, but it's really
two communities that don't have a lot of common language.
And one of the things that we're really building is a community of people who really either
have a foot niche camp and we have a bunch of those people who are truly bilingual, or
even if they don't, they have a real willingness to kind of reach out across the chasm and have
a meaningful collaboration.
And I think that's absolutely essential because so much of what we do really requires this
interaction between both sides.
And what we find today is that once you bridge that gap, you actually are in some ways
programming into programming language simultaneously.
There is the programming language of whatever TensorFlow or PyTorch, which we all are used
to when we think about computational modules.
But then there is equally valid experimental modules that you can kind of fit in.
Like here's a set of CRISPR guides, introduce those CRISPR guides into the following set
of cells, differentiate this set of cells into the following lineage.
And I'm not saying each of those steps is easy.
They're actually harder in some ways than the computational steps because it's a little,
I mean, big subologies finicky and these are live cells and they don't always do what
they're told, unlike the bits in the computer, which generally do do what they're told.
But you can still create a level of abstraction on those biological steps that you can incorporate
into your overall procedure.
So your procedure now has blocks that are computational and blocks that are biological.
And it's a single, almost integrated process where these different pieces fit together.
And the whole is considerably larger than the sum of the parts where the typical approach
is, okay, the biologists create some data, throw it over the fence, and then someone does
the analysis.
When you have this sort of real integration of those two steps, the space of what you
can do is obviously exponentially larger.
And therefore, it opens up capabilities to even, to think about problems that you would
never even have thought about far less been able to solve without the integration of those
tools.
I've got to imagine as the CEO of a company that depends so heavily on finding people
with these two independently unique skillsets, not to mention together, you maybe have an
interesting perspective on auto-ML and lowering the barriers to getting to the, at least on
the computational side, any reaction to that?
I think that auto-ML is a great enabler in some ways in that when you get to the point
that you have defined a problem that has well defined input output specifications, you
know, you want to train your algorithm on the following, you want to train whatever predictor
on the following dataset with these inputs and these outputs and the subjective function,
then it allows you to avoid some of the annoying nitty gritty of hyperparameter architecture
search.
It doesn't do at all is tell you what problems to solve and what's the right data to create
because we create our own data, so what data do you even create?
What's the right objective to train the model too?
Because that actually matters if you train your algorithm to regression and might not do
as good a job at classification and vice versa and which is the right one for the problem
that you're trying to solve is not clear and this is the simplest example that one can do.
So I think it's going to help but I don't think it's going to solve the problem for us
and I think also for a lot of other people because the heart of what a really good machine
learning person can do is understand the domain enough that you can identify in your problems
if no one has thought about.
It sounds like that's the stage of the journey that your company is at very much so.
And I think that's hopefully the stage of the journey that we will be at for a long
time, not that we won't be solving some of the problems that we come up with today,
but I think there's so many of those problems that one could imagine making a big impact
on in the drug discovery and development process that even once we nail some of the earlier
problems and maybe move more towards this iterative refinement mode, there's going to
be new problems that we're going to have to tackle.
When you think about the broad landscape of problems that gets us most quickly to a
healthier population, how do you segment those or think about that landscape in terms
of where are the opportunities for folks that are interested in applying machine learning
to make folks healthier?
Oh, I see.
The drug discovery and in particular, you know, you're working on a specific niche within
drug discovery.
What are some other things that you think are interesting?
Well, I think we're working today on a particular phase in the drug discovery process.
It's not by any means the place that will end up because this for us is the beginning
of the journey towards building what I hope will be the, you know, first fully data-enabled,
data-driven drug discovery and development companies so that every step of the process
is now based on data production and machine learning as core technologies.
And I think that you've seen companies like that emerge in other parts of the tech space
where, for instance, Amazon is a fully data-enabled retail company and it's not just in, you
know, the early days of just being a little bit better at recommending items to you and
having the orders be all managed automatically now to every step of the way they're enabled
using data and machine learning and technology.
We hope to be doing that for drug discovery and development.
Before we go to other areas, are the traditional drug discovery companies that far behind?
You know, I think there are islands in some of those companies where they're trying to
bring in machine learning and technology to accelerate things, you know, and it varies.
There was a recent announcement about one company that's using it to accelerate some manufacturing
processes and another one that's using it to enable better design of small molecule binders
to a particular protein. Most of those efforts fall into the category of, here's a problem
I'm already solving anyway. I'm solving it, not maybe not in the perfect way, maybe it's
too slow, maybe I can introduce some additional optimization, so I'm going to use a machine
learning document.
It's probably going to reduce the cost by whatever 20% or something like that. That's great,
it's not transformative. I don't know of any companies that are in the process of saying,
okay, no, we're just going to have to rethink the process from the ground up.
One way of thinking about this, I don't know if it's true, is to ask whether of the big tech
giants that are really fully data enabled from beginning to end, did any of them actually emerge
from the existing incumbents? Amazon did not emerge from Walmart and Netflix didn't emerge
from blockbusters or one of the Hollywood studios and Google didn't emerge from the yellow pages.
These are all kind of the non-peck enabled precursors to those companies and in some cases,
you need to ask yourself, okay, if I was building this from scratch, what would it look like?
So, answer the second part of your earlier question. I think clearly there's opportunities
beyond drug discovery and development. There's some interesting work happening in the device space
in terms of using the mobile phone that we all carry in our pockets to give us both
better tracking of our health state and better nudges to take a healthier lifestyle. Move us
towards the tricorder? Yeah, move us forward to tricorder. I think the challenges here are that the
folks that carry these devices and require those lifestyle changes are not nearly as obedient as
the folks on Star Trek. So compliance is an issue. I think there's some interesting work that's
happening in that space, but it's a road that has its own challenges. It's not maybe as much of
a scientific challenge, but there's a lot of questions about human psychology and affecting
behavior change. I think there's some interesting work that's happening in hospitals as well as
potentially in insurance companies for early warning systems for getting people into care or
warning the physicians and an emergency care ward that sounds a lot to crash or about to have
a sepsis attack or something, and I think that's an interesting space. I think there's a lot of
places where one can ask with the new tools that we have in hand the new ability to collect large
amounts of data. What are problems that we can just solve that almost really ever try to solve
before? Awesome. Awesome. Well, definitely thanks so much for taking some time to share with us
what you're up to. Clearly very exciting stuff and pleasure meeting and chatting with you.
It's been a pleasure for me too. Thank you for having me. Thank you.
All right, everyone. That's our show for today. For more information on today's guest or our
NURPS podcast series, head over to twimlai.com slash NURPS 2019. Thanks once again to Shell for
sponsoring this week's series. Check out the shell.ai Residency program by typing shell.ai
into your browsers address bar. Thanks so much for listening. Happy holidays and catch you next time.
