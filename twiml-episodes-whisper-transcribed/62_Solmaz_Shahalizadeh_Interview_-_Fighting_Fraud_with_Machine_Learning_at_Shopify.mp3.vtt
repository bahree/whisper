WEBVTT

00:00.000 --> 00:15.920
Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting

00:15.920 --> 00:20.880
people doing interesting things in machine learning and artificial intelligence.

00:20.880 --> 00:23.400
I'm your host Sam Charrington.

00:23.400 --> 00:27.760
The podcast you're about to hear is the first of a series of shows recorded at the Georgian

00:27.760 --> 00:31.600
Partners Portfolio Conference last week in Toronto.

00:31.600 --> 00:37.120
My guess for this show is Solma's Shah Alizadeh, director of merchant services algorithms

00:37.120 --> 00:38.960
at Shopify.

00:38.960 --> 00:43.760
Solma's gave a great talk at the GPPC, focused on her team's experiences applying machine

00:43.760 --> 00:47.920
learning to fight fraud and improve merchant satisfaction.

00:47.920 --> 00:53.960
Solma's and I dig into, step by step, the process they used to transition from a legacy, rules

00:53.960 --> 01:00.920
based fraud detection system to a more scalable, flexible one based on machine learning models.

01:00.920 --> 01:05.240
We discussed the importance of well-defined project scope, tips and traps when selecting

01:05.240 --> 01:10.140
features to train your models, and the various models, transformations and pipelines, the

01:10.140 --> 01:15.880
Shopify team selected, as well as how they use PMML to make their Python models available

01:15.880 --> 01:19.040
to their Ruby on Rails web applications.

01:19.040 --> 01:23.360
Georgian Partners is a venture capital firm whose investment thesis is that certain tech

01:23.360 --> 01:29.280
trends change every aspect of a software business over time, including business goals, product

01:29.280 --> 01:34.960
plans, people in skills, technology platforms, pricing and packaging.

01:34.960 --> 01:39.520
Georgian invests in those companies best positioned to take advantage of these trends and then

01:39.520 --> 01:44.280
works closely with those companies to develop and execute the strategies necessary to make

01:44.280 --> 01:46.000
it happen.

01:46.000 --> 01:50.560
Applied AI is one of the trends they're investing in as our conversational business and

01:50.560 --> 01:52.320
security first.

01:52.320 --> 01:55.760
Georgian sponsored this series and we thank them for their support.

01:55.760 --> 02:01.600
To learn more about Georgian, visit twimmalai.com slash Georgian, where you'll also be able

02:01.600 --> 02:08.040
to download white papers on their principles of applied AI and conversational business.

02:08.040 --> 02:12.520
Before we jump in, if you're in New York City on October 30th and 31st, we hope you'll

02:12.520 --> 02:16.920
join us at the NYU Future Labs AI Summit and Happy Hour.

02:16.920 --> 02:20.840
As you may remember, we attended the inaugural Summit back in April.

02:20.840 --> 02:25.160
The fall event features more great speakers, including Karina Cortez, head of research

02:25.160 --> 02:30.560
at Google New York, David Venturelli, science operations manager at NASA Ames Quantum

02:30.560 --> 02:36.760
AI Lab, and Dennis Mortensen, CEO and founder of startup x.ai.

02:36.760 --> 02:44.760
For the event homepage, visit AI Summit 2017.futurelabs.nyc and for 25% off tickets, use

02:44.760 --> 02:47.280
the code twimmal25.

02:47.280 --> 02:53.040
For details on the Happy Hour, visit our events page at twimmalai.com slash events.

02:53.040 --> 02:55.520
And now on to the show.

02:55.520 --> 03:02.880
All right, everyone.

03:02.880 --> 03:09.440
I am here at the Georgian Partners Conference and I have the pleasure of being with Solma

03:09.440 --> 03:14.160
Shah Ali Zadeh, who is Director of the Data Team at Shopify.

03:14.160 --> 03:16.840
Solma is welcome to this week in Machine Learning and AI.

03:16.840 --> 03:18.600
Thank you very much for having me, Sam.

03:18.600 --> 03:23.000
It's great to have you, and I especially love when I get an opportunity to interview

03:23.000 --> 03:27.280
folks who have listened to the podcast before, so thank you so much for listening.

03:27.280 --> 03:28.280
You're welcome.

03:28.280 --> 03:29.280
Yeah.

03:29.280 --> 03:31.280
I subscribe, I think, like nine minutes ago or something like that.

03:31.280 --> 03:32.280
Awesome.

03:32.280 --> 03:33.280
Awesome.

03:33.280 --> 03:34.280
Thank you so much.

03:34.280 --> 03:37.400
Why don't we get started by having you tell us a little bit about your background and

03:37.400 --> 03:40.400
how you got into data science and machine learning?

03:40.400 --> 03:41.400
Sure.

03:41.400 --> 03:43.840
In my undergrad, I studied computer science.

03:43.840 --> 03:47.080
And towards the end of my undergrad, one thing I realized is that I'm really interested

03:47.080 --> 03:51.080
and passionate about using computer science and the little bit of machine learning that

03:51.080 --> 03:53.800
I knew at that stage to solve problems in other domains.

03:53.800 --> 03:59.520
So as I figured that out, I did a master's in bioinformatics and I went to Sweden and I studied

03:59.520 --> 04:03.040
there and as part of my thesis project, I actually worked with Sloan Kettering Cancer

04:03.040 --> 04:07.480
Center in New York, trying to predict what happens in the cell after you give it sort

04:07.480 --> 04:11.880
of multiple perturbations or different drug cocktails and how the structure of the cell might

04:11.880 --> 04:12.880
change.

04:12.880 --> 04:16.040
And at the time, we used actually recurrent neural networks which were not hot then because

04:16.040 --> 04:20.000
that was 2006 and everyone was like giving us grief about like, oh, this was 80s, like

04:20.000 --> 04:24.320
this was cool in 80s while you're using it now, but it lent itself very well to our problem.

04:24.320 --> 04:29.520
So we used it there and then after that, I went to McGill in Montreal and I did another

04:29.520 --> 04:33.640
master's that I was like more focused on machine learning and compare science and as part

04:33.640 --> 04:34.640
of my thesis project.

04:34.640 --> 04:39.520
I worked very closely with the department of oncology and molecular biology and we used

04:39.520 --> 04:41.560
my career rate data to print it.

04:41.560 --> 04:42.560
My career rate?

04:42.560 --> 04:46.440
Yeah, so it's like a very simplistic way to explain it is like you have a chip and on

04:46.440 --> 04:50.680
it, you have these probes that you can measure the expression of different genes.

04:50.680 --> 04:51.680
So human body?

04:51.680 --> 04:52.680
My DGPCR.

04:52.680 --> 04:56.720
Sort of yeah, but like scaled that to thousands of genes on it.

04:56.720 --> 05:00.960
So we had the data on that and we were trying to use that data which we had captured at

05:00.960 --> 05:05.720
the moment before people have got any treatment to see if there is any signal in that data

05:05.720 --> 05:09.800
that we can save there likely to have a recurrence of the breast cancer.

05:09.800 --> 05:13.680
And the usage of that is that by knowing it, you can sort of like not give as much of

05:13.680 --> 05:18.160
a hard treatment to people who don't need it and also plan for the recurrence for people

05:18.160 --> 05:19.160
that are likely to get it.

05:19.160 --> 05:20.840
So that worked really well.

05:20.840 --> 05:25.640
But one of the key learnings I had there was that if you are building a computational

05:25.640 --> 05:30.000
solution and you want people in other domains to use it, the first thing you have to learn

05:30.000 --> 05:34.240
is to actually speak their language and understand how they explain their problems.

05:34.240 --> 05:39.240
So remember as like a rookie grad student, I had this presentation, all of my p-values

05:39.240 --> 05:43.720
and all the like statistical factors about why my predictor was awesome is on that presentation

05:43.720 --> 05:47.800
and I started and you could see like after the second slide, the lighting like the eyes

05:47.800 --> 05:51.760
of the molecular biologist is gone, like they are not listening, there's nothing keeping

05:51.760 --> 05:52.760
them going.

05:52.760 --> 05:59.200
And then through that I learned how to understand what's the actual real challenge for my collaborators

05:59.200 --> 06:02.360
and how to explain my solutions in relation to that.

06:02.360 --> 06:05.800
And then like towards the end of my degree it was like really engaging like I felt like

06:05.800 --> 06:10.360
I understand in their domain on top of machine learning and I think that was like a learning

06:10.360 --> 06:11.960
that I kept with myself.

06:11.960 --> 06:16.840
And then for a while I worked there more instantly, an investment bank and then I joined Shopify

06:16.840 --> 06:18.240
four years ago.

06:18.240 --> 06:22.680
So Shopify is a leading cloud-based commerce platform, it allows you to sell on multiple

06:22.680 --> 06:29.760
channels from like brick and mortar stores to huge, huge sales and enterprise like Tesla

06:29.760 --> 06:33.920
Motors or Budweiser, all of these big brands, but also very small merchants as well.

06:33.920 --> 06:38.640
So I joined Shopify, we were in the process of like changing our data warehouse and prepping

06:38.640 --> 06:39.640
for IPO.

06:39.640 --> 06:43.040
So it was really important to make sure we really understand the data we were capturing

06:43.040 --> 06:47.040
and we can also have like clear definitions for the metrics that we were going to share

06:47.040 --> 06:48.040
and all of that.

06:48.040 --> 06:49.360
So I was part of the team that worked on that.

06:49.360 --> 06:52.760
But then after IPO we realized, well, we have all of these machine learning expertise

06:52.760 --> 06:57.240
in house and we have also all of this data about different aspects of commerce.

06:57.240 --> 06:58.240
Right.

06:58.240 --> 07:02.080
And beauty of commerce is that it's like messy, it's real world, you know, there's merchant

07:02.080 --> 07:06.760
trying to fulfill orders in their basement of their home and their people having like thousands

07:06.760 --> 07:09.400
of orders coming to a second and they have to deal with that.

07:09.400 --> 07:14.400
So that brings with itself so many opportunities to take repetitive tasks out of the daily

07:14.400 --> 07:18.400
life of a merchant and an entrepreneur and give them back either the gift of time or

07:18.400 --> 07:21.280
the gift of money that they were spending on something else.

07:21.280 --> 07:24.440
And that's like has been like fascinating part of my job at Shopify.

07:24.440 --> 07:29.880
So last two years I focused mostly on machine learning teams, data science teams that build

07:29.880 --> 07:31.280
products that are powered by data.

07:31.280 --> 07:36.080
So one of this is our order fraud detection that runs on real time on every single order.

07:36.080 --> 07:41.440
The other one is our cash advance product where we basically give cash to our merchants

07:41.440 --> 07:44.960
because we think that's the amount and that's the right time to give them the capital

07:44.960 --> 07:48.120
to help them grow their business and they return that money.

07:48.120 --> 07:51.960
And now we're trying to like bring sort of this like basic level smartness to other

07:51.960 --> 07:55.520
products such as that are shipping and fulfillment and things like that.

07:55.520 --> 07:56.520
Okay.

07:56.520 --> 07:57.520
That's where I am.

07:57.520 --> 08:03.920
We spoke at length about the order fraud problem and your approach and solution to that earlier

08:03.920 --> 08:05.400
today at the conference.

08:05.400 --> 08:09.600
And why don't you tell us a little bit about the problem there, the context that you're

08:09.600 --> 08:11.840
trying to apply machine learning to.

08:11.840 --> 08:12.840
Sure.

08:12.840 --> 08:16.800
So as I was talking today, what happens is like all of our merchants big or small have

08:16.800 --> 08:17.800
one thing in common.

08:17.800 --> 08:20.320
They are there to make sales to succeed.

08:20.320 --> 08:23.760
What happens is that they see an order come through their store, they go ahead and fulfill

08:23.760 --> 08:28.640
it and then because so they look at the order nothing looks suspicious, they fulfill it

08:28.640 --> 08:33.600
like six months goes by and then they receive a charge back from the credit card company.

08:33.600 --> 08:37.680
So they're out of the item because they've already fulfilled it and sent it to the customer.

08:37.680 --> 08:42.640
They are out of the money for the sale because the credit card company refunds the the amount

08:42.640 --> 08:44.560
and they also receive a charge back fee.

08:44.560 --> 08:46.160
That really cuts into their cash flow.

08:46.160 --> 08:49.760
So you don't have to have too many charge backs to feel the impact.

08:49.760 --> 08:52.160
The other side of it is the emotional factor.

08:52.160 --> 08:56.200
So we are saying like, okay, you know, you focus on building the best product that you

08:56.200 --> 08:58.240
can, putting it in front of the right audience.

08:58.240 --> 09:03.800
And now we now we somehow have to tell them to be okay that somebody across the universe

09:03.800 --> 09:07.720
from you is using stolen credit card information to buy from you like that on an emotional

09:07.720 --> 09:09.480
level is unsettling.

09:09.480 --> 09:14.480
The other thing, the other reason we pick it as one of the first areas to tackle with machine

09:14.480 --> 09:19.120
learning is the fact that it's really back office work like becoming an expert in fraud

09:19.120 --> 09:22.400
detection is not going to make you a better product designer.

09:22.400 --> 09:24.400
It's just not a core skills of our emergency.

09:24.400 --> 09:26.000
The customer should have to think about.

09:26.000 --> 09:27.000
Exactly.

09:27.000 --> 09:30.840
So that's where we thought, okay, we have the data, we have the knowledge and we can scale

09:30.840 --> 09:35.520
the solution so that not only the big merchants can benefit from it, but the merchants who

09:35.520 --> 09:39.760
start on the platform and on their very first order, they can get this analysis that

09:39.760 --> 09:42.560
back by a decade worse up their the worth of data.

09:42.560 --> 09:46.600
And that's why we picked this problem as the first one to tackle with machine learning.

09:46.600 --> 09:53.880
Okay, was the problem previously being managed manually where there's some group of analysts

09:53.880 --> 09:59.120
that were doing this or was there not a prior solution in place for order fraud detection?

09:59.120 --> 10:02.920
We had actually a prior solution and we have a group of risk analysts in house, so it's

10:02.920 --> 10:07.560
like a combination of the two, but the prior solution was built like five years ago and

10:07.560 --> 10:11.320
I think it was good for the time it was built, but it had very hard-coded rules.

10:11.320 --> 10:17.120
So it would say things like if the order was placed using a web proxy, then probably it's

10:17.120 --> 10:18.120
fraud.

10:18.120 --> 10:19.120
Right now, we see many people use web proxies.

10:19.120 --> 10:23.800
Even like you're here, you want to order something from US, you probably use a web proxy.

10:23.800 --> 10:26.400
But we're not going to the details of the rules that was there.

10:26.400 --> 10:29.800
The problem was like the rules were not learning on it.

10:29.800 --> 10:30.800
Exactly.

10:30.800 --> 10:31.800
Yeah, exactly.

10:31.800 --> 10:32.800
They were static.

10:32.800 --> 10:37.760
So I think it serves our product for the time it was working, but also we have had this

10:37.760 --> 10:39.960
crazy hockey stick growth.

10:39.960 --> 10:45.720
So as we have had this many more merchants and more visibility into sales, it became

10:45.720 --> 10:50.200
apparent that we can make a difference by using our own data.

10:50.200 --> 10:53.480
So what were the steps in kind of getting to a solution to this?

10:53.480 --> 10:56.320
What was the first thing that you had to figure out?

10:56.320 --> 10:59.800
The first thing that you have to figure out, and it's common across any machine learning

10:59.800 --> 11:03.800
problem, is actually try to define what you're trying for self.

11:03.800 --> 11:06.600
So it sounds very basic, right?

11:06.600 --> 11:08.000
We're trying to fight fraud.

11:08.000 --> 11:09.600
Yeah, exactly.

11:09.600 --> 11:13.680
But then they find like, okay, I want to cash fraudulent transactions, but I actually want

11:13.680 --> 11:18.200
to do it before the merchant has gone ahead and fulfilled the order.

11:18.200 --> 11:22.520
So that brings some practicality requirements to the solution that we offer.

11:22.520 --> 11:27.840
So any machine learning algorithm and system I build, it has to be able to run on every

11:27.840 --> 11:33.400
single order as they go through without slowing down the platform, without having downstream

11:33.400 --> 11:35.160
processes have to wait for it.

11:35.160 --> 11:39.720
So that would itself brought some hard requirements on the kind of solutions we can do.

11:39.720 --> 11:43.760
The next step was, of course, okay, so we have a classification problem, so we want to

11:43.760 --> 11:46.960
classify these transactions to fraud and non fraud.

11:46.960 --> 11:51.280
Let's see if we can actually clearly define what's the fraudulent order.

11:51.280 --> 11:57.880
So we did some digging there to make sure like our definition is correct and also can capture

11:57.880 --> 11:59.680
if they're anomalies or they're changes.

11:59.680 --> 12:04.520
So for example, if we are relying on dispute codes from payment gateway or a bank and

12:04.520 --> 12:09.000
that happens to change, like let's make sure we have automatic detection in place or

12:09.000 --> 12:13.760
some way so we learn, okay, you know what, the fact that I haven't received anymore fraud

12:13.760 --> 12:19.080
is not because fraud has gone down or has gone up, it's just the code that I used to capture

12:19.080 --> 12:20.080
fraud has changed.

12:20.080 --> 12:21.080
Okay.

12:21.080 --> 12:23.840
So that making sure that basically the targets of your prediction are correct and then we

12:23.840 --> 12:25.920
got to like, investigating inputs.

12:25.920 --> 12:30.600
And that's kind of an interesting story because you know, I talked about like the massive

12:30.600 --> 12:36.240
pools of data we have, like over the last year we saw just last year, 100 million customers

12:36.240 --> 12:38.840
placed orders on Shopify store.

12:38.840 --> 12:42.760
So for these customers, we know the path they took to go to the product page, how much

12:42.760 --> 12:47.000
time they spend, what's their color preference, so we have all of this information.

12:47.000 --> 12:52.560
But at the same time, when you tackle a prediction problem, you also have to know, okay, of all

12:52.560 --> 12:56.360
of these features that I have, which of them I'm actually going to have available at the

12:56.360 --> 12:58.240
time that I'm making the decision.

12:58.240 --> 13:05.240
So for example, if I'm going to use the number of orders this customer has placed in the

13:05.240 --> 13:10.680
past, like, how am I going to have that aggregate count of orders available at the time of production?

13:10.680 --> 13:14.520
And that brings again, like another layer of practicality to features that you can put

13:14.520 --> 13:15.520
into your model.

13:15.520 --> 13:18.720
So we went through that right now, we can actually, we've built like internal services where

13:18.720 --> 13:22.240
we can get aggregate data, we can get real-time data aggregations.

13:22.240 --> 13:24.880
And that of course gives a boost to the models.

13:24.880 --> 13:30.760
Part of that is the architectural challenge of making sure that data is available.

13:30.760 --> 13:35.640
But then in your talk, you also mentioned, you know, at one point you were building your

13:35.640 --> 13:41.640
model around some feature that actually wouldn't be available to the model for months later.

13:41.640 --> 13:43.320
Yeah, that's true, actually.

13:43.320 --> 13:48.320
So that's one of the main challenges in fraud is that people can decide to place it charged

13:48.320 --> 13:52.760
back up to a year, credit card companies allow you to take a year and say, like, look,

13:52.760 --> 13:55.320
I believe this was a fraudulent charge at my card.

13:55.320 --> 13:58.760
So we started looking at data and realized, okay, most of the fraud actually comes back

13:58.760 --> 14:00.200
within the six months.

14:00.200 --> 14:04.640
So we define our target as like, okay, has this transaction resulted in charge back in

14:04.640 --> 14:05.640
six months or not?

14:05.640 --> 14:06.640
Okay.

14:06.640 --> 14:10.320
But that also means for any transaction, we actually know the full ground.

14:10.320 --> 14:15.080
We have to wait for the full ground truth for up to six months.

14:15.080 --> 14:16.400
So that's a challenge.

14:16.400 --> 14:20.160
So we have to make sure, okay, are there any leading indicators that we can use?

14:20.160 --> 14:25.400
So if I add a new feature to a model and I want to see how that's going to work for new

14:25.400 --> 14:29.160
orders, I have to actually wait six months to see what the prediction, if the prediction

14:29.160 --> 14:30.480
is correct or not.

14:30.480 --> 14:33.280
So some of it we deal with it by using historical data.

14:33.280 --> 14:38.280
If we have the feature available in past, if not, we depend if the feature is like really

14:38.280 --> 14:40.600
out of nowhere, we don't know we have to wait.

14:40.600 --> 14:45.040
But if the feature, like if we can get a degree of confidence with leading indicators and

14:45.040 --> 14:49.920
we go with that, we say, okay, has the ratio of fraud we've seen within the two weeks gone

14:49.920 --> 14:50.920
off or down.

14:50.920 --> 14:54.160
So we try to do that to have like a faster iteration speed.

14:54.160 --> 14:55.160
Okay.

14:55.160 --> 15:01.320
And even in terms of the definition of fraud, you know, I'm just thinking out, you know,

15:01.320 --> 15:06.840
there's, you know, stolen credit cards, you know, there's people that I always worry

15:06.840 --> 15:11.360
about this as like a, you know, an eBay seller, like you sell something and you ship it

15:11.360 --> 15:13.760
to someone they say they never got it, but they got it.

15:13.760 --> 15:18.640
That's another, like did you did a lot have to go into actually defining the types of

15:18.640 --> 15:23.840
fraud, did you model for every kind of fraud or just a specific subset of the possible

15:23.840 --> 15:25.920
fraudulent universe?

15:25.920 --> 15:30.600
For the first step, we actually decided to select a subset of fraudulent universe, as you

15:30.600 --> 15:35.400
say, because the features and the characteristics that we were studying and trying to understand

15:35.400 --> 15:37.360
were more around the financial fraud.

15:37.360 --> 15:42.440
So people using stolen credit card, we are looking at adding other models and capabilities

15:42.440 --> 15:47.640
to pick things like item that is described and also with better shipping information integration,

15:47.640 --> 15:50.120
we can also see if the item was delivered or not.

15:50.120 --> 15:54.600
But for the first version, it was really important for us to have a very clean cut definition.

15:54.600 --> 15:55.600
Yeah.

15:55.600 --> 15:59.520
So we went with the charges that we thought were fraudulent due to financial reasons.

15:59.520 --> 16:03.440
This also included things that the merchant had gone in the admin and canceled due to fraud

16:03.440 --> 16:07.960
either because of a call they got from the credit card company or the bank and we called

16:07.960 --> 16:09.040
those ones as fraud as well.

16:09.040 --> 16:13.880
So we were very sure and of course we, we have this internal platform built on top of

16:13.880 --> 16:19.680
Spark and Pi Spark and we, like, we made our definitions into jobs with unit tests that

16:19.680 --> 16:20.680
run on schedule.

16:20.680 --> 16:25.360
So as a byproduct of this, regardless of which part of the team you work on and what

16:25.360 --> 16:29.400
day of the week you query the database, you're always going to get the same orders as being

16:29.400 --> 16:32.080
fraud and same orders and being non-frog.

16:32.080 --> 16:38.680
And that helps a lot in being able to sort of validate results and models as we go ahead.

16:38.680 --> 16:39.680
Okay.

16:39.680 --> 16:42.880
So you have your definitions set like what's next?

16:42.880 --> 16:43.880
Okay.

16:43.880 --> 16:44.880
So we have a definition.

16:44.880 --> 16:46.800
So now we basically have labeled data.

16:46.800 --> 16:47.800
So what are the inputs?

16:47.800 --> 16:50.240
What are the features that we're going to put into models?

16:50.240 --> 16:54.520
So we started with very basic things like things around payment gateways and credit cards

16:54.520 --> 16:57.400
and those were like the easiest things to think about.

16:57.400 --> 17:00.360
But we had to also for that do some checks like make sure.

17:00.360 --> 17:04.040
So we had also another feature where we started looking at it and looked predictive.

17:04.040 --> 17:07.600
But then we realized, yeah, but this feature has not been fired for the last six months.

17:07.600 --> 17:11.440
So actually I can't use it because for whatever reason we're not producing it anymore.

17:11.440 --> 17:14.720
Meaning it just got pulled out of the platform somewhere and it needs you.

17:14.720 --> 17:15.720
Okay.

17:15.720 --> 17:16.720
So that's the thing.

17:16.720 --> 17:17.720
Like there's lots of data.

17:17.720 --> 17:18.720
Like there's massive amounts of data.

17:18.720 --> 17:20.640
But you also have to understand like who's producing it?

17:20.640 --> 17:23.680
What's the expectation level unavailability on?

17:23.680 --> 17:24.680
Right.

17:24.680 --> 17:28.200
And sometimes like for example, orders that come from like a point of sale are not going

17:28.200 --> 17:30.160
to have all the features of the orders on the web.

17:30.160 --> 17:34.480
So the features you pick if they're going to run across different gateways and across

17:34.480 --> 17:39.320
different channels have to have a representative value for all of these scenarios.

17:39.320 --> 17:43.000
Otherwise the model is going to be biased towards one versus the other.

17:43.000 --> 17:46.120
So we realized that feature recently mattered a lot.

17:46.120 --> 17:47.520
Feature frequency mattered a lot.

17:47.520 --> 17:49.880
How often are we going to see this feature?

17:49.880 --> 17:54.640
And then the distribution of values and how often it's going to be null or not present.

17:54.640 --> 17:59.920
So what ones we figured out this list, one of the things we always focused on on Shopify

17:59.920 --> 18:02.200
is like, how can we scale this?

18:02.200 --> 18:06.800
So I don't want every single data scientist in the team to every day have to like code

18:06.800 --> 18:10.320
how to figure out these things out and have these checklists in their mind.

18:10.320 --> 18:14.240
So because I think there's like way more interesting things that they can do.

18:14.240 --> 18:18.800
So we made templates like we made Python template codes that they can run a new feature

18:18.800 --> 18:21.880
through and it would produce this sort of descriptive statistics.

18:21.880 --> 18:25.000
So none of it is super complicated, but it's just the fact that we have thought through

18:25.000 --> 18:28.960
this step gives us a boost in the speed for our delivery.

18:28.960 --> 18:33.040
So now if you're a part of the team and you want to add a new feature before you even

18:33.040 --> 18:37.480
put it in a model, you can run it through this sort of scripts or I put on notebook or

18:37.480 --> 18:38.480
a job.

18:38.480 --> 18:41.840
And you would get a report that says, okay, you know, over the last 12 months, this is when

18:41.840 --> 18:44.920
this feature has been null, this is when it has been missing.

18:44.920 --> 18:49.440
These are the distribution of values you see for it is how it looks across different segmentations.

18:49.440 --> 18:56.160
So that's something that's very simple, but in action, it helps a lot.

18:56.160 --> 19:02.960
And is that those tools are they primarily used for kind of exploratory analysis?

19:02.960 --> 19:08.320
Or are they do you have like a whole framework for back testing and things like that?

19:08.320 --> 19:12.800
So this specific alone one we use for exploration because we're trying to add features into new

19:12.800 --> 19:14.720
models and see how they work.

19:14.720 --> 19:18.520
We do exploration by pulling these features that have passed the checks and the target

19:18.520 --> 19:21.160
and then trying a different learning models.

19:21.160 --> 19:25.480
And we started with the simplest ones that are really easy to explain and easy to sort

19:25.480 --> 19:29.760
of debug as well because we knew we had to scale it for 500,000 versions.

19:29.760 --> 19:33.400
So for the first one, it's okay, we're going to go with a random forest.

19:33.400 --> 19:37.880
So then we have a pipeline where you define your features, you define the transforms you

19:37.880 --> 19:40.600
want to apply on those features and the models.

19:40.600 --> 19:46.000
So for example, I want to say, so as an input data, I get where the order is placed.

19:46.000 --> 19:51.880
But I transform it to a feature that says, is the order placed on a tablet or not?

19:51.880 --> 19:55.400
So it's like a level of change you do on top of a feature, but it's really important

19:55.400 --> 20:00.560
that that's transformation is well defined and also well understood by the downstream

20:00.560 --> 20:02.080
parts of the pipeline.

20:02.080 --> 20:08.360
So after the transformation, we also have the model training and then we do our testing.

20:08.360 --> 20:12.560
And then we, so then we optimize for different metrics and we really tie those metrics based

20:12.560 --> 20:17.560
on what's accepted in the fraud detection industry rather than like optimizing for, you know,

20:17.560 --> 20:19.320
for true positives or true negatives.

20:19.320 --> 20:23.680
So you really want to balance, you want them in merchants to be able to take as much

20:23.680 --> 20:25.560
sale as they can with peace of mind.

20:25.560 --> 20:27.720
So that's the optimization.

20:27.720 --> 20:35.400
So elaborate a little bit on the, you know, this difference between the kind of these generally

20:35.400 --> 20:42.800
accepted metrics versus the ones you might otherwise track is the idea that, you know,

20:42.800 --> 20:47.720
you can't report AUC to a merchant because that doesn't mean anything to them or is it

20:47.720 --> 20:53.240
or are there, you know, industry specific terminologies or metrics that you need to kind

20:53.240 --> 20:54.640
of map things to?

20:54.640 --> 20:55.640
Sure.

20:55.640 --> 20:57.480
So there are industry metrics within frauds.

20:57.480 --> 21:02.280
For example, like, it varies a little bit, but above 95% of your orders should be accepted.

21:02.280 --> 21:04.320
Like there's 95% of your transactions.

21:04.320 --> 21:07.320
We actually don't have to worry about, we're not going to be fraud.

21:07.320 --> 21:11.680
It's within the next 5%, how much of it do you ask a person to investigate, call the

21:11.680 --> 21:15.480
customer, try to verify it a little bit of extra steps or it's the ones that you say,

21:15.480 --> 21:17.760
you know, this is fraud and you have to go and cancel it.

21:17.760 --> 21:22.200
So we have built in our pipeline, we can actually pass it the product metrics and see, okay,

21:22.200 --> 21:26.680
this is the bucket size of each recommendation that I want you to have and optimize for metrics

21:26.680 --> 21:28.480
within this bucket sizes.

21:28.480 --> 21:34.880
So that's one way to go because then we actually know what the practical impact of this metric

21:34.880 --> 21:36.200
is going to be on the merchant.

21:36.200 --> 21:38.920
We also look at the losses that they would have.

21:38.920 --> 21:43.720
So the value of accepting this order and receiving a charge rack versus like a loss of

21:43.720 --> 21:47.760
not having like not actually letting the order go through and we try to optimize for

21:47.760 --> 21:48.760
that.

21:48.760 --> 21:53.040
But yeah, I would say like if you want to remember one thing is like the metric that you

21:53.040 --> 21:58.240
use for tuning your model has to be something with very understandable user impact because

21:58.240 --> 22:03.000
this model, this product, going to wild and power real like people.

22:03.000 --> 22:06.680
So we have to have an understanding of what happens if I actually pushed just too much

22:06.680 --> 22:11.000
one way or the other and what the impact is going to be on the user.

22:11.000 --> 22:14.200
So you've got your model now trained up.

22:14.200 --> 22:15.200
What's next?

22:15.200 --> 22:17.360
What's next is what we call production back test.

22:17.360 --> 22:22.680
So in that, what we do is that we say, okay, for the most recent six months of the data,

22:22.680 --> 22:27.960
which as I said, we might not always have all of the prediction results ready.

22:27.960 --> 22:32.720
We are going to still run the model and do the prediction and look at the distribution

22:32.720 --> 22:36.560
of the predictions across the six months and across different merchants.

22:36.560 --> 22:40.720
So we're going to see, okay, we train this model to say put X percent of the orders in

22:40.720 --> 22:41.720
cancel bucket.

22:41.720 --> 22:44.200
Is it doing the same in the most recent orders?

22:44.200 --> 22:46.880
Has something changed in the patterns that's not allowing for that?

22:46.880 --> 22:51.520
And then we even go find our grain and we say, let's look at the model predictions within

22:51.520 --> 22:53.520
the segments of our user rate.

22:53.520 --> 22:57.800
So let's look at people in a specific geography or in a specific channel to see if there are

22:57.800 --> 22:58.800
any problems there.

22:58.800 --> 23:04.640
And then we go once that deeper and that if we have merchants with individual commendations

23:04.640 --> 23:05.880
have changed significantly.

23:05.880 --> 23:09.400
That means that person is going to have a very different experience when they log in

23:09.400 --> 23:10.400
that day.

23:10.400 --> 23:13.560
So if that's the case, we reach out to them ahead of time and we let them know that this

23:13.560 --> 23:14.560
change is happening.

23:14.560 --> 23:18.640
And this is why this model is better and this is why your experience is likely to change.

23:18.640 --> 23:22.400
And they're actually really positive because we have to focus on making their business

23:22.400 --> 23:23.400
better.

23:23.400 --> 23:24.400
Okay.

23:24.400 --> 23:26.920
And what are the things that trigger that last scenario?

23:26.920 --> 23:33.680
Is that when for whatever reason, that merchant or that category becomes a target of fraud

23:33.680 --> 23:41.800
or is it something more statistical, drift in a distribution or is it stuff that you've

23:41.800 --> 23:45.240
done in terms of just tweaking models?

23:45.240 --> 23:50.000
Most of the ones we've seen so far is by addition of features that are totally different.

23:50.000 --> 23:54.800
So for example, if I start looking at historical data on the merchants and I see a cookie this

23:54.800 --> 23:58.800
merchant is more likely to have fraud or less likely to have fraud, that is what's changing.

23:58.800 --> 24:02.680
So most of the time it's introducing not a single new feature, but a class of new features.

24:02.680 --> 24:08.120
Like historical values or when we start looking at the features about the browsing behavior,

24:08.120 --> 24:10.760
then that brings an extra level of detail.

24:10.760 --> 24:16.640
That being said, even when I say that change is drastic, it meets some checks, but it's

24:16.640 --> 24:20.680
just like for one person, maybe one day they get like 10 more orders than they are used

24:20.680 --> 24:23.760
to to investigate or 10 less orders to investigate.

24:23.760 --> 24:27.680
And you'd be surprised how much people build their own workflows and their own understanding

24:27.680 --> 24:32.320
of them, so you want them to feel safe and protected because that's the whole goal of this

24:32.320 --> 24:35.520
product is for the merchant to feel safe and protected like that.

24:35.520 --> 24:36.520
Right.

24:36.520 --> 24:37.520
Right.

24:37.520 --> 24:38.520
Yeah.

24:38.520 --> 24:39.520
Okay.

24:39.520 --> 24:41.320
So you've done your back testing.

24:41.320 --> 24:42.320
Are we done?

24:42.320 --> 24:43.320
Are we?

24:43.320 --> 24:44.320
No, no, no.

24:44.320 --> 24:48.240
So we've done all of these things, but then we've done all of it in mobile data land.

24:48.240 --> 24:52.280
It's like everything in data and everything in data in our company is done with Python,

24:52.280 --> 24:57.760
Python, but Shopify is a very big Ruby and Rails application and even the services we build

24:57.760 --> 24:59.720
inside it, they're all Ruby and Rails.

24:59.720 --> 25:04.080
So we have to find a way to transform this model that we have made in Python to run in

25:04.080 --> 25:07.760
a Ruby application, which is a risk application.

25:07.760 --> 25:11.320
So for that, there are many different ways for us because we know there are many other

25:11.320 --> 25:14.760
applications in Shopify that also use Ruby and Rails.

25:14.760 --> 25:19.560
We wanted to find a way to see if we can actually run our machine learning models in in a Ruby

25:19.560 --> 25:20.560
application.

25:20.560 --> 25:26.880
So what we did is we went with this model serialization definition, which is called PMML.

25:26.880 --> 25:29.000
So it's predicted modeling markup language.

25:29.000 --> 25:30.400
It's been around for decades.

25:30.400 --> 25:34.600
It was mostly used in academia, but now it's having a comeback and different languages

25:34.600 --> 25:37.200
and packages have PMML transformations.

25:37.200 --> 25:42.120
So in scikit-learn, you can save your model in PMML or you can save your model in PMML.

25:42.120 --> 25:43.320
And it's very similar to XML.

25:43.320 --> 25:45.400
So it's like, it has a very well defined spec.

25:45.400 --> 25:50.320
So you define your inputs, you define your transformations, the model and then decision-making

25:50.320 --> 25:51.320
at the end.

25:51.320 --> 25:53.160
And so it kind of ties into our pipeline.

25:53.160 --> 25:57.720
And we got ideas from that for the levels of abstraction that we put in our pipeline.

25:57.720 --> 26:01.560
So what we do is like, once we get the model, we serialize it to PMML.

26:01.560 --> 26:06.000
And then we have built a gem that we're planning to open source next year.

26:06.000 --> 26:08.400
Basically, it's a Ruby interpreter for PMML.

26:08.400 --> 26:14.080
So once that gem is included in the Ruby application, it's able to sort of like understand this

26:14.080 --> 26:18.280
PMML model and evaluate orders using it as they come true.

26:18.280 --> 26:19.960
So then we have the model.

26:19.960 --> 26:25.480
Before we go past that, I've been waiting for us to get to this point because I first

26:25.480 --> 26:29.680
came across PMML probably like four or five years ago or so and couldn't find anyone

26:29.680 --> 26:31.480
who is really doing anything with it.

26:31.480 --> 26:32.960
And there's another one.

26:32.960 --> 26:36.280
I forget the name of it, but it's another kind of model serialization.

26:36.280 --> 26:38.560
I'm sure there are a bunch of model serialization things.

26:38.560 --> 26:44.080
But so you serialize this model and you have your inputs and your outputs, what's the

26:44.080 --> 26:48.680
level of abstraction of the serialized model?

26:48.680 --> 26:54.080
Like are you telling it like a model type, like this is a random forest and then it's up

26:54.080 --> 27:01.440
to the implementation that is interpreting it to actually know how to implement a random

27:01.440 --> 27:06.520
forest or is there some other mechanism for actually implementing the model?

27:06.520 --> 27:10.560
So PMML spec, for example, has something that says, okay, this is a logistic regression.

27:10.560 --> 27:13.320
These are the inputs I need for a logistic regression.

27:13.320 --> 27:15.880
And this is how I'm going to give you the output.

27:15.880 --> 27:20.720
But the actual implementation of the code that does it, it can be done in any language.

27:20.720 --> 27:25.640
So what it is is literally just a transform mechanism for one language to the other one.

27:25.640 --> 27:29.760
So it's an XML document, it can open, then you can see it, then it has like the inputs

27:29.760 --> 27:35.720
and the model definition and the sort of weights and things that go into the model and how

27:35.720 --> 27:37.800
it's going to make the decision at the end.

27:37.800 --> 27:42.680
But it doesn't do anything, right, it doesn't have any execution engine type to do.

27:42.680 --> 27:43.680
Right.

27:43.680 --> 27:51.840
So my question is, do you ever run into issues where I guess if your deployment environment

27:51.840 --> 27:57.280
is all Ruby, it would have to be something like, you know, you've got some weird dependency

27:57.280 --> 28:03.040
thing where you've got one version of, you know, one Rails app that's, you know, pinned

28:03.040 --> 28:10.400
to, you know, some Ruby machine learning library and version and you have another that's

28:10.400 --> 28:15.440
pinned to another version and because you're serializing this model at like a level of

28:15.440 --> 28:19.120
abstraction higher, you get different results based on where you run it.

28:19.120 --> 28:24.680
So right now we have one like one gem that we use across Shopify and right now we are

28:24.680 --> 28:26.720
using it in a single application.

28:26.720 --> 28:30.640
But like any gem, if we don't pay attention to which version of the gem is running, we

28:30.640 --> 28:32.840
can run into problems, but we have checks for that.

28:32.840 --> 28:36.960
So what we do is that when the model is ready, but it's still not ready to meet the user.

28:36.960 --> 28:40.560
So what we do is that we do what we call live model practice.

28:40.560 --> 28:43.040
So we deploy the model in the application.

28:43.040 --> 28:47.200
So as the orders come true, it evaluates them, it makes a score and a recommendation,

28:47.200 --> 28:51.120
but instead of powering the users, but it only writes it to a Kafka topic.

28:51.120 --> 28:55.560
So what we do is that we're also observing the same data in data land, right?

28:55.560 --> 28:56.800
And we're constantly training.

28:56.800 --> 29:02.000
So what we do is we compare what did the, in data we predict using this inputs and what

29:02.000 --> 29:04.560
do we, do we predict in production using this inputs?

29:04.560 --> 29:09.400
I mean, match those and it's only when we match like 100% match between the new systems

29:09.400 --> 29:12.400
that we pick it out of the shadow mode and we power users.

29:12.400 --> 29:17.840
And is this process, is this an automated process or is this data scientist, you know,

29:17.840 --> 29:24.680
kind of manually overseeing this transition into production and making sure that monitoring

29:24.680 --> 29:29.800
the performance over time and then hitting a button to deploy it.

29:29.800 --> 29:31.120
So parts of it are automated.

29:31.120 --> 29:34.920
So like the, the jobs that reconcile the two are automated, the reports that show you

29:34.920 --> 29:37.080
what the reconciliation looks like is automated.

29:37.080 --> 29:41.080
The last step of just like making sure everything reconciled and then pushing the deploy,

29:41.080 --> 29:42.360
no, that's not automated right now.

29:42.360 --> 29:47.280
And I do want us to get a bit more experience before we like fully automate that or at

29:47.280 --> 29:50.160
least learn how to catch things if something goes wrong.

29:50.160 --> 29:54.920
The other benefit of live model practice is that also it gives us some metrics and like

29:54.920 --> 29:57.800
real understanding around performance of this new model.

29:57.800 --> 30:01.040
So as we make more sophisticated models, we are reaching out to more internal services

30:01.040 --> 30:05.400
to get data with different SLAs and models, they're getting more complicated.

30:05.400 --> 30:07.120
So what's the runtime of it going to be?

30:07.120 --> 30:12.000
And I remember I said like we'd really want to have the fraud detection and risk analysis

30:12.000 --> 30:13.800
ready as soon as the order is placed.

30:13.800 --> 30:17.320
So it allows us to see, okay, what's the real performance of this model?

30:17.320 --> 30:21.800
And once all of that are good, then the model is ready to meet the user and hopefully

30:21.800 --> 30:24.880
the user will be delighted by the results.

30:24.880 --> 30:32.440
And do you currently deploy new models out to all users or do you do like A-B testing

30:32.440 --> 30:33.440
or something like that?

30:33.440 --> 30:37.840
Do you feel like all the steps you've taken up to now mean that you don't have to do

30:37.840 --> 30:40.840
like partial deployments and A-B testing?

30:40.840 --> 30:45.520
Right now we deployed to all users, but I foresee there are other things that we want

30:45.520 --> 30:46.520
to do.

30:46.520 --> 30:48.840
We want to be able to look at a voting scheme between different models.

30:48.840 --> 30:54.840
We want to be able to have more specialized models for specific segments of the users.

30:54.840 --> 30:58.080
So when we get to that, we're going to do more A-B testing.

30:58.080 --> 31:00.680
So it's something that I foresee in our future for sure.

31:00.680 --> 31:04.600
And then we have to do holdout sets so that we're not impacting the actual real prediction

31:04.600 --> 31:06.440
and yeah, the fun with that.

31:06.440 --> 31:07.760
How far do you see that going?

31:07.760 --> 31:12.720
I mean, you can, the way you describe that you can or I can envision like, you know, different

31:12.720 --> 31:16.520
models for kind of arbitrarily small customer segments.

31:16.520 --> 31:17.840
Yeah, you can do that.

31:17.840 --> 31:18.840
It's actually with the pipeline.

31:18.840 --> 31:19.840
It's very easy.

31:19.840 --> 31:20.840
Okay.

31:20.840 --> 31:23.800
But then it becomes like just because we can do it, should we do it or not?

31:23.800 --> 31:26.040
Because then it brings also a maintenance.

31:26.040 --> 31:27.040
There's an overhead.

31:27.040 --> 31:28.640
Yeah, overhead associated with it.

31:28.640 --> 31:32.040
But yeah, in terms of like platform and scale, yes, we can do it.

31:32.040 --> 31:35.160
But one of the things that's interesting is like by us going through this problem, specifically

31:35.160 --> 31:39.360
focused on fraud, we actually have built this pipeline that not can be used in other parts

31:39.360 --> 31:40.520
of the platform as well.

31:40.520 --> 31:45.760
So if I want to deploy an algorithm that tells our shipping service, like what are the default

31:45.760 --> 31:49.640
dimensions of something that the user has added, I can use the same pipeline, like the

31:49.640 --> 31:53.680
same encoding of model and then deploying it and running it in another Ruby app.

31:53.680 --> 31:58.240
So those are the steps that we wanted to make easy because I think companies in general,

31:58.240 --> 32:02.680
they use AI or today they encounter AI in two different ways.

32:02.680 --> 32:06.960
Sometimes the company is built as an AI company and the challenge they have is finding the

32:06.960 --> 32:07.960
data.

32:07.960 --> 32:11.160
And sometimes the company is built, the business model is working perfectly.

32:11.160 --> 32:14.960
They have ton of data and now they want to introduce machine learning to parts of existing

32:14.960 --> 32:15.960
company.

32:15.960 --> 32:18.000
And I think we are in the latter one right now.

32:18.000 --> 32:23.400
So what matters for me right now on my team is to be able to sort of unlock that capability

32:23.400 --> 32:28.760
across many services so that we can give the benefit you know at scale to different parts

32:28.760 --> 32:29.760
of the platform.

32:29.760 --> 32:30.760
Right.

32:30.760 --> 32:31.760
Right.

32:31.760 --> 32:32.760
Interesting.

32:32.760 --> 32:36.200
So you've got the, you've got this pipeline going and you said it was a number of times

32:36.200 --> 32:43.320
Spark and Python and is the data, is the data HDFS, is that why you're using Spark primarily

32:43.320 --> 32:44.320
or?

32:44.320 --> 32:45.320
Yeah.

32:45.320 --> 32:50.400
So our internal platform that was initially built as an ETL extract transform load system

32:50.400 --> 32:54.640
uses Spark, PySpark and it's really good because it, well, the volume of our data is really

32:54.640 --> 32:57.240
high and Python is really easy to adapt.

32:57.240 --> 33:01.440
So PySpark was like a sweet spot for us as a company to adapt and we built this like

33:01.440 --> 33:04.720
platform so it abstracts so many things from the user.

33:04.720 --> 33:08.760
So I'm now with the Spark DataFrames that's actually like really easy for anyone who has

33:08.760 --> 33:12.760
worked with tabular data and SQL to like write a Spark job.

33:12.760 --> 33:16.960
So we use that because we also have like an amazing team of data engineers that have built

33:16.960 --> 33:22.200
this platform and by free we get so many things, I was actually just talking to someone upstairs

33:22.200 --> 33:25.880
we get things like metadata on every input data that we have.

33:25.880 --> 33:31.920
So for every model, I have a UUID and by that I can go and see what was the Gitcha of

33:31.920 --> 33:33.920
the code that ran this model.

33:33.920 --> 33:34.920
What was the what?

33:34.920 --> 33:37.360
Gitcha like the, oh, the Gitcha, I've got it.

33:37.360 --> 33:41.880
And then the path to the, the gith commit of the model.

33:41.880 --> 33:46.200
So I can exactly go and reproduce what version of the code was run.

33:46.200 --> 33:51.480
And I also have the information on the sort of snapshot of the data that was run as input.

33:51.480 --> 33:56.120
So all of these capabilities to reproduce, to be able to audit, to be able to like go

33:56.120 --> 34:01.120
back and check or reproduce, those come for free like my team didn't have to build those

34:01.120 --> 34:02.120
are like, yeah, exactly.

34:02.120 --> 34:06.080
Those are amazing things that were built for us and we just use them.

34:06.080 --> 34:07.080
Okay.

34:07.080 --> 34:08.080
Nice.

34:08.080 --> 34:14.280
Is all that infrastructure and Spark is that, is that all deploy time or not deploy time

34:14.280 --> 34:18.360
but like inference time or is that involved in training as well?

34:18.360 --> 34:19.360
It depends.

34:19.360 --> 34:20.360
Okay.

34:20.360 --> 34:21.840
So Spark has machine learning library.

34:21.840 --> 34:25.560
So we use it for training as well, depending on the models that we are using.

34:25.560 --> 34:30.160
But it's well known that like scikit-learn has just like a broader set of machine learning

34:30.160 --> 34:31.480
capabilities implemented.

34:31.480 --> 34:36.080
So I'm hoping that the community, including our company gives back to Spark by adding

34:36.080 --> 34:37.400
this different learning algorithms.

34:37.400 --> 34:43.360
But yeah, we sort of go between like for loading of the data for doing all the transforms.

34:43.360 --> 34:48.360
Also the things are at Spark level, but then for sort of training, we use scikit-learn

34:48.360 --> 34:49.360
heavily.

34:49.360 --> 34:50.360
Okay.

34:50.360 --> 34:51.360
So it's a mix of two.

34:51.360 --> 34:53.080
It also makes onboarding people really easy.

34:53.080 --> 34:58.480
So many people in the data field are picking data or they've already worked in data on Python.

34:58.480 --> 35:03.240
So it's like a familiar interface and it just breaks that barrier very easily.

35:03.240 --> 35:05.680
And it's a lot more accessible than Spark.

35:05.680 --> 35:06.680
Exactly.

35:06.680 --> 35:07.680
Yes, it is.

35:07.680 --> 35:08.680
Okay.

35:08.680 --> 35:09.680
Awesome.

35:09.680 --> 35:16.080
So you would want to leave folks with or left folks with it in your presentation?

35:16.080 --> 35:21.600
So right now I've focused a lot on the mechanics of making this model, be a life.

35:21.600 --> 35:26.520
Part of that is because I had the luxury of being in the same company, being in the same

35:26.520 --> 35:31.240
domain for like two and a half years before we even started tackling this problem.

35:31.240 --> 35:35.680
That means like the understanding of what the features mean or where do I have to poke

35:35.680 --> 35:39.880
to find out what the future means sort of came with me and my team.

35:39.880 --> 35:44.440
So I think one thing that I want to emphasize to people is that try really hard to understand

35:44.440 --> 35:48.440
the domain and the problem you're trying to solve because at the end of the day, so

35:48.440 --> 35:52.240
many times, machine learning sort of builds up on top of heuristics that already humans

35:52.240 --> 35:53.840
that are in that field know.

35:53.840 --> 35:57.920
So having spending that time that does not feel like you're working on a fancy learning

35:57.920 --> 36:02.920
algorithm is actually one of the most important parts of having a successful data product.

36:02.920 --> 36:03.920
Yeah.

36:03.920 --> 36:04.920
Interesting.

36:04.920 --> 36:09.800
Well, thank you so much for taking the time, so much to sit down with me.

36:09.800 --> 36:15.240
I enjoyed your presentation and I enjoyed talking to you about your presentation and

36:15.240 --> 36:16.240
I appreciate it.

36:16.240 --> 36:17.240
Oh, thanks a lot.

36:17.240 --> 36:20.240
And thanks for having me and keep producing this awesome podcast.

36:20.240 --> 36:24.240
Thanks so much.

36:24.240 --> 36:27.200
All right, everyone.

36:27.200 --> 36:29.240
That's our show for today.

36:29.240 --> 36:33.640
Thanks so much for listening and for your continued feedback and support.

36:33.640 --> 36:38.560
For more information on solmars or any of the topics we covered in this episode, head

36:38.560 --> 36:42.640
on over to twimlai.com slash talk slash 60.

36:42.640 --> 36:50.520
To follow along with the Georgian partner series, visit twimlai.com slash GPC 2017.

36:50.520 --> 36:56.360
Of course, you can send along feedback or questions via Twitter at twimlai or at Sam

36:56.360 --> 37:00.120
Charrington or leave a comment on the show notes page.

37:00.120 --> 37:03.800
Thanks once again to Georgian partners for their sponsorship of the show.

37:03.800 --> 37:08.880
Be sure to check out their white papers, which you can find by visiting twimlai.com slash

37:08.880 --> 37:09.880
Georgian.

37:09.880 --> 37:38.400
Thanks again for listening and catch you next time.

