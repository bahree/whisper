WEBVTT

00:00.000 --> 00:15.920
Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting

00:15.920 --> 00:20.880
people doing interesting things in machine learning and artificial intelligence.

00:20.880 --> 00:23.560
I'm your host Sam Charrington.

00:23.560 --> 00:27.760
This show you are about to hear as part of a series of shows recorded in San Francisco

00:27.760 --> 00:32.280
at the Artificial Intelligence Conference, which was hosted by our friends at O'Reilly

00:32.280 --> 00:34.480
in Intel Nirvana.

00:34.480 --> 00:39.680
In addition to their support for the event itself, Intel Nirvana is also our sponsor for this

00:39.680 --> 00:42.040
series of podcasts from the event.

00:42.040 --> 00:46.280
A huge thanks to them for their continued support of this show.

00:46.280 --> 00:51.280
Make sure you check out my interview with Navine Rao, VP and GM of Intel's AI products

00:51.280 --> 00:56.600
group, and Scott Appland, director of Intel's developer network, which you can find at

00:56.600 --> 01:00.800
twimbleai.com slash talk slash 51.

01:00.800 --> 01:06.520
At the AI conference, Intel Nirvana announced DevCloud, a cloud hosted hardware and software

01:06.520 --> 01:12.920
platform for learning, sandboxing and accelerating the development of AI solutions.

01:12.920 --> 01:19.160
The DevCloud will be available to 200,000 developers, researchers, academics and startups

01:19.160 --> 01:23.040
via the Intel Nirvana AI Academy this month.

01:23.040 --> 01:29.640
For more information on the DevCloud or the AI Academy, visit intelnervana.com slash

01:29.640 --> 01:31.720
DevCloud.

01:31.720 --> 01:36.740
In this episode, I talk with Ian Stoika, professor of computer science and director of the

01:36.740 --> 01:39.040
RISE lab at UC Berkeley.

01:39.040 --> 01:45.480
Ian joined us after he gave his talk, building reinforcement learning applications with Ray.

01:45.480 --> 01:52.080
We dive into Ray, a new distributed computing platform for RL, as well as RL generally,

01:52.080 --> 01:57.400
along with some of the other projects the RISE lab is working on, like Clipper and Tigra.

01:57.400 --> 01:59.640
This was a pretty interesting talk.

01:59.640 --> 02:00.640
Enjoy.

02:00.640 --> 02:13.800
Alright everyone, I'm here at the AI conference, presented by O'Reilly and Intel Nirvana,

02:13.800 --> 02:17.120
and I've got the pleasure of being seated with Ian Stoika.

02:17.120 --> 02:22.000
Ian is the executive chairman and co-founder of Databricks, and also a professor

02:22.000 --> 02:25.600
at Berkeley and director of the RISE lab.

02:25.600 --> 02:28.560
Ian, welcome to this weekend machine learning in AI.

02:28.560 --> 02:29.560
Thank you.

02:29.560 --> 02:30.560
Thanks for having me.

02:30.560 --> 02:31.560
Absolutely.

02:31.560 --> 02:35.160
So why don't we get started by having you tell us a little bit about your background,

02:35.160 --> 02:42.120
how you got interested in machine learning and what your area of research interest is?

02:42.120 --> 02:48.440
I joined Berkeley in 2001, and at that time I was actually doing networking, and then

02:48.440 --> 02:55.080
I moved around and did a little bit of peer-to-peer systems, and then started working on big

02:55.080 --> 02:56.080
data.

02:56.080 --> 03:02.080
On big data, in the context of another lab at Berkeley called Amplab, we developed a few

03:02.080 --> 03:09.760
systems, including Apache Spark, Apache Mesos, and Alusio, formerly known as Stachion.

03:09.760 --> 03:16.400
Basically, what happens is that we develop these systems because people get more and

03:16.400 --> 03:23.520
more data and organizations ingest more and more data, and then once they have the data

03:23.520 --> 03:29.640
whether it's user logs, machine logs, or any other data, they try to make sense of this

03:29.640 --> 03:30.640
data.

03:30.640 --> 03:33.840
They get to try to get some insights.

03:33.840 --> 03:40.640
So Apache Spark was an effort to process large scales this kind of data.

03:40.640 --> 03:46.840
Now also in the context of the Apache Spark, you look at what people do after they get

03:46.840 --> 03:51.920
some insights of the data, they want to make some decisions to take some actions based

03:51.920 --> 03:58.280
on the data, to improve our targeting, to do medical diagnosis, and things like that.

03:58.280 --> 04:04.880
So now the next step again in getting value out of the data is basically applying AI, machine

04:04.880 --> 04:11.640
learning to get intelligent, actionable decisions.

04:11.640 --> 04:18.280
That's why I am now doing that research in the context of systems and machine learning

04:18.280 --> 04:19.960
and AI.

04:19.960 --> 04:26.920
And as part of that, we just started this new lab at Berkeley called Rise, where Rise stands

04:26.920 --> 04:34.440
for real-time intelligence and secure execution, and this lab is about building systems

04:34.440 --> 04:44.320
and tools and algorithms to support applications which make real-time decisions on live data with

04:44.320 --> 04:45.320
strong security.

04:45.320 --> 04:46.320
Okay.

04:46.320 --> 04:52.200
And in fact, that's one of the areas that Spark really distinguished itself in that real-time,

04:52.200 --> 04:57.480
streaming, interactive decision-making, at least relative to Hadoop, which was kind

04:57.480 --> 04:59.920
of the incumbent system at the time.

04:59.920 --> 05:00.920
Yes.

05:00.920 --> 05:01.920
Yes.

05:01.920 --> 05:02.920
Definitely.

05:02.920 --> 05:09.480
So the truth, Spark was compared with, at that time, when we created it, compared with

05:09.480 --> 05:17.160
MapReduce, provide great support for interactive queries, iterative computation like machine learning

05:17.160 --> 05:21.960
algorithms, and later for streaming.

05:21.960 --> 05:30.240
So right now, going forward, one big angle we are focusing on is security.

05:30.240 --> 05:36.240
So everyone wants to take and to make personalized decisions.

05:36.240 --> 05:37.240
Right.

05:37.240 --> 05:44.000
So the question here, how can you make personalized decisions without violating the confidentiality

05:44.000 --> 05:47.440
of the users, while preserving the confidentiality of the users?

05:47.440 --> 05:48.440
Okay.

05:48.440 --> 05:50.200
So this is one big challenge.

05:50.200 --> 05:58.800
The other aspect is about, for instance, when you create this, with Spark, you create

05:58.800 --> 06:06.360
these models using deep learning or many other approaches, machine learning approaches,

06:06.360 --> 06:10.600
but this model eventually, you are going to serve that model.

06:10.600 --> 06:11.600
Right?

06:11.600 --> 06:18.400
So the users, you know, when the users contact you, the user make a query, look at the recommendation,

06:18.400 --> 06:24.600
you get, you give back the inference part, it's a inference part, it's a give back right

06:24.600 --> 06:25.600
answer.

06:25.600 --> 06:32.920
So another direction is focusing on building this kind of prediction serving or model serving

06:32.920 --> 06:35.160
systems.

06:35.160 --> 06:41.400
And another direction is that, you know, when you look at the new generation of this machine

06:41.400 --> 06:47.960
learning applications, like for round-dream enforcement learning, they have different patterns.

06:47.960 --> 06:53.400
They do a lot of very small simulations, and you know, we build also systems for that,

06:53.400 --> 06:58.600
which are a better fit for this kind of very fine-grained computations.

06:58.600 --> 06:59.600
Okay.

06:59.600 --> 07:03.320
And here at the conference, you were talking about Ray, which is one of the systems that

07:03.320 --> 07:04.320
you built.

07:04.320 --> 07:08.520
I was talking about Ray, which is in this category.

07:08.520 --> 07:18.640
So think about Ray, is the ability to execute a lot of fine-grained tasks, which are dynamically

07:18.640 --> 07:25.960
connected, dynamically depend from each other, like tasks of milliseconds, takes only milliseconds.

07:25.960 --> 07:31.160
So you can be, you can run millions of this task per second, so it's very fine-grained.

07:31.160 --> 07:36.040
And each of these tasks can be simulation, like simulations you play a game, right?

07:36.040 --> 07:41.120
You move, you know, simulate the moves of the pieces, and then you see whether you own

07:41.120 --> 07:43.160
or you lost, right?

07:43.160 --> 07:48.320
And one of the workloads is reinforcement learning, which reinforcement learning, you can see

07:48.320 --> 07:54.880
something like a generalization of supervised learning in which the labels of the action

07:54.880 --> 08:00.880
you make is sparse, as sparse, not every action has a label, it's good or bad.

08:00.880 --> 08:04.960
Like for instance, when you play a game, when you make a move, you don't necessarily know

08:04.960 --> 08:06.680
whether this move is great.

08:06.680 --> 08:10.880
You need to wait until the end of the game, maybe to see whether it's good or bad.

08:10.880 --> 08:13.240
Also the feedback is delayed, right?

08:13.240 --> 08:19.640
In supervised learning, in many times you know whether I show you some picture and you

08:19.640 --> 08:25.640
give me, say, this is a cat or a dog, I know instantly whether it's good or bad.

08:25.640 --> 08:30.400
And also it's about reinforcement learning, it's also assumed that you interact with

08:30.400 --> 08:34.240
the environment continuously and you change the environment, right?

08:34.240 --> 08:40.840
Like with, again, with playing games, self-driving cars or dialogue systems, right?

08:40.840 --> 08:46.360
This is a dialogue, right, and you need to, it's continuously interacting and building

08:46.360 --> 08:52.800
the context about our conversations and based your next actions, you know, next answers

08:52.800 --> 08:56.360
and questions based on this context, right?

08:56.360 --> 09:01.560
So that's basically, it is, and this reinforcement learning in many, one of the workload characteristics

09:01.560 --> 09:06.200
all requires to handle this very small fine grain simulation.

09:06.200 --> 09:11.240
Okay, so, yeah, we've actually talked about reinforcement learning on the podcast quite

09:11.240 --> 09:12.240
a bit.

09:12.240 --> 09:14.240
We've had Peter Avil and Sergei B.

09:14.240 --> 09:15.240
Yeah, yeah.

09:15.240 --> 09:18.640
That is an expert, I'm not sure how they cannot do that.

09:18.640 --> 09:24.720
Well, you know, so what the question that comes up for me is, how does Ray compare to an

09:24.720 --> 09:31.200
environment like OpenAI Gym or OpenAI Universe that are used also in context of reinforcement

09:31.200 --> 09:32.200
learning?

09:32.200 --> 09:38.000
It's very complementar, the OpenAI Gym and they provide you with virtual worlds, right,

09:38.000 --> 09:45.440
and so forth, the simulators, the Ray which provides you with a plumbing to execute those

09:45.440 --> 09:52.760
simulations and to get the results and to update the policies that is mapping between

09:52.760 --> 09:57.320
the state of the environment or the observation of the environment and the action you are

09:57.320 --> 09:59.960
going to take at large scale.

09:59.960 --> 10:04.880
So, is it too simple to say that Ray is like a dupe for reinforcement learning, it sounds

10:04.880 --> 10:10.200
like you've got a bunch of tasks or you're less than about a cluster and then mapping

10:10.200 --> 10:11.200
the results back?

10:11.200 --> 10:15.280
Yeah, you can say that in the first approximation and then you take the results and you update

10:15.280 --> 10:16.280
the policy.

10:16.280 --> 10:17.280
Right, right.

10:17.280 --> 10:21.520
And then you repeat until the policy, you're happy with the policy, the policy converges.

10:21.520 --> 10:22.520
Right, right.

10:22.520 --> 10:26.520
So, you mentioned this attribution problem, right, you're playing this game, you've got

10:26.520 --> 10:33.560
this reinforcement, this learner that's involved in this environment, a game, for example.

10:33.560 --> 10:40.160
And you can't really know whether it's action, you know, is positive or negative until

10:40.160 --> 10:42.120
much later on.

10:42.120 --> 10:47.000
If you don't know the ultimate benefit, how do then do you gain any knowledge from these

10:47.000 --> 10:49.080
micro simulations that you've been describing?

10:49.080 --> 10:54.680
Yeah, because a simulation each game, it's, you take some actions after each action, you

10:54.680 --> 11:00.120
have a state which is a modified state of the environment and eventually you have a reward.

11:00.120 --> 11:03.920
With sometimes you do not know whether the reward can be zero, you don't know it's good

11:03.920 --> 11:05.760
or bad, right?

11:05.760 --> 11:06.760
Right.

11:06.760 --> 11:11.080
And after you take, according to the current policy, you are going, which again, you fill

11:11.080 --> 11:16.360
the state, takes the action, you get the next state and the reward, you feed again the

11:16.360 --> 11:18.680
policy, you get the next action and so forth.

11:18.680 --> 11:24.080
So, you get these trajectories, which is a succession of states and rewards.

11:24.080 --> 11:29.560
So, now, eventually at the end, you are going to see whether you achieve your task or

11:29.560 --> 11:33.320
not, whether you want the game or you lost the game.

11:33.320 --> 11:38.160
If it's a robot, whether the task say you're moving on objects or a successful or not,

11:38.160 --> 11:41.360
right, or solving a puzzle, right?

11:41.360 --> 11:48.040
So then, once at the end, you, for instance, see that eventually see whether you're successful

11:48.040 --> 11:52.880
or not, then if you are successful at a very high level, you are going to go back and

11:52.880 --> 12:00.320
reinforce all these actions you've taken, meaning, reinforcing, meaning that next time

12:00.320 --> 12:06.600
when you try, after you update the policy, it's a higher chance to take these actions.

12:06.600 --> 12:12.520
If the actions led you to say, losing a game, then you are going to weaken these actions.

12:12.520 --> 12:16.560
You make them less likely to be taken when you are in a similar situation.

12:16.560 --> 12:18.600
That's basically what it is.

12:18.600 --> 12:19.600
It's like humans, right?

12:19.600 --> 12:25.080
It's like you play a game and so forth, and if you own, it's likely that you are going

12:25.080 --> 12:28.440
to repeat some of the kind of moves you might.

12:28.440 --> 12:35.080
If I'm playing Tik Tok To, the opening move is always in the middle.

12:35.080 --> 12:39.440
That's very true.

12:39.440 --> 12:46.800
I imagine a big part of what it's, or at least one part of what it's trying to do is,

12:46.800 --> 12:52.440
and this is also, in a lot of ways, a dupe-like, is managing the state because you're shuffling

12:52.440 --> 12:57.440
state for all of these little simulations that you're running all the time.

12:57.440 --> 13:00.120
Human is a state as well.

13:00.120 --> 13:08.400
There is an object store, so you keep that state in the object store, so tasks which run

13:08.400 --> 13:11.280
on different nodes can have access to the state.

13:11.280 --> 13:18.320
You also try to provide full tolerance if the node fails, so you make sure that the

13:18.320 --> 13:25.080
computation still is going to make progress and doing that correctly.

13:25.080 --> 13:31.960
These are some of the issues you need to deal with.

13:31.960 --> 13:36.960
You were presenting on RAY specifically, so maybe what were some of the main points that

13:36.960 --> 13:40.840
you wanted to lead with the attendees that you're talking?

13:40.840 --> 13:41.840
Yes.

13:41.840 --> 13:47.480
I think the high level point is that right now, over the past 10 years, there has been

13:47.480 --> 13:50.880
a tremendous progress in AI, right?

13:50.880 --> 13:52.400
Many great applications.

13:52.400 --> 13:59.320
Right now, as we look forward, the application becomes more and more sophisticated.

13:59.320 --> 14:05.760
While most of the applications we develop so far were based on supervised or unsupervised

14:05.760 --> 14:10.760
learning, the new applications, we believe, they are going to be more and more

14:10.760 --> 14:15.320
based on reinforcement learning, and again, the reason is that the new applications are

14:15.320 --> 14:21.480
more about interacting with the environment, around them, learning from this interaction

14:21.480 --> 14:23.960
and affecting the environment.

14:23.960 --> 14:28.720
We talk about games, we talk about dialogue systems, but you can look, think about the

14:28.720 --> 14:29.720
health applications.

14:29.720 --> 14:35.160
You know, they're constantly monitoring you and then give you health services.

14:35.160 --> 14:40.560
How about the next level of detail, so you talk about the experience of using RAY or

14:40.560 --> 14:41.560
architecture?

14:41.560 --> 14:42.560
Yes.

14:42.560 --> 14:46.840
So basically, that's what I've started, but it's again, like you said, you know, for instance,

14:46.840 --> 14:54.560
if you do not have a feedback after each action, after each move, that means that the space

14:54.560 --> 15:00.240
you need to explore in terms of solution space is larger because I need to make a series

15:00.240 --> 15:03.720
of moves because I know it's good or bad, right?

15:03.720 --> 15:07.120
So more possibilities to go wrong, right?

15:07.120 --> 15:09.320
So that's one of the characteristics.

15:09.320 --> 15:16.000
The computation requirements are increasing, and that's why one way, if you cannot for simulations,

15:16.000 --> 15:20.600
that's a very good way to do because you can do the simulation fast.

15:20.600 --> 15:25.320
So now, like for instance, playing games or things like that, you do a lot of simulations,

15:25.320 --> 15:26.320
right?

15:26.320 --> 15:31.560
Even with robots, you try to simulate the physical world, right?

15:31.560 --> 15:34.240
Because you can learn much faster, right?

15:34.240 --> 15:35.240
So that's the key.

15:35.240 --> 15:40.520
So the key is basically, then, the computation pattern is basically saying, I have a policy,

15:40.520 --> 15:44.640
I'm going to evaluate the policy by running many simulations, right?

15:44.640 --> 15:51.560
And I'm going to feed in their outputs, these trajectories, right, what happened, and update

15:51.560 --> 15:54.920
the policy, and run again, right?

15:54.920 --> 16:00.920
So that's kind of, it's a pretty regular computation because different simulations they can take

16:00.920 --> 16:05.880
different amounts of time, like for instance, if you play a game of chess, I can lose in

16:05.880 --> 16:10.560
three moves, or it may take 60 moves to win, right?

16:10.560 --> 16:17.040
And you want to learn as soon as some of this simulation happen, you want to learn from

16:17.040 --> 16:18.440
them and update the policies.

16:18.440 --> 16:23.200
You don't want to, you know, if I run 100,000 simulations, evaluate the policy, I don't

16:23.200 --> 16:24.960
want to wait for all of them, right?

16:24.960 --> 16:28.240
So this is kind of a computation is more irregular.

16:28.240 --> 16:35.200
Also, the other thing is that, you know, many of these policies are implemented by neural

16:35.200 --> 16:38.120
networks, neural networks, which run on GPUs.

16:38.120 --> 16:45.320
So you want also to support more heterogeneous hardware, and also the kind of patterns, you

16:45.320 --> 16:50.680
are talking only about simulations, but then you want to search to be able to search the

16:50.680 --> 16:53.720
space, the solution space, and things like that.

16:53.720 --> 17:02.840
So you end up basically with a very fine grain computation graph, you want to execute on heterogeneous

17:02.840 --> 17:03.840
hardware.

17:03.840 --> 17:04.840
So that's what raised.

17:04.840 --> 17:09.880
So how it's really achieving that, it's using this in memory object store to share the

17:09.880 --> 17:10.880
data.

17:10.880 --> 17:14.400
So it's very fast on a single machine, you share memory.

17:14.400 --> 17:18.800
On the back end is a scheduling, where it totally distributes a scheduler.

17:18.800 --> 17:25.160
Each note can schedule locally, and then when it's overloaded or the inputs of the task

17:25.160 --> 17:29.680
are not available locally, it's only then it's going to contact a global scheduler.

17:29.680 --> 17:34.400
We also replicate the global scheduler, so we do a lot of these kind of things to try

17:34.400 --> 17:37.480
to scale and to have very high throughput.

17:37.480 --> 17:41.680
And finally, we also provide the bindings in Python.

17:41.680 --> 17:42.680
Why Python?

17:42.680 --> 17:50.840
Because most of the AI community develops in Python, and now it seems to be the most popular

17:50.840 --> 17:51.840
language.

17:51.840 --> 17:52.840
Right.

17:52.840 --> 18:01.360
Where Spark is Java and Scala, and also has binding, it was developed in Scala, so obviously

18:01.360 --> 18:05.600
has API in Scala, but also has in Java and Python.

18:05.600 --> 18:09.720
And Python actually is very popular API and of course SQL.

18:09.720 --> 18:14.240
I guess early on I had the impression that some of those early efforts with Python were

18:14.240 --> 18:17.560
kind of like second-class citizens, has it evolved to be more?

18:17.560 --> 18:18.560
We're not evolving.

18:18.560 --> 18:25.920
I think that a very large percentage of users use Python, so they're improving fast.

18:25.920 --> 18:30.400
So where is Ray on the maturity cycle?

18:30.400 --> 18:32.640
You know, it's still an early project.

18:32.640 --> 18:38.480
We had a first release a few months ago, we are going to have soon the second release,

18:38.480 --> 18:44.000
but it's still early, so we have just a few users.

18:44.000 --> 18:50.760
But we believe that going forward, this reinforcement learning will play a major role in the future

18:50.760 --> 18:56.520
of AI, and I think that the usage adoption of Ray will grow as well.

18:56.520 --> 18:59.720
What size environments have you tested?

18:59.720 --> 19:05.400
Oh, we tested these over hundreds of nodes, so we are at that level now.

19:05.400 --> 19:10.840
But it's again, maturity is not only about how scalable it is, there are vastness, and

19:10.840 --> 19:18.040
there's a kind of features you need, monitoring, and things like that, you need really to have

19:18.040 --> 19:20.800
to deploy something like Ray in production.

19:20.800 --> 19:25.840
Does Ray lend itself to being deployed in cloud environments?

19:25.840 --> 19:26.840
No.

19:26.840 --> 19:31.160
You can deploy equally like Spark and many others, you can deploy in the cloud or you can

19:31.160 --> 19:32.400
deploy on-prem.

19:32.400 --> 19:33.400
Okay.

19:33.400 --> 19:36.000
Does it work with Spark in any particular way?

19:36.000 --> 19:37.000
Of course, of course.

19:37.000 --> 19:38.520
What are the uses there?

19:38.520 --> 19:43.640
I mentioned about this object store, so actually for the object store, you use Apache

19:43.640 --> 19:44.800
Arrow.

19:44.800 --> 19:52.560
So it's easy to Apache Arrow to exchange the data between Spark and Ray.

19:52.560 --> 19:59.200
So Ray has a much lower level API, it's again, it's targeted for this particular reinforcement

19:59.200 --> 20:02.080
learning and distribute AI applications.

20:02.080 --> 20:09.760
Well Spark has a higher level and it's in general, it's very, very mature API, very easy

20:09.760 --> 20:14.240
to use when it comes to processing big amounts of data.

20:14.240 --> 20:20.120
And thinking about that API, what's the kind of fundamental unit of work, or the unit

20:20.120 --> 20:21.120
of work is a task.

20:21.120 --> 20:25.880
A task, so you can add a decorator on a task and basically that task will be your Android

20:25.880 --> 20:26.880
module.

20:26.880 --> 20:27.880
Okay.

20:27.880 --> 20:35.080
Moving from a task to reinforcement learner is all of that in user code or it is Ray provide

20:35.080 --> 20:39.480
any abstractions for reinforcement learning in particular.

20:39.480 --> 20:44.200
For reinforcement learning, right now, we are in the next release, we are going to add

20:44.200 --> 20:48.360
a new library of reinforcement learning algorithms.

20:48.360 --> 20:49.360
Okay.

20:49.360 --> 20:55.840
It will be a small library as the beginning, but we are going to implement, it's going

20:55.840 --> 21:01.240
to offer the implementation of some of the most common reinforcement learning algorithms

21:01.240 --> 21:03.040
and we'll take it from there.

21:03.040 --> 21:04.040
Okay.

21:04.040 --> 21:08.040
And so you will be able to kind of use that with the same decorator model, you decorate

21:08.040 --> 21:12.880
your task and then it will be, yeah, it will be some function calls, right?

21:12.880 --> 21:14.680
So it will be an idea.

21:14.680 --> 21:15.680
Okay.

21:15.680 --> 21:16.680
Great.

21:16.680 --> 21:21.560
Based on your experience with Spark, how do you see Ray evolving?

21:21.560 --> 21:26.120
So you know, it's very hard to predict, right?

21:26.120 --> 21:31.800
I mean, when we start this Spark, we never thought that he's going to become so popular

21:31.800 --> 21:34.480
and it depends a lot of things.

21:34.480 --> 21:41.040
So for us, for now is that we want to build as with Ray, we want to build a platform which

21:41.040 --> 21:48.200
allows us to speed up the research and the application in reinforcement learning.

21:48.200 --> 21:50.880
So that's our first goal.

21:50.880 --> 21:57.200
And it's again, we are talking about only us at Berkeley, but obviously across the board,

21:57.200 --> 21:59.600
you know, it's academia and industry.

21:59.600 --> 22:02.520
So that's kind of, you know, our goal.

22:02.520 --> 22:07.480
So your focus is on making a tool that helps accelerate research and if, you know, the

22:07.480 --> 22:12.360
Spark-like success comes and the Spark-like success comes, but, you know, your focus

22:12.360 --> 22:17.120
on the kind of the user problem is excellent, excellent.

22:17.120 --> 22:20.160
Are there any companies offering like commercial support for Ray?

22:20.160 --> 22:21.160
No, not yet.

22:21.160 --> 22:22.160
So early for that.

22:22.160 --> 22:28.840
There are some companies that are playing with it, but yeah, it's like I mentioned, in order

22:28.840 --> 22:32.640
for someone to put in production, it still has a little bit to go.

22:32.640 --> 22:33.640
Okay.

22:33.640 --> 22:36.120
You know, we are moving fast.

22:36.120 --> 22:37.600
It will take a bit of time.

22:37.600 --> 22:38.600
Yeah.

22:38.600 --> 22:39.600
Yeah.

22:39.600 --> 22:44.480
So you mentioned some other projects around security and other things.

22:44.480 --> 22:48.960
The projects all kind of tie together, like, can you think of like the Ryze Lab is creating

22:48.960 --> 22:53.160
this platform and the security thing, talks to Ray, talks to the next thing, or are they

22:53.160 --> 22:54.160
more or less?

22:54.160 --> 22:55.640
So there are a bunch of projects.

22:55.640 --> 23:00.120
I was actually surprised, like, you know, considering Ryze's relatively new lab, there

23:00.120 --> 23:02.680
were a huge number of projects on the website.

23:02.680 --> 23:03.680
Yes.

23:03.680 --> 23:04.680
Yes.

23:04.680 --> 23:06.680
We have quite a few people.

23:06.680 --> 23:08.440
Quite a few students.

23:08.440 --> 23:12.600
But it did start on this year, generally, officially at least.

23:12.600 --> 23:13.920
So we have a few other projects.

23:13.920 --> 23:18.120
Actually, there are some projects also, like you mentioned, you know, in the Spark-wise

23:18.120 --> 23:21.200
also about real-time and so forth, streaming.

23:21.200 --> 23:24.040
We have a few projects in the context of Spark.

23:24.040 --> 23:30.160
One is about accelerating and reducing the latency of streaming, called Rizl.

23:30.160 --> 23:36.720
And actually, there is a talk later today from Intel, which we collaborate with this platform

23:36.720 --> 23:42.480
big deal, which is experimenting with Rizl and they are going to present some results

23:42.480 --> 23:44.480
today.

23:44.480 --> 23:51.280
And then there is another project called Tegra, it's about how to process time-evolving

23:51.280 --> 23:52.280
graphs.

23:52.280 --> 23:57.720
So you know that Spark also has graphics, which is in graph frames, which are some libraries

23:57.720 --> 24:00.120
to provide graph processing.

24:00.120 --> 24:05.840
So Tegra takes that at the next level and basically saying that most of the systems today

24:05.840 --> 24:08.760
are processing static graphs.

24:08.760 --> 24:11.800
So you want to process the graphs as they change.

24:11.800 --> 24:19.800
And also, you want to be able to ask questions about past instances of the same graph.

24:19.800 --> 24:25.680
So what are your friends, you know, January 15th this year, right?

24:25.680 --> 24:32.160
And how are these friends changing over the former spirit, right, who are the new friends

24:32.160 --> 24:38.800
you got, who are the friends, you know, like, so this is the kind of questions you want

24:38.800 --> 24:39.800
to answer.

24:39.800 --> 24:42.240
The Tegra project tried to answer.

24:42.240 --> 24:46.440
On the security side, I would say it's one project called opaque.

24:46.440 --> 24:52.640
So opaque, it actually, it's again, it's like taking Spark SQL and making it more secure.

24:52.640 --> 24:54.560
What do I mean by that?

24:54.560 --> 25:00.040
You know, today there are solutions, you encrypt the data address, maybe in motion, but

25:00.040 --> 25:05.440
still they do not defend, for instance, if the operating system or other applications

25:05.440 --> 25:08.400
are compromised, right?

25:08.400 --> 25:14.720
So opaque uses, you know, new developments of hardware enclaves, which now is used by

25:14.720 --> 25:16.280
many companies, right?

25:16.280 --> 25:21.360
Also Apple is their Morrison announcement for this cheap, bionic chip right on the iPhone

25:21.360 --> 25:23.640
to run their neural network.

25:23.640 --> 25:30.960
But these enclaves basically defend the application, the code you run within the enclave, even

25:30.960 --> 25:34.720
if the operating system high-pipervisor are compromised.

25:34.720 --> 25:40.160
So basically we try, so zero, opaque is about providing Spark SQL, taking Spark SQL and

25:40.160 --> 25:46.200
providing Spark SQL functionality, but now it's secured against this kind of very strong

25:46.200 --> 25:47.200
attacks.

25:47.200 --> 25:53.040
By running in a hardware enclave, running part of it, like the operators running the hardware

25:53.040 --> 25:54.040
in place.

25:54.040 --> 25:55.040
Okay.

25:55.040 --> 25:56.040
Oh well.

25:56.040 --> 25:58.840
So, and the last one, maybe I want to, well, there are two and one.

25:58.840 --> 26:04.680
I want to mention one is Clipper, this is about prediction service, so there we develop

26:04.680 --> 26:12.280
this very modular architecture, so you can serve models which are developed with very different

26:12.280 --> 26:13.280
systems.

26:13.280 --> 26:21.280
Like, you know, of course, Spark, it will be actually probably the first prediction serving

26:21.280 --> 26:26.960
system which will provide native support for Spark models and develop in others, in many

26:26.960 --> 26:30.120
other frameworks like TensorFlow and others.

26:30.120 --> 26:34.160
So that's all that Clipper and the last one I want to mention is ground.

26:34.160 --> 26:36.720
So ground, it's about managing the metadata.

26:36.720 --> 26:38.800
This is one of very important problems, right?

26:38.800 --> 26:44.400
You have data which is generated and modified, you know, think about an enterprise, different

26:44.400 --> 26:45.640
source is different people.

26:45.640 --> 26:52.680
So it's about really tracking the metadata and answering, being able to answer the questions

26:52.680 --> 26:56.640
who create this data, where the data is coming from, who modifies the data, who do

26:56.640 --> 27:02.720
it is the data, and then on top of that, of course, you can have access control and authorization.

27:02.720 --> 27:07.120
So this is kind of a few projects we are working on.

27:07.120 --> 27:10.800
Ground makes me think a little bit of, like, it might be an interesting blockchain application.

27:10.800 --> 27:15.120
There's a blockchain application, there's another project which is based on blockchain

27:15.120 --> 27:19.000
which is doing about authorization, yes, it's called WAVE.

27:19.000 --> 27:20.000
WAVE?

27:20.000 --> 27:21.000
WAVE.

27:21.000 --> 27:22.000
Okay.

27:22.000 --> 27:23.000
Interesting.

27:23.000 --> 27:24.000
Great.

27:24.000 --> 27:26.080
Well, before we finish up, is there anything else that you'd like to leave listeners

27:26.080 --> 27:27.080
with?

27:27.080 --> 27:31.440
Well, I think you're very nice here, as far as this question says, I know you don't

27:31.440 --> 27:35.920
want to stay with this guy, hey, so I'll save something for the next time.

27:35.920 --> 27:36.920
Great.

27:36.920 --> 27:37.920
Well, thanks so much, Ian.

27:37.920 --> 27:38.920
It was a pleasure having you on the show.

27:38.920 --> 27:39.920
Thank you.

27:39.920 --> 27:40.920
Thank you.

27:40.920 --> 27:47.680
All right, everyone, that's our show for today.

27:47.680 --> 27:53.240
Thanks so much for listening, and of course, for your ongoing feedback and support.

27:53.240 --> 27:58.080
For more information on Ian, or any of the other topics covered in this episode, head

27:58.080 --> 28:03.200
on over to twimlai.com slash talk slash 55.

28:03.200 --> 28:10.160
For the rest of this series, head over to twimlai.com slash AI SF 2017.

28:10.160 --> 28:16.400
And please, please, please send us any questions or comments that you may have for us or our guests,

28:16.400 --> 28:23.640
via Twitter, at twimlai or at Sam Charington, or leave a comment on the show notes page.

28:23.640 --> 28:28.240
There are a ton of great conferences coming up through the end of the year, to stay up to

28:28.240 --> 28:33.400
date on which events will be attending, and hopefully to meet us there, check out our

28:33.400 --> 28:42.800
new events page at twimlai.com slash events, twimlai.com slash events.

28:42.800 --> 28:54.280
Thanks again for listening, and catch you next time.

