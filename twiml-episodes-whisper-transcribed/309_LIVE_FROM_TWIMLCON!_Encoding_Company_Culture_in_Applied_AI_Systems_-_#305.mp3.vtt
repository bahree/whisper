WEBVTT

00:00.000 --> 00:04.960
The conversation you're about to hear was recorded live at Twomokon AI

00:04.960 --> 00:11.720
platforms. For more coverage of Twomokon, visit Twomokon.com-news or follow us

00:11.720 --> 00:17.680
on Twitter at Twomokon AI. But first, a word from our sponsor.

00:17.680 --> 00:23.840
Thanks to our friends at SIGOPT for being a founding sponsor of Twomokon AI

00:23.840 --> 00:29.120
platforms. SIGOPT then bites you to watch CEO Scott Clark's upcoming webinar

00:29.120 --> 00:33.600
outlining the critical capabilities customers prioritize when building

00:33.600 --> 00:38.160
machine learning platforms. He'll draw on experiences working with algorithmic

00:38.160 --> 00:41.840
trading firms that represent over 300 billion dollars in assets under

00:41.840 --> 00:47.160
management and enterprises with over 500 billion in market capitalization to

00:47.160 --> 00:53.280
summarize these trade-offs. Head over to twomolai.com-slash-sIGOPT to register.

00:53.280 --> 01:04.400
Our next guest is Deepak Agrawal. Deepak is vice-president of engineering at

01:04.400 --> 01:10.560
LinkedIn and he's particularly passionate about the connection between the

01:10.560 --> 01:16.200
organization's investment in machine learning and AI and the value that it

01:16.200 --> 01:19.600
creates. And we're going to explore that in our chat.

01:19.600 --> 01:24.960
Let's get started by, I think everybody knows LinkedIn. It's not something

01:24.960 --> 01:27.840
that we need to spend a lot of time explaining. How many of you don't have an

01:27.840 --> 01:35.040
account on LinkedIn? Okay, everybody, market saturation.

01:35.040 --> 01:39.280
But let's maybe get started by talking about some other ways that LinkedIn is

01:39.280 --> 01:44.400
using machine learning. Yeah, so we often say that LinkedIn machine learning is

01:44.400 --> 01:49.040
like oxygen, right? So everything we do has machine learning built

01:49.040 --> 01:53.040
inside it. Like if you go to LinkedIn, the first thing you want to do when you

01:53.040 --> 01:57.840
join LinkedIn is get connected to people who can help you. Now, how do we do

01:57.840 --> 02:00.640
that? We recommend you people that you can connect to. That's all powered

02:00.640 --> 02:04.720
through machine learning. Once you get connected to people, then you start

02:04.720 --> 02:09.280
consuming content that they produce on your news feed. And you know, there is

02:09.280 --> 02:13.440
information overload on the feed. You can see so many content. So what kind of

02:13.440 --> 02:17.040
content do you want to see? And when do you want to see? Like, for instance, if

02:17.040 --> 02:21.840
you're looking for a job, you want to see job recommendations. But if you're

02:21.840 --> 02:24.720
not looking for a job, if you're very happy and you want to learn more deep

02:24.720 --> 02:29.680
learning and if you are connected to Andrew and he publishes something, then you

02:29.680 --> 02:34.000
want to see that on your feed. So you need algorithms to scale this process.

02:34.000 --> 02:38.240
Again, that's all powered through machine learning. If you are a market

02:38.240 --> 02:42.800
here and you want to target the right audience, the entire advertising

02:42.800 --> 02:46.240
ecosystem, we all know works through machine learning. If you're recommending

02:46.240 --> 02:50.960
jobs that you can, that, you know, even if you're not looking for a job, we

02:50.960 --> 02:54.320
still recommend you jobs because there is always a better opportunity out

02:54.320 --> 02:58.160
there for all of you. That's all through machine learning. If you're a recruiter

02:58.160 --> 03:00.560
and trying to source candidate, it's all through machine learning. If you're a

03:00.560 --> 03:04.240
salesperson trying to close a deal, who are the decision makers? How do you

03:04.240 --> 03:08.480
reach to a decision maker? So everything we do on LinkedIn, product, whatever

03:08.480 --> 03:12.400
you see on the app, it's all powered through machine learning. And you know,

03:12.400 --> 03:15.280
finally, you know, this is something that goes behind the scene. We have to keep

03:15.280 --> 03:18.560
the site safe, right? There are, you know, there are a lot of bad actors out

03:18.560 --> 03:22.960
there producing content that should not even reach you. You know, there are

03:22.960 --> 03:26.560
people who create fake profiles. I mean, I've seen a lot of fake profiles of

03:26.560 --> 03:30.960
famous people. And that's not a good thing, right? So just to keep the ecosystem

03:30.960 --> 03:35.600
clean, that's again, machine learning plays a very important role in that as well.

03:35.600 --> 03:39.520
So everything we do at LinkedIn is powered through machine learning. In fact,

03:39.520 --> 03:43.360
when we create a new product idea, in addition to product managers,

03:43.360 --> 03:46.960
engineering managers, designers, we also have a machine learning force

03:46.960 --> 03:50.960
and sitting there, right? When we are designing the product, because we believe

03:50.960 --> 03:56.560
that's the right way to do things. I mean, you know, the UI that you create,

03:56.560 --> 04:01.600
if that can actually build some important feedback loop, that can play a big

04:01.600 --> 04:04.320
difference. I mean, Andrew was talking about collecting label data. Well,

04:04.320 --> 04:07.920
how do you actually ensure that you collect the right label data? We have to

04:07.920 --> 04:11.280
actually start working on it at the design phase. It's too late if you don't

04:11.280 --> 04:14.800
pay attention to it at that stage. Then you have to build very complicated

04:14.800 --> 04:18.320
model that essentially do guessing, right? So you don't have to guess if you can

04:18.320 --> 04:22.080
actually get the right data. And so that's why it's very important to start that

04:22.080 --> 04:27.440
process from the very beginning of machine learning process of product

04:27.440 --> 04:33.360
process. One of the things that has always fascinated me in my conversations with

04:33.360 --> 04:39.120
folks at LinkedIn is we think of LinkedIn relative to a more traditional

04:39.120 --> 04:42.560
enterprise as kind of a digital native company born on the web.

04:42.560 --> 04:49.520
You know, the product is web. But in a lot of ways, the company has evolved

04:49.520 --> 04:53.760
similarly. You know, it's initial investments in machine learning and

04:53.760 --> 04:57.680
what the way it's supporting machine learning today are very different. Can

04:57.680 --> 05:01.360
you talk a little bit about the journey at LinkedIn and how

05:01.360 --> 05:06.000
ML and AI has evolved over the years? Yeah, so LinkedIn was always a data

05:06.000 --> 05:10.240
first company, right? Like if you all remember, the word data science was

05:10.240 --> 05:14.960
coined by DJ Patil at LinkedIn. So we were always very savvy about data. We

05:14.960 --> 05:21.040
knew our businesses all about the data, the unique data we possess, right? So

05:21.040 --> 05:23.440
we were always doing data science. We were always doing data product

05:23.440 --> 05:28.000
innovation. We also started doing machine learning very early on. Like in 2007,

05:28.000 --> 05:32.640
the first machine learning product, real product was the people recommendations,

05:32.640 --> 05:36.560
right? So in those days, we would compute, we will have simple machine learning

05:36.560 --> 05:40.240
models, of course. You know, so we'll have a simple model, the features of

05:40.240 --> 05:42.960
these models. There were a handful of features, but they're very carefully

05:42.960 --> 05:47.120
tweaked based on intuition. Once we have that model, then productionizing these

05:47.120 --> 05:51.920
models at that scale was still very difficult. So we will actually build

05:51.920 --> 05:56.800
Hadoop systems that will do the ranking and scoring offline, right? So because

05:56.800 --> 06:00.800
online was not very well developed. And then we'll run these processes

06:00.800 --> 06:05.360
every day, right? So search was another system that we actually developed

06:05.360 --> 06:09.680
very early on that used machine learning. Fast forward 2012, we got more

06:09.680 --> 06:13.280
sophisticated, right? So the first sophistication we added in terms of

06:13.280 --> 06:16.640
machine learning was in our advertising system, right? So the advertising

06:16.640 --> 06:19.600
system, we most of our other recommended systems are based on simple

06:19.600 --> 06:23.360
collaborative filtering idea at those times. You know, people who bought this,

06:23.360 --> 06:26.800
also bought this. But then advertising was the first place where we added a lot

06:26.800 --> 06:30.960
of sophistication. We added, we build near real-time systems, we build online

06:30.960 --> 06:35.520
systems that can score things at runtime, more complex models,

06:35.520 --> 06:40.320
and you know, encouraged by the success we got there, we then

06:40.320 --> 06:43.360
attacked the feed problem, the news feed problem. For those of you who have been

06:43.360 --> 06:47.440
using LinkedIn for a long time, I'm sure most of you will tell me today the

06:47.440 --> 06:50.960
news feed is much better than what it was five years ago. That's all due to

06:50.960 --> 06:54.160
machine learning, right? I mean, so a lot of work happened to kind of add

06:54.160 --> 06:58.000
sophistication to the news feed algorithm. And once we got success in these

06:58.000 --> 07:01.760
two big applications, then we started thinking of how do we generalize it across

07:01.760 --> 07:06.400
the board, right? Why, why just advertising? Why is this news feed? Why can't we build

07:06.400 --> 07:10.160
a platform that can actually generalize it to everywhere? And that's what we

07:10.160 --> 07:13.200
have been doing for the last few years. And so we have a program at LinkedIn

07:13.200 --> 07:16.560
called ProML, Productive Machine Learning. And again, I think a lot of

07:16.560 --> 07:21.040
companies have a platform. But I mean, one unique thing about our platform is,

07:21.040 --> 07:25.120
you know, we are building a platform with a very strong opinion, right? So

07:25.120 --> 07:29.280
you can build a machine learning platform that can cater to a lot of

07:29.280 --> 07:32.080
tail users, right? So if you're a cloud company, you're going to build a

07:32.080 --> 07:35.760
machine learning platform that can cater to the needs of a diverse set of

07:35.760 --> 07:40.240
customers. That's not our goal, right? We know that our ROI is going to come from

07:40.240 --> 07:44.160
a few big applications. And the platform we have built is really

07:44.160 --> 07:47.520
suitable more for that, right? So large scale recommender systems,

07:47.520 --> 07:51.680
large scale search systems, large scale classification problems. These are the

07:51.680 --> 07:56.160
problems that we face. And our platform is really geared towards that, right? So

07:56.160 --> 08:00.960
we also know that we have reached a point where without adding more sophistication

08:00.960 --> 08:04.080
to our systems, you're not going to get the ROI that we used to get.

08:04.080 --> 08:06.960
So I give you an example, like, you know, two years ago,

08:06.960 --> 08:11.440
for our job recommendations system, we revamped the model. We kind of moved

08:11.440 --> 08:16.640
away from a simple linear model to something more complex, involving deep learning.

08:16.640 --> 08:19.520
And, you know, involving something that is a homegrown technology called

08:19.520 --> 08:22.800
generalized mixed model. So I mean, I'm not going to technical detail, but these

08:22.800 --> 08:26.000
are very high dimensional model. We had a model with GMM.

08:26.000 --> 08:30.240
Yeah, exactly. This is the old technique in statistics, known since the 70s.

08:30.240 --> 08:35.680
And in statistics, it was applied to application that 500 patients.

08:35.680 --> 08:39.440
Now, you know, those 500 patients become half a billion patients. And then suddenly

08:39.440 --> 08:44.320
the explosion in the number of parameters in complexity increases a lot, right?

08:44.320 --> 08:50.240
So we applied that and, you know, we found a 30% improvement in result. That was stunning.

08:50.240 --> 08:54.320
And so we were all very happy. But for the next six months, nothing happened.

08:54.320 --> 08:56.800
And then like, it was very surprising. Like, okay, well,

08:57.760 --> 09:01.920
did the engineers all go to Hawaii or what's happening? Like, why is nothing moving?

09:02.480 --> 09:06.160
And what we realized is when we introduced that complexity,

09:06.880 --> 09:11.200
the tooling did not keep pace with that, right? So it became very hard for the subsequent

09:11.200 --> 09:15.280
engineers to kind of iterate on this model, because we didn't build the appropriate tooling that

09:15.280 --> 09:19.040
will enable them to kind of iterate. So that was the realization.

09:19.040 --> 09:23.120
And I know this is not going to work as we actually start introducing more sophistication,

09:23.120 --> 09:28.080
the industrial process will only work if the engineers are still productive.

09:28.080 --> 09:33.200
And in order to improve productivity, when you add more complexity, especially for such large

09:33.200 --> 09:36.960
skill distributed systems, if you really want them to run efficiently, if you want them to run

09:36.960 --> 09:42.720
in a reliable fashion, you have to make sure that the tooling and infrastructure can keep pace

09:42.720 --> 09:47.280
with the innovation that we are doing. And so that was really the impetus of this project that

09:47.280 --> 09:51.040
we kind of run called ProML. And we actually run it very rigorously. We are not just building

09:51.040 --> 09:56.400
platform components. We actually measured the success of that. So every week, we measured the

09:56.400 --> 10:02.800
number of successful experiments we have run. So there are a lot of experiments, our engineers run,

10:02.800 --> 10:07.040
but we only track the number of successful experiments, right? Because otherwise you can start

10:07.040 --> 10:11.200
cheating, right? Like someone can just go parameters sweep on the grid and say, okay, I did a

10:11.200 --> 10:15.440
parameter sweep of 100 different values. And so I ran 100 experiments. I don't care. I mean,

10:15.440 --> 10:20.160
you know, you can run 100 experiments or two experiments. Did how many of them succeeded, right? So

10:20.160 --> 10:25.440
that's our metric. And we have seen like more than 30% improvement in the number of successful

10:25.440 --> 10:32.000
experiment that we run on the site after introducing this program. We've still not done. There is still

10:32.000 --> 10:36.880
a long way to go, but you know, this has been really useful for us. It has kind of also bought

10:36.880 --> 10:40.960
the teams together. So earlier, you know, if you don't have a standardized way of doing things,

10:40.960 --> 10:45.520
no matter how hard you try, the culture in the feed team would be different from the culture in

10:45.520 --> 10:49.440
the jobs team, right? And that's not good, right? I mean, we don't want to create different

10:49.440 --> 10:54.160
cultures in the same company. In fact, we want, given that we are a centralized organization,

10:54.160 --> 10:59.040
we want people to flow from one area to the other, right? So you did your tour of duty on the

10:59.040 --> 11:03.120
feed and you should just go to the jobs team and learn about the jobs product. And you should be

11:03.120 --> 11:08.240
productive in a day, right? And that is only possible if you standardize things. And so this project

11:08.240 --> 11:13.440
has also helped us to standardize things and not kind of deviate too much. That's a really

11:13.440 --> 11:22.640
interesting observation. I talked to a lot of people who put the idea of culture against the idea

11:22.640 --> 11:29.360
of technology in a sense. You know, technology is not most important in its culture, but you're

11:29.360 --> 11:34.320
talking about the relationship between the two and technology supports creating the culture

11:34.320 --> 11:40.960
in an important way. Yes, so in engineering, the process, which is called CICD, right? Continuous

11:40.960 --> 11:46.000
integration, continuous development. If you look at different companies, they use different tools.

11:46.000 --> 11:53.200
And to me, that process, the tools you use is actually a reflection of your culture, right? So the way

11:53.200 --> 11:58.240
you do machine learning really reflects the culture you're trying to build. Like for instance,

11:58.960 --> 12:03.360
there are many companies who actually build standardized tooling just for model creation.

12:04.320 --> 12:07.600
Once you've created the model, the deployment can happen in very different ways.

12:08.320 --> 12:13.440
Yeah, I'm not saying that's good or bad, but that kind of tells you the culture of the company,

12:13.440 --> 12:19.760
right? We don't do that. We just want to standardize the entire end-to-end machine learning

12:20.320 --> 12:24.640
culture. And then, you know, if you work at LinkedIn, that's really the culture that we have,

12:24.640 --> 12:30.640
right? Because we believe that's the best way for us to increase the ROI of machine learning.

12:30.640 --> 12:36.560
To increase ROI, there are several components. You have to be cost effective. How do you become cost

12:36.560 --> 12:42.000
effective? You have to be efficient. You have to be productive. And you also have to innovate,

12:42.000 --> 12:45.680
right? Because if you don't innovate, then no matter how productive you are, you're going to run

12:45.680 --> 12:51.520
experiment that will only give you marginal returns after a while. So the innovation has to continue,

12:51.520 --> 12:55.520
because innovation is not easy. We all know that. If you don't innovate for the next six

12:55.520 --> 13:00.640
months, you certainly would not stumble upon a breakthrough that's going to completely change

13:00.640 --> 13:06.320
the game, right? So you have to continuously innovate. And you cannot give your engineers time

13:06.320 --> 13:10.800
to innovate if they are not productive. If they take too much time to do their job, when are they

13:10.800 --> 13:18.080
going to get the time to innovate, right? So innovation, productivity, efficiency, and those are three.

13:18.080 --> 13:22.960
So innovation, productivity, efficiency, like these things, they are all entangled. They all have

13:22.960 --> 13:27.200
to be attacked together. You cannot just say, oh, I'm not going to worry about productivity. I'm

13:27.200 --> 13:31.760
just going to worry about innovation. It doesn't work that way, right? Because, you know, the time that

13:31.760 --> 13:37.600
an engineer has and, you know, the kind of investment you can make in the program is fixed, right?

13:37.600 --> 13:42.400
You have a fixed budget problem and you have to really solve these three components in the best

13:42.400 --> 13:50.000
possible way, depending on your business needs. Yeah. So we talked about platform abstractly thus far.

13:50.000 --> 13:56.800
Can you give us an overview of ProML and the kind of major components, what's in place now,

13:56.800 --> 14:00.960
the direction you're heading with it, et cetera? Yeah. So our aspirational goal is the entire

14:00.960 --> 14:06.480
end-to-end machine learning process should become completely automated, right? Like once a scientist

14:06.480 --> 14:10.720
understands the business problem, they understand the kind of models they need to build,

14:11.280 --> 14:16.240
to solve that machine learning problem. From there on, everything else should become automated.

14:16.240 --> 14:21.120
And obviously that's very easy to say, we all know it's not very difficult to do. I don't think

14:21.120 --> 14:26.640
anyone has solved that, but that's the aspiration at least. So just like any other end-to-end machine

14:26.640 --> 14:31.120
learning platform, we have a model creation process, right? So how do you create models very easily?

14:31.120 --> 14:35.600
How do you compose different kinds of ideas together? Like if someone wants to take XGBoost and

14:35.600 --> 14:41.680
GLM makes and neural network and try it out, like it should be very easy to try that. So model

14:41.680 --> 14:47.680
creation, data preparation, you know, that's one major component. Once you've built that,

14:47.680 --> 14:52.080
then how do you deploy these models in production? Once you've deployed the model in production,

14:52.080 --> 14:56.000
then how do you make sure that you're not babysitting the model, right? We don't want our scientists

14:56.000 --> 15:00.000
to become babysitters, right? Because that's not a good way to run a machine learning program.

15:00.000 --> 15:04.080
So the maintenance of these models, once they're in production, should be almost automated.

15:04.080 --> 15:08.240
And what do I mean by that? If the model is running in production, you still have to retrain

15:08.240 --> 15:12.240
the model because the world around you is changing every day. It's not a stationary process.

15:12.240 --> 15:16.880
So every day, the data should flow in the directory automatically. If for whatever reason,

15:16.880 --> 15:21.840
the data does not flow, there should be an alert and the person who's running the model should

15:21.840 --> 15:26.960
kind of know about it and then they should have a fall-off graceful degradation. If everything

15:26.960 --> 15:31.040
is going well, then the data should come in and you should be automatically building systems that

15:31.040 --> 15:36.320
can retrain the model and deploy it in production without any issues, right? And so this should not

15:36.320 --> 15:42.400
require any human intervention, right? The rest of the things, model creation and all that stuff,

15:42.400 --> 15:48.160
that's where the scientists need to spend the time. So one unique feature that we have added

15:48.160 --> 15:53.680
to this entire ProML, which may be distinct from other is we created this notion of a feature marketplace.

15:54.480 --> 15:58.560
Right? So we all know in order to create machine learning models, features are the most important thing.

15:58.560 --> 16:02.560
And so we don't want the scientists to be starting from the scratch, right? So there is a marketplace

16:02.560 --> 16:06.720
where we have actually created a lot of features for you, right? Think of them as prefabrication,

16:07.360 --> 16:13.040
prefabricated features, right? For instance, based on all the activity of users on the site,

16:13.040 --> 16:17.520
we know who has a strong job intent. We know who clicked on an ad in the last two. So these are

16:17.520 --> 16:22.560
features that are all available to you. Based on your profile, we know what are your standardized

16:22.560 --> 16:27.280
title and all that. So if you want to build a machine learning task, you can just grab these features

16:27.280 --> 16:33.440
from the feature marketplace using a consistent API, we call it frame. And that makes it super easy

16:33.440 --> 16:39.120
for you to start building a model right from the very word go. And you will actually reach 80%

16:39.120 --> 16:44.240
there, right? So that's very easy. Now, if the additional 10% improvement can add a lot of

16:44.240 --> 16:48.560
business ROI, then you better spend more time on that because otherwise the first cut should be

16:48.560 --> 16:54.320
something that you should be able to do very quickly. So I alluded to this earlier, but

16:54.320 --> 17:01.280
and you did as well. LinkedIn was very early on in this space. You made initial investments based

17:01.280 --> 17:07.680
on your technology landscape at the time, very Hadoop centric, but that's needed to evolve

17:07.680 --> 17:14.000
over the years. Can you talk about that evolution and the challenges that it presented the way

17:14.000 --> 17:20.560
you managed it? Yeah, so that was not an easy journey. So I think when we started the advertising

17:20.560 --> 17:26.000
problem, for instance, we wanted to do very large scale modeling as I told you, but Hadoop is not

17:26.000 --> 17:32.800
very amenable to that kind of computation. So we actually did algorithmic innovations to do

17:32.800 --> 17:38.080
those distributed computations. So for instance, the first version of model fitting algorithm we

17:38.080 --> 17:44.560
launched on the site was based on ADMM, right? So ADMM can actually help you do computation in pieces

17:44.560 --> 17:48.320
and then you can patch it all together by using a simple computer. And that was very amenable to

17:48.320 --> 17:55.120
Hadoop and that's how we started. We got the ball rolling and we saw enormous ROI system and

17:55.120 --> 17:59.280
then Spark came along. We are one of the very early adopters of Spark even when it was not

17:59.280 --> 18:03.920
very stable. And so we believed that Spark is going to solve a lot of our problems. So we invested

18:03.920 --> 18:10.960
very early in Spark. We worked in fact with folks in MLlib. In fact, a couple of the folks who actually

18:10.960 --> 18:18.080
joined MLlib were at LinkedIn. So we had a great relationship with them and we then really

18:18.080 --> 18:23.040
focused on its Spark first-class citizen in our ecosystem. That helped us a lot to scale these

18:23.040 --> 18:28.400
algorithms. And then finally TensorFlow came in, right? And so, you know, when with the deep learning,

18:28.960 --> 18:36.480
again, TensorFlow, TensorFlow in those days had a lot of great things, but it was not, it didn't have

18:36.480 --> 18:41.040
everything that would help us deploy it into it. So we had to build things like Tony and other

18:41.040 --> 18:46.000
things that we have open source. So that is that spin really our journey. And I think all along,

18:46.000 --> 18:50.800
we had very strong support from the business to do what we really needed to because the ROI

18:50.800 --> 18:55.360
was always there. So that is the key lesson for me, right? Whenever you're building a machine

18:55.360 --> 19:00.640
learning program, you know, you know, when you're going to your executives, just show them the money

19:00.640 --> 19:07.600
and then everything else becomes much easier. If you can't prove the ROI, then it becomes hard.

19:07.600 --> 19:14.880
So be the money. Yeah. You can do a lot of research, but the program should act tangible and

19:14.880 --> 19:18.800
direct value to the business. And if you can ensure that balance, I think you are in great shape.

19:18.800 --> 19:24.080
Then you get all the support you need. Because otherwise, you know, going to your executive team

19:24.080 --> 19:29.600
in the early days of deep learning and telling them, okay, you need to help us get a GPU cluster.

19:29.600 --> 19:34.480
It's going to cost $20 million. It's not a very easy argument to make a way. You have to wait

19:34.480 --> 19:40.240
for a while before they actually buy into that. But you know, if you if you have a track record

19:40.240 --> 19:46.720
of delivering, then these things work out much much much more easily. We alluded to this and my

19:46.720 --> 19:52.960
conversation with Andrew earlier, you have to manage that portfolio and expectations very carefully.

19:52.960 --> 19:57.360
Right. You know, what you can deliver in your term, what the overall vision is. How have you done

19:57.360 --> 20:05.040
that? Yeah, I think you're just just described my job. But yeah, so I mean, that's what a portfolio

20:05.040 --> 20:09.840
manager would do. Right. So I mean, I have this philosophy, as I said, like, you know, so there are

20:09.840 --> 20:14.240
three components to your portfolio. One is the core investment. So this is something that you do to

20:14.240 --> 20:19.120
add value to your business on a consistent basis. Like is machine learning able to improve

20:19.840 --> 20:25.200
the number of the engagement that members have on our side? Is machine learning able to help us

20:26.160 --> 20:31.920
create more revenue? Help our customers have a better experience? Is machine learning able to

20:31.920 --> 20:37.760
keep our sites safe and clean? Right. So these are core investments. We cannot for not to do those.

20:37.760 --> 20:44.320
Right. So that's a bulk of the investment. Like I would say that's 60%. The other 30% is more strategic

20:44.320 --> 20:49.600
investment. So we know that deep learning is going to help us in the next six months. And after

20:49.600 --> 20:54.880
all the model iteration that our engineers are doing with ProML, you know, even if they can do a

20:54.880 --> 20:58.960
lot of iteration after a while, these methods are going to only give marginal gains. In fact, you

20:58.960 --> 21:03.600
know, it will, it will actually put us in a lot of trouble because there will be so many experiments

21:03.600 --> 21:08.960
queued up. And the amount of experimental budget you have is still constant. You cannot run

21:08.960 --> 21:13.840
hundreds of experiments on the side, even if you can, because the experimental budget is fixed.

21:13.840 --> 21:19.440
Right. So you have to invest in strategic initiatives. So that in six months, you get something new,

21:19.440 --> 21:24.000
which is going to give you a big gain. Right. And so the experiments you're running will actually

21:24.000 --> 21:28.560
still produce a lot of ROI. So that's the strategic bucket. And the 10% is more venture bed. Right.

21:28.560 --> 21:34.720
So new product ideas, things like, okay, how can we do deep reinforcement learning and production

21:34.720 --> 21:40.320
is it in the future? How do we build chat boards that can change the entire way users interact

21:40.320 --> 21:45.040
with each other on LinkedIn? These are all venture beds and ideas. So that's roughly the portfolio

21:45.040 --> 21:52.000
we tried to maintain like 70, 20, 10. And so far it has worked pretty, pretty well. We also do

21:52.000 --> 21:57.040
other things like to encourage grassroots innovation. We have something called the ideas program.

21:57.040 --> 22:02.000
So every quarter, anyone in the organization can actually submit an idea they want to work on.

22:02.560 --> 22:07.440
And then there is a committee who actually looks at all the ideas and we'll select the top 10 ideas.

22:07.440 --> 22:12.800
And those ideas get funded through our normal resource pool. So this is just to make sure that

22:12.800 --> 22:18.160
everyone, it's not always top down. The ideas are not always top down. And some of the ideas,

22:18.720 --> 22:22.400
some of the best ideas we have seen actually come from that grassroots innovation.

22:22.400 --> 22:28.320
Our organization is also very energized. They know that we are in the innovation business here.

22:28.320 --> 22:34.320
This is how we actually produce ROI. So it's also important to create that culture of innovation.

22:34.320 --> 22:39.920
And you also organize hackathons so that folks can actually experiment with new product ideas

22:39.920 --> 22:46.000
or prototypes and so on and so forth. So it's really that culture that is very important I think

22:46.000 --> 22:50.640
that helps you create that energy and keep doing that innovation. And then we have an awesome

22:50.640 --> 22:56.560
infrastructure team that actually partners with us to make sure that we are productive and we are

22:56.560 --> 23:06.560
efficient. Okay. You mentioned earlier that you're very deliberate about measuring the experimental

23:06.560 --> 23:13.680
velocity of teams at LinkedIn. Can you dig into that a little bit deeper? Yeah. So just as we

23:13.680 --> 23:18.720
have a centralized AI organization at LinkedIn, we also have a central experimentation platform.

23:18.720 --> 23:24.000
Yeah. Right. So every AB test, you run, we document that, we log that data.

23:25.120 --> 23:31.280
And based on those AB experiments, we have readouts of things that succeeded in production.

23:31.280 --> 23:37.120
So we kind of aggregate all of that information and make sure that we kind of have a dashboard

23:37.120 --> 23:42.240
and we look at it every week and we measure things. Just as we will measure any business metric,

23:42.240 --> 23:46.480
just as you will, you know, anyone's just as in your business metric, there are seasonality,

23:46.480 --> 23:52.480
right? Like in the summer, people take vacation and we see a drop. If I see a very big drop,

23:52.480 --> 23:57.520
then that's an alarm bell, like what is happening. And then I realize, oh, while everyone is attending

23:57.520 --> 24:02.480
a conference, so okay, well, that's why we didn't have an hour experience. So we are very deliberate

24:02.480 --> 24:07.200
and we are very particular about that. We in fact, enhanced it now, right? So now we started with

24:07.200 --> 24:12.320
the total number of successful experiments. Now I'm asking even the teams to label their experiments.

24:12.320 --> 24:18.320
So is it a large t-shirt or medium-sized t-shirt or a small t-shirt, right? And it's very interesting

24:18.320 --> 24:23.680
how teams kind of put those labels and then we discuss about it. Like, you know, you can give a

24:23.680 --> 24:30.880
hard time to your team. You know, you're a good way. I'm a good manager. So why do you think this

24:30.880 --> 24:36.240
is a large experiment? And you know, it's okay. Do you think you can't think bigger than this?

24:36.240 --> 24:42.560
And so it's actually a nice thing for you to even get a sense of the culture that you have in

24:42.560 --> 24:47.680
your organization. Like, who thinks what is big? What is medium? What is large? And over time,

24:47.680 --> 24:52.320
you can actually create this culture where you are actually asking folks to run fewer numbers of

24:52.320 --> 24:57.440
larger experiment because it is great to say we can run a lot of experiment where every experiment

24:57.440 --> 25:02.720
has a cost associated with it. And I don't mean infrastructure cost only. I mean opportunity cost.

25:02.720 --> 25:06.880
Right? If someone is running an experiment, they're going to wait. Then for two, after two days,

25:06.880 --> 25:10.720
they're going to look at the results. And then they're going to move on to the next idea.

25:10.720 --> 25:15.440
So this is a hidden, economists call it the hidden cost, right? So there is a cost to every experiment

25:15.440 --> 25:22.080
you run. And the best way for an organization to become really effective is to run a large number

25:22.080 --> 25:27.920
of high value experiment, not a large number of experiments. That is not all enough, right? I mean,

25:27.920 --> 25:33.040
because in many cases, I've seen people run parameter sweep experiments. I mean, those experiments

25:33.040 --> 25:37.840
should be all automated. So you have to actually have forensics on the experiments as well.

25:38.320 --> 25:43.200
Once you become a large organization, the next step is to kind of analyze, have telemetry on your

25:43.200 --> 25:48.080
experiments and then figure out what experiments can be automated and encourage your engineers to

25:48.080 --> 25:54.640
work on the big things rather than on things that can be automated. You mentioned in opening things

25:54.640 --> 26:03.840
up that LinkedIn was very early in adopting data science. And in that period of time, you know,

26:03.840 --> 26:09.280
once that term caught on, you know, everyone who worked in this space was a data scientist.

26:09.280 --> 26:14.960
Since then, the roles have evolved. We've got machine learning engineers, we've got platform

26:14.960 --> 26:19.760
teams. Yeah, what does that evolution look like at LinkedIn and where do things stand now?

26:19.760 --> 26:26.720
Yeah, so that evolution, like in any other place, is still evolving, like we all know. I mean,

26:26.720 --> 26:31.680
to me, there are four pillars of AI or data science, whatever you call it. And that's how, actually,

26:31.680 --> 26:37.120
five machine learning computer science, of course, that's the key. Then statistics,

26:37.920 --> 26:43.520
optimization and economics and systems engineering, right? So these are really the five pillars. And

26:43.520 --> 26:48.720
when people from these five disciplines come together and work together to solve hard

26:48.720 --> 26:54.960
problem, that is what creates the magic, right? Now, I think labels are labels that keep changing

26:54.960 --> 27:00.800
over time. I don't think at LinkedIn, we pay too much attention to the labels. We just make sure

27:00.800 --> 27:05.520
that these five disciplines can always come together and work. And then, you know, once, so that's

27:05.520 --> 27:10.640
the technology side, but technology side is also not enough to create awesome machine learning

27:10.640 --> 27:14.800
solutions, like for instance, the most important thing in a machine learning problem is to define

27:14.800 --> 27:21.920
the problem. What are you trying to solve? And you cannot define the problem unless you interface

27:21.920 --> 27:26.160
very closely with the domain experts, you know? So you have to work very closely with the product

27:26.160 --> 27:31.360
team. And in this day and age, you have to worry about security, you have to worry about legal. So

27:31.360 --> 27:39.280
you have to interface with the legal team, right? So it takes a village to get AI, right? And so,

27:39.280 --> 27:44.640
I think that's how we think about AI at LinkedIn. And in order to actually make sure that everyone

27:44.640 --> 27:48.800
understands the basic of AI at LinkedIn, we have actually created what we call the AI Academy.

27:49.440 --> 27:54.800
And so it has three levels of courses. The AI 100 is general awareness. In fact, I also

27:54.800 --> 27:58.320
teach there, we're passionate about teaching, right? So this is like a two-day course. We have our

27:58.320 --> 28:03.200
best people kind of teach to everyone in the company what AI is all about. And it starts from

28:03.200 --> 28:09.120
data, right? I mean, you cannot be AI first company without being a data first company.

28:10.000 --> 28:14.000
Because at least today, we don't know how to do AI without data. Maybe in the future,

28:14.000 --> 28:17.440
we may create a new theory that can help us do AI without data. But right now,

28:18.160 --> 28:25.120
we all know we need data to do AI. So data first to AI first. And that education is a very

28:25.120 --> 28:30.240
important component. Then we have AI 200. So if you are an engineer who has taken Andrew's course,

28:30.240 --> 28:34.160
but now want to get your hands dirty, you can actually get your hands dirty through AI.

28:34.160 --> 28:40.000
200. And AI 300 is an internship program. You can be an intern in the AI organization. Someone

28:40.000 --> 28:44.320
is going to work very closely with you. And you will be actually deploying things in production,

28:44.320 --> 28:51.040
right? So the education and this collaboration and tooling, this is really how we run AI at LinkedIn.

28:51.040 --> 28:56.160
And we always say, it takes a village to get AI, right? And we have seen that first time.

28:56.160 --> 29:01.280
You know, if you try to create an AI program in a company in a silo, it would never work.

29:01.920 --> 29:07.840
You just have to do it together. It's too big to be solved by one discipline is my opinion.

29:07.840 --> 29:19.360
So maybe to start to close out, I'd love to hear your vision for where ML is going at LinkedIn.

29:19.360 --> 29:22.160
And maybe even more broadly in the industry.

29:22.160 --> 29:27.120
Yeah, so I think we have reached a point where deep learning is really helping us a lot.

29:27.120 --> 29:32.240
And you know, we are able to solve some very traditional problems that were very hard to solve,

29:32.240 --> 29:38.240
in a very spectacular way. But I still think Andrew was also talking about that.

29:38.240 --> 29:42.480
Machine learning is still not very accessible broadly, right? Only a few companies

29:42.480 --> 29:49.920
that have the talent and that have the culture that they've built in this space for a long time,

29:49.920 --> 29:54.160
they are the ones who are really reaping a lot of benefit now. So how do we make machine learning

29:54.160 --> 29:59.920
more accessible broadly so that everyone can benefit from that? I think that's a very challenging

29:59.920 --> 30:04.800
problem. It's not easy, right? Like, you know, if you don't even understand how to manage data

30:04.800 --> 30:09.520
to think that one fine day you'll just wake up and run a deep learning model and we'll all start

30:09.520 --> 30:15.280
working automatically, that's a pipe thing. That doesn't happen, right? So getting there would be a

30:15.280 --> 30:21.440
very big task for us. Once we get there, then I think the computational cost is also something

30:21.440 --> 30:26.720
that we all have to think about. So GPUs are not just costly, they are also bad for the environment.

30:26.720 --> 30:31.760
Right? And so, you know, we don't want a planet where we are actually doing so much compute

30:32.560 --> 30:36.320
without being careful of what we're doing to our environment and to our planet, right? So

30:37.520 --> 30:41.840
if you want to be cost effective, you have to figure out ways to kind of do the computation

30:41.840 --> 30:46.240
in a more efficient way. I think that I believe it's going to become a very important

30:46.960 --> 30:51.600
topic for all of us. And finally, responsible AI, right? And again, I don't want to steal

30:51.600 --> 30:57.840
thunder. There is a panel on this later at the conference, but doing AI in an ethical way,

30:57.840 --> 31:04.240
doing AI in a responsible way, making sure the privacy of individuals and everyone else is kind

31:04.240 --> 31:08.720
of a dear to and we're not having data breaches and all that stuff, right? So that's again,

31:08.720 --> 31:12.960
a very important topic for all of us because there's no point in creating a very powerful technology

31:12.960 --> 31:17.200
if it works against the humans, right? I mean, we need to create technology that helps us

31:17.200 --> 31:23.840
become better, right? And so this is again another big area for all of us to think about in the

31:23.840 --> 31:28.480
future. It's not about where this will go. It is all about where we want to take it to, right?

31:28.480 --> 31:32.320
So this is one thing where I don't want to make a prediction. I want to kind of appeal to all of

31:32.320 --> 31:37.520
you that we all think about it very deeply and make sure that we are all using the technology

31:37.520 --> 31:43.280
in a way that is going to help the human race, not hurt the human race. Awesome. Well, Deepak,

31:43.280 --> 31:55.120
thanks so much for joining us. Thank you. All right, everyone. I hope you enjoyed our show

31:55.120 --> 32:01.680
straight from the main stage at TwomoCon AI Platforms. For more information about today's show,

32:01.680 --> 32:09.120
visit TwomoAI.com. And for more TwomoCon coverage, visit TwomoCon.com slash news.

32:09.120 --> 32:12.640
Thanks so much for listening and catch you next time.

