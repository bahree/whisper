WEBVTT

00:00.000 --> 00:16.320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

00:16.320 --> 00:21.480
people, doing interesting things in machine learning and artificial intelligence.

00:21.480 --> 00:31.440
I'm your host Sam Charrington.

00:31.440 --> 00:37.960
Alright Twimble listeners, this is your last chance to register for my upcoming AI Summit.

00:37.960 --> 00:43.880
The event takes place next week, April 30th and May 1st in Las Vegas, Nevada, in conjunction

00:43.880 --> 00:46.920
with the Interop ITX Conference.

00:46.920 --> 00:51.600
I'll be presenting an introductory session on ML and AI Fundamentals and we'll have

00:51.600 --> 00:57.880
deep dive sessions on AI for NLP and conversational applications, computer vision and Internet

00:57.880 --> 00:59.200
of Things.

00:59.200 --> 01:04.320
Then we'll be discussing more strategic topics including data collection and annotation,

01:04.320 --> 01:11.320
operationalizing and managing AI and building out your organization's AI culture and strategy.

01:11.320 --> 01:15.680
Attendees will also have a chance to discuss their organization's unique AI opportunities

01:15.680 --> 01:20.040
and challenges with our panel of distinguished experts.

01:20.040 --> 01:26.120
To learn more about the event, head on over to Twimbleai.com slash AI Summit for Details

01:26.120 --> 01:29.600
and a Discount Code.

01:29.600 --> 01:34.920
In this episode, I'm joined by Marco Couturi, professor of Statistics at University Paris

01:34.920 --> 01:36.240
Saclay.

01:36.240 --> 01:40.640
Marco and I spent some time discussing his work on Optimal Transport Theory at NIPS last

01:40.640 --> 01:41.960
year.

01:41.960 --> 01:46.560
In our discussion, Marco explains Optimal Transport which provides a way for us to compare

01:46.560 --> 01:48.680
probability measures.

01:48.680 --> 01:53.480
We look at ways Optimal Transport can be used across machine learning applications including

01:53.480 --> 01:57.000
graphical NLP and image examples.

01:57.000 --> 02:02.080
We also touch on GANs or Generative Adversarial Networks and some of the challenges they

02:02.080 --> 02:05.040
represent to the research community.

02:05.040 --> 02:07.080
And now on to the show.

02:07.080 --> 02:14.720
All right, everyone, I am here in Long Beach, California at the NIPS conference and I am

02:14.720 --> 02:17.240
with Marco Couturi.

02:17.240 --> 02:22.400
Marco is a professor of Statistics at University Paris Saclay.

02:22.400 --> 02:23.400
Ah.

02:23.400 --> 02:27.200
University Paris Saclay is how I should say it.

02:27.200 --> 02:30.200
You're perfectly fine.

02:30.200 --> 02:31.800
Marco, welcome to the podcast.

02:31.800 --> 02:32.800
Thanks.

02:32.800 --> 02:34.200
Thanks for having me.

02:34.200 --> 02:38.280
So why don't we get started by having you tell us a little bit about your background.

02:38.280 --> 02:44.080
Are you coming at things from a stats perspective, but applying them to the world of machine learning

02:44.080 --> 02:45.080
in AI?

02:45.080 --> 02:47.280
How did you combine these interests?

02:47.280 --> 02:48.280
Yeah, exactly.

02:48.280 --> 02:52.080
So I'm here at NIPS, so I might maybe start with that.

02:52.080 --> 02:59.600
I've been coming at NIPS for about, I think, 12 years maybe since I was a PhD student.

02:59.600 --> 03:01.080
And you haven't missed any?

03:01.080 --> 03:05.680
Oh, no, I have missed a few right in the middle, but let's say I've probably been here more

03:05.680 --> 03:07.560
often than not during those years.

03:07.560 --> 03:08.560
Okay.

03:08.560 --> 03:11.520
So probably maybe seven times, something like that.

03:11.520 --> 03:18.160
And my background is, so as you mentioned, I have more of a mathematical training in statistics,

03:18.160 --> 03:23.960
maybe a little bit of CS, to blend of everything because I come from a French chronicle, so

03:23.960 --> 03:28.080
those are universities that try to give you a pretty general training.

03:28.080 --> 03:32.120
And when I was a PhD student, I was interested in machine learning already, always doing

03:32.120 --> 03:33.120
bioinformatics.

03:33.120 --> 03:37.200
I mean, trying to do bioinformatics with some mathematical tools.

03:37.200 --> 03:39.840
And then I kept from there, and I did a bit of finance.

03:39.840 --> 03:45.160
I worked also in the financial industry for a short while and worked as a lecturer in

03:45.160 --> 03:47.160
the US for a while as well.

03:47.160 --> 03:48.160
You said an actor?

03:48.160 --> 03:49.160
I'm sorry.

03:49.160 --> 03:50.160
As a lecturer?

03:50.160 --> 03:51.160
Lecturer.

03:51.160 --> 03:52.160
I'm sorry.

03:52.160 --> 03:53.160
No.

03:53.160 --> 03:54.160
That is exotic.

03:54.160 --> 03:55.160
No, no.

03:55.160 --> 03:56.160
Definitely more.

03:56.160 --> 04:01.520
And I was in Princeton for a while, and then I worked in Japan as well.

04:01.520 --> 04:05.480
For a long time, I was a professor there for six years, and then just one year ago, actually,

04:05.480 --> 04:06.480
I came back to France.

04:06.480 --> 04:07.480
Okay.

04:07.480 --> 04:09.440
So I looked back to France.

04:09.440 --> 04:15.440
And yes, as you said, my training is more math, so I'm more interested in how can you

04:15.440 --> 04:22.440
say, well-behaved, well-posed problems, mathematical problems that we can think of, and try to

04:22.440 --> 04:28.520
reply them, and make them work, and actually the messy setting of the real data, et cetera.

04:28.520 --> 04:29.520
Okay.

04:29.520 --> 04:36.080
And so one of those mathematical tools that you're working on now is called Optimal Transport.

04:36.080 --> 04:41.680
And in fact, you're doing a, or you did, a tutorial on that here at Nibb's.

04:41.680 --> 04:43.000
What is Optimal Transport?

04:43.000 --> 04:44.000
Yeah.

04:44.000 --> 04:50.480
So it's been a bit my, I couldn't say my main subject of research for me, for about maybe

04:50.480 --> 04:58.560
three, four years now, before that I was, so maybe if you see Nibb's through years, there's

04:58.560 --> 05:03.960
waves of technologies and ideas and methods that come by, and so one of them was current

05:03.960 --> 05:06.040
methods, and that was when I was a PhD student.

05:06.040 --> 05:07.040
Okay.

05:07.040 --> 05:13.680
And then it started receiving when deep learning exploded, and then I, well, I had this,

05:13.680 --> 05:19.600
an interest for this theory for a while, the theory dates back to the 18th century.

05:19.600 --> 05:21.880
And it was developed by a French mathematician.

05:21.880 --> 05:27.800
It's very intuitive concepts, but for a while, this was not, there was no real progress.

05:27.800 --> 05:32.120
And then this was really discovered in the 20th century by a, by a Russian mathematician

05:32.120 --> 05:37.360
called Kantorovich, and he got a Nobel Prize, actually, 70s in economics, for, for work

05:37.360 --> 05:40.000
on this, in this theory.

05:40.000 --> 05:43.920
And then more recently, there's a also very famous French mathematician.

05:43.920 --> 05:49.280
You might know him, he's now, he was recently elected, like in the Congress, in the French

05:49.280 --> 05:53.320
Congress, and he also got the Fiat Medal, which, you know, is like the, the biggest

05:53.320 --> 05:57.040
in the prize you could get in mathematics is Nibb's, like, Vylanie.

05:57.040 --> 06:01.120
And so this is a really beautiful theory that's, that's, that the many mathematicians

06:01.120 --> 06:04.240
have been investigating for, for the last decades.

06:04.240 --> 06:08.600
And yeah, I think recently we made some progress on the numerical side, computational

06:08.600 --> 06:12.520
side, and this can become very relevant in machine learning.

06:12.520 --> 06:18.960
So across those three, four years, we, we've seen a wave of papers that have, I think,

06:18.960 --> 06:23.480
brought forward, proved a bit that this was useful, and now the tutorial was a way to,

06:23.480 --> 06:26.040
to showcase a bit, all, all that progress.

06:26.040 --> 06:27.040
Okay.

06:27.040 --> 06:33.040
So for someone who's not familiar with optimal transport, what is it, and what is it trying

06:33.040 --> 06:34.040
to achieve?

06:34.040 --> 06:37.040
Yeah, so let's just break it down and, you know, two words, right?

06:37.040 --> 06:38.040
Okay.

06:38.040 --> 06:39.040
It's optimal and transposed.

06:39.040 --> 06:41.440
So why do we need, like, transport?

06:41.440 --> 06:46.880
Transport is actually, it really dates back to very simple idea of, of transporting goods

06:46.880 --> 06:49.120
from where we produce them to where we need them.

06:49.120 --> 06:50.120
Okay.

06:50.120 --> 06:54.320
And typically, of course, if you only have, let's say, one mine and one factory, then there's

06:54.320 --> 06:56.160
no, not many questions you should ask.

06:56.160 --> 07:00.720
I mean, you just bring, whatever the mine produces, and you bring it to the factory, to,

07:00.720 --> 07:02.920
to transform it into some other goods.

07:02.920 --> 07:06.520
But now suppose that you have a map, and you have a lot of mines everywhere in the, on

07:06.520 --> 07:12.960
the map, and they are, they all produce this, you know, a raw resource in different quantities,

07:12.960 --> 07:16.600
and then you have factories all over the map as well, and they all need that resource

07:16.600 --> 07:19.880
to, to, to, for them to keep on producing final goods.

07:19.880 --> 07:24.520
And the question is, how you move, you know, those, those resources around in an optimal

07:24.520 --> 07:25.520
way, in a cheap way.

07:25.520 --> 07:29.280
So it's a, it's really related to a deep problem, you know, in operations research, or just

07:29.280 --> 07:33.320
industrial problem, which is, uh, optimally move things.

07:33.320 --> 07:39.600
And this was, the reason why this really boomed as a theory in World War II, is that this

07:39.600 --> 07:45.400
is probably when really, we had a lot of stuff to move, and really, we really wanted

07:45.400 --> 07:46.400
to be optimal.

07:46.400 --> 07:51.040
But not just a matter of profits, or trying to squeeze more money, or anything, this was

07:51.040 --> 07:52.040
just vital.

07:52.040 --> 07:56.960
You'd have to, there were some, you know, front lines, you would need 10,000 soldiers there,

07:56.960 --> 08:01.120
20,000 soldiers there, 20,000 soldiers there, etc., and then you had them in, in, another

08:01.120 --> 08:04.520
place, another location, and you really had to find out very quickly, where was the

08:04.520 --> 08:05.520
best way to do that.

08:05.520 --> 08:09.520
And so, unfortunately, I mean, during World War II, I guess, none of those techniques

08:09.520 --> 08:14.360
actually were used, but they, that situation triggered a lot of thinking, and that's why

08:14.360 --> 08:18.200
you control it, and actually other people here in the US, as well, Hitchcock, Kupmans,

08:18.200 --> 08:22.120
there's lots of names that are associated with this problem.

08:22.120 --> 08:28.000
And then, this was really a topic that was mostly investigated by people in operations

08:28.000 --> 08:29.000
we search.

08:29.000 --> 08:35.560
But then gradually, this idea of transporting things, in an optimal way, also found applications

08:35.560 --> 08:42.240
and, for instance, in physics, like you would try to guess how a distribution of gas would

08:42.240 --> 08:45.800
slowly evolve towards another distribution of gas.

08:45.800 --> 08:50.600
I mean, there's many, many different settings where you're basically trying to compare those

08:50.600 --> 08:56.320
two things, like the probability distribution of what, what you have, and the probability

08:56.320 --> 09:01.680
distribution of the, what you would like to get, and how to transform it optimally.

09:01.680 --> 09:05.800
And so, this is why mathematicians got interested in that.

09:05.800 --> 09:10.480
And from our point of view, basically, it boils down to the following thing.

09:10.480 --> 09:15.960
This idea of optimal transport provides a fresh way to look at what we call probability

09:15.960 --> 09:16.960
distributions.

09:16.960 --> 09:22.280
Probability distributions basically quantifying how things are likely to happen.

09:22.280 --> 09:26.920
And they also quantify, basically, where we observe data.

09:26.920 --> 09:29.320
What kind of images do we observe in the real world, right?

09:29.320 --> 09:31.160
We don't observe just any kind of image.

09:31.160 --> 09:33.480
We observe images with a certain structure.

09:33.480 --> 09:40.560
And so those observations form a probability distribution, and we try to come up with

09:40.560 --> 09:46.080
probability distributions of the world, which are, we, as modellers, is what we can do.

09:46.080 --> 09:51.560
And we try to compare them, and this is where the theory is actually useful.

09:51.560 --> 09:56.960
And so, I'll stick with this background question.

09:56.960 --> 10:01.600
Is the implication then in the context of World War II?

10:01.600 --> 10:06.440
Like they were probably, probabilistically modeling troop movements, as opposed to determining,

10:06.440 --> 10:08.440
I would imagine that to be more deterministic.

10:08.440 --> 10:11.440
So, I'm going to move these troops from A to B as opposed to-

10:11.440 --> 10:12.440
Yeah, very good point.

10:12.440 --> 10:20.400
So, here, let me just tell you how you could see, imagine probabilistic troop movements.

10:20.400 --> 10:27.600
Imagine you have front-line A and front-line B, and you need 10,000 soldiers at A and NB,

10:27.600 --> 10:28.600
okay?

10:28.600 --> 10:33.080
Because somewhat the enemy is coming, so we really need 10,000 guys there, and 10,000

10:33.080 --> 10:34.080
guys there.

10:34.080 --> 10:38.600
So, suppose you have two barracks, each with 10,000 soldiers.

10:38.600 --> 10:45.160
So, a very naive way would be to, let's say, send all the 10,000 soldiers of the back

10:45.160 --> 10:50.880
one to front-line A, 10,000 soldiers from back two to front-line B, okay?

10:50.880 --> 10:53.680
So there's no real probabilistic thing, right?

10:53.680 --> 10:57.320
It's just everyone goes there, and then everyone goes there.

10:57.320 --> 11:01.920
It sounds great, but imagine that, for some reason, you're a bit suspicious that maybe

11:01.920 --> 11:08.240
something might happen to a 10,000 soldiers that are sent through just one road.

11:08.240 --> 11:14.200
So one thing you might try to do is split those 10,000s into five and five, and the other

11:14.200 --> 11:19.000
barrack also is split into five and five, and so you send simultaneously 5,000 soldiers

11:19.000 --> 11:21.000
to each of the front-line B.

11:21.000 --> 11:22.000
You see what I mean?

11:22.000 --> 11:23.000
Yeah, so now you've got four routes.

11:23.000 --> 11:24.000
Exactly.

11:24.000 --> 11:28.720
Four people, four groups of people, five thousand people moving around.

11:28.720 --> 11:31.880
So it's not clear you gain a lot by doing that, right?

11:31.880 --> 11:36.960
Because somewhat maybe if barrack one was very close to front-line A, and barrack two

11:36.960 --> 11:41.760
was very close to front-line B, then you're actually asking people to cross basically over

11:41.760 --> 11:44.480
and have this much longer trip, right?

11:44.480 --> 11:49.280
But what you've gained is that you have something, maybe this is where maybe the probabilistic

11:49.280 --> 11:55.240
aspect can be seen like, each soldier basically from back one had chanced one over two to

11:55.240 --> 12:00.760
go to A or B, or on the other hand, if you want to just stick to the optimal plan, you

12:00.760 --> 12:03.640
will just send them directly to the closest to front-line, right?

12:03.640 --> 12:09.440
Is what you're trying to model probabilistically the chance that they'd actually get there,

12:09.440 --> 12:11.880
like the danger of their route or some kind of...

12:11.880 --> 12:16.480
Well, this is something I might have in mind to actually choose the one where they split

12:16.480 --> 12:22.000
into two instead of just going directly to the best possible, I mean, the shortest basically

12:22.000 --> 12:23.000
front-line point.

12:23.000 --> 12:31.400
But what I'm saying is that when you have to distribute resources in a network like this

12:31.400 --> 12:38.280
from a starting point to an ending point, well, sometimes you can find that the best possible

12:38.280 --> 12:45.360
way, the cheapest possible way is to do this a bit deterministically as you imply, there's

12:45.360 --> 12:49.480
no probability you just send everyone to the best possible route.

12:49.480 --> 12:54.600
But you could add some little fuzziness, so you know what, maybe in case there's a problem

12:54.600 --> 12:59.080
with that route, maybe this breaks down, this bridge breaks down, if this bridge breaks

12:59.080 --> 13:03.800
down, maybe no one will get front-line B and then I'm really in a mess, right?

13:03.800 --> 13:08.080
So to hedge up it yourself, you might want to choose something that's a bit more fuzzy,

13:08.080 --> 13:10.000
a bit more probabilistic.

13:10.000 --> 13:15.600
And on the other hand, if you really go towards the fuzziest possible way to do this, where

13:15.600 --> 13:19.680
you split everyone equally, then you see that there's a problem because you're paying

13:19.680 --> 13:26.480
very high cost usually, in terms of travel, because maybe front-line B is like 100 miles

13:26.480 --> 13:31.720
away, from like A's, maybe 10 miles away from the back one, and you're actually asking

13:31.720 --> 13:35.720
for the half of your troops to walk those hour and a half.

13:35.720 --> 13:41.760
So see, there's this, there's a trade-off somewhat, in terms of robustness, on the one hand,

13:41.760 --> 13:46.200
you would like to do what's cheap, on the other hand, I didn't need to be a fuzziness,

13:46.200 --> 13:49.560
makes it a bit more robust against any problem that might occur.

13:49.560 --> 13:55.080
So here, I'm using this, you know, vocabulary of maybe there's a risk or there's something

13:55.080 --> 14:00.120
that I beg down, etc, but in, from mathematical perspective, what this really boils down to

14:00.120 --> 14:07.120
is more of something, which is, you know, the stability of the solution is, you know,

14:07.120 --> 14:12.640
having a little bit of fuzzier things makes it a bit, makes it a bit more stable, having

14:12.640 --> 14:18.120
something that's really routing everyone to a, just one in one location makes things a

14:18.120 --> 14:19.120
bit unstable.

14:19.120 --> 14:20.120
And so why-

14:20.120 --> 14:23.920
Make you think of, if you're familiar with Nassim Taleb's anti-fragility?

14:23.920 --> 14:29.640
Yeah, exactly, you could think of this, so we're going a bit away from basically my exact

14:29.640 --> 14:32.720
technical contribution, but actually the idea is exactly that.

14:32.720 --> 14:39.640
And my, all my, part of my talk, so first I explain this problem of moving around optimally

14:39.640 --> 14:40.640
things.

14:40.640 --> 14:44.640
This moving around optimally things helps you figure out whether two things are close,

14:44.640 --> 14:46.040
whether two distributions are close.

14:46.040 --> 14:50.720
If there's a way, if there's a cheap way to move all the troops to all the frontlines,

14:50.720 --> 14:54.000
then it means that the two distributions are very close.

14:54.000 --> 14:59.680
But if you write down the equations, and this is why Kantorovich called the Nobel Prize,

14:59.680 --> 15:02.920
this yields something called a linear program.

15:02.920 --> 15:08.280
It's one major, very important, family of optimization problems.

15:08.280 --> 15:13.680
And the way this was formalized 50 years ago was just linear programming, so let's go

15:13.680 --> 15:17.080
for the cheapest possible distribution.

15:17.080 --> 15:21.400
Now what I was arguing a bit during the tutorial is that you might trade off a little bit of

15:21.400 --> 15:26.520
that optimality in terms of the cost, and accept a little bit of fuzziness.

15:26.520 --> 15:31.600
And this makes things much better behave computationally when you run all those algorithms.

15:31.600 --> 15:38.280
So yeah, actually this is a meaning, meaning that introducing the randomness not only has

15:38.280 --> 15:43.600
these properties, these beneficial properties, and kind of the physical world we're talking

15:43.600 --> 15:50.680
about troop movements, but also introducing probability and randomness allows you to give

15:50.680 --> 15:54.040
you a more computationally efficient solution in solving the linear program?

15:54.040 --> 16:00.840
Yes, like one million times faster, like in some relevant dimensionalities, let's

16:00.840 --> 16:06.400
say you have, let's say 10,000 barracks, 10,000 frontlines, and you want to compute this

16:06.400 --> 16:11.600
optimal transport, where it turns out that if you use, if you're willing to take a little

16:11.600 --> 16:16.760
bit of fuzziness, not that many, but just a little bit, then it can actually, you can

16:16.760 --> 16:23.760
compute this transport, and maybe let's say, let's say one second, okay, for the sake

16:23.760 --> 16:28.160
of the argument, but if you were to solve it with a linear program, that would be like

16:28.160 --> 16:32.840
one million times slower, that would really be much longer.

16:32.840 --> 16:37.920
And the other thing that's also very relevant I think in this NIPPS conference is that computing

16:37.920 --> 16:43.760
transport with that little fuzziness, not so much, but just a little bit, allows you to

16:43.760 --> 16:50.360
compute everything with GPUs, so everything becomes a parallel, very embarrassingly parallel

16:50.360 --> 16:55.640
algorithm, and this is one of the also big sources of speed up, whereas doing it the linear

16:55.640 --> 17:00.720
programming way, the way that was proposed in the 50s or 60s by such great names as

17:00.720 --> 17:06.160
George Tansik, people that invented all those algorithms in 50s or 60s, those algorithms

17:06.160 --> 17:11.600
don't parallelize well with GPUs, and this is the main reason why we're trying to get

17:11.600 --> 17:17.080
a bit away from them, and a bit more towards those algorithms, but because they include

17:17.080 --> 17:22.280
some fuzziness, a little bit of fuzziness, for some reason, you go down in the math, you

17:22.280 --> 17:28.680
observe that you can cast them as matrix products, and so everything works well on GPUs.

17:28.680 --> 17:34.200
So if I want to push a bit the analogy, this field of optimal transport has been around

17:34.200 --> 17:41.760
for many years, I propose this little, to add this little fuzziness, and everything all

17:41.760 --> 17:47.520
of a sudden can be executed on GPUs, so it looks a bit like someone that comes at a machine

17:47.520 --> 17:52.280
learning conference maybe 10 years ago, and says, oh, I have deep learning algorithms,

17:52.280 --> 17:56.240
not only are they great, actually they run on GPUs, and because they run on GPUs they

17:56.240 --> 18:01.360
are going to get much better performance, because we know this was one of the main drivers

18:01.360 --> 18:09.520
of innovation for deep learning was the fact that they could really well exploit GPUs.

18:09.520 --> 18:18.600
So then are you also applying optimal transport to machine learning problems, or are you,

18:18.600 --> 18:23.600
you know, is this a method for doing better work with traditional types of problems that

18:23.600 --> 18:26.040
we think of as optimal transport problems?

18:26.040 --> 18:31.240
So yeah, it's a very good question, so my point of view right now is that,

18:31.240 --> 18:34.640
so maybe I didn't stress this enough, but what really optimal transport allows you to

18:34.640 --> 18:40.320
do is to compute a distance between probability distributions, and by this I mean, let me

18:40.320 --> 18:45.560
just give you a very concrete example, and this was actually presented at NIP last year

18:45.560 --> 18:51.920
by another team, so you have, suppose you want to compare two text documents, so very

18:51.920 --> 18:58.360
standard approach is to see those documents as bags of words, right, just a long list

18:58.360 --> 19:03.640
of words and another long list of words, right, so I think it's a very relevant question

19:03.640 --> 19:09.240
to say how can we compute the distance between those two bags of words, right, so there

19:09.240 --> 19:15.040
are a few approaches that exist, and one of them is using optimal transport, so basically

19:15.040 --> 19:20.720
what this optimal transport approach requires is that you are able to define a distance

19:20.720 --> 19:27.840
between pairs of words, so basically if you can define a distance between pairs of words,

19:27.840 --> 19:31.720
optimal transport allows you to define a distance between histograms of words, it's

19:31.720 --> 19:39.040
like a meta distance, so you have a distance on just, I don't know, the word cat is that

19:39.040 --> 19:44.880
meters away or miles away from the word, let's say dog, and then very close to TT, for

19:44.880 --> 19:49.080
instance, if you can actually quantify that distance for every pair of words, I will be

19:49.080 --> 19:52.680
able to come up with a distance that can quantify the distance between histograms of words,

19:52.680 --> 19:57.160
cloud, point clouds, you know, those point clouds that we see very often on, yeah, word

19:57.160 --> 20:02.840
clouds, so optimal transport allows you to compute that, that very accurate distance between

20:02.840 --> 20:11.840
word clouds, and now what I'm trying to use is, suppose that my goal is not only to compute

20:11.840 --> 20:16.640
the distance between two things, but also to use that distance as what is called a loss

20:16.640 --> 20:23.840
function, something that allows you to quantify whether your prediction kind of matches what

20:23.840 --> 20:28.280
you should observe, so imagine that, imagine, so this is another example I was taking

20:28.280 --> 20:35.240
for a nips paper two years ago, imagine that your task is to predict from an image cloud

20:35.240 --> 20:43.000
of tags, okay, you just see an image, there's a tree, sun, bridge, etcetera, imagine now

20:43.000 --> 20:48.200
that you have a predictor that takes that image and says that, on that image actually

20:48.200 --> 20:54.080
it's a forest, there's sunlight, and there is a river, so as you can see I've managed

20:54.080 --> 21:01.080
to find a few tags that are somewhat overlapping the original tags that are the ground truth,

21:01.080 --> 21:05.480
but it would be really useful if I could accurately quantify how different those two histograms

21:05.480 --> 21:11.760
of words are, and that's where optimal transport can be used, and so what I've shown in the

21:11.760 --> 21:16.320
tutorial yesterday is that you can not only use optimal transport as a distance just

21:16.320 --> 21:22.680
to put a geometry on the space of histograms, but you can use that as a loss in the learning

21:22.680 --> 21:27.520
algorithm, so whenever, whenever I will, so very often for instance an algorithm just

21:27.520 --> 21:34.200
produces one guess, one guess, well just think about a meta algorithm that would not produce

21:34.200 --> 21:39.480
one guess, but the histogram of guesses, the probability distribution, some confidence

21:39.480 --> 21:44.320
on basically several possible guesses, well then it's useful to use optimal transport

21:44.320 --> 21:49.440
theory to compare those histograms of guesses, instead of maybe you've heard about this,

21:49.440 --> 21:55.840
a coolback library divergence, or other, those are, that there is a very important block

21:55.840 --> 22:00.720
in many machine learning algorithms, including deep learning algorithms, which is this cross-centropy

22:00.720 --> 22:06.080
loss, or coolback library loss, well this is another loss that has been around for centuries

22:06.080 --> 22:12.920
to compare to probability distributions, basically my main agenda is to maybe remove that loss

22:12.920 --> 22:20.440
and replace it by this optimal transport loss, okay, and is the primary, what are the advantages

22:20.440 --> 22:27.400
in doing, so you mentioned the computational advantage with our ability to do these in parallel,

22:27.400 --> 22:32.040
is that something that you can't do with cross-entropy loss, actually it's the other

22:32.040 --> 22:37.840
way around, cross-entropy is very popular because it's very, very simple, it's very, very

22:37.840 --> 22:43.760
easy to compute, but it uses very little information, the sense that, imagine that you have two

22:43.760 --> 22:49.400
histograms of words, and I compute the cross-entropy between them, all the coolback library

22:49.400 --> 22:57.600
divergence, what that won't be able to detect is whether words are synonyms or not, in

22:57.600 --> 23:02.240
the sense that if, for instance, one of my histograms, the one that I want to predict

23:02.240 --> 23:08.200
as a word cut, and for some reason my machine predicted the word kitty, in the cross-entropy

23:08.200 --> 23:13.840
world, those will be two completely different coordinates, and there's no, there will,

23:13.840 --> 23:20.360
three relative, there's no, basically the cross-entropy formula will just say, okay cut appeared

23:20.360 --> 23:27.400
here in the, in the ground truth, my guess was kitty, so I have put a zero on cut, then

23:27.400 --> 23:30.800
you pay a price, and then on the other hand, the ground truth didn't have the word kitty,

23:30.800 --> 23:34.920
but I predicted it, so you also pay the price, you pay the price twice if you want, whereas

23:34.920 --> 23:40.200
this optimal transport loss allows you to say, oh, come on, actually, the ground truth

23:40.200 --> 23:45.400
is cut, and my prediction is kitty, but I know that cutting kitty are very close, I mean,

23:45.400 --> 23:49.480
they're almost the same, they're almost synonyms, so I'm paying the very small price, because

23:49.480 --> 23:55.840
I'm able to somewhat transport the word cut onto the word kitty, and that's the main

23:55.840 --> 23:59.360
idea where this gives a loss, which is more flexible if you want.

23:59.360 --> 24:05.120
Now, it sounds like, in this case, it's calling them on, like, word embeddings, and word

24:05.120 --> 24:08.320
spaces, and so do you need to do all that in order to use it?

24:08.320 --> 24:10.920
Exactly, so the thing is, is that an alternative, or are they complementary?

24:10.920 --> 24:17.480
No, so exactly, so basically, so in those papers, those people are called the word movers

24:17.480 --> 24:21.920
distance, because the idea of a tumultranspactory has many names, it's called Vassarstein distance,

24:21.920 --> 24:26.600
earth movers distance, and so it was recently discovered by this word, word movers distance,

24:26.600 --> 24:31.560
where you're actually computing the distance between two point clouds of word embeddings.

24:31.560 --> 24:32.560
Okay.

24:32.560 --> 24:36.560
So you can imagine that one text is just a bunch of points in dimension 100, those are the

24:36.560 --> 24:41.840
embedding, these embedding dimensions of the words, and another point cloud is another

24:41.840 --> 24:46.920
text in dimension 100, and then you're trying to transport them optimally, and what you

24:46.920 --> 24:51.520
need to do to get there is a notion of distance between every pair of words, right?

24:51.520 --> 24:53.760
So that's what the word embeddings give us.

24:53.760 --> 24:58.200
So what embeddings are just the fuel for this, I mean, just the main ingredient that allows

24:58.200 --> 25:00.560
us to put a bit of tumultransport.

25:00.560 --> 25:01.800
Okay, interesting.

25:01.800 --> 25:06.400
So there's an application in NLP, what are some of our machine learning applications?

25:06.400 --> 25:11.520
So we have, so let me just list you what the application where things really worked very,

25:11.520 --> 25:12.520
very well.

25:12.520 --> 25:13.520
Okay.

25:13.520 --> 25:15.480
So the ones where it works really nicely is graphics.

25:15.480 --> 25:16.480
Okay.

25:16.480 --> 25:20.040
So it's a bit away from machine learning, because graphics people are mostly concerned with

25:20.040 --> 25:22.000
low dimensional shapes.

25:22.000 --> 25:29.840
So we've had a few successes, you know, doing interpolations of shapes, and I mean, we have

25:29.840 --> 25:34.160
a few C-graph papers, and what are the specific examples?

25:34.160 --> 25:38.160
So when there's the C-V, well, imagine that you have a probability distribution, which

25:38.160 --> 25:42.720
is a shape, and another probability distribution that's a shape, what optimal transport allows

25:42.720 --> 25:48.480
you to do is to do interpolations between them, like more things, without very, very little

25:48.480 --> 25:51.000
assumptions on the data.

25:51.000 --> 25:53.080
So what we've seen is that this can be used.

25:53.080 --> 25:57.440
So what can you give me a more kind of concrete example of where you'd have these probabilistic

25:57.440 --> 25:58.440
shapes?

25:58.440 --> 25:59.440
Yeah.

25:59.440 --> 26:04.640
So it's one of those cases where a picture actually is worth a thousand words, but we have

26:04.640 --> 26:10.920
those pictures in the C-graph papers where you would see, for instance, a duck, like

26:10.920 --> 26:17.160
a duck, like a kid having a bath, that would gradually morph into some cow or any other

26:17.160 --> 26:18.160
shape.

26:18.160 --> 26:23.880
So you can actually compute interpolations between tens of shapes, one another, the application

26:23.880 --> 26:25.400
of the brain imaging.

26:25.400 --> 26:31.000
We can try to morph one brain into another without making any assumption on actually the

26:31.000 --> 26:33.800
parametric assumption of the shape of the brain, for instance.

26:33.800 --> 26:37.400
And so optimal transport is very natural for that.

26:37.400 --> 26:38.400
Right, right.

26:38.400 --> 26:41.680
So you've got your beginning distribution, your end distribution, and now this allows

26:41.680 --> 26:46.880
you to compute intermediate states at any point in time or point in, which makes sense

26:46.880 --> 26:47.880
geometrically.

26:47.880 --> 26:51.480
So it's very easy to compute intermediate probability distribution.

26:51.480 --> 26:56.920
You just add one to the other, divide by two, then you will probably get those weird

26:56.920 --> 26:57.920
effects.

26:57.920 --> 26:58.920
Yeah.

26:58.920 --> 27:01.800
So optimal transport actually allows you to morph continuously from one to the other.

27:01.800 --> 27:02.800
Okay.

27:02.800 --> 27:04.640
And this was discovered in the late 90s.

27:04.640 --> 27:09.760
For many years, we had no idea how to compute it, because it was actually very, very heavy.

27:09.760 --> 27:13.560
And those ideas that I told you about about adding a little bit of fuzziness, exploiting

27:13.560 --> 27:16.560
GPUs, basically make it possible now.

27:16.560 --> 27:21.800
And so you can average not only two, but maybe let's say 10, 20 brains and things like

27:21.800 --> 27:22.800
that.

27:22.800 --> 27:24.280
So that can help you from templates.

27:24.280 --> 27:30.520
So this is what the few, those are the examples where there are actually visually a bit

27:30.520 --> 27:31.520
more striking.

27:31.520 --> 27:32.520
Okay.

27:32.520 --> 27:36.280
So dimension, we can, we as humans can get a few of what's going on.

27:36.280 --> 27:39.680
Then the NLP applications, I think, are pretty exciting.

27:39.680 --> 27:44.080
And then there has been the water application, the NLP, the NLP, the NLP, the dimension.

27:44.080 --> 27:48.520
And then we had a, there's a lot of imaging also applications with a distribution of color

27:48.520 --> 27:54.160
experiences, like image palettes, more thing one image with a given palette, changing the

27:54.160 --> 28:00.360
palette of an image, for instance, you want to, yeah, the palette is, has colors reminiscent

28:00.360 --> 28:01.360
of autumn.

28:01.360 --> 28:03.320
And you want them to be all of a certain kind of blueish, etc.

28:03.320 --> 28:06.000
So there's lots of applications like this.

28:06.000 --> 28:10.080
But what we've seen in machine learning recently, and which is pretty exciting, is more

28:10.080 --> 28:17.480
applications to a generative models, those models that are able to create images.

28:17.480 --> 28:23.960
And so there's been a few papers in the last year, actually, in just one year from now.

28:23.960 --> 28:25.920
And one of them is called the Vassarstein Gan.

28:25.920 --> 28:27.680
It has attracted a lot of attention.

28:27.680 --> 28:30.280
We also tried to write a few papers in that line.

28:30.280 --> 28:34.280
So here in that case, the, the story is as follows.

28:34.280 --> 28:38.880
You can see the distribution of natural images in the world, let's say, as a probability

28:38.880 --> 28:42.720
distribution of the space of all possible images, right?

28:42.720 --> 28:48.040
We think of an image as a three channels, and a hundred times a hundred pixels, it's

28:48.040 --> 28:52.000
basically a vector of dimension, 30,000.

28:52.000 --> 28:56.120
But there's very few actually in that space of dimension, 30,000.

28:56.120 --> 28:59.240
There's not that many images that are natural in there, right?

28:59.240 --> 29:04.120
You were to basically write a map of every image that can be natural.

29:04.120 --> 29:05.120
Right.

29:05.120 --> 29:06.440
It would be very thin there, right?

29:06.440 --> 29:10.240
So the problem of generative model is to create a probabilistic model, something that can

29:10.240 --> 29:16.240
generate images that fall not too far from that manifold of natural images.

29:16.240 --> 29:19.360
And this is a statistically very, very messy problem.

29:19.360 --> 29:24.840
And so what you want to do is imagine that I have a model that generates images.

29:24.840 --> 29:29.080
It will have its own manifold in that space of dimension, 30,000.

29:29.080 --> 29:33.720
And then I collect data that's another manifold in dimension, 30,000.

29:33.720 --> 29:40.200
And so the point is I want my manifold to look like the manifold of natural images, right?

29:40.200 --> 29:44.200
And I need to be able to say how far I am, right?

29:44.200 --> 29:46.720
That's the whole point of doing gradient descent.

29:46.720 --> 29:51.600
It's quantifying how far my manifold is from the real manifold, and trying to make that

29:51.600 --> 29:53.960
distance smaller and smaller.

29:53.960 --> 30:02.400
And what people found out just about one year ago, is that many usual distances like

30:02.400 --> 30:08.080
cross-entropy, you know, kubak library, et cetera, were not well suited for that problem.

30:08.080 --> 30:11.800
Because we are really talking about very degenerate manifolds.

30:11.800 --> 30:17.960
And what we use kubak library for is typically to compare gaussians, for instance, so to compare

30:17.960 --> 30:19.880
things that are very smooth.

30:19.880 --> 30:23.640
And so when you have this problem of trying to quantify the distance between one manifold

30:23.640 --> 30:27.040
and another, optimal transport can be useful.

30:27.040 --> 30:29.840
And so there's been a lot of interest for that.

30:29.840 --> 30:36.600
There's Vassarstein Gaan has burned a lot of papers, and there's many, many tricks that

30:36.600 --> 30:41.600
people are trying to use to make this work, because it's computationally challenging.

30:41.600 --> 30:44.400
But then again, in the world of Gaan's, it's very difficult to measure performance.

30:44.400 --> 30:45.400
Right.

30:45.400 --> 30:49.320
Right now, everybody's a bit frustrated, but I think there's still a bit of progress there.

30:49.320 --> 30:52.400
I mean, still progress that's being made now.

30:52.400 --> 30:57.080
I haven't picked up on the frustration around Gaan, as much as the enthusiasm.

30:57.080 --> 31:02.280
Where did you talk to frustration specifically around the ability to measure performance?

31:02.280 --> 31:03.280
Yeah, exactly.

31:03.280 --> 31:08.200
I mean, if you look at the papers, most of them end up being a collection of the images

31:08.200 --> 31:17.040
that are being generated, and that's not a very scientifically correct way of, yeah.

31:17.040 --> 31:21.000
You want some more objective measure of performance, but that in itself is already a big

31:21.000 --> 31:22.520
scientific problem, right?

31:22.520 --> 31:24.880
Trying to find a good measure of performance there.

31:24.880 --> 31:25.880
Okay.

31:25.880 --> 31:26.880
Interesting.

31:26.880 --> 31:28.920
I really enjoyed learning about optimal transport.

31:28.920 --> 31:30.920
Any final words or...

31:30.920 --> 31:31.920
Oh, yes.

31:31.920 --> 31:36.480
So we've written a small, I'm just doing it to put some little advertisement for a survey

31:36.480 --> 31:38.160
that we've written recently.

31:38.160 --> 31:46.640
So if you go to Optimal Transport and just one word, Optimal Transport.github.io, you

31:46.640 --> 31:50.760
will find the slides of the tutorial and you will find also a survey, a 200 pages survey

31:50.760 --> 31:53.800
that I've written with my colleague Gaby Alperin.

31:53.800 --> 31:54.800
Okay.

31:54.800 --> 31:58.080
Interesting. And is there...

31:58.080 --> 31:59.080
In terms of...

31:59.080 --> 32:01.080
You mentioned Github and I'm thinking implementation.

32:01.080 --> 32:02.080
Yes, it is.

32:02.080 --> 32:03.080
It could be available.

32:03.080 --> 32:04.080
Yes, of course.

32:04.080 --> 32:05.080
Okay.

32:05.080 --> 32:11.080
The good thing is that, as I was telling you earlier, all of this eventually is actually

32:11.080 --> 32:12.080
very simple to code.

32:12.080 --> 32:17.680
I mean, I wish I could tell you we have really a great toolbox that, but actually, if

32:17.680 --> 32:20.160
you really look a bit at the papers, it's just...

32:20.160 --> 32:21.960
The code is just a few lines.

32:21.960 --> 32:22.960
It's just...

32:22.960 --> 32:23.960
Okay.

32:23.960 --> 32:24.960
Put that loss...

32:24.960 --> 32:29.400
This magic Optimal Transport loss in your learning problem just requires you writing

32:29.400 --> 32:32.480
maybe a wild loop and three lines in the wild loop.

32:32.480 --> 32:33.480
Okay.

32:33.480 --> 32:34.480
So it's more...

32:34.480 --> 32:37.960
What I mean, this right up your alley, it's more about the math that gave you those three

32:37.960 --> 32:38.960
lines and the three lines themselves.

32:38.960 --> 32:39.960
Exactly.

32:39.960 --> 32:40.960
There's not much in there involved, yes.

32:40.960 --> 32:41.960
Okay.

32:41.960 --> 32:42.960
Awesome.

32:42.960 --> 32:45.200
Well, Marco, thanks so much for taking the time to chat with me.

32:45.200 --> 32:46.200
Thank you so much.

32:46.200 --> 32:47.200
It was really a pleasure.

32:47.200 --> 32:53.360
All right, everyone, that's our show for today.

32:53.360 --> 32:58.400
For more information on Marco or any of the topics covered in this episode, head on over

32:58.400 --> 33:02.840
to twimmaleye.com slash talks slash 131.

33:02.840 --> 33:20.000
And of course, thanks so much for listening and catch you next time.

