1
00:00:00,000 --> 00:00:03,920
Hello everyone and welcome to the podcast.

2
00:00:03,920 --> 00:00:09,040
If you're looking for this week in machine learning and AI, you are in the right place.

3
00:00:09,040 --> 00:00:12,760
But if you're a regular listener, you may be wondering what happened to the teaser and

4
00:00:12,760 --> 00:00:14,640
the awesome intro music.

5
00:00:14,640 --> 00:00:19,120
Well, this is going to be a different kind of show, so I've skipped the usual intro to

6
00:00:19,120 --> 00:00:21,640
avoid confusion.

7
00:00:21,640 --> 00:00:25,200
This week we're going to take a break from the news format, and I've got a really interesting

8
00:00:25,200 --> 00:00:28,120
interview to share with you in its place.

9
00:00:28,120 --> 00:00:32,760
As you may recall, I spent Thursday at the Rangle conference in San Francisco, which

10
00:00:32,760 --> 00:00:37,120
was organized by Claude Ara, who was kind enough to sponsor the past couple of episodes

11
00:00:37,120 --> 00:00:38,720
of the podcast.

12
00:00:38,720 --> 00:00:40,360
I'm really glad I went to that event.

13
00:00:40,360 --> 00:00:44,880
The program was super solid and I met a bunch of great people.

14
00:00:44,880 --> 00:00:49,480
One of those people was Claire Cortell, whose work was discussed in one of the very first

15
00:00:49,480 --> 00:00:55,080
episodes of the podcast, and she was kind enough to agree to be interviewed for the show.

16
00:00:55,080 --> 00:00:59,040
We had a really fun discussion and touched on a bunch of interesting topics, including

17
00:00:59,040 --> 00:01:04,320
her background and what she's been up to, the open source data science master's project

18
00:01:04,320 --> 00:01:11,120
that she created, getting beyond the beginner's plateau in machine learning and data science,

19
00:01:11,120 --> 00:01:15,240
hybrid AI, which is of course the topic of the article of hers that we talked about

20
00:01:15,240 --> 00:01:20,920
on the podcast, and a recurring topic both here on this week in machine learning and

21
00:01:20,920 --> 00:01:26,880
AI, but also at the Rangle conference, and that is algorithmic ethics.

22
00:01:26,880 --> 00:01:31,040
Before we jump into the interview, a few quick logistic notes.

23
00:01:31,040 --> 00:01:32,880
First, what about the news?

24
00:01:32,880 --> 00:01:37,160
Well, if you've already signed up for the email newsletter that I've been talking about

25
00:01:37,160 --> 00:01:41,520
on the past few podcast episodes, you'll be receiving a summary of the week's news

26
00:01:41,520 --> 00:01:44,800
right in your inbox on Monday morning.

27
00:01:44,800 --> 00:01:49,960
If not, it's not too late to sign up at twomlai.com slash newsletter.

28
00:01:49,960 --> 00:01:54,960
Second, if you're excited about machine learning and AI and you've got research or writing

29
00:01:54,960 --> 00:02:02,280
skills, I'm looking for correspondence to contribute to the podcast and or the twomlai.com website.

30
00:02:02,280 --> 00:02:07,080
Shoot me a note at sam at twomlai.com if you're interested.

31
00:02:07,080 --> 00:02:11,720
Finally, I had a blast doing this interview and I want to know what you think about it and

32
00:02:11,720 --> 00:02:13,840
the interview format in general.

33
00:02:13,840 --> 00:02:22,120
As always, you can reach out to me at at Sam Charrington on Twitter, S-A-M-C-H-A-R-R-I-N-G-T-O-N

34
00:02:22,120 --> 00:02:24,920
with your comments, questions, or suggestions.

35
00:02:24,920 --> 00:02:30,880
All right, let's get to it onto the interview.

36
00:02:30,880 --> 00:02:41,360
All right, so I'm here with Claire Corthel at the Rangle conference in San Francisco.

37
00:02:41,360 --> 00:02:43,960
Hey, Claire, right to finally meet you in person.

38
00:02:43,960 --> 00:02:45,960
Hi, great to meet you in person, too, Sam.

39
00:02:45,960 --> 00:02:53,560
Yeah, so what's particularly exciting about getting to talk to you is I talked about

40
00:02:53,560 --> 00:03:00,400
your post a few, I guess it was one like the second podcast I did.

41
00:03:00,400 --> 00:03:05,720
You wrote that post around the same time on the hybrid AI and I thought that that was

42
00:03:05,720 --> 00:03:08,960
a really interesting post and it was one of the things that I talked about on the podcast.

43
00:03:08,960 --> 00:03:14,360
So I'd be looking forward to catching up with you on that as well as kind of getting an

44
00:03:14,360 --> 00:03:18,200
update on what you're up to and what you've been digging into.

45
00:03:18,200 --> 00:03:23,360
So maybe we can talk a little bit about your background and kind of how you got into

46
00:03:23,360 --> 00:03:25,240
data science and machine learning.

47
00:03:25,240 --> 00:03:26,640
Yeah, not a problem.

48
00:03:26,640 --> 00:03:32,840
So I'm a little bit weird because I'm not a theoretical physicist or some of us in

49
00:03:32,840 --> 00:03:37,360
data science are applied physicists, too, but I'm not in that camp.

50
00:03:37,360 --> 00:03:42,880
I actually started with product design and came into it from that perspective.

51
00:03:42,880 --> 00:03:50,440
So I was actually working on a very small product, kind of a startup within a startup when

52
00:03:50,440 --> 00:03:54,400
I decided that I wanted to understand more about our users and what they were doing.

53
00:03:54,400 --> 00:03:59,280
So I went to the parent company and I said, hey, I don't know much about this, but I think

54
00:03:59,280 --> 00:04:04,160
I should look into our user logs and try and understand more about how people are accessing

55
00:04:04,160 --> 00:04:09,080
the product and what might be happening and do some basic analytics and the head of

56
00:04:09,080 --> 00:04:12,880
engineering kind of looked at me and goes, hey, what logs?

57
00:04:12,880 --> 00:04:16,760
And I thought, well, this is problematic.

58
00:04:16,760 --> 00:04:20,400
How am I going to learn about my users if I don't have that?

59
00:04:20,400 --> 00:04:26,080
And it kind of started me on this long rabbit hole, which is now turned into my career.

60
00:04:26,080 --> 00:04:29,400
And I actually very much overshot that I did not end up in analytics.

61
00:04:29,400 --> 00:04:36,560
I work on machine learning applications and that problem space now, but I came at it from

62
00:04:36,560 --> 00:04:37,560
that perspective.

63
00:04:37,560 --> 00:04:44,600
And at that point, I was really looking for a new direction and decided to invest the

64
00:04:44,600 --> 00:04:51,240
next seven months in learning everything I could to prepare myself for a career and data

65
00:04:51,240 --> 00:04:52,520
side at data science.

66
00:04:52,520 --> 00:04:58,000
So I built a curriculum because at the time, I think insight had just started.

67
00:04:58,000 --> 00:05:04,680
So this was early 2013 and there weren't any academies that focused on this.

68
00:05:04,680 --> 00:05:10,720
So I built a curriculum around that and published it on GitHub and that's become a popular

69
00:05:10,720 --> 00:05:15,040
resource for people who want to get into data science and understand what it's all about

70
00:05:15,040 --> 00:05:17,520
because there wasn't really a road map at that point.

71
00:05:17,520 --> 00:05:21,800
But after the open data science masters, is that what that's all about?

72
00:05:21,800 --> 00:05:28,160
Open source data science masters, yes, the very descriptive, unimaginative name that

73
00:05:28,160 --> 00:05:29,160
I gave it.

74
00:05:29,160 --> 00:05:31,880
So it's exactly what it is though.

75
00:05:31,880 --> 00:05:38,760
So that was a really challenging project to work on.

76
00:05:38,760 --> 00:05:43,120
And I'm really happy that I was able to put that up in the open source world and to give

77
00:05:43,120 --> 00:05:44,120
you a preview.

78
00:05:44,120 --> 00:05:45,760
I'm working on a second version.

79
00:05:45,760 --> 00:05:49,720
There were breaking changes that floated up from Coursera who took a bunch of their

80
00:05:49,720 --> 00:05:50,720
courses offline.

81
00:05:50,720 --> 00:06:00,000
And so, yeah, I'm working on bubbling up those dependency problems and fixing them.

82
00:06:00,000 --> 00:06:02,360
So after that, I went to a company called Madermark.

83
00:06:02,360 --> 00:06:05,040
They're actually around the corner from where we are now.

84
00:06:05,040 --> 00:06:08,120
And they try to measure private company growth.

85
00:06:08,120 --> 00:06:14,000
So very similar to what Bloomberg does for public market companies and at the time when

86
00:06:14,000 --> 00:06:18,440
I joined that company, the CTO was ready to divest in machine learning.

87
00:06:18,440 --> 00:06:22,520
He was convinced it was not going to solve the problems that they were facing.

88
00:06:22,520 --> 00:06:30,040
It wasn't going to pose a reasonable set of solutions for their near term and midterm

89
00:06:30,040 --> 00:06:31,240
goals as a company.

90
00:06:31,240 --> 00:06:39,520
And I, on my first day, designed the key components of how we would move that strategy forward.

91
00:06:39,520 --> 00:06:45,800
That company now has a, I think it's a 10 person team working on data analysis and machine

92
00:06:45,800 --> 00:06:53,280
learning and they're going strong and I, I moved on from that to consulting about a year

93
00:06:53,280 --> 00:06:58,280
and a half ago and have been working with companies of various stages and sizes on getting

94
00:06:58,280 --> 00:07:02,320
started with data science or getting started with new functions within data science that

95
00:07:02,320 --> 00:07:03,680
they want to spin up on.

96
00:07:03,680 --> 00:07:09,320
So helping them understand how to get from A to B and what it's going to cost them for

97
00:07:09,320 --> 00:07:11,880
a solution space that they're investigating.

98
00:07:11,880 --> 00:07:17,960
There's a lot there to dig into on the open source data science masters.

99
00:07:17,960 --> 00:07:22,520
Now, was that a little bit of kind of building the, building the parachute as you're jumping

100
00:07:22,520 --> 00:07:25,960
out of the plane or building the airplane as you're taking off that kind of thing where

101
00:07:25,960 --> 00:07:27,480
you were learning as you went to want?

102
00:07:27,480 --> 00:07:28,480
Right.

103
00:07:28,480 --> 00:07:31,720
Or collecting laundry as I was falling out of the sky, I guess.

104
00:07:31,720 --> 00:07:32,720
Something like that.

105
00:07:32,720 --> 00:07:33,720
There's it.

106
00:07:33,720 --> 00:07:34,720
There's an appropriate analogy.

107
00:07:34,720 --> 00:07:35,720
Right.

108
00:07:35,720 --> 00:07:36,720
I like that.

109
00:07:36,720 --> 00:07:37,720
Absolutely.

110
00:07:37,720 --> 00:07:42,800
It was perhaps most challenging because I had to rewrite it as it was going.

111
00:07:42,800 --> 00:07:48,800
So I would continually check in with people I knew in industry and try and navigate to figure

112
00:07:48,800 --> 00:07:54,600
out what, what skills were actually applicable, what kind of depth I needed to go in on particular

113
00:07:54,600 --> 00:07:55,600
topics.

114
00:07:55,600 --> 00:08:00,320
What was actually key to understand and to the state, this is something that I, I hear a

115
00:08:00,320 --> 00:08:05,720
lot about from people who are hiring managers that when they try to hire people who are very

116
00:08:05,720 --> 00:08:12,440
fresh to the fields, sometimes they don't have the wealth of intuition distributed into

117
00:08:12,440 --> 00:08:13,520
the right places.

118
00:08:13,520 --> 00:08:17,800
So they may know how to build a model, but they don't know how to validate it and they

119
00:08:17,800 --> 00:08:26,400
don't know perhaps how to test data or work with data sets that are very messy.

120
00:08:26,400 --> 00:08:35,920
There are various kind of drawbacks to having a self-guided education and having to retarget

121
00:08:35,920 --> 00:08:38,800
that as you go is certainly challenging.

122
00:08:38,800 --> 00:08:39,800
So.

123
00:08:39,800 --> 00:08:44,480
And were you, did you learn yourself through a self-guided kind of approach?

124
00:08:44,480 --> 00:08:48,640
Did you collect all this by, you know, in the process of learning it or what was, I

125
00:08:48,640 --> 00:08:52,320
guess, what was the background that you brought to getting into data science?

126
00:08:52,320 --> 00:08:53,320
Where did you start?

127
00:08:53,320 --> 00:08:54,320
Yeah.

128
00:08:54,320 --> 00:08:55,320
I will be clear.

129
00:08:55,320 --> 00:09:00,320
I have a degree from Stanford in product design, but it's through a department called Science

130
00:09:00,320 --> 00:09:05,360
Technology and Society, and it's actually a hybrid engineering program.

131
00:09:05,360 --> 00:09:09,360
So you take two engineering tracks at once, and then it ties together with this ethics

132
00:09:09,360 --> 00:09:14,760
component, which is part of the reason that I talk a lot about ethics publicly, which

133
00:09:14,760 --> 00:09:16,800
we had an STS at RPI also.

134
00:09:16,800 --> 00:09:17,800
Oh, that's great.

135
00:09:17,800 --> 00:09:18,800
That's great.

136
00:09:18,800 --> 00:09:21,000
They snuck it into various places.

137
00:09:21,000 --> 00:09:27,480
It's a sister program to Simpsis, which became a more known program recently, because

138
00:09:27,480 --> 00:09:32,720
one of the Instagram founders came from that program, in any case, a very small program

139
00:09:32,720 --> 00:09:33,720
at that point in time.

140
00:09:33,720 --> 00:09:41,280
Now it's one of the biggest, and I focused in computer science and product design through

141
00:09:41,280 --> 00:09:42,280
mechanical engineering.

142
00:09:42,280 --> 00:09:46,520
So it was very product focused, but through those two lenses of engineering.

143
00:09:46,520 --> 00:09:53,120
So I came into this with a background in web stack engineering and UX and full digital

144
00:09:53,120 --> 00:09:58,800
product design, but I wasn't coming at it from having no programming experience.

145
00:09:58,800 --> 00:10:05,720
So moving into a Python workflow and using tools and technologies like SQL that I'd seen

146
00:10:05,720 --> 00:10:08,800
before was not the primary challenge.

147
00:10:08,800 --> 00:10:12,600
So I definitely wasn't starting from zero, like some people do.

148
00:10:12,600 --> 00:10:13,600
Yeah.

149
00:10:13,600 --> 00:10:16,640
What about the STATS component, where did that come from?

150
00:10:16,640 --> 00:10:21,920
I had taken some STATS classes in college, but there was actually one that I loved, and

151
00:10:21,920 --> 00:10:25,960
the professor thought I was the weirdest person in his course, I'm sure, because it was

152
00:10:25,960 --> 00:10:28,680
a bunch of people who wanted to go into management consulting.

153
00:10:28,680 --> 00:10:35,920
It was a kind of operational statistics class, like how do you understand how cars pass

154
00:10:35,920 --> 00:10:41,080
through four toll booths when you have, you know, two of them open at any given time

155
00:10:41,080 --> 00:10:45,640
block, you know, it's kind of convex optimization problems.

156
00:10:45,640 --> 00:10:51,160
And I thought it was just the most interesting stuff, and I couldn't come up with an application

157
00:10:51,160 --> 00:10:54,200
that was anywhere close to a career that I thought I might have.

158
00:10:54,200 --> 00:10:57,200
I just had no idea how this stuff would be applicable.

159
00:10:57,200 --> 00:10:58,640
Same thing with linguistics.

160
00:10:58,640 --> 00:11:04,920
I, for a long time, would read linguistics textbooks and read a lot of known Trump ski

161
00:11:04,920 --> 00:11:09,920
when I was in high school and more theory behind it, love that stuff.

162
00:11:09,920 --> 00:11:13,000
My parents thought I was going to major in it, and I said, it's not applicable, I can't

163
00:11:13,000 --> 00:11:14,000
use it.

164
00:11:14,000 --> 00:11:15,000
Yeah.

165
00:11:15,000 --> 00:11:19,480
Of course, flash forward to several years later, and I'm actually working quite a lot

166
00:11:19,480 --> 00:11:25,560
with unstructured text, and that's actually the biggest request that I hear in the market

167
00:11:25,560 --> 00:11:26,920
as a consultant.

168
00:11:26,920 --> 00:11:33,800
How do we work with text and understand it through a lens that works for us and isn't

169
00:11:33,800 --> 00:11:38,720
just a word cloud or a count of various themes coming up?

170
00:11:38,720 --> 00:11:46,640
How do we understand it from the perspective of the customer service industry, or we talked

171
00:11:46,640 --> 00:11:51,640
here earlier about data science and HR and understanding feedback, those types of applications

172
00:11:51,640 --> 00:11:55,880
are becoming very popular and widely requested.

173
00:11:55,880 --> 00:11:58,520
So things always come back, right?

174
00:11:58,520 --> 00:12:06,200
One of my favorite designers has this, well, the thing that he paints on billboards.

175
00:12:06,200 --> 00:12:12,480
These guys, Steven Seagmeister, he says, everything I do always comes back to me, and I think

176
00:12:12,480 --> 00:12:18,000
about those all the time, because there are always these little things, these little vignettes

177
00:12:18,000 --> 00:12:22,160
that you take, and you never quite know when you're going to come back to that, and it's

178
00:12:22,160 --> 00:12:24,680
going to be relevant to what you're doing.

179
00:12:24,680 --> 00:12:25,680
Yeah.

180
00:12:25,680 --> 00:12:26,680
Yeah.

181
00:12:26,680 --> 00:12:31,040
So you've built the open source data science masters for folks that are starting at

182
00:12:31,040 --> 00:12:35,440
zero and trying to work their way to, or starting someplace, and trying to work their

183
00:12:35,440 --> 00:12:37,880
way forward.

184
00:12:37,880 --> 00:12:43,080
What are the things that you find folks struggle with the most?

185
00:12:43,080 --> 00:12:47,360
The biggest challenge for the curriculum and for people going through it right now, I

186
00:12:47,360 --> 00:12:53,280
will say this is the two-sided problem, is that people can't find problems that are

187
00:12:53,280 --> 00:12:58,000
appropriately scoped to showcase their talent, and the curriculum can't necessarily provide

188
00:12:58,000 --> 00:12:59,160
that right now.

189
00:12:59,160 --> 00:13:07,000
I have investigated how I might go about providing sample data sets and questions alongside

190
00:13:07,000 --> 00:13:13,200
them that would give you a take-home package of something that would showcase your skills,

191
00:13:13,200 --> 00:13:19,320
but it's actually a lot more work than you do expect, and it's very difficult because

192
00:13:19,320 --> 00:13:21,440
it's a scoping problem at its heart.

193
00:13:21,440 --> 00:13:25,600
You have to have something that has enough depth, but isn't overwhelming, and can showcase

194
00:13:25,600 --> 00:13:33,120
a bunch of different skills, so it's a big challenge for people to sell themselves through

195
00:13:33,120 --> 00:13:37,880
providing that type of portfolio piece.

196
00:13:37,880 --> 00:13:43,560
And at this point, I think Kaggle does a really good job of curating data sets and providing

197
00:13:43,560 --> 00:13:51,480
conversations around analysis and modeling and predictive algorithms and ways to approach

198
00:13:51,480 --> 00:13:54,920
problems, and I usually direct people.

199
00:13:54,920 --> 00:13:58,600
I was going to ask you if Kaggle was one of the places that you point people.

200
00:13:58,600 --> 00:13:59,600
Yeah.

201
00:13:59,600 --> 00:14:04,400
I think they do a really good job with that, so they, I mean, their entire model is built

202
00:14:04,400 --> 00:14:13,840
around that type of work, appropriately, scoping questions around a set of data, and a lot

203
00:14:13,840 --> 00:14:17,640
of people to work on it, and sometimes rewarding them for that.

204
00:14:17,640 --> 00:14:18,640
Yeah.

205
00:14:18,640 --> 00:14:24,320
It's interesting that the podcast has a lot of folks that are somewhere on that curve.

206
00:14:24,320 --> 00:14:31,920
I hear from folks every once in a while asking about how they might apply machine learning

207
00:14:31,920 --> 00:14:38,080
AI to health care, or some problem that they have an interest in.

208
00:14:38,080 --> 00:14:42,640
And it's difficult to manage that scope as a beginner in part because you don't know

209
00:14:42,640 --> 00:14:44,640
what you don't know, right?

210
00:14:44,640 --> 00:14:45,640
Absolutely.

211
00:14:45,640 --> 00:14:50,680
But at the same time, a lot of times when you go to some of the public forums where people

212
00:14:50,680 --> 00:14:54,840
are asking, how do I learn this stuff?

213
00:14:54,840 --> 00:14:58,880
People will say, well, go take this course or course, that course or course, and then

214
00:14:58,880 --> 00:15:00,400
go work on a project.

215
00:15:00,400 --> 00:15:05,640
And the gap between take this course or course, and then go work on a project is actually

216
00:15:05,640 --> 00:15:06,640
pretty huge.

217
00:15:06,640 --> 00:15:07,640
Yes.

218
00:15:07,640 --> 00:15:08,640
Yes.

219
00:15:08,640 --> 00:15:13,760
And it's a gap that you have to fill with building analytical intuition, which is something

220
00:15:13,760 --> 00:15:16,720
that is very hard to teach, but is very learnable.

221
00:15:16,720 --> 00:15:24,600
So there's that counter intuition there that it's something that you can learn, and it's

222
00:15:24,600 --> 00:15:29,320
best learned from other people, but it's very hard to learn it from a book.

223
00:15:29,320 --> 00:15:36,560
So I do encourage people to use that practice, and for example, looking at a Kaggle competition

224
00:15:36,560 --> 00:15:40,960
around health care data and taking a stab at it without seeing what other people are

225
00:15:40,960 --> 00:15:45,320
working on, given a question, and then coming back to see how other people address that

226
00:15:45,320 --> 00:15:51,920
question, very useful workflow, and does provide you some of the asynchronous communication

227
00:15:51,920 --> 00:15:56,440
that you would otherwise have in person in a company.

228
00:15:56,440 --> 00:16:01,960
The other key component there that I think is really helpful is to have questions that

229
00:16:01,960 --> 00:16:05,760
are actually appropriate for the data, and to be very strict about your own workflow

230
00:16:05,760 --> 00:16:10,400
when you're answering that question, because you can get lost in the weeds everywhere.

231
00:16:10,400 --> 00:16:17,160
And in fact, I'd say most data science teams, their biggest struggle is not necessarily

232
00:16:17,160 --> 00:16:23,000
with structure, but with the rigor of having questions that they can actually test and

233
00:16:23,000 --> 00:16:27,120
hypotheses that they can actually test against.

234
00:16:27,120 --> 00:16:34,000
And I certainly do know teams that have a more R&D approach, and that can lead you to interesting

235
00:16:34,000 --> 00:16:41,600
places, but it doesn't necessarily help you answer a question, because you're not necessarily

236
00:16:41,600 --> 00:16:43,640
restricting yourself to that path.

237
00:16:43,640 --> 00:16:44,640
Yeah.

238
00:16:44,640 --> 00:16:45,640
Yeah.

239
00:16:45,640 --> 00:16:46,640
So you've got a teaching vent.

240
00:16:46,640 --> 00:16:50,240
You put together this set of resources, and the natural step consulting, right?

241
00:16:50,240 --> 00:16:53,640
We're here teaching clients that are actually trying to build these teams.

242
00:16:53,640 --> 00:16:54,640
Yeah, exactly.

243
00:16:54,640 --> 00:16:55,640
Exactly.

244
00:16:55,640 --> 00:16:56,640
It is very natural.

245
00:16:56,640 --> 00:17:03,120
And I think the biggest reward that I get in consulting is when I work with someone

246
00:17:03,120 --> 00:17:09,880
who's a little less technical or more distant from the data science, and they start to understand

247
00:17:09,880 --> 00:17:16,480
and into it as people who would be new to data science, they start to into it about what's

248
00:17:16,480 --> 00:17:21,000
going on with the data and how you can answer the question with the data and why it's appropriate

249
00:17:21,000 --> 00:17:26,800
or not appropriate and what manipulations they need to make and what type of data they

250
00:17:26,800 --> 00:17:33,200
need to make in intuitions about what they can do with it.

251
00:17:33,200 --> 00:17:35,160
So that's really rewarding.

252
00:17:35,160 --> 00:17:39,760
I've had the pleasure of working with a couple of product teams, and product teams are great

253
00:17:39,760 --> 00:17:44,680
because they have a vision for what they want as an outcome, and that outcome is really

254
00:17:44,680 --> 00:17:53,160
helpful, much like a driving question or hypothesis to guide you through a set of possible solutions

255
00:17:53,160 --> 00:17:55,600
with a lot of rigor and direction.

256
00:17:55,600 --> 00:18:01,520
So that's been really rewarding to see product managers saying, hey, I think this will work

257
00:18:01,520 --> 00:18:06,680
because I know that it worked in this other case, and we learned about that a couple of

258
00:18:06,680 --> 00:18:09,200
weeks ago, and it is very much like teaching.

259
00:18:09,200 --> 00:18:14,480
Now knowing a little bit about your background now, I can almost imagine the context out of

260
00:18:14,480 --> 00:18:20,680
which the hybrid AI blog post came, you know, product teams telling you, oh, can we just

261
00:18:20,680 --> 00:18:24,160
throw AI at this and not have any humans in the loop?

262
00:18:24,160 --> 00:18:26,160
Yeah.

263
00:18:26,160 --> 00:18:27,160
Did you hear a lot of that?

264
00:18:27,160 --> 00:18:34,040
I've definitely heard that like, well, we know that we have to have some mostly human

265
00:18:34,040 --> 00:18:38,640
approach, and we can use some predictive technology alongside them for a while, but we're

266
00:18:38,640 --> 00:18:41,040
really shooting for 100% at the end.

267
00:18:41,040 --> 00:18:49,280
And that ultimate vision is very problematic because as I explain in the post, and I can

268
00:18:49,280 --> 00:18:53,800
summarize that briefly, but hybrid AI in cases where you need to make sure that you

269
00:18:53,800 --> 00:19:01,120
need people to look at data where you're not certain how to predict an outcome or classify

270
00:19:01,120 --> 00:19:10,240
or whatever your objective is, have them look at that poorly or less confidently predicted

271
00:19:10,240 --> 00:19:18,240
data and make their own judgment about what should happen, allowing that to happen incorporates

272
00:19:18,240 --> 00:19:25,000
the possibility of future outcomes and future inputs.

273
00:19:25,000 --> 00:19:29,040
In cases where you haven't seen everything that you could possibly see because in the

274
00:19:29,040 --> 00:19:34,320
future there will be new and different options, it's only necessary that you would always

275
00:19:34,320 --> 00:19:43,640
involve people because you have to incorporate those new opportunities and those new possibilities.

276
00:19:43,640 --> 00:19:50,200
So I think tempering our expectations about how much work computers will do and what type

277
00:19:50,200 --> 00:19:56,560
of work they will do is really key to building the right solutions because otherwise we don't

278
00:19:56,560 --> 00:20:04,680
have a good Pareto 8020 approach to our problems where we can say, hey, let's set aside this part

279
00:20:04,680 --> 00:20:08,240
of the problem because we know it's always going to be too hard for the computer.

280
00:20:08,240 --> 00:20:14,560
It will cost us 80% of our time to solve 20% of the problem and it doesn't actually make

281
00:20:14,560 --> 00:20:15,560
sense.

282
00:20:15,560 --> 00:20:17,000
It's just route that to people.

283
00:20:17,000 --> 00:20:22,680
We might learn more about that problem space in the future, but we also know that there's

284
00:20:22,680 --> 00:20:27,040
kind of slop that we always need to account for and that's important.

285
00:20:27,040 --> 00:20:33,600
And it sounds like you don't think we're anywhere near, you know, closing that gap, getting

286
00:20:33,600 --> 00:20:37,240
to the humans out of the loop.

287
00:20:37,240 --> 00:20:40,840
It depends on the application you're looking at, absolutely.

288
00:20:40,840 --> 00:20:46,600
We used a human and the loop system at Mattermark for various tasks on the machine learning

289
00:20:46,600 --> 00:20:52,080
team and we actually had people in-house in addition to some systems where we had outside

290
00:20:52,080 --> 00:20:54,720
labeling done.

291
00:20:54,720 --> 00:20:57,520
And we had used vendors for that type of thing.

292
00:20:57,520 --> 00:21:07,040
There are a couple good options there, but I think there's a lot of work to do.

293
00:21:07,040 --> 00:21:13,920
I think the pragmatism on the shape of the solution, the solution space that's possible

294
00:21:13,920 --> 00:21:20,760
to achieve in a reasonable amount of time and any sort of reasonable cost for a solution

295
00:21:20,760 --> 00:21:25,840
all drive us toward this hybrid case.

296
00:21:25,840 --> 00:21:34,760
And you also discover pretty interesting things when you use people or services that have

297
00:21:34,760 --> 00:21:40,080
people labeling data or providing feedback because they will give you more information

298
00:21:40,080 --> 00:21:42,000
than you asked for in some cases.

299
00:21:42,000 --> 00:21:48,080
And we actually have had people in the past come back to us, find our email addresses.

300
00:21:48,080 --> 00:21:52,400
I don't think they were given them, so they actually went out and did research, found

301
00:21:52,400 --> 00:21:55,640
out how to email us and said, hey, you asked me this question.

302
00:21:55,640 --> 00:21:59,200
I actually think there's kind of an issue with how you phrased it.

303
00:21:59,200 --> 00:22:01,760
It doesn't fully address this other issue.

304
00:22:01,760 --> 00:22:05,320
Have you thought about that? I'm worried that I answered the question wrongly.

305
00:22:05,320 --> 00:22:10,400
I mean, these are people that were on your labeling team that felt so compelled to.

306
00:22:10,400 --> 00:22:13,920
These are actually people that weren't even on the team.

307
00:22:13,920 --> 00:22:19,400
They were a part of an outsource group of people that were paid to work on that data.

308
00:22:19,400 --> 00:22:26,120
So you learn a lot because you get more perspectives and more eyes on the data, which is always

309
00:22:26,120 --> 00:22:30,440
a good thing, especially when you're thinking about blind spots that you might have.

310
00:22:30,440 --> 00:22:35,120
Yeah, I thought even one of the simple things that I thought was pretty interesting about

311
00:22:35,120 --> 00:22:40,680
that post was you presented some kind of broad brush stats, and I don't remember the specific

312
00:22:40,680 --> 00:22:51,080
stats about something along the lines of AI by itself right now, a machine learning solution

313
00:22:51,080 --> 00:22:57,200
can get to 90% accuracy for generalized speech interpretation.

314
00:22:57,200 --> 00:23:01,440
But in order to really be usable, it needs to be 98 or something like that.

315
00:23:01,440 --> 00:23:07,320
I forget the numbers, but I think, you know, I don't think people think about that enough.

316
00:23:07,320 --> 00:23:12,640
They don't. They don't. So it's funny.

317
00:23:12,640 --> 00:23:17,040
When humans look at data, they have a very different perception of it than when they look

318
00:23:17,040 --> 00:23:24,480
at the metrics about the data. So for example, if you have a classifier for five classes,

319
00:23:24,480 --> 00:23:31,560
and you look at a 75% accurate classifier over all of those classes, it will look like garbage

320
00:23:31,560 --> 00:23:38,120
to you as a human. Even though that's pretty high, relatively speaking, and you probably

321
00:23:38,120 --> 00:23:45,760
did some work to get it to that point, you would probably still call it an unacceptable

322
00:23:45,760 --> 00:23:52,080
option or a non-prefable classifier because that's, it looks like garbage to you.

323
00:23:52,080 --> 00:23:59,600
I think the cases where that garbage matters is where we have to worry about the way that

324
00:23:59,600 --> 00:24:07,960
we build hybrid into solving that last component and getting to 99% or 95% or whatever we need

325
00:24:07,960 --> 00:24:14,800
to feel good about the application. For example, if we're predicting the health outcomes

326
00:24:14,800 --> 00:24:21,560
for a person, that's a very high stakes prediction, we would probably want to skew much further

327
00:24:21,560 --> 00:24:29,400
in the hybrid direction or in the human augmented direction than otherwise because the stakes

328
00:24:29,400 --> 00:24:35,920
are actually very high. So I think when we start to discriminate between types of applications,

329
00:24:35,920 --> 00:24:42,360
that's where we see this coming in. But even for consumer applications like Google

330
00:24:42,360 --> 00:24:47,920
Knowledge Cards, things like that, people still curate a lot of that information. It's

331
00:24:47,920 --> 00:24:53,760
not necessarily summaries that are generated by a computer. Sometimes there are people

332
00:24:53,760 --> 00:25:02,560
that are taught to create that data in a particular way. And I think we saw a great example

333
00:25:02,560 --> 00:25:09,840
of this a couple of weeks ago when news about how Facebook curates news articles came out.

334
00:25:09,840 --> 00:25:20,240
And that's a very good example of how your definitions of taxonomies, your acceptance of

335
00:25:20,240 --> 00:25:28,360
how things are classified and your incorporation of new information all impact your end user.

336
00:25:28,360 --> 00:25:37,120
And sometimes in very critical ways, it might sway how someone votes. It might give someone

337
00:25:37,120 --> 00:25:43,800
a perception of the world that they otherwise might not have in that case. So I think we're

338
00:25:43,800 --> 00:25:49,560
starting to see the impacts as well from consumer applications that we thought were not so

339
00:25:49,560 --> 00:26:00,040
high in terms of risk. And I look forward to seeing what they invest in at Facebook because

340
00:26:00,040 --> 00:26:06,520
I think I would wager that they have people working on this that have an eye on how to

341
00:26:06,520 --> 00:26:12,560
make this better. But at the end of the day, you do end up in a semi-political discussion

342
00:26:12,560 --> 00:26:19,480
about what fair and balanced means in journalism. And it becomes very domain specific. So I think

343
00:26:19,480 --> 00:26:26,600
it's healthy for society to grapple with that and for us to think very critically about

344
00:26:26,600 --> 00:26:30,200
how these things are actually working and sort of just engineering them away and having

345
00:26:30,200 --> 00:26:39,480
a 100% machine solution. Are you aware of anyone, any groups working on the problem of

346
00:26:39,480 --> 00:26:47,160
hybrid, either from an academic, other academic research topic areas in there somewhere or tools,

347
00:26:47,160 --> 00:26:52,280
platforms, or is it, you know, everyone kind of figuring this out on their own building

348
00:26:52,280 --> 00:26:56,920
their own custom thing? And that's just the state of the art right now.

349
00:26:56,920 --> 00:27:04,480
So the short story is that a lot of companies do build their own custom platforms for

350
00:27:04,480 --> 00:27:13,160
doing this. They usually leverage some sort of marketplace for data entry, data annotation,

351
00:27:13,160 --> 00:27:21,200
a question answering, and broader products like Amazon Turk is a very broad product. You

352
00:27:21,200 --> 00:27:28,200
can arbitrarily give people tasks and you place a bid on how much you would pay people for

353
00:27:28,200 --> 00:27:34,680
those tasks and they can choose to accept it. So a lot of companies will use that platform

354
00:27:34,680 --> 00:27:41,880
and build on top of it and do a lot of integration of that type of system on the back end. So in

355
00:27:41,880 --> 00:27:49,880
some ways, you know, they call this artificial artificial intelligence. In some ways, the

356
00:27:49,880 --> 00:27:55,080
component is actually a technology interface itself, which is very interesting to think

357
00:27:55,080 --> 00:28:01,200
about because there are people on the other side of the other side of the technology.

358
00:28:01,200 --> 00:28:08,680
But there are a couple other vendors that do things to support hybrid. Crowdflower is one

359
00:28:08,680 --> 00:28:17,760
in San Francisco that does some of that. To my knowledge, they do interobserver validation

360
00:28:17,760 --> 00:28:25,440
basically to give you multiple, multiple sets of eyes on a given answer to a question

361
00:28:25,440 --> 00:28:33,080
to ensure that it's correct so you don't have bigger sets of errors or unmeasurable error

362
00:28:33,080 --> 00:28:38,280
and you know where things are going to be more ambiguous. That in itself can be very

363
00:28:38,280 --> 00:28:45,080
valuable to because you can, you can basically say, here's this big set of data or this big

364
00:28:45,080 --> 00:28:49,760
set of questions, let's say, how would you answer these questions and give it to multiple

365
00:28:49,760 --> 00:28:55,120
people and you'll find out where people disagree and that tells you more about the ambiguity

366
00:28:55,120 --> 00:28:59,960
of the problem space and where you're going to have to make stronger decisions about what

367
00:28:59,960 --> 00:29:05,400
you think is right. So that's been a really helpful thing for clients to understand in

368
00:29:05,400 --> 00:29:13,520
the past and I think they have a pretty good understanding of how that works and we'll

369
00:29:13,520 --> 00:29:20,440
see if they build more products around that. Do you have a, you know, top three takeaways

370
00:29:20,440 --> 00:29:28,800
that, you know, you found that clients, you know, as you look across a set of clients,

371
00:29:28,800 --> 00:29:32,600
you know, these are the top three things that, you know, they all, you know, either learned

372
00:29:32,600 --> 00:29:38,600
or need to learn in order to be successful at this stuff? I can tell you the first one

373
00:29:38,600 --> 00:29:45,640
is always know what your question is. Be very precise and know exactly what the answer

374
00:29:45,640 --> 00:29:51,840
would look like if you saw it. So if you see the answer, you'll know that the right thing

375
00:29:51,840 --> 00:29:58,000
is happening. I certainly worked with companies that say, hey, we have all this data, we want

376
00:29:58,000 --> 00:30:03,160
to learn from it and I say, great, what do you want to learn? And they say anything. And

377
00:30:03,160 --> 00:30:08,920
so that's, that's a perfectly healthy and normal place to start, but at that point, you don't

378
00:30:08,920 --> 00:30:12,960
have a question where you can build anything. So you have to formulate questions and decide

379
00:30:12,960 --> 00:30:17,800
what's actually valuable for your business, which is more of a business and product space

380
00:30:17,800 --> 00:30:25,560
question formulation task. So that strategic involvement has necessarily become part of

381
00:30:25,560 --> 00:30:31,800
the business. Coming from a product background, I can appreciate that. I think there are

382
00:30:31,800 --> 00:30:38,480
a lot of other independent consultants and people I know who work solely on questions after

383
00:30:38,480 --> 00:30:42,440
they've been fully formed. And they say, you know, once you have the specs ready, happy

384
00:30:42,440 --> 00:30:50,720
to work on it. But otherwise, it's, it's not, it's not what we do. And that initial step

385
00:30:50,720 --> 00:30:55,760
of defining your question and knowing that it's an appropriate question for the data, it

386
00:30:55,760 --> 00:31:03,000
really is the space where we, we thrive and help our clients succeed. So if they can come

387
00:31:03,000 --> 00:31:08,640
in with a strong understanding of what they have and what they want, that's always better.

388
00:31:08,640 --> 00:31:15,520
I think that's true broadly in business. But we think what else? So I was just having

389
00:31:15,520 --> 00:31:22,160
a really good conversation over lunch with a couple people about how one of the things

390
00:31:22,160 --> 00:31:30,320
that we don't see as often in data science, machine learning, land is a strong leadership

391
00:31:30,320 --> 00:31:37,640
that knows how to market really well. So a lot of what I've seen data science team struggle

392
00:31:37,640 --> 00:31:42,520
with is marketing themselves internally or marketing themselves up and managing up

393
00:31:42,520 --> 00:31:50,720
to see sweet or the VP of engineering, whoever it is. And it's, it's really important

394
00:31:50,720 --> 00:31:57,080
to develop those soft skills and understand what your value is relative to the company.

395
00:31:57,080 --> 00:32:02,160
Sure. And I can say that. But at the end of the day, it's actually extremely difficult

396
00:32:02,160 --> 00:32:10,520
to define that value because your systems may be giving some feedback to a business team

397
00:32:10,520 --> 00:32:15,680
that allows them to make better decisions, but really they're making their own decisions

398
00:32:15,680 --> 00:32:20,640
and they're supporting them with data in some cases. But you don't know what the investments

399
00:32:20,640 --> 00:32:25,640
would have looked like otherwise. And so comparing the alternate universe that you might have

400
00:32:25,640 --> 00:32:31,280
been in had you not had the technology that your team is building can be extremely difficult

401
00:32:31,280 --> 00:32:36,880
to quantify. But that is part of the work of leadership. Right.

402
00:32:36,880 --> 00:32:45,800
Clearly. So I look forward to seeing more breakout leaders that are really good at that.

403
00:32:45,800 --> 00:32:51,960
And I think it'll necessarily be something that we see in the next few years. I wouldn't

404
00:32:51,960 --> 00:32:55,840
call myself a pessimist, but I would say we're kind of high on the hype cycle right now.

405
00:32:55,840 --> 00:33:01,600
And I'm not optimistic that we're going, the market will continue going up. So I'm

406
00:33:01,600 --> 00:33:08,160
speaking. It goes in a cycle of company saying, hey, we're going to make this big investment

407
00:33:08,160 --> 00:33:15,080
data science. We think that data science is a very valuable investment for us for these

408
00:33:15,080 --> 00:33:21,520
reasons. And then a couple of years later, they come back to the team and they say, so

409
00:33:21,520 --> 00:33:27,000
how have we done? And at that point, the team really needs to sell what they've done.

410
00:33:27,000 --> 00:33:31,640
Ideally, they'd be selling that along the way as well. And I think we're coming to the

411
00:33:31,640 --> 00:33:37,320
end of one of those periods where companies expect to see those big wins and teams really

412
00:33:37,320 --> 00:33:43,880
need to justify their existence and be able to move the needle and describe how they're

413
00:33:43,880 --> 00:33:50,360
moving the needle. Yeah. Yeah. Soft skills. Yes. Soft skills. Soft skills. Yeah. Take that

414
00:33:50,360 --> 00:33:55,200
vent diagram of all the things you were supposed to be as an Amazon. I just add like four more

415
00:33:55,200 --> 00:34:05,000
things to it. No problem. Yep. Nice. Nice. So that's two, but they're big. So. Yeah.

416
00:34:05,000 --> 00:34:09,240
Well, on the third, the third is probably just manage expectations, right?

417
00:34:09,240 --> 00:34:15,000
Oh, it's always. And any other, you know, any other of a number of sets of things in terms

418
00:34:15,000 --> 00:34:22,360
of the expectations always. You said you said it exactly right. Yeah. Managing the expectations

419
00:34:22,360 --> 00:34:28,200
is probably the biggest thing I do with clients. The first thing I say is I can't do anything

420
00:34:28,200 --> 00:34:34,360
to pull a big win out of hat. I won't be pulling a big win out of hat for you. If you still

421
00:34:34,360 --> 00:34:38,680
want to talk about this and you want to find out what this technology can do for you and

422
00:34:38,680 --> 00:34:44,120
how it can incrementally improve your business and create new opportunities for products. Let's

423
00:34:44,120 --> 00:34:47,640
talk about that. But it's not going to surface anything that you don't know about your own

424
00:34:47,640 --> 00:34:52,920
business because frankly, you know about your business. Your business is an existence. So you've

425
00:34:52,920 --> 00:35:01,720
must have some deeper understanding of what you're doing. And when I look at your deal flow,

426
00:35:01,720 --> 00:35:05,400
your best customers are your best customers. I'm not going to tell you that there's,

427
00:35:05,400 --> 00:35:11,720
there's a, there's a sleeper whale somewhere deep inside Salesforce. And that's okay.

428
00:35:12,520 --> 00:35:17,800
I think give you better confidence to make decisions and understand the differential value

429
00:35:17,800 --> 00:35:23,720
between things. But no promises. You really can't make promises. Yeah, absolutely.

430
00:35:23,720 --> 00:35:35,560
So, you know, going back to this conversation around hybrid AI, we started to talk about, you know,

431
00:35:35,560 --> 00:35:44,920
the role that humans in a loop play relative to, you know, their biases and, and, you know,

432
00:35:44,920 --> 00:35:50,520
quote unquote algorithmic bias and things like that, which actually that was the kickoff panel

433
00:35:50,520 --> 00:35:56,920
here at the Rangle conference. Is that that's something that you're spending some time looking

434
00:35:56,920 --> 00:36:05,960
at now as well, right? Yes. So these things are all interwoven in some way. The active learning

435
00:36:05,960 --> 00:36:15,960
and human and the loop patterns of hybrid are certainly ways to combat actively reinforcing

436
00:36:15,960 --> 00:36:24,360
pre-existing bias. If you construct a system to do expensive or amplify or both, you can do either

437
00:36:24,360 --> 00:36:31,880
end both. It depends on what you know about what you're doing, right? So if the example I give is

438
00:36:32,440 --> 00:36:39,160
a model that was built at one of my previous employers where we wanted to predict who would start

439
00:36:39,880 --> 00:36:44,760
a startup and leave their job and start a startup within the next six months. And the company had

440
00:36:44,760 --> 00:36:49,320
an intent to build this model, create a list of people that were going to start companies soon,

441
00:36:50,040 --> 00:36:56,840
and sell that list to investors as a type of pre-crime, basically algorithmic pre-crime for

442
00:36:56,840 --> 00:37:01,400
seed stage funds. And they could get in early, before people even knew that they were going to start

443
00:37:01,400 --> 00:37:10,120
companies, which is a like fascinating concept. So they used a number of factors to make this

444
00:37:10,120 --> 00:37:16,280
prediction like where you had gone to college, what kind of degree you had, what your job title was,

445
00:37:17,880 --> 00:37:24,200
what your previous employers were, there was a bucketing for the prestige of your college.

446
00:37:24,200 --> 00:37:29,480
So you know the IVs were at the top and kind of cascaded down through bigger institutions,

447
00:37:30,120 --> 00:37:38,440
and that included age. So what we ultimately saw when we predicted who

448
00:37:38,440 --> 00:37:43,800
would be a founder in the next six months was pretty interesting because all of those factors

449
00:37:43,800 --> 00:37:51,800
seemed to be directly relevant to how a person's career would develop them to be a founder in the

450
00:37:51,800 --> 00:37:59,720
future. And interestingly, a lot of the people in the list were 30-year-olds management, ex-management

451
00:37:59,720 --> 00:38:09,160
consulting, or ex-I bankers who were white males. And it didn't deviate too far from that.

452
00:38:09,160 --> 00:38:15,720
And at the time, I thought, you know, this is this is pretty uncomfortable, but I don't really

453
00:38:15,720 --> 00:38:25,080
know why. And it took me, it took me about a year to examine that emotional little more deeply.

454
00:38:25,080 --> 00:38:30,760
And underneath is actually a very good reason to be concerned because though you are making

455
00:38:30,760 --> 00:38:37,480
a prediction on characteristics that you think are fundamentally predictive of an outcome,

456
00:38:37,480 --> 00:38:47,960
they have bias from the world rolled up into those factors. So all of the decisions that were made

457
00:38:47,960 --> 00:38:52,760
to allow people to get to where they were and become founders in the previous

458
00:38:52,760 --> 00:39:00,680
state of the world and the training data is your prior for your prediction of who will be

459
00:39:01,160 --> 00:39:09,080
a future founder. And if you don't explicitly observe that, you know, I think I clicked through

460
00:39:09,080 --> 00:39:15,800
on LinkedIn to maybe that top 120 people on this list. And that's the only way that I knew

461
00:39:15,800 --> 00:39:23,880
that there was a certain split of like where people came from in terms of

462
00:39:23,880 --> 00:39:32,520
home country or home state, where people came from in terms of in in terms of

463
00:39:35,400 --> 00:39:42,040
age, all of these other characteristics, but even name can be ambiguous for what gender you are.

464
00:39:42,040 --> 00:39:47,800
So I didn't get a sense of who was actually in this list until I went and looked at it and we

465
00:39:47,800 --> 00:39:52,280
didn't have columns that said male female. We didn't test against that. We didn't predict on that,

466
00:39:52,280 --> 00:39:59,640
but there were only 13 women at the top of the list or near the top of the list. And I thought,

467
00:39:59,640 --> 00:40:08,840
well, that's that's somehow unfair, right? And I think looking back on that at the time,

468
00:40:08,840 --> 00:40:18,680
it was we were not talking about that specific type of diversity in the market for founders.

469
00:40:19,320 --> 00:40:25,560
Now that that conversation is happening more, it's become more unambiguous case where you can say

470
00:40:27,320 --> 00:40:34,760
all of the priors, all of the pattern matching, so to speak, literally coming from VCs is being

471
00:40:34,760 --> 00:40:39,480
encoded into the algorithm that's making this ultimate prediction. And that's not okay. So

472
00:40:40,280 --> 00:40:45,000
the question then becomes what do we do? And there are a couple of people doing really great work

473
00:40:45,000 --> 00:40:51,640
on this. I think there's one department at Carnegie Mellon where someone's coming up with validation

474
00:40:51,640 --> 00:40:59,080
metrics that will help you test against the the characteristics you know you might have

475
00:40:59,080 --> 00:41:07,480
bias outcomes on. So in this case, you would say how many men and women are there in our outcome?

476
00:41:08,040 --> 00:41:16,440
Does it fit our expectation for what we would want to happen? And I think the the real key insight

477
00:41:16,440 --> 00:41:25,800
here is that we want to build algorithms that will construct a world that we want to live in rather

478
00:41:25,800 --> 00:41:34,200
than a world that existed in the past. We know the flaws of our current society to a large extent

479
00:41:34,200 --> 00:41:39,800
and some people more than others, but as long as we can be vulnerable to one another

480
00:41:41,800 --> 00:41:52,760
and try and validate that we are not reinforcing unjust actions from the past and just perpetuating

481
00:41:52,760 --> 00:41:58,360
them with algorithms in the future, that is actually key to our work. And that's really important for

482
00:41:58,360 --> 00:42:07,800
us to carry as a torch going forward. So I'm very excited that we had actually one talk so far

483
00:42:07,800 --> 00:42:15,640
and we will have another talk today about algorithmic bias and harm and how these systems affect

484
00:42:15,640 --> 00:42:24,040
users and I think it's a conversation that needs to gain more traction in the practitioner space

485
00:42:24,040 --> 00:42:30,280
and we need to examine our own practices which more closely and know what we're doing.

486
00:42:32,360 --> 00:42:36,760
Perhaps the most egregious example of this in the press lately was a

487
00:42:36,760 --> 00:42:45,640
algorithm that police stations were using across the country to predict recidivism which

488
00:42:47,160 --> 00:42:53,560
is the one that was exposed in the pro-publica article. Yes, exactly. And they did a bunch of work

489
00:42:53,560 --> 00:42:59,320
that they put up on GitHub along with the data set to explain what was happening

490
00:42:59,320 --> 00:43:07,000
and how they had analyzed the outcomes and as far as they could see what was happening in that

491
00:43:07,000 --> 00:43:13,560
technology and I believe the company is still holding it as private IP so even the police

492
00:43:13,560 --> 00:43:19,640
departments don't understand how this model works. The journalist did a really good job of saying,

493
00:43:19,640 --> 00:43:25,800
this is a big problem, here are the metrics and here is the full explanation with the data of

494
00:43:25,800 --> 00:43:33,320
what's so wrong with this beyond the anecdotal evidence of this is predicting that one person who

495
00:43:33,320 --> 00:43:41,240
has committed one petty crime is more dangerous than someone who's a repeat criminal and has been

496
00:43:41,240 --> 00:43:51,240
violent. I think it's a really egregious case but I don't want to say that it's good these

497
00:43:51,240 --> 00:43:57,640
things happen but I think a few high profile cases will push the regulatory system to become

498
00:43:57,640 --> 00:44:05,480
more serious about this so insofar as like it has to get worse if it gets better. I'm hoping that

499
00:44:05,480 --> 00:44:11,240
that we can get out ahead of that as practitioners but regulation will certainly be getting there as

500
00:44:11,240 --> 00:44:17,240
more and more of these cases are uncovered. Do you have a vision for how regulation can play here

501
00:44:17,240 --> 00:44:22,440
without overly suppressing innovation which is a big concern that you hear on the other side?

502
00:44:23,000 --> 00:44:32,200
It is and a good example of where you see that is in loan assessment and the finance space where

503
00:44:32,200 --> 00:44:39,400
there are very strong regulations about how how you make decisions about what credit lines people

504
00:44:39,400 --> 00:44:47,080
will get so given again like things that got worse before they got better redlining in the past

505
00:44:47,080 --> 00:44:53,800
and other actions that have been taken that were deemed not legal after that. That environment has

506
00:44:53,800 --> 00:45:05,880
responded extremely strongly to that. There's a pretty good understanding of what is important

507
00:45:05,880 --> 00:45:11,080
in making that work transparent so that you can actually give someone feedback on why they were

508
00:45:11,080 --> 00:45:18,120
rejected for a loan or why they were given a certain loan amount or a certain credit line

509
00:45:18,120 --> 00:45:26,760
and while that's important I think there are improvements further to be made so I would

510
00:45:26,760 --> 00:45:32,600
expect that if the regulation comes down really hard in the way that it has on that industry

511
00:45:32,600 --> 00:45:38,280
if it comes down similarly on others as I think we're seeing that you just becoming interested in

512
00:45:38,280 --> 00:45:48,360
then it can stifle innovation and probably grind it to a very slow pace but we're resilient,

513
00:45:48,360 --> 00:45:56,760
we'll figure out ways to justify our existence and how we do our work and I think that's very healthy

514
00:45:56,760 --> 00:46:04,840
in the ecosystem and the the oven flow of these factors and regulation and innovation are always

515
00:46:04,840 --> 00:46:11,640
battling it out and the faster we can get out ahead of it and say no no no we actually know

516
00:46:11,640 --> 00:46:15,720
what we're doing and we actually know how this works and we are justifying these things and we are

517
00:46:16,440 --> 00:46:23,480
taking the appropriate precautions and trying to be as self-critical as possible and doing so

518
00:46:23,480 --> 00:46:31,080
honestly if we can do that then the regulation will be the regulatory environment will be very

519
00:46:31,080 --> 00:46:39,240
different when it finally comes to bear in these other areas that aren't just credit-worthyness.

520
00:46:39,240 --> 00:46:45,960
Yeah great great well we've got additional talks here to go check out anything you want to

521
00:46:45,960 --> 00:46:54,360
leave folks with point folks to I would say keep watching the algorithmic harm and ethics

522
00:46:54,360 --> 00:47:03,080
arena there's a lot of work being done there and there are people that are finding great solutions

523
00:47:03,080 --> 00:47:07,640
and people that are also of course always coming out with more critique and

524
00:47:09,080 --> 00:47:14,120
interesting philosophical perspectives to consider so stay involved in that conversation

525
00:47:14,120 --> 00:47:19,960
because it's an active one and everyone can be part of it. Yeah that's that's great and I think

526
00:47:19,960 --> 00:47:28,280
the you know your comment about you know AI encoding the future that we want as opposed to the

527
00:47:28,280 --> 00:47:36,200
past that we you know that we know I think it's a great one very very optimistic. Yes yes if you

528
00:47:36,200 --> 00:47:42,840
care about that stay involved in the conversation and and yeah be a part of it. Great all right thanks

529
00:47:42,840 --> 00:47:53,960
so much Claire. Thanks Sam. All right everyone that's our show for today I really hope you enjoyed

530
00:47:53,960 --> 00:47:59,480
the interview and thanks so much for listening. Of course you can find the notes for this and every

531
00:47:59,480 --> 00:48:07,800
show at the Twimalei.com website twimlai.com the notes for this particular show can be found at

532
00:48:07,800 --> 00:48:16,520
twimalei.com slash 11 the number 11 as always I really appreciate getting your tweets and emails

533
00:48:16,520 --> 00:48:23,640
and newsletter subscriptions and iTunes reviews so by all means keep them coming. Of course we'd

534
00:48:23,640 --> 00:48:29,800
love to have you join the conversation you can tweet me at Sam Charrington. Claire is Claire

535
00:48:29,800 --> 00:48:38,120
Corfell and I'm also increasingly using the Twimalei Twitter handle. T-W-I-M-L-A-I. Looking forward

536
00:48:38,120 --> 00:49:02,280
to hearing from you and catch you next time.

