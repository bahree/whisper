Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting
people doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
Last week was a big week for the podcast, I announced the first anniversary of the show.
This week I want to start the show by thanking everyone who's participated in our first
anniversary contest.
We asked you to comment on the show notes page or post an iTunes review and wow did you
deliver?
Your stories have been personal, thoughtful and downright encouraging.
I've got a couple that I'd like to share and really it's so hard just to pick a couple
of these but first Andrew posted on the show notes page.
I've used this podcast to maintain a pulse on current ML and AI.
Big thank you for both helping me in my day to day but also getting me interested in ML
in general when I first started listening.
Heck, I've been listening to this podcast when I was a student, then when I was an intern
and then when I was an entry level analyst and then when I was promoted to analyst and
now as a senior analyst where I am now is in no small part thanks to this podcast.
Wow, that's a ton of ground to have covered in just a year, Andrew, congrats.
We are so proud to have been a small part of your success.
This next one has a bit of a backstory.
Bill B123 posted a five star review on iTunes titled sold on the deep interview format.
Bill said new interview style format was not initially good.
I gave it two stars initially later raised it to four stars.
Now I'm sold great podcast with tons of insights and learning.
Keep up the great work Sam.
Now I remember when Bill's first review hit the decision to switch to the interview format
was really tough for me, but I knew it was necessary for my efforts to be sustainable.
Bill's initial two star review really, really hurt.
I think at the time him or another user said the interviews were just a bit too fluffy
and that I was like the Tim Ferriss of AI.
I wasn't really sure what to do with that because I kind of like Tim Ferriss's podcast,
but I really took that to heart and I was pumped when Bill raised his review to four stars.
And now I'm super excited to see that I've earned Bill's fifth star.
Thanks so much Bill.
Starting from nothing I never imagined this podcast would begin to blossom into such an awesome
community of users.
I say begin because we still have so much ground to cover and we are truly just getting started.
For those who have not yet had a chance to enter the contest, please visit twimmolai.com
slash birthday for more info.
Don't forget first prize gets a bronze pass to the O'Reilly AI conference this June,
which is an $1,800 value.
One prize gets a Google home powered by AI, of course, and everyone that participates
gets a couple of Twimmolaptop stickers.
The contest ends June 1st and winners will be announced on the second.
If you've posted a review on iTunes to enter the contest, please reach out to us at team
at twimmolai.com to let us know who you are.
All right, this week on the show, my guest is Deep Varma.
This President of Data Engineering at Real Estate Startup Trulia.
Deep has run data engineering teams in Silicon Valley for well over a decade, and he's now
responsible for the engineering efforts supporting Trulia's big data technology platform, which
encompasses everything from data acquisition and management to data science and algorithms.
In the show, we discuss all of that with an emphasis on Trulia's data engineering pipeline
and their personalization platform, as well as how they use computer vision, deep learning,
and natural language generation to deliver their product.
Along the way, Deep offers great insights into what he calls offensive versus defensive
data science and the difference between data driven decision making versus products.
Another great interview, and I'm sure you'll enjoy it, and now on to the show.
All right.
Hey, everyone.
I am on the line with Deep Varma.
Deep is Vice President of Data Engineering with Trulia, and I'm excited to have Deep join
us.
Deep, how are you doing today?
I'm doing great. It's not that hot. The California and San Francisco have been hot for the last
few days, but seems like the fog is coming back, so I'm definitely doing amazing.
How are you, Sam?
Nice, nice.
Well, I'm doing very well, and I'm really looking forward to our conversation and to learning
a little bit more about how you guys use data at Trulia.
Why don't we get started by having you talk a little bit about your background and how
you got into working with data?
Yeah.
I think you asked me a great question, because when I go and I speak in some of the schools
to help undergrads or the grads, those who are doing the management, and one of the guys
I think few months back in Berkeley asked me the similar kind of a question that Deep
why data, and I think Sam, it goes back to my reasoning, my mindset from the childhood
where I was always looking into the reasons why this is this, why this is this, and when
I get into my master's and computer science, I still remember there were old databases
which some of, like you may know and you may not know, it's the DBs or the Fox base.
Those were the early versions or the manifestations of the structured databases coming in, and
I was always been very interested and then entering into my first job in IBM, it's where
we are working on those XML directs change, how we are going to have the directs change
between one entity and another entity, how the web services discovery locator are going
to come into the picture.
So early on the foundation was where I've been from the get go from my own personal desire
to look into the answers as well as exposure to the early technologies.
Get me into the databases and entering into then my journey where exploring why and what
I realized is at the end of the day, we are always surrounded by data and the data doesn't
mean that it has to be a textual data, the data is how we interact with each other when
we are making phone calls to the people, when I'm searching for something, and that's
I think it's 2001, 2002 time frame was started getting into my DNA that, you know, my God,
you know, every day when I were I interact with anyone, anything I do, it's the data.
And this is where I got into Yahoo.
So then that was a step, you know, we're getting into Yahoo, helping advertisers and publishers,
you know, try to render good quality ads to the consumers.
This is all again, the platform is, you know, how you understand your consumers better.
And then going into my startups and, you know, looking into the data again, where we are
looking into, you know, how the data floating from one system to other system, what predictions
we can do.
So in nutshell, I will say it is, this is how I got into the data and I think is for me,
my behavior, sometimes I'm at a point, Sam, I will tell you, when I go home, my wife
have to remind me, honey, you're back home, don't think from a data point of view, just
think, you know, you're back home, data reasoning is not going to work here.
That's funny.
That's funny.
I did my, some of my grad school work on queuing theory and my wife is so tired of me
analyzing lines and queuing scenarios and banks and grocery stores and trying to tell
her which line she should be in.
So I definitely relate to that.
Yeah.
I see you were in Yahoo back in the glory days.
Oh, you trust me, you know, those were the glory days and I still miss those days because,
you know, Yahoo was the center of attraction and the talent was huge there.
So I worked there for four years and, you know, unfortunately, Yahoo is no longer Yahoo.
But it was amazing.
Absolutely.
Were you involved in, did you use sedupe or were you involved in kind of the development
and advancement of the dupe at that time?
Yes.
So when I went in, we were trying to, so Yahoo bought this company over here and we were
trying to integrate Yahoo's like a platform back and platform to get the search keywords.
So this is where Hadoop pipelines were integral part of the data flow between the systems
that how we get the data from our Pasadena based company and then into our system.
So yeah, I was not deeply like I was not part of the Hadoop ecosystem, but I was one of
the consumers of the Hadoop system to get the data I'm floating around.
Okay.
Okay.
And then now at, at Trulia, tell me a little bit about your role.
It sounds like at least from LinkedIn that you've got a pretty broad set of responsibilities
spanning everything from kind of your data platform, you know, I'm sure there's some Hadoop
ecosystem, something in there somewhere to, you know, the data science and the applications
that run on top of it.
Is that right?
That's fair.
And let me walk you through first why Trulia?
I think that's to me is the biggest piece which inspired me to join Trulia and do you mind
Sam if I ask you?
Do you rent a home or do you have your own home?
Yes.
Awesome.
I'm pretty sure you're going to relate to this story.
So this is way back in 1999 when we decided to buy our first home, you know, it is me and
my wife, you know, the data was there, but the data was in storage like I have to go
into police stations, I have to go into counties, I have to go into those areas to collect
the data, we take this data, then me and my wife sit together, we go through the listings,
we look into the neighborhood, we used to maintain our excel sheet, oh, let's look
into this listing and it was an cumbersome process and it was an emotional journey for
us to go through this exercise and it took us months to buy our first home and we did
it finally, right?
And that inspired me to join Trulia because, you know, when you think it's, how can we
use this data when I, when I was in my early conversations with Trulia, that was the biggest
thing for me is, you know, how I'm going to come and join Trulia and make an impact to
build Trulia has more of a data driven product company and first thing in my mind was the
use case of me 1999 buying a home that deep, can I make four millions of consumer that
journey much more meaningful, much more enjoyable by using this data?
So that's how my journey began with Trulia.
Now just to go a little bit more detail into my role in Trulia, I think it's, you know,
Trulia is, you know, our number one goal and this is where our founders, they saw a huge
opportunity to change this marketplace by providing information and insights to our
consumers to help them make the right decision and, you know, and make this journey home
search journey easy and enjoyable.
So, you know, with that mindset, with that goal, what our founder set in fourth, we,
we continue my goal was to continue this and provide amazing experiences to our consumers
and we are investing a lot in our personalization, big data, machine learning platforms to help
consumers like me to, you know, find their perfect home in much more efficient and better
way than I did, you know, years back.
So that's in not show what role is and I'm happy to dive into more details about those
technologies, what I'm talking about.
Why don't we start with you talking a little bit about the data products and what are,
from a consumer experience perspective, how do, how is, you know, machine learning
in AI and the various data products that you create on your team?
How is that surface to the truly a user?
Yeah.
So I think, you know, it's, first of all, data driven product companies, you know, Sam,
there is a big mindset needs to happen and I'm just, you know, three years back when
I joined Trulia, my philosophy was always being to transform and use this data more on their
offensive strategy rather than defensive strategy and I'm going to go into answer your question
but I just wanted to give you a little bit more details because, you know, when you think
about the data driven companies, there are two aspects of the data driven company.
One is the data driven decision making and other is the data driven product and the
decision making is more your product analytics, you know, where you launch a feature and then
you, oh, is this feature working or this feature is not working?
But my one goal is to transform this data driven decision making more from a defensive to
offensive by saying, is this feature going to work?
So that's the one component and the second component, which is the discussion we are having
today is around the data driven product company and this is where the way it surfaces
to our consumers, our average consumer have no idea when they come to Trulia, when they
engage with Trulia via mobile web app or our browser or desktop applications.
They don't know that we use the data.
It is basically pretty much embedded in their user experience.
It is embedded when they explore, when they start their search journey.
It is our responsibility to understand their behavior, what they're looking into and what
we have done Sam is we have built an underlying personalization platform first and so think
about that as our foundation and you know, this is where we have our consumers unique preferences,
search criteria and you know, what they're looking into like deep is looking into quiet
neighborhood, good school district in mission district of San Francisco, that's the personalization
platform.
On top of it, we have our machine learning pillars and there are many pillars what we have
invested in machine learning, the first one is our computer vision and deep learning,
the second one is our recommender engine, the third one is our user engagement models
and the fourth one is our natural language processing or the natural language generation.
So these are the machine learning pillars what we have and the, you know, and then we
use all those pieces in tandem like machine learning pillars, personalization platform,
all together to give that experience to our consumers when, when you come to our side
and you look into, you know, the photos, when you look into the, when you receive an
email from Julia, when you receive a push notification from Julia, all this is part
of our machine learning technologies, which the goal is to engage our consumers and give
them much more relevance experience during their stay with Julia and I will go definitely
this conversation.
I will give you more details around each and every component what we're going to talk.
But if you have any question, I'm happy to ask address that first.
Okay.
There's, there's just so much in there to dig into, I like the, I like the distinction
between the decision making versus the products and you mentioned specifically also this kind
of dichotomy between offensive and defensive.
Can you elaborate on that a bit more?
Yeah.
Yeah, definitely.
I think is being in the Silicon Valley for close to two decades now and I have seen startup
companies coming up and, you know, their focus is mostly around building the products.
There are companies like Google's and Facebook's are definitely, you know, those who are more
data driven, but I have seen early on when the company start building the products, their
focus is never been the data side, they have couple of analytics who are staying behind
the scene.
Oh, should I change my pricing?
Should I make my pricing for this consumer do this?
It's always an after five, six years, if they have to, you know, raise more funds or
they're about to go public or when they go public, now the mindset changes, oh, what is
my differentiator?
How I'm going to differentiate my product?
How I'm going to bring my consumers back?
How I'm going to engage my consumers?
And this is where the offensive strategy comes into a picture that why not we have the
companies start thinking about the data from the get go?
What data you're collecting?
How do you build?
And I think that's a struggle and that struggle brings it to the point where companies have
to go back and reinvest their resources, their millions of dollars to rebuild their architecture
because if you think about the data, Sam, at the end of the day, this conversation what
you and me are having is so much we are talking about, you know, artificial intelligence, machine
learning, we are collecting this data and we are compacting into a podcast, but these
are the signals, right?
So there is a quality of the data, integrity of the data, how do we take all those things
and bundle up so that the organizations are thinking about changing the direction from
the get go rather than after the fact and after years thinking about.
So that's the way, you know, I think and I think I wanted to differentiate between the
two aspects which I talked earlier and I just want to make sure that both you and me
are on the same page.
One is the decision making and other is the product building, both of those facets requires
the data, decision making is our amazing analytics teams rather than them working on the data
and saying, is it working?
I want to transform that to, is it going to work?
That's the big differentiation and the product what we talked about, you know, that's the
second facet of it, does it make sense so far?
Yeah, no, it does make sense and, you know, I think the transformation that you describe
is kind of going, you know, maybe it's a different cut at that defensive versus offensive
and a lot in another way or put another way, you're trying to get teams to stop building,
you know, rear view mirror analytics and start, you know, building analytics that predicts
what's, you know, going to be happening in front of the windshield.
You go, I think you nailed it better than me, right?
It says the right way to explain it, right?
Yeah, and I think that that is, you know, that transformation is something that's happening
very broadly in, you know, industry, not just in technology companies, but also in enterprises
and it's, you know, that need to look out the windshield and not be stuck, you know, reading
reports that took weeks to create, that reflect the previous quarter and aren't really even
relevant anymore.
I think that's why, you know, enterprises are kind of grasping onto, you know, machine
learning and AI-based solutions as a way to kind of give them that forward-looking view.
Yeah, and I will add one more thing Sam here, so also, you know, when you think about
any enterprise, any startup, any technology company, at the end of the day, you know, all
the work is done by the people and you have the limited resources.
And you know, you are building a product, rather build a product which is going to work
in the marketplace.
Like, no one has this magic wand to say this is going to work, but this, this front,
you know, this offensive strategy or, you know, whatever way we want to say, it helps
us to align our resources in the right direction, too, so that we can change the direction of
our ship going in the true north rather than, you know, going the south direction and
then bring it back.
Right.
Right.
Before we dive into the platform that you built, you know, one thing does strike me is
that, you know, perhaps more than some other companies truly is, you know, truly is product.
This offering is data, right?
I'm not making some assumptions, but I'm assuming that you're, you know, sourcing, you
know, a bunch of different fees, and you even describe some of these, you know, you're
MLS listings, your county, you know, data feeds, maybe pulling in from good schools and
other sites that are producing aggregate data on, you know, schools and crime and all
these things.
Like, your fundamental data is so fundamental to the thing, the things that you do.
Before you even get to how do you kind of, what have you built and what have you learned
about aggregating all of this data and a little bit of a, a little bit of context for this.
I often hear or, you know, I recently produced an event called the Future of Data Summit
and we had speakers talking about, you know, different aspects of AI and several of them
got up and said, you know, well, in order to do, you know, machine learning and AI, you
have to have the data.
And I think that's true, but, you know, it kind of glosses over the fact that sometimes
you have to get the data, not just like, it's not just sitting there waiting to be explored.
You have to go and find it.
And it seems like a lot of what you did is, is going to find it.
And so, you know, how did you, you know, to what extent does your team get involved in
that and what's your platform for enabling that?
Yeah.
I'm so glad talking to you, right, because you are nailing down exactly the points where
I'm passionate about.
So when you, when you think about there are two pieces to the data and I'm going to make
it very simple, first to start with, like when someone goes to Google and they search
on a Google, the first thing what they're doing is they are giving the search engine
on their intent, what I'm searching for.
And then Google has this content, which is, they went ahead by crawlers and all those
things by building the relevancy and all those things.
So consumer gives the intent, Google has this massive databases of the content and then
the magic in the middle, which takes the intent and content and matches up, which we call
as a relevancy and give it to the consumer, where consumer feels happy about it, right?
Right.
Now in the same context, Julia also had the two parts of the data.
One is the consumer for whom we are building this product.
And then the content where we get this content from.
And I think, you know, this is the listings as you talked about.
This is the public records, which you talked about.
Now, schools data, the crime data, you know, the commute data.
I think it is, that's the difference between 1999 and 2017, where we have the technologies
like real time messaging systems like Kafka, we have strong topologies or the streaming
systems.
We have those Hadoop or Spark technologies where we can make it easier to ingest those
data into our system.
So we have, and this data is pretty open, right, I have written on my blog, roughly we
have 2.5 million active four cell listings on our system.
So across US, you have agents, those who are working with consumers to sell their home,
they enter this information into analysis, how this data comes in from analysis to our
system, then you know, when you sell your home, when you buy your home, you pay your
taxes, you have these assessments and the taxes, which are going into the counties, how
we get this data.
So I think my team involves at the end of the day, whatever you see on Julia's side,
it is my team's responsibility to use technologies to bring this data in the raw form first.
And then enrich this data, because when you think about, you know, you are MLS 1 and
you will come and say, 1, 2, 3, 4, first street and you can spell first street as FIRSD
or someone just come and write number 1 first street.
So we have to have this magic in the middle to join all this data and say this data is
for this property.
And then once we know that, you know, the geolocation, when we had the address cleansing,
address normalization, and then when we work on the enrichment piece, enrichment pieces
for this listing, this is the historical information about the property, this is when
the property was sold last time, this is the Texas information, this listing is 2 minutes
away from the public transit system, this is the school.
And then we go through this enrichment process.
Once we had that enrichment process, it goes into our indexes, which is, you know, we
use our solar technology and we have built, you know, our API layers on top of it, which
can take up to 10 to 15,000 requests per second to serve our front end technologies like
web apps and, you know, mobile web or whatever it is.
So what I explained it to you on surface, it looks like a big process, which takes days
and days and days, interestingly enough, when the listing hits the marketplace, by the
time it goes from one point with the enrichment to the front end, it is less than 15 minutes
where we show the data.
So this is all because of the technologies what we have enables us to give this content
to consumers faster.
So I just talked about the content piece, which is, you know, all the data flowing around
and I think most likely, when you're ready, we will jump into the intent piece, which
is the personalization and all of us, but does it make sense so far?
It does.
It does and I still have tons of questions on that content side.
Um, so thinking about the various ways that you likely get data, I'm imagining maybe
three and I'm there probably many more, but I'm imagining, you know, some data is coming
you via feeds.
Maybe this is like the listing, some data is coming you, coming to you via streams and
some data is coming to you via, um, in batches, um, like, can you characterize like how much
of the data is each and are there cat, is there a category that I'm missing in, in this
and, um, in, and then like where do you, you know, where do you land it?
How much of, you know, what you're doing is, you know, real time, stream based kind
of, uh, processing?
Um, yeah, so I think it's, it depends upon the set of the data, like, so when you think
about the listings, listings are majority of our listings are the stream based, which
are real time, because you know, listings, here's the market, but then when you think about
the public records, which is these assessments, Texas information, that is mostly the batch
based, what we have, and then you have the school data, which is, you know, not, it's not
changing on a daily basis, that's a feed based.
Then you have a crime data, which is, you know, more the streaming thing.
So I think it is, it depends upon the data set, what we, so we have the technologies where
we define, if the data needs to be refreshed more frequently, we use the streaming technologies
and otherwise, you know, we use batch based systems.
We have invested in building our, some of the systems uses the Lambda technologies.
So this is, you know, the real time plus the batch base on an ugly basis, we run the
full Lambda and then make sure that there is, you know, the accuracy on the quality of
the data being implemented.
There are some places we also use Kapa.
I don't know if you heard about Kapa.
So Kapa is also, you know, the real time, but the batch base.
So I think it's at the end of the day, my team have built those pipelines.
Some pipeline uses, you know, the Kafka messages to strong typologies, the streaming technologies.
Some places we have the spark where we need the data to be processed much more faster.
So I think it is at the different data set, at the different refresh SLA and based on those
refreshing SLAs, we tend to, you know, bring it to our systems.
And before we move on, why don't you give us a brief overview of Lambda architectures
and you mentioned Kapa as well.
Yeah, yeah.
And you mentioned strom as well, this is streaming and having come across those.
Yeah.
So I think let's start with the streaming, right?
So I think what's happening is the streaming, the real time streaming and the processing.
So, you know, when we have those messages, so think about if there are 2.5 million active
listings are there across, you know, US, when they hit our system, they're coming into
our messaging layer and there are different messaging technologies are there.
We are using Kafka, Kinesis, mix of that.
From there, we move into the streaming and, you know, the streaming can be spark or it
can be a strom topology.
There is a place where we use strom topologies because when the listing hits, remember
early on I was telling you that we need to do geo cleanse address cleansing, address
normalization and then, you know, the enrichment.
So this is where the goals that we had the spouts and the goals of our strom topology where
spouts are the when which is ingesting the data and then, you know, the goals are the
one which are making the decision making, you know, let I have to perform step xyz.
So the strom topology helps us in the real time, take the stream of the data, perform
the enrichment, perform the cleansing and then go and persist it into our new messaging
layer from the data can be sent over.
So that's the strom topology.
Lambda is, you know, Lambda is being there in the marketplace for long and what Lambda
means is, you know, look at the on a daily basis when we are getting millions and millions
of messages and, you know, it comes into our system.
It is very important for us to maintain the accuracy and the quality of the data.
So on a nightly basis, we rerun the whole data set what we have collected on this day
just to make sure that if there are gaps, we fill those gaps using the Lambda architecture
and which will give us the higher level of accuracy and the quality of the data coming
into our system.
So the only difference is in, in Lambda, you have to write your code base differently
to consume the batch base.
But when you move into the Kapa architecture, you don't need to write the separate code
base.
You can have the similar code base which is used during the real time streaming and you
can use the same code base for your batch based typologies also.
So Kapa enables you to do that.
So that's how we use those three technologies which I just talked about.
Okay.
All right.
Great.
We'll include some links to these and the shun outs.
I've come across Lambda architecture before, but Kapa architecture is new to me.
Yeah, it is coming pretty new in the marketplace last couple of years.
I see people using it move.
Okay.
Great.
So you've ingested all of this data and you've used technologies like Lambda and Kapa architecture,
it's Apache Storm and other technologies, Kafka, Q's and messaging and all these things
to get all this data in to enrich it.
And where do you, where do you land it?
So if your question on the landing is, where do we persist it?
Right.
Yeah.
So we persisted in our solar index.
So solar is the search technology.
And when you think about, you know, we have this millions and millions of rows coming in.
Do we need to, if there are 100 attributes in a row, do we need to persist everything
in the solar?
So we basically, all the searchable things we stored in our solar and then the things
which are not searchable, which are just an augmentation of the data can go into any
of the no SQL databases or some places where we feel the data is much more structured and
we don't need, we use the relational databases also where the volume is pretty small.
So we use my SQL and that's how we persist.
So we use, you know, solar, HBase, Redis, DynamoDB, MySQL and I'm pretty sure, you know,
Aerospike is another one which we recently started using the key value pair systems.
So we use a very wide variety of the databases here in Trulia.
And again, it all boils down to the use cases where do we need to store what?
Right.
Right.
So the individual teams that are working on, you know, given products that are surfaced
through the site and they choose whatever data store makes the most sense for their use
cases.
Is that right?
Yes.
Yes and no.
So right.
So for example, you know, if we know the latency is a big thing for us, then storing that
in HBase may not make sense.
So people may decide to use, you know, Redis or they may go with the DynamoDB.
So I think we, we have the some guidelines around like the biggest thing for us is build
the databases and see the latency, right, because we have the API.
We have abstracted all the data as an API layer on top of those systems so that when
front and team comes and says, give me all the data for this listing, then this API goes
across the different systems or the databases to bring the data, stitch it together.
So latency plays a major role.
But to some extent, what you were trying to say, yes, the decentralization of teams definitely
enable us to have teams pick the technologies, what they want to pick.
We don't put so many guidelines except for the latency as one of the prerequisite making
sure that we pick the right technologies.
Okay.
Okay.
All right, so then all that in place.
Let's jump into the personalization platform and the stuff that you're doing on top of
it.
Here you go.
That's the fun piece, right?
It's a, right.
I really love that piece, I think.
Now we, what we talked about in last few minutes was mostly around the content, right?
So now we need to start thinking from an intent point of use.
When consumers, they come to Trulia, you know, when they are interacting with our website
or mobile app or mobile web or email at any given moment of time, what we have seen,
our consumers are generating those signals.
And the signals are nothing but their intent, you know, deep is looking into, you know,
a listing in no value of a San Francisco, which is in quite neighborhood.
That's a signal.
And then deep is looking into photos, you know, and what kind of the photos deep is looking
into.
So we have this stream of data flowing into our system, what signals and what we internally
call those as an events and events are generated by consumer interacting with those product.
So we basically take those events and our personalization platform and, you know, collect those
events.
The events are just think about, you know, if Sam goes to Trulia and you look into some
site, you know, on an average, like when you look into a specific property, Sam is going
to generate an average of 20 events, you know, within few minutes of your interaction.
So we have this again, the real time messaging layer, which collects those signals and,
you know, we have Trulia has millions of consumers, which are active on a monthly basis.
So when they send those signals, we bring it to our Kafka layer.
And from the Kafka layer, we basically brings it again, we use a streaming technologies
like Spark or Strong again for the intent site of the technologies tag too.
And this is where either we have the real time machine learning models in place or we have
some aggregated systems where those signals are getting evaluated, right?
Okay, we just see deep or we saw an anonymous consumer, we take all those data and then
we persist it into our caching layer where that caching layer, which can be, you know,
the edge base is our persistence layer for all the personalization platform, but then
the caching here is the RERIS, what we have.
Okay.
So if Sam is pretty much active on our site, then Sam moves from edge based to RERIS.
That's how we make because of the latency.
So at the end, you know, this personalization platform stores, Sam's unique preferences,
search criteria is, you know, Sam is looking into your Sam owns this two bedroom, three
baths in St. Louis area in a quiet neighborhood, Sam is looking into this.
So I think that's the personalization platform is a very foundational aspect, which drives
rest of the other thing.
Then on top of, I'm going to move over to machine learning systems.
Is that fine now?
Sure.
Yeah.
So when you think about this personalization platform is put into place, which is like an
engine, which is working on a daily basis by itself, our first machine learning platform
is computer vision and the deep learning.
This is where we, we've been leading this industry in the computer vision and the deep
learning for years, where, you know, computer vision, right, it's a system which we have
built, where we have, you know, trained our systems, machines to look into photos and
they can see, oh, I'm looking into a photo of, you know, the swimming pool or I'm looking
into a photo of a kitchen, which has a granite countertop.
So that's the computer vision, what we have implemented and then what we do is all those
unique attributes, the data, which comes out of the system, powers our home page and in
our home page, you will see what we call as collections.
The collections are nothing but the group of properties, which we bring it together.
So you may see collections like, you know, homes with swimming pools or home with remodder
homes or homes with kitchen, granite countertop.
So those are the collections, which we powers our home page and the more our consumers
engage with these collections, the more inside we get into our consumers.
So that's the one use case of our computer vision.
The second use case of our computer vision is, you know, I'm pretty sure you, me and
all the consumers when they start their home buying journey.
The first thing they do is they come into site like Trulia and the search for neighborhood
then they go to a listing and then they start looking into the photos of the home.
That's how the journey starts.
And if those photos are not engaging and if those photos are not telling story, consumers
are going to lose their interest and they will keep moving into the second and third.
So what we have done is we using a conventional neural networks, CNN models, we have invested
in understanding, you know, the scene types of the photos, whether the photo is appropriate
or not, like some, someone can just put a photo of a dog.
So we say great, you know, we can see it is not a photo of a home, it is a photo of
a dog.
And then the quality of a photo is this photo is blur, is this photo is much more clear.
So these three things, what we take out from our CNN models is the quality of a photo,
appropriateness and the scene type, we score those things and then the highest performing
photo, what we call as our hero image.
So what we do is most attractive photo, when you start your journey, we put the most
attractive photo for you first, so that your engagement becomes much more better with
Trulia.
So that's the second use case of our machine learning computer vision.
And what we have seen by investing in those technologies, you know, there are double digit
increase in inquiries for our listing.
So that's the one piece, make sense over.
And is the, is that lift based on, do you think primarily just getting the right listing
in front of the, the right person who's likely to like it or is it, you know, getting rid
of the, or kind of suppressing the listings that aren't, you know, good in general, you
know, is there any one factor that drives the kind of results that you've seen?
Yeah, so I think our relevancy is driven mostly by the consumer's behavior, what consumers
are interested into.
And so we basically just based on the consumer needs and this is where the personalization
platform comes into a play to drive that computer vision on the serving side, what to
serve to the consumer.
Mm-hmm.
Makes sense?
So, yes.
Two different users, you know, say my wife and I are kind of collaboratively shopping for
a home as husbands and wives tend to do.
You know, she might, when she goes to the site, she might see pool pictures first and I
might see kitchen pictures first or what have you, depending on what our, what our interests
are.
And these aren't interests that we've explicitly shared with you their interests that
you've derived from the various, you know, signals from watching the way we interact with
the site.
That's fair.
And that's how basically the more you engage, the more we know about you, because if you
come for the first time, we really don't know about you, right?
It's basically we need to reach enough confidence level to serve you the right content.
But yes, your assessment was pretty good.
Mm-hmm.
It's funny, I can't help, but the thing that I, as a, as someone who travels a lot and
as a result uses Yelp a lot, I am always complaining about just how dumb the Yelp app is.
And I don't think I've ever done it on the podcast before, but I wish they were doing
more of what you're doing.
When I land in a city, I pretty much like open up Yelp and like type in Thai or type
in in every time trying to find a place to eat.
And I always wonder like, why doesn't it just show me what it knows that I'm going to
be looking for, what it should know that I'm going to be looking for.
So maybe I'll use that rant as a segue into like what are the challenges that you've
seen or what do you think, you know, what's the barrier to, you know, more companies, you
know, having technology that enables them to better know and, and personalize to their
customers.
So just to understand, so your question is, what is the biggest barrier to investing in
this kind of technologies?
Yeah.
Yeah.
I think it is mostly around making sure, remember early on, we talked about the data driven
product companies that how do you understand the strength or the data what you have with
you?
And I think it's, it's the first it needs to start from the top level, the commitment
from the top level, that's the area we want to invest in.
And the second thing Sam is rather than boiling the ocean, right, or let's solve all the problems
in one go, pick the small use cases to evangelize within organization so that, you know, product
people and the other stakeholders can bought into those concepts because, you know, AI or
the machine learning is still in very infancy stage, you know, we have not reached the point
where everyone understands.
So my recommendation to the people is definitely, you know, bring an evangelist, build a small
use cases, show the value prop, back it up with the data, build slowly and gradually build
the tsunami.
And when this tsunami is going to hit, then everyone is going to bought into this.
So that's the way what I look into.
Yeah, that's a great articulation of the process.
So you guys have also done some writing on your engineering blog about how you use natural
language processing and in particular natural language generation.
Can you talk a little bit about that use case?
Sure.
So yeah, so I think, you know, thinking about, so it all starts again for us.
We don't start from thinking about machine learning first.
We always start from thinking about the consumer first.
That's the number one goal.
And what we started seeing, there are thousands and thousands of cities or neighborhoods across
US when consumers come to our side, you know, they're looking for more information about
this side.
They're looking into more information about that neighborhood.
And we said, great, now we, you know, how can we use the data, what we have to build
the story and one way you can do is, you know, you can have the human beings as editors
and let them write the stories about the cities and the neighborhoods.
But when you go into this kind of a massive scale, there is no way that's going to work.
And that's where we said, okay, great, now let's rely on the machine learning technologies
to solve that problem.
And this is where we leaned on our natural language generation system.
So what we do is, we look into a location and we have built this feature extractor.
The feature extractor look into, you know, what are the restaurants close by?
What are the commute systems looks like?
Is the price going up for that neighborhood, is it going down?
So we extract the features.
And then once those features comes out, what we have is a document planner, which looks
into the features we have a document planner.
But before we go into a description generator, we have built our content bank.
So think about the content bank is where we build the sentences based on the features
that if we say this, like this neighborhood is a Victorian style homes.
So our content bank is going to have a sentence which will say Victorian, this neighborhood
has Victorian homes.
So we have this content bank and that content bank is built based on some of the crowdsourcing,
which definitely we do, but we use our data mining and machine learning technologies to
look into the data to build those content bank.
So now think about the document planner coming out for a neighborhood or a city or a specific
location, which has all the feature sets.
We have a content bank and this is where then we use the description generator.
So description generator, take the document planner, take the content bank bank and use
the NLG to generate the content for that specific location.
So that's how we use NLG and it's been going great on that front.
So what I think I hear you saying, and this can be instructive to folks that want to
use this, is that as opposed to trying the thoroughbunch of data to some natural language
generation system and hoping for it to generate something that makes sense, you guys have
broken up the problem and structured it in such a way that first year, you're identifying
the, you call them features of a given neighborhood, maybe not features in the sense of a training
of machine learning algorithm, but they're just attributes of a neighborhood and you kind
of structure your descriptions so that you will highlight one or more of these attributes.
And then the content bank, what I thought I heard was that you kind of have a set of
templates or rough structures of the way you talk about different things.
So you kind of have a template for how you talk about, you know, a neighborhood composition
in terms of its architecture, maybe some templates for restaurants, things like that.
And then, you know, all of that, you know, those attributes that you decided to highlight
in a given description and these, this set of templates are utilized by this description
generated to create something that, you know, sounds more human and is more readable and
usable than, you know, what you might get if you just threw all the data against a neural
net of some sort.
Yeah.
One clarification, the feature is a structure.
It uses the data mining technologies to extract the attributes what you're talking about.
So yes, it goes into a neighborhood.
It uses the data mining in generates attribute and you're right spot on on the content bank
in a simple form.
You can think about the templates or you can think about, you know, which defines the much
more vocabulary, which is easily understood by our consumers.
Mm-hmm.
That's great.
That's great.
Well, I know you're bumping up against a time constraint here.
I think this is a great, you know, use case, we spent a lot more time on the, you know,
data engineering, data acquisition side than we usually do on the podcast and, you know,
I enjoyed geeking out a little bit on some of that stuff.
But it sounds like you guys are doing really, really awesome things.
And so thank you so much for being on the show and sharing them with us.
Great.
Thanks, Sam.
All right.
Thanks, Deep.
Bye-bye.
All right, everyone, that's our show for today.
Once again, thanks so much for listening and for your continued support.
Don't forget to leave your review or comment and there are one year anniversary listener
appreciation content.
The full details can be found at TwomoAI.com slash birthday.
And of course, you can leave your questions and comments over on the show notes page at twomoai.com
slash talk slash 2525 for your final links to deep and the various resources we mentioned
in the show.
Thanks so much for listening and catch you next time.
