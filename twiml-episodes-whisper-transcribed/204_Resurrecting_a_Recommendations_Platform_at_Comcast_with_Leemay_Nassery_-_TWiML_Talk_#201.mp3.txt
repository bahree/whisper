Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting
people, doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington. For those challenged with promoting the use of machine learning
in an organization and making it more accessible, a key to success is to support data scientists
and machine learning engineers with modern processes, tools and platforms.
This is a topic we're excited to address here on the podcast with the AI Platforms podcast
series that you're currently listening to, as well as a series of e-books that we'll
be publishing on the topic.
The first of these e-books takes a bottoms up look at AI platforms and is focused on the
open source Kubernetes project, which is used to deliver scalable machine learning infrastructure
at places like Airbnb, booking.com and open AI.
The second book in the series looks at scaling data science and ML engineering from the top
down, exploring the internal platforms, companies like Airbnb, Facebook and Uber have built,
and what enterprises can learn from them.
If these are topics that you're interested in, and especially if part of your job involves
making machine learning more accessible, I'd encourage you to visit twimbleai.com slash
AI platforms and sign up to be notified as soon as these books are published.
In this episode of our AI Platforms series, we're joined by Lima Nasseri, senior engineering
manager and head of the recommendations team at Comcast.
Lima spoke at the strange loop conference a few months ago on resurrecting a recommendations
platform.
In our conversation, Lima and I discuss how she and her team resurrected Comcasts Xfinity
X1 recommendations platform, including rebuilding the data pipeline, the machine learning process,
and the deployment and training of their updated models.
We also touch on the importance of A-B testing and maintaining their rebuilt infrastructure.
This is a very fun interview, which I know you'll enjoy.
And now on to the show.
Alright everyone, I am on the line with Lima Nasseri.
Lima is a senior engineering manager at Comcast.
Lima, welcome to this week in machine learning and AI.
Hi Sam, thanks for having me.
Also, why don't we get started by talking a little bit about your background.
You've been at Comcast for nine years, and as of about 14 months ago, you have been leading
the recommendations group there.
What are you doing in and around recommendations at Comcast?
Yeah, sure, definitely.
Yeah, so I've been here for a while, most recently, so the past 14 months, like you've said,
I switched over to leading the recommendations team.
So when I did that switch, it was a really small team.
Other companies, their recommendations team, or really their personalization team, is
one of the most important and biggest part of their content discovery platforms or content
discovery teams.
At Comcast, that wasn't quite the case.
About 14 months ago, it was about three people.
The main focus, unfortunately, at the time was just training a model and then deploying
that model in production to surface predictions or to surface what people should watch on
cable in one way or another.
It was a one size fits all model, so essentially, we had one model that did everything.
So regardless of the context in which the quote unquote predictions were being served,
we still use the same model, and we didn't tweak it as much as we should, based on where
it was being surfaced on X1, X1 being our content discovery platform for cable.
So anyway, so it was a small team.
So it was pretty much like an engineer on the team, less management, more engineering,
which was fun for me, and it's tripled in size since then, which is great.
But the goal of when I switched over to that team was to essentially resurrect the recommendations
platform, which sounds kind of morbid, but I like the eliteration, so that's the name
of my talk, strangely, resurrecting a recommendations platform.
Nice.
And so the focus of your talk is, what, how you kind of went in, a commando style and
like, essentially, essentially, so the focus of the talk is really building a end-to-end
machine learning platform.
And coming from my background, I majored in computer science, and then I didn't obviously
start in AI and machine learning.
I instead started in the web service tier, so I built web services that got millions
of requests per day, as you can imagine, like we get the scale at Comcast is pretty high.
So having that background of building web services and platforms that scale really helped
this team to go from simply having this really old school platform where it was a bunch
of machine learning jobs that ran on physical infrastructure that took hours and hours to
complete, to building that to be more event driven and real time updates, and also like
owning the end-to-end data collection, you know, requests from the client, everything.
That's essentially what we did.
We went from owning just the model to owning, surfacing the recommendations, training the
model, evaluating the model, and then having multiple models in production.
I thought it was interesting when you describe the world that you walked into when you
transitioned into this role as, unfortunately, we just had a model and we're making some
recommendations with machine learning, like a lot of companies are, you know, trying
to get to that.
Right.
That's a really good point.
I think the unfortunate aspect of that was, as an engineer, the code and the infrastructure
was not where I wanted it to be.
For example, and you'll hear this in so many machine learning talks where they talk about
the end-to-end, the infrastructure is probably the most important part.
And at the time, we had like a 13-node cluster running community addition of our Hadoop distribution.
So if there were any problems, we couldn't call anyone.
And again, it was MapReduce, which, you know, one significant benefit of MapReduce is
all I owe.
So as long as you have space, you really fail.
But again, that meant that the jobs ran forever.
I mean, we had a job that we recently deprecated, which was exciting, that literally took 18
hours.
So it's still responsibility was to sessionize data so that it could feed our model.
Wow.
Yeah, 18 hours.
I mean, imagine what you can get done in 18 hours, and this job still wasn't done.
Wow.
Right.
So walk us through this process of resurrecting this recommendation platform.
Yeah.
That sounds good.
So kind of in my mind, when I first took this on, I broke it down to four main parts,
the data platform.
So we wanted to build a proper data pipeline to consume the data into our ecosystem so
that we could easily aggregate those usage events of what customers are doing on our
platform so that it could feed a model.
And you'll ask anyone that does anything with machine learning, the data is sometimes
the hardest part.
Sometimes it actually hurts me to hear, I'll hear researchers at Comcast say that they're
working on a model.
And then I hear them talk about the pain that they take into creating the training data
set.
So that's what we were trying to abstract.
So the data platform was the first part.
And this is without going too deeply into this part before you walk through them.
I'm imagining that much of what you're dealing with is kind of the traditional telco, cable
service provider type of data sources, provisioning systems, and that kind of thing.
Or are you dealing with any of that stuff or is this?
Definitely not.
No, it's actually, I think it was a lot of, it's what people are doing on the platform.
So it's just the interactive side, you don't have to talk about that.
Yeah, like what they're clicking on, what they're watching, what they're not watching,
what they get surfaced, but then are ignoring.
So it's our content discovery.
So which do they watch on TV?
Some more of that side.
Definitely not the old school cable side.
Okay.
I try to stay away from that part.
So yeah, so breaking it up into four pieces, there's the data, the data tier.
And then there was just orchestrating the machine learning aspect.
So easily orchestrating the key steps that are obviously required to build a model and
then foresee that model into production.
That was actually one of my goals which we achieved was to have more than one model.
And that's not for a big company like this, that's not a crazy thing to have, to have
a model that's fine tuned to suggest content that is available for rent versus having a
model that's fine tuned for suggesting music recommendations for customers that have
a higher propensity to watch music videos on on Comcast.
So orchestrating the process and training, evaluating, and then of course deploying.
The model was the second part that we had to do because previously we lived in a world
where we just had one and we just had to make sure we had to keep the lights on that
one machine learning, like quote unquote, that one.
So yeah, so the ability to experiment and deploy was the second thing that we wanted
to tackle.
And the third part was AB testing.
So before I switched to the recommendations team, I actually worked on a big data platform
which I could use that experience for this team.
So it worked out fairly well.
But while also on that team we built an AB testing platform which meant we essentially
created a system that tagged accounts.
And then if you tag like or if you tag users then you should be able to give them different
types of experiences based on that tag, you know, super high level, AB testing logic.
But what was ironic is we built that and then we had nothing to AB test.
So the luxury of moving to the recommendations platform is there's so much to AB test.
So I pulled that over with me when I made this switch.
And then last but not least was the infrastructure problem that I mentioned earlier.
As we grew as a company, the infrastructure did not grow.
So the more customers that we were getting onto the X1 platform, our infrastructure stayed
the same.
So that's somewhat mind boggling if you think about that.
Like the more data you get, the more you need to scale out your platform.
And one of the changes we made to support that was of course moving to the cloud.
So yeah, at high levels I think those were the four key pieces needed to resurrect a
recommendations platform.
Okay.
Nice.
Well, let's dig in.
Yeah.
We can start at the beginning.
The data platform building out this data pipeline.
We're starting with interaction data, click stream data, that kind of thing.
What do you need from a pipeline perspective to support building these kinds of models
of scale?
Right.
So the biggest issue that we saw with what I call the legacy platform, which is the one
that ran on the physical infrastructure, the MapReduce specific platform.
What I noticed was that we had so much code to get the data in the right format.
And that's I'm sure you'll hear that all the time with machine learning.
It's always getting the data in the right format at the right time.
So what we did is we decided to go the serverless route.
So we have a battalion of Lambda functions that consume the events in real time.
So as the event happens, we eventually consume that into our into our data platform.
And then right at that moment, we transform it to meet the specific requirements of the
platform.
So, fortunately, at Comcast, and I know this is a problem outside of Comcast, allow
the data is originally in plain text.
And plain text obviously doesn't play well with any data processing framework.
So the first step typically is transforming those into a proper schema, which is usually
Avro or some binary format.
And then once we have that, then we start enriching the data so that it has more context,
like simple things, like if they've watched this TV show in rich, it was some metadata.
This TV show is the genre or the year it was made or what language it was in.
And then once we've enriched it with that metadata, the next Lambda function was aggregating
it for the user.
So it was kind of, the goal was to build this seamless event-driven platform that we could
kind of close our eyes and it would just do what we expected.
It wasn't like scheduled.
It wasn't based on scheduling jobs and some ask a ban or oozee or some job scheduler
was, when the event happens, something is triggered and it goes through the process.
I really was not expecting to hear serverless come up in this conversation.
That's really, sorry about that.
I mean, typically pipeline, you know, we're talking about like ETL or workflow engines,
like Airflow or something like that.
This is super interesting that you want the serverless route.
When you are ready in AWS for this application.
No, we weren't at all, which was, it took far longer than I thought, because again,
it wasn't like a lift and shift to AWS, this is our first introduction, specifically
my, at least my first introduction to AWS.
So the learning curve was somewhat steep, but it really paid off.
And I mean, you mentioned Airflow.
So we do utilize Airflow for the second part of the four key parts to building a machine
learning platform.
And that was for the orchestration of when we do what.
But yeah, so I kind of, when I hear the word ETL, it kind of, I get like, it kind of
irks me because it's so much more than ETL, you know, if you build a data platform that,
you know, is avoiding the construct of a pipeline jungle, it could be like somewhat kind
of beautiful, you know.
How specifically does serverless help you avoid the pipeline jungle?
Oh, yeah, that's a good question.
Well, specifically in the Lambda route, what was nice about Lambda's is that very, very
well-managed in AWS, meaning like when, so obviously, data flux should work.
So it's depending on the day of the week, when it comes to content consumption on TV, you'll
see Sundays and Thursdays, the usage is far higher.
So Lambda's scaled in and out for us, which was awesome.
And just the ability to have them be triggered based on some other, like, some other action
was also critical, which so that, yeah, so that's the route.
That's the reason why I think serverless in the Lambda context worked for this data pipeline.
Now, I haven't done a ton with serverless hands-on, but the attempts that I've made
to play with it, it just seemed like a configuration mess.
It was a little bit actually, you know, hard to kind of wrap my head around, like how
you manage these kind of function artifacts, you know, the way that you would manage, you
know, version control and, like, how you tie everything together, like, was there a big
learning curve either?
There was.
So we utilized Terraform, and that helps the ton Terraform and Jenkins to handle the whole
version control and putting the artifact into a place where AWS could reference it.
But I think the incorporation of Terraform made everything that you said a lot easier.
But it did take its time.
We didn't start with Terraform.
We started with, you know, just AWS CLI and it was a big mess.
So going to Terraform route helps a ton.
Now, that's kind of blowing my mind.
And I think of Terraform as like laying down images and containers and stuff like that.
What does that have to do with serverless?
I mean, so you could deploy your land of functions using Terraform, right?
Or you could configure them using Terraform.
Okay.
So yes, I mean, a lot of what we do, it utilizes, like, the goal is never to put anything
in AWS without using Terraform in one way or another.
Got it.
So your functions are essentially configuration, that's the point and you're using Terraform
as configuration management and it's managing the like battalion of functions that need
to be triggered.
Yeah.
Right.
I'm kind of trying to figure out how far to geek out on serverless, like, how far you use.
How did you get your developers comfortable from a tooling perspective with Lambda functions?
Like, is it to the point now where there's kind of IDE support or it's?
So I think the biggest struggle we've had is testing changes locally.
I think that and we're still struggling with that.
I know there are frameworks out there that we could use.
We just haven't had the, you know, time or the resources to experiment with them.
But at some point, we were essentially deploying and making changes in production in our,
you know, quote unquote production platform.
We did live in a space where we had both the legacy and the production at the same time.
So that was a kind of a luxury in that we can make changes to our data pipeline.
There are all these Lambda functions in our AWS production platform.
And then if any, if we mess anything up, we could always rely on our physical infrastructure
to copy the data that we may have, you know, dropped because we made a mistake.
But yeah, I think the biggest struggle was being careful for what we did.
And also the other biggest role was the cost.
I'm there was this one time where we enabled CloudWatch to test one of our Lambda functions.
And we had it on debug mode.
And unfortunately, like after just like three or so hours, it resulted in 20K.
Wow.
Expenses.
Yeah.
It's a lot.
It's a ton.
And it was just four hours.
I remember frantically figuring out, like, how do we turn this off?
Because it wasn't one of my Lambda functions.
What caused that?
You shifted over too much traffic to it or something like that?
Well, we just turned on debug logging.
Ah, okay.
Okay.
And so we're sending a ton of logs to invoke this, you know, we're invoking these Lambda functions.
It was just like the logs that we specifically wrote.
It was all the logs that were created by all the dependencies that are pulled in.
So we were seeing, you know, logs from Apache, logs, everything and its mother was being
logged.
So 20K.
Yeah.
That's a good story.
Wow.
Yeah.
Wow.
Yeah.
Now we're really careful.
But it could have been way worse.
I was, I made a joke with the team that if that were me, it probably would have been like 60K.
Wow.
Because I lost a ton.
And so how about the stringing together of the Lambda functions?
It sounds like you were, you're essentially creating these pipeline in Lambda functions is
presumably AWS makes that pretty easy if you stay within the Lambda environment.
Is that right?
Yeah.
Definitely.
So it's, it's very like action oriented.
So these Lambda functions are triggered based on these defined actions that we specify.
So for example, it all starts with consuming data from Kinesis or Kafka or some event stream.
The Lambda function gets triggered to, you know, pull or automatically Kinesis triggers
itself.
And it reads data from the event stream and then we say, OK, this is the raw because, you
know, data 101 never get rid of the raw data because you may, you know, mess up how
you transform it.
So we stick the raw data into our bucket, our s3 bucket.
And then there's another Lambda function that's, you know, waiting for these events to
be written to that path.
So it gets triggered on that.
So every time there's like a new file that's created, another subsequent Lambda function
gets triggered and it does the enrichment aspect.
And then once, once we've enriched that file, the usage events in that file, we write
that back out to a different part of that s3 bucket.
And then there's a subsequent, different Lambda function that, you know, starts aggregating
the data to start building the feature data set that the models eventually consume, if
that makes sense.
Like events mean, it does make sense.
So I'm going to ask again, a question that I asked earlier, you referred to, I forget
actually the specific wording, but like avoiding a pipeline hell or pipeline spaghetti or
something like that.
You've essentially created some number of pipelines here.
Yeah.
What specifically do you avoid, you know, that you had to deal with before?
Before the data was discopied from one cluster to another cluster.
Not often that discopy would fail or there would be, you know, the data would lack in its
full sense.
Like they had dropped events and we wouldn't know until we realized like, until there was
until our QA team was like, this box clearly watched the sopranos, but we're not, we're
not seeing that in the usage data that's being fed into the training model.
So it was just a lack of visibility is what we gained visibility of the most important
part of a machine learning platform, our data pipeline.
And just being able to, you know, have alerts when we see like all of a sudden, a drop
in events, is it because of our infrastructure or is it because of upstream clients?
We never had that before because again, we didn't, we relied on another quote unquote data
like to provide us that data.
So the kind of, at, at a missity for lack of a better term of the, like ownership, the
functional environment gave you a lot more transparency and ownership of the different
pieces.
And kind of forced you to think about transforming an individual piece of data at a time like
a log entry as opposed to doing big batch runs of transformations or something.
Right.
Right.
Right.
Right.
Yeah.
So avoiding the 18 hour MapReduce job for just doing it in somewhat real time.
And so that's the data platform.
Yeah, so then there's the second part, the like orchestrating of the machine learning machine
learning side.
And that's where we did you, we just, we have started to use airflow, which is great
that you mentioned that.
But yeah, so the key part to that was before it was, there was no way for us to introduce
a new model into our platform.
There was just the one, the one machine learning model and that's it.
And it did everything for us.
It was a super high level.
It was ALS matrix factorization.
We were find it in different ways and we, we somewhat tested those changes, but once
we made a change to like our similarity matrix or anything specific to the model itself,
there was no way of partitioning subscribers to only get this new algorithm.
It's everyone gets it.
So just being able to train a new or new model or change and then evaluate that in an offline
fashion and then push that into production.
And only for like a subset of subscribers was the next big effort that we focused on.
And how did you get there?
Yeah, definitely.
So one part we started utilizing the AB testing platform that I brought from my other team.
And then two, making the, the way that what we had to introduce, which also led to like
a reason why my previous like work history was perfect for this team, we introduced our
own service layer.
So before it was our logic was super really tightly coupled with the client.
So if we think about it in a very high level fashion, this is obviously not how it's
implemented.
When you watch TV, you see a bunch of options on your TV saying, you know, these are
the TV shows you should watch here, the top movies for you.
We were super tightly coupled to that infrastructure.
So if we wanted to introduce a new model, there was no way to tell the, the cable box or
the set up box, hey, go, go get this model for just this account.
So adding a service layer in between us allowed us to be more independent in serving what
we wanted to serve when, if that makes sense.
So you're able to encapsulate the, the business logic of what model to use to respond to a specific
prediction requests at the service layer, as opposed to just kind of serving up just
raw model results.
Yep.
Just in serving raw predictions, regardless.
Yep.
So this is the next layer, which is we started to get context.
We started asking the client to pass this context into what, where this is being served so
that we can later, specifically if we're looking at, you know, a view on X1 or on, on your
cable box, it's like, here, here's all the music videos.
Let's go to the model that's finally tuned for music.
So yeah, so it was a combination of a few things.
It was one being able to get the data in the right format so that we could easily train
it on different data.
So different feature sets to introducing that service layer in between the producing the
predictions and serving the predictions to the client.
And three, we beefed up our evaluation metrics before when we had this model, we really
kind of was flying blind, I think is the phrase.
I'm not sure.
But we had really no idea how our models were performing.
We would run, and this is before I joined the team, I used to say they, some getting
used to saying we, we would train our model and then our evaluation metrics were essentially
just precision.
And it would run once a month or so, which is not ideal, especially for a customer-facing
platform.
So we did spend a lot of time, this is kind of like a weekend project for me.
I spent a lot of time on building apps that surface what are like our recall or precision,
what that was at an hourly basis.
So we could see this historical view of how our predictions are performing.
And then we could use that as a baseline to make changes.
And was that easy to do with your pre-existing models, meaning were they sufficiently
instrumented for you to be able to even report on their performance, or did you have to do
a lot of work to instrument them?
That's a good question.
So it did take a good amount of work to be able to easily grab what we would predict
right now, because what I wanted to do is I wanted to build, or what I did do.
What I built was an online offline recall platform, which essentially all it did was right
now, I'm going to check all the events that people are like, all the TV shows that a subset
of subscribers are watching.
And then right in this moment, I want to check what our recommendations platform is suggesting
and do they match up, you know, basic recall evaluation.
The hardest part was being able to easily be able to check right now what we are predicting
because it was encapsulated like deep in this, you know, legacy platform, which utilized
couch-based.
So introducing that service layer made it easy.
So essentially the evaluation platform that we built, all it does is it hits our service
layer.
We pass it, you know, context for who the user is.
And then it returns for this specific model what we would recommend.
And then we just check that with what they're actually watching.
You mentioned couch-based, presumably you, you know, ran your 18-hour map, or
reduced jobs, created a model, and it started in a database.
Are you hitting a model based in code now, or are you also kind of caching it in some
database data structure?
Yeah.
So right now we're at, we're going to routes, they're, we're going one route where there's
a model that's in base in a, you know, a recommender backend that we incorporate into
our web service, that's work in progress, all the work that we've done since moving
from our legacy platform has been pre-computing recommendations and writing it into Redis.
We've moved away from couch-based.
And only because, again, Redis is more of a AWS managed service, so it's easy to stand
up and scale out.
So yes, that's the goal.
The goal is to also go the route of, of having the model be encapsulated in code and then
also put into our web service, but yeah, we'll see how that goes.
And would you envision in doing that, putting the actual predictions behind a Lambda function
or does inference take too long for?
Yeah, I, I think I would probably keep our service serverless side or a Lambda route in
the back.
I think for now, keeping, I'm a bit nervous on the performance of Lambda functions with
anything that's like customer impacting.
So you built out the capability to use multiple models and benchmark those, the models relative
to the previous system.
You mentioned earlier that part of what you wanted to do on the machine learning side
is just make it easier to experiment with models and with the data and to deploy.
What have you done in that regard?
Yeah, definitely.
So in that regard, to be able to easily experiment deploy, we started Ulyse Redshift.
So one of the big parts of machine learning and any model is your model is as good as
the data that you train it on.
So we started to experiment with what if we, and this is basic constructs that I'm sure
other content discovery platforms already went down.
What if we started to train our model based on different criteria?
So, and this is before we went down the deep learning route where we're playing with
hyper parameters.
This is just the basic our similarity matrix and everything that's like just super simple
machine learning, let's start playing with that.
So what we started to do is first, let's put our data in a place that we can easily access
so that we could train models quickly and not have to spend days babysitting them.
So we put all our training data in Redshift, which is like a big database, I think Oracle,
but for big data.
And what that allowed us to do was start segmenting subscribers based on attributes.
So I could explain one of the models that we recently deployed to production at a high
level.
What we first did is we group subscribers based on their location.
So people in Philly, versus people in DC, versus people in New York.
And then just on those small clusters, we started grouping them based on what they're
similar to watch to each other.
First we started clustering based, segmenting based on locality.
Then we started clustering within that based on what they watched.
And then we fed those accounts into our training, our models to produce recommendations.
We found what that was, the model trained a lot quicker.
We could do them in parallel, so instead of waiting hours and hours for one model to train
based on all of our subscriber base, we had multiple models training at the same time.
And then where we leveraged Redis and our web services that we could easily, based on
the key, write those predictions to Redis for this web service to pick up.
I'm hearing the folks talking about doing training directly against the data warehouse,
whether it's Redshift or BigQuery or something else.
A lot more than I used to, and I don't know if that's because it's just more business
as opposed to academic or more production as opposed to toy problems or what.
I think it's this evolution of engineers, because at least what I've observed is before
there was, you know, research, traditional researchers, and then there were engineers
who kind of had it played a blind eye to everything that went on with machine learning.
I think we're like starting to build machine learning engineers.
And that's like the sweet spot where they can understand the intricacies and the detail
oriented that's required for training models and evaluating them.
But then they also know how to build platforms that can, you know, manage terabytes and petabytes
worth of data to be able to access, if that makes sense.
And so the data warehouse enabled you to kind of slice and thus easily access this data
query language, yep.
Create kind of new features on the fly, presumably, did you do anything in terms of creating
a feature store or trying to achieve feature reusability across models?
Yep.
So we did a few things.
One, we tried to go down the route of, when we created a feature store, we would create
a view.
For some reason, we found difficult creating a view in Redshift at the time.
So what we ended up doing was we would output that data to a different part of S3.
So we have our, you know, our partition, our path in S3 where our bigger training data
set exists and, you know, that data's offloaded or unloaded into Redshift.
Once we came to a point where we've evaluated a model and then we're like, okay, we wanted
to play that production, but we want to save the training, the feature set.
We would output that into a specific, you know, training data set part of our S3 bucket.
And if you're developing new features for a model and you need to backfill, is that
something you do manually or do you have some kind of automation in there?
So the only automation that we've gotten to at this point is utilizing Airflow to continuously
update the tables to the tables in our, what we call our training tables in Redshift.
We haven't gotten to the point where we are automated to offloading the feature data
set, but that's definitely something we'd like to look into.
And then kind of back to this idea of experimentation.
How are you managing experiments in terms of, you know, recording that iterative process?
Right, right.
So there's a lot of metadata that goes along with experimenting, right?
It's the version of the model, the, you know, the time frame in which you extracted the
training data set, you know, was it the last three months or the last nine months or
the last 13 months, the instance type that you use, the GPUs.
So we offload that metadata into dynamo DB at the time, or currently.
And we often reference that when we have to deploy the model's production.
But right now are what we call our, our model metadata store, it's reutilizing dynamo.
Is that a manual process to keep that up to date as you're experimenting?
Yeah, it's one of the steps in our Airflow workflow is once we've went before we start
training a model, we first insert a record that describes like what the model is, here's
the training data sets going after it's typically part of every, it's the first step before
we start training it.
And then the last step is it outputs the time, like how long it took.
And then, you know, the recall about the high level evaluation, the recall evaluation
or the precision that was determined after the predictions were produced.
It's part of the end and flow.
But I didn't, I didn't think I made that connection earlier part of the way you're using
Airflow is to manage the training workflow.
Yep, yep, yep.
And it took us a while to get there before we were super manual.
You know, we would execute a job, wait, and then I'll put this data into a Google Excel
sheet.
You know, like it was, and then we realized, wait, we should do something about this.
And so where there are multiple steps to get to using Airflow, or did you realize the
problem and then, you know, big bang, you've put the Airflow solution in place.
It was fairly straightforward.
In the past, we've, I mean, we've dealt with things like oozy workflows that were traditional
with the Hadoop infrastructure.
Airflow was really straightforward from my perspective, at least.
And it seems to be the best practice what everyone else is using.
I remember we were going down the route of looking at something else and then we had a few
blog posts that were suggesting stick with Airflow, so it's what we did.
For training, do you have multiple workflows for different types of training jobs, or have
you kind of abstracted training as a workflow and you've got different ways to parameterize
that?
We have different, so at the moment, that's a good question of parameterizing.
That's actually the dream would be to parameterize that.
Right now, we have different workflows, but we also have like a lot of different types
of algorithms, because you know, like there's the typical recommendations isn't just producing
recommendations, there's a relevancy aspect to it too, so our relevancy models require
a different workflow than our core recommendations models.
So right now, there are different workflows.
It'd be great to get to the point where they're parameterized, as we'll see, hopefully
one day.
Last bit on this experimentation piece, have you incorporated any aspect of automated hyperparameter
tuning?
That would be great.
Actually, that's one of, that's on our to-do list.
One, our research team would love that.
So we are, as I mentioned earlier, in the previous world, or 14 months ago, we had this
one machine learning algorithm that was one size fits all, it was at super high levels
and ALS, matrix factorization, and what we wanted to do more, especially with everyone's
talking about AI and ML, we've started to go down three routes.
So we mentioned the utilizing, clustering, and recommendations together, so that's where
out that's already in production.
Our research team is going down the route of utilizing deep learning models that are somewhat
based off of word-to-vec, and that's where we want to do exactly what you say with the
hyperparameters.
We're not quite there yet.
We're still very much in the experimentation phase with this deep learning model that's
similar to word-to-vec.
So we'll see.
If it gains momentum, maybe we'll start abstracting that, but as of now, we've seen some limitations,
so we'll see how far this goes.
So if you hit experimentation, deployment, you've talked about server-side and redis,
and then you've alluded to this next step, which is A-B testing previously, but I get the
impression that there's more to it than what we've talked about so far.
Yeah, definitely.
So A-B testing is my sweet spot, just because I built this platform prior to joining
the recommendations team.
And the A-B testing platform that we built, what it does is it allows for in partnership
with our web service, it allows for a certain set of customers to get one model, and then
another subset of customers get a different model.
And then, of course, it always takes a new account breaking out the control.
One thing that I always say when we talk about A-B testing, it's quite expensive.
It does take time, so when in doubt, try to do as much offline evaluation as you can.
And it's also, at least with TV viewing behavior, there are sometimes where you just
you can't run an A-B test, like on Comcast, like working at Comcast.
You can't run an A-B test during the Olympics, the data fluctuates so much.
It's so skewed to Olympics that for that entire month, you can't experiment, right?
So we get that a lot in the TV world.
So when I mean A-B testing is expensive, when we find a good time where data is inflectioning,
the usage patterns are, you can predict, then we execute an A-B test.
And what that platform entails is a few parts.
It's the tagging of putting essentially traits onto accounts, and then associating those
similar traits or tags that are on counts to the models so that the models are only surfaced
for those accounts, or those customers, sorry, I'm talking such cable lingo.
So yeah, so it facilitates that, it orchestrates allowing different models to be available for
different customers.
And then the second part is the metrics aspect, which is deriving analytics on how the
customers are actually interacting with those new rows or those new content, the new content
that's being surfaced.
So your A-B testing is primarily kind of at scale across the client base.
Yeah.
The luxuries that we have millions of subscribers, so the sample size, we could do something
like we're A-B testing something where it's like a million.
So there's never a doubt whether this meets statistical significance because our sample
sizes are so huge.
We talked a little bit about kind of evaluation metrics before in the context of when you
kind of instrumented your previous model, what kind of instrumentation do you have on the
current model and how does that play into your A-B testing system?
Right.
So they're actually, they're very tangential to each other.
The A-B testing framework is more so on the, I guess the front office side of our platform
whereas the orchestration of our models is more of our back office side because if at some
point one of the models isn't performing as we expect, we need to be able to easily turn
that off for customers.
So utilizing what the web service to be able to gate us from doing that, to enable us
to do that to gate different features from different customers, I guess is the key difference.
That makes sense.
So meaning you're collecting metrics on model performance and primarily consuming, consuming
those metrics within the web service tier so that you can take action if predictions
start to degrade.
Yeah, quicker action.
Right.
And it's not like a historic, like for the evaluation of models and typically what we call
the back office side, we have like historical usage, right?
For A-B testing, we really just care of what just happened the past day or the past two
days or the past week.
So keeping that small subset of usage from the recent time frame is a lot easier on the
web service side than it would be in the, if we were keeping historical data.
Got it, got it.
So you're the A-B testing is pure kind of online.
You've got two models in flight, you've labeled traffic and you're serving up different models
and you're just comparing very short-term results between those models.
Exactly.
What do you tend to see in terms of model, shelf life, model degradation, that kind of thing
for these kind of recommendation models that you're building?
That's a good question.
So the biggest, I guess, issue that I've seen is the frequency in which we need to train
these models.
If the customer keeps seeing the same exact pre-computer recommendations over and over
again, it gets kind of old, right?
So being able to frequently update the pre-computed recommendations that are produced by our models
is something that we're trying to experiment on how we could do more often.
For example, waiting higher, what you just just watched for something that you've watched
six months ago.
That's something that we're trying to evaluate how we could not overemphasize something
that you just watched, but some sweet spot where it's just enough where you see the impact
and what you're being served.
The last piece of the puzzle was infrastructure.
Right.
We kind of discussed that a bit with everything that we mentioned with the AWS or public
cloud.
But it was fairly straightforward as the legacy platform that we'd built was primarily
running on this physical infrastructure, and we weren't at the point where we could scale
that out where we could add more nodes to the Hadoop cluster.
So the easiest decision was moving up to the public cloud, AWS.
Of course, with every new decision where you make or you're dramatically changing something,
some aspect of your platform, you find new problems, your old problems go away, but you
get new problems.
And one of those is, it's very easy to spend money in AWS.
It's very easy to lose money in AWS, so that was one of the struggles that we've been
working through.
And another aspect is it's very easy just to start picking new technologies.
There's something new in AWS every month that feels like maintaining the, yeah, at least
maintaining the key tech stack, and then not trying to move too far from it.
Otherwise, you're just going to build this, this ginormous technical debt where you're
continually slaying the learn and then update and then you just realize, like, wait, like
there's this example, we went down the path of utilizing, before we went down lambdas,
we decided to use Kafka Connect for consuming data from Kafka.
And because that wasn't specifically managed by AWS, there was a lot of inherent difficulties
with monitoring and getting the logs and just seeing how it's performing that we realized
we can't just pick any technology.
We have to really think of the technology that we picked since we're in this public cloud
space.
So yeah, so I was, there's actually whenever I talk about infrastructure and I ever make
the case for more budget, I often referenced this, there was this paper that was, that was
published at the NIPPS conference by these Google developers, it was something on the lines.
I've read the exact titles, like hidden technical debt in machine learning systems or machine
learning platforms.
Machine learning is the high interest credit card of technical debt.
Yeah, exactly.
Yeah, exactly.
It's my favorite paper.
It's a great paper.
So I guess that I often reference it when I have to make the case for our cloud infrastructure
needs more budget or you need to do this because it essentially explains that it's quite,
it's really easy to start building technical debt when implementing a platform like this,
regardless of any platform.
And in that paper, they go into detail, the risk factors, you know, data dependency and
configuration issues, things that we've talked about today.
What I also really like about that paper is that it like specifically states that like machine
learning systems have a special capacity for like acquiring this technical debt.
I mean, a lot of platforms that, you know, that was written with code do this, but then
there's an additional set of debt that can be gained from machine learning issues.
And I think moving our infrastructure to AWS, we realize that really quickly.
And so how do you think that the machine learning in the cloud kind of accentuates that?
Well, of course, there's the budget aspect and then even there's just there are easy ways
where we, it's really easy to deploy to the cloud.
And whereas before there were always constraints where, oh, we don't have enough space, like
we're just, we're just going to keep collecting data and then we're going to just delete
the last, you know, the most historical data we have that's not a problem in AWS.
Like, of course, there's glacier and there are ways to archive data, but you can kind
of keep, you know, adding and picking up the latest and greatest, you know, version of
Spark and then just releasing with that and not realize that you're, you're kind of building
this platform that has so many different pieces because it's easy to just hop on and,
you know, spin up an EC2 instance do that and or, you know, spin up an EMR instance to run
this clustering algorithm and then you kind of just forget about it, whereas in a physical
infrastructure, you can only do as much as you're allowed to, right?
Otherwise, right?
Right, right.
You have backlog of jobs waiting.
Yeah, that's really interesting.
There's a story in there somewhere like the double edged sort of agility in the cloud
like you.
Right.
There's friction in the real world, not the real world because cloud is real for sure
at this point, but the traditional, there's friction and inefficiencies, I guess, that
makes you think about the way you do things and cloud in taking away some of that makes
it easy to not think about some of the things you're doing, which leads to debt.
That's really interesting.
Very well said.
Thank you.
Those are essentially the four parts of resurrecting a recommendation platform.
Nice.
How do you characterize the business impact of all this?
That's a great question.
So that was actually the biggest question I had in all honesty when I first switched over
this team about 14 months ago is I just couldn't see the value that these recommendations were
producing.
I mean, I saw that we are recommending content, but we weren't really measuring it.
Were we taking to account what the business wants us to recommend?
So that's something that we've been playing with a lot.
We've been specifically building models that are taking into account what the business
wants.
For example, promoting content that is specifically given to us by a provider.
Let's see if we can figure out the customers who are likely to watch that and include
that in their recommendations, whereas previously it would have just been at the bottom of the
barrel.
We would have never looked at it.
So definitely taking into account what the business goals are we've been trying to consider
on the recommendations team, and also from a business perspective, how they're being
surfaced, there's something that we've been abtesting.
It's just the names of the rows we're serving content.
We're going down the path.
We are on the path in surfacing because you watched Wonder Woman, how does that play against
a row that's surfacing the same content, but it's called top movie picks for you.
So leveraging how the business works and what works best and taking that into account
in our machine learning platform and utilizing abtesting is kind of the big picture to see
if we can increase hours watched or increase engagement by making small tweaks this platform.
Well, Lima, thank you so much.
This is a really, really fun conversation.
Thank you.
Thanks for having me.
All right, everyone.
That's our show for today.
For more information on Lima or any of the topics covered in this episode, head over
to twimmelai.com slash talk slash 201.
To learn more about our AI platform series or to download our eBooks, visit twimmelai.com
slash AI platforms.
As always, thanks so much for listening and catch you next time.
