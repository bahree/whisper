1
00:00:00,000 --> 00:00:16,320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

2
00:00:16,320 --> 00:00:21,480
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:21,480 --> 00:00:31,200
I'm your host Sam Charrington.

4
00:00:31,200 --> 00:00:36,160
Alright Twimble listeners, this is your last chance to join the conversation and share

5
00:00:36,160 --> 00:00:39,400
your take on home and personal AI.

6
00:00:39,400 --> 00:00:47,600
Entries for our My AI Contest close on Sunday, February 25th at 1159 PM Eastern.

7
00:00:47,600 --> 00:00:54,800
So go ahead, hit pause right now, jump on over to twimbleai.com slash My AI, check out some

8
00:00:54,800 --> 00:00:58,240
of the existing entries and submit yours.

9
00:00:58,240 --> 00:01:03,360
Before we move on, I want to give a quick shout out and thanks to everyone who has submitted

10
00:01:03,360 --> 00:01:12,200
a video so far this week, so cheers to you Matt, Bradley, Lawrence and Mohit.

11
00:01:12,200 --> 00:01:17,300
In this episode, I speak with Shreverm Notarajan, Associate Professor in the Department of

12
00:01:17,300 --> 00:01:20,220
Computer Science at UT Dallas.

13
00:01:20,220 --> 00:01:25,060
While at nips a few months back, Shreverm and I sat down to discuss his work on statistical

14
00:01:25,060 --> 00:01:28,500
relational artificial intelligence.

15
00:01:28,500 --> 00:01:33,260
Star AI is the combination of probabilistic and statistical machine learning techniques

16
00:01:33,260 --> 00:01:35,860
with relational databases.

17
00:01:35,860 --> 00:01:40,740
We cover systems learning on top of relational databases and making predictions with relational

18
00:01:40,740 --> 00:01:44,900
data with quite a few examples from the healthcare field.

19
00:01:44,900 --> 00:01:50,420
Shreverm and his collaborators have also developed Boost SRL, a gradient boosting based

20
00:01:50,420 --> 00:01:55,020
approach to learning different types of statistical relational models.

21
00:01:55,020 --> 00:01:59,220
We briefly touched on this along with other implementation approaches.

22
00:01:59,220 --> 00:02:04,380
Alright, let's do it.

23
00:02:04,380 --> 00:02:08,500
Alright everyone, I am here in Long Beach, California.

24
00:02:08,500 --> 00:02:13,220
I'd say sunny Long Beach, California, but it's actually fairly late in the evening.

25
00:02:13,220 --> 00:02:18,820
My first interview here at nips, in fact, the first time I've attended the nips conference,

26
00:02:18,820 --> 00:02:22,620
and I've got the pleasure of being seated with Shreverm Notarajan.

27
00:02:22,620 --> 00:02:28,500
Shreverm is an Associate Professor of Computer Science at the University of Texas at Dallas.

28
00:02:28,500 --> 00:02:31,180
Shreverm, welcome to this week in Machine Learning and AI.

29
00:02:31,180 --> 00:02:33,140
Hey, great to be here.

30
00:02:33,140 --> 00:02:36,340
Why don't we get started by having you tell us a little bit about your background and

31
00:02:36,340 --> 00:02:40,860
how you got interested in machine learning and artificial intelligence?

32
00:02:40,860 --> 00:02:46,260
So my interests are in machine learning, artificial intelligence, primarily in this field called

33
00:02:46,260 --> 00:02:51,780
Statistical Relational Artificial Intelligence, about which we are giving a tutorial tomorrow,

34
00:02:51,780 --> 00:02:54,060
which is why I'm here at nips.

35
00:02:54,060 --> 00:03:00,220
And the application and adaptation of these algorithms to many real problems, focusing

36
00:03:00,220 --> 00:03:07,580
mainly on healthcare type problems, but also on natural language, understanding, finance

37
00:03:07,580 --> 00:03:10,820
types of problems, but mainly focusing on healthcare problems.

38
00:03:10,820 --> 00:03:11,820
How did I get interested?

39
00:03:11,820 --> 00:03:14,780
I think it started from a grad school.

40
00:03:14,780 --> 00:03:20,260
When I came here for masters at Oregon State, my interest was primarily just computer science

41
00:03:20,260 --> 00:03:22,100
in the broad area of computer science.

42
00:03:22,100 --> 00:03:27,700
But I took artificial intelligence courses under Professor Prasad Thali Polly, who went

43
00:03:27,700 --> 00:03:30,860
on to later become my PhD advisor.

44
00:03:30,860 --> 00:03:33,340
And I know I just liked the style of teaching.

45
00:03:33,340 --> 00:03:38,980
I liked the fact that the artificial intelligence and machine learning techniques combined two

46
00:03:38,980 --> 00:03:43,340
of my favorite topics, math and statistics, with computer science.

47
00:03:43,340 --> 00:03:49,420
So the combination of math and computer science kind of looted me into AI.

48
00:03:49,420 --> 00:03:54,020
So my thesis was primarily in artificial intelligence and machine learning, didn't have too much

49
00:03:54,020 --> 00:03:55,020
healthcare problems.

50
00:03:55,020 --> 00:04:00,300
When I did a post-doc at University of Wisconsin-Madison, Professor David Page and Professor Jude

51
00:04:00,300 --> 00:04:06,420
Shavelic introduced me to the possibility of using machine learning and artificial intelligence

52
00:04:06,420 --> 00:04:08,420
to solve healthcare problems.

53
00:04:08,420 --> 00:04:14,180
We are looking at electronic health data, electronic health record data from Marshfield, Wisconsin.

54
00:04:14,180 --> 00:04:20,580
That got me interested into kind of like personalized medicine, now it's called precision health.

55
00:04:20,580 --> 00:04:25,820
So I got interested in adapting our algorithms for these problems.

56
00:04:25,820 --> 00:04:28,420
So I think that's how my journey has happened so far.

57
00:04:28,420 --> 00:04:29,420
Okay.

58
00:04:29,420 --> 00:04:30,420
Great.

59
00:04:30,420 --> 00:04:35,340
And so you mentioned that you're doing a tutorial tomorrow, right?

60
00:04:35,340 --> 00:04:44,180
It's on statistical artificial intelligence or star AI, which is a great acronym.

61
00:04:44,180 --> 00:04:45,180
Thanks.

62
00:04:45,180 --> 00:04:46,180
What is star AI?

63
00:04:46,180 --> 00:04:53,820
So star AI is the combination of probabilistic or classical statistical machine learning with

64
00:04:53,820 --> 00:04:57,740
more logical or relational artificial intelligence.

65
00:04:57,740 --> 00:05:05,220
The idea being that so the traditional artificial intelligence techniques and machine learning

66
00:05:05,220 --> 00:05:09,220
techniques make a lot of assumptions on the data.

67
00:05:09,220 --> 00:05:12,460
They need a lot of preprocessing, they need a lot of engineering.

68
00:05:12,460 --> 00:05:20,060
Whereas what the star AI feels, star AI methods try and do is kind of look at the data in

69
00:05:20,060 --> 00:05:24,380
a kind of a more holistic manner, look at the data in the natural form, in the relational

70
00:05:24,380 --> 00:05:25,380
form.

71
00:05:25,380 --> 00:05:26,380
Okay.

72
00:05:26,380 --> 00:05:29,660
A classic example is most of these machine learning algorithms do what is called as an

73
00:05:29,660 --> 00:05:37,260
IID assumption, independent and identically distributed, which means each of us is described

74
00:05:37,260 --> 00:05:41,140
by the same features or same set of attributes.

75
00:05:41,140 --> 00:05:44,060
And each of us is drawn independent of each other.

76
00:05:44,060 --> 00:05:47,620
And then you learn a classifier, you learn a predictor on top of us.

77
00:05:47,620 --> 00:05:50,020
Whereas if you look at real data, it's not true.

78
00:05:50,020 --> 00:05:54,900
The probability of you having diabetes or me having diabetes depends on your parents.

79
00:05:54,900 --> 00:05:58,660
And in my case, my parents and my parents and so on.

80
00:05:58,660 --> 00:06:00,180
So the family history matters.

81
00:06:00,180 --> 00:06:01,180
Sure.

82
00:06:01,180 --> 00:06:06,100
I can see that's particularly not true in the case of diabetes or disease.

83
00:06:06,100 --> 00:06:09,740
Are there other examples outside of the healthcare realm or that assumption?

84
00:06:09,740 --> 00:06:11,740
Oh, even social behavior, right?

85
00:06:11,740 --> 00:06:17,180
How some, even some small thing, the probability of somebody smoking depends on the fact that

86
00:06:17,180 --> 00:06:20,300
their social network friend smokes are not.

87
00:06:20,300 --> 00:06:26,180
How popular a particular person is, let's say in a scientific field, depends on who

88
00:06:26,180 --> 00:06:31,660
is or her co-authors, his collaborators are, so how the popularity levels of the collaborators.

89
00:06:31,660 --> 00:06:36,100
So if you take classical machine learning methods, they kind of project these multi-relational

90
00:06:36,100 --> 00:06:41,500
data into a single fixed flat form and they operate on that.

91
00:06:41,500 --> 00:06:44,140
Statistical relational learning methods on the other hand, allow the data to be in a

92
00:06:44,140 --> 00:06:45,540
structural form.

93
00:06:45,540 --> 00:06:50,420
And we try to learn using the power of first-order logic and relational logic.

94
00:06:50,420 --> 00:06:55,420
So think of learning in a relational database using statistical methods.

95
00:06:55,420 --> 00:06:56,420
Okay.

96
00:06:56,420 --> 00:07:00,580
So that's why it's called statistical relational artificial intelligence, because it takes

97
00:07:00,580 --> 00:07:05,580
the classical machine learning methods but kind of upgrade them to learn on relational

98
00:07:05,580 --> 00:07:07,700
data, allowing data to be in the natural form.

99
00:07:07,700 --> 00:07:11,580
So you take any data, electronic health records or classic examples, but you look at

100
00:07:11,580 --> 00:07:17,380
social networks or you look at author citations, you look at movie databases, everything

101
00:07:17,380 --> 00:07:18,380
right?

102
00:07:18,380 --> 00:07:23,580
Most real data, now Google stores, everybody stores the data in a relational database.

103
00:07:23,580 --> 00:07:27,620
But somehow when we are learning it, they are transformed into a different format.

104
00:07:27,620 --> 00:07:33,420
They are aggregated, they are somehow shortened, and they are all compressed into one form,

105
00:07:33,420 --> 00:07:36,860
and then the algorithms run on those compressed form.

106
00:07:36,860 --> 00:07:41,500
In our case, what we are saying is, well, the data be in its pure form, which is relational.

107
00:07:41,500 --> 00:07:47,060
Somehow, the models be learned at a relational level, which is why we sometimes call these

108
00:07:47,060 --> 00:07:52,460
models as lifted models, because they are lifted from a flat representation to a much

109
00:07:52,460 --> 00:07:55,540
more rational representation.

110
00:07:55,540 --> 00:07:59,340
Now when you first started going through this, the first thought that jumped out in my

111
00:07:59,340 --> 00:08:01,340
mind was relational database.

112
00:08:01,340 --> 00:08:08,540
But then as you described it from the context of different distributions and kind of moving

113
00:08:08,540 --> 00:08:12,980
beyond the independent, identically distributed assumption, I thought he's not talking about

114
00:08:12,980 --> 00:08:14,500
relational database stuff.

115
00:08:14,500 --> 00:08:17,340
But then you come back full circle to relational database.

116
00:08:17,340 --> 00:08:24,740
Can you elaborate on the relationship between statistical relational AI and relational databases?

117
00:08:24,740 --> 00:08:27,060
So relational databases are representations.

118
00:08:27,060 --> 00:08:31,820
So relational databases are how the data is actually stored on whatever you're storing them

119
00:08:31,820 --> 00:08:32,820
on.

120
00:08:32,820 --> 00:08:37,020
The statistical relational AI says, I'll take the structure of the relational database and

121
00:08:37,020 --> 00:08:39,300
learn a model upon the structure.

122
00:08:39,300 --> 00:08:46,620
So what happens is, so let's just take a simple example in an IID framework in the classical

123
00:08:46,620 --> 00:08:53,100
machine learning framework, each of us, you and I, your father, my father, we all become

124
00:08:53,100 --> 00:08:56,540
individual examples for a classifier.

125
00:08:56,540 --> 00:09:04,100
In these relational models, however, your family becomes one mega example, my family becomes

126
00:09:04,100 --> 00:09:05,100
one mega example.

127
00:09:05,100 --> 00:09:10,380
We may be connected, I don't know, when we go like 20 levels up.

128
00:09:10,380 --> 00:09:15,620
But for whatever data we have, we could be different sets of mega examples.

129
00:09:15,620 --> 00:09:21,180
And we are, so what these statistical relational learning does is learn at these mega example

130
00:09:21,180 --> 00:09:23,940
levels, then at individual example levels.

131
00:09:23,940 --> 00:09:26,300
So how are they related to databases?

132
00:09:26,300 --> 00:09:30,900
If you look at a database and I start, let's say, with you, or let's say we start from

133
00:09:30,900 --> 00:09:36,060
Shreya Ram, that's easier to explain in an academic context, then you go to my advisor,

134
00:09:36,060 --> 00:09:38,500
my advisor's collaborators, my collaborators.

135
00:09:38,500 --> 00:09:43,620
You can form a small network of people around me and basically use that information to predict

136
00:09:43,620 --> 00:09:47,300
something about me, whether I'll be successful, whether I'll, I'm going to have a paper next

137
00:09:47,300 --> 00:09:49,460
year or whether I'm going to have a grant next year.

138
00:09:49,460 --> 00:09:53,660
You may be able to predict that by looking at whom I'm working with, what topics I'm working

139
00:09:53,660 --> 00:09:56,700
on, how popular it is and so on and so forth.

140
00:09:56,700 --> 00:10:00,300
When it comes to relational data, you're basically looking at all the relations that are connected

141
00:10:00,300 --> 00:10:04,660
to me and figuring out how you can make some predictions about me.

142
00:10:04,660 --> 00:10:12,340
This is true, not just in citations, it's true in many of these health care problems,

143
00:10:12,340 --> 00:10:16,900
it's true in natural language problems, it's true in any real problem that you look

144
00:10:16,900 --> 00:10:17,900
at.

145
00:10:17,900 --> 00:10:22,900
You have objects, people are objects and they are related and you have objects everywhere,

146
00:10:22,900 --> 00:10:27,740
people are objects, things are objects and emails are objects and projects are objects

147
00:10:27,740 --> 00:10:32,820
and then we are all related to emails and projects and papers and so on and so forth.

148
00:10:32,820 --> 00:10:35,940
Then you start talking about relations, how are these relations?

149
00:10:35,940 --> 00:10:39,340
Maybe two people work on a project, right?

150
00:10:39,340 --> 00:10:44,380
Two people work for another person and that person reports to an organization and the

151
00:10:44,380 --> 00:10:48,540
organization is managed by somebody so there's always these situations that you can tease

152
00:10:48,540 --> 00:10:49,540
out from a database.

153
00:10:49,540 --> 00:10:54,020
So it's very related to a database and you can think of what we do as learning on top

154
00:10:54,020 --> 00:10:56,820
of a database.

155
00:10:56,820 --> 00:11:03,340
So I'm envisioning then, well I guess there's two directions that I want to go with

156
00:11:03,340 --> 00:11:12,180
as one is the extent to which what you're specifically talking about is from an implementation

157
00:11:12,180 --> 00:11:19,940
related to learning on databases, meaning taking advantage of schemas and primary keys

158
00:11:19,940 --> 00:11:24,500
and things like that, that's one possibility.

159
00:11:24,500 --> 00:11:32,740
But then I'm also hearing something that sounds to me like whereas we might have in traditional

160
00:11:32,740 --> 00:11:41,500
machine learning a set of independent examples in statistical relational AI you've got this

161
00:11:41,500 --> 00:11:48,300
set of examples and kind of overlaid by some connectivity graph and it sounds like

162
00:11:48,300 --> 00:11:55,020
what you're doing in a sense is maybe finding different ways to kind of featureize that

163
00:11:55,020 --> 00:11:58,220
graph and use it in the training process.

164
00:11:58,220 --> 00:12:04,340
That's actually one one's reasonably simple way of understanding what we're doing.

165
00:12:04,340 --> 00:12:08,220
Reasonably simple is a great place to start, that's true.

166
00:12:08,220 --> 00:12:11,580
The reason I'm saying reasonably simple is because we really do not construct too many

167
00:12:11,580 --> 00:12:12,580
features.

168
00:12:12,580 --> 00:12:17,380
We let the features be defined based on the relationships themselves.

169
00:12:17,380 --> 00:12:23,740
So for instance let's go back and take a classic example of movies, I want to predict

170
00:12:23,740 --> 00:12:27,620
that the next Marvel movie, how much money is it going to make and then you want to

171
00:12:27,620 --> 00:12:31,180
look at, well typically people look at history, how things change and so on.

172
00:12:31,180 --> 00:12:34,620
But you could also look at the actors and you could look at the different actors that

173
00:12:34,620 --> 00:12:38,860
are involved in a movie and you could say well these lead actors have typically tend

174
00:12:38,860 --> 00:12:43,500
to get a lot of box office collection and so on and so forth.

175
00:12:43,500 --> 00:12:48,180
So what we are trying to do is we are looking at the fact that one Marvel movie might

176
00:12:48,180 --> 00:12:56,660
have five stars, five big entertainment stars, like big actors and then another movie

177
00:12:56,660 --> 00:13:01,660
on the other hand might be carried solo by a single, you know, Iron Man, is Iron Man

178
00:13:01,660 --> 00:13:03,340
even Marvel or are they decent?

179
00:13:03,340 --> 00:13:04,340
That's Marvel.

180
00:13:04,340 --> 00:13:05,340
Okay, I got that right.

181
00:13:05,340 --> 00:13:11,980
So Robert Downey himself carries one movie with him, but let's say the Avengers are

182
00:13:11,980 --> 00:13:14,540
some other one is carried by five, six people.

183
00:13:14,540 --> 00:13:16,780
Does that necessarily mean that it's better than one?

184
00:13:16,780 --> 00:13:18,140
We don't know that, right?

185
00:13:18,140 --> 00:13:22,580
And what typical machine learning methods tend to do is kind of collapse them into one group.

186
00:13:22,580 --> 00:13:26,500
We on the other hand say well, how about we let the data speak for itself?

187
00:13:26,500 --> 00:13:30,900
We each of them combine some way and I look at the previous Avengers, look at who's directing

188
00:13:30,900 --> 00:13:33,060
who's imparting.

189
00:13:33,060 --> 00:13:36,780
So from that perspective, I'm not doing too much feature engineering, but I'm figuring

190
00:13:36,780 --> 00:13:43,340
out how can I use this graph of people and make that use that to make a prediction that

191
00:13:43,340 --> 00:13:48,020
I cannot with a normal machine learning algorithm.

192
00:13:48,020 --> 00:13:51,340
The other problem is here is the problem, okay?

193
00:13:51,340 --> 00:13:55,380
So let's say you in classical machine learning, you make an assumption.

194
00:13:55,380 --> 00:13:58,100
Well, I'm going to look at the top five actors.

195
00:13:58,100 --> 00:14:02,660
But let's say suddenly Marvel decides to make this mega Avengers movie with 20 actors,

196
00:14:02,660 --> 00:14:03,660
right?

197
00:14:03,660 --> 00:14:07,900
Okay, so you've got to go back to your model and say, ooh, man, we went from five to 20.

198
00:14:07,900 --> 00:14:08,900
Right.

199
00:14:08,900 --> 00:14:09,900
And he's just your model.

200
00:14:09,900 --> 00:14:10,900
Exactly.

201
00:14:10,900 --> 00:14:13,020
You might want to, you cannot generalize that easily.

202
00:14:13,020 --> 00:14:14,020
Right.

203
00:14:14,020 --> 00:14:17,380
We on the other hand make no assumption on the number of individuals, number of objects.

204
00:14:17,380 --> 00:14:20,540
We can say as long as, you know, they're actors in a movie, we're going to use their

205
00:14:20,540 --> 00:14:22,260
information to make predictions.

206
00:14:22,260 --> 00:14:26,980
So that's where the power of relational representations come in.

207
00:14:26,980 --> 00:14:27,980
Okay.

208
00:14:27,980 --> 00:14:28,980
And they're kind of tied.

209
00:14:28,980 --> 00:14:29,980
Actually, you ask me two questions.

210
00:14:29,980 --> 00:14:33,780
One is on the implementation aspect, the one on the, and I don't think they are too different.

211
00:14:33,780 --> 00:14:34,780
Okay.

212
00:14:34,780 --> 00:14:37,860
Because when you think about how these are ultimately implemented, they are in some

213
00:14:37,860 --> 00:14:42,580
sense implemented based on either a logical representation, the power of first-order logic,

214
00:14:42,580 --> 00:14:47,180
as people know for a long time in computer science, or from a relational representation as

215
00:14:47,180 --> 00:14:48,180
a database.

216
00:14:48,180 --> 00:14:53,020
So it's I, so which is why in US, this is called statistical relational learning.

217
00:14:53,020 --> 00:14:56,180
Because most of the work is done on top of relational databases.

218
00:14:56,180 --> 00:14:59,460
In Europe, this is called probabilistic logic learning.

219
00:14:59,460 --> 00:15:03,660
Because most of the machinery underlying is a logical representation.

220
00:15:03,660 --> 00:15:06,860
And both of them do minimal feature construction.

221
00:15:06,860 --> 00:15:09,780
They kind of learn at the level of the databases.

222
00:15:09,780 --> 00:15:13,500
The cool thing is they are statistical and probabilistic because you can all of the data

223
00:15:13,500 --> 00:15:15,020
to be noisy.

224
00:15:15,020 --> 00:15:16,060
So that's what happens.

225
00:15:16,060 --> 00:15:18,220
So our data can be noisy.

226
00:15:18,220 --> 00:15:19,900
Our models are robust to noise.

227
00:15:19,900 --> 00:15:24,700
Our models are robust to changing number of individuals, number of parameters, number

228
00:15:24,700 --> 00:15:27,060
of objects in the world.

229
00:15:27,060 --> 00:15:33,700
And so we learn at the level of objects and relations, not at the level of specific people.

230
00:15:33,700 --> 00:15:34,700
Okay.

231
00:15:34,700 --> 00:15:38,140
And I think that's why these models are powerful.

232
00:15:38,140 --> 00:15:39,140
Okay.

233
00:15:39,140 --> 00:15:41,140
And so how does it work?

234
00:15:41,140 --> 00:15:45,940
How does learning work or how does kind of what's the, you know, the method that you

235
00:15:45,940 --> 00:15:46,940
applied?

236
00:15:46,940 --> 00:15:50,300
So what we do, for instance, I'm going to talk about one specific method that we do,

237
00:15:50,300 --> 00:15:54,300
which is called relational functional gradient boosting.

238
00:15:54,300 --> 00:15:57,980
There is this famous gradient boosting technique inside machine learning.

239
00:15:57,980 --> 00:16:02,540
What we are doing is we are elevating it to the relational setting.

240
00:16:02,540 --> 00:16:06,500
So it's called lifted gradian boosting.

241
00:16:06,500 --> 00:16:07,660
And the idea is very simple.

242
00:16:07,660 --> 00:16:10,980
So let's say I'm interested, let's just take a simple example.

243
00:16:10,980 --> 00:16:13,940
I'm interested in predicting if somebody has a heart attack.

244
00:16:13,940 --> 00:16:14,940
Okay.

245
00:16:14,940 --> 00:16:18,100
Let's look at the database, let's say I'm a positive example in that I have a heart

246
00:16:18,100 --> 00:16:19,100
attack.

247
00:16:19,100 --> 00:16:21,500
Sam, you're a negative example, you look much fitter than I am.

248
00:16:21,500 --> 00:16:24,180
So you don't have a heart attack.

249
00:16:24,180 --> 00:16:29,180
And what happens is my model uses my attributes and it starts to learn.

250
00:16:29,180 --> 00:16:33,940
And then it says, who is the father of this person diabetic, turns out my father is.

251
00:16:33,940 --> 00:16:34,940
Okay.

252
00:16:34,940 --> 00:16:38,060
And then is the, is the cholesterol level of this person pretty high?

253
00:16:38,060 --> 00:16:43,300
If so, then the probability of a heart attack is, let's say, my model comes up with me having

254
00:16:43,300 --> 00:16:44,300
a probability of 0.7.

255
00:16:44,300 --> 00:16:45,860
It's a heart attack.

256
00:16:45,860 --> 00:16:49,020
The same model comes with you to have a probability of 0.13.

257
00:16:49,020 --> 00:16:50,020
Okay.

258
00:16:50,020 --> 00:16:54,540
Now, my, because I'm a positive example, it should say that I have a probability of

259
00:16:54,540 --> 00:17:01,060
1 to have a heart attack, but I said 0.7, which means 1 minus 0.7, 0.3 is the mistake

260
00:17:01,060 --> 00:17:03,180
that the model makes on me.

261
00:17:03,180 --> 00:17:07,740
Your probability should have been 0, because that's, you don't have a heart attack according

262
00:17:07,740 --> 00:17:08,740
to the data.

263
00:17:08,740 --> 00:17:11,700
But my model said 0.13 as your probability.

264
00:17:11,700 --> 00:17:16,780
So, your weight, so to say, is minus 0.13, that is the mistake.

265
00:17:16,780 --> 00:17:22,340
So, every positive example will have a weight of greater than or equal to 0.

266
00:17:22,340 --> 00:17:26,020
Every negative example, like you, will have a weight of less than or equal to 0.

267
00:17:26,020 --> 00:17:30,380
So, what we do is, we learn a small model, regression model, it would be a class which

268
00:17:30,380 --> 00:17:35,820
says, if the father is, father of this person is diabetic, and if this person's cholesterol

269
00:17:35,820 --> 00:17:41,740
level in the last three months is greater than, I don't know, let's say the H.D.L.

270
00:17:41,740 --> 00:17:46,820
cholesterol is less than 30, then the probability of heart attack is this.

271
00:17:46,820 --> 00:17:51,420
You could go do other things, for instance, that says, the relational power comes from

272
00:17:51,420 --> 00:17:56,540
the fact that your class could say something like, if any of his close family members, and

273
00:17:56,540 --> 00:18:02,660
this close family members could mean, my dad, my dad's dad, my dad's brother, my mom's

274
00:18:02,660 --> 00:18:06,700
brothers and so on and so forth, and you will have completely different number of uncles

275
00:18:06,700 --> 00:18:13,820
and aunts as I do, so you could say something like, if a close family member has a predisposition

276
00:18:13,820 --> 00:18:17,540
for heart attack, then the probability of heart attack for me is so much.

277
00:18:17,540 --> 00:18:22,100
And you could say that, because of the fact that I am not defining, I can define close

278
00:18:22,100 --> 00:18:28,660
family member to be either an uncle or an aunt or a first cousin or my grandparents

279
00:18:28,660 --> 00:18:33,060
on either side and so on and so forth, and we are not restricting the number.

280
00:18:33,060 --> 00:18:35,100
You just look into the data and you can learn it.

281
00:18:35,100 --> 00:18:41,980
So what the model does is, it tries to find out these factors that matter, put them together

282
00:18:41,980 --> 00:18:46,620
and make a prediction, and then it finds which mistake, where it makes a mistake, learn

283
00:18:46,620 --> 00:18:49,580
something else to fix that mistake.

284
00:18:49,580 --> 00:18:56,500
So for people who don't have low H.D.L., and whose father don't have, let's say, diabetes,

285
00:18:56,500 --> 00:19:01,260
it could be that they smoke a lot and drink a lot, and for them there is a different issue.

286
00:19:01,260 --> 00:19:05,500
So what the model does is, oh, okay, I got freedom mostly covered, there's only a small

287
00:19:05,500 --> 00:19:09,380
mistake on freedom, but there is this other person who has a heart attack, whom I am not

288
00:19:09,380 --> 00:19:10,380
covered.

289
00:19:10,380 --> 00:19:11,380
So let me focus on that person.

290
00:19:11,380 --> 00:19:16,900
So it kind of fixes these mistakes repeatedly and learns one robust model in the end.

291
00:19:16,900 --> 00:19:21,340
And it scales up pretty well, we have, the code is available online, so people can look

292
00:19:21,340 --> 00:19:25,220
at that from my webpage as well, okay.

293
00:19:25,220 --> 00:19:26,220
Simple things.

294
00:19:26,220 --> 00:19:31,740
So one, you talked about heart attacks, smoking, cholesterol levels, these are all things

295
00:19:31,740 --> 00:19:36,460
that I would associate as being features in a traditional model, but you said that you

296
00:19:36,460 --> 00:19:39,460
don't really get into features as much with this.

297
00:19:39,460 --> 00:19:42,060
How do you, how do you recognize all that?

298
00:19:42,060 --> 00:19:43,900
So I have to make it clearer.

299
00:19:43,900 --> 00:19:47,700
In the sense that these are all features, so everything is a feature.

300
00:19:47,700 --> 00:19:50,700
What I'm saying is we don't really do a lot of feature engineering.

301
00:19:50,700 --> 00:19:55,580
We don't say, oh, for instance, a beautiful question, right?

302
00:19:55,580 --> 00:19:59,220
So let's say you go to the hospital, again, you're looking much fitter than I am, you'll

303
00:19:59,220 --> 00:20:02,340
go to the hospital maybe once a year to do your annual checkup.

304
00:20:02,340 --> 00:20:05,380
I go to the hospital three or four times a year, okay.

305
00:20:05,380 --> 00:20:09,580
Now think about how a classical machine learning algorithm will do.

306
00:20:09,580 --> 00:20:14,780
You will have one measurement of cholesterol, I will have four measurements of cholesterol.

307
00:20:14,780 --> 00:20:21,140
I might have my A1Cs recorded every, actually four, it's good for a year, so four A1Cs

308
00:20:21,140 --> 00:20:25,220
which is every three months, I'm going to record my blood sugar level.

309
00:20:25,220 --> 00:20:28,860
And so I have multiple measurements and you have only one measurement.

310
00:20:28,860 --> 00:20:30,180
How do we do this?

311
00:20:30,180 --> 00:20:33,180
Well, classical methods say min, max, average.

312
00:20:33,180 --> 00:20:37,660
So it takes an average of my cholesterol level, it takes a minimum maximum and puts them

313
00:20:37,660 --> 00:20:38,660
as features.

314
00:20:38,660 --> 00:20:41,260
This is what we call as feature engineering, okay?

315
00:20:41,260 --> 00:20:42,580
We don't do that.

316
00:20:42,580 --> 00:20:47,380
We say, if the cholesterol level in the last one year has ever been greater than this.

317
00:20:47,380 --> 00:20:49,260
And that can be written by logic.

318
00:20:49,260 --> 00:20:54,900
So logic statement says there exists a cholesterol level, this is called quantification in logic.

319
00:20:54,900 --> 00:20:58,620
And so they kind of, so we don't even do that min, max, etc.

320
00:20:58,620 --> 00:20:59,620
They can be done.

321
00:20:59,620 --> 00:21:01,300
If needed, we can do it.

322
00:21:01,300 --> 00:21:03,540
But doing that is a subset of what we can do.

323
00:21:03,540 --> 00:21:07,940
We can just let the data as it is, which says that anytime in the last five years, Srinams

324
00:21:07,940 --> 00:21:14,020
cholesterol level has been this, anytime, after the age of 55, this person's A1C has been

325
00:21:14,020 --> 00:21:15,020
this.

326
00:21:15,020 --> 00:21:16,460
And so we can tease that out.

327
00:21:16,460 --> 00:21:20,700
So what I mean by feature engineering is we don't construct specific features for specific

328
00:21:20,700 --> 00:21:21,700
problems.

329
00:21:21,700 --> 00:21:23,300
You just let the data be in its natural form.

330
00:21:23,300 --> 00:21:24,620
It still sounds like features.

331
00:21:24,620 --> 00:21:29,900
It sounds like the if statement is like a one-hard encoding on whether their cholesterol

332
00:21:29,900 --> 00:21:30,900
has been high.

333
00:21:30,900 --> 00:21:33,020
So cholesterol is a classic example, okay?

334
00:21:33,020 --> 00:21:35,940
Cholesterol might not be the best example to illustrate the difference.

335
00:21:35,940 --> 00:21:39,140
Like the smoking properties of a friend, okay?

336
00:21:39,140 --> 00:21:45,700
If you're close friends mocks, the probability of having a heart rate, I guess, 0.3 or 0.4.

337
00:21:45,700 --> 00:21:51,580
How do you define that in a propositional or in a normal setting?

338
00:21:51,580 --> 00:21:52,580
Okay.

339
00:21:52,580 --> 00:21:56,420
I want to say if a closer and closer features fall down because they're not related

340
00:21:56,420 --> 00:22:02,180
to the individual entity, whereas you've got this broad universes captured by all these

341
00:22:02,180 --> 00:22:03,180
relationships.

342
00:22:03,180 --> 00:22:06,940
And it's all features, but they're not like features of me.

343
00:22:06,940 --> 00:22:08,540
It's features of this network.

344
00:22:08,540 --> 00:22:12,100
It's features of this network and the size of the network varies between people.

345
00:22:12,100 --> 00:22:13,100
Right.

346
00:22:13,100 --> 00:22:14,100
So in the end, everything is a feature.

347
00:22:14,100 --> 00:22:15,100
You're right.

348
00:22:15,100 --> 00:22:16,100
Right.

349
00:22:16,100 --> 00:22:17,100
But what is the feature of?

350
00:22:17,100 --> 00:22:20,300
My point is that we don't do specific feature engineering continuously.

351
00:22:20,300 --> 00:22:24,100
We let the data in its natural form, which is I could talk about objects and relations

352
00:22:24,100 --> 00:22:26,500
and features of those objects and features of those relations.

353
00:22:26,500 --> 00:22:27,500
Right.

354
00:22:27,500 --> 00:22:28,500
Okay.

355
00:22:28,500 --> 00:22:31,100
I just have to say, well, again, you might have five close friends.

356
00:22:31,100 --> 00:22:32,740
I might have three close friends.

357
00:22:32,740 --> 00:22:33,740
How do you encode this?

358
00:22:33,740 --> 00:22:34,740
Right.

359
00:22:34,740 --> 00:22:35,740
Right.

360
00:22:35,740 --> 00:22:36,740
Right.

361
00:22:36,740 --> 00:22:39,380
But as our models can just handle them because it just says, who is that one friend?

362
00:22:39,380 --> 00:22:40,380
Is that two friends?

363
00:22:40,380 --> 00:22:42,180
You can just look at it and you can learn from it.

364
00:22:42,180 --> 00:22:45,180
It's easy to do that.

365
00:22:45,180 --> 00:22:49,700
I thought you were going someplace that you didn't end up going.

366
00:22:49,700 --> 00:22:57,260
It strikes me that you gave the example of the multiple.

367
00:22:57,260 --> 00:23:01,380
You go into the hospital multiple times a year and get your four A1C readings.

368
00:23:01,380 --> 00:23:04,660
I go in one and get my one.

369
00:23:04,660 --> 00:23:13,180
And then you write your rule that basically looks to see if I have had a higher A1C reading

370
00:23:13,180 --> 00:23:15,900
over the past 10 years.

371
00:23:15,900 --> 00:23:19,660
It strikes me that you've lost a lot of information.

372
00:23:19,660 --> 00:23:23,260
Maybe there's information in the fact that you actually want four times and I only want

373
00:23:23,260 --> 00:23:24,260
one.

374
00:23:24,260 --> 00:23:25,260
That's a great point.

375
00:23:25,260 --> 00:23:27,980
It's not necessarily that we lose that information.

376
00:23:27,980 --> 00:23:30,660
If you want to take that, you can record that too.

377
00:23:30,660 --> 00:23:35,860
Actually, the two things that I want to clarify, first, I don't write the rules.

378
00:23:35,860 --> 00:23:39,020
Because of this boosting algorithm, rules are automatically learned from data.

379
00:23:39,020 --> 00:23:41,780
So we use only data and we use some domain knowledge.

380
00:23:41,780 --> 00:23:45,140
Actually, I work on what is called a knowledge-based machine learning.

381
00:23:45,140 --> 00:23:48,060
Now we call it a human and the loop machine learning.

382
00:23:48,060 --> 00:23:50,740
There is a human expert.

383
00:23:50,740 --> 00:23:51,740
We work with cardiologists.

384
00:23:51,740 --> 00:23:52,740
We work with radiologists.

385
00:23:52,740 --> 00:23:57,780
We work with neuro-radiologists and so on and so forth.

386
00:23:57,780 --> 00:24:05,380
We have a lot of collaborations in medical groups, so that's a slightly more of the rules

387
00:24:05,380 --> 00:24:07,100
learned by algorithms.

388
00:24:07,100 --> 00:24:09,100
The rules are learned by algorithms.

389
00:24:09,100 --> 00:24:11,740
What they give us is a little bit more knowledge.

390
00:24:11,740 --> 00:24:16,780
They can say things like, this guy is much more difficult to predict than this person.

391
00:24:16,780 --> 00:24:21,980
Then we tell the data, focus on this person or a person could come and say, I'll come

392
00:24:21,980 --> 00:24:25,580
to that about the experts in a little bit after I answer this question.

393
00:24:25,580 --> 00:24:27,820
So we don't provide the rules.

394
00:24:27,820 --> 00:24:29,980
The rules are learned automatically from the data.

395
00:24:29,980 --> 00:24:32,820
And the rules themselves are capable of learning that.

396
00:24:32,820 --> 00:24:38,140
The rules themselves are capable of saying, if the number of misses matter, then it can

397
00:24:38,140 --> 00:24:45,340
say, actually, if the number of misses is greater than 4, and in that number, if there

398
00:24:45,340 --> 00:24:48,380
is a1c reading greater than this much.

399
00:24:48,380 --> 00:24:52,580
So when you say you don't learn the rules, does that mean you don't learn?

400
00:24:52,580 --> 00:24:54,220
No, we don't write the rules.

401
00:24:54,220 --> 00:24:55,220
Right.

402
00:24:55,220 --> 00:24:56,220
We learn the rules.

403
00:24:56,220 --> 00:25:02,940
When you say you learn the rules, are you learning the parameters of the rules?

404
00:25:02,940 --> 00:25:03,940
No.

405
00:25:03,940 --> 00:25:04,940
You learn the rules themselves.

406
00:25:04,940 --> 00:25:11,980
We learn the rules, along with the parameters, which is why, so if you think of complex neural

407
00:25:11,980 --> 00:25:16,660
network or deep network, where writing these features out, the features themselves can

408
00:25:16,660 --> 00:25:18,180
be learned by us.

409
00:25:18,180 --> 00:25:23,980
So we learn these rules, which are basically if then else statements, and then we parameterize

410
00:25:23,980 --> 00:25:29,340
them with probabilities, or real numbers, depending on whatever interpretation you take.

411
00:25:29,340 --> 00:25:31,580
But we learn the rules from data.

412
00:25:31,580 --> 00:25:33,340
So yeah, so we learn it from data.

413
00:25:33,340 --> 00:25:38,060
Now, going back to the other point that I wanted to make about experts.

414
00:25:38,060 --> 00:25:39,540
Experts can provide a lot of things.

415
00:25:39,540 --> 00:25:43,900
Experts can tell us things like, in some cases, false positives are more important than

416
00:25:43,900 --> 00:25:44,900
false negatives.

417
00:25:44,900 --> 00:25:45,900
Okay.

418
00:25:45,900 --> 00:25:53,140
So for instance, in recommendation systems, I'm recommending a job to you, or I get

419
00:25:53,140 --> 00:25:57,460
these linked in recommendations, which keep telling me there is a post doc position open.

420
00:25:57,460 --> 00:25:58,460
Okay.

421
00:25:58,460 --> 00:26:00,900
And I'm like, come on, man, I did my time, that was seven years back, I don't want to

422
00:26:00,900 --> 00:26:01,900
do this again.

423
00:26:01,900 --> 00:26:02,900
Right.

424
00:26:02,900 --> 00:26:03,900
Right.

425
00:26:03,900 --> 00:26:05,620
So that is a false positive, it's a false positive.

426
00:26:05,620 --> 00:26:09,860
There, from a recommendation system perspective, that needs to be eliminated.

427
00:26:09,860 --> 00:26:14,220
If you tell me four jobs, only four jobs, and those are all relevant to what I'm looking

428
00:26:14,220 --> 00:26:16,060
at, I'm going to trust your system.

429
00:26:16,060 --> 00:26:20,500
So out of the potentially four thousand jobs, you may list only four jobs, but those are

430
00:26:20,500 --> 00:26:21,500
important to me.

431
00:26:21,500 --> 00:26:22,500
So I'll look at it.

432
00:26:22,500 --> 00:26:27,620
If you give me 20 jobs, sort of which only four are relevant, the rest are not false positives.

433
00:26:27,620 --> 00:26:30,020
Then I'm going to lose trust in your system.

434
00:26:30,020 --> 00:26:35,180
Now think exactly the opposite side of an epidemic, you're interested in predicting Ebola,

435
00:26:35,180 --> 00:26:36,180
for instance.

436
00:26:36,180 --> 00:26:37,180
Right.

437
00:26:37,180 --> 00:26:40,460
It's okay to quarantine four more people.

438
00:26:40,460 --> 00:26:41,460
What is the worst thing they're going to do?

439
00:26:41,460 --> 00:26:44,580
They're going to sue the city.

440
00:26:44,580 --> 00:26:49,620
Million dollars each, at most 10 million dollars is what we're going to lose.

441
00:26:49,620 --> 00:26:54,540
Think about releasing four people with Ebola into the general community, then it becomes

442
00:26:54,540 --> 00:26:58,420
an epidemic and costs us billions of dollars just to talk in terms of numbers.

443
00:26:58,420 --> 00:27:02,580
So what I'm trying to illustrate here is, in one case of recommendation system, false

444
00:27:02,580 --> 00:27:05,580
positives are more important than false negatives.

445
00:27:05,580 --> 00:27:09,620
In another case, false negatives are much more dangerous than false positives.

446
00:27:09,620 --> 00:27:13,140
An expert could come and tell us, no, no, no, this is the case where this is more important

447
00:27:13,140 --> 00:27:14,740
than the other one.

448
00:27:14,740 --> 00:27:18,740
And what we do is we think of it as a knob and turn this knob on the classifier and say,

449
00:27:18,740 --> 00:27:21,380
oh, you know what, that's what it is.

450
00:27:21,380 --> 00:27:28,660
And in another example, a domain expert could say, Shriram, for people, what I have observed

451
00:27:28,660 --> 00:27:34,700
in my experience is that, for people with high cholesterol, if they have high BMI, then

452
00:27:34,700 --> 00:27:38,500
the risk of a heart attack is higher than people with lower cholesterol.

453
00:27:38,500 --> 00:27:41,580
So even people with high BMI have two different effects.

454
00:27:41,580 --> 00:27:47,940
They are not exactly telling me how these things influence, except they give a qualitative

455
00:27:47,940 --> 00:27:52,420
understanding between two things and a target of heart attack.

456
00:27:52,420 --> 00:27:56,700
So that's called monotonicity, synergies, they call qualitative relationships.

457
00:27:56,700 --> 00:27:57,700
We can take that.

458
00:27:57,700 --> 00:27:58,700
You can take preferences.

459
00:27:58,700 --> 00:28:01,500
How do you encode that kind of thing?

460
00:28:01,500 --> 00:28:03,260
I'll come to that in a second.

461
00:28:03,260 --> 00:28:05,740
I'll just illustrate this one thing and I'll come to that.

462
00:28:05,740 --> 00:28:09,940
So third thing, for instance, I was in a college town, Bloomington, Indiana.

463
00:28:09,940 --> 00:28:13,420
Let's assume that you're trying to build a robot and the robot is sitting on top of

464
00:28:13,420 --> 00:28:19,700
a building, having a donut in one hand, and observing how people stop at stop signs.

465
00:28:19,700 --> 00:28:21,460
People stop at stop signs.

466
00:28:21,460 --> 00:28:27,140
I'm in a university town, which means only 50% of the people stop at stop signs.

467
00:28:27,140 --> 00:28:33,020
Then the machine could ask the human, hey, I'm looking at this data, but only 50% seem

468
00:28:33,020 --> 00:28:34,020
to be stopping.

469
00:28:34,020 --> 00:28:35,220
What do you think I should do?

470
00:28:35,220 --> 00:28:39,860
Then the expert could say, well, I prefer that you stop at stop signs if it is safe to

471
00:28:39,860 --> 00:28:40,860
stop.

472
00:28:40,860 --> 00:28:44,660
Maybe there is an extreme case where, you know, you don't want to stop in a stop sign

473
00:28:44,660 --> 00:28:47,180
because it's dangerous actually to stop, right?

474
00:28:47,180 --> 00:28:49,900
You might put you in danger or somebody in the, I don't know.

475
00:28:49,900 --> 00:28:54,460
I can imagine such situations with autonomous cars, for instance.

476
00:28:54,460 --> 00:28:59,380
And you say something soft that says, I prefer that you stop at stop signs.

477
00:28:59,380 --> 00:29:01,140
And then it says, okay, great.

478
00:29:01,140 --> 00:29:06,980
Then what our algorithm does is it takes these statements as constraints.

479
00:29:06,980 --> 00:29:10,380
And then it combines that with the data that I have, okay?

480
00:29:10,380 --> 00:29:15,580
So there could be regions in the data where there is a lot of mistakes, like stop signs.

481
00:29:15,580 --> 00:29:19,740
Then it says, ooh, the data has a lot of mistakes, but the expert has told me that this is

482
00:29:19,740 --> 00:29:21,860
what he or she prefers.

483
00:29:21,860 --> 00:29:25,940
So what I'm going to do is take what he or she tells me, combine it with my data.

484
00:29:25,940 --> 00:29:30,460
And wherever the data conflicts the expert, or the expert conflicts the data, I'm going

485
00:29:30,460 --> 00:29:34,900
to weigh that lower, wherever the agree, I'm going to weigh that higher, and kind of

486
00:29:34,900 --> 00:29:36,580
incorporate that into my model.

487
00:29:36,580 --> 00:29:43,100
So what I do is I get these human inputs as constraints to the learning algorithm.

488
00:29:43,100 --> 00:29:50,220
And we have shown across several publications that such constraints improve performance.

489
00:29:50,220 --> 00:29:54,580
And very recently what we have been trying to do is also do a little bit more, which is

490
00:29:54,580 --> 00:30:00,460
let the algorithm ask questions, instead of the expert telling, so when we started this

491
00:30:00,460 --> 00:30:04,820
research, the expert has to say everything that he or she knows about the problem.

492
00:30:04,820 --> 00:30:06,740
Then we take them as constraints and learn.

493
00:30:06,740 --> 00:30:10,500
But now we have gone the other way and we have said, we'll start looking at the data.

494
00:30:10,500 --> 00:30:15,660
In the regions where we have extreme uncertainty about things that we are trying to learn,

495
00:30:15,660 --> 00:30:18,340
we will solicit information from the expert.

496
00:30:18,340 --> 00:30:22,100
So we call it actively advice, seeking advice.

497
00:30:22,100 --> 00:30:26,900
So the key is for the machine to know what it knows, and solicit information about what

498
00:30:26,900 --> 00:30:28,220
it does not know.

499
00:30:28,220 --> 00:30:32,980
And the expert could say something, it takes that back into the model, adjust it again

500
00:30:32,980 --> 00:30:36,700
because it's an iterative learning method that we have.

501
00:30:36,700 --> 00:30:39,100
It can go back and try and fix its mistakes.

502
00:30:39,100 --> 00:30:43,820
So let's put this on the stack after the answer to the encoding question.

503
00:30:43,820 --> 00:30:51,620
But it strikes me there that there's a huge gap between what the algorithm is likely to

504
00:30:51,620 --> 00:30:53,100
surface.

505
00:30:53,100 --> 00:30:59,980
Some factor is, there's a high degree of ambiguity in some factor in something that you

506
00:30:59,980 --> 00:31:06,460
can present to a doctor that makes that intelligible and kind of elicits useful advice to feed

507
00:31:06,460 --> 00:31:10,140
back into the algorithm, which goes back to the encoding question.

508
00:31:10,140 --> 00:31:12,180
Like how does all that part work?

509
00:31:12,180 --> 00:31:13,180
Wonderful question.

510
00:31:13,180 --> 00:31:14,420
That is a fantastic question.

511
00:31:14,420 --> 00:31:18,340
And that is another reason why these logical models are more useful.

512
00:31:18,340 --> 00:31:19,340
Okay.

513
00:31:19,340 --> 00:31:20,340
Here's the problem.

514
00:31:20,340 --> 00:31:26,660
If I now use a very complex learning system underneath, then it's going to say, feature

515
00:31:26,660 --> 00:31:31,020
one, seven, six, three, five, four, two, one, seems very feared for me.

516
00:31:31,020 --> 00:31:32,020
And I go to the doctor.

517
00:31:32,020 --> 00:31:34,020
The doctors are going to know anything, right?

518
00:31:34,020 --> 00:31:36,340
On the other hand, mine is a logical class.

519
00:31:36,340 --> 00:31:38,100
It's an if-then-else statement.

520
00:31:38,100 --> 00:31:45,900
So it could say, for people who have slightly lower HDL level, but on the other hand,

521
00:31:45,900 --> 00:31:48,940
they triglycerate level seems to be good.

522
00:31:48,940 --> 00:31:52,980
There seems to be a lot of distribution over these hot attacks and diabetes.

523
00:31:52,980 --> 00:31:54,500
What do I do?

524
00:31:54,500 --> 00:32:01,580
So what happens is because of the fact that we are learning these knowledge at the level

525
00:32:01,580 --> 00:32:06,460
of the data themselves, the relational schema themselves, it's easier to present to the

526
00:32:06,460 --> 00:32:09,060
doctor because they know what the schema looks like.

527
00:32:09,060 --> 00:32:10,300
It's not just the doctors.

528
00:32:10,300 --> 00:32:12,300
We work again with financial experts.

529
00:32:12,300 --> 00:32:14,460
So again, our legal experts.

530
00:32:14,460 --> 00:32:15,500
So we have these documents.

531
00:32:15,500 --> 00:32:19,420
And we can say, well, you can't say something like, if the parse tree comes up with this

532
00:32:19,420 --> 00:32:23,860
noun phrase, no, but we can extract the says that says this part of the sentence that

533
00:32:23,860 --> 00:32:27,820
seems to be always conflicted with this part of the sentence, ways that.

534
00:32:27,820 --> 00:32:29,700
And we can ask that question.

535
00:32:29,700 --> 00:32:34,580
And that is only because of the fact that we are learning a much more representative model,

536
00:32:34,580 --> 00:32:38,900
much more interpretable model than a typical complex model errors.

537
00:32:38,900 --> 00:32:44,420
So that's because of the fact that you're learning if object, object type, objects value,

538
00:32:44,420 --> 00:32:47,820
objects relations are like this, then something, right?

539
00:32:47,820 --> 00:32:48,820
Okay.

540
00:32:48,820 --> 00:32:50,060
So then you can present this to the user.

541
00:32:50,060 --> 00:32:56,260
So that's another power that comes from these relational models that you don't get with

542
00:32:56,260 --> 00:32:57,260
standard models.

543
00:32:57,260 --> 00:32:58,260
Yeah.

544
00:32:58,260 --> 00:33:02,780
So can you talk a little bit about how you benchmark this relative to other approaches

545
00:33:02,780 --> 00:33:04,980
and what kind of results you've seen?

546
00:33:04,980 --> 00:33:07,820
So that's something that we do all the time extensively.

547
00:33:07,820 --> 00:33:12,180
What we try and do is we take these models and try to, for instance, if I have to run

548
00:33:12,180 --> 00:33:16,620
a deep belief network or something, we try to create as we spend a lot of time engineering

549
00:33:16,620 --> 00:33:18,540
as much as possible.

550
00:33:18,540 --> 00:33:23,820
And try to engineer this a lot and then create many, many, many features.

551
00:33:23,820 --> 00:33:28,260
And you try to use a standard machine learning algorithm as your baseline.

552
00:33:28,260 --> 00:33:32,860
Now what happens is for the current data set, it might learn a good performance, right?

553
00:33:32,860 --> 00:33:37,580
On your training set, it could actually give pretty good performance because most of these

554
00:33:37,580 --> 00:33:42,580
machine learning algorithms can learn well if you engineer your features very well.

555
00:33:42,580 --> 00:33:43,580
And that's doable.

556
00:33:43,580 --> 00:33:44,580
We do that.

557
00:33:44,580 --> 00:33:45,580
Right.

558
00:33:45,580 --> 00:33:49,220
So the real problem comes when it comes to generalization.

559
00:33:49,220 --> 00:33:52,140
So most of the benchmarking that we do is on generalization.

560
00:33:52,140 --> 00:33:56,900
So for instance, you learn about whether a student works with a professor from one university,

561
00:33:56,900 --> 00:33:57,900
right?

562
00:33:57,900 --> 00:33:59,220
You test on that university.

563
00:33:59,220 --> 00:34:01,020
Most of these algorithm works great.

564
00:34:01,020 --> 00:34:03,620
Now what you do is you go to a different university and deploy this model.

565
00:34:03,620 --> 00:34:04,620
Right.

566
00:34:04,620 --> 00:34:08,340
And they kind of break down because you have to create new features, new things.

567
00:34:08,340 --> 00:34:12,100
Whereas these relational learning algorithms, because of the fact that they learn not at

568
00:34:12,100 --> 00:34:16,100
the level of the individual professors, but at the level of these groups of people, sets

569
00:34:16,100 --> 00:34:19,700
of people, it's much more easy to adapt and transfer.

570
00:34:19,700 --> 00:34:24,300
So when we do this benchmarking, we try to figure out, let's say, other networks, which

571
00:34:24,300 --> 00:34:29,540
is why a classic example is, let's say you've used all of Shredam's family network data.

572
00:34:29,540 --> 00:34:32,460
What I'll do is when I'm testing it, I'll go to Sam's data.

573
00:34:32,460 --> 00:34:36,340
And I see, does Sam's data work on my model?

574
00:34:36,340 --> 00:34:41,660
And then you do that with these classical methods, some of the times you don't get them.

575
00:34:41,660 --> 00:34:46,380
Sometimes, if you know what you're going to test on, you can certainly engineer and try

576
00:34:46,380 --> 00:34:47,380
and do that.

577
00:34:47,380 --> 00:34:53,340
But most of the times, these relational models work beautifully on this problem as well.

578
00:34:53,340 --> 00:34:57,100
Yeah, I hear people talk all the time about like overfitting on ImageNet and things like

579
00:34:57,100 --> 00:35:00,700
that in the industry, in the field.

580
00:35:00,700 --> 00:35:06,340
Do you feel that this notion of kind of focusing on generalization as opposed to performance

581
00:35:06,340 --> 00:35:12,780
is like underappreciated or under pursued in the space? Definitely, definitely, but the

582
00:35:12,780 --> 00:35:19,660
good thing is that there's been a new conscience on looking at generalization as an important

583
00:35:19,660 --> 00:35:20,660
aspect.

584
00:35:20,660 --> 00:35:27,220
But initially, there's been a lot of work on the science of experimental methodology of

585
00:35:27,220 --> 00:35:28,220
machine learning.

586
00:35:28,220 --> 00:35:30,580
There's been a lot of work on empirical machine learning.

587
00:35:30,580 --> 00:35:34,420
I think that's an extremely important research direction.

588
00:35:34,420 --> 00:35:41,220
People kind of lost a little bit of that oversight when we went and developed aggressively more

589
00:35:41,220 --> 00:35:43,420
and more and more newer models.

590
00:35:43,420 --> 00:35:48,100
But I think now people have started realizing that because of this abundance of data sources,

591
00:35:48,100 --> 00:35:51,460
we have these multiple data sources that we have to somehow integrate and work with.

592
00:35:51,460 --> 00:35:55,220
Suddenly, generalization again has become an important issue.

593
00:35:55,220 --> 00:36:01,980
So from my limited whatever, 15, 16 year experience, I've seen that there is a lot of interest

594
00:36:01,980 --> 00:36:06,940
now in making sure that we build models that generalize across populations, across data

595
00:36:06,940 --> 00:36:10,740
sets, across diverse data sources.

596
00:36:10,740 --> 00:36:14,300
And I think that's certainly an important research direction.

597
00:36:14,300 --> 00:36:18,740
And which is one of the reasons we want to make sure that any model that you develop is

598
00:36:18,740 --> 00:36:20,540
not pipe to that particular thing.

599
00:36:20,540 --> 00:36:21,540
Right.

600
00:36:21,540 --> 00:36:25,660
Which is why when I see many of these people talk about AI, we're all a little bit worried

601
00:36:25,660 --> 00:36:33,260
because it is such a specific domain specific solution that you are always worried when

602
00:36:33,260 --> 00:36:34,260
it's going to break.

603
00:36:34,260 --> 00:36:38,780
And it could break very easily if you don't think about generalization.

604
00:36:38,780 --> 00:36:44,740
Are there standard and well accepted ways of measuring generalization or do you feel

605
00:36:44,740 --> 00:36:48,780
like everyone kind of figures it out, you know, presents their own results in their

606
00:36:48,780 --> 00:36:51,500
own way in papers?

607
00:36:51,500 --> 00:36:58,260
There's a lot of work on generalization errors and understanding generalization errors,

608
00:36:58,260 --> 00:37:05,260
cross validation and the whole bias variance theory is trying to figure out how the bias

609
00:37:05,260 --> 00:37:10,300
on a particular data set is going to affect the variance across multiple data sets.

610
00:37:10,300 --> 00:37:14,460
So there's definitely a lot of understanding and statistical machine learning on this

611
00:37:14,460 --> 00:37:16,220
notion of generalization error.

612
00:37:16,220 --> 00:37:19,220
And people have definitely looked at it.

613
00:37:19,220 --> 00:37:23,020
But I think when it comes to the practical implementation, people tend to ignore it.

614
00:37:23,020 --> 00:37:27,980
They tend to kind of evaluate them in a very narrow field.

615
00:37:27,980 --> 00:37:33,380
I've even had students, both in my class and in my research group, sometimes build these

616
00:37:33,380 --> 00:37:38,260
so-called tuning sets where you fix your parameters using these tuning set for your learning

617
00:37:38,260 --> 00:37:39,900
algorithm from test sets.

618
00:37:39,900 --> 00:37:40,900
You should never do that.

619
00:37:40,900 --> 00:37:41,900
That's like cheating.

620
00:37:41,900 --> 00:37:45,620
That's like having a practice question from the final midterm or the final exam that

621
00:37:45,620 --> 00:37:46,620
you have.

622
00:37:46,620 --> 00:37:48,060
You're going to have a practice question on that.

623
00:37:48,060 --> 00:37:49,060
So that's cheating.

624
00:37:49,060 --> 00:37:50,740
There's students who do that all the time.

625
00:37:50,740 --> 00:37:56,140
So we tend to try and teach them that's not the right way to do it and we try to show

626
00:37:56,140 --> 00:37:57,300
them a proper way to do it.

627
00:37:57,300 --> 00:38:04,900
So there's definitely a lot of work on generalization and understanding generalization.

628
00:38:04,900 --> 00:38:07,020
But remember, it's within a certain domain.

629
00:38:07,020 --> 00:38:09,060
Can we do it across domains?

630
00:38:09,060 --> 00:38:10,820
Maybe, maybe not.

631
00:38:10,820 --> 00:38:15,900
Can you take something like learning how to drive a car and figure out how to drive a plane

632
00:38:15,900 --> 00:38:18,260
or fly a plane?

633
00:38:18,260 --> 00:38:19,260
No.

634
00:38:19,260 --> 00:38:24,300
But maybe driving a car in US or India, I can't imagine doing that easily, but you could

635
00:38:24,300 --> 00:38:25,300
do that.

636
00:38:25,300 --> 00:38:31,340
You could try and understand how does it generalize across multiple countries and try to understand

637
00:38:31,340 --> 00:38:34,660
how the rules of the domains change.

638
00:38:34,660 --> 00:38:38,340
At some point it gets into artificial general intelligence, right?

639
00:38:38,340 --> 00:38:39,340
Exactly.

640
00:38:39,340 --> 00:38:41,540
If you can generalize across every domain, that's what that's the goal.

641
00:38:41,540 --> 00:38:43,260
Yeah, well, that's a goal.

642
00:38:43,260 --> 00:38:45,500
That's a goal, but I don't know.

643
00:38:45,500 --> 00:38:48,740
I'm not saying that you need something that is very general.

644
00:38:48,740 --> 00:38:54,020
I'm saying that you need something that is generalizable enough inside the same problem

645
00:38:54,020 --> 00:38:55,020
domain.

646
00:38:55,020 --> 00:38:56,020
Right.

647
00:38:56,020 --> 00:38:57,020
Yeah, I think we are not.

648
00:38:57,020 --> 00:39:02,780
If I build a model, as I said, for driving in US, that should be able to drive in Europe,

649
00:39:02,780 --> 00:39:06,820
if not in India, let's say, that level of chaos may be difficult, but maybe at least

650
00:39:06,820 --> 00:39:09,500
in Europe, maybe in London, it should be able to drive a car.

651
00:39:09,500 --> 00:39:13,420
If I teach it how to drive a car in LA, and that amount of generalization should be there

652
00:39:13,420 --> 00:39:14,420
at the very least.

653
00:39:14,420 --> 00:39:16,660
That would be cool to drive a car in India.

654
00:39:16,660 --> 00:39:18,420
That would be awesome to achieve it.

655
00:39:18,420 --> 00:39:22,940
But I'm not really looking for this one system that drives and flies at the same time.

656
00:39:22,940 --> 00:39:23,940
Right.

657
00:39:23,940 --> 00:39:28,940
In fact, I'm worried that that might lose some specific knowledge that a driving agent

658
00:39:28,940 --> 00:39:30,940
will have, that the flying agent will not.

659
00:39:30,940 --> 00:39:31,940
Yeah.

660
00:39:31,940 --> 00:39:32,940
So that's the balance.

661
00:39:32,940 --> 00:39:40,460
How much general knowledge do you want and how much specific, I guess, what's the right

662
00:39:40,460 --> 00:39:43,420
word here, specific skill set you want for solving that problem?

663
00:39:43,420 --> 00:39:47,980
I think that's the difference that we have to find, and the sweet spot depends on the

664
00:39:47,980 --> 00:39:49,540
problem domain.

665
00:39:49,540 --> 00:39:55,060
In some problems, it's, so think about it, like, you have a problem and you go to a doctor,

666
00:39:55,060 --> 00:40:00,020
and this doctor is a neurologist, and now you want to talk a little bit about diabetes.

667
00:40:00,020 --> 00:40:03,900
This person is going to tell you something, but they're going to say, this is what I know,

668
00:40:03,900 --> 00:40:09,180
and you better talk to somebody who knows this, and you go there, or like a cancer, you

669
00:40:09,180 --> 00:40:11,060
want to go to an oncologist, right?

670
00:40:11,060 --> 00:40:15,580
So, but before that, you have a family friend who's a radiologist, radiologist is going

671
00:40:15,580 --> 00:40:20,260
to give you a lot of information, but then you say, but I'll still refer you to this oncologist

672
00:40:20,260 --> 00:40:21,260
for your treatment plans.

673
00:40:21,260 --> 00:40:22,260
Yeah.

674
00:40:22,260 --> 00:40:23,700
And that's what you want, even with systems.

675
00:40:23,700 --> 00:40:26,900
You don't really want systems that say, oh, I can solve this, and this, and this, and

676
00:40:26,900 --> 00:40:27,900
this.

677
00:40:27,900 --> 00:40:28,900
Right.

678
00:40:28,900 --> 00:40:29,900
And so not too generalizable.

679
00:40:29,900 --> 00:40:30,900
Right.

680
00:40:30,900 --> 00:40:31,900
Right.

681
00:40:31,900 --> 00:40:36,900
But, so artificial general intelligence is great, but we also have to accept that, that

682
00:40:36,900 --> 00:40:37,900
comes at a cost.

683
00:40:37,900 --> 00:40:38,900
It's a contextual.

684
00:40:38,900 --> 00:40:39,900
Yes.

685
00:40:39,900 --> 00:40:43,020
Um, you mentioned you had code up on your site.

686
00:40:43,020 --> 00:40:44,020
Yeah.

687
00:40:44,020 --> 00:40:48,540
Tell me a little bit about the code, is it, is it kind of code applied to the healthcare

688
00:40:48,540 --> 00:40:49,540
site?

689
00:40:49,540 --> 00:40:50,540
So, that's a great question.

690
00:40:50,540 --> 00:40:51,540
No.

691
00:40:51,540 --> 00:40:52,540
You know, general algorithm or?

692
00:40:52,540 --> 00:40:53,540
It's a general algorithm.

693
00:40:53,540 --> 00:40:56,060
It's a boosting algorithm that anybody can download.

694
00:40:56,060 --> 00:40:59,140
It's a gradient boosting, but operates on relational databases.

695
00:40:59,140 --> 00:41:00,140
Okay.

696
00:41:00,140 --> 00:41:04,740
We have an extensive tutorial on how to convert your data to our format, how to run the

697
00:41:04,740 --> 00:41:05,740
code.

698
00:41:05,740 --> 00:41:08,620
We can learn multiple types of models, relational probabilistic models.

699
00:41:08,620 --> 00:41:12,100
We can learn multiple, we can take the human inputs on this.

700
00:41:12,100 --> 00:41:15,940
We actually have a wrapper that can do natural language extractions.

701
00:41:15,940 --> 00:41:20,620
So you have text data, but you want to, let's say, figure out who, who is married to

702
00:41:20,620 --> 00:41:24,580
whom based on paragraphs reading some, some CNN articles or something.

703
00:41:24,580 --> 00:41:25,580
Okay.

704
00:41:25,580 --> 00:41:27,620
And you can, you can post that problem.

705
00:41:27,620 --> 00:41:28,940
We have shown how to do it.

706
00:41:28,940 --> 00:41:32,820
So this is an extensive tutorial on how to use this.

707
00:41:32,820 --> 00:41:33,820
Okay.

708
00:41:33,820 --> 00:41:35,220
It's available off of my web page.

709
00:41:35,220 --> 00:41:36,220
Okay.

710
00:41:36,220 --> 00:41:40,940
Go to my lab deck page through my web page in the software.

711
00:41:40,940 --> 00:41:41,940
It's there.

712
00:41:41,940 --> 00:41:42,940
Okay.

713
00:41:42,940 --> 00:41:43,940
And people can download it and use it.

714
00:41:43,940 --> 00:41:44,940
It's a general purpose software.

715
00:41:44,940 --> 00:41:45,940
Okay.

716
00:41:45,940 --> 00:41:47,540
Not only applied to health problems.

717
00:41:47,540 --> 00:41:48,540
Okay.

718
00:41:48,540 --> 00:41:50,820
When it comes to health problems, there is always a catch.

719
00:41:50,820 --> 00:41:54,980
We have to be very careful on what we release and what we don't and so on and so forth.

720
00:41:54,980 --> 00:41:55,980
So yeah.

721
00:41:55,980 --> 00:41:56,980
Okay.

722
00:41:56,980 --> 00:42:04,940
And then we've been talking about relational databases, but there is a whole set of, a

723
00:42:04,940 --> 00:42:08,780
whole type of database called graph databases.

724
00:42:08,780 --> 00:42:09,780
Yes.

725
00:42:09,780 --> 00:42:12,180
Does your method apply to graph databases as well?

726
00:42:12,180 --> 00:42:13,180
Fantastic question.

727
00:42:13,180 --> 00:42:14,180
Yes.

728
00:42:14,180 --> 00:42:15,980
The answer is yes, because graph is a relation for us.

729
00:42:15,980 --> 00:42:16,980
Right.

730
00:42:16,980 --> 00:42:17,980
Right.

731
00:42:17,980 --> 00:42:18,980
Notes and adjust our relations.

732
00:42:18,980 --> 00:42:21,300
So relation is this common word that we use.

733
00:42:21,300 --> 00:42:22,300
Yeah.

734
00:42:22,300 --> 00:42:23,620
So graphs are just relations.

735
00:42:23,620 --> 00:42:28,540
And actually, because one of the most important step inside our optimization function is

736
00:42:28,540 --> 00:42:29,540
to count.

737
00:42:29,540 --> 00:42:33,740
You have to count the number of instances, so count the number of papers that somebody wrote.

738
00:42:33,740 --> 00:42:37,780
Count the number of friends that somebody has in Facebook and so on and so forth.

739
00:42:37,780 --> 00:42:39,860
And that counting is actually a complex problem.

740
00:42:39,860 --> 00:42:44,900
So we actually use graph databases to accelerate our counts.

741
00:42:44,900 --> 00:42:53,140
So we do use sometimes a temporary representation of a graph database to accelerate our line.

742
00:42:53,140 --> 00:42:58,340
So for us, graph databases and databases, graphs, they're all structured problems that

743
00:42:58,340 --> 00:43:01,140
we can handle using the formalism of logic.

744
00:43:01,140 --> 00:43:02,140
Absolutely.

745
00:43:02,140 --> 00:43:04,060
That's the answer is yes.

746
00:43:04,060 --> 00:43:10,220
And what we're trying to do is actually do exactly that, build a version that is more

747
00:43:10,220 --> 00:43:15,140
optimized for graph databases, because graph databases are much simpler sometimes than

748
00:43:15,140 --> 00:43:20,260
the full logic, like psych-carp knowledge base or never-ending learning material or

749
00:43:20,260 --> 00:43:21,580
nail-knowledge base.

750
00:43:21,580 --> 00:43:25,220
Those are logical knowledge bases and those are huge.

751
00:43:25,220 --> 00:43:27,220
What did they call it?

752
00:43:27,220 --> 00:43:28,220
Psych-carp.

753
00:43:28,220 --> 00:43:29,220
Yeah.

754
00:43:29,220 --> 00:43:31,900
Psych-carp is this company that's been running a learning.

755
00:43:31,900 --> 00:43:35,140
It's been building a knowledge base for over 35, 40 years.

756
00:43:35,140 --> 00:43:39,820
And so those are like giant knowledge bases, whereas graph databases are much easier

757
00:43:39,820 --> 00:43:41,700
to manage and work with.

758
00:43:41,700 --> 00:43:43,980
So they may be even faster for us to learn with.

759
00:43:43,980 --> 00:43:44,980
So yeah, absolutely.

760
00:43:44,980 --> 00:43:48,500
Learning with graph databases is something that we do all the time.

761
00:43:48,500 --> 00:43:53,740
And so you're out doing the tutorial kind of evangelizing this approach.

762
00:43:53,740 --> 00:43:54,940
What's the end goal for you?

763
00:43:54,940 --> 00:43:57,780
Is it you trying to get grad students and postdocs?

764
00:43:57,780 --> 00:44:03,620
You're trying to get industry to adapt this, what's the motivation and vision?

765
00:44:03,620 --> 00:44:07,820
I think we want people to be aware of this big goal of statistical relational AI.

766
00:44:07,820 --> 00:44:12,140
We want people to know that deep learning is not everything and we want people to know

767
00:44:12,140 --> 00:44:17,860
that there are other learning areas that focus on some richer, probably more important

768
00:44:17,860 --> 00:44:18,860
questions.

769
00:44:18,860 --> 00:44:23,300
In terms of the four of us who are giving the tutorial actually have large groups ourselves

770
00:44:23,300 --> 00:44:26,020
so it's not like we are trying to recruit people here.

771
00:44:26,020 --> 00:44:27,020
Okay.

772
00:44:27,020 --> 00:44:32,020
Learning one conference at a time to kind of show that this field is now actually much

773
00:44:32,020 --> 00:44:34,780
more matured than what it was 10 years back.

774
00:44:34,780 --> 00:44:39,620
And we are trying to tell people that there is a lot of opportunities inside our field.

775
00:44:39,620 --> 00:44:48,260
So combining deep models with logical models, combining matrix factorization with relational

776
00:44:48,260 --> 00:44:49,260
data.

777
00:44:49,260 --> 00:44:53,020
So there's a lot of opportunities for this and that's what we are highlighting in the

778
00:44:53,020 --> 00:44:57,940
tutorial tomorrow is inviting more people to work on similar problems like we do and

779
00:44:57,940 --> 00:44:59,580
that's the ultimate goal for us.

780
00:44:59,580 --> 00:45:04,100
It's kind of show people that this is an important research area and problem and that all of us

781
00:45:04,100 --> 00:45:05,980
together can contribute to.

782
00:45:05,980 --> 00:45:09,740
So we want more people to work on these problems and that's our end goal.

783
00:45:09,740 --> 00:45:10,740
Great.

784
00:45:10,740 --> 00:45:20,020
Are there specific examples of like case studies or like health centers or products that

785
00:45:20,020 --> 00:45:21,020
use this?

786
00:45:21,020 --> 00:45:23,020
Sure.

787
00:45:23,020 --> 00:45:24,020
That's a very good question.

788
00:45:24,020 --> 00:45:32,580
So I think one of the case is this deep dive by Professor Kirisre from Stanford that's

789
00:45:32,580 --> 00:45:34,660
now been bottled by Apple.

790
00:45:34,660 --> 00:45:40,820
So this is this probabilistic databases which kind of is statistical relational AI in some

791
00:45:40,820 --> 00:45:41,820
sense.

792
00:45:41,820 --> 00:45:46,860
That automatically extracted information, knowledge extraction from videos and images and

793
00:45:46,860 --> 00:45:47,860
text.

794
00:45:47,860 --> 00:45:49,740
And so that's a classic case.

795
00:45:49,740 --> 00:45:54,140
We have been working with hospitals on trying to see how we can put the data back into their

796
00:45:54,140 --> 00:45:59,340
learning system for making predictions on the number of hospital raid missions, the

797
00:45:59,340 --> 00:46:02,260
number of procedures that need to be done on some persons.

798
00:46:02,260 --> 00:46:06,980
We recently have got some very good success on using such models on predicting postpartum

799
00:46:06,980 --> 00:46:13,900
depression by looking at the network of women, sorry network of people that the women are

800
00:46:13,900 --> 00:46:15,820
in touch with.

801
00:46:15,820 --> 00:46:21,100
And so we are trying to talk to people to see how this can go out to the market.

802
00:46:21,100 --> 00:46:28,780
We work with a particular bank on looking at their legal documents and figuring out if

803
00:46:28,780 --> 00:46:35,460
a new document comes in, does this match the standards of the company, of the bank.

804
00:46:35,460 --> 00:46:39,540
And we can do that automatically with like 98% or 99% accuracy.

805
00:46:39,540 --> 00:46:44,740
And the ones that we fail, we can flag it and show it to the human expert who currently

806
00:46:44,740 --> 00:46:47,420
looks at it.

807
00:46:47,420 --> 00:46:51,580
So yes, there's a lot of these case studies out there.

808
00:46:51,580 --> 00:46:54,180
But again, we are trying to do the deployment at this point.

809
00:46:54,180 --> 00:46:58,700
So maybe hopefully if I talk to you in two years, I'll be able to say, well, I probably

810
00:46:58,700 --> 00:47:02,940
will not be able to say company A uses it, but I can say there is a company that uses it.

811
00:47:02,940 --> 00:47:04,180
Same thing with recommendation systems.

812
00:47:04,180 --> 00:47:07,940
The job recommendation system is actually a real project that we have done with the

813
00:47:07,940 --> 00:47:08,940
real company.

814
00:47:08,940 --> 00:47:09,940
Okay.

815
00:47:09,940 --> 00:47:13,020
And they are looking to use our product in their collaborative filtering system.

816
00:47:13,020 --> 00:47:14,020
Okay.

817
00:47:14,020 --> 00:47:17,100
There's a lot of success stories on this, actually, which is what we're going to highlight

818
00:47:17,100 --> 00:47:18,100
tomorrow.

819
00:47:18,100 --> 00:47:19,100
You should drop by.

820
00:47:19,100 --> 00:47:20,100
Okay.

821
00:47:20,100 --> 00:47:21,100
Awesome.

822
00:47:21,100 --> 00:47:22,100
Well, sure.

823
00:47:22,100 --> 00:47:23,100
Well, sure.

824
00:47:23,100 --> 00:47:24,100
Thanks so much for joining me.

825
00:47:24,100 --> 00:47:26,740
I really appreciate having an opportunity to learn about statistical relational AI.

826
00:47:26,740 --> 00:47:27,740
Thank you.

827
00:47:27,740 --> 00:47:33,060
Any final words or places that you like to point folks to or anything else?

828
00:47:33,060 --> 00:47:34,060
Oh, thanks.

829
00:47:34,060 --> 00:47:36,140
There is a book on statistical relational AI.

830
00:47:36,140 --> 00:47:37,340
Of course, that time I caught that off.

831
00:47:37,340 --> 00:47:41,500
Actually, the four of us who wrote the book are the four who are giving tutorials tomorrow.

832
00:47:41,500 --> 00:47:42,500
Okay.

833
00:47:42,500 --> 00:47:47,140
There's a series of workshops that happen every year that the four of us again founded

834
00:47:47,140 --> 00:47:52,740
10 years, eight, nine years back, I don't know, eight, nine years back, I'm getting old.

835
00:47:52,740 --> 00:47:55,300
And but now people are running it.

836
00:47:55,300 --> 00:47:57,220
It's kind of self-sustaining in its own.

837
00:47:57,220 --> 00:48:02,660
So I invite people to look at this problems of statistical relational AI and contact any

838
00:48:02,660 --> 00:48:03,660
of us.

839
00:48:03,660 --> 00:48:07,180
If you need any directions or anything else, that's all I'm very, very happy to help.

840
00:48:07,180 --> 00:48:08,180
Awesome.

841
00:48:08,180 --> 00:48:09,180
Thanks so much.

842
00:48:09,180 --> 00:48:10,180
Thank you.

843
00:48:10,180 --> 00:48:11,180
Thanks.

844
00:48:11,180 --> 00:48:13,660
All right, everyone.

845
00:48:13,660 --> 00:48:15,780
That's our show for today.

846
00:48:15,780 --> 00:48:20,820
For more information on Shrewroom or any of the topics covering in this episode, head

847
00:48:20,820 --> 00:48:26,580
on over to twimmolai.com slash talk slash 113.

848
00:48:26,580 --> 00:48:33,180
Definitely remember to submit your thoughts on AI in your life at twimmolai.com slash my

849
00:48:33,180 --> 00:48:34,660
AI.

850
00:48:34,660 --> 00:48:41,660
And of course, thanks so much for listening and catch you next time.

