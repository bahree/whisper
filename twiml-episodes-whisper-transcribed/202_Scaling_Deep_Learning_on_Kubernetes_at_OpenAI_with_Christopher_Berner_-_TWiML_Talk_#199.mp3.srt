1
00:00:00,000 --> 00:00:16,320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

2
00:00:16,320 --> 00:00:21,480
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:21,480 --> 00:00:31,480
I'm your host Sam Charrington.

4
00:00:31,480 --> 00:00:35,440
For those challenges with promoting the use of machine learning in an organization and

5
00:00:35,440 --> 00:00:40,480
making it more accessible, one key to success is supporting data scientists and machine

6
00:00:40,480 --> 00:00:44,840
learning engineers with modern processes, tooling and platforms.

7
00:00:44,840 --> 00:00:49,480
This is a topic we're super excited to address here on the podcast with the AI Platforms

8
00:00:49,480 --> 00:00:54,120
Podcast series that you're currently listening to, as well as a series of e-books that will

9
00:00:54,120 --> 00:00:56,600
be publishing on the topic.

10
00:00:56,600 --> 00:01:01,000
The first of these books takes a bottoms up look at AI Platforms and is focused on the

11
00:01:01,000 --> 00:01:06,000
open source Kubernetes project, which is used to deliver scalable machine learning infrastructure

12
00:01:06,000 --> 00:01:11,160
at places like Airbnb, booking.com and open AI.

13
00:01:11,160 --> 00:01:15,160
The second book in the series looks at scaling data science and machine learning engineering

14
00:01:15,160 --> 00:01:20,600
from the top down, exploring the internal platforms that companies like Airbnb, Facebook

15
00:01:20,600 --> 00:01:25,440
and Uber have built and what enterprises can learn from them.

16
00:01:25,440 --> 00:01:29,560
If these are topics you're interested in and especially if part of your job involves

17
00:01:29,560 --> 00:01:35,600
making machine learning more accessible, I'd encourage you to visit twimbleai.com slash

18
00:01:35,600 --> 00:01:41,880
AI platforms and sign up to be notified as soon as these books are published.

19
00:01:41,880 --> 00:01:47,080
If you found yourself saying, Cuba, what, Cuba, who, when listening to this intro, this

20
00:01:47,080 --> 00:01:48,800
is the show for you.

21
00:01:48,800 --> 00:01:54,640
In this episode of our AI Platforms series, we're joined by open AI's head of infrastructure,

22
00:01:54,640 --> 00:01:56,360
Christopher Burner.

23
00:01:56,360 --> 00:02:01,280
Chris has played a key role in overhauling open AI's deep learning infrastructure in the

24
00:02:01,280 --> 00:02:04,240
course of his two years with the company.

25
00:02:04,240 --> 00:02:09,000
In our conversation, we discussed the evolution of open AI's deep learning platform, the core

26
00:02:09,000 --> 00:02:14,080
principles which have guided that evolution and its current architecture.

27
00:02:14,080 --> 00:02:18,960
We dig deep into the use of Kubernetes and discuss various ecosystem players and projects

28
00:02:18,960 --> 00:02:22,280
that support running deep learning at scale.

29
00:02:22,280 --> 00:02:24,920
And now on to the show.

30
00:02:24,920 --> 00:02:27,640
All right, everyone.

31
00:02:27,640 --> 00:02:30,280
I am on the line with Christopher Burner.

32
00:02:30,280 --> 00:02:33,600
Chris is the head of infrastructure at open AI.

33
00:02:33,600 --> 00:02:36,360
Chris, welcome to this week in machine learning and AI.

34
00:02:36,360 --> 00:02:37,640
Thanks a lot.

35
00:02:37,640 --> 00:02:42,520
Awesome. So why don't we get started by having you tell us a little bit about your background.

36
00:02:42,520 --> 00:02:46,080
You are coming up on almost two years at open AI now.

37
00:02:46,080 --> 00:02:47,560
How did you get to open AI?

38
00:02:47,560 --> 00:02:48,560
Yeah.

39
00:02:48,560 --> 00:02:49,560
Well, let's see.

40
00:02:49,560 --> 00:02:55,120
So before open AI, I was at Facebook, I worked there for about four and a half years.

41
00:02:55,120 --> 00:02:59,200
And before that, I worked at a couple of startups at Facebook.

42
00:02:59,200 --> 00:03:02,960
I worked on the newsfeed ranking team for about a year.

43
00:03:02,960 --> 00:03:08,720
So it's got just a little bit of introductory experience in machine learning.

44
00:03:08,720 --> 00:03:15,440
And then I worked in the data infrastructure team, worked on a distributed SQL query engine

45
00:03:15,440 --> 00:03:18,040
called Presto, which is open source.

46
00:03:18,040 --> 00:03:23,480
So I have a fair bit of background and big data analytics and data warehousing.

47
00:03:23,480 --> 00:03:28,280
And that has certainly helped me get up to speed with all of the large scale infrastructure

48
00:03:28,280 --> 00:03:31,400
that's required for deep learning these days.

49
00:03:31,400 --> 00:03:37,000
And yeah, I've just been learning all about the infrastructure that you need for machine

50
00:03:37,000 --> 00:03:39,920
learning training and specialized infrastructure there.

51
00:03:39,920 --> 00:03:47,040
And so open AI is certainly doing machine learning at large scale.

52
00:03:47,040 --> 00:03:53,400
Maybe we can start out by having you, you know, we've, I've talked to several people

53
00:03:53,400 --> 00:03:59,680
from open AI on the podcast before, but maybe you can start by providing an overview of

54
00:03:59,680 --> 00:04:09,120
some of the larger projects that kind of stressed the need for a platform for machine learning

55
00:04:09,120 --> 00:04:10,680
at open AI.

56
00:04:10,680 --> 00:04:11,680
Yeah.

57
00:04:11,680 --> 00:04:19,320
Well, certainly our largest project is our data to research project.

58
00:04:19,320 --> 00:04:23,720
And in the past few months, we've announced a bunch of the results with open AI five

59
00:04:23,720 --> 00:04:24,720
there.

60
00:04:24,720 --> 00:04:32,520
And yeah, that's a very large system, trans on hundreds of GPUs and over 100,000 virtual

61
00:04:32,520 --> 00:04:39,600
CPU cores runs in one of our clusters in Google compute engine.

62
00:04:39,600 --> 00:04:41,480
And yeah, that's definitely our largest.

63
00:04:41,480 --> 00:04:47,760
We have several other projects that are also quite large though, our robotics team also

64
00:04:47,760 --> 00:04:49,720
does some very large scale training.

65
00:04:49,720 --> 00:04:55,560
I've actually done a little bit of work on large scale image net training.

66
00:04:55,560 --> 00:05:00,960
And yeah, that's like one or 200 GPUs, I'd say kind of all of our teams have things that

67
00:05:00,960 --> 00:05:05,560
are on the medium to large size, but data is definitely our largest.

68
00:05:05,560 --> 00:05:06,560
Nice.

69
00:05:06,560 --> 00:05:07,560
Nice.

70
00:05:07,560 --> 00:05:10,720
And I spoke to Christy not too long ago about the data project.

71
00:05:10,720 --> 00:05:16,400
So I'd refer anyone who wants to hear more about that to that interview.

72
00:05:16,400 --> 00:05:24,520
In terms of supporting these types of projects, when you started at open AI, was there much

73
00:05:24,520 --> 00:05:29,480
established or have you kind of had a hand in building it up from scratch?

74
00:05:29,480 --> 00:05:32,600
Yeah, some of both.

75
00:05:32,600 --> 00:05:38,960
So definitely we already had Kubernetes clusters set up when I joined.

76
00:05:38,960 --> 00:05:43,880
They were definitely architected differently than they are today.

77
00:05:43,880 --> 00:05:48,360
And yeah, I've been involved in changing the architecture of them to make them scale

78
00:05:48,360 --> 00:05:51,640
better and also isolate faults better.

79
00:05:51,640 --> 00:05:54,480
So that was in place.

80
00:05:54,480 --> 00:06:00,800
We've also made some changes in terms of our storage technologies and a bunch of the frameworks

81
00:06:00,800 --> 00:06:02,800
that we use for running experiments.

82
00:06:02,800 --> 00:06:08,480
I think that's actually one of the areas that we've seen the largest changes we had.

83
00:06:08,480 --> 00:06:12,680
Two or three different frameworks that existed when I joined and I don't think any of those

84
00:06:12,680 --> 00:06:13,680
are still around.

85
00:06:13,680 --> 00:06:17,880
We've moved on to completely different ones, so yeah, so a mix of things that still exist

86
00:06:17,880 --> 00:06:19,120
and a bunch that has changed.

87
00:06:19,120 --> 00:06:26,240
You mentioned that a big part of the change was driven around scalability and I think it

88
00:06:26,240 --> 00:06:28,680
was reliability.

89
00:06:28,680 --> 00:06:33,240
Those are a couple of things that have driven change, but taking a step back are there when

90
00:06:33,240 --> 00:06:40,280
you think about the characteristics that you're trying to be able to provide to the research

91
00:06:40,280 --> 00:06:44,960
teams from an infrastructure perspective.

92
00:06:44,960 --> 00:06:51,640
Those are the two or most important things or are there a set of things that you think

93
00:06:51,640 --> 00:06:57,320
that they require of you to be able to do their jobs effectively?

94
00:06:57,320 --> 00:07:01,280
Yeah, so those are definitely two very important ones.

95
00:07:01,280 --> 00:07:04,520
There's a couple others that I would add.

96
00:07:04,520 --> 00:07:12,880
One is access to cutting-edge hardware and we run some machines in our own data centers

97
00:07:12,880 --> 00:07:16,840
because we want access to hardware that's not available in the cloud and so yeah, that's

98
00:07:16,840 --> 00:07:23,400
another dimension and then yeah, I guess sort of another aspect, but it's more on the

99
00:07:23,400 --> 00:07:29,960
tooling side is just ease of use that people want to be able to launch their experiments

100
00:07:29,960 --> 00:07:35,760
quickly and interact quickly and so that's another important aspect of infrastructure.

101
00:07:35,760 --> 00:07:43,480
And it sounds like the support for the framework of choice for the researchers is another

102
00:07:43,480 --> 00:07:48,000
one in that you've already kind of gone through a transition on the framework side.

103
00:07:48,000 --> 00:07:51,320
You don't want to lock them into a specific choice there.

104
00:07:51,320 --> 00:07:56,720
Yeah, we try to provide as much flexibility as possible to our researchers.

105
00:07:56,720 --> 00:08:01,200
Like I mentioned, there were some old research tools that I don't think people are using

106
00:08:01,200 --> 00:08:10,320
now because they've invented new ones that are better and similarly we don't want to

107
00:08:10,320 --> 00:08:14,640
be limited to running experiments just in the cloud and that's one of the benefits that

108
00:08:14,640 --> 00:08:20,080
we see from Kubernetes is that it's very easy to port your experiments from the cloud

109
00:08:20,080 --> 00:08:25,200
to our on-premise clusters where we may have access to different hardware and it makes

110
00:08:25,200 --> 00:08:29,360
that transition pretty seamless and the so the frameworks that you mentioned that folks

111
00:08:29,360 --> 00:08:35,080
weren't using that folks that you transition from are those with those internal frameworks

112
00:08:35,080 --> 00:08:39,440
or these all open source frameworks, but you're just using different ones now.

113
00:08:39,440 --> 00:08:43,280
Are we talking about things like TensorFlow and PyTorch or are we talking about internal

114
00:08:43,280 --> 00:08:45,320
tools that have been developed at OpenAI?

115
00:08:45,320 --> 00:08:48,480
Yeah, so those particular ones were internal.

116
00:08:48,480 --> 00:08:49,480
Okay.

117
00:08:49,480 --> 00:08:54,960
It was a few different frameworks for running and managing experiments and visualizing the

118
00:08:54,960 --> 00:08:59,920
results from them and things like that and now I'd say like TensorFlow board is pretty

119
00:08:59,920 --> 00:09:00,920
popular.

120
00:09:00,920 --> 00:09:07,680
That's kind of replaced some of the need for our custom tooling in one particular area.

121
00:09:07,680 --> 00:09:13,880
But yeah, also TensorFlow, I'd say that's definitely our most popular sort of machine learning

122
00:09:13,880 --> 00:09:14,880
framework.

123
00:09:14,880 --> 00:09:20,000
I'd guess that 90% of our code is probably TensorFlow, but there's also starting to be

124
00:09:20,000 --> 00:09:25,800
significant contingent of people who are using PyTorch, so some amount of change in that

125
00:09:25,800 --> 00:09:26,800
area too.

126
00:09:26,800 --> 00:09:27,800
Okay.

127
00:09:27,800 --> 00:09:38,280
Before we dive into kind of an architectural overview of the platform that you've set

128
00:09:38,280 --> 00:09:47,560
up, can you walk us through kind of the level above that, like the workflow or the processes

129
00:09:47,560 --> 00:09:51,320
as you see it, that you're trying to support.

130
00:09:51,320 --> 00:09:56,760
So for example, you mentioned experiment management and how some of the tooling there is shifted

131
00:09:56,760 --> 00:09:59,520
from internal to tensor board.

132
00:09:59,520 --> 00:10:04,920
What are the ways in which you think about the functional requirements of the machine learning

133
00:10:04,920 --> 00:10:07,000
researchers that you're supporting?

134
00:10:07,000 --> 00:10:08,000
Yeah, certainly.

135
00:10:08,000 --> 00:10:10,400
Yeah, I guess at a really high level.

136
00:10:10,400 --> 00:10:17,360
Pretty much everything that we do is research and experimentation and training models.

137
00:10:17,360 --> 00:10:22,760
Which makes us a little bit different than a lot of other companies that are doing machine

138
00:10:22,760 --> 00:10:30,040
learning because they're often then going on to integrate those into a product and deploy

139
00:10:30,040 --> 00:10:34,520
that to all of their users, whereas we're really just focused on the research side of things.

140
00:10:34,520 --> 00:10:41,640
And so we try to optimize our tools for iterating quickly and exploring a lot of different research

141
00:10:41,640 --> 00:10:49,400
directions and providing the flexibility to do experiments in all different areas.

142
00:10:49,400 --> 00:10:53,800
So we've got a number of teams that do reinforcement learning.

143
00:10:53,800 --> 00:10:58,760
But we also have teams doing supervised learning and unsupervised learning.

144
00:10:58,760 --> 00:11:03,160
So a whole bunch of different types of problems that they're trying to solve.

145
00:11:03,160 --> 00:11:08,600
But kind of all of it comes back to training models and collecting the results from them

146
00:11:08,600 --> 00:11:12,840
and then being able to interact quickly on the next version of their research ideas.

147
00:11:12,840 --> 00:11:13,840
Okay.

148
00:11:13,840 --> 00:11:21,480
So whereas in a non-research enterprise, something, some of the front end work of integrating

149
00:11:21,480 --> 00:11:25,760
with data warehouses and things like that isn't so critical for you.

150
00:11:25,760 --> 00:11:31,800
And some of the backend model management tasks where you're trying to manage productionized

151
00:11:31,800 --> 00:11:32,800
models.

152
00:11:32,800 --> 00:11:38,680
It's also not as critical for you or not at all critical for you.

153
00:11:38,680 --> 00:11:44,480
The part in the middle, this experiment management piece is, that has to be done really well

154
00:11:44,480 --> 00:11:50,440
to make sure that the folks who are supporting are able to work most efficiently.

155
00:11:50,440 --> 00:11:51,440
Yeah, exactly.

156
00:11:51,440 --> 00:11:53,720
Because that's pretty much all that we do.

157
00:11:53,720 --> 00:11:56,120
And so that's really where we focus our effort.

158
00:11:56,120 --> 00:12:02,920
And when you think of that job from a quote unquote head of infrastructure perspective,

159
00:12:02,920 --> 00:12:04,680
are you thinking of it?

160
00:12:04,680 --> 00:12:10,080
I guess I have this picture on my head of infrastructure looking down like hardware

161
00:12:10,080 --> 00:12:18,760
and platforms and frameworks and infrastructure looking up like tools that manage that process

162
00:12:18,760 --> 00:12:21,200
of experiment management, for example.

163
00:12:21,200 --> 00:12:26,760
Are you kind of, you know, top to bottom there or are you, you know, just the bottom part

164
00:12:26,760 --> 00:12:27,760
or?

165
00:12:27,760 --> 00:12:32,040
Yeah, I would say that we do kind of a bit of everything, but for the most part, we're

166
00:12:32,040 --> 00:12:35,920
focused on providing the compute clusters.

167
00:12:35,920 --> 00:12:41,000
And yeah, I guess also running like the storage clusters and other things that are related,

168
00:12:41,000 --> 00:12:46,320
but to sort of providing, yeah, the bottom layers of making sure that everyone has access

169
00:12:46,320 --> 00:12:49,520
to the compute and the hardware that they need.

170
00:12:49,520 --> 00:12:54,360
We do also work a bit on the tooling side.

171
00:12:54,360 --> 00:13:00,120
Most of our experimentation management tools have been developed by our research teams,

172
00:13:00,120 --> 00:13:07,160
but we certainly help maintain a few of those and help support them with the new features.

173
00:13:07,160 --> 00:13:15,800
And so I think infrastructure's role has kind of expanded and actually in some ways,

174
00:13:15,800 --> 00:13:21,200
I suppose, contracted to, we used to have other tools that we ran and managed, and now

175
00:13:21,200 --> 00:13:25,600
it's said that those aren't so useful, so we no longer run those.

176
00:13:25,600 --> 00:13:31,560
But kind of the main focus of the team is on providing really big compute clusters, but

177
00:13:31,560 --> 00:13:35,400
then we've also got a lot of other responsibilities on tooling side and other places that we work

178
00:13:35,400 --> 00:13:36,400
on.

179
00:13:36,400 --> 00:13:40,280
One of the things that I found interesting when I look at what you're doing, you've

180
00:13:40,280 --> 00:13:45,800
published OpenAI as published quite a bit on its infrastructure and some of the things

181
00:13:45,800 --> 00:13:52,840
that it's done, and one of the things that I found really interesting is the very strong

182
00:13:52,840 --> 00:13:55,600
commitment to kind of this multi-cloud world.

183
00:13:55,600 --> 00:13:59,400
You've got experiments running on Google, you've got experiments running on Azure, you've

184
00:13:59,400 --> 00:14:03,640
got experiments running on AWS, and of course, on premises.

185
00:14:03,640 --> 00:14:08,680
Can you talk a little bit about the motivation there?

186
00:14:08,680 --> 00:14:16,240
And what's driven the need to support, it's got to be a more complex environment than

187
00:14:16,240 --> 00:14:18,320
just standardizing in one place?

188
00:14:18,320 --> 00:14:20,200
Yeah, for sure.

189
00:14:20,200 --> 00:14:22,760
I guess there's kind of a few different factors.

190
00:14:22,760 --> 00:14:29,920
So one, like I mentioned, is access to kind of a chartware, so we can get whatever we want

191
00:14:29,920 --> 00:14:35,720
in our on-prem clusters, but the cloud also has a lot of benefits, you can quickly scale

192
00:14:35,720 --> 00:14:36,720
things up.

193
00:14:36,720 --> 00:14:43,920
They've got lots of nice APIs and auxiliary services that you can take advantage of, and

194
00:14:43,920 --> 00:14:49,600
so we'd like to have a presence with all of the major cloud providers so that when they

195
00:14:49,600 --> 00:14:57,440
launch a new type of NVIDIA GPU, or when Google announces their new TPUs, we want to be

196
00:14:57,440 --> 00:15:00,160
sure that we have access to those and that we can use them.

197
00:15:00,160 --> 00:15:05,360
And so that's one thing that drives our multi-cloud strategy.

198
00:15:05,360 --> 00:15:10,720
Another part of it is strategic partnerships and economics.

199
00:15:10,720 --> 00:15:17,240
We want to be able to take advantage of whatever cloud we can get the best pricing in, and

200
00:15:17,240 --> 00:15:25,640
so that certainly has advantages to being able to move our workloads between clouds and

201
00:15:25,640 --> 00:15:27,440
our own cluster, of course.

202
00:15:27,440 --> 00:15:32,520
And so you've got these disparate research workloads.

203
00:15:32,520 --> 00:15:40,760
You've got these multiple infrastructure environments, clouds plus your on-premises cluster.

204
00:15:40,760 --> 00:15:49,360
And as we've alluded to, Kubernetes is a part of the platform that ties all this together.

205
00:15:49,360 --> 00:15:54,640
Taking a step back from that, when you describe the platform for machine learning and deep

206
00:15:54,640 --> 00:15:57,760
learning at OpenAI, how do you describe it?

207
00:15:57,760 --> 00:15:59,240
What are the major pieces?

208
00:15:59,240 --> 00:16:01,400
Do you have names for things?

209
00:16:01,400 --> 00:16:05,720
What kind of walk us through the platform?

210
00:16:05,720 --> 00:16:07,720
Yeah, for sure.

211
00:16:07,720 --> 00:16:10,680
Yeah, so Kubernetes is a big piece of it.

212
00:16:10,680 --> 00:16:14,520
We run three different production clusters.

213
00:16:14,520 --> 00:16:19,360
We name all of our production clusters, animals alphabetically.

214
00:16:19,360 --> 00:16:24,640
So the latest one, we're up to J now, which is Jaguar.

215
00:16:24,640 --> 00:16:26,000
So that's our newest one.

216
00:16:26,000 --> 00:16:29,560
We've got two others, Horus and Ibus.

217
00:16:29,560 --> 00:16:38,080
So each of those is a Kubernetes cluster limited to a specific cloud and geographic region

218
00:16:38,080 --> 00:16:40,680
or our on-prem cluster.

219
00:16:40,680 --> 00:16:45,840
And then sort of outside of that, we've got a number of tools.

220
00:16:45,840 --> 00:16:52,720
So the rapid experimentation tool is the one that our data research team developed.

221
00:16:52,720 --> 00:16:59,400
And that does sort of all of their experimentation management and provides a bunch of core building

222
00:16:59,400 --> 00:17:02,360
pieces for building their experiments as well.

223
00:17:02,360 --> 00:17:06,000
And that's one that our robotics team uses as well.

224
00:17:06,000 --> 00:17:09,240
And a couple other teams have experimented with.

225
00:17:09,240 --> 00:17:13,960
So that has really helped us, I'd say, move quickly and be able to launch new research

226
00:17:13,960 --> 00:17:18,400
experiments in the reinforcement learning space because that's what that tool is optimized

227
00:17:18,400 --> 00:17:20,120
for.

228
00:17:20,120 --> 00:17:24,880
Those core building pieces, can you give us some examples of what those are?

229
00:17:24,880 --> 00:17:33,520
Yeah, so they've got an implementation of the PPO algorithm of the what algorithm?

230
00:17:33,520 --> 00:17:35,720
PPO, proximal policy optimization.

231
00:17:35,720 --> 00:17:36,720
Got it.

232
00:17:36,720 --> 00:17:37,720
Okay.

233
00:17:37,720 --> 00:17:38,720
Yeah.

234
00:17:38,720 --> 00:17:44,080
They've got a special implementation for synchronizing multiple GPUs that are spread across

235
00:17:44,080 --> 00:17:49,640
machines to do distributed training, a bunch of infrastructure just kind of for tracking

236
00:17:49,640 --> 00:17:56,200
the experiments, managing optimizers, managing tiers of machines that are running the environment

237
00:17:56,200 --> 00:18:01,800
like the data to game or a physics simulator, streaming that experience back to the optimizer

238
00:18:01,800 --> 00:18:03,960
machines.

239
00:18:03,960 --> 00:18:09,160
So kind of managing the application level distributed system that has to do all the training.

240
00:18:09,160 --> 00:18:16,720
Is that tool used for multiple experiments and what's the I'm curious how customized

241
00:18:16,720 --> 00:18:24,240
that is for a specific application or experiment or if it's, you know, can it can a new experiment

242
00:18:24,240 --> 00:18:28,840
kind of plug into it pretty easily and take advantage of all the different sub components

243
00:18:28,840 --> 00:18:30,240
that it offers?

244
00:18:30,240 --> 00:18:31,240
Yeah.

245
00:18:31,240 --> 00:18:38,160
So it started out quite specialized for the data team and then has expanded into a more

246
00:18:38,160 --> 00:18:42,880
general research tool that we're now using in a number of teams across the company.

247
00:18:42,880 --> 00:18:46,960
So the robotics team was the second team that started using it and they've applied it

248
00:18:46,960 --> 00:18:54,760
to some of their robotic hand experiments and that kind of turned out to be actually a

249
00:18:54,760 --> 00:18:55,760
similar problem.

250
00:18:55,760 --> 00:19:02,680
You replace the data to game with a physics simulator and then a lot of the other components

251
00:19:02,680 --> 00:19:04,880
really fit together nicely.

252
00:19:04,880 --> 00:19:09,480
And now we've got at least one other team that's starting to use it and they also I believe

253
00:19:09,480 --> 00:19:13,160
we're able to get started really quickly, adapt it for their experiments.

254
00:19:13,160 --> 00:19:18,800
So I think a lot of our teams that are doing reinforcement learning are able to just

255
00:19:18,800 --> 00:19:20,920
move their experiments over to it really quickly.

256
00:19:20,920 --> 00:19:21,920
Okay.

257
00:19:21,920 --> 00:19:22,920
Cool.

258
00:19:22,920 --> 00:19:27,600
And so you mentioned you've got these three production clusters and just to clarify,

259
00:19:27,600 --> 00:19:33,120
those are the three clusters on prem and then you've got additional clusters in the

260
00:19:33,120 --> 00:19:34,120
cloud.

261
00:19:34,120 --> 00:19:35,120
Is that right?

262
00:19:35,120 --> 00:19:37,720
Oh no, those are our three Kubernetes clusters.

263
00:19:37,720 --> 00:19:40,520
So one of them is on prem, the other two are on the cloud.

264
00:19:40,520 --> 00:19:41,520
Okay.

265
00:19:41,520 --> 00:19:50,680
And do the three clusters support presumably different research workloads simultaneously

266
00:19:50,680 --> 00:19:57,160
or are they dedicated to a specific research workload at a time?

267
00:19:57,160 --> 00:19:58,160
Yeah.

268
00:19:58,160 --> 00:20:01,440
So it varies a bit.

269
00:20:01,440 --> 00:20:07,640
And because research projects can do the change in scope over time.

270
00:20:07,640 --> 00:20:11,520
And yeah, they kind of ebb and flow in terms of their compute needs.

271
00:20:11,520 --> 00:20:13,840
It changes over time.

272
00:20:13,840 --> 00:20:17,520
Right at the moment, I believe all three clusters are being shared.

273
00:20:17,520 --> 00:20:21,760
I don't think any of them are dedicated to a single team at the moment.

274
00:20:21,760 --> 00:20:27,320
But it tends to be that like two or three teams will share one cluster and that kind of

275
00:20:27,320 --> 00:20:33,800
makes it easier for them to know how much capacity they're going to be able to use.

276
00:20:33,800 --> 00:20:37,560
And it's just kind of easier for the infrastructure team also to know who it is.

277
00:20:37,560 --> 00:20:40,040
It tends to be using each cluster.

278
00:20:40,040 --> 00:20:43,360
But at the moment, we run all three of them, the shared cluster is where anyone is free

279
00:20:43,360 --> 00:20:44,680
to use the capacity.

280
00:20:44,680 --> 00:20:45,680
Taking a step back.

281
00:20:45,680 --> 00:20:50,280
So Kubernetes deals with containerized workloads.

282
00:20:50,280 --> 00:20:55,600
So the training workload, for example, needs to be containerized.

283
00:20:55,600 --> 00:21:00,200
That's not necessarily a skill that the typical data scientist has.

284
00:21:00,200 --> 00:21:06,520
One of the things that I read is that you've built some tooling that kind of does that

285
00:21:06,520 --> 00:21:12,080
for the researchers so they don't have to get into the weeds in terms of containerizing

286
00:21:12,080 --> 00:21:13,080
their workloads.

287
00:21:13,080 --> 00:21:14,800
Can you talk a little bit about that?

288
00:21:14,800 --> 00:21:15,800
Yeah.

289
00:21:15,800 --> 00:21:19,320
So one of the things that we try to do in general is provide a lot of flexibility in terms

290
00:21:19,320 --> 00:21:25,760
of tooling because different teams and different researchers may have a different level of comfort

291
00:21:25,760 --> 00:21:27,680
with different tool sets.

292
00:21:27,680 --> 00:21:33,960
And so yeah, people can use Kubernetes directly and build their Docker containers and there's

293
00:21:33,960 --> 00:21:36,080
a bunch of people who are comfortable with that.

294
00:21:36,080 --> 00:21:37,080
That's what they do.

295
00:21:37,080 --> 00:21:43,760
But then, yeah, there's also a set of people that prefer different types of interfaces

296
00:21:43,760 --> 00:21:48,840
and maybe something but abstracts way all of the container details.

297
00:21:48,840 --> 00:21:57,720
And so another tool that we have called ARCALL, it handles all of the Docker containers

298
00:21:57,720 --> 00:22:05,200
and, in fact, abstracts away sort of all of the Kubernetes details so that you just

299
00:22:05,200 --> 00:22:11,320
have your Python code in a directory and it knows where to find it, uploads it to the

300
00:22:11,320 --> 00:22:17,000
cluster, launches your containers, launches all your experiments, and then you can just

301
00:22:17,000 --> 00:22:23,200
focus on writing your TensorFlow and other Python code and not even need to necessarily

302
00:22:23,200 --> 00:22:27,120
know that it's running inside a Docker container or really that it's a Kubernetes cluster

303
00:22:27,120 --> 00:22:28,120
even.

304
00:22:28,120 --> 00:22:33,880
And are the researchers there typically working with like Git repositories?

305
00:22:33,880 --> 00:22:38,960
So are they checking in code and are you pulling from those Git repos when you're containerizing

306
00:22:38,960 --> 00:22:43,040
or is it all just slurping stuff up from directories on the desktop?

307
00:22:43,040 --> 00:22:48,480
Yeah, we use Git quite extensively here.

308
00:22:48,480 --> 00:22:54,440
And I guess, again, kind of a mix of things depending on the workflow that researchers

309
00:22:54,440 --> 00:23:00,440
and most convenient, yeah, some of them are launching their experiments from code that

310
00:23:00,440 --> 00:23:05,520
they have locally, although that's almost always in a Git repository just from their local

311
00:23:05,520 --> 00:23:14,520
checkout for other ones, yeah, maybe that it's built by a container, a build engine

312
00:23:14,520 --> 00:23:18,800
like QA, and then we're deploying a container from that registry.

313
00:23:18,800 --> 00:23:26,240
Earlier on you mentioned that your use of Kubernetes is kind of evolved from when you started

314
00:23:26,240 --> 00:23:35,360
to today, can you talk a little bit about that journey in general, how your use of Kubernetes

315
00:23:35,360 --> 00:23:41,840
has evolved as Kubernetes has evolved, what some of the big challenges with the different

316
00:23:41,840 --> 00:23:48,040
phases that you've gone through with your architecture has been and how you've overcome

317
00:23:48,040 --> 00:23:49,040
those?

318
00:23:49,040 --> 00:23:52,080
Yeah, it has changed in a number of ways.

319
00:23:52,080 --> 00:23:56,080
Certainly one is the scale, our clusters have gotten way bigger than when I first started

320
00:23:56,080 --> 00:24:02,080
but they've also sort of transitioned in their design.

321
00:24:02,080 --> 00:24:08,800
So when I first joined the design that we had was a single production Kubernetes cluster

322
00:24:08,800 --> 00:24:18,840
and it was a single multi-cloud cluster, so it spanned both AWS and Azure and it sort

323
00:24:18,840 --> 00:24:24,000
of led to some behavior that was difficult to reason about, I don't think for how many

324
00:24:24,000 --> 00:24:29,040
people run Kubernetes clusters where a single cluster is cross-cloud and especially when

325
00:24:29,040 --> 00:24:35,160
we wanted to scale that up significantly to thousands of machines, we switched over

326
00:24:35,160 --> 00:24:45,280
to a model where we have multiple Kubernetes clusters and each one is limited to a specific

327
00:24:45,280 --> 00:24:53,280
physical region, so either are on-prem clusters or availability zone in the cloud.

328
00:24:53,280 --> 00:24:58,640
And that made it a little easier to reason about some of the network aspects of it and

329
00:24:58,640 --> 00:25:03,000
not need to worry about some of the cross-cloud issues that we'd run into.

330
00:25:03,000 --> 00:25:05,200
So that really allowed us to scale more.

331
00:25:05,200 --> 00:25:09,680
Can you give me an example of some of the issues that you ran into that kind of led to that

332
00:25:09,680 --> 00:25:10,680
shift?

333
00:25:10,680 --> 00:25:12,360
What were the kinds of things that you'd see?

334
00:25:12,360 --> 00:25:13,360
Yeah, for sure.

335
00:25:13,360 --> 00:25:22,000
Also because we were needed to run this cross-cloud, we had the control plan in AWS and then we

336
00:25:22,000 --> 00:25:29,440
had a bunch of IP sect tunnels that connected to the other availability zones where the

337
00:25:29,440 --> 00:25:31,040
workers were running in.

338
00:25:31,040 --> 00:25:35,840
So we would have one set of workers running in like Azure's East Coast region and one

339
00:25:35,840 --> 00:25:42,640
West Coast region and all of these would talk back to the Kubernetes control plan over

340
00:25:42,640 --> 00:25:45,720
this IP sect tunnel back in AWS.

341
00:25:45,720 --> 00:25:52,720
But this meant that that IP sect tunnel was kind of a single point of failure if that went

342
00:25:52,720 --> 00:25:53,720
down.

343
00:25:53,720 --> 00:25:57,080
Not necessarily the entire cluster, but a large section of the cluster because that whole

344
00:25:57,080 --> 00:26:01,560
group of workers would now be cut off from the control plan.

345
00:26:01,560 --> 00:26:04,800
So yeah, there was that problem.

346
00:26:04,800 --> 00:26:12,240
Then there was also the problem of if you wanted to use something like Amazon's file system

347
00:26:12,240 --> 00:26:17,840
like in on the name right now, EFS, I think it's called EBS or S3.

348
00:26:17,840 --> 00:26:25,320
Yeah, S3 I guess was a little bit less problematic, but certainly EBS, that's something that you'd

349
00:26:25,320 --> 00:26:26,320
like to use.

350
00:26:26,320 --> 00:26:30,480
But now if some of your workers are in Azure, then you can only use it in half your cluster

351
00:26:30,480 --> 00:26:35,440
and then that's really confusing if you end up with sometimes you can use it and sometimes

352
00:26:35,440 --> 00:26:44,440
you can't and then I guess one of the most difficult to overcome problems was that if you

353
00:26:44,440 --> 00:26:48,680
scheduled your job into the cluster, it was possible for your job to get split so that

354
00:26:48,680 --> 00:26:51,840
half of it was in Azure and half of it was in AWS.

355
00:26:51,840 --> 00:26:56,240
And now if there was any network communication between them, that would just go very, very

356
00:26:56,240 --> 00:26:57,240
poorly.

357
00:26:57,240 --> 00:26:58,240
Right.

358
00:26:58,240 --> 00:26:59,240
Right.

359
00:26:59,240 --> 00:27:04,600
So you had to be very careful to specify that your job should only be in one cloud and

360
00:27:04,600 --> 00:27:05,600
not the other.

361
00:27:05,600 --> 00:27:06,600
Okay.

362
00:27:06,600 --> 00:27:11,760
But kind of all of that led to us just changing the model so that one Kubernetes cluster, one

363
00:27:11,760 --> 00:27:14,600
availability is out, then you don't have to worry about any of that.

364
00:27:14,600 --> 00:27:15,600
Right.

365
00:27:15,600 --> 00:27:16,600
Yeah, it's interesting.

366
00:27:16,600 --> 00:27:22,240
I think in the Kubernetes community, certainly there's a lot of talk about hybrid cloud and

367
00:27:22,240 --> 00:27:29,240
multi cloud and even the ability to run a single Kubernetes cluster spanning multiple

368
00:27:29,240 --> 00:27:30,240
clouds.

369
00:27:30,240 --> 00:27:35,680
It's actually document and not necessarily thoroughly, but it's there, but I don't get

370
00:27:35,680 --> 00:27:40,760
the impression that a lot of people do it for reasons like the ones that you're describing.

371
00:27:40,760 --> 00:27:41,760
Yeah.

372
00:27:41,760 --> 00:27:46,040
So some tough problems and we were trying to solve them pretty early in the development

373
00:27:46,040 --> 00:27:47,040
of Kubernetes.

374
00:27:47,040 --> 00:27:53,200
I imagine if we tried to do it now, the experience might be a little smoother, but some of those

375
00:27:53,200 --> 00:27:56,160
are just difficult problems to solve.

376
00:27:56,160 --> 00:28:00,840
And then I guess the other area that our Kubernetes clusters have evolved significantly

377
00:28:00,840 --> 00:28:05,240
since I joined is our on-prem cluster.

378
00:28:05,240 --> 00:28:08,400
So that did not exist when I joined.

379
00:28:08,400 --> 00:28:13,520
And as one that we've been continuing to scale up, just as we have more and more demand

380
00:28:13,520 --> 00:28:18,400
for cutting edge hardware that's not available in the clouds.

381
00:28:18,400 --> 00:28:26,080
And so with that particular motivation, front and center, in particular cutting edge hardware,

382
00:28:26,080 --> 00:28:32,640
for example, you have access to an Nvidia DGX, I'm sure you have access to all the latest

383
00:28:32,640 --> 00:28:34,520
and greatest GPUs.

384
00:28:34,520 --> 00:28:42,200
Are you able to utilize Kubernetes features like labels and things like that to target

385
00:28:42,200 --> 00:28:48,200
workloads to these specific cutting edge hardware or do you just kind of throw them all

386
00:28:48,200 --> 00:28:53,120
into the pool and let them land wherever they land and kind of each incremental hardware

387
00:28:53,120 --> 00:28:56,120
piece just adds what it can bring?

388
00:28:56,120 --> 00:28:57,760
Yeah, so we definitely do a bit of both.

389
00:28:57,760 --> 00:29:05,640
We use labels pretty extensively to provide finer grand control of exactly what hardware

390
00:29:05,640 --> 00:29:08,040
you get scheduled on as much as possible.

391
00:29:08,040 --> 00:29:14,680
We also try to keep our clusters homogeneous or mostly homogeneous so that you don't need

392
00:29:14,680 --> 00:29:19,480
to put too much thought into specifying exactly what you want, but yeah, it's a bit of

393
00:29:19,480 --> 00:29:20,480
both.

394
00:29:20,480 --> 00:29:26,440
Do you find in general that the out of the box Kubernetes scheduler does what you need

395
00:29:26,440 --> 00:29:28,440
for these types of workloads?

396
00:29:28,440 --> 00:29:35,120
Yeah, so we've made some various tweaks to it, although we're still using the upstream

397
00:29:35,120 --> 00:29:39,720
scheduler and I've just made like config changes and then sort of built some services on

398
00:29:39,720 --> 00:29:42,640
top of Kubernetes.

399
00:29:42,640 --> 00:29:50,360
We now have a system that uses the mutating webhooks feature along with a controller

400
00:29:50,360 --> 00:29:55,240
and taints that sort of manage the resources in our cluster so that certain groups of

401
00:29:55,240 --> 00:29:58,240
machines can be dedicated to a certain team.

402
00:29:58,240 --> 00:30:04,120
And then when members of that team submit their pods, those pods will receive a toleration

403
00:30:04,120 --> 00:30:06,720
that allows them to run on those reserved machines.

404
00:30:06,720 --> 00:30:10,560
Is that an alternative to using namespaces or is that something that works in conjunction

405
00:30:10,560 --> 00:30:13,560
with namespaces to provide that feature?

406
00:30:13,560 --> 00:30:17,280
Yeah, so it works in conjunction with namespaces.

407
00:30:17,280 --> 00:30:24,280
So here of every researcher their own namespace and then most teams also get their own namespace

408
00:30:24,280 --> 00:30:28,080
and they can have one if they ask for it, just kind of depends on how they like to structure

409
00:30:28,080 --> 00:30:29,560
their workflow.

410
00:30:29,560 --> 00:30:37,040
And then we integrate all of this back into our directory service so that it knows which

411
00:30:37,040 --> 00:30:43,560
people are on which team and then the pods in those namespaces can then be granted a

412
00:30:43,560 --> 00:30:47,160
toleration to run on capacity that's reserved for that team.

413
00:30:47,160 --> 00:30:51,760
So yeah, that's an area where we consider building all of this into a custom scheduler

414
00:30:51,760 --> 00:30:56,800
but it seemed much easier to support if we just built it as a feature on top of the Kubernetes

415
00:30:56,800 --> 00:31:01,240
APIs that could then interact with the scheduler in the normal way.

416
00:31:01,240 --> 00:31:06,880
And I guess the one significant config change that we've made to the default Kubernetes

417
00:31:06,880 --> 00:31:14,400
scheduler is that the default one prefers to spread out the pods in like a deployment

418
00:31:14,400 --> 00:31:18,120
or a job as much as possible, which is good for fault tolerance.

419
00:31:18,120 --> 00:31:23,720
But for us, we tend to want the opposite, it would actually like as many pods as possible

420
00:31:23,720 --> 00:31:29,720
to be packed into single machines so that it leaves other machines completely empty.

421
00:31:29,720 --> 00:31:35,200
And that's good for two reasons one is that if someone submits a bunch of small pods,

422
00:31:35,200 --> 00:31:38,000
we don't want them to fill up tiny pieces of every machine.

423
00:31:38,000 --> 00:31:43,160
We want them to packed into a few so that their whole machines available for people that

424
00:31:43,160 --> 00:31:45,320
want to use an entire machine.

425
00:31:45,320 --> 00:31:49,520
And then the other thing that it really helps us with is auto scaling.

426
00:31:49,520 --> 00:31:54,800
So in our cloud clusters, we auto scaled them up and down depending on demand.

427
00:31:54,800 --> 00:32:00,160
And so you want to have your machines utilized as heavily as possible so that you can scale

428
00:32:00,160 --> 00:32:02,640
in the unused machines.

429
00:32:02,640 --> 00:32:09,880
What does a small mean in that context is it's based on kind of a real time metric like utilization

430
00:32:09,880 --> 00:32:11,960
or is it something else?

431
00:32:11,960 --> 00:32:12,960
Oh, yeah.

432
00:32:12,960 --> 00:32:17,600
So yeah, in terms of small, I mean like how many resources are being requested.

433
00:32:17,600 --> 00:32:25,200
So I would consider it small if you were requesting only one GPU or maybe you're only requesting

434
00:32:25,200 --> 00:32:33,520
like half a dozen CPU cores, whereas like our bigger pods, those are usually requesting

435
00:32:33,520 --> 00:32:40,680
HGPUs or maybe four GPUs and then they're requesting dozens or maybe even 50 or 60 CPU

436
00:32:40,680 --> 00:32:41,680
cores.

437
00:32:41,680 --> 00:32:53,280
How do you manage if at all the issue of folks submitting jobs, IE pods and reserving

438
00:32:53,280 --> 00:32:58,120
essentially these resources, but the pods not actually, you know, being idle essentially.

439
00:32:58,120 --> 00:33:00,480
Is that something that you actively deal with?

440
00:33:00,480 --> 00:33:04,000
Yeah, so yeah, I guess there's kind of two parts of it.

441
00:33:04,000 --> 00:33:10,920
We just kind of trust all of our researchers to use the clusters resources well.

442
00:33:10,920 --> 00:33:16,600
And so we don't have any like checking of are you using the resources that you requested.

443
00:33:16,600 --> 00:33:26,000
So what we do have is a sort of internal billing system where we monitor how many pods you

444
00:33:26,000 --> 00:33:29,800
had running and how many resources they requested and how long those resources were requested

445
00:33:29,800 --> 00:33:31,280
for.

446
00:33:31,280 --> 00:33:34,920
And then it rolls up your billing every day.

447
00:33:34,920 --> 00:33:42,360
And you can see a report of this is how much money effectively you spent on experiments.

448
00:33:42,360 --> 00:33:46,280
And so yeah, if you were leaving a bunch of pods running the ride, it would show up in

449
00:33:46,280 --> 00:33:47,280
your bill.

450
00:33:47,280 --> 00:33:52,560
And is that something that you had to build or was that a project in the Kubernetes ecosystem

451
00:33:52,560 --> 00:33:56,200
that you were just able to kind of turn on and point to your clusters?

452
00:33:56,200 --> 00:34:02,200
Yeah, it's something that we built, although it's actually very little code will probably

453
00:34:02,200 --> 00:34:06,240
open source it if someone else doesn't open source a better solution that we end up switching

454
00:34:06,240 --> 00:34:07,240
to.

455
00:34:07,240 --> 00:34:15,520
But it's based on Prometheus, so Prometheus already monitors all of the metrics of what

456
00:34:15,520 --> 00:34:19,880
pods were running when were they running for how much did they request.

457
00:34:19,880 --> 00:34:26,280
And so really this is just a pretty simple Python script that query is Prometheus aggregates

458
00:34:26,280 --> 00:34:33,080
all of that data groups it by namespace and then integrates with our directory service

459
00:34:33,080 --> 00:34:38,400
to figure out which team and people's namespace is belong to and that rolls up all the cost

460
00:34:38,400 --> 00:34:39,400
by team.

461
00:34:39,400 --> 00:34:40,400
Okay, cool.

462
00:34:40,400 --> 00:34:46,200
And it sounds like you're running upstream and you mentioned Prometheus, what other tools

463
00:34:46,200 --> 00:34:51,880
are you running or projects in conjunction with your use of Kubernetes?

464
00:34:51,880 --> 00:34:53,560
Yeah, let's see.

465
00:34:53,560 --> 00:35:01,800
So we use the cluster auto-scaler, so we use that for scaling our clusters.

466
00:35:01,800 --> 00:35:11,200
It's Prometheus, we use Heapster, we use a flannel for an Overland network, we've got

467
00:35:11,200 --> 00:35:18,880
some deployments of the cluster, although that's sort of lusterfully related to Kubernetes.

468
00:35:18,880 --> 00:35:21,800
And I guess those are kind of the main ones.

469
00:35:21,800 --> 00:35:24,440
Okay, you mentioned Gluster.

470
00:35:24,440 --> 00:35:30,480
What are the different ways that you deal with storage in the context of these clusters?

471
00:35:30,480 --> 00:35:36,400
Yeah, so that's one of the areas that we're, I feel like we have an OK solution now, but

472
00:35:36,400 --> 00:35:39,040
I'd really like us to have a great solution.

473
00:35:39,040 --> 00:35:41,360
It's a tough problem.

474
00:35:41,360 --> 00:35:43,960
It is a very tough problem.

475
00:35:43,960 --> 00:35:50,160
So we've got Gluster deployed and it's been reasonably good.

476
00:35:50,160 --> 00:35:56,360
We haven't quite gotten the performance that we want out of it, but it's definitely convenient.

477
00:35:56,360 --> 00:36:03,480
And yeah, teams certainly have some data storage in S3 and Google Cloud Store and Azure's

478
00:36:03,480 --> 00:36:04,480
Blob Store.

479
00:36:04,480 --> 00:36:08,320
Yeah, so I spread out over those.

480
00:36:08,320 --> 00:36:17,920
For our Cloud Kubernetes clusters, we use whatever sort of EBS equivalent they have.

481
00:36:17,920 --> 00:36:21,680
Google, that's their persistent volumes, that's really convenient.

482
00:36:21,680 --> 00:36:26,680
For our on-prem cluster, I think what we're going to end up deploying is Saf, possibly through

483
00:36:26,680 --> 00:36:31,800
the REC project, but we haven't started on that yet.

484
00:36:31,800 --> 00:36:35,560
So it's a little bit hard to say what we're going to end up with, but that seems like

485
00:36:35,560 --> 00:36:37,280
it has a lot of promise.

486
00:36:37,280 --> 00:36:46,920
It sounds like then there's a strong inclination towards a distributed storage solution that

487
00:36:46,920 --> 00:36:53,160
kind of runs on the existing compute infrastructure as opposed to some standalone monolithic kind

488
00:36:53,160 --> 00:36:55,720
of NFS type of thing.

489
00:36:55,720 --> 00:37:02,240
Yeah, so I guess what we have found, so I guess Gluster sort of falls more into the single

490
00:37:02,240 --> 00:37:04,920
shared file system like NFS.

491
00:37:04,920 --> 00:37:10,200
And what we've found is that the performance hasn't been quite as good as what we want,

492
00:37:10,200 --> 00:37:14,640
although maybe that could be resolved by tuning some of the configuration settings or adding

493
00:37:14,640 --> 00:37:22,400
more hardware, but the other issue has been fault domains that when one team really starts

494
00:37:22,400 --> 00:37:29,760
hammering the storage, then it causes a disruption in service for everyone.

495
00:37:29,760 --> 00:37:35,080
And so one of the potential benefits that we see of something like Saf is hopefully better

496
00:37:35,080 --> 00:37:42,400
fault isolation where that won't happen because you'll have your separate or weblock devices.

497
00:37:42,400 --> 00:37:50,120
You mentioned a flannel for networking, how has that dealing with the networking been

498
00:37:50,120 --> 00:37:54,160
since you've gone away from the multi-cloud?

499
00:37:54,160 --> 00:38:00,440
Is that kind of addressed all your issues or is it still a challenge for you?

500
00:38:00,440 --> 00:38:06,680
Yeah, I'd say that that really simplified things and these days I don't worry too much

501
00:38:06,680 --> 00:38:08,720
about our networking.

502
00:38:08,720 --> 00:38:16,720
So flannel has worked out nicely, we have integrated all of our clusters together via VPN so the

503
00:38:16,720 --> 00:38:22,280
researchers can connect all of them very easily and that's worked out well.

504
00:38:22,280 --> 00:38:27,560
And with our on-prem cluster we've kind of figured out some of the performance aspects originally

505
00:38:27,560 --> 00:38:33,560
when we had deployed flannel, we had been deployed it in a way that wasn't as performant

506
00:38:33,560 --> 00:38:38,280
but when we switched over to, so we're now using their direct routing feature.

507
00:38:38,280 --> 00:38:41,800
And that really resolved the performance problems that we were seeing there.

508
00:38:41,800 --> 00:38:52,320
So yeah, it's pretty much been a non-issue since we made some re-architectures to our clusters.

509
00:38:52,320 --> 00:39:01,120
And one of the advantages of the single Kubernetes cluster as opposed to a federated would

510
00:39:01,120 --> 00:39:06,600
be that the researchers don't need to think about where a particular workload goes.

511
00:39:06,600 --> 00:39:10,960
Have you found another way to abstract that or do they just have to, you know, do they

512
00:39:10,960 --> 00:39:14,920
target a specific cluster when they're deploying workloads?

513
00:39:14,920 --> 00:39:18,120
Yeah, for sure.

514
00:39:18,120 --> 00:39:25,360
So our experimentation tools handle a little bit of that of trying to abstract it away so

515
00:39:25,360 --> 00:39:29,160
that the researchers don't need to worry about it so much.

516
00:39:29,160 --> 00:39:35,600
Longer term I've been sort of loosely following the upstream work that's being done in federated

517
00:39:35,600 --> 00:39:40,000
Kubernetes and how that all is going to work.

518
00:39:40,000 --> 00:39:44,520
I think at some point in the future, we're really hoping to switch over to a model like

519
00:39:44,520 --> 00:39:49,760
that where there is a federated control plane that can then handle a schedule length that

520
00:39:49,760 --> 00:39:54,640
looks like a single cluster, but yeah, not there yet.

521
00:39:54,640 --> 00:39:56,480
Right, right, right.

522
00:39:56,480 --> 00:40:02,920
Speaking of which, what's been your experience with Kubernetes as it evolves?

523
00:40:02,920 --> 00:40:08,520
How have changes on the Kubernetes side changed the way you use it?

524
00:40:08,520 --> 00:40:15,280
Maybe starting, was there any, did you run into any hurdles early on that you had to

525
00:40:15,280 --> 00:40:22,480
maybe develop around or hack or hound that Kubernetes eventually caught up with what

526
00:40:22,480 --> 00:40:29,160
your needs were, whether it's in terms of its operation or scalability or reliability

527
00:40:29,160 --> 00:40:33,720
or things like that, or how's that whole experience been for you?

528
00:40:33,720 --> 00:40:35,520
Yeah, definitely.

529
00:40:35,520 --> 00:40:39,200
So I'd say in general, super positive.

530
00:40:39,200 --> 00:40:45,040
The direction that Kubernetes has been going in has really just continued to address more

531
00:40:45,040 --> 00:40:48,640
and more of the pain points that we've seen.

532
00:40:48,640 --> 00:40:55,000
So yeah, I guess a few specific ones, and we were probably one of the first people using

533
00:40:55,000 --> 00:41:04,480
Kubernetes to schedule GPU devices, and so we were using the alpha feature that added support

534
00:41:04,480 --> 00:41:12,200
for that, I believe as soon as it was released, and so we did have to build up some additional

535
00:41:12,200 --> 00:41:20,360
tooling for handling that and making it work seamlessly, like in particular, the device

536
00:41:20,360 --> 00:41:23,320
drivers need to be mounted into your container.

537
00:41:23,320 --> 00:41:27,480
And so we had kind of a whole way of managing that with Simbling so that people could do

538
00:41:27,480 --> 00:41:28,480
it.

539
00:41:28,480 --> 00:41:32,320
Now that Nvidia has released their device plugin, that's kind of all been automated for

540
00:41:32,320 --> 00:41:37,520
us, and so we switched over to that in all of our clusters, and it has simplified all

541
00:41:37,520 --> 00:41:43,520
of the management of the GPU devices there, and also like on the scalability side, I think

542
00:41:43,520 --> 00:41:49,640
Kubernetes now officially has been tested up to like 5,000 nodes in the past.

543
00:41:49,640 --> 00:41:58,400
That was not as true, and we had to do something to work around various scalability issues,

544
00:41:58,400 --> 00:42:04,160
but now a lot of that has just been handled by upstream where they have optimized things.

545
00:42:04,160 --> 00:42:09,840
You mentioned in terms of things that you're looking forward to, you mentioned some of the

546
00:42:09,840 --> 00:42:14,640
work around federated Kubernetes, are there other things that you see on the horizon

547
00:42:14,640 --> 00:42:21,000
for Kubernetes or more broadly that ecosystem that you're excited about?

548
00:42:21,000 --> 00:42:24,280
Yeah, definitely federated Kubernetes.

549
00:42:24,280 --> 00:42:29,040
Yeah, earlier I mentioned the Ruck project, and that's definitely one that I've been

550
00:42:29,040 --> 00:42:34,240
following and have quite a bit of an interest in.

551
00:42:34,240 --> 00:42:39,480
It looks like it might make it really easy to run Seth at our on-prem clusters, so that

552
00:42:39,480 --> 00:42:42,080
would be really exciting for us.

553
00:42:42,080 --> 00:42:47,280
So I'm hoping that that project continues to get more traction.

554
00:42:47,280 --> 00:42:54,760
And beyond that, one of the things that we're looking at next is on the network side.

555
00:42:54,760 --> 00:43:01,720
We're planning to enable Rocky, the RDMA over-converged Ethernet protocol in our on-prem

556
00:43:01,720 --> 00:43:07,760
clusters, and so this is one of the things that we can't get in the cloud, that's why

557
00:43:07,760 --> 00:43:09,320
we have our on-prem cluster.

558
00:43:09,320 --> 00:43:16,440
So I'm not familiar with that one, you said Rocky, and RDMA would be for remote memory

559
00:43:16,440 --> 00:43:17,440
access.

560
00:43:17,440 --> 00:43:18,440
Yeah, exactly.

561
00:43:18,440 --> 00:43:19,440
Are you familiar with Infiniband?

562
00:43:19,440 --> 00:43:20,640
Oh, yeah, sure.

563
00:43:20,640 --> 00:43:27,440
Yeah, okay, so Rocky is basically the same thing as Infiniband, but it goes over Ethernet

564
00:43:27,440 --> 00:43:31,800
instead of the custom Infiniband protocol.

565
00:43:31,800 --> 00:43:32,800
Oh, cool.

566
00:43:32,800 --> 00:43:39,040
Yeah, so it's really cool, because then you can have all of your normal Ethernet workloads

567
00:43:39,040 --> 00:43:43,440
that are doing TCP or UDP or whatever.

568
00:43:43,440 --> 00:43:49,800
And then you can also have your RDMA applications that are going over exactly the same links.

569
00:43:49,800 --> 00:43:51,840
And yeah, so we're pretty excited about that.

570
00:43:51,840 --> 00:43:58,120
I think that's really going to improve both throughput that we can get, and also really

571
00:43:58,120 --> 00:43:59,880
cut down on latency.

572
00:43:59,880 --> 00:44:07,160
And Infiniband has become pretty standard for some of these high performance computing workloads,

573
00:44:07,160 --> 00:44:09,240
because of its performance.

574
00:44:09,240 --> 00:44:12,280
Are you able to, how close do you get to that?

575
00:44:12,280 --> 00:44:14,600
And this is over 100G Ethernet?

576
00:44:14,600 --> 00:44:21,080
Yeah, so we've done some testing on 100G Ethernet, and I believe have seen up to like

577
00:44:21,080 --> 00:44:23,400
97% like utilization, so.

578
00:44:23,400 --> 00:44:24,400
Oh, wow.

579
00:44:24,400 --> 00:44:25,400
Pretty awesome.

580
00:44:25,400 --> 00:44:31,880
If I remember correctly, on the Infiniband side, correct me if I'm mischaracterizing this,

581
00:44:31,880 --> 00:44:40,080
my impression is that it's been limited to kind of these niche use cases like HPC, because

582
00:44:40,080 --> 00:44:45,840
it requires that the applications need to be modified to take advantage of this memory

583
00:44:45,840 --> 00:44:46,920
access.

584
00:44:46,920 --> 00:44:47,920
Is that true?

585
00:44:47,920 --> 00:44:51,120
And will the same thing need to happen with Rocky?

586
00:44:51,120 --> 00:44:53,440
Yeah, so it is true.

587
00:44:53,440 --> 00:44:56,600
And yes, the same thing will need to happen.

588
00:44:56,600 --> 00:44:59,360
But at least you don't need a whole separate network infrastructure.

589
00:44:59,360 --> 00:45:00,360
Right.

590
00:45:00,360 --> 00:45:05,560
At least you don't need to buy completely new network cards and new switches and everything.

591
00:45:05,560 --> 00:45:08,040
So then it can just happen at the application level.

592
00:45:08,040 --> 00:45:13,120
And really, what I'm hoping will happen is that we can handle all of it in the framework.

593
00:45:13,120 --> 00:45:18,080
And then you'll just have like a, you know, op that you plug into your TensorFlow graph,

594
00:45:18,080 --> 00:45:25,400
which is the, you know, all reduce over our DMA operation.

595
00:45:25,400 --> 00:45:31,000
And then people that are doing research won't have to worry about any of that, but that's

596
00:45:31,000 --> 00:45:34,920
still to be seen how that works out and how all of this is going to integrate into the

597
00:45:34,920 --> 00:45:35,920
Kubernetes ecosystem.

598
00:45:35,920 --> 00:45:36,920
Nice.

599
00:45:36,920 --> 00:45:37,920
Nice.

600
00:45:37,920 --> 00:45:42,440
And you mentioned something that made me realize that one area that we skipped was how

601
00:45:42,440 --> 00:45:45,320
you're doing distributed training.

602
00:45:45,320 --> 00:45:46,520
Can you talk a little bit about that?

603
00:45:46,520 --> 00:45:51,360
There's a number of solutions for TensorFlow, for example, there's out of the box, there's

604
00:45:51,360 --> 00:45:55,960
porovod, there are some others like how have you approached distributed training?

605
00:45:55,960 --> 00:45:56,960
Yeah.

606
00:45:56,960 --> 00:46:02,800
So that's one where we have a number of different approaches.

607
00:46:02,800 --> 00:46:06,080
So we're definitely using porovod.

608
00:46:06,080 --> 00:46:07,080
I've used it a bit.

609
00:46:07,080 --> 00:46:09,240
I know a bunch of other teams are using it.

610
00:46:09,240 --> 00:46:12,440
It's worked out quite well for us.

611
00:46:12,440 --> 00:46:17,880
We have some people that are just like directly using MPI for Pi.

612
00:46:17,880 --> 00:46:22,040
I think that has also worked well.

613
00:46:22,040 --> 00:46:28,480
I believe there's at least one custom implementation of all reduce, and one of our teams has written.

614
00:46:28,480 --> 00:46:33,600
So yeah, kind of a few different solutions haven't converged on anything particular.

615
00:46:33,600 --> 00:46:39,760
And the idea is, in part, because everything is containerized, they just containerized

616
00:46:39,760 --> 00:46:45,840
those components of their distributed training, deploy them out via Kubernetes, and it all

617
00:46:45,840 --> 00:46:48,240
kind of works like anything else.

618
00:46:48,240 --> 00:46:49,240
Exactly.

619
00:46:49,240 --> 00:46:50,240
Cool.

620
00:46:50,240 --> 00:46:56,520
Well, any thoughts or words of wisdom for folks that are interested in Kubernetes as a platform

621
00:46:56,520 --> 00:47:00,680
for ML and DL and just getting started?

622
00:47:00,680 --> 00:47:01,680
Yeah.

623
00:47:01,680 --> 00:47:09,280
I'd say that the portability aspect of it and just the use of running it and scaling it is

624
00:47:09,280 --> 00:47:12,400
something that has really benefited us a lot.

625
00:47:12,400 --> 00:47:16,640
Especially if those are important factors that are recommended.

626
00:47:16,640 --> 00:47:23,440
And in terms of ease of running it, sometimes Kubernetes or at least early on Kubernetes had

627
00:47:23,440 --> 00:47:26,640
this reputation for not being particularly easy.

628
00:47:26,640 --> 00:47:34,040
What's the support burden on your team for supporting the cluster bin like or the clusters?

629
00:47:34,040 --> 00:47:36,680
Granted that you're already had a pretty tremendous scale.

630
00:47:36,680 --> 00:47:45,880
Yeah, well, and I would say that that early reputation did reflect our experience too.

631
00:47:45,880 --> 00:47:50,880
When I joined, we had a lot of problems that we were resulting.

632
00:47:50,880 --> 00:47:55,400
And there was a pretty non-trivial support burden.

633
00:47:55,400 --> 00:48:02,460
But both as Kubernetes has evolved and as we have evolved our way of managing it and

634
00:48:02,460 --> 00:48:08,440
understood some of the aspects they need to pay attention to, like EtsyD in particular,

635
00:48:08,440 --> 00:48:14,840
really want to make sure that you've provisioned enough IOPS and everything for that.

636
00:48:14,840 --> 00:48:22,000
Now we've gotten to a point where I would say there's a very, very little maintenance burden.

637
00:48:22,000 --> 00:48:29,440
We have an on-call rotation, of course, but in terms of critical issues, I'd say it's

638
00:48:29,440 --> 00:48:33,240
rare that we have even one problem a month.

639
00:48:33,240 --> 00:48:34,240
Oh, wow.

640
00:48:34,240 --> 00:48:39,400
So let me re-ask that last question about words of wisdom and light of all of the kind of

641
00:48:39,400 --> 00:48:45,280
revealed wisdom over the past coming up on two years now that's gone into allowing

642
00:48:45,280 --> 00:48:48,640
it to be easy to maintain.

643
00:48:48,640 --> 00:48:53,720
You mentioned, make sure you provision enough IOPS to your EtsyD.

644
00:48:53,720 --> 00:48:58,920
What are some of the other things that folks need to do in order to have a good experience

645
00:48:58,920 --> 00:49:01,760
managing Kubernetes for these kinds of workloads?

646
00:49:01,760 --> 00:49:02,760
Yeah.

647
00:49:02,760 --> 00:49:09,320
So we actually published a whole blog post back in January or February that talks about some

648
00:49:09,320 --> 00:49:10,560
of the top ones.

649
00:49:10,560 --> 00:49:15,080
So yeah, definitely, EtsyD is one of those.

650
00:49:15,080 --> 00:49:18,760
And most of these, there are really issues that you're only going to hit once you go

651
00:49:18,760 --> 00:49:20,600
beyond a few hundred machines.

652
00:49:20,600 --> 00:49:23,960
So you can run it at a pretty good scale and not have to worry about much of it.

653
00:49:23,960 --> 00:49:28,200
But yeah, once you get into the thousand or multiple thousands of machines, there's a bunch

654
00:49:28,200 --> 00:49:29,200
of stuff to be aware of.

655
00:49:29,200 --> 00:49:36,560
So, yeah, EtsyD was one of them ran into some funny network issues.

656
00:49:36,560 --> 00:49:42,400
I spent a good couple of days learning all about ARP caches and how those work.

657
00:49:42,400 --> 00:49:47,200
And it was predical that I had learned about back in college and figured out never needs

658
00:49:47,200 --> 00:49:48,200
to know about again.

659
00:49:48,200 --> 00:49:53,600
But very important to make sure that the ARP cache is big enough.

660
00:49:53,600 --> 00:50:00,320
The combination of using flannel means that you need a significant amount of space in

661
00:50:00,320 --> 00:50:01,640
your ARP cache.

662
00:50:01,640 --> 00:50:06,480
If you've got thousands of containers that are talking to each other or talking to a single

663
00:50:06,480 --> 00:50:09,400
one, like the CUBE DNS service.

664
00:50:09,400 --> 00:50:15,840
So there's a few other issues that we enumerated in that blog post, but it's really not stuff

665
00:50:15,840 --> 00:50:18,440
that you're going to hit until you've got many hundreds of machines.

666
00:50:18,440 --> 00:50:21,480
Well, we'll link to that in the show notes for sure.

667
00:50:21,480 --> 00:50:25,760
Chris, thanks so much for taking the time to chat with me about this super interesting

668
00:50:25,760 --> 00:50:26,760
stuff.

669
00:50:26,760 --> 00:50:33,480
Likewise, yeah, thanks so much for having me.

670
00:50:33,480 --> 00:50:36,280
Alright everyone, that's our show for today.

671
00:50:36,280 --> 00:50:41,080
For more information on Christopher or any of the topics covered in this episode, head

672
00:50:41,080 --> 00:50:46,880
on over to twimmelai.com slash talk slash 199.

673
00:50:46,880 --> 00:50:53,000
To learn more about our AI platform series or to get our e-books, visit twimmelai.com slash

674
00:50:53,000 --> 00:50:54,840
AI platforms.

675
00:50:54,840 --> 00:51:24,800
As always, thanks so much for listening and catch you next time.

