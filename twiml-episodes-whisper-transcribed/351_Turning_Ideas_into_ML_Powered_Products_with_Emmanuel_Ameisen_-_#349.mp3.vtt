WEBVTT

00:00.000 --> 00:13.400
Welcome to the Twimal AI Podcast.

00:13.400 --> 00:16.880
I'm your host, Sam Charrington.

00:16.880 --> 00:23.840
Hey, what's up everyone?

00:23.840 --> 00:28.320
I am super excited to bring you this interview with my friend, Emmanuel Amazon, whose

00:28.320 --> 00:34.120
new book, Building Machine Learning Powered Applications, just hit bookstores.

00:34.120 --> 00:38.880
In our conversation, as in the book, we explore how to approach machine learning projects

00:38.880 --> 00:44.960
systematically, from idea all the way to working product, including formulating your problem

00:44.960 --> 00:51.240
and creating a plan, building a working pipeline and initial data set, evolving your models,

00:51.240 --> 00:54.120
and deploying and monitoring them.

00:54.120 --> 00:58.120
These are the same concepts covered in a new study group we're launching starting this

00:58.120 --> 01:05.320
Saturday, February 22nd, around the AI Enterprise Workflow Specialization on Coursera.

01:05.320 --> 01:09.960
If you're an aspiring or practicing data scientist or ML developer, and you want to level

01:09.960 --> 01:14.760
up on the broader set of skills required to deliver models and business settings, I really

01:14.760 --> 01:16.560
encourage you to join us.

01:16.560 --> 01:23.200
To learn more about the study group, visit twimalai.com slash AIEW, where you can catch a recorded

01:23.200 --> 01:27.080
session I held with Ray Lopez, the instructor for the courses.

01:27.080 --> 01:32.040
There, you'll also find instructions for joining the study group, which is totally free,

01:32.040 --> 01:36.000
and how you can get a free month of access to Coursera to boot.

01:36.000 --> 01:41.120
Once again, the website is twimalai.com slash AIEW.

01:41.120 --> 01:46.040
For more on the interview and a link to the book, visit the show notes page at twimalai.com

01:46.040 --> 01:49.560
slash talk slash 349.

01:49.560 --> 01:56.040
And now on to the show.

01:56.040 --> 02:01.880
Alright everyone, I am on the line with Emanuel Amazing Emanuel is a machine learning engineer

02:01.880 --> 02:02.880
at Stripe.

02:02.880 --> 02:08.280
Emanuel, welcome finally, I should say, to the twimalai podcast.

02:08.280 --> 02:09.280
How are you my friend?

02:09.280 --> 02:10.280
Thanks for having me.

02:10.280 --> 02:11.280
I'm great.

02:11.280 --> 02:12.280
How are you?

02:12.280 --> 02:13.280
I am doing well.

02:13.280 --> 02:14.280
I'm doing well.

02:14.280 --> 02:18.720
So Emanuel and I have known each other for at least a couple years now, maybe more.

02:18.720 --> 02:25.360
When you met, when you were at insight, you were responsible for the insight data science

02:25.360 --> 02:27.360
AI program.

02:27.360 --> 02:29.280
What was your official responsibility?

02:29.280 --> 02:34.440
Because I've also, I've interviewed Ross on the show as well with insight.

02:34.440 --> 02:35.440
Yeah.

02:35.440 --> 02:36.440
What was your official role there?

02:36.440 --> 02:41.680
So I had a role very similar to Ross's, Ross was my equivalent in New York.

02:41.680 --> 02:44.520
So I was leading the AI program there.

02:44.520 --> 02:49.440
So we have a sort of professional education programs, fellowships in many different domains,

02:49.440 --> 02:55.120
state science, state engineering, and we have one in AI and so I was leading that one.

02:55.120 --> 02:59.840
And so usually we start these interviews by having folks share a little bit about their

02:59.840 --> 03:00.840
journey.

03:00.840 --> 03:07.640
Why don't you tell us how you got to Stripe and perhaps more importantly, how you got

03:07.640 --> 03:09.440
to being a published author.

03:09.440 --> 03:10.440
Congratulations.

03:10.440 --> 03:13.280
Yeah, happy to.

03:13.280 --> 03:20.520
So I started off, so I think like many people in this field, my fashion firm, I started

03:20.520 --> 03:24.480
with Jeff Hinton's Coursera classes way back in the day.

03:24.480 --> 03:25.480
Okay.

03:25.480 --> 03:29.400
I feel like for most reports, either Jeff Hinton or Andrew Eng that got them started.

03:29.400 --> 03:34.480
Team Andrew here, nothing against Jeff.

03:34.480 --> 03:38.480
And then after that, I started my career, my professional career, I did data science at

03:38.480 --> 03:43.280
the start up in the Bay Area, which got acquired by zip car later.

03:43.280 --> 03:46.800
I spent two years there and actually after that, I joined insight.

03:46.800 --> 03:51.480
So after having been a data scientist, I joined a role that was much more about professional

03:51.480 --> 03:53.680
education and mentorship.

03:53.680 --> 03:59.480
And so for a couple years at insight, I mentored in aggregate, it was over a hundred fellows

03:59.480 --> 04:04.760
that were PhDs and engineers that wanted to transition and get a job in like the field

04:04.760 --> 04:06.280
of machine learning.

04:06.280 --> 04:11.720
So that was amazing and through my work there, I learned a lot about what it takes to transition

04:11.720 --> 04:14.720
and what it takes to build successful ML projects.

04:14.720 --> 04:17.880
A lot of that is in the book you mentioned.

04:17.880 --> 04:22.040
After a couple of years there, I went to go back towards something more of a nice C role

04:22.040 --> 04:24.000
for me, more of an individual contributor.

04:24.000 --> 04:30.000
And so I went at Stripe because it sort of had the perfect blend of what I was looking

04:30.000 --> 04:34.680
for in a role, which was it blends very heavy and challenging machine learning with sort

04:34.680 --> 04:38.800
of very heavy engineering requirements, which I think is where the field is going in

04:38.800 --> 04:39.800
general.

04:39.800 --> 04:41.520
And so I wanted to do more of that.

04:41.520 --> 04:46.400
We haven't mentioned the title of the book yet, but it is building machine learning powered

04:46.400 --> 04:50.480
applications going from idea to product.

04:50.480 --> 04:52.680
When did the book become available?

04:52.680 --> 04:55.120
The book became available last week.

04:55.120 --> 04:56.120
Nice.

04:56.120 --> 04:57.120
Nice.

04:57.120 --> 05:00.960
And so I have one of the first copies of it right here in my hand and it was signed by

05:00.960 --> 05:01.960
you.

05:01.960 --> 05:02.960
Thank you very much.

05:02.960 --> 05:08.400
So the book has got this subtitle going from idea to product.

05:08.400 --> 05:11.240
Is it a conceptual book, a technical book?

05:11.240 --> 05:12.240
Yeah.

05:12.240 --> 05:13.480
That's a good question.

05:13.480 --> 05:16.800
The title comes from the desire scope of the book.

05:16.800 --> 05:23.200
So the desire scope is really to give tools to aspiring engineers and data scientists

05:23.200 --> 05:29.200
to go from sort of either a VM has an idea or you have an idea to you have something in

05:29.200 --> 05:34.840
production that is actually being used by real people.

05:34.840 --> 05:38.000
And it's a bit of a blend of conceptual and technical.

05:38.000 --> 05:41.000
It is technical in a sense that there are many code examples.

05:41.000 --> 05:45.000
There is a set of notebooks that accompany the book and there's actually an entire prototype

05:45.000 --> 05:47.560
application that we build together throughout the book.

05:47.560 --> 05:49.600
And at the end of the book, you can just, it has a GitHub repo.

05:49.600 --> 05:51.160
You can go and try it out.

05:51.160 --> 05:55.880
But it's also conceptual in a sense that a lot of these topics are more about how you

05:55.880 --> 05:59.840
frame problems than just like copying code off of Stack Overflow.

05:59.840 --> 06:04.520
And so there's interviews with data science leaders that have none of this sort of thing.

06:04.520 --> 06:08.320
There's an entire section about like data ethics and how you think about shipping models

06:08.320 --> 06:10.200
and when you shouldn't, shouldn't.

06:10.200 --> 06:12.400
So it's a bit of a hybrid book in that sense.

06:12.400 --> 06:13.400
Nice.

06:13.400 --> 06:14.400
Nice.

06:14.400 --> 06:19.360
I flipped through it and saw some of my favorite folks in here, Monica Burgatti and Rob

06:19.360 --> 06:22.800
Monroe and I guess we know some of the same people.

06:22.800 --> 06:23.800
Yeah.

06:23.800 --> 06:24.800
Yeah.

06:24.800 --> 06:25.800
Small world.

06:25.800 --> 06:31.000
There's things that I noticed and I haven't gone through them in a lot of detail.

06:31.000 --> 06:36.160
I mentioned to you that like I literally just got this out of the mail room here.

06:36.160 --> 06:43.040
But the structure of the book is that you develop a sample application.

06:43.040 --> 06:47.920
As you mentioned and the sample app is predictive text.

06:47.920 --> 06:50.880
How did you pick that app for context?

06:50.880 --> 06:51.880
Yeah.

06:51.880 --> 06:55.680
That was one of, first of all, that's a really good question because that was one of the

06:55.680 --> 07:00.120
parts that I went over the most times and just changed my mind, changed my mind very many

07:00.120 --> 07:03.040
times about which application should be the running example.

07:03.040 --> 07:06.680
And in fact, I had a conversation with Monica Burgatti where I pitched her on one of my

07:06.680 --> 07:11.600
initial ideas and she told me it was a terrible idea and I should definitely not what wasn't.

07:11.600 --> 07:18.480
I wanted to do something that was like it would listen to like politicians speeches and

07:18.480 --> 07:22.000
then compare with how they vote and tell you whether like what they were saying in their

07:22.000 --> 07:24.360
speeches sort of aligned with how they were actually voting.

07:24.360 --> 07:26.360
Like a fact checker kind of thing.

07:26.360 --> 07:27.360
Yeah.

07:27.360 --> 07:29.360
Like some sort of automatic fact checker.

07:29.360 --> 07:30.360
It would have been timely.

07:30.360 --> 07:31.360
Right.

07:31.360 --> 07:32.360
It would have been timely.

07:32.360 --> 07:33.360
That's what I was thinking.

07:33.360 --> 07:35.480
But it would have also been pretty hard.

07:35.480 --> 07:41.120
The sort of reconciliation of what's true and like what's not true very quickly gets

07:41.120 --> 07:43.320
into the realm of opinion.

07:43.320 --> 07:48.280
And if you add to like the errors of a machine learning model to the nuanced worldview,

07:48.280 --> 07:51.360
it's like definitely one of those examples of apps that could do more harm than good.

07:51.360 --> 07:57.360
So Monica talked to me off the ledge on that one, but what were some of your other ideas?

07:57.360 --> 08:01.840
Well, I consider doing initially sort of computer vision examples because does are always

08:01.840 --> 08:07.080
the more striking to necessarily be newer folks to the field like it's, you know, an image

08:07.080 --> 08:13.280
is like a very powerful example, but I was about 1000 or it's exactly, but I felt like

08:13.280 --> 08:17.360
they were sort of overused and most amount tutorials are some sort of that computer vision

08:17.360 --> 08:18.360
thing nowadays.

08:18.360 --> 08:22.680
So I want to do something different and I want to do tabular data because that's what

08:22.680 --> 08:28.640
I think what most people do in their day to day for most companies.

08:28.640 --> 08:33.560
But I felt like there was less room for a standalone product there, like sort of like bring

08:33.560 --> 08:38.920
your own tabular data has less less of a ring to it than like bring your own writing.

08:38.920 --> 08:45.520
So I ended up settling on an LP and then I wanted to do something that was most ML products

08:45.520 --> 08:46.520
aren't just one model.

08:46.520 --> 08:51.120
They're not just, you know, like you have a model that solves your use case perfectly.

08:51.120 --> 08:52.120
And then you just chip it.

08:52.120 --> 08:57.440
They're usually a combination of sort of heuristics and rules and models and engineering work.

08:57.440 --> 08:59.560
And so I wanted to probably then reflect that.

08:59.560 --> 09:05.320
And when I was thinking of products that did that today, sort of writing and assisting

09:05.320 --> 09:08.840
people to write better is a crucial example of that where you have you can check for grammar

09:08.840 --> 09:12.800
and that's just rules or you can check for vocabulary or for a variety of things and then

09:12.800 --> 09:16.040
you can also help them improve their style and that's more something that you can learn

09:16.040 --> 09:17.040
with them out.

09:17.040 --> 09:20.760
So it was a nice blend that sort of reflects what happens in the real world, I think.

09:20.760 --> 09:24.600
So does that mean that somewhere in this book there are lots of rejects?

09:24.600 --> 09:25.600
No.

09:25.600 --> 09:31.360
We've chosen not to go down that path, but there could be.

09:31.360 --> 09:36.280
The book starts with very simple, so like instead of regular expressions, it's simple

09:36.280 --> 09:37.280
word counts.

09:37.280 --> 09:38.960
Like, oh, how many adverbs are you using?

09:38.960 --> 09:39.960
Are you using?

09:39.960 --> 09:41.680
And a little too much for that sort of stuff.

09:41.680 --> 09:48.160
And so what's the overall kind of path or structure through the book?

09:48.160 --> 09:53.560
And kind of more importantly, what does it say about the way that you think folks need

09:53.560 --> 09:55.920
to approach these kinds of projects?

09:55.920 --> 09:56.920
Yeah.

09:56.920 --> 10:01.400
The book is broadly separated to four stages.

10:01.400 --> 10:06.560
I think generally makes sense for most ML projects.

10:06.560 --> 10:10.600
And I think a lot of time people focus a lot on training models and then you talk to

10:10.600 --> 10:14.960
experienced data scientists and you hear, right, oh, 95% of the job is they're looking

10:14.960 --> 10:18.320
at the data and shipping the model, not really training the model.

10:18.320 --> 10:22.800
And so this book purposely sort of almost ignores training models, it just assumes that

10:22.800 --> 10:26.840
you can figure that part out with the really good courses around.

10:26.840 --> 10:30.920
And so the four approaches are sort of, or the four parts are going from whatever your

10:30.920 --> 10:36.360
goal is, your product, what your company's doing, what you want to do to an ML approach

10:36.360 --> 10:40.960
into a plan for that ML approach, because I think at insight and as of course, I've seen

10:40.960 --> 10:44.600
just many projects fail just because it's the wrong ML approach.

10:44.600 --> 10:48.800
And if you had just chosen a slightly different approach, you'd be in a much better spot.

10:48.800 --> 10:54.000
The second step is sort of building your MVP and that's something that was definitely

10:54.000 --> 10:55.000
the motto at insight.

10:55.000 --> 10:59.240
And I think that insight applied very well was sort of encouraging people to start extremely

10:59.240 --> 11:05.400
simple and build a full project before they go diving down the rabbit hole of research.

11:05.400 --> 11:09.040
The third part is I think one of the ones where I sat down with the most fellows over

11:09.040 --> 11:13.480
my time at insight, which is like, how do you debug models more often like, if your model

11:13.480 --> 11:17.800
is either not working or if it's working, but the performance isn't sufficient, how do

11:17.800 --> 11:20.040
you know what you should do next?

11:20.040 --> 11:23.000
How can you sort of take a deep dive into what your model is doing, what your data looks

11:23.000 --> 11:27.400
like to actually decide what you do in your next iteration cycle?

11:27.400 --> 11:32.080
And then the fourth step is sort of like deployment, monitoring and the concerns that come

11:32.080 --> 11:36.240
with showing the real world to a model and a model to the real world.

11:36.240 --> 11:43.560
Yeah, I am really appreciating all of the focus that kind of this real world ML and AI has

11:43.560 --> 11:48.680
been getting over the, I don't know if I would say past year, past couple of years.

11:48.680 --> 11:55.000
I mean, it's something that we've spent a lot of time focusing on and led us to produce

11:55.000 --> 12:02.000
the TwilmoCon AI platforms conference last week to kind of talk to how folks in real

12:02.000 --> 12:05.480
organizations are tackling these broader problems.

12:05.480 --> 12:09.320
But even just over the weekend on Twitter, like I'm seeing tweets all the time, like, hey,

12:09.320 --> 12:13.120
it's not just about the model, it's not just about the model anymore.

12:13.120 --> 12:18.640
And there seems to be a kind of growing recognition that, you know, not so much recognition, but

12:18.640 --> 12:25.480
more really appreciation that that's the case and that kind of the broader workflow that

12:25.480 --> 12:32.440
takes you from an idea or a problem identification to getting a solution in production is, you

12:32.440 --> 12:37.800
know, multifaceted and involves much more than just training up a model.

12:37.800 --> 12:38.800
Right.

12:38.800 --> 12:43.200
And to be fair, right, it used to be that just training up a model was pretty hard and

12:43.200 --> 12:46.920
maybe you need a team of people that understood sort of the internals deeply.

12:46.920 --> 12:51.760
But now, because the tooling has evolved so much, it actually is the case that the tooling

12:51.760 --> 12:56.280
so good and the courses are so good to train models that that becomes relatively simpler

12:56.280 --> 12:57.280
than the rest.

12:57.280 --> 12:58.280
Right.

12:58.280 --> 12:59.280
Right.

12:59.280 --> 13:04.080
Yeah, we're actually launching a study group in just a couple of weeks now.

13:04.080 --> 13:09.880
We do these study groups as part of our community where we'll do online courses together and

13:09.880 --> 13:14.960
we've done a bunch of fast AI courses and Stanford courses.

13:14.960 --> 13:22.080
But we're doing one starting with kind of an intro webinar on the 15th of February and

13:22.080 --> 13:28.600
then continuing on after that on this AI enterprise workflow course.

13:28.600 --> 13:33.720
This really interesting, unlike any kind of formal, this is on Coursera, unlike any formal

13:33.720 --> 13:38.800
courses I've seen, this one touches on a bunch of the things that you cover in the book.

13:38.800 --> 13:41.960
So, you know, how do you structure data collection?

13:41.960 --> 13:46.160
How do you kind of visualize and analyze your data explored in an exploratory mode and

13:46.160 --> 13:49.040
kind of test hypotheses?

13:49.040 --> 13:55.600
How do you identify data biases in the process of collecting your data?

13:55.600 --> 14:01.040
You mentioned using multiple models, you know, how do you use multiple models together

14:01.040 --> 14:06.080
with heuristics in order to build a solution and then, you know, unit testing?

14:06.080 --> 14:10.800
Like, when do you see that come up in a machine learning course, I'm both never.

14:10.800 --> 14:14.800
And then monitoring a model in production, deploying models with microservices.

14:14.800 --> 14:19.480
So, I'm really looking forward to that and I'll be putting a link into our show notes.

14:19.480 --> 14:23.960
So, anyone else that wants to take this course with me can do so.

14:23.960 --> 14:30.760
But it sounds like you are also of the belief that, you know, this is where the field needs

14:30.760 --> 14:37.960
to go in terms of actually getting value out of machine learning, kind of thinking about

14:37.960 --> 14:39.440
it more holistically.

14:39.440 --> 14:41.360
Yeah, I think so.

14:41.360 --> 14:47.680
I think researches like the class you described are really some of the most valuable classes

14:47.680 --> 14:52.560
or lessons that everyone can learn right now because this comes from a couple of things

14:52.560 --> 14:57.040
which is ad zip code, ads drive, and to some extent, and insight as well.

14:57.040 --> 15:01.000
You notice that the people that are able to contribute the fastest are sort of the ones

15:01.000 --> 15:04.640
that have the appreciation for the whole workflow.

15:04.640 --> 15:08.760
In fact, a lot of the times you'll see companies sort of be scope out projects for people

15:08.760 --> 15:12.560
that are like maybe interning, they're only here for a few months, and those projects

15:12.560 --> 15:14.480
are like just training the model, right?

15:14.480 --> 15:15.480
It's like they've done everything.

15:15.480 --> 15:18.920
They've like thought about the product, they've decided that for this product, this is

15:18.920 --> 15:22.480
the model we need, this is the data we have, they've prepared the data, they've decided

15:22.480 --> 15:25.520
how they're going to serve the model, and they're like, hey, you know, you have, you're

15:25.520 --> 15:28.720
here only for eight weeks or something, here's like a model training.

15:28.720 --> 15:33.840
And so while that's, you know, a fun project, it turns out that if you want to have a meaningful

15:33.840 --> 15:38.440
impact doing all of these other parts is what's going to be most helpful to your

15:38.440 --> 15:42.240
career at a larger company, or even if you're building your own startup, right, to just

15:42.240 --> 15:43.800
actually getting it out the door.

15:43.800 --> 15:49.560
Yeah, I love that you're only 45 pages into this book before you're into the build your

15:49.560 --> 15:54.600
own end-to-end or build your first end-to-end pipeline chapter.

15:54.600 --> 15:57.880
And that chapter in the middle of that chapter is like testing.

15:57.880 --> 15:58.880
Yeah.

15:58.880 --> 16:02.720
So there's, at the end of the first chapter, I'll also introduce you with Monica Regatti,

16:02.720 --> 16:07.880
and she has this great concept that she talks about, she talks about the impact bottleneck.

16:07.880 --> 16:11.920
And sometimes, one of the examples she gives, she says like, well, sometimes your ML might

16:11.920 --> 16:16.920
be perfect, but your product is still dead, you're still dead in the water, because you've

16:16.920 --> 16:22.080
sort of, you know, kind of misunderstood a fundamental need of like how your users would

16:22.080 --> 16:23.480
actually interact with it.

16:23.480 --> 16:27.560
And this is more of maybe like a general, you know, software engineering, like first you

16:27.560 --> 16:32.520
should talk to users before you build it, but it's also a good ML tip to say like, well,

16:32.520 --> 16:38.680
you should, as quickly as possible, get to the point where you can show the like UI to

16:38.680 --> 16:42.640
a friend or a user, have them try it, and then it gives out results.

16:42.640 --> 16:45.960
Even if you don't have a good model, even if it's a heuristic, just because then you might

16:45.960 --> 16:48.920
notice that like, oh, they're using it in a completely different way than you thought.

16:48.920 --> 16:50.800
And that's super complicated model that you thought you needed.

16:50.800 --> 16:51.800
You actually don't need it at all.

16:51.800 --> 16:52.800
You need something else.

16:52.800 --> 17:00.360
Yeah, that's kind of applying the lean startup type of approach, which I don't even

17:00.360 --> 17:04.320
know if it deserves a net, or if it has a net, you know, we need to call it by a name

17:04.320 --> 17:05.320
anymore.

17:05.320 --> 17:09.880
It's kind of how we do things with MVPs, but it's kind of applying that approach to flesh

17:09.880 --> 17:14.600
out some of the, you know, where it's just a bad idea.

17:14.600 --> 17:15.600
Exactly.

17:15.600 --> 17:21.280
And I think that approach is like more and more valuable as the iteration time goes up,

17:21.280 --> 17:22.280
right?

17:22.280 --> 17:25.840
So like, if you don't show it as someone, but it takes you, you know, 50 minutes to build,

17:25.840 --> 17:26.840
you've lost 50 minutes.

17:26.840 --> 17:27.840
That's okay.

17:27.840 --> 17:30.520
So if you don't show it as someone and you need to train a model and get the data and

17:30.520 --> 17:34.000
that whole process is going to take you two months, then you've lost a lot of time by

17:34.000 --> 17:35.000
not doing it.

17:35.000 --> 17:40.440
You know, what I'm curious about is there's, you know, the steps that you need to do to

17:40.440 --> 17:42.280
kind of build this sample app.

17:42.280 --> 17:47.520
And then there's kind of the broader, you know, things that you need to think about to

17:47.520 --> 17:53.360
apply this methodology to your own problems.

17:53.360 --> 17:54.360
Right.

17:54.360 --> 17:58.480
So things like walking folks through building the end pipeline for this app or acquiring

17:58.480 --> 17:59.480
a data set.

17:59.480 --> 18:04.200
Like, how do you make sure it's tangible enough that, you know, you're moving, you're making

18:04.200 --> 18:08.640
progress on your example application, but also broad enough that they can take it and

18:08.640 --> 18:09.640
run with it.

18:09.640 --> 18:10.640
Yeah.

18:10.640 --> 18:12.520
I think that's a, that's a really great question.

18:12.520 --> 18:15.960
It's really hard to think about how you balance that.

18:15.960 --> 18:21.160
In fact, the reason, a big part of the reason for having a sample app is to hold myself

18:21.160 --> 18:26.080
accountable so that my advice would actually be practical and the methods that I gave were

18:26.080 --> 18:27.080
actually true.

18:27.080 --> 18:28.080
Right.

18:28.080 --> 18:30.040
If I, if I tell you something that's actually not going to work, then I'm going to have

18:30.040 --> 18:32.840
a hard time demonstrating to you that it works on this toy example.

18:32.840 --> 18:33.840
Yeah.

18:33.840 --> 18:38.560
And so most chapters are structured in a way where I try to give broad, like, this is

18:38.560 --> 18:44.080
how you generally look for a data set and try to explore it.

18:44.080 --> 18:48.560
Like, for example, you generally, like, if you have a current approach right, because

18:48.560 --> 18:52.080
you've built your plan in the previous chapter and you're saying, well, like, we're going

18:52.080 --> 18:55.920
to, we're going to use this current approach, trying to think of problems that have sort

18:55.920 --> 18:59.800
of the same kind of approach, even if they're in a completely different domain.

18:59.800 --> 19:05.800
So maybe you're doing, you know, like, some, like, reaction prediction for molecules,

19:05.800 --> 19:08.400
but maybe that looks just like text translation in a way, right?

19:08.400 --> 19:12.280
You're predicting a sequence because in other sequence, you can try to find that form,

19:12.280 --> 19:16.120
that first data set, the like translation data set, see if your approach works on that

19:16.120 --> 19:18.000
and then change to your current data set.

19:18.000 --> 19:19.960
That's sort of like a general, how you do it.

19:19.960 --> 19:24.040
And then I illustrate it as an example with, okay, well, for this ML editor thing, this

19:24.040 --> 19:27.520
is what, like, these are the data sets we're actually going to use.

19:27.520 --> 19:29.800
So for most concepts, I try to do both.

19:29.800 --> 19:36.320
The couple of examples or chapters that we've talked about, like, what are the, the broad

19:36.320 --> 19:41.280
principles that folks need to be thinking about when they're building their first pipelines,

19:41.280 --> 19:43.480
when they're acquiring their initial data sets?

19:43.480 --> 19:44.480
Yeah.

19:44.480 --> 19:49.480
So I think we skipped a little bit over the first section, but I think there's, there's

19:49.480 --> 19:53.200
a broad principle in the first section of, like, going for a product to an ML approach

19:53.200 --> 19:57.000
that I generally find really valuable and find that, I don't know, I think everybody

19:57.000 --> 19:58.000
should do.

19:58.000 --> 20:02.240
Kind of sets the tone for the rest of it, it sounds like, yeah.

20:02.240 --> 20:08.560
Which is that for, for the same product goal, you have, again, many ways to do it, many

20:08.560 --> 20:10.440
ways to tackle that problem using machine learning.

20:10.440 --> 20:16.520
So an example I use is, let's say that you're, you know, a retailer and you have an online

20:16.520 --> 20:20.520
catalog of items and you want to help people when they're, when they're typing something

20:20.520 --> 20:25.360
in the search bar to find the category or the types of items that, you know, they, they

20:25.360 --> 20:27.000
would want to buy.

20:27.000 --> 20:29.760
One way to do this, and maybe the simplest way to think about it is you're like, oh, well,

20:29.760 --> 20:33.040
somebody's writing something in the search bar, I'm going to try to, like, autocomplete

20:33.040 --> 20:34.440
the rest of what they're going to say, right?

20:34.440 --> 20:39.080
So if they type hands, maybe you autocomplete, like, handbag or something, right?

20:39.080 --> 20:45.000
A slightly, maybe like, different approach is they type something and you try to identify

20:45.000 --> 20:47.840
which words in their query are relevant to products.

20:47.840 --> 20:52.160
So if they're like, I want, you know, like, a handbag of this brand, like, you try to

20:52.160 --> 20:55.200
find the name of the brand.

20:55.200 --> 20:59.200
And then a third approach that's even simpler is they type anything in that bar and all

20:59.200 --> 21:03.440
you do is you try to like classify it and like, is it about handbags, is it about jeans,

21:03.440 --> 21:05.640
is it about, you know, shoes or something else?

21:05.640 --> 21:10.480
And those three approaches, all would have a slightly different UI, right?

21:10.480 --> 21:13.320
You would build sort of the way you show results and the way you show suggestions slightly

21:13.320 --> 21:14.320
differently.

21:14.320 --> 21:19.080
But they all are basically each successive approach is like an order of magnitude easier

21:19.080 --> 21:22.600
to do than the one before, especially if you don't have much data, right?

21:22.600 --> 21:26.240
If you don't have much data, that final approach building a classifier is something where

21:26.240 --> 21:30.720
you can label data for a few hours and you'll have a classifier that's decent.

21:30.720 --> 21:33.880
If you want to do the sort of like, not a distraction part, that's going to take you

21:33.880 --> 21:36.560
maybe a couple of days, but you can probably get something that's pretty good.

21:36.560 --> 21:40.640
If you want to go do the full like language model approach of predicting the next tokens,

21:40.640 --> 21:43.880
that's going to require a lot of data and you're also going to have a lot more variance

21:43.880 --> 21:47.720
where like, sometimes you might predict some crazy things and unless you have the engineering

21:47.720 --> 21:52.160
resources to add that filtering layer on top, that'll make it very hard to shift.

21:52.160 --> 21:56.920
And so I think like a question that generally people should ask is like, what is the absolute

21:56.920 --> 22:02.800
simplest model in that bucket of model and I give sort of like a hierarchy of models?

22:02.800 --> 22:06.680
What is the simplest model that you could use that could solve what you're currently

22:06.680 --> 22:07.920
trying to do?

22:07.920 --> 22:11.080
And then if that model's too simple and it's not good enough, it's fine.

22:11.080 --> 22:14.240
You can improve on it, but you'll have spent you know, a couple hours building it instead

22:14.240 --> 22:17.240
of a couple months being building a model that doesn't work.

22:17.240 --> 22:21.720
And so with that in mind, we go into building the pipeline and acquiring the data set, what

22:21.720 --> 22:23.880
are some of the broad principles there?

22:23.880 --> 22:30.120
Yeah, so I think there's this idea that I would say for any data project that you tackle,

22:30.120 --> 22:34.240
any new project, you should spend like a couple hours looking at the data.

22:34.240 --> 22:37.640
And the couple hours is definitely the minimum time you should spend looking at the day.

22:37.640 --> 22:38.640
You could spend a lot of more time.

22:38.640 --> 22:43.600
That's fine, but you should never spend less than a few hours looking at the data.

22:43.600 --> 22:44.920
Looking at the data means a few things.

22:44.920 --> 22:50.200
And I found that in my experience of insight, often people think about it in terms of aggregates,

22:50.200 --> 22:51.200
right?

22:51.200 --> 22:54.120
They're like, okay, well, now I'm looking at this database of like texts, what's the

22:54.120 --> 22:58.360
average length of a sentence, you know, how many different words are there.

22:58.360 --> 23:02.800
And that's fine, and that's something that you should do, both to find errors or things

23:02.800 --> 23:03.800
that surprise you.

23:03.800 --> 23:07.560
You're like, oh, I was looking at like a database of tweets and somehow like all the tweets

23:07.560 --> 23:10.000
are one word long, just probably something wrong.

23:10.000 --> 23:14.000
But you should also actually look at individual examples, and that's something that Rob

23:14.000 --> 23:17.960
Monroe actually talks about a lot, where a lot of these individual examples will be the

23:17.960 --> 23:20.440
source for which model you end up choosing, right?

23:20.440 --> 23:23.280
So you have your product goal on one side, you've said like, okay, well, maybe like a classifier

23:23.280 --> 23:25.080
is a single thing I can start with.

23:25.080 --> 23:30.280
But then looking at the data will give you your initial set of like features or types

23:30.280 --> 23:35.720
of model that you think could reasonably capture what you're trying to identify in this dataset.

23:35.720 --> 23:40.400
And so in the chapter, I go into, well, how can you look at individual examples in a

23:40.400 --> 23:41.400
dataset?

23:41.400 --> 23:42.400
That's hard, right?

23:42.400 --> 23:46.120
If you have a dataset with like 10,000 examples or 10 million, you're not going to look

23:46.120 --> 23:47.120
at all of them.

23:47.120 --> 23:52.560
And so there's some methods where you can basically use some NLP approaches or some

23:52.560 --> 23:55.320
approaches from other domains to embed your data.

23:55.320 --> 23:59.760
And then once you have this sort of like map of all your data points, you look into each

23:59.760 --> 24:04.080
specific sub area and you're like, oh, like this sub area, you know, for me, for the

24:04.080 --> 24:07.200
example, it was questions on Stack Overflow.

24:07.200 --> 24:11.320
So this area of questions, like they all seem to be about, you know, the English language.

24:11.320 --> 24:15.160
And so they use a lot of like maybe complex words and they have like longer sentences.

24:15.160 --> 24:18.840
Oh, this area, you know, is all about like non-native speakers, that sort of stuff.

24:18.840 --> 24:22.480
And so then it helps you like build features where it's like, okay, well, how can I identify

24:22.480 --> 24:24.840
a good question in this area or in that area?

24:24.840 --> 24:31.680
Do you have a sense for how you know when you have done enough looking at your data in

24:31.680 --> 24:39.520
aggregate or as individual items that you're, you know, you're ready to move on?

24:39.520 --> 24:47.760
Is there a feeling there or a sense or a checklist or something that beyond just, hey, I've,

24:47.760 --> 24:52.800
you know, looking at my watch here, the two hours is up, I can move on to the fun stuff.

24:52.800 --> 24:57.600
Like what, what should you have taken away from that experience?

24:57.600 --> 25:03.800
Yeah, that's something that I think obviously is going to depend on your data sense.

25:03.800 --> 25:04.800
So you're right.

25:04.800 --> 25:05.800
Sometimes it's going to be two hours.

25:05.800 --> 25:07.280
Sometimes it's maybe going to be two days.

25:07.280 --> 25:10.200
Sometimes you're going to realize that something's very wrong and you, you shouldn't move

25:10.200 --> 25:11.200
on, right?

25:11.200 --> 25:14.440
If you realize that something's off, you should go back to the drawing board and sort of

25:14.440 --> 25:18.520
like get another data set or look at why your data is looking all weird.

25:18.520 --> 25:23.040
But in the case where you are ready to move on to a model, I'd say that that's when you

25:23.040 --> 25:28.120
have a strong hypothesis about how your model will actually do its work.

25:28.120 --> 25:32.560
So you know, we've talked about like the retailer that like you type queries and then it tells

25:32.560 --> 25:34.080
you like, oh, is it a handbag?

25:34.080 --> 25:35.160
Is it like something else?

25:35.160 --> 25:39.920
Well, if you look at a bunch of examples and you realize that really there's, you know,

25:39.920 --> 25:46.080
only two or three ways that people ever say handbag or jeans that the vocabulary is like

25:46.080 --> 25:47.840
pretty simple.

25:47.840 --> 25:49.440
That sentences are usually pretty short.

25:49.440 --> 25:54.800
You can say like, okay, well, for this approach, like, you know, sort of like a simple bag

25:54.800 --> 25:59.360
of words with like word counts will work because I'm reasonably confident that like all

25:59.360 --> 26:05.320
I need to know is like what, which of these three words is in the sentence.

26:05.320 --> 26:09.280
More generally, I think what I'm trying to say is when you build your first model, you

26:09.280 --> 26:12.360
should already have the part goal, that section one.

26:12.360 --> 26:17.000
And then a hypothesis about how your model, like, what will make your model succeed?

26:17.000 --> 26:20.120
That's like the part two of looking at the data set because then once you've trained

26:20.120 --> 26:23.720
your model, what you want to do is you want to check your results against your assumption.

26:23.720 --> 26:28.200
You want to say like, okay, well, I thought that like because sentences are short, you know,

26:28.200 --> 26:32.760
the model would be easily able to pick up on different words, but it turns out that

26:32.760 --> 26:35.000
it's not, or it turns out that it's not performing on these.

26:35.000 --> 26:38.160
And then you can go back to the data, right, and say like, okay, well, let's look at the

26:38.160 --> 26:41.720
examples that my model got wrong and like, why did it get these wrong?

26:41.720 --> 26:43.240
And then make a new hypothesis.

26:43.240 --> 26:47.520
And that's the fastest way and generally the most, the best way to iterate because it'll

26:47.520 --> 26:52.120
actually let you understand how your model is working, which once you're ready to deploy

26:52.120 --> 26:54.640
it, you'd much rather be like, oh, well, I'm pretty sure that this is how my model's

26:54.640 --> 26:58.120
making decisions, rather than like, I trained a model, I got a really high score.

26:58.120 --> 27:00.280
I hope everything goes well.

27:00.280 --> 27:05.360
Do you talk about explainability in the context of models?

27:05.360 --> 27:10.280
That's something that is getting a lot of attention for folks that are, you know, particularly

27:10.280 --> 27:13.880
working in business or enterprise types of environments.

27:13.880 --> 27:14.880
Yeah.

27:14.880 --> 27:17.120
This is a topic that it gets here.

27:17.120 --> 27:19.160
A lot of attention, a lot of debate.

27:19.160 --> 27:22.200
There's generally a few ways to look at explainability.

27:22.200 --> 27:24.120
I say there's a lot of attention.

27:24.120 --> 27:29.000
And I'm asking primarily from the perspective of having a sense of what your model is doing

27:29.000 --> 27:31.320
so that you can better debug it.

27:31.320 --> 27:32.320
Yeah.

27:32.320 --> 27:33.720
So there's two parts to that.

27:33.720 --> 27:38.560
One part is to have a sense and to debug it, you need to look at like the internals of

27:38.560 --> 27:39.560
your model.

27:39.560 --> 27:43.120
And then you can look at like its feature importance, its coefficients, that sort of stuff that

27:43.120 --> 27:46.040
gives you some sense of explainability.

27:46.040 --> 27:51.120
But then there's actually looking at individual data examples or even like multiple data

27:51.120 --> 27:55.360
examples, but trends in results on examples, which I think gives you a much better sense

27:55.360 --> 27:56.360
of explainability.

27:56.360 --> 28:03.080
And what I mean by that is looking at what are the like 10 examples where you're, let's

28:03.080 --> 28:08.880
say your classifier was the most confident that it was class one, but your class where

28:08.880 --> 28:10.040
it was wrong.

28:10.040 --> 28:12.520
What are the ones where it was most confident of the other class?

28:12.520 --> 28:17.080
What are the ones where it was the most unsure, like looking at those examples and seeing

28:17.080 --> 28:23.080
like, oh, you know, it seems like every time there's a question that's 16 sentences long,

28:23.080 --> 28:26.840
the model just doesn't know what's going on, it just gives up.

28:26.840 --> 28:28.640
That gives you one sense of explainability.

28:28.640 --> 28:31.640
And the final sense of explainability that I think a lot of people talk about is black

28:31.640 --> 28:32.640
box explainers.

28:32.640 --> 28:37.680
They're like shop values or lime and we actually use those pretty heavily in the book because

28:37.680 --> 28:41.920
you can use them to power suggestions for your users.

28:41.920 --> 28:45.520
So again, the example of a case study is like something that's going to help you write

28:45.520 --> 28:46.520
better.

28:46.520 --> 28:48.560
So you give it something you wrote, a question you've wrote.

28:48.560 --> 28:52.400
And then we have our model that predicts whether you wrote something basically good or bad

28:52.400 --> 28:54.080
for some definition of good or bad.

28:54.080 --> 28:59.480
And then we use lime to say like, oh, you know, we said that this question was like 60%

28:59.480 --> 29:04.520
good, these are the features that if you were to change them, would push your questions

29:04.520 --> 29:06.120
towards the positive class more, right?

29:06.120 --> 29:09.080
Well, it seems like your question's much too long.

29:09.080 --> 29:12.400
So if you cut it down, actually, like we would have said that it was much better that

29:12.400 --> 29:13.400
sort of stuff.

29:13.400 --> 29:17.560
So you can use explainability to power sort of user-facing suggestions.

29:17.560 --> 29:18.560
Interesting.

29:18.560 --> 29:19.560
Yeah.

29:19.560 --> 29:20.560
Is that done commonly?

29:20.560 --> 29:22.560
Do you see that a lot?

29:22.560 --> 29:24.640
I see that in a few cases.

29:24.640 --> 29:30.560
But I think there's no example is perfect, and so the emulator definitely has a few things

29:30.560 --> 29:34.080
where it's like, well, how wildly applicable is that?

29:34.080 --> 29:38.800
But out of companies that do that sort of stuff that do help people write better, actually

29:38.800 --> 29:46.320
have an interview with Chris Harland, who is from a textio, where they do that for general

29:46.320 --> 29:48.640
job postings and job communication.

29:48.640 --> 29:54.360
And they do mention using sort of similar methods, at least to surface potential features,

29:54.360 --> 29:59.440
because whenever you're doing writing recommendations, the real challenge is that you want your users

29:59.440 --> 30:00.440
to understand them.

30:00.440 --> 30:04.520
It's explainability becomes crucial because if I give you anything and you don't understand

30:04.520 --> 30:07.400
why I'm recommending it to you, then you're not going to use my product.

30:07.400 --> 30:10.640
And so for a subset of ML products, explainability isn't just a bonus.

30:10.640 --> 30:12.480
It's what the product is.

30:12.480 --> 30:16.440
You mentioned Lime for folks that want to learn more about that.

30:16.440 --> 30:24.520
You can check out my seventh interview ever with Carlos Guestrin back in October of 2016.

30:24.520 --> 30:28.440
What are some of the other things that come up from a debugging perspective?

30:28.440 --> 30:32.360
I think from the debugging perspective, we end up going back to the iteration loop conversation

30:32.360 --> 30:38.800
we had earlier, which is one debugging tip that actually Ross gave me, which you interviewed

30:38.800 --> 30:42.160
earlier, and that is used widely on site and elsewhere.

30:42.160 --> 30:47.960
I've seen it use the industry is when your model doesn't work, not just when it's, when

30:47.960 --> 30:52.360
you're not happy with this current score, but when something is not working, you should

30:52.360 --> 30:57.400
cut down your data set to one or two examples and then get your model to work.

30:57.400 --> 31:02.400
Like by working, what I mean is just get your model to train and then output predictions.

31:02.400 --> 31:06.480
They'll be completely random because you're overfitting on a couple examples.

31:06.480 --> 31:07.480
But that alone.

31:07.480 --> 31:09.240
But if you can't do that.

31:09.240 --> 31:10.240
Exactly.

31:10.240 --> 31:11.240
Exactly.

31:11.240 --> 31:16.720
And so there's sort of like that that's helped so many fellows and honestly, like colleagues

31:16.720 --> 31:21.400
at AdFuse companies where like if every training run takes four hours and at the end, you

31:21.400 --> 31:25.640
have like, you know, the shape mismatch or something like it is the most infuriating process

31:25.640 --> 31:26.640
you can go through.

31:26.640 --> 31:33.600
So should you just start there or should you regress there when things aren't working?

31:33.600 --> 31:37.560
I think there's nothing wrong with sort of starting with the Hail Mary of like I'm going

31:37.560 --> 31:38.840
to write my code.

31:38.840 --> 31:41.040
This should work.

31:41.040 --> 31:44.040
And then if it doesn't, then I think the first step is going there.

31:44.040 --> 31:48.880
Like if you've written your first training loop and like something's very wrong, the first

31:48.880 --> 31:53.520
step is like, okay, you know, you go to your first line and you like take X train equals

31:53.520 --> 31:57.640
like X train, period two, like you just take a couple examples and then you run it again

31:57.640 --> 31:59.400
and you try to get that to work.

31:59.400 --> 32:02.320
And in fact, in debugging, I mentioned like three steps.

32:02.320 --> 32:06.440
The first step is that, which I call debugging the wiring, like making sure that data can

32:06.440 --> 32:08.240
go back and forth.

32:08.240 --> 32:13.040
And I think the first step is debugging training performance.

32:13.040 --> 32:16.640
Once you've debugged that first aspect, what you want to see is like, can I over fit

32:16.640 --> 32:17.840
on my data set?

32:17.840 --> 32:22.440
And so if you take a data set of like, you know, 2000 examples, can you train a model

32:22.440 --> 32:24.640
that becomes very good on this data set?

32:24.640 --> 32:26.720
Once more, this model isn't going to be good in production because you've trained it

32:26.720 --> 32:27.720
to just be good on data.

32:27.720 --> 32:28.720
It's already seen.

32:28.720 --> 32:32.240
But again, if you can't do that, you're probably not going to be able to train a model

32:32.240 --> 32:36.240
that learns general things at all if it can't learn local features.

32:36.240 --> 32:38.400
And then finally, you debug generalization.

32:38.400 --> 32:43.200
And so these three successive steps are our steps, and honestly, I think most program

32:43.200 --> 32:47.320
directors and insight, for example, are very used to like helping their fellows walk

32:47.320 --> 32:49.280
through each of these steps successfully.

32:49.280 --> 32:52.440
But I haven't seen as many resources just sort of like outline them.

32:52.440 --> 32:56.280
So if you just follow that recipe, usually you'll debug your models much, much faster.

32:56.280 --> 33:01.880
It's like the hierarchy of like what you should debug in three steps.

33:01.880 --> 33:04.800
What categories of models are covered in the book?

33:04.800 --> 33:11.120
Is it, you mentioned TF IDF, and you also mentioned that you're composing multiple models

33:11.120 --> 33:16.400
are using both kind of traditional Python models,

33:16.400 --> 33:22.760
scikit-learn-ish types of models, or are you doing deep learning as well?

33:22.760 --> 33:27.840
What's the portfolio look like?

33:27.840 --> 33:31.040
That's a, let me think, how many?

33:31.040 --> 33:32.880
We have a bat.

33:32.880 --> 33:36.400
We have three models that are used in the main example.

33:36.400 --> 33:41.640
And then I'd say like probably half a dozen examples of other models as like sort of separate

33:41.640 --> 33:42.640
from the case study.

33:42.640 --> 33:48.120
It's like this is how you would use VGG to extract features from images, for example.

33:48.120 --> 33:52.480
For the main application, the models we use are pretty simple.

33:52.480 --> 33:55.120
It starts with a heuristic, that's not even a model.

33:55.120 --> 33:57.560
So you haven't written a birth version yet?

33:57.560 --> 33:58.560
No, I have not.

33:58.560 --> 34:02.520
I have purposefully stayed away from birth.

34:02.520 --> 34:13.080
I don't have anything against more recent approaches, of course, they're breakthroughs.

34:13.080 --> 34:15.360
But I think one, they're covered enough.

34:15.360 --> 34:19.800
I think if you talk to somebody that's either new to the field or even working in LPN,

34:19.800 --> 34:24.760
you ask them, out of all of the blog posts and research articles on things you've read

34:24.760 --> 34:30.120
recently, how many of them were about deep language models, and they'd probably say 85%.

34:30.120 --> 34:34.000
So I didn't feel like I needed to sort of add my break onto that cathedral.

34:34.000 --> 34:40.320
At the same time, a lot of the advice about building practical applications apply regardless

34:40.320 --> 34:42.480
of the model that you use.

34:42.480 --> 34:47.640
And so I felt that for readers, it was best if I kept with models that were as simple

34:47.640 --> 34:51.600
as possible because I didn't want to just add complexity for the sake of adding complexity.

34:51.600 --> 34:55.480
I think if you were to use birth or something more complicated, you could probably get just

34:55.480 --> 34:58.080
a better performance metric on some of these models.

34:58.080 --> 35:02.640
But to the point of this book, that's something that you do in iteration number four, number

35:02.640 --> 35:04.040
five, number six, number seven.

35:04.040 --> 35:08.400
The book shows you the first three iterations, which is one, you build your select first

35:08.400 --> 35:13.280
sub at a crappy heuristic, number two, you build a simple model, number three, you build

35:13.280 --> 35:16.360
a slightly more complicated model, and number four, actually, and I guess this is spoiler

35:16.360 --> 35:17.360
alert.

35:17.360 --> 35:20.000
But you look at your more complicated model and you realize it's too complicated and you

35:20.000 --> 35:23.800
remove some of the useless features to make, so the final model we use in prototype.

35:23.800 --> 35:29.840
So you wouldn't be following your own advice if you jump right into Burton chapter two.

35:29.840 --> 35:30.840
Exactly.

35:30.840 --> 35:34.360
If somebody wants to write a sequel to this book, I think there's like a lot of rooms to

35:34.360 --> 35:39.280
try, Bert and GPT-2 and other more complicated approaches, but I think, yeah, in general,

35:39.280 --> 35:42.720
even in what would that be called, building more complicated, machine learning pattern

35:42.720 --> 35:43.720
applications?

35:43.720 --> 35:46.760
Yeah, it would be called building applications the third year.

35:46.760 --> 35:51.240
When you join a team at a company, the first year they're doing this, and then if the

35:51.240 --> 35:55.560
team's been around for like 10 years, they're just throwing anything they can at the wall,

35:55.560 --> 35:58.800
they're like, oh, let's try it, Bert, you know, let's try anything.

35:58.800 --> 36:03.960
We didn't cover the feature importance of you spend quite a bit of time on feature importance.

36:03.960 --> 36:05.720
How do you see that coming up?

36:05.720 --> 36:11.040
Yeah, I think in this case, right, it was especially useful because we make suggestions for

36:11.040 --> 36:12.720
users based on the features of our application.

36:12.720 --> 36:19.280
So that application specific as opposed to a general step in the process?

36:19.280 --> 36:20.280
Good question.

36:20.280 --> 36:26.600
The importance is a step for debugging, where especially if you're doing a, sort of if you're

36:26.600 --> 36:30.320
using a model that has many features either because it's a deep learning model and just

36:30.320 --> 36:33.840
does its own feature generation, or you know, you just have like a tabular data set with

36:33.840 --> 36:38.160
thousands of features, looking at feature importance can help you check the assumptions

36:38.160 --> 36:41.880
that we talked about, meaning like you've made assumptions, you said like, well, because

36:41.880 --> 36:45.080
length of a question is important, it'll be an important feature.

36:45.080 --> 36:48.360
And then you look at your features and it's not, you know, it's like the importance is

36:48.360 --> 36:50.840
zero, then your assumption was wrong.

36:50.840 --> 36:56.120
And so as part of your iteration cycle, looking at feature importance is really valuable.

36:56.120 --> 36:59.320
The specific chapter you're talking about, which is using feature importance to make

36:59.320 --> 37:04.280
recommendations, I would say is the only chapter in the whole book that's very, very specific

37:04.280 --> 37:08.800
to the, to the emulator, it's sort of, it was actually reviewers of the books that like

37:08.800 --> 37:12.480
the book is great, but I feel like we should have a deep dive into the, the sort of like

37:12.480 --> 37:14.800
emulator at some point that's like wraps things up.

37:14.800 --> 37:18.200
And so this is, this is the books attempt at that of like, okay, well, let's take everything

37:18.200 --> 37:21.560
together and actually get the emulator to like a, a ready product.

37:21.560 --> 37:25.920
And then the last part of the book goes into deployment and modern, monitoring.

37:25.920 --> 37:28.960
What are the, the key takeaways there?

37:28.960 --> 37:29.960
Yeah.

37:29.960 --> 37:33.760
So there's, there's like three main aspects that are covered there.

37:33.760 --> 37:39.960
One is the, just the, the ethics of deployment and the things that you should think about

37:39.960 --> 37:41.520
when you deploy them on models.

37:41.520 --> 37:45.560
I, this chapter is mostly about resources that I share.

37:45.560 --> 37:49.400
There's actually an excellent, an excellent free or highly book about data ethics that I

37:49.400 --> 37:50.880
linked to in this, in this chapter.

37:50.880 --> 37:56.040
And there's, there's a lot of, there's a big body of work, but I try to just give some,

37:56.040 --> 38:00.760
some aspects that you might want to, want to think about based on recent research.

38:00.760 --> 38:06.400
Then the other two aspects are just what is the engineering work around models, both

38:06.400 --> 38:09.440
as you deploy them and once you've deployed them.

38:09.440 --> 38:16.280
And so a lot of takeaways from the engineering work are for most complicated models.

38:16.280 --> 38:20.520
So I think famously the Google smart reply.

38:20.520 --> 38:25.160
So Google smart replies, you get an email and they suggest those three responses that you

38:25.160 --> 38:26.160
could use.

38:26.160 --> 38:28.520
And not the new version where you press tab and they're just all going to place the one

38:28.520 --> 38:33.440
where I just suggest, sort of, yeah, responses you can just click on.

38:33.440 --> 38:37.920
That model is a relatively complicated model and because of that, it fails on a non-zero

38:37.920 --> 38:39.120
number of emails.

38:39.120 --> 38:43.320
And so before running that model, they have what's called a filtering model, which is a

38:43.320 --> 38:47.600
much simpler model, which the goal of that model is to say, like, should we run our model

38:47.600 --> 38:48.600
or not?

38:48.600 --> 38:50.480
And so I cover like tricks like these tricks, right?

38:50.480 --> 38:55.320
For example, like, can you, when should you decide that it's worthwhile to build a first

38:55.320 --> 38:59.960
model that's like a much simpler one that will save you compute time and save you from

38:59.960 --> 39:04.240
showing ridiculous results to your users if, if like, this input is not suited for your

39:04.240 --> 39:05.960
complex model?

39:05.960 --> 39:11.760
And then the last chapter goes into sort of like CICD and monitoring for ML and like what

39:11.760 --> 39:14.840
you can look at when you're like, maybe testing or when you're putting a new model in production

39:14.840 --> 39:18.720
of a lot of companies that started doing what's called like shadow deployments where like

39:18.720 --> 39:22.720
you put a model into shadow, which means that it's just like a real model except that

39:22.720 --> 39:26.400
you don't actually use what it produces, but it's sort of like a final way to test it.

39:26.400 --> 39:27.400
Yeah.

39:27.400 --> 39:28.400
Nice.

39:28.400 --> 39:36.480
So, you know, I referred back to the Twoma kind of platform's conference and some of

39:36.480 --> 39:44.040
the writing that I've been doing on AI, ML platforms and kind of open source and commercial

39:44.040 --> 39:48.600
products that like enable you to build out these workflows and kind of manage these workflows

39:48.600 --> 39:49.600
for you.

39:49.600 --> 39:56.040
I'm assuming that you're not like building all of this in one of those environments.

39:56.040 --> 40:00.280
How are you like stringing together the pieces that you're doing?

40:00.280 --> 40:03.600
Is it all kind of standard vanilla Python?

40:03.600 --> 40:06.240
Are you building on vanilla Python?

40:06.240 --> 40:13.360
Are you using any particular kind of approach to make this modular or are you worried about

40:13.360 --> 40:14.360
that?

40:14.360 --> 40:18.600
You know, how are you pulling this all these workflow components together?

40:18.600 --> 40:23.080
I mean, there is a world where, you know, in like a few years, something comes out.

40:23.080 --> 40:27.360
It's like the TensorFlow, but just for all of machine learning, maybe it'll be TensorFlow.

40:27.360 --> 40:28.360
And this is all obsolete.

40:28.360 --> 40:31.080
Or if you're just like, well, you could just, you know, like write one line of this to

40:31.080 --> 40:33.760
serve new framework.

40:33.760 --> 40:35.520
And I'm willing to take that risk.

40:35.520 --> 40:41.120
In the current state, I feel like while there's many useful tools, it wouldn't necessarily

40:41.120 --> 40:43.880
be what readers are looking for.

40:43.880 --> 40:47.880
Like they're not necessarily looking to learn how to use Scoopflow or, you know, how to

40:47.880 --> 40:53.920
use Airflow for service scheduling DAGs. So I kept things pretty lightweight where most

40:53.920 --> 40:55.720
of it is in raw Python.

40:55.720 --> 40:59.760
You know, there's simple unit testing, Jupyter notebooks to illustrate concepts.

40:59.760 --> 41:04.160
And then for the serving side, built a simple flask app with some examples of like how

41:04.160 --> 41:06.560
you cache requests.

41:06.560 --> 41:10.000
And that I think serves the purposes of the book.

41:10.000 --> 41:14.280
Well, whenever possible, I've added sort of links to resources where it's like, well,

41:14.280 --> 41:18.960
if you want to know more about how you, for example, build models on device, like you might

41:18.960 --> 41:21.520
want to check out this resource.

41:21.520 --> 41:25.560
But for the actual code examples, again, I wanted to, because the book already has the

41:25.560 --> 41:29.600
tall order of covering all the machine learning, I felt like I went a bit pretty focused.

41:29.600 --> 41:31.400
Cool.

41:31.400 --> 41:37.680
So once should we expect the, you know, volume two second edition, you're next, your

41:37.680 --> 41:42.760
next book after a long vacation.

41:42.760 --> 41:47.000
Yeah, I mean, depending on how well this book goes, it was really, it's actually a really

41:47.000 --> 41:49.240
enjoyable process to write it with O'Reilly.

41:49.240 --> 41:52.480
The process of writing a book, you know, I would recommend to no one that's horrible.

41:52.480 --> 41:56.520
But O'Reilly made it as unhorrible as possible.

41:56.520 --> 41:57.520
So nice.

41:57.520 --> 41:58.520
Awesome.

41:58.520 --> 42:02.160
Well, Emmanuel is great catching up with you.

42:02.160 --> 42:05.640
Congratulations on getting this book published.

42:05.640 --> 42:09.160
We've been chatting about it for a bit, at least conceptually.

42:09.160 --> 42:14.600
You know, that it was something that you're laboring under and I'm super excited to, you

42:14.600 --> 42:18.720
know, see it, have an opportunity to hold it in my hand and chat with you about it on

42:18.720 --> 42:19.720
the show.

42:19.720 --> 42:20.720
Awesome.

42:20.720 --> 42:21.720
Thank you, Sam.

42:21.720 --> 42:22.720
All right.

42:22.720 --> 42:23.720
Thank you.

42:23.720 --> 42:24.720
All right, everyone.

42:24.720 --> 42:26.720
That's our show for today.

42:26.720 --> 42:33.520
To learn more about Emmanuel or his new book, visit twomalai.com slash talk slash 349.

42:33.520 --> 42:38.920
For more information on the AI Enterprise workflow study group, visit twomalai.com slash

42:38.920 --> 42:39.920
AIEW.

42:39.920 --> 42:46.120
Of course, if you like what you hear on the podcast, we would be very grateful if you

42:46.120 --> 42:50.680
subscribe, rate, and review the show on your favorite pod catcher.

42:50.680 --> 42:51.680
All right.

42:51.680 --> 43:18.440
Thanks so much for listening and catch you next time.

