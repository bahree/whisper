WEBVTT

00:00.000 --> 00:17.480
Hello everyone, I am back here at NYU Future Labs at the AI Nexus Accelerator speaking

00:17.480 --> 00:23.480
with Mika Perra and Hamid Zaheri from Cambrian Intelligence.

00:23.480 --> 00:25.480
All right guys, say hi.

00:25.480 --> 00:26.480
Hi.

00:26.480 --> 00:27.880
Great to be here.

00:27.880 --> 00:29.880
Hi everyone.

00:29.880 --> 00:32.880
So tell us about Cambrian Intelligence, what are you guys up to?

00:32.880 --> 00:40.160
Yes, the Cambrian Intelligence, the word Cambrian comes from Cambrian Explosion that happened

00:40.160 --> 00:42.240
some 60 million years ago.

00:42.240 --> 00:49.640
So we are developing robot always made of AI modules.

00:49.640 --> 00:59.520
And that can be used to do various tasks and solve to enable robotics in every industry

00:59.520 --> 01:00.520
possible.

01:00.520 --> 01:01.520
Okay.

01:01.520 --> 01:09.040
And with regards to the analogy to the Cambrian Explosion, what's being exploded?

01:09.040 --> 01:10.800
Like what have we seen proliferate here?

01:10.800 --> 01:12.960
Is it the intelligence itself?

01:12.960 --> 01:13.960
Is it robotics?

01:13.960 --> 01:14.960
Is it?

01:14.960 --> 01:20.360
Yeah, so we believe that sticking this narrow AI applications into modules, we can achieve

01:20.360 --> 01:25.680
much, much more and we can more rapidly develop different applications for robotics.

01:25.680 --> 01:31.640
So basically, what we are doing is making robots more intelligent.

01:31.640 --> 01:38.440
And so maybe tell us a little bit about your personal backgrounds, how did you get to

01:38.440 --> 01:39.440
where you are?

01:39.440 --> 01:40.440
Yeah, sure.

01:40.440 --> 01:47.400
So I have a background in aerospace and from shoot guard in embedded systems and robotics.

01:47.400 --> 01:48.400
Okay.

01:48.400 --> 01:56.000
And I moved to London, then there are MedMika, Mika has a background in cognitive science

01:56.000 --> 01:58.880
and deep learning.

01:58.880 --> 02:06.240
And there we figured that we both share passion for robotics and for artificial intelligence.

02:06.240 --> 02:09.120
And we decided to start this company together.

02:09.120 --> 02:10.120
Oh, nice.

02:10.120 --> 02:13.080
And how long have you been doing it?

02:13.080 --> 02:15.080
A little bit over all that year.

02:15.080 --> 02:16.080
Okay.

02:16.080 --> 02:20.160
If you've been at it for around a year, where are you at as a company?

02:20.160 --> 02:21.160
Yeah.

02:21.160 --> 02:29.800
So right now we are working for pilots for a large auto manufacturer and like to implement

02:29.800 --> 02:36.840
the first application out of our operating system on industrial robots.

02:36.840 --> 02:43.480
So we had started on developing that and we planned to release the platform more pro-dera

02:43.480 --> 02:45.400
late this year.

02:45.400 --> 02:51.760
What specific problem are you going after that an auto maker would have?

02:51.760 --> 02:57.960
So right now robots are programmed point by point to accomplish different tasks.

02:57.960 --> 02:59.760
The robot is blind.

02:59.760 --> 03:06.960
You put some specific parts into some specific locations with the accuracy of sub-millimeter.

03:06.960 --> 03:13.040
And the robot is programmed to just go there and pick it apart and place it some place.

03:13.040 --> 03:17.640
It's already defined for a robot, but robot has no knowledge about the environment or the

03:17.640 --> 03:20.600
task that it's trying to accomplish.

03:20.600 --> 03:23.760
But we're doing, we're trying to make that robot more intelligent.

03:23.760 --> 03:31.080
So the robot can have understanding of the environment and can accomplish the task by self-learning.

03:31.080 --> 03:36.720
Then there won't be any need for programming the robot.

03:36.720 --> 03:41.840
We're basically changing the abstraction level of robot programming from low level programming

03:41.840 --> 03:49.040
of the robot to be decode to high level, just specifying the task and the specific location

03:49.040 --> 03:52.800
and the initial location, the target location and the robot can figure out on its own how

03:52.800 --> 03:55.600
to accomplish the task.

03:55.600 --> 04:03.640
So I've seen a bunch of videos from a variety of different sources actually with showing

04:03.640 --> 04:10.880
robots trying to do these pick and place types of operations and there are a number of challenges

04:10.880 --> 04:18.880
including, you mentioned the millimeter resolution with what you need to program the robot.

04:18.880 --> 04:24.560
What are some of the other challenges that you see folks having with the traditional approach

04:24.560 --> 04:26.920
to programming these robots?

04:26.920 --> 04:33.440
A beauty program in this robot is really time-consuming and it's not just the programming itself,

04:33.440 --> 04:39.280
but the robot changes the behavior with environmental factors like even life and conditions or

04:39.280 --> 04:43.280
temperature of the environment and so on can affect how the robot behaves.

04:43.280 --> 04:49.760
So every time these conditions are changing they need to reprogram the robot.

04:49.760 --> 04:56.200
So our approach is to add that level of intelligent to the robot that the robot can adapt to the

04:56.200 --> 05:06.000
new changes of the environment and it basically can learn how to optimize for different tasks.

05:06.000 --> 05:12.080
Current challenges, it includes localizing the part with the accuracy that we mentioned,

05:13.200 --> 05:18.240
extracting the pose of the part, grasping the part from the right location and with the right timing

05:18.240 --> 05:25.520
and so on. And these are the challenges we want to remove from the programming and let the robot

05:25.520 --> 05:29.040
to learn.

05:29.040 --> 05:31.600
And why the auto industry?

05:31.600 --> 05:41.600
So other industry, currently 2.3 million industrial robots out there operating as we speak and 40

05:41.600 --> 05:50.800
percent of those in car industry, automotive industry and kind of out of car industry,

05:50.800 --> 05:55.360
about 70 percent operating the body shop which is where they make the chassis of the car.

05:56.240 --> 06:00.240
So these are the pictures and videos you see with the car kind of going down the line

06:00.240 --> 06:06.720
starting from the frame and then putting all the, and those are massive, massive robots that are

06:06.720 --> 06:10.320
open. Yeah and they require like massive holes to operate on.

06:11.600 --> 06:17.920
And it's a big, big task to configure like when you can imagine when a new car model comes in

06:17.920 --> 06:24.240
with all the different variants and how many parts the car is made of. So you can imagine the

06:24.240 --> 06:29.600
task of programming all this in which all robot excels down at the production line.

06:29.600 --> 06:33.760
So it's massive task. Do you have a sense for how long that usually takes

06:33.760 --> 06:39.680
for when a new, you know, once I've completed design on a new car to complete the,

06:40.640 --> 06:44.160
you know, the programming of the line to get it to market?

06:44.160 --> 06:50.640
Yeah so typically they practice the production like they have 100 cars, 200 cars and so on and

06:50.640 --> 06:58.960
often even with the production run practice like you typically change the part in dimension of form

06:58.960 --> 07:06.400
to fit in better, but it's like you're talking in days of like very, very hard ramp up production.

07:10.480 --> 07:14.640
I've also seen where, you know, there are companies that are trying to,

07:17.440 --> 07:22.160
there are companies that are trying to solve this robot programming problem by,

07:23.040 --> 07:28.160
as opposed to, you know, the traditional programming approach basically have the robot like,

07:28.160 --> 07:33.200
you know, shadow a human or you kind of move the robot the way you want the move to move the robot

07:33.200 --> 07:39.040
to, you manually manipulate the robot in order to teach it. It doesn't sound like that's what

07:39.040 --> 07:46.560
you guys are doing. That's part of our platform. Okay so tell us more about, you dig into the platform

07:46.560 --> 07:52.720
a little bit and how, how a company would use it. So the basic idea of the platform is to enable

07:52.720 --> 07:58.800
people to have natural interaction with the robot. If there are tasks that the robot can self-learn,

07:59.920 --> 08:05.680
the robot can do it using a platform if there's a necessity for the human to have a simple

08:05.680 --> 08:11.280
interaction with the robot to show some specific tasks. Okay. Still possible using our platform.

08:12.720 --> 08:17.440
Basically we are trying to remove the need for the human to have expertise in the robotics

08:17.440 --> 08:22.000
in order to interact with the robots. We want normal human without that kind of expertise to be

08:22.000 --> 08:27.200
able to interact with the robot and be able to teach it simple tasks or even more communicate in

08:27.200 --> 08:34.400
the future hopefully. So kind of like we see that all like teaching by demonstration or even like

08:35.600 --> 08:42.080
teaching by scripting or like even voice commands like we see this like blend in in our platform

08:42.080 --> 08:47.520
and some point in the future. So you can have any industrial, any robotic arm on your table and you

08:47.520 --> 08:54.240
can ask it to grab your coffee cup and then it's able to do that. So that's that's where we are

08:54.240 --> 09:02.320
aiming at. And now today the robots aren't even typically or to what degree do the robots

09:02.320 --> 09:10.080
typically already have visual sensors or is that something that that has to be outfitted in

09:10.080 --> 09:17.840
order for a company to use your approach? Yeah so we kind of for industrial robots in the kind

09:17.840 --> 09:24.320
of so we're going to retrofit robot industry robots with a specific set of sensors so it can

09:24.320 --> 09:29.520
sense our own it's environment. Okay. We're not trying anything to manufacture any specific

09:29.520 --> 09:35.520
sensor we're trying to use off the shelf sensors and not really expensive ones. Okay. And

09:35.520 --> 09:41.680
what kinds of sensors primarily visual or are there other types of sensors that go into?

09:41.680 --> 09:47.120
We started by visual sensors but they're going to be a sense of fusion using different inputs

09:47.120 --> 09:56.240
to enhance the performance. Okay. I had a conversation with someone actually from one of the

09:56.240 --> 10:02.240
large auto manufacturers where they mentioned that one of the challenges that they were having in

10:02.240 --> 10:12.480
this space was that the robotics manufacturers are increasingly trying to you know they see one

10:12.480 --> 10:18.240
of the big they see the data that the robots are producing in terms of the sensors that they

10:18.240 --> 10:24.240
already put on the data and and they're operating data as kind of proprietary and they're trying

10:24.240 --> 10:31.520
to sell that now to the automakers and are you finding have you come across anything like that

10:31.520 --> 10:41.840
where the the manufacturers are you know having issues automating because the robotics suppliers

10:41.840 --> 10:47.200
are making it more difficult for them. Yeah of course like many especially big manufacturers

10:47.200 --> 10:54.080
like they are very sensitive about the data like what they have inside the inside this factory

10:54.080 --> 11:02.640
so they don't obviously want it to go away anywhere. What when you go when you're working with a client

11:02.640 --> 11:09.520
like this large auto manufacturer what challenges do you run into as they seek to use the your product?

11:12.080 --> 11:19.600
Yes obviously like with a for quite small company like us startup face and then

11:19.600 --> 11:26.560
scaling up to a multiple factories around the world so that's like that's something like where

11:26.560 --> 11:32.560
you need to ramp up your operation force and so on and so but we are not like we are iterating there

11:32.560 --> 11:39.840
so in order to gain the trust of those big manufacturers they definitely need certain level of

11:39.840 --> 11:50.800
safety and precision. That should be proven over time and they need to trust the product

11:50.800 --> 11:57.840
and make sure it works before they can deploy manufacturing. So what's their experience with the

11:57.840 --> 12:06.320
product when they you know you guys come in and get them set up with I imagine a small use case

12:06.320 --> 12:10.000
to start to build that trust. What does it look like from their perspective?

12:11.280 --> 12:19.520
Yes at the moment we are still so like our product is not yet in operation so yeah so it's going

12:19.520 --> 12:25.600
there. What do you envision the user experience to be when you get a new customer onboard?

12:25.600 --> 12:31.680
Yes so the user experience will be very simple so like for them like instead of now programming

12:31.680 --> 12:40.400
the robot they can just upload the cat model of that specific part they want to be creeped and

12:40.400 --> 12:46.240
then they specify the end location where it needs to go and so it's that simple.

12:46.240 --> 13:01.280
And you mentioned so you as opposed to having to explicitly tell the robot you know give it

13:01.280 --> 13:07.360
coordinates of a pick location you're giving them what instead or are you giving them coordinates

13:07.360 --> 13:12.960
but just not the dimensions of the item or yeah yeah so the problem we are solving is not

13:12.960 --> 13:20.640
them so typically that's where the part needs to go for example most of the operations are

13:20.640 --> 13:27.440
welding these parts or gluing these parts together. So the end point is typically could be

13:28.400 --> 13:36.640
pretty determined but where do you grasp like that part if you now they need to manufacture

13:36.640 --> 13:44.240
custom manufacturer chicks so that they hold stack of parts very very accurately and with every

13:44.240 --> 13:48.960
new part if you have new model or something like you need to custom manufacturer a new chick and

13:48.960 --> 13:55.680
this is obviously a pick so we can just like stack paths arbitrarily and so they still leave the

13:55.680 --> 14:02.160
creeping location is specified still the robot can figure out from exactly that location but

14:02.160 --> 14:07.280
the parts doesn't have to be in that exact orientation for the robot in order to find given if

14:07.280 --> 14:12.480
they have some randomness in their orientation the robot can still extract the pose of the object

14:12.480 --> 14:18.560
and recognize that part and the creeping location and grasp from there. So there are two

14:20.320 --> 14:25.200
two problems in there one is kind of the bucket of parts problem right so how do I know what

14:25.200 --> 14:32.800
the orientation is but then there's also are you also enabling them to use general grippers as

14:32.800 --> 14:38.080
opposed to custom made grippers for a given part or for the specific task of manufacturing they

14:38.080 --> 14:46.240
usually have their own specific tools or groupers the systems to be integrated with that but in the

14:46.240 --> 14:54.720
future the robot can learn to use different kind of tools. So maybe let's dive into the technology

14:54.720 --> 15:00.640
a little bit you know what machine learning and AI techniques are you using to make all this work?

15:00.640 --> 15:06.480
Sure so in the robot industry at the moment you don't have a kind of common operating system

15:06.480 --> 15:12.960
that the robot can use every manufacturer to make their own custom programs it's really low level

15:12.960 --> 15:22.800
programming we're trying to make it a bit more general something that imagined operating system

15:22.800 --> 15:27.360
that has a low level interface that can interface different kind of robots and that are high level

15:27.360 --> 15:34.080
modules like recognizing the parts grasping the parts or collision awareness or any intelligent

15:34.080 --> 15:38.800
that you need in the robot behavior that are abstracted from the low level communication and they

15:38.800 --> 15:47.680
can grow and get better over time and this makes our operating system. And so how would you characterize

15:47.680 --> 15:58.240
where you are with the product so far? So far we've integrated with three different robot

15:58.240 --> 16:08.000
manufacturers I can name it it's Kuka, Universal Robots and Stubli and we're implementing some

16:08.000 --> 16:14.080
basic AI modules but later we're going to open up this platform for people to add more skills

16:14.080 --> 16:18.960
to the robot and expand the platform over time. So what are some examples of the AI modules

16:18.960 --> 16:25.760
that you've implemented? So right now we are working on the very accurate localization so that's

16:25.760 --> 16:33.760
the key value adding module in our product at the moment and then we've added our recognition

16:34.560 --> 16:43.600
and then simple robot controllers. So what goes into localization? What are the challenges there

16:43.600 --> 16:50.640
and how are you attacking those? Yeah so basic is kind of like where the uncertainty comes from

16:50.640 --> 16:57.520
is like what you can receive the resolution from your sensors and then kind of how you can fit

16:57.520 --> 17:07.520
that into your model and kind of get accurate enough prediction over time and like over multiple

17:07.520 --> 17:13.600
samples like and what do you do and about like going back to the modules like you can think

17:14.400 --> 17:19.600
different kind of modules like you can think of like safety modules where you can detect people

17:19.600 --> 17:26.480
if people somehow enter this cell where the robot is operating so you can have these sorts of

17:26.480 --> 17:32.000
all you can have force feedback where you can recognize different forces when you're installing

17:32.000 --> 17:42.480
some components and so on and so are you are you building everything from the ground up from scratch

17:42.480 --> 17:51.440
or are you using various open source libraries for the analytics parts? It's a combination of both

17:52.800 --> 17:59.280
we need some certain tools we cannot develop everything from scratch but there is a certain level

17:59.280 --> 18:06.480
of modification that we need to make to the existing tools to make them usable for that precision

18:06.480 --> 18:11.120
required for manufacturing and that's the work that we need to accomplish. Anything that we might

18:11.120 --> 18:18.560
know in terms of tools are you running on top of yeah on the deep learning side we use TensorFlow

18:18.560 --> 18:25.200
so that's great yeah we also use on the robotic side kind of like low level commands we use

18:25.200 --> 18:34.320
ROS so in the robotic research there's this platform called ROS a robot operating system and it has

18:35.120 --> 18:41.360
kind of distributed architecture no it can run in parallel and they're coming to come and get

18:41.360 --> 18:47.760
into the master mode and so on we're using the idea the whole platform like what has been developed

18:47.760 --> 18:55.200
for research is not suitable for manufacturing in terms of safety and so on but the idea is

18:55.200 --> 19:01.280
great so we're trying to adapt those ideas but implement our own version that makes it suitable

19:01.280 --> 19:09.440
for precision required for manufacturing. Okay that was great great anything else you'd like to

19:09.440 --> 19:19.360
share about what you guys are up to? Yeah we'll share like in the bit later this year just follow us

19:19.360 --> 19:28.720
on Twitter and check out website and you'll hear more about us. Okay awesome how can folks what is

19:28.720 --> 19:40.800
your Twitter and working folks find you online? Yeah it's called Keynes or C4-1 NT. C4-1 NT. Okay and

19:40.800 --> 19:48.160
that's the Twitter or is that the website? Yeah and the website is C-A-I-N-T that's O-O

19:48.160 --> 19:54.240
okay got it awesome well thanks so much for being on the show it's great to hear about what you

19:54.240 --> 20:05.360
guys are up to. Thanks for having me. Yeah thanks for having us.

