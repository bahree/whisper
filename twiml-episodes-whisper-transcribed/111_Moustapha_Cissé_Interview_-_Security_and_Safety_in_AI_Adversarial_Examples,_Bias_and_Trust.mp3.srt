1
00:00:00,000 --> 00:00:16,320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

2
00:00:16,320 --> 00:00:21,480
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:21,480 --> 00:00:32,960
I'm your host Sam Charrington.

4
00:00:32,960 --> 00:00:38,520
While at the Nips conference back in December, I attended the Black and AI workshop and dinner

5
00:00:38,520 --> 00:00:42,160
and had a chance to meet a bunch of amazing people.

6
00:00:42,160 --> 00:00:46,000
This week on the show, we'll be highlighting some of the great work being done by folks

7
00:00:46,000 --> 00:00:48,320
in this community.

8
00:00:48,320 --> 00:00:53,800
In this episode, I'm joined by Mustafa Cise, research scientist at Facebook AI Research

9
00:00:53,800 --> 00:00:56,600
Lab or Fair in Paris.

10
00:00:56,600 --> 00:01:01,640
Mustafa's broad research interests include the security and safety of AI systems, and

11
00:01:01,640 --> 00:01:06,440
we spend some time discussing his work on adversarial examples and systems that are robust

12
00:01:06,440 --> 00:01:08,720
to adversarial attacks.

13
00:01:08,720 --> 00:01:13,560
More broadly, we discuss the role of bias in datasets and explore his vision for models

14
00:01:13,560 --> 00:01:17,960
that can identify these biases and adjust the way they train themselves in order to

15
00:01:17,960 --> 00:01:21,160
avoid taking them on.

16
00:01:21,160 --> 00:01:27,400
Before we dive in, we want to hear about your experiences with home and personal AI.

17
00:01:27,400 --> 00:01:31,120
So we launched my AI video contest last week.

18
00:01:31,120 --> 00:01:37,000
Please head on over to twimbleai.com slash myai and take a few minutes to submit your thoughts

19
00:01:37,000 --> 00:01:42,560
on the role AI is playing in your home and personal life, your favorite example of home

20
00:01:42,560 --> 00:01:47,320
or personal AI, the AI that you really want to see in your lifetime.

21
00:01:47,320 --> 00:01:50,120
Or just where you see this all going.

22
00:01:50,120 --> 00:01:55,560
The entries with the most likes will win great prizes, including a Cosmo and a Lighthouse,

23
00:01:55,560 --> 00:02:00,720
both of which we discussed on our AI and consumer electronic series last week.

24
00:02:00,720 --> 00:02:03,760
And now on to the show.

25
00:02:03,760 --> 00:02:08,680
Alright everyone, I am on the line with Mustafa Cise.

26
00:02:08,680 --> 00:02:14,000
Mustafa is a research scientist at Facebook AI Research Lab and we had the pleasure of

27
00:02:14,000 --> 00:02:21,200
meeting at the recent nips event and particularly we hung out quite a bit at the black and AI,

28
00:02:21,200 --> 00:02:23,400
both the workshop and the dinner.

29
00:02:23,400 --> 00:02:25,560
Mustafa, it is great to reconnect with you.

30
00:02:25,560 --> 00:02:26,560
How are you doing?

31
00:02:26,560 --> 00:02:27,560
I'm great.

32
00:02:27,560 --> 00:02:28,560
How about you?

33
00:02:28,560 --> 00:02:29,560
Fantastic.

34
00:02:29,560 --> 00:02:30,560
Fantastic.

35
00:02:30,560 --> 00:02:37,320
So the tradition here is for me to give you an opportunity to introduce yourself to

36
00:02:37,320 --> 00:02:38,320
the audience.

37
00:02:38,320 --> 00:02:42,720
So why don't you tell us a little bit about your background and how you got interested

38
00:02:42,720 --> 00:02:44,360
in AI?

39
00:02:44,360 --> 00:02:45,360
Sure.

40
00:02:45,360 --> 00:02:46,360
So I'm Mustafa.

41
00:02:46,360 --> 00:02:49,760
I was born and raised in Senegal in West Africa.

42
00:02:49,760 --> 00:02:54,120
If you have never been there, you should, it's a fantastic country, just a little bit of

43
00:02:54,120 --> 00:02:55,120
ad.

44
00:02:55,120 --> 00:02:56,960
Nice.

45
00:02:56,960 --> 00:03:01,760
So I was born and raised there and I did most of my education there.

46
00:03:01,760 --> 00:03:07,920
I went to the University Gaston Berger, where I studied mathematics and physics.

47
00:03:07,920 --> 00:03:11,800
And it's during one of these courses that I really got interested into AI.

48
00:03:11,800 --> 00:03:15,400
It was an algorithmic course where we had a project.

49
00:03:15,400 --> 00:03:22,560
The task was to design an AI, basically, to solve the game of AOLA, which is a strategic

50
00:03:22,560 --> 00:03:29,480
game, a bit like Checkers or chess, but very popular in West Africa.

51
00:03:29,480 --> 00:03:34,880
And most of the systems that were designed by the students were based on a rule base.

52
00:03:34,880 --> 00:03:38,080
If you do it, if then else rules, basically.

53
00:03:38,080 --> 00:03:46,960
So I really wanted to design a system that learned from different games and learns to solve

54
00:03:46,960 --> 00:03:49,000
the game, basically.

55
00:03:49,000 --> 00:03:51,200
So that's how I got into AI.

56
00:03:51,200 --> 00:03:58,520
I started watching videos on YouTube and teaching myself some of the basic algorithms.

57
00:03:58,520 --> 00:03:59,520
Okay.

58
00:03:59,520 --> 00:04:00,520
Awesome.

59
00:04:00,520 --> 00:04:01,520
Awesome.

60
00:04:01,520 --> 00:04:08,000
And so how did you make your way from there to your current role at Facebook AI research?

61
00:04:08,000 --> 00:04:12,400
So, after that experience, I really enjoyed it.

62
00:04:12,400 --> 00:04:18,480
And I kind of realized that this was what I wanted to do for the rest of my life.

63
00:04:18,480 --> 00:04:21,000
I could contribute in this area.

64
00:04:21,000 --> 00:04:29,160
So I decided to go abroad because there were now not an advanced course or degree in AI

65
00:04:29,160 --> 00:04:30,480
in the university.

66
00:04:30,480 --> 00:04:34,120
So I went to France to study for a master's.

67
00:04:34,120 --> 00:04:40,640
So I studied, I spent the first year in Paris at the University of Paris in Marie Curie.

68
00:04:40,640 --> 00:04:46,040
Then I spent the second year at the University of Montreal.

69
00:04:46,040 --> 00:04:49,520
Then I came back to Paris to do a PhD there.

70
00:04:49,520 --> 00:04:56,920
And then after I graduated from my PhD, I did a one-year postdoc and after that postdoc,

71
00:04:56,920 --> 00:05:00,080
I came to Facebook AI research.

72
00:05:00,080 --> 00:05:01,080
So it's been...

73
00:05:01,080 --> 00:05:03,160
You're based in New York cities, all right?

74
00:05:03,160 --> 00:05:07,080
Actually, I'm based in Paris even though I spent some time in New York sometimes.

75
00:05:07,080 --> 00:05:08,080
Oh, okay.

76
00:05:08,080 --> 00:05:09,080
But I'm based in Paris.

77
00:05:09,080 --> 00:05:14,960
So we have a live in Paris with about 30 researchers and engineers.

78
00:05:14,960 --> 00:05:15,960
Oh, nice.

79
00:05:15,960 --> 00:05:16,960
Yeah.

80
00:05:16,960 --> 00:05:17,960
Nice.

81
00:05:17,960 --> 00:05:23,080
And so maybe we can start out by having you tell us a little bit about your research interests

82
00:05:23,080 --> 00:05:25,400
and the kind of things you're working on nowadays.

83
00:05:25,400 --> 00:05:26,400
Yeah.

84
00:05:26,400 --> 00:05:32,040
So I'm interested in AI and machine learning at large.

85
00:05:32,040 --> 00:05:37,000
But these days I spend most of my time working on trust in AI.

86
00:05:37,000 --> 00:05:48,120
So by trust, I mean all the topics pertaining to safety and security in AI, fairness and

87
00:05:48,120 --> 00:05:51,280
biases and also interpretability.

88
00:05:51,280 --> 00:05:56,320
I think these are very important topics that are gaining some momentum now but that have

89
00:05:56,320 --> 00:05:59,720
been overlooked for some time.

90
00:05:59,720 --> 00:06:08,160
And it's very important that as a community, we focus on building systems that can be trusted

91
00:06:08,160 --> 00:06:14,880
and that can be used broadly to say.

92
00:06:14,880 --> 00:06:18,080
So that's what I spend most of my time working on right now.

93
00:06:18,080 --> 00:06:19,880
Okay.

94
00:06:19,880 --> 00:06:25,200
Can you maybe give me some examples of your recent research you've done in these areas?

95
00:06:25,200 --> 00:06:26,200
Sure.

96
00:06:26,200 --> 00:06:34,320
So in the area of security and safety, I have worked on understanding the topic of adversarial

97
00:06:34,320 --> 00:06:35,720
examples.

98
00:06:35,720 --> 00:06:43,720
So adversarial examples are examples that the model, the models of an AI system sees or

99
00:06:43,720 --> 00:06:51,320
hears or depending on the modality, but that makes the model behave in a completely different

100
00:06:51,320 --> 00:06:59,360
way that it should behave that is in a completely unexpected way.

101
00:06:59,360 --> 00:07:05,400
So these are malicious examples and they tell us a lot about how the models that we are

102
00:07:05,400 --> 00:07:10,840
building right now are kind of not very well understood.

103
00:07:10,840 --> 00:07:17,000
So I have done some work with colleagues in understanding how these adversarial examples

104
00:07:17,000 --> 00:07:23,360
arise and how to generate them for speech recognition or for semantic segmentation which

105
00:07:23,360 --> 00:07:29,280
are very challenging tasks, but which have applications in personal assistance or in self-driving

106
00:07:29,280 --> 00:07:30,680
cars.

107
00:07:30,680 --> 00:07:35,360
And it's very important to be able to create these adversarial examples because that's

108
00:07:35,360 --> 00:07:41,000
the first step in order to evaluate the robustness of an AI.

109
00:07:41,000 --> 00:07:45,160
So then we moved on working on defenses against these adversarial examples.

110
00:07:45,160 --> 00:07:50,560
We have also done some work in this area which have been recently published with some

111
00:07:50,560 --> 00:07:57,240
colleagues here and another line of work is also this area of biases and fairness.

112
00:07:57,240 --> 00:08:05,040
So recently we have shown in a study that the models basically the deep learning models

113
00:08:05,040 --> 00:08:10,440
that you use to train to recognize images when you train them on a popular benchmark

114
00:08:10,440 --> 00:08:13,080
data set which is imagined at.

115
00:08:13,080 --> 00:08:19,760
They learn quite some biased decision making process and this is somehow unexpected because

116
00:08:19,760 --> 00:08:24,160
most of the time the data set is considered as balance, but there are explanations for

117
00:08:24,160 --> 00:08:25,160
these.

118
00:08:25,160 --> 00:08:32,160
So we have observed some interesting biases ranging from racial biases for example and

119
00:08:32,160 --> 00:08:34,200
all this is described in the paper.

120
00:08:34,200 --> 00:08:35,200
Okay.

121
00:08:35,200 --> 00:08:41,240
Yeah, I think I've talked about adversarial examples in my newsletter but I don't think

122
00:08:41,240 --> 00:08:46,640
we've really dug into it in the podcast, at least not in a lot of detail.

123
00:08:46,640 --> 00:08:53,040
So maybe let's start there and for folks who aren't familiar with that whole field,

124
00:08:53,040 --> 00:08:58,280
do you have some favorite examples of adversarial examples?

125
00:08:58,280 --> 00:08:59,280
Sure.

126
00:08:59,280 --> 00:09:08,360
So imagine you have a system that is supposed to tell you what is in a picture.

127
00:09:08,360 --> 00:09:15,240
So if you present the system with a picture it says, this is a dog or this is a cat etc.

128
00:09:15,240 --> 00:09:21,320
So it is possible to inject to the picture that you're presenting to the image.

129
00:09:21,320 --> 00:09:28,200
Some very structured noise that is imperceptible to the human eye but that will change the

130
00:09:28,200 --> 00:09:29,840
decision of the system.

131
00:09:29,840 --> 00:09:36,160
So you can have two seemingly identical pictures of dog of the same dog and for one of them

132
00:09:36,160 --> 00:09:40,240
the system will say this is a dog and for the second one the system will say this is

133
00:09:40,240 --> 00:09:44,280
a cat or this is a car or any other thing.

134
00:09:44,280 --> 00:09:51,320
So this is kind of intriguing because these systems are very accurate normally and we tend

135
00:09:51,320 --> 00:09:56,760
to compare them to humans because they are very accurate and when we measure their accuracy

136
00:09:56,760 --> 00:10:02,720
we say hey, usually we say hey look the system is as accurate as a human but on the flip side

137
00:10:02,720 --> 00:10:08,160
a human is very robust to these kind of perturbations like no matter how much perturbation you

138
00:10:08,160 --> 00:10:14,200
add to an example if a human looks at two pictures that are similar she will be able

139
00:10:14,200 --> 00:10:20,200
to say that this is a dog and this is a dog as well but if the system can be tricked

140
00:10:20,200 --> 00:10:27,200
by these slight modifications that are imperceptible to the human eyes it means something.

141
00:10:27,200 --> 00:10:33,480
It says a lot about the current understanding or the lack of understanding that we have

142
00:10:33,480 --> 00:10:39,240
about the behavior of these models and it makes it an interesting topic to study.

143
00:10:39,240 --> 00:10:45,160
So this is basically the high level explanation of adversarial examples.

144
00:10:45,160 --> 00:10:53,880
By the way I gave an example speaking about topics or speaking about images, image recognition

145
00:10:53,880 --> 00:11:00,160
but I think that's what most of us associate with adversarial examples with scenes a bunch

146
00:11:00,160 --> 00:11:03,800
of these but it's not just images.

147
00:11:03,800 --> 00:11:04,800
Absolutely.

148
00:11:04,800 --> 00:11:11,280
So we have shown in a recent paper a generic method called Houdini that allows you to generate

149
00:11:11,280 --> 00:11:17,080
adversarial examples not only for images but also for speech recognition task where you

150
00:11:17,080 --> 00:11:23,520
can add some noise in an audio file such that humans cannot distinguish the two audio

151
00:11:23,520 --> 00:11:30,720
files but a speech recognition system will be completely interpreted to audio files in

152
00:11:30,720 --> 00:11:32,280
a completely different way.

153
00:11:32,280 --> 00:11:34,720
So it's a very broad topic.

154
00:11:34,720 --> 00:11:35,720
Oh wow.

155
00:11:35,720 --> 00:11:42,120
So the example that comes to mind for me is if you can create an audio file that says

156
00:11:42,120 --> 00:11:49,800
one thing but interprets the system as saying okay Google you could really mess with people's

157
00:11:49,800 --> 00:11:53,680
their virtual assistants and things like that that they've got around their home.

158
00:11:53,680 --> 00:11:54,680
Sure.

159
00:11:54,680 --> 00:11:59,920
That's one one scenario you could imagine definitely.

160
00:11:59,920 --> 00:12:01,720
That's one scenario you could imagine.

161
00:12:01,720 --> 00:12:10,040
To what extent are the current adversarial examples that work in general like to what

162
00:12:10,040 --> 00:12:16,040
extent is that model specific meaning is the work that's happening around adversarial

163
00:12:16,040 --> 00:12:25,120
examples and defenses is it all explicitly impact specific types of models you know deep

164
00:12:25,120 --> 00:12:29,240
neural networks or specific types of deep neural networks or specific architectures

165
00:12:29,240 --> 00:12:35,680
of networks or is it a broader phenomenon that can apply more generally.

166
00:12:35,680 --> 00:12:41,400
So I would say that this is a broad phenomenon that applies to different class of model

167
00:12:41,400 --> 00:12:45,520
branching from deep neural networks of course people talk about it because it's the most

168
00:12:45,520 --> 00:12:50,360
familiar family of functions right now but it also applies to separate vector machines

169
00:12:50,360 --> 00:12:53,120
or to decision trees and all sorts of things.

170
00:12:53,120 --> 00:12:59,160
That being said the kind of model that you that the specific type of model that you're

171
00:12:59,160 --> 00:13:07,080
that you're using can have properties that make it less robust to adversarial examples.

172
00:13:07,080 --> 00:13:11,640
This is true across the different family of models but depending on the conditioning

173
00:13:11,640 --> 00:13:17,800
and the nice some nice properties that the model may have they may be more or less robust

174
00:13:17,800 --> 00:13:19,840
to adversarial examples.

175
00:13:19,840 --> 00:13:27,160
However for the defenses there are defenses that exploit the nature of the model sometimes

176
00:13:27,160 --> 00:13:32,520
but there are also some defenses that are agnostic to the type of model that you consider.

177
00:13:32,520 --> 00:13:38,880
For example we recently proposed in a paper defenses against adversarial examples that

178
00:13:38,880 --> 00:13:45,800
are just based on transformations of the input in order to remove the noise that has

179
00:13:45,800 --> 00:13:49,720
been injected to the input.

180
00:13:49,720 --> 00:13:54,800
So this is typically completely agnostic to the type of model that you consider.

181
00:13:54,800 --> 00:13:56,800
Okay.

182
00:13:56,800 --> 00:14:03,560
These adversarial examples they you know they're maliciously created what's the process

183
00:14:03,560 --> 00:14:06,600
generally for creating them.

184
00:14:06,600 --> 00:14:13,320
So the process for creating an adversarial example is actually the inverse process for

185
00:14:13,320 --> 00:14:15,000
training the model.

186
00:14:15,000 --> 00:14:21,760
So when you train the model you show to the model an image of a dog and you measure how

187
00:14:21,760 --> 00:14:25,120
well the model recognizes that this is a log.

188
00:14:25,120 --> 00:14:33,640
This is a dog and you reinforce that decision if it is a positive one and or you you kind

189
00:14:33,640 --> 00:14:40,320
of encourage the model to change its decision if the decision is not good.

190
00:14:40,320 --> 00:14:45,240
So when you create an adversarial example you do the reverse process and the way you

191
00:14:45,240 --> 00:14:51,600
do it is that we show an example of a dog to the model and when the model says yes this

192
00:14:51,600 --> 00:15:02,760
is a dog then you calculate from the model the right direction in which you need to move

193
00:15:02,760 --> 00:15:09,680
the example in the input space in order to change slowly the decision of the model which

194
00:15:09,680 --> 00:15:17,280
means that your base on the decision of the model in order to know how to trick it.

195
00:15:17,280 --> 00:15:20,440
So it's the opposite way of training the model.

196
00:15:20,440 --> 00:15:27,040
So the subtlety here is that changing the decision of the model does not require a big

197
00:15:27,040 --> 00:15:29,600
change in the input space.

198
00:15:29,600 --> 00:15:36,320
In fact if you consider images at the pixel level that change is always almost imperceptible

199
00:15:36,320 --> 00:15:43,840
by a human eye but it is surprisingly sufficient for flipping the decision of a model.

200
00:15:43,840 --> 00:15:50,400
You know there are technologies in the case of images and audio as well where you can have

201
00:15:50,400 --> 00:15:56,600
like these digital watermarks where you are doing similar things right you are placing

202
00:15:56,600 --> 00:16:03,600
some kind of watermark I guess in that case it is not noise per se but you are altering

203
00:16:03,600 --> 00:16:12,240
the image in a way that is imperceptible to the human observer but has some meaning to

204
00:16:12,240 --> 00:16:14,080
the system.

205
00:16:14,080 --> 00:16:17,920
I guess I'm just sharing that as kind of an example of how that process is.

206
00:16:17,920 --> 00:16:25,880
It's very similar it's not the same thing but it's very similar that you manage to put

207
00:16:25,880 --> 00:16:33,120
in the input some information that makes the model behave in some way but that is basically

208
00:16:33,120 --> 00:16:38,800
imperceptible and you are definitely right pointing to the watermarks because it's very

209
00:16:38,800 --> 00:16:40,880
similar in spirit.

210
00:16:40,880 --> 00:16:47,640
And so there are a bunch of folks that are researching these adversarial examples.

211
00:16:47,640 --> 00:16:53,760
What's the current research thrust is it trying to identify different ways of generating

212
00:16:53,760 --> 00:17:00,440
them or have we all kind of is there you know only you know that general way that you

213
00:17:00,440 --> 00:17:05,760
describe to create them and everyone is working on defense or what.

214
00:17:05,760 --> 00:17:09,400
So there are different rules for generating adversarial examples but they basically exploit

215
00:17:09,400 --> 00:17:12,320
the same information which is the gradient.

216
00:17:12,320 --> 00:17:16,480
In one way or another it can be direct when you have access to the full information

217
00:17:16,480 --> 00:17:22,320
about the model or it can be direct where you kind of train a substitute of a model and

218
00:17:22,320 --> 00:17:26,640
generate the gradient from it and transfer the adversarial example to another model.

219
00:17:26,640 --> 00:17:32,040
So there are a few different attacks that do exist but for now I have to say that attackers

220
00:17:32,040 --> 00:17:37,280
have an edge because it's much more difficult to full a model than to protect it from adversarial

221
00:17:37,280 --> 00:17:38,280
examples.

222
00:17:38,280 --> 00:17:43,080
However in my opinion adversarial examples are not just interesting from a security point

223
00:17:43,080 --> 00:17:44,080
of view.

224
00:17:44,080 --> 00:17:51,040
Of course this is very important because it's critical when you want to use these powerful

225
00:17:51,040 --> 00:17:57,760
and attractive deep learning models for example in certain types of applications.

226
00:17:57,760 --> 00:18:04,680
But another aspect of the adversarial example is that they tell us something about the models

227
00:18:04,680 --> 00:18:08,480
that we love and that we use on a daily basis.

228
00:18:08,480 --> 00:18:14,600
They say that there are things that we do not understand because this behavior is

229
00:18:14,600 --> 00:18:17,200
sort of pathological.

230
00:18:17,200 --> 00:18:23,720
So they trigger interesting research questions beyond the security perspective but on the

231
00:18:23,720 --> 00:18:28,520
very nature of these models and the learning algorithms that we use.

232
00:18:28,520 --> 00:18:33,560
And we have also found with other colleagues that adversarial examples can be used for

233
00:18:33,560 --> 00:18:39,480
other purposes and in our case it was to exhibit biases that the model may have learned

234
00:18:39,480 --> 00:18:43,760
from the data which was quite interesting to see.

235
00:18:43,760 --> 00:18:47,320
Can you elaborate on that and what the implications are?

236
00:18:47,320 --> 00:18:53,720
When you have a model so basically machine learning models or you design a parametric

237
00:18:53,720 --> 00:19:00,880
model you take some data you apply a learning rule and you train the model from the data.

238
00:19:00,880 --> 00:19:09,720
So you can expect that if your model is trained to optimize for some criteria it will behave

239
00:19:09,720 --> 00:19:16,160
very well if the learning algorithm is good, if you have sufficient data, if the model

240
00:19:16,160 --> 00:19:25,200
that you have chosen is powerful enough that it will behave in some expected way, right?

241
00:19:25,200 --> 00:19:29,960
But so many your input data is follows a similar distribution.

242
00:19:29,960 --> 00:19:35,960
Absolutely, that's the back hypothesis, very good observation.

243
00:19:35,960 --> 00:19:44,680
So the point here is that your model may have captured some regularities that are present

244
00:19:44,680 --> 00:19:52,000
in the data but that you may not notice because when you test the model or when you evaluate

245
00:19:52,000 --> 00:19:58,840
it you are not using the right criteria that could show you the broad scope of everything

246
00:19:58,840 --> 00:20:00,800
your model have learned.

247
00:20:00,800 --> 00:20:03,920
So I can give you an example.

248
00:20:03,920 --> 00:20:11,760
So we have found that, for example, if you take a popular model like ResNet, residual networks

249
00:20:11,760 --> 00:20:18,680
which are very popular in computer vision and you train them on ImageNet using all the

250
00:20:18,680 --> 00:20:24,880
sophisticated learning algorithms then you will achieve some excellent performance as far

251
00:20:24,880 --> 00:20:30,680
as accuracy is concerned but when you look deeply into why the model predicts what it

252
00:20:30,680 --> 00:20:33,600
predicts you can see some very funny things.

253
00:20:33,600 --> 00:20:40,040
So for example we found out that if you consider the class the category basketball.

254
00:20:40,040 --> 00:20:47,520
So for some reason the model tend to consider that if you show an input with a black person

255
00:20:47,520 --> 00:20:52,960
to the model it will predict that this is a picture whose corresponding category is

256
00:20:52,960 --> 00:21:00,760
basketball and this is no matter whether it is a basketball or not doesn't matter.

257
00:21:00,760 --> 00:21:05,560
And the reason is the model has picked up some biases in the data.

258
00:21:05,560 --> 00:21:13,680
So that suggests this actually and it's true also for different sort of biases.

259
00:21:13,680 --> 00:21:19,240
It almost makes me think that like all models over fit, it's just some of them over

260
00:21:19,240 --> 00:21:26,400
fit too much but you know the other ones are overfitting in ways that you know might produce

261
00:21:26,400 --> 00:21:32,720
acceptable results until you know we're throwing a curve ball and see something that we didn't

262
00:21:32,720 --> 00:21:33,720
expect.

263
00:21:33,720 --> 00:21:34,720
Absolutely.

264
00:21:34,720 --> 00:21:37,520
This is a very good observation.

265
00:21:37,520 --> 00:21:42,880
So many of the models which if they are not properly regularized or if the last function

266
00:21:42,880 --> 00:21:52,440
the objective, the criteria that you're optimizing for does not take into account these problems

267
00:21:52,440 --> 00:21:57,680
they may well learn different sort of biases from the data.

268
00:21:57,680 --> 00:22:00,880
And these biases are numerous and diverse by the way.

269
00:22:00,880 --> 00:22:09,480
So I always put it this way I think you are what you eat and for models for the models

270
00:22:09,480 --> 00:22:14,160
the data sets they're trained on right now are just junk food.

271
00:22:14,160 --> 00:22:19,140
So we should we should not expect them to behave differently.

272
00:22:19,140 --> 00:22:25,240
If we don't endow the models with the ability to pick precisely the examples they should

273
00:22:25,240 --> 00:22:29,600
learn from and what they should learn and what they should not learn.

274
00:22:29,600 --> 00:22:34,320
Just the way we should we we do it with the kids for example where the kid takes something

275
00:22:34,320 --> 00:22:42,160
and puts it in her mouth if it is something that a kid can eat you probably can let her

276
00:22:42,160 --> 00:22:48,920
do but if it is not then you will stop her and say hey you cannot do this and as adult

277
00:22:48,920 --> 00:22:52,920
as well sometimes we say oh I can eat this I cannot eat that because there is there is

278
00:22:52,920 --> 00:22:57,960
a lot of fat here there is not enough there is a lot of sugar here etc.

279
00:22:57,960 --> 00:23:05,360
So we should endow the models with the ability to do some self criticism and introspection

280
00:23:05,360 --> 00:23:10,360
to say I should learn from this data set I should learn from this data point or I should

281
00:23:10,360 --> 00:23:13,720
not learn from this data point etc.

282
00:23:13,720 --> 00:23:19,000
So these are very fascinating questions I spend a lot of time thinking and working

283
00:23:19,000 --> 00:23:21,000
on these days.

284
00:23:21,000 --> 00:23:28,240
So I wanted to drill down into that last point you made about endowing the model with

285
00:23:28,240 --> 00:23:34,480
this ability to kind of discriminate between good and bad data but before we do that we

286
00:23:34,480 --> 00:23:42,120
kind of got to the bias by way of the adversarial examples and I think you were making a connection

287
00:23:42,120 --> 00:23:46,640
there that I didn't fully catch and I want to make sure that if that is what you were

288
00:23:46,640 --> 00:23:48,800
trying to do that we make that clear.

289
00:23:48,800 --> 00:23:54,920
Sure so the reason why adversarial examples can be interesting to exhibit the biases

290
00:23:54,920 --> 00:24:00,760
the model may have learned is actually very simple so when you learn a classifier basically

291
00:24:00,760 --> 00:24:07,520
what you are trying to do is to find a decision boundary to draw a line that says that this

292
00:24:07,520 --> 00:24:12,560
side of the line is the cat and the other side of the line is the dogs.

293
00:24:12,560 --> 00:24:16,960
This is very simple and you can visualize it very well if the model is linear but if

294
00:24:16,960 --> 00:24:20,800
the model is non-linear it is much more challenging.

295
00:24:20,800 --> 00:24:28,000
And the way adversarial examples are built it is by taking a point of a specific class

296
00:24:28,000 --> 00:24:36,160
and moving it slowly but in a very straightforward way to the other side of the decision boundary.

297
00:24:36,160 --> 00:24:39,680
So not all examples are created equal.

298
00:24:39,680 --> 00:24:46,480
If an example is close to the decision boundary in the latent space then you will not need

299
00:24:46,480 --> 00:24:52,960
to move it a lot in order to take it to the other side of the decision boundary.

300
00:24:52,960 --> 00:24:58,080
And if an example is far from the decision boundary you will have a hard time taking it

301
00:24:58,080 --> 00:25:03,000
to the decision boundary to the other side of it meaning that you will have to add a

302
00:25:03,000 --> 00:25:05,920
lot of noise, a significant amount of noise.

303
00:25:05,920 --> 00:25:11,560
But the examples that are close to the decision boundary are those examples for which the

304
00:25:11,560 --> 00:25:17,640
model have learned a representation that is not very robust when you consider the concept

305
00:25:17,640 --> 00:25:19,440
they should belong to.

306
00:25:19,440 --> 00:25:26,960
So if you take a picture of a dog and for which you can flip the decision of the model very

307
00:25:26,960 --> 00:25:34,040
easily by adding very slightly very small amount of noise that means that the model is

308
00:25:34,040 --> 00:25:38,760
not very sure about what this is and that is called criticism.

309
00:25:38,760 --> 00:25:40,480
So this is a criticism for the model.

310
00:25:40,480 --> 00:25:45,440
It's something that the model barely knows what it is but it's not very sure.

311
00:25:45,440 --> 00:25:51,720
But if you take a picture of a dog and you add a lot of noise and still you have a hard

312
00:25:51,720 --> 00:25:57,800
time flipping the decision of the model then the model is pretty confident what this is.

313
00:25:57,800 --> 00:26:03,400
So and that is what the model considers as being prototypical.

314
00:26:03,400 --> 00:26:11,680
So after learning from some data you can take another dataset, some test dataset and

315
00:26:11,680 --> 00:26:16,800
and by using the adversary the procedure for generating adversarial examples you can

316
00:26:16,800 --> 00:26:22,840
see how much noise you should add to some data points in order to change the decision

317
00:26:22,840 --> 00:26:30,080
of the classifier and use that as a proxy of how prototypical an example is to the model.

318
00:26:30,080 --> 00:26:33,960
So the examples that are prototypical to the model to what the model have learned about

319
00:26:33,960 --> 00:26:39,040
a given concept will be hard to change their decision.

320
00:26:39,040 --> 00:26:41,840
But for the others it will be easy to change their decision.

321
00:26:41,840 --> 00:26:46,120
And if you look at the prototypical example they will tell you what what the model has

322
00:26:46,120 --> 00:26:49,320
really learned what it is very confident about.

323
00:26:49,320 --> 00:26:54,960
And that's that's actually what we used and in order to discover what what the model considered

324
00:26:54,960 --> 00:26:57,680
really as being pictures of basketball.

325
00:26:57,680 --> 00:27:02,000
And when we looked at it it was all pictures of basketball basically with just blood

326
00:27:02,000 --> 00:27:04,600
persons inside and never white persons.

327
00:27:04,600 --> 00:27:09,160
And when we looked at pictures of that or criticisms which is you know some pictures

328
00:27:09,160 --> 00:27:13,840
that the model classifies pretty well but it's not very sure about it it's at the edge

329
00:27:13,840 --> 00:27:24,240
of its knowledge then it's very it's pictures that are mostly populated with white persons.

330
00:27:24,240 --> 00:27:32,120
And all this is basically validated on different categories and different classes.

331
00:27:32,120 --> 00:27:38,560
Similarly we found that when the model considers us being prototypical offered an image belonging

332
00:27:38,560 --> 00:27:45,600
to the category ping pong is basically a picture with an Asian in it.

333
00:27:45,600 --> 00:27:52,480
So so so after when people see this kind of things they will say hey the model is is racist.

334
00:27:52,480 --> 00:27:56,840
In fact a model cannot be racist because it's not intelligent the models that we build

335
00:27:56,840 --> 00:27:59,320
are accurate but they are not intelligent.

336
00:27:59,320 --> 00:28:04,640
And whatever they learn whatever they whatever behavior they exhibit they just learned

337
00:28:04,640 --> 00:28:06,560
it from the data they were trained on.

338
00:28:06,560 --> 00:28:11,240
So somehow these models are the mirror of what we are.

339
00:28:11,240 --> 00:28:17,320
So they they just tell us something about the process we use to collect the data to

340
00:28:17,320 --> 00:28:23,160
train the model and sometimes they even tell us something about those who collected that

341
00:28:23,160 --> 00:28:26,000
data and those who built the model.

342
00:28:26,000 --> 00:28:31,640
So so I will take this occasion to emphasize on something that is very important regarding

343
00:28:31,640 --> 00:28:35,880
diversity in in our community.

344
00:28:35,880 --> 00:28:42,920
It's actually critical and not just because it's fashionable or it's because it sounds cool.

345
00:28:42,920 --> 00:28:49,080
It's critical that as a community we become more open and more diverse because the models

346
00:28:49,080 --> 00:28:56,440
that we build and the data sets these models learn from if we want to have a broad impact

347
00:28:56,440 --> 00:29:03,440
at the global level worldwide and build technology that is representative of the human

348
00:29:03,440 --> 00:29:09,600
beings on this planet and not just a specific population which tends to be westerns and

349
00:29:09,600 --> 00:29:16,240
North Americans it's important that we become more open and more diverse so that everybody

350
00:29:16,240 --> 00:29:25,040
has the tools and and the the techniques to build the technology to solve its own problems.

351
00:29:25,040 --> 00:29:32,400
Well everyone has the tools and techniques but also those organizations that are building

352
00:29:32,400 --> 00:29:38,280
you know these types of systems you know have a more natural inclination to structure

353
00:29:38,280 --> 00:29:44,040
their data sets for example so that they're more robust.

354
00:29:44,040 --> 00:29:50,760
Actually I'm not even sure that everybody has the tools and techniques because let's

355
00:29:50,760 --> 00:29:55,280
say let's consider the machine learning community for example.

356
00:29:55,280 --> 00:30:01,080
So this community every year organizes conferences which the tool of the most important ones being

357
00:30:01,080 --> 00:30:09,720
Nips and ICML and they are basically every year organized in a western country either in

358
00:30:09,720 --> 00:30:16,200
Europe or in the US with some exceptions ICML has been organized previously in China for

359
00:30:16,200 --> 00:30:23,640
example and last year in Australia which I consider being a western country as well.

360
00:30:23,640 --> 00:30:30,360
So it is very difficult for people from other parts of the world to get into the community

361
00:30:30,360 --> 00:30:38,280
just because the places where these gatherings happen are not easily accessible to them

362
00:30:38,280 --> 00:30:44,760
and that's one thing right that's one thing and when you are when you are European or

363
00:30:44,760 --> 00:30:48,280
American you may not see these things because you have the right passport that allows you

364
00:30:48,280 --> 00:30:53,480
to go everywhere but when you are not then then you can see this you know I just give you

365
00:30:53,480 --> 00:31:00,280
an example this year I had two papers accepted for publication at ICML it was in Australia

366
00:31:00,280 --> 00:31:04,840
I could not go there because they did not grant me a visa.

367
00:31:04,840 --> 00:31:09,640
For many people this is they don't even believe it when you say it but it's true and it's

368
00:31:09,640 --> 00:31:19,240
just like the random the normal the routine for many people for many people so that's one

369
00:31:19,240 --> 00:31:25,640
thing and a second thing is that even for those who manage to attend these conferences

370
00:31:25,640 --> 00:31:32,120
if they are not established in a lab that is in one of these countries it can be very difficult

371
00:31:32,120 --> 00:31:39,320
to do some research some you know interesting and important research because most of what we do

372
00:31:39,320 --> 00:31:47,560
right now requires huge amount of computational resources which are not available in many countries

373
00:31:47,560 --> 00:31:57,400
so we need to do something so you know there are various initiatives to make the community more

374
00:31:57,400 --> 00:32:05,800
open and more inclusive some of which I can name are the women in machine learning and also the

375
00:32:05,800 --> 00:32:12,920
black in AI which we organize this year we also have this initiative called Data Science Africa

376
00:32:12,920 --> 00:32:19,080
which we organize every year it's a summer school where we teach machine learning to local

377
00:32:19,080 --> 00:32:25,000
students chat with very popular so far so they're initiatives but I think we need much more

378
00:32:25,880 --> 00:32:31,320
I also throw in a shout out to the deep learning in Dava which happened in South Africa for the

379
00:32:31,320 --> 00:32:37,400
first time last year and they're planning a follow on this year as well yes yes that that also is

380
00:32:37,400 --> 00:32:45,320
an is an interesting initiative that was very helpful in this direction as well interesting so

381
00:32:46,360 --> 00:32:53,800
and I guess in that way all of the you know I think that that that kind of ties together all of

382
00:32:53,800 --> 00:33:00,440
the various things that you work on that may seem like separate and distinct areas of research

383
00:33:00,440 --> 00:33:06,680
they're not really separate they they fit in what I think as a person I should be doing so I

384
00:33:06,680 --> 00:33:12,120
am committed to build an axiological artificial intelligence and by axiological I mean

385
00:33:12,760 --> 00:33:17,880
an artificial intelligence that is aligned with the value of the society in which it operates

386
00:33:19,000 --> 00:33:25,000
and and to be aligned with the value of the society in which you operate means that you were

387
00:33:25,640 --> 00:33:31,960
aware of the biases that you should avoid but also you were aware of the utility that you can

388
00:33:31,960 --> 00:33:38,360
have the problems that you may be you may solve and also the guarantees of level of safety and

389
00:33:38,360 --> 00:33:46,600
security so that's that's the general scope of what gives me busy right now which brings us back

390
00:33:47,400 --> 00:33:53,960
to that previous comment that I kind of put a bookmarker on and that was kind of the distinction

391
00:33:53,960 --> 00:34:07,000
you made between us as the creators of the AI being aware of bias in data sets and factors like

392
00:34:07,000 --> 00:34:14,680
that versus the models themselves being aware of these things and evolving through training

393
00:34:14,680 --> 00:34:20,520
or design or other things to I guess the way you put it was be selective about the data they

394
00:34:20,520 --> 00:34:26,040
consume can you elaborate on on that and the kind of work you're seeing happen there so

395
00:34:26,680 --> 00:34:33,080
so what I said is that you are what you eat and the data the models are trained on right now is

396
00:34:33,080 --> 00:34:43,240
basically junk food so what I meant is that we do not put a lot of effort into our cells first

397
00:34:43,240 --> 00:34:50,040
looking into this data and seeing what the model what should be there and what should not be there

398
00:34:50,040 --> 00:34:56,520
so for example if you collect the data that that serves to train a model which will be used

399
00:34:57,080 --> 00:35:03,000
worldwide then you make sure that that data is representative of the population worldwide

400
00:35:03,000 --> 00:35:08,200
that's that's that's the basic first step to do I get that part I thought you also maybe this is

401
00:35:08,200 --> 00:35:12,920
where you're getting with the second step I thought you were also suggesting that you're also

402
00:35:12,920 --> 00:35:21,080
exploring ways to teach the models yes to know yes that's that's that's where I'm coming so that's

403
00:35:21,080 --> 00:35:27,320
so the first part is the data part itself actually there is the degree zero which which even comes

404
00:35:27,320 --> 00:35:36,760
from before the the data part the zero the zero step step zero is to make sure that the people

405
00:35:36,760 --> 00:35:45,320
who work on the data and on the model are diverse as possible because only that way they will be

406
00:35:45,320 --> 00:35:51,080
able to notice these biases and of course the first step after that is to make sure that the

407
00:35:51,080 --> 00:35:59,480
data itself is representative of the population but then after that which which a more little

408
00:35:59,480 --> 00:36:06,680
bit more technical question is the models should be in doubt with the ability to select what they

409
00:36:06,680 --> 00:36:13,000
should learn from and what they should not learn from and what what kind of features they should be

410
00:36:13,000 --> 00:36:23,800
invariant of and and this is a this is a less explored territory because there is definitely

411
00:36:23,800 --> 00:36:31,960
some work in the area of making the models invariant to gender or invariant to race etc but I think

412
00:36:31,960 --> 00:36:38,440
we should we should in order to design models that are intelligent but that are free from the

413
00:36:38,440 --> 00:36:46,520
biases we should just take inspiration from ourselves you know we do we humans do some do

414
00:36:46,520 --> 00:36:54,280
introspection basically we when we see a book and we read the book we don't believe everything in

415
00:36:54,280 --> 00:37:00,360
the book right we say this makes sense this doesn't make sense we criticize many of the things

416
00:37:00,360 --> 00:37:07,080
and that's also part of learning but right now that learning is completely positively supervised

417
00:37:07,080 --> 00:37:13,400
which means that when you show when you show an example to a model the the label it is associated

418
00:37:13,400 --> 00:37:19,560
with is exactly what you want to learn from so we should we should end out the model to be able

419
00:37:19,560 --> 00:37:25,960
to to be able to criticize the data it is learning from and to select what it should learn from

420
00:37:25,960 --> 00:37:31,400
according to some criteria and what it should not learn from according to other criteria and

421
00:37:31,400 --> 00:37:37,400
and to me this is an excited an exciting direction research direction which which has not been

422
00:37:37,400 --> 00:37:44,840
much explored I have to say but but it's something that we should work more on are there any examples

423
00:37:44,840 --> 00:37:51,640
of of this even very simple ones that have been explored in the research so recently I published

424
00:37:51,640 --> 00:37:57,800
a paper with a colleague of mine where we try to do something like this we try to we try but but

425
00:37:57,800 --> 00:38:07,320
for a model that is already learned we we try to design a method that allows an operator to take

426
00:38:07,320 --> 00:38:14,840
the model and apply the algorithm to in order to in order to to exhibit the biases of that

427
00:38:14,840 --> 00:38:21,160
that the devices that have been learned by the model but then one could one could imagine that

428
00:38:21,160 --> 00:38:28,280
this algorithm is applied recursively to the model while the model is being trained and that's

429
00:38:28,280 --> 00:38:33,800
where it gets interesting it's like again type of training something like that something like that

430
00:38:33,800 --> 00:38:42,040
not not not necessarily again because in this way is I don't see it really as being invariant

431
00:38:42,040 --> 00:38:51,320
something but but but really as being able to criticize itself and also to to decide which data

432
00:38:51,320 --> 00:38:56,120
it should learn from and which which one it should not learn from so more in an active learning

433
00:38:56,120 --> 00:39:03,160
fashion but it's a bit more complicated than that so so it's it's it's it's an interesting

434
00:39:03,720 --> 00:39:07,960
and challenging research direction I'm very excited about what were the results of that

435
00:39:07,960 --> 00:39:13,160
that paper how far did you get with it so what we thought is some of the results that I mentioned

436
00:39:13,160 --> 00:39:18,440
before so we thought that if you take a state of your computer vision model image classification

437
00:39:18,440 --> 00:39:25,000
model and train it on a most standard data you will see stuff appearing that are not expected

438
00:39:25,000 --> 00:39:29,480
for example you will see that the model considers if you basically if you show it a picture of

439
00:39:29,480 --> 00:39:34,920
Barack Obama it would classify as basketball and we have we have real examples like that in the

440
00:39:34,920 --> 00:39:39,400
paper and if you show it a picture of the president of China it would classify it as ping pong

441
00:39:40,200 --> 00:39:47,240
and and and this can be shocking but it's just it's just some biases that are present in the data

442
00:39:47,240 --> 00:39:52,840
in fact there is something very interesting that we show that actually we found out which is

443
00:39:53,560 --> 00:40:00,120
we we thought that this was due to the imbalance in the in the class basketball for example

444
00:40:00,120 --> 00:40:05,960
which we thought that the class basketball ball only contained pictures with mostly black people

445
00:40:05,960 --> 00:40:10,840
but when we I've assumed this whole time that that's what you're uh that that was what you were

446
00:40:10,840 --> 00:40:17,000
saying actually that's not what happened so yeah so we we we we we we computed the statistics and

447
00:40:17,000 --> 00:40:22,920
what we found out is that the class basketball basically contains as many pictures with white

448
00:40:22,920 --> 00:40:27,400
people as pictures with black people so if you take a picture the probability that there is white

449
00:40:27,400 --> 00:40:33,960
people is roughly the same as a as a as a probability that you see a black person in it but what

450
00:40:33,960 --> 00:40:39,880
happened actually is that if you maybe black people are much you know less much more underrepresented

451
00:40:39,880 --> 00:40:44,520
in your data center so they're more uniquely associated absolutely absolutely so it's this is

452
00:40:44,520 --> 00:40:49,080
it's a mutual information problem so if you look at the if you look at just the first order

453
00:40:50,600 --> 00:40:55,240
observations then you may not really see what's happening but if you look at the problem more

454
00:40:55,240 --> 00:41:00,840
broadly then you see that it's exactly what you what you what you said so that uh even in the

455
00:41:00,840 --> 00:41:06,840
data set black people are more uniquely associated with the class basketball which is a bit problematic

456
00:41:06,840 --> 00:41:12,200
because uh again it say something about how the data set was collected and I'm sure that the

457
00:41:12,200 --> 00:41:18,120
people who collected the data set they did put at the time a lot of care to make uh you know

458
00:41:18,120 --> 00:41:24,760
everything uh as correct as possible but still you can see the type of uh biases that can

459
00:41:24,760 --> 00:41:32,840
arise uh from the data set yeah and so so we're talking about algorithms that can learn to identify

460
00:41:32,840 --> 00:41:39,160
these uh these issues and you mentioned the ping pong and basketball examples in that research

461
00:41:39,160 --> 00:41:45,640
did your algorithm you know were these kind of handpicked examples or did your supervisory

462
00:41:45,640 --> 00:41:50,840
algorithm find these examples within the first model yeah it's it's it's really not handpicked

463
00:41:50,840 --> 00:41:55,000
actually I gave them I gave these examples because it's racial biases and people know what it means

464
00:41:55,880 --> 00:42:01,160
but uh but we found other things for example when for the class traffic light we found out that the

465
00:42:01,160 --> 00:42:08,120
model whenever there is in an image you a blue sky with a bar you know some some black bar in it

466
00:42:08,120 --> 00:42:12,920
the the model will always predict traffic light just because doesn't matter whether there is

467
00:42:12,920 --> 00:42:17,720
traffic light or not it just because in most pictures with traffic light you see the this you

468
00:42:17,720 --> 00:42:24,680
know this middle this iron bar with in a blue sky or something so it's it's really you know

469
00:42:24,680 --> 00:42:29,640
it's really a lot of different biases but I just I just give these two as examples because

470
00:42:29,640 --> 00:42:37,480
people can relate with it and know exactly what that means can you envision like or or at least

471
00:42:37,480 --> 00:42:44,760
I'm envisioning as you're describing this maybe um some kind of meta annotation process where

472
00:42:44,760 --> 00:42:50,600
you know you you you initially annotate your data but then you run and you train a model and then

473
00:42:50,600 --> 00:42:55,560
you kind of apply this algorithm to the model and it shows you all of these biases and then you

474
00:42:55,560 --> 00:43:00,920
have this meta annotation step where folks are identifying whether these biases are valid or

475
00:43:00,920 --> 00:43:05,720
something like that and using that to retrain your model do you envision something like that actually

476
00:43:05,720 --> 00:43:10,040
I will take it one step further I will I will remove even the human in the second loop

477
00:43:10,040 --> 00:43:16,040
if things are done if things are done properly ideally what I would like to see is that a model

478
00:43:16,040 --> 00:43:22,920
learn from the data in an initial step and itself criticizes itself and says I should not have

479
00:43:22,920 --> 00:43:29,880
learned this and that and then goes in a second round of learning and in a be more careful

480
00:43:30,440 --> 00:43:38,360
what it learns from which data point etc and iteratively produces a final model that is at least

481
00:43:38,360 --> 00:43:46,440
um less biased because you never can have a model that is completely free from all biases that's

482
00:43:46,440 --> 00:43:53,640
just not possible um but you can mitigate it and and in my opinion that's something that we

483
00:43:53,640 --> 00:43:59,880
definitely can do this question maybe getting further ahead of you know where you are with this

484
00:43:59,880 --> 00:44:04,680
research but you know given and given this exact any of these examples really but this example of

485
00:44:04,680 --> 00:44:13,880
the blue sky and the bar and the the traffic light the way we train these examples or these models

486
00:44:13,880 --> 00:44:21,000
rather uh you know what what do you envision the mechanism being inside that training loop

487
00:44:21,000 --> 00:44:27,720
that allows the model to even know you know what the thing is that it's supposed to be

488
00:44:27,720 --> 00:44:35,880
uh paying attention to so there are two things here the first is the the learning rule that we're

489
00:44:35,880 --> 00:44:41,000
using so currently most of these models are trained in a completely supervised way so we say hey

490
00:44:41,000 --> 00:44:47,160
this is an image of a traffic light and then the model kind of learn to figure out these things

491
00:44:47,160 --> 00:44:55,400
uh so I think that we should reduce the supervision that's that's one one thing and reducing the

492
00:44:55,400 --> 00:45:02,440
supervision having a learning algorithm that needs less supervision will make definitely the

493
00:45:02,440 --> 00:45:07,960
algorithm more robust because it will have access to much more data which is unsupervised and

494
00:45:07,960 --> 00:45:15,000
it will the labeling process itself is very noisy in general so the the model may be exposed to

495
00:45:15,000 --> 00:45:23,960
less noise and it it may less overfit so if the model is able to to leverage more of the unsupervised

496
00:45:23,960 --> 00:45:29,960
data it it may end up less overfitting so that's one thing the second thing is that

497
00:45:30,600 --> 00:45:35,720
the objective functions that the model is optimizing should be designed in a way

498
00:45:36,840 --> 00:45:44,760
that makes the model avoid learning some biases and that's that's where the that's also one

499
00:45:44,760 --> 00:45:51,160
change that needs to happen in the way we supervise the model so so so I just give you an example so

500
00:45:51,160 --> 00:45:57,240
right now the way we trained if you take a any state of the other image that model it was trained

501
00:45:58,360 --> 00:46:06,760
in examples that have pictures and label and the and the model has no idea how the labels are

502
00:46:06,760 --> 00:46:13,640
related to each other it it ended up somehow figuring it out and I should not even say figuring it

503
00:46:13,640 --> 00:46:20,040
out but it ended up you know producing representations yes representations where cats are

504
00:46:20,040 --> 00:46:25,800
close to dogs etc but it's not really the case so your model can make completely stupid mistakes

505
00:46:26,520 --> 00:46:32,920
so if a cat is misclassified as a dog it's it's a wrong decision but it's still fine because

506
00:46:32,920 --> 00:46:38,520
semantically we know that they are closer to each other than a cat being misclassified as a plane

507
00:46:38,520 --> 00:46:47,880
for example and and you know so far we have not managed to train the models to to really take

508
00:46:47,880 --> 00:46:53,320
into account this decision in order to improve their accuracy and the type of errors they take

509
00:46:53,320 --> 00:46:59,960
so there are a lot of attempts in the literature but in my opinion has not been very convincing

510
00:46:59,960 --> 00:47:08,120
so there is a lot of things to explore in these directions yeah it's it sounds like

511
00:47:09,080 --> 00:47:17,000
there's definitely a lot to explore and furthermore these you know the kind of the direction

512
00:47:17,000 --> 00:47:22,760
that you're proposing is a fairly as dramatic too strong you know a dramatic shift from kind

513
00:47:22,760 --> 00:47:30,760
of the way we train these models today well at some point we just need to to change paradigms and

514
00:47:30,760 --> 00:47:37,320
that's that's how that's how research works I'm not claiming that what I'm saying is the right

515
00:47:37,320 --> 00:47:44,040
thing to do I mean it's probably not but I feel that I observed that we're reaching the limits

516
00:47:44,040 --> 00:47:49,800
of most of the the learning algorithms and the models that we're currently using can offer

517
00:47:50,360 --> 00:47:56,680
and probably what we take us to the next level is not in the box in which we're thinking right now

518
00:47:56,680 --> 00:48:05,400
so we should probably take a step back and and look at the the very principles of the learning

519
00:48:05,400 --> 00:48:11,080
algorithms that we're using and think them in a way that can take us to the next level

520
00:48:11,080 --> 00:48:17,720
awesome awesome well I think that sounds like a good place to wrap up Mustafa thank you so much

521
00:48:17,720 --> 00:48:24,920
for taking the time to chat with us any final words that you'd like to add so thank you very much

522
00:48:24,920 --> 00:48:29,640
for inviting me it has been a pleasure to discuss with you I just would like to mention a last

523
00:48:29,640 --> 00:48:37,800
thing which is there is a lot of research currently in the area of bias and fairness and and

524
00:48:37,800 --> 00:48:45,400
and all of these very interesting and fascinating topics but there is also one thing that in my

525
00:48:45,400 --> 00:48:51,160
opinion is a bit overlooked in the community which is fairness and bias is not only in the models

526
00:48:51,160 --> 00:48:57,400
that we design or the datasets the models learn from it starts with the problems that we consider

527
00:48:58,680 --> 00:49:05,160
if we start by considering problems that are important for a specific population in the world

528
00:49:05,160 --> 00:49:10,760
and just focus on solving these type of problems the end resource is the datasets that we'll be

529
00:49:10,760 --> 00:49:16,440
using and the models that will be that will be learning from these datasets will obviously be biased

530
00:49:17,320 --> 00:49:25,160
so I think as a community we should also open ourselves to consider problems that are of interest

531
00:49:25,160 --> 00:49:34,360
at the global level and I will just give you an example to conclude where there is definitely

532
00:49:35,400 --> 00:49:41,880
it's definitely an exciting direction all the work happening in self-driving cars etc

533
00:49:43,080 --> 00:49:49,160
and a lot of resources is being poured into this and it's important for the for humanity in general

534
00:49:49,160 --> 00:49:55,960
I think but right now this is important for a very very tiny percentage of the population of

535
00:49:55,960 --> 00:50:02,920
this planet and there are challenging challenges out there where as a community we could have a huge

536
00:50:02,920 --> 00:50:09,000
impact but we are still overlooking and we should open ourselves to that and that I just would like

537
00:50:09,000 --> 00:50:15,400
to conclude by mentioning this but thank you very much for inviting me to this podcast I was

538
00:50:15,400 --> 00:50:24,440
very happy to discuss with you absolutely thank you so much thank you all right everyone that's

539
00:50:24,440 --> 00:50:31,560
our show for today remember we want to hear from you on AI in the home and in our personal lives

540
00:50:31,560 --> 00:50:38,440
head on over to twimmalei.com slash my AI to share your thoughts for more information on Mustafa

541
00:50:38,440 --> 00:50:45,000
or any of the topics covered in this episode head on over to twimmalei.com slash talk slash 108

542
00:50:45,000 --> 00:51:10,280
thanks for listening and catch you next time

