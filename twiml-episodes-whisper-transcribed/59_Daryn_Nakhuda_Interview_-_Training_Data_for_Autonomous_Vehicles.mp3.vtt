WEBVTT

00:00.000 --> 00:15.920
Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting

00:15.920 --> 00:20.880
people doing interesting things in machine learning and artificial intelligence.

00:20.880 --> 00:23.240
I'm your host Sam Charrington.

00:23.240 --> 00:27.040
Well team, I guess I'll just jump straight to the bad news.

00:27.040 --> 00:31.080
This week we shared with excitement the news about a special Halloween event that we were

00:31.080 --> 00:34.480
planning for October 30th in New York City.

00:34.480 --> 00:39.920
Well, due to unforeseen events beyond our control, the event is now canceled.

00:39.920 --> 00:45.720
We were really really looking forward to it and are incredibly disappointed about its cancellation.

00:45.720 --> 00:49.960
If you purchase tickets via either the event bright or splash that pages, you should

00:49.960 --> 00:52.280
have been automatically refunded.

00:52.280 --> 00:56.520
The good news though, is that you can still connect with us on Monday evening because

00:56.520 --> 01:01.400
those will be headed to the NYU Future Labs AI Summit Happy Hour.

01:01.400 --> 01:06.040
If you're in New York City, we hope you'll join us at the happy hour and more importantly

01:06.040 --> 01:08.040
the AI Summit itself.

01:08.040 --> 01:13.280
As you may remember, we attended the inaugural Summit back in April and had a great time

01:13.280 --> 01:17.360
and delivered some great interviews, all of which will link to in the show notes for

01:17.360 --> 01:19.200
your listening pleasure.

01:19.200 --> 01:24.400
This year's event features more great speakers, including Karina Cortez, head of research

01:24.400 --> 01:30.520
at Google New York, David Venturelli, science operations manager at NASA Ames Quantum

01:30.520 --> 01:37.360
AI Lab, and Dennis Mortensen, CEO and founder of StartupX.ai.

01:37.360 --> 01:47.000
For the event homepage, visit aiSummit2017.futurelabs.nyc, and for 25% off tickets, use the

01:47.000 --> 01:49.800
code Twimmel25.

01:49.800 --> 01:55.520
You can find links to this and more great events on our new events page at twimmelai.events,

01:55.520 --> 02:02.720
and of course, this shows notes page at twimmelai.com slash talk slash 57.

02:02.720 --> 02:08.600
The episode you're about to hear is the first of a new series of shows on autonomous vehicles.

02:08.600 --> 02:13.280
Now we all know that self-driving cars is one of the hottest topics in machine learning

02:13.280 --> 02:18.280
in AI, so of course, we had to dig a little deeper into the space.

02:18.280 --> 02:22.760
And to get us started on this journey, I'm excited to present this interview with Darren

02:22.760 --> 02:26.840
Nakuda, CEO and co-founder of Mighty AI.

02:26.840 --> 02:30.560
Darren and I discussed the many challenges of collecting training data for autonomous

02:30.560 --> 02:36.640
vehicles, along with some thoughts on human-powered insights and annotation, semantic segmentation,

02:36.640 --> 02:39.040
and a ton more great stuff.

02:39.040 --> 02:44.160
You may not realize it, but if you're a long-time listener, you already know Mighty AI from

02:44.160 --> 02:50.840
my interview with their lead data scientist Angie Hugeback for twimmeltalk number six.

02:50.840 --> 02:55.000
It is so hard to believe that that was over 50 shows ago.

02:55.000 --> 02:59.160
Mighty AI was one of the first sponsors of this podcast, and it's great to have them

02:59.160 --> 03:01.680
back as a sponsor for this series.

03:01.680 --> 03:06.360
As you'll hear, the company delivers training and validation data to firms building computer

03:06.360 --> 03:09.000
vision models for autonomous vehicles.

03:09.000 --> 03:13.400
Their platform combines guaranteed accuracy with scale and expertise.

03:13.400 --> 03:18.720
Based to their full stack of annotation software, consulting and managed services, proprietary

03:18.720 --> 03:23.480
machine learning, and global community of pre-qualified annotators.

03:23.480 --> 03:31.760
We thank Mighty AI for being a valued sponsor, so please be sure to visit them at www.mty.ai

03:31.760 --> 03:46.880
to learn more and follow them on Twitter at www.mty.ai.

03:46.880 --> 03:52.680
Alright everyone, I am on the line with Darren Nikuda, the CEO of Mighty AI.

03:52.680 --> 03:58.640
Mighty AI is a company that you've heard from on the podcast before, but in fact they

03:58.640 --> 04:06.480
were formerly called spare five, and we interviewed one of their lead data scientists, Angie Hugeback

04:06.480 --> 04:10.720
back on twimmeltalk number six, just about a year ago.

04:10.720 --> 04:12.400
Darren, welcome to the show.

04:12.400 --> 04:13.400
Thanks Sam.

04:13.400 --> 04:15.120
It's great to have you on.

04:15.120 --> 04:19.480
Why don't we start by having you introduce yourself and talk a little bit about your

04:19.480 --> 04:21.120
role at Mighty AI?

04:21.120 --> 04:26.920
Sure, so my name is Darren Nikuda, I'm the CEO and one of the founders of Mighty AI.

04:26.920 --> 04:30.880
So we've been working at Mighty AI, as you said formerly known as spare five for about

04:30.880 --> 04:36.360
three years, and really trying to harness human insights and human power into building

04:36.360 --> 04:39.960
better training data sets for artificial intelligence.

04:39.960 --> 04:42.240
And tell us a little bit about your background.

04:42.240 --> 04:48.000
Sure, so my background has been in software engineering for about 20 years, mainly in

04:48.000 --> 04:53.960
internet technology, so everything from e-commerce and communications platforms through marketplaces

04:53.960 --> 04:57.720
and yeah, okay.

04:57.720 --> 05:04.040
And is Mighty AI the your kind of first for and to the AI space or have you been doing

05:04.040 --> 05:05.440
that for a while?

05:05.440 --> 05:11.040
So Mighty AI is really a first for in the AI, but really using human insights has been

05:11.040 --> 05:12.280
something I've been doing for a while.

05:12.280 --> 05:17.040
So at both startups as well as when I worked in Amazon, I leveraged mechanical Turk and

05:17.040 --> 05:21.640
other platforms to use humans to augment what we could do with our systems.

05:21.640 --> 05:24.200
Okay, awesome.

05:24.200 --> 05:31.400
So since the conversation with Angie, again, just under a year ago, it sounds like you

05:31.400 --> 05:36.480
guys have gotten a lot more focused and in particular, you're spending a lot of time

05:36.480 --> 05:39.400
in the autonomous vehicle space.

05:39.400 --> 05:42.480
Can you tell us a little bit about what you're up to there?

05:42.480 --> 05:43.480
Sure.

05:43.480 --> 05:48.720
So when we started a few years ago, we were really focused on human powered insights for

05:48.720 --> 05:50.240
almost anything.

05:50.240 --> 05:54.960
And what we realized was what really set us apart was our focus on quality.

05:54.960 --> 05:59.240
So like you talked about with Angie, you know, a year ago, really building our own models

05:59.240 --> 06:05.000
for user reputation and data, data quality predictions was key to our success.

06:05.000 --> 06:09.480
And really that was resonating with customers who were focusing on building training data,

06:09.480 --> 06:13.360
for building, you know, models where they need a really highly accurate data.

06:13.360 --> 06:18.160
And that boiled down to natural language and computer vision and really where we saw

06:18.160 --> 06:22.320
a lot of focus was on the computer vision side, specifically in autonomous driving, which

06:22.320 --> 06:25.440
is, you know, a huge field as you've, as you've seen.

06:25.440 --> 06:30.840
And we had a lot of a lot of demand there for really specialized, really highly accurate

06:30.840 --> 06:31.840
data.

06:31.840 --> 06:34.680
So we decided to focus purely on that area.

06:34.680 --> 06:40.080
And I've mentioned the conversation with Angie and you've mentioned human powered insights

06:40.080 --> 06:41.080
a couple of times.

06:41.080 --> 06:46.880
And I think I may be taking for granted that folks will have heard that podcast, but I

06:46.880 --> 06:48.280
probably shouldn't do that.

06:48.280 --> 06:53.800
So why don't you take a second to kind of step back and really walk through what you

06:53.800 --> 06:58.360
guys do so that we can, you know, make sure everyone's on the same page on that?

06:58.360 --> 06:59.360
Sure.

06:59.360 --> 07:05.240
So what we have a platform called Spare 5, which is basically a community of people around

07:05.240 --> 07:10.200
the world who we give a small microtask to and they are able to perform those.

07:10.200 --> 07:14.880
We have a quality control system in which that we can review and manage that both automatically

07:14.880 --> 07:16.640
and with other people.

07:16.640 --> 07:20.400
And what we deliver to our customers is a high quality result.

07:20.400 --> 07:22.520
So they'll come to us with a requirement.

07:22.520 --> 07:27.400
For example, a photograph and some requirements as far as what types of things in the photo

07:27.400 --> 07:28.400
need to be labeled.

07:28.400 --> 07:32.720
In the case of autonomous driving, that might be drawing bounding boxes around pedestrians

07:32.720 --> 07:37.040
and vehicles on the road, or it could be something like segmenting every pixel of the

07:37.040 --> 07:40.000
image into semantic class.

07:40.000 --> 07:45.400
And we'll build a workflow and we'll go through that and have humans do that and then using

07:45.400 --> 07:49.680
the combination of the humans and our AI deliver back a result to them that they can then

07:49.680 --> 07:51.480
use to build their own models.

07:51.480 --> 07:52.480
Okay.

07:52.480 --> 07:59.440
And you were previously doing this for folks that operated in a variety of market segments,

07:59.440 --> 08:06.320
but you've again focused more tightly on autonomous vehicles for some time now.

08:06.320 --> 08:12.400
Can you talk a little bit about some of what makes that market unique for what you're doing?

08:12.400 --> 08:18.880
Sure. I think autonomous vehicles, especially on the computer vision side, is really a great

08:18.880 --> 08:23.720
example of what needs to happen in order to build highly accurate models.

08:23.720 --> 08:27.960
In a lot of the other use cases that we dealt with in the past, there was a lot of flexibility

08:27.960 --> 08:32.960
or more subjective insights as to taste, like in retail or something like that, where you're

08:32.960 --> 08:38.480
much more focused on things that are not life and death and safety related.

08:38.480 --> 08:43.560
And with the vehicles, it really is about getting as much data as possible, with a lot

08:43.560 --> 08:48.600
of diversity as possible in getting it labeled in an accurate way in which we can feel comfortable

08:48.600 --> 08:54.240
that we can take this model, train a system, integrate all the sensors in the controls

08:54.240 --> 08:59.560
and put a car on the road and have it drive with humans and other cars right next to it.

08:59.560 --> 09:06.600
There are a number of different perspectives on the right way to do autonomous vehicles

09:06.600 --> 09:11.520
in terms of the different types of sensors.

09:11.520 --> 09:18.760
There seems to be one world view that's very heavily computer vision focused and looks

09:18.760 --> 09:24.480
at the camera as the ultimate end all be all sensor and there seems to be another point

09:24.480 --> 09:30.640
of view that's a little bit more integrative and includes LIDAR and other types of sensors.

09:30.640 --> 09:32.920
Do you guys have any perspective on that?

09:32.920 --> 09:38.760
Yeah, so most of the work we're doing is on the image side, camera side, but really our

09:38.760 --> 09:42.800
perspective is that in order to have a car drive like a human it needs to have the sensors

09:42.800 --> 09:43.800
of a human, right?

09:43.800 --> 09:50.320
So there's more than just your eyes, so that's where other sensors come in and maybe humans

09:50.320 --> 09:54.000
don't have a built-in LIDAR system, but we do have a sensor surrounding, right?

09:54.000 --> 09:59.960
So it's not just our eyes, but it's you sound so when you think about radar or ultrasonic

09:59.960 --> 10:06.720
other contexts that's more 360 than just a front camera or a back camera or yeah.

10:06.720 --> 10:10.560
Yeah, I rely pretty heavily on my spidey sense, which is about as close to LIDAR as I'm

10:10.560 --> 10:11.560
going to get.

10:11.560 --> 10:14.400
Sure, I mean, there's things that you pick up as you're driving, you know, like you

10:14.400 --> 10:18.720
see a person way down the road on the sidewalk and you're going to be thinking about will

10:18.720 --> 10:19.720
they cross or not?

10:19.720 --> 10:22.680
Maybe that is, you know, a camera seeing that, but also the intent of which way are they

10:22.680 --> 10:23.680
moving?

10:23.680 --> 10:24.680
What are they doing?

10:24.680 --> 10:26.040
Like in just what is your experience?

10:26.040 --> 10:29.560
Like in downtown Seattle, people usually stop at the crosswalk.

10:29.560 --> 10:31.800
Not really the case, the rest of the world.

10:31.800 --> 10:32.800
Right, right.

10:32.800 --> 10:40.640
Can you talk a little bit about where the service that you are providing fits into kind

10:40.640 --> 10:46.320
of the broader pipeline that your customers are deploying?

10:46.320 --> 10:47.320
Sure.

10:47.320 --> 10:52.440
And maybe as a prelude to that, you can talk a little bit about the customers that you

10:52.440 --> 10:56.400
target and any customers that you can name and kind of what they're working on.

10:56.400 --> 11:01.320
Yeah, so we work with a variety of customers, really everybody you could picture in the

11:01.320 --> 11:02.320
automotive space.

11:02.320 --> 11:07.080
So that could be the OEMs, which are the car manufacturers, the tier one suppliers who

11:07.080 --> 11:10.560
are the people who traditionally have provided parts, but are also now providing integrated

11:10.560 --> 11:11.560
systems.

11:11.560 --> 11:13.760
And then, you know, what we'll call disruptors.

11:13.760 --> 11:18.320
So companies like Uber that, you know, are using autonomous driving, maybe not as their

11:18.320 --> 11:23.560
core business, but as part of their broader offering and then startups who are purely focused

11:23.560 --> 11:27.360
on, we've never been in the automotive space before, maybe we have some individual experience

11:27.360 --> 11:31.120
but now we're going to go straight after kind of full autonomy.

11:31.120 --> 11:34.720
And so that's, you know, a wide range of customers.

11:34.720 --> 11:38.200
Most of them are starting out with a car on the road.

11:38.200 --> 11:40.880
So I cooked with whatever sensors they have.

11:40.880 --> 11:44.960
And then they're taking that data and the requirements coming from their research team,

11:44.960 --> 11:49.440
which might be object detection or might be, you know, semantic segmentation, it could

11:49.440 --> 11:56.240
be a combination of them, and then they give us their raw data, which is video or still

11:56.240 --> 11:58.800
extracted still frames in the requirements.

11:58.800 --> 12:03.280
And then we have to develop a workflow in order to give them back label data.

12:03.280 --> 12:04.520
Okay.

12:04.520 --> 12:10.360
And so object detection, that sounds pretty obvious in terms of what that means on face

12:10.360 --> 12:17.440
value, but are there nuances that are part of the process there that folks don't generally

12:17.440 --> 12:19.840
think of when they hear the phrase object detection?

12:19.840 --> 12:20.840
Oh, absolutely.

12:20.840 --> 12:27.080
I think more so in autonomous vehicles than in other spaces where it's not just about,

12:27.080 --> 12:31.200
you know, what shape is this thing so I can decide whether it's a car or a truck, but

12:31.200 --> 12:36.080
even when you see a truck, you might have, you know, four jeeps.

12:36.080 --> 12:41.760
One is a male delivery van, one's an ice cream delivery truck, one's a passenger vehicle.

12:41.760 --> 12:46.600
And those nuances actually become really important when you think about driving patterns.

12:46.600 --> 12:50.520
A ice cream truck may have kids running out to it, you know, as it drives down the street,

12:50.520 --> 12:54.800
a male man might be driving on the wrong side of the road and, you know, stopping very

12:54.800 --> 12:56.800
often at mailboxes.

12:56.800 --> 13:03.960
And, you know, who knows what a passenger vehicle might do, right, right, right.

13:03.960 --> 13:08.240
And then you also mentioned scene segmentation, tell us about that.

13:08.240 --> 13:09.240
Sure.

13:09.240 --> 13:13.800
So, you know, another part of thinking about computer vision is not just, you know,

13:13.800 --> 13:17.320
what are the objects in front of you, but really what is your context.

13:17.320 --> 13:22.000
So in the segmentation, what we're doing is labeling really every pixel that's in the

13:22.000 --> 13:28.840
field of view, whether that's a road or marking on the road or a vehicle or pedestrian with

13:28.840 --> 13:32.360
different sorts or vegetation and buildings and curbs.

13:32.360 --> 13:35.760
So really making sure that we have enough information about the different types of things

13:35.760 --> 13:40.120
that you're looking at that you can make better decisions.

13:40.120 --> 13:48.520
And so every individual pixel gets a label and the pixels are labeled essentially as objects

13:48.520 --> 13:55.000
or is there a like a fixed vocabulary that you're labeling the pixels with or is it across

13:55.000 --> 13:58.520
a, you know, a broad spectrum of objects.

13:58.520 --> 14:04.040
So the taxonomy of labels changes based on our customer requirements, but you can think

14:04.040 --> 14:09.360
of them in broader terms kind of as classes or as types of things.

14:09.360 --> 14:15.440
So it's not necessarily an object, but something like the sky or vegetation or an individual

14:15.440 --> 14:16.440
car.

14:16.440 --> 14:20.440
And usually from that, what we're doing is labeling everything we can see and then there's

14:20.440 --> 14:22.120
additional labeling steps afterwards.

14:22.120 --> 14:26.800
So we might break down vehicles into like I was describing earlier, various specific types

14:26.800 --> 14:27.800
of vehicles.

14:27.800 --> 14:32.120
And a lot of this also changes based on, you know, the location of the footage because

14:32.120 --> 14:36.840
terminology can change, the types of road markings can change based on what part of the world

14:36.840 --> 14:37.840
you're in.

14:37.840 --> 14:39.600
Hmm.

14:39.600 --> 14:43.960
Can you take us a little deeper into how you do all this?

14:43.960 --> 14:49.600
And in particular, I'm really interested in hearing how that's evolved from when you

14:49.600 --> 14:56.360
were tackling the problem more broadly to doing this specifically for the autonomous vehicle

14:56.360 --> 14:57.360
market.

14:57.360 --> 14:58.360
Sure.

14:58.360 --> 15:02.840
So probably the best example I can walk you through is on that semantic segmentation

15:02.840 --> 15:04.720
that we just spoke about because it's really hard.

15:04.720 --> 15:08.400
I mean, just thinking about the amount of time it would take to figure out how to label

15:08.400 --> 15:11.960
every pixel in an image of regards to tools.

15:11.960 --> 15:15.960
And so when we first started out, you know, especially with the spare five app, we were

15:15.960 --> 15:18.120
a mobile mobile only app.

15:18.120 --> 15:22.160
And what we've done is we we solve that platform, but we've also developed a desktop client

15:22.160 --> 15:25.320
or a web web based desktop view.

15:25.320 --> 15:27.240
And really that was about preference.

15:27.240 --> 15:30.720
So some people really like working on the tablet with their fingers or with the stylist,

15:30.720 --> 15:32.760
other people really like using the large screen.

15:32.760 --> 15:36.560
And either way, you need to be able to zoom in and really get within a couple of pixels

15:36.560 --> 15:40.360
of an edge when you're doing this type of drawing.

15:40.360 --> 15:42.600
And you know, work flow wise, we learned a lot.

15:42.600 --> 15:47.200
When we first started doing this, we said, okay, we've got a list of 75 classes of things

15:47.200 --> 15:48.880
that are in an image.

15:48.880 --> 15:51.720
And we built a couple tools to help you draw polygons.

15:51.720 --> 15:56.120
So you could go click around a shape and make a closed polygon and say this belongs to

15:56.120 --> 15:58.440
the sky or a car.

15:58.440 --> 16:04.440
And what we found was one, it was really hard to instruct humans on 75 different things

16:04.440 --> 16:05.440
at once.

16:05.440 --> 16:09.040
So even if we gave them a long instructional set and quiz them, it's a lot to keep in

16:09.040 --> 16:10.040
your mind.

16:10.040 --> 16:12.080
Two, it takes a long time.

16:12.080 --> 16:17.200
So drawing and labeling any one of these images from start to finish might take an hour

16:17.200 --> 16:18.200
of your time.

16:18.200 --> 16:22.680
And three, if you made mistakes, it was really hard to figure out where the mistake was,

16:22.680 --> 16:26.360
how to pick up on it, how to have somebody else come in and fix it or how to have you,

16:26.360 --> 16:27.360
you fix it.

16:27.360 --> 16:31.080
So we're not just throwing out an hour of effort.

16:31.080 --> 16:35.880
So we iterated on that process several times as far as an overall workflow.

16:35.880 --> 16:41.560
And our first pass of that was, well, instead of doing 75 classes at once, we'll do one.

16:41.560 --> 16:47.720
So let's have people focus purely on pedestrians or purely on vehicles.

16:47.720 --> 16:52.600
And that helped a lot because it made them really focus on the instructions as far as

16:52.600 --> 16:57.600
where to draw the lines, what accounts like do you go around the tires, do you go around

16:57.600 --> 17:01.040
the bumpers, how tight do you have to be.

17:01.040 --> 17:05.480
But it still is very time consuming, especially if you think about something like highway,

17:05.480 --> 17:10.120
highway scene in the middle of rush hour, where there's 50 cars within the field of

17:10.120 --> 17:11.120
view.

17:11.120 --> 17:13.320
And some of them really fire out on the horizon.

17:13.320 --> 17:19.040
And so when you switched to that, that first iterative step, did you go from a model

17:19.040 --> 17:25.680
where you would have like one worker work on all of the various things in an image to

17:25.680 --> 17:32.640
one where the image would kind of pass through steps and be routed to like the pedestrian

17:32.640 --> 17:36.480
team and the tree team and the vehicle team, that kind of thing.

17:36.480 --> 17:37.480
Yeah, that's right.

17:37.480 --> 17:39.720
And I think it's a little foreshadowing of our next step.

17:39.720 --> 17:47.000
But yeah, so what we had was for one person, one image, one person doing one class, so 75

17:47.000 --> 17:50.080
people, roughly for a full image.

17:50.080 --> 17:51.080
Right.

17:51.080 --> 17:55.960
And then the next, what we realized was the time between these different tasks was hard

17:55.960 --> 17:56.960
to predict.

17:56.960 --> 18:01.920
And it was still pretty exhausting to do all every single car in a scene.

18:01.920 --> 18:07.920
So the next iteration was really what we call recursion in our world, but it's basically

18:07.920 --> 18:12.600
we present the image with all the previous activity that's been done to it.

18:12.600 --> 18:18.520
So if there's 30 cars and 25 of them have already been boxed, we'll show you an image with

18:18.520 --> 18:25.400
25 boxed cars and say, are there more cars in the picture that haven't been labeled?

18:25.400 --> 18:30.000
And if they say yes, then we give them the drawing tool and say, draw the shape around

18:30.000 --> 18:31.240
one of the cars.

18:31.240 --> 18:33.400
So just do one at a time.

18:33.400 --> 18:37.440
And so they will outline a car, they'll label it, you know, according to which kind of

18:37.440 --> 18:41.080
class it belongs to and they'll hit next next.

18:41.080 --> 18:45.000
To that point, you know, we've taken something that took an hour or more to do the entire

18:45.000 --> 18:50.520
image to a short, you know, minute or two task to get it right, which allows us both to

18:50.520 --> 18:55.840
let, you know, have that individual unit of work be reviewed, both by our automated systems

18:55.840 --> 18:58.640
as well as by, you know, our reviewers.

18:58.640 --> 19:03.600
And then taking that and aggregating all of those, all the individual cars and all the

19:03.600 --> 19:07.880
individual, you know, different classes into that final composite image, just gives us

19:07.880 --> 19:11.920
a lot more flexibility into actually how quickly things can run because things can run

19:11.920 --> 19:16.800
in parallel as well as, you know, the quality because we have a lot finer green control

19:16.800 --> 19:21.240
as far as what we keep and what we don't keep and even how we, how we edit things.

19:21.240 --> 19:22.240
Hmm.

19:22.240 --> 19:25.640
Now, a couple of questions jump out at me.

19:25.640 --> 19:30.480
The first is have you thought about making this into like a capture?

19:30.480 --> 19:34.480
It seems like the, if you can simplify the UI enough, it seems like the perfect task

19:34.480 --> 19:39.040
to turn into a capture and just let people who are trying to sign into their bank or whatever

19:39.040 --> 19:41.000
do all this work for you.

19:41.000 --> 19:46.200
Yeah, I mean, you know, one thing is there are some really specific instructions as far

19:46.200 --> 19:48.920
as the different types of classes and labels we want.

19:48.920 --> 19:52.920
So in some cases like, you know, once something's been drawn and we just need to have you categorize

19:52.920 --> 19:54.600
it, that would make sense.

19:54.600 --> 19:59.560
But typically there's, you know, enough enough kind of context and we need to train and

19:59.560 --> 20:02.800
instruct people on just for them to really do a good job.

20:02.800 --> 20:08.040
Yeah, and I realized that part of the value proposition that you are bringing to the

20:08.040 --> 20:13.120
table is that you, as opposed to what someone might be able to find with the mechanical

20:13.120 --> 20:18.560
Turk you've got a pool of workers that you've, that you've taught how to do these kind of

20:18.560 --> 20:23.880
classification tasks and so the accuracy is higher and things like that.

20:23.880 --> 20:27.640
And so the, the suggestion is a little bit, you know, tongue in cheek, but it, you know,

20:27.640 --> 20:31.440
I've been getting a lot of these captures recently that are, you know, it'll show you

20:31.440 --> 20:36.840
a scene and it'll say, pick all the squares that have street signs in them and it makes

20:36.840 --> 20:42.880
me wonder if it's, you know, something someone like you folks doing, you know, basically

20:42.880 --> 20:48.120
farming out their object detection to, you know, folks that are trying to sign into websites.

20:48.120 --> 20:49.120
Yeah, certainly.

20:49.120 --> 20:53.960
I'm sure that, you know, recapture, which is owned by Google, that would be very obviously

20:53.960 --> 20:56.960
a use case for them to leverage.

20:56.960 --> 20:57.960
Right.

20:57.960 --> 21:03.600
And, you know, what we found is, like you said, there's a lot of instruction, we have annotators

21:03.600 --> 21:04.600
around the world.

21:04.600 --> 21:08.480
And so we have a community, a really large community, either in 155 countries around the

21:08.480 --> 21:09.480
world.

21:09.480 --> 21:13.440
And really it's us communicating with them and really getting alignment so that when we

21:13.440 --> 21:18.240
have somebody doing these tasks, the same person is going through, you know, the same workflow

21:18.240 --> 21:21.920
multiple times and they really aren't, aren't just a stranger being shown an image and

21:21.920 --> 21:27.520
saying, which one has a street sign, but really given really specific instructions and also

21:27.520 --> 21:33.040
giving them a way to engage with us because inevitably as you have this large data set,

21:33.040 --> 21:36.960
you'll run into, you know, places where the instructions need to be clarified or where

21:36.960 --> 21:43.040
they're confused as to, well, what do I do if there's a car, like partially, including

21:43.040 --> 21:47.400
another car, like how to, which way do I want to draw the boxes, that kind of thing that

21:47.400 --> 21:51.600
really requires us to interact with them a lot more closely.

21:51.600 --> 22:02.720
Okay, so my next question is, you have clearly, or will have clearly accumulated a ton

22:02.720 --> 22:09.920
of, you know, label data sets here is the, is a future step, automate, you know, building

22:09.920 --> 22:18.280
some AI models that automate the, some of this or, you know, for example, predict which

22:18.280 --> 22:25.160
of the pixels are sky or put a bounding box around the sky and ask the humans to correct

22:25.160 --> 22:27.200
as opposed to draw a new.

22:27.200 --> 22:33.920
Yeah, that's absolutely somewhere where we're going to be focusing is how to make the process

22:33.920 --> 22:34.920
more efficient.

22:34.920 --> 22:39.640
So that's both some automation up front as well as assistive tools so that we can make

22:39.640 --> 22:41.560
the community just perform better.

22:41.560 --> 22:46.840
So right now our drawing tools are very manual where you're clicking every point in a

22:46.840 --> 22:51.360
pixel in a polygon so that you can get a really sharp edge.

22:51.360 --> 22:55.080
But there's no reason why with edge detection and other techniques, we can't make that

22:55.080 --> 23:00.480
an easier process for the, for the community members as well as, as you said, if we can

23:00.480 --> 23:04.880
take that process, I was describing that workflow of recursion where we have, you know, for

23:04.880 --> 23:08.040
40 cars, we have 40 people going through doing each car.

23:08.040 --> 23:12.880
If we can skip the first 25 because we can do that in an automated way, that, that'll

23:12.880 --> 23:13.880
be great.

23:13.880 --> 23:18.720
That's not to say we ever want to replace the human because there's a huge value to

23:18.720 --> 23:23.000
kind of having that human eye, that human judgment because ultimately they are only going

23:23.000 --> 23:25.160
to be as smart as the people who are training it.

23:25.160 --> 23:30.560
But over time, if we can kind of up level them into doing more, you know, less of the

23:30.560 --> 23:35.320
road task and more of the task that really involve a human's particular judgments, that's

23:35.320 --> 23:37.080
where we want to get to.

23:37.080 --> 23:38.080
Right.

23:38.080 --> 23:45.560
I think the, you know, the safety of the autonomous vehicle is going to be, will be proportional

23:45.560 --> 23:51.560
to the amount of data that has been, you know, properly labeled and used in training.

23:51.560 --> 23:58.760
And so I think it just strikes me that, you know, there is tons more, you know, seen data

23:58.760 --> 24:00.960
to be processed.

24:00.960 --> 24:06.440
And even if you automated that easy, you know, 40 to 80 percent, there's still going

24:06.440 --> 24:12.960
to be plenty of work for the humans to do to do that, the more difficult task.

24:12.960 --> 24:13.960
Absolutely.

24:13.960 --> 24:17.840
And, you know, this is an industry where 95 percent accuracy isn't going to cut it,

24:17.840 --> 24:18.840
right?

24:18.840 --> 24:19.840
There's lives in the line.

24:19.840 --> 24:20.840
It's about safety.

24:20.840 --> 24:23.480
So you're really trying to get, you know, as perfect as you can get, and that's really

24:23.480 --> 24:24.480
going to take iteration.

24:24.480 --> 24:29.240
That's going to take always having a human in the loop to make sure that, you know,

24:29.240 --> 24:33.120
there isn't a misjudgment at this point where we're talking about training and validation

24:33.120 --> 24:37.040
before we're even talking about putting this onto the road and it's the wild.

24:37.040 --> 24:38.040
Mm-hmm.

24:38.040 --> 24:44.720
Do you think at all about the, any of the research that's happening around adversarial examples

24:44.720 --> 24:50.160
and there are some that are particularly focused on, I guess it's a little bit of a different

24:50.160 --> 24:53.800
context from where you're focused since your focus is on human annotation, but there's

24:53.800 --> 24:58.320
some research that looks at, you know, things that ways that you can manipulate images

24:58.320 --> 25:04.040
so that a neuron that will look at a stop sign and see a giraffe or whatever, is that,

25:04.040 --> 25:05.760
is that on your radar at all?

25:05.760 --> 25:10.280
Yeah, you know, it's definitely an area that we've considered both from that adversarial

25:10.280 --> 25:14.040
side and the generative kind of synthetic data side.

25:14.040 --> 25:20.400
And really, I think the more that we're tied to what's in the wild, whether it is, you

25:20.400 --> 25:25.200
know, running into stop signs that have been vandalized in a way that are intentionally

25:25.200 --> 25:31.640
trying to, you know, confuse models and vision systems or whether it's about getting a greater

25:31.640 --> 25:32.640
diversity.

25:32.640 --> 25:36.720
You know, one of the biggest challenges that auto manufacturers or people who are focusing

25:36.720 --> 25:42.560
on autonomous driving have is just how different, you know, scenarios are around the world.

25:42.560 --> 25:44.080
And also rare cases.

25:44.080 --> 25:47.800
So it's not just about our stop signs being in a different language or a different shape

25:47.800 --> 25:52.080
or the road markings being different, but even the types of vehicles we see on the road,

25:52.080 --> 25:55.880
right, like a pickup truck in the U.S. is pretty different from a pickup truck in parts

25:55.880 --> 25:57.360
of Asia, right?

25:57.360 --> 26:01.320
And, you know, there's a lot of rare cases a few months ago here in the Northwest, there

26:01.320 --> 26:05.120
was like a tractor trailer that turned over in the middle of the highway with a bunch

26:05.120 --> 26:06.920
of like, slime eels in the back.

26:06.920 --> 26:07.920
Right, I remember that.

26:07.920 --> 26:11.720
And, you know, it's like, how would, you know, what would you do if you were the car behind,

26:11.720 --> 26:15.480
you know, the autonomous vehicle behind that truck as that happened?

26:15.480 --> 26:20.120
And currently, even with hundreds of thousands of hours of footage, the odds of getting something

26:20.120 --> 26:23.000
like that on tape is going to be difficult, right?

26:23.000 --> 26:24.320
So, or low.

26:24.320 --> 26:31.080
So really, I think there is a balance of how do we augment what we have with other scenarios

26:31.080 --> 26:35.920
and other things to get that bigger picture of what could possibly happen?

26:35.920 --> 26:37.160
Hmm.

26:37.160 --> 26:41.880
Along those lines, you know, granted that for a lot of the companies in this space, their

26:41.880 --> 26:49.480
data is a core element of their IP and ability to differentiate.

26:49.480 --> 26:57.600
But are you aware of any movements to create like data consortia, for example, where, you

26:57.600 --> 27:03.440
know, OEMs would contribute their data with, you know, all in the agreement that they

27:03.440 --> 27:09.120
would get data back so that, you know, they may have cars operating in North America

27:09.120 --> 27:15.000
and, you know, they can contribute their data and get access to data from, you know, that's

27:15.000 --> 27:19.680
based in other geographies, and is that something that, A, is that something that is, you

27:19.680 --> 27:24.120
know, happening that you're aware of, or, and B, is that something that you might be

27:24.120 --> 27:25.640
able to help facilitate?

27:25.640 --> 27:26.640
Absolutely.

27:26.640 --> 27:32.520
So I think right now, it's, you know, there's a lot of secrecy in this industry, so everybody

27:32.520 --> 27:38.440
keeps their images, their data, even their requirements, as far as what they're labeling

27:38.440 --> 27:39.640
pretty close.

27:39.640 --> 27:44.080
But there are, there are starting to form more partnerships, companies working together.

27:44.080 --> 27:49.000
I think both for the reasons you described, as well as just, you know, everybody's working

27:49.000 --> 27:50.240
on slightly different angles.

27:50.240 --> 27:54.760
So if they can leverage each other's to build a solution and come to market sooner or

27:54.760 --> 27:57.920
be the first, I think they're going to, you know, embrace that.

27:57.920 --> 28:03.160
And where we can fit in is we, there's certainly a place in which we can leverage the data

28:03.160 --> 28:07.280
that we've already labeled and help people distribute that and manage that.

28:07.280 --> 28:11.320
So we're not duplicating as much effort, but we are really thinking about how to build,

28:11.320 --> 28:14.680
you know, a really useful kind of full data set.

28:14.680 --> 28:16.400
Right, right.

28:16.400 --> 28:22.760
So let's maybe dive back into the, you know, the process and, and the lessons learned

28:22.760 --> 28:29.440
and how that's expressed itself in technology that you've developed, anything else in terms

28:29.440 --> 28:35.120
of specifics, you know, things that you've observed specific to the autonomous vehicle

28:35.120 --> 28:36.120
market?

28:36.120 --> 28:37.120
Sure.

28:37.120 --> 28:40.800
You know, a lot of things I think could fit a broader market, but really by focusing

28:40.800 --> 28:46.280
here, it's allowed us to dive deep and not be, you know, distracted by what's going

28:46.280 --> 28:50.600
on in linguistics and natural language processing versus, you know, different parts of robotics

28:50.600 --> 28:51.600
and vision.

28:51.600 --> 28:57.080
But, you know, all of these, all of these approaches that require humans require a lot

28:57.080 --> 29:01.360
of, you know, management of the humans, right.

29:01.360 --> 29:06.040
So as far as really working to make sure that we can translate requirements to something

29:06.040 --> 29:10.760
can be understood, making sure that we understand when people are making mistakes,

29:10.760 --> 29:12.520
what is the, what is the reason behind it?

29:12.520 --> 29:16.920
There's actually, you know, a lot of psychology to, you know, why, why do we get bad data?

29:16.920 --> 29:18.600
Is it because people are being fraudulent?

29:18.600 --> 29:20.760
Is it because we didn't explain it right?

29:20.760 --> 29:23.960
Is it because we didn't even think about the scenario or is it because we explained it

29:23.960 --> 29:27.440
in a way that they're actually being consistent with what we told them to do, but we were, you

29:27.440 --> 29:30.000
know, we were wrong or we misunderstood something.

29:30.000 --> 29:31.600
So it's a really iterative process.

29:31.600 --> 29:35.640
It's, it's not something where you can just say there's a one-size-fits-all tool dropping

29:35.640 --> 29:40.520
your data, use a generic community and get, get good data out.

29:40.520 --> 29:44.120
And we've definitely, I think, learned that more than anything over the past few years

29:44.120 --> 29:48.320
as far as how much we need to understand really specific requirements, as well as how

29:48.320 --> 29:51.520
those fit with data that changes over time.

29:51.520 --> 29:55.760
And how do you, how does your platform express those requirements?

29:55.760 --> 30:02.960
Are they kind of hard-coded in for each project that you take on or do you have element

30:02.960 --> 30:06.040
of the platform that's like a rules engine or something like that?

30:06.040 --> 30:09.040
I'm trying to wrap my head around how I might implement something like that.

30:09.040 --> 30:11.840
So, you know, it's a combination of many things.

30:11.840 --> 30:16.080
So as I said, it's been an iterative process over the past few years as far as us developing

30:16.080 --> 30:17.080
it.

30:17.080 --> 30:21.280
On the instructional side, you know, we spent a lot of time on the instructional design

30:21.280 --> 30:27.320
as far as just making sure that once we internally have understood all the requirements and translated

30:27.320 --> 30:32.160
them into something that our community can understand, we're both giving them enough information

30:32.160 --> 30:37.720
in small enough pieces that they can understand how to use the tool, how to follow instructions

30:37.720 --> 30:41.560
for very specific tasks for a particular customer.

30:41.560 --> 30:47.320
So, you know, even the definitions of how to box or how to, you know, draw a shape around

30:47.320 --> 30:49.880
a vehicle might change from project to project.

30:49.880 --> 30:54.400
And so, making sure that within the context of what they're doing, we're constantly reminding

30:54.400 --> 30:58.640
them of the exact rules and then, you know, and testing them.

30:58.640 --> 31:04.080
So we do have ways to, you know, inject known task and make sure that they are meeting

31:04.080 --> 31:08.400
the right accuracy level as well as getting feedback constantly so we can tell them, you know,

31:08.400 --> 31:13.480
you're doing a great job at, you're drawing, but your labels are consistently or sometimes

31:13.480 --> 31:14.480
off in some way.

31:14.480 --> 31:19.160
Like you keep categorizing a, you know, a box van as a pickup and really they're two different

31:19.160 --> 31:20.160
types of things.

31:20.160 --> 31:25.400
So, you know, we try to have as much feedback as we can as well as the upfront instructions.

31:25.400 --> 31:30.280
And the upfront instructions, it's really, it's written, it's showing photographs and

31:30.280 --> 31:35.360
showing them examples of good and bad and then it's even sometimes going in and producing

31:35.360 --> 31:41.320
videos that really talk about a nuanced detail that is easier to express with words and

31:41.320 --> 31:45.440
in motion than it is with just a, you know, a paragraph and an image.

31:45.440 --> 31:49.920
And part of that too is, you know, we have a international community so making sure that

31:49.920 --> 31:56.360
we're conveying these in the language they understand, you know, there's no reason why

31:56.360 --> 32:01.280
some of these tasks can be done better by one language or one community than another.

32:01.280 --> 32:04.880
And it's really up to us to make sure that we're, we're opening it to the right people.

32:04.880 --> 32:09.400
If we have a community that as it speaks, you know, is natively Spanish speaking, if you

32:09.400 --> 32:13.720
give them very nuanced technical instructions in English, it's going to be harder to understand

32:13.720 --> 32:15.680
than if we give it to them in Spanish, for example.

32:15.680 --> 32:20.000
So that's the type of thing that we have to think about whenever we're doing our targeting

32:20.000 --> 32:24.320
as far as who's going to have access to this task as well as making sure that, you know,

32:24.320 --> 32:31.200
between us and our customers that there's alignment for a given task and to be more specific

32:31.200 --> 32:40.400
for a given scene and an object within a scene, how much redundancy is there in the process

32:40.400 --> 32:45.640
meaning, you know, for a given frame of a video, how many times you're asking someone

32:45.640 --> 32:52.840
to label a given object before you have that confidence level that it's done correctly.

32:52.840 --> 32:57.240
Is there a ton of redundancy in the process or have you managed to kind of filter that

32:57.240 --> 32:58.240
out?

32:58.240 --> 32:59.760
There's not a ton of redundancy.

32:59.760 --> 33:04.080
What early on in the process, we may have multiple people doing tasks in order to get

33:04.080 --> 33:09.320
a better understanding for the types of differences we'll see as people do the task.

33:09.320 --> 33:14.360
But ultimately, that's part of what makes our system work really well is that we get

33:14.360 --> 33:15.640
more efficient over time.

33:15.640 --> 33:18.360
We have less people doing it over time.

33:18.360 --> 33:19.360
Mm-hmm.

33:19.360 --> 33:20.360
Okay.

33:20.360 --> 33:25.960
Yeah, so unlike a traditional crowdsourcing model where your only quality control mechanism

33:25.960 --> 33:30.600
is looking for consensus or asking, you know, 10 people and saying, six of them agreed

33:30.600 --> 33:32.320
so that must be the right answer.

33:32.320 --> 33:36.600
We've tried to be a little bit more intentional and intelligent about how we make these decisions

33:36.600 --> 33:40.520
using our reputation engines and some of our other internal models.

33:40.520 --> 33:41.520
Okay.

33:41.520 --> 33:46.720
Now, this is maybe something that I should have asked earlier, but do you have any, can

33:46.720 --> 33:53.960
you share any data points that can help us contextualize the scope of the challenge

33:53.960 --> 33:59.720
within the autonomous vehicle space or, you know, the volume of data in that space or

33:59.720 --> 34:02.640
that you're focused, that you're working with in particular?

34:02.640 --> 34:03.640
Sure.

34:03.640 --> 34:07.040
So, you know, right now there's a few cars on the road.

34:07.040 --> 34:10.880
I think it's the easiest way to think about it, you know, all these, each company has

34:10.880 --> 34:14.520
a handful of cars at best collecting data.

34:14.520 --> 34:19.560
And even any one of those cars might be collecting, you know, terabyte of video a day.

34:19.560 --> 34:22.240
And most of that doesn't need to be human labelled.

34:22.240 --> 34:26.880
But there is, you know, significant volume, especially when you think about the diversity

34:26.880 --> 34:31.640
problems we were talking about earlier as far as that's one car on one road or one set

34:31.640 --> 34:38.040
of roads in one area of the world, you know, in the valley or in Germany or in Michigan.

34:38.040 --> 34:42.320
And so really, as these fleets develop, that's just going to scale exponentially, right?

34:42.320 --> 34:47.240
We're going to have both the test fleets, which will be hopefully located around the world

34:47.240 --> 34:53.120
in collecting different types of data, so not just images, but light art and other sensors.

34:53.120 --> 34:57.600
And then when we get into production, where we're going to start looking really for validation

34:57.600 --> 35:01.040
and feedback loops, especially when, you know, a system gets triggered.

35:01.040 --> 35:06.040
So for talking about an event-based system and we have emergency braking triggered, you're

35:06.040 --> 35:09.680
going to want to have a human validate was that the right, you know, right thing to do

35:09.680 --> 35:11.160
or not.

35:11.160 --> 35:16.560
And so I think over time, we end up with more and more use cases that are going to require

35:16.560 --> 35:20.920
human insight, even beyond just the raw data that's being captured.

35:20.920 --> 35:27.440
And part of the art will be figuring out what to annotate or what things need better labeling

35:27.440 --> 35:32.080
or what things don't, because obviously we can't take petabytes of data a day and process

35:32.080 --> 35:35.520
that in a meaningful way that's going to really improve things.

35:35.520 --> 35:44.440
Alright, do you often get tasks that are incremental in nature, meaning you've got, as opposed

35:44.440 --> 35:53.000
to processing all of the scenes or objects within, or all of the objects within the scene,

35:53.000 --> 36:01.800
the particular use case calls for only, you know, only the road dividers or signs or

36:01.800 --> 36:06.520
things like, it sounds like that's a typical thing for you to do, is that true?

36:06.520 --> 36:07.520
Yeah, definitely.

36:07.520 --> 36:13.160
I mean, it's actually, might be telling about what parts of the problem any one customer

36:13.160 --> 36:15.320
is focusing on a given time, right?

36:15.320 --> 36:21.840
So lane markings are obviously, you know, a very discrete task as far as, you know, looking

36:21.840 --> 36:27.560
at exits in dashed line, solid lines, road boundaries, and then pedestrians would be

36:27.560 --> 36:32.880
another really good example, as far as trying to understand what, you know, urban scene,

36:32.880 --> 36:35.800
where the pedestrians are located and how they're moving over time.

36:35.800 --> 36:39.800
So are they, you know, likely hit somebody to cross the roadway or cross in front of

36:39.800 --> 36:44.080
the vehicle or just stand around to, you know, to bus stop, we kind of have to understand

36:44.080 --> 36:46.680
that that's a bus stop where people just stand, they're not going to cross the street,

36:46.680 --> 36:51.800
they're not going to move anyway, they'll disappear magically, you know, in a couple of

36:51.800 --> 36:54.040
frames after the bus passes by.

36:54.040 --> 36:59.000
You know, there's things like that that I think really are individual areas of focus.

36:59.000 --> 37:04.360
So beyond the general kind of computer vision, like building a better eye for the camera

37:04.360 --> 37:10.600
is, is building, you know, context and building semantic understanding that I think are involved

37:10.600 --> 37:14.680
involved more of these discrete tasks.

37:14.680 --> 37:22.280
So do you do any labeling, do you do any, for lack of a better term on thinking of this

37:22.280 --> 37:28.440
like first derivative labeling, like as opposed to saying, you know, that's a pedestrian labeling,

37:28.440 --> 37:35.800
the pedestrian is walking in direction, you know, X or at a speed that you can calculate

37:35.800 --> 37:40.040
based on the, you know, the timestamps on different images and things like that.

37:40.040 --> 37:44.520
Yeah, so we definitely do tracking across video.

37:44.520 --> 37:48.840
So in that, that's actually, there's two ways to do that, right?

37:48.840 --> 37:53.000
You can either derive it from two still frames or you can play a video, which can sometimes

37:53.000 --> 37:57.160
be helpful as far as understanding what else is going on in the frame and just getting all

37:57.160 --> 37:58.640
that data at once.

37:58.640 --> 38:03.840
So having one person view five seconds can, you might give you more information as far as

38:03.840 --> 38:09.520
like if the rate of movement changes, like person's walking, then the, you know, the cross-signal

38:09.520 --> 38:13.640
turns to a blinking hand, they start walking faster or something like that.

38:13.640 --> 38:17.760
It's, you know, it's kind of helpful to see that happen or, you know, cars turning into

38:17.760 --> 38:20.400
their lane and so they stop in the middle of the road.

38:20.400 --> 38:23.600
Like, it's a little harder to see that when you're talking about an individual frame one

38:23.600 --> 38:26.880
at a time, even if you're trying to piece that data back together.

38:26.880 --> 38:31.760
And there are definitely nuances where it's not just a box around a person, but like I said,

38:31.760 --> 38:35.040
it's what kind of person, you know, are they, do they have a stroller?

38:35.040 --> 38:36.400
You know, are they, are they walking?

38:36.400 --> 38:39.480
Are they, are they distracted in some way?

38:39.480 --> 38:42.240
And then their orientation, so what direction are they moving?

38:42.240 --> 38:45.320
All of that metadata kind of feeds into it where you end up with an annotation that's

38:45.320 --> 38:50.040
not just an image with a box in coordinates, but it's an image with a box with a coordinate

38:50.040 --> 38:55.360
so it has a lot of metadata that might be related to this point in time versus the same

38:55.360 --> 39:00.000
image, you know, later point in time that has a lot of shared metadata, but also certain

39:00.000 --> 39:02.240
things change.

39:02.240 --> 39:09.400
And it sounds like you're also able to uniquely identify and track not just a person in

39:09.400 --> 39:16.480
a box, but, you know, person X in a box, you know, in frame one across, you know, all of

39:16.480 --> 39:19.720
the frames in a segment in which they're visible.

39:19.720 --> 39:20.720
Yeah.

39:20.720 --> 39:21.920
I mean, that's hugely important, right?

39:21.920 --> 39:24.520
To have that instance level kind of tracking.

39:24.520 --> 39:29.000
So you can say our car is changing lanes or somebody crossing the street or is it just

39:29.000 --> 39:31.720
that there's different people throughout the scene, right?

39:31.720 --> 39:34.800
So it's really important to know that kind of tracking.

39:34.800 --> 39:42.400
Do you have a sense for who is kind of leading the field in terms of data collection?

39:42.400 --> 39:47.360
You mentioned that, you know, most of the folks that are doing this have, you know, one

39:47.360 --> 39:54.600
or two cars out there, but you know, certainly Google's got more cars, at least they've got,

39:54.600 --> 39:59.680
you know, they've got a lot of cars that they've instrumented for maps that are capturing

39:59.680 --> 40:05.320
some of the same types of data and, you know, Tesla's got a lot of cars out there with

40:05.320 --> 40:10.720
cameras mounted and who's your, what's your sense for who's, you know, got the most

40:10.720 --> 40:15.960
data, visual data on vehicles, real life, you know, in the wild vehicles.

40:15.960 --> 40:20.240
Well, you know, I don't know that I would name a particular company, but really think

40:20.240 --> 40:26.200
about companies that have vehicles in the wild, which might be, you know, manufactured

40:26.200 --> 40:29.240
in production vehicles or could be fleets.

40:29.240 --> 40:36.080
So certainly there are companies that are trying to, say, distribute dash cams across, you

40:36.080 --> 40:41.880
know, consumer market so that they can capture video and use it for building, you know, autonomous

40:41.880 --> 40:44.720
systems while also providing value to the end customer.

40:44.720 --> 40:51.480
There's also things like taxis or ubers where there's an inherent value of that data to

40:51.480 --> 40:52.480
the driver.

40:52.480 --> 40:55.920
So there's a reason for them to put this device in their car, but there's also the value

40:55.920 --> 40:56.920
of the data collection.

40:56.920 --> 40:59.480
So I think ultimately there's going to be a couple of different strategies.

40:59.480 --> 41:03.400
It's not necessarily going to be, you have to produce cars and get them out there.

41:03.400 --> 41:04.400
Right.

41:04.400 --> 41:09.640
You have to produce a way to collect this data that's meaningful for people so that they're

41:09.640 --> 41:10.640
willing to do it.

41:10.640 --> 41:17.000
I've not seen the, you know, free dash cam if you give us the ability to, you know, use

41:17.000 --> 41:22.400
the data is who, do you have a, you know, specifically someone who's doing that?

41:22.400 --> 41:27.400
There are a couple of companies I can find the names for, for you later a little bit later.

41:27.400 --> 41:28.400
Okay.

41:28.400 --> 41:31.440
There's one company called Nexar that has a dash cam app.

41:31.440 --> 41:35.240
There's another company that's doing it specifically around ride sharing.

41:35.240 --> 41:37.520
So that's inside outside camera, right?

41:37.520 --> 41:41.360
So the idea of being, you're going to have, you know, you're going to see your customers

41:41.360 --> 41:44.840
in the back in case there's any situation where you have an abusive customer or an accident,

41:44.840 --> 41:49.120
you need to have that liability coverage as well as you've got your, you know, your front

41:49.120 --> 41:53.960
camera for, for actions and that kind of thing.

41:53.960 --> 41:57.840
In formulating that question, I hadn't even really thought about all of the dash cams

41:57.840 --> 42:04.680
and the various cameras that are mounted on public safety vehicles and utility fleets

42:04.680 --> 42:06.160
and things like that.

42:06.160 --> 42:09.960
There's just a ton of image data out there.

42:09.960 --> 42:10.960
Yeah.

42:10.960 --> 42:14.040
Well, you know, if you think about companies that have been working on mapping for a long

42:14.040 --> 42:18.080
time, like you mentioned the Google Street View and the Google Maps cameras, but even

42:18.080 --> 42:23.040
beyond that, you know, any, any fleet or any, you know, all of us who can carry a smartphone

42:23.040 --> 42:27.720
in our, in our pocket, so have some apps running in the background with location awareness.

42:27.720 --> 42:32.640
You know, that's all valid data as far as understanding kind of movement patterns in

42:32.640 --> 42:37.000
that, you know, that alone might not be enough with that in tandem with, with the camera

42:37.000 --> 42:41.480
becomes hugely valuable in that in tandem with, with high, high def maps can tell you

42:41.480 --> 42:43.760
when there's patterns that are changing.

42:43.760 --> 42:50.280
Is anyone doing anything as far as integrating and visual data collected via drones in

42:50.280 --> 42:51.280
the space?

42:51.280 --> 42:55.400
You know, I think that's a whole separate field as far as on the mapping side, for sure,

42:55.400 --> 43:01.240
on the actual vehicle driving systems, not that I know of.

43:01.240 --> 43:08.120
And is that because you really need to kind of be, you know, for the visual data to be,

43:08.120 --> 43:13.000
to have the unique perspective of the vehicle to be useful or because, you know, just hasn't

43:13.000 --> 43:14.000
happened yet?

43:14.000 --> 43:16.160
I think it's probably a little of both.

43:16.160 --> 43:21.600
So certainly companies think that one of their unique advantages is not just the footage

43:21.600 --> 43:25.520
they're collecting, but the way they're collecting it, whether it's using multiple sensors

43:25.520 --> 43:27.960
in a certain way, in certain positions.

43:27.960 --> 43:30.160
So you know, where do they locate their cameras?

43:30.160 --> 43:31.680
Are they using a stereo camera?

43:31.680 --> 43:34.360
Are they using, you know, side cameras as well?

43:34.360 --> 43:38.120
Wide field of view camera in tandem.

43:38.120 --> 43:43.800
So I think there is value to that, you know, uniqueness, but also, you know, I think we're

43:43.800 --> 43:46.800
just going to figure out what the right combination of data is.

43:46.800 --> 43:50.880
Obviously, you know, the more we can get the better, and for certain things like figuring

43:50.880 --> 43:54.760
out, you know, a big picture view of your current area, it would be great if you had something

43:54.760 --> 43:58.800
flying above you the whole time that could see, you know, a wider, further distance and

43:58.800 --> 44:03.800
a wider range than your eyes or your front cameras might see.

44:03.800 --> 44:05.800
Interesting.

44:05.800 --> 44:06.800
Interesting.

44:06.800 --> 44:10.720
What questions should I be asking that I might not have asked yet?

44:10.720 --> 44:15.720
Are there other thing areas that we might want to dig into before we start to wrap things

44:15.720 --> 44:16.720
up?

44:16.720 --> 44:21.200
Well, you know, I think we've covered a lot of a lot of good topics.

44:21.200 --> 44:27.480
I think when we talk about how people approach this problem, so it might be interesting.

44:27.480 --> 44:31.240
Before, you know, before my DAI, when you talk about other crowdsourcing or you talk about

44:31.240 --> 44:35.520
like doing it yourself, one of the biggest challenges is about the quality control, like

44:35.520 --> 44:37.840
I said, the instructions and all of that.

44:37.840 --> 44:41.760
But even when you're trying to do it in-house with your own team who knows all the requirements

44:41.760 --> 44:46.200
and all the instructions, it's really about that scale and diversity.

44:46.200 --> 44:53.440
And so I think, you know, really iterating that or kind of thinking about the fact that

44:53.440 --> 44:57.400
this data in Seattle is different from the data in Detroit, which is different than the

44:57.400 --> 45:01.160
data in Stuttgart or Singapore or any of these parts of the world.

45:01.160 --> 45:02.560
I think that's pretty key.

45:02.560 --> 45:07.200
So I mean, like you asked earlier about how to distribute, you know, how to get collect

45:07.200 --> 45:11.520
more data, it's not just like drive the same car in the same route over and over and over

45:11.520 --> 45:12.520
and over again.

45:12.520 --> 45:13.520
Like you do need to do that for a little bit.

45:13.520 --> 45:14.520
Right, right.

45:14.520 --> 45:17.600
But really it's about the diversity and then the understanding because, you know, your

45:17.600 --> 45:22.160
labelers in the U.S. might not even recognize what does it mean when I see a zigzag line

45:22.160 --> 45:26.280
on the road, on the side of the road in Europe, where, you know, we might go, oh, that's

45:26.280 --> 45:29.120
a no parking zone or that's, you know, that's a merge area, right?

45:29.120 --> 45:32.520
Like there's, so there's so much like context and so much localization that I think

45:32.520 --> 45:35.280
is easy to overlook.

45:35.280 --> 45:38.040
Even if you're somebody who's traveled and you know that you need to go learn the rules

45:38.040 --> 45:42.320
of the road somewhere else, it's like a lot of things that a human can adapt to that.

45:42.320 --> 45:46.160
If you're thinking about a system that really is just looking at what it sees and not having

45:46.160 --> 45:50.520
that higher level understanding, it's a really hard problem, right?

45:50.520 --> 45:54.160
Like I've never seen the zigzag road is that I mean, I have to like, you know, slalom

45:54.160 --> 45:59.200
my way down the road, right, as a car or does it just mean, like, stay away from that

45:59.200 --> 46:00.200
line, right?

46:00.200 --> 46:06.960
But the car figure out wouldn't be surprised to see some autonomous vehicle see one of those

46:06.960 --> 46:12.000
markings and just go go crazy as far as, you know, what it's supposed to do.

46:12.000 --> 46:13.520
Yeah, it's interesting.

46:13.520 --> 46:21.560
There's so much of this problem space that is, you know, the benefits from, from having

46:21.560 --> 46:26.840
that intelligence and the human or the computer intelligence and the human intelligence

46:26.840 --> 46:28.440
kind of melded together, right?

46:28.440 --> 46:32.680
So it's not just this training data labeling problem that we've talked about.

46:32.680 --> 46:40.960
You've made a very strong case for the power of combining human insight with automated,

46:40.960 --> 46:47.040
you know, automated tools, but even within the vehicle itself, there's a, there are folks

46:47.040 --> 46:53.000
doing research on, you know, how the car can benefit from the input of just looking at

46:53.000 --> 46:57.440
the driver and, you know, understanding what their, you know, state is what they're looking

46:57.440 --> 46:58.440
at.

46:58.440 --> 46:59.440
Things like that.

46:59.440 --> 47:00.440
Absolutely.

47:00.440 --> 47:04.120
Yeah, I think ultimately when we get, when we get to a point where we have full autonomy,

47:04.120 --> 47:09.080
we're going to be in a safer world, right, where we don't have distraction or distraction

47:09.080 --> 47:12.160
doesn't matter, you know, it's okay to sit there and start your phone if you're not the

47:12.160 --> 47:15.040
one driving and you don't need to be the one who's ready to grab the wheel.

47:15.040 --> 47:19.200
So we're talking about early stages where you need to be attentive and have your hands

47:19.200 --> 47:21.040
on the wheel or close to it.

47:21.040 --> 47:25.520
But as we get further down the road, you know, you take the best of the humans, which

47:25.520 --> 47:30.400
is the judgment and the vision and the decision-making processes, and you take away, you know,

47:30.400 --> 47:34.360
the fatigue and the distraction and the things that, you know, are going on in our lives

47:34.360 --> 47:39.240
and make it hard for us to stay focused, and I think you're going to end up in a better

47:39.240 --> 47:40.240
place.

47:40.240 --> 47:41.240
Awesome.

47:41.240 --> 47:47.640
Well, that is the hope and, you know, the vision behind autonomous vehicles for is, you

47:47.640 --> 47:54.160
know, as much as people talk about, you know, for example, the economic issues associated

47:54.160 --> 47:59.240
with deploying a bunch of autonomous vehicles in terms of labor and things like that, the,

47:59.240 --> 48:05.720
you know, the promise to stave off, you know, the, you know, huge numbers of vehicular related

48:05.720 --> 48:07.880
deaths, you know, that occur around the world.

48:07.880 --> 48:08.880
That's just huge.

48:08.880 --> 48:09.880
Yeah, absolutely.

48:09.880 --> 48:10.880
Well, awesome.

48:10.880 --> 48:12.880
Well, I really enjoy this conversation.

48:12.880 --> 48:18.360
Thank you so much for taking the time, and I really appreciate, you know, getting a

48:18.360 --> 48:23.720
chance to catch up with, with my DAI and hear about what you're doing in this space.

48:23.720 --> 48:24.720
Great.

48:24.720 --> 48:25.720
Thank you, Sam.

48:25.720 --> 48:26.720
Have a great day.

48:26.720 --> 48:27.720
Thanks, Darren.

48:27.720 --> 48:31.680
All right, everyone.

48:31.680 --> 48:33.520
That's our show for today.

48:33.520 --> 48:39.840
Thanks so much for listening, and of course, for your ongoing feedback and support.

48:39.840 --> 48:44.800
For more information on Darren, or any of the other topics covered in this episode, head

48:44.800 --> 48:49.520
on over to twimlai.com slash talk slash 57.

48:49.520 --> 48:57.320
To keep track of this autonomous vehicle series, visit twimlai.com slash AV2017.

48:57.320 --> 49:02.080
Please, please, please remember to send us any comments or questions you may have for us

49:02.080 --> 49:08.840
or our guests via Twitter, at Twimlai, or at Sam Charington, or leave a comment on the

49:08.840 --> 49:10.440
show notes page.

49:10.440 --> 49:21.080
Thanks again for listening, and catch you next time.

