1
00:00:00,000 --> 00:00:16,320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

2
00:00:16,320 --> 00:00:21,480
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:21,480 --> 00:00:32,040
I'm your host Sam Charrington.

4
00:00:32,040 --> 00:00:38,280
Today we're joined by Gary Brockman, Senior Director of Product Management at Qualcomm Technologies.

5
00:00:38,280 --> 00:00:43,080
Gary, who got his start in AI through music, now leads AI in machine learning strategy

6
00:00:43,080 --> 00:00:47,440
and product planning for the company with a focus that includes the Qualcomm Snapdragon

7
00:00:47,440 --> 00:00:49,640
mobile platforms.

8
00:00:49,640 --> 00:00:54,520
In our conversation, we discuss AI on mobile devices and at the edge, including popular

9
00:00:54,520 --> 00:00:59,320
use cases and explore some of the various acceleration technologies offered by Qualcomm

10
00:00:59,320 --> 00:01:01,600
and others that enable them.

11
00:01:01,600 --> 00:01:05,960
We also dig into the state of AI on devices from the application developer's perspective

12
00:01:05,960 --> 00:01:10,680
and how various acceleration technologies fit together to help developers bring new products

13
00:01:10,680 --> 00:01:13,160
to market.

14
00:01:13,160 --> 00:01:17,120
Before we get going, I'd like to send a huge thanks to Qualcomm for sponsoring today's

15
00:01:17,120 --> 00:01:18,120
show.

16
00:01:18,120 --> 00:01:22,080
As you'll hear in my conversation with Gary, Qualcomm has been in the AI space for well

17
00:01:22,080 --> 00:01:25,920
over a decade now, powering some of the latest and greatest Android devices with their

18
00:01:25,920 --> 00:01:28,120
Snapdragon chipset.

19
00:01:28,120 --> 00:01:32,520
From their strong footing in the mobile space, Qualcomm now has the goal of making AI at

20
00:01:32,520 --> 00:01:34,720
the edge ubiquitous.

21
00:01:34,720 --> 00:01:38,440
To find out more about what they're up to and how they plan to get there, visit twimble

22
00:01:38,440 --> 00:01:48,440
AI dot com slash Qualcomm and now on to the show.

23
00:01:48,440 --> 00:01:53,600
Gary is the Senior Director of Product Management at Qualcomm.

24
00:01:53,600 --> 00:01:56,400
Gary, welcome to this week in Machine Learning and AI.

25
00:01:56,400 --> 00:01:58,160
Thank you and happy new year.

26
00:01:58,160 --> 00:01:59,160
Happy new year to you.

27
00:01:59,160 --> 00:02:04,800
I am excited for this new year and as I mentioned to you, as we're chatting before we started

28
00:02:04,800 --> 00:02:10,920
rolling today is my birthday making this a, well, it's always very cool to kind of have

29
00:02:10,920 --> 00:02:13,640
the new year on my birthday line.

30
00:02:13,640 --> 00:02:18,000
But I'm excited to make this my first interview of the year.

31
00:02:18,000 --> 00:02:21,480
And I'm happy to share your birthday with you, Sam.

32
00:02:21,480 --> 00:02:22,480
Fantastic.

33
00:02:22,480 --> 00:02:23,480
Fantastic.

34
00:02:23,480 --> 00:02:29,080
Why don't we get started by having you tell us a little bit about your background.

35
00:02:29,080 --> 00:02:35,040
You got into artificial intelligence initially by way of music, is that right?

36
00:02:35,040 --> 00:02:36,040
Yeah, that's right.

37
00:02:36,040 --> 00:02:40,040
I guess my, my interest in AI started way back.

38
00:02:40,040 --> 00:02:43,880
I guess it would have been at the turn of the century.

39
00:02:43,880 --> 00:02:48,640
I was working at a company called Music Match, which had developed a jukebox software application

40
00:02:48,640 --> 00:02:49,640
for the PC.

41
00:02:49,640 --> 00:02:56,160
But part of that offering was a personal music recommendation engine that was a proprietary

42
00:02:56,160 --> 00:02:59,240
engine that Music Match had created.

43
00:02:59,240 --> 00:03:06,000
And it was using collaborative filtering algorithms to monitor the listening behavior

44
00:03:06,000 --> 00:03:11,200
of the users of our jukebox software and what they, what they were listening to, what they

45
00:03:11,200 --> 00:03:13,680
skipped, what they listened to fully.

46
00:03:13,680 --> 00:03:19,680
And we were able to really develop some really rich correlations between artists and listeners

47
00:03:19,680 --> 00:03:26,400
that had, that transcended any traditional tagging data like genre or year or era would

48
00:03:26,400 --> 00:03:27,880
have you.

49
00:03:27,880 --> 00:03:32,800
Just looking, listening to or actually watching what people are observing, what listeners

50
00:03:32,800 --> 00:03:37,360
were actually doing, not what they were saying, not what they said they liked or disliked.

51
00:03:37,360 --> 00:03:42,240
But what they were actually listening to and then taking the correlations between individual

52
00:03:42,240 --> 00:03:45,640
listeners and then myself or another somebody else.

53
00:03:45,640 --> 00:03:53,040
And that combined, the combined influences came up or resulted in some very unique recommendations.

54
00:03:53,040 --> 00:03:57,440
My background early on was in music and digital music and MP3 compression.

55
00:03:57,440 --> 00:04:04,000
And when I came across, when I experienced what Music Match had to offer and help promote

56
00:04:04,000 --> 00:04:10,320
their recommendation technology, which also powered personalized radio and on demand streaming

57
00:04:10,320 --> 00:04:13,720
recommendations, that got me hooked.

58
00:04:13,720 --> 00:04:18,840
And I carried that with me for a number of years and up until four years ago, when I had

59
00:04:18,840 --> 00:04:24,280
an opportunity at Qualcomm to bring some AI machine learning technology at a corporate

60
00:04:24,280 --> 00:04:30,760
R&D into the commercial side of the house and release it across our Snapdragon platform,

61
00:04:30,760 --> 00:04:36,080
kind of tying those things together over the past four years has really been a great experience

62
00:04:36,080 --> 00:04:37,800
and quite rewarding.

63
00:04:37,800 --> 00:04:42,520
Why don't we maybe spend a little bit of time talking about Qualcomm for those who are

64
00:04:42,520 --> 00:04:46,600
not familiar with the company and what you're up to?

65
00:04:46,600 --> 00:04:52,080
I know in our newsletter, we've talked about Qualcomm quite a bit, particularly over

66
00:04:52,080 --> 00:05:02,280
the last year, everything from the Snapdragon platform launch in December, prior to that,

67
00:05:02,280 --> 00:05:08,800
the extended reality release that you did in June.

68
00:05:08,800 --> 00:05:16,280
Some new chips in April and I think maybe in January or something, there was a neural

69
00:05:16,280 --> 00:05:22,280
processing engine and a hexagon vector processor, all really interesting stuff that I'm excited

70
00:05:22,280 --> 00:05:24,720
to talk to you about today.

71
00:05:24,720 --> 00:05:29,680
But for folks who've kind of seen those pieces and don't really know what the big picture,

72
00:05:29,680 --> 00:05:32,280
what's Qualcomm up to in this space?

73
00:05:32,280 --> 00:05:39,320
Well, if you go back in time, we've been a mobile innovator for over 30 years now, most

74
00:05:39,320 --> 00:05:46,440
of it's starting with cellular and since the early 90s, we've been focused on the connectivity

75
00:05:46,440 --> 00:05:51,560
side with every G transition, we call it from 3G to 4G and out of 5G.

76
00:05:51,560 --> 00:05:57,400
We also got into the chip business in the 90s because others were having some difficulty

77
00:05:57,400 --> 00:06:04,000
developing 2G and 3G chips and then that was a springboard for us to then move the internet

78
00:06:04,000 --> 00:06:06,760
from the PC onto a mobile phone.

79
00:06:06,760 --> 00:06:11,680
We saw this trend happening very early on and tried to hasten that with investment in

80
00:06:11,680 --> 00:06:15,280
both connectivity and silicon.

81
00:06:15,280 --> 00:06:19,760
And then we introduced the Snapdragon mobile platform a little over a decade ago.

82
00:06:19,760 --> 00:06:24,360
In fact, we're probably coming up on 11 years now and that and Snapdragon is now what

83
00:06:24,360 --> 00:06:29,360
powers the majority of Android phones in the marketplace globally.

84
00:06:29,360 --> 00:06:34,360
What we've done on the Snapdragon side specifically, when it comes to compute on a handset or in

85
00:06:34,360 --> 00:06:41,760
any other embedded device, IoT, automotive, etc., we have leveraged the compute capabilities

86
00:06:41,760 --> 00:06:47,440
that we developed over that past decade to drive what we have seen over the past four

87
00:06:47,440 --> 00:06:52,680
years as being a very quick movement of an artificial intelligence and machine learning

88
00:06:52,680 --> 00:06:59,800
based workloads that have been mostly relegated to the cloud on the server side or the data

89
00:06:59,800 --> 00:07:03,920
center, but are matriculating to the edge.

90
00:07:03,920 --> 00:07:09,160
Mobile phone is really kind of the primary focus for us, but we do look at other verticals

91
00:07:09,160 --> 00:07:15,760
like IoT and automotive as I mentioned, but the activity from Qualcomm standpoint has

92
00:07:15,760 --> 00:07:21,680
been very strong on 5G, but also an AI over the past four years and specifically on device

93
00:07:21,680 --> 00:07:31,160
AI and ensuring that any device for the Snapdragon processor is able to efficiently run and accelerate

94
00:07:31,160 --> 00:07:33,880
AI algorithms in a power efficient way.

95
00:07:33,880 --> 00:07:40,520
Okay, so to make sure I understand that trajectory, the companies started out basically building

96
00:07:40,520 --> 00:07:48,120
the or not started out, but one of the companies big moves was really building the chips that

97
00:07:48,120 --> 00:07:57,760
allowed devices to connect to wireless networks like 3G, 4G, 5G, and then from that kind

98
00:07:57,760 --> 00:08:04,600
of presence in the mobile space moved into, you know, as smartphones arrived, moved

99
00:08:04,600 --> 00:08:12,800
into providing the compute platform or the compute chips for the smartphones and now

100
00:08:12,800 --> 00:08:18,680
are providing an of the AI acceleration extensions to that compute platform.

101
00:08:18,680 --> 00:08:20,680
Is that the right way to think about it?

102
00:08:20,680 --> 00:08:22,800
Yeah, I think that's the right continuum.

103
00:08:22,800 --> 00:08:25,880
Those are the three big pillars, if you will.

104
00:08:25,880 --> 00:08:34,200
Okay, and your responsibility there is focused on AI strategy and product planning.

105
00:08:34,200 --> 00:08:37,480
How long has the company been thinking about AI?

106
00:08:37,480 --> 00:08:39,800
That's a good question.

107
00:08:39,800 --> 00:08:44,000
We've been, it's interesting that most folks don't know how long we've been focused on

108
00:08:44,000 --> 00:08:45,440
this, but it's been over a decade.

109
00:08:45,440 --> 00:08:51,760
We really started investing in deep learning back in or machine learning back in 2007.

110
00:08:51,760 --> 00:08:55,960
We've had a heritage in computer vision, but in the research side of the house, we've

111
00:08:55,960 --> 00:09:00,120
been looking at, you know, everything from spiky neural networks to deep learning going

112
00:09:00,120 --> 00:09:04,440
back as far as 2007, 2008.

113
00:09:04,440 --> 00:09:10,600
Our research group has actually been driving most of the activity over that 11, 12 year

114
00:09:10,600 --> 00:09:19,440
period, but in the past four years, as I say, four of the last 11 years, we've actually

115
00:09:19,440 --> 00:09:26,120
been releasing commercial grade software as well as hardware acceleration to accommodate

116
00:09:26,120 --> 00:09:31,680
what we see is at this present, a tidal wave of workloads that are arriving on mobile.

117
00:09:31,680 --> 00:09:38,280
We started our first, or at least we introduced our first mobile SOC that was optimized for

118
00:09:38,280 --> 00:09:44,280
on device machine learning with a Snapdragon 820 back in 2015, and we're now in our fourth

119
00:09:44,280 --> 00:09:46,480
generation.

120
00:09:46,480 --> 00:09:51,880
Taking a little quick step back, I mean, my role at Qualcomm and our team's role is to

121
00:09:51,880 --> 00:09:57,280
look at what we need to do to accommodate this new class of software and algorithms on

122
00:09:57,280 --> 00:10:00,160
device and all as well as in the device.

123
00:10:00,160 --> 00:10:02,920
So at Qualcomm, we don't focus just on the end product.

124
00:10:02,920 --> 00:10:07,640
We're actually looking at AI and machine learning as being integral to how we develop products

125
00:10:07,640 --> 00:10:08,920
and also run our business.

126
00:10:08,920 --> 00:10:15,760
So my team's responsibility is to look across the Snapdragon portfolio where we can optimize

127
00:10:15,760 --> 00:10:19,880
up and down the tiers, whether it's in a lower tier all the way up to the premium tier

128
00:10:19,880 --> 00:10:26,960
SOC, but also look at ways that as we develop our products within the organization, how can

129
00:10:26,960 --> 00:10:33,520
we apply machine learning to make those processes more cost efficient, reduce time, and in

130
00:10:33,520 --> 00:10:38,400
cases where it's possible, generate a creative revenue from that effort.

131
00:10:38,400 --> 00:10:44,600
You mentioned SOC, and that's system on chip, which is basically the effectively the CPU

132
00:10:44,600 --> 00:10:46,000
for these devices.

133
00:10:46,000 --> 00:10:50,880
Yeah, it's actually, the SOC would be a complete system.

134
00:10:50,880 --> 00:11:00,040
So the CPU, maybe the heart of the chip itself is the primary processor, but the SOC is

135
00:11:00,040 --> 00:11:07,160
defined as all the components that are necessary to drive what that device is capable of doing.

136
00:11:07,160 --> 00:11:13,480
So in the case of Snapdragon, the entire Snapdragon portfolio is an SOC portfolio and

137
00:11:13,480 --> 00:11:24,640
each has at least our cryo CPU, our Adreno GPU, and in the mid to high to premium tier,

138
00:11:24,640 --> 00:11:28,120
you'll find our hexagon.

139
00:11:28,120 --> 00:11:32,680
We do have DSPs, or I say the hexagon DSP.

140
00:11:32,680 --> 00:11:39,000
The DSP is pretty prevalent across the portfolio from a modem standpoint, but from an AI standpoint,

141
00:11:39,000 --> 00:11:47,360
the hexagon family includes vector processors, which are present in our premium tier and

142
00:11:47,360 --> 00:11:52,560
top tier SOC, so 800 down to 600.

143
00:11:52,560 --> 00:11:57,320
And that vector processor up until recently has been a primary engine for running on

144
00:11:57,320 --> 00:11:59,000
device AI.

145
00:11:59,000 --> 00:12:07,800
What are the primary use cases that you're focused on from a on device AI perspective?

146
00:12:07,800 --> 00:12:13,440
So there are a number that are I guess today and literally over the past 18 to 24 months

147
00:12:13,440 --> 00:12:19,280
that have become kind of table stakes have been things like just basic object recognition.

148
00:12:19,280 --> 00:12:23,400
So if you hold up your camera, you can detect whether something in the field of view is

149
00:12:23,400 --> 00:12:28,960
a specific class of object, let's say a car, a cat, or a dog, or what have you.

150
00:12:28,960 --> 00:12:33,360
And then taking that a little bit further to the facial recognition.

151
00:12:33,360 --> 00:12:39,880
So now if you look at any phone today, either you're going to have a phone that has a specific

152
00:12:39,880 --> 00:12:44,840
camera module that allows the device to take kind of a 3D depth mapped image of your face

153
00:12:44,840 --> 00:12:50,240
so that you can unlock the phone, but also maybe even make mobile payments depending on

154
00:12:50,240 --> 00:12:53,600
the type of phone you have and the region that you're in.

155
00:12:53,600 --> 00:12:59,840
With AI, we're seeing that the specifically in facial recognition, all the extra hardware

156
00:12:59,840 --> 00:13:06,000
that's been necessary to drive that really isn't necessary across the board because AI

157
00:13:06,000 --> 00:13:10,520
algorithms specifically are able to do depth mapping on their own with single images

158
00:13:10,520 --> 00:13:13,080
now and single camera.

159
00:13:13,080 --> 00:13:18,240
So facial recognition for unlocking a phone and making mobile payments is becoming fairly

160
00:13:18,240 --> 00:13:20,920
common voice activation.

161
00:13:20,920 --> 00:13:27,280
So being able to do detecting keyword on a mobile phone or any other device, I think that's

162
00:13:27,280 --> 00:13:31,320
becoming commonplace and again, that's happening locally, it's not happening in the cloud,

163
00:13:31,320 --> 00:13:37,520
that recognition is immediate so that you can have pretty low latency.

164
00:13:37,520 --> 00:13:42,560
There are other camera effects like bouquets, you can segment your portrait or they may

165
00:13:42,560 --> 00:13:47,520
call it portrait mode, what have you, where you segment your body or the subject's body

166
00:13:47,520 --> 00:13:53,120
from the background so you can blur that out or create a separate background and put

167
00:13:53,120 --> 00:13:58,840
the individual in a completely separate environment visually.

168
00:13:58,840 --> 00:14:04,120
The entire camera pipeline, most of the features that you see in a mobile camera today, whether

169
00:14:04,120 --> 00:14:10,440
it's HDR or even being able to do things like super resolution where you bring clarity

170
00:14:10,440 --> 00:14:15,480
to a picture that doesn't have, that isn't high resolution to start with.

171
00:14:15,480 --> 00:14:21,800
All of those features are now, they started off as computer vision based and are now becoming

172
00:14:21,800 --> 00:14:26,320
driven by deep neural networks on the device to solve for the same problem to achieve better

173
00:14:26,320 --> 00:14:33,560
accuracy and in many cases give the developer a more generalized approach to how they

174
00:14:33,560 --> 00:14:36,120
instantiate those features.

175
00:14:36,120 --> 00:14:38,720
Those are probably the most common today.

176
00:14:38,720 --> 00:14:46,680
It doesn't surprise me that the camera pipeline is all being processed on device but I'm

177
00:14:46,680 --> 00:14:54,120
a bit surprised to hear that things like object recognition, face recognition, voice recognition

178
00:14:54,120 --> 00:14:56,400
are being done on device.

179
00:14:56,400 --> 00:15:01,960
The thing that I'm thinking of most is like OK Google or the facial recognition or the

180
00:15:01,960 --> 00:15:06,840
people recognition and like Google photos and I just assume that that's the app and

181
00:15:06,840 --> 00:15:13,240
it's talking back to Google's cloud and all of the machine learning AI is happening between

182
00:15:13,240 --> 00:15:18,600
those two pieces of software but it sounds like more of it's happening on the device than

183
00:15:18,600 --> 00:15:19,600
I might think.

184
00:15:19,600 --> 00:15:21,120
I think that's right.

185
00:15:21,120 --> 00:15:26,720
If you look at anything that deals with biometric data, whether it's your face or your voice,

186
00:15:26,720 --> 00:15:31,960
that's very specific to you and that data needs to be kept secure and private and there

187
00:15:31,960 --> 00:15:42,000
is today given the state of kind of compute on device as well as the ability of CNNs and

188
00:15:42,000 --> 00:15:45,680
RNNs to be able to run locally in a pretty efficient way.

189
00:15:45,680 --> 00:15:53,120
There's really no reason why you as an individual should have to rely on a cloud and present

190
00:15:53,120 --> 00:15:58,560
your personal data to that, to the data center of the cloud in order to achieve some level

191
00:15:58,560 --> 00:16:02,160
of utility or some utility on the phone.

192
00:16:02,160 --> 00:16:07,120
So specifically with facial recognition and I guess I should take a step back and say

193
00:16:07,120 --> 00:16:13,040
that there's a security and privacy of been probably one of the primary reasons we focus

194
00:16:13,040 --> 00:16:18,320
so much on device as opposed to relying solely on the cloud for some of these functions.

195
00:16:18,320 --> 00:16:22,560
That in performance, so low latency because everybody wants things immediate.

196
00:16:22,560 --> 00:16:27,640
But there is again no reason why you should have to share your voice print or your voice

197
00:16:27,640 --> 00:16:33,960
data or your facial information or any other biometric data for that matter with a third

198
00:16:33,960 --> 00:16:40,120
party to let's say unlock a phone and do what you need to do on that device.

199
00:16:40,120 --> 00:16:48,720
So what you see today, if you're looking at OK Google or if you're on an iOS device,

200
00:16:48,720 --> 00:16:54,120
there's a balance between what's happening on the phone and what's happening in the cloud.

201
00:16:54,120 --> 00:16:57,040
Some cases as a consumer you're not going to know and I don't think that and the intention

202
00:16:57,040 --> 00:17:00,560
is that you shouldn't know which you should get as the best user experience.

203
00:17:00,560 --> 00:17:05,480
What we try to do is focus on ensuring that whatever workload does run on the device

204
00:17:05,480 --> 00:17:09,040
is executed in a power efficient and performant manner.

205
00:17:09,040 --> 00:17:11,640
But yeah, there's a variance.

206
00:17:11,640 --> 00:17:15,600
Some things do happen on the device and some things are happening in the cloud.

207
00:17:15,600 --> 00:17:21,720
And how about from an application developer perspective, if I'm developing an app and

208
00:17:21,720 --> 00:17:29,360
maybe everything's probably different if I'm Google, but if I'm just an app developer,

209
00:17:29,360 --> 00:17:39,240
Android app developer or iOS app developer, and I want to use AI, either via your mobile

210
00:17:39,240 --> 00:17:49,080
TensorFlow or something like that or the iOS extensions, do I have to think a lot about

211
00:17:49,080 --> 00:17:55,640
on device AI and how to take full advantage of the hardware or we at the point where

212
00:17:55,640 --> 00:18:00,200
that's all transparent to me and whatever framework I'm using, figures out the best way

213
00:18:00,200 --> 00:18:02,400
to do things.

214
00:18:02,400 --> 00:18:07,800
I think we're still at a point where there's experimentation, there's very little standardization

215
00:18:07,800 --> 00:18:13,840
across the industry, whether it's from benchmarking or common APIs to access the underlying

216
00:18:13,840 --> 00:18:20,240
compute on different devices, irrespective of the chip that is driving that device.

217
00:18:20,240 --> 00:18:25,120
In our portfolio, we have right now we have a handful of tools that are all part of what

218
00:18:25,120 --> 00:18:32,080
we call the Qualcomm AI engine and this AI engine is the sum of a number of parts.

219
00:18:32,080 --> 00:18:38,680
It's the primary compute engines that we have on our chip, so the CPU, the GPU, our hexagon

220
00:18:38,680 --> 00:18:44,240
vector processor, and something that we just recently introduced with Snapdragon 855,

221
00:18:44,240 --> 00:18:51,000
we call a tensor accelerator, which is a dedicated AI processor embedded in the Snapdragon

222
00:18:51,000 --> 00:18:56,560
855 chip, we can talk about that at another time or later, but those are the hardware

223
00:18:56,560 --> 00:18:57,560
elements.

224
00:18:57,560 --> 00:19:01,640
That's where the job gets done and each of those cores has a different power and performance

225
00:19:01,640 --> 00:19:09,000
profile to accommodate what would be, let's say, KPIs that the developer has set out

226
00:19:09,000 --> 00:19:13,720
for a specific user experience, let's say we're going to say facial recognition or detecting

227
00:19:13,720 --> 00:19:20,960
a keyword or somebody's voice, each of those features has

228
00:19:20,960 --> 00:19:28,680
a different tolerance for latency and a different tolerance for power, so by providing multiple

229
00:19:28,680 --> 00:19:33,920
compute engines from a hardware standpoint, it gives the developer choice.

230
00:19:33,920 --> 00:19:40,000
On the software side, specifically in our portfolio, we have SDKs like the neural processing

231
00:19:40,000 --> 00:19:45,120
engine, which I think you mentioned at the beginning, which is a very easy to use tool for

232
00:19:45,120 --> 00:19:49,960
a developer who has trained in neural network offline and wants to run it on a Snapdragon

233
00:19:49,960 --> 00:19:56,600
device, when they bring that model to the device using the neural processing SDK, all they

234
00:19:56,600 --> 00:20:01,800
have to do is make a simple API call to run on any of the compute elements, whether it's

235
00:20:01,800 --> 00:20:07,320
the CPU, GPU, or the vector process, or tensor accelerator, and then a way they go.

236
00:20:07,320 --> 00:20:08,400
It's very straightforward.

237
00:20:08,400 --> 00:20:14,920
You don't have to get down to the lower level close to the metal and rule up your sleeves,

238
00:20:14,920 --> 00:20:18,200
but we do have tools for those that want to get their hands dirty.

239
00:20:18,200 --> 00:20:25,160
We have libraries like Hexagon NN, which is a neural network library for Hexagon, if

240
00:20:25,160 --> 00:20:30,920
you want to just write directly to the Hexagon processor and not worry about the other cores.

241
00:20:30,920 --> 00:20:39,280
We have math libraries for CPU, and then if you're familiar with OpenCL, the Adreno GPU,

242
00:20:39,280 --> 00:20:43,520
our Adreno GPU supports OpenCL, so you can program directly with the GPU, so there's

243
00:20:43,520 --> 00:20:50,240
a variety of different ways with our own technology that you can access the compute on Snapdragon,

244
00:20:50,240 --> 00:20:54,040
and all of that is part of what we call the AI engine.

245
00:20:54,040 --> 00:21:02,360
There's also in mobile specifically, and I'd say with Android, Google released the Android

246
00:21:02,360 --> 00:21:08,520
NNAPI or neural networking API back in Android O, which we supported from the very beginning

247
00:21:08,520 --> 00:21:10,760
on Snapdragon.

248
00:21:10,760 --> 00:21:17,800
Another time we expect that that API will become the primary, the dominant way, that in Android

249
00:21:17,800 --> 00:21:26,960
if a developer wants to run a built-in application that's running DNNs online, they'll use Android

250
00:21:26,960 --> 00:21:30,240
NN or NNAPI as the primary interface.

251
00:21:30,240 --> 00:21:37,120
That would be, I guess, the first movement toward standardization when it comes to how

252
00:21:37,120 --> 00:21:42,440
you would access different chips in the Android environment.

253
00:21:42,440 --> 00:21:46,720
But I'd say by and large, there's still quite a bit of activity.

254
00:21:46,720 --> 00:21:53,600
I use the term Wild West a lot because it really is the Wild West, and it's exciting.

255
00:21:53,600 --> 00:22:02,000
There's so much activity, the amount of innovation, the collaboration between organizations that

256
00:22:02,000 --> 00:22:09,320
have found themselves to be the most contentious competitors are now collaborating to advance

257
00:22:09,320 --> 00:22:16,200
overall machine learning and artificial intelligence at a pace that I think most of us, we look

258
00:22:16,200 --> 00:22:17,200
at it.

259
00:22:17,200 --> 00:22:22,040
There's no hyperbolic term that could be used that doesn't apply, it actually does apply,

260
00:22:22,040 --> 00:22:24,360
it's moving so fast.

261
00:22:24,360 --> 00:22:33,520
Again, putting the developer head on, it sounds like particularly in the Android world,

262
00:22:33,520 --> 00:22:39,280
which kind of historically suffers with a very high degree of fragmentation.

263
00:22:39,280 --> 00:22:44,320
It sounds like it may still be a frustrating experience to figure out which of these tools

264
00:22:44,320 --> 00:22:52,640
and APIs and things like that I need to use to take advantage of the underlying, all of

265
00:22:52,640 --> 00:22:55,600
the potential underlying hardware possibilities.

266
00:22:55,600 --> 00:22:58,520
Yeah, I guess it depends too on the device class.

267
00:22:58,520 --> 00:23:02,760
In mobile, it's probably a little bit more complicated because there are multitude of

268
00:23:02,760 --> 00:23:06,600
SOCs that are out there that are driving different mobile phones.

269
00:23:06,600 --> 00:23:10,360
That's where Android NN as an API or as an interface may actually solve a lot of that

270
00:23:10,360 --> 00:23:15,480
problem because if you have a common way to interface with the underlying hardware and

271
00:23:15,480 --> 00:23:21,960
each of the SOC manufacturers, chip manufacturers like Qualcomm are doing our job under the hood,

272
00:23:21,960 --> 00:23:27,480
which we have on our side, developing drivers for each of our compute cores that are optimized

273
00:23:27,480 --> 00:23:33,880
specifically for, let's say, NNAPI, then that frustration from a developer standpoint

274
00:23:33,880 --> 00:23:34,880
is minimized.

275
00:23:34,880 --> 00:23:37,240
In fact, it's reduced considerably.

276
00:23:37,240 --> 00:23:42,160
When you deal with proprietary SDKs, they still have an impact and still have a benefit

277
00:23:42,160 --> 00:23:47,480
in a broad mobile environment, but they then become a little bit more, I guess the

278
00:23:47,480 --> 00:23:53,120
priority around those become increases when you get into dedicated devices like connected

279
00:23:53,120 --> 00:23:58,720
cameras or speakers where you know that the SOC that's powering that is the same one

280
00:23:58,720 --> 00:24:06,360
across the board and as a developer, you have less variability so you can be more confident.

281
00:24:06,360 --> 00:24:12,000
But I do think that there's, for at least for the next few years, there's going to continue

282
00:24:12,000 --> 00:24:17,400
to be a lot of experimentation, there'll be a balance between standardization and maybe

283
00:24:17,400 --> 00:24:25,000
a slight degradation in performance versus proprietary software execution and the ultimate performance

284
00:24:25,000 --> 00:24:29,720
that balance will always be there, but the good thing is that there will be choice.

285
00:24:29,720 --> 00:24:36,280
If I go back where we started this conversation and music, now I've tried to develop an analogy

286
00:24:36,280 --> 00:24:42,120
or a comparison when I was doing early on in my college days when I was doing digital

287
00:24:42,120 --> 00:24:47,960
music production, everything was very manual, it was all purpose built, you didn't have

288
00:24:47,960 --> 00:24:53,920
any way to generalize, cut and paste and sample and sampling was actually done with analog

289
00:24:53,920 --> 00:24:59,200
devices like cassette tapes and vinyl records.

290
00:24:59,200 --> 00:25:05,880
Over time, music production software became more advanced and abstracted a lot of that

291
00:25:05,880 --> 00:25:13,200
heavy lifting and purpose built programming that you had to do as a producer and then cut

292
00:25:13,200 --> 00:25:15,880
and paste became kind of the way that you went.

293
00:25:15,880 --> 00:25:21,280
So if you want to replicate a specific phrase, you could do that, just cut and paste, cut

294
00:25:21,280 --> 00:25:26,040
and paste, and now it's so easy to make music as a novice.

295
00:25:26,040 --> 00:25:32,720
All the tools are there at your disposal, it's very, it's democratized, that's probably

296
00:25:32,720 --> 00:25:34,240
the best way to put it.

297
00:25:34,240 --> 00:25:41,560
The pace that we see in, let's say machine learning and AI tools and frameworks like TensorFlow

298
00:25:41,560 --> 00:25:48,760
and PyTorch and the efforts like the OpenNural Network exchange is an example, which is another

299
00:25:48,760 --> 00:25:54,360
way to give a developer the latitude to use whatever framework they wish to express

300
00:25:54,360 --> 00:26:00,200
their model in and not have to worry about what the underlying hardware supports.

301
00:26:00,200 --> 00:26:04,440
Monarchs as an interchange format is one that, again, makes that job a lot easier.

302
00:26:04,440 --> 00:26:10,040
With all these different tools and advancements, I think you're going to see a democratization.

303
00:26:10,040 --> 00:26:15,240
In fact, there's already a democratization of AI happening outside of just the technology

304
00:26:15,240 --> 00:26:21,840
development, like an education, you know, Andrew Ng's courses on Coursera.

305
00:26:21,840 --> 00:26:26,360
There are government entities around the globe that are pushing AI machine learning

306
00:26:26,360 --> 00:26:29,200
education in high school and in college.

307
00:26:29,200 --> 00:26:31,240
It's, it's, it's remarkable.

308
00:26:31,240 --> 00:26:38,080
So I think this short term, there's certainly pain and, you know, a lot of experimentation.

309
00:26:38,080 --> 00:26:42,280
But we're, I think you're already seeing a number of examples where things are consolidating

310
00:26:42,280 --> 00:26:46,800
and I'll use that term, the democratization is already underway.

311
00:26:46,800 --> 00:26:53,360
You made an interesting point earlier in, you know, when I, in kind of progressing through

312
00:26:53,360 --> 00:27:01,560
this conversation, I've been thinking mostly mobile devices, i.e. handsets, smartphones.

313
00:27:01,560 --> 00:27:07,080
But you reference kind of device classes and, quote unquote, IoT devices, whether they're

314
00:27:07,080 --> 00:27:13,880
smart speakers or, you know, by extension, kind of industrial, IoT devices, can you give

315
00:27:13,880 --> 00:27:20,120
us a sense of the relative size of each of those markets from, I don't know what makes

316
00:27:20,120 --> 00:27:25,240
sense like a number of devices or do you have a sense of that?

317
00:27:25,240 --> 00:27:30,840
I don't have, I don't have, you can slice and dice that in a number of ways.

318
00:27:30,840 --> 00:27:35,320
I think the one, I think the, the one data point that keeps getting thrown around when

319
00:27:35,320 --> 00:27:42,360
it comes to specific devices outside of mobile, I think the, the smart speaker market, I've

320
00:27:42,360 --> 00:27:45,600
unfortunately, I have to forgive me, I don't have the number off the top of my head and

321
00:27:45,600 --> 00:27:47,720
I don't want to give you an incorrect number.

322
00:27:47,720 --> 00:27:56,360
But the smart, the smart speaker market driven it primarily by the echo in the Alexa platform,

323
00:27:56,360 --> 00:27:58,600
I think is probably one of the standouts.

324
00:27:58,600 --> 00:28:07,000
Security cameras are very prevalent, especially from an enterprise standpoint, but the smart

325
00:28:07,000 --> 00:28:13,360
speaker is a consumer device is probably the standout category leader, if you will,

326
00:28:13,360 --> 00:28:18,840
on IoT, even any other embedded device outside of mobile.

327
00:28:18,840 --> 00:28:24,040
That particular device, and I have to speak for my own personal, I'll speak for my own

328
00:28:24,040 --> 00:28:29,160
personal view first and then I'll talk about it from an industry standpoint.

329
00:28:29,160 --> 00:28:34,680
I'm one of these individuals that despite my comfort with technology and an audio, which

330
00:28:34,680 --> 00:28:39,320
is really at the core of what I've been focused on most of my career and my personal life,

331
00:28:39,320 --> 00:28:45,680
talking to an inanimate object has been uncomfortable, whether it's a speaker or a mobile phone

332
00:28:45,680 --> 00:28:51,760
or what have you, I see people do it and I had been awkward for me for the longest time,

333
00:28:51,760 --> 00:28:56,640
but what that, you know, echo class device was able to do is kind of break down that wall

334
00:28:56,640 --> 00:29:02,640
between a consumer and the device and create the beginning of what is a relationship.

335
00:29:02,640 --> 00:29:06,680
And I think the relationship that we will have with the devices around us will be based

336
00:29:06,680 --> 00:29:13,680
in large part on voice and how, you know, not just about command and control, where you

337
00:29:13,680 --> 00:29:23,000
know, ask Alexa or Cortana or Siri or what have you, for a to provide something, so make

338
00:29:23,000 --> 00:29:30,640
a request, over time that's going to become more real, more lifelike, today it's very,

339
00:29:30,640 --> 00:29:36,640
it's still a little bit, I don't say awkward, but it's not, it's not like you're talking

340
00:29:36,640 --> 00:29:41,200
to a human being, you know you're talking to a device, whether that device or whether

341
00:29:41,200 --> 00:29:48,240
the intelligence is in the cloud or on the device, you know that it's not, it's not a

342
00:29:48,240 --> 00:29:53,400
companion, it's not somebody you have a relationship with, but the movements toward

343
00:29:53,400 --> 00:29:58,000
making a lot of that work happen on the device, a lot of the algorithm processing on the

344
00:29:58,000 --> 00:30:04,000
device is going to allow for that conversation to become really more of a conversation.

345
00:30:04,000 --> 00:30:12,200
And the gaps between the device providing a response or are you talking to the device,

346
00:30:12,200 --> 00:30:16,840
those gaps will be reduced or lower latency, but then also being able to detect more about

347
00:30:16,840 --> 00:30:22,560
what you as an individual or how you feel, like what is it in your voice that the device

348
00:30:22,560 --> 00:30:30,240
itself or the algorithm can detect, so sentiment, like are you happy or you sad, angry, and

349
00:30:30,240 --> 00:30:38,680
then being able to provide even richer, a richer response and maybe even proactively engage.

350
00:30:38,680 --> 00:30:44,880
So that category and without providing a, without providing a number, which again, I don't

351
00:30:44,880 --> 00:30:49,280
want to, I don't want to call attention to a number that may be incorrect because I

352
00:30:49,280 --> 00:30:54,480
don't have that off the top of my head, but I think that device category specifically

353
00:30:54,480 --> 00:31:01,040
and the size of it, which is I think the largest of all IoT devices, that's going to be one

354
00:31:01,040 --> 00:31:04,560
of the most interesting ones to watch for all the reasons that I just mentioned.

355
00:31:04,560 --> 00:31:09,840
I think that that interaction, that real relationship, the companionship that you're going

356
00:31:09,840 --> 00:31:13,320
to have with a device over time is going to be largely driven by voice.

357
00:31:13,320 --> 00:31:15,040
That's why that category is so hot.

358
00:31:15,040 --> 00:31:16,040
It's interesting.

359
00:31:16,040 --> 00:31:26,440
We thought about the lack of the ability to do robust on device, audio processing, AI

360
00:31:26,440 --> 00:31:32,760
audio processing as a key limitation of the user experience, but now that you say

361
00:31:32,760 --> 00:31:37,200
that I totally get it, like you say something to one of these devices, it kind of you're

362
00:31:37,200 --> 00:31:41,000
waiting for a long time, and a lot of that is just kind of the latency of talking to

363
00:31:41,000 --> 00:31:44,600
a cloud and having that then have to come back.

364
00:31:44,600 --> 00:31:45,600
Exactly.

365
00:31:45,600 --> 00:31:51,160
I don't know how many times your internet's going down at your house, but if you have

366
00:31:51,160 --> 00:31:56,520
one of these devices, the minute you ask for something, you get a response, sorry, I

367
00:31:56,520 --> 00:31:57,520
can't help you right now.

368
00:31:57,520 --> 00:31:59,280
It's like, well, you should be able to help me.

369
00:31:59,280 --> 00:32:05,960
You should at least engage with me in such a way that it minimizes that frustration.

370
00:32:05,960 --> 00:32:08,880
There's enough processing capability.

371
00:32:08,880 --> 00:32:13,680
The algorithms are becoming far more efficient to run on device.

372
00:32:13,680 --> 00:32:19,560
You can handle more keywords, natural language processing, and overall voice UI as a category.

373
00:32:19,560 --> 00:32:21,920
There's so much movement there.

374
00:32:21,920 --> 00:32:27,440
There's no reason why the device could not provide more utility and that level of personal

375
00:32:27,440 --> 00:32:33,200
interaction moving forward, and it'll just get better over time.

376
00:32:33,200 --> 00:32:34,200
Yeah.

377
00:32:34,200 --> 00:32:35,720
I imagine that's coming very soon.

378
00:32:35,720 --> 00:32:39,320
There's no reason why the device shouldn't at least be able to turn off the lights or something

379
00:32:39,320 --> 00:32:41,320
like that when it's disconnected.

380
00:32:41,320 --> 00:32:50,200
So maybe taking a bit of a step back, one of the first things I mentioned was the Snapdragon

381
00:32:50,200 --> 00:32:57,560
855 platform launch you did back in December.

382
00:32:57,560 --> 00:33:05,320
I kind of want to use that as an on-trade of talking about what these platforms mean.

383
00:33:05,320 --> 00:33:12,880
And specifically, when we, you mentioned some of the components of a platform like the

384
00:33:12,880 --> 00:33:21,160
CPU GPU, hexagon, AI accelerator, I want to get a mental model for these sound like in

385
00:33:21,160 --> 00:33:26,760
some ways like overlapping components in terms of functionality, or at least they could

386
00:33:26,760 --> 00:33:31,000
be used in overlapping ways.

387
00:33:31,000 --> 00:33:37,320
What are the different pieces and how to developers, how should developers or users think about

388
00:33:37,320 --> 00:33:41,760
using them and what directions are each of them going?

389
00:33:41,760 --> 00:33:48,000
So if we use the Snapdragon 855 introduction as the springboard for this one, Snapdragon

390
00:33:48,000 --> 00:33:54,040
855 is what we consider to have our fourth generation AI engine.

391
00:33:54,040 --> 00:33:59,360
I mentioned the AI engine as being the kind of the sound of the parts that make on-device

392
00:33:59,360 --> 00:34:01,960
machine learning acceleration possible.

393
00:34:01,960 --> 00:34:06,680
In Snapdragon 855, we've done a number of things to all the compute elements.

394
00:34:06,680 --> 00:34:13,560
We've added more arithmetic logic units to our CPUs, about 50 percent more.

395
00:34:13,560 --> 00:34:21,040
On the CPU side, we've incorporated dot product instructions that increase AI performance

396
00:34:21,040 --> 00:34:26,680
specifically at 8-bit fixed by four times or 4x.

397
00:34:26,680 --> 00:34:28,880
And then on the hexagon side.

398
00:34:28,880 --> 00:34:35,200
If you followed our path over these fast forward generations, hexagon has had a vector

399
00:34:35,200 --> 00:34:40,720
processor where we at least we've had the hexagon vector processor in our chips going

400
00:34:40,720 --> 00:34:46,560
all the way back from Snapdragon 820 to 835, 845, and now 855.

401
00:34:46,560 --> 00:34:51,680
And that's been one of the primary engines for doing on-device AI.

402
00:34:51,680 --> 00:34:58,600
So that we actually doubled the size or doubled the number of hexagon vector extensions

403
00:34:58,600 --> 00:35:03,120
as we call them between 845 and 855.

404
00:35:03,120 --> 00:35:08,280
And then we added for the first time a dedicated AI accelerator.

405
00:35:08,280 --> 00:35:10,360
We call it a tensor accelerator.

406
00:35:10,360 --> 00:35:19,280
It's part of the hexagon family and its sole purpose is to process DNNs on device.

407
00:35:19,280 --> 00:35:26,200
The hexagon vector extensions had been the prior in prior generations had been a primary

408
00:35:26,200 --> 00:35:31,800
engine for that, but vector extensions also have utility and doing vision processing

409
00:35:31,800 --> 00:35:33,280
and other compute functions.

410
00:35:33,280 --> 00:35:38,320
So it wasn't dedicated solely to AI.

411
00:35:38,320 --> 00:35:44,040
But even with the addition of this tensor accelerator, that does not mean that the rest of the

412
00:35:44,040 --> 00:35:50,560
family of compute engines doesn't participate to your point, to your question, are there

413
00:35:50,560 --> 00:35:55,960
specific reasons why you would want to use one engine versus another or one compute

414
00:35:55,960 --> 00:35:59,520
element versus another to run an AI algorithm?

415
00:35:59,520 --> 00:36:03,720
And the answer goes back to something, I think I mentioned earlier, and it has to do with

416
00:36:03,720 --> 00:36:10,360
power and performance and what you want to get out of that particular user experience

417
00:36:10,360 --> 00:36:13,280
or how you want that user experience to perform.

418
00:36:13,280 --> 00:36:16,400
Also, what else is happening in the system?

419
00:36:16,400 --> 00:36:24,880
So if the phone is doing other tasks or as part of that user experience, you're doing

420
00:36:24,880 --> 00:36:33,080
graphics processing or you need the display to be highly performant.

421
00:36:33,080 --> 00:36:38,280
You may want to choose a different place to run an AI algorithm if the GPU has a higher

422
00:36:38,280 --> 00:36:43,560
workload or the same is true for the vector extensions.

423
00:36:43,560 --> 00:36:48,400
You may want to have a little bit more flexibility in where you run that AI algorithm.

424
00:36:48,400 --> 00:36:54,680
And then from a pure AI standpoint, what we've done to combine or in the system where

425
00:36:54,680 --> 00:37:01,880
combining hexagon vector extensions and our new tensor accelerator is a developer, you

426
00:37:01,880 --> 00:37:06,360
can utilize both of those, which gives you a kind of a balance between what would be

427
00:37:06,360 --> 00:37:13,360
programmable AI processing in the form of the vector extensions and then your dedicated

428
00:37:13,360 --> 00:37:16,040
acceleration with the tensor accelerator.

429
00:37:16,040 --> 00:37:21,040
You can combine those two to give, I guess I'll say I don't like to use the term ultimate,

430
00:37:21,040 --> 00:37:25,840
but the best performance possible by combining those two compute elements.

431
00:37:25,840 --> 00:37:32,840
But again, taking one step back, we don't look at AI as a one-size-fits-all problem,

432
00:37:32,840 --> 00:37:38,400
or at least it's not one-size-fits-all, so a one-compute architecture is not going to

433
00:37:38,400 --> 00:37:44,880
solve all the problems that you see or satisfy all the use cases that you see on device.

434
00:37:44,880 --> 00:37:48,840
It's just we're not at a stage and I don't know if we'll ever get to that stage to be

435
00:37:48,840 --> 00:37:50,640
quite honest.

436
00:37:50,640 --> 00:37:56,320
But at least with A55, we've provided a balance between what is our standard portfolio of

437
00:37:56,320 --> 00:38:02,120
compute with different programming capabilities and then this new dedicated accelerator, again

438
00:38:02,120 --> 00:38:07,240
each with a different power and performance profile to solve for the various use cases

439
00:38:07,240 --> 00:38:14,520
and tolerances that the developer has for the, when they're developing those features.

440
00:38:14,520 --> 00:38:20,400
In addition, and I guess I'll build on top of that, there's a big movement in the industry

441
00:38:20,400 --> 00:38:26,000
across the board, whether it's in, whether it's in the data center or it's on device,

442
00:38:26,000 --> 00:38:31,560
but dedicated acceleration, dedicated AI processors, there's various terms that are used.

443
00:38:31,560 --> 00:38:37,760
There's NPU for neural processing or there's VPU or IPU.

444
00:38:37,760 --> 00:38:43,880
All of these, there's a very big movement in that respect across the board in mobile,

445
00:38:43,880 --> 00:38:49,720
in IoT and automotive, and there is a demand for that and a need for that today.

446
00:38:49,720 --> 00:38:52,880
But that doesn't mean that that's where everything's going to go.

447
00:38:52,880 --> 00:38:58,840
That doesn't mean that a dedicated processor, dedicated processor is the answer to all the

448
00:38:58,840 --> 00:39:03,680
problems that you're going to see or satisfying all the use cases that you're going to see

449
00:39:03,680 --> 00:39:05,240
in various device classes.

450
00:39:05,240 --> 00:39:10,640
You do need a wide portfolio of compute to accommodate the level of variability.

451
00:39:10,640 --> 00:39:15,120
That is certainly there today and it's going to be there to, it'll be there for the foreseeable

452
00:39:15,120 --> 00:39:16,120
future.

453
00:39:16,120 --> 00:39:25,560
When you speak about these dedicated processors, does the tensor accelerator, for example, fit

454
00:39:25,560 --> 00:39:31,480
into that or are you specifically thinking about kind of an off board, off SOC processor?

455
00:39:31,480 --> 00:39:37,680
No, I think the tensor accelerator is an example of that.

456
00:39:37,680 --> 00:39:41,040
If you look at dedicated acceleration, there actually are two different vectors.

457
00:39:41,040 --> 00:39:45,400
One is embedded, which would be in the SOC or in the chip.

458
00:39:45,400 --> 00:39:55,440
In mobile, Qualcomm and others are or have introduced dedicated acceleration.

459
00:39:55,440 --> 00:40:01,400
Some of those are kind of reworked digital signal processors.

460
00:40:01,400 --> 00:40:09,840
In our case, we have a ground up architecture or from the ground up custom built IP.

461
00:40:09,840 --> 00:40:13,120
It's specifically designed for DNN processing.

462
00:40:13,120 --> 00:40:17,360
In tensor accelerator, but hexagon is kind of the reworked DSP.

463
00:40:17,360 --> 00:40:25,040
Yes, so hexagon, yeah, hexagon is a, it does actually a number of DSPs in the hexagon

464
00:40:25,040 --> 00:40:26,040
portfolio.

465
00:40:26,040 --> 00:40:30,360
There's a modem DSP, there's a compute DSP.

466
00:40:30,360 --> 00:40:34,560
In the past, hexagon has been associated with digital signal processing and even the

467
00:40:34,560 --> 00:40:39,400
vector extensions are part of our compute DSP.

468
00:40:39,400 --> 00:40:45,880
But the tensor accelerator is a totally unique architecture.

469
00:40:45,880 --> 00:40:53,840
In the market today, there are embedded accelerators in SOCs, in ours and others.

470
00:40:53,840 --> 00:41:00,560
There are also folks that are developing kind of off-chip dedicated AI chips that just

471
00:41:00,560 --> 00:41:03,680
do AI processing.

472
00:41:03,680 --> 00:41:08,880
They can augment what is on the primary application processor where they can be used independently.

473
00:41:08,880 --> 00:41:16,200
But there's again, going back to the, there's such a big movement and innovation is happening

474
00:41:16,200 --> 00:41:22,000
so fast that I guess the more, the bigger the sandbox that you provide a developer, the

475
00:41:22,000 --> 00:41:23,960
more they'll use it.

476
00:41:23,960 --> 00:41:30,440
In the case of mobile, there's a balance between what you provide in terms of overall compute,

477
00:41:30,440 --> 00:41:33,720
the variety of compute and the cost associated with it.

478
00:41:33,720 --> 00:41:41,040
You have less latitudes when it comes to kind of a primary SOC like Snapdragon to provide

479
00:41:41,040 --> 00:41:47,080
a massive AI accelerator and there's really not any need to in mobile, but there might

480
00:41:47,080 --> 00:41:52,080
be other instances where you do need a pure dedicated AI processing and quite a bit of

481
00:41:52,080 --> 00:41:55,160
it outside of mobile.

482
00:41:55,160 --> 00:42:03,480
On the software side of things, thinking about software platforms, I would call that

483
00:42:03,480 --> 00:42:11,480
kind of idea like a very unopinionated platform versus platforms that have very, this is the

484
00:42:11,480 --> 00:42:15,560
way to do it, very, very strong opinions.

485
00:42:15,560 --> 00:42:21,160
Do you think there's that analogy apply here and there are some implications of that on

486
00:42:21,160 --> 00:42:23,560
the software side.

487
00:42:23,560 --> 00:42:30,400
Oftentimes, developers want more opinionated platforms because they, they're less choices

488
00:42:30,400 --> 00:42:31,880
that they have to make.

489
00:42:31,880 --> 00:42:36,120
Is that analogy apply and what are the implications of it here?

490
00:42:36,120 --> 00:42:42,020
Yeah, I think if you had a going back in our conversation talking about developer pain

491
00:42:42,020 --> 00:42:49,680
points and what makes the developer's life easier difficult.

492
00:42:49,680 --> 00:42:56,040
We're not seeing a consistency across the board because there's still a ramping up of

493
00:42:56,040 --> 00:42:57,040
expertise.

494
00:42:57,040 --> 00:43:03,040
When we started this, I'll just speak from where I sit from four years ago when I began

495
00:43:03,040 --> 00:43:12,680
this process in this project in Qualcomm, developer savvy was, well, there wasn't much.

496
00:43:12,680 --> 00:43:17,920
A lot of abstraction and providing what would be turnkey solutions like an object classifier

497
00:43:17,920 --> 00:43:23,080
or image classifier or seeing classifier was what we started off with because nobody

498
00:43:23,080 --> 00:43:27,400
really understood how to do the job.

499
00:43:27,400 --> 00:43:32,200
They could train a model, but they didn't know where they had to run it on target.

500
00:43:32,200 --> 00:43:40,160
We've gone so far ahead or moved so far ahead just in four years that now developers are

501
00:43:40,160 --> 00:43:41,160
very savvy.

502
00:43:41,160 --> 00:43:42,160
The tools are there.

503
00:43:42,160 --> 00:43:44,400
There's a variety of them.

504
00:43:44,400 --> 00:43:52,000
The education is there and I think the desire across the desire from many of the developers

505
00:43:52,000 --> 00:43:56,720
that we deal with today and customers that we have today, they want more flexibility.

506
00:43:56,720 --> 00:43:59,760
They want more of an ability to experiment.

507
00:43:59,760 --> 00:44:08,920
They don't want to be stuck in a box and focus on a very specific programming method or

508
00:44:08,920 --> 00:44:13,160
way to access a particular compute engine.

509
00:44:13,160 --> 00:44:15,320
They want more variability.

510
00:44:15,320 --> 00:44:17,480
They want more flexibility.

511
00:44:17,480 --> 00:44:20,760
The openness is becoming more prevalent.

512
00:44:20,760 --> 00:44:25,600
It's not to say that proprietary tools and being very fixed in the approach to, let's

513
00:44:25,600 --> 00:44:31,120
say, running a natural language processing algorithm or something on a purpose-built

514
00:44:31,120 --> 00:44:38,240
device isn't helpful, but we are actually seeing more developers looking for choice and

515
00:44:38,240 --> 00:44:40,440
flexibility and modularity.

516
00:44:40,440 --> 00:44:44,200
They want to get closer to the metal when it comes to programming and they'd like to

517
00:44:44,200 --> 00:44:48,560
be able to do it in multiple ways.

518
00:44:48,560 --> 00:44:51,440
That's a trend, but I wouldn't say that that's a dominant trend.

519
00:44:51,440 --> 00:44:57,160
I think that's still, there's nothing that settles out to say that a proprietary approach

520
00:44:57,160 --> 00:45:01,720
and a fixed approach is the best, maybe over time that will be depending on the use case

521
00:45:01,720 --> 00:45:06,840
for the device class, but I still think it's highly variable at this point.

522
00:45:06,840 --> 00:45:11,640
We're speaking very early in January, it'll be a couple of weeks before folks hear this,

523
00:45:11,640 --> 00:45:16,320
but we're at the very beginning of 2019.

524
00:45:16,320 --> 00:45:23,440
What do you see happening in the space in 2019 and beyond?

525
00:45:23,440 --> 00:45:29,240
There are a couple of areas that I think we're looking at, and we see kind of emerging

526
00:45:29,240 --> 00:45:30,240
over time.

527
00:45:30,240 --> 00:45:37,320
One is the area of on-device learning, so today the lion's share of all the workloads that

528
00:45:37,320 --> 00:45:43,200
we see in mobile and in other devices, it's all inference, so it's the application or

529
00:45:43,200 --> 00:45:48,160
the utilization of a train model running on target to deliver that use case or solve

530
00:45:48,160 --> 00:45:49,160
that problem.

531
00:45:49,160 --> 00:45:56,000
There's very little learning happening, but we see a kind of a movement toward taking

532
00:45:56,000 --> 00:46:00,160
a train model, having it run on target, but also being able to take in data from the

533
00:46:00,160 --> 00:46:06,520
various sensors on device and augment that model to become more aware or contentionally

534
00:46:06,520 --> 00:46:12,840
aware of its environment to provide more personalization for consumers.

535
00:46:12,840 --> 00:46:16,760
Many of different techniques that are being explored in the industry, there's reinforcement

536
00:46:16,760 --> 00:46:24,400
learning and others, but I think that's going to be a trend that we see in 2019.

537
00:46:24,400 --> 00:46:28,840
Hardware acceleration or dedicated hardware for AI processing will continue to be a big

538
00:46:28,840 --> 00:46:41,160
movement, both in the embedded side as well as off-chip standalone AI processors.

539
00:46:41,160 --> 00:46:45,880
Unchmarking is probably one of the areas that I've mentioned Wild West a minute ago, but

540
00:46:45,880 --> 00:46:48,880
I think that certainly applies here.

541
00:46:48,880 --> 00:46:53,120
There's a lot of, I guess, confusion in the marketplace today when it comes to what

542
00:46:53,120 --> 00:46:59,280
is, first off, what is AI, and then secondarily, how do you measure its effectiveness?

543
00:46:59,280 --> 00:47:05,880
How do you measure the performance of running a particular convolutional neural network

544
00:47:05,880 --> 00:47:12,680
for a particular use case, or a specific network type or class like a ResNet or VGG or

545
00:47:12,680 --> 00:47:13,680
what have you?

546
00:47:13,680 --> 00:47:19,000
How do you actually measure that in a real-world setting and then compare hardware platforms

547
00:47:19,000 --> 00:47:20,400
to each other?

548
00:47:20,400 --> 00:47:26,080
Right now, there's a variety of entities that have sprung up to try to tackle that problem,

549
00:47:26,080 --> 00:47:34,440
but there's no consistency with methodology or formula or the underlying network classes

550
00:47:34,440 --> 00:47:38,680
that are used to actually do the measurement and ultimately what the benefit is.

551
00:47:38,680 --> 00:47:43,360
What is the final outcome of this and why does it matter?

552
00:47:43,360 --> 00:47:48,520
Benchmarks and graphics and CPU and in other cases are all very well settled, and there's

553
00:47:48,520 --> 00:47:54,400
still a lot of gamesmanship that goes on in those categories, but specifically in AI,

554
00:47:54,400 --> 00:48:02,920
that's an area that is quite fluid and it changes, I think, day-to-day.

555
00:48:02,920 --> 00:48:07,040
We see some of that settling out this year, and it would be good because it will help

556
00:48:07,040 --> 00:48:14,880
with OEMs making choices about who to pick as their SAC vendor, and it will help consumers

557
00:48:14,880 --> 00:48:19,320
understand a little bit better what is the benefit of what's happening on the device?

558
00:48:19,320 --> 00:48:20,320
Why does this matter?

559
00:48:20,320 --> 00:48:26,520
Why is this making my life more interesting and compelling and enjoyable?

560
00:48:26,520 --> 00:48:31,960
And then the last one, I think, is the movement that we're pushing very hard and something

561
00:48:31,960 --> 00:48:35,520
that we've been leading the charge on for a few years is 5G.

562
00:48:35,520 --> 00:48:40,680
So 5G connectivity isn't just going to make that connection between the data center and

563
00:48:40,680 --> 00:48:48,920
the device more efficient and lower latency, but devices will be able to share information

564
00:48:48,920 --> 00:48:52,000
with each other in ways that they haven't previously.

565
00:48:52,000 --> 00:48:59,120
And the combination of 5G and AI, albeit 5G, will be at the starting point in 2019, you'll

566
00:48:59,120 --> 00:49:06,640
probably see developers trying to take advantage of that connectivity platform in a way that

567
00:49:06,640 --> 00:49:10,280
they haven't in the past and being able to make devices connect to each other in a more

568
00:49:10,280 --> 00:49:11,760
intelligent way.

569
00:49:11,760 --> 00:49:16,280
We see a big future with a combination of 5G and AI, in fact, that's where we're spending

570
00:49:16,280 --> 00:49:19,200
most of our time and effort these days at Qualcomm.

571
00:49:19,200 --> 00:49:23,920
One of the things you mentioned kind of peak my interest in the context of on-device learning

572
00:49:23,920 --> 00:49:29,640
is reinforcement learning. I typically think of that as like running lots of simulations

573
00:49:29,640 --> 00:49:34,720
taking a long time, which doesn't seem like something that I'd necessarily want to do

574
00:49:34,720 --> 00:49:42,040
on a device. Are you aware of specific things that are happening to address that in the

575
00:49:42,040 --> 00:49:44,520
context of on-device learning?

576
00:49:44,520 --> 00:49:48,280
I don't have any examples to share with you now and you're right that a lot of what's

577
00:49:48,280 --> 00:49:53,440
happening today and we look at different processes that we're trying to explore, like, reinforcement

578
00:49:53,440 --> 00:50:03,920
learning, usage, off-target or not on-device, but improving processes that would end up

579
00:50:03,920 --> 00:50:09,120
running on the device. I don't have any examples and I just pulled that out as one potential,

580
00:50:09,120 --> 00:50:17,760
but nothing specifically that I could point to. But on-device learning specifically, the

581
00:50:17,760 --> 00:50:25,800
desire or the demand from developers and even our OEM partners to be able to take better

582
00:50:25,800 --> 00:50:31,600
advantage of the incoming data from the various sensors on the device, you know, microphone

583
00:50:31,600 --> 00:50:36,960
and camera and accelerometer to be able to provide a more personalized experience in

584
00:50:36,960 --> 00:50:42,000
general. There's a there's high demand there and I think you're going to see, you'll

585
00:50:42,000 --> 00:50:47,280
see more effort, maybe some experimentation, but there'll be more emphasis place on trying

586
00:50:47,280 --> 00:50:56,240
to make the device itself, not just a kind of fix with a specific intelligence level,

587
00:50:56,240 --> 00:51:01,160
but one that would be heightened by way of additional context that it's able to grok

588
00:51:01,160 --> 00:51:04,400
from the world around it. Well, Gary, thanks so much for taking

589
00:51:04,400 --> 00:51:09,480
the time to chat with us. I really learned a lot and I enjoyed the conversation.

590
00:51:09,480 --> 00:51:14,000
Yeah, thanks very much, Sam. I appreciate it and thanks for let me share your birthday

591
00:51:14,000 --> 00:51:19,200
with you. Awesome. Thanks. Take care. You too.

592
00:51:19,200 --> 00:51:27,040
All right, everyone. That's our show for today. For more information on Gary or any of

593
00:51:27,040 --> 00:51:33,760
the topics covered in this show, visit twimmalei.com slash talk slash 223. Be sure to check out

594
00:51:33,760 --> 00:51:40,880
what Qualcomm is up to at twimmalei.com slash Qualcomm. As always, thanks so much for listening

595
00:51:40,880 --> 00:51:47,880
and catch you next time.

