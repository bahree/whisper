WEBVTT

00:00.000 --> 00:16.320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

00:16.320 --> 00:21.480
people, doing interesting things in machine learning and artificial intelligence.

00:21.480 --> 00:31.480
I'm your host Sam Charrington.

00:31.480 --> 00:36.320
Today's episode is part of a series of shows on the topic of AI for the benefit of society

00:36.320 --> 00:39.760
that were excited to have partnered with Microsoft to produce.

00:39.760 --> 00:43.480
In this show, we're joined by Lucas Joplin and Zach Parisa.

00:43.480 --> 00:48.040
Lucas is the chief environmental officer and Microsoft, spearheading the company's

00:48.040 --> 00:52.880
five-year $50 million AI for Earth commitment, which seeks to apply machine learning and

00:52.880 --> 01:00.160
artificial intelligence across four key environmental areas, agriculture, water, biodiversity, and climate

01:00.160 --> 01:01.400
change.

01:01.400 --> 01:06.600
Zach is co-founder and president of Sylvia Terra, a Microsoft AI for Earth grantee whose mission

01:06.600 --> 01:12.720
is to help people use modern data sources to better manage forest habitats and ecosystems.

01:12.720 --> 01:16.680
In our conversation, we discussed the ways that machine learning and AI can be used to

01:16.680 --> 01:23.200
advance our understanding of forests and other ecosystems and support conservation efforts.

01:23.200 --> 01:27.360
We discussed how Sylvia Terra uses computer vision and data from a wide array of sensors

01:27.360 --> 01:32.960
like LIDAR, combined with AI, to yield more detailed small area estimates of the various

01:32.960 --> 01:35.080
species in our forests.

01:35.080 --> 01:39.800
And we also discussed another AI for Earth project, Wild Me, a computer vision based

01:39.800 --> 01:46.840
wild life conservation project that we discussed with Jason Holmberg back in episode 166.

01:46.840 --> 01:51.520
Before diving in, I'd like to thank Microsoft for their support of the show and their sponsorship

01:51.520 --> 01:54.120
of this series.

01:54.120 --> 01:58.160
Microsoft is committed to ensuring the responsible development and use of AI and is empowering

01:58.160 --> 02:03.400
people around the world with this intelligent technology to help solve previously intractable

02:03.400 --> 02:09.960
societal challenges, spanning sustainability, accessibility, and humanitarian action.

02:09.960 --> 02:14.040
Learn more about their plan at Microsoft.ai.

02:14.040 --> 02:15.880
Enjoy the show.

02:15.880 --> 02:19.400
Alright, everyone.

02:19.400 --> 02:25.600
I am here with Lucas Joppa and Zach Parissa.

02:25.600 --> 02:27.560
Lucas is the CEO of Microsoft.

02:27.560 --> 02:33.000
No, not that CEO, but the Chief Environmental Officer.

02:33.000 --> 02:36.760
And Zach is the co-founder and president of Sylvia Terra.

02:36.760 --> 02:40.040
Lucas and Zach, welcome to this Week in Machine Learning and AI.

02:40.040 --> 02:41.040
Thanks for having us here.

02:41.040 --> 02:42.040
It's a huge pleasure.

02:42.040 --> 02:43.040
Great to be here.

02:43.040 --> 02:44.040
Awesome.

02:44.040 --> 02:45.800
So let's dive right in.

02:45.800 --> 02:51.480
We'll be talking about Microsoft's AI for Earth initiative.

02:51.480 --> 02:58.280
But before we jump into that, Lucas, as the CEO of Microsoft, I think I'm going to run

02:58.280 --> 03:03.280
this one all day.

03:03.280 --> 03:06.560
Tell me a little bit about your background on how you came to be the CEO of Microsoft.

03:06.560 --> 03:07.560
Yeah, sure.

03:07.560 --> 03:10.560
So I would say I never dreamed of being the CEO of anything.

03:10.560 --> 03:11.560
That's for sure.

03:11.560 --> 03:17.240
Particularly in the standard context of it, much less what it means in my specific title

03:17.240 --> 03:18.560
is the Chief Environmental Officer.

03:18.560 --> 03:22.760
I mean, I grew up kind of in far northern rural Wisconsin.

03:22.760 --> 03:24.600
I was obsessed with being outside.

03:24.600 --> 03:29.960
And my approach to kind of school and life in general was, how can I get done with anything

03:29.960 --> 03:32.840
that I need to get done with so I can go play out in the woods?

03:32.840 --> 03:41.000
I think I thought I was going to grow to be a game warden or something similar to that.

03:41.000 --> 03:44.800
And technology was not a big factor in my life as well.

03:44.800 --> 03:49.720
I mean, I've never had a computer growing up or a TV or anything else.

03:49.720 --> 03:56.960
And I eventually found my way into university, started discovering that I was really interested

03:56.960 --> 04:03.000
in thinking about a career in environmental science, studied wildlife ecology, again,

04:03.000 --> 04:06.760
not the traditional career path for somebody at Microsoft.

04:06.760 --> 04:11.220
One often spent a little time in the United States Peace Corps in Malawi working for the

04:11.220 --> 04:13.240
Department of National Parks and Wildlife.

04:13.240 --> 04:15.400
Then came back and did my PhD in ecology.

04:15.400 --> 04:21.080
And it was really then that I kind of started to put together this, the two kind of incredible

04:21.080 --> 04:27.000
ages that I think we're alive in today and the way kind of ICR world, which is that we're

04:27.000 --> 04:29.600
doing business here at the intersection of the information age.

04:29.600 --> 04:36.120
And then this also incredible age of negative human impacts on Earth's natural systems.

04:36.120 --> 04:42.080
And it was during my PhD, I just was really struggling with what's the right way to do science

04:42.080 --> 04:45.440
at a way that scales with the scale of the problem.

04:45.440 --> 04:49.680
And that's when computing, programming, machine learning, all kind of came flooding into my

04:49.680 --> 04:55.240
life at the same time and it up at Microsoft and Microsoft Research leading programs in

04:55.240 --> 04:59.720
environmental and computer science and then things just progressed from there.

04:59.720 --> 05:05.880
You actively involved in academic research and a number of organizations, can you share

05:05.880 --> 05:06.880
a little bit about that?

05:06.880 --> 05:08.920
We talked about it a bit earlier.

05:08.920 --> 05:15.400
Yeah, sure, once you live long enough in the academic world, the kind of the Pavlovian

05:15.400 --> 05:21.280
response towards some of the rewards that that environment installs in you, I mean, I'm

05:21.280 --> 05:25.480
not proud to say it, but since I'm not proud, I should just say it, I'm still that academic

05:25.480 --> 05:31.760
that checks their citations every day when I wake up over breakfast.

05:31.760 --> 05:37.160
So while I definitely have a much larger and more expanded purview of roles and responsibilities

05:37.160 --> 05:43.640
here at Microsoft, I still think, you know, science is important, science is what drives

05:43.640 --> 05:48.800
all of the kind of environmental sustainability decisions that we make here at this company.

05:48.800 --> 05:55.240
It's what ultimately led to why we invested in this program AI for Earth.

05:55.240 --> 05:59.560
And I firmly believe that you have to understand the details.

05:59.560 --> 06:03.400
If you're going to, if you're going to try to lead an organization somewhere with a big

06:03.400 --> 06:08.080
picture of vision, if you don't understand the details, if you don't understand the science,

06:08.080 --> 06:09.400
then it's difficult to do that.

06:09.400 --> 06:13.440
And just the way my brain works, the easiest way to understand the details is to kind of

06:13.440 --> 06:17.400
get your hands dirty and be in there with the rest of the world kind of trying to build

06:17.400 --> 06:18.800
the solutions of the future.

06:18.800 --> 06:21.080
And so that's where academic research for me comes in.

06:21.080 --> 06:26.440
It's just that opportunity to actually like go really deep and work on kind of both sides

06:26.440 --> 06:27.440
of the equation.

06:27.440 --> 06:29.160
I still publish in the environmental science literature.

06:29.160 --> 06:35.520
I still publish in the computer science literature and, you know, the most depressing thing about

06:35.520 --> 06:40.400
that is how few of us there are that do both of those things.

06:40.400 --> 06:46.160
And you know, it's one of the things that I spend a lot of my time every day doing is

06:46.160 --> 06:49.120
just trying to bring those two worlds together.

06:49.120 --> 06:52.960
And so yeah, and publishing is a fantastic way to do that.

06:52.960 --> 06:54.960
And Zach, you're a forester.

06:54.960 --> 06:55.960
Yeah.

06:55.960 --> 06:56.960
Yeah.

06:56.960 --> 06:58.960
I didn't know that was a thing beyond the Subaru.

06:58.960 --> 06:59.960
Right.

06:59.960 --> 07:00.960
Sure enough.

07:00.960 --> 07:01.960
Yeah.

07:01.960 --> 07:04.000
It is absolutely a thing.

07:04.000 --> 07:08.280
And kind of an exciting, I think, there's kind of a rebirth in forestry now.

07:08.280 --> 07:13.760
And so I'm hoping that it'll become a more broadly known thing here before too long.

07:13.760 --> 07:16.840
But tell us about your background and about Soviet terror.

07:16.840 --> 07:17.840
Yeah.

07:17.840 --> 07:18.840
Sure.

07:18.840 --> 07:24.640
So my, the start of my story actually isn't terribly dissimilar than Lucas's.

07:24.640 --> 07:29.760
So I grew up in North Alabama, though, not Wisconsin, but in a, in kind of this funny place

07:29.760 --> 07:36.800
that was like North Alabama's, you know, covered in woods, but it also has a NASA installation

07:36.800 --> 07:38.120
in Huntsville, Alabama.

07:38.120 --> 07:41.360
And my youth was basically just spent in the woods.

07:41.360 --> 07:45.680
When I, when I was in first grade, I wanted to be an entomologist.

07:45.680 --> 07:48.960
When I was in third grade, I wanted to be a zoologist.

07:48.960 --> 07:55.080
I went through, you know, geology and so on and so forth until I finally met somebody

07:55.080 --> 07:56.160
who was a forester.

07:56.160 --> 07:59.720
And I, you know, until you meet somebody and you have somebody sort of walk you through

07:59.720 --> 08:02.400
what that is, it's, it's kind of an obscure field.

08:02.400 --> 08:09.480
And what that is to me is sort of the confluence of economics and ecology for me.

08:09.480 --> 08:13.640
And so it was this brilliant opportunity at the time where it lets the way that I saw

08:13.640 --> 08:19.280
it because it brought together everything that I cared about, you know, from the ecology

08:19.280 --> 08:26.400
side, you know, insects and soils, geology, the interconnected nature of all of those,

08:26.400 --> 08:28.080
all of those systems.

08:28.080 --> 08:34.200
And, but also the economic side, sort of, you know, not only what, what the forest is,

08:34.200 --> 08:40.000
but also what we want it to be and how we value that as a society and how we mean to take

08:40.000 --> 08:46.040
it from one place now, which is where we find it today, to where we want it to be and

08:46.040 --> 08:49.880
sort of what we, you know, what we believe we need.

08:49.880 --> 08:52.880
And so that was, that was my entrance into it.

08:52.880 --> 08:55.800
And I believed I would carry that out.

08:55.800 --> 09:03.200
I would, I would live and work as a forester by managing some tract of land for some owner

09:03.200 --> 09:08.880
more than that's public or private, but that I would be focused on that landscape.

09:08.880 --> 09:16.240
And going through undergrad, what I became really interested in were oddly and a surprise

09:16.240 --> 09:24.280
to, you know, to me was the quantitative aspects of certain problems like insects in a forest.

09:24.280 --> 09:30.040
When I first got into forestry, you know, my freshman year, there was a massive outbreak

09:30.040 --> 09:34.120
of southern pine beetle in the US south and it was killing lots of pine trees.

09:34.120 --> 09:38.560
And so that was a really compelling problem to me because it relates so much not only

09:38.560 --> 09:43.600
to the, you know, that trees themselves in the beetle, but also how we've managed them

09:43.600 --> 09:48.320
historically and sort of what, how that impacts local economies, that type of thing.

09:48.320 --> 09:55.640
And so, so I really, I started into pheromone plume modeling of all things in a forested

09:55.640 --> 10:00.360
system and trying to take measurements of concentrations of pheromones in locations

10:00.360 --> 10:05.600
and backtrack to where that originated from in the winter to try and deal with these

10:05.600 --> 10:10.760
beetles more, more effectively. And what that, what I learned from that or what I sort

10:10.760 --> 10:18.280
of, what I gathered was, was that there was this incredible ability to scale up my interests

10:18.280 --> 10:23.160
to, to, you know, to still focus on the things that I loved most, but to look at them with

10:23.160 --> 10:28.920
a different lens and to potentially affect change in a different way than I conceived

10:28.920 --> 10:35.160
of before. And so, you know, I wound up doing work in Brazil. You know, I was really interested

10:35.160 --> 10:41.440
in tropical forestry. I took some time off from undergrad to do that and, you know, worked

10:41.440 --> 10:48.280
in, like other areas Bolivia in, in South America. And there I got to see situations where

10:48.280 --> 10:53.680
people were dependent on different aspects of, of land in different ways and more direct

10:53.680 --> 10:59.200
ways than I think I was familiar with from my youth in, in the US South, where, you know,

10:59.200 --> 11:04.160
they were hurting animals. They were collecting nuts, fruits, things like that. They're collecting

11:04.160 --> 11:10.960
fuel wood to, to stay warm to cook. And they were also wanting to, to sell wood into a market

11:10.960 --> 11:17.280
and to, to develop as, as communities. And so forestry is about trade-offs. You, you

11:17.280 --> 11:22.680
know, there are a lot of things that you, we can do. And there are a lot of potential

11:22.680 --> 11:30.120
futures that we have before us. But we have to address the complexity of those systems

11:30.120 --> 11:35.680
in more comprehensive ways than we have in the past. There's far more than just a timber

11:35.680 --> 11:41.840
market now. There's far more than just to concern for delivery of wood to build houses.

11:41.840 --> 11:45.920
And that's, you know, when we spoke just a little bit before, but that was experienced

11:45.920 --> 11:51.360
very acutely here in the Pacific Northwest when people were confronting the issue of whether

11:51.360 --> 11:57.040
we had enough spotted owl habitat or spotted owls themselves or, or not. And whether

11:57.040 --> 12:02.200
we had managed appropriately in the past to accommodate those and, you know, and what,

12:02.200 --> 12:06.560
and everything that's related to that, to that species, all the habitats and other species

12:06.560 --> 12:12.280
that are related or whether we haven't, whether we've failed. And if we needed to go back

12:12.280 --> 12:18.280
and reconsider the ways that we make decisions. And that was, that was a really freighted conversation.

12:18.280 --> 12:25.920
That was, it, it brought, I mean, people to the, to kind of boiling points. And that

12:25.920 --> 12:29.840
was before my time, really, before I really entered into the profession in any meaningful

12:29.840 --> 12:36.040
way. But, but that type of conversation goes on now. And it's even more complicated. And

12:36.040 --> 12:41.480
there are more issues than, and more dimensions that we have to consider than there were then.

12:41.480 --> 12:47.800
And to have constructive conversations, we have to have information to, to inform those,

12:47.800 --> 12:54.520
those discussions, to facilitate the kind of communication that yields solutions that

12:54.520 --> 12:59.920
people can live with. And I'm presuming that that need is what led you to found Sylvia

12:59.920 --> 13:06.160
Tara. It is. Yeah. Absolutely. And so what is Sylvia Tara? What is the, what do we do? Yeah.

13:06.160 --> 13:12.280
Failing to answer the questions here. So Sylvia Tara is, we, we provide information,

13:12.280 --> 13:16.200
just like what I, you know, what I was speaking about there. Our objective is to help people

13:16.200 --> 13:24.200
use modern data sources, like remotely sensed information from satellites, from, you know,

13:24.200 --> 13:32.880
from aerial bases, from UAVs, and modern modeling techniques to, to help get more resolution

13:32.880 --> 13:38.000
on information and get more accuracy and precision on information, not only just about trees,

13:38.000 --> 13:43.160
but, but about habitats and, and beyond. And so that's, that's the focus of our company.

13:43.160 --> 13:47.280
You know, we've been at this for about nine years. A lot of the folks that we work with

13:47.280 --> 13:52.960
are timber companies. We also work with non like environmental NGOs. We work with government

13:52.960 --> 13:58.680
agencies. And all of them, you know, they have effectively the same questions. They have

13:58.680 --> 14:06.400
very similar, similar needs. And so, you know, initially, up until now, we've been providing

14:06.400 --> 14:10.840
data project to project to help them answer those critical questions that they, that they

14:10.840 --> 14:16.120
confront on a regular basis. And the, you know, I guess the, the reason I'm, I'm in this

14:16.120 --> 14:21.240
room with you all here today is that we, we were able to start working with Microsoft AI

14:21.240 --> 14:29.040
for Earth, and to, to begin to scale and expand that work, to build a, a foundational data

14:29.040 --> 14:34.080
set that we can, that we can start to use to answer these, these questions and to build

14:34.080 --> 14:37.480
on to, to improve our ability to manage for the future.

14:37.480 --> 14:44.640
That's maybe a good segue to taking a step back and look as what is AI for Earth?

14:44.640 --> 14:49.360
Sure. Well, I mean, thinking the context of this conversation, you can think about it.

14:49.360 --> 14:53.960
What is AI for Earth? It's why a reformed forester who's now the co-founder of a startup

14:53.960 --> 14:57.520
and a reformed wildlife ecologist who's now the chief environmental officer at Microsoft

14:57.520 --> 15:01.520
are at a table talking with you on twimble. I feel like we're in this recursive

15:01.520 --> 15:08.520
process, right? Yeah. No, exactly that. I can't even see you guys anymore. I'm just staring

15:08.520 --> 15:12.560
at myself in an infinity mirror here. So what AI for Earth is is as of Tuesday of this

15:12.560 --> 15:19.920
week, a one-year-old program. Okay. Happy birthday. Thank you. Thank you. It was fantastic.

15:19.920 --> 15:24.160
We spent it celebrating with our colleagues at National Geographic in Washington, D.C.

15:24.160 --> 15:30.200
In the woods? Yeah. Unfortunately, no, but at the founder's table of one of the most,

15:30.200 --> 15:37.880
you know, iconic and exploration-driven organizations in the world, right? And so it was an incredible

15:37.880 --> 15:44.520
time. But what AI for Earth is is a five-year $50 million commitment on behalf of Microsoft

15:44.520 --> 15:49.840
to deploy our 35 years. It will actually a little bit more than 35 years of fundamental

15:49.840 --> 15:58.640
research in the core fields of AI and machine learning to deploy those to affect change

15:58.640 --> 16:03.520
in these forkey areas of environment that we care deeply about, which is agriculture,

16:03.520 --> 16:08.440
water, biodiversity, and climate change. And the reason that we're doing that is because,

16:08.440 --> 16:13.400
you know, we recognize that at Microsoft, you know, I already spoke about this kind of

16:13.400 --> 16:19.320
tale of two ages, really. This time of this information age and this time of incredible

16:19.320 --> 16:23.560
and negative impacts of human activities on Earth's natural systems. And you look and

16:23.560 --> 16:28.040
you realize that as a society, we're facing almost an unprecedented challenge. We somehow

16:28.040 --> 16:31.600
have to figure out how to mitigate and adapt to changing climates, insure resilient water

16:31.600 --> 16:37.120
supplies, sustainably feed a human population rapidly growing to 10 billion people all while

16:37.120 --> 16:41.880
stemming this ongoing and catastrophic loss of biodiversity that we see around the world.

16:41.880 --> 16:46.200
And we've got to do that while ensuring that the human experience continues to improve

16:46.200 --> 16:51.200
all around the world for everybody that, you know, economic growth and prosperity continue

16:51.200 --> 16:55.280
to grow. And so, you know, that's why I say it's an unprecedented challenge. I mean,

16:55.280 --> 17:01.040
the scope and the scale are just incredible. And if you look at, um, at the scope and

17:01.040 --> 17:05.080
scale, the problem, and you step back and you ask yourselves, uh, the same question as

17:05.080 --> 17:11.400
a company that I asked myself during my PhD, which is, well, what are the things that are

17:11.400 --> 17:17.320
growing in the same exponential fashion as the scale and complexity of that challenge

17:17.320 --> 17:22.680
of our environmental challenge? Well, pretty much the only trends that are that are happening

17:22.680 --> 17:27.920
in an analogous fashion are in the tech sector. And particularly in the broader field of

17:27.920 --> 17:32.840
AI and the more narrow kind of machine learning learning approaches that are getting a lot

17:32.840 --> 17:39.320
of attention today. And so, um, and so that's when we decided to put together this program

17:39.320 --> 17:43.480
to actually say, hey, you know what, we've been investing as a company for over a decade

17:43.480 --> 17:47.720
at the intersection of environmental science and computer science. I led research programs

17:47.720 --> 17:52.840
in our blue sky research, um, division called Microsoft research for, for a fair number of

17:52.840 --> 17:59.560
years on that. But then the technology reached a point, the criticality of the, of the societal

17:59.560 --> 18:04.360
challenge, I think reached a point that it was time for a company like Microsoft to step

18:04.360 --> 18:10.400
in and actually start to deploy some of those resources and deploy them in ways that ensure

18:10.400 --> 18:15.800
that we ultimately change the way that we monitor model and then ultimately manage

18:15.800 --> 18:21.080
Earth's natural systems in a, in a way that we've never been able to before. And we started

18:21.080 --> 18:27.920
out, as I said a year ago with basically nothing but aspiration. Uh, we looked back this

18:27.920 --> 18:35.480
past Tuesday, uh, on, uh, this event that we had national geographic where we inducted

18:35.480 --> 18:42.960
a new set of grantees into our, into our portfolio and realized that in that short year,

18:42.960 --> 18:48.240
we'd set up relationships with organizations all over the world, over 200 organizations

18:48.240 --> 18:53.360
all over the world, each that are dedicated to taking a machine learning first approach

18:53.360 --> 18:58.320
to solving challenges in these, in these four domain areas that we focus on. They're

18:58.320 --> 19:03.120
on all set, they're working on all seven continents now over 50 countries in the world, 34

19:03.120 --> 19:07.760
countries here in the United States. And then today get the opportunity to sit down with

19:07.760 --> 19:12.960
one of, one of the grantees, right, to hear a little bit more about, you know, just their

19:12.960 --> 19:19.880
particular experience. Um, and, uh, and talk about the ways that, that machine learning

19:19.880 --> 19:27.320
in particular can fundamentally change our ability to, um, to understand what's going

19:27.320 --> 19:34.280
on on planet Earth. Because I think that most people don't take the time to step back

19:34.280 --> 19:44.160
and realize when they hear terms like information age, just how narcissistic that really is,

19:44.160 --> 19:50.120
that almost every bit of information that we've been collecting is about ourselves, right?

19:50.120 --> 19:54.360
It's about where the nearest Starbucks is. It's about what people who searched for also

19:54.360 --> 20:01.120
searched for, right? And it's at the peril of ignoring the rest of, rest of life on Earth

20:01.120 --> 20:05.560
and the ways that it supports us and our economies. It's what Sylvia Terra, I think, is, is

20:05.560 --> 20:10.440
so focused on is using vast amounts of data, new approaches and machine learning to actually

20:10.440 --> 20:17.160
just ask some simple questions. Like where are all the trees in the United States? We don't

20:17.160 --> 20:23.560
know answers to things like that. I mean, that just blows my mind, you know? Um, and so,

20:23.560 --> 20:28.720
um, that's where a lot of this came from. It's just a fundamental desire to change, change

20:28.720 --> 20:34.920
our ability to monitor and model life on Earth. I guess that isn't all that simple, but,

20:34.920 --> 20:39.920
but I also think it's completely and totally doable, right? I mean, look, look at where

20:39.920 --> 20:44.320
we've come from, from an information process and capacity over the past 25 years to where

20:44.320 --> 20:48.400
we are today. I mean, if you would have tried to predict every little bit of it, it would

20:48.400 --> 20:52.560
have been impossible, but, um, but it kind of seems pre-ardane now that you look back

20:52.560 --> 20:53.560
at it.

20:53.560 --> 21:00.680
When I think about the types of systems that we've been talking about thus far, both the

21:00.680 --> 21:07.280
economic systems, political systems, as well as the biological systems, it jumps out

21:07.280 --> 21:15.440
of me that there's a tremendous amount of complexity in those systems and machine learning,

21:15.440 --> 21:22.360
deep learning in particular has this great ability to, like, pick out patterns and abstract

21:22.360 --> 21:28.200
away from complexity, which kind of says to me, oh, it's a no-brainer to apply machine

21:28.200 --> 21:35.840
learning to this. Um, but then, you know, we're still very early on in our ability to put

21:35.840 --> 21:43.880
these, you know, these machine learning to work. And I guess, um, curious, yeah, maybe,

21:43.880 --> 21:49.320
maybe for you, Zach, like, where do you think the opportunity is with applying machine

21:49.320 --> 21:57.640
learning and AI for, uh, the types of problems that, uh, concern you in particular with regard

21:57.640 --> 21:58.640
to forest?

21:58.640 --> 22:04.840
Yeah, yeah, absolutely. So one, I guess, kind of listening, listening to Lucas there,

22:04.840 --> 22:09.280
you know, one thing that kind of jumps out at me from, from when you first spoke in

22:09.280 --> 22:13.200
that, uh, your response to the second question there is, there are lots of people that are

22:13.200 --> 22:17.160
very interested in natural resources and there are lots of people that are very interested

22:17.160 --> 22:21.000
in machine learning and AI, but it is a very small community of people. It's, I think it's

22:21.000 --> 22:25.440
rare that you, you know, it's uncommon to start out believing you're going to spend all

22:25.440 --> 22:31.520
your time outside and then find yourself curled up in front of some code. Uh, so, you

22:31.520 --> 22:37.200
know, the first thing, you know, I think there's a lot of opportunity for, um, you know,

22:37.200 --> 22:41.520
for people to make that leap and to see, just to begin to see that as a more natural,

22:41.520 --> 22:48.120
uh, a more natural thing, uh, because the, the questions are very, they're very complex.

22:48.120 --> 22:54.680
And so, um, again, just like Lucas said, most of our, most of our focus has been on how

22:54.680 --> 23:01.720
to market to somebody to buy a cup of coffee here versus there, um, and how to think about

23:01.720 --> 23:07.680
social networks and how to think about marketing networks and transportation networks.

23:07.680 --> 23:12.680
And I think, you know, it's, it's exciting to see that begin to percolate down and transition

23:12.680 --> 23:21.120
to, uh, the story behind, um, how all of those materials come into our, into our world

23:21.120 --> 23:25.400
and life. The fact is that everything around us, or I think the surprising fact is that

23:25.400 --> 23:29.040
everything around us, every, every little bit of technology and everything that built

23:29.040 --> 23:33.200
this, this room that, you know, that we're in or that your listeners are in, those things

23:33.200 --> 23:39.760
were either grown or mined. Every piece of that, uh, every little bit has some geographic

23:39.760 --> 23:46.840
story, some geographic story, some physical story, some environmental story. And if we

23:46.840 --> 23:52.360
were to be confronted with all of those stories, you know, just from one day of our consumption,

23:52.360 --> 23:57.120
one day of us interacting as we normally do, uh, it would take us years to even sift

23:57.120 --> 24:02.000
through all of those stories. There's, there's no way, there's no way, but those stories

24:02.000 --> 24:09.400
all amassed to have a, a very large impact in how we all live. And so to me, that is the

24:09.400 --> 24:14.640
huge opportunity here. You know, we, you know, we with, with Microsoft AI for Earth have

24:14.640 --> 24:19.480
worked on this, this, uh, data set for the continental US at high resolution to inform

24:19.480 --> 24:26.320
about, you know, down to species and diameters, where, where trees are and what those structures

24:26.320 --> 24:33.640
and compositions are and moving forward, what they could be. Um, but that's not going to

24:33.640 --> 24:40.200
stop, you know, the fact that we are all consumers and that while we have a conservation, uh,

24:40.200 --> 24:46.640
need, we also have a consumptive need. And there's, I think there's so much opportunity

24:46.640 --> 24:52.400
to begin to, to investigate how we balance that and how we feel about that and to engage

24:52.400 --> 24:58.840
a meaningful conversation as, you know, at, at multiple levels in society about how that

24:58.840 --> 25:05.080
can best be done. So like, you know, ask about opportunities. I mean, I, I was never excited

25:05.080 --> 25:11.160
about AI or stats or machine learning for the sake of, you know, I mean, it is awesome.

25:11.160 --> 25:16.320
I now understand that. But it, uh, you know, it's, and I, I do get jammed up about, you

25:16.320 --> 25:21.480
know, exciting advances there. But it's about what it can answer. I mean, that's, that's

25:21.480 --> 25:25.840
what drew me out of the woods and put me in front of a computer. It was the ability to

25:25.840 --> 25:32.520
start to, to, to even think about those, those big questions, uh, and, and put it all like,

25:32.520 --> 25:39.160
like distill it to, to something simple and right in front of us. And, and so that's, that's

25:39.160 --> 25:44.280
the opportunity. It, it allows us to know more about our world and ourselves and to create

25:44.280 --> 25:50.240
a better, uh, a better world and a, and sort of a better image of our, of ourselves.

25:50.240 --> 25:57.080
Can we maybe dig into a little bit more detail of either the data set that you just mentioned

25:57.080 --> 26:05.000
or another project and talk through, uh, the process through which Soviet era uses machine

26:05.000 --> 26:10.720
learning, the challenges that you run into, um, maybe walk us through a scenario.

26:10.720 --> 26:14.320
Sure. Absolutely. And, and I'll just briefly kind of tell you where we're coming from.

26:14.320 --> 26:18.320
People have been managing forests for, you know, hundreds, you know, a couple hundred

26:18.320 --> 26:23.040
years. And in the US, about a hundred, a hundred plus, uh, and they needed information

26:23.040 --> 26:28.400
then as they do now, they, uh, but to get that, they would do a statistical survey. They

26:28.400 --> 26:32.400
would go, you know, the go and put measurements in and you work up an average and you make

26:32.400 --> 26:39.320
a plan based on that average. Um, that has been effective and it's, you know, it's what

26:39.320 --> 26:44.720
people use a lot still today. But, uh, what, what we were, what we're focused on doing

26:44.720 --> 26:50.720
is bringing imagery into bear in model assisted and model based methods to yield small area

26:50.720 --> 26:57.720
estimates. Uh, and, you know, for us, it's at a 15 meter resolution. Uh, and for a 15 meter

26:57.720 --> 27:03.920
pixel, what we're predicting is the number of stems, their sizes and species. And when

27:03.920 --> 27:08.800
I say size, I mean, the diameter of the, the trunk of the tree at four and a half feet

27:08.800 --> 27:13.960
off the ground. And from there to, you know, in a hierarchical context to predict them,

27:13.960 --> 27:21.560
maybe the height of the tree or the ratio of crown to, uh, to just clear bowl at the bottom.

27:21.560 --> 27:26.800
And from there, maybe they're herbaceous, you know, since we can infer or predict maybe

27:26.800 --> 27:31.440
the light conditions, uh, under that forest, how much herbaceous, uh, plant matter there

27:31.440 --> 27:36.720
may be there, uh, carrying that forward. How many, um, how many herbivores that could

27:36.720 --> 27:42.760
support scaling that up, how many large carnivores that could support. But for now, the, the,

27:42.760 --> 27:47.040
the primary piece, this, uh, foundational data set that we've worked out with Microsoft

27:47.040 --> 27:54.000
on is the tree list information for each one of those pixels, which hasn't existed before.

27:54.000 --> 28:00.320
But that opens up so many doors, uh, for what, what we can begin to build on to and model

28:00.320 --> 28:07.160
further, further down the line. And so at a resolution of 15 meters, a single pixel

28:07.160 --> 28:14.520
might contain how many trees contain, it could contain an awful lot. Um, you know, easily,

28:14.520 --> 28:17.960
and this is the tricky thing is a tree could be as small as a seedling. It could be as large

28:17.960 --> 28:24.120
as a Sequoia. So it could, it could have less than one, right? Uh, it could have 300, you

28:24.120 --> 28:28.760
know, packed, but, you know, small tiny little trees packed in tightly. And this is the fundamental

28:28.760 --> 28:34.640
difference about what we're working on here, you know, to me, then, you know, then, then

28:34.640 --> 28:40.320
where we're coming from, which is we need to transition away from the binary, or like,

28:40.320 --> 28:46.000
basically qualitative classifications, forest non-forest. That's not actually that informative

28:46.000 --> 28:51.800
about what that forest can, uh, you know, what habitat it can provide, what, you know,

28:51.800 --> 28:57.440
what maybe we need to do or not do to ensure that it's the type of forest that's going to

28:57.440 --> 29:03.160
continue providing the things we care about, clean water, you know, carbon out of the atmosphere,

29:03.160 --> 29:07.400
um, would to build this table, you know, those, those are the types of things. And so beginning

29:07.400 --> 29:13.560
to quantify those, those aspects is very important. And when I began working with this, um,

29:13.560 --> 29:18.040
you know, there, everything was on the table. I mean, we were, there, there was the potential

29:18.040 --> 29:25.480
to use LIDAR and neural nets, uh, to try and, uh, clarify discrete trees. Um, we do not

29:25.480 --> 29:35.960
do that at, you know, I, uh, for various reasons, largely bias in results. Um, but, uh, for us,

29:35.960 --> 29:41.640
you know, parting out species became a massive problem. So if you have, let's say, 40 trees

29:41.640 --> 29:47.400
of multiple species in one pixel, how do you begin to differentiate those when you're looking

29:47.400 --> 29:54.760
at one pixel of data from lots of imagery sources? And, you know, that, that's, uh, that was

29:54.760 --> 29:58.680
a technical challenge. So one of, one of the things that I think is interesting about this

29:58.680 --> 30:05.720
is like, you're talking about forestry, right? And whether or not people know it's a profession,

30:05.720 --> 30:09.400
it's an extremely old one, you know, some of the people that go on, you don't think that,

30:09.400 --> 30:12.600
you don't think that you're going to be talking about machine learning. You also don't think that

30:12.600 --> 30:17.160
you're necessarily going to be talking about philosophy or existential questions, but you asked

30:17.160 --> 30:23.320
to, you asked a question about 15 meter resolution, right? Which, when you work with organizations

30:23.320 --> 30:28.520
like Sylvia Tara that are looking down at the world and asking what is there, you end up having

30:28.520 --> 30:34.520
these existential conversations about what is a thing, right? At what level should we be taking

30:34.520 --> 30:38.680
data points to be able to feed into these machine learning algorithms? Because when you incorporate

30:38.680 --> 30:42.920
the Z dimension or the Z dimension or whatever you want to call it, whatever part of this planet

30:42.920 --> 30:52.920
Earth is from, you can be, you can be looking down at, in, multitude of different objects, right? And

30:52.920 --> 30:58.520
depending on what sensor you're using, you may only see one of them or you may see many of them

30:58.520 --> 31:04.360
if you're using something like LIDAR and you're able to kind of get your, sent your, your laser

31:04.360 --> 31:10.040
sensors enough to see enough of those things. And so you, you start struggling with all of these

31:10.040 --> 31:17.560
questions that are actually fairly unarticulated in the modern machine learning literature quite frankly,

31:17.560 --> 31:23.560
where, you know, all the standard libraries take in a 300 by 300 pixel, you know, this image and

31:23.560 --> 31:30.440
they all have these harsh expectations and, you know, and sure, maybe we think we all left the

31:30.440 --> 31:34.760
world of frequencies, statistics behind, but we still carry over kind of the ghosts of a lot of

31:34.760 --> 31:40.920
those, you know, harsh binary classification results. And so it's just fascinating, I think,

31:40.920 --> 31:47.160
to think about not just like what's hard in the, in the forestry space and how modern machine

31:47.160 --> 31:51.880
learning techniques can help transform that, but also what the problems in the applications

31:51.880 --> 31:56.920
of an organization like Sylvia Terra and then the rest of our AI first grantees, what that brings

31:56.920 --> 32:02.760
back to the machine learning community, which is what's hard here, right? Why, why can't we just

32:02.760 --> 32:08.360
take all the deep neural network advances that we've made and just voila, we've solved all of the

32:08.360 --> 32:15.320
world's problems, right? It's because, as you said, we're still at the infancy of a lot of what

32:15.320 --> 32:23.960
we hope to achieve in machine learning. We just also recognize the severely short amount of time

32:23.960 --> 32:29.000
that we have to answer some of these bigger and, you know, kind of environmental questions. And

32:29.000 --> 32:33.880
so we have got to take everything that we have at our disposal and start to deploy it.

32:33.880 --> 32:40.760
You mentioned sensors and light hours, kind of a very specific curiosity question. I've always

32:40.760 --> 32:47.240
associated light hours like a local, you know, a very short-range local sensing mechanism.

32:48.040 --> 32:50.200
Is that not the case? Can you do light hour from satellites?

32:51.080 --> 32:57.080
Yes. Yes. Plane. What are all the sensors? A new sensor was just launched, you know, a couple

32:57.080 --> 33:01.720
what weeks ago. Something like that. So there's a new, there's a Jedi sensor that's going,

33:01.720 --> 33:08.280
it's called Jedi. I'm used to it now, but I was going to say, use the light or Luke. Jedi,

33:08.280 --> 33:14.840
Jedi is his, his, no, no. Well, NASA is a designation, but they're strapping this thing onto,

33:14.840 --> 33:23.000
onto the space station. It's going to be pulsing down, you know, not the poles, but basically everything

33:23.000 --> 33:31.400
between. And I think it's full waveform, lidar. And yeah, so absolutely, and even historically,

33:31.400 --> 33:38.440
there was ISAT, which was a satellite-based lidar sensor. But moreover and more commonly in forestry,

33:40.120 --> 33:45.640
and a lot of the, you know, even in urban areas, they're collecting lidar information from airplanes

33:45.640 --> 33:50.680
at, you know, at different altitudes and different point densities, something, you know, a common one

33:50.680 --> 33:57.400
might be like 12 or 24 points per square meter. And those, you know, when you see that over forest

33:57.400 --> 34:02.680
at Canopy, some of those pulses reach the ground. And so the best elevation models that you see in

34:02.680 --> 34:08.760
the US right now are lidar-derived elevation models. And that's the source of a lot of the

34:08.760 --> 34:13.000
information that we're getting. And you see it in a lot of floodplain areas, the Mississippi Delta

34:13.000 --> 34:17.880
area so that we can better understand how flooding may occur or may not occur in certain areas.

34:17.880 --> 34:22.920
One more thing that I'm always struck by when you when you start thinking about remote sensing and

34:22.920 --> 34:27.320
just sensing in general is applied to environmental systems is that as we start to take a more

34:27.320 --> 34:33.640
digital or computational approach to sensing, we almost by definition have got to start taking a more

34:33.640 --> 34:39.480
machine learning approach to driving insights. Because what computers are able to do, and I don't

34:39.480 --> 34:43.960
know, maybe I'm just missing the conversation or maybe the conversation isn't as fully kind of

34:43.960 --> 34:48.600
articulated as it could be, but computers are able to sense the world and so many more

34:48.600 --> 34:54.680
dimensions than people are, right? And why do we model? Well, we model because we need a

34:54.680 --> 35:01.400
simplifying function to help us understand an already complex world. And so what was already

35:01.400 --> 35:07.720
complex according to our five senses has now become exponentially more complicated with things like

35:07.720 --> 35:14.120
hyperspectral resolution monitoring where you're getting thousands of bands back of imagery plus

35:14.120 --> 35:20.520
things like LIDAR that are getting 24 points per square meter. You can't humans don't even know,

35:20.520 --> 35:25.000
it's interesting. People always complain that they don't understand what the net what the layers

35:25.000 --> 35:31.240
and a deep neural network do. We also have no idea how to even interpret most of the signals that

35:31.240 --> 35:35.080
are coming back from the most advanced sensors in the world because they don't correspond to

35:35.080 --> 35:41.640
dimensionality that we that we live in. I was just going to ask that when I've talked to folks

35:41.640 --> 35:48.840
that are using LIDAR in the context of self-driving vehicles, and this whole idea of sensor fusion

35:49.640 --> 35:54.760
comes into play and making sense of all these disparate data sources, you know, that in that

35:54.760 --> 36:01.160
example are very local. And now we're talking about, you know, global data sources or at least,

36:01.160 --> 36:07.720
you know, much larger scale and, you know, with overlapping tiles and capabilities. There's

36:08.360 --> 36:16.840
a ton of complexity there is are those? Is that type of complexity, some of the complexity that

36:17.800 --> 36:23.880
your company is working on managing or do you count on upstream providers to kind of sort a lot

36:23.880 --> 36:28.360
of that out for you? So that's exactly the type of complexity that we deal with. I mean, there

36:28.360 --> 36:35.560
are an enormous pool of potential data sources that exist and they all have, you know, potentially

36:35.560 --> 36:40.920
very useful attributes and some of them less so. They have different time stamps associated with

36:40.920 --> 36:45.400
them and there's one very nice thing about measuring forests as long as you don't mess with them.

36:45.400 --> 36:50.200
They tend not to move too much. So trees are, you know, they're pretty willing subjects to be,

36:50.200 --> 36:54.200
you know, just to be measured, but they are always changing. There's growth associated,

36:54.200 --> 36:59.240
there's natural, there's naturally occurring disturbance, there's, you know, human cause disturbance

36:59.240 --> 37:04.440
and both of those we want to keep track of. But yeah, what, you know, what I see our role

37:05.480 --> 37:12.600
right now as being is taking that, you know, that massive pool of potential sources of remotely

37:12.600 --> 37:20.760
sensitive data and the very small and often underappreciated pool of field measurements,

37:20.760 --> 37:27.160
the things that we actually might care about and translating between those things and creating

37:27.160 --> 37:33.240
something that is more highly resolved, more accurate, more precise and more useful

37:33.240 --> 37:37.800
than what could otherwise be achieved. So yeah, draw the signal out of the noise, the classic,

37:37.800 --> 37:44.520
you know, classic tale. I think if I look at kind of the full portfolio of AI for Earth grantees,

37:44.520 --> 37:53.160
well over 200, you see that at least in my mind, Sylvia Terra is, as an organization,

37:53.160 --> 37:58.120
one of the most mature, right? They're actually, they're, they're out of the lab,

37:58.120 --> 38:03.400
they're a startup business model, et cetera, et cetera. And when I think about why that is in

38:03.400 --> 38:07.640
the context of machine learning, why they're able to take advantage of that, it's because of one

38:07.640 --> 38:15.240
thing that we just heard, which is they're taking advantage of these ground-based data points

38:15.240 --> 38:21.000
that they can use to train their models, right? And that's because forestry is something that is

38:21.000 --> 38:25.400
so inherently tied to our broader economy that we have here in the United States and all around

38:25.400 --> 38:31.240
the world, a history of going out boots on the ground and putting a tape measure around a tree.

38:31.240 --> 38:40.520
And a GPS signal next to it and saying, this tree is here. It's of this height and it's of this

38:40.520 --> 38:47.960
species. And that's so rare in the broader environmental space, right? And that's, it's one of the

38:47.960 --> 38:53.560
reasons that I think organizations like Sylvia Terra are unfortunately kind of standing alone in

38:53.560 --> 38:58.440
many respects is because there's so few data sets. It's called machine learning because we're

38:58.440 --> 39:03.720
teaching computers, right? And to teach you have to be taught or to be taught, you need to be shown

39:03.720 --> 39:10.040
examples. And it's why we've seen so significant of advances in other fields of machine learning,

39:10.040 --> 39:17.720
but not in others. And there's just so few annotations in our space that when you come into a forestry

39:17.720 --> 39:22.600
space where the US government has paid money for the past hundred years to go out and figure all

39:22.600 --> 39:27.720
this out, companies like Sylvia Terra can stand on top of that and really just kind of zoom off

39:27.720 --> 39:33.880
ahead. But they are in many ways the exception to the rule, which is unfortunate, I think.

39:35.080 --> 39:40.200
Do you find that the kind of work that you're doing, you know, we talked about the

39:41.560 --> 39:48.600
sensing and pulling all that information together. Does this put you at the kind of the research

39:48.600 --> 39:54.680
frontier of using machine learning techniques? Or are you able to use off the shelf types of

39:54.680 --> 39:59.880
models? Where does your work fall in the spectrum of complexity?

40:02.840 --> 40:09.320
Maybe complexity is not the right word, just in terms of the innovation cycle. Are you

40:09.320 --> 40:15.000
able to apply things that people are doing in other fields pretty readily? Or are you having to

40:15.000 --> 40:19.880
push the limits and pull right out of academic research or things like that?

40:19.880 --> 40:25.320
You know, it's a little bit of both. I mean, our core algorithm has been, you know,

40:25.320 --> 40:32.200
it's matured over the last nine years of doing the work that we have. And we're a small team.

40:32.200 --> 40:41.000
I mean, we're 10 people effectively. And I guess when I got into this, I originally, like when I

40:41.000 --> 40:46.920
thought this quant path was something that was really resonated with me that I connected with

40:46.920 --> 40:51.960
and that I saw a value in, I originally then thought I was going to be a professor.

40:51.960 --> 40:56.920
I would be a researcher somewhere. I would be putting papers out because that must be how change

40:56.920 --> 41:02.040
happens. And my path changed when I went around to people that I'd worked with in industry and

41:02.040 --> 41:08.440
asked them what papers they were reading to affect to change the way that they worked. What was the

41:08.440 --> 41:14.040
most influential journals that they were reading? And the answer was that they weren't reading the

41:14.040 --> 41:19.640
journals. They were busy managing land and that they wanted a tool, not a publication.

41:20.520 --> 41:26.120
And that, I mean, that was a little eye-opening. And so that's what I, you know,

41:27.160 --> 41:32.840
max my other, Max Nova, my co-founder and I sort of set about to do is build tools.

41:33.400 --> 41:39.480
But I don't, I don't really accept like a full dichotomy between, you know, you know,

41:39.480 --> 41:45.880
is it researcher? Is it kind of off the shelf type stuff? I mean, we pride ourselves in our,

41:45.880 --> 41:50.120
you know, in our ability not only to understand the systems that we're working in, but also

41:51.000 --> 41:58.920
to be abreast of what's happening in modern computational techniques and modeling effort,

41:58.920 --> 42:04.920
you know, modeling tools. So, which I imagine everybody would probably say, right? Like everybody

42:04.920 --> 42:10.440
would like to tell you, no, we're right on the edge. But we're also, what I, the funny thing that

42:10.440 --> 42:15.480
I learned when I got into this on the app, I'm on the applied side. I mean, I, I talk with people

42:15.480 --> 42:20.040
that are trying to figure out wildfire modeling and how to, how to pick which communities to,

42:20.840 --> 42:28.120
to, you know, to allocate funds and effort to help manage a forest to prevent catastrophic fires.

42:28.120 --> 42:31.640
I work with people that are trying to figure out how to manage for forest carbon. I work with

42:31.640 --> 42:36.680
people that try and figure out how to manage forests to deliver wood to a mill to make paper.

42:38.280 --> 42:44.520
But what's, I guess, striking to me from the, from where I started to now is I thought that what

42:44.520 --> 42:50.120
people needed to see was the math. I thought I would show up at their offices and be like, good

42:50.120 --> 42:55.400
news. We figured it out. Check this new method out. Check, you know, we pipe in this data. We put

42:55.400 --> 43:00.680
in these measurements from the ground. We're able to model this more effectively now. And what I,

43:00.680 --> 43:06.760
what I learned is that if, if I can't communicate effectively about, about what we've done,

43:06.760 --> 43:11.960
if it really truly seems like magic, then it is, by definite, it's incredible in like the,

43:11.960 --> 43:17.480
in the truest sense of the word, it is not credible, you know, and, and credibility counts.

43:18.200 --> 43:27.960
And so I, in some cases where, when we're working with people, we may not use the most fantastic

43:27.960 --> 43:33.560
new thing. We may use something that is slightly more costly in terms of input data that it

43:33.560 --> 43:41.080
requires or costly in terms of model fit. But that is more easily understood and explained and

43:41.080 --> 43:50.280
more robust to, like the boot test, you know, you go out and it, it, it just makes sense. So I,

43:50.280 --> 43:58.120
that's, you know, and Lucas, does that experience ring true for the other, the other grantees that

43:58.120 --> 44:02.360
you work with or is there, are there a spectrum of experiences there in terms of where they are

44:02.360 --> 44:10.360
and applying some, some of our grantees are using almost commodity services at this moment,

44:10.360 --> 44:16.200
you know, I mean, Microsoft, for instance, has a, has a service called custom vision AI,

44:16.200 --> 44:21.960
sorry, custom vision API, where you just, they want to do some of our grantees want to do

44:22.920 --> 44:27.800
simple image recognition tasks and the service works for them. And all they, they literally just

44:27.800 --> 44:32.840
drag in a whole bunch of photos of one type and a whole bunch of photos of another type and the

44:32.840 --> 44:38.280
system learns it and produces a result for them. And that's fine, right? So that's pretty far

44:38.280 --> 44:44.600
on the one side of just like commoditized services. Then there's other grantees that are out there

44:44.600 --> 44:51.960
creating exceptionally custom algorithms for their work. I think we've got a grantee

44:53.160 --> 44:59.720
called wildme that does basically facial recognition for species so that they can provide better

44:59.720 --> 45:05.960
wildlife population estimates of species like giraffe and zebra, things that they can, you

45:05.960 --> 45:10.840
know, everybody knows a giraffe or everybody's heard that every giraffe's pattern is unique,

45:10.840 --> 45:14.920
but look at a couple photos of giraffes and you'll realize just how hard it is for the human eye

45:14.920 --> 45:22.040
to spot those differences, right? So they're building algorithms to differentiate any particular,

45:22.040 --> 45:28.760
you know, zebra giraffe and then plug those into statistical models for estimating populations.

45:29.560 --> 45:33.800
There's nothing off the shelf that does that. In fact, most of the main libraries they have to go

45:33.800 --> 45:41.560
back and modify the core code of. So it's a full, full spectrum and we're willing to support

45:41.560 --> 45:47.640
all of it, right? Because what we're trying to get people to understand is, well, first and foremost,

45:47.640 --> 45:52.680
we're just trying to break down the access barrier, right? We want to ensure that budget isn't a

45:52.680 --> 45:57.640
barrier to getting this stuff done because I can assure you and many of your listeners are aware,

45:57.640 --> 46:03.160
sometimes the latest machine learning kind of approaches can be fairly expensive. If not,

46:03.160 --> 46:07.320
you know, it might be an open source library, but somebody needs a thousand GPUs to run this

46:07.320 --> 46:14.360
sitting on, right? So we make sure that the infrastructure gets in the hands of folks, et cetera,

46:14.360 --> 46:19.480
but it's also just kind of awareness that you could be thinking about this. You don't have to be,

46:20.200 --> 46:25.880
we want the world's leading machine learning scientists to be thinking about what they could be doing,

46:25.880 --> 46:29.000
but we don't want the rest of the world to think that they have to be one of the world's

46:29.000 --> 46:34.280
machine learning experts to have a crack at this, right? That there is software and services

46:34.280 --> 46:39.720
that can help them as well. So we see the full spectrum and I think it's super healthy.

46:41.000 --> 46:45.240
We also see the full spectrum of kind of, if I would encapsulate what Zach was saying there

46:45.240 --> 46:51.240
in just kind of two words of interest in what we would call explainable AI, right?

46:51.240 --> 47:00.760
Do people really care why an algorithm said that this was a draft and that was a zebra?

47:01.320 --> 47:07.800
Not really. You don't have to explain that to them, right? Do they want to understand why

47:08.680 --> 47:15.720
some decision support algorithm like a land, like a spatial optimization algorithm that assigns

47:15.720 --> 47:21.160
this part of the country or this part of the county into protected land and this part into

47:21.160 --> 47:28.360
industrial use and this part into urban growth and expansion? How that works? And why people thought

47:28.360 --> 47:33.560
that this was the better policy than that? Probably so. Yes, they do, right? So I think, you know, people,

47:33.560 --> 47:37.240
there's, I think there's a lot of hand-ranging and angst right now around conversations,

47:37.240 --> 47:43.000
like explainable AI, I don't remember. And I think it's just like, it's no different than the

47:43.000 --> 47:50.760
conversation we've always had about modeling, which is why it's a model of a complex system.

47:51.400 --> 47:57.640
Why are you building it? If it's being built to just do a simple classification task and it's

47:57.640 --> 48:04.280
easy for a human to go and check the accuracy left or right, then great, you know, you can use

48:04.280 --> 48:09.800
some really advanced statistical techniques. If it's something that if that model instead is a

48:09.800 --> 48:16.520
model of, for instance, a human decision process, then I think the onus on kind of explainability is

48:16.520 --> 48:26.040
much higher. So along those lines, we've used computation to understand the environment, climate

48:26.040 --> 48:32.200
for a very long time, you know, whether, for example, has, you know, been a great focus of

48:32.200 --> 48:37.640
high-performance computing, you know, taking a step back from, you know, the fact that we're all

48:37.640 --> 48:44.200
really excited about AI, do you, where do you think AI offers unique opportunities relative to

48:44.200 --> 48:49.640
the things that we've done for a long time? Sure. Well, I mean, the answer to that will be

48:49.640 --> 48:53.720
super complex. I'll try to make it simple. And, you know, you mentioned whether, I think, sure,

48:53.720 --> 49:01.320
there's no question that statistics and math and then kind of the computational platforms that

49:01.320 --> 49:05.960
started to support them over or the recent decades have been used for environmental monitoring.

49:05.960 --> 49:10.920
I mean, Fisher was, you know, it goes all the way back to some of these guys. We're biologists,

49:10.920 --> 49:18.840
right? The bigger question is, why are we kind of excited about this today? And for me,

49:18.840 --> 49:26.760
it really is the full kind of broad definition of what we mean by AI. It's the recognition that

49:26.760 --> 49:32.200
we're finally deploying computers, computing systems that can collect unprecedented amounts of data.

49:32.200 --> 49:36.520
And that's just amounts. But, you know, we were talking about the full kind of crazy dimensionality

49:36.520 --> 49:44.040
of the data that we're starting to take on. So we've got this breakthrough in data. We've got

49:44.040 --> 49:51.080
this breakthrough in infrastructure where you can, you know, I made a joke about needing 1,000

49:51.080 --> 49:59.240
GPUs. Well, if you need one 1,000, 10,000, you just got to turn a knob these days and get access

49:59.240 --> 50:04.200
to it. And wherever you are on that knob is still a lot cheaper than a supercomputer. Extremely.

50:05.240 --> 50:12.840
So, and then we have made crazy advances in just a whole plethora of algorithms. But for a lot

50:12.840 --> 50:19.480
of the most important ones, we've directly accelerated the compute through the perspective of those

50:19.480 --> 50:25.000
algorithms, right? So for the first time. And then, of course, we've made it so easy to deploy

50:25.000 --> 50:31.080
these algorithms as web-based services as APIs, right? And then, of course, the software infrastructure

50:31.080 --> 50:37.560
stack and all of that is incredible. So, and we've made it commodity level infrastructure.

50:37.560 --> 50:42.200
Anybody can get access to this stuff. You know, you hear this term democratizing AI. What we

50:42.200 --> 50:47.800
mean by that is bringing it all into a stack that anybody can use. You don't need access to a

50:47.800 --> 50:53.080
government run supercomputer anymore. So that's all one side of it. The other thing is from

50:53.080 --> 50:58.840
Weather is a great example here where traditional weather force casting was strong numerical

50:58.840 --> 51:05.320
simulation, right? And that's one type of math, right? But there wasn't a lot of learning

51:05.320 --> 51:11.800
in real time about what was going on. We took a physical process. We built a model that we

51:11.800 --> 51:17.320
thought strongly corresponded with it. And then we ran numerical simulations of it fast forward.

51:17.320 --> 51:21.400
And yeah, just for the simulation perspective, you need a lot of compute. But then the question

51:21.400 --> 51:26.440
is, but all sorts of crazy things happen when we do that that we don't quite understand, right?

51:26.440 --> 51:30.840
Little eddy fluxes happen in some atmospheric layer or whatever. And we don't really know why.

51:31.880 --> 51:38.440
And then the weather community started using machine learning to not necessarily learn why,

51:38.440 --> 51:43.800
but to be able to predict for one reason or another when those things were going to come. And

51:43.800 --> 51:49.240
weather forecasting got a lot better. Same thing is happening now in climate modeling as well.

51:49.240 --> 51:55.320
We know there's things that we just can't do from our traditional approach to climate modeling.

51:55.320 --> 52:00.920
There's a whole new group that's just kind of spun out that's taking a purely machine learning

52:00.920 --> 52:06.840
first approach to building a new climate model for the world. And not positioning themselves

52:06.840 --> 52:13.960
as better, but positioning themselves as complementary. And so I think that there's a lot of,

52:13.960 --> 52:22.120
um, there's a lot of, uh, work that's just happened in commoditizing all of this stuff as well as,

52:22.120 --> 52:27.560
you know, recognizing that while we've taken a hugely mathematical statistical and computational

52:27.560 --> 52:32.920
approach to doing some of the stuff in the in the past, machine learning is a different approach,

52:32.920 --> 52:39.880
right? It's a data driven approach. Um, and that can be very complementary. And we've seen it accelerate

52:39.880 --> 52:45.160
extremely economically important things like weather cap forecasting, forestry, agriculture,

52:45.160 --> 52:52.120
and on and on. As we wind up, Zach, can you share something that you're particularly excited about

52:52.120 --> 52:59.560
kind of looking forward in terms of the application of AI to forestry? Yeah. So it's absolutely,

52:59.560 --> 53:05.240
I mean, obviously we're excited to be releasing this, this data set, but it's, it's really about what

53:05.240 --> 53:13.480
it enables. Um, we're, we're excited to see more, uh, nuanced and, uh, reactive markets

53:13.480 --> 53:21.640
around environmental services like species habitat, carbon, water, uh, be informed by, by these

53:21.640 --> 53:29.400
type of data. And, uh, to play a part in, in that process to integrate these, uh, these concerns

53:29.400 --> 53:36.280
into, into ongoing management decisions. So that's, I mean, that's the, that's the, the biggest

53:36.280 --> 53:41.000
piece. It's what you can, what you can do with, with this information is you move it from, well,

53:41.000 --> 53:47.080
from data to information to, to decisions. And Lucas, how about from, from your practice, you

53:47.960 --> 53:55.480
look at this from both a, uh, you know, very technical and resource perspective, but also as

53:55.480 --> 54:02.280
managing and interacting with this portfolio of innovators that are working in this space. What,

54:02.840 --> 54:09.080
what are you excited about? Well, ultimately what I'm the, the future I kind of see and the way

54:09.080 --> 54:14.600
that we've structured the whole program is we think the world fundamentally needs is the ability,

54:15.000 --> 54:20.440
or what, what society needs is the ability to query the planet by x, y and t.

54:20.440 --> 54:27.080
Hmm. We need to be able to understask questions, just like we ask some potentially Z. What's that?

54:27.080 --> 54:32.520
No, Z. No, Z. Well, so I was, I was actually speaking with my team the other day and I sent a

54:32.520 --> 54:38.360
slide that said x, y, t of posture, you know, uh, a positive z. And I was like, and I said

54:38.360 --> 54:44.520
stretch goal, right? So, um, so, uh, yeah, if we get the z dimension, then, then I'll be, then I

54:44.520 --> 54:49.000
can retire. Uh, but no, I think, you know, ultimately that's where we need to go. We need to be

54:49.000 --> 54:53.640
able to allow people to ask for any particular piece of land or water. What was there? What's there?

54:53.640 --> 54:59.640
Now, what could be there? And a, and empower policymakers to figure out what should be there,

54:59.640 --> 55:04.840
right? We're far from that. Now, Microsoft's always had kind of a empowering and ecosystem

55:04.840 --> 55:09.640
of customers and partners approach. We don't look, you know, we don't look at the world and say,

55:09.640 --> 55:17.080
oh, say we buy into my x, y, t vision. We don't see that as some fantastical crystal ball

55:17.080 --> 55:23.080
that the world spins around and taps on, right? We see it as a constellation of services

55:23.720 --> 55:31.160
and products and solutions brought by all sectors. And so what we're looking to do is engage with

55:31.160 --> 55:36.920
the Sylvia terrors of the world. Unfortunately, there are far too few at the moment. So engage with

55:36.920 --> 55:43.560
those that are there. Bring up the next generation and the next and the next until eventually,

55:43.560 --> 55:49.320
there's kind of a self-supporting community of machine learning kind of born, you know, we talk

55:49.320 --> 55:55.160
about born digital. I kind of think about born machine learning, you know, these organizations

55:55.160 --> 56:01.640
that it's just baked into their DNA. But the organization is not, it doesn't exist because of

56:01.640 --> 56:09.160
machine learning. It exists because of the challenges that we face in the environmental space.

56:09.160 --> 56:17.160
They just are capable of ingesting machine learning approaches natively and efficiently.

56:17.160 --> 56:22.680
And treat space and time as first-class data citizens in this world of machine learning.

56:23.720 --> 56:28.760
Fantastic. Well, Lucas and Zach, thanks so much for taking the time to chat with me.

56:29.320 --> 56:31.240
Thank you. It was a pleasure. Yeah. Thanks, Sam. Appreciate it.

56:31.240 --> 56:41.000
All right, everyone. That's our show for today. For more information on Lucas, Zach, or any of

56:41.000 --> 56:48.680
the topics covered in the show, visit twimmelai.com slash talk slash 228. To follow along with the AI

56:48.680 --> 56:56.760
for the benefit of society series, visit twimmelai.com slash AI for society. As always, thanks so much

56:56.760 --> 57:06.760
for listening and catch you next time.

