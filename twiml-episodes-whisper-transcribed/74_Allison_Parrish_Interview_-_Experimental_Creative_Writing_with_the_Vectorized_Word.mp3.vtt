WEBVTT

00:00.000 --> 00:15.920
Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting

00:15.920 --> 00:20.880
people doing interesting things in machine learning and artificial intelligence.

00:20.880 --> 00:23.720
I'm your host Sam Charrington.

00:23.720 --> 00:28.520
A big thanks to everyone who participated in last week's Twimble Online Meetup, and

00:28.520 --> 00:31.280
it's Kevin T from Sigopt for presenting.

00:31.280 --> 00:35.520
You can find the slides for his presentation in the Meetup Slack channel, as well as in

00:35.520 --> 00:37.440
this week's show notes.

00:37.440 --> 00:41.800
Our final Meetup of the Year will be held on Wednesday, December 13th.

00:41.800 --> 00:47.000
Make sure to bring your thoughts on the top machine learning and AI stories for 2017

00:47.000 --> 00:49.160
for our discussion segment.

00:49.160 --> 00:54.800
For the main presentation, prior Twimble Talk guest Bruno Gonzalez will be discussing

00:54.800 --> 01:01.280
the paper understanding deep learning requires rethinking generalization by Shi Huang Zhang

01:01.280 --> 01:04.680
from MIT and Google Brain and others.

01:04.680 --> 01:09.800
You can find more details and register at twimla.com slash Meetup.

01:09.800 --> 01:14.120
If you receive my newsletter, you already know this, but Twimble is growing and we're

01:14.120 --> 01:19.840
looking for an energetic and passionate community manager to help expand our programs.

01:19.840 --> 01:24.240
This position can be remote, but if you happen to be in St. Louis, all the better.

01:24.240 --> 01:27.960
If you're interested, please reach out to me for additional details.

01:27.960 --> 01:31.920
I should mention that if you don't already get my newsletter, you are really missing

01:31.920 --> 01:37.160
out and should visit twimla.com slash newsletter to sign up.

01:37.160 --> 01:43.240
Now the show you're about to hear is part of our Strange Loop 2017 series brought to you

01:43.240 --> 01:46.240
by our friends at Nexusos.

01:46.240 --> 01:50.640
Nexusos is a company focused on making machine learning more easily accessible to enterprise

01:50.640 --> 01:52.240
developers.

01:52.240 --> 01:56.160
Their machine learning API meets developers where they're at, regardless of their mastery

01:56.160 --> 02:01.400
of data science, so they can start cutting up predictive applications immediately and

02:01.400 --> 02:04.240
in their preferred programming language.

02:04.240 --> 02:08.600
It's as simple as loading your data and selecting the type of problem you want to solve.

02:08.600 --> 02:13.480
Their automated platform trains and selects the best model fit for your data and then outputs

02:13.480 --> 02:15.040
predictions.

02:15.040 --> 02:19.360
To learn more about Nexusos, be sure to check out the first episode in this series at

02:19.360 --> 02:27.640
twimla.com slash talk slash 69, where I speak with co-founders Ryan Sevy and Jason Montgomery.

02:27.640 --> 02:33.080
Be sure to also get your free Nexusos API key and discover how to start leveraging machine

02:33.080 --> 02:38.200
learning in your next project at nexosos.com slash twimble.

02:38.200 --> 02:44.400
In this episode, I speak with Allison Parrish, Poet and Professor at NYU in the Interactive

02:44.400 --> 02:46.520
Telecommunications Department.

02:46.520 --> 02:53.080
Allison's work centers around generated poetry via artificial intelligence and machine learning.

02:53.080 --> 02:56.920
She joins me prior to her conference talk on experimental creative writing with the

02:56.920 --> 02:58.680
vectorized word.

02:58.680 --> 03:04.480
In our time together, we discuss some of her research into computational poetry generation,

03:04.480 --> 03:09.720
actually performing AI-produced poetry and some of the methods and processes she uses

03:09.720 --> 03:20.040
for generating her work.

03:20.040 --> 03:26.600
Alright everyone, I am here at Strange Loop in St. Louis and I am with Allison Parrish.

03:26.600 --> 03:32.760
Allison is a teacher with the Interactive Telecommunications program at NYU and she is

03:32.760 --> 03:38.320
actually speaking later on today on experimental creative writing with the vectorized word.

03:38.320 --> 03:41.080
Welcome to the podcast Allison, it's great to have you.

03:41.080 --> 03:42.080
Thank you, it's great to be here.

03:42.080 --> 03:45.080
So why don't we get started by having you tell us a little bit about your background

03:45.080 --> 03:49.400
and how you got interested in machine learning and artificial intelligence?

03:49.400 --> 03:58.320
Okay, so I am a poet, sort of what I like to call myself primarily, but my educational

03:58.320 --> 04:02.640
background originally as an undergraduate was in linguistics and I've been a computer

04:02.640 --> 04:05.080
programmer for a really long time.

04:05.080 --> 04:09.040
Most of the poetry that I write has to do or is produced with procedures, so I write

04:09.040 --> 04:15.400
computer programs that produce poetry and I've been doing that for a really long time.

04:15.400 --> 04:18.760
Mostly I've done like Twitter bots and stuff like that.

04:18.760 --> 04:27.200
And sort of my poetic research right now is about like how do we find new ways of composition?

04:27.200 --> 04:32.640
Like what are new means of like putting text down onto the page?

04:32.640 --> 04:37.540
Just like me, generally I think of poetry we think of like a really artistic person

04:37.540 --> 04:41.640
or whatever who gets super inspired by nature or something and it's like looking out

04:41.640 --> 04:48.200
the kitchen window and sees the glint of dew off of the flower of a petal right to poem

04:48.200 --> 04:49.200
based on that inspiration.

04:49.200 --> 04:52.440
Oh, how do you do how you write it exactly?

04:52.440 --> 04:59.680
I'm interested in thinking about how can we expand the possible languages and methods

04:59.680 --> 05:05.800
that poets can use to write and how does that like you know what are the other poetic

05:05.800 --> 05:10.200
effects that can be made with text other than just like the sort of the conventional

05:10.200 --> 05:14.000
poetic or the conventional narrative way that the text works.

05:14.000 --> 05:18.400
So procedure is a great way to address that is the computer, you know the computer can

05:18.400 --> 05:22.720
choose something at random, the computer can have a system of rules for putting stuff

05:22.720 --> 05:27.880
together and those rules don't necessarily conform with our conventional ideas about how

05:27.880 --> 05:29.800
our text goes together.

05:29.800 --> 05:35.720
So I primarily have been thinking about artificial intelligence and machine learning as they've

05:35.720 --> 05:39.800
been in my practice for the past couple of years as a way to just do more of that work

05:39.800 --> 05:48.120
of trying to create tools for poets and programmers to be expressive with language in ways that

05:48.120 --> 05:49.720
they haven't been able to be.

05:49.720 --> 05:50.720
Okay.

05:50.720 --> 05:56.320
There's been a lot of work recently with folks basically training these LSTM neural

05:56.320 --> 06:02.120
formats with, you know, tons of text and then having them produce, I got to imagine

06:02.120 --> 06:06.160
someone's in poetry but movie scripts and all kinds of stuff.

06:06.160 --> 06:10.880
You do any of that as well or it sounds like you're looking for more structure in the

06:10.880 --> 06:11.880
type of things.

06:11.880 --> 06:18.640
I don't know if it's more structure or less structure or different structure.

06:18.640 --> 06:22.480
I mean, so I keep up with that research and I should have looked this up.

06:22.480 --> 06:28.960
There's the Turing tests and the creative arts, I think it's called, I haven't seen that

06:28.960 --> 06:33.120
but I imagine it's, when you look at this, do you think it was written by human?

06:33.120 --> 06:34.120
Yeah.

06:34.120 --> 06:35.120
That's exactly what it is.

06:35.120 --> 06:38.040
And they have this prize and when they first started out, it was just like a couple

06:38.040 --> 06:39.040
of different genres.

06:39.040 --> 06:40.040
They've had a different stuff.

06:40.040 --> 06:41.040
Now they haven't.

06:41.040 --> 06:44.920
They've had a prize for like the best sonnet and by best what they mean is the one that

06:44.920 --> 06:48.560
can like trick a person into thinking that it was written by a person.

06:48.560 --> 06:49.560
Okay.

06:49.560 --> 06:55.800
And that basically means that like there's always been a literature on computational poetry

06:55.800 --> 07:00.160
generation, but for like the past couple of years, it's all just been like sonnet generators,

07:00.160 --> 07:03.480
sonnet generators, sonnet generators, sonnet generators, sonnet generators.

07:03.480 --> 07:07.480
All with like slightly different methodologies, some of which you're using like deep learning

07:07.480 --> 07:08.680
some of which aren't.

07:08.680 --> 07:09.680
Okay.

07:09.680 --> 07:14.080
And I think that's super interesting research to keep up with that like those researchers

07:14.080 --> 07:18.640
are framing their problem that they're solving in terms of verisimilitude, like they're

07:18.640 --> 07:25.320
trying to make poems that resemble poems that are written by a person in a conventional

07:25.320 --> 07:27.120
typical manner.

07:27.120 --> 07:33.360
And then like in actually like in the paper in order to evaluate the work, they'll like

07:33.360 --> 07:38.800
do like a survey or something and ask people to judge whether or not it was conventionally

07:38.800 --> 07:41.240
created or not or what they do it.

07:41.240 --> 07:44.920
Which for me is like that's super uninteresting.

07:44.920 --> 07:47.640
It's like, I mean, that's a poet.

07:47.640 --> 07:51.000
As like a researcher, it's like, well, of course, that's cool.

07:51.000 --> 07:52.000
That's interesting.

07:52.000 --> 07:53.000
Right.

07:53.000 --> 07:57.600
But as a poet, it's like we already have a pretty efficient machine for writing sonnets

07:57.600 --> 07:59.000
and that's a person.

07:59.000 --> 08:03.440
Like if you find a person you give them $10,000 and you say, well, you write a sonnet,

08:03.440 --> 08:05.440
you're probably going to get a pretty good sonnet.

08:05.440 --> 08:09.880
I don't think anybody's been paid $10,000 to write a sonnet in the past, you know, a hundred

08:09.880 --> 08:11.680
years or whatever.

08:11.680 --> 08:17.040
So for me, I'm more interested in with that kind of stuff, I'm more interested in using

08:17.040 --> 08:24.080
machine learning for making open new avenues for being expressive in unexpected ways.

08:24.080 --> 08:28.120
I guess the answer is like I'm like trying to pick and choose from that research, the

08:28.120 --> 08:31.880
stuff that can be helpful to me and the stuff that's been really helpful to me over the

08:31.880 --> 08:37.440
past couple of years is just like this idea of a word vector and how it opens up this

08:37.440 --> 08:43.800
ability to compose texts differently because instead of working with discrete units,

08:43.800 --> 08:48.200
you're working with like just this kind of like blob that can be like stretched and molded

08:48.200 --> 08:50.080
and stuff like that.

08:50.080 --> 08:53.680
And I think that's like really useful raw material for a poet.

08:53.680 --> 08:54.680
Interesting.

08:54.680 --> 09:00.240
So I imagine your talk was about how you can stretch and mold the words with this blob.

09:00.240 --> 09:01.240
Right.

09:01.240 --> 09:05.960
So maybe walk us through kind of how you presented those ideas, what were you trying to

09:05.960 --> 09:06.960
leave folks with?

09:06.960 --> 09:14.760
The thing that I'm trying to leave people with is more just an idea of here's what computers

09:14.760 --> 09:17.800
can do with language that might not occur to people individually.

09:17.800 --> 09:23.280
I don't think that like the particular research that I'm doing is like that answer to computer

09:23.280 --> 09:25.320
generated poetry.

09:25.320 --> 09:29.800
But the stuff that I've been doing recently is, so if you take like these pre-trained

09:29.800 --> 09:35.480
word vectors from an algorithm like word to vac or the the glove vectors from Stanford,

09:35.480 --> 09:41.880
basically just get like, you know, four gigabytes of vectors that associate a word or a text

09:41.880 --> 09:45.240
file that associates a word with the vector of numbers.

09:45.240 --> 09:50.800
And once you have that vector of numbers, you can arrange them in sort of like a matrix

09:50.800 --> 09:54.640
that every word in a text corresponds to a vector.

09:54.640 --> 09:58.920
At that point, it's weirdly just like audio or image data.

09:58.920 --> 10:03.840
Like image data only has like four dimensions, let's say like the great red, green, blue,

10:03.840 --> 10:09.160
and alpha channel audio data usually just has the one dimension like the amplitude at

10:09.160 --> 10:11.320
that particular sample point.

10:11.320 --> 10:16.480
But text even though even though that data is like 300 dimensional or whatever from like

10:16.480 --> 10:18.640
a mathematical standpoint, it's the same.

10:18.640 --> 10:20.080
So there's like no reason.

10:20.080 --> 10:24.760
You can basically just like the same function that you would use in like, what's the name

10:24.760 --> 10:28.560
of the stupid Python library that has all of the.

10:28.560 --> 10:29.560
So I can learn.

10:29.560 --> 10:30.560
Yes.

10:30.560 --> 10:31.560
I use this every day.

10:31.560 --> 10:32.560
I don't know.

10:32.560 --> 10:36.560
I'd need any function inside get learned or numpy or whatever that you could like put

10:36.560 --> 10:41.000
an image file into and then get another image out and do that with the word vector as

10:41.000 --> 10:42.000
well.

10:42.000 --> 10:46.200
So this is like, this is an experiment that I did while I was doing a residency last

10:46.200 --> 10:52.240
summer, which is basically just like, what does it look like when you blend two text

10:52.240 --> 10:53.240
together?

10:53.240 --> 10:56.360
You just have like, here's here's text one, which is a matrix of word vectors, here's

10:56.360 --> 11:00.600
text two of the matrix of word vectors, what if you just like cross-fade between them?

11:00.600 --> 11:01.600
Okay.

11:01.600 --> 11:06.880
And then you can do things like blur the text, like apply, like algorithm to just like

11:06.880 --> 11:11.600
average the surrounding pixels, the same way that you would do it with an image or, oh

11:11.600 --> 11:16.640
wow, or resample it, like you can actually like take a text and then just say, instead

11:16.640 --> 11:22.040
of being 50 word vectors along, this will be five word vectors along with you again.

11:22.040 --> 11:27.320
And that's actually like one of the ways of doing text summarization is just like average

11:27.320 --> 11:28.920
all of the vectors in a sentence.

11:28.920 --> 11:32.200
And that average vector is like the thing that you use to represent the meaning of the

11:32.200 --> 11:33.200
sentence.

11:33.200 --> 11:36.840
There's like other more sophisticated ways to do that, but that's like surprisingly

11:36.840 --> 11:38.400
performs well on all of these tasks.

11:38.400 --> 11:39.400
Oh wow.

11:39.400 --> 11:43.480
Yeah, and then so that's the kind of like thoughts that I've been bringing to this recently

11:43.480 --> 11:47.360
in terms of the operations you can perform on the vectors.

11:47.360 --> 11:53.280
And the idea of that, the idea behind that is just that now you can with word vectors

11:53.280 --> 12:00.400
as the medium for text, suddenly you can think of composing a text not consisting of like

12:00.400 --> 12:07.000
typing letters on a keyboard or selecting like words from autocomplete, but instead you

12:07.000 --> 12:11.960
have this like very physical analog continuous way of saying like, you know, I can turn

12:11.960 --> 12:17.400
a knob, like I could turn like a physical knob to make this text longer or shorter or

12:17.400 --> 12:23.000
to add in these semantics from this other text, it's really interesting.

12:23.000 --> 12:30.120
And now I'm taking that very literally, well not a literal knob, but I'm thinking of it

12:30.120 --> 12:35.040
would be kind of cool to have some kind of goo representation of all of these vectors

12:35.040 --> 12:42.040
and use that in a way to generate poetry or other written works like, are we there yet?

12:42.040 --> 12:43.040
Is that something like that?

12:43.040 --> 12:49.640
Oh, I've actually, I've been performing poetry with an interface like that with an actual

12:49.640 --> 12:57.040
like MIDI controller with actual, and one of the things that is in the talk is I'm working

12:57.040 --> 13:02.120
on sort of like a prototype for an interactive environment that's sort of like, do you know

13:02.120 --> 13:05.080
like like processing the interactive programming framework?

13:05.080 --> 13:06.080
I've heard it.

13:06.080 --> 13:07.080
I've heard the phrase.

13:07.080 --> 13:11.920
It basically like, it opens up a canvas object in the browser and then gives you a function

13:11.920 --> 13:15.160
that runs 30 times a second or whatever.

13:15.160 --> 13:19.360
And then inside of that function, you can call like a function to draw any lips or whatever

13:19.360 --> 13:21.440
and you can have it follow the mouse and stuff like that.

13:21.440 --> 13:26.960
So I am working on a prototype interface that's sort of like that except for word vectors.

13:26.960 --> 13:31.200
So instead of like a pixel buffer, the main thing that you're operating on in real time

13:31.200 --> 13:34.520
is a buffer of word vectors.

13:34.520 --> 13:39.080
And then on every frame, it's like showing you the word vector in the database closest

13:39.080 --> 13:41.720
to whatever that value happens to be.

13:41.720 --> 13:46.760
So I mean, the thing about it is that like, I think a lot of the problems that I'm trying

13:46.760 --> 13:49.600
to solve are poetic problems, right?

13:49.600 --> 13:54.000
And I think that a lot of like artificial intelligence researchers, you know, because

13:54.000 --> 13:59.760
this is what they're interested in are interested in solving these problems of like, you know,

13:59.760 --> 14:04.280
let's think about how to formalize what a poem is so that we can generate better poems,

14:04.280 --> 14:05.280
right?

14:05.280 --> 14:09.400
Because that's sort of like the teleology of artificial intelligence research is to like

14:09.400 --> 14:16.280
duplicate flavor, but I'm more interested in so that like the stuff that my tool makes

14:16.280 --> 14:19.000
is not stuff that makes sense in the conventional way.

14:19.000 --> 14:24.840
It's less, it's more cartridge-stined, mainstream contemporary poet.

14:24.840 --> 14:29.040
I can't think of a mainstream contemporary poet that I want to like make fun of for making

14:29.040 --> 14:31.040
poetry that makes too much sense.

14:31.040 --> 14:33.000
So yeah, that's that's sort of my goal.

14:33.000 --> 14:36.040
Is it like a linearity?

14:36.040 --> 14:42.560
It's like less linear than what you typically expect or is that too simple a characterization?

14:42.560 --> 14:43.840
What do you mean by linear?

14:43.840 --> 14:51.000
I guess I'm imagining the poems at the tool, that the thing that you're, the poems that

14:51.000 --> 14:57.080
you're creating, being more, I guess I'm imagining just like random words popping up.

14:57.080 --> 14:58.080
Yeah.

14:58.080 --> 14:59.080
Is it kind of like that?

14:59.080 --> 15:00.080
Or is it kind of like that?

15:00.080 --> 15:01.080
Okay.

15:01.080 --> 15:02.080
It comes out looking like random words.

15:02.080 --> 15:07.960
But yet there's some fundamental element of it that's still at least to use as this

15:07.960 --> 15:08.960
is poetry.

15:08.960 --> 15:09.960
Yeah.

15:09.960 --> 15:13.720
I think the thing that's interesting to me is the thing that that has been interesting

15:13.720 --> 15:19.960
to me as I've been presenting this work, I mean I, I, I present this work a lot to,

15:19.960 --> 15:25.320
to people in, you know, machine learning, artificial intelligence research, but then

15:25.320 --> 15:28.920
I also just like perform at regular poetry readings.

15:28.920 --> 15:34.160
And the thing that's actually been interesting and encouraging to me is that like these metaphors

15:34.160 --> 15:40.800
actually make sense to people like that idea of blending between two texts like averaging

15:40.800 --> 15:45.880
two words and finding the word in the middle and then making an entire text out of those

15:45.880 --> 15:48.320
words that are blended between the two.

15:48.320 --> 15:53.040
As long as you can show like some delta as long as you can say like here's where it started

15:53.040 --> 15:57.160
and here's where it's ending up, people will appreciate what's happening at each of

15:57.160 --> 15:59.640
these of those intermediary steps.

15:59.640 --> 16:05.520
I think this is a rule about poetry in general, the more that people understand the rules

16:05.520 --> 16:11.400
that condition the text, the more that they understand like the aesthetics of the mechanisms,

16:11.400 --> 16:13.360
the more they actually understand the text itself.

16:13.360 --> 16:14.360
Okay.

16:14.360 --> 16:20.320
So that's what I'm, the, the idea is that like if you just looked at it in isolation,

16:20.320 --> 16:25.920
you might think oh this is nonsense, but once you understand the, the procedure behind

16:25.920 --> 16:30.320
it, once you understand what's going into it, then suddenly it becomes this like massive

16:30.320 --> 16:34.680
new ability of this massive new way of thinking about language and text.

16:34.680 --> 16:39.320
That question of interpretability is like super, super important to me.

16:39.320 --> 16:44.960
Interpretability in the sense you just described, understanding the context for a particular poem

16:44.960 --> 16:50.400
and the receiver's ability to interpret its significance.

16:50.400 --> 16:51.400
Right.

16:51.400 --> 16:57.920
I have like, if you think about in, in the visual arts and with music, there are all kinds

16:57.920 --> 17:01.200
of different ways of interpreting those artifacts.

17:01.200 --> 17:02.200
Sure.

17:02.200 --> 17:09.040
Like, since the invention of photography almost all visual art has turned away from depiction.

17:09.040 --> 17:14.840
Like, people still paint things, but since, you know, as soon as photography was invented,

17:14.840 --> 17:19.440
you had impressionism and interpreting and impressionist painting is very different

17:19.440 --> 17:25.200
from interpreting a painting that like actually depicts something, interpreting like Jackson

17:25.200 --> 17:30.080
Pollock painting or a Mark Rothko painting or something like that is also very different.

17:30.080 --> 17:33.280
So there's this like whole new language for interpreting art that doesn't have to do

17:33.280 --> 17:36.680
with that conventional way of interpreting it.

17:36.680 --> 17:41.200
And the same thing with, with music, like we have, music with music it's even more, it's

17:41.200 --> 17:47.200
even more striking because music is almost never, you know, intended to convey a thought,

17:47.200 --> 17:48.200
right?

17:48.200 --> 17:52.200
Sometimes it is, and we have languages of citizens for doing that.

17:52.200 --> 17:59.360
I mean, like, when Bach was writing a chorale or whatever, he wasn't like, that wasn't

17:59.360 --> 18:04.520
meaning to say, like, you know, remember to buy eggs at the grocery store or whatever,

18:04.520 --> 18:05.520
right?

18:05.520 --> 18:09.760
Like, it was, it was trying to, it was trying to create a particular aesthetic effect.

18:09.760 --> 18:10.760
Okay.

18:10.760 --> 18:16.040
And then there's some music, like, you know, music with lyrics or whatever, or like, music

18:16.040 --> 18:20.880
with light motif from the romantic era, like those do have like specific symbolism and

18:20.880 --> 18:22.720
they can be interpreted.

18:22.720 --> 18:29.000
But in general, with music, like it's not, you don't listen to a song hoping to like,

18:29.000 --> 18:33.960
immediately retrieve like a propositional sentence that the person says, right?

18:33.960 --> 18:36.040
The song isn't something coded phrase.

18:36.040 --> 18:37.040
Right.

18:37.040 --> 18:38.040
Right.

18:38.040 --> 18:39.040
Exactly.

18:39.040 --> 18:41.960
It might include phrases, but it's not, it is not itself a way of language to fix

18:41.960 --> 18:48.000
expression, or I should say, like, a way to communicate a single idea.

18:48.000 --> 18:51.440
And lots of poets have been working with that same concept for a really long time.

18:51.440 --> 18:59.000
Like, that's been part of like the modernist tradition since data, maybe even earlier.

18:59.000 --> 19:03.360
But I think a lot of contemporary poets who are working in this area are sort of stuck

19:03.360 --> 19:09.120
on the idea that either it has to make total sense in a conventional way, or it has to

19:09.120 --> 19:10.960
be completely nonsense at all.

19:10.960 --> 19:17.840
Like it's either just nonsense, unreadable nonsense, or it's this thing that is conventional

19:17.840 --> 19:19.920
and to brag the way that it means.

19:19.920 --> 19:23.320
And I'm trying to investigate that little middle area.

19:23.320 --> 19:24.320
Yeah.

19:24.320 --> 19:28.120
And that's where machine learning is so useful, because like that's sort of, for me, at

19:28.120 --> 19:31.360
least aesthetically, that's the same place that machine learning inhabits.

19:31.360 --> 19:38.200
It's like, well, it's not, it's not a person in society with all of that context.

19:38.200 --> 19:45.560
It's sort of like trying to imitate and learn some of those same patterns from data.

19:45.560 --> 19:49.520
And when machine learning is most interesting to me is when it's when it's like slightly

19:49.520 --> 19:52.040
broken, when it gives you stuff, it's unexpected.

19:52.040 --> 19:59.840
So you've got this apparatus that allows you to manipulate word vectors and produce poetry.

19:59.840 --> 20:05.200
But there's also this other step that is producing the word vectors, the vectorization from

20:05.200 --> 20:10.160
a corpus that usually influences the result.

20:10.160 --> 20:12.760
Do you incorporate that step in as well?

20:12.760 --> 20:19.680
Or do you take that as a given for a piece or a body of work?

20:19.680 --> 20:25.440
With the semantic stuff, I've been taking it as a given, because I don't want to spend

20:25.440 --> 20:33.800
the money on the easy-to-manage to do a big word-to-back thing on gigabytes and gigabytes.

20:33.800 --> 20:37.400
I think there's something to be said about using that materiality.

20:37.400 --> 20:43.600
A lot has been written about the bias that is in word vectors and how, you know, like

20:43.600 --> 20:49.920
the analogies it'll show like, man is too programmer as woman is to housewife for whatever.

20:49.920 --> 20:55.960
I kind of like how that materiality is present in the stuff that comes out of us.

20:55.960 --> 21:01.080
Part of the reason of making stuff, of making poetry is to expose those, to expose and

21:01.080 --> 21:08.040
help people understand the ways that language is biased.

21:08.040 --> 21:12.720
So I don't see that as being entirely incompatible with what I'm trying to do.

21:12.720 --> 21:18.320
And so do you use a specific pre-trained word-to-back model?

21:18.320 --> 21:21.120
I've been using the glove vectors recently.

21:21.120 --> 21:22.120
Glove vectors.

21:22.120 --> 21:26.960
So there's a, and those are from Stanford and they're not, yeah, it's a different algorithm

21:26.960 --> 21:29.720
from word-to-back but it basically ends up doing the same thing.

21:29.720 --> 21:34.320
But it's not just the algorithm, it's a vector system that's pre-trained for you.

21:34.320 --> 21:38.040
Yeah, you just download the text file and it's like word and then 300 numbers.

21:38.040 --> 21:39.840
Just got it, got it.

21:39.840 --> 21:45.640
And there's a wonderful library for Python called Spacey for doing natural language processing

21:45.640 --> 21:46.640
stuff.

21:46.640 --> 21:47.640
Okay.

21:47.640 --> 21:50.040
And it comes with those vectors by default or at least it did in most recent versions.

21:50.040 --> 21:51.040
Okay.

21:51.040 --> 21:52.320
So those are like super easy to use.

21:52.320 --> 21:53.320
Oh nice.

21:53.320 --> 21:56.520
The other thing I've been working on recently, and this is the thing that I'm giving

21:56.520 --> 22:04.120
a talk on next week, experimentally, eyeing games is phonetic similarity vectors.

22:04.120 --> 22:09.000
So the same way that word vectors give you like two words with similar meanings will have

22:09.000 --> 22:10.000
similar vectors.

22:10.000 --> 22:11.000
Right.

22:11.000 --> 22:14.960
The thing that I've been working on for like the past year and I'm an arts professor.

22:14.960 --> 22:17.440
So I don't have a lot of resources to work on this.

22:17.440 --> 22:21.840
I'm sure a professor in an actual computer science department would have been able to

22:21.840 --> 22:23.320
get a grad student to do this.

22:23.320 --> 22:30.800
But for me, it was a new adventure, was trying to come up with a system of vectors where two

22:30.800 --> 22:36.320
words with similar pronunciations will have similar vectors so that like the word back

22:36.320 --> 22:39.640
and pack would have vectors that are similar to each other.

22:39.640 --> 22:44.840
The idea being that then you could come up with vector representations of sentences or

22:44.840 --> 22:51.040
lines of poetry and then determine how similar they are in sound and then do other interesting

22:51.040 --> 22:52.040
stuff with that.

22:52.040 --> 22:56.480
The same way that you can like do analogies with word vectors, you'd be able to do analogies

22:56.480 --> 22:58.000
with the way the word sounds.

22:58.000 --> 22:59.000
Huh.

22:59.000 --> 23:00.000
Interesting.

23:00.000 --> 23:05.120
It also makes me think of, you know, so much of conventional poetry or rhyming words

23:05.120 --> 23:09.280
and you could do something similar with rhyming vectors or something.

23:09.280 --> 23:10.280
Yeah.

23:10.280 --> 23:11.280
Yeah.

23:11.280 --> 23:18.840
Rhyming is difficult because it's like really easy to, it's really easy to like mechanically

23:18.840 --> 23:21.720
identify rhymes and attacks.

23:21.720 --> 23:26.960
If you have a phonetic transcription of the text, then like you can just look at like particular

23:26.960 --> 23:31.800
phonemes in particular places and then you know whether or not it rhymes.

23:31.800 --> 23:37.920
Finding slant rhymes is harder because then you have to have some criterion that tells

23:37.920 --> 23:43.800
you how similar are these two sounds and if they're similar enough that it counts as a

23:43.800 --> 23:44.800
slant rhyme.

23:44.800 --> 23:45.800
What did example of a slant rhyme?

23:45.800 --> 23:50.760
So a slant rhyme would be like, it would be like in a song where somebody tries to rhyme

23:50.760 --> 23:57.280
like, hit with pick or something like that where the sound of the end doesn't quite match.

23:57.280 --> 23:58.280
Okay.

23:58.280 --> 24:02.280
And like almost all contemporary lyrics use slant rhymes all the time.

24:02.280 --> 24:03.280
Okay.

24:03.280 --> 24:05.800
I wish I had a better example at the top of my head.

24:05.800 --> 24:11.080
But the point is that like it doesn't have to be precise for a decount as a rhyme.

24:11.080 --> 24:12.080
Okay.

24:12.080 --> 24:17.120
And so a lot of the poetry papers, a lot of the poetry generation papers that you read,

24:17.120 --> 24:21.920
now we're like sort of trying to find rule-based ways to solve that problem.

24:21.920 --> 24:25.560
Like even in a paper that's otherwise all about deep learning, we'll have this like little

24:25.560 --> 24:30.720
section that's like, well and then we use the sequence of if-else statements to determine

24:30.720 --> 24:32.720
whether or not these are slant rhymes.

24:32.720 --> 24:33.720
Okay.

24:33.720 --> 24:39.440
One of the points of doing the phonetic similarity of actors would be able to like just say like,

24:39.440 --> 24:46.840
okay do your surface syntactic and semantic generation of the text and then compare the

24:46.840 --> 24:51.240
phonetic similarity, then you could even like compare the phonetic similarity across

24:51.240 --> 24:56.400
like moving windows inside of the text and you'd be able to like detect where two lines

24:56.400 --> 25:00.920
are most similar in the way that they sound so that you'd be able to have like sort of

25:00.920 --> 25:04.080
like a useful tool for doing that kind of set land detection.

25:04.080 --> 25:05.080
Okay.

25:05.080 --> 25:10.480
But then also poetry uses sound in ways that isn't just rhymes, right?

25:10.480 --> 25:11.480
Right.

25:11.480 --> 25:16.760
Especially in like contemporary hip-hop like there's just like all an entire line will

25:16.760 --> 25:21.960
consist of sounds that are similar in some way and there's like this really complicated

25:21.960 --> 25:26.840
interplay between between lines where sounds are not quite the same but they're similar

25:26.840 --> 25:31.680
and they produce these from like an articulatory standpoint, the way that your mouth moves,

25:31.680 --> 25:33.800
they have this particular feeling.

25:33.800 --> 25:38.000
And then like you know classical poets or even like romantic poets or whatever, you'll

25:38.000 --> 25:43.360
see those patches of things like eliteration and ascendance and stuff like that, all

25:43.360 --> 25:48.480
of which you can study on like a rule-based basis, but one of the things that I wanted

25:48.480 --> 25:54.640
to be able to do with these phonetic similarity vectors is allow you to just write an equation

25:54.640 --> 25:59.760
just like look at a text and then you have a number that represents what it sounds like

25:59.760 --> 26:04.160
and then you can just you know use regular Euclidian and Neurocene distance to or

26:04.160 --> 26:08.240
this similarity to compare those two.

26:08.240 --> 26:12.600
And so whereas the previous work that you were describing is more about generation, this

26:12.600 --> 26:14.320
is more about analysis?

26:14.320 --> 26:16.520
No, it's it's the more about generation.

26:16.520 --> 26:23.640
No, again, I'm trying to solve poetic problems, not like linguistic or research problems.

26:23.640 --> 26:27.800
That was just like this kind of happy result of making these vectors.

26:27.800 --> 26:32.720
I was I made the vectors because I was I was working on a book of poetry for for counter

26:32.720 --> 26:39.080
path and I what I wanted to be able to do is just take the entire corpus and poetry in

26:39.080 --> 26:45.120
Project Gutenberg and I basically do a random walk through the sounds of it.

26:45.120 --> 26:52.280
So arrange every line of poetry in this database of public domain text and then just be able

26:52.280 --> 26:57.240
to go from one line to the next based on how similar they sounded to each other.

26:57.240 --> 27:02.000
That was the original like poetic idea because I love I love like poetic flow.

27:02.000 --> 27:06.640
I love like that that effect that you have in a poem where it just feels like all of

27:06.640 --> 27:07.960
the sounds are coming together.

27:07.960 --> 27:12.720
And if you could get that from this huge repository of poems, that'd be pretty cool.

27:12.720 --> 27:13.720
Right.

27:13.720 --> 27:18.600
So that was like my main impetus, but then I was like, I should write a paper about this.

27:18.600 --> 27:23.400
Papers usually have a section on whether or not it meets some criterion and their success.

27:23.400 --> 27:26.200
I should try to make that determination.

27:26.200 --> 27:29.880
And so then there's like this whole literature about phonetic similarity and I was able

27:29.880 --> 27:34.320
to like pull a couple of test data sets out of there and prove that like actually the

27:34.320 --> 27:37.840
algorithm that I was working on, you know, was performed pretty well on these tests.

27:37.840 --> 27:39.080
Oh, wow.

27:39.080 --> 27:43.000
And so from that, like these ideas of how you might be able to apply it in these other

27:43.000 --> 27:44.880
contexts, just sort of valid naturally.

27:44.880 --> 27:49.520
And I don't think, you know, because I'm an artist and I only have undergraduate degree

27:49.520 --> 27:54.400
in linguistics, I don't think that the solution that I came up with is like the ultimate best

27:54.400 --> 27:57.880
solution, but it definitely helped me solve my problem.

27:57.880 --> 27:58.880
Awesome.

27:58.880 --> 27:59.880
Awesome.

27:59.880 --> 28:20.040
Well, thanks so much for sharing all this with us is really I don't get to talk about poetry

28:20.040 --> 28:21.040
very often.

28:21.040 --> 28:22.040
People don't.

28:22.040 --> 28:26.120
And it makes me realize how little I know about it, so I need to figure out how to fix

28:26.120 --> 28:27.120
that.

28:27.120 --> 28:31.120
I mean, as anything about poetry, even poets don't know anything about poetry.

28:31.120 --> 28:32.920
Is that part of the essence of the thing?

28:32.920 --> 28:34.680
I think it partially is.

28:34.680 --> 28:39.520
I found recently that I'd like to read about poetry more than I like poetry itself.

28:39.520 --> 28:42.600
So maybe that's a good spring thing.

28:42.600 --> 28:43.600
Awesome.

28:43.600 --> 28:47.600
Well, thanks so much, Allison.

28:47.600 --> 28:53.200
All right, everyone, that's our show for today.

28:53.200 --> 28:57.920
Thanks so much for listening and for your continued feedback and support.

28:57.920 --> 29:03.120
For more information on Allison or any of the topics covered in this episode, head on

29:03.120 --> 29:07.520
over to twomlai.com slash talk slash 72.

29:07.520 --> 29:15.000
To follow along with our strange loop 2017 series, visit twomlai.com slash ST loop.

29:15.000 --> 29:20.760
Of course, you can send along your feedback or questions via Twitter to act twomlai or

29:20.760 --> 29:25.760
at Sam Charrington or leave a comment right on the show notes page.

29:25.760 --> 29:29.120
Thanks again to Nexusos for their sponsorship of the show.

29:29.120 --> 29:36.160
Check out twomlai.com slash talk slash 69 to hear my interview with the company founders

29:36.160 --> 29:43.640
and visit nexosis.com slash twimmel for more information and to try their API for free.

29:43.640 --> 29:53.640
Thanks again for listening and catch you next time.

