1
00:00:00,000 --> 00:00:04,720
Hey, what's up everyone? I'm super excited to kick off another episode of the Twoma AI podcast

2
00:00:04,720 --> 00:00:10,960
I am of course your host Sam Charrington and today I am coming to you live from the future frequency

3
00:00:10,960 --> 00:00:18,240
podcast studio at the AWS re-invent conference in Las Vegas, Nevada. I am joined by Michael Kerns

4
00:00:18,880 --> 00:00:24,160
Michael is a professor in the Department of Computer and Information Science at UPenn as well as an

5
00:00:24,160 --> 00:00:30,960
Amazon scholar with a focus on fairness and privacy in machine learning and related topics at AWS.

6
00:00:31,840 --> 00:00:36,320
A quick note if I sound a little funny do not try to adjust your audio settings it is me

7
00:00:37,200 --> 00:00:42,800
after a few days here at re-invent and in this zero humidity desert my usual podcast voice is

8
00:00:42,800 --> 00:00:50,240
given away to a little bit of a very white slow jams voice but before we get going be sure to

9
00:00:50,240 --> 00:00:53,760
take a moment to hit that subscribe button wherever you're listening to today's show and if you

10
00:00:53,760 --> 00:00:59,360
want to check out the studio you can bounce over to youtube to check us out. Michael welcome to the

11
00:00:59,360 --> 00:01:04,320
podcast thank you great to be here it's great to have you on the show we had the the pleasure of

12
00:01:04,320 --> 00:01:11,120
sitting next to one another at a dinner a couple of months ago I guess in New York and had a great

13
00:01:11,120 --> 00:01:17,440
conversation I'm gonna struggle to try and recreate a lot of that conversation because we touched

14
00:01:17,440 --> 00:01:24,640
we touched on you know everything from philosophy to your work of course but you know thanks once

15
00:01:24,640 --> 00:01:29,840
again for for joining me here yeah I'm excited to be here I'd love to get started by having you

16
00:01:29,840 --> 00:01:35,040
share a little bit about your background and how you came to work in machine learning yeah so first

17
00:01:35,040 --> 00:01:40,480
of all I'm an old-timer I've been around a long time and so I went to graduate school in the late

18
00:01:40,480 --> 00:01:47,520
1980s just to set the context for your listeners who might be considerably younger these days um the

19
00:01:47,520 --> 00:01:53,360
field of machine learning barely existed in the late 1980s uh-huh the conferences that we now know

20
00:01:53,360 --> 00:01:58,320
as noreps and icml were really in their first couple of years at that point and machine learning

21
00:01:58,320 --> 00:02:05,360
at that time was considered sort of a a boutique obscure subfield of the then discredited larger

22
00:02:05,360 --> 00:02:10,880
field of AI which was going through its famous AI winter so suffice to say I've seen a lot of

23
00:02:10,880 --> 00:02:16,880
change in my career I initially came to machine learning from really an algorithmic and theoretical

24
00:02:16,880 --> 00:02:23,360
angle so in those early days when you know machine learning barely existed in particular

25
00:02:23,360 --> 00:02:27,680
there weren't sort of formal foundations or models for thinking about machine learning the way

26
00:02:27,680 --> 00:02:33,360
we're used to now and at that time there was also of course the ongoing kind of debate between

27
00:02:33,360 --> 00:02:38,720
people that came to AI from a more logical formalism and those that were starting to adopt more

28
00:02:38,720 --> 00:02:45,760
probabilistic formalism which of course is by far and away dominated today and so at that time

29
00:02:45,760 --> 00:02:51,280
there were very few ways of thinking rigorously about machine learning and making comparisons between

30
00:02:51,280 --> 00:02:56,080
different algorithms or even saying what would constitute good performance for an algorithm

31
00:02:56,080 --> 00:03:00,320
and as somebody who came to computer science in general really through the theoretical computer

32
00:03:00,320 --> 00:03:08,880
science approach you know I liked theoretical computer science a lot and still do but I was always

33
00:03:08,880 --> 00:03:13,520
interested in its application to sort of unusual areas so a lot of theoretical computer science is

34
00:03:13,520 --> 00:03:18,480
about very practical algorithmic problems like you know traveling salesmen for instance

35
00:03:19,200 --> 00:03:24,480
and I knew that wasn't kind of my bag and so I you know went to graduate school particularly to

36
00:03:24,480 --> 00:03:29,600
work with somebody who was starting to think my advisor a less valiant who was starting to think

37
00:03:29,600 --> 00:03:34,880
about kind of mathematical ways and algorithmic ways of thinking about machine learning and so I

38
00:03:34,880 --> 00:03:40,160
did that and then you know spent the first decade of my career at the late great think tank bell

39
00:03:40,160 --> 00:03:46,400
laboratories in Marie-Hell New Jersey where by the way a lot of you know major the luminaries of

40
00:03:46,400 --> 00:03:53,120
the field like Yannick-Lacoon were colleagues and Rob Shapiri and Vladimir Vapnik it was just a

41
00:03:53,120 --> 00:03:57,920
gold and a year for the early days of machine learning had great colleagues and all of our time to

42
00:03:57,920 --> 00:04:02,960
do research and eventually we all migrated you know to universities and then in some cases back

43
00:04:02,960 --> 00:04:07,920
to industry as well but that's already how I came to the topic I don't recall if I mentioned to you

44
00:04:07,920 --> 00:04:14,480
that I did a summer at Bell Labs I was based in Whipney okay New Jersey during grad school

45
00:04:14,480 --> 00:04:20,960
okay I focused on statistical modeling of computer networks okay yeah yeah it was just it was

46
00:04:20,960 --> 00:04:25,360
a great place it was I mean it still exists but it's not it's not the same yeah yeah it's a

47
00:04:25,360 --> 00:04:31,840
wonderful and I didn't think of it this way at the time but it was a great alternative to being

48
00:04:31,840 --> 00:04:38,400
a junior faculty member somewhere because you know you had all of your time for research you didn't

49
00:04:38,400 --> 00:04:43,120
need to think about teaching or sitting on committees or writing grants or the like and so in

50
00:04:43,120 --> 00:04:48,400
some ways if you were really dialed in on your research you could be much more productive from a

51
00:04:48,400 --> 00:04:53,520
research perspective than you could have been in an analogous position as a junior faculty member

52
00:04:53,520 --> 00:04:58,160
where you would have had all these other you know concerns as well as the pressure of getting tenure

53
00:04:58,720 --> 00:05:04,160
and as you mentioned the brain trust there was just it was great yeah yeah all of us whenever we

54
00:05:04,160 --> 00:05:08,880
get together and you know we we try to do that at the big conferences when we're all there we're

55
00:05:08,880 --> 00:05:14,560
always you know immediately devolved into reminiscing and telling stories from that decade it was great

56
00:05:15,760 --> 00:05:20,720
and so tell us a little bit about the focus of your research nowadays yeah so you know I've been

57
00:05:20,720 --> 00:05:25,520
working in machine learning for a long time and as time has gone on I've kind of evolved a bit

58
00:05:25,520 --> 00:05:31,680
too and so even though I still kind of come at many problems primarily from a mathematical algorithmic

59
00:05:31,680 --> 00:05:38,400
perspective first I do get involved in quite a bit of experimental work and you know I think like

60
00:05:38,400 --> 00:05:43,840
everyone else in machine learning I've watched roughly the last decade with you know some amount

61
00:05:43,840 --> 00:05:49,840
of surprise in alarm I mean this field that was nothing when I started in it is now this successful

62
00:05:49,840 --> 00:05:57,040
standalone industry and you know just to give some subjective history of let's say the time

63
00:05:57,040 --> 00:06:02,480
since early you know early 2010s when deep learning first started to become a powerful technology

64
00:06:03,360 --> 00:06:08,160
you know I think I along with men you know my colleagues kind of shared in the excitement

65
00:06:08,160 --> 00:06:14,640
of that initial period and you know big problems being solved that had you know before before

66
00:06:14,640 --> 00:06:22,720
that been very intractable and then you know around 2015 or 2016 all of us scientists at the same

67
00:06:22,720 --> 00:06:29,440
time that society learned it realized that there are harmful side effects to trained models

68
00:06:29,440 --> 00:06:35,680
in ML if one is not careful and so there was a bit of a buzzkill I think around you know 2015

69
00:06:35,680 --> 00:06:42,640
2016 and you know like many of my colleagues I think my first thought about this is like okay these

70
00:06:42,640 --> 00:06:48,160
are serious problems we do not want to be training models that are you know making consequential

71
00:06:48,160 --> 00:06:53,600
decisions about ordinary people that exhibit you know significant demographic bias for instance

72
00:06:54,400 --> 00:06:59,840
and being a scientist first and foremost my first thought on this topic was to think about

73
00:06:59,840 --> 00:07:04,160
technical solutions to those problems you know in other words if we don't like something about

74
00:07:04,160 --> 00:07:09,680
the behavior of our trained models I mean after all we trained them so we could think about changing

75
00:07:09,680 --> 00:07:14,560
the way we train them in the first place rather than waiting for harms to occur and then looking

76
00:07:14,560 --> 00:07:20,560
for non-technical solutions yeah I think I've come to appreciate especially in the time I've

77
00:07:20,560 --> 00:07:27,600
spent an Amazon that you need non-technical solutions as well and that includes you know diversity

78
00:07:27,600 --> 00:07:32,880
of inputs to the design process of products and services diversity of technical teams that are

79
00:07:32,880 --> 00:07:39,440
training models you need people with legal regulatory public policy backgrounds as well but

80
00:07:39,440 --> 00:07:45,360
but the way I got into you know what what is now called responsible AI was primarily first thinking

81
00:07:45,360 --> 00:07:50,720
about how could we change the way we do ML in a way that would mitigate things like demographic

82
00:07:50,720 --> 00:08:00,800
bias privacy leaks and the like and we'll return and jump deeper into your research on those areas

83
00:08:00,800 --> 00:08:08,240
but before we do share a little bit about your role at Amazon yeah so I'm first of all part of this

84
00:08:08,240 --> 00:08:14,480
very clever mechanism that Amazon has called the Amazon Scholar Program which makes it very very

85
00:08:14,480 --> 00:08:22,080
flexible and easy for people like me to spend significant time at Amazon while firmly maintaining

86
00:08:22,080 --> 00:08:27,600
our roles in academia and also to be able to ramp you know sort of our commitment level to Amazon

87
00:08:27,600 --> 00:08:36,320
up and down so for example you know utility academics yes exactly and so right right and so I've

88
00:08:36,320 --> 00:08:42,080
spent the last three summers full time at Amazon and I'm quite involved during the year and so I'm

89
00:08:42,080 --> 00:08:48,160
basically part of a constellation of teams within AWS that think seriously about many different

90
00:08:48,160 --> 00:08:55,840
aspects of responsible AI one of those teams is a centralized team that I participated in the

91
00:08:55,840 --> 00:09:03,280
formation of when I joined back in the summer of 2020 which is designed to be a centralized team

92
00:09:03,280 --> 00:09:09,120
that works with the product and service teams on careful quantitative assessments of different

93
00:09:09,120 --> 00:09:13,920
responsible AI principles in our trained models and services so this would include things like

94
00:09:13,920 --> 00:09:18,960
demographic fairness it would include things like thinking about you know whether what are the

95
00:09:18,960 --> 00:09:24,480
risks of that a trained model might exfiltrate inadvertently properties of the training data for

96
00:09:24,480 --> 00:09:30,400
example we think about robustness explainability you know all the things you hear about when the

97
00:09:30,400 --> 00:09:37,200
topic of responsible AI is mentioned you know I should note that even prior to my arrival when I

98
00:09:37,200 --> 00:09:42,960
got there was very clear to me that many of the product and service teams were already doing

99
00:09:42,960 --> 00:09:48,320
this kind of work even though it wasn't called responsible AI at the time very seriously on their

100
00:09:48,320 --> 00:09:53,760
own so for example when I started talking to people who work on Amazon transcribe which is our

101
00:09:53,760 --> 00:09:59,680
speech transcription service you know I the first thing I learned that surprised me is that just

102
00:09:59,680 --> 00:10:05,680
in North American English alone there are dozens and dozens of regional accents and dialects

103
00:10:05,680 --> 00:10:10,640
and each one of these has different properties and presents different challenges for speech recognition

104
00:10:10,640 --> 00:10:17,680
in transcription and so long before I showed up that team diligently goes out and collects and

105
00:10:17,680 --> 00:10:25,120
annotates spoken you know spoken data that's then transcripted by transcribed by humans in order

106
00:10:25,120 --> 00:10:31,520
to do both bias assessments and to improve training of our models what was different about this

107
00:10:31,520 --> 00:10:37,360
centralized team was that there were a couple of reasons for it one was it felt that this topic

108
00:10:37,360 --> 00:10:43,200
was becoming sufficiently important and serious that it merited having a centralized team that first

109
00:10:43,200 --> 00:10:50,000
of all had a certain arms length objectivity and distance from the product teams themselves

110
00:10:50,000 --> 00:10:53,920
but of course we need to work with the product and service teams on these audits and then the other

111
00:10:53,920 --> 00:11:02,480
part of this centralized team is meant to sort of codify best practices collect data sets that

112
00:11:02,480 --> 00:11:07,920
might be able to be used for assessments on multiple different products and services and eventually

113
00:11:07,920 --> 00:11:13,440
I think develop platforms and tooling around responsible AI that can be turned back to the product

114
00:11:13,440 --> 00:11:19,840
teams to make their work more efficient and higher quality and so that's that's sort of the science

115
00:11:19,840 --> 00:11:26,560
end of the work I do at Amazon but then I often get pulled into discussions about you know how

116
00:11:26,560 --> 00:11:33,520
we talk about responsible AI in public via PR and you know analyst relations had get occasionally

117
00:11:33,520 --> 00:11:39,360
involved in public policy discussions and legal and regulatory discussions and that's an entirely

118
00:11:39,360 --> 00:11:45,440
you know that's a very interesting evolving landscape itself that I think we'll see a lot of

119
00:11:45,440 --> 00:11:51,600
important developments in the next five years or so and this is kind of an approximate description

120
00:11:51,600 --> 00:11:58,000
of my portfolio within AWS with that in mind maybe one of the things we can jump into is the

121
00:11:58,000 --> 00:12:04,960
announcement year this week of the service cards it was part of a broader umbrella of ML governance

122
00:12:04,960 --> 00:12:14,160
capabilities that was announced as part of the the SageMaker product family of course you know

123
00:12:14,160 --> 00:12:19,680
I'm presuming by name alone that you know credit you know it goes to folks behind the original

124
00:12:19,680 --> 00:12:26,640
model cards paper absolutely like Debraji and like Mitchell to make a brew and others right you talk

125
00:12:26,640 --> 00:12:32,480
a little bit about that and yeah so first of all there were two related but distinct announcements

126
00:12:32,480 --> 00:12:38,160
today one was on sort of model cards within SageMaker and that's of course really customer facing

127
00:12:38,160 --> 00:12:43,440
that's for our customers who have their own data have the expertise to do their own machine learning

128
00:12:43,440 --> 00:12:50,000
but want to do machine learning in a responsible way so SageMaker model cards is meant to help our

129
00:12:50,000 --> 00:12:55,200
customers in that regard not a little bit more distant from that effort okay on the other hand

130
00:12:55,200 --> 00:13:00,560
the service cards that we announced for three of our major kind of vertical AI services around

131
00:13:00,560 --> 00:13:09,200
face recognition speech transcription and identity validation from like government identities

132
00:13:09,200 --> 00:13:15,760
government ID cards these are closely related to model cards and obviously that particular paper

133
00:13:15,760 --> 00:13:21,200
was an inspiration not just to us but I think to many people in the responsible AI community

134
00:13:22,000 --> 00:13:26,400
you'll notice that we don't call those model cards that we call them service cards and there's

135
00:13:26,400 --> 00:13:32,160
a very deliberate reason for that one of them is our typical AI service will have many many models

136
00:13:32,160 --> 00:13:37,040
behind it to give a very concrete example if you think about the problem of face recognition

137
00:13:37,040 --> 00:13:42,480
you know a good face recognition engine will deploy multiple models there will be one model

138
00:13:42,480 --> 00:13:47,840
that just identifies the bounding boxes around the faces in an image there will be perhaps a

139
00:13:47,840 --> 00:13:53,200
separate model which makes adjustments to those faces for instance if your head was tilted in

140
00:13:53,200 --> 00:13:59,760
your selfie it'll write it so that it's oriented properly and then of course there's finally the

141
00:13:59,760 --> 00:14:04,720
model which is actually doing the matching and discard deciding whether this face matches you

142
00:14:04,720 --> 00:14:10,960
know the one that's on file for you for example and so since our customers and the end users of our

143
00:14:10,960 --> 00:14:17,280
customers experience like the holistic system and to end not the individual models we call them

144
00:14:17,280 --> 00:14:22,240
service cards because they're really like model cards but for the entire service right the thing

145
00:14:22,240 --> 00:14:28,640
that you would kind of experience at the API level and yeah for that distinction I had not heard

146
00:14:28,640 --> 00:14:33,280
the service cards announcement okay yeah so when you mentioned service cards earlier I thought

147
00:14:33,280 --> 00:14:38,480
we were using two different names yeah yeah yeah so they're they're related but distinct yeah

148
00:14:38,480 --> 00:14:46,240
and so to say a little bit more about them you know so first of all a lot of work went into these

149
00:14:46,240 --> 00:14:52,640
cards first of all there's the underlying technical quantitative assessments that sort of form the

150
00:14:52,640 --> 00:14:57,760
quantitative backbone of the information on the service cards but of course you know these cards

151
00:14:57,760 --> 00:15:02,640
got reviewed by many non technical people as well and many stakeholders you know including legal

152
00:15:02,640 --> 00:15:07,360
need to weigh in on as well so it's been a very interesting process because it's been of you know

153
00:15:07,360 --> 00:15:13,840
internally a very diverse multidisciplinary process to kind of converge on on what we wanted

154
00:15:13,840 --> 00:15:20,880
to put on these cards just hearing that it sounds like the cards kind of worked as designed it was

155
00:15:20,880 --> 00:15:26,880
not taking information that you already had produced and you know formatting it according to

156
00:15:26,880 --> 00:15:33,120
some card the card the the process of creating the service card inspired some set of work beyond

157
00:15:33,120 --> 00:15:37,280
what was already in place for these services yeah although I would say that the technical work you

158
00:15:37,280 --> 00:15:43,120
know the the the actual assessments of things like bias or robustness or privacy that works been

159
00:15:43,120 --> 00:15:50,880
ongoing since long before I even showed up at Amazon okay and and so you know what I think the

160
00:15:50,880 --> 00:15:56,480
most important thing about this announcement is that we're committing to doing this on an ongoing

161
00:15:56,480 --> 00:16:01,280
basis right we're like setting a standard for ourselves and when you look at the cards which

162
00:16:01,280 --> 00:16:07,040
have actually been released right there's a couple of purposes they serve first and foremost it's

163
00:16:07,040 --> 00:16:13,360
to communicate information to our customers about some details about how the models were trained

164
00:16:14,240 --> 00:16:22,160
how we perceive the intended use what we perceive the intended use cases to be a little bit about

165
00:16:22,160 --> 00:16:27,280
you know our quantitative assessments of demographic bias and we can talk a little bit more about what

166
00:16:27,280 --> 00:16:32,720
we say there and what we don't say and why and I think they're sort of good reasons for why we say

167
00:16:32,720 --> 00:16:38,000
the amount that we say now but all that being said I think this is a you know this is a baby step

168
00:16:38,000 --> 00:16:44,160
for us but it's a big baby step for us and I think that the most important part is the commitment to

169
00:16:44,160 --> 00:16:50,000
do this for sort of all of our AI services going forward and not just to do it once and say like

170
00:16:50,000 --> 00:16:56,160
okay now we've done it first transcribe we're no check that off because use cases change the

171
00:16:56,160 --> 00:17:03,840
data being fed to these services change and so these cards we need to revisit them at some cadence

172
00:17:03,840 --> 00:17:08,720
and redo the quantitative assessments redo the language on the guidance that we provide to

173
00:17:08,720 --> 00:17:13,600
customers on the cards and so that's I think what I'm most excited about which is you know there's

174
00:17:13,600 --> 00:17:19,040
the thing that the literal cards that were released today but then there's the commitments to a

175
00:17:19,040 --> 00:17:24,800
process and I think that that isn't many ways the thing that will have the greatest internal

176
00:17:24,800 --> 00:17:31,200
traction within AWS because we you know we kind of as the saying goes and Amazon we've walked through

177
00:17:31,200 --> 00:17:38,320
a one way door and and because we're walking through a one day way door I think we are you know in

178
00:17:38,320 --> 00:17:45,040
my view as a scientist we're you know naturally a bit conservative about how much detail we reveal

179
00:17:45,040 --> 00:17:51,040
at the beginning but I personally I strongly expect it over time it's not just that we'll do more

180
00:17:51,040 --> 00:17:55,600
of these cards and revisit the ones we've already done I think the information on them will evolve

181
00:17:55,600 --> 00:18:01,680
and it'll evolve in a way that starts providing more quantitative detail as we go forward so this is

182
00:18:01,680 --> 00:18:07,680
like our our first dipping of the toe in the water on that last point you're referring to the detail

183
00:18:07,680 --> 00:18:13,360
that you're providing about the services that are documented in the card as opposed to the details

184
00:18:13,360 --> 00:18:20,800
of the process of creating the cards yeah yeah and so but I think both will evolve right I mean I think

185
00:18:20,800 --> 00:18:26,560
the language would do we would we use the cards the the way you know and also the amount that we the

186
00:18:26,560 --> 00:18:31,120
amount of detail we provide on the underlying sort of quantitative assessments that are in some

187
00:18:31,120 --> 00:18:40,080
ways the technical backbone of the cards you mentioned some nuances in the the way that you present

188
00:18:40,080 --> 00:18:48,000
certain measurements can you talk a little bit more about that yeah so a couple things so first

189
00:18:48,000 --> 00:18:53,760
of all there was a lot of healthy internal discussion and debate about how much and what to say

190
00:18:53,760 --> 00:18:59,440
about our demographic bias assessments which are quite extensive you know if I were to show you

191
00:18:59,440 --> 00:19:07,840
the full details of them in the end we decided to you know give information about what the demographic

192
00:19:07,840 --> 00:19:14,560
groups we investigated were and of course these vary radically sometimes by service right because

193
00:19:14,560 --> 00:19:21,120
in something like a face recognition service you have you know you you you have visible features

194
00:19:21,120 --> 00:19:26,880
like skin tone hair length the jewelry things like this that are you know correlated with different

195
00:19:26,880 --> 00:19:32,160
demographic groups in spoken language you don't have that right but you do have regional accents

196
00:19:32,160 --> 00:19:38,080
in dialect so like what groups you're going to kind of audit for for lack of a better term and

197
00:19:38,080 --> 00:19:42,960
what kinds of data sets you're going to cure age to do that assessment going very radically by

198
00:19:42,960 --> 00:19:49,920
different services there was debate you know about whether we should release information like okay

199
00:19:49,920 --> 00:19:56,480
you know on this service the worst performing group among these demographic groups was such and such

200
00:19:56,480 --> 00:20:03,120
group and I think there's two very good reasons that in the end we decided not to do that at this point

201
00:20:03,120 --> 00:20:09,200
one is is that the honest scientific truth is that the identity of the group with the worst

202
00:20:09,200 --> 00:20:14,880
performance and what that worst performance looks like can vary radically from data set to data set

203
00:20:15,600 --> 00:20:20,640
so it really can be the case that just on the problem with speech recognition you know different

204
00:20:20,640 --> 00:20:25,280
benchmark data sets the the group that is the best performing and the word group that has the

205
00:20:25,280 --> 00:20:32,240
worst performance can completely change from one data set to another the other common I think

206
00:20:32,240 --> 00:20:36,880
that's worth making is that and this is again some sort of a lesson I've learned at Amazon

207
00:20:37,920 --> 00:20:43,760
the vast majority of kind of fairness notions in the scientific literature on on the topic

208
00:20:44,320 --> 00:20:50,160
essentially adopt some kind of equalization of harm notion it's like okay we're building a model

209
00:20:50,160 --> 00:20:57,040
for consumer lending we think the biggest harm is like a false is a false negative I I predict

210
00:20:57,040 --> 00:21:01,120
that you will not repay alone and so I don't give it going to you whereas in fact you were a

211
00:21:01,120 --> 00:21:05,680
credit worthy and would have repaid it and so then I settled something like okay across these

212
00:21:05,680 --> 00:21:13,920
different combinations of racial and gender groups I want to equalize the false rejection rate

213
00:21:13,920 --> 00:21:21,360
across different groups okay and I think we hopefully don't think that way within AWS and the

214
00:21:21,360 --> 00:21:28,880
reason for that is a couplefold first of all it can just be the case right that some groups present

215
00:21:28,880 --> 00:21:34,240
a greater challenge on a particular problem to another group and so if you insist on equalizing

216
00:21:34,240 --> 00:21:39,520
rates of harm across different groups it could be that the only way you can achieve that is to

217
00:21:39,520 --> 00:21:45,920
deliberately do worse on groups that you're doing better on in order to raise their rate of harm

218
00:21:45,920 --> 00:21:50,240
up to match that of the worst performing group what's an example of that I think the simplest

219
00:21:50,240 --> 00:21:57,600
example would be it may not always be this way but in general things like you know facial hair

220
00:21:57,600 --> 00:22:02,960
and sunglasses present a challenge to face recognition right because there's some kind of

221
00:22:02,960 --> 00:22:08,480
occlusion of your underlying facial structure okay that may not always be the case by the way

222
00:22:08,480 --> 00:22:14,080
maybe at some point we'll figure out ways for instance of you know detecting bone structure

223
00:22:14,080 --> 00:22:19,120
better in a way that would let us kind of see through facial hair but to the extent that it's

224
00:22:19,120 --> 00:22:24,080
makes sense to people that right now facial hair makes face recognition more difficult

225
00:22:24,080 --> 00:22:29,760
if there's a culture or a demographic group in which that is common it's going to be you know

226
00:22:29,760 --> 00:22:35,360
that's it's going to be a it's a harder challenge from a scientific standpoint so the view we adopt

227
00:22:35,360 --> 00:22:40,560
instead rather than saying like well you know successes when we equalize the error rates across

228
00:22:40,560 --> 00:22:48,400
groups our goal is to make every error group error rate as small as we possibly can even if that

229
00:22:48,400 --> 00:22:53,760
doesn't mean that we can equalize all of them and we don't want to do the sort of nonsensical

230
00:22:53,760 --> 00:22:59,280
thing from a product and performance standpoint of you know in the interest of some academic

231
00:22:59,280 --> 00:23:04,480
notion of fairness deliberately doing worse on some group and so the technical work that goes

232
00:23:04,480 --> 00:23:09,600
under that of course involves you know you find out what your worst performing group isn't usually

233
00:23:09,600 --> 00:23:14,640
not always but usually the best solution to get an improvement on that group is to go out and get

234
00:23:14,640 --> 00:23:19,920
better and more data for that particular group but it's because of these two reasons one is that we

235
00:23:19,920 --> 00:23:25,360
don't think in this equalization term and also what the worst and best performing groups are can

236
00:23:25,360 --> 00:23:30,000
change radically from data set to data set we give kind of high level guidance on like what the

237
00:23:30,000 --> 00:23:37,120
worst performing group number was but without sort of saying this was the specific group

238
00:23:37,120 --> 00:23:43,680
that witnessed that number now you could have provided additional information and

239
00:23:43,680 --> 00:23:50,720
specified the data set why did you choose not to do that yeah there was also a healthy internal

240
00:23:50,720 --> 00:23:55,360
debate about how much say about data sets and I think in the initial cards that we're releasing

241
00:23:55,360 --> 00:24:04,320
today we say relatively little about that part of that is because you know first of all

242
00:24:04,320 --> 00:24:09,920
many many data sets go into the training of our models as well as the assessment and all you know

243
00:24:09,920 --> 00:24:15,200
quite often there's many more assessment data sets in the workshop or at least they're very

244
00:24:15,200 --> 00:24:20,080
designed to be different right because you're essentially trying to do stress tests of models so

245
00:24:20,080 --> 00:24:24,800
you know you normally would expect to get very good performance on the type of data that you trained

246
00:24:24,800 --> 00:24:31,440
on but when you start stress testing different use cases things will deliberately will look worse

247
00:24:32,960 --> 00:24:40,080
I think there was also the fear that since so much goes into the training of an ML model and

248
00:24:40,080 --> 00:24:46,960
your technical viewers will know this you know the the cartoon view of machine learning is that

249
00:24:46,960 --> 00:24:53,360
it's a very streamlined almost button pushing process right I get a data set you know and I

250
00:24:53,360 --> 00:24:58,240
I push some button and pie torch now it comes in my model and great but you know the like just

251
00:24:58,880 --> 00:25:04,720
I don't I don't think I'm giving away any big secret at least among the scientists of your viewers

252
00:25:04,720 --> 00:25:10,880
that the amount of artisanal tinkering that goes into modern machine learning is just mind-boggling

253
00:25:10,880 --> 00:25:16,400
and in many ways is actually kind of increased with the rise of deep learning because you know there's

254
00:25:16,400 --> 00:25:20,720
what's the architecture how deep is the network how wide is the network what exactly are this

255
00:25:20,720 --> 00:25:27,840
reactivation units what is the architecture between layers do you have convolutional units etc etc

256
00:25:27,840 --> 00:25:33,200
and the honest truth is that you know even though we have rigorous and effective train test

257
00:25:33,200 --> 00:25:38,640
methodology the way the soup is made is there's a lot of trial and error and you know very things and

258
00:25:38,640 --> 00:25:44,800
so sort of releasing just information about the data sets without sort of the context of the

259
00:25:44,800 --> 00:25:51,440
rest of the training I think we thought it would mislead customers in particular into kind of

260
00:25:51,440 --> 00:25:56,080
equating the training process with just the properties of the data set you're kind of saying that

261
00:25:58,240 --> 00:26:04,000
that reproducibility as a goal is kind of intractable for what you were trying to accomplish

262
00:26:04,000 --> 00:26:09,280
and so you didn't look exactly right so much detail that someone might want to reproduce exactly

263
00:26:09,280 --> 00:26:13,680
exactly like we didn't want to give the illusion that we think these cards have enough

264
00:26:13,680 --> 00:26:21,440
information for you to go try to replicate what they clearly don't and and and we also you know

265
00:26:21,440 --> 00:26:26,480
the other thing is the goal of these cards I think even in their original conception as model

266
00:26:26,480 --> 00:26:33,120
cards in the paper that you you mentioned you know these are meant to be short and readable to

267
00:26:33,120 --> 00:26:38,880
a very wide audience and so the more kind of technical minutia you get into the longer these

268
00:26:38,880 --> 00:26:43,520
cards will become the less they'll be they won't be like cards anymore they'll be like manuals

269
00:26:44,400 --> 00:26:50,560
and the and the audience for them will become more and more limited I mean many people before

270
00:26:50,560 --> 00:26:56,880
offered the analogy of these are you know sort of like the analog of nutrition labels on food

271
00:26:56,880 --> 00:27:01,440
right it's like ordinary shoppers should be able you know who care about it should be able to

272
00:27:01,440 --> 00:27:06,480
pick it up and you know say like oh I don't like some of these ingredients or I'm allergic to them

273
00:27:06,480 --> 00:27:13,600
um and so that that's kind of the goal here that being said I do expect that as time goes on we

274
00:27:13,600 --> 00:27:20,880
will you know our our cards will evolve to say more about our data sets and other topics as well

275
00:27:20,880 --> 00:27:28,320
including possibly more quantitative information about demographic performance okay one one of the

276
00:27:28,320 --> 00:27:36,960
things that we've talked talked thus far about naturally is um data sets comes up a lot in this topic

277
00:27:38,160 --> 00:27:43,040
there was a period of time where there was a pretty vocal contentious argument about

278
00:27:44,000 --> 00:27:50,960
our algorithms bias versus our data sets bias I think that was maybe a couple of years ago

279
00:27:50,960 --> 00:27:56,480
that that really flared up are you still seeing that argument play out or what's your take there

280
00:27:56,480 --> 00:28:03,520
yeah I mean I think both of the things you said are true right so I definitely think it's true

281
00:28:03,520 --> 00:28:09,120
that if you have heavily biased data coming into your training process and you train models

282
00:28:09,120 --> 00:28:15,520
in the ordinary way that doesn't attempt to you know look for or correct that bias then you

283
00:28:15,520 --> 00:28:20,320
should expect to get it in your model as well models are doing yeah it's not the only way though

284
00:28:20,320 --> 00:28:26,880
right and I think maybe the more subtle point that is less widely realized or discussed is that

285
00:28:27,600 --> 00:28:32,000
I mean first of all every data set has some kind of bias right it might not be demographic bias

286
00:28:32,000 --> 00:28:37,200
but it's gathered under certain conditions and if you train a model on data under those conditions

287
00:28:37,200 --> 00:28:43,520
and then try running it on data from radically different conditions you know bad things will

288
00:28:43,520 --> 00:28:47,680
happen it might not be demographic bias but it'll certainly at least be poor performance

289
00:28:47,680 --> 00:28:52,720
right but the other thing can happen is even if you have a data set that you've scrupulously

290
00:28:52,720 --> 00:28:58,240
you know verified is free of at least the demographic biases that you've checked for and care

291
00:28:58,240 --> 00:29:05,040
about you could still end up with a trained model that was heavily biased against one or another

292
00:29:05,040 --> 00:29:10,960
demographic group and the reason for this is that again at a high level it's pretty simple right

293
00:29:10,960 --> 00:29:16,480
machine learning doesn't give you for free anything that you didn't explicitly ask for and it

294
00:29:16,480 --> 00:29:21,440
also doesn't avoid things that you wanted to avoid that you didn't explicitly tell it to avoid

295
00:29:21,440 --> 00:29:27,920
so for instance I'm training a large neural network on some data set that is free of demographic

296
00:29:27,920 --> 00:29:34,480
bias whatever that means but you know the training process is a journey through this very high

297
00:29:34,480 --> 00:29:40,560
dimensional parameter space looking for sort of the minimum error point on the data right and if

298
00:29:40,560 --> 00:29:48,160
it happens to be the case that you know there's a point in the point that minimizes the error

299
00:29:48,160 --> 00:29:55,600
in model space happens to do very poorly on some particular demographic group even though there

300
00:29:55,600 --> 00:30:01,680
might have been a different point even a nearby point in model space that had only slightly higher

301
00:30:01,680 --> 00:30:07,120
overall error but did much better on that demographic group well since you just said no find

302
00:30:07,120 --> 00:30:12,320
the minimum error and you didn't mention anything about like by the way if there happens to be a

303
00:30:12,320 --> 00:30:19,120
point you know in model space that has only infinitesimally larger error but does much better from

304
00:30:19,120 --> 00:30:24,160
a demographic fairness perspective than pick that one then you're not going to get that right and

305
00:30:24,160 --> 00:30:29,680
so at a conceptual level the solution to this is pretty straightforward right instead of solving

306
00:30:29,680 --> 00:30:35,360
what we would you know in technical jargon call a straight up optimization problem minimize the

307
00:30:35,360 --> 00:30:40,640
error in model space on the data you solve a constrained optimization problem right where the

308
00:30:40,640 --> 00:30:46,640
constrained optimization problem is minimize the error on the data subject to these fairness

309
00:30:46,640 --> 00:30:51,680
conditions and this is where kind of the research gets interesting because you know you have to

310
00:30:51,680 --> 00:30:56,160
figure out you know this these are computers and algorithms after all I can't just sort of wave

311
00:30:56,160 --> 00:31:02,000
my hands and say I have to like be able to mathematically specify the same way that the error objective

312
00:31:02,000 --> 00:31:07,040
is mathematically specified I need to be able to mathematically specify what the fairness

313
00:31:07,040 --> 00:31:12,880
constraints are and there's more than one choice for what those constraints look like and those

314
00:31:12,880 --> 00:31:17,520
correspond to sort of different mathematical definitions of fairness and there are different

315
00:31:17,520 --> 00:31:23,440
algorithmic ways of trying to sort of bind a solution to this constrained optimization problem

316
00:31:23,440 --> 00:31:30,080
and this has been sort of a subject of very very active research over the past seven or so years

317
00:31:30,080 --> 00:31:36,240
how does that dichotomy if that's where I turn play out in practice at a place like amazon and I

318
00:31:36,240 --> 00:31:44,800
guess I'm asking are are you know biases in the data sets you know top of mind and easier to

319
00:31:44,800 --> 00:31:52,720
root out than biases in the algorithmic process and you know or or not so much you just have to

320
00:31:52,720 --> 00:31:59,520
be aware that they're there and yeah I don't think I have a a binary answer to that question I think

321
00:31:59,520 --> 00:32:06,320
my intuition is that you know especially in the era of deep learning the training of models has

322
00:32:06,320 --> 00:32:14,240
become very very computationally intensive and very expensive and we are now training very very

323
00:32:14,240 --> 00:32:22,480
large models and so there is a complexity and opaqueness to that process that's you know in my

324
00:32:22,480 --> 00:32:28,960
mind perhaps greater than the mysteries of the data set that goes into that process right I mean

325
00:32:28,960 --> 00:32:36,000
usually if you have certain demographic groups in mind and and by the way you have data annotated

326
00:32:36,000 --> 00:32:41,440
by those demographic groups because if you don't know the demo if you're you know if you say okay I

327
00:32:41,440 --> 00:32:47,840
want to you know make sure that I'm fair by race for example but the data I have is not annotated

328
00:32:47,840 --> 00:32:54,080
by race I can't even audit right so we we need that kind of data or some way or some proxy for

329
00:32:54,080 --> 00:33:03,200
that kind of data but I feel like the problem of assessing whether a data set has bias is is at

330
00:33:03,200 --> 00:33:10,000
this point maybe a more straightforward problem than you know thinking about whether your training

331
00:33:10,000 --> 00:33:16,800
process might inadvertently lead to biases and you know just to touch on a topic that we can

332
00:33:16,800 --> 00:33:23,840
discuss more if you want a good example I would give is that you know you think about the rise of

333
00:33:23,840 --> 00:33:30,560
these very powerful generative models in the past few years you know large language models by GPT

334
00:33:30,560 --> 00:33:37,520
three and things like Dolly you know you enter in some prefix text and you know it auto completes

335
00:33:37,520 --> 00:33:43,520
with you know sentient grammatically correct you know sometimes quite compelling text I mean I

336
00:33:43,520 --> 00:33:51,040
found in experimenting with these things sentient your opinion you put in you put in you know if you

337
00:33:51,040 --> 00:33:57,280
type in prefix text it's kind of like you might have from a novel it has emotional human content

338
00:33:57,280 --> 00:34:02,240
in it you know you get out of completions that are like a short story and you know not always but

339
00:34:02,240 --> 00:34:05,760
sometimes I've been sitting there really thinking you know this is pretty good I think I want to

340
00:34:05,760 --> 00:34:10,160
keep reading this yeah but of course there's some limit on the on the length of the completion if

341
00:34:10,160 --> 00:34:15,280
you're using the other source versions of the short story sort of stops in mid sentence but

342
00:34:15,280 --> 00:34:21,120
you know if you think about what for instance fairness would even mean in these kind of large

343
00:34:21,120 --> 00:34:26,320
language models I mean we have a very good handle I think in relative terms scientifically

344
00:34:26,320 --> 00:34:31,840
on notions of fairness for simple prediction problems like classification or regression

345
00:34:32,480 --> 00:34:38,560
but like what would it mean for a large language model to be fair right I think this is an important

346
00:34:38,560 --> 00:34:46,320
scientific question that we are only beginning to kind of grapple with at all and to say a

347
00:34:46,320 --> 00:34:52,640
little bit more about what I mean you know I could give you very very narrow senses in which I

348
00:34:52,640 --> 00:35:00,800
might ask for a language model to be fair so for example in my own kind of anecdotal experimentation

349
00:35:00,800 --> 00:35:07,120
with some of these models I find that if you type in some prefix of text that mentions an

350
00:35:07,120 --> 00:35:12,960
ungendered name like Chris for instance but you don't use any pronouns so you haven't committed

351
00:35:12,960 --> 00:35:19,680
to the gender of Chris it almost always autocompletes with male pronouns okay so I could say okay you

352
00:35:19,680 --> 00:35:27,280
know a fair language model should you know for this list of ungendered names in North America have a

353
00:35:27,280 --> 00:35:34,320
roughly equal mixture of pronouns in the autocompletion but when you think about distribution of

354
00:35:34,320 --> 00:35:39,120
Chris's is that the gender distribution of Chris I don't know I don't know you know you so you

355
00:35:39,120 --> 00:35:44,080
could ask whether it should match or you know or what what it is right just to but my point here is

356
00:35:44,080 --> 00:35:49,920
that you know anybody who's played around with these things would almost certainly criticize what

357
00:35:49,920 --> 00:35:57,760
I just proposed is like oh my god that is just such a narrow definition of fairness given the power

358
00:35:57,760 --> 00:36:03,280
and the complexity of the output of these models and I think that critique would be right but that's

359
00:36:03,280 --> 00:36:08,320
kind of you know that's where our thinking is right now and I think you know that this is an

360
00:36:08,320 --> 00:36:12,960
area that is going to present a great challenge to the research community in coming years.

361
00:36:14,000 --> 00:36:18,160
Have you seen any early research attempting to address the question?

362
00:36:18,960 --> 00:36:24,240
Well I mean you do have the famous you know word embedding paper from I think roughly 2015 or

363
00:36:24,240 --> 00:36:31,280
2016 but I still think of that which is a very nice paper and a very influential paper but it's

364
00:36:31,280 --> 00:36:36,160
still kind of talking about one particular kind of bias right which is kind of the association

365
00:36:36,880 --> 00:36:42,880
with between gender and the occupations and and other you know sort of other parts of speech

366
00:36:44,640 --> 00:36:50,160
and so you know I think something that we think about a lot is like toxicity in language models

367
00:36:50,160 --> 00:36:58,960
and is this model you know generating slightly more negative sentiment completions when prompted

368
00:36:58,960 --> 00:37:05,360
with you know one type of prompt maybe related to some demographic group or identity versus another

369
00:37:06,160 --> 00:37:10,880
another really interesting thing is you know and this is why I think this area is so fertile

370
00:37:11,760 --> 00:37:16,800
for both research and just you know you know as a society thinking about these problems

371
00:37:17,600 --> 00:37:23,680
there have been a couple of recent papers in which the use case of a large language model

372
00:37:23,680 --> 00:37:31,280
was really deliberately to replicate the biases and correlations that are present in society.

373
00:37:31,280 --> 00:37:37,680
So in one that just came out like a month or so ago the author is basically there's like I

374
00:37:37,680 --> 00:37:41,920
can't remember the name of it I don't think it's the American census survey it's been some other

375
00:37:41,920 --> 00:37:49,920
there's some you know long-standing survey that some organization does I'm blanking on the exact name

376
00:37:49,920 --> 00:37:55,280
of it but the details are less important than what they did this organization goes out and

377
00:37:55,280 --> 00:38:01,040
interviews Americans on their views on various topics so they will go out and sit down with

378
00:38:01,040 --> 00:38:08,640
real citizens take demographic information like this is a housewife you know age 45 who lives

379
00:38:08,640 --> 00:38:12,640
in the Midwest and then they'll you know so they'll collect a bunch of demographic information

380
00:38:12,640 --> 00:38:17,360
and they'll ask the subject their views on things like gun control and abortion and other

381
00:38:17,360 --> 00:38:24,000
controversial topics okay and they publish these things at some cadence and so in this paper about

382
00:38:24,000 --> 00:38:30,480
a month ago they basically used they tried to use a large language model to replicate the numerical

383
00:38:30,480 --> 00:38:36,240
findings of such a survey by designing prompts and those prompts would say something like well

384
00:38:36,240 --> 00:38:42,880
you know Christina is a housewife who lives in the Midwest her attitude on gun control is and then

385
00:38:42,880 --> 00:38:48,000
they you know push the large language model button and then they elicit from that some response

386
00:38:48,000 --> 00:38:53,200
and then they tabulated them yeah and the upshot was that you know they found that you could pretty

387
00:38:53,200 --> 00:39:00,640
up could approximately fairly well replicate the numerical findings of the actual survey by the LLM

388
00:39:01,440 --> 00:39:06,640
and the reason I'm going down this rabbit hole is that this is a use case we can just you know

389
00:39:06,640 --> 00:39:11,840
people can decide whether they think this is a productive use case or not but this is a use case

390
00:39:11,840 --> 00:39:19,360
in which you wouldn't want the the LLM to have been eradicated for any kinds of correlations for

391
00:39:19,360 --> 00:39:25,120
instance between where people live and their views on certain topics or their gender in certain

392
00:39:25,120 --> 00:39:30,240
topics and so it's easy to sort of say like oh well fairness should sort of eradicate these types

393
00:39:30,240 --> 00:39:35,920
of correlations that exist in society but there might it could be that the most valuable use cases

394
00:39:35,920 --> 00:39:43,520
actually are to capture those biases in these very powerful systems. Just thinking about that

395
00:39:43,520 --> 00:39:48,400
from a practical perspective you're you're painting a picture in which perhaps

396
00:39:51,280 --> 00:39:56,240
part of the way these language models are you know rolled out and used more broadly is

397
00:39:56,240 --> 00:40:01,920
you know controlling for the bias of the language model based on the needs of a particular use case

398
00:40:01,920 --> 00:40:08,000
and you know from that perspective to your point there's no single the the goal isn't even a single

399
00:40:08,000 --> 00:40:13,840
definition of fairness it's fairness with regards to a use case and you design for that. Yeah

400
00:40:13,840 --> 00:40:19,440
how you see things. Yeah I mean but I mean I mean so I guess I could imagine a future in which

401
00:40:19,440 --> 00:40:26,800
you know the user of the LLM goes to some dashboard and sort of you know sets sets a bunch of

402
00:40:26,800 --> 00:40:33,600
dials in a way that like okay you know I don't I want you to preserve the bias between gender

403
00:40:33,600 --> 00:40:38,000
and you know or a correlation between gender and attitudes on gun control.

404
00:40:38,000 --> 00:40:42,240
Were you going to want the output of this to be rated R versus rated PG 13 versus rated

405
00:40:42,240 --> 00:40:47,120
PG? Yeah yeah yeah and sort of toxicity of course is a whole other you know separate can of

406
00:40:47,120 --> 00:40:54,720
worms that fairness and you know these are very very thorny issues and so you know it's an

407
00:40:54,720 --> 00:41:00,400
interesting time because as as the power of these things is grown you know to the point that

408
00:41:00,400 --> 00:41:07,760
you know even people in the field like me can be amazed by what they can do you know there's

409
00:41:07,760 --> 00:41:13,280
this sense that okay there's some very interesting conceptual and science questions here but there's

410
00:41:13,280 --> 00:41:20,720
also a pretty serious responsibility on the AI community to you know control this stuff and to

411
00:41:20,720 --> 00:41:27,200
you know and to set guidelines for its use and to decide what use cases are appropriate and

412
00:41:27,200 --> 00:41:32,640
decide what kinds of generative models we should even be building at all even though it might be

413
00:41:32,640 --> 00:41:40,800
possible and so I think you know in many ways these generative models have kind of pushed to the

414
00:41:40,800 --> 00:41:46,640
four some very very difficult questions that weren't quite present when we were just building

415
00:41:46,640 --> 00:41:52,400
powerful models for making point predictions yeah yeah awesome well Michael it's been wonderful

416
00:41:52,400 --> 00:41:55,920
to have an opportunity to chat with you yeah yeah I think we did a pretty good

417
00:41:55,920 --> 00:42:00,160
approximation to our New York dinner conversation so it's been a pleasure and I hope to

418
00:42:00,160 --> 00:42:18,080
hope to see you again soon maybe at a dinner in New York my pleasure thank you

