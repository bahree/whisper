1
00:00:00,000 --> 00:00:16,320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

2
00:00:16,320 --> 00:00:21,480
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:21,480 --> 00:00:32,600
I'm your host Sam Charrington.

4
00:00:32,600 --> 00:00:36,800
We'd like to send a huge shout out to our friends at Waits and Biosys for their sponsorship

5
00:00:36,800 --> 00:00:43,000
and support not only for this podcast, but our upcoming event, Twimblecon AI Platforms.

6
00:00:43,000 --> 00:00:47,200
As you'll hear in my conversation with Lucas, Waits and Biosys believes that the more accessible

7
00:00:47,200 --> 00:00:52,280
transparent and collaborative the process of model training becomes, the safer and better

8
00:00:52,280 --> 00:00:54,480
the models we build will be.

9
00:00:54,480 --> 00:00:58,640
To enable this, they offer an experiment tracking platform for deep learning researchers

10
00:00:58,640 --> 00:01:00,040
and practitioners.

11
00:01:00,040 --> 00:01:04,640
To learn more about Waits and Biosys and get started tracking your modeling experiments,

12
00:01:04,640 --> 00:01:13,280
visit www.wndb.com, w-a-n-d-b.com.

13
00:01:13,280 --> 00:01:24,480
And now on to the show.

14
00:01:24,480 --> 00:01:53,600
Thank you for joining us for this week in machine learning and AI.

15
00:01:53,600 --> 00:01:57,960
You know, so with that in mind, probably a little bit of history and your background

16
00:01:57,960 --> 00:02:03,680
is probably a good place to start so we can start to kind of get the decoder ring in place.

17
00:02:03,680 --> 00:02:04,680
Totally.

18
00:02:04,680 --> 00:02:09,720
So, yeah, you know, I started my professional career at a company called Yahoo.

19
00:02:09,720 --> 00:02:14,280
I don't know if you remember them, but back in Santa Clara.

20
00:02:14,280 --> 00:02:15,280
Yeah, yeah, exactly.

21
00:02:15,280 --> 00:02:21,520
Back in 2004, 2005, I was actually working to convert their kind of rule-based system

22
00:02:21,520 --> 00:02:25,360
into a machine learning ranking system.

23
00:02:25,360 --> 00:02:30,360
So you know, ranking search results is one of the first real applications of ML.

24
00:02:30,360 --> 00:02:34,800
And then I went to a startup called PowerSet to do kind of the same thing that became Microsoft

25
00:02:34,800 --> 00:02:35,800
Bing.

26
00:02:35,800 --> 00:02:41,360
And yeah, this is back in maybe 2007, 2008 and power set initially based in Atlanta.

27
00:02:41,360 --> 00:02:43,360
Am I thinking of the right company?

28
00:02:43,360 --> 00:02:46,200
No, I don't think it was based in San Francisco.

29
00:02:46,200 --> 00:02:47,200
Okay.

30
00:02:47,200 --> 00:02:51,520
I mean, it was a search kind of a natural language search company.

31
00:02:51,520 --> 00:02:52,520
It had a lot of great ideas.

32
00:02:52,520 --> 00:02:56,840
Actually, a lot of the folks working there have gone on to do impressive stuff like the,

33
00:02:56,840 --> 00:03:01,520
you know, the GitHub founders were working there and Descartes Labs is another big kind

34
00:03:01,520 --> 00:03:02,520
of aerial imaging company.

35
00:03:02,520 --> 00:03:04,960
It was kind of, it was a fun place to work.

36
00:03:04,960 --> 00:03:08,320
There were a lot of super smart people there.

37
00:03:08,320 --> 00:03:13,000
And it was definitely way ahead of its time, right, trying to do deep natural language processing

38
00:03:13,000 --> 00:03:14,520
applied to search.

39
00:03:14,520 --> 00:03:15,520
Yeah.

40
00:03:15,520 --> 00:03:20,520
And kind of what I was seeing at both the bigger company and the startup was machine learning

41
00:03:20,520 --> 00:03:25,760
had tons of promise, but tons of obstacles to kind of make it work in the real world.

42
00:03:25,760 --> 00:03:30,120
And that's always kind of what's driven me is the applications of machine learning.

43
00:03:30,120 --> 00:03:34,680
And so I founded a company originally called Crowdflower that became Figure 8, which was

44
00:03:34,680 --> 00:03:40,040
all about getting high quality training data because that's such a bottleneck for building

45
00:03:40,040 --> 00:03:42,800
and deploying machine learning models.

46
00:03:42,800 --> 00:03:48,400
But one of the things that I saw when I was running Crowdflower and later Figure 8 was that

47
00:03:48,400 --> 00:03:52,000
there's a lot of problems that happened downstream, right, as people tried to, you know, take

48
00:03:52,000 --> 00:03:54,560
the training data and turn it into live production models.

49
00:03:54,560 --> 00:03:57,640
And so, you know, I think the next problem that people run into once you have the training

50
00:03:57,640 --> 00:04:03,640
data is figuring out a sane way to manage all the experiments that you do when you train

51
00:04:03,640 --> 00:04:04,640
your models.

52
00:04:04,640 --> 00:04:08,000
And so that was the inspiration behind starting weights and biases.

53
00:04:08,000 --> 00:04:15,720
You know, I think every decade, maybe I'd do another step in the machine learning.

54
00:04:15,720 --> 00:04:24,240
So I'm curious, what is the craziest thing you've seen in terms of the way people are managing

55
00:04:24,240 --> 00:04:25,240
experiments?

56
00:04:25,240 --> 00:04:30,640
Like, you know, I tell people all the time is I'm kind of laying out a, you know, landscape

57
00:04:30,640 --> 00:04:37,680
of, you know, machine learning and the process and how experiment management happens.

58
00:04:37,680 --> 00:04:41,520
Like, you know, sometimes you're better off if it's not even happening.

59
00:04:41,520 --> 00:04:47,320
Like, I've seen like crazy foul names with like hyperparameters in them and stuff like

60
00:04:47,320 --> 00:04:48,320
that.

61
00:04:48,320 --> 00:04:49,800
Like, do you see that kind of stuff?

62
00:04:49,800 --> 00:04:50,800
Oh, yeah.

63
00:04:50,800 --> 00:04:55,440
I mean, I think back to, you know, I remember in grad school, actually, I was, I was fairly

64
00:04:55,440 --> 00:05:00,160
sloppy at managing experiments, which is maybe why I am so passionate about this space.

65
00:05:00,160 --> 00:05:06,120
But I had a huge single file that I added in EMAX and yes, sporadically put notes in

66
00:05:06,120 --> 00:05:12,080
for all the things that I tried and I had, you know, kind of a crazy file naming scheme,

67
00:05:12,080 --> 00:05:16,040
you know, that would kind of evolve over time, if you will.

68
00:05:16,040 --> 00:05:20,440
But you know, actually when I was starting with advices, I, I spent a lot of time with

69
00:05:20,440 --> 00:05:24,400
my friends kind of studying the different ways that people do experiment management and

70
00:05:24,400 --> 00:05:29,080
actually think the most common approach is to have, you know, sets of directors, sets

71
00:05:29,080 --> 00:05:30,080
of files.

72
00:05:30,080 --> 00:05:34,600
And then these days, I think people typically use a Google doc and put notes for, for

73
00:05:34,600 --> 00:05:35,600
each of their runs.

74
00:05:35,600 --> 00:05:39,000
You know, I think everybody, you know, I've looked at lots of different people's, you know,

75
00:05:39,000 --> 00:05:40,800
Google Docs or whatever they use.

76
00:05:40,800 --> 00:05:45,320
And, you know, I mean, these are some of the most incomprehensible pieces of text.

77
00:05:45,320 --> 00:05:49,080
Like, I always wondered if people could actually go back and figure out what they were doing

78
00:05:49,080 --> 00:05:51,880
even, you know, a few weeks before.

79
00:05:51,880 --> 00:05:52,880
Right, right.

80
00:05:52,880 --> 00:06:00,600
So, yeah, I mean, I've come across everything from post-it notes to lab notebooks, to Google

81
00:06:00,600 --> 00:06:08,360
Docs, to spreadsheets, to, you know, the crazy thousand character long founding with all

82
00:06:08,360 --> 00:06:12,440
of the parameters dot pickle, you know, it's like, yeah, yeah, yeah, exactly.

83
00:06:12,440 --> 00:06:17,840
And then you start calling things like dash fixed and then like, dash fixed, dash fixed again.

84
00:06:17,840 --> 00:06:18,840
Right, right.

85
00:06:18,840 --> 00:06:19,840
Dash final, dash final, dash final.

86
00:06:19,840 --> 00:06:20,840
Yeah, yeah, exactly.

87
00:06:20,840 --> 00:06:24,840
That's the real way that they pattern.

88
00:06:24,840 --> 00:06:25,840
Right.

89
00:06:25,840 --> 00:06:26,840
Right.

90
00:06:26,840 --> 00:06:30,680
Hey, everyone.

91
00:06:30,680 --> 00:06:31,680
Sam here.

92
00:06:31,680 --> 00:06:35,880
Our conference, Chumul Khan AI platforms is right around the corner.

93
00:06:35,880 --> 00:06:41,800
So I want to take a minute here to share a bit about the now supersized to track agenda.

94
00:06:41,800 --> 00:06:46,360
On top of our great keynote interviews, including the one with Andrew Ng that I am so looking

95
00:06:46,360 --> 00:06:51,080
forward to, our technical case studies track will feature speakers from companies like

96
00:06:51,080 --> 00:06:56,800
Capital One, Comcast, Levi's and Zappos, all sharing about the architectures and approaches

97
00:06:56,800 --> 00:07:00,760
they've developed to support their machine learning and deep learning workflows.

98
00:07:00,760 --> 00:07:05,360
And the enabling technologies track will focus on tools and solutions that can help organizations

99
00:07:05,360 --> 00:07:11,200
like yours automate and scale various aspects of your machine learning pipelines.

100
00:07:11,200 --> 00:07:13,440
We have a ton of awesome speakers lined up for you.

101
00:07:13,440 --> 00:07:19,080
So head on over to twomulcon.com slash speakers to check out the agenda.

102
00:07:19,080 --> 00:07:22,720
The conference will be held in San Francisco on October 1st and 2nd.

103
00:07:22,720 --> 00:07:27,880
I encourage you to register now as tickets are going fast. The early rate is ending soon

104
00:07:27,880 --> 00:07:31,200
and you definitely don't want to miss out on this event.

105
00:07:31,200 --> 00:07:35,920
PS, stick around to the end of the interview with Lucas and you'll get a discount code

106
00:07:35,920 --> 00:07:39,000
good for 20% off of registration.

107
00:07:39,000 --> 00:07:41,240
Hope to see you there.

108
00:07:41,240 --> 00:07:47,160
So you kind of did some investigation as you're starting the company and found that people

109
00:07:47,160 --> 00:07:56,600
were all over the map kind of tending towards file and directory based experiment management.

110
00:07:56,600 --> 00:07:58,640
What issues did that cause for them?

111
00:07:58,640 --> 00:08:01,360
Or was it just fine but not pretty?

112
00:08:01,360 --> 00:08:06,360
Well, I think the biggest issue is just remembering what you did, right?

113
00:08:06,360 --> 00:08:14,000
So it actually reminds me a lot of, I remember my first job when I was in college at a summer

114
00:08:14,000 --> 00:08:18,160
job programming and honestly, I didn't really know about version control at the time, didn't

115
00:08:18,160 --> 00:08:19,160
really trust it.

116
00:08:19,160 --> 00:08:25,440
You know, not the best and most organized programmer and I remember I would have all these files

117
00:08:25,440 --> 00:08:26,440
and anything.

118
00:08:26,440 --> 00:08:33,200
What was the version control scheme of choice in your day?

119
00:08:33,200 --> 00:08:34,200
Oh, yeah, what was it?

120
00:08:34,200 --> 00:08:35,200
It was before SVN.

121
00:08:35,200 --> 00:08:36,200
It was CVS.

122
00:08:36,200 --> 00:08:37,200
CVS?

123
00:08:37,200 --> 00:08:38,200
CVS, yes, CVS.

124
00:08:38,200 --> 00:08:39,200
That was the first one.

125
00:08:39,200 --> 00:08:40,200
Yep.

126
00:08:40,200 --> 00:08:47,200
I remember when I first discovered CVS, it was just like, oh, man, this is awesome.

127
00:08:47,200 --> 00:08:54,440
Actually, keep track of this stuff and kind of roll back to something I did before.

128
00:08:54,440 --> 00:09:01,280
But the initial pain, I remember this originally trying to write papers, you collect a bunch

129
00:09:01,280 --> 00:09:04,280
of metrics on the run that you want to put in your paper and then you realize, oh, there's

130
00:09:04,280 --> 00:09:09,000
actually another metric that I'd like to include in my table and it's really hard to go

131
00:09:09,000 --> 00:09:13,520
back and recreate all the past experiments that you did.

132
00:09:13,520 --> 00:09:19,280
Even if you snapchat the code, the problem is with machine learning, there's more than

133
00:09:19,280 --> 00:09:20,280
just the code.

134
00:09:20,280 --> 00:09:27,440
There's the code and there's the hyper parameters that you used and there's the data set

135
00:09:27,440 --> 00:09:29,680
that you input into your run.

136
00:09:29,680 --> 00:09:35,720
So if you're not careful, it can be really tricky to even, people talk about the reproducibility

137
00:09:35,720 --> 00:09:41,400
crisis in machine learning, right, because it's hard to reproduce other people's runs.

138
00:09:41,400 --> 00:09:45,880
But forget about reproducing other people's runs, try to reproduce in a year, your past

139
00:09:45,880 --> 00:09:50,240
self, your one month ago self set, runs, you know, I don't know how many people can actually

140
00:09:50,240 --> 00:09:51,240
do that.

141
00:09:51,240 --> 00:09:52,240
Yeah.

142
00:09:52,240 --> 00:10:00,360
And so the big part of that is understanding the hyper parameters and settings associated

143
00:10:00,360 --> 00:10:07,840
with a given run, but you also mentioned the data, are you doing anything there on the

144
00:10:07,840 --> 00:10:12,960
weights and biases side or what are you seeing there?

145
00:10:12,960 --> 00:10:17,000
You know, we actually, in weights and biases, we don't yet like snapchat the data for

146
00:10:17,000 --> 00:10:18,000
your version, the data.

147
00:10:18,000 --> 00:10:20,920
There are some really interesting technologies out there.

148
00:10:20,920 --> 00:10:24,880
We just haven't seen a level of adoption yet that we're sure that something's kind of

149
00:10:24,880 --> 00:10:27,800
becoming the standard, but we're watching that.

150
00:10:27,800 --> 00:10:34,000
So we actually just, we snapchat your, the status of your code.

151
00:10:34,000 --> 00:10:39,400
So we'll take not just your latest Git commit, but any diff from your commit, because, you

152
00:10:39,400 --> 00:10:43,200
know, it flows a little different with ML where I think people are running lots and lots

153
00:10:43,200 --> 00:10:46,360
of experiments and you kind of want to snapchat every point, you don't necessarily want

154
00:10:46,360 --> 00:10:51,840
to have a Git commit or you can't necessarily rely on a user to do that between each run

155
00:10:51,840 --> 00:10:52,840
that you do.

156
00:10:52,840 --> 00:10:57,200
So we, we snapchat with just your latest commit and then a patch.

157
00:10:57,200 --> 00:11:00,640
And then we also, you know, capture hyper parameters.

158
00:11:00,640 --> 00:11:05,480
So I think the, the last step is the, the data, but we don't, we don't actually do that

159
00:11:05,480 --> 00:11:06,480
yet.

160
00:11:06,480 --> 00:11:09,480
Although, of course, if you're using some system where you have some pointer to the,

161
00:11:09,480 --> 00:11:13,880
the data you have, you could input into our system, you know, like a path to your data

162
00:11:13,880 --> 00:11:17,120
or some output of a, a data versioning system.

163
00:11:17,120 --> 00:11:22,640
And I should mention that by the time this podcast is published, the, my ebook will be

164
00:11:22,640 --> 00:11:28,760
out on machine learning platforms and there is a section where I talk about some of the

165
00:11:28,760 --> 00:11:32,360
technologies that are out there for data management and versioning.

166
00:11:32,360 --> 00:11:33,560
Oh, what are you seeing?

167
00:11:33,560 --> 00:11:41,760
I'm curious what, what do you think the ones that I've seen out there are, are the ones

168
00:11:41,760 --> 00:11:47,560
that come to mind at least are, there's a starter called DVC, which is like, I guess

169
00:11:47,560 --> 00:11:51,680
the sense for a data version control, but they also do versioning of models and stuff

170
00:11:51,680 --> 00:11:52,680
like that.

171
00:11:52,680 --> 00:12:00,440
And there's packet arm, I don't have a lot of data on adoption of either.

172
00:12:00,440 --> 00:12:07,560
But from a dedicated to solving this kind of problem, you know, a machine learning problem

173
00:12:07,560 --> 00:12:15,120
from a data perspective, both of those are probably the, the, the pure plays that come

174
00:12:15,120 --> 00:12:24,080
to mind, but they're also a number of kind of these end to end machine learning platforms

175
00:12:24,080 --> 00:12:32,280
that come at it from a data perspective or have like some kind of data versioning capability

176
00:12:32,280 --> 00:12:38,560
that kind of underlies the way that they handle data in their platforms.

177
00:12:38,560 --> 00:12:42,360
And we actually had an interesting conversation the last time.

178
00:12:42,360 --> 00:12:48,600
We spoke about the whole broad structure of this machine learning platform market and

179
00:12:48,600 --> 00:12:53,800
how you have all these kind of specialists and then you have all these end or or kind

180
00:12:53,800 --> 00:13:03,720
of wide folks that are trying to address like all of the problems in machine learning.

181
00:13:03,720 --> 00:13:08,600
And they all kind of start from somewhere or come at it from some angle and for some

182
00:13:08,600 --> 00:13:13,320
of them, the angle that they come at it from is data management, totally.

183
00:13:13,320 --> 00:13:19,800
But so it sounds like your perspective is that you don't need to fully solve that problem

184
00:13:19,800 --> 00:13:28,080
to get to some level of reproducibility or at least kind of useful utility.

185
00:13:28,080 --> 00:13:29,080
Yeah, exactly.

186
00:13:29,080 --> 00:13:33,560
I mean, it's just that back and sort of say that the sort of value prop of the experiment

187
00:13:33,560 --> 00:13:37,200
tracking tool we have and there's sort of three parts to it, right?

188
00:13:37,200 --> 00:13:44,320
So one is this kind of versioning and reproducibility and we want to make it as lightweight as possible.

189
00:13:44,320 --> 00:13:49,600
So we don't to have true reproducibility, we'd have to put a lot of constraints around

190
00:13:49,600 --> 00:13:56,520
you and so we just try to track as much as we can simply and you can add more things

191
00:13:56,520 --> 00:13:57,520
to track.

192
00:13:57,520 --> 00:14:02,680
But that's just one piece of I think experiment tracking is the sort of the versioning.

193
00:14:02,680 --> 00:14:08,160
I think the sort of second piece of experiment tracking is around visualizing what's

194
00:14:08,160 --> 00:14:09,160
happening, right?

195
00:14:09,160 --> 00:14:14,560
So, as your model's training, you want to see typically lots of different accuracies

196
00:14:14,560 --> 00:14:21,200
and lost curves kind of over time and over other axes and you also want to these days

197
00:14:21,200 --> 00:14:27,720
compare across tens, hundreds or we even see thousands or tens of thousands of experiments

198
00:14:27,720 --> 00:14:32,480
and there may be many different ways to look at kind of what's the best model or what's

199
00:14:32,480 --> 00:14:35,320
the best set of input hyper parameters.

200
00:14:35,320 --> 00:14:39,720
That's sort of a second value proposition that you get really well out of our tool and

201
00:14:39,720 --> 00:14:42,720
then the third thing is collaboration, right?

202
00:14:42,720 --> 00:14:48,040
So being able to kind of share the work that you did with a colleague or with someone at

203
00:14:48,040 --> 00:14:54,240
a different organization or even with your future self in a way that they can understand.

204
00:14:54,240 --> 00:14:59,080
So you kind of get all of those things with our tool and you sort of get them to the extent

205
00:14:59,080 --> 00:15:00,080
that you buy in.

206
00:15:00,080 --> 00:15:05,800
So a big thing for me is actually not to build a kind of end-to-end platform that's super

207
00:15:05,800 --> 00:15:10,200
heavy weight that requires a lot of upfront cost.

208
00:15:10,200 --> 00:15:15,920
So we try to make it just two or three lines of code that you add to your working thing

209
00:15:15,920 --> 00:15:20,560
and it can run anywhere, you can run an Amazon, it can run on an Jupyter notebook, it

210
00:15:20,560 --> 00:15:25,160
can run on prem, all these things kind of work and then you get reasonable defaults.

211
00:15:25,160 --> 00:15:30,640
And as you kind of add things like hooking us up to your Git state, hooking us up to your

212
00:15:30,640 --> 00:15:35,080
data version control, hooking us up to your underlying platforms, then we're able to kind

213
00:15:35,080 --> 00:15:36,080
of give you more things.

214
00:15:36,080 --> 00:15:40,280
And sort of the more information that you give and the more that you tell us about your

215
00:15:40,280 --> 00:15:44,440
specific state, the more we can give you full reproducibility.

216
00:15:44,440 --> 00:15:50,080
What are the couple of lines of code that you're inserting to get started doing?

217
00:15:50,080 --> 00:15:57,320
So basically on your client side, we have a Python library, so you basically do a pip install

218
00:15:57,320 --> 00:16:00,640
WNB and then you import a library.

219
00:16:00,640 --> 00:16:05,920
And then actually if you're using Keras, we have a one-liner that will instrument your

220
00:16:05,920 --> 00:16:09,800
code so you'll actually get a lot of value in a single line.

221
00:16:09,800 --> 00:16:16,520
If you're using PyTorch or TensorFlow, it's maybe two or three lines you call WNB.init

222
00:16:16,520 --> 00:16:22,400
and you pass in your configuration and then in PyTorch you call WNB watch on your model.

223
00:16:22,400 --> 00:16:29,440
In TensorFlow, we use a hook so you'll basically use a WNB hook so that during your training

224
00:16:29,440 --> 00:16:31,680
it keeps reporting to our model.

225
00:16:31,680 --> 00:16:35,000
And also if you're using TensorFlow, we have a different way of integrating.

226
00:16:35,000 --> 00:16:40,520
So we basically kind of worked with the sort of status quo of how people monitor things

227
00:16:40,520 --> 00:16:44,680
today and kind of made a really lightweight way to connect to that.

228
00:16:44,680 --> 00:16:50,280
And so you're using whatever native model introspection tools are available through these

229
00:16:50,280 --> 00:16:56,440
frameworks to figure out things like the model type and parameters and all that kind

230
00:16:56,440 --> 00:16:57,440
of stuff.

231
00:16:57,440 --> 00:17:00,120
If you don't have to pass that in explicitly to the library.

232
00:17:00,120 --> 00:17:01,120
Yeah, exactly.

233
00:17:01,120 --> 00:17:02,800
So we figure out what we can.

234
00:17:02,800 --> 00:17:08,160
So Keras and TensorFlow and PyTorch all have different ways of doing this.

235
00:17:08,160 --> 00:17:16,720
So we'll take your TensorFlow flags or your Keras config parameters and we'll save those.

236
00:17:16,720 --> 00:17:19,880
But if you want to save extra stuff, we make that really easy.

237
00:17:19,880 --> 00:17:25,680
It's just a single line of essentially it's a dictionary of inputs and you can add to

238
00:17:25,680 --> 00:17:26,680
that dictionary.

239
00:17:26,680 --> 00:17:30,800
You mentioned PyTorch and TensorFlow and Keras.

240
00:17:30,800 --> 00:17:35,600
These are all deep learning frameworks.

241
00:17:35,600 --> 00:17:39,920
Is deep learning the only use case for weights and biases?

242
00:17:39,920 --> 00:17:46,920
Do you tell folks to look elsewhere if they're trying to experiment tracking for more traditional

243
00:17:46,920 --> 00:17:50,960
models or not using one of these frameworks?

244
00:17:50,960 --> 00:17:51,960
Mostly.

245
00:17:51,960 --> 00:17:56,360
So our application is framework agnostic in the sense that you can use it with any framework.

246
00:17:56,360 --> 00:18:01,120
So you can do the configuration tracking and the logging with, you know, with scikit

247
00:18:01,120 --> 00:18:05,920
learn or XG boost or anything like that. I think that where experiment tracking becomes

248
00:18:05,920 --> 00:18:11,880
more valuable is when your experiments take longer or you want to do complicated hyper

249
00:18:11,880 --> 00:18:13,520
parameter searching.

250
00:18:13,520 --> 00:18:15,960
And we see more of that with deep learning.

251
00:18:15,960 --> 00:18:21,120
So just as a company, we've really focused on deep learning and these frameworks to kind

252
00:18:21,120 --> 00:18:23,960
of give you these magical installs.

253
00:18:23,960 --> 00:18:27,240
If you're using a different framework, it's going to take you some more lines.

254
00:18:27,240 --> 00:18:32,200
Although, you know, we've had people, we've had the community basically submit a integration

255
00:18:32,200 --> 00:18:34,320
for fast.ai.

256
00:18:34,320 --> 00:18:37,680
So we now have a community provided fast.ai integration.

257
00:18:37,680 --> 00:18:42,040
And then we had an enthusiastic employee build a Jack's integration.

258
00:18:42,040 --> 00:18:44,280
I don't know if anyone, do you know Jack's?

259
00:18:44,280 --> 00:18:45,280
What's Jack's?

260
00:18:45,280 --> 00:18:48,640
It's like a newer kind of even lighter weight.

261
00:18:48,640 --> 00:18:51,440
I guess you might call it deep learning framework.

262
00:18:51,440 --> 00:18:57,320
And then we have started to see people use us with XG boost and scikit learn.

263
00:18:57,320 --> 00:19:00,280
So we're working on kind of making that more native.

264
00:19:00,280 --> 00:19:04,000
And we did a study group.

265
00:19:04,000 --> 00:19:10,880
One of our study groups associated with the Twoma Meetup was studying or working through

266
00:19:10,880 --> 00:19:17,640
the full stack deep learning course by Peter Abil and others.

267
00:19:17,640 --> 00:19:25,000
And I guess weight's and biases is like a standard part of that course or something

268
00:19:25,000 --> 00:19:29,720
that you're told about and told to install in the course because we had a bunch of chatter

269
00:19:29,720 --> 00:19:34,480
in our slack about weight's and biases and people sharing screenshots and stuff like

270
00:19:34,480 --> 00:19:35,480
that.

271
00:19:35,480 --> 00:19:36,480
Yeah.

272
00:19:36,480 --> 00:19:40,680
And we saw a bunch of, of Tumofux come in from that, which is, which is fun for us.

273
00:19:40,680 --> 00:19:41,680
Oh, awesome.

274
00:19:41,680 --> 00:19:44,560
And by the way, anyone listening, you should, you should just know if you reach out to

275
00:19:44,560 --> 00:19:47,040
our, our little chat in the bottom right.

276
00:19:47,040 --> 00:19:50,440
It's not, it doesn't go to like some sales rep, it mainly goes to me.

277
00:19:50,440 --> 00:19:57,080
So I'm happy to give you tech support and please reach out and tell us, you know, kind

278
00:19:57,080 --> 00:19:58,080
of who you are.

279
00:19:58,080 --> 00:20:02,040
We're not so big that we don't, you know, kind of want to know what people's issues are.

280
00:20:02,040 --> 00:20:10,040
So the first of these kind of main value propses versioning and we've, we're primarily

281
00:20:10,040 --> 00:20:16,840
talking about keeping track of your model parameters, your models as well.

282
00:20:16,840 --> 00:20:22,200
You're not tracking those, you're just connecting or kind of tracking, uh, get commits of the

283
00:20:22,200 --> 00:20:23,200
models themselves.

284
00:20:23,200 --> 00:20:24,200
Is that right?

285
00:20:24,200 --> 00:20:27,000
So again, everything is sort of, you know, opt in, right?

286
00:20:27,000 --> 00:20:30,640
You know, so, so we work with people that, you know, have various levels of sensitivity.

287
00:20:30,640 --> 00:20:34,160
So, um, and, and we really, you know, we have a strong point of view here where I really

288
00:20:34,160 --> 00:20:38,960
don't want to be a end to end framework where you have to buy into everything to get value.

289
00:20:38,960 --> 00:20:39,960
Yeah.

290
00:20:39,960 --> 00:20:43,960
Um, so, you know, if you want us to, we'll keep track of your, your get shot and we'll

291
00:20:43,960 --> 00:20:47,600
keep a patch against the get, um, your latest get commits.

292
00:20:47,600 --> 00:20:51,160
So we'll know the state of your code if you want us to also, if you want us to, we'll

293
00:20:51,160 --> 00:20:54,960
save your, um, model files either during training or at the end of training.

294
00:20:54,960 --> 00:20:58,840
So, or, or any actually any other artifact that is important for running your model.

295
00:20:58,840 --> 00:21:03,400
So there is a saving component to this, which is important to a lot of our, our users.

296
00:21:03,400 --> 00:21:09,080
If you don't already have a pipeline built out that programmatically saves your model

297
00:21:09,080 --> 00:21:18,160
parameter someplace and commits your code someplace all in the context of a run, you could do that

298
00:21:18,160 --> 00:21:19,160
all through way to buy.

299
00:21:19,160 --> 00:21:20,160
So it sounds like.

300
00:21:20,160 --> 00:21:21,160
Yeah, exactly.

301
00:21:21,160 --> 00:21:25,160
And if you already do have some kind of, um, pipeline where it's somewhere saved, then

302
00:21:25,160 --> 00:21:28,760
maybe the best thing to do is just actually save a link to it, um, in our application.

303
00:21:28,760 --> 00:21:33,640
You know, the important thing is that, um, the run gets associated with all the, the files

304
00:21:33,640 --> 00:21:35,040
you'd need to reproduce it.

305
00:21:35,040 --> 00:21:40,040
Okay. And so the next thing that comes up is, uh, visualization.

306
00:21:40,040 --> 00:21:45,080
And when I think about like versioning and, and like saving model parameters and visualization,

307
00:21:45,080 --> 00:21:48,000
the first thing that comes to mind for me is tensor board.

308
00:21:48,000 --> 00:21:53,960
Are, are you trying to compete with that or replace it or do they complement each other

309
00:21:53,960 --> 00:21:54,960
somehow?

310
00:21:54,960 --> 00:21:55,960
Yeah.

311
00:21:55,960 --> 00:21:57,960
I mean, I think it's, it's super complimentary.

312
00:21:57,960 --> 00:22:02,960
So, you know, one thing that we actually do, which people find useful is we will host

313
00:22:02,960 --> 00:22:03,960
your tensor board.

314
00:22:03,960 --> 00:22:09,240
We send us a TF events files and artifact, um, you know, we recognize that and we'll actually

315
00:22:09,240 --> 00:22:12,960
pop open a hosted tensor board for you in the cloud.

316
00:22:12,960 --> 00:22:18,680
Um, you know, my big issue, there's sort of two things, two ways that we improve on, um,

317
00:22:18,680 --> 00:22:19,680
tensor board.

318
00:22:19,680 --> 00:22:23,520
Which I actually think is an excellent tool, um, you know, the, the, the, the, the first

319
00:22:23,520 --> 00:22:27,760
thing that we improve on is that, um, tensor board tends to be a femoral.

320
00:22:27,760 --> 00:22:30,280
So, you know, you, you typically run it locally.

321
00:22:30,280 --> 00:22:34,440
And you know, one thing we, I saw a lot when I went around, um, and talked to folks about

322
00:22:34,440 --> 00:22:38,480
how they were doing their experiment tracking today is they were literally taking screenshots

323
00:22:38,480 --> 00:22:41,880
of their, um, tensor board and, and posting it into Slack.

324
00:22:41,880 --> 00:22:46,240
And that, that seems a little, um, you know, that seems like a little crazy to me, or,

325
00:22:46,240 --> 00:22:51,200
I guess it seems like there's an opportunity for, um, that to be hosted forever, right?

326
00:22:51,200 --> 00:22:57,840
So if you, um, if you use weights and biases, then all this stuff, all these graphs that

327
00:22:57,840 --> 00:23:03,280
you make and all the, um, all the runs that you do, they're hosted forever, or as until

328
00:23:03,280 --> 00:23:05,360
you delete them, um, in the cloud.

329
00:23:05,360 --> 00:23:07,920
So I think that's a much better practice, right?

330
00:23:07,920 --> 00:23:12,920
Because, you know, if you shut down your tensor board server, um, you know, your colleagues

331
00:23:12,920 --> 00:23:14,360
may still want to look at what you're doing, right?

332
00:23:14,360 --> 00:23:19,400
So that, the, the, the sort of like static, permanent URL is kind of the first improvement.

333
00:23:19,400 --> 00:23:21,240
I think the weights and biases has.

334
00:23:21,240 --> 00:23:22,240
Mm-hmm.

335
00:23:22,240 --> 00:23:27,000
And then the second thing, um, is I think that tensor board starts to struggle when you're

336
00:23:27,000 --> 00:23:31,600
comparing lots and lots of models with lots and lots of data points, uh, elaborate on

337
00:23:31,600 --> 00:23:32,600
that.

338
00:23:32,600 --> 00:23:34,600
Where in particular does it struggle?

339
00:23:34,600 --> 00:23:36,840
So there's kind of two ways that it can struggle, right?

340
00:23:36,840 --> 00:23:43,760
So one is if you, if you run a model over millions and millions of, um, data points, it

341
00:23:43,760 --> 00:23:46,200
doesn't, um, ever really start to sample, right?

342
00:23:46,200 --> 00:23:49,440
So you know, if, if you have a run that, that, you know, you run it for like a couple

343
00:23:49,440 --> 00:23:55,440
of weeks, um, you know, it, uh, it can just be actually literally slow, um, super slow,

344
00:23:55,440 --> 00:24:01,000
right to, to run it even in your browser, um, and the second thing is that if you want

345
00:24:01,000 --> 00:24:05,360
to compare lots of runs, I don't think that tensor board was kind of originally designed

346
00:24:05,360 --> 00:24:06,360
for that.

347
00:24:06,360 --> 00:24:11,360
So there is ways to compare, you know, three, four, five, um, runs, but what we typically

348
00:24:11,360 --> 00:24:16,160
see is people will do, you know, hundreds or thousands of runs with different, um, you

349
00:24:16,160 --> 00:24:19,920
know, kind of hyper parameters and configurations and they want to mark some is, hey, you know,

350
00:24:19,920 --> 00:24:22,600
these were bass lines and, you know, here's what I was doing here and here's what I was

351
00:24:22,600 --> 00:24:23,600
doing here.

352
00:24:23,600 --> 00:24:27,160
And, you know, the typical way people do it in tensor board is they start to do that crazy

353
00:24:27,160 --> 00:24:32,160
long, um, file name thing and then kind of search over them with regular expressions.

354
00:24:32,160 --> 00:24:34,840
And, um, you know, that just, it doesn't scale, right?

355
00:24:34,840 --> 00:24:39,960
So when you do, um, you know, when you, when you're really doing like serious, um, evaluate

356
00:24:39,960 --> 00:24:43,400
equations, I mean, I, I don't want to knock on, um, tensor board, I think it's a, it's

357
00:24:43,400 --> 00:24:47,240
a great tool for regular expressions, or regular expressions, that's a lot of regular

358
00:24:47,240 --> 00:24:48,240
expressions.

359
00:24:48,240 --> 00:24:49,240
Yeah.

360
00:24:49,240 --> 00:24:50,240
Totally, totally.

361
00:24:50,240 --> 00:24:51,240
Um, you know, yeah, good point.

362
00:24:51,240 --> 00:24:57,520
I definitely don't want to, um, insult regular expressions, um, but, um, I think that, uh,

363
00:24:57,520 --> 00:25:02,280
we have a tool that's more designed for a, kind of where you get to down the road when

364
00:25:02,280 --> 00:25:07,920
you have, you know, hundreds of runs and you want to kind of filter and group things, um,

365
00:25:07,920 --> 00:25:08,920
in different ways.

366
00:25:08,920 --> 00:25:13,480
And I should say it is a, it can be a little tricky to monitor things on kind of big distributed

367
00:25:13,480 --> 00:25:14,480
runs.

368
00:25:14,480 --> 00:25:17,200
So when you're running across like multiple machines, um, that's also a case that we

369
00:25:17,200 --> 00:25:20,240
really focused on at, at weights and biases.

370
00:25:20,240 --> 00:25:25,440
Kind of a distributed training scenario where you're, you've got multiple machines training

371
00:25:25,440 --> 00:25:26,440
a single model.

372
00:25:26,440 --> 00:25:27,440
Yeah, exactly.

373
00:25:27,440 --> 00:25:31,160
I mean, it's kind of makes sense if you're running on multiple machines or a single model,

374
00:25:31,160 --> 00:25:36,280
um, to have everybody kind of reporting to like a single centralized place, um, universes

375
00:25:36,280 --> 00:25:41,080
everybody writing out, you know, kind of files locally, you know, I think it's just different,

376
00:25:41,080 --> 00:25:42,400
different design goals.

377
00:25:42,400 --> 00:25:46,560
And so, um, I would say, I would say weights and biases is complimentary with, um, tensor

378
00:25:46,560 --> 00:25:50,200
board, but it is kind of the closest, I'd say it's the most common experiment track

379
00:25:50,200 --> 00:25:52,000
and thing that we, that we see today.

380
00:25:52,000 --> 00:25:55,000
So it is the right place to compare weights and biases.

381
00:25:55,000 --> 00:26:00,920
Uh, and so then the third, uh, element that you mentioned is collaboration.

382
00:26:00,920 --> 00:26:06,800
Uh, I imagine just having that static URL is, uh, not having to screenshot and send it

383
00:26:06,800 --> 00:26:08,120
in Slack.

384
00:26:08,120 --> 00:26:13,680
It probably, uh, is a starting point for like collaboration, but the, are you doing something

385
00:26:13,680 --> 00:26:16,240
explicit, uh, around collaboration?

386
00:26:16,240 --> 00:26:17,240
Yeah, totally.

387
00:26:17,240 --> 00:26:22,080
I mean, I think, um, so I think Adrian, uh, guidance on your podcast from TRI and talked

388
00:26:22,080 --> 00:26:26,480
a little bit about, um, using our tool for collaboration, but, um, you know, they've,

389
00:26:26,480 --> 00:26:29,880
they've sort of talked about it publicly and, you know, opening up is kind of done a case

390
00:26:29,880 --> 00:26:33,480
study with us and how they use us to, um, collaborate.

391
00:26:33,480 --> 00:26:38,400
But I think, I think a lot of it is just around, um, helping people get more systematic

392
00:26:38,400 --> 00:26:42,080
about their training so that it's possible for other people to pick up your work.

393
00:26:42,080 --> 00:26:44,760
And, you know, the regular fashion thing we talked about is a great example, right?

394
00:26:44,760 --> 00:26:50,280
Like if you put in little notes in your run names for your hyper parameters, that might

395
00:26:50,280 --> 00:26:54,840
make sense to you, but it could be really hard for your colleague, um, to come in and do

396
00:26:54,840 --> 00:26:59,360
a similar analysis if they don't know, um, you know, what's your, you know, exactly what

397
00:26:59,360 --> 00:27:01,600
your run names mean and, and what you were doing.

398
00:27:01,600 --> 00:27:05,320
So, you know, we make it a lot easier to have human readable names and then also share

399
00:27:05,320 --> 00:27:07,440
your projects with your colleagues.

400
00:27:07,440 --> 00:27:11,520
So what happens is, you know, you can, you can basically set up a report and you can, you

401
00:27:11,520 --> 00:27:14,720
can put notes in that report and like literally type out, okay, here's what I was doing.

402
00:27:14,720 --> 00:27:18,720
Here, here's the different runs and then a colleague can, you know, go and look at any of

403
00:27:18,720 --> 00:27:23,600
those individual runs and see what happened and then also look at the aggregate, um, statistics

404
00:27:23,600 --> 00:27:26,320
and then also kind of build their own, um, analysis.

405
00:27:26,320 --> 00:27:32,200
So I think where, where waits and biases becomes this like really beloved tool, um, is

406
00:27:32,200 --> 00:27:36,240
when our, our customers start to use it for collaboration because, because that's just

407
00:27:36,240 --> 00:27:39,840
something you kind of can't get out of anything else at, at least right now.

408
00:27:39,840 --> 00:27:46,520
It doesn't sound like though that you're necessarily trying to build the, I don't know what you

409
00:27:46,520 --> 00:27:51,760
would call this thing, the enterprise Facebook for models or something like that where like

410
00:27:51,760 --> 00:27:56,560
every user has a feed and all of their runs going their feed and people are commenting

411
00:27:56,560 --> 00:27:58,920
on each other's runs and that kind of thing.

412
00:27:58,920 --> 00:28:03,920
No, I mean, I think, um, I think we want to do support, you know, discussions outside

413
00:28:03,920 --> 00:28:04,920
of our tool.

414
00:28:04,920 --> 00:28:07,320
I mean, I think there's lots of good ways to, you know, so you have a Slack integration,

415
00:28:07,320 --> 00:28:10,000
you know, so you can, you can post this stuff into Slack.

416
00:28:10,000 --> 00:28:15,280
Um, but I don't think that, that machine learning has necessarily that different of a workflow

417
00:28:15,280 --> 00:28:20,560
that we want to, you know, try to, try to make our own version of a, of a feed, um, although

418
00:28:20,560 --> 00:28:24,440
I would say, you know, we are experimenting with a thing that's been, I'm really excited

419
00:28:24,440 --> 00:28:29,520
about called benchmarks where people can collaborate across, um, organizations.

420
00:28:29,520 --> 00:28:36,680
So we can take an open source, um, you know, machine learning project, um, and then people

421
00:28:36,680 --> 00:28:38,920
around the world can submit their results on it, right?

422
00:28:38,920 --> 00:28:44,280
So they can, they can modify the code and show their accuracy on, you know, their own

423
00:28:44,280 --> 00:28:46,040
data sets or their own setups.

424
00:28:46,040 --> 00:28:47,600
And so, have you done any of those?

425
00:28:47,600 --> 00:28:53,760
What's an example of, of a project that, and some, you know, folks submitting these benchmarks?

426
00:28:53,760 --> 00:28:54,760
Yeah.

427
00:28:54,760 --> 00:28:58,040
So you can find them on our website, but I think I'm one that was kind of fun was, you

428
00:28:58,040 --> 00:29:02,960
know, Giffy, the company gave us a whole ton of gifts of cats.

429
00:29:02,960 --> 00:29:07,520
And so we did kind of a video frame, um, prediction benchmarks.

430
00:29:07,520 --> 00:29:11,840
So, you know, you get the first five frames, I believe, and then you predict the next five

431
00:29:11,840 --> 00:29:12,840
frames.

432
00:29:12,840 --> 00:29:16,600
And so, you know, what we actually saw was all the kind of different strategies that people

433
00:29:16,600 --> 00:29:21,280
use for, um, video frame prediction, how well they work on this data set, but I thought

434
00:29:21,280 --> 00:29:24,480
it was particularly cool about the way the benchmark was set up is that you can actually

435
00:29:24,480 --> 00:29:31,160
go in and look at all the different submissions, um, models and all of their kind of, um, data

436
00:29:31,160 --> 00:29:36,080
in a standard as way, um, or even kind of sort the submissions based on, you know, different

437
00:29:36,080 --> 00:29:37,960
metrics and then pixel distance.

438
00:29:37,960 --> 00:29:41,800
So, you know, different algorithms might work better depending on your, and metric, and,

439
00:29:41,800 --> 00:29:45,520
you know, we have all the different models of people submitted, and then, you know, we're

440
00:29:45,520 --> 00:29:50,440
doing one now on, um, you know, drought prediction, you know, to kind of help, um, to help

441
00:29:50,440 --> 00:29:52,760
folks, you know, figure out where, where droughts are happening.

442
00:29:52,760 --> 00:29:56,280
It's actually sort of a, apparently like a cactus identifier, because, you know, they,

443
00:29:56,280 --> 00:29:59,480
they, the state of the Arctic, it's just like using the amount of green, but the problem

444
00:29:59,480 --> 00:30:03,760
is, you know, cactus is our green, and, you know, that can be like a challenge to, um,

445
00:30:03,760 --> 00:30:07,080
to kind of know where, where, where, you know, droughts are happening on like a small

446
00:30:07,080 --> 00:30:08,080
scale.

447
00:30:08,080 --> 00:30:10,440
And, you know, we have a whole bunch of other benchmarks, it's kind of a new feature,

448
00:30:10,440 --> 00:30:13,840
so it's still, um, we're kind of still seeing how people use it.

449
00:30:13,840 --> 00:30:19,120
But, um, what I like about it is I think there is a lot of room for collaboration across

450
00:30:19,120 --> 00:30:22,360
teams, and there's sort of a rich, kind of culture of it, and machine learning because

451
00:30:22,360 --> 00:30:24,760
so much of it comes out of academia, right?

452
00:30:24,760 --> 00:30:28,680
But, um, you know, I think one of the challenges with, when you get a research paper is that,

453
00:30:28,680 --> 00:30:32,080
you know, you get a small table of results, but you don't get to see all the different

454
00:30:32,080 --> 00:30:35,280
things that the researcher tried and all the, you know, kind of all the paths that they

455
00:30:35,280 --> 00:30:39,240
went down that, that didn't work, but, um, you know, if, if their stuff is instrumented

456
00:30:39,240 --> 00:30:44,200
with weights and biases, you can actually have this really, um, detailed record of, of

457
00:30:44,200 --> 00:30:48,640
lots and lots of different things, and, and kind of start from any point in the process.

458
00:30:48,640 --> 00:30:50,000
Has anyone done that?

459
00:30:50,000 --> 00:30:56,360
Have you seen a paper that cited one of these static URLs with, uh, all of their experimental

460
00:30:56,360 --> 00:30:57,360
results?

461
00:30:57,360 --> 00:31:01,520
Yeah, we just had our first one, actually, so, uh, really knew the, you know, the paper

462
00:31:01,520 --> 00:31:05,240
publishing process is long, but yeah, we actually just, we just had our first one.

463
00:31:05,240 --> 00:31:06,240
What was the paper?

464
00:31:06,240 --> 00:31:10,520
I mean, I think we'll see a lot more, uh, down the road because we did make our, we

465
00:31:10,520 --> 00:31:14,200
make our product, um, free, academics, because we really liked the Seuce case, but this

466
00:31:14,200 --> 00:31:18,480
was called a machine learning techniques for detecting, identifying linguistic patterns

467
00:31:18,480 --> 00:31:20,600
in, um, news media.

468
00:31:20,600 --> 00:31:21,760
Is that literally what they did?

469
00:31:21,760 --> 00:31:25,760
They kind of gave the static weights and biases, Lincoln, you can go in and look at all

470
00:31:25,760 --> 00:31:29,040
of their various experiment runs or, yeah, yeah, exactly.

471
00:31:29,040 --> 00:31:34,000
I mean, we probably should make a more systematic way to do this, but, um, for the emancipation,

472
00:31:34,000 --> 00:31:37,560
um, you know, for their reports, you can go in and look at their, you know, look at

473
00:31:37,560 --> 00:31:40,200
their report and, and get more detail.

474
00:31:40,200 --> 00:31:47,360
When you're talking to folks that are, uh, outside of academia on the enterprise side,

475
00:31:47,360 --> 00:31:52,680
and they're, you know, getting serious with deep learning and starting to figure out

476
00:31:52,680 --> 00:31:59,000
how they can build some more structure around their approach.

477
00:31:59,000 --> 00:32:04,240
What are the, I guess I have a bunch of questions around your experiences there.

478
00:32:04,240 --> 00:32:09,880
Are there, are there, do you find cultural issues, quote, unquote, around, you know, them

479
00:32:09,880 --> 00:32:15,840
adopting a tool like this or some, are there patterns around which teams are more likely

480
00:32:15,840 --> 00:32:23,720
to kind of get it and, you know, want it or, um, is it, is it kind of random?

481
00:32:23,720 --> 00:32:28,120
No, no, I mean, we've, we've designed this tool with a really particular end user in

482
00:32:28,120 --> 00:32:29,120
mind.

483
00:32:29,120 --> 00:32:34,240
Um, so this is a tool for researchers that are working on deep learning.

484
00:32:34,240 --> 00:32:39,800
So we work with companies that have, um, researchers that are training deep learning models.

485
00:32:39,800 --> 00:32:43,720
And we tend to work with, you know, companies that are making a bigger investment in that

486
00:32:43,720 --> 00:32:44,720
today.

487
00:32:44,720 --> 00:32:49,040
You know, you work with folks like, um, you know, GitHub and, and, you know, Blue River,

488
00:32:49,040 --> 00:32:54,160
which is bought by John Deere, um, and, you know, a whole bunch of kind of robotics and,

489
00:32:54,160 --> 00:32:57,680
and aerial imaging companies and, and that's, that's because those are the companies that

490
00:32:57,680 --> 00:33:02,080
are right now making the biggest investments in deep learning.

491
00:33:02,080 --> 00:33:06,720
And so, you know, our strategy has been, you know, rather than kind of focus on the sort

492
00:33:06,720 --> 00:33:12,040
of mass democratization of, of AI to focus on, let's, let's look at what the companies

493
00:33:12,040 --> 00:33:17,560
that are, um, most advanced doing and make the bet that other companies are going to follow

494
00:33:17,560 --> 00:33:20,280
along and do kind of similar techniques.

495
00:33:20,280 --> 00:33:23,880
So, you know, a lot of people disagree with that strategy and they say, well, you know,

496
00:33:23,880 --> 00:33:27,640
you should build, you know, the, the tools that, um, you know, proctor and gamble needs

497
00:33:27,640 --> 00:33:32,880
is very different than the tools that, you know, maybe an open area or a, a Google needs.

498
00:33:32,880 --> 00:33:36,480
Um, but I guess my perspective is, is a little different.

499
00:33:36,480 --> 00:33:40,800
I think that, you know, I think that, that proctor and gamble may not be training lots and

500
00:33:40,800 --> 00:33:43,200
lots of deep learning model sales, they are training some.

501
00:33:43,200 --> 00:33:46,440
I think that over time, they're going to train more and more and they're going to want

502
00:33:46,440 --> 00:33:49,640
to bring that expertise in house.

503
00:33:49,640 --> 00:33:54,160
So, I want my tool to make things, you know, easier for, for machine learning researchers,

504
00:33:54,160 --> 00:33:58,160
but I don't necessarily want to make the machine learning research drops lead or kind

505
00:33:58,160 --> 00:34:02,240
of, um, you know, automate every, every part of the process like, you know, some of these

506
00:34:02,240 --> 00:34:06,000
like, um, you know, auto email types of, of projects.

507
00:34:06,000 --> 00:34:10,840
So we typically go in, we're typically get adopted by researchers inside of companies

508
00:34:10,840 --> 00:34:15,480
and then end up selling a larger license as the, um, business aside, they want to standard

509
00:34:15,480 --> 00:34:16,840
eyes on our tool.

510
00:34:16,840 --> 00:34:20,520
And you, you're describing this target user as a researcher.

511
00:34:20,520 --> 00:34:28,320
Is that, uh, universal or do you find that the user is split across folks that are

512
00:34:28,320 --> 00:34:33,880
formally called researchers and data scientists, machine learning engineers and other things

513
00:34:33,880 --> 00:34:34,880
we see?

514
00:34:34,880 --> 00:34:39,240
Well, of course, I mean, Sam, you know, this market, well, also, I mean, the titles

515
00:34:39,240 --> 00:34:40,640
is total chaos, right?

516
00:34:40,640 --> 00:34:44,240
So, I was kind of getting it there.

517
00:34:44,240 --> 00:34:49,480
Yeah, I mean, you know, as, as you know, right, it's, it's hard to tell, um, you know,

518
00:34:49,480 --> 00:34:53,640
these days from a title, what someone's actually doing, the important thing to us really,

519
00:34:53,640 --> 00:34:58,240
like our, our kind of qualifier is that they're actually training, um, you know, machine learning

520
00:34:58,240 --> 00:35:03,400
models and, and generally training, at least some of the models in one of the frameworks

521
00:35:03,400 --> 00:35:05,240
that we have kind of first order support for.

522
00:35:05,240 --> 00:35:09,640
So that's, you know, as I said, Cara's center flow, pie torture, you know, maybe fast AI.

523
00:35:09,640 --> 00:35:15,400
So those are the folks that are most likely to benefit, um, from our tool.

524
00:35:15,400 --> 00:35:23,560
Have you come across folks that are using fast AI, I don't know if commercially is the

525
00:35:23,560 --> 00:35:29,200
right word or organizationally, like, you know, at a company or research organization,

526
00:35:29,200 --> 00:35:34,080
um, meaning outside of the kind of educational context?

527
00:35:34,080 --> 00:35:39,720
Yeah, I would say yes, it's not, I would say it's not, um, it's mostly in an educational

528
00:35:39,720 --> 00:35:40,720
context.

529
00:35:40,720 --> 00:35:43,360
And sometimes it's a little, um, hard to tell.

530
00:35:43,360 --> 00:35:48,080
I think educational products and some of these orgs can bleed into production products,

531
00:35:48,080 --> 00:35:49,080
you know, yeah, yeah.

532
00:35:49,080 --> 00:35:50,080
Yeah.

533
00:35:50,080 --> 00:35:56,880
You know, gradually, um, so, um, we, we actually honestly, we do see some fast AI and

534
00:35:56,880 --> 00:35:59,400
stuff that looks like it's head and towards production.

535
00:35:59,400 --> 00:36:03,600
I don't know that I could point to something where it's actually going to get deployed,

536
00:36:03,600 --> 00:36:06,520
but, you know, it is really pie torch, um, you know, under the hood.

537
00:36:06,520 --> 00:36:12,760
And I would say one insight that I have is we do see pie torch in a production context

538
00:36:12,760 --> 00:36:16,600
a lot more than I think, um, you know, it's reputation to have you believe.

539
00:36:16,600 --> 00:36:22,320
So, um, people are definitely deploying pie torch successfully into real world applications.

540
00:36:22,320 --> 00:36:25,240
So I don't see why fast AI wouldn't allow you to do that.

541
00:36:25,240 --> 00:36:28,640
I mean, you know, folks have different opinions about fast AI, you know, some, some seem

542
00:36:28,640 --> 00:36:30,320
to love it, some seem to hate it.

543
00:36:30,320 --> 00:36:33,840
But I think it, you know, I think if somebody really loved it, it shouldn't be out of the

544
00:36:33,840 --> 00:36:34,840
question to deploy it.

545
00:36:34,840 --> 00:36:35,840
Yeah.

546
00:36:35,840 --> 00:36:43,480
Um, do you, is there any interaction with, uh, tools like Onyx here?

547
00:36:43,480 --> 00:36:45,280
Um, not really.

548
00:36:45,280 --> 00:36:50,480
We haven't seen a lot of Onyx and we've seen that file format used to save, um, models

549
00:36:50,480 --> 00:36:55,200
occasionally, um, but, um, you know, for whatever reason, we, we, I haven't known

550
00:36:55,200 --> 00:37:02,000
it's a lot of that, you know, if you think about the machine learning pipeline or the,

551
00:37:02,000 --> 00:37:06,720
what are the things that are, you know, I guess we've talked about at some of the things

552
00:37:06,720 --> 00:37:11,840
that are like immediately upstream or downshing from you, you know, there's get repositories

553
00:37:11,840 --> 00:37:18,600
and collaboration and things like that are there, uh, things that, you know, folks really

554
00:37:18,600 --> 00:37:24,360
need to have in place in order to take advantage of experiment management or things that,

555
00:37:24,360 --> 00:37:28,080
you know, once you have experiment management, oh, wow, this whole new world opens up

556
00:37:28,080 --> 00:37:33,880
for you and you can do X, Y, Z, yeah, totally, um, you know, it's funny, it's funny to

557
00:37:33,880 --> 00:37:36,800
say this, but I think some folks, you know, come and don't realize this.

558
00:37:36,800 --> 00:37:41,040
I mean, you do need to actually be doing experiments to take advantage of experiment,

559
00:37:41,040 --> 00:37:48,720
it's not, um, you know, to be fair, it's, it's not totally trivial, right?

560
00:37:48,720 --> 00:37:53,120
So, you know, to actually start training, you know, deep learning models, you need, uh,

561
00:37:53,120 --> 00:37:56,600
you need some folks with experience in that, you know, you need machines that can do that

562
00:37:56,600 --> 00:38:01,240
and, and a lot of people want kind of a software layer that helps them with that.

563
00:38:01,240 --> 00:38:05,880
So we've made a fair amount of effort to integrate, um, with, with, um, the different stuff

564
00:38:05,880 --> 00:38:06,880
that we see.

565
00:38:06,880 --> 00:38:10,280
So, you know, we see a fair amount of number of folks using SageMaker.

566
00:38:10,280 --> 00:38:12,760
So, you know, we built the first class integration with that.

567
00:38:12,760 --> 00:38:17,400
We see a lot of excitement around ML flow and cube flow, so we built integrations with

568
00:38:17,400 --> 00:38:18,400
those tools.

569
00:38:18,400 --> 00:38:23,360
And our, our vision is to just run on top of, um, anywhere the people want to train their

570
00:38:23,360 --> 00:38:24,360
models.

571
00:38:24,360 --> 00:38:29,440
ML flow and cube flow sound so similar when you say them like that, but they're totally

572
00:38:29,440 --> 00:38:30,440
different things.

573
00:38:30,440 --> 00:38:36,480
I would have thought of ML flow, or I, I tend to think of ML flow as more of, uh, alternative

574
00:38:36,480 --> 00:38:40,440
to what you're doing, whereas cube flows like more infrastructure and orchestration.

575
00:38:40,440 --> 00:38:42,760
Yeah, but they've kind of bleed into each other.

576
00:38:42,760 --> 00:38:45,800
It's a cube flow, I think of as more infrastructure and orchestration.

577
00:38:45,800 --> 00:38:49,520
I think ML flow, um, you know, at least what they tell me is that they're kind of trying

578
00:38:49,520 --> 00:38:54,720
to be more of a sort of standard, um, set of APIs around, um, training.

579
00:38:54,720 --> 00:39:00,800
Um, as of course as they become kind of a standard way that different, um, you know, ML services

580
00:39:00,800 --> 00:39:01,800
can talk to each other.

581
00:39:01,800 --> 00:39:03,440
We want to, we want to work with them.

582
00:39:03,440 --> 00:39:04,440
Yeah.

583
00:39:04,440 --> 00:39:05,440
But yeah, good point.

584
00:39:05,440 --> 00:39:06,440
It's funny.

585
00:39:06,440 --> 00:39:07,960
It's like, as I say all these things, I'm thinking this podcast is going to be so out

586
00:39:07,960 --> 00:39:08,960
of date.

587
00:39:08,960 --> 00:39:09,960
Six months.

588
00:39:09,960 --> 00:39:16,440
Well, you know, that is a challenge, I, when I did the Kubernetes ebook in, that was

589
00:39:16,440 --> 00:39:19,560
published last November.

590
00:39:19,560 --> 00:39:28,000
And you know, on the one hand, I was racing to get it ready for CubeCon in, uh, in Seattle.

591
00:39:28,000 --> 00:39:32,600
But then there was a whole bunch of stuff that was announced that totally, uh, made a good

592
00:39:32,600 --> 00:39:37,840
chunk of it obsolete, you know, and there was a bunch of updating that needs to be done.

593
00:39:37,840 --> 00:39:45,440
And this, uh, the ML platforms paper, it probably mentions, I don't know, there's probably

594
00:39:45,440 --> 00:39:50,440
like 30, 50 tools or something that are mentioned in this thing.

595
00:39:50,440 --> 00:39:55,640
And I'm sure, you know, within a month of being published, some of them will be acquired,

596
00:39:55,640 --> 00:40:00,320
some of them will be, will disappear, some of them will evolve as kind of, you know,

597
00:40:00,320 --> 00:40:07,080
defective standards or market leaders or whatever it's, it is such a chaotic, um, and

598
00:40:07,080 --> 00:40:12,520
therefore exciting marketplace and time to be, you know, in and following this marketplace.

599
00:40:12,520 --> 00:40:13,520
Yeah.

600
00:40:13,520 --> 00:40:14,520
Absolutely.

601
00:40:14,520 --> 00:40:18,760
Yeah, I guess you have, you have the hardest job of trying to keep track of all this.

602
00:40:18,760 --> 00:40:20,920
It is crazy.

603
00:40:20,920 --> 00:40:26,360
It is crazy how much activity is happening in this space.

604
00:40:26,360 --> 00:40:27,360
Yeah.

605
00:40:27,360 --> 00:40:31,080
So anyway, you know, I think upstream, you know, we, we try to integrate with people

606
00:40:31,080 --> 00:40:37,960
just as, as folks, you know, ask for us and people typically have, you know, some kind

607
00:40:37,960 --> 00:40:40,800
of platform to do their runs, some kind of, sometimes some kind of, um, pipeline management

608
00:40:40,800 --> 00:40:41,800
or workflow management.

609
00:40:41,800 --> 00:40:46,000
Um, you know, they often have kind of a training data solution, um, I strongly recommend

610
00:40:46,000 --> 00:40:47,000
to figure it.

611
00:40:47,000 --> 00:40:48,000
And I'm totally unbiased.

612
00:40:48,000 --> 00:40:58,040
And then, um, you know, downstream from us, um, you know, there's, um, production deployment,

613
00:40:58,040 --> 00:41:01,680
I think it's kind of the next, um, the next step.

614
00:41:01,680 --> 00:41:07,120
And, um, you know, we mostly see folks doing that, um, on an ad hoc basis, although, you

615
00:41:07,120 --> 00:41:09,000
know, we've started to see a whole bunch of companies around that.

616
00:41:09,000 --> 00:41:13,640
So I'm sure that, um, you know, that's going to start, um, start picking up, you know,

617
00:41:13,640 --> 00:41:17,960
as, as the stuff becomes more real and, and, and people started to care about that more.

618
00:41:17,960 --> 00:41:22,280
Is, is there anything else we should cover before we wrap up or any kind of parting thoughts?

619
00:41:22,280 --> 00:41:26,600
No, you know, I just want to say, you know, the, it's a big goal for me is to make, make

620
00:41:26,600 --> 00:41:31,320
our software really, really easy to try and really low, kind of lock in.

621
00:41:31,320 --> 00:41:34,960
So, you know, I guess my, my asked the folks, you know, if you've listened this far, you

622
00:41:34,960 --> 00:41:39,960
probably get more out of spending five minutes, um, integrating weights and biases, um, into,

623
00:41:39,960 --> 00:41:40,960
into your tool.

624
00:41:40,960 --> 00:41:45,840
So, you know, it, it really is like a, kind of a five minute install, um, and we make the

625
00:41:45,840 --> 00:41:52,240
product free for individuals and free for, you know, academics and, um, easy to export

626
00:41:52,240 --> 00:41:53,240
your data.

627
00:41:53,240 --> 00:41:57,080
So, hopefully it's, it's pretty low commitment because, you know, my goal here is that, you

628
00:41:57,080 --> 00:42:01,160
know, everyone, you know, gives our experiment tracking tool just to try and, and, you know,

629
00:42:01,160 --> 00:42:04,640
if they have any questions or concerns, they, you know, write into our little intercom bubble

630
00:42:04,640 --> 00:42:06,200
and, um, ask for help.

631
00:42:06,200 --> 00:42:13,600
Well, we will be sure to point folks to a, uh, a place where they can find weights and

632
00:42:13,600 --> 00:42:16,400
biases and do that, explore it more.

633
00:42:16,400 --> 00:42:21,400
You, you refer to it as software, but it is software plus, like software as a service,

634
00:42:21,400 --> 00:42:22,400
right?

635
00:42:22,400 --> 00:42:23,400
Yeah.

636
00:42:23,400 --> 00:42:29,160
You're accessing the, the dashboards and the visualizations through your website.

637
00:42:29,160 --> 00:42:33,840
Do you have folks, uh, asking you to make it available, kind of behind the firewall

638
00:42:33,840 --> 00:42:34,840
are on premises?

639
00:42:34,840 --> 00:42:35,840
Yeah.

640
00:42:35,840 --> 00:42:36,840
We're selling that today.

641
00:42:36,840 --> 00:42:37,840
Oh, you are.

642
00:42:37,840 --> 00:42:38,840
Okay.

643
00:42:38,840 --> 00:42:39,840
So, you have, for example, to use it in that way.

644
00:42:39,840 --> 00:42:40,840
Okay.

645
00:42:40,840 --> 00:42:41,840
And we're doing lots of installs like that.

646
00:42:41,840 --> 00:42:42,840
Yeah.

647
00:42:42,840 --> 00:42:43,840
Cool.

648
00:42:43,840 --> 00:42:48,080
Uh, well, Lucas, thanks so much for taking the time to chat very, uh, very fun conversation.

649
00:42:48,080 --> 00:42:49,880
Uh, always learn a ton.

650
00:42:49,880 --> 00:42:50,880
Likewise.

651
00:42:50,880 --> 00:42:58,640
All right, everyone, that's our show for today.

652
00:42:58,640 --> 00:43:05,560
For more information on Lucas or for today's show notes, visit twomalai.com slash shows.

653
00:43:05,560 --> 00:43:10,000
Thanks again to weights and biases for their sponsorship of today's show and twomalcon

654
00:43:10,000 --> 00:43:11,480
AI platforms.

655
00:43:11,480 --> 00:43:17,360
Visit WNB.com to learn more for 20% off of your twomalcon registration.

656
00:43:17,360 --> 00:43:23,040
Make sure to use the code WNB20, W-A-N-D-B20.

657
00:43:23,040 --> 00:43:49,800
As always, thanks so much for listening and catch you next time.

