1
00:00:00,000 --> 00:00:16,320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

2
00:00:16,320 --> 00:00:21,480
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:21,480 --> 00:00:35,360
I'm your host Sam Charrington. Hey everyone, Sam again with another quick Twimble

4
00:00:35,360 --> 00:00:41,440
Con update. One of the things that's been especially exciting to see is the number of organizations

5
00:00:41,440 --> 00:00:47,320
sending multiple people in some cases entire teams to Twimble Con to learn about scaling

6
00:00:47,320 --> 00:00:52,280
and operationalizing machine learning. A full third of the companies attending are sending

7
00:00:52,280 --> 00:00:59,400
groups in many cases three four and five people. This is awesome. Seeing so many teams attending

8
00:00:59,400 --> 00:01:03,680
is a great indicator that folks really see the opportunity associated with improving the

9
00:01:03,680 --> 00:01:08,040
efficiency of their data science and machine learning operations and are excited about

10
00:01:08,040 --> 00:01:12,520
the conversations we'll be curating at the event. If you'd like to attend Twimble

11
00:01:12,520 --> 00:01:18,880
Con with your team just reach out to me at Sam at Twimlai.com and let's make it happen.

12
00:01:18,880 --> 00:01:22,520
Of course you're welcome to reach out to me if you want to attend as an individual or

13
00:01:22,520 --> 00:01:31,120
just head over to twimblecon.com slash register to sign up. All right on to today's show.

14
00:01:31,120 --> 00:01:36,800
All right everyone I am on the line with Anubav Jane. Anubav is a staff scientist and

15
00:01:36,800 --> 00:01:42,200
chemist at Lawrence Berkeley National Lab as well as the group lead for the hacking materials

16
00:01:42,200 --> 00:01:45,880
research group. Anubav welcome to this week in machine learning and AI.

17
00:01:45,880 --> 00:01:49,840
Yeah it's good to be here. I've actually learned a lot by listening to your podcast so

18
00:01:49,840 --> 00:01:54,280
it's really exciting to be able to share my work with with your listeners as well.

19
00:01:54,280 --> 00:01:59,680
That is fantastic to hear. So the group you lead is called the hacking materials research

20
00:01:59,680 --> 00:02:05,600
group. That is such a compelling name. Tell us a little bit more about your kind of focus

21
00:02:05,600 --> 00:02:08,000
and charter and what you're trying to accomplish.

22
00:02:08,000 --> 00:02:13,520
Yeah so what we're really trying to do with the research group is to imply computing to

23
00:02:13,520 --> 00:02:18,440
accelerate the process of finding new materials for functional applications. So new chemical

24
00:02:18,440 --> 00:02:23,960
compositions or new crystal structures that have really interesting properties. And these

25
00:02:23,960 --> 00:02:29,720
days it's actually possible to use computer simulations and also data mining to really

26
00:02:29,720 --> 00:02:35,640
change the way that that process is being done. So my group works both on theoretical

27
00:02:35,640 --> 00:02:40,440
methods to predict materials properties. And it also works on data mining and machine

28
00:02:40,440 --> 00:02:43,680
learning techniques to help accelerate materials design.

29
00:02:43,680 --> 00:02:50,440
Awesome and so what led you to combining computing and machine learning and materials.

30
00:02:50,440 --> 00:02:51,440
What was your path?

31
00:02:51,440 --> 00:02:56,240
Yeah so I got into material science because I really liked kind of its central premise

32
00:02:56,240 --> 00:03:00,880
which is that you know everything whether it's an airplane or a computer processor or

33
00:03:00,880 --> 00:03:06,440
a battery. It's all made of materials and the properties of those materials really determine

34
00:03:06,440 --> 00:03:10,640
the fundamental limits on the performance of every device. And you know many people have

35
00:03:10,640 --> 00:03:15,120
pointed out that you know whenever there's the fundamental materials advance we often

36
00:03:15,120 --> 00:03:20,360
name like an entire historical era after it you know the stone age the iron age the age

37
00:03:20,360 --> 00:03:24,840
of plastics or the silicon age. And so you know I've really been interested in you know

38
00:03:24,840 --> 00:03:25,840
how can we actually.

39
00:03:25,840 --> 00:03:29,040
I hadn't heard that before that sounds amazing.

40
00:03:29,040 --> 00:03:34,520
Yeah I mean really underscores the importance of materials and the opportunity is available

41
00:03:34,520 --> 00:03:39,200
now that we're kind of applying machine learning and data science to the field.

42
00:03:39,200 --> 00:03:43,280
Yeah and there's also sort of new applications like quantum computing you might have heard

43
00:03:43,280 --> 00:03:48,040
of or you know new biological technologies that really require materials advancements as

44
00:03:48,040 --> 00:03:49,040
well.

45
00:03:49,040 --> 00:03:54,480
But now you know typically material science has been very data poor because the only way

46
00:03:54,480 --> 00:03:58,840
conventionally to get information about materials was to perform an experiment. That means

47
00:03:58,840 --> 00:04:02,880
you have to synthesize the material put in the machine and it might be like $10,000

48
00:04:02,880 --> 00:04:05,200
per data point that you're collecting.

49
00:04:05,200 --> 00:04:09,800
And so the way that I got into this field is that during my PhD I worked with someone

50
00:04:09,800 --> 00:04:13,480
named Garrett Saider who was at MIT at the time he's now at Lawrence Berkeley Lab like I

51
00:04:13,480 --> 00:04:14,480
am.

52
00:04:14,480 --> 00:04:18,520
But what we were doing was we were using a particular type of computer simulation that's

53
00:04:18,520 --> 00:04:23,280
rooted in quantum mechanics. And then we were using simulations that actually predict

54
00:04:23,280 --> 00:04:27,480
the properties of materials do kind of a virtual measurement in the computer. And we

55
00:04:27,480 --> 00:04:32,320
could do this for maybe like one to $10 and computing costs per material. We could scale

56
00:04:32,320 --> 00:04:37,200
that over computing cluster so it was easy to parallelize. And then it was easy to create

57
00:04:37,200 --> 00:04:41,200
these databases and materials properties based on simulation data.

58
00:04:41,200 --> 00:04:45,600
So now we had a way to actually generate large materials data sets and maybe even two

59
00:04:45,600 --> 00:04:49,920
large materials data sets where you know conventional analyses were difficult to actually

60
00:04:49,920 --> 00:04:52,520
figure out what were the trends in that data.

61
00:04:52,520 --> 00:04:55,480
So I got into data science and machine learning.

62
00:04:55,480 --> 00:04:59,560
So we're generating some of these large databases of simulated materials properties as a way

63
00:04:59,560 --> 00:05:03,440
to kind of figure out what more information we could extract from some of that materials

64
00:05:03,440 --> 00:05:04,440
data.

65
00:05:04,440 --> 00:05:11,320
That's an interesting trend that I talked to folks about kind of across different domains.

66
00:05:11,320 --> 00:05:16,120
The integration of the use of simulated data and machine learning. I think one of the

67
00:05:16,120 --> 00:05:23,760
most recent conversations on this topic was in the area of astronomy if I'm remembering

68
00:05:23,760 --> 00:05:28,960
correctly, talk to me a little bit more about how you are combining that simulated data

69
00:05:28,960 --> 00:05:30,960
and machine learning.

70
00:05:30,960 --> 00:05:36,360
Yeah, so typically the way that it's done now isn't a kind of a tiered screening process

71
00:05:36,360 --> 00:05:37,360
for new materials.

72
00:05:37,360 --> 00:05:42,840
So when you have an idea about a new materials or a new chemical space that might be used

73
00:05:42,840 --> 00:05:47,000
for functional application, you might first do a machine learning prediction and that machine

74
00:05:47,000 --> 00:05:51,840
learning might be trained on simulated data because there usually isn't enough experimental

75
00:05:51,840 --> 00:05:55,800
data to train on. So you first do like a machine learning prediction to see what might

76
00:05:55,800 --> 00:06:00,600
be the interesting materials candidates within this broad chemical space.

77
00:06:00,600 --> 00:06:04,240
Then you might run some simulations on the things that are interesting for machine learning.

78
00:06:04,240 --> 00:06:09,520
The simulations are a little bit more expensive and complicated and time consuming to do.

79
00:06:09,520 --> 00:06:15,040
And then finally, if it passes both of those tests, you can actually do experiments.

80
00:06:15,040 --> 00:06:18,360
You might want to actually target doing some real experiments on that material.

81
00:06:18,360 --> 00:06:24,040
So that's one paradigm in which the techniques are working together.

82
00:06:24,040 --> 00:06:29,960
There's also kind of active learning and adaptive design type frameworks where you do this whole

83
00:06:29,960 --> 00:06:31,360
thing in a loop.

84
00:06:31,360 --> 00:06:35,920
So you do iterative machine learning and based on results of the experiments, you retrain

85
00:06:35,920 --> 00:06:40,760
your machine learning model, run the computations and do this whole thing in a loop as well.

86
00:06:40,760 --> 00:06:45,160
So there's a few different kind of paradigms in which you can mix these different techniques.

87
00:06:45,160 --> 00:06:50,480
Do you apply different types of domain adaptation techniques when you're trying to use the simulated

88
00:06:50,480 --> 00:06:52,600
data in the materials domain?

89
00:06:52,600 --> 00:06:57,360
Is that a technique that's effective in the kind of for-cure doing?

90
00:06:57,360 --> 00:07:02,320
Yeah, so I'm actually unfamiliar with the term domain adaptation technique.

91
00:07:02,320 --> 00:07:03,320
What does that refer to?

92
00:07:03,320 --> 00:07:08,440
I guess, you know, similar in some ways to data augmentation.

93
00:07:08,440 --> 00:07:15,000
It's really take, for example, in autonomous vehicles, you know, one of the things

94
00:07:15,000 --> 00:07:19,840
that you hear people doing is training on, like, you know, different types of simulations

95
00:07:19,840 --> 00:07:25,440
like video games, but the models that are trained in these simulated environments don't

96
00:07:25,440 --> 00:07:32,040
generalize well to the real world, so they'll do different types of things to adapt the

97
00:07:32,040 --> 00:07:38,640
or modify the simulation to help the model generalize or perform.

98
00:07:38,640 --> 00:07:44,720
So, for example, they might, you know, take, you know, daylight scenes and turn them into

99
00:07:44,720 --> 00:07:48,560
nighttime scenes or manipulate them so they seem more realistic.

100
00:07:48,560 --> 00:07:53,960
I'm wondering if there are any kinds of tools or techniques that you apply to make models

101
00:07:53,960 --> 00:07:59,920
trained on simulated data rather more applicable to the real world interactions?

102
00:07:59,920 --> 00:08:07,080
Yeah, so I would say there's two parallel threads, none of which is maybe exactly what you're

103
00:08:07,080 --> 00:08:09,160
mentioning here.

104
00:08:09,160 --> 00:08:12,760
One of them is just kind of rooted in physics and trying to make the models as realistic

105
00:08:12,760 --> 00:08:16,000
as possible by improving the underlying physics of those models.

106
00:08:16,000 --> 00:08:20,080
So, try to just get them to be closer and closer to reality.

107
00:08:20,080 --> 00:08:24,680
The second kind of approach that's taken that's somewhat similar is kind of a multi-fidelity

108
00:08:24,680 --> 00:08:29,920
approach where you have some experimental data, you have a whole bunch of computational

109
00:08:29,920 --> 00:08:36,000
data, and then you maybe use some of the computational predictions as features to help train

110
00:08:36,000 --> 00:08:40,120
your experimental model, your model training on experimental data.

111
00:08:40,120 --> 00:08:45,240
So, I think those sorts of approaches exist, but I haven't seen any sort of, you know, systematic

112
00:08:45,240 --> 00:08:50,560
transformation between computational data and kind of real world results other than those

113
00:08:50,560 --> 00:08:51,560
sorts of examples.

114
00:08:51,560 --> 00:08:53,040
Oh, really interesting stuff.

115
00:08:53,040 --> 00:08:55,960
Now, I probably should have mentioned this earlier.

116
00:08:55,960 --> 00:09:02,120
We're primarily going to be focused not necessarily on some of the materials applications of machine

117
00:09:02,120 --> 00:09:09,520
learning that you're working on, but rather some work that you recently published on applying

118
00:09:09,520 --> 00:09:15,280
natural language processing to capturing information from academic literature.

119
00:09:15,280 --> 00:09:21,280
So, maybe if you can talk a little bit about that work and its motivations and the connection

120
00:09:21,280 --> 00:09:25,680
between it and what you're doing on the material side.

121
00:09:25,680 --> 00:09:33,240
Yeah, so, you know, there are many, many millions, probably 50 to 100 million research articles,

122
00:09:33,240 --> 00:09:37,120
scientific research articles that have been published, and there's probably been trillions

123
00:09:37,120 --> 00:09:41,640
of dollars of investment in actually funding the research studies that typically the only

124
00:09:41,640 --> 00:09:45,400
output of those research studies is a published research article.

125
00:09:45,400 --> 00:09:50,640
So there is a ton of knowledge and a ton of opportunity in terms of extracting information

126
00:09:50,640 --> 00:09:53,640
from published research articles.

127
00:09:53,640 --> 00:09:58,040
And today, you know, it's kind of crazy, but the main way to actually get information

128
00:09:58,040 --> 00:10:02,000
from research articles is to have, you know, researchers read them.

129
00:10:02,000 --> 00:10:04,080
And that process is really, you know, limited.

130
00:10:04,080 --> 00:10:08,720
I mean, as a researcher, I probably read a few dozen articles, like really read a few

131
00:10:08,720 --> 00:10:13,640
dozen articles per year, maybe for some people, it's a few hundred, but there are, you know,

132
00:10:13,640 --> 00:10:19,040
thousands of articles published on any domain in any given year.

133
00:10:19,040 --> 00:10:23,880
So what we were trying to do in our study was to design some kind of a system that would

134
00:10:23,880 --> 00:10:30,000
be able to, let's say, read these articles in some sense and synthesize information about

135
00:10:30,000 --> 00:10:36,520
what those articles were saying, and be able to conceptualize, you know, what was happening

136
00:10:36,520 --> 00:10:40,360
in material science by reading these articles.

137
00:10:40,360 --> 00:10:45,560
And not only be able to have these internal conceptions of what the articles were talking

138
00:10:45,560 --> 00:10:52,560
about or to make predictions that could be testable and help guide researchers in their studies.

139
00:10:52,560 --> 00:10:57,640
And so in this paper, what we did was we actually collected the abstracts of over three million

140
00:10:57,640 --> 00:11:01,160
articles restricted to material science.

141
00:11:01,160 --> 00:11:04,240
We did some data pre-processing on those articles.

142
00:11:04,240 --> 00:11:08,600
And then we actually trained an unsupervised algorithm called Word2Vec.

143
00:11:08,600 --> 00:11:11,240
So this algorithm didn't receive any label data.

144
00:11:11,240 --> 00:11:13,680
It didn't receive any chemical training.

145
00:11:13,680 --> 00:11:19,840
But we found that by applying this algorithm on just the data set of material science articles,

146
00:11:19,840 --> 00:11:24,960
it was able to conceptualize concepts like the periodic table.

147
00:11:24,960 --> 00:11:30,720
And it could also predict what functional, what materials should be studied for functional

148
00:11:30,720 --> 00:11:32,200
applications.

149
00:11:32,200 --> 00:11:37,800
We found that through this procedure, we were able to predict, you know, materials that

150
00:11:37,800 --> 00:11:41,880
should be studied for various applications many years before they were actually reported

151
00:11:41,880 --> 00:11:43,240
in the literature.

152
00:11:43,240 --> 00:11:47,560
So we did this process where we actually kind of virtually went back in time, made some

153
00:11:47,560 --> 00:11:51,040
predictions about what material scientists should be studying, what materials they should

154
00:11:51,040 --> 00:11:53,120
be studying for different applications.

155
00:11:53,120 --> 00:11:57,600
And then we saw, you know, in reality, that they actually study those materials that we

156
00:11:57,600 --> 00:12:00,600
predicted over the next few years.

157
00:12:00,600 --> 00:12:04,320
And we found that, you know, we could actually predict at a very high rate what researchers

158
00:12:04,320 --> 00:12:05,840
would be studying in the future.

159
00:12:05,840 --> 00:12:14,400
Well, if we can pause here and maybe unpack some of that, I think probably many listeners

160
00:12:14,400 --> 00:12:21,400
are generally familiar with Word2Vec and the concept of Word embeddings.

161
00:12:21,400 --> 00:12:26,320
We've covered it quite a bit on the podcast.

162
00:12:26,320 --> 00:12:31,360
But you say that, you know, in creating an embedding model, you're able to, the, or the

163
00:12:31,360 --> 00:12:35,960
model is able to conceptualize the, the periodic table.

164
00:12:35,960 --> 00:12:39,520
What does that mean for a model to be able to conceptualize the periodic table or what

165
00:12:39,520 --> 00:12:41,440
are you trying to express with that?

166
00:12:41,440 --> 00:12:42,440
Yeah.

167
00:12:42,440 --> 00:12:46,360
So, you know, as you mentioned, there's been a lot of research into Word2Vec and people have

168
00:12:46,360 --> 00:12:51,160
already demonstrated many times that it can, you know, understand grammatical relationships.

169
00:12:51,160 --> 00:12:56,720
It can also understand semantic relationships like the concept of gender or capitals, etc.

170
00:12:56,720 --> 00:13:00,720
What we're trying to show with this study is that Word2Vec also captures scientific

171
00:13:00,720 --> 00:13:02,520
relationships.

172
00:13:02,520 --> 00:13:06,280
So it actually captures knowledge that requires some amount of scientific knowledge in order

173
00:13:06,280 --> 00:13:10,680
to, in order to, to express that relationship.

174
00:13:10,680 --> 00:13:15,960
So for the periodic table, specifically, what we did is we, we trained these Word embeddings

175
00:13:15,960 --> 00:13:18,800
and then we looked at Word embeddings of the chemical elements.

176
00:13:18,800 --> 00:13:23,600
So words like helium, words like sodium, words like lithium.

177
00:13:23,600 --> 00:13:27,360
And so these are 200 dimensional vectors that represent these words.

178
00:13:27,360 --> 00:13:31,480
And then we projected those 200 dimensional embeddings into two dimensions, like the

179
00:13:31,480 --> 00:13:33,120
periodic table.

180
00:13:33,120 --> 00:13:38,800
We used T-SNE as the embedding mechanism, sorry, as the projection mechanism.

181
00:13:38,800 --> 00:13:42,080
And then we found that when we actually projected these Word embeddings, which were trained

182
00:13:42,080 --> 00:13:46,520
just on the way that chemists and material scientists mentioned these elements in the

183
00:13:46,520 --> 00:13:51,320
course of their research, we actually got the same sorts of trends that you saw on the

184
00:13:51,320 --> 00:13:52,800
periodic table.

185
00:13:52,800 --> 00:13:57,800
So without really being explicitly trained on what the structure of the periodic table

186
00:13:57,800 --> 00:14:02,440
was, we could recover that structure simply by reading these articles and, you know,

187
00:14:02,440 --> 00:14:06,400
projecting these embeddings down to two dimensions.

188
00:14:06,400 --> 00:14:12,960
And just to interrupt again, when you say recover that structure, meaning if you somehow

189
00:14:12,960 --> 00:14:17,560
visualize this two dimensional embedding space and squint the right way, you can kind

190
00:14:17,560 --> 00:14:23,600
of see the periodic table in there, or you can systematically, you know, further transform

191
00:14:23,600 --> 00:14:28,320
this projection and get to something very close to the periodic table.

192
00:14:28,320 --> 00:14:32,760
Yeah, so we did kind of two types of tests.

193
00:14:32,760 --> 00:14:36,920
The first one is more similar to what you described at first, which is that when you project

194
00:14:36,920 --> 00:14:41,480
the elements in two dimensions, you kind of see these clusters that correspond to the

195
00:14:41,480 --> 00:14:43,520
types of groups in the periodic table.

196
00:14:43,520 --> 00:14:46,440
So things like the alkali metals are all grouped together.

197
00:14:46,440 --> 00:14:49,160
The halogens are all grouped together, et cetera.

198
00:14:49,160 --> 00:14:53,680
And there's also structure within the clusters that you can see that correspond to moving

199
00:14:53,680 --> 00:14:56,680
to certain directions in the periodic table.

200
00:14:56,680 --> 00:15:01,120
But now, you know, you're, I think you've talked on this show before about how T-SNE can

201
00:15:01,120 --> 00:15:04,080
distort things and distort distances, et cetera.

202
00:15:04,080 --> 00:15:08,400
And so in addition to kind of just that T-SNE visualization and qualitatively looking

203
00:15:08,400 --> 00:15:13,880
at it, we did another thing, which is we actually first took these two hundred dimensional

204
00:15:13,880 --> 00:15:19,040
word embeddings and projected them down to 15 dimensions using just PCA.

205
00:15:19,040 --> 00:15:24,280
And then we saw whether certain directions in that 15 dimensional embedding space corresponded

206
00:15:24,280 --> 00:15:26,920
to various directions in the periodic table.

207
00:15:26,920 --> 00:15:31,440
So for example, we saw whether there were certain directions in this 15 dimensional space

208
00:15:31,440 --> 00:15:36,800
that corresponded to increasing the atomic weight in the periodic table or increasing the

209
00:15:36,800 --> 00:15:39,840
electro negativity in the periodic table.

210
00:15:39,840 --> 00:15:44,200
So what we were really doing is to see are there certain directions in this embedding space

211
00:15:44,200 --> 00:15:48,640
that correspond to moving in certain directions in the periodic table?

212
00:15:48,640 --> 00:15:53,040
And we found high correspondence in that as well, things like our squared values of about

213
00:15:53,040 --> 00:15:54,360
point eight or so.

214
00:15:54,360 --> 00:15:58,800
So we also did a more quantitative test of whether the information in the periodic table

215
00:15:58,800 --> 00:16:00,320
was being captured by these embeddings.

216
00:16:00,320 --> 00:16:02,320
Oh, really interesting.

217
00:16:02,320 --> 00:16:10,600
Now, can you take a moment to kind of review or overview T-SNE and PCA for those that aren't

218
00:16:10,600 --> 00:16:15,400
familiar with those terms and kind of how they play out in these experiments?

219
00:16:15,400 --> 00:16:21,440
Yeah, so T-SNE is 10s for T-Stochastic Neighbor embedding.

220
00:16:21,440 --> 00:16:25,920
It's just a way to take a high dimensional space like the 200 dimensional word embeddings

221
00:16:25,920 --> 00:16:29,880
we've been talking about and projected them into a lower dimension in a way that tries

222
00:16:29,880 --> 00:16:35,240
to preserve distances as closely as possible.

223
00:16:35,240 --> 00:16:38,640
And PCA stands for Principal Component Analysis.

224
00:16:38,640 --> 00:16:44,440
And it's a way to try and figure out kind of new directions, so new dimensions along which

225
00:16:44,440 --> 00:16:49,840
to project our 200 dimensional vectors that try to capture as much information as possible

226
00:16:49,840 --> 00:16:53,040
while reducing the number of dimensions down.

227
00:16:53,040 --> 00:16:57,920
So in this case, in order to do, in order to test whether we were quantitatively reproducing

228
00:16:57,920 --> 00:17:02,520
the periodic table, because there's only 100 elements and there's 200 dimensions in our

229
00:17:02,520 --> 00:17:06,560
word embeddings to do the test fairly to see whether we were capturing these directions

230
00:17:06,560 --> 00:17:11,760
properly, we actually reduce it dimensionally down to 15 dimensions to make a fair test.

231
00:17:11,760 --> 00:17:12,760
Awesome.

232
00:17:12,760 --> 00:17:13,760
Awesome.

233
00:17:13,760 --> 00:17:20,720
And so you created this embedding space, you kind of compared it to your intuition and

234
00:17:20,720 --> 00:17:26,400
knowledge of chemistry and physics and found that there is some structure there that

235
00:17:26,400 --> 00:17:31,640
resembles what we know about the way the world works, i.e. the periodic table.

236
00:17:31,640 --> 00:17:41,360
And then with this as a basis, you're using it to try to predict materials that will be

237
00:17:41,360 --> 00:17:44,840
useful in further research and literature.

238
00:17:44,840 --> 00:17:49,440
How do you make that jump from the embedding space to these kinds of predictions?

239
00:17:49,440 --> 00:17:50,440
Yeah.

240
00:17:50,440 --> 00:17:56,280
So when you train these embeddings, you typically give it some kind of a task in which

241
00:17:56,280 --> 00:17:58,080
you train the embeddings on.

242
00:17:58,080 --> 00:18:03,320
So in the case of word to veck, the task that you train the embeddings on, there's multiple

243
00:18:03,320 --> 00:18:06,200
versions, but we use something called skipgram.

244
00:18:06,200 --> 00:18:10,080
And the task there is that given a certain word, that word could be a chemical element

245
00:18:10,080 --> 00:18:14,720
like helium, it could be an application word like thermoelectric, or it could be just any

246
00:18:14,720 --> 00:18:16,120
old word.

247
00:18:16,120 --> 00:18:21,560
Given a word as an input, the skipgram model tries to predict what words will appear next

248
00:18:21,560 --> 00:18:24,440
to that word in your document corpus.

249
00:18:24,440 --> 00:18:29,400
We use a window size of eight, so plus or minus eight words from our target word.

250
00:18:29,400 --> 00:18:33,800
So what we're training these embeddings on, the information that they represent is what

251
00:18:33,800 --> 00:18:34,800
are the neighbors?

252
00:18:34,800 --> 00:18:40,880
What are the types of neighboring words that occur next to each target word?

253
00:18:40,880 --> 00:18:46,240
So now we have basically for every word in our corpus, an embedding that represents what

254
00:18:46,240 --> 00:18:51,240
sorts of words are expected to appear next to that word in a scientific abstract, in

255
00:18:51,240 --> 00:18:53,440
a material science abstract.

256
00:18:53,440 --> 00:18:58,520
Now the way that we make this predictive is, a lot of times when people train word to

257
00:18:58,520 --> 00:19:02,320
veck or similar embedding algorithms, once they get the embeddings, they throw away

258
00:19:02,320 --> 00:19:03,320
the task.

259
00:19:03,320 --> 00:19:05,040
The task is not really important anymore.

260
00:19:05,040 --> 00:19:07,920
You have the embeddings and then you work with those.

261
00:19:07,920 --> 00:19:12,360
What really the lead author of this paper did, his name is Vahey Chattoyan, he decided

262
00:19:12,360 --> 00:19:16,400
not to throw away the task and he decided to use that task directly.

263
00:19:16,400 --> 00:19:20,880
And what he said is, well listen, I want a thermoelectric material, I have a model that

264
00:19:20,880 --> 00:19:25,040
can predict what sorts of words are likely to appear in a scientific abstract next to

265
00:19:25,040 --> 00:19:28,560
the word thermoelectric, that's directly my word to veck model.

266
00:19:28,560 --> 00:19:34,240
So let me use my word to veck model to actually directly predict, in some sense, what words

267
00:19:34,240 --> 00:19:36,920
will appear next to the word thermoelectric.

268
00:19:36,920 --> 00:19:40,000
And then he filtered those down to just chemical composition words.

269
00:19:40,000 --> 00:19:44,880
So which chemical compositions will likely appear next to the word thermoelectric?

270
00:19:44,880 --> 00:19:48,840
And most of those chemical compositions predicted by the model are things that were already

271
00:19:48,840 --> 00:19:54,080
known to be thermoelectric, so things that occurred in our corpus many, many times.

272
00:19:54,080 --> 00:19:58,960
But then when Vahey got down, so let's say the 333rd prediction, he found something new.

273
00:19:58,960 --> 00:20:02,880
He said, okay, here's a chemical composition that was never explicitly mentioned in our

274
00:20:02,880 --> 00:20:10,760
corpus as a thermoelectric, yet it's ranking pretty high in the prediction of likely to

275
00:20:10,760 --> 00:20:12,960
appear next to the word thermoelectric.

276
00:20:12,960 --> 00:20:17,120
So maybe the model is telling us something, it's telling us that there's a high likelihood

277
00:20:17,120 --> 00:20:20,960
that someone's going to publish a research article with this chemical composition next

278
00:20:20,960 --> 00:20:25,320
to the word thermoelectric, even though no one has ever published that article before.

279
00:20:25,320 --> 00:20:31,440
So he essentially turned the word to veck into a way to identify gaps in the research literature

280
00:20:31,440 --> 00:20:34,640
by using these embeddings.

281
00:20:34,640 --> 00:20:40,280
How do you validate your interpretation of a finding like that?

282
00:20:40,280 --> 00:20:51,040
You know, imagining that that 331rd could be a chemical compound that has another name

283
00:20:51,040 --> 00:20:58,840
or another kind of composition and has appeared, but under this other name or maybe it's just

284
00:20:58,840 --> 00:20:59,840
random.

285
00:20:59,840 --> 00:21:07,000
Like, is there, how do you validate that this is actually a function of the model that

286
00:21:07,000 --> 00:21:10,680
you created and not just noise?

287
00:21:10,680 --> 00:21:11,680
Sure.

288
00:21:11,680 --> 00:21:15,920
Yeah, so I should first say that we're pretty confident that the materials that we predict

289
00:21:15,920 --> 00:21:20,640
as new predictions are actually things that haven't been studied before.

290
00:21:20,640 --> 00:21:24,440
Because first of all, we can check our abstract corpus to make sure that that compound has

291
00:21:24,440 --> 00:21:28,920
not appeared next to the word thermoelectric or any other word that would indicate studying

292
00:21:28,920 --> 00:21:31,240
that application before.

293
00:21:31,240 --> 00:21:36,200
So part of it is that is using our corpus to figure out that this has been studied before

294
00:21:36,200 --> 00:21:40,440
and then we also did a lot of manual checking whenever we published a prediction on the paper

295
00:21:40,440 --> 00:21:45,240
just to make sure that this chemical composition that we're predicting is not actually something

296
00:21:45,240 --> 00:21:46,640
that has been studied.

297
00:21:46,640 --> 00:21:50,720
So I think we're pretty confident that it's actually a new prediction, but then whether

298
00:21:50,720 --> 00:21:55,160
it's correct or not is I think a more complicated question.

299
00:21:55,160 --> 00:21:57,840
The best validation would be to actually do the experiments.

300
00:21:57,840 --> 00:22:02,040
So to make the material to characterize it and then measure the thermoelectric properties,

301
00:22:02,040 --> 00:22:03,040
let's say.

302
00:22:03,040 --> 00:22:04,040
Sure.

303
00:22:04,040 --> 00:22:05,040
Yeah, that sounds expensive.

304
00:22:05,040 --> 00:22:06,040
Yeah.

305
00:22:06,040 --> 00:22:10,280
So we're working with collaborators right now on a couple of those top 50 materials that

306
00:22:10,280 --> 00:22:15,400
we predicted in our paper, but it will probably be like six months or so before we actually

307
00:22:15,400 --> 00:22:20,120
find out what the results are just because they have to first synthesize the material,

308
00:22:20,120 --> 00:22:24,480
they have to purify it, they have to characterize it and they have to optimize and dope it and

309
00:22:24,480 --> 00:22:25,480
all that stuff.

310
00:22:25,480 --> 00:22:29,240
So it's going to take a few months to figure out whether the ones that we picked out of

311
00:22:29,240 --> 00:22:31,120
the 50 are actually interesting or not.

312
00:22:31,120 --> 00:22:37,840
I mean, I imagine it's even interesting if the model can come up with plausible candidates,

313
00:22:37,840 --> 00:22:39,880
even if they don't ultimately work.

314
00:22:39,880 --> 00:22:43,320
I mean, that's what scientists do.

315
00:22:43,320 --> 00:22:54,280
I guess I'm trying to get at a more kind of technical validation that it's not just

316
00:22:54,280 --> 00:23:05,960
kind of random, you're finding these plausible predictions more consistently than just any

317
00:23:05,960 --> 00:23:12,840
combinations of molecules that appear that can be made up from the embedding space.

318
00:23:12,840 --> 00:23:14,480
Does that question make sense?

319
00:23:14,480 --> 00:23:15,480
Yeah, yeah.

320
00:23:15,480 --> 00:23:16,480
So, absolutely.

321
00:23:16,480 --> 00:23:20,760
And so in the absence of experiments, we actually did three types of alternate validation.

322
00:23:20,760 --> 00:23:26,880
So I think experiments would be the best, you know, you can't argue with it type validation.

323
00:23:26,880 --> 00:23:30,600
But then in the absence of experiments, we did three other types of validation.

324
00:23:30,600 --> 00:23:35,120
The first type of validation we did is that we had an independent data set of simulated

325
00:23:35,120 --> 00:23:36,800
data on thermal electrics.

326
00:23:36,800 --> 00:23:40,360
So these are based on, you know, physics-based simulations.

327
00:23:40,360 --> 00:23:45,280
So the two data sets between NLP and the simulations, they don't talk to each other.

328
00:23:45,280 --> 00:23:49,320
And then what we did was we looked at correspondence between the ranking of thermal electrics that

329
00:23:49,320 --> 00:23:52,480
we did using natural language processing technique.

330
00:23:52,480 --> 00:23:57,760
And the thermal electric properties as calculated by the simulated data.

331
00:23:57,760 --> 00:24:02,720
And what we found is like pretty remarkable, which is that if you look at, for example,

332
00:24:02,720 --> 00:24:09,720
the top 10 materials in the NLP rankings, there are a lot of them are like the 99th percentile

333
00:24:09,720 --> 00:24:15,280
99.5th percentile, et cetera, of the actual simulated data properties.

334
00:24:15,280 --> 00:24:20,400
So there's a high correspondence between the word-to-vec rankings, the NLP rankings,

335
00:24:20,400 --> 00:24:22,000
and the simulated data.

336
00:24:22,000 --> 00:24:26,560
And that's not only on known materials where you would expect that the word-to-vec would

337
00:24:26,560 --> 00:24:30,320
pick up that gets a known thermal electrics, so I'm going to rank it highly, but also

338
00:24:30,320 --> 00:24:31,560
on unknown materials.

339
00:24:31,560 --> 00:24:37,800
So we have simulated data on the predicted data and the things that we predict to be high

340
00:24:37,800 --> 00:24:42,200
with NLP are also high in the simulations for predictions.

341
00:24:42,200 --> 00:24:49,240
So that's kind of one validation is just consistency between simulations and the NLP.

342
00:24:49,240 --> 00:24:55,160
The second type of test that we did was against experimental data.

343
00:24:55,160 --> 00:25:00,440
And here what we tried to do is to say, because experimental data would be all on known thermal

344
00:25:00,440 --> 00:25:06,080
electric materials, we actually tried to make sure that the quality of the ranking also

345
00:25:06,080 --> 00:25:09,920
corresponded to the quality of the experimental results, so the quality of the ranking and

346
00:25:09,920 --> 00:25:11,400
the experimental results.

347
00:25:11,400 --> 00:25:15,960
So things that had a higher word-to-vec ranking would also have a higher experimental-based

348
00:25:15,960 --> 00:25:17,040
ranking.

349
00:25:17,040 --> 00:25:22,080
And so that second validation also panned out where there was kind of a rank correlation

350
00:25:22,080 --> 00:25:26,480
between our NLP results and between the actual experimental data.

351
00:25:26,480 --> 00:25:31,800
And then finally the third type of validation that we did was basically a forecasting-based

352
00:25:31,800 --> 00:25:33,440
cross-validation.

353
00:25:33,440 --> 00:25:38,920
And what we did is we created these virtual data sets where we asked the question, well,

354
00:25:38,920 --> 00:25:42,920
if we had invented this technique back in the year 2000.

355
00:25:42,920 --> 00:25:47,560
So we threw out all the data of abstracts from past the year 2000, and we trained the

356
00:25:47,560 --> 00:25:53,320
model only in the year 2000, for example, on data up to the year 2000.

357
00:25:53,320 --> 00:25:57,200
And then we made predictions at a particular point in time.

358
00:25:57,200 --> 00:26:01,240
And then we saw what actually happened in the research community in the next 19 years

359
00:26:01,240 --> 00:26:04,000
from 2001 to 2019.

360
00:26:04,000 --> 00:26:08,720
We did those predictions that we made using the 2000 data set actually pat out over the

361
00:26:08,720 --> 00:26:10,320
next 19 years.

362
00:26:10,320 --> 00:26:15,920
And we repeated this process for every year from the year 2000 to 2018 or so.

363
00:26:15,920 --> 00:26:20,320
Where every year we kind of virtually predicted what the algorithm would have predicted.

364
00:26:20,320 --> 00:26:24,080
And then compared that to what was actually observed in the research literature.

365
00:26:24,080 --> 00:26:28,880
And we found that the rate at which the materials that were predicted by the algorithm were

366
00:26:28,880 --> 00:26:32,840
actually reported in literature was much, much higher than if you had just picked random

367
00:26:32,840 --> 00:26:35,840
materials from the data set at the time.

368
00:26:35,840 --> 00:26:39,800
And even more than if you had used some kind of chemical heuristics and simulation-based

369
00:26:39,800 --> 00:26:42,920
heuristics to pick the materials from at the time.

370
00:26:42,920 --> 00:26:48,000
So something like, you know, within the first five years, you're like eight times more

371
00:26:48,000 --> 00:26:51,600
likely to get a thermal electric material using this algorithm versus just picking a

372
00:26:51,600 --> 00:26:53,200
random material.

373
00:26:53,200 --> 00:26:55,280
So that was like a third type of validation we did.

374
00:26:55,280 --> 00:26:59,640
And we found through that type of validation that often you could find new thermal electric

375
00:26:59,640 --> 00:27:04,600
materials or suggest new thermal electric materials five to ten years before they were first

376
00:27:04,600 --> 00:27:05,920
reported in the literature.

377
00:27:05,920 --> 00:27:08,160
Oh, that's super interesting.

378
00:27:08,160 --> 00:27:17,320
So it sounds like you did find that the algorithm was the predictions that the algorithm

379
00:27:17,320 --> 00:27:23,920
came up with tended to be kind of like low hanging fruit in a sense that they were found,

380
00:27:23,920 --> 00:27:26,840
you know, within the fur- they tended to be found within the first five years as opposed

381
00:27:26,840 --> 00:27:29,560
to, you know, something that I guess I wouldn't expect

382
00:27:29,560 --> 00:27:34,040
us of an algorithm to come up with truly novel compositions, you know, things that to

383
00:27:34,040 --> 00:27:38,480
20 years to figure out is that basically in line with what you saw?

384
00:27:38,480 --> 00:27:42,960
Yeah, well, so when you look at the predictions that the algorithm makes, so if you look

385
00:27:42,960 --> 00:27:47,640
at, for example, the predictions from right now that the algorithm makes, some of them,

386
00:27:47,640 --> 00:27:52,520
a lot of them, I would say, are what I would call adjacent to known thermal electrics.

387
00:27:52,520 --> 00:27:56,240
So these are things that, you know, maybe are fairly likely that someone would find in

388
00:27:56,240 --> 00:28:00,400
the next few years anyway, even if it weren't for the algorithm.

389
00:28:00,400 --> 00:28:05,680
But I will say that, you know, a five-year acceleration is actually a pretty big deal.

390
00:28:05,680 --> 00:28:10,560
The typical timescale for kind of materials development is usually quoted around 20 years

391
00:28:10,560 --> 00:28:13,480
based on an article that was published in the 90s.

392
00:28:13,480 --> 00:28:18,560
And so cutting five years off of that is actually a, would be considered a major milestone,

393
00:28:18,560 --> 00:28:19,560
I would say.

394
00:28:19,560 --> 00:28:20,560
Absolutely.

395
00:28:20,560 --> 00:28:25,800
Yeah, so, you know, so a lot of the predictions are, I would say, you know, chemically

396
00:28:25,800 --> 00:28:29,920
adjacent to some of the known thermal electrics and probably would be found in a few years

397
00:28:29,920 --> 00:28:31,440
regardless.

398
00:28:31,440 --> 00:28:36,960
But then there are also a bunch of predictions in there that look, would look strange.

399
00:28:36,960 --> 00:28:41,960
I would say to someone who is interested in thermoelectric materials.

400
00:28:41,960 --> 00:28:47,040
And those materials, I think, are more of the wild card because, you know, they could

401
00:28:47,040 --> 00:28:50,560
be strange and they could just be incorrect.

402
00:28:50,560 --> 00:28:54,240
But they could also be strange and they could also be correct at the same time, in which

403
00:28:54,240 --> 00:28:59,040
case it would be very interesting because it's finding really unconventional materials

404
00:28:59,040 --> 00:29:03,680
that, you know, may or not have been studied if it weren't for the algorithm even after

405
00:29:03,680 --> 00:29:06,640
10 years or even after 20 years.

406
00:29:06,640 --> 00:29:10,560
So, you know, it will really require someone to really, the only way to really test that

407
00:29:10,560 --> 00:29:15,840
out is to do experimental tests on some of them and see whether they pan out or not.

408
00:29:15,840 --> 00:29:18,680
But yeah, there's a mix of things that, you know, maybe would have been found in the

409
00:29:18,680 --> 00:29:24,480
next five years or so and things that probably wouldn't be studied even after quite some

410
00:29:24,480 --> 00:29:25,480
time.

411
00:29:25,480 --> 00:29:30,640
But I would emphasize that even five years is actually considered to be a very big acceleration.

412
00:29:30,640 --> 00:29:34,440
And is the testing that you're currently doing with your collaborators?

413
00:29:34,440 --> 00:29:40,280
Is that focused on either end of the spectrum or do you have folks looking kind of across

414
00:29:40,280 --> 00:29:46,720
the adjacent materials as well as the ones that are, you know, strange looking?

415
00:29:46,720 --> 00:29:52,000
Yeah, so, unfortunately, we don't have any dedicated funding to pay experimentalists to

416
00:29:52,000 --> 00:29:54,520
just look at whatever materials we'd want them to look at.

417
00:29:54,520 --> 00:29:58,400
If we had that, yeah, if we had that, I think we would try to spread it out a little

418
00:29:58,400 --> 00:30:01,240
bit more and do kind of both.

419
00:30:01,240 --> 00:30:04,480
But because we're kind of working, these experimentalists that are collaborating with us, they're

420
00:30:04,480 --> 00:30:06,960
doing it essentially in their spare time.

421
00:30:06,960 --> 00:30:11,200
And so they typically will work on materials that are similar to things that they're already

422
00:30:11,200 --> 00:30:16,400
interested in, so things that are, you know, more simple to make and things that are, you

423
00:30:16,400 --> 00:30:20,280
know, kind of within their comfort zone of synthesis and characterization.

424
00:30:20,280 --> 00:30:24,480
So I would say that the couple that we're working on are more in the adjacent category

425
00:30:24,480 --> 00:30:27,520
than the wild and crazy category.

426
00:30:27,520 --> 00:30:33,240
And is the predictions that the model is making?

427
00:30:33,240 --> 00:30:38,840
Is it predicting structure in addition to components?

428
00:30:38,840 --> 00:30:46,800
For example, I'm thinking if you gave it a prompt, like liquid or a wet, and it came

429
00:30:46,800 --> 00:30:51,280
up with hydrogen and oxygen, you know, that, you know, it's still missing the, you know,

430
00:30:51,280 --> 00:30:54,640
the two in H2O and that configuration.

431
00:30:54,640 --> 00:31:00,840
Is it giving you that or is it, does it leave that bit to the imagination, so to speak?

432
00:31:00,840 --> 00:31:04,600
Yeah, so it's giving you a full chemical composition.

433
00:31:04,600 --> 00:31:08,440
So it would give you the H2O, but it isn't giving you a structure.

434
00:31:08,440 --> 00:31:13,280
So it isn't telling you whether it's ice or water or water vapor, for example.

435
00:31:13,280 --> 00:31:18,160
And that is actually one of the big limitations of the study, you know, in material science,

436
00:31:18,160 --> 00:31:22,160
one of the main things you study is how the arrangement of atoms, not just the chemical

437
00:31:22,160 --> 00:31:25,560
formula, but the arrangement of atoms affect properties.

438
00:31:25,560 --> 00:31:30,240
And so for example, if you think of just carbon, carbon can exist as diamond, it can exist

439
00:31:30,240 --> 00:31:33,280
as graphite, or it could exist as like a nano tube.

440
00:31:33,280 --> 00:31:37,600
And in our model, all those carbons are the same, same carbon.

441
00:31:37,600 --> 00:31:41,840
And you know, you could imagine that a more advanced embedding technique, so some of the

442
00:31:41,840 --> 00:31:47,480
context-sensitive embeddings, something like BERT, for example, might be able to distinguish

443
00:31:47,480 --> 00:31:52,840
between those carbons and provide different predictions for carbon that are synthesized

444
00:31:52,840 --> 00:31:56,080
or that are, have different types of structures within them.

445
00:31:56,080 --> 00:31:59,680
But within our word-to-back model, where every carbon is the same regardless of whether

446
00:31:59,680 --> 00:32:04,440
it's diamond or graphite, it's really just the chemical formula that's being used to

447
00:32:04,440 --> 00:32:05,840
make the prediction.

448
00:32:05,840 --> 00:32:12,200
How is it coming out with the chemical formula for these different compounds?

449
00:32:12,200 --> 00:32:16,440
Yeah, so that's actually another one of the limitations of the current approach, which

450
00:32:16,440 --> 00:32:21,120
is that the current approach can only rank chemical formulas that are observed in the

451
00:32:21,120 --> 00:32:23,760
abstract corpus for an application.

452
00:32:23,760 --> 00:32:27,720
So it's not inventing new chemical formulas, it's only taking the list of, you know, tens

453
00:32:27,720 --> 00:32:32,360
of thousands of chemical formulas that it's detected and raking them for a particular

454
00:32:32,360 --> 00:32:33,360
application.

455
00:32:33,360 --> 00:32:39,920
So maybe we're talking about, you know, things that have thermal electric properties,

456
00:32:39,920 --> 00:32:45,880
but the only reason why the algorithm, the model, knows about this particular compound

457
00:32:45,880 --> 00:32:49,680
is because someone looked at it for something else at some other time.

458
00:32:49,680 --> 00:32:50,920
Yeah, that's right.

459
00:32:50,920 --> 00:32:55,720
And so, you know, one of the associations that it might make, for example, is that the

460
00:32:55,720 --> 00:32:59,800
algorithm has found that there is some overlap between compounds that are studied for photovoltaic

461
00:32:59,800 --> 00:33:02,760
applications and thermal electric applications.

462
00:33:02,760 --> 00:33:07,040
So it might detect, for example, that there's a particular material that has been studied

463
00:33:07,040 --> 00:33:09,080
for photovoltaic applications.

464
00:33:09,080 --> 00:33:12,360
It might have some other keywords that it likes that are also associated with thermal

465
00:33:12,360 --> 00:33:17,320
electric, like Chalkoginite or, you know, Hoysler, except, for example, and decide that,

466
00:33:17,320 --> 00:33:20,680
you know, here's something that has a lot of keywords that tend to associate with thermal

467
00:33:20,680 --> 00:33:23,200
electric, but hasn't been studied before.

468
00:33:23,200 --> 00:33:26,040
So it's making those sorts of connections across the literature.

469
00:33:26,040 --> 00:33:35,320
And so, in order to train this embedding and build out this model, did you hear where

470
00:33:35,320 --> 00:33:41,280
there are things that you needed to kind of invent or innovate on beyond kind of word

471
00:33:41,280 --> 00:33:48,440
to veck as, you know, it's becoming increasingly common, or was it a fairly straightforward application

472
00:33:48,440 --> 00:33:51,280
of word to veck to get to this?

473
00:33:51,280 --> 00:33:55,760
Yeah, so, you know, unfortunately, one of the barriers to doing scientific literature

474
00:33:55,760 --> 00:33:58,880
mining is that the data sets are not easily available.

475
00:33:58,880 --> 00:34:01,480
Is the scientific literature itself?

476
00:34:01,480 --> 00:34:05,840
Yeah, well, because all the abstracts and everything are under published for agreements.

477
00:34:05,840 --> 00:34:09,760
And so, for example, we couldn't publish the data set of abstracts to share with everybody.

478
00:34:09,760 --> 00:34:12,280
We were legally not allowed to.

479
00:34:12,280 --> 00:34:16,040
And so, and so what that implies is that anytime you do a study like this, you have to spend

480
00:34:16,040 --> 00:34:20,640
a lot of time in collecting the data and pre-processing the data and tokenization and all that

481
00:34:20,640 --> 00:34:21,640
stuff.

482
00:34:21,640 --> 00:34:27,840
And just to be clear, so your entire model is based only on abstracts or the full content

483
00:34:27,840 --> 00:34:29,640
of the articles.

484
00:34:29,640 --> 00:34:36,600
Yeah, so what we showed in the paper was based only on abstracts, so no full text there.

485
00:34:36,600 --> 00:34:40,840
And so, in terms of whether it was just straight to word to veck or, you know, something more

486
00:34:40,840 --> 00:34:47,520
custom, I would say all the customization came from the actual pre-processing of the data.

487
00:34:47,520 --> 00:34:52,800
So things like being able to detect chemical formulas and doing the tokenization properly,

488
00:34:52,800 --> 00:34:54,600
things like that.

489
00:34:54,600 --> 00:34:59,440
And being able to collect the data in the first place was a bit of an effort as well.

490
00:34:59,440 --> 00:35:02,600
And so, because we had to spend a lot of time in that process, we didn't have a lot of

491
00:35:02,600 --> 00:35:05,640
time to actually work with the algorithm itself.

492
00:35:05,640 --> 00:35:09,440
We also spent a lot of time verifying whether the results were correct or not.

493
00:35:09,440 --> 00:35:12,040
So that's where most of our time went.

494
00:35:12,040 --> 00:35:16,240
So this was, I would say, a pretty straightforward application of word to veck.

495
00:35:16,240 --> 00:35:17,680
We did test against glove.

496
00:35:17,680 --> 00:35:20,360
We did do some hyperprimer optimization, things like that.

497
00:35:20,360 --> 00:35:25,720
But definitely, I wouldn't say we made any conceptual leaps in NLP during the course of

498
00:35:25,720 --> 00:35:26,720
this study.

499
00:35:26,720 --> 00:35:27,720
Sure.

500
00:35:27,720 --> 00:35:28,720
Yeah.

501
00:35:28,720 --> 00:35:31,960
And that question was not at all to take away from what you did, but rather see if there

502
00:35:31,960 --> 00:35:39,440
was any hidden element that it would be worth exploring further.

503
00:35:39,440 --> 00:35:45,720
I think what I'm hearing is very consistent with what I hear speaking with lots of folks

504
00:35:45,720 --> 00:35:52,000
is actually applying this stuff involves a ton of work setting up the data pipelines.

505
00:35:52,000 --> 00:35:55,160
And in your case, you've been getting access to the data.

506
00:35:55,160 --> 00:35:56,160
Yeah.

507
00:35:56,160 --> 00:36:00,880
And in some sense, it's kind of interesting to know that even one of the simplest word

508
00:36:00,880 --> 00:36:05,440
embedding algorithms is capable of giving good results on this problem.

509
00:36:05,440 --> 00:36:10,680
And so it really kind of begs the question, well, what if we were to use more advanced

510
00:36:10,680 --> 00:36:12,600
techniques on this problem?

511
00:36:12,600 --> 00:36:17,320
Or if we were to expand the data set to the full text, what more might be possible?

512
00:36:17,320 --> 00:36:21,800
I mean, if we're already seeing such good results using simple techniques, there could be

513
00:36:21,800 --> 00:36:26,920
a lot of opportunity here if we were to actually spend even more time to do this a little

514
00:36:26,920 --> 00:36:28,720
bit more carefully.

515
00:36:28,720 --> 00:36:37,000
And so if you were to kind of think about the opportunities that are ahead of you kind

516
00:36:37,000 --> 00:36:43,360
of in this direction, how do you rank them, what are the things you're most excited

517
00:36:43,360 --> 00:36:48,240
about kind of building on and where do you think the true opportunities lie?

518
00:36:48,240 --> 00:36:49,240
Yeah.

519
00:36:49,240 --> 00:36:54,360
So actually, you know, a lot of the motivation for this study was not necessarily to make

520
00:36:54,360 --> 00:36:59,840
predictive models, but just to really improve the process of information retrieval from

521
00:36:59,840 --> 00:37:01,640
the scientific literature.

522
00:37:01,640 --> 00:37:07,360
And I think there's really a lot of opportunity in going from this mechanism where scientists

523
00:37:07,360 --> 00:37:12,480
are just browsing articles one by one for relevant information to really using algorithms

524
00:37:12,480 --> 00:37:17,400
to getting the information that they need just in time from the research literature.

525
00:37:17,400 --> 00:37:22,520
So to kind of use an example or analogy, you know, back in the early days of the web,

526
00:37:22,520 --> 00:37:27,040
if you wanted to look up Planck's constant, you would have to find a bunch of science websites

527
00:37:27,040 --> 00:37:31,080
browse through them and see whether one of them had the value that you wanted.

528
00:37:31,080 --> 00:37:34,560
And today, you just type Planck's constant into Google and it just immediately returns

529
00:37:34,560 --> 00:37:35,560
the answer.

530
00:37:35,560 --> 00:37:37,200
You don't have to browse anything.

531
00:37:37,200 --> 00:37:41,640
And so I think there's a lot of opportunity, just an information retrieval from these

532
00:37:41,640 --> 00:37:43,640
very scientific abstracts.

533
00:37:43,640 --> 00:37:48,800
In terms of the actual predictive models and hypothesis generation, we already touched

534
00:37:48,800 --> 00:37:54,680
upon the ability to make predictions on materials that are not in the data set already.

535
00:37:54,680 --> 00:38:00,040
So how can we actually invent word embeddings for hypothetical chemical compositions?

536
00:38:00,040 --> 00:38:02,680
That's an area that we're looking into right now.

537
00:38:02,680 --> 00:38:07,280
We're also interested, as we spoke about before, of these more context sensitive embeddings

538
00:38:07,280 --> 00:38:12,400
and these more advanced methods that might help us distinguish between terms and materials

539
00:38:12,400 --> 00:38:16,720
that aren't really the same, but are the same currently in our work-to-vec model.

540
00:38:16,720 --> 00:38:20,520
So these are all areas that I think we will be touching upon in the future.

541
00:38:20,520 --> 00:38:25,160
What haven't I asked you about this research that will be worth exploring?

542
00:38:25,160 --> 00:38:31,480
Yeah, you know, I think one thing that I just want to mention is that somewhat ironically,

543
00:38:31,480 --> 00:38:41,440
this paper is behind the paywall, but the publisher, so the publisher, Springer Nature,

544
00:38:41,440 --> 00:38:45,480
they've actually made the full text available to read for free on research gate, so hopefully

545
00:38:45,480 --> 00:38:48,960
we can put a link to that in the show notes so that people that want to read the paper

546
00:38:48,960 --> 00:38:54,560
but don't necessarily have access to it can actually legally read the paper.

547
00:38:54,560 --> 00:38:57,560
So I think that's one of the main things I just wanted to mention about this.

548
00:38:57,560 --> 00:39:02,880
Okay, that's awesome, and we will definitely include that link in the show notes.

549
00:39:02,880 --> 00:39:10,160
On above, thank you so much for, well, A, listening to the podcast and B, jumping on

550
00:39:10,160 --> 00:39:14,720
to share a bit of what you're working on with all of us.

551
00:39:14,720 --> 00:39:15,720
Thanks so much.

552
00:39:15,720 --> 00:39:17,720
Yeah, it's been a pleasure.

553
00:39:17,720 --> 00:39:18,720
Thanks.

554
00:39:18,720 --> 00:39:24,120
All right, everyone, that's our show for today.

555
00:39:24,120 --> 00:39:30,120
For more information on today's show, visit twomolai.com slash shows.

556
00:39:30,120 --> 00:39:35,960
Make sure you head over to twomolcan.com to learn more about the Twomolcan AI Platforms

557
00:39:35,960 --> 00:39:37,440
Conference.

558
00:39:37,440 --> 00:39:51,400
As always, thanks so much for listening and catch you next time.

