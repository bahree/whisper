WEBVTT

00:00.000 --> 00:05.740
Hey, everybody. Sam here. We've got some great news to share and also a favorite

00:05.740 --> 00:12.000
to ask. We're in the running for this year's People's Choice podcast awards in both the

00:12.000 --> 00:17.600
People's Choice and Technology categories and we would really appreciate your support.

00:17.600 --> 00:23.640
To nominate us, you'll just head over to twimlai.com slash nominate where we've linked to and

00:23.640 --> 00:28.640
embedded the nomination form from the award site. There, you'll need to input your

00:28.640 --> 00:34.440
information and create a listener nomination account. Once you get to the ballot, just find

00:34.440 --> 00:39.760
and select this week in machine learning and AI on the nomination list for both the

00:39.760 --> 00:46.840
Adam Curry People's Choice Award and the this week in tech technology category. As you

00:46.840 --> 00:52.280
know, we really, really appreciate each listener and would love to share in this accomplishment

00:52.280 --> 01:00.800
with you. Remember, that url is twimlai.com slash nominate. Feel free to hit pause and take a

01:00.800 --> 01:04.640
moment to nominate us now.

01:04.640 --> 01:20.480
Hello and welcome to another episode of Twimletalk, the podcast why interview interesting people, doing

01:20.480 --> 01:25.960
interesting things in machine learning and artificial intelligence. I'm your host, Sam

01:25.960 --> 01:27.960
Charrington.

01:27.960 --> 01:41.560
A quick update about the upcoming Twimla Online Meetup. On Tuesday, July 17th at 5pm Pacific

01:41.560 --> 01:46.720
time, Nick Teague will lead a discussion on the paper Quantum Machine Learning by Jacob

01:46.720 --> 01:52.320
Diamante at all, which explores how to devise and implement concrete quantum software for

01:52.320 --> 01:58.760
machine learning tasks. If you haven't joined our meetup yet, visit twimlai.com slash meetup

01:58.760 --> 02:03.040
to do so. In this episode, I'm joined by Alexander

02:03.040 --> 02:09.920
Chikovsky, director of data services at Munich Germany based career platform, Expertier.

02:09.920 --> 02:14.840
In this podcast, we explore Alex's journey to implement machine learning at Expertier.

02:14.840 --> 02:20.040
In particular, Alex and I discussed the Expertier NLP pipeline and how it's evolved over time

02:20.040 --> 02:24.720
to address the company's need for greater automation and the way it processes jobs on

02:24.720 --> 02:30.520
its platform. We also discuss Alex's work with deep learning based NLP models, including

02:30.520 --> 02:36.480
models like VDCNN and Facebook's fast text offering, which he's particularly excited

02:36.480 --> 02:37.480
about.

02:37.480 --> 02:43.280
Finally, we briefly touch on some recent papers that look at transfer learning for NLP,

02:43.280 --> 02:48.000
how Alex keeps up with academic papers in general, and a few tips for people looking

02:48.000 --> 02:55.360
to inject ML and DL into their products or projects. And now on to the show.

02:55.360 --> 03:05.160
All right, everyone. I am on the line with Alex Chikovsky. Alex is director of data services

03:05.160 --> 03:09.520
at Expertier. Alex, welcome to this weekend machine learning in AI.

03:09.520 --> 03:13.080
Hi, hi Sam. Thank you for having me. Great to be on the show.

03:13.080 --> 03:17.080
So why don't we get started by having you tell us a little bit about your background

03:17.080 --> 03:22.680
and how you got into machine learning and data science. You come at it from the business

03:22.680 --> 03:25.320
angle, which is kind of an interesting starting place, huh?

03:25.320 --> 03:32.440
Yeah, exactly. So I studied economics major team finances statistics in the beautiful

03:32.440 --> 03:40.640
city of Munich. And then till 2014, I used to manage a data processing team at Expertier,

03:40.640 --> 03:46.280
which I'll tell you what the company does. And the data processing team was like very

03:46.280 --> 03:52.560
large entity of the companies, like the core team. It had about 100 people doing all kinds

03:52.560 --> 03:58.880
of stuff related to processing job data in 12 different markets, seven different languages,

03:58.880 --> 04:05.120
so quite a lot of complexity. And my role was to kind of manage the team, put a direction

04:05.120 --> 04:11.520
on it, make sure that everything runs more than efficient, that the quality is good to

04:11.520 --> 04:17.040
develop some new web based tool because our whole data processing stack was Ruby on

04:17.040 --> 04:23.600
the Rails application developed internally. And I kind of got into machine learning by

04:23.600 --> 04:35.280
chance. So back in 2014, beginning of 2014, we had like a local optimum of efficiency,

04:35.280 --> 04:41.880
so we had very high cost per job, which was like our internal KPI. And we were looking

04:41.880 --> 04:49.320
of options of how to drive this cost down. So pretty much all the operational projects

04:49.320 --> 04:54.040
that we could do to improve this were done, the technologically speaking, the platform

04:54.040 --> 05:01.840
was very streamlined. So somehow someone came with the idea to use support vector machines

05:01.840 --> 05:07.560
to kind of assist the classification process of the jobs, which is like the part that takes

05:07.560 --> 05:15.160
a lot of time. And we had a very first implementation that did not really work out very well,

05:15.160 --> 05:21.840
it was not taken very well by team. And then still kind of interesting results. So the

05:21.840 --> 05:27.560
management and I kind of said, okay, let's look at what this machine learning thing can

05:27.560 --> 05:35.240
do. So we did not have any capacity internally. So no one was actually familiar with machine

05:35.240 --> 05:42.880
learning in the company. So we talked to a few external agencies to kind of get competency

05:42.880 --> 05:50.720
from outside and help us to build one of the first prototype that we used in the beginning.

05:50.720 --> 05:57.440
So more or less, I really come from the business side and I got into machine learning by chance

05:57.440 --> 06:05.440
so to say it may help folks contextualize some of what you described by digging into

06:05.440 --> 06:11.480
expertise and what the company does and what some of the main problems that you've been

06:11.480 --> 06:19.640
tasked to solve there are. Yeah, all right. So, um, expert here is a closed career platform

06:19.640 --> 06:26.440
for high paid professionals, which means that the jobs that are offered on this platform

06:26.440 --> 06:35.240
have a very high salary. So they start at above 60,000 euros or about 100,000 US dollars

06:35.240 --> 06:43.680
in the US. And it's a closed network where recruiters can contact candidates. And the

06:43.680 --> 06:49.560
reason why expert here has been so good at solving this problem of offering jobs to candidates

06:49.560 --> 06:57.840
is because we have a very extensive job classification, right? So if you go on a typical job board

06:57.840 --> 07:02.640
today and you would search for a job, so let's say I would go to one of the large job boards

07:02.640 --> 07:08.720
and I would search for a CEO position and everyone can do this experiment. I would most likely

07:08.720 --> 07:15.440
get all kinds of jobs that have direct text match that has something to do with reporting

07:15.440 --> 07:23.880
to the CEO or assistant to the CEO and maybe somewhere in the fifth or in the sixth result

07:23.880 --> 07:29.440
page, I would get like a job that really matches. So that's kind of the problem of traditional

07:29.440 --> 07:34.440
job search, the food tech search. And expert here solved this by adding a few additional

07:34.440 --> 07:43.440
layers on the search site. So for example, we would have something called a career level,

07:43.440 --> 07:50.080
right, where we would put jobs in different categories like a job for someone who is out

07:50.080 --> 07:58.400
of university or a job that is for someone who leads projects or for someone who manages

07:58.400 --> 08:04.360
teams or manages managers that manages team and so on. So you would be able to use these

08:04.360 --> 08:11.320
filters and kind of dig really deep into finding a job that is really interesting for you

08:11.320 --> 08:17.720
on the one hand. But on the other hand, if you set up your profile correctly, you would

08:17.720 --> 08:24.720
get suggestions that are very relevant for you. So expert here has a very detailed job

08:24.720 --> 08:32.760
toxonomy, classification of jobs. So for example, in industries, we have about 630 industries.

08:32.760 --> 08:38.680
So if you're looking for a job in consulting or in risk and restructuring consulting,

08:38.680 --> 08:44.760
we have it. It's there and we can offer it to you, right? So this is kind of the problem

08:44.760 --> 08:51.160
that the team was trying to solve. So putting jobs in these extensive categories, right?

08:51.160 --> 09:01.000
You know, the jobs, the listings sourced from public postings or they sourced privately,

09:01.000 --> 09:09.040
proprietary listings that can be, and the question is really, how much noise is in those

09:09.040 --> 09:14.760
listings or if you're sourcing them privately, are they cleaner than you might expect from

09:14.760 --> 09:22.560
just a crawled set of listings? Yeah, that's a good question. So back when we started

09:22.560 --> 09:28.720
in 2014, everything was very clean because we had kind of automated this whole crawling

09:28.720 --> 09:36.040
process. It was still not completely automatic. So people still had to trigger them, but the

09:36.040 --> 09:42.800
content of the job, so the job text was very clean. A little bit later, once we kind of

09:42.800 --> 09:49.360
started crawling on large scale, another problem came where we would have a text that it's

09:49.360 --> 09:55.440
not related to the job. So back when we were building this first iteration of the solution,

09:55.440 --> 09:59.400
we had fairly clean job descriptions, I would say.

09:59.400 --> 10:04.280
You kind of had this point in the business where you, you know, things were going okay,

10:04.280 --> 10:09.680
but you needed to figure out a way to kind of make the next leap in productivity and

10:09.680 --> 10:18.040
you came across machine learning as a possible opportunity. And you said it didn't necessarily

10:18.040 --> 10:23.280
work, but it was promising enough that you continued to invest in it. What did you

10:23.280 --> 10:26.240
do then? What was the next step?

10:26.240 --> 10:32.720
Yeah, so the next step was, first of all, of course, convinced management that there

10:32.720 --> 10:39.360
is an opportunity there. So we came up with a very high, a very motivational goal for the team,

10:39.360 --> 10:48.320
so we would like to like cut our cost by 50% and at the same time increase the output by a factor

10:48.320 --> 10:54.800
of two, the team, the team output. And as I mentioned, in the beginning, because we didn't have

10:54.800 --> 11:02.720
any competency, we kind of talked to a few external agencies, we gave them data sets and they were,

11:02.720 --> 11:10.960
so to say in a competition of showing us who can do better. And one of the agencies got

11:11.600 --> 11:17.360
quite alright results. So we said, okay, let's give it a go. I mean, the cost perspective was okay,

11:17.360 --> 11:21.440
so there was nothing to lose. And we decided to start working with them.

11:21.440 --> 11:28.800
And what even gave you the confidence to put that kind of an aggressive goal on the table

11:28.800 --> 11:37.600
after an experiment that didn't necessarily prove out that you can do it? It sounds like it

11:37.600 --> 11:41.600
wasn't a foregone conclusion that you'd be successful when you threw that out on the table.

11:43.040 --> 11:50.800
So this goal came mostly from our CEO, so I guess that from his experience of being a business

11:50.800 --> 11:57.280
leader, he had to set very high goals to motivate people, like me and the other people in the team.

11:59.200 --> 12:04.320
So yeah, I mean, the initial results were not really satisfying, but they were still kind of

12:04.320 --> 12:10.880
interesting. So we saw that the support lecture machines, even though they only took the job title

12:10.880 --> 12:18.560
and a part of the text kind of got a few of the functions of the jobs correctly. So we said,

12:18.560 --> 12:25.760
okay, we need a large goal here. So it's either a goal, a bigger goal home. And I think from a

12:25.760 --> 12:32.240
purely psychological perspective, if you have such a huge goal, you're not chasing the next 10%,

12:33.200 --> 12:40.240
it's kind of more motivating. That's for me, and this is how I would do it if I were to implement

12:40.240 --> 12:45.920
some kind of a new machine learning process in a company that's going to change completely

12:45.920 --> 12:51.520
the partiums. I would really set high goals. How did the process kind of evolve over time

12:52.160 --> 12:56.560
with this? What were the steps, I guess, to achieve in that goal? Did you achieve that goal?

12:56.560 --> 13:02.720
What were the steps to getting there? Yeah, so in order to keep people interested,

13:02.720 --> 13:12.400
yeah, we actually, we did achieve the goal. So we got our cost curve down to 30% from what the

13:12.400 --> 13:20.640
baseline was when we started. So this is like 70% saving. But the most interesting part was that

13:21.520 --> 13:29.200
we tripled the amount of jobs that we had on the platform back then. And for a lot of people,

13:29.920 --> 13:35.920
this probably doesn't sound like a lot to triple the content of jobs. But as I imagine,

13:35.920 --> 13:41.040
the beginning expert here focuses on these high-level jobs, right? So jobs above 100,000

13:41.040 --> 13:49.920
US dollars. And if you look at an average page of a company that has 1000 jobs, maybe 5% of them

13:49.920 --> 13:58.800
would be relevant for this type of market. So getting to tripling the amount of jobs that we had

13:58.800 --> 14:05.280
on the platform, we actually had to increase our processing power. So the amount of jobs that we

14:05.280 --> 14:13.280
pushed through the platform, we call it assembly line. So if the team used to be able to do like

14:13.280 --> 14:22.000
manually, about 50, maybe 60,000 jobs a month, today we process close to 3 million every day.

14:22.000 --> 14:28.320
So you can like imagine the gains that we had. That's huge difference.

14:28.320 --> 14:29.600
Yeah, that's huge.

14:29.600 --> 14:35.760
Yeah. And so there was a significant part of the workflow previously that was based on manual

14:35.760 --> 14:40.800
categorization. And that's all or some portion of some large portion of that. It sounds like it's

14:40.800 --> 14:48.320
been replaced with machine categorization. Yeah, this is correct. So this was, so we kind of built,

14:48.320 --> 14:55.440
we kind of drew a picture of our complete value chain like from how a job comes to the platform.

14:55.440 --> 15:02.880
Every step that it needs to go through before it's like finished and processed to be served to a

15:02.880 --> 15:10.240
customer. And we identified which are the areas that took most of the time. And I mean, it was

15:10.240 --> 15:15.120
not surprising, but the classification was like the biggest one. So that's where we started. So

15:16.400 --> 15:24.640
if I look at the way the team has evolved today, it's mostly the work is now kind of quality

15:24.640 --> 15:29.760
related to work. So there's very little classification going on. Maybe for jobs where the model is

15:29.760 --> 15:38.640
still not very sure about the classification that it provides. And so how many iterations of your

15:39.440 --> 15:45.280
pipeline have you gone through at this point? Did you kind of settle in on the, whatever the

15:45.280 --> 15:52.640
first thing you did with the the winning consulting company after you set up on this path or have

15:52.640 --> 16:01.120
you iterated your pipeline a few times? I would say that up until, I mean, you can see also

16:01.120 --> 16:08.560
including today, we are constantly iterating. So we started with, if you think about our data

16:08.560 --> 16:16.320
pipe, it had in the beginning four services, the four classification services. So today we have

16:16.320 --> 16:22.160
close to 30 different services that are related to these data processing steps. So not all of them

16:22.160 --> 16:27.680
are machine learning based, but I would say probably half of them are. So the way we started was

16:27.680 --> 16:33.840
that the agency was like very optimistic. They were great guys based in Munich. So I think the very

16:33.840 --> 16:39.680
first thing that we started was like very classical support vector machine type of classification,

16:39.680 --> 16:48.000
you know, back of words. And it kind of worked for, I think we started with English first.

16:48.000 --> 16:53.920
That's actually a good moment to mention that expert year supports seven different languages.

16:54.640 --> 17:01.280
And if we look at English, it actually, we have to look at it twice because you have US English

17:01.280 --> 17:06.560
and British English. And the way that those two countries write their jobs description is

17:06.560 --> 17:14.880
completely completely different. So it took us about a year and a half and hundreds of different

17:14.880 --> 17:22.160
iterations, which I will explain exactly what this means. To come to a moment where we said,

17:22.160 --> 17:29.120
okay, we're happy with what we have now. So let's focus on quality. We focused on quality. We saw

17:29.120 --> 17:34.240
that there are some areas to improve. We saw that there are some opportunities to get more data

17:34.240 --> 17:39.200
and then we said, okay, you know, so many options. So let's build a real data science department

17:39.200 --> 17:45.520
in the company. That's kind of how the whole thing evolved. But literations looked a little bit

17:45.520 --> 17:52.720
like this. So the agency built the first models. And as I said, we looked at the results. They were

17:52.720 --> 17:59.440
not very impressive. I would say, I would say something in the area. So if we take, for example,

17:59.440 --> 18:08.640
function, so expert here has 19 different functions for a job. So stuff like sales, marketing, IT,

18:08.640 --> 18:15.680
production, manufacturing. I think in the beginning, we had like an F1 score of maybe 45,

18:15.680 --> 18:24.400
so maybe 50%, which was not in no way production ready. And we would look at the mistakes.

18:24.400 --> 18:30.320
And there were, of course, the traditional mistakes where you would see that the classifiers were

18:30.320 --> 18:38.480
not able to abstract well enough. But there were also other types of mistakes where actually

18:38.480 --> 18:44.160
the classifier did the correct job. And then once we looked at the validation, the validated job,

18:44.160 --> 18:51.760
we saw that the job was initially classified wrong. And we would then talk to the person that

18:51.760 --> 18:58.240
created this job. And we would like try to understand why this mistake happened. And this was like

18:58.240 --> 19:05.280
one of the first eye openers where we found out that people have humans and stuff, feelings and

19:05.280 --> 19:11.440
stuff. So it would something like this would happen like a person would say, hey, you know,

19:11.440 --> 19:17.360
that day I remember my girlfriend broke up with me. So I was like in such bad mood and I didn't

19:17.360 --> 19:23.200
really, I wasn't really careful in what I was doing. Right. So you'd have these type of cases.

19:24.560 --> 19:30.800
We had these quality managers and they would control the quality of the work like random samples

19:30.800 --> 19:37.600
of people that were a little bit too junior. And there were a few people that were more,

19:39.120 --> 19:45.200
you know, they were more careful. So if you were a researcher, that's how we call the people that

19:45.200 --> 19:50.960
were classifying the jobs. So if you're a researcher, you know, okay, so today I have person A,

19:50.960 --> 19:57.200
who is very careful at what he or she does. And tomorrow, I have person B. So this kind of

19:57.200 --> 20:03.680
impacts the way I also work. So this is of course a challenge because it means that the data

20:03.680 --> 20:09.120
said that we had to work with was in no way perfect. And we actually had initially the assumption

20:09.120 --> 20:15.440
that we have these perfect data sets, which was not the case. So this is kind of a extreme cold shower

20:15.440 --> 20:24.000
to begin with. And so did you go back and kind of clean up all your labels or did you do something

20:24.000 --> 20:29.840
else? Yeah, this was, yeah, this was the initial plan to kind of go through the data set, take out

20:29.840 --> 20:37.280
the bad stuff. We had this internal hierarchy where we knew if a person was junior or more senior,

20:37.280 --> 20:45.600
so we kind of sorted stuff out there. But it still wasn't good enough. So what we did back then

20:45.600 --> 20:52.480
was this is kind of the moment where we said, okay, a pure machine learning solution is not going

20:52.480 --> 20:59.200
to work out. So maybe let's think if there is a hybrid solution that we can use. So a combination

20:59.200 --> 21:07.440
of a classifier, additional features that we would extract from the text, so like pure text

21:07.440 --> 21:15.200
analytics. And we would try to enforce the business logic in certain cases where the model fails

21:15.200 --> 21:21.120
because the abstraction is too complicated. And in order for people to really understand this,

21:21.120 --> 21:28.080
we have to come back at how people like really, really read jobs. So if you have a traditional

21:28.080 --> 21:35.040
job description, right? I'm trying to guess the career level of this job. So a human would normally

21:35.040 --> 21:42.000
start scanning the job description for specific keywords. So if I'm trying to find out if the

21:42.000 --> 21:48.880
job is a senior one, I would look for keywords like four years of work experience, right? Or five

21:48.880 --> 21:55.920
years of work experience. But because you have different companies and different languages and

21:55.920 --> 22:02.080
different company size and different cultures in the companies, the way they write their

22:02.080 --> 22:07.440
descriptions very heterogeneous, it's very different. So it's really hard to reach an abstraction

22:07.440 --> 22:13.440
there. And if one person writes four years of work experience and our company would write

22:14.400 --> 22:21.920
two to five years of work experience or they would write five years of relevant work experience.

22:21.920 --> 22:29.920
Or yeah, you get the picture. So it's kind of an extreme amount of complexity there when you try

22:29.920 --> 22:40.160
to classify this text. And at this point, was that okay? Let's do bag of words, extract. No,

22:40.160 --> 22:48.960
sorry, not bag of words. Let's do five grams, see what are the most common combinations of words

22:48.960 --> 22:53.920
related to the career level. Career level was very complicated. This was like the most

22:53.920 --> 23:04.160
complicated one to classify. And we would then identify these, so to say, key text pieces and then

23:04.160 --> 23:12.240
we would expand them. So that in the end of the day, when you had your you had like an assemble

23:12.240 --> 23:18.480
model, but I don't know if there is an effort, honestly. So you have a classifier that gives

23:18.480 --> 23:24.000
some kind of prediction, then you'd try to additionally see if you have matched some specific

23:24.800 --> 23:29.360
semantic structures in the sentences, you would also try to see if you have matched some

23:31.440 --> 23:37.440
parts of the text that you extracted in a text analysis part. And you would also

23:38.400 --> 23:45.120
compare a classifier on the job title and description separately. And all of these information,

23:45.120 --> 23:52.480
it will like go into one big model that will then use the product or machines to do like the final

23:52.480 --> 23:59.440
decision of the classification of the job. And these took a lot of iterations. I would say

24:00.160 --> 24:06.240
we used to probably do per language per per meter more than 100 iterations,

24:06.240 --> 24:16.640
which means like eight languages, three main parameters, 24. Yeah, that's quite a lot. I think

24:16.640 --> 24:25.360
you get the picture. And you mean in terms of training cycles or in terms of just the

24:25.360 --> 24:37.840
versions of the model over time. So we would clean up the data set where we find inconsistencies,

24:37.840 --> 24:44.960
we would retrain the models, then we would extract some new pieces from the text that are somehow

24:45.680 --> 24:52.240
relevant for the text analytic part of the model. We would do some more iterations. We would

24:52.240 --> 25:00.080
find out, for example, that specific job categories are not abstracted that well. And there we

25:00.080 --> 25:06.240
would do like a very very hard rule that it's enforced and has a very high score that goes into

25:06.240 --> 25:13.200
the final model. Interesting. And how long did this process take calendar wise? Was this,

25:13.200 --> 25:17.120
you know, over the course of a couple of years or, you know, much shorter than that?

25:17.120 --> 25:26.640
So English, we kind of covered in three months. So we got, I mean, we said that just to get

25:26.640 --> 25:33.920
a understanding of all our KPIs. So we said that obviously we cannot get a score of 100%.

25:34.480 --> 25:41.280
So we would be very happy if we get something like 55 or 60% of the jobs

25:41.280 --> 25:50.160
to be classified at least like 95% correct. So like 60% Rico and 90% precision. And in order to get

25:50.160 --> 25:59.440
there, it took us about three months for English, US and UK, another maybe four or five months for

25:59.440 --> 26:05.680
German because there was differences in the Swiss German and the German German. And

26:05.680 --> 26:16.240
somewhere in the beginning of 2016, we were done with French, Spanish, Italian and Dutch.

26:17.200 --> 26:23.920
So I would say a year and a half, maybe close to two years with fine-tuning afterwards.

26:25.600 --> 26:33.360
And when you look at the kind of the direction that this is all going, do you see, have you

26:33.360 --> 26:40.320
started looking at some of the deep learning-based NLP models and beddings and things like that? And do

26:40.320 --> 26:48.720
you see that changing your general approach, like taking away the need to do this hierarchical

26:48.720 --> 26:55.440
type of model with both the, you know, traditional NLP text analytics stuff and the

26:57.040 --> 27:02.320
ML classifiers or do you expect that you'll always have some degree of that hanging around?

27:02.320 --> 27:10.960
Yeah, that's a very good question. So my hope is that at some point of time we will be able to

27:10.960 --> 27:18.160
completely drop this whole text analytics and business rules part. I mean, it doesn't take that much

27:18.160 --> 27:26.160
time to manage them nowadays. We do a lot more retraining. But we started a very, so deep learning

27:26.160 --> 27:36.240
was such a huge fuss in 2017. So I think in June or July 2017, my team and I decided to kind

27:36.240 --> 27:43.120
explore deep learning for text classification because like historically speaking, and if you do

27:43.120 --> 27:48.880
this whole academic reviews, deep learning was always praised, you know, for text classification.

27:48.880 --> 27:55.760
So we tried quite a lot of stuff. So we tried very deep convolutional neural networks,

27:55.760 --> 28:05.040
VDCNN. We tried HDO text. So it's like a hierarchical deep learning. They got two pretty good

28:05.040 --> 28:11.200
numbers. I would say if we look at career level, right? So out of the box, very little fine tuning.

28:11.200 --> 28:22.320
We got to 80% quite quickly. But what really struck me, it was somehow by chance, one of the

28:22.320 --> 28:28.800
people from our team, Viet, he found out about fast text from Facebook. I don't know if you're

28:28.800 --> 28:34.800
familiar with this one. That's a text classification framework from Facebook, which you can actually train

28:34.800 --> 28:43.200
on a prem normal computer on a desktop. And fast text really blew everything away. We were

28:43.200 --> 28:50.800
literally not building our eyes. So on average, it outperformed all deep learning frameworks,

28:50.800 --> 28:56.560
we tried by two, three, maybe four percent. But the difference in the training time is, you know,

28:56.560 --> 29:05.280
for fast text training on 600,000 jobs, so 6000 data points, 600,000 data points,

29:06.560 --> 29:14.560
trying to guess the four classes for career level. Fast text takes 2.5 minutes to train on a desktop

29:14.560 --> 29:21.680
and VDCNN takes 16 hours on a 12 gigabyte GPU. So yeah.

29:21.680 --> 29:30.240
And what about the sample efficiency? Were there any mark differences in the amount of data you

29:30.240 --> 29:36.000
needed to provide the different models? Well, when it comes to data, we kind of decided to go

29:36.000 --> 29:43.440
all-in because we have millions of hand classified samples. So the really, really, the point was,

29:43.440 --> 29:49.600
okay, just try to throw in everything in that we know that it's good and see what comes out.

29:49.600 --> 29:56.960
Okay. So with regards to this, there were not a lot of problems. There is one specific problem

29:56.960 --> 30:02.560
where we are kind of experimenting right now with a little something from your show, the zero shot

30:02.560 --> 30:11.440
machine learning. So we have a specific case where we have very few examples present as data set.

30:11.440 --> 30:16.880
So we are kind of playing around with this one. I think there is a very interesting paper from 2017.

30:16.880 --> 30:26.320
But yeah, deep learning was not the cure for us, to be honest. So we decided to stick to fast text.

30:26.320 --> 30:36.400
So the fast text classifier, are you currently using that in production and how much of

30:37.360 --> 30:41.520
all of the other stuff that you had, did it replace or do you see it replacing?

30:41.520 --> 30:51.360
So yeah, we do use it in production. So as I said, we have this one model that kind of takes the

30:51.360 --> 30:59.520
input from all of the other predictors. And fast text is also one that gives an additional

31:00.880 --> 31:07.040
an additional score there for jobs where we are not sure of how the general classification

31:07.040 --> 31:16.560
would perform. And I would really like to like use it completely. But I think that from a risk

31:16.560 --> 31:24.240
perspective, I would still like to stick to this kind of ensemble solution that we have for a while

31:24.240 --> 31:30.720
because as I said, we invested a lot of time in creating this whole business logic and textuality

31:30.720 --> 31:39.040
crews. I wouldn't throw them out immediately. But looking at the future, if I'm going to do

31:39.040 --> 31:45.280
retraining, I would definitely focus the energy on the fast text part because I think that this

31:45.280 --> 31:50.960
really works very well for jobs. I don't know what is exactly. Maybe it's the length of the text.

31:51.680 --> 31:56.960
Chops kind of have, on average, 2,000 characters. So maybe that's why fast text works very well.

31:56.960 --> 32:04.160
Maybe because of the number of features that it can work with. But I think we'll keep the original

32:04.160 --> 32:10.320
model for a while still. But if I can give a tip to anyone who starts into text classification,

32:10.320 --> 32:14.400
it would be too looking to fast text. It's really amazing. It's really amazing.

32:15.200 --> 32:23.520
So there was recently a post by Jeremy Howard and Seb Gruber talking about some work that they

32:23.520 --> 32:30.800
were doing in applying transfer learning to NLP. I forget what models they use. But do you have

32:30.800 --> 32:37.840
you come across that by any chance? Yeah. I'm just looking for one of the last papers that I

32:38.640 --> 32:44.640
decided to use recently. It's called Structural, something in the area of structural

32:44.640 --> 32:50.720
sentence similarities, the machine for short text. I think this is kind of related to transfer

32:50.720 --> 32:55.600
learning. Then there was a fast AI. I think they also have.

32:57.120 --> 33:03.280
Yeah, we're probably thinking about the same one. Jeremy Howard and Sebastien Ruder.

33:03.280 --> 33:12.160
Yeah. Yeah. Yeah. Oh my god. Yeah. So once I wrote it, I was like, wow, wow. We got to try it

33:12.160 --> 33:20.080
out. It's in our, it's in our Gira system. It's prioritized. So the moment the team gets a little

33:20.080 --> 33:26.160
bit of air, they're trying it. They're going to try it out because there is only this graph. It's

33:26.160 --> 33:34.560
amazing. It's well. Yeah. So it sounds like a big part of your, I don't know, with workflows the

33:34.560 --> 33:40.480
right word, but a big part of the way you approach this problem and kind of staying ahead of the

33:40.480 --> 33:48.480
curve is tracking the academic research and trying stuff out as you come across stuff that looks

33:48.480 --> 33:55.600
interesting. Yeah. That's, this is absolutely correct. We cannot try to see what comes out in

33:55.600 --> 34:03.840
the space. It's really hard to really monitor everything. So I'm just trying to get bits and

34:03.840 --> 34:09.600
pieces from everywhere and then try occasionally stuff out that looks promising. I mean, it's really

34:09.600 --> 34:16.640
important to kind of see what kind of data sets are used in the papers because like from my experience,

34:16.640 --> 34:22.880
from what I saw in the deep learning part that we did all these tests with. If someone is interested,

34:22.880 --> 34:28.640
there is a very long presentation on my LinkedIn profile with all the tests that we did.

34:29.600 --> 34:37.200
So these papers, right, they use the academic papers, they use like very nicely balanced data sets.

34:38.000 --> 34:45.200
And they don't necessarily have a touch to reality, like, right? So the way we work is the data

34:45.200 --> 34:52.640
sets that we have are like very unbalanced and they need a lot of fine tuning. So when I see

34:52.640 --> 34:57.360
these benchmarks of the deep learning models to other models in the papers, it looks very promising

34:57.360 --> 35:04.960
but once you try it on your own data, the picture is completely different. So I still try to be like

35:04.960 --> 35:10.880
innovative, try stuff out whenever we have time, but we also try to stick to the kind of established

35:10.880 --> 35:17.200
things, like fast text, for example. Is the way you've addressed the imbalance nature of your

35:17.200 --> 35:23.200
data set? Is there anything different that you do there beyond the general stuff that we've talked

35:23.200 --> 35:28.800
about, the, you know, applying the business rules and things like that? So for the deep learning

35:28.800 --> 35:36.240
part, we tried, of course, oversimpling. So we did like this with and without oversimpling.

35:36.240 --> 35:46.000
It's especially relevant for specific classifications where, you know, you don't have a lot of jobs

35:46.000 --> 35:53.520
in the sea level position, right? So you probably have like 1000 data points instead of 1 million,

35:53.520 --> 36:01.040
which would be in the lower career levels. So the way what we tried out and it worked out quite well

36:01.040 --> 36:09.440
was to do, to combine classes. Probably there is some term for this in the academic world,

36:09.440 --> 36:15.360
but we would kind of keep lower classes where we had a lot of data points in one classifier,

36:15.360 --> 36:22.320
right? So you'll have class one, class two, class three, and then class four would be a combined

36:22.320 --> 36:27.520
of the classes where we had very little data. So this classifier would do like the first,

36:27.520 --> 36:34.160
the first decision, you know, is it one, two, three, or four? And four could be then

36:35.200 --> 36:39.840
class four could be then additional three more classes. And there, for example, we find out that

36:39.840 --> 36:45.280
FASTX really captured very well this differentiation on the high classes that the deep learning model

36:45.280 --> 36:51.280
did not abstract, was not able to abstract. So this is kind of my experience on the in-balance

36:51.280 --> 36:58.320
data sets. Very cool. Any other, any other experiences that you'd like to share? Oh yeah.

36:58.320 --> 37:03.600
So for people that kind of rolling out machine learning services in the organization,

37:04.720 --> 37:11.440
I have a few tips. So first of all, as I said, you know, do some big and bold goals,

37:12.160 --> 37:18.000
like double something or triple something so that you can kind of try to challenge the

37:18.000 --> 37:23.280
status quo that you have in your organization. But at the same time, you have to be, of course,

37:23.280 --> 37:30.240
able to balance by not setting the project scope to large. Because if you set the project scope

37:30.240 --> 37:36.640
to large, then you have a lot of unknown factors, and you have a two optimistic roadmap,

37:36.640 --> 37:42.880
and especially if you learn working a large company, you know, politics can be kind of a bummer

37:42.880 --> 37:50.880
for data scientists because it tends to over promise. Then once you are kind of ready to roll out

37:50.880 --> 37:57.680
your solution, my suggestion would be to kind of identify the people in your team that are

37:59.200 --> 38:05.760
the very, you know, motivated curious people that are interested in trying new stuff out,

38:06.400 --> 38:12.000
and first play it out with them a little bit, and after that roll it out to the whole organization.

38:12.000 --> 38:18.240
So I would use this kind of small core team to improve the solution as much as I can,

38:18.240 --> 38:24.320
and I would then roll it out into the whole organization because people are generally very

38:24.320 --> 38:31.520
skeptic by nature. We had this kind of experience. So you give this amazing solution, right? And

38:32.560 --> 38:38.400
the person says, sees that it kind of automates his or her job, so it helps him or her a lot.

38:38.400 --> 38:47.040
And the first thing that they try to find out is why it cannot work or how it is wrong, right?

38:47.040 --> 38:52.400
So they don't focus on kind of improving the solution, but they kind of focus on how to convince

38:52.400 --> 38:59.040
you that this is never going to work out. And this was a big fight in the organization. So

38:59.040 --> 39:07.120
this was a very kind of an interesting experience. And be careful with this like first implementations

39:08.240 --> 39:14.480
because if you, if you kind of fail there, then you're going to have the general team who

39:14.480 --> 39:20.560
have some kind of skepticism for the whole project, and it's hard to get away from it. So my

39:20.560 --> 39:27.360
suggestion would be to rather release something once you're like very sure that it's working well

39:27.360 --> 39:36.800
instead of kind of releasing too early and not being good enough. And with regards to, we should

39:36.800 --> 39:42.960
like starting with machine learning, the cloud providers from my perspective are great. So

39:43.760 --> 39:50.080
these are industry leaders. And we see this kind of trend of having off the shelf solutions

39:50.080 --> 39:55.600
for machine learning. So from my perspective, this is going to be the future. And I already, I mean,

39:55.600 --> 40:02.400
I'll talk a lot about it when conferences and people tend to not believe me, but I think that

40:02.400 --> 40:07.360
in a few years, we are going to have a lot more solutions like drag and drop your data. And

40:08.080 --> 40:15.040
we'll figure it out for you. And here's an API, you know, and just, you know, send us everything

40:15.040 --> 40:21.760
that you want on this API. And we'll give you back the result. This is the way I think data science

40:21.760 --> 40:27.920
had always developing. Well, Alex, thanks so much for taking the time to chat with me.

40:27.920 --> 40:28.720
Yeah, thank you, Sam.

40:33.040 --> 40:38.880
All right, everyone, that's our show for today. For more information on Alexander or any of the

40:38.880 --> 40:46.560
topics covered in this episode, head over to twomlai.com slash talk slash 161. If you didn't hit

40:46.560 --> 40:51.120
pause and nominate us for the podcast People's Choice Awards at the beginning of the show,

40:51.120 --> 40:58.240
I'd like to encourage you to jump over to twomlai.com slash nominate right now and send us your love

40:58.240 --> 41:28.080
and your vote. As always, thanks so much for listening and catch you next time.

