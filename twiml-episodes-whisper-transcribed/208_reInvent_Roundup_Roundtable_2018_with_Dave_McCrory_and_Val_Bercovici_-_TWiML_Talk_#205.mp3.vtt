WEBVTT

00:00.000 --> 00:16.320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

00:16.320 --> 00:21.480
people, doing interesting things in machine learning and artificial intelligence.

00:21.480 --> 00:31.320
I'm your host Sam Charrington.

00:31.320 --> 00:35.960
This week on the podcast, we feature a few of my many conversations from last week's AWS

00:35.960 --> 00:38.880
Reinvent Conference in Las Vegas.

00:38.880 --> 00:44.200
For today's show, I'm excited to present our second annual Reinvent Roundtable Roundup.

00:44.200 --> 00:48.920
I had a blast last week learning about all the new ML&AI products and services announced

00:48.920 --> 00:50.600
by AWS.

00:50.600 --> 00:54.360
If you missed the news coming out of Reinvent, or you want to know more about what one

00:54.360 --> 00:59.200
of the biggest AI platform providers is up to, you'll want to stay tuned because we discussed

00:59.200 --> 01:03.960
many of their new offerings in this show.

01:03.960 --> 01:09.200
This year, I'm joined by my friends Dave McCrory, VP of Software Engineering at Wise.io

01:09.200 --> 01:15.040
at GE Digital, and Val Berkevici, founder and CEO of pencil data.

01:15.040 --> 01:21.040
We cover all of AWS's most important ML&AI announcements, including SageMaker Groundtruth,

01:21.040 --> 01:27.520
reinforcement learning and Neo, DeepRacer, Inferentia, and ElasticInference, ML Marketplace

01:27.520 --> 01:31.320
Personalized, Forecast, TextTrack, and more.

01:31.320 --> 01:35.120
We don't do these kinds of discussions very often on the show, but I always enjoy it when

01:35.120 --> 01:38.360
we do, and I hope you do too.

01:38.360 --> 01:43.960
Before we dive in, I'm at Neurips this week in Montreal, and CubeCon next week in Seattle,

01:43.960 --> 01:47.840
and I'd love to connect with any listeners and attendants or in the area.

01:47.840 --> 01:52.560
Feel free to shoot me a message via at Sam Charington on Twitter via email or the Twimble

01:52.560 --> 01:57.920
AI website, or, if you see me, don't be afraid to say hi.

01:57.920 --> 02:02.800
If you're heading to Neurips, look for the listener meetup and AI platform's meetup topics

02:02.800 --> 02:04.880
I've posted in the Hooba app.

02:04.880 --> 02:06.240
See you around.

02:06.240 --> 02:08.400
And now, onto the show.

02:08.400 --> 02:14.240
Hey, what's up, everyone?

02:14.240 --> 02:21.120
I am here at ReInvent, and this is becoming a bit of a tradition, isn't it Dave?

02:21.120 --> 02:22.720
It is indeed.

02:22.720 --> 02:29.360
And the tradition that I will unveil is our second annual ReInvent Roundup Roundtable,

02:29.360 --> 02:35.320
in which we discuss all of the cool things that happened here at ReInvent, all of the

02:35.320 --> 02:37.720
announcements, etc.

02:37.720 --> 02:44.360
And so before, or as kind of our first step in jumping in, I would like the two of you

02:44.360 --> 02:52.840
to introduce yourselves, and I guess Dave, since you are our veteran panelist, why don't

02:52.840 --> 02:53.840
you go first?

02:53.840 --> 02:56.480
Certainly, my name is Dave McCrory.

02:56.480 --> 03:06.480
I am the VP of Engineering for WISEIO, a division of GE Digital, that works on machine

03:06.480 --> 03:09.480
learning and industrial internet of things.

03:09.480 --> 03:10.480
Awesome.

03:10.480 --> 03:11.480
And Val.

03:11.480 --> 03:12.480
Yes, I am.

03:12.480 --> 03:16.480
And Dave, it's my honor to be here on the second Roundup ReInvent Roundup.

03:16.480 --> 03:19.680
My name is Val Burkivichi, and it's very wise if you say I'm going to let me pronounce

03:19.680 --> 03:24.080
my name first, because it always ships up.

03:24.080 --> 03:25.880
Always ships up some people.

03:25.880 --> 03:29.440
I am CEO and co-founder of pencil data.

03:29.440 --> 03:33.240
And our tagline is, we tamper-proof digital transformation.

03:33.240 --> 03:36.280
But that has a lot of deeper meanings as well.

03:36.280 --> 03:40.600
Before that, some people might remember me from your podcast when I was CTO of Net

03:40.600 --> 03:42.600
App and Solid Fire.

03:42.600 --> 03:48.800
And you know, I was thinking earlier about what is always so cool for me about ReInvent

03:48.800 --> 03:53.640
is that I get to see so many people that I've known for so long.

03:53.640 --> 03:58.160
Were both of you guys also at the very first ReInvent?

03:58.160 --> 04:02.240
I was not at the first, but I think I've been at every ReInvent since.

04:02.240 --> 04:03.240
Okay.

04:03.240 --> 04:04.240
I think I'm a lot like Dave.

04:04.240 --> 04:08.080
I have to check my records. I know I lobbied hard at Net App to be at the first one.

04:08.080 --> 04:11.480
I don't remember whether we succeeded the first year or that second year for sure.

04:11.480 --> 04:12.480
Okay.

04:12.480 --> 04:13.480
Nice.

04:13.480 --> 04:16.920
I was at the first and second, and I think I missed one or two in between, and I've

04:16.920 --> 04:19.480
been at the last three, I think there are.

04:19.480 --> 04:20.640
I don't know.

04:20.640 --> 04:27.280
I think we do not have enough time to talk about everything that was unveiled and announced

04:27.280 --> 04:28.880
here at ReInvent.

04:28.880 --> 04:34.880
And of course, we'll want to kind of wait our discussion points a little bit to the

04:34.880 --> 04:43.800
ML and AI side of things, but maybe a good way to start is to just ask each of you, what

04:43.800 --> 04:49.760
are you most excited about based on what you heard announced here at ReInvent?

04:49.760 --> 04:55.320
In my opinion, there are a couple of things that are really exciting.

04:55.320 --> 05:06.360
I think the Lambda layers function, which really comes down to being able to run different

05:06.360 --> 05:12.680
runtimes on Lambda is probably the biggest and most exciting thing to me.

05:12.680 --> 05:18.960
I think that's going to have a profound impact on the use of Lambda and the growth of Lambda,

05:18.960 --> 05:25.720
but I see it plugging into a lot of the machine learning kind of problems that we had that

05:25.720 --> 05:31.960
were much harder to use Lambda pre-being able to bring your own libraries, dependencies

05:31.960 --> 05:33.640
and execution environments.

05:33.640 --> 05:35.280
I think that's big.

05:35.280 --> 05:44.440
I also think it really adds, I guess, another layer of difficulty if you're a platform

05:44.440 --> 05:51.160
is a service vendor, not that there weren't already some difficulties in some forms or

05:51.160 --> 05:55.080
others, but I think layers definitely adds to that.

05:55.080 --> 06:02.880
So is layers the ability to support multiple or arbitrary languages for Lambda functions

06:02.880 --> 06:05.000
or is it more than that?

06:05.000 --> 06:10.520
It is indeed the ability to do the runtimes, so that would be safe if you wanted to run

06:10.520 --> 06:19.360
or go or Python or Java or pick your favorite language, Rust, it doesn't matter.

06:19.360 --> 06:24.320
But then along with that, being able to have the libraries and dependencies that go with

06:24.320 --> 06:30.400
that, so that formerly, if you wanted to run a Lambda, it was very, very basic code that

06:30.400 --> 06:33.720
you were essentially just gluing two things together with.

06:33.720 --> 06:38.840
You might have something trigger, something else, et cetera, but it was kind of limited

06:38.840 --> 06:42.280
from that perspective, very scalable, but also very limited.

06:42.280 --> 06:49.400
With layers, you end up with so many different abilities that you wouldn't have before, prior

06:49.400 --> 06:57.440
with just regular Lambda, and I would say that makes it incredibly attractive in my mind.

06:57.440 --> 07:04.720
You can now do much more complex things even within the Lambda functions themselves because

07:04.720 --> 07:07.960
of the ability to do this.

07:07.960 --> 07:14.520
What you still cannot do is directly tie it to persistence, so there's still no guarantee

07:14.520 --> 07:16.520
of persistence.

07:16.520 --> 07:20.040
If that makes sense, that's kind of the one missing thing.

07:20.040 --> 07:23.800
Other than that, I guess sky's the limit, so to speak.

07:23.800 --> 07:28.320
They also added some other functionality, but the layers is the most exciting to me.

07:28.320 --> 07:32.680
I'd say the other thing, since I mentioned that there was more than one, the other thing

07:32.680 --> 07:36.200
was the Kafka streaming capability.

07:36.200 --> 07:43.800
But now, basically AWS will run Kafka for you, so Amazon Managed Streaming for Kafka is

07:43.800 --> 07:49.720
pretty exciting in my mind, being able to just use Kafka instead of having to set it

07:49.720 --> 07:53.160
up, configure it, manage it, and maintain it.

07:53.160 --> 07:54.360
That's pretty exciting.

07:54.360 --> 08:01.320
If I was someone that was running Confluent Kafka on my own, and I used AWS, this would

08:01.320 --> 08:06.600
really make me pause and think about, do I want to keep doing that, or would I rather

08:06.600 --> 08:08.720
just let Amazon do it?

08:08.720 --> 08:18.080
Kafka is a big part of a lot of data pipelines, but I also recently interviewed a woman

08:18.080 --> 08:25.040
named Lima Nasseri from Comcast on the podcast to build out a data pipeline for a recommendation

08:25.040 --> 08:27.600
system using Lambda.

08:27.600 --> 08:32.480
I thought it was really cool that her team was able to take advantage of Lambda functions

08:32.480 --> 08:38.800
to do data transformations and things like that, so I think that'll be increasingly part

08:38.800 --> 08:45.320
of the machine learning toolkit and not just the traditional app dev toolkit.

08:45.320 --> 08:50.400
That's right, and Nasseri has personalized, so for me, if we take us all the way back

08:50.400 --> 08:57.240
to Wednesday yesterday, I'm thinking of Andy's keynote, and we were talking about the

08:57.240 --> 09:01.680
very first couple of reinvents where the big announcement was a new instance type, or some

09:01.680 --> 09:07.800
new lower pricing for EC2, or RS3, et cetera, and we've come such a long way since then

09:07.800 --> 09:11.760
where the major highlights, even though I think there were some pretty cool storage highlights

09:11.760 --> 09:15.760
for me, were definitely all the machine learning introductions.

09:15.760 --> 09:20.680
There's so many, but I think my personal favorite is the SageMaker Ground Truth, but that's

09:20.680 --> 09:24.400
one of the things I wanted to have when I first got into machine learning, and I just couldn't

09:24.400 --> 09:26.160
believe it wasn't there.

09:26.160 --> 09:29.840
But I don't know if you guys might be more current with the Google Suite or the Google

09:29.840 --> 09:34.680
Cloud portfolio than me, but I'm not sure if they have a similar service yet, so it just

09:34.680 --> 09:39.840
strikes me as AWS continues to just be more in tune with what their customers really

09:39.840 --> 09:43.480
want, and they're executing and delivering really well.

09:43.480 --> 09:52.640
And so SageMaker Ground Truth is a new offering that basically allows users to create these labeling

09:52.640 --> 09:56.280
pipelines.

09:56.280 --> 10:02.600
And how do you want to go through the details of that, how, how, okay.

10:02.600 --> 10:07.200
So such a big part of an average data scientist's work, if there is such a thing as an average

10:07.200 --> 10:11.880
data scientist, is actually setting up supervised training, right?

10:11.880 --> 10:16.480
That supervised learning that's effectively what most machine learning is nowadays, and

10:16.480 --> 10:22.520
there's a very manual human dependency ironically on that part, which is to actually label data.

10:22.520 --> 10:27.120
So a lot of the captures and recaptures that we see sometimes are asked to identify objects

10:27.120 --> 10:32.080
on a four by four frame of pictures, is us being mechanical turks of doing labeling

10:32.080 --> 10:38.120
for other people to label, you know, features in an image and so forth, the ability to apply

10:38.120 --> 10:43.800
machine learning intelligently, and use inference to automatically label the huge portion

10:43.800 --> 10:46.840
of these data sets now without all that manual effort.

10:46.840 --> 10:51.160
I think it's a huge step forward in the productivity of data scientists and the ability

10:51.160 --> 10:54.720
to generate, you know, more knowledge models going forward.

10:54.720 --> 10:59.800
It's part of that theme, I think Samu and I discussed yesterday, which is that AWS ironically

10:59.800 --> 11:05.040
better than Google at the moment is doing a really good job of democratizing AI, democratizing

11:05.040 --> 11:10.400
machine learning for people, democratizing the data science field itself.

11:10.400 --> 11:13.640
And that's, that's good for the industry, right, that brings more people into it.

11:13.640 --> 11:16.840
It brings more projects forward.

11:16.840 --> 11:20.200
It promotes more knowledge models, and I, you know, I've got a selfish motivation as well

11:20.200 --> 11:25.280
and that my company helps with the reproducibility of all that once we have more candidates

11:25.280 --> 11:27.960
for, you know, promotion into production.

11:27.960 --> 11:34.560
The thing that I found most interesting about ground truth is that it's the labeling,

11:34.560 --> 11:39.560
you know, it kind of ties into these third party labeling services.

11:39.560 --> 11:45.560
So mechanical turk is one example, but they, I think they announced like seven different

11:45.560 --> 11:53.200
partners, including figure eight, but it's not just that there's a pipeline that includes

11:53.200 --> 11:54.280
active learning.

11:54.280 --> 12:04.120
So they are optimizing what needs to be labeled basically there, they're going to choose

12:04.120 --> 12:09.400
what needs to be manually labeled according to some optimization function as opposed to

12:09.400 --> 12:17.000
randomly at the idea being that you kind of manage and keep low the costs of the labeling

12:17.000 --> 12:18.000
process.

12:18.000 --> 12:19.000
That is pretty cool.

12:19.000 --> 12:20.000
Yeah.

12:20.000 --> 12:23.000
That's by far the exciting part to me, you know, the fact that there's better integration

12:23.000 --> 12:27.440
with mechanical turks is an important sort of workflow improvement.

12:27.440 --> 12:32.000
But the fact that we're finally being able to sort of, you know, go, go native, if you

12:32.000 --> 12:36.960
will, or eat our own dog food and actually automate the function of labeling by and large

12:36.960 --> 12:41.640
is, you know, even if it's not a complete solution today, the fact that that's the goal

12:41.640 --> 12:45.280
and the fact that we have it initial operating is very exciting to me.

12:45.280 --> 12:50.640
So now I've got to say you surprised me here, given what your company's focused on nowadays,

12:50.640 --> 12:57.520
I had a thought that your favorite announcement was one of a couple of others.

12:57.520 --> 13:02.440
Well, there's definitely on the blockchain side there was, there was the expected, right?

13:02.440 --> 13:08.120
So AWS is kind of playing catch up to IBM and Oracle with regards to offering managed

13:08.120 --> 13:13.360
hyper ledger blockchains, but also having the nice option of managed Ethereum blockchain,

13:13.360 --> 13:15.040
which I thought was super cool.

13:15.040 --> 13:19.600
The wild card, which I think has got a lot of people buzzing, definitely myself included,

13:19.600 --> 13:24.000
is QLDB, the, I'm not sure why they named it quantum ledger, but certainly the quantum

13:24.000 --> 13:26.920
ledger database is super exciting.

13:26.920 --> 13:32.040
And it actually corresponds to talks I've been giving it or Riley this year around has

13:32.040 --> 13:37.800
this very famous blog in the, just before the peak of the, the cryptocurrency media called

13:37.800 --> 13:43.800
a fat protocol blog, which argued that there'd be a lot of network effect value in a protocol

13:43.800 --> 13:49.720
more than the apps and that was before there were 2,000 versions of Ethereum, so, 3,000 potential

13:49.720 --> 13:50.720
network effects.

13:50.720 --> 13:55.480
So the sad protocol blog was brilliant, you know, in 2016, but did not stand a test of

13:55.480 --> 13:56.480
time.

13:56.480 --> 13:59.880
And at least the enterprise reality is very much that, you know, there will be a few

13:59.880 --> 14:04.920
key platforms, blockchain, and secure ledger platforms, such as QLDB, that enterprise

14:04.920 --> 14:08.640
is standard, a standardized on, but there won't be 2,000 of them.

14:08.640 --> 14:12.680
And that the network effect will indeed go to the platform and the application layers

14:12.680 --> 14:14.840
where the value is closer to the user.

14:14.840 --> 14:21.240
Well, it's my understanding that the quantum and QLDB comes from the name of the internal

14:21.240 --> 14:28.760
AWS system that they kind of extracted this from, whether, you know, encode or in idea.

14:28.760 --> 14:34.360
And what's so exciting about that relative to, you know, the managed blockchain that,

14:34.360 --> 14:37.640
you know, folks kind of expected them to come out with.

14:37.640 --> 14:41.360
So what most people don't realize when you're talking about a managed blockchain service

14:41.360 --> 14:46.440
is it addresses 2 of the 3 impediments towards adoption and enterprise.

14:46.440 --> 14:51.200
That first one being sort of the design and creation or building of a blockchain.

14:51.200 --> 14:55.480
And the second one, which is very important after that, the ongoing operations and management

14:55.480 --> 14:57.240
of that blockchain.

14:57.240 --> 15:01.400
But the third one that, you know, the actual buyers or, you know, budget owners of these

15:01.400 --> 15:06.360
technologies, the executives that would want to see results that isn't addressed by this

15:06.360 --> 15:10.520
or any other managed blockchain service is the usability of the blockchain.

15:10.520 --> 15:16.080
It's actually tying into your application that's actually seeing either, you know, operational

15:16.080 --> 15:20.720
efficiencies or greater compliance or ideally even, you know, new business models that were

15:20.720 --> 15:22.240
impossible before.

15:22.240 --> 15:26.920
And that's the part that, you know, wasn't announced really by a managed blockchain service.

15:26.920 --> 15:32.520
But they inched a lot closer with QLDB, where now you've got, you know, a fully ready-made

15:32.520 --> 15:34.160
solution to just consume.

15:34.160 --> 15:38.360
It's not about thinking how you would build QLDB or how you'd manage it.

15:38.360 --> 15:42.240
Obviously, AWS is greater than president doing that for you.

15:42.240 --> 15:43.520
It's ready for consumption.

15:43.520 --> 15:46.600
So that's what excited me very, very much about QLDB.

15:46.600 --> 15:52.080
And it certainly just is another instantiation of soon to be popular service that validates

15:52.080 --> 15:53.400
the market to pencil data.

15:53.400 --> 15:54.400
Right.

15:54.400 --> 15:55.400
Right.

15:55.400 --> 16:01.200
So for me, and I guess I'm sure you guys felt like this as well, it's kind of like a kid

16:01.200 --> 16:04.560
in the candy store, so many cool new things.

16:04.560 --> 16:06.960
It's really hard to pick just one.

16:06.960 --> 16:12.320
But if I really, really, really had to pick just one, the thing that I'm super excited

16:12.320 --> 16:18.120
about is the SageMaker reinforcement learning announcement.

16:18.120 --> 16:24.160
And I've been a fan of and talking about and learning about reinforcement learning for

16:24.160 --> 16:29.480
a bit now, folks on the podcast have heard a lot of conversations about that.

16:29.480 --> 16:37.120
But what is always part of those conversations is how hard it is and how, you know, unstable

16:37.120 --> 16:43.560
the algorithm training processes, how difficult it is to get the right reward function, all

16:43.560 --> 16:52.600
of these things, and how hard it is to ultimately apply it in a real enterprise environment.

16:52.600 --> 17:00.960
And we've seen folks take swipes at making that easier, bonsai, has been a sponsor of

17:00.960 --> 17:08.200
this podcast that in our industrial AI series last year, and they were acquired by Microsoft

17:08.200 --> 17:10.120
because they did such a great job at this.

17:10.120 --> 17:15.440
But AWS has such a massive presence in the marketplace.

17:15.440 --> 17:21.000
There's a real opportunity for them to bring a lot of attention to reinforcement learning.

17:21.000 --> 17:26.960
And as a result, kind of start this virtuous cycle of, you know, more eyes on it, you

17:26.960 --> 17:34.760
know, lower the impediments to doing it and make it much more easy to extract value out

17:34.760 --> 17:35.760
of it.

17:35.760 --> 17:37.440
So I'm super excited about that.

17:37.440 --> 17:43.880
It's an ascension to SageMaker, which is their kind of data science as a service platform.

17:43.880 --> 17:50.440
It supports a wide variety of 2D and 3D simulation physics environments, including open

17:50.440 --> 17:56.880
gym and Sumerian, which is their 3D environment, RoboMaker, which is another one of their

17:56.880 --> 18:02.080
announcements, Roz, which is the robotics operating system.

18:02.080 --> 18:04.760
It's got a lot of the pieces in place.

18:04.760 --> 18:10.800
You know, the one thing that's kind of interesting about this relative to AWS is typical MO.

18:10.800 --> 18:15.760
They tend to be, you know, this is a lot further out in front, I think, than they tend to like

18:15.760 --> 18:16.760
to be, right?

18:16.760 --> 18:22.280
They kind of pride themselves on being very responsive and only building stuff when like

18:22.280 --> 18:25.440
real customers are asking for it.

18:25.440 --> 18:29.800
And oftentimes commoditizing something that already exists, right?

18:29.800 --> 18:31.880
Like S3, we knew what storage was.

18:31.880 --> 18:33.800
They didn't invent that.

18:33.800 --> 18:37.840
EC2, they, you know, made servers more accessible.

18:37.840 --> 18:41.600
Yeah, they're not inventing reinforcement learning either.

18:41.600 --> 18:44.720
So I don't know that this analogy, you know, fits very well.

18:44.720 --> 18:48.920
But it's certainly, we're certainly way out ahead of the market in some ways.

18:48.920 --> 18:54.760
And hopefully, you know, what I'm excited about is just accelerating that process because

18:54.760 --> 18:59.200
the challenge with deep learning is that it's so data-intensive, labeled data-intensive

18:59.200 --> 19:06.480
and reinforcement learning plus simulation promises to fix that for a good many applications.

19:06.480 --> 19:12.040
And this is a quick question I'd have for you, Sam, is, you know, empirically I'm seeing

19:12.040 --> 19:17.120
a ton of just, you know, image recognition, text and voice recognition and the related

19:17.120 --> 19:19.520
neural networks that correspond with that.

19:19.520 --> 19:24.680
I haven't seen a lot of successful production, you know, transfer learning or reinforcement

19:24.680 --> 19:26.600
learning projects in place.

19:26.600 --> 19:30.560
You know, do you think that would kickstart that and would open up a new front, new frontier

19:30.560 --> 19:33.760
actually in the home machine learning world?

19:33.760 --> 19:38.760
So I would say, you know, I would kind of pick apart transfer and reinforcement learning.

19:38.760 --> 19:44.560
I think transfer learning is one of those things that people maybe don't talk about as much.

19:44.560 --> 19:50.000
It's kind of receded into the background, but it is, I think foundational to a lot of

19:50.000 --> 19:56.920
what folks are doing in the image domain, like it's very popular to pick up a pre-trained

19:56.920 --> 19:58.440
image net model.

19:58.440 --> 20:03.760
And if not use that out of the gate, like tweak that, use, you know, or fine tune it,

20:03.760 --> 20:10.920
they'll say, and kind of start with that to avoid having to train everything from scratch.

20:10.920 --> 20:16.920
And in fact, there's a lot of work that's been done recently by Jeremy Howard and Sebastian

20:16.920 --> 20:22.200
Rooter, who have both been on the podcast relatively recently to apply that same transfer

20:22.200 --> 20:27.040
learning concept to natural language processing.

20:27.040 --> 20:34.040
But reinforcement learning, on the other hand, like it's still largely the domain of like

20:34.040 --> 20:39.920
playing video games, it's like the thing that research labs are pursuing as a stepping

20:39.920 --> 20:48.200
stone to general artificial intelligence, the only folks that very, I've seen very few

20:48.200 --> 20:54.040
folks applying deep reinforcement learning in a kind of commercial enterprise business

20:54.040 --> 20:58.040
kind of way, the bonsai again being the one.

20:58.040 --> 21:05.440
Are you talking about, so you changed it to deep reinforcement learning versus reinforcement

21:05.440 --> 21:06.440
learning?

21:06.440 --> 21:11.640
And we also touched on the idea of transfer.

21:11.640 --> 21:16.040
I tried to be careful there because, yes, reinforcement learning does have a history

21:16.040 --> 21:22.960
that goes back to, I don't know when, I think the 60s, like it's been around for a while

21:22.960 --> 21:30.320
and it's been used as a kind of standard optimization algorithm.

21:30.320 --> 21:33.920
I think it's related to like multi-arm bandits and things like that.

21:33.920 --> 21:39.920
I don't know a ton about it and how it's been used, but what's different is deep reinforcement

21:39.920 --> 21:41.000
learning.

21:41.000 --> 21:45.760
It's promised, but also the challenge is associated with making it work.

21:45.760 --> 21:46.760
Got it.

21:46.760 --> 21:51.840
And I've seen a lot of Elon Musk headlines around how open AI is beating all sorts of professional

21:51.840 --> 21:55.800
e-sports gamers and stuff like that, but to your point, I haven't seen a lot of commercial

21:55.800 --> 21:57.280
applications ever yet.

21:57.280 --> 21:58.280
Yeah.

21:58.280 --> 22:05.960
And if Amazon scale says anything, maybe we'll see that change over the next couple years.

22:05.960 --> 22:11.520
I did want to hit on something interesting since we were talking about different things

22:11.520 --> 22:17.120
that we believed were the most significant.

22:17.120 --> 22:24.800
I've been kind of perusing and I know I had mentioned as my favorite and most impactful

22:24.800 --> 22:34.240
being the Lambda Layers functionality, but I did kind of a interesting perusal on Twitter

22:34.240 --> 22:43.000
and it appears that the most popular announced thing out there is the outpost announcement

22:43.000 --> 22:44.680
out of all of them.

22:44.680 --> 22:54.800
And by a fairly large margin, like three-ish X roughly compared to everything else, actually

22:54.800 --> 22:55.800
that's not true.

22:55.800 --> 23:05.480
Looking, the runtimes to Lambda is about, let's say, two thirds as popular as the outpost

23:05.480 --> 23:06.480
announcement.

23:06.480 --> 23:11.040
Although taking into account it was announced six hours ago, maybe it'll surpass it,

23:11.040 --> 23:14.240
but outpost is pretty darn popular.

23:14.240 --> 23:21.040
And I think it is pretty significant both for ML and such and outside of ML, really.

23:21.040 --> 23:25.640
The other announcement that we didn't talk about that I do think has big impact for ML

23:25.640 --> 23:30.920
is the, and I don't remember the name of it now, but it's the elastic something that

23:30.920 --> 23:38.120
you can attach a GPU to an EC2 instance, I see that is pretty big as well.

23:38.120 --> 23:44.200
So let's start there and then kind of work our way back to outpost.

23:44.200 --> 23:48.600
I was in a conversation with someone yesterday, maybe it was you now, when we were talking

23:48.600 --> 23:53.040
about what was kind of, what was our most exciting, what were the things we were most exciting

23:53.040 --> 23:54.040
about.

23:54.040 --> 23:59.880
And in that conversation, I prefaced it by, there's the thing I'm kind of geeked about,

23:59.880 --> 24:04.440
kid in a candy store style, and the thing I think is going to be most impactful.

24:04.440 --> 24:08.760
And that thing is the elastic inference.

24:08.760 --> 24:15.360
So Andy, Jesse, I heard a couple of numbers thrown around, and in fact, this was a question

24:15.360 --> 24:23.240
that I was asking folks to share data points on Twitter several weeks ago, but the data

24:23.240 --> 24:31.120
points that AWS folks were throwing around were that anywhere from 10 to 20%, 10 to 30%

24:31.120 --> 24:42.320
of the cost of doing machine learning is training, and 70 to 90% of the costs is inference.

24:42.320 --> 24:49.560
And the example that someone pointed me to when I was first looking for this, that's really

24:49.560 --> 25:01.640
hit home for me was Google, it turns out that their investment in the TPU building chips

25:01.640 --> 25:08.920
and as much as they're investing in that now on multiple versions of these chips, that

25:08.920 --> 25:13.200
all started when Jeff Dean kind of did this back of the envelope calculation that said

25:13.200 --> 25:20.400
that our user base does just three seconds of voice search per day, the inference would

25:20.400 --> 25:27.160
basically eat us alive and have us needing to build some multiple of them needing to build

25:27.160 --> 25:33.200
some multiple of the number of data centers that they already had.

25:33.200 --> 25:43.720
And so attacking that inference costs and AWS is throwing down numbers like 75% reductions

25:43.720 --> 25:52.640
in inference costs is potentially quite huge for folks that are doing ML and a cloud.

25:52.640 --> 25:58.080
Yeah, my question actually curiosity more than question is I always have this mental

25:58.080 --> 26:02.440
model of training being done in the cloud, but inference at the edge and a bunch of the

26:02.440 --> 26:06.520
use cases for customers who are working with autonomous vehicles, driverless taxis and

26:06.520 --> 26:08.440
so forth are very much that.

26:08.440 --> 26:12.360
But do you have any idea what the percentage is of inference if there is a breakdown roughly

26:12.360 --> 26:15.800
between cloud and edge?

26:15.800 --> 26:16.800
That is a good question.

26:16.800 --> 26:19.000
I don't have any data.

26:19.000 --> 26:26.960
I think what everyone's excited about for edge is that even if whatever the number is

26:26.960 --> 26:32.320
today, there's kind of, you know, whenever you see like somebody joked about it on did Andy

26:32.320 --> 26:38.080
Jesse joke about this on stage or maybe it was at the analyst summit, but like you always

26:38.080 --> 26:44.080
see this exponential curve in the number of devices that are going to be out in the edge.

26:44.080 --> 26:50.760
And there is a presumption that a lot of inference is going to be moving to the edge for, you

26:50.760 --> 26:54.600
know, latency and bandwidth reasons and all other things.

26:54.600 --> 27:01.520
And in fact, they had an announcement there, I think, yeah, actually so it's SageMaker

27:01.520 --> 27:03.240
Neo did.

27:03.240 --> 27:05.480
I don't think it showed up in the keynote.

27:05.480 --> 27:07.520
Did you guys catch that one?

27:07.520 --> 27:08.520
I did not.

27:08.520 --> 27:10.520
I did not see it in the keynote.

27:10.520 --> 27:13.920
I don't remember seeing it anywhere.

27:13.920 --> 27:21.600
So SageMaker Neo is a essentially it's, it's a, they're open sourcing it.

27:21.600 --> 27:25.080
It's an open source model compiler.

27:25.080 --> 27:32.800
And so they kind of pitching it as this train wants run anywhere, but they've built in some,

27:32.800 --> 27:41.720
some deep optimization into this model compiler so that they're touting two X performance and

27:41.720 --> 27:51.760
one tenth the size for your models and they're targeting a wide variety of hardware targets

27:51.760 --> 28:00.240
including edge devices and some of these like compute and memory constrained device targets.

28:00.240 --> 28:01.520
That one should be pretty cool.

28:01.520 --> 28:06.040
And it kind of begs the question to Dave's point, you know, how low or how far can they

28:06.040 --> 28:08.040
take out post and future generations?

28:08.040 --> 28:12.480
And they bring it down to you know Raspberry Pi style and maybe not the elasticity of it

28:12.480 --> 28:16.880
still have the exact same control and management flow that you'd enjoy in a cloud.

28:16.880 --> 28:19.520
So I never thought of outpost quite like that.

28:19.520 --> 28:25.680
So maybe we should jump into outpost and what it is and why are folks excited about it.

28:25.680 --> 28:27.240
I think that's a good idea.

28:27.240 --> 28:34.440
I think it's going to be the bridge that gets people that that had reasons why they needed

28:34.440 --> 28:37.680
physical equipment on premises.

28:37.680 --> 28:44.960
This doesn't give them the excuse that Amazon is only available in Amazon data centers.

28:44.960 --> 28:53.960
You now have flexibility to have equipment running on premises that is the same as what

28:53.960 --> 29:02.880
AWS runs in its data centers will to you all on site as well as offering the capability

29:02.880 --> 29:06.080
to run and be either in the cloud or on site.

29:06.080 --> 29:08.880
I see that as a pretty big deal.

29:08.880 --> 29:10.480
It really isn't.

29:10.480 --> 29:15.040
You know, here's a couple of the maybe non intuitive questions that come to mind for me is

29:15.040 --> 29:20.400
how are Amazon and partners like we are going to prove compliance?

29:20.400 --> 29:25.120
You know with data, the necessity laws or other privacy laws internationally with regards

29:25.120 --> 29:29.800
to what kind of metadata and what kind of control, you know, really flows back and forth

29:29.800 --> 29:34.800
between one of their regions and an on-prem implementation of outpost.

29:34.800 --> 29:36.840
I'd love to get more details on that.

29:36.840 --> 29:46.400
Well, one of the things that was not in abundance around this announcement was details.

29:46.400 --> 29:49.600
What we know is that this is AWS Design Hardware.

29:49.600 --> 29:55.440
It's the same hardware they say that runs in their data centers, but it's delivered and

29:55.440 --> 29:59.960
installed on the customer premises.

29:59.960 --> 30:05.520
There are supposedly two flavors of it.

30:05.520 --> 30:09.440
What's not clear at all are the hardware specs, and they said they're not talking about

30:09.440 --> 30:11.160
that until next year.

30:11.160 --> 30:14.680
There are two kind of flavors of it.

30:14.680 --> 30:21.680
One of them is, you know, runs as VMware kind of flavor of AWS.

30:21.680 --> 30:26.600
And the other is kind of traditional EC2 instances.

30:26.600 --> 30:32.520
So it appears like an extension of your AWS Availability Zone.

30:32.520 --> 30:33.720
That begs my questions.

30:33.720 --> 30:37.760
Yeah, I would want to feel like as a developer is an operator, I'd want to operate like

30:37.760 --> 30:38.760
that.

30:38.760 --> 30:42.200
But you know, when I think about overseas deployments or even things like China, there's

30:42.200 --> 30:46.040
a lot of those detailed questions that need to be answered to see just how broadly applicable

30:46.040 --> 30:47.040
this could be.

30:47.040 --> 30:52.680
You know, this is not new in the kind of the world of cloud.

30:52.680 --> 30:57.680
Azure Soft has Azure Stack, which is kind of the same idea.

30:57.680 --> 31:04.840
AWS is saying that I guess they talked about EC2.

31:04.840 --> 31:10.560
They talked about cloud formation templates and AMIs being available.

31:10.560 --> 31:15.680
They said that some of the other services like RDS and SageMaker will be supported.

31:15.680 --> 31:21.160
It's not clear when that will be is could take a while.

31:21.160 --> 31:26.760
I'd love to see how much of some of these cool new, you know, Lambda services and runtimes

31:26.760 --> 31:32.480
are available without post as well, because quite naturally, me like most SaaS companies

31:32.480 --> 31:37.720
would love to be able to answer yes when certain customers ask to run your functionality

31:37.720 --> 31:38.960
on Prem.

31:38.960 --> 31:42.840
This offers a promise of that and based on some of the answers to the detailed questions

31:42.840 --> 31:47.960
now or later, this could be game changing in terms of really helping the entire SaaS

31:47.960 --> 31:51.960
industry penetrate deeper into the enterprise, you know, on the back of AWS.

31:51.960 --> 31:56.160
I would argue you could already do that with the green grass.

31:56.160 --> 32:01.040
You can already run Lambda functions and several of the other capabilities green grass

32:01.040 --> 32:06.720
on premises, along with some of the capabilities that Snowball provides.

32:06.720 --> 32:09.040
And that was another announcement that was made.

32:09.040 --> 32:16.240
I don't know if it was pre the conference or not, where the new specs of Snowball, which

32:16.240 --> 32:22.440
were pretty impressive, and that's another thing that you can run on premises along with

32:22.440 --> 32:23.440
green grass.

32:23.440 --> 32:28.600
And the green grass stuff, people are running all sorts of Lambda functions and such already.

32:28.600 --> 32:30.080
And that's cool.

32:30.080 --> 32:35.800
If I could see DynamoDB and S3 and obviously Lambda as we've mentioned, and just a few basic

32:35.800 --> 32:41.160
easy to instance types available on prem with the same control flow and the same developer

32:41.160 --> 32:45.720
interfaces that that's a game changer at least for from my SaaS offering.

32:45.720 --> 32:54.800
So before we kind of run down the rest of the ML and AI announcements, there are a few

32:54.800 --> 33:05.640
candidates for the wackiest new product or new offering or the most unexpected Val, what's

33:05.640 --> 33:07.880
on that list for you?

33:07.880 --> 33:08.880
I wouldn't call it wacky.

33:08.880 --> 33:11.120
I'd call it super geeky cool, which is deep racer.

33:11.120 --> 33:12.120
Right?

33:12.120 --> 33:16.440
I'll put you guys on a spot and say if you've pre-ordered yours already, I think I remember

33:16.440 --> 33:25.680
your answer Sam, not sure, but I have not ordered that although I did, I did order the deep

33:25.680 --> 33:29.760
lens when it was available and got that played with that quite a bit.

33:29.760 --> 33:31.080
Very cool.

33:31.080 --> 33:36.320
The other wacky thing and it belongs in the rumor mill was that Amazon AWS, we're doing

33:36.320 --> 33:40.800
something with all the photos of the attendees that appear on the badges with regards to

33:40.800 --> 33:48.040
just propping up their own image training data sets for their services.

33:48.040 --> 33:49.040
Yeah.

33:49.040 --> 33:52.000
Again, that's just a rumor us on Twitter that caught my attention.

33:52.000 --> 33:53.000
Okay.

33:53.000 --> 33:58.040
I would say wackiest for me is AWS station.

33:58.040 --> 33:59.040
Yes.

33:59.040 --> 34:05.680
And so that caught me off guard.

34:05.680 --> 34:14.000
A fully managed ground station as a service for when you want to launch a satellite.

34:14.000 --> 34:18.200
That I was not seeing, I did not see that one coming at all.

34:18.200 --> 34:23.800
I can finally plan that trip to the Sahara desert.

34:23.800 --> 34:31.000
They also announced RoboMaker, which is this environment for like building robotics applications

34:31.000 --> 34:35.640
that I kind of, I mean, they did some stuff with with with Ross.

34:35.640 --> 34:38.280
Last year, I think.

34:38.280 --> 34:42.480
And so maybe this is an extension of that, but I, you know, they're doing like it's a full

34:42.480 --> 34:47.560
robotics development environment, it's got simulation environment, they're doing fleet

34:47.560 --> 34:48.560
management.

34:48.560 --> 34:54.000
That's that's one of the biggest things that I thought was exciting about that was the

34:54.000 --> 35:01.160
simulator where you can, where you can see not only a single robot and interfacing with

35:01.160 --> 35:07.280
that, but you can simulate that fleet of robots and how they will behave without needing

35:07.280 --> 35:08.640
all that equipment.

35:08.640 --> 35:14.680
I think that's a game changer in and of itself for people doing robotic projects.

35:14.680 --> 35:15.680
Mm-hmm.

35:15.680 --> 35:19.720
And that's actually cool, but ironically, I think that was a Tuesday announcement, right?

35:19.720 --> 35:23.720
So it almost got wrapped up by me.

35:23.720 --> 35:28.120
I'd love to see again whether or not so much reference customers, but just see examples

35:28.120 --> 35:32.840
of how people are applying this, even in small ways, you know, whether it's interesting enough

35:32.840 --> 35:38.360
for, you know, robotic assembly and assembly lines, whether it's helping with logistics

35:38.360 --> 35:44.040
of shipping or other interesting applications, particularly ones where there's a lot of

35:44.040 --> 35:49.320
health hazards to humans and robots could do those tasks much more safely.

35:49.320 --> 35:50.320
Mm-hmm.

35:50.320 --> 35:56.520
Speaking of early announcements that kind of got, you know, swept up and forgotten in the

35:56.520 --> 36:05.320
melee here, there was an interesting announcement in what has now become known as pre-invents,

36:05.320 --> 36:09.000
the, you know, all of the announcements before reinvent.

36:09.000 --> 36:15.760
We've got a name for that now, so as one of the pre-invent announcements, they announced

36:15.760 --> 36:16.760
predictive scaling.

36:16.760 --> 36:18.760
Did anyone catch that?

36:18.760 --> 36:22.320
I think out of the sight of my eye, but I never actually internalized it until you

36:22.320 --> 36:27.080
brought it up right now, that's stupid.

36:27.080 --> 36:30.120
Was that tied to some of the automated tiering in S3 that they announced?

36:30.120 --> 36:31.920
Ah, so that's another interesting one.

36:31.920 --> 36:40.240
So I don't know, you know, maybe kind of ideologically tied, well, definitely ideologically tied,

36:40.240 --> 36:41.560
and that's what's kind of interesting here.

36:41.560 --> 36:47.200
So predictive scaling is basically, I'm assuming it works, you know, in conjunction with like

36:47.200 --> 36:53.640
cloud formation and some of the other auto-scaling mechanisms, but the idea is that they've got

36:53.640 --> 37:02.080
machine learning models now that are making predictive scaling decisions as opposed to

37:02.080 --> 37:07.920
the operator having to define a set of rules around, you know, memory or CPU utilization

37:07.920 --> 37:09.760
or what have you.

37:09.760 --> 37:15.920
And so this is particularly interesting for folks that have applications that maybe have

37:15.920 --> 37:23.680
a long warm up time or take a long time to, you know, stand up for whatever reason.

37:23.680 --> 37:28.880
Yeah, I was thinking, yeah, Lambda cold start, right, that sounds like an ideal application

37:28.880 --> 37:30.480
for that.

37:30.480 --> 37:37.760
And so this, the S3 thing, I forget what that one is called, but it's similar like they're

37:37.760 --> 37:47.920
going to be predictive tiering or something like particularly staging data across these.

37:47.920 --> 37:51.800
Storage tiering is like a good decade, 15 year old technology that's been applied, you

37:51.800 --> 37:57.760
know, particularly in the days of high cache systems like the MC Symmetrics when it first

37:57.760 --> 38:02.920
came out and then in the area of flash now with memory tiers and flash and disk tiers.

38:02.920 --> 38:09.480
But they've always been fairly, you know, static models or very simplistic algorithms.

38:09.480 --> 38:13.280
And I think some people are trying to, you know, I remember, because it's not hopefully

38:13.280 --> 38:16.600
not confidential and it's been more than 12 months at NetApp, we were definitely trying

38:16.600 --> 38:20.920
to do some more clever things with regards to on tap caching, but this seems to take

38:20.920 --> 38:24.720
it to a whole other level because of obviously the resources available for both the training

38:24.720 --> 38:29.760
and the inference to apply, you know, the right decision to whichever data needs to move

38:29.760 --> 38:30.920
between tiers.

38:30.920 --> 38:38.120
So these two announcements collectively for me are a strong candidate for kind of a most

38:38.120 --> 38:46.320
exciting thing at re-invent and it's the idea that machine learning is becoming part

38:46.320 --> 38:52.600
of the infrastructure, right, part of the way that we're able to deliver, you know, scalable

38:52.600 --> 38:53.600
infrastructure.

38:53.600 --> 38:58.320
So, you know, Jeff Dean, obviously Google does a lot of this and AWS does a lot of this

38:58.320 --> 39:05.320
stuff internally that we don't know so much about, but Jeff Dean at Google, for example,

39:05.320 --> 39:13.240
has been doing talks on this for quite a while about how, you know, we can use machine learning

39:13.240 --> 39:19.560
for a ton of things to make infrastructure more effective, like query optimization and

39:19.560 --> 39:25.400
cache optimization and, you know, predictive scaling and tiering.

39:25.400 --> 39:30.400
But I think this is just the beginning of this wave.

39:30.400 --> 39:33.440
And that's one of my favorite topics actually.

39:33.440 --> 39:37.000
Probably remember, I think the title of Jeff's paper was the case for learned indexes.

39:37.000 --> 39:41.880
It was actually a eco-authored, it was a bunch of other researchers, but it was fascinating

39:41.880 --> 39:49.360
that it ties into a blog Andre Carpathy from Tesla Rotor on software 2.0 and how, you

39:49.360 --> 39:54.440
know, incredibly valuable, you know, web properties such as the actual Google homepage, the

39:54.440 --> 40:00.520
search page is anywhere between 15 to 20 percent machine-generated code now, no longer

40:00.520 --> 40:02.520
human-generated code.

40:02.520 --> 40:06.680
And you know, more to Jeff's point, you're able to take a lot of these low-level routines

40:06.680 --> 40:12.480
with a storage or compute resource management index management and so forth and be better

40:12.480 --> 40:17.360
at it with thousands of instances of machine inference versus just a few smart DBAs or,

40:17.360 --> 40:22.120
you know, other content managers and that's interesting, but also potentially disturbing

40:22.120 --> 40:24.800
trend if it's not managed carefully.

40:24.800 --> 40:30.600
So I'm going to run down these announcements and just jump in if you've got, if it strikes

40:30.600 --> 40:34.640
you as interesting or you've got anything you want to chime in on.

40:34.640 --> 40:40.400
So we talked about SageMaker ground truth, SageMaker reinforcement learning, deep racer,

40:40.400 --> 40:44.480
minus on order, valve is on order.

40:44.480 --> 40:50.200
I wonder, well, by the time this podcast is published, it will be more expensive.

40:50.200 --> 40:54.040
There is a special on these things during re-invent.

40:54.040 --> 41:01.960
So I tweeted about it, hopefully you saw that, if you're interested, EC2, P3, DN instances,

41:01.960 --> 41:06.120
one of you guys referred to this earlier.

41:06.120 --> 41:07.120
Yes.

41:07.120 --> 41:08.120
Yes.

41:08.120 --> 41:17.520
So this is the AWS's largest compute instance, P3, but with now 100 gig networking and VD

41:17.520 --> 41:26.440
V100, 32 gigs per GPU, Skylake, VCPUs with the AVX 512 instruction set.

41:26.440 --> 41:34.200
That sounds like a mother of all instance for today for this week.

41:34.200 --> 41:37.720
It's a massive amount of compute power and an instance type that you can spin up on

41:37.720 --> 41:38.720
demand.

41:38.720 --> 41:39.720
Yeah.

41:39.720 --> 41:40.720
Yeah.

41:40.720 --> 41:44.920
It all comes back to the ability to rent a supercomputer right for as little time as you

41:44.920 --> 41:45.920
need.

41:45.920 --> 41:49.360
It's amazing, one of the amazing attributes of cloud overall.

41:49.360 --> 41:51.920
So we talked about elastic inference.

41:51.920 --> 41:58.560
We did not talk about another interesting announcement, AWS Inferentia.

41:58.560 --> 42:00.840
Anyone catch that one?

42:00.840 --> 42:01.840
I did not.

42:01.840 --> 42:03.800
What is that mean either?

42:03.800 --> 42:12.880
AWS Inferentia is their forthcoming high performance machine learning chip.

42:12.880 --> 42:19.800
So this is a product of their acquisition of Annapurna Labs.

42:19.800 --> 42:22.480
It is slated to come out next year.

42:22.480 --> 42:30.640
It is inference, I believe, well, Inferentia is focused on inference, right?

42:30.640 --> 42:37.960
And each of these chips will do hundreds of tensor operations per second.

42:37.960 --> 42:43.360
And you can cluster these chips to scale that to the thousands.

42:43.360 --> 42:49.440
And so there's kind of this interesting one, two punched between elastic inference, which

42:49.440 --> 42:54.560
is, I think this is GA now, right?

42:54.560 --> 43:01.800
And Inferentia, so elastic inference is going to promises to lower inference cost by up

43:01.800 --> 43:04.640
to 75%.

43:04.640 --> 43:11.400
GA when it comes online, they're saying offers further reduction of cost by 10X.

43:11.400 --> 43:12.400
Wow.

43:12.400 --> 43:13.400
Wow.

43:13.400 --> 43:14.400
Wow.

43:14.400 --> 43:17.600
So this actually corresponds to two related announcements this week.

43:17.600 --> 43:23.440
One was obviously support for, I don't know, initially, or just more ARM instance types.

43:23.440 --> 43:26.840
And the other one that was in an Amazon announcement, but just happened the same week, was the

43:26.840 --> 43:30.120
risk five announcement by the Linux Foundation.

43:30.120 --> 43:33.920
And some of the potential opportunities that offers people like Amazon to offer really

43:33.920 --> 43:38.400
interesting customized chips, that's like this for almost any high volume application

43:38.400 --> 43:40.240
with a 10X impact.

43:40.240 --> 43:41.240
Interesting.

43:41.240 --> 43:42.240
Interesting.

43:42.240 --> 43:48.640
And so clearly, AWS is not alone in chasing down this custom inference chip, custom inference

43:48.640 --> 43:49.640
hardware.

43:49.640 --> 43:57.920
We talked about the Google TPU, Microsoft, is there's brain wave, or is that something different?

43:57.920 --> 44:03.280
Yeah, I know they've had, was it, these are certainly custom FPGA with Azure, but maybe

44:03.280 --> 44:07.720
even some custom A6, but am I the only one now that starts to feel like this is the

44:07.720 --> 44:12.440
mainframe error all over again, with everyone running completely custom hardware and some

44:12.440 --> 44:17.600
thin shame of compatibility, maybe at the container level between the clouds themselves.

44:17.600 --> 44:24.840
Well, or the model compiler level, I'll say maker Neo, wait, are we saying that AWS is

44:24.840 --> 44:33.080
becoming IBM that all of them are actually, except for maybe IBM ironically, you're

44:33.080 --> 44:40.240
doing with power, but yeah, I was talking to someone and we were saying that AWS was becoming

44:40.240 --> 44:51.040
more like an IBM in Microsoft of the 90s and that we were seeing, I think we said Microsoft

44:51.040 --> 44:57.880
was behaving more like, like Apple, Apple was behaving, who's Apple behaving, Apple

44:57.880 --> 44:59.760
was behaving more like,

44:59.760 --> 45:09.040
Oracle, Dell, in the 90s, I lived through those errors of Microsoft could kill an industry

45:09.040 --> 45:15.160
with a pre-announced press release and it does seem like Amazon, particularly AWS has

45:15.160 --> 45:19.480
that power now in the tech industry to just announce or pre-announce something in freeze

45:19.480 --> 45:22.240
like an entire new market segment.

45:22.240 --> 45:28.240
I mean, if you look at outpost, I wouldn't be looking to buy hardware to build my new

45:28.240 --> 45:34.840
VMware or a competitive cloud thing until I was able to see what they were going to offer

45:34.840 --> 45:37.560
and what the cost structure and such looked like.

45:37.560 --> 45:41.920
Yeah, at a macro level, you can probably start the countdown clock as to how many more

45:41.920 --> 45:46.720
hardware refresh cycles there will be for people if, as you say, Dave, if the specs and

45:46.720 --> 45:50.280
the price performance match up to expectations.

45:50.280 --> 45:54.600
The machine learning offerings that we ran through are kind of under the broad category

45:54.600 --> 46:01.440
of new stuff and then there's more like improvements and incremental stuff and tools and there

46:01.440 --> 46:04.680
are a bunch of those as well.

46:04.680 --> 46:12.760
So one of those was, they announced earlier in the week, a marketplace for machine learning.

46:12.760 --> 46:20.600
So kind of like an ML app store to anyone that catches your eye, but I'd actually caught

46:20.600 --> 46:21.600
my eye.

46:21.600 --> 46:26.600
I noticed, of course, they weren't the first to do this, but just the fact that in combination,

46:26.600 --> 46:30.880
with integrated workflow for a data scientist and all the other cool tools that are either

46:30.880 --> 46:36.000
maturing, which I think the maturing of all these services announced in prior years might

46:36.000 --> 46:41.160
be the unsung hero of these announcements because they're really usable now and very functional.

46:41.160 --> 46:46.360
But the fact that now you have this kind of marketplace also available on AWS is with

46:46.360 --> 46:50.640
partners as well as natively from the rest of Amazon is very, very cool.

46:50.640 --> 46:56.000
If they do share some of the models they use in the proper Amazon retail side, then that

46:56.000 --> 47:00.360
could also be a game changer with regards to e-commerce everywhere.

47:00.360 --> 47:03.960
One of the things that I thought was interesting about this, if I'm not mixing it up with

47:03.960 --> 47:09.840
another announcement is that it's all based on when I first heard it describe, I thought,

47:09.840 --> 47:16.760
okay, well, how is this different from labeling an amy as machine learning, but this is all

47:16.760 --> 47:18.280
based on containers?

47:18.280 --> 47:24.360
Yeah, I forgot that detail, but it makes it a bit more granular for sure.

47:24.360 --> 47:25.360
Yeah.

47:25.360 --> 47:34.720
So it's interesting, it's based on containers, they've got some ability to, like the partner

47:34.720 --> 47:41.080
amies, you can monetize them, and they've got folks like Figure 8 and Deep Vision, H2O,

47:41.080 --> 47:48.960
H2O and some others that are kind of already part of this new ecosystem.

47:48.960 --> 47:52.720
And yeah, how is this going to tie into the comments you made earlier in the podcast on transfer

47:52.720 --> 47:58.680
learning and the ability to just buy a model, maybe tweak it or not and just apply it to

47:58.680 --> 48:03.560
some other thing and be super productive much early on in a data science cycle?

48:03.560 --> 48:11.800
I mean, I think that's where this heads, I think the interesting thing will be, to me,

48:11.800 --> 48:16.640
can they evolve it where I can see what the specific specs of the performance of this

48:16.640 --> 48:22.680
model are and compare similar models and look at the performance and accuracy that they

48:22.680 --> 48:29.440
offer and decide which one is best for me, and at that point, there's not that much incentive

48:29.440 --> 48:33.920
for me to go out and build my own model from scratch.

48:33.920 --> 48:37.160
How about a recommendation and engine for we just described Dave?

48:37.160 --> 48:38.560
Of course.

48:38.560 --> 48:46.320
I'm like slowly wrapping my head around what this is, and I've talked to folks at AWS

48:46.320 --> 48:48.120
about this yesterday.

48:48.120 --> 48:55.560
I think the thing that caught me up is that the folks that they are rolling out as partners

48:55.560 --> 49:00.280
like TIPCO and figure eight, I think of them as like application and tools and platform

49:00.280 --> 49:05.640
vendors, and so I think that these are like entire systems that we could already put those

49:05.640 --> 49:10.360
in AMIs and spin them up, and so what's the big deal?

49:10.360 --> 49:16.680
But I guess the reason why this container thing is important, going back to what was

49:16.680 --> 49:22.520
clear to you and is sinking in for me is that those are just companies that are publishing

49:22.520 --> 49:32.120
into a model marketplace and repository that you can access from SageMaker as opposed

49:32.120 --> 49:38.520
to like spinning up some new product or spinning up some whole product in an instance.

49:38.520 --> 49:41.040
It's broke-growing algorithms.

49:41.040 --> 49:47.320
So there are folks that play in this space, algorithmia is one, a company who I've got

49:47.320 --> 49:50.560
a lot of respect for, they're doing really interesting things.

49:50.560 --> 49:55.120
It'd be interesting to see, you know, how this plays out for them.

49:55.120 --> 50:02.360
So the next set of announcements are, I think, all around these, like the highest tier

50:02.360 --> 50:14.640
and the ML stack, these cognitive APIs or these AI as a service APIs, and there are a couple

50:14.640 --> 50:18.360
of these that I found really interesting.

50:18.360 --> 50:27.640
So there's one that's called Personalize, and the idea is that it is a recommendation

50:27.640 --> 50:33.200
system kind of as a service, recommendations as a service, and then there's one that's

50:33.200 --> 50:38.160
called Forecasts, and that is Forecasting as a service.

50:38.160 --> 50:43.880
And, you know, their pitch for this is that recommendations and forecasts are both, forecasting

50:43.880 --> 50:50.680
are both these examples of things that are really hard to do generically.

50:50.680 --> 50:58.320
Like your product catalog and data sources, you know, whether it's click stream or logs

50:58.320 --> 51:01.840
or whatever, that's very customized to you.

51:01.840 --> 51:06.400
And likewise, things that you need to forecast are very customized to you.

51:06.400 --> 51:12.400
And so it's very difficult to offer just a generic personalization API or forecasting

51:12.400 --> 51:17.560
API, but, and this is what's really interesting.

51:17.560 --> 51:21.440
And they didn't explain it like this, but it was kind of a light bulb as soon as they

51:21.440 --> 51:23.160
set it for me.

51:23.160 --> 51:29.760
What they've really done with these is like, it's an application level version of AutoML.

51:29.760 --> 51:36.200
Like so what Google did with AutoML is like, it's a machine learning thing, right?

51:36.200 --> 51:42.160
You give it a bunch of images, and, you know, they're going to do architecture

51:42.160 --> 51:50.520
search and optimization to give you to give you a better, you know, image labeling API.

51:50.520 --> 51:53.000
But this is, that's same idea.

51:53.000 --> 51:58.160
You're giving it your data, and it's going to do architecture search and data cleansing

51:58.160 --> 52:05.440
and like a whole set of steps that is doing is basically doing AutoML for you, but at

52:05.440 --> 52:12.760
at a much higher level, at an application or really a business level, which is really cool.

52:12.760 --> 52:13.760
I agree.

52:13.760 --> 52:17.280
I would love to try this, you know, with very common businesses that have to forecast,

52:17.280 --> 52:19.720
you know, how much supply they're going to buy.

52:19.720 --> 52:24.240
Forecasts were kind of staffed to get a need to manufacture or assemble something.

52:24.240 --> 52:25.240
Forecasts were kind of shipping.

52:25.240 --> 52:28.960
They're going to have to reserve ahead of time forecast how long it's going to take

52:28.960 --> 52:32.520
for customers to pay them, and you know, is that going to be quicker than when the payments

52:32.520 --> 52:34.160
have to make their suppliers.

52:34.160 --> 52:38.520
This can impact literally almost every aspect of a business, and it's going to be exciting

52:38.520 --> 52:41.200
to see what people do with it next year and year after that.

52:41.200 --> 52:42.200
Mm-hmm.

52:42.200 --> 52:44.480
So I pulled up my list of the things that they're doing.

52:44.480 --> 52:49.480
So they're loading your data, they're inspecting your data, they're identifying features,

52:49.480 --> 52:53.920
they're selecting algorithms, they're selecting hyperparameters, they're training and optimizing

52:53.920 --> 52:59.080
models, they're building a feature store, they're deploying and hosting these models or

52:59.080 --> 53:03.560
so serving them up for you, and they're caching, they're creating real-time caches.

53:03.560 --> 53:09.560
So they're doing all of this stuff, you know, specific to your data, that is pretty cool.

53:09.560 --> 53:10.560
Yeah.

53:10.560 --> 53:15.400
So a couple of other services that they announced, Comprehend Medical, so Comprehend

53:15.400 --> 53:27.320
is their, like, text extraction service, and Comprehend Medical is a vertical medical,

53:27.320 --> 53:31.120
obviously, oriented version of that.

53:31.120 --> 53:38.640
And then another one that I think is kind of the, maybe like the low-key, super high

53:38.640 --> 53:45.600
utility, you know, not sexy, announcement of this whole, you know, of all of these is

53:45.600 --> 53:48.680
a new one called Textract.

53:48.680 --> 53:58.640
And it's basically a souped up OCR system that you give it a PDF, or they say almost

53:58.640 --> 54:03.640
any document, and it'll extract the text and data from that document, but it does it

54:03.640 --> 54:09.200
in a way that's like, if there are tables in that document, it will extract those tables.

54:09.200 --> 54:17.000
If there are, you know, you know, kind of lists, you know, it'll extract key value pairs.

54:17.000 --> 54:23.800
It, I heard someone say this, it wasn't in any of the slides I saw, but like it, it sounds

54:23.800 --> 54:29.840
like it will retain some of the structure of the documents, it'll deliver bounding

54:29.840 --> 54:36.680
boxes for where, you know, document features are like tables and stuff like that.

54:36.680 --> 54:41.880
And you don't need to know any machine learning to do any of that stuff.

54:41.880 --> 54:49.800
I can see, I can envision, you know, whole businesses kind of just built on kind of commercializing

54:49.800 --> 54:50.800
this service.

54:50.800 --> 54:56.200
Yeah, like I remember still having PTSD from doing expenses at an app with like an antiquated

54:56.200 --> 55:00.440
1990s Oracle, like expense system, and so this could revolutionize that, right?

55:00.440 --> 55:05.800
If you have expenseify or concur, integrate this, hopefully very quickly, it could just

55:05.800 --> 55:10.800
increase productivity and employing morale so much just for expense reporting alone.

55:10.800 --> 55:13.600
I was curious how that would work.

55:13.600 --> 55:18.840
You would upload receipt images and have it generate the expense report.

55:18.840 --> 55:21.600
Exactly, zero touch almost.

55:21.600 --> 55:27.880
The use case that they gave for this was tax forms.

55:27.880 --> 55:36.480
Like, you know, there's one canonical W-9 form, but there are, you know, hundreds of

55:36.480 --> 55:39.520
different kind of versions of W-9 forms.

55:39.520 --> 55:43.560
Like as long as you capture the information and the lines, you could come up with your own

55:43.560 --> 55:46.640
custom version of that form and apparently everyone has.

55:46.640 --> 55:56.600
And so automating, pulling, extracting the data from those forms into a form that you

55:56.600 --> 56:02.000
can then dump into a database is difficult because you have all these variants of these

56:02.000 --> 56:04.160
forms.

56:04.160 --> 56:10.200
You know, if you think of like invoices, you know, it's structured or semi-structured if

56:10.200 --> 56:12.000
you prefer data, right?

56:12.000 --> 56:16.600
It's not a picture, it's not a bunch of texts, but OCR just treats it, you know, it starts

56:16.600 --> 56:22.040
with a picture and gets out a bunch of texts, but it loses all of the semantic meaning in

56:22.040 --> 56:23.040
these forms.

56:23.040 --> 56:29.560
And so what text extract is meant to do is to preserve all of that using, of course, machine

56:29.560 --> 56:32.480
learning, which is pretty cool.

56:32.480 --> 56:35.920
I see text, but the bigger one that I see would be medical.

56:35.920 --> 56:36.920
Yeah.

56:36.920 --> 56:38.920
Talk about that.

56:38.920 --> 56:40.520
That's huge.

56:40.520 --> 56:47.600
Think about all of the medical forms, things that you would submit to insurance.

56:47.600 --> 56:53.040
All of that, if you were a hospital or medical, medical group and you deal with submitting

56:53.040 --> 57:01.200
to different insurance companies, I see lots and lots of applicability for that.

57:01.200 --> 57:07.000
Let alone the myriad of different, as you pointed out, text forms and such.

57:07.000 --> 57:08.200
I would see that as big.

57:08.200 --> 57:15.600
The other thing that would be big would be legal in maintaining the forms because a lot

57:15.600 --> 57:20.880
of different legal documents that you file with courts and such also follow a standardized

57:20.880 --> 57:27.040
format for that court or maybe even for that state, depending, but at least for, say,

57:27.040 --> 57:34.200
that specific county, but it will change for others, but they, but all of those documents

57:34.200 --> 57:39.640
for that specific court will follow the same format, at least for long periods of time,

57:39.640 --> 57:41.320
say, decades.

57:41.320 --> 57:47.120
So being able to maintain that would also be huge, but those are just a few examples

57:47.120 --> 57:50.640
that come to mind that are very, very large have broad applicability.

57:50.640 --> 57:51.640
Mm-hmm.

57:51.640 --> 57:54.000
Yeah, the semantic processing of images is one thing.

57:54.000 --> 57:59.520
I'm wondering how they'll be able to extend this to other localized languages, so to speak,

57:59.520 --> 58:04.880
because I just came back from some trips to China and that massive Belt and Road Initiative

58:04.880 --> 58:10.480
they have, touches so many countries and so many cultures and languages and written

58:10.480 --> 58:15.440
characters and scripts that as this mature, this could be another game changer for how

58:15.440 --> 58:19.520
you process that kind of very heterogeneous supply chain.

58:19.520 --> 58:22.400
Yeah, so it's, you're coming about medical.

58:22.400 --> 58:26.840
Does raise an interesting question about the relationship between the new comprehend

58:26.840 --> 58:33.640
medical offering and textract and do they already, you know, talk to one another, does

58:33.640 --> 58:39.120
textract use comprehend medical for that very domain specific thing or can you do that

58:39.120 --> 58:42.800
or is that something that's coming in the future?

58:42.800 --> 58:44.800
Interesting stuff there.

58:44.800 --> 58:52.320
So I think we made it through the ML related stuff amazingly in an hour, but there's

58:52.320 --> 58:57.840
a bunch more stuff does anyone have or do you guys have like your favorite thing that

58:57.840 --> 59:00.280
we haven't talked about yet?

59:00.280 --> 59:01.280
We covered a lot.

59:01.280 --> 59:06.640
I think there's a whole bunch of as we talked earlier, maturing security offerings just

59:06.640 --> 59:12.640
for better overall governance and supervision of the various policies you have and the monitoring

59:12.640 --> 59:17.480
tools and the probably endless dashboards now that are available to you in terms of

59:17.480 --> 59:24.080
monitoring activity, so the whole notion of just a maturity overall of security and compliance

59:24.080 --> 59:30.440
tools and tool sets for AWS customers is one of those unsung heroic set of announcements

59:30.440 --> 59:32.640
this year for me as well.

59:32.640 --> 59:33.640
How about you, Dave?

59:33.640 --> 59:34.640
I don't know.

59:34.640 --> 59:41.840
I think for me, the, I think the biggest thing is the absolute number of deeply innovative

59:41.840 --> 59:48.560
announcements with lots of meat behind them, lots and lots of things in the ML and the

59:48.560 --> 59:56.720
infrastructure space, both and it makes me wonder if it's not time for them to change

59:56.720 --> 59:58.000
the conference.

59:58.000 --> 01:00:03.720
I think that we're at the point where there are too many announcements.

01:00:03.720 --> 01:00:09.560
The topics are too broad, they cover too much and the conference itself, it's not possible

01:00:09.560 --> 01:00:16.400
to go if you have an interest that spreads even a couple of these for you to successfully

01:00:16.400 --> 01:00:22.880
go and attend the workshops and sessions and so when a conference reaches that, it's

01:00:22.880 --> 01:00:29.240
usually time for the conference to split out into more separate conferences that cover

01:00:29.240 --> 01:00:34.360
each of those kind of domains and I think reinvent is at that point.

01:00:34.360 --> 01:00:35.360
That's my opinion.

01:00:35.360 --> 01:00:40.160
As an attendee, I would say I agree with Dave, right, I'm not sure if you're a presenter

01:00:40.160 --> 01:00:44.840
or you know, exhibitor or actually the one that hosts the conference that the economies

01:00:44.840 --> 01:00:49.520
of scale would be attractive but as an attendee, I couldn't agree more with Dave and I think

01:00:49.520 --> 01:00:54.280
moreover, we noticed earlier on that there's more and more pre-announcements happening

01:00:54.280 --> 01:00:58.880
because you can only squeeze in one keynote a year for major announcements and spreading

01:00:58.880 --> 01:01:05.040
the conferences out, both location wise, venue wise chronologically would let them announce

01:01:05.040 --> 01:01:09.360
I think more timely availability of these cool new services that they keep announcing.

01:01:09.360 --> 01:01:13.800
I also have a meta comment but I'm going to hold that for one second because I do have

01:01:13.800 --> 01:01:20.920
one more product announcement that I'm really excited about and it's kind of like adjacent

01:01:20.920 --> 01:01:23.800
to machine learning stuff.

01:01:23.800 --> 01:01:26.400
Did you catch the Lake Formation announcement?

01:01:26.400 --> 01:01:27.880
Yes.

01:01:27.880 --> 01:01:30.480
Lake Formation is pretty cool.

01:01:30.480 --> 01:01:39.920
So I guess the meta to Lake Formation is that AWS this year, I think for the first time,

01:01:39.920 --> 01:01:45.240
has been super aggressive about positioning S3 as a data lake.

01:01:45.240 --> 01:01:48.960
I have not heard them talk about it like that before.

01:01:48.960 --> 01:01:56.280
And so Lake Formation is this new offering that basically automates the data pipeline for

01:01:56.280 --> 01:02:01.400
getting data into S3.

01:02:01.400 --> 01:02:13.200
So talking about like crawling your on-premises data stores and cataloging and cleansing data,

01:02:13.200 --> 01:02:25.520
getting it into S3, organizing it, deduplication and it can crawl relational databases as well

01:02:25.520 --> 01:02:32.160
as semi-structured and unstructured data sources.

01:02:32.160 --> 01:02:40.840
When I think about and talk to folks about the path to kind of repeatable enterprise machine

01:02:40.840 --> 01:02:47.400
learning, like the stumbling block for a lot of people, like kind of stuck in the blocks

01:02:47.400 --> 01:02:53.640
is having a centralized data repository, whether you call it a data warehouse or it is a data

01:02:53.640 --> 01:03:00.920
warehouse or a data lake or a fabric or whatever, I guess a big deal and it's cool to see them

01:03:00.920 --> 01:03:01.920
taking that on.

01:03:01.920 --> 01:03:06.880
Yeah, in fact, not only that, but I think they buried the lead a little bit with Lake Formation

01:03:06.880 --> 01:03:11.000
and that this can revolutionize things like GDPR and other kinds of privacy compliance

01:03:11.000 --> 01:03:16.240
efforts where the discovery part of this alone, particularly as more and more data is either

01:03:16.240 --> 01:03:21.400
managed in conjunction with our post at both on-prem and off-prem, the discovery part and then

01:03:21.400 --> 01:03:26.160
the cataloging part is that mature as you can automatically categorize data and create

01:03:26.160 --> 01:03:29.640
automated master data models and then you can do all sorts of interesting recommendation

01:03:29.640 --> 01:03:31.400
engines based on that.

01:03:31.400 --> 01:03:35.360
When you tie some of those workflows together that is a really revolutionary in this world

01:03:35.360 --> 01:03:37.840
of compliance and spending a lot of time in.

01:03:37.840 --> 01:03:38.840
Awesome.

01:03:38.840 --> 01:03:39.840
Yeah.

01:03:39.840 --> 01:03:40.840
I agree.

01:03:40.840 --> 01:03:50.400
I think a lot of people have been using S3 is kind of their mini data lake, so to speak.

01:03:50.400 --> 01:03:56.040
They'll dump as much as they can in, but they're still kind of limited in their options with

01:03:56.040 --> 01:04:02.360
what they can do and how it works, although they keep expanding S3's functionality constantly.

01:04:02.360 --> 01:04:12.200
I still think this taking on the data lake, especially for the impact it has with analysis

01:04:12.200 --> 01:04:15.760
and machine learning is a big deal.

01:04:15.760 --> 01:04:16.760
It really is.

01:04:16.760 --> 01:04:17.760
That's true.

01:04:17.760 --> 01:04:27.080
My meta comment that I was saving was one of the things that they've taken some steps

01:04:27.080 --> 01:04:33.520
towards addressing with their, oh, they also announced like a whole ML training and certification

01:04:33.520 --> 01:04:43.120
curriculum, by the way, or that, oh, that, right, exactly, and they'd previously announced

01:04:43.120 --> 01:04:51.040
like ML partner competencies, but one of the things that struck me in going into the Expo,

01:04:51.040 --> 01:04:58.400
which is massive, like several hundred exhibiting companies in the Expo, very few of them, even

01:04:58.400 --> 01:05:04.840
at this stage, our ML and AI related, like a lot of them have, or do other things that

01:05:04.840 --> 01:05:12.680
are kind of, have their ML and AI story as vendors do, tech vendors do, but in terms of

01:05:12.680 --> 01:05:22.840
kind of pure play, ML and AI partners, you know, whether services or product ISVs, software

01:05:22.840 --> 01:05:28.000
as a service, the pickings were relatively slim.

01:05:28.000 --> 01:05:32.200
And it kind of brought to mind a question that I started kind of asking myself last year,

01:05:32.200 --> 01:05:37.200
which is like, does AWS have an ecosystem problem around machine learning?

01:05:37.200 --> 01:05:44.760
Like, is it not, you know, open enough or inclusive enough or, you know, something that they're

01:05:44.760 --> 01:05:51.680
not able to kind of catalyze partners to get on board or make it worth their while?

01:05:51.680 --> 01:05:53.080
Or is it just too early?

01:05:53.080 --> 01:05:57.320
It's something I noticed actually about the machine learning, you know, industry and

01:05:57.320 --> 01:06:02.120
markets, I went as a whole, is by and large, very few of those companies, if any, grow

01:06:02.120 --> 01:06:06.680
up to be big, independent successes, and, you know, consequently, there's just a lot of

01:06:06.680 --> 01:06:08.880
acryhiring going on.

01:06:08.880 --> 01:06:13.880
So I don't know if these companies reach a stage where they can actually get enough capital

01:06:13.880 --> 01:06:19.000
to have a branding style of marketing campaign that capital and tons of companies, like

01:06:19.000 --> 01:06:24.800
the storage companies, my confirmation bias goes to, I noticed, I noticed so many giant,

01:06:24.800 --> 01:06:29.640
you know, storage, you know, logos and branding on the miracle mile screen, you know, across

01:06:29.640 --> 01:06:34.640
from the area in the Cosmo, and I just saw a lot of marketing and branding dollars going,

01:06:34.640 --> 01:06:36.800
you know, from infrastructure companies.

01:06:36.800 --> 01:06:40.560
But to your point, not so much more, a lot of the smaller innovative companies and spaces

01:06:40.560 --> 01:06:41.560
like ML.

01:06:41.560 --> 01:06:42.560
Hmm.

01:06:42.560 --> 01:06:43.560
Interesting.

01:06:43.560 --> 01:06:44.560
Take for sure.

01:06:44.560 --> 01:06:51.320
I think that just speaks more to my point about splitting the conference into smaller conferences.

01:06:51.320 --> 01:06:52.320
Right.

01:06:52.320 --> 01:06:53.320
Right.

01:06:53.320 --> 01:06:59.200
If you're a ML or AI pure play company, like the, the hit rate here, the signal to noise

01:06:59.200 --> 01:07:01.560
ratios got to be pretty low still.

01:07:01.560 --> 01:07:02.560
Yeah.

01:07:02.560 --> 01:07:03.560
Unfortunately.

01:07:03.560 --> 01:07:08.680
Uh, I could get with that, Dave, uh, just, let's start a movement right now.

01:07:08.680 --> 01:07:11.880
Don't make it any later in the year.

01:07:11.880 --> 01:07:13.680
Oh, yeah.

01:07:13.680 --> 01:07:15.480
Agreed.

01:07:15.480 --> 01:07:21.520
Um, you know, I think we should, uh, I think we should reach out to Berner and, uh, and

01:07:21.520 --> 01:07:26.160
and, and Jesse and tell them that they're awesome.

01:07:26.160 --> 01:07:33.480
Well, guys, I've got to pack up and bolt to the airport, uh, but it was awesome or recapping.

01:07:33.480 --> 01:07:35.920
Uh, reinvent with you guys.

01:07:35.920 --> 01:07:40.000
Thanks so much for, uh, hopping on with me at a great time.

01:07:40.000 --> 01:07:41.400
Thanks for including me this year.

01:07:41.400 --> 01:07:45.680
And yeah, to Dave's point, maybe we'll have, you know, more mini recaps going forward

01:07:45.680 --> 01:07:47.760
when they split up these conferences.

01:07:47.760 --> 01:07:48.760
Nice.

01:07:48.760 --> 01:07:49.760
Nice.

01:07:49.760 --> 01:07:50.760
Awesome.

01:07:50.760 --> 01:07:51.760
Take care, guys.

01:07:51.760 --> 01:07:52.760
Bye.

01:07:52.760 --> 01:07:53.760
Bye.

01:07:53.760 --> 01:07:54.760
Bye.

01:07:54.760 --> 01:07:55.760
Bye.

01:07:55.760 --> 01:07:56.760
All right, everyone.

01:07:56.760 --> 01:08:01.880
That's our show for today for more information on Dave, Val, or any of the topics

01:08:01.880 --> 01:08:08.000
covered in this episode, visit twimmaleye.com slash talk slash 205.

01:08:08.000 --> 01:08:12.080
If you're a fan of the show and you haven't already done so, or if you're a new listener

01:08:12.080 --> 01:08:17.360
and you like what you hear, visit your Apple or Google podcast app and leave us a five-star

01:08:17.360 --> 01:08:18.760
rating and review.

01:08:18.760 --> 01:08:23.040
You reviews help inspire us to create more and better content and they help new listeners

01:08:23.040 --> 01:08:24.600
find the show.

01:08:24.600 --> 01:08:34.320
As always, thanks so much for listening and catch you next time.

