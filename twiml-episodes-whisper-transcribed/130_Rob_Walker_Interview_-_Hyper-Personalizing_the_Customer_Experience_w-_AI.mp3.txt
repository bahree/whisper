Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting
people, doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
In this episode I speak with Rob Walker, vice president of decision management and analytics
at Pegasystems, a leading provider of software for customer engagement and operational excellence.
Rob and I discussed what's required for enterprises to fully realize the vision of providing
a hyper personalized customer experience and how machine learning and AI can be used
to determine the next best action an organization should take to optimize sales, service, retention
and risk at every step in the customer relationship.
Along the way we dig into a couple of key areas, specifically some of the techniques his organization
uses to allow customers to manage the trade-off between model performance and transparency,
particularly in light of new laws like GDPR, and how all of this ties to an enterprise's
ability to manage bias and ethical issues when deploying machine learning.
We cover a lot of ground in this one and I think you'll find Rob's perspective really
interesting.
If you're looking for more enterprise perspectives on the latest in AI, robotics, customer engagement
and digital process automation technology, you should check out Pegasystems upcoming
conference Pegaworld, which will be held June 3rd to 6th at the MGM Grand in Las Vegas, Nevada.
I'll be there, Rob will be there, and so will IT and experience leaders from organizations
like Afflack, Anthem, Genworth Financial, JP Morgan Chase, and more.
To learn more, visit Pegaworld, and as a Twymilistner, be sure to use the discount code PW18TWIML
for $150 off of your registration.
Thanks to Pegaworld for sponsoring this episode of the podcast, and now on to the show.
All right everyone, I am on the line with Rob Walker.
Rob is Vice President of Decision Management and Analytics with Pegasystems.
Rob, welcome to the podcast.
Thank you.
It is great to have you on, as is the tradition here on the podcast, why don't we start out
by having you tell us a little bit about your background and how you got involved in
Analytics and AI?
Yeah, thank you.
Happy to do so.
Yeah, I was really attracted to AI very early on.
I mean, this is like, so in the 90s, when I was at the university, and I was doing my computer
science studies, the specialization I chose was AI, which obviously is not quite the AI
we have right now, but on the other hand, you know, much has actually remained the same,
just a day die I think has changed a lot.
But that was a really fun sort of specialization, and later I got my PhD in AI as well, specifically.
And after that, I thought this was like, we're now talking like 2000 to approximately.
I was really, you know, getting ready to do something in business.
So I co-founded a company, company called KIQ, which was really specializing in what
I would say is Applied AI, so that was early days.
I was really like, apply predictive analytics, but at a very high scale around, you know, customer
engagement specifically, but predicting things like credit, risk, or customer behavior
in general, but at a very large scale, and then applying it in real time.
And some facts about that was that we got the venture capital for that company, like moments
before the 2000, you know, crash.
So we just got the money in to start, so that was a really, really good timing.
And then four years later, yeah, that was lucky.
And AI wasn't as hot then as it is now, so that was double lucky.
We got that going, sold it into some pretty large companies, and we're then acquired
four years later by a company called Cordient, which is a Cupertino company next door
neighbors with Apple, who really used that technology and AI around customer experience
and personalization.
And then Pegasystems acquired Cordient for that particular thing as well.
So it's a long-winded thing, a long-winded way of saying that I've been in this space
for a very long time, and it hasn't let many things distract me from using AI in a very
practical way with sort of, you know, pretty large companies.
And what's your focus at Pegas now?
So at Pegas, it's really like, we have this thing called, you know, the customer brain
or the customer decision hub, but it's a little bit more technical, and it's basically
one of the core elements of everything we do, especially around customer engagement.
So it's helping us hyper-personalize interactions in lots of different channels, very high volume
kind of things, and so things digital presence on the website or mobile, but at the same
time, you know, decide on next best actions for outbound communications, or if this is
a bank in ATM, every single interaction we're trying to optimize, and that's not just AI,
but AI is a very critical component of that.
And are you optimizing it primarily from what perspective, from a financial perspective,
or you mentioned customer experience earlier from a broader customer experience perspective?
How do you think about that?
Yeah, so the optimization function is basically critical.
So I spent a lot of time, you know, talking to, especially the business people, but also
with the data scientists in these large companies, so to decide on what their metric is, right?
So we do next best action, and best, typically, is a combination of, like, future value, right?
So it's not, we're, as best practice, we don't try to, for instance, if this is about,
you know, a web interaction, it's not about, okay, what should we be selling you, or what
should you be doing?
It's really trying to optimize future value of the relationship, and that means that, yeah,
there are products, maybe, that you're selling, but you can also try to nurture the relationship
you can look at MPS scores that you're trying to optimize.
So we're not specifically prescribing what the best metric is, but it's typically a proxy
for future value of that relationship, that then every action is trying to optimize.
Can you maybe walk us through a more concrete example of that?
It doesn't have to be a specific customer, but if you've got a specific one in mind, just
to put that kind of little process that you outlined for us into context?
Sure.
Yeah, let's take a large bank, for instance, and they've been on record, like Royal Bank
of Scotland, or PNC Bank in the US, both of them, there are many other banks that are
using this kind of stuff, but what they are doing is basically make sure that at every
opportunity they have, when they engage in with a customer in any channel, so it can
be sort of an old-fashioned email, or even more old-fashioned printed email, but typically
obviously now the digital channels on the web, in the mobile, in the app everywhere, every
single interaction, they're basically pinging the AI, which is then predicting all sorts
of behaviors, like likelihood to turn, likelihood to, if this is a bank of getting into financial
trouble, and maybe not repaying alone, obviously propensities around the whole host or product
or product combinations that the bank would be selling them, and then a bunch of optimization
algorithms, but also rules put in by humans to decide on, okay, if that's the case, then
contextually, right now, for this interaction, and that maybe one interaction out of billions
in a single day, this is what we should be doing, and that's sort of the process we're
going through, and this could be, this is an offer, this is what we show on the website,
this is a notification on your mobile phone, this is a prompt on the ATM terminal, any
of those kind of things.
I'm wondering what your take is on kind of where we are, I guess, as an industry in this
process of applying AI and analytics to the customer experience, right?
So calling it AI is new, but a lot of what you're describing, folks have tried to use predictive
analytics for quite some time to optimize the next offer or the next step in managing
a relationship.
And personally, when I think about my relationships with large enterprises like banks, like telecom
companies, I don't really get the sense that they're optimizing anything about that relationship.
It feels very static, and I wonder what the barriers are to them realizing this kind of
vision of a more personalized kind of relationship with me as their customer.
Yeah, I think that, and that's obviously many, many do not, and there are, it's amazing
given the benefits that I think we, but also other companies show, all of this is data
driven.
So it's not particularly hard to show sort of the delta on some of these, on some of
these things.
And the reason it's, and I have to say it's going fast, so I mean, we certainly don't complain
about traction, but it's not going as fast as you would expect, because it's not how
these large organizations are organized, and this is nothing to do with the technology,
although I'll touch on that in a second as well, but it's also just culturally, you know,
people in these big banks or big communications companies or other companies are incentivized
on a specific part of the business, right?
So if you're talking to a bank, in your example, you know, the people that want to sell
you credit cards do their thing, typically pretty old-fashioned as well.
And then the people that want to sell you mortgages do their thing.
And then there are, you know, ten other lines of businesses that do their thing, and it
all comes together, you know, at the point of contact, but it's not a concerted effort.
It's not optimized for the bank or for you.
It's basically trying to make sense of all sort of suboptimal, you know, goals.
That's why it feels incoherent and inconsistent, and typically then all the different channels
are not connected to this one brain in the middle either, right?
So anyway, there's a lot of ways to make mistakes or to, you know, get suboptimal results
as you are experiencing.
So what are you seeing as the main ways that companies are trying to dismantle these silos
and create better experiences?
Yeah, well, obviously they're not, you know, these big companies are not charity.
So they, I think the big move is, well, there's twofold.
One is they actually have a lot of data, right?
I mean, obviously there's a whole big data discussion, but they have a lot of it now.
And it's in much better quality.
So if I rewind even like ten years, you know, when everybody was building, you know,
data warehouses and things like that, we're in an incomparably better state on data.
And also the awareness of that, you know, that these big companies have that data is,
it has grown, has grown tremendously.
And then the question is, can they basically, based on what AI can, can do to optimize,
you know, these, these interactions?
Do they believe that, you know, there's a tangible result and we're talking about a lot
of money, right?
And that, I think, is the biggest reason for a lot of these big companies pivoting towards
this because it's so easy to show once you get it going, what the benefits are, right?
Both in customer satisfaction and in, you know, bottom line benefits that it's, it becomes
pretty hard to, you know, neglect that kind of progress.
Can you talk a little bit about the process that company might take to identify, you know,
where it's biggest opportunities lie?
I imagine for a lot of these companies, there are tons of ways that they could apply analytics
and AI to create better outcomes and prioritizing them becomes a bit of a challenge in and
of itself.
Yeah.
Not that that's true.
And that becomes a huge, huge challenge.
What we typically do is to, and this is much easier than it was, like not that low
ago because of all the references and these tangible results.
So I have a lot easier conversations than I had before.
But one of the things that I think radically changes, but also really brings the need of
AI to the forefront is that if you become, like, really customer centric and you have
these one-to-one relationships, the whole model is changing, right?
I mean, a traditional bank or large company would have marketing campaigns where obviously
data scientists would create their predictive models and do all their magic and create
segments and do targeting.
But that's all based on sending out a message for a particular product or offer, right?
If you actually put the customer at the center and now you have AI figuring out all of their
behaviors that are relevant, you know, from a risk perspective, retention perspective,
surface perspective, as well as sales, now you have maybe a thousand different predictive
models and you need to optimize with all of those outputs as inputs.
Now, that's the optimization challenge that, on the one hand, gives you a lot better
customer satisfaction because it's, the relevance will go through the roof and so will
bottom line benefits.
But it means that a lot of companies or a lot of people in these companies now need to
collaborate where they didn't have to before, right?
Because the markets people would do one thing, the cars people would do a different thing.
Now it's all about the customer and AI has now a big vote in what should be done or very
strong opinions about, you know, recommendations that it makes about what the next best action
would be.
So it sounds like the first step is really at least for the companies that you're working
with and the approach that you're taking with them.
The first step is really centralizing, centralizing is maybe not the right word, but kind of establishing
this customer-centric perspective, which is kind of strange for me to hear in some sense
because like, you know, one-to-one marketing, 360 degree marketing, these kinds of, these
are 20 plus year old terms and we just have tools and approaches that we can apply to
them now.
But it's a little surprising that it's still so hard to get companies to buy into this
as an idea.
Yeah.
Well, partly it's still cultural, but partly also, honestly, that is because you know, one-to-one
marketing or one-to-one interactions may be, you know, not the latest idea, I'm sure that,
you know, we heard that maybe in the 90s or something like that.
But to actually make it real, you do need AI.
So I think that is really the breakthrough of the last 10 years, right, that you can have
the self-learning models, adaptive models, all sorts of AI algorithms that these big companies
can trust to turn through their data, come up with propensities, learn on the fly, execute
contextually, and make them a lot of money.
I mean, that's not a given, right, and one approach even now is to, for instance, say,
let's ring fans just 1% of your customers, right, and let's see, you know, the difference
of this new approach and contrast it with the control group of 99%, just to make sure
that you can trust it, and you know, that's when you see the benefits, and they're not
like marginal benefits, so you know, that sort of propels the project forward.
So one of the other topics that I hear come up a lot in the context of enterprise applications
of AI as the whole idea of explainability or transparency.
This is coming to the fore, you know, a bit more of a pointed fashion in Europe in the context
of GDPR, and I'm wondering if this is something that you run into with the companies that
you're talking to?
Yes, and that's very important.
So we have GDPR sort of on the European side, which is many things, but one of the things
is that, you know, you need to be able to explain any kind of AI or algorithm that is making
meaningful decisions, so that's obviously a lot of wiggle space around what is meaningful,
but clearly, you know, you can imagine that this is about, you know, alone or some big
financial stake or risk calculations, that is a meaningful decision.
So that's GDPR, that's one thing, but equally outside Europe as well, using AI as a black
box for these, especially for the larger companies, these large enterprise companies is just
too high a risk, right?
So they have a whole compliance process around AI and explainability is very important.
And for instance, one approach that we're taking really at the core of the technology
is to give controls that are not naive about this, right?
You can obviously say, let's insist on transparent AI, which obviously, you know, as everybody
will understand clearly, will come at the cost, right?
I mean, transparency because it's a cost.
It means that the algorithm cannot be as effective as it could otherwise be, so you can't
just switch it off and say, oh, you can only do regression models or whatever it is.
So we believe that at the core of these AI systems, there needs to be a control that allows
much more granular control around transparency.
Say, for instance, for risk or more regulatory areas, you may really insist on transparent
models and we'll define that in a moment.
Whereas maybe in marketing or in image recognition, you are, you know, you can go all out with
your, you know, opaque algorithms like deep learning or, you know, XG boost models or
assemble models, all of these things that are, you know, become much more intractable.
So that's a discussion we have a lot around AI and I think the trust in AI is really important
to make this whole thing fly.
And you mentioned that you would define transparency.
How do you articulate that to customers?
Yeah, so there are multiple ways, obviously, the naive way of this is basically tagging
the algorithm.
I mean, they're obviously, you know, like deep learning algorithms are notoriously opaque.
But more technically, what we do is basically look at the model output and then reverse engineering
it with transparent methods like, for instance, decision trees and then the complexity of
a decision tree that is trying to reverse engineer the model then becomes the size of that
decision tree then becomes a proxy for transparency or opacity really.
Can you elaborate on that a bit?
So you're training arbitrary models independent of kind of this transparency versus opacity
metric and then you're like fitting a decision tree to them and depending on the complexity
of the resulting decision tree, that tells you whether the models should be classified
as transparent versus opaque.
Did I get that right?
Yeah, that's basically, that's basically correct.
So obviously we don't apply to any, you know, area, but especially around customer engagement.
So what we're doing, we're getting all of these predictions of behavior from neural
mass or genetic algorithms or, you know, more transparent algorithms like decision trees
or regression models, whatever the case may be, some may be built by data scientists,
some may be built automatically by the AI, but basically once we get all of these scores
or all of these propensities, we are done going to fit it, as you said, with a transparent
algorithm and that in itself will give us a good view of, you know, how opaque it
is.
So it sounds like underneath or kind of embedded in this process is some, I don't know
if you met an optimizer or some model, some auto ML, I guess, is one way to describe
it that is, you've got to, you've got a problem.
You're testing a bunch of different models or training a bunch of different models on
this data and you coming up with parameterized models.
And each of those, you know, on the one hand, you know, you've got some model that's the
best model.
It may be a model that's not natively transparent.
And if that's the case, then you walk back from that model using this process of training
a decision tree on it to get like the, you know, the closest transparent model to that
model.
Yeah.
That's one way of, there are actually multiple approaches.
So that's one way of doing it.
And basically, you say, this is the closest transparent model.
The other way of doing it is saying, well, whenever these two models agree on a particular
prediction or whatever classification or whatever it is, if they agree, we'll just take
the transparent model and then we only worry about the 5% of cases where they disagree.
And then we make a choice based on the area we're in.
So if we're in a regulatory area where we have to explain this, maybe GDPR, maybe it's
our own compliance office as in these large companies will then actually go either all
the way with the transparent model or that's where we switch to, you know, a more opaque
model and take the risk that we don't completely understand how it works, but only for a small
percentage of the, of the data points.
There are a lot of activities, both on a research side as well as in companies that are using
AI to try to create explainability out of otherwise opaque algorithms through the, kind
of the architecture of those algorithms, are you involved in that as well?
Yes.
So we're trying to, and honestly, we're also going to see sort of where the market goes,
right?
But to our own technology in this case, so we currently look at like, you know, reverse
engineering, these basically these scoring bins with decision tree algorithm as a good proxy.
But and maybe that's good enough, but there are other algorithms that, you know, specifically
are trying to reverse engineer these models and if they work better, we'll use those.
So I think the point for us as sort of an enterprise AI vendor for around customer engagement
is that we need to give these companies the control over this.
Otherwise, they can't use it to the, to the largest extent.
And that would be, you know, not a good thing for their customers and obviously for these
organizations.
Yeah.
I guess that's an interesting topic in and of itself, kind of the, there's a, seems
to be a broader debate around even asking this question, like even, you know, insisting
on transparency or explainability for some of these algorithms, there seems to be at
least in the Twitter echo chamber, a vocal group of folks that object to the idea of requiring
transparency for algorithms, you know, in, in some sense, because, you know, I guess the
argument is that it limits innovation, it provides unreasonable constraints, the example
that you hear a lot is, hey, we don't know how the brain works, but we make decisions
all the time and we're comfortable with that.
You know, I'm wondering what your perspective is on kind of that broader argument.
Yeah.
So I think that's a fascinating debate and suddenly the scientist in me is, is definitely
with that school.
It's like, it's an, it's an, it's a constraint on AI that, you know, as a rule, if that would
actually be, if we would legislate that away, you know, the opaque algorithms, I think
that would be a bad thing, also not sustainable, by the way, because I think it's not just
about business benefits and, and, and, and if we go, you know, maybe zoom in on this
a little bit because I do think this is fascinating.
I think a lot of people don't seem to realize or don't want to realize that even opaque
AI, like the human brain perhaps, you know, if it makes better decisions, that's not always
a good thing just for companies, right?
It's also probably a good thing for, for humans, right?
It may be that you are not getting the loan that you should otherwise get or it may be
that you get diagnosed with something before a human can, and those are good things.
So I'm also off to school that we should not, you know, rule out, opaque algorithms.
I do, however, believe that certainly in the face where we're now, where, you know,
the greater population needs to gain trust in these kind of things that we need to control
mechanism that allows you to allow opaque algorithm where you are comfortable, and even then
you need all sorts of bias tests and ethical tests at the end, but also I'll be able to
restrict it with the same control algorithm or control mechanism in areas that are very
heavily regulated.
Otherwise, we would never see AI there.
You mentioned bias and ethical test.
This is something that I've been writing about a bit in my newsletter of late.
I'm wondering what work you've been doing in that area.
Yeah.
So we have been, and I think you can tell from the emphasis on this, you know, on this
called control mechanism, we really believe that for AI to be trusted and accepted and
have, you know, large scale adoption, we do need these kind of control mechanisms.
At the same time, even with these control mechanisms, we have the thing, so we call it revision
management, which is the overall sort of police on any of these customer strategies that
you put in place, right?
It's not just about AI, it's also about performance, it's about all sorts of things.
Because in our case, the decision layer is used at such a scale interacting with, you
know, as I said, billions of interactions that you really need to get it right.
So we have this sort of revision manager that makes sure that can, you know, that all of
them QA is done.
And at that level, we believe the ethical test and bias tests need to be formally incorporated.
So you don't have just a business officer signing off on the business benefits and an IT
manager signing off on the SLAs and all these kind of things.
You should probably also have an ethical officer saying, listen, this is showing a bias
that it goes beyond what we as a brand think is acceptable.
And, you know, I think you should do that across the board, but especially if you allow
some of the more opaque algorithms, which I think you should in some areas, I think without
these tests, and not just as a kind of an afterthought, but part of your whole QA process,
I think is critical for large companies.
Because some of these new technologies and approaches get popularized, one of the things
you start to see is new job titles emerge that kind of tell you where enterprises are going.
So I remember when chief security officer and chief data officer came about, and now
people are talking about, you know, whether there should be a chief AI officer, do you
see many enterprises now that have a chief ethics officer or do you see that as something
that's emerging how far away from something like that, or where does it have been lived
within, you know, today's enterprise?
Yeah, I think I haven't seen it tightly yet, but I think it's probably now with the
compliance officer or maybe the risk officer, but probably the compliance officer.
But I think we will definitely see that, especially when customers are interacting with AI
more directly, right?
And a lot of what I'm talking about, they don't necessarily see, right?
They may see a personalized page, but you don't know the magic behind that, right?
But when we see people talking to, you know, cognitive agents and chatbots, and it's
very obvious that they are AI, I think people will become much more sensitive to that.
And I can imagine, yeah, a chief AI officer or ethics officer is something we will see.
But definitely when we do these large implementations in this revision process, an ethical sign-off
is part of the process.
And I can ignore it, but you know, that's what you should be putting in.
And my sense is that the kind of science around making that ethical sign-off, you know,
isn't very well developed at this point, how are companies managing that?
How do they know when they should be signing off?
What kind of process are they taking to determine whether an algorithm is, you know, ethically
acceptable?
Yeah, so I think that's currently, first of all, that's in its infancy, right?
And what we are prescribing is basically looking at, like, you know, distributions of outcomes
and then comparing it with, you know, the process, the more transparent process or the
human process.
But that only works, for instance, for instance, let's assume that your AI is used by a big
bank and it's turning racist, right?
Can easily happen.
And obviously, and this is a discussion, I think, where we're now having a lot, with all
the big data and also the efficiency of the AI, obviously race, you know, doesn't have
to be, you know, in your data or gender or age for it, obviously, to, you know, to use
these concepts.
So anyway, that's, I think that's a given for this audience.
But just to be precise, is that if you have an opaque algorithm and you have big data,
there is no way that you can tell from the, you know, the algorithm that it is, you know,
it is racist or has some other, you know, weirdness.
The thing is, you can only test that if you have a test for it.
And that may mean if you really don't have some, and I'm just making a brace as an example.
If you don't actually carry that in your database, you cannot, you know, test the distribution,
get a loan, see if it's materially or statistically significantly different from your desired policy,
which should be, obviously, completely, politically correct.
So that presents a little bit of a catch 22 and that I imagine organizations have strived,
have strived to keep those criteria out of their databases or is that not the case?
No.
So yes.
So for some of these things like, like, you know, age and gender, they probably have
it for other purposes, so you can do it for other things.
And this could be sexual orientation or political affiliation, all of these things.
You probably will need a panel, you know, so you actually have a specific group of representative
group of customers that you do ask for these attributes.
So you can check your bias and your algorithm.
But I haven't, I haven't seen that as part of a formal process yet.
Okay.
Okay.
So you mentioned, I forgot the terms you use, but you in describing the process for kind
of putting the final stamp of approval on a model.
You talked about some of the testing steps and, you know, all this brings to mind the
broader context of scaling and operationalizing AI within the enterprise.
What are some of the things that you're seeing with regards to putting these models into
production and how do they correlate to, are we learning things from, you know, other,
you know, scaling and operationalizing processes like DevOps on the application development
side or, you know, how how evolved and mature is this process now?
Yeah.
I think this is sort of more the area of like decision management, right?
So it's this model execution layer that I think is getting is getting pretty mature, right?
But I think that it's an important thing.
If I look at the breakthroughs in AI, I'm not even thinking so much about the algorithmic
sort of progression.
And obviously we have seen, you know, very cool new algorithms, but, you know, a lot of
it is still is still the same.
I think the two things that have changed the most are the data that's available, both
to learn, but also to apply the AI to in production and the speed, right?
So we're not talking about taking like one predictive model into production, even if
it's a complex predictive model, I'm talking about thousands of them, right?
Thousands of them combined with all sorts of financial rules and cutoffs and thresholds
and inventory and constraints, all of these things together are used multiple times in
a single interaction to continuously predict or decide on this next best action.
That's that's a big scale.
If you're a company, the size of sprint or one of the one of the big banks, like Citibank
or Royal Bank of Scotland, that's like such a volume of interaction to apply a thousand
predictive models, thousands of rules, and then every single time something changes in
the context, like a customer says, yes or no, or maybe or this is too expensive or clicks
or doesn't click, and then you recalculate it.
That was just completely impossible just maybe five, six years ago.
And that's I think also makes AI and control over AI so important right now.
And so are you are you seeing companies do specific things to try to wrangle, you know,
all of these models that they have in production, is that something that your tools are doing
or you're seeing them using open source tools, what do you think the landscape looks
like?
Now, well, what I'm seeing is like I think the open source tools still specifically tend
to, you know, the mostly I think the data scientist community, right?
So they're obviously getting very good at, you know, getting you all sorts of algorithms.
And I think this honestly, especially on the big cloud platforms will become much more
of a commodity, right, where they will just have, you know, APIs around predictive modeling
and all sorts of things.
I think where it gets a lot more complex and challenging is to have a company in which
the business people themselves are trained enough and confident enough to build these big
customer strategies looking at evidence, right?
And this is a very big pivot.
So this brings us all the way back to this one to one, right?
If you figure out for one person and you have to execute a thousand propensity models
and then all of your rules and constraints and all these kind of things to defigure
out what the next best action is.
And then the customer says, yeah, but I think it should be a hundred dollars less.
I'm just making this up.
And we recalculate if that's acceptable at that scale, when you have like 50 million
customers using all of these different channels, that I think is the challenge.
And I think there's no, I haven't seen any open source thing dealing with that.
I think where we're looking at, for instance, is basically abstracting from whatever tool
or set of tools are giving us the predictive insight because we think that will
become a commodity, right?
We complement it with lots of very high-scale self-learning stuff, but I think in the end
that becomes a commodity and it's building decisions out of that and make that transparent
and do that at the scale so that these are huge companies can use it in every single
interaction channel.
I think that's not easy.
And I don't think open source will get us there quickly.
So just to make sure I understand your perspective on that, the part that you're abstracting
away at the level that you're thinking about it is the underlying, kind of modeling,
model building and perhaps even the models themselves.
And then the part where you mentioned the part that you're focused on is the decisioning
piece.
But there, I guess it strikes me that there is a middle piece which is kind of the scaling
and operationalization.
Does that exist today, is that something that we're still as an industry building and
figuring out in your perspective or where do you see that evolving?
No, I think we're there.
Certainly we are doing that for the large companies, but we are consuming and this is one of
the challenges, right?
We are consuming these predictive models if somebody loves R or one of the modeling tools.
We would consume that.
There's an exchange format called PMML that you can use to sort of help standardize that.
The thing is, if you want to have AI sort of at the forefront of customer engagement,
you need to be able to execute these models contextually, contextually.
So we're no longer, you know, in an area or a time where we would have big, bad jobs
and all of these models would put scores in databases and then we'd retrieve the scores.
So now we know the likelihood you're going to buy this is 0.8 and the risk is 0.2 because
we need to execute them contextually like we do in a really human conversation.
So the models need to be executed contextually and that, I think, is much more of the technical
challenge than getting the models in the first place, although at the moment we still feel
like there's a lot of model laboratories out there, right?
These tools where data scientists can have 100 different algorithms to build the better
they want, which is cool and we will consume that.
But a model factory to actually give the business, you know, the throughputs and the volume
of models that they need, probably not there yet, but I think that will become a commodity.
Awesome.
Awesome.
Well Rob, to wrap up, do you have any final thoughts that you'd like to share with the audience?
I think we touched on a lot.
We've got a lot of ground.
All right.
Awesome.
Well, it's been great getting to chat with you about this stuff.
There is certainly a lot of interesting things happening as enterprises wrap their arms
around AI, but also there's, you know, they've, you know, while AI is kind of the hot new
term, there's, you know, they've certainly been working on analytics, decision management,
predictive stuff for quite a while and it's interesting to see how these things co-evolve
together.
So thank you for taking some time to share your perspective with us.
You're very welcome.
Thank you.
Thanks again to Rob and Pegasystems for sponsoring this show.
And of course, make sure you head on over to pegaworld.com to learn more about the conference
and be sure to use the code PW1820 for 150 off of registration.
Thanks so much for listening and catch you next time.
