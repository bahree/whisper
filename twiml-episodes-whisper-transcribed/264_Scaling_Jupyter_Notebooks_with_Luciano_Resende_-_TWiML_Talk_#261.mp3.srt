1
00:00:00,000 --> 00:00:16,320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

2
00:00:16,320 --> 00:00:21,480
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:21,480 --> 00:00:31,160
I'm your host Sam Charrington.

4
00:00:31,160 --> 00:00:35,280
This week on the podcast we're featuring a series of shows that highlight just a few of

5
00:00:35,280 --> 00:00:39,800
the great innovations and innovators at the intersection of three very important and

6
00:00:39,800 --> 00:00:46,280
familiar topics, data science, the Python programming language and open source software.

7
00:00:46,280 --> 00:00:49,960
To better understand our listeners' views on the importance of open source and the projects

8
00:00:49,960 --> 00:00:54,520
and players in this space, I'm conducting a survey, which I'd be very grateful if you

9
00:00:54,520 --> 00:00:57,040
took a moment to complete.

10
00:00:57,040 --> 00:01:01,760
To access the survey, visit Twimbleai.com slash Python survey.

11
00:01:01,760 --> 00:01:09,800
Please hit pause now and we'll wait for you to get back.

12
00:01:09,800 --> 00:01:15,880
That's twimbleai.com slash Python survey.

13
00:01:15,880 --> 00:01:19,840
Before we dive into the show, I'd like to send a huge thanks to our sponsor for this

14
00:01:19,840 --> 00:01:22,200
series IBM.

15
00:01:22,200 --> 00:01:26,680
Speaking of open source, IBM has a long history of engaging in and supporting open source

16
00:01:26,680 --> 00:01:32,160
projects that are important to enterprise data science, projects like Hadoop, Spark,

17
00:01:32,160 --> 00:01:35,680
Jupiter and Cubeflow to name just a few.

18
00:01:35,680 --> 00:01:41,640
IBM also hosts the IBM data science community, which is a place for enterprise data scientists

19
00:01:41,640 --> 00:01:47,320
looking to learn, share and engage with their peers and industry renowned practitioners.

20
00:01:47,320 --> 00:01:52,440
Here you'll find informative tutorials and case studies, Q&As with leaders in the field

21
00:01:52,440 --> 00:01:57,800
and a lively forum covering a variety of topics of interest to both beginning and experience

22
00:01:57,800 --> 00:01:59,480
data scientists.

23
00:01:59,480 --> 00:02:05,160
Check out and join the IBM data science community by visiting IBM.com slash community slash

24
00:02:05,160 --> 00:02:06,160
data science.

25
00:02:06,160 --> 00:02:09,760
All right, everyone.

26
00:02:09,760 --> 00:02:12,320
I am on the line with Luciano Recende.

27
00:02:12,320 --> 00:02:19,600
Luciano is an open source AI platform architect at IBM and part of the center for open source

28
00:02:19,600 --> 00:02:24,600
data and AI technology there, also known as code eight.

29
00:02:24,600 --> 00:02:27,320
Luciano, welcome to this weekend machine learning and AI.

30
00:02:27,320 --> 00:02:28,320
Hi, Sam.

31
00:02:28,320 --> 00:02:29,640
Thanks for having me.

32
00:02:29,640 --> 00:02:33,880
So Luciano is great to have you on the show and I'm looking forward to chatting with you

33
00:02:33,880 --> 00:02:40,280
about a project that you spent a lot of time working on in particular Jupiter notebook

34
00:02:40,280 --> 00:02:42,880
and the Jupiter notebook ecosystem.

35
00:02:42,880 --> 00:02:47,160
But before we dive into that, I'd love to hear a little bit more about your background

36
00:02:47,160 --> 00:02:52,680
and how you got started working at this, the intersection of open source and data and

37
00:02:52,680 --> 00:02:53,680
AI.

38
00:02:53,680 --> 00:02:54,680
Great.

39
00:02:54,680 --> 00:02:55,680
Let me start.

40
00:02:55,680 --> 00:02:58,880
So like I've been working with open source for a long time.

41
00:02:58,880 --> 00:03:03,880
I've been contributing to projects that are patching in other areas on the open source

42
00:03:03,880 --> 00:03:11,840
AI ecosystem for over 10 years and more towards getting into this Jupiter ecosystem part.

43
00:03:11,840 --> 00:03:18,520
A couple of years ago, we started seeing a more and more adoption of the notebooks as

44
00:03:18,520 --> 00:03:24,760
a way for the interactive development for data science and recently for AI as well.

45
00:03:24,760 --> 00:03:33,240
And when talking to external or internal customers, we had a need to basically scale that

46
00:03:33,240 --> 00:03:37,360
platform for a large number of concurrent users.

47
00:03:37,360 --> 00:03:43,160
So like you go to a financial institution and they might have like 500 data scientists

48
00:03:43,160 --> 00:03:47,880
that come in the morning will launch their notebook and they need to be able to do all

49
00:03:47,880 --> 00:03:52,640
the crunch of the data that they have to start getting insights from that.

50
00:03:52,640 --> 00:03:55,440
And we didn't have a very good solution for that.

51
00:03:55,440 --> 00:04:02,160
We also were looking to building the IPM platform, the Watson Studio and we needed a similar

52
00:04:02,160 --> 00:04:03,160
solution.

53
00:04:03,160 --> 00:04:07,320
And that's how we started the Jupiter Enterprise Gateway.

54
00:04:07,320 --> 00:04:13,680
Basically, what we do in Jupiter Enterprise Gateway, we are a lightweight, multi-tenant,

55
00:04:13,680 --> 00:04:19,920
scalable and secure gateway that enables Jupiter notebooks to start sharing resources across

56
00:04:19,920 --> 00:04:23,680
an Apache Spark cluster or a Kubernetes cluster.

57
00:04:23,680 --> 00:04:27,680
And we focus a lot on the enterprise and cloud use gays for that.

58
00:04:27,680 --> 00:04:37,400
I'm starting to hear quite a bit of interest on the part of enterprises and folks that

59
00:04:37,400 --> 00:04:42,640
are working in building out platforms and infrastructure for data science and machine

60
00:04:42,640 --> 00:04:52,560
learning teams about the idea of kind of standing up Jupiter notebook instances for their teams.

61
00:04:52,560 --> 00:04:58,960
As you kind of think about the way Jupiter is being used in large organizations, what

62
00:04:58,960 --> 00:05:02,720
are some of the big challenges that folks run into?

63
00:05:02,720 --> 00:05:09,320
So Jupiter notebooks, they are evolution from Python console.

64
00:05:09,320 --> 00:05:17,440
And because of that, we see it some limitations that came from that past instances.

65
00:05:17,440 --> 00:05:25,600
And Jupiter by default, it's a single user kind of like environment and all the kernels

66
00:05:25,600 --> 00:05:34,440
which are the actually run time of the data executes the code, it's default to run as a

67
00:05:34,440 --> 00:05:36,520
local process.

68
00:05:36,520 --> 00:05:42,240
And what we see is that it doesn't scale to the needs that we need on enterprise or large

69
00:05:42,240 --> 00:05:43,560
deployments.

70
00:05:43,560 --> 00:05:49,880
Then on on the Jupiter ecosystems, we started seeing a lot of like projects that are kind

71
00:05:49,880 --> 00:05:55,760
of like built as an add-on to Jupiter and start solving some of those limitations.

72
00:05:55,760 --> 00:06:01,880
And the idea is that you can start building itself service platform for data science and

73
00:06:01,880 --> 00:06:02,880
AI.

74
00:06:02,880 --> 00:06:10,640
So when we look, for example, Jupiter gives you the ability to start providing multi-user

75
00:06:10,640 --> 00:06:16,480
authentication for those notebooks, and then Jupiter Enterprise Gateway starts to scale

76
00:06:16,480 --> 00:06:21,920
the runtime and distribute that into either a Sparkless or Kubernetes cluster.

77
00:06:21,920 --> 00:06:26,560
And that's the direction that we are seeing things going to the Jupiter ecosystem at this

78
00:06:26,560 --> 00:06:27,560
days.

79
00:06:27,560 --> 00:06:29,840
Okay, yeah, that was going to be one of my questions.

80
00:06:29,840 --> 00:06:37,800
The differences between Jupiter Hub and Jupiter Enterprise Gateway, is it all about kind

81
00:06:37,800 --> 00:06:42,320
of where the back end kernels are run or are there other differences?

82
00:06:42,320 --> 00:06:52,800
So as I mentioned, we need a way to start launching an environment for multiple users,

83
00:06:52,800 --> 00:06:56,000
so provide a multi-tenant support.

84
00:06:56,000 --> 00:07:00,880
And what Jupiter Hub does, it's kind of like a launcher of a Jupiter server.

85
00:07:00,880 --> 00:07:07,680
So each user that requests an environment gets a Jupiter server runtime for doing

86
00:07:07,680 --> 00:07:10,000
its computation.

87
00:07:10,000 --> 00:07:15,080
Then when you get that environment, you then start using multiple notebooks.

88
00:07:15,080 --> 00:07:21,080
Those notebooks require a kernel to execute to the code.

89
00:07:21,080 --> 00:07:26,640
And then that's when Jupiter Enterprise Gateway comes into mind so that you can have a very

90
00:07:26,640 --> 00:07:32,920
lean Jupiter server environment launched by Jupiter Hub.

91
00:07:32,920 --> 00:07:39,240
And the runtime then, it goes across a cluster and you can start having multiple notebooks

92
00:07:39,240 --> 00:07:40,960
at the same time.

93
00:07:40,960 --> 00:07:45,600
But if you close a notebook, you let go those resources and make it available to other

94
00:07:45,600 --> 00:07:48,480
data scientists on the platform.

95
00:07:48,480 --> 00:07:54,480
It also enables you to start sharing more expensive resources, like for example, you're

96
00:07:54,480 --> 00:07:57,640
doing AI and you need GPUs.

97
00:07:57,640 --> 00:08:04,560
We provided a way to containerize that and then be able to associate the number of GPUs,

98
00:08:04,560 --> 00:08:09,000
for example, or the resources per kernel that it gets launched.

99
00:08:09,000 --> 00:08:13,640
And because those are decoupled, those come and go as you're using them and not necessarily

100
00:08:13,640 --> 00:08:16,240
the whole time that your notebook server is up.

101
00:08:16,240 --> 00:08:22,760
Okay, so it sounds like then the typical organization will use both Jupiter Hub as kind

102
00:08:22,760 --> 00:08:28,600
of the front end and then Enterprise Gateway as a way to distribute the backend kernels.

103
00:08:28,600 --> 00:08:32,360
But they're not alternative products, they're complimentary.

104
00:08:32,360 --> 00:08:35,760
They are definitely complimentary.

105
00:08:35,760 --> 00:08:39,360
They work very well together.

106
00:08:39,360 --> 00:08:40,360
Yes.

107
00:08:40,360 --> 00:08:47,200
So you mentioned both Spark and Kubernetes as services that you can use to provide the

108
00:08:47,200 --> 00:08:55,240
backend distributed computing framework for these kernels.

109
00:08:55,240 --> 00:08:58,040
What's the current mix that you see of those two options?

110
00:08:58,040 --> 00:09:00,640
Is one more popular than the other?

111
00:09:00,640 --> 00:09:07,320
I think it really depends on the type of workloads that you're processing.

112
00:09:07,320 --> 00:09:14,240
What I see Spark and use it is more for analytics where you have a lot of data and you might

113
00:09:14,240 --> 00:09:18,200
start processing crushing those data and then applying some kind of like a machine learning

114
00:09:18,200 --> 00:09:20,240
on top of that.

115
00:09:20,240 --> 00:09:27,960
And with the recent release that we have, the Enterprise Gateway 2.0, we support not

116
00:09:27,960 --> 00:09:35,760
only if you have like a legacy Spark deployment based on like a HDFS and I do distribution,

117
00:09:35,760 --> 00:09:38,920
but we also support Spark on Kubernetes.

118
00:09:38,920 --> 00:09:43,560
So if you're doing analytics, you're most likely going to use Spark runtime and that's

119
00:09:43,560 --> 00:09:47,080
why we initially started supporting that.

120
00:09:47,080 --> 00:09:56,040
With more of the AI workloads, what we have seen is a lot of like AI platforms for training

121
00:09:56,040 --> 00:10:03,960
models and deploying models all based on Kubernetes and the reason I think for that is that

122
00:10:03,960 --> 00:10:11,840
Kubernetes provides you kind of like the right sandbox with containers and that enables

123
00:10:11,840 --> 00:10:19,280
you to, for example, start a Python kernel, but if you're doing a TensorFlow model, you'll

124
00:10:19,280 --> 00:10:26,240
just kind of say, okay, I want to Python kernel based on a TensorFlow image and you start

125
00:10:26,240 --> 00:10:30,920
mixing and matching your environment with the kernel that you need and then you can have

126
00:10:30,920 --> 00:10:38,960
a very specific environment and close it in the container and another AI engineer might

127
00:10:38,960 --> 00:10:45,560
completely have a completely different one and Kubernetes manage and although deployment

128
00:10:45,560 --> 00:10:50,840
anything that needs to happen on that, and we manage the lifecycle and give the connectivity

129
00:10:50,840 --> 00:10:59,880
to notebooks to be able to actually execute and train your models on a Kubernetes environment.

130
00:10:59,880 --> 00:11:06,880
Now, I've had several conversations with organizations, Airbnb is one that comes to mind

131
00:11:06,880 --> 00:11:17,440
that have invested pretty heavily in customizing Jupyter Notebook and in some ways, maybe they've

132
00:11:17,440 --> 00:11:23,840
duplicated what you've done or went to build it because what you've done either wasn't

133
00:11:23,840 --> 00:11:31,360
ready on their time frame or didn't offer features that they needed and I'm wondering

134
00:11:31,360 --> 00:11:41,480
in general, where do you see folks investing heavily in customizing Jupyter to build it

135
00:11:41,480 --> 00:11:49,080
into their workflows? What are some of the key gaps maybe in what's currently available

136
00:11:49,080 --> 00:11:55,440
or some of the ways that organizations might want it to be more tightly integrated?

137
00:11:55,440 --> 00:12:00,960
What I've been seeing in the industry is a lot of customization around your internal

138
00:12:00,960 --> 00:12:05,520
environment and integration with some of the tools.

139
00:12:05,520 --> 00:12:12,160
You might need to get some of the outputs of your notebooks to a given platform that

140
00:12:12,160 --> 00:12:18,560
is available only internally and that's a lot of the things that I've been seeing.

141
00:12:18,560 --> 00:12:25,840
When you go, I think with Jupyter Lab, which is a new UI for the Jupyter Notebook, those

142
00:12:25,840 --> 00:12:32,000
kinds of customizations and ability to start bringing up widgets are going to become

143
00:12:32,000 --> 00:12:41,360
much more flexible and easy to deploy or customize and then in terms of runtime, I think

144
00:12:41,360 --> 00:12:49,920
what I've been seeing is a lot of like trying to accomplish multi-tenant by deploying multiple

145
00:12:49,920 --> 00:12:56,720
environments for different users, but it's a duplication of that environment and not like

146
00:12:56,720 --> 00:13:04,080
a decoupling the actual execution of the kernel with the Jupyter Notebook and that's

147
00:13:04,080 --> 00:13:06,720
how Jupyter and Fifthgate became to do.

148
00:13:06,720 --> 00:13:12,640
Can you elaborate on that last part when you say it's duplication and what sense?

149
00:13:12,640 --> 00:13:23,400
So Jupyter Hub, as you were saying, was available a lot prior than enterprise data and initially

150
00:13:23,400 --> 00:13:28,520
what I've seen people doing before enterprise data is that they will start a Jupyter

151
00:13:28,520 --> 00:13:36,160
server for each user and then that instance of the Jupyter server will have to have all

152
00:13:36,160 --> 00:13:43,440
the resources required for any type of workload that the user needs to do.

153
00:13:43,440 --> 00:13:53,200
That is an okay solution, but all those resources get locked for a long time and in that case,

154
00:13:53,200 --> 00:13:59,280
I think a lot of those resources might be in an idle position and not be able to be reused

155
00:13:59,280 --> 00:14:05,600
by other members of your platform or other data scientists that wants to do some computations.

156
00:14:05,600 --> 00:14:12,240
You also mentioned integrating with internal systems and security and things like that

157
00:14:12,240 --> 00:14:15,840
as a big motivator for folks.

158
00:14:15,840 --> 00:14:23,840
The Jupyter Notebook and this ecosystem, APIs, do they lend themselves to folks doing this

159
00:14:23,840 --> 00:14:26,960
kind of integration, is it difficult?

160
00:14:26,960 --> 00:14:33,240
Where is it on the scale of built for this and easy to do or really difficult to do and

161
00:14:33,240 --> 00:14:36,000
requires a lot of jumping through hoops?

162
00:14:36,000 --> 00:14:37,320
That's a good question.

163
00:14:37,320 --> 00:14:40,360
Let me look into a different perspective here.

164
00:14:40,360 --> 00:14:48,400
Imagine your data scientists and you're running your workloads with Jupyter.

165
00:14:48,400 --> 00:14:54,400
You will not see any difference in terms of user experience if you're using enterprise

166
00:14:54,400 --> 00:14:57,960
gateway on the backhand or not.

167
00:14:57,960 --> 00:15:03,680
If you are a kind of like DevOps and you are providing that environment for your data

168
00:15:03,680 --> 00:15:11,160
scientists, pretty much what you need to do is customize the kernel definition and instead

169
00:15:11,160 --> 00:15:17,480
of saying that it's just a local kernel, you will switch to a different lifecycle manager

170
00:15:17,480 --> 00:15:23,320
and say, this is a remote kernel, we call it process proxy, we have different lifecycle

171
00:15:23,320 --> 00:15:29,680
managers for each resource manager so we have one for yarn, we will have one for Kubernetes

172
00:15:29,680 --> 00:15:35,880
and that is extensible so people have been coming in and providing others as well and

173
00:15:35,880 --> 00:15:41,800
depending on the one that you choose, the resource manager that you choose, you can then

174
00:15:41,800 --> 00:15:44,480
add additional information needed.

175
00:15:44,480 --> 00:15:50,080
So for example, if you choose yarn, you might want to pass like your Spark home information.

176
00:15:50,080 --> 00:15:56,280
If you choose Kubernetes, you might not need anything specific or depends on how it goes

177
00:15:56,280 --> 00:15:58,800
into terms of configurations.

178
00:15:58,800 --> 00:16:06,880
But once that is properly configured, we handle most of the hard work internally on the

179
00:16:06,880 --> 00:16:12,640
tool by automating all the lifecycle like the starting the kernel remotely figuring out

180
00:16:12,640 --> 00:16:19,640
where it actually mounted, providing the configuration back to Jupyter so that the end-to-end

181
00:16:19,640 --> 00:16:23,520
communication can be connected.

182
00:16:23,520 --> 00:16:30,880
So it's to certain extent not very hard to get an environment going and we also have

183
00:16:30,880 --> 00:16:39,240
been doing lots of tools like SQL scripts where you can deploy enterprise gateway from scratch

184
00:16:39,240 --> 00:16:47,560
or from an existing environment for both Spark and Kubernetes run types.

185
00:16:47,560 --> 00:16:56,720
So the work to create this remotely executable or invocable kernel, was that already in existence

186
00:16:56,720 --> 00:17:02,560
in the Jupyter ecosystem or is that part of the work that enterprise gateway needed

187
00:17:02,560 --> 00:17:07,040
to do in order to exist really?

188
00:17:07,040 --> 00:17:12,320
Yes, so the kernels in itself is nothing new that we created.

189
00:17:12,320 --> 00:17:17,680
The kernels were available before in Jupyter and today I think we have a list of like over

190
00:17:17,680 --> 00:17:19,680
100 kernels available.

191
00:17:19,680 --> 00:17:25,920
The kernels, they will abstract the languages that you are using.

192
00:17:25,920 --> 00:17:31,280
So for example, if you are doing Python, you have a Python kernel, if you're doing

193
00:17:31,280 --> 00:17:33,600
Scala, you have a Scala kernel.

194
00:17:33,600 --> 00:17:38,720
What enterprise gateway brings is the ability to decouple those that originally were running

195
00:17:38,720 --> 00:17:41,800
as local process to be run remotely.

196
00:17:41,800 --> 00:17:45,520
That's the main thing that enterprise gateway brings.

197
00:17:45,520 --> 00:17:53,960
And around that came a lot of requirements such as security, how to handle multi-tenant

198
00:17:53,960 --> 00:17:58,760
so that the users that are actually requesting the kernel are the ones that are running the

199
00:17:58,760 --> 00:18:01,000
kernels on the ground times as well.

200
00:18:01,000 --> 00:18:08,200
And we started leveraging the capabilities that we had to handle all of those additional

201
00:18:08,200 --> 00:18:10,800
requirements in enterprise gateway.

202
00:18:10,800 --> 00:18:18,280
So occasionally here from folks that are using Kubernetes for these types of workloads that

203
00:18:18,280 --> 00:18:24,080
they run into challenges with the kind of out-of-the-box Kubernetes scheduler.

204
00:18:24,080 --> 00:18:27,840
Is that something that you've seen for running these kernels?

205
00:18:27,840 --> 00:18:34,360
We haven't experienced any particular issue with the schedule at the moment.

206
00:18:34,360 --> 00:18:38,440
We also haven't heard from any users.

207
00:18:38,440 --> 00:18:44,480
I think most of the users at the moment are going more for the Kubernetes environment and

208
00:18:44,480 --> 00:18:48,480
we haven't heard anything specifically to that from them yet.

209
00:18:48,480 --> 00:18:53,600
I guess that one of the other things that I hear from folks about the way they want to

210
00:18:53,600 --> 00:19:04,120
use Jupyter is to integrate it with, for example, more tightly with Git repositories and shared

211
00:19:04,120 --> 00:19:11,400
file systems and kind of have these resources easily accessible by data scientists and machine

212
00:19:11,400 --> 00:19:16,120
learning engineers, kind of wherever they spin up their notebooks.

213
00:19:16,120 --> 00:19:18,840
Is that something that you see as well?

214
00:19:18,840 --> 00:19:24,720
And is it a piece of what's offered by the current notebook ecosystem?

215
00:19:24,720 --> 00:19:28,280
That's definitely something that I hear a lot.

216
00:19:28,280 --> 00:19:36,640
I think the Jupyter notebook and as well the new UI Jupyter lab have the ability to have

217
00:19:36,640 --> 00:19:39,480
the extensions built on top of that.

218
00:19:39,480 --> 00:19:46,520
Particularly on the Jupyter lab, I've seen, for example, integration with Git repositories

219
00:19:46,520 --> 00:19:54,000
and you can easily or transparently get things back and forth based on how you're progressing

220
00:19:54,000 --> 00:19:55,400
with your notebook.

221
00:19:55,400 --> 00:19:59,880
I think another thing that I also hear a lot, and currently I don't think it's available

222
00:19:59,880 --> 00:20:06,480
or at least not easily available on the open source, is the ability to start doing collaboration.

223
00:20:06,480 --> 00:20:12,120
That is something that, like sharing and having multiple people looking to the notebook

224
00:20:12,120 --> 00:20:14,280
and maybe trying to collaborate on that.

225
00:20:14,280 --> 00:20:18,440
I think it's one thing that really needs to come and be available on the open source

226
00:20:18,440 --> 00:20:21,720
of the Jupyter ecosystem and that's not available today.

227
00:20:21,720 --> 00:20:27,400
Yeah, I think Google Collab is an offering that comes to mind that does a pretty nice

228
00:20:27,400 --> 00:20:34,920
job at showing what a collaborative notebook experience might look like.

229
00:20:34,920 --> 00:20:40,440
And I can imagine that's kind of creating a lot of demand for folks that want to use

230
00:20:40,440 --> 00:20:45,120
something similar, but in a native Jupyter type of an environment.

231
00:20:45,120 --> 00:20:46,120
That is correct.

232
00:20:46,120 --> 00:20:52,480
I mean, I've seen commercial solutions that start offering those, but in the open source

233
00:20:52,480 --> 00:20:58,640
or more like on the academia, we haven't seen an open source solution that handles that.

234
00:20:58,640 --> 00:20:59,640
Okay.

235
00:20:59,640 --> 00:21:07,600
You mentioned that there are hundreds of kernels for Jupyter, which I actually find kind

236
00:21:07,600 --> 00:21:08,600
of surprising.

237
00:21:08,600 --> 00:21:14,760
I mean, when I click down that kernels button, it only ever says, I thought, two or three.

238
00:21:14,760 --> 00:21:23,720
I'm wondering, to what extent my experience is reflective broadly or in general, how much

239
00:21:23,720 --> 00:21:31,000
of the Jupyter ecosystem is kind of Python centric and is Python users and how much of

240
00:21:31,000 --> 00:21:35,960
it folks that are trying to do other things beyond Python?

241
00:21:35,960 --> 00:21:42,720
I think that we at least commercially like on more enterprise environments, what we have

242
00:21:42,720 --> 00:21:51,160
seen is Python, the number one, R, the second, and those are mostly data scientists.

243
00:21:51,160 --> 00:21:57,440
Then some data engineers, particularly the ones working on analytics workloads, they

244
00:21:57,440 --> 00:22:03,000
also use the Scala for particular integrating with Spark.

245
00:22:03,000 --> 00:22:08,680
And you mostly see Python because Python, like if you have an environment like an account

246
00:22:08,680 --> 00:22:15,720
or any other way, Python most likely will be natively available for you.

247
00:22:15,720 --> 00:22:22,400
The other kernels are, you can easily install all those, they will just after installation

248
00:22:22,400 --> 00:22:28,200
start appearing as one of the options into your Jupyter notebook environment.

249
00:22:28,200 --> 00:22:36,320
And particularly in academia, then you start seeing Julia, C++ and other kernels that are

250
00:22:36,320 --> 00:22:39,360
very used on that community.

251
00:22:39,360 --> 00:22:44,800
So that's why you have at least like over a hundred kernels available today.

252
00:22:44,800 --> 00:22:45,800
Okay.

253
00:22:45,800 --> 00:22:54,600
And for folks that are, can you maybe are there specific companies or examples that you

254
00:22:54,600 --> 00:23:01,440
can share about what folks have managed to do with the enterprise gateway?

255
00:23:01,440 --> 00:23:06,120
One of the examples that I can give you internally is IBM Watson Studio.

256
00:23:06,120 --> 00:23:11,800
In order to be able to scale the notebook environment that we provide as part of Watson Studio,

257
00:23:11,800 --> 00:23:18,240
we use enterprise gateway to not only remote to the kernels, but also to give you a choice

258
00:23:18,240 --> 00:23:20,560
on where you want to run those kernels.

259
00:23:20,560 --> 00:23:26,200
We have different run times that we support and you can select those run times as where

260
00:23:26,200 --> 00:23:33,960
you want to run where you want to have the execution of the notebook runtime.

261
00:23:33,960 --> 00:23:39,960
Another example, and this is just from strata, we were at strata yesterday and PayPal was

262
00:23:39,960 --> 00:23:46,400
giving a very interesting presentation on how they were using enterprise gateway to be

263
00:23:46,400 --> 00:23:51,720
able to view the internal platform for data scientists and AI.

264
00:23:51,720 --> 00:23:53,880
So these are two examples that comes to mind.

265
00:23:53,880 --> 00:23:58,160
There are probably more, but particularly because we are open source, we only hear from

266
00:23:58,160 --> 00:24:04,640
them when they have an issue that gets reported or maybe like to a conference or something

267
00:24:04,640 --> 00:24:05,640
like that.

268
00:24:05,640 --> 00:24:07,600
Is it currently at a production ready?

269
00:24:07,600 --> 00:24:11,800
State sounds like PayPal at least in IBM or using it in production.

270
00:24:11,800 --> 00:24:13,920
How mature would you say it is?

271
00:24:13,920 --> 00:24:21,560
So we have two code streams that we are maintaining at Jupyter Enterprise Gateway.

272
00:24:21,560 --> 00:24:29,120
We have what we call the OneX release, which supports Spark, is mostly focused on analytics

273
00:24:29,120 --> 00:24:35,200
workloads and that has been stable and being used internally by products and have been

274
00:24:35,200 --> 00:24:38,600
available in the open source for a long time.

275
00:24:38,600 --> 00:24:43,880
Probably over a year or around close to a year and a half.

276
00:24:43,880 --> 00:24:50,400
And we have maintaining that in terms of like any new bugs that we see around that or any

277
00:24:50,400 --> 00:24:55,360
dependencies that needs to be updated for security reasons or anything like that.

278
00:24:55,360 --> 00:25:02,120
In parallel, we then are working on our tool release, which two days ago, we launched

279
00:25:02,120 --> 00:25:06,960
the tool RC1 release candidate one.

280
00:25:06,960 --> 00:25:12,160
And that brings a lot of innovation that we are doing on Kubernetes environment, having

281
00:25:12,160 --> 00:25:18,760
said that based on the questions and issues that we are getting, we have seen a lot of people

282
00:25:18,760 --> 00:25:26,240
experimenting, going more towards that direction and using the tool already to view their analytics

283
00:25:26,240 --> 00:25:29,200
environments on top of Kubernetes.

284
00:25:29,200 --> 00:25:36,920
So hopefully that will also be available as a GA in the next couple of weeks, based on

285
00:25:36,920 --> 00:25:39,520
whatever feedback we get from the RC1.

286
00:25:39,520 --> 00:25:45,360
And is the Enterprise Gateway functionality independent of the kernel that's being used

287
00:25:45,360 --> 00:25:49,240
or there's some limitations or dependencies there?

288
00:25:49,240 --> 00:25:53,120
So it is supposed to be independent.

289
00:25:53,120 --> 00:26:00,520
What we have done, we created a layer called kernel launcher that gives us the ability

290
00:26:00,520 --> 00:26:08,600
to enhance the kernels functionality, particularly for startup and shutdown.

291
00:26:08,600 --> 00:26:16,200
So we have tested a lot with Python, DR, and Scala kernels, which are the most requested

292
00:26:16,200 --> 00:26:19,160
and the most used for us.

293
00:26:19,160 --> 00:26:23,480
And things that we do is like, for example, if you are starting a kernel and that kernel

294
00:26:23,480 --> 00:26:29,760
is connecting to Spark, we create, for example, a parallel thread to start the Spark context.

295
00:26:29,760 --> 00:26:35,720
So it gives you a better and quicker user experience for the startup of the kernel.

296
00:26:35,720 --> 00:26:42,840
We need the ability to provide remote signaling so you can hit stop the kernel and things like

297
00:26:42,840 --> 00:26:43,840
that.

298
00:26:43,840 --> 00:26:50,480
And we need to flow that externally to a remote kernel and the launcher does that as well.

299
00:26:50,480 --> 00:26:58,520
But that kernel launcher can be easily ported or used for a different kernel.

300
00:26:58,520 --> 00:27:03,800
So the addition of new kernels shouldn't be very hard to incorporate into Enterprise

301
00:27:03,800 --> 00:27:04,800
Gateway.

302
00:27:04,800 --> 00:27:14,200
Here's about the requirements that users have when architecting environments for Enterprise

303
00:27:14,200 --> 00:27:15,600
Gateway.

304
00:27:15,600 --> 00:27:22,880
One of the things that comes to mind, for example, is when you're running your kernel locally,

305
00:27:22,880 --> 00:27:25,840
you've got typically data locally.

306
00:27:25,840 --> 00:27:31,400
If you're starting to remote your kernel, then the kernels operating against data that's

307
00:27:31,400 --> 00:27:39,040
remote, I'm wondering if there are network or bandwidth considerations or any kinds of

308
00:27:39,040 --> 00:27:45,640
issues that an organization might want to think about as they're architecting an Enterprise

309
00:27:45,640 --> 00:27:54,640
Gateway environment and generally how straightforward that process is or their tools that are

310
00:27:54,640 --> 00:27:58,880
available for capacity planning, things like that.

311
00:27:58,880 --> 00:28:02,720
I completely agree with the type of considerations that you're bringing up.

312
00:28:02,720 --> 00:28:09,000
I just, from what I'm seeing, these are not specific to when you're trying to deploy Enterprise

313
00:28:09,000 --> 00:28:10,000
Gateway.

314
00:28:10,000 --> 00:28:11,680
And it's a request more in general.

315
00:28:11,680 --> 00:28:18,840
So if you're doing analytics, the data will be most likely in a Spark cluster, but in

316
00:28:18,840 --> 00:28:25,720
a distributed file system like HDFS or some kind of like object storage environment.

317
00:28:25,720 --> 00:28:30,840
And you have like shuffling of data going back and forth, independent if you're using Enterprise

318
00:28:30,840 --> 00:28:32,320
Gateway or not.

319
00:28:32,320 --> 00:28:39,280
So that should have been part of like planning the deployment of Spark in particular.

320
00:28:39,280 --> 00:28:47,160
If we consider the Kubernetes environment, Kubernetes we most likely have multiple microservices

321
00:28:47,160 --> 00:28:56,480
being together to bring up a solution and you have similar type of constraints regarding

322
00:28:56,480 --> 00:29:00,880
data flowing and any other things like that.

323
00:29:00,880 --> 00:29:07,440
The easiest way that I've been seeing in more like a Kubernetes environment is we give

324
00:29:07,440 --> 00:29:16,280
the ability to do mattings into volumes into the specific container and by doing that,

325
00:29:16,280 --> 00:29:25,200
you can have the user environment available in that kernel image and that helps you alleviate

326
00:29:25,200 --> 00:29:34,160
most of these constraints by maybe providing data, maybe providing environment with specific

327
00:29:34,160 --> 00:29:40,760
packages that you might need for a given user on that mount and that should solve some

328
00:29:40,760 --> 00:29:41,760
of these problems.

329
00:29:41,760 --> 00:29:50,600
So it sounds like there's nothing particularly exotic or unknown or that needs special attention

330
00:29:50,600 --> 00:29:51,600
here.

331
00:29:51,600 --> 00:29:57,040
It's kind of the typical considerations when architecting a distributed environment for

332
00:29:57,040 --> 00:29:58,920
these kinds of workloads.

333
00:29:58,920 --> 00:29:59,920
Exactly.

334
00:29:59,920 --> 00:30:07,640
Granted that this capability that we're talking about particularly with Kubernetes is something

335
00:30:07,640 --> 00:30:12,440
that was just announced a couple of days ago or released a couple of days ago.

336
00:30:12,440 --> 00:30:19,280
But are there any other kinds of issues that you hear a lot about that folks run into

337
00:30:19,280 --> 00:30:26,680
that maybe you wish more people would think about before they embark on this path?

338
00:30:26,680 --> 00:30:32,680
Maybe a little bit unrelated but in the context of the Geoctery ecosystem, what I see a lot

339
00:30:32,680 --> 00:30:45,360
of discussions of what is the notion of a notebook and how it incorporates on your, let's

340
00:30:45,360 --> 00:30:49,960
say, model training pipeline and things like that.

341
00:30:49,960 --> 00:30:58,320
And most cases that I've seen, people would use the notebook for kind of like evaluation,

342
00:30:58,320 --> 00:31:03,000
they can look at the data and play with your model.

343
00:31:03,000 --> 00:31:09,600
But we'll actually convert the notebook into a Python file, for example, when they are

344
00:31:09,600 --> 00:31:15,160
kind of like ready to do a big training or start getting close to go to production.

345
00:31:15,160 --> 00:31:21,040
In my mind, the notebooks can become a little bit more kind of like first class citizens

346
00:31:21,040 --> 00:31:27,560
on those pipelines for model training and actually be one of the steps, actually be the

347
00:31:27,560 --> 00:31:34,200
notebook in itself and not necessarily go to a pipeline as a converted Python file.

348
00:31:34,200 --> 00:31:39,040
And I think once we have that, it's going to be much more productive, the data scientists

349
00:31:39,040 --> 00:31:46,800
can come up with something that then goes directly into the pipeline, produces the model and

350
00:31:46,800 --> 00:31:52,680
then can recreate the model later on if needed without any kind of like a collaboration maybe

351
00:31:52,680 --> 00:31:57,120
with a system engineer to convert to a Python or anything like that?

352
00:31:57,120 --> 00:32:04,680
And what needs to happen in order to get there from a kind of Jupyter ecosystem perspective?

353
00:32:04,680 --> 00:32:07,800
We have been seeing tools that it starts to enable that.

354
00:32:07,800 --> 00:32:19,400
I think it's mostly in a state of mind of like how people see the notebooks and they initially

355
00:32:19,400 --> 00:32:25,040
were a lot used for kind of like experimenting with data, trying to understand the data

356
00:32:25,040 --> 00:32:31,000
and trying to build models, but not necessarily maybe because those tools were not available

357
00:32:31,000 --> 00:32:32,880
yet go into production.

358
00:32:32,880 --> 00:32:36,960
And I think today we have seen like the Netflix coming out with paper, meal, a couple

359
00:32:36,960 --> 00:32:42,040
of other schedulers also coming out and supporting notebooks.

360
00:32:42,040 --> 00:32:48,520
My hope is that I will start seeing more of that being part of the pipeline for model

361
00:32:48,520 --> 00:32:49,520
development.

362
00:32:49,520 --> 00:32:52,200
All right, Luciano, this has been really great.

363
00:32:52,200 --> 00:32:59,920
I'm wondering if you've got any final thoughts or tips or ideas for folks that are using

364
00:32:59,920 --> 00:33:06,880
Jupyter, maybe as individual data scientists and want to use it more broadly in their

365
00:33:06,880 --> 00:33:07,880
enterprises?

366
00:33:07,880 --> 00:33:08,880
Oh, definitely.

367
00:33:08,880 --> 00:33:15,920
I think one of the things that I've seen and it's interesting that a lot of people still

368
00:33:15,920 --> 00:33:22,560
use and know about the Jupyter, what I call like classic UI, but not aware of like Jupyter

369
00:33:22,560 --> 00:33:23,560
Lab.

370
00:33:23,560 --> 00:33:32,400
Jupyter Lab gives you a lot of like flexibility and a better UI to do your model development

371
00:33:32,400 --> 00:33:37,800
and that gives you ability to start having like time in those multiple tabs all together

372
00:33:37,800 --> 00:33:42,680
and provide a much more integrated development environment for the model development.

373
00:33:42,680 --> 00:33:43,680
Awesome.

374
00:33:43,680 --> 00:33:46,200
If folks should definitely check that out.

375
00:33:46,200 --> 00:33:49,520
Well, Luciano, thanks so much for taking the time to chat with me.

376
00:33:49,520 --> 00:33:52,680
Okay, thanks, Sam.

377
00:33:52,680 --> 00:33:55,760
All right, everyone.

378
00:33:55,760 --> 00:33:57,800
That's our show for today.

379
00:33:57,800 --> 00:34:02,480
If you like what you've heard here, please do us a huge favor and tell your friends about

380
00:34:02,480 --> 00:34:03,640
the show.

381
00:34:03,640 --> 00:34:07,720
And if you haven't already hit that subscribe button yourself, make sure you do so you

382
00:34:07,720 --> 00:34:11,720
don't miss any of the great episodes we've got in store for you.

383
00:34:11,720 --> 00:34:15,080
As always, thanks so much for listening and catch you next time.

