1
00:00:00,000 --> 00:00:05,440
All right, everyone. I am on the line with Beaty Chen. Beaty is a PhD candidate in the Department

2
00:00:05,440 --> 00:00:11,680
of Computer Science at Rice University. Beaty, welcome to the Twomo AI podcast. Thanks.

3
00:00:12,880 --> 00:00:19,120
So I had the opportunity to hear Beaty's presentation at the systems workshop.

4
00:00:19,120 --> 00:00:24,560
MLs always get confused. ML for system systems for ML. This was systems for ML, correct?

5
00:00:24,560 --> 00:00:30,160
Yeah. They changed the name recently to we submit this paper to the conference. They first

6
00:00:30,160 --> 00:00:35,520
have it called MLs. And then they later change it to system because the trademarks stuff.

7
00:00:35,520 --> 00:00:41,920
Yeah. Well, I think there are two different workshops with very similar with all the same

8
00:00:41,920 --> 00:00:52,480
words, but in different orders. Yes. But in any case, you presented at the workshop on your

9
00:00:52,480 --> 00:00:59,040
paper slide, which we will go into. But before we do that, share with us a little bit about your

10
00:00:59,040 --> 00:01:06,080
background, what sparked your interest in machine learning, particular on kind of this intersection

11
00:01:06,080 --> 00:01:14,320
of hardware and algorithms. How did this all come about? Sure. So before I study my PhD at rice,

12
00:01:14,320 --> 00:01:20,880
I would have my anagra in University of California Berkeley. And I was actually doing some

13
00:01:20,880 --> 00:01:26,800
network's research. It's kind of still computer science, but a little bit irrelevant.

14
00:01:27,760 --> 00:01:34,400
But that kind of yeah, that actually I did my grad my grad work and networks also like

15
00:01:34,400 --> 00:01:38,000
stochastic modeling and all that kind of stuff. Is that the stuff you were doing?

16
00:01:38,000 --> 00:01:45,200
No, not the same different kind of networks. Yeah. Okay. Yeah. So we're kind of doing like a

17
00:01:45,200 --> 00:01:53,280
software defined building. Yeah. So that's what's kind of close to the AI part because you want

18
00:01:53,280 --> 00:01:58,080
to automate it in some process and you want to predict something for people. Yeah.

19
00:01:59,360 --> 00:02:05,600
Yeah. Then when I come to rice, I get interested in the machine learning algorithms because I saw

20
00:02:06,560 --> 00:02:11,920
I always have this in mind. All the problems we're trying to attack in computer science,

21
00:02:11,920 --> 00:02:18,480
not all like most of it, the bottlenecks are usually the computation. Not all the problems are

22
00:02:18,480 --> 00:02:24,720
showing this part, but if you think about it, we usually have a brute force way of doing things.

23
00:02:24,720 --> 00:02:31,600
And if we do have the computation of power or we have the time to achieve it, then the problem

24
00:02:31,600 --> 00:02:37,600
stops. But the problem is we usually don't have that time and computation of power. So for the

25
00:02:37,600 --> 00:02:44,080
algorithm part, what is good is reducing from big O N square to big O N or big O N to cancel

26
00:02:44,080 --> 00:02:50,880
in is usually making that possible. So on the way I'm doing research is kind of I always think

27
00:02:50,880 --> 00:02:55,600
about what is the brute force way, what is the naive way I can chip this and can I speed it up?

28
00:02:56,480 --> 00:03:01,600
Either in time wise or reducing the memory, any part that is blocking this happening

29
00:03:01,600 --> 00:03:08,640
to produce the optimal solution and we can do approximation. And that's why I kind of doing all

30
00:03:08,640 --> 00:03:13,760
the randomized algorithms in my PhD because that's going to help with the memory and time.

31
00:03:14,240 --> 00:03:20,640
Meaning the algorithms are take advantage of randomization or you just are working on a bunch

32
00:03:20,640 --> 00:03:27,600
of random different projects. That is also true because you need our special weapon,

33
00:03:27,600 --> 00:03:32,240
this randomized the algorithm called locality sends to be hashing all kind of applications

34
00:03:32,240 --> 00:03:43,280
to tackle different problems. Both. Got it. Got it. So this locality hashing is kind of well

35
00:03:43,280 --> 00:03:50,800
obviously at the core of the paper you presented at the MSS workshop slide. What is the full name of

36
00:03:50,800 --> 00:03:58,400
that paper? Let me see we changed a bunch of times. I think on the first version we're trying to

37
00:03:58,400 --> 00:04:05,120
saying that we're defending for the smart algorithms over specialized hardwares. And then we found

38
00:04:05,120 --> 00:04:11,840
a kind of offensive to some people because we're not saying hardware development is not good. We're

39
00:04:11,840 --> 00:04:18,400
just saying that we also need more progress on the algorithm side. So we're just changing the name

40
00:04:18,400 --> 00:04:26,560
back to we're beating like side trying to beat GPU over like CPU stuff. So it's like the result

41
00:04:26,560 --> 00:04:34,400
of the paper. So nobody gets offended by the CPU GPUs beating up on CPUs except Intel, right?

42
00:04:37,440 --> 00:04:42,240
Yeah, we actually have three more collaborators after the first version of the paper came out.

43
00:04:42,240 --> 00:04:48,000
They also help us further speed it up and by using all those Intel tricks.

44
00:04:48,000 --> 00:04:54,400
Before we get into the kind of the core innovation of the paper, what is the motivation? You've

45
00:04:54,400 --> 00:05:00,800
hinted at this also. Everything takes too long but maybe talk a little bit more about the specific

46
00:05:00,800 --> 00:05:07,360
problem that you're trying to solve with this paper. Right. So we all know that now all the

47
00:05:07,360 --> 00:05:13,840
applications like vision and LP and they all kind of have the state of art by using the neural

48
00:05:13,840 --> 00:05:18,560
networks. But the problem one bottleneck of the neural network is the computation. That's why

49
00:05:19,840 --> 00:05:26,560
the inspiring GPUs were invented and then people have the computation of power to kind of do this

50
00:05:26,560 --> 00:05:31,680
all complicated computations. So the major bottleneck is still the matrix multiplication.

51
00:05:32,400 --> 00:05:38,720
And if you can do like large matrix multiplication really fast, then that's kind of solving

52
00:05:38,720 --> 00:05:45,520
half of the computational problem in your networks. And that's why we started from those. The problem

53
00:05:45,520 --> 00:05:52,160
which is like called extreme classification. So basically, for example, you have your Amazon

54
00:05:52,160 --> 00:06:00,000
user. So you're typing something like I want like key shoes. And then in the back end, hopefully

55
00:06:00,000 --> 00:06:08,960
you're going to have a black loss which can help you decide which products to show to you.

56
00:06:08,960 --> 00:06:13,920
Then this kind of reduced to like extreme classification because you're typing a query that's

57
00:06:13,920 --> 00:06:19,600
your query belong to for 1 million classes, which one is it? One class can be added as shoes.

58
00:06:19,600 --> 00:06:26,000
One class can be like Nike run issues like that. So for this kind of problem, the major computation

59
00:06:26,000 --> 00:06:31,600
is the last layer because you have 1 million classes. Then you have to do all this,

60
00:06:32,240 --> 00:06:39,440
the major computation over there. And then we're thinking about is GPU the only way to solve this

61
00:06:39,440 --> 00:06:45,200
problem. And because we realized for this kind of problem, even GPU, the speed up is not enough

62
00:06:45,200 --> 00:06:52,320
because like the high memory excess. And then we're thinking about all the computations necessary.

63
00:06:52,320 --> 00:06:58,640
Then the answer is actually no because majority of it is redundant because in the neural networks,

64
00:06:59,600 --> 00:07:06,320
if like the heater layers, usually you care about the high activations, which is like to

65
00:07:06,320 --> 00:07:13,840
inner product producing high results. And for the last layer is like even worse because for those

66
00:07:13,840 --> 00:07:20,400
kind of extreme classification, what you care is about Nike shoes, Adidas shoes, but you don't

67
00:07:20,400 --> 00:07:26,480
care about a headphone. So those irrelevance of what the computation is kind of wasted.

68
00:07:27,120 --> 00:07:32,960
But the problem is before you do the computation, you don't know which part are important,

69
00:07:32,960 --> 00:07:40,400
which part does not. And then the hatching algorithms is exactly good for these kind of situations,

70
00:07:40,400 --> 00:07:46,080
kind of predict which computation are going to give you like high density so that you just compute

71
00:07:46,080 --> 00:07:52,560
those to go through approximation of the few for the full computation. And then in this way,

72
00:07:53,360 --> 00:07:59,120
all the computations in the neural networks will be sparse and including the backwards.

73
00:08:00,480 --> 00:08:04,640
So that's how we come up with the problem and find the solution.

74
00:08:05,600 --> 00:08:12,160
So you mentioned hatching algorithms are useful here and telling you which of your

75
00:08:12,160 --> 00:08:19,600
computations are going to be useful. Why and how is that? Because usually locality sensitive

76
00:08:19,600 --> 00:08:26,720
hatching is very useful for the search or ranking industry. The reason why it is good for finding

77
00:08:26,720 --> 00:08:33,280
the nearest neighbors. And in the in the neural network space, what is near neighbors is like if two

78
00:08:33,280 --> 00:08:39,680
vectors attend to have the similar direction, then the inner product will tend to be higher.

79
00:08:39,680 --> 00:08:46,240
Let's assume that a uniform norm here. So in this way, vector A and B are neighbors

80
00:08:46,240 --> 00:08:52,400
if they can produce high inner product. The hatching is kind of if you know vector A,

81
00:08:52,400 --> 00:08:59,280
they will, they will, we can spend cash in time find all the b's which are close to a

82
00:08:59,280 --> 00:09:06,800
neighbors of A and producing those b's which has the higher inner products with A. And that's why

83
00:09:06,800 --> 00:09:13,760
I call it like a useful algorithm exactly for this kind of situation. Got it. So locality sensitive

84
00:09:13,760 --> 00:09:18,400
hatching, that's something that has been around. That's not something you created. Yeah. That's

85
00:09:18,400 --> 00:09:24,800
been like 20 years or so. Been around for 20 years. And what are some of the other applications

86
00:09:24,800 --> 00:09:32,640
in which it tends to come up? You mentioned search. Yeah. Usually so for locality sensitive hatching,

87
00:09:32,640 --> 00:09:41,520
one big application is if you have let's say 1 million images or 1 billion images, you can't

88
00:09:41,520 --> 00:09:49,120
and then you have a image of query on hand, for example, a cat. You cannot search linearly for

89
00:09:49,840 --> 00:09:57,280
the 1 billion images in a database because that's kind of cause you take a long time. Yeah.

90
00:09:57,280 --> 00:10:04,240
So one, the only smart way is you save the 1 million images based on some kind of structures,

91
00:10:04,240 --> 00:10:10,240
which we call it like neighbors goes to the same bucket so that whenever you have a query,

92
00:10:10,240 --> 00:10:15,520
they will quickly use the same way to compute the hash code and go to the bucket which contains

93
00:10:15,520 --> 00:10:21,120
all the neighbors so that your search time is bigger one. Otherwise, when you type something

94
00:10:21,120 --> 00:10:27,760
on Google, it's probably take you ages to search for all the information currently we have now. So

95
00:10:27,760 --> 00:10:33,120
we have to reduce the search time to big old lines that have big old end and that's all the

96
00:10:33,120 --> 00:10:42,560
hashing algorithms are useful. Producing the hash is it kind of a deterministic process or is

97
00:10:42,560 --> 00:10:47,520
there a formula or is it a learned process? Are you learning the hashes across the data set?

98
00:10:47,520 --> 00:10:54,160
So there are two categories. One is the learned one and we're actually on the other side is

99
00:10:54,160 --> 00:11:00,320
like data independent. But the hash functions are not deterministic is joined from some distribution

100
00:11:00,320 --> 00:11:07,680
and all the hash functions are all in guarantee that the collate like the for example, you have two

101
00:11:07,680 --> 00:11:15,680
item are exactly the same and then the probability that searching that element will be one. But if the

102
00:11:15,680 --> 00:11:22,000
element has some minor differences, then the probability will be different. So by calling probability

103
00:11:22,000 --> 00:11:28,320
is each time you generate the hash function is probably different ones. So we can only say that

104
00:11:28,320 --> 00:11:33,600
it's with high probability we're going to retrieve the neighbors. It seems like if the hash function

105
00:11:33,600 --> 00:11:40,000
is going to determine the proximity of different pieces of data it needs to know something

106
00:11:40,000 --> 00:11:47,280
structural about the data and I'm wondering then does that mean that you have to do a lot of

107
00:11:47,280 --> 00:11:56,880
work up front to create this hash function based on a given data set and then anytime you want to

108
00:11:56,880 --> 00:12:04,720
look at a different type of data do you have to redo that? No, but we actually provide the freedom

109
00:12:04,720 --> 00:12:11,920
for users to implement their own hash functions. So there are four things we're providing for

110
00:12:11,920 --> 00:12:18,560
different kind of similarities. I think for hash functions, there are two lines over the first

111
00:12:18,560 --> 00:12:23,840
one is the learned hash functions. We're currently not going to that line but people have freedom to

112
00:12:23,840 --> 00:12:29,840
adding those solutions to our framework as well. But the second one is we're focusing on the

113
00:12:29,840 --> 00:12:36,480
data independent one. By data independent, I'm saying that not only based on the data but also

114
00:12:36,480 --> 00:12:42,800
based on different similarities. For example, the most useful ones in our framework currently

115
00:12:42,800 --> 00:12:49,680
is cosine similarity. I would just talk about if you want two vectors to determine which vectors are

116
00:12:49,680 --> 00:12:55,440
around one query vector, then cosine similarity might be the most useful one because that's kind of

117
00:12:55,440 --> 00:13:01,760
determining the angle, which is the direction. And another one is like the ranking. So they don't

118
00:13:01,760 --> 00:13:08,160
care about the normal vector or like the sequence of the vector, but they care about the rank like

119
00:13:08,160 --> 00:13:13,600
for a given vector. The first one, the first dimension is larger than the second, the second is

120
00:13:13,600 --> 00:13:18,640
larger than the foot. Like this kind of preserved order, that's extremely useful for images.

121
00:13:18,640 --> 00:13:25,440
And there are a third one, which is called main hash, which is kind of preserved the

122
00:13:25,440 --> 00:13:31,680
jacot similarities, like for a two given set is preserved the similarity between it. So it's

123
00:13:31,680 --> 00:13:38,400
actually for different similarity and different kinds of data, you can use different kinds of

124
00:13:38,400 --> 00:13:47,120
hash functions to achieve the same guarantees. And I'm recently also getting to the learn hash

125
00:13:47,120 --> 00:13:53,680
function that part, but the one particular problem that is extremely trouble our framework is,

126
00:13:53,680 --> 00:14:00,960
our framework is not, we're going to change the hash functions or review the hash tables once in a

127
00:14:00,960 --> 00:14:07,040
while because we want to preserve the randomness, but the rebuilding part for those learned hash

128
00:14:07,040 --> 00:14:13,440
functions will be very long. Yeah, so that's a good thing about the data independent hash,

129
00:14:13,440 --> 00:14:22,080
because it is not time consuming. But the interest in the learned hash functions is that they

130
00:14:22,080 --> 00:14:28,960
might be more accurate or allow you to express different types of similarity. Yeah, that is

131
00:14:28,960 --> 00:14:36,320
correct. That's why we are seriously considering this case in our framework, because our frame

132
00:14:36,320 --> 00:14:45,200
ultimately wants to spend very less time predicting which neurons should be active, and then do

133
00:14:45,200 --> 00:14:52,080
all the computation on CPU. There's actually very hard tasks. You have to do it very quickly.

134
00:14:52,720 --> 00:14:58,400
Otherwise, that overhead is going to kill you. So that's why I'm talking about like for the

135
00:14:58,400 --> 00:15:05,920
learn hash function, although it's very accurate, it's going to help us like spend maybe 10 times, if the

136
00:15:05,920 --> 00:15:15,040
current time of computing hash functions, and that's going to make the framework maybe comparatively

137
00:15:15,040 --> 00:15:23,760
okay with GPU, but not obvious speed up. So the solution is doable. I think there's a time and

138
00:15:23,760 --> 00:15:30,800
accuracy trade-off here, but we think the independent hash functions are along the sweet spots.

139
00:15:31,360 --> 00:15:37,840
And so what's the relationship between the hash function and the idea of adaptive sparsity?

140
00:15:37,840 --> 00:15:45,120
That's a concept that you mention in the paper. Right. So just considering the random dropout

141
00:15:45,120 --> 00:15:50,640
is actually like each time you're just giving sparsity to the neural network randomly,

142
00:15:50,640 --> 00:15:58,160
so that you're is highly possible that you're going to meet some important parts if you do that

143
00:15:58,160 --> 00:16:06,560
randomly. So by adaptive sparsity here is that's why like for the random dropout, the sparsity can

144
00:16:06,560 --> 00:16:14,720
maximum goes to like 50% or so, otherwise you will decrease your accuracy. But we do another

145
00:16:14,720 --> 00:16:23,280
experiment and it's actually finding a lot of like previous papers to if we can compute all the

146
00:16:23,280 --> 00:16:32,000
computations and we only take maybe 5% or 1% of nodes as the new of the active nodes, then the

147
00:16:32,000 --> 00:16:38,560
accuracy will not drop. But the problem is you have to make the huge computation to know which ones

148
00:16:38,560 --> 00:16:45,600
we're going to produce you the top ones. So that's inspiring us like we do approximate like 5%

149
00:16:45,600 --> 00:16:54,160
or 10%. But so the speed is kind of close to the random dropout, but the effectiveness of it

150
00:16:54,160 --> 00:17:00,880
is close to the adaptive top K. So that's why we find the sweet spot in this too.

151
00:17:00,880 --> 00:17:07,200
Does that mean that you're applying the location sensitive hashing in two places?

152
00:17:07,200 --> 00:17:14,480
One at the very end for your classifier and then also more deeply in the dropout.

153
00:17:15,120 --> 00:17:23,040
So dropout is actually in parallel with our framework. I'm just giving an example of how the

154
00:17:23,040 --> 00:17:29,120
hashing is going to help. If hashing is doing a random selection of the nodes is exactly the same

155
00:17:29,120 --> 00:17:37,760
as the dropout. But if is computing the real top K, then that's called the the adaptive

156
00:17:37,760 --> 00:17:44,320
sparsity. But it's somewhere in between it's using the same time is very efficient as the random

157
00:17:44,320 --> 00:17:51,680
dropout because the query only take bigger one time. But it's going to retrieve you the nodes that

158
00:17:51,680 --> 00:17:59,120
is highly possible in that top K. So I'm just like giving an intuition of what hashing with selecting.

159
00:17:59,120 --> 00:18:07,280
It's nothing but just selecting adaptively the active nodes for each layer. So what is very

160
00:18:07,280 --> 00:18:13,920
useful in the case of extreme classification is because of the full computing like the majority

161
00:18:13,920 --> 00:18:19,440
of the computation is in the last layer because of the number of classes. So that's going to reduce

162
00:18:19,440 --> 00:18:25,440
a lot of computation. But for the usual fully connecting neural network, each layer we can

163
00:18:25,440 --> 00:18:32,560
build different hash tables and we can do is sparsity over each layer. So that's not restricted

164
00:18:32,560 --> 00:18:36,800
to the last layer. Are there other elements of this approach that we should be sure to cover?

165
00:18:36,800 --> 00:18:45,120
There's the application of the hashing. Do you have to do anything special in terms of your

166
00:18:45,120 --> 00:18:51,360
data collection or preparation or should you just be able to apply this technique?

167
00:18:53,360 --> 00:19:00,240
So we're actually exploring two directions of extension to this work. So this is our current

168
00:19:00,240 --> 00:19:07,280
framework support the fully connected neural networks. But people are more interested in when they

169
00:19:07,280 --> 00:19:13,760
have like the NLP or vision problem they want the CN or LSDM. Currently what we can do is at least

170
00:19:13,760 --> 00:19:21,360
for last layer we can apply the same technique. But the problem is for CN or LSDM in a mid like those

171
00:19:21,360 --> 00:19:28,960
heat layers we have to investigate there to see what advantage is the hash in the approach.

172
00:19:28,960 --> 00:19:35,040
And for not a line of work we're thinking about the distributed because it's well known that for

173
00:19:35,040 --> 00:19:42,640
the distributed setting the computational bottleneck is the communication. And our slide has

174
00:19:42,640 --> 00:19:50,880
is natural advantage here because of the we're only selecting for example 1% or 5% of the active

175
00:19:50,880 --> 00:19:58,000
nodes for each layer so that in the back propagation the gradient updates is also very sparse.

176
00:19:58,000 --> 00:20:03,200
So that for the distributed setting this is natural very efficient.

177
00:20:03,840 --> 00:20:11,840
For the same reasons that we've talked about previously intelligently using the hashing to kind

178
00:20:11,840 --> 00:20:18,160
of identify and strip out the sparsity. Yeah. How have you kind of measured the results and

179
00:20:18,160 --> 00:20:24,640
compared this approach to others and then what kind of results have you seen? Yeah for the results

180
00:20:24,640 --> 00:20:33,680
part that's the most impact for people care about. So for GPU comparison we're using the current

181
00:20:33,680 --> 00:20:42,320
state of art framework TensorFlow and also use also run the same program on a via 100 GPU which is

182
00:20:42,320 --> 00:20:49,680
currently kind of the state of art people use to run to perform all the experiments for those

183
00:20:49,680 --> 00:21:01,280
applications. And also for TensorFlow CPU we also try to see that for all those CPU tricks

184
00:21:01,280 --> 00:21:08,080
they're using are we beating them? And obviously you will be slower than the TensorFlow GPU

185
00:21:08,080 --> 00:21:15,040
but we still do some comparison there to see like there's three lines which ours is the most

186
00:21:15,040 --> 00:21:23,520
efficient and then TensorFlow GPU on via 100 and TensorFlow CPU on the same CPU machine we're using

187
00:21:23,520 --> 00:21:30,320
because we want to still compare with the CPU otherwise we can use a super powerful CPU machine

188
00:21:30,320 --> 00:21:36,880
and then that's then using that to be the GPU is not necessary for people because we want people

189
00:21:36,880 --> 00:21:44,320
to do the same thing on CPU which is not that costly as using like a via 100 or this kind of

190
00:21:45,520 --> 00:21:53,680
expensive GPU machines. And also there's a nice finding about how many cores we include

191
00:21:53,680 --> 00:22:02,960
in our CPU computation so that we can beat the TensorFlow GPU. It seems like 8 cores to 16 cores

192
00:22:02,960 --> 00:22:15,120
is enough to outperform the same task on GPU via 100. So although we're in the in the paper

193
00:22:15,120 --> 00:22:22,800
we're using like a 44 core machine but we do the obligation study to see where it intersects

194
00:22:22,800 --> 00:22:29,920
the performance is actually 8 to 16 cores somewhere in the middle. So with 8 to 16 cores you're

195
00:22:29,920 --> 00:22:39,840
able to to what outperform TensorFlow on a GPU? Yes. Okay. What's the problem that you're

196
00:22:39,840 --> 00:22:47,120
that you're benchmarking this on? Those are extreme classification tasks. Oh, extreme classification.

197
00:22:47,120 --> 00:22:54,240
And can you talk a little bit about are there established data sets and benchmarks for extreme

198
00:22:54,240 --> 00:23:01,280
classification? Can you talk a little bit about those? Yes. So there's a whole repo about all those

199
00:23:01,280 --> 00:23:08,320
for example the example that I gave earlier for Amazon data is usually like yeah you have query

200
00:23:08,320 --> 00:23:12,960
and you have all the products that's like extreme classification or sometimes you have

201
00:23:12,960 --> 00:23:18,800
a user typing query and then you have a search results for the finally the website you are

202
00:23:18,800 --> 00:23:25,360
clicking. That's like another case. Yeah. So all those benchmarks including one repo, all the

203
00:23:25,360 --> 00:23:33,920
extreme classification field like people use those as the benchmarks. For the Amazon you said

204
00:23:33,920 --> 00:23:42,320
how many classes are there? For the data set we're using is 670K. Okay. And then use another

205
00:23:42,320 --> 00:23:49,440
delicious data set that's like the one where you're predicting links. Yeah, it was like 200K.

206
00:23:50,080 --> 00:23:56,240
So you've got a first set of diagrams where you've got the three lines and that one is looking at

207
00:23:56,240 --> 00:24:02,960
the slide accuracy and then you have time on the bottom what is the time? Is that like convergence time

208
00:24:02,960 --> 00:24:08,880
or else? Yes, like training time. Training time? That's a lot scale. Yeah. Okay. So you've got pretty

209
00:24:08,880 --> 00:24:14,320
strong results here. Do you have you had anyone take this and implement it in practice?

210
00:24:15,440 --> 00:24:23,840
Kind of an industry or for a kind of a live application? Yeah. I receive a couple of emails requesting

211
00:24:23,840 --> 00:24:30,640
we have a license there and then people can implement it and Intel is extremely of course there will

212
00:24:30,640 --> 00:24:37,600
be super interesting. Yeah, they're working on it to include to their some kind of libraries

213
00:24:37,600 --> 00:24:44,640
in the future too. There's one thing that I want to share. So the all the speedouts we're getting

214
00:24:44,640 --> 00:24:52,000
here is not about all those CPL plans or implementation tricks. We don't even use cool glass or this kind

215
00:24:52,000 --> 00:24:59,040
of libraries for speeding up the computation which is use all the primitives and all the

216
00:24:59,040 --> 00:25:04,480
computations we're saving here because of the smart algorithms. So because we're only choosing

217
00:25:04,480 --> 00:25:13,600
five percent of the nose active nose so that the computation was saved. So I think this framework

218
00:25:14,640 --> 00:25:21,840
if viewed like we can have further speed up if those techniques tricks are applying to this.

219
00:25:21,840 --> 00:25:29,280
But this is just like a prototype which proves that we can do this with smart algorithms.

220
00:25:29,280 --> 00:25:36,080
Kind of going back to the result you found that the I found the number here the convergence time

221
00:25:36,080 --> 00:25:44,560
required is over two and a half times less than with TensorFlow GPU and then I was looking for your

222
00:25:45,280 --> 00:25:52,480
accuracy number here. How does it compare in terms of accuracy? There's no accuracy drop.

223
00:25:52,480 --> 00:25:57,840
Okay, so you're able to maintain accuracy but um, converge more quickly. Yes. So you've done your

224
00:25:57,840 --> 00:26:07,920
benchmarking for this extreme classification task which has inherent sparsity to it. Are there

225
00:26:07,920 --> 00:26:15,360
other types of problems that you think that this would easily lend itself to due to the because

226
00:26:15,360 --> 00:26:22,960
they also have high degrees of sparsity or do you see this as you know are there ways to kind of

227
00:26:22,960 --> 00:26:29,840
adapt this approach so that it you can apply it to use cases where they're not quite as inherently

228
00:26:29,840 --> 00:26:36,480
sparse. So you're talking about the approach we're having, does that work? The approach you're taking

229
00:26:36,480 --> 00:26:41,760
and the applicability and you know what are the requirements of the scenarios in which it would

230
00:26:41,760 --> 00:26:48,320
apply is you know would they have to be as sparse as the extreme classification task or can

231
00:26:48,320 --> 00:26:55,920
you see some ways that they can apply more broadly. You can be applied broadly as well as I

232
00:26:55,920 --> 00:27:02,160
just mentioned for example, we're currently working on if we can use this speeding up on the

233
00:27:02,160 --> 00:27:10,000
training of BERT which is very popular recently. So the difficulty of applying this to an

234
00:27:10,000 --> 00:27:17,120
existing framework is we have to profile where is the computation bottleneck because speeding up

235
00:27:17,120 --> 00:27:25,280
the non bottleneck won't help and because we're producing some overhead as well and that's not

236
00:27:25,280 --> 00:27:32,480
gonna suite the whole process up anyway. So we have to find the bottleneck of the training and if

237
00:27:32,480 --> 00:27:38,880
this technique can in this technique needs the sparsity. So if we do experiment saying that

238
00:27:39,680 --> 00:27:46,880
we're replacing our algorithm with like the real top K and it's decreasing the accuracy too

239
00:27:46,880 --> 00:27:56,960
much then it's not worth investigating. But if we're for example choose the real top 5% of the

240
00:27:56,960 --> 00:28:03,040
active nodes and the accuracy still maintains, then in this case there's hope for us to do this

241
00:28:03,040 --> 00:28:08,320
approximation because like the kind of like the ground choose works but now we just need to speed it

242
00:28:08,320 --> 00:28:20,080
up. So it depends on the architectures of the network. So it's not something that you would

243
00:28:20,080 --> 00:28:25,280
envision ever being kind of a generic tool in the toolkit that you would apply to different

244
00:28:25,280 --> 00:28:32,480
types of problems. You have to kind of hand tailor the application of this technique to the different

245
00:28:32,480 --> 00:28:39,920
to different algorithms and where they're bottleneck. Why I think this is still generic is because

246
00:28:39,920 --> 00:28:46,560
in all those neural networks or architectures anyway reduced to the matrix multiplication problem.

247
00:28:47,440 --> 00:28:55,360
Once that one is there this should be useful but it's just like we need to see which where do we

248
00:28:55,360 --> 00:29:03,360
where does it worth it to apply to. And we're also looking for like a GPU implementation of the same

249
00:29:03,360 --> 00:29:11,120
method as well because the major bottleneck for this technique currently going to GPU is because

250
00:29:11,120 --> 00:29:17,920
of the hashing algorithm because for those many years hashing only has the CPU implementation

251
00:29:17,920 --> 00:29:23,600
because it requires a lot of memory because it's remembering something right we save something

252
00:29:23,600 --> 00:29:32,560
in the hash table. And recently we see a step forward to the dynamic hash tables and GPUs

253
00:29:33,120 --> 00:29:40,560
from UC Davis folks. So for those I kind of were saying that it might be possible to implement

254
00:29:42,560 --> 00:29:50,560
like a locality sensitive hashing on GPU and then this can further speed up GPU and that's like

255
00:29:50,560 --> 00:29:57,600
also a possible future direction of the framework. Awesome awesome well maybe thanks so much

256
00:29:57,600 --> 00:30:27,440
for taking the time to share a little bit about what you're working on. It's very cool stuff. Thanks.

