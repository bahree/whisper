WEBVTT

00:00.000 --> 00:15.920
Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting

00:15.920 --> 00:20.880
people doing interesting things in machine learning and artificial intelligence.

00:20.880 --> 00:23.560
I'm your host Sam Charrington.

00:23.560 --> 00:26.720
A few updates before we jump into our show.

00:26.720 --> 00:31.360
This week, October 10th through 11th, I'll be in Montreal for the rework deep learning

00:31.360 --> 00:32.560
summit.

00:32.560 --> 00:37.400
We held a flash giveaway last week for one lucky listener to get a chance to join me

00:37.400 --> 00:38.880
at the conference.

00:38.880 --> 00:42.280
And that winner is Matt Stone, a listener from Toronto.

00:42.280 --> 00:46.760
Thanks to everyone who entered and be on the lookout for more giveaways to close out

00:46.760 --> 00:47.760
the year.

00:47.760 --> 00:51.220
Hey, if you're in the Montreal area, I'd love to connect.

00:51.220 --> 00:56.040
Reach out to me via Twitter or the contact page at twimlai.com if you'd like to meet

00:56.040 --> 00:57.040
up.

00:57.040 --> 01:01.520
The show you're about to hear is the first of a series of shows recorded in San Francisco

01:01.520 --> 01:06.120
at the Artificial Intelligence Conference, which was hosted by our friends at O'Reilly

01:06.120 --> 01:07.800
and Intel Nirvana.

01:07.800 --> 01:12.640
In addition to their support for the event itself, Intel Nirvana is also our sponsor for this

01:12.640 --> 01:15.040
series of podcasts from the event.

01:15.040 --> 01:19.080
A huge thanks to them for their continued support of the show.

01:19.080 --> 01:24.120
In this episode, I speak with Navine Rao, Vice President and General Manager of Intel's

01:24.120 --> 01:28.840
AI Products Group and Scott Appland, Director of Intel's Developer Network.

01:28.840 --> 01:33.400
It's been a few months since we last spoke with Navine, so he gives us a quick update on

01:33.400 --> 01:38.040
what Intel has been up to and we discuss his perspective on some recent events in the

01:38.040 --> 01:39.560
AI ecosystem.

01:39.560 --> 01:45.280
Then, Scott and I dig into Intel Nirvana's new dev cloud offering, which was announced

01:45.280 --> 01:46.600
at the conference.

01:46.600 --> 01:52.000
We also discussed the Intel Nirvana AI Academy, a new portal offering hands-on learning

01:52.000 --> 01:57.080
tools and other resources for various aspects of machine learning and AI.

01:57.080 --> 02:02.080
In addition to my conversations with Navine and Scott, this show is packed with more interviews

02:02.080 --> 02:08.520
that I know you'll love, including Paul Tepper of Nuance, on conversational interfaces,

02:08.520 --> 02:16.120
Gunner Carlson of Stanford and IASD on topological data analysis, Laura Freilich and Mo Patel

02:16.120 --> 02:21.800
of Think Big Analytics on some interesting machine learning use cases they've implemented,

02:21.800 --> 02:27.760
and Ian Steuiger of UC Berkeley on his work at the Ryze Lab and Ray, a new platform for

02:27.760 --> 02:29.520
reinforcement learning.

02:29.520 --> 02:34.360
Finally, a quick reminder about the upcoming Twimble Online Meetup.

02:34.360 --> 02:40.680
On Wednesday, October 18th, at 3pm Pacific time, we'll be discussing the paper visual

02:40.680 --> 02:47.640
attribute transfer through deep image analogy by Jing Li Yao and others from Microsoft Research.

02:47.640 --> 02:50.400
The discussion will be led by Duncan Stothers.

02:50.400 --> 02:54.720
To join the meetup, or to catch up on what you missed from the first two meetups, visit

02:54.720 --> 03:00.400
twimbleai.com slash meetup, and now on to the show.

03:00.400 --> 03:08.520
All right, everyone.

03:08.520 --> 03:14.600
I am here at the AI conference brought to you by O'Reilly and Intel Nirvana and I'm here

03:14.600 --> 03:20.920
with Naveen Rao, who is the general manager of Intel's Artificial Intelligence products

03:20.920 --> 03:21.920
group.

03:21.920 --> 03:22.920
Welcome, Naveen.

03:22.920 --> 03:23.920
Thanks, Sam.

03:23.920 --> 03:24.920
Great to be here.

03:24.920 --> 03:25.920
It's great to have you here.

03:25.920 --> 03:30.520
We're going to spend a few minutes this morning just catching up on what Intel Nirvana

03:30.520 --> 03:35.120
has been up to, and then we've got a really interesting interview schedule to talk about

03:35.120 --> 03:38.760
one of the products that you guys are announcing here at the event.

03:38.760 --> 03:40.560
Sounds great.

03:40.560 --> 03:41.560
What's new?

03:41.560 --> 03:44.760
Well, it's been a very eventful few months.

03:44.760 --> 03:49.200
We started the Artificial Intelligence products group about five months ago now, and really

03:49.200 --> 03:53.960
getting resources around Intel aligned around this focus area now.

03:53.960 --> 03:58.200
The company has been super supportive from the top down to make this happen, and I think

03:58.200 --> 04:02.320
it's a reflection of the importance of AI to Intel.

04:02.320 --> 04:05.840
We want to be at the forefront of it, not just from the product standpoint, but also

04:05.840 --> 04:08.800
from the research standpoint.

04:08.800 --> 04:11.880
And so you made some announcements at the event today.

04:11.880 --> 04:13.400
Tell us a little bit about those.

04:13.400 --> 04:15.760
A lot of it is around our connection with developers.

04:15.760 --> 04:21.280
The Dev Cloud was one where we can connect with developers, get them working on our tools,

04:21.280 --> 04:22.920
but also training people.

04:22.920 --> 04:28.280
There's a big appetite right now for knowledge and understanding how to build AI solutions

04:28.280 --> 04:31.080
and how to actually solve industry problems.

04:31.080 --> 04:36.920
We put together a very nice set of tutorials in Tel Nirvana Academy, and also having computational

04:36.920 --> 04:41.280
support for those tools and actually computational resources available is what the Dev Cloud is

04:41.280 --> 04:42.280
about.

04:42.280 --> 04:46.480
And you know, that'll actually go hand in hand with Intel Nirvana Cloud coming up in the

04:46.480 --> 04:49.720
future where you can get access to the latest and greatest technologies.

04:49.720 --> 04:50.720
Okay.

04:50.720 --> 04:55.160
So where can folks find the tools that you, the documentation and tools that you released

04:55.160 --> 04:58.960
from Intel Nirvana.com or just Google Intel Nirvana Academy?

04:58.960 --> 04:59.960
Okay.

04:59.960 --> 05:00.960
Awesome.

05:00.960 --> 05:02.920
And so is the Dev Cloud?

05:02.920 --> 05:06.880
What's the relationship between the Dev Cloud and Intel Nirvana Cloud that's forthcoming

05:06.880 --> 05:07.880
to the Dev Cloud?

05:07.880 --> 05:11.840
The Dev Cloud is really meant for developers and education, whereas the Intel Nirvana

05:11.840 --> 05:16.000
Cloud is actually the enterprise grade solution for companies.

05:16.000 --> 05:20.480
So you know, one is more, here's access to our tools and a package format.

05:20.480 --> 05:24.920
The other thing is the Intel Nirvana Cloud is really our latest technology and a very kind

05:24.920 --> 05:26.960
of low cost way of getting access to it.

05:26.960 --> 05:27.960
Okay.

05:27.960 --> 05:32.640
And then you've got a keynote tomorrow that you're doing with Steve Jarvison.

05:32.640 --> 05:34.360
What are you going to be talking about there?

05:34.360 --> 05:38.760
Well, as you know, Steve may have been, as you may know, Steve was one of our early investors

05:38.760 --> 05:43.800
at Nirvana and has really become an industry expert in this area.

05:43.800 --> 05:47.800
And you know, I find it very fortunate to have him involved with us from the start guiding

05:47.800 --> 05:48.800
some of our direction.

05:48.800 --> 05:52.360
And it'll really just be a conversation between us talking about some of the things that

05:52.360 --> 05:57.640
are exciting coming up, you know, how hardware is going to evolve for the future of AI and

05:57.640 --> 05:59.400
drive the future of AI.

05:59.400 --> 06:00.400
What is the future of AI?

06:00.400 --> 06:01.760
What kind of problems will you be solving?

06:01.760 --> 06:02.760
Why is it important?

06:02.760 --> 06:05.480
It's really going to be a conversation and I think it should be entertaining.

06:05.480 --> 06:07.080
Oh, what is the future of AI?

06:07.080 --> 06:09.280
I'm going to save that for tomorrow.

06:09.280 --> 06:13.120
Well, I promise you no one will hear this before tomorrow.

06:13.120 --> 06:14.120
Okay.

06:14.120 --> 06:19.440
I think one of the main messages from my perspective is AI is really a set of techniques

06:19.440 --> 06:21.920
that allow us to scale intelligence.

06:21.920 --> 06:27.200
We can now take in more resources that we could possibly do as individual humans, you

06:27.200 --> 06:30.920
know, loosely coupled by language that we're hitting that fundamental limitation now

06:30.920 --> 06:33.760
because we've gotten really good at gathering data.

06:33.760 --> 06:37.960
And now we have too much data and we actually can't do much with it as individuals.

06:37.960 --> 06:39.360
Humans can't scale that level.

06:39.360 --> 06:41.560
So we need to build technologies to allow us to do it.

06:41.560 --> 06:46.000
And it's really kind of continuing along the evolution of the scientific method.

06:46.000 --> 06:51.520
Humans forms this procedure to really make testable, repeatable results and try to get

06:51.520 --> 06:53.080
to the ground truth of the world.

06:53.080 --> 06:54.320
And I think this is part of that, right?

06:54.320 --> 06:56.000
We just want to do it on a bigger scale.

06:56.000 --> 06:59.520
You know, one human can only experience so much, but we can actually now put devices

06:59.520 --> 07:02.400
out in the world that can experience a lot and bring it all together.

07:02.400 --> 07:06.640
So that's really what I think AI is about and why it's important.

07:06.640 --> 07:10.440
It's really allowing us to continue along the evolution of what it means to be human.

07:10.440 --> 07:11.440
Mm-hmm.

07:11.440 --> 07:15.400
And one of the things that came up in the key notes this morning was, it's related

07:15.400 --> 07:16.840
to a comment you just made.

07:16.840 --> 07:22.200
We've become really good at gathering and collecting data, but not necessarily so good at collecting

07:22.200 --> 07:28.280
label data sets that are trainable to create machine learning models and AI models, is

07:28.280 --> 07:30.440
Intel doing anything in that space at all?

07:30.440 --> 07:31.440
Yeah.

07:31.440 --> 07:34.080
So, I mean, there are three fundamental types of learning.

07:34.080 --> 07:36.360
There's what you just described would be called supervised learning.

07:36.360 --> 07:40.480
This is really about taking data that's already been annotated by human and saying this

07:40.480 --> 07:43.240
is what we want to get out of a model.

07:43.240 --> 07:46.000
Reinforcement learning is more like kind of how you train your dog.

07:46.000 --> 07:49.520
Dog gets something good, you give him a treat, and he does that good thing more often.

07:49.520 --> 07:53.520
The other, I would say, the big frontier right now is unsupervised learning.

07:53.520 --> 07:54.520
Right.

07:54.520 --> 07:57.600
And that's really about finding potentially useful structure and data before you know

07:57.600 --> 07:59.280
what you want to do with it.

07:59.280 --> 08:04.880
And that's really what needs to be cracked to enable this kind of scalability, because

08:04.880 --> 08:07.720
actually going through and labeling all the data in the world is not possible.

08:07.720 --> 08:08.720
Right.

08:08.720 --> 08:12.480
Just if there's too much already, if we froze the world today and handed out 100 megabytes

08:12.480 --> 08:16.480
to every man, woman, and child on the planet, we would take us 30 years to get through

08:16.480 --> 08:17.480
all of it.

08:17.480 --> 08:19.200
So, you know, that's not something we can, that's possible.

08:19.200 --> 08:22.760
The only way to do it is to crack it from an algorithmic standpoint and then throw

08:22.760 --> 08:25.600
the computational horsepower at it that we're building.

08:25.600 --> 08:29.000
So that's, I think, one of the big, big things we're going to see in the next couple of

08:29.000 --> 08:31.920
years is research into the unsupervised training world.

08:31.920 --> 08:36.120
And we are doing that as well and providing tools, so our researchers do move this field

08:36.120 --> 08:37.120
forward.

08:37.120 --> 08:42.280
Are there any particular examples of progress in that space that comes to mind for you,

08:42.280 --> 08:45.480
or that you've helped, your tools are helping facilitate?

08:45.480 --> 08:49.280
I think GANs, generative adversarial networks are a big, big one.

08:49.280 --> 08:53.120
This is something that started becoming popular about a year, year and a half ago, showing

08:53.120 --> 08:55.040
some really promising results now.

08:55.040 --> 08:59.320
And this is towards more unsupervised kind of method.

08:59.320 --> 09:01.800
What our tools have helped there is really the speed.

09:01.800 --> 09:03.400
GANs are computationally intensive.

09:03.400 --> 09:09.720
They take a lot of horsepower, and a lot of time, and, you know, four years ago, you

09:09.720 --> 09:10.720
simply couldn't do it.

09:10.720 --> 09:16.880
You just didn't have the tools and computational capabilities that we have today by providing

09:16.880 --> 09:21.720
better tools faster, more scalable, larger parameter memories, these kinds of things.

09:21.720 --> 09:23.600
We can actually drive these fields forward.

09:23.600 --> 09:27.720
And that's the way I see it as, like, when we put out a new architecture that has fundamental

09:27.720 --> 09:31.840
and new capabilities, smart researchers are going to do things with it that we had never

09:31.840 --> 09:32.840
even thought of.

09:32.840 --> 09:33.840
And that's what I think is really cool.

09:33.840 --> 09:37.840
It's not the stuff that we're thinking about, that should work, but it's other stuff

09:37.840 --> 09:38.840
that's going to happen.

09:38.840 --> 09:39.840
Yeah.

09:39.840 --> 09:40.840
Very interesting.

09:40.840 --> 09:45.240
One of the things I remember from our last conversation was we were talking about Intel

09:45.240 --> 09:49.720
and Intel's role in the space, and you made a comment to the effect that, hey, you

09:49.720 --> 09:52.520
know, we're in the first inning, is that, right?

09:52.520 --> 09:58.880
And in the context of tools, I think last time we talked a little bit about the support

09:58.880 --> 10:05.640
for Nervonograph and some of the products projects that were announced last time at the AI

10:05.640 --> 10:07.640
conference in New York.

10:07.640 --> 10:12.000
And a lot of the conversation was around support for TensorFlow as kind of one of these

10:12.000 --> 10:13.680
back-end frameworks.

10:13.680 --> 10:17.440
And interestingly enough, like, I think, you know, a few months ago, everyone thought

10:17.440 --> 10:21.520
everyone thought that TensorFlow was like the Crown King and the game was over.

10:21.520 --> 10:25.200
And now, like, out of nowhere, we're talking about PyTorch all over the place.

10:25.200 --> 10:28.400
And I'm just going to go there.

10:28.400 --> 10:31.960
I'm just wondering if you have any, you know, thoughts or perspective on, you know, the

10:31.960 --> 10:36.280
market and how it will evolve and what we should expect to see.

10:36.280 --> 10:38.400
You know, you can look at the past.

10:38.400 --> 10:40.560
We saw the same thing happen with web technologies, right?

10:40.560 --> 10:42.160
Or the same thing happening.

10:42.160 --> 10:47.160
There's a new web technology to do responsive dynamic web pages out every year.

10:47.160 --> 10:50.800
I can't keep up because I don't do that stuff anymore.

10:50.800 --> 10:54.440
So I think you're going to see a very similar thing happening here when there's, you know,

10:54.440 --> 10:59.640
a new set of capabilities that kind of catapult some set of researchers, everyone was going

10:59.640 --> 11:00.640
to want to use it.

11:00.640 --> 11:01.640
Right.

11:01.640 --> 11:06.040
TensorFlow was kind of favored for the last eight months, nine months.

11:06.040 --> 11:07.040
We'll see what happens in the future.

11:07.040 --> 11:09.560
I think there's plenty of room for innovation here.

11:09.560 --> 11:11.280
It's not a done deal by any means.

11:11.280 --> 11:12.440
I think I even said that then.

11:12.440 --> 11:13.960
It's like, yeah, I've seen this happen.

11:13.960 --> 11:15.680
I've seen this game before.

11:15.680 --> 11:17.720
And, you know, we'll support what's out there, of course.

11:17.720 --> 11:22.600
You know, we want to provide innovations through our own software stack that we own and open

11:22.600 --> 11:27.040
source neon on top of end graph, but we obviously want to support what the community is using

11:27.040 --> 11:28.040
more broadly.

11:28.040 --> 11:31.880
And if that's pie towards great, if it's TensorFlow, great too, we have no problem with

11:31.880 --> 11:32.880
it.

11:32.880 --> 11:36.360
I think, you know, we're providing that computational substrate under the hood.

11:36.360 --> 11:40.640
And so we want to make sure that researchers have access to what they need.

11:40.640 --> 11:47.680
More broadly, what is the role of ecosystems and enabling AI solutions and AI tools?

11:47.680 --> 11:51.520
To evolve ecosystems are absolutely key, right?

11:51.520 --> 11:53.640
There's not one person who can do this on their own, right?

11:53.640 --> 11:55.120
It's just not going to happen.

11:55.120 --> 11:57.280
Not even one single big company, I think.

11:57.280 --> 12:01.600
There's too many smart people working on this problem to not work together in a common

12:01.600 --> 12:03.920
set of tools in a common language, right?

12:03.920 --> 12:06.840
So the way the big problems are going to solve, like, some supervised problems, things

12:06.840 --> 12:11.480
like that, is really through this openness of publishing, you know, people putting stuff

12:11.480 --> 12:12.480
on archive is great.

12:12.480 --> 12:16.640
But then having peer review and, you know, papers accepted by NIPs and ICML, these big

12:16.640 --> 12:20.520
conferences, and then providing the code to actually show the implementation.

12:20.520 --> 12:23.640
That's been a very virtuous cycle, I think.

12:23.640 --> 12:25.080
We've seen a lot of innovation happen quickly.

12:25.080 --> 12:29.360
You get something out there, the codes out there, you can download it, play with it, modify

12:29.360 --> 12:32.320
it, and then put out the next thing on top of it, right?

12:32.320 --> 12:35.880
I think it's actually a great paradigm for any kind of innovation.

12:35.880 --> 12:38.320
Yeah, absolutely, absolutely.

12:38.320 --> 12:39.920
Anything else you'd like to share with us?

12:39.920 --> 12:43.200
Yeah, I think, you know, it's going to be very exciting in the next year or so.

12:43.200 --> 12:46.440
We're going to be making a lot of announcements about some of the things coming to fruition.

12:46.440 --> 12:49.120
I mean, we're making a lot of bets right now.

12:49.120 --> 12:54.720
You know, we've come from a place where Intel is a CPU manufacturer, and that's how people

12:54.720 --> 12:58.600
are perceiving it a year ago, and now we're going to be providing leadership in AI.

12:58.600 --> 13:01.680
I think it's very exciting to see that transition and be part of it.

13:01.680 --> 13:04.240
So stay tuned for some cool things coming up.

13:04.240 --> 13:08.360
Along the line of some of those bets, you recently, or Brian recently published a blog

13:08.360 --> 13:13.280
post that talked about the billion dollars that the company is investing across the AI

13:13.280 --> 13:14.280
spectrum.

13:14.280 --> 13:15.280
Any comments on that?

13:15.280 --> 13:18.120
Yeah, there were a number of startups mentioned and some other things.

13:18.120 --> 13:23.000
Yeah, I mean, Intel is a huge, if not the biggest VC in the valley.

13:23.000 --> 13:25.400
So we invest in a lot of different kinds of companies.

13:25.400 --> 13:30.560
I mean, obviously, we look at it from strategic value standpoint, but also just from a, you

13:30.560 --> 13:33.000
know, like a monetary return, just like a VC.

13:33.000 --> 13:37.320
So we've placed a lot of bets there, and, you know, we want to see the ecosystem to build

13:37.320 --> 13:40.080
and innovate, and startups will wear a lot of that happens.

13:40.080 --> 13:43.720
Then beyond that, some of that investment is also internally what we're doing in aligning

13:43.720 --> 13:45.480
our resources around.

13:45.480 --> 13:51.160
So I think it's, again, it's a really good readout of the emphasis and focus we're putting

13:51.160 --> 13:52.160
on AI at Intel.

13:52.160 --> 13:53.160
Yeah.

13:53.160 --> 13:54.160
Absolutely.

13:54.160 --> 13:55.160
Well, great.

13:55.160 --> 13:58.160
Thanks so much, Naveen, for joining us and looking forward to catching up with you next

13:58.160 --> 13:59.160
time.

13:59.160 --> 14:00.160
Absolutely.

14:00.160 --> 14:01.160
Great talking with you.

14:01.160 --> 14:02.160
Great.

14:02.160 --> 14:03.160
All right.

14:03.160 --> 14:12.840
Everyone, I'm here at the AI conference with Scott Appland, who is the director of developer

14:12.840 --> 14:19.040
programs with Intel Nirvana, and we're here to talk about the dev cloud that was launched

14:19.040 --> 14:20.040
today.

14:20.040 --> 14:21.040
Welcome, Scott.

14:21.040 --> 14:22.040
Well, thank you.

14:22.040 --> 14:23.040
Good to be here.

14:23.040 --> 14:24.040
Great to have you.

14:24.040 --> 14:27.200
So it sounds like you had a big launch last night.

14:27.200 --> 14:28.440
How did it go?

14:28.440 --> 14:29.440
Really went well.

14:29.440 --> 14:35.120
So back about 10 months ago in November last year, we announced our AI Academy.

14:35.120 --> 14:42.000
So this was our program for helping students, developers, teachers really have more, learn

14:42.000 --> 14:45.600
more about AI, how to use it, get access to the technology and the tools.

14:45.600 --> 14:46.960
So we kicked that off.

14:46.960 --> 14:53.000
Today, we really announced the next stage of that, which is making cloud compute accessible

14:53.000 --> 14:57.240
to broad set of developers and students with our new Nirvana dev cloud.

14:57.240 --> 14:58.240
Okay.

14:58.240 --> 15:02.440
So tell us a little bit about the dev cloud and what the focus is there.

15:02.440 --> 15:07.720
And in particular, you know, compute, unlike a few years ago, compute is much more readily

15:07.720 --> 15:13.040
available on various public facing consumer clouds.

15:13.040 --> 15:14.840
What's different about dev cloud?

15:14.840 --> 15:15.840
Yeah.

15:15.840 --> 15:21.360
Well, you know, one of the challenges for folks getting started in AI is actually having

15:21.360 --> 15:27.080
access and being able to get started with compute in a way that's economical for them

15:27.080 --> 15:28.960
without having to invest in it.

15:28.960 --> 15:33.760
So for all of our Academy members, they'll have free access to the dev cloud.

15:33.760 --> 15:40.200
And this is a, yeah, this is a very large scale cluster and will have the latest Xeon scalable

15:40.200 --> 15:42.320
processors on there.

15:42.320 --> 15:47.160
And developers can sign up, they'll be, they can use it to sandbox, new projects they're

15:47.160 --> 15:51.080
working on, they can use it for their homework exercises they're doing in class.

15:51.080 --> 15:55.200
They can do just test out things or if they have a compelling project, they really want

15:55.200 --> 16:00.560
to get started and use it as Academy members, they sign up, they'll be given access.

16:00.560 --> 16:05.160
They can start for four weeks and use it for four weeks and then at the end of the four

16:05.160 --> 16:08.400
weeks, if they need it longer, then post the project they're working on.

16:08.400 --> 16:11.160
We'd like to see what they're working on and let other developers see what they're

16:11.160 --> 16:16.360
working on and they can get extended for another four weeks or even longer if it's necessary.

16:16.360 --> 16:22.200
So in this way, they're getting access to as much cloud compute as they're going to need

16:22.200 --> 16:24.840
for quite a while to get them up and started.

16:24.840 --> 16:29.520
It's not for commercial production type of applications, but a great way to get started.

16:29.520 --> 16:36.080
Oh, great, is there use of the dev cloud limited to, it doesn't sound like it's limited

16:36.080 --> 16:41.720
to just exercises that are part of the Academy, they can do any project that they come up

16:41.720 --> 16:42.720
with.

16:42.720 --> 16:43.720
Yeah.

16:43.720 --> 16:46.920
Let me just spend a few more minutes talking about the Academy what it is and you can

16:46.920 --> 16:49.400
understand how they use the dev cloud.

16:49.400 --> 16:53.200
So the Academy includes a lot of learning resources.

16:53.200 --> 16:59.760
So tutorials, online classes, webinars, webcasts, from basic getting started, machine learning

16:59.760 --> 17:04.280
101, deep learning 101, and intermediate to advanced.

17:04.280 --> 17:07.960
So a really good curriculum to learn about AI.

17:07.960 --> 17:12.520
But if you're already a professional developer and you just want to say, let's use the latest

17:12.520 --> 17:17.920
software that's optimized, let's maybe use neon framework or some other new Intel

17:17.920 --> 17:22.320
technology, you'll have access to that in the Academy and then you'll have technical

17:22.320 --> 17:23.600
support.

17:23.600 --> 17:29.920
So we have a real mix of professional developers, students are getting started, graduate students

17:29.920 --> 17:34.800
are working on research projects and they can all use the dev cloud.

17:34.800 --> 17:42.000
For example, some of, so just last night we had a dev jam and here in San Francisco is

17:42.000 --> 17:49.680
kind of a day zero of the O'Reilly conference and we had about 500 developers, students,

17:49.680 --> 17:56.480
startups attend this and on the stage I did a fireside chat with six of our students

17:56.480 --> 17:58.720
who are working on projects.

17:58.720 --> 18:02.440
And the types of projects that we're going on were just amazing, especially when you look

18:02.440 --> 18:03.960
at the variety.

18:03.960 --> 18:11.280
So one of them was doing a project on epilepsy and he was doing EG scans and connectivity

18:11.280 --> 18:17.960
analysis of the brain and using that to help patients manage epilepsy and predict seizures

18:17.960 --> 18:19.320
and things like that.

18:19.320 --> 18:21.960
So he's working on just a fascinating project there.

18:21.960 --> 18:26.160
Another student was doing a trail cam project.

18:26.160 --> 18:32.000
So if you're out in the wild and you have a trail cam, not only does it turn on record

18:32.000 --> 18:36.800
when a wildlife passed by, but it will try to recognize what type of wildlife that is

18:36.800 --> 18:38.120
and a lurch.

18:38.120 --> 18:43.040
So if it's a day they're run, yes it's a dangerous one and tell the campers nearby time to

18:43.040 --> 18:44.320
get out of there.

18:44.320 --> 18:51.040
Another student is working on mosquito detection and identification because mosquitoes there's

18:51.040 --> 18:55.480
thousands of varieties, but there's only a small number that are real dangerous that

18:55.480 --> 18:58.360
maybe carry malaria or Zika.

18:58.360 --> 19:04.200
So with his phone, he does create an application where it would take a photo, recognize what

19:04.200 --> 19:06.560
type of mosquitoes and tell if it's a dangerous type.

19:06.560 --> 19:07.560
Interesting.

19:07.560 --> 19:14.200
So that's just a sample, oh another fun one that I liked was a student from Rutgers who

19:14.200 --> 19:21.160
has developed an application where it will scan your head, look at your facial structure,

19:21.160 --> 19:28.440
your head structure and then based on popular hairstyles that you have similar facial structures

19:28.440 --> 19:33.360
recommend hairstyles for you and maybe a beard style as well.

19:33.360 --> 19:37.160
So you never have to worry anymore about what you should get to haircut, just have this

19:37.160 --> 19:40.400
app tell you exactly what your hair should look like and go get a cut like that.

19:40.400 --> 19:41.400
Oh wow.

19:41.400 --> 19:46.000
Quite a variety of things that they're working on and they have access to the dev cloud

19:46.000 --> 19:51.400
and can and to our technical support as well because we announced also today that we have

19:51.400 --> 19:58.360
a partnership with Tata Consulting Services to put in place an AI center of excellence.

19:58.360 --> 20:04.280
And we're going to leverage TCS's expertise in AI and use that to help to support the

20:04.280 --> 20:05.560
Academy members.

20:05.560 --> 20:10.160
So when they need to get stuck on a project or using the dev cloud, we'll have special

20:10.160 --> 20:14.720
engineers who can support them and provide the support to help them and they'll be located

20:14.720 --> 20:18.400
around the world because TCS is a worldwide organization.

20:18.400 --> 20:22.600
Now TCS is a consulting company it's hard to get a consulting company to do anything

20:22.600 --> 20:30.280
without dollars changing hands like if I'm an Academy member and I run into something

20:30.280 --> 20:35.440
and I need some help like how does that do I raise my hand and yeah.

20:35.440 --> 20:38.840
So the beauty of this is for the Academy members it's free.

20:38.840 --> 20:43.680
Of course there's a business model for Tata Consultancy in this as well but for Academy

20:43.680 --> 20:50.760
members, both the dev cloud and the support and the tutorials and training it's all free.

20:50.760 --> 20:58.160
And our desire is to help as many students, developers get smart, learning and discover new

20:58.160 --> 21:01.280
ways of using it as possible.

21:01.280 --> 21:07.880
And then is there a mechanism whereby if I say I'm working on something it starts out

21:07.880 --> 21:12.280
as a little project, side project in the corner of the office or something like that and

21:12.280 --> 21:18.600
then it grows into something that's more important for my company, like to get more help.

21:18.600 --> 21:20.360
Yeah, we do look for that.

21:20.360 --> 21:25.280
So we're always kind of monitoring and watching for those really cool things to emerge and

21:25.280 --> 21:27.560
see how we can help them be more successful.

21:27.560 --> 21:32.880
A lot of times they'll start to attract all kinds of support and interest in general but

21:32.880 --> 21:37.680
early on it's really helpful that we can say hey, this guy's got something going here.

21:37.680 --> 21:42.520
Let's give him a little extra boost so we're looking for that constantly and on my team

21:42.520 --> 21:47.800
what we've developed to do this is a program called Student Ambassadors.

21:47.800 --> 21:53.000
So since we rolled out the Academy, we've been going out to universities worldwide and

21:53.000 --> 21:57.880
run workshops, AI workshops and we'll introduce technology to give them the basics and then

21:57.880 --> 22:02.520
we'll hear what the students are working on and those students are really passionate

22:02.520 --> 22:07.560
doing something really cool, we'll see if they want to become ambassadors for Intel.

22:07.560 --> 22:12.960
And if they're interested then we'll give them even more training, more access to technology

22:12.960 --> 22:18.480
and in return just ask them to go tell other students about what they're doing and share

22:18.480 --> 22:19.800
their knowledge.

22:19.800 --> 22:25.320
So these six that were on the stage with me last night were ambassadors, one from UC Santa

22:25.320 --> 22:32.520
Barbara, one from MIT, one from Rutgers, one from let's see ASU and that's just the US.

22:32.520 --> 22:36.960
We started to have ambassadors all across the world from India, China, lots of them in

22:36.960 --> 22:42.680
Europe as well and I'd say those are the cream of the crop that we're really monitoring

22:42.680 --> 22:45.880
to see what cool things are they going to come up with.

22:45.880 --> 22:51.160
And if someone's listening and wants to get involved in the ambassador program, is there

22:51.160 --> 22:55.680
a mechanism for signaling their interests or do they just get involved in the Academy

22:55.680 --> 22:58.400
and do cool things and you'll figure you'll find them?

22:58.400 --> 22:59.960
No, they actually can apply.

22:59.960 --> 23:06.160
So online at the Academy you can just search on Intel Nirvana Academy online and you'll

23:06.160 --> 23:11.840
find it and then there's a apply to become a student ambassador.

23:11.840 --> 23:15.920
And you can fill out what you're working on, why you want to be an ambassador and then

23:15.920 --> 23:20.640
we'll have someone follow up to do an interview and see if you're ready for that.

23:20.640 --> 23:25.800
Okay, so interesting, sounds like the DevCloud is supported by some pretty interesting programs.

23:25.800 --> 23:33.680
In terms of the DevCloud itself, if I'm an AI developer and I'm coming to, I discover

23:33.680 --> 23:43.160
the Academy and DevCloud and have an existing tool chain instead of tools that I'm using,

23:43.160 --> 23:48.960
will those work on the DevCloud or do I need to port what I'm doing to the Intel Nirvana

23:48.960 --> 23:52.400
tool stack in order to use the DevCloud?

23:52.400 --> 23:56.880
There's a really good chance that they will already be preloaded on the DevCloud.

23:56.880 --> 24:01.440
So the DevCloud is not just for specific frameworks.

24:01.440 --> 24:07.680
It's really for, first of all, the Intel Xeon scalable processor is our hardware platform

24:07.680 --> 24:13.120
and that will extend that when we have the new flavors of Nirvana technology come out.

24:13.120 --> 24:15.240
So we'll keep building that hardware.

24:15.240 --> 24:22.200
But on the software side, it's whatever software that will run great and that developers

24:22.200 --> 24:23.160
and students want to use.

24:23.160 --> 24:29.040
So today, the DevCloud supports Neon, Spudges, TensorFlow, Spudges, Cafe, Spudges,

24:29.040 --> 24:33.960
Deano, Keras, most all the popular frameworks.

24:33.960 --> 24:37.800
And then Intel spent a lot of time optimizing them to run well on CPU.

24:37.800 --> 24:45.600
If you go back a couple of years ago, the frameworks ran great on GPUs but not so well on CPUs.

24:45.600 --> 24:51.080
We put a lot of investment over the last nine months or so and seen the performance improve

24:51.080 --> 24:56.040
up to 100X on running those frameworks on CPUs.

24:56.040 --> 25:03.040
So now we have optimized frameworks, pretty much the choice of the user gets to pick that

25:03.040 --> 25:11.360
and then he'll get 200GB of secure storage area for his files and load up his, you know,

25:11.360 --> 25:17.000
queue up his project and then we'll run it in the batch mode and then he gets notified

25:17.000 --> 25:19.800
when it's ready and can do it again.

25:19.800 --> 25:24.480
And so I suppose there are no GPUs in the DevCloud.

25:24.480 --> 25:27.360
No, they were not necessary.

25:27.360 --> 25:32.720
And so you talked about some of the performance metrics that you've seen recently.

25:32.720 --> 25:36.560
These are for this 100X this improvement over time.

25:36.560 --> 25:40.600
This is for training or inference or both.

25:40.600 --> 25:42.280
That's for training really.

25:42.280 --> 25:47.880
And you know, deep learning, first of all, machine learning, the vast majority of the

25:47.880 --> 25:53.960
servers that are running machine learning workloads are powered by Xeon platforms, Xeon servers.

25:53.960 --> 25:59.200
Deep learning is a subset and a fairly small but important subset of machine learning.

25:59.200 --> 26:04.320
And that's an area where we've seen the software optimizations make a huge difference on

26:04.320 --> 26:05.920
the time to train.

26:05.920 --> 26:13.400
And so developers now are seeing, you know, what used to take months down to minutes to

26:13.400 --> 26:16.400
do training on Xeon scalable processor.

26:16.400 --> 26:19.600
So the DevCloud will give you access to that.

26:19.600 --> 26:23.720
If developers have it tried it lately, they should try it now and see really, you know,

26:23.720 --> 26:25.920
the great performance will get.

26:25.920 --> 26:32.680
And so the 100X performance in the months down to minutes, that's relative to past performance

26:32.680 --> 26:36.440
on the Xeon with CPUs.

26:36.440 --> 26:41.480
Do you have any published comparisons relative to GPUs?

26:41.480 --> 26:46.440
And I'll just note that a lot of it is really based on the software optimization more

26:46.440 --> 26:47.440
than anything.

26:47.440 --> 26:49.360
It's made seem the huge difference.

26:49.360 --> 26:55.880
And we're seeing folks in the industry now start to publish really good results on the

26:55.880 --> 26:59.440
training site as well, compared to other alternatives in the market.

26:59.440 --> 27:04.320
So I don't know that we have any yet published on our site, but in general, we're seeing

27:04.320 --> 27:06.520
some really good results in the industry.

27:06.520 --> 27:10.160
And then we're seeing really good results on cloud service providers and what they're

27:10.160 --> 27:12.200
using as well for the technology.

27:12.200 --> 27:19.280
You mentioned cloud service providers is DevCloud hosted by Intel Nirvana, or have you

27:19.280 --> 27:23.360
partnered with one or more of the cloud service providers to make it available?

27:23.360 --> 27:29.160
It's really a part of our Nirvana cloud and really aligning those together.

27:29.160 --> 27:35.000
For the service providers, AWS or Google Cloud, they have great services today.

27:35.000 --> 27:39.240
They're using Xeon scalable and developer students can use those too.

27:39.240 --> 27:43.120
However, it can become costly for a student just getting started.

27:43.120 --> 27:48.680
So this is a way for them to stand bucks and get started and then move into a CSP model

27:48.680 --> 27:51.240
when they have a business that can support that.

27:51.240 --> 27:52.240
Okay.

27:52.240 --> 27:53.240
Okay.

27:53.240 --> 28:00.960
So then the Intel Nirvana cloud is, that's something that you're building and built and

28:00.960 --> 28:07.320
are building and host in your own data centers and manage independent of the large cloud

28:07.320 --> 28:08.320
providers.

28:08.320 --> 28:09.320
That's correct.

28:09.320 --> 28:10.320
That's correct.

28:10.320 --> 28:11.320
Right.

28:11.320 --> 28:17.240
And another thing I'll mention too is that Intel, one of the differentiators from Intel

28:17.240 --> 28:23.680
is we have such a broad portfolio of offerings and from the data center to the edge.

28:23.680 --> 28:28.040
And in the data center, of course, there's a training, there's inference on the Xeon,

28:28.040 --> 28:32.640
but we also have the FPGA product line and recently Microsoft announced that they're going

28:32.640 --> 28:39.000
to use the Stratics 10 FPGA for their brainwave project to really power all of the inference

28:39.000 --> 28:42.640
in their DL platform and brainwave.

28:42.640 --> 28:47.880
And we see a lot of opportunity with FPGAs as well, both in the data center and also at

28:47.880 --> 28:49.840
the edge for the inference piece.

28:49.840 --> 28:53.600
Can you tell us a little bit about brainwave or folks that missed that announcement?

28:53.600 --> 29:00.080
Well, I'm not the necessarily expert on brainwave, but it is Microsoft's deep learning platform

29:00.080 --> 29:07.200
and they announced it's going to use FPGA and Stratics 10 for it because of the low latency,

29:07.200 --> 29:09.160
low power, high throughput.

29:09.160 --> 29:14.760
It provides and the flexibility, FPGA is great in the flexibility it provides as AI evolves

29:14.760 --> 29:15.760
and changes.

29:15.760 --> 29:16.760
Right.

29:16.760 --> 29:17.760
Okay.

29:17.760 --> 29:18.760
All right, great.

29:18.760 --> 29:22.400
And then the last thing I'll mention too is if we talk about the portfolio is we also

29:22.400 --> 29:28.200
have at the edge, we have people developing solutions at the edge for deep learning that

29:28.200 --> 29:36.440
use Adam, core processor, Movidius for the video processing and whole host of this.

29:36.440 --> 29:39.960
All of this is going to be part of the academy too, so developers can not only learn about

29:39.960 --> 29:44.240
okay, I want to do the training, the inference in the data center, but at the edge too and

29:44.240 --> 29:46.440
put full end to end solution.

29:46.440 --> 29:47.440
Okay.

29:47.440 --> 29:48.440
Awesome.

29:48.440 --> 29:49.440
Awesome.

29:49.440 --> 29:55.160
One question I've got for you is in your role as overseeing developer programs, there

29:55.160 --> 30:02.160
are some interesting differences between the needs of traditional enterprise developers

30:02.160 --> 30:09.160
and more data science users, you know, data scientists that, you know, may also probably

30:09.160 --> 30:12.640
also fall under developer programs for you.

30:12.640 --> 30:17.400
Can you talk a little bit about how, you know, the different offerings you have that, you

30:17.400 --> 30:21.520
know, target these different communities and more generally, like how you're looking

30:21.520 --> 30:27.680
at these communities and how you plan, you know, evolve your offerings to serve both

30:27.680 --> 30:29.440
of them and give them what they need?

30:29.440 --> 30:30.440
Sure.

30:30.440 --> 30:31.440
And that is a great point.

30:31.440 --> 30:38.640
AI really introduced us to new types of an audience for us because I run developer programs

30:38.640 --> 30:46.120
for Intel and we cover the gamut from server to mobile and game developers and IOT developers

30:46.120 --> 30:51.520
across the board and we jumped into AI, yeah, all of a sudden there's this data scientist

30:51.520 --> 30:57.520
and they're like, okay, we're used to maybe dealing more with C++ developers and helping

30:57.520 --> 31:03.680
them optimize their code for the latest hardware that we had and for data scientists, it's

31:03.680 --> 31:08.320
a different ball game and first of all, most of them are using Python.

31:08.320 --> 31:11.760
What we've had to do is, okay, let's focus on how do we help them?

31:11.760 --> 31:15.720
They're going to be using frameworks, they need optimized frameworks, they need optimized

31:15.720 --> 31:21.600
versions of Python, they need really to understand the basics of machine learning and how to

31:21.600 --> 31:29.200
manage the data, how to apply machine learning versus how to really code in many cases.

31:29.200 --> 31:34.960
So we really had to look at this differently and start to say, okay, for these guys, let's

31:34.960 --> 31:37.640
teach them the basics on how to get started.

31:37.640 --> 31:42.640
Let's look at the tools that they will need from, like Python, for example.

31:42.640 --> 31:47.080
We have an Intel distribution of Python that is really good for performance and really

31:47.080 --> 31:48.920
good for AI and developer.

31:48.920 --> 31:51.760
And Intel distribution of Python?

31:51.760 --> 31:53.760
Yes, it's a parallel Python.

31:53.760 --> 31:54.760
Really?

31:54.760 --> 31:58.680
Oh yeah, so when we tell data scientists about this too, they're actually really excited

31:58.680 --> 32:01.560
as well because it can really help the performance of their application.

32:01.560 --> 32:04.480
Where do you find this Intel distribution of Python?

32:04.480 --> 32:09.080
It's easy to find, you just search on exactly that, you'll find it, you'll find it on our

32:09.080 --> 32:12.240
Academy, we have lots of information about it there and the benefits.

32:12.240 --> 32:16.240
And is it new kind of as part of the Academy or has it been around for?

32:16.240 --> 32:21.240
It's been around a little while, but it's not that long, I'd say, yeah, last year, I believe

32:21.240 --> 32:24.560
it was when we rolled it out in 2016.

32:24.560 --> 32:28.920
And you said it's optimized around parallel and distributed?

32:28.920 --> 32:35.120
Yeah, particularly for parallel, running really well in a parallel environment and giving

32:35.120 --> 32:37.320
the best performance out of Python applications.

32:37.320 --> 32:42.920
And what's an example of a parallel environment and a workload that you might use this Python

32:42.920 --> 32:44.800
distribution with?

32:44.800 --> 32:51.240
Well, I don't know if I have a real good example right there, but let me tell you about an example

32:51.240 --> 32:56.120
that something we did just recently, we just wrapped up as we did a contest with Kaggle.com.

32:56.120 --> 32:58.720
You heard it from other Kaggle.

32:58.720 --> 33:04.560
And we said, let's find a partner in the industry who wants to solve a real world AI problem

33:04.560 --> 33:05.560
and then eat some help.

33:05.560 --> 33:11.360
And so we partnered with mobile ODT and they're all about early detection of cancer and

33:11.360 --> 33:16.440
how do they do, how do they provide low cost devices to early deduct cancer?

33:16.440 --> 33:21.200
And so we, they provided a data set of 10,000 images, okay?

33:21.200 --> 33:26.160
And we provided the developers with parallel Python with access.

33:26.160 --> 33:29.320
This was an early version of our dev cloud, we were kind of in the pilot mode.

33:29.320 --> 33:33.400
So they got parallel Python, they got a dev cloud, they got, in this case, it was optimized

33:33.400 --> 33:40.520
cafe and we said, all right, whoever can provide the best algorithms and become the best

33:40.520 --> 33:45.360
at the detection of the images that are cancerous will win lots of good prizes.

33:45.360 --> 33:47.360
And the response was great.

33:47.360 --> 33:54.760
We had up to 1,000 data scientists and developers competing on this and accessing the dev cloud

33:54.760 --> 33:56.320
on a daily basis.

33:56.320 --> 34:01.080
So it was a great test of this, it was a great test of the tool set and the whole model

34:01.080 --> 34:02.080
here.

34:02.080 --> 34:07.400
And at the end of the day, mobile ODT was really excited about what they learned and are

34:07.400 --> 34:11.640
now falling out with the winners to say, okay, how do we productize what you've done

34:11.640 --> 34:13.640
here together?

34:13.640 --> 34:14.640
Interesting.

34:14.640 --> 34:19.480
I think a lot of folks use Python from, you know, they'll use like condos or some of these

34:19.480 --> 34:26.680
other Python distributions like does, do you envision, you know, partnerships to, how

34:26.680 --> 34:29.160
do you get this distribution of Python out there, right?

34:29.160 --> 34:34.560
To have people aren't, a lot of, I imagine to not know about it that a lot of people don't

34:34.560 --> 34:35.560
know about it.

34:35.560 --> 34:43.000
And I mean, I've seen this, this, yeah, I think just based on other things that I've seen

34:43.000 --> 34:47.040
Intel do in the past like, for example, in the Hadoop space, right?

34:47.040 --> 34:52.800
So there's a ton of interesting innovations that have happened in taking, you know, Intel's

34:52.800 --> 34:58.960
deep knowledge of the hardware and like kind of driving that into, you know, the mainstream

34:58.960 --> 35:05.240
Hadoop distributions and making them, you know, just making them out of the box more

35:05.240 --> 35:10.360
performant based on or more secure, like there's some security and encryption stuff in

35:10.360 --> 35:16.840
the Hadoop example, you know, I know of examples where, you know, folks from Intel have partnered

35:16.840 --> 35:21.920
with, you know, vendors like Docker and, you know, other, you know, lots of, lots of open

35:21.920 --> 35:23.760
source engagement.

35:23.760 --> 35:27.280
But in this Python case, like, how do you get this, how do you get this out into the

35:27.280 --> 35:28.280
wild?

35:28.280 --> 35:33.720
Well, a lot of that happens organically if you have a, you know, good product and there's

35:33.720 --> 35:38.040
a Python community out there, which is quite active and they start to hear about the word

35:38.040 --> 35:39.760
will spread organically.

35:39.760 --> 35:46.620
But to help that, we're becoming pretty proactive on awareness, thriving just social media

35:46.620 --> 35:53.560
outreach, some digital online marketing and other academy awareness in general where Python

35:53.560 --> 35:58.320
is becoming a bigger message there that, hey, did you know this distribution is available

35:58.320 --> 35:59.960
and with these benefits?

35:59.960 --> 36:04.280
So I think it's a combination of those and it won't be long when it's before it's pretty

36:04.280 --> 36:06.800
well known within that community.

36:06.800 --> 36:08.600
Oh, interesting.

36:08.600 --> 36:11.640
Any other cool stuff to tell me about that I didn't know about it?

36:11.640 --> 36:14.160
I don't think so.

36:14.160 --> 36:15.600
I think we hit on the main ones.

36:15.600 --> 36:19.760
I think the mention is that the academy is growing fast.

36:19.760 --> 36:27.040
We already have, coming up on a 50,000 members and we're, we've been running workshops

36:27.040 --> 36:32.200
around the world at universities, we'll have probably 200 universities that will be participating

36:32.200 --> 36:39.600
by the end of this year and trained about, I think we're on 25,000 developers and students

36:39.600 --> 36:41.120
so far this year.

36:41.120 --> 36:46.040
So it's really ramping up fast and we have some big aggressive goals, really helping to

36:46.040 --> 36:51.760
bring AI to the masses and through this dev cloud make compute available, through the

36:51.760 --> 36:56.840
training, help them learn about it, the technical support with TCS, we're pretty serious

36:56.840 --> 37:02.640
about helping people learn and grow and use AI to do really cool things.

37:02.640 --> 37:08.600
Well, it's a huge growth opportunity so I think it's a smart move and makes sense.

37:08.600 --> 37:14.120
It makes me think a little bit of Apple's strategy back in the day to get into colleges

37:14.120 --> 37:19.600
and universities with the Macintosh which help propel them later on.

37:19.600 --> 37:24.680
So congrats on that and congrats on the success of the academy so far.

37:24.680 --> 37:26.120
Thank you very much.

37:26.120 --> 37:27.120
Yeah.

37:27.120 --> 37:28.120
Thanks so much.

37:28.120 --> 37:29.120
God.

37:29.120 --> 37:35.200
All right, everyone, that's our show for today.

37:35.200 --> 37:40.120
Thank you so much for listening and for your ongoing feedback and support.

37:40.120 --> 37:45.320
For more information on Naveen and Scott, for links to dev cloud or the AI Academy and

37:45.320 --> 37:51.360
any of the other topics covered in this episode, head on over to twimaleigh.com slash talk slash

37:51.360 --> 37:53.360
51.

37:53.360 --> 38:00.520
For the rest of this series, visit twimaleigh.com slash AISF 2017.

38:00.520 --> 38:05.520
And please, please, please send us any questions or comments that you may have for us or for

38:05.520 --> 38:11.880
our guests via Twitter at twimaleigh or at Sam Charrington or leave a comment on the

38:11.880 --> 38:13.600
show notes page.

38:13.600 --> 38:17.440
There are a ton of great conferences coming up through the end of the year.

38:17.440 --> 38:21.960
To keep up to date on which events will be attending and hopefully meet us there, check

38:21.960 --> 38:31.560
out our new events page at twimaleigh.com slash events, twimaleigh.com slash events.

38:31.560 --> 39:00.080
Thanks again for listening and catch you next time.

