1
00:00:00,000 --> 00:00:16,320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

2
00:00:16,320 --> 00:00:21,480
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:21,480 --> 00:00:34,040
I'm your host, Sam Charrington, Hey Twimble listeners, if you're a fan of our AI

4
00:00:34,040 --> 00:00:39,680
platforms coverage and especially if you download our first AI platforms ebook, Kubernetes

5
00:00:39,680 --> 00:00:44,800
for machine learning, deep learning and AI, then today is your day.

6
00:00:44,800 --> 00:00:49,440
While we've been working tirelessly on Twimblecom, I have also been working on Book 2 in the

7
00:00:49,440 --> 00:00:53,440
series, the definitive guide to machine learning platforms.

8
00:00:53,440 --> 00:01:00,080
And I am happy to say that it is finally available to download now.

9
00:01:00,080 --> 00:01:04,880
We created the book as a resource for ML, AI and data science leaders and innovators

10
00:01:04,880 --> 00:01:09,120
to help guide their efforts in scaling and industrializing machine learning and deep learning

11
00:01:09,120 --> 00:01:11,320
in their organizations.

12
00:01:11,320 --> 00:01:16,440
In this book, we address questions such as why invest in increasing your organization's

13
00:01:16,440 --> 00:01:20,160
capacity to deliver machine learning and deep learning models.

14
00:01:20,160 --> 00:01:23,720
What are the key barriers to delivering ML and DL models?

15
00:01:23,720 --> 00:01:28,560
How of organizations like Facebook, Airbnb and LinkedIn overcome these challenges and

16
00:01:28,560 --> 00:01:32,240
how can their learnings be applied to your organization?

17
00:01:32,240 --> 00:01:35,840
What are the state of the art, machine learning platforms and tools and how can you put

18
00:01:35,840 --> 00:01:37,480
them to use?

19
00:01:37,480 --> 00:01:42,280
How can you develop an AI platform strategy to support your organization's goals?

20
00:01:42,280 --> 00:01:47,960
To access the definitive guide to machine learning platforms, visit Twimbleai.com slash

21
00:01:47,960 --> 00:01:50,200
AI platforms.

22
00:01:50,200 --> 00:01:53,480
And now onto the show.

23
00:01:53,480 --> 00:01:56,960
All right, everyone.

24
00:01:56,960 --> 00:01:59,240
I am on the line with Olivier Bachem.

25
00:01:59,240 --> 00:02:02,240
Olivier is a research scientist at Google Brain.

26
00:02:02,240 --> 00:02:05,600
Olivier, welcome to this week at Machine Learning and AI.

27
00:02:05,600 --> 00:02:06,600
Hi, Simon.

28
00:02:06,600 --> 00:02:07,600
Thanks for having me here.

29
00:02:07,600 --> 00:02:08,600
Yeah, absolutely.

30
00:02:08,600 --> 00:02:14,920
I'm excited to learn more about the environment that you worked on.

31
00:02:14,920 --> 00:02:17,920
It's called Google Research Football.

32
00:02:17,920 --> 00:02:19,240
We'll dive into that.

33
00:02:19,240 --> 00:02:23,480
But before we do, I'd love to explore your background a little bit.

34
00:02:23,480 --> 00:02:27,760
You did your PhD in large-scale clustering problems.

35
00:02:27,760 --> 00:02:32,240
Tell us a little bit about that, how you got to that and what you've been working on

36
00:02:32,240 --> 00:02:33,240
more recently.

37
00:02:33,240 --> 00:02:34,240
Yes.

38
00:02:34,240 --> 00:02:39,720
As you said, I did my PhD at Dieter Suryk on large-scale clustering.

39
00:02:39,720 --> 00:02:46,760
So one thing, I can't be the main question that I consider my PhD is, how can we go and

40
00:02:46,760 --> 00:02:51,600
apply clustering methods such as k-means clustering to very large data sets?

41
00:02:51,600 --> 00:02:54,720
And I essentially worked on two major topics.

42
00:02:54,720 --> 00:03:00,080
One was called, or is called core sets where the idea is you have a very large data set.

43
00:03:00,080 --> 00:03:04,960
That you want to run your machine learning or your clustering algorithm, not on a full

44
00:03:04,960 --> 00:03:09,680
data set, but on a smaller set because it might just take too long if you have billions

45
00:03:09,680 --> 00:03:11,040
of data points.

46
00:03:11,040 --> 00:03:16,760
And now core sets, they're a subset of your original data that you can find very efficiently.

47
00:03:16,760 --> 00:03:22,160
But that when you train on this smaller subset, you get solutions that are proofably competitive

48
00:03:22,160 --> 00:03:24,800
as if you would have trained on the full data set.

49
00:03:24,800 --> 00:03:31,320
That was one topic, and the second one was related to k-means clustering as well, where

50
00:03:31,320 --> 00:03:36,760
many people use an algorithm called k-means++, which is a smart way to initialize a clustering

51
00:03:36,760 --> 00:03:37,760
algorithm.

52
00:03:37,760 --> 00:03:42,960
And we worked on ways to make it much faster by, again, only looking at parts of the data

53
00:03:42,960 --> 00:03:46,840
or maybe looking at it once or twice instead of multiple times.

54
00:03:46,840 --> 00:03:47,840
Cool.

55
00:03:47,840 --> 00:03:53,440
And when you talk about the core sets and having provably competitive results is that an

56
00:03:53,440 --> 00:04:00,280
exercise in structuring your sample subset so that it has similar statistical properties

57
00:04:00,280 --> 00:04:02,440
as the larger data set.

58
00:04:02,440 --> 00:04:07,920
It is, to some point, it is also about finding the data points that are very important.

59
00:04:07,920 --> 00:04:12,440
Turns out in many machine learning problems, but in particular in clustering, you can

60
00:04:12,440 --> 00:04:17,000
have billions of data points, but some data points, maybe in regions where you have very

61
00:04:17,000 --> 00:04:23,320
few, not a lot of data, and now you want to capture this data because it can contribute

62
00:04:23,320 --> 00:04:27,440
a large part to the area of your model, whereas in regions of the space where you have a

63
00:04:27,440 --> 00:04:31,200
lot of data, you want to take several data points and kind of group them together.

64
00:04:31,200 --> 00:04:35,960
So I would say that's the intuition behind the approach, but then you have to kind of

65
00:04:35,960 --> 00:04:39,320
do this properly in order to actually get the article guarantees.

66
00:04:39,320 --> 00:04:40,320
Okay.

67
00:04:40,320 --> 00:04:44,320
This sounds like in that regard it's somewhat related to active learning.

68
00:04:44,320 --> 00:04:46,120
Well, yes and no.

69
00:04:46,120 --> 00:04:49,600
I think the yes part is it's about finding important points here.

70
00:04:49,600 --> 00:04:55,560
The key idea is you want to do this, I mean, here we're doing this essentially in an unsupervised

71
00:04:55,560 --> 00:04:56,560
setting, right?

72
00:04:56,560 --> 00:05:00,000
If you're doing clustering, you don't get labels, but it's more about actually which data

73
00:05:00,000 --> 00:05:02,600
do you want to look at more closely?

74
00:05:02,600 --> 00:05:03,600
Right.

75
00:05:03,600 --> 00:05:06,600
And I make conceptually as opposed to in practice.

76
00:05:06,600 --> 00:05:07,600
Yes.

77
00:05:07,600 --> 00:05:08,600
Yeah.

78
00:05:08,600 --> 00:05:14,360
And so that was your PhD, what sparked your interest in doing a PhD in machine learning at

79
00:05:14,360 --> 00:05:15,360
all?

80
00:05:15,360 --> 00:05:21,440
Yeah, so I have a bit of a non-traditional background, I did my undergrads in economics.

81
00:05:21,440 --> 00:05:23,880
I worked in different companies.

82
00:05:23,880 --> 00:05:31,040
I did a masters in quant finance, worked in some startups to the statistics master.

83
00:05:31,040 --> 00:05:36,040
And during my masters in statistics, I ran across these cool algorithms where you could

84
00:05:36,040 --> 00:05:39,200
solve statistical problems in a very applied way.

85
00:05:39,200 --> 00:05:44,240
I thought this was pretty cool and immediately after my masters, I thought it would be cool

86
00:05:44,240 --> 00:05:46,240
to do a PhD in the Explorer.

87
00:05:46,240 --> 00:05:47,240
All right.

88
00:05:47,240 --> 00:05:48,240
Cool.

89
00:05:48,240 --> 00:05:51,680
And what are some of the things that you've been working on at Google Brain?

90
00:05:51,680 --> 00:05:52,680
Yes.

91
00:05:52,680 --> 00:05:58,080
So at Google Brain, I've done quite a lot of different projects, which I thought was really

92
00:05:58,080 --> 00:06:02,080
cool, but I would broadly group them into three different categories.

93
00:06:02,080 --> 00:06:09,200
So in the last year, I've mainly led a big effort on research into learning what's

94
00:06:09,200 --> 00:06:11,800
called disentangled representations.

95
00:06:11,800 --> 00:06:14,680
Tell us a little bit more about that, what are disentangled representations?

96
00:06:14,680 --> 00:06:21,280
Yes, so the key idea is that in many settings in machine learning, you see a lot of high-dimensional

97
00:06:21,280 --> 00:06:28,200
observations, such as images or video or audio, but that these observations are not truly

98
00:06:28,200 --> 00:06:33,560
high-dimensional, but that they are the result of a lower-dimensional set of ground-truth

99
00:06:33,560 --> 00:06:36,000
factors of variation.

100
00:06:36,000 --> 00:06:40,160
And now, when you do representation learning, you essentially want to capture a learning

101
00:06:40,160 --> 00:06:45,760
function that takes such a high-dimensional observation and captures some form of information

102
00:06:45,760 --> 00:06:47,280
about that observation.

103
00:06:47,280 --> 00:06:53,560
And disentanglement or disentanglement representations is one property that you might want to get

104
00:06:53,560 --> 00:06:57,640
when you're doing representation learning, which is you want to capture these different

105
00:06:57,640 --> 00:07:01,760
factors of variation in your actual representation.

106
00:07:01,760 --> 00:07:07,000
So one example would be if you have an image with an object in the middle, right, or somewhere

107
00:07:07,000 --> 00:07:11,440
on the image, you could say, for example, one of the factors of variation might be there

108
00:07:11,440 --> 00:07:16,320
is an object at some position, with some size, with some color, and you want to capture

109
00:07:16,320 --> 00:07:20,360
each of these properties independently in representation.

110
00:07:20,360 --> 00:07:26,320
And can you give us a sense of within the realm of disentanglement representations, what's

111
00:07:26,320 --> 00:07:30,960
kind of the state of the art, what are the key tests and research?

112
00:07:30,960 --> 00:07:36,080
Yeah, that's actually a pretty cool question, because this is essentially the question I

113
00:07:36,080 --> 00:07:40,200
asked myself about the year ago, when I started working on this topic, there was a lot of

114
00:07:40,200 --> 00:07:47,640
papers on this topic doing new methods, new metrics, and we started a project actually

115
00:07:47,640 --> 00:07:49,960
benchmarking all of these methods.

116
00:07:49,960 --> 00:07:55,800
It's called a result in a paper called challenging common assumptions in the unsupervised learning

117
00:07:55,800 --> 00:08:00,320
of disentanglement representations, where we actually found that when you actually train

118
00:08:00,320 --> 00:08:05,120
a lot of these models, and we did a large scale study with train more than 10,000 such

119
00:08:05,120 --> 00:08:12,600
disentanglement models, we found that a lot of things happen that are quite surprising.

120
00:08:12,600 --> 00:08:18,840
Now the approach that we took is going to be, we thought of it, okay, what's the approach?

121
00:08:18,840 --> 00:08:23,560
If we are a new researcher, we get a new dataset, we want to find a disentanglement representation,

122
00:08:23,560 --> 00:08:25,680
what are the questions we actually have to answer?

123
00:08:25,680 --> 00:08:29,160
And as you mentioned, I guess the first question you want to answer is which model should

124
00:08:29,160 --> 00:08:30,160
I use, right?

125
00:08:30,160 --> 00:08:31,520
What is the state of the art model?

126
00:08:31,520 --> 00:08:37,480
The second question is how would you select model parameters, hyper parameters to actually

127
00:08:37,480 --> 00:08:42,440
train your model, and thirdly, you might not just train one model, but many different models,

128
00:08:42,440 --> 00:08:46,560
and you might have to select the model that you actually want to use.

129
00:08:46,560 --> 00:08:52,040
And what we kind of found is actually very interesting that the model that you use is

130
00:08:52,040 --> 00:08:53,040
not that important.

131
00:08:53,040 --> 00:08:57,360
It seems that all the methods that were proposed, and when you do a large scale evaluation

132
00:08:57,360 --> 00:09:03,960
on many dataset with many metrics to measure disentanglement, you kind of find out that all

133
00:09:03,960 --> 00:09:09,600
of them can give you very good disentanglement representations at the same time.

134
00:09:09,600 --> 00:09:18,120
So yeah, having said that, if you, what we also found is, it's very hard to select hyper

135
00:09:18,120 --> 00:09:23,000
parameters, because one of the hard parts in learning disentanglement representations

136
00:09:23,000 --> 00:09:28,000
is that when you train these models and you evaluate them, you know what the ground truth

137
00:09:28,000 --> 00:09:29,760
factors of variations are.

138
00:09:29,760 --> 00:09:33,960
But many of these methods that you actually want to use, the call assumption, the whole

139
00:09:33,960 --> 00:09:38,000
point of using them is that you don't know the ground truth factors of variation.

140
00:09:38,000 --> 00:09:41,200
So when you do the training, you have to be kind of strict, you're not allowed to look

141
00:09:41,200 --> 00:09:45,880
at the labels, because that would become cheating in a sense, because in a settings where

142
00:09:45,880 --> 00:09:49,840
you actually want to use such a model, you don't have access to these labels.

143
00:09:49,840 --> 00:09:53,920
So one of the hard parts is it's very hard to select good hyper parameters, because you

144
00:09:53,920 --> 00:09:57,240
cannot compute the disentanglement scores when you're off to it's going to use it.

145
00:09:57,240 --> 00:10:02,080
It turns out models are quite sensitive to hyper parameters, which may not be, may not

146
00:10:02,080 --> 00:10:03,080
be surprising.

147
00:10:03,080 --> 00:10:07,720
There is some hope that you can transfer hyper parameters across different data sets,

148
00:10:07,720 --> 00:10:12,720
but that brings you kind of to the last point, right, how do you actually select a model

149
00:10:12,720 --> 00:10:16,320
when you've trained many models with the same hyper parameters, what turns out there

150
00:10:16,320 --> 00:10:21,520
is even for a given model and the given set of hyper parameters, you can get vastly different

151
00:10:21,520 --> 00:10:22,520
results.

152
00:10:22,520 --> 00:10:26,880
You can, if you train 100 models, you're going to get some that are very, well, doesn't

153
00:10:26,880 --> 00:10:27,880
tell.

154
00:10:27,880 --> 00:10:30,120
You have some that are completely entangled.

155
00:10:30,120 --> 00:10:33,880
And the hard part is without looking at the labels or without looking at the models

156
00:10:33,880 --> 00:10:39,600
yourself as a human, it's very hard to say which of these models you should be using.

157
00:10:39,600 --> 00:10:44,560
We looked at different strategies on how to do this and maybe to illustrate with a kind

158
00:10:44,560 --> 00:10:50,080
of comparison with the best thing that we could find is even if you kind of do a smart

159
00:10:50,080 --> 00:10:56,560
strategy of finding good hyper parameters, you're going to beat a random model across

160
00:10:56,560 --> 00:11:01,280
all the models I've trained only slightly more than at the coin flip.

161
00:11:01,280 --> 00:11:04,360
So slightly more at 50, than 50%, so I mean 60%.

162
00:11:04,360 --> 00:11:07,760
Well, when I hear you describe that work, I hear a couple of different things.

163
00:11:07,760 --> 00:11:14,440
One, I hear this like core focus on the disentanglement versus entanglement, which I'm

164
00:11:14,440 --> 00:11:21,440
kind of conceptualizing as like an orthogonality of these different, I don't know what you'd

165
00:11:21,440 --> 00:11:25,640
even call them, not models, but spaces representation.

166
00:11:25,640 --> 00:11:27,640
Yeah, yeah, yeah, thank you.

167
00:11:27,640 --> 00:11:35,520
Also in the example you gave where you've got an image and the disentangled representations

168
00:11:35,520 --> 00:11:40,040
are, you know, an object and color and size and things like that.

169
00:11:40,040 --> 00:11:47,760
It almost starts to sound like a holy grail of AI or AGI where you're like, there's true

170
00:11:47,760 --> 00:11:48,760
intelligence there.

171
00:11:48,760 --> 00:11:54,120
And the question that comes to mind is, you know, to what degree are these disentangled

172
00:11:54,120 --> 00:12:01,120
representations generally that explainable or aligned with our intuition and how we as

173
00:12:01,120 --> 00:12:07,000
humans would interpret what's happening in an image or are they just some orthogonal

174
00:12:07,000 --> 00:12:11,400
established set of representations that don't necessarily map to the way that we would

175
00:12:11,400 --> 00:12:13,680
interpret an image.

176
00:12:13,680 --> 00:12:16,120
So I think there's maybe two points.

177
00:12:16,120 --> 00:12:21,400
So the first point, this is also one that we're making a paper is that if you look at a task

178
00:12:21,400 --> 00:12:26,040
because it's an unsupervised learning task, it is actually in defined and we have an

179
00:12:26,040 --> 00:12:32,080
impossibility results, which kind of shows that no algorithm can do, I mean, this is my

180
00:12:32,080 --> 00:12:36,640
interpretation, no algorithm, kind of to learn disentanglement representations can do

181
00:12:36,640 --> 00:12:43,000
so consistently for all possible world models of the world or all possible processes that

182
00:12:43,000 --> 00:12:44,000
generate your data.

183
00:12:44,000 --> 00:12:47,720
And I think that brings us kind of to that second point, right?

184
00:12:47,720 --> 00:12:57,160
This essentially means that you require inductive biases onto kind of the models which match

185
00:12:57,160 --> 00:13:01,000
the data that you want to use the models for, right?

186
00:13:01,000 --> 00:13:06,080
And I guess one of the hopes or like the key idea while a lot of people are excited about

187
00:13:06,080 --> 00:13:10,960
disentanglement representations is that there are such good inductive biases, right?

188
00:13:10,960 --> 00:13:15,000
And if you were to find a disentanglement representations, and this is kind of follow-up

189
00:13:15,000 --> 00:13:20,760
work that we have looked into, it seems that you get quite some benefits in terms of the

190
00:13:20,760 --> 00:13:26,000
representations you get, we found that for non-trivial downstream tasks, when you look

191
00:13:26,000 --> 00:13:32,120
at tasks that require reasoning, abstract reasoning about the world that you will do

192
00:13:32,120 --> 00:13:37,200
better if your representation is disentangled versus if you looked at, or at least we

193
00:13:37,200 --> 00:13:42,240
saw that this was correlated with your representation being entangled.

194
00:13:42,240 --> 00:13:46,600
I think that's kind of the hope that some of your algorithms or your models that you train,

195
00:13:46,600 --> 00:13:51,600
they will generalize better to stuff that you have not seen before and things like this.

196
00:13:51,600 --> 00:13:54,640
And I think that's why a lot of people are excited about this.

197
00:13:54,640 --> 00:13:59,600
But I completely agree it matters how you define what is interpretable or not, right?

198
00:13:59,600 --> 00:14:00,600
Yeah, yeah.

199
00:14:00,600 --> 00:14:01,600
Okay.

200
00:14:01,600 --> 00:14:06,560
Well, for the sake of time, we'll skip some of the work you've done on generative

201
00:14:06,560 --> 00:14:07,560
model.

202
00:14:07,560 --> 00:14:13,040
That's the other thing that you spend a bunch of time on at brain and dive into the research

203
00:14:13,040 --> 00:14:16,720
that was recently published on Google Research Football.

204
00:14:16,720 --> 00:14:22,040
Tell us a little bit about that project and its motivation and how it came about.

205
00:14:22,040 --> 00:14:24,640
Sure, sure.

206
00:14:24,640 --> 00:14:33,800
So basically, I think the whole project started in late summer last year.

207
00:14:33,800 --> 00:14:39,560
It basically started as a kind of cool idea in our team where we have, I mean, one thing

208
00:14:39,560 --> 00:14:46,800
you have to see about Google Brain at least in Zurich where I've been, it's very collaborative.

209
00:14:46,800 --> 00:14:51,680
We share a lot of ideas over informal sessions like when you go for lunch, when you go

210
00:14:51,680 --> 00:14:58,400
for coffee, but also in kind of within a team, and remember, we had this idea that we could

211
00:14:58,400 --> 00:15:08,000
use reinforcement learning to play video or kind of video games that model soccer, right?

212
00:15:08,000 --> 00:15:14,280
I guess a lot of people have played in the youth such video games and we started discussing

213
00:15:14,280 --> 00:15:20,440
about this in order to pros, whether to cons, and out of this, we started kind of the

214
00:15:20,440 --> 00:15:23,080
game, a bit of traction within the team.

215
00:15:23,080 --> 00:15:28,640
And we started investigating, would this be possible, would it be feasible?

216
00:15:28,640 --> 00:15:35,960
And we came across this open source football game called Gameplay Football, and we thought,

217
00:15:35,960 --> 00:15:38,400
okay, let's see what we can do with this.

218
00:15:38,400 --> 00:15:45,240
And at the same time, we, as we wanted to do, or some people in our group, were doing

219
00:15:45,240 --> 00:15:49,560
research on reinforcement learning, we started noticing a lot of benefits that would be

220
00:15:49,560 --> 00:15:58,320
had if we would build this into a fully-flanched modern reinforcement learning environment.

221
00:15:58,320 --> 00:15:59,320
So we started, yeah?

222
00:15:59,320 --> 00:16:04,120
Oh, no, I was just going to ask the obvious question, which is, so what are some of those

223
00:16:04,120 --> 00:16:05,120
benefits?

224
00:16:05,120 --> 00:16:12,320
Yeah, so I mean, the first benefit that I personally see is its open source.

225
00:16:12,320 --> 00:16:19,400
So this gave us a lot of flexibility to modify the needs for a modern reinforcement

226
00:16:19,400 --> 00:16:20,400
learning environment.

227
00:16:20,400 --> 00:16:26,200
So it's primarily built for research, and that gives us a whole bunch of benefits.

228
00:16:26,200 --> 00:16:31,800
And the first benefit, and I mean, that is also inherent to the game of football, is

229
00:16:31,800 --> 00:16:34,800
that it is an on-tribal problem, right?

230
00:16:34,800 --> 00:16:40,720
It's actually, if you look at it, it's quite hard to play, and let me if we iterate on

231
00:16:40,720 --> 00:16:43,840
or kind of explain what we're actually doing here.

232
00:16:43,840 --> 00:16:47,600
So this is in the context of reinforcement learning, where you have to teach an agent

233
00:16:47,600 --> 00:16:53,720
to interact with an environment, here the agent has to control, and this is in the basic

234
00:16:53,720 --> 00:16:58,920
form of this environment, has to control the active player on one of the teams, and it

235
00:16:58,920 --> 00:17:04,360
has to do actions such as, okay, I'm going to walk to the right, to the left, to the top

236
00:17:04,360 --> 00:17:09,760
or the bottom, and it has to pass around, shoot as you would, as a human if you were controlling

237
00:17:09,760 --> 00:17:13,200
a team in a football video game.

238
00:17:13,200 --> 00:17:18,000
And now, one of the benefits is that this is actually kind of, if you think of the game

239
00:17:18,000 --> 00:17:21,760
of football, where you start in the middle of game with a kickoff, you have to pass around,

240
00:17:21,760 --> 00:17:26,240
you have to play around your opponent's defense, you have to come up with a strategy, and

241
00:17:26,240 --> 00:17:31,520
you have to score, that is actually a non-tribal task, and it's challenging task.

242
00:17:31,520 --> 00:17:37,920
In particular, if you have different difficulties of opposing teams, as already one of the advantages

243
00:17:37,920 --> 00:17:44,920
we saw, as you change the difficulty of the strength of the opposing team, you can change

244
00:17:44,920 --> 00:17:49,800
the difficulty of the learning task.

245
00:17:49,800 --> 00:17:53,320
Similarly, you can make it much easier, you can say, well, let's say that pitches empty

246
00:17:53,320 --> 00:17:58,480
and they just have to score a goal against no goalkeeper, that is even much easier, right?

247
00:17:58,480 --> 00:18:04,240
So one of the benefits that we saw was that it's very easy to adjust the difficulty of

248
00:18:04,240 --> 00:18:07,600
the game, while still not being trivial, right?

249
00:18:07,600 --> 00:18:12,000
And that is kind of in contrast to maybe other environments that may be rather easy to

250
00:18:12,000 --> 00:18:17,760
solve, or just very hard computation expensive to run stuff on.

251
00:18:17,760 --> 00:18:23,200
The same time, as it's open source, we were able to build a lot of features into the

252
00:18:23,200 --> 00:18:26,800
game that help with doing enforcement learning research.

253
00:18:26,800 --> 00:18:32,960
So one thing we could do is we can turn on and turn off soasticity, we can make the game

254
00:18:32,960 --> 00:18:38,800
deterministic, then it turns out, or this kind of still an open question research is whether

255
00:18:38,800 --> 00:18:44,400
some algorithms generalize from a setting, from an environment that is deterministic to

256
00:18:44,400 --> 00:18:46,640
a sarcastic one, right?

257
00:18:46,640 --> 00:18:49,920
The game is focused on controlling the player that has the ball.

258
00:18:49,920 --> 00:18:54,720
The players that don't have the ball, as well as the opposing team, is there some kind

259
00:18:54,720 --> 00:18:58,880
of traditional, quote unquote, game AI that's controlling those?

260
00:18:58,880 --> 00:18:59,880
Yes.

261
00:18:59,880 --> 00:19:05,880
Maybe the one thing to keep in mind here, so the way we structure this is we build what

262
00:19:05,880 --> 00:19:13,200
we call the football engine, which is a simulation of a game of football, and it supports a lot

263
00:19:13,200 --> 00:19:14,200
of different features.

264
00:19:14,200 --> 00:19:20,240
One is it supports a built-in rules-based AI, as you would probably find in a professional

265
00:19:20,240 --> 00:19:21,960
video game.

266
00:19:21,960 --> 00:19:26,160
And at the same time, it supports a lot of features for research.

267
00:19:26,160 --> 00:19:32,000
And we chose that, OK, so if you have to control all the players in your team, your toss becomes

268
00:19:32,000 --> 00:19:33,000
much harder.

269
00:19:33,000 --> 00:19:36,800
We did some additional experiments and turns out if you have to control more players, this

270
00:19:36,800 --> 00:19:37,960
becomes harder.

271
00:19:37,960 --> 00:19:44,720
So in the basic version on what we call football benchmarks, which is in this setting, we consider

272
00:19:44,720 --> 00:19:49,200
the opposing players and your teammates to be part of the environment and they play with

273
00:19:49,200 --> 00:19:52,280
the built-in rules-based AI.

274
00:19:52,280 --> 00:19:58,920
In that case, you don't control them, right, and these are basically traditional reinforcement

275
00:19:58,920 --> 00:20:04,200
learning problems, and we chose three of them as benchmark problems where people can

276
00:20:04,200 --> 00:20:05,960
compare different algorithms.

277
00:20:05,960 --> 00:20:10,840
But at the same time, the football engine that we built supports actually controlling all

278
00:20:10,840 --> 00:20:16,800
of the players, or you can control all the players of one side, or we can even do a mix.

279
00:20:16,800 --> 00:20:22,040
I think in the paper we, with this experiment, where we control between one and three players.

280
00:20:22,040 --> 00:20:27,040
So that is completely flexible, and that's, again, one of the benefits of having a modern

281
00:20:27,040 --> 00:20:30,720
reinforcement learning environment that is open source.

282
00:20:30,720 --> 00:20:37,680
You mentioned that you chose three scenarios for your benchmarking.

283
00:20:37,680 --> 00:20:40,000
How are those three scenarios distinguished?

284
00:20:40,000 --> 00:20:46,440
Yes, so what we decided is, so we built these football benchmarks, which we considered

285
00:20:46,440 --> 00:20:52,480
the current benchmarks, problems in this environment, and it's essentially you play a game

286
00:20:52,480 --> 00:20:59,080
of 11 versus 11 football against three different of these rule-based opponents, and the only

287
00:20:59,080 --> 00:21:04,840
thing changes is the difficulty of how well these different opponents, how well the opponent

288
00:21:04,840 --> 00:21:05,840
plays.

289
00:21:05,840 --> 00:21:11,120
Okay, so some parameter to the rule-based AI that's driving them.

290
00:21:11,120 --> 00:21:12,120
Yes.

291
00:21:12,120 --> 00:21:16,800
Exactly, I'm kind of how quickly it reacts to you, yeah, how well it plays.

292
00:21:16,800 --> 00:21:21,200
As you would, if you play a, you know, a football game yourself, you can choose, you know,

293
00:21:21,200 --> 00:21:25,760
between an easy medium and a hard, here it's essentially the same thing.

294
00:21:25,760 --> 00:21:32,920
But the key motivation for actually doing this is also to accommodate that different researchers

295
00:21:32,920 --> 00:21:35,960
may have different computational budgets.

296
00:21:35,960 --> 00:21:43,120
So it turns out, if we are running currently with the algorithms that we tried and we found

297
00:21:43,120 --> 00:21:48,280
that if you run on medium and hard problems, it becomes quickly very challenging and you

298
00:21:48,280 --> 00:21:54,520
might have to go to distributed implantations of the funnagrums, you need quite a lot of

299
00:21:54,520 --> 00:21:59,600
computation resources, but at the same time, you also want to provide benchmark problems

300
00:21:59,600 --> 00:22:06,360
where you could go with a single machine, maybe a single GPU, and you could actually test

301
00:22:06,360 --> 00:22:10,360
your ideas and compare against other algorithms, right?

302
00:22:10,360 --> 00:22:14,800
Yeah, and I guess, yeah, there's different areas, right, of reinforcement learning, such

303
00:22:14,800 --> 00:22:21,440
as learning with a few samples, I think this is one of the very hard problem reinforcement

304
00:22:21,440 --> 00:22:23,040
languages, also very relevant.

305
00:22:23,040 --> 00:22:29,000
And even if you don't have a lot of computational resources, you can still go and benefit from

306
00:22:29,000 --> 00:22:30,000
this environment.

307
00:22:30,000 --> 00:22:31,000
Okay.

308
00:22:31,000 --> 00:22:37,440
Yeah, I mean, one thing also, we also added what we call the football academy.

309
00:22:37,440 --> 00:22:44,320
And this is essentially, as I said before, like the having that flexible game engine allows

310
00:22:44,320 --> 00:22:47,840
us to really kind of change what is happening on the pitch.

311
00:22:47,840 --> 00:22:52,640
And one inspiration we took when football teams trained right, they might do some drills,

312
00:22:52,640 --> 00:22:53,920
and we can actually model this, right?

313
00:22:53,920 --> 00:23:00,600
We can say, well, let's not have 11 versus 11 players, maybe let's play 3 versus 2 counter

314
00:23:00,600 --> 00:23:01,600
attack.

315
00:23:01,600 --> 00:23:07,360
And we define a set of different, such academy scenarios as we call them, they start with

316
00:23:07,360 --> 00:23:09,480
very easy ones where you're close to the goal.

317
00:23:09,480 --> 00:23:13,160
There is no goalkeeper and you just have to score, so you have to eventually learn how

318
00:23:13,160 --> 00:23:19,040
to walk into the goal or how to kick the ball into the goal to more difficult problems

319
00:23:19,040 --> 00:23:26,040
where you play with 11 players versus goalkeeper or, as I said before, counter attack scenarios

320
00:23:26,040 --> 00:23:31,280
that kind of accommodate very different game situations, but also which come with very

321
00:23:31,280 --> 00:23:33,600
different difficulty levels.

322
00:23:33,600 --> 00:23:39,280
And maybe the key thing here is the easy ones, right, where you just have to score on

323
00:23:39,280 --> 00:23:43,920
easy, you can like, if you're on a single machine, you can get results with current algorithms

324
00:23:43,920 --> 00:23:50,560
within minutes, or let's say 10, 20 minutes instead of having to wait a full day and block

325
00:23:50,560 --> 00:23:56,000
a full machine before you can tweak your algorithm or check your implementation.

326
00:23:56,000 --> 00:24:00,240
So I think that's also like one of the key advantages of the environment, you can go

327
00:24:00,240 --> 00:24:05,560
and start with very simple cases and kind of see where does it break without always

328
00:24:05,560 --> 00:24:09,160
having to wait a very long time in between.

329
00:24:09,160 --> 00:24:14,040
You can also define your own scenarios, so there is a lot of flexibility and I'm pretty

330
00:24:14,040 --> 00:24:15,040
excited about it.

331
00:24:15,040 --> 00:24:21,920
I suspect this is going to be well beyond the scope of what you have looked at so far,

332
00:24:21,920 --> 00:24:28,840
certainly at this paper, but have you explored kind of the idea of transfer learning or

333
00:24:28,840 --> 00:24:33,720
curriculum learning, like training an agent on these academy scenarios and how that might

334
00:24:33,720 --> 00:24:37,320
impact performance on the game as a whole?

335
00:24:37,320 --> 00:24:44,200
Yes, so in the last few months, we've been very busy pushing out or kind of pushing out

336
00:24:44,200 --> 00:24:49,600
this environment and fixing bugs and running the experiments we added in this paper.

337
00:24:49,600 --> 00:24:53,880
It has certainly been on our mind and one of the motivations actually for the football

338
00:24:53,880 --> 00:24:59,240
academy was to enable curriculum learning.

339
00:24:59,240 --> 00:25:05,880
We didn't include results here, we kind of, in the paper we include benchmark results

340
00:25:05,880 --> 00:25:11,640
for the different academy scenarios to show how hard they are and this role, as I mentioned

341
00:25:11,640 --> 00:25:18,280
before, where you have a different variety of tasks, which are different difficulties,

342
00:25:18,280 --> 00:25:22,000
but you can use this to go and build your own curriculum, right, and you can see does

343
00:25:22,000 --> 00:25:29,880
thus fare better at actually playing the 11-versa-lem game than if I just start to play the 11-versa-lem game.

344
00:25:29,880 --> 00:25:36,120
You've talked about some of the benefits of your environment in terms of it being open-source

345
00:25:36,120 --> 00:25:42,040
and some of the features that you've built in, but I'm curious taking a step back are there,

346
00:25:43,560 --> 00:25:55,800
any thoughts in terms of some unique properties of football slash soccer as a task for reinforcement

347
00:25:55,800 --> 00:26:02,360
learning agents, there are a ton of RL simulation types of environments, everything from the tower

348
00:26:02,360 --> 00:26:11,480
challenges, things to very domain-specific to the more general, teach some agent to walk

349
00:26:11,480 --> 00:26:19,400
kind of environment, plus Atari games, why is football potentially interesting or more

350
00:26:19,400 --> 00:26:22,280
interesting than some of these many other tasks?

351
00:26:22,280 --> 00:26:31,080
Yes, so I think the key benefit of football is twofold, but the first one is I think the

352
00:26:31,080 --> 00:26:37,880
task as we have it now is challenging even for modern reinforcement learning algorithms.

353
00:26:37,880 --> 00:26:43,560
If you look at, for example, the hard scenario, even playing against the fixed rules-based AI

354
00:26:43,560 --> 00:26:52,600
with the general learning algorithm, we need to run a lot of several hundred millions of steps

355
00:26:52,600 --> 00:26:57,960
in a distributed setting where we have several hundred machines actually running this game, right?

356
00:26:57,960 --> 00:27:04,920
So I think having a challenging task is very important. Now, when you do research here,

357
00:27:04,920 --> 00:27:08,760
every research is kind of free how to choose the approach, but one of the hopes is that

358
00:27:08,760 --> 00:27:17,240
if you take a general learning algorithm, which is not specific to football, that it would also

359
00:27:17,240 --> 00:27:26,040
transfer to other under-settings, right? Now, in terms of how does it compare to different

360
00:27:26,040 --> 00:27:33,080
environments? I think the key part where I mentioned before is that we can adjust the difficulty,

361
00:27:33,080 --> 00:27:38,680
this allows you to do research on a variety of different difficulty levels. I think that's

362
00:27:38,680 --> 00:27:47,480
different to prior environments, such as Atari, which I guess now people would consider rather

363
00:27:47,480 --> 00:27:51,640
on the easy side. On the other hand, there has been reinforcement learning research on other

364
00:27:51,640 --> 00:27:59,480
video games, such as Starcraft 2 or Dota 2, which is rather under computationally expensive side.

365
00:27:59,480 --> 00:28:04,360
Here you get kind of everything, right? At the same time, it's also open source, so you don't

366
00:28:04,360 --> 00:28:12,840
need access to a potentially a closed source binary. The other part, and I think that's also one

367
00:28:12,840 --> 00:28:19,640
thing we are very excited about. There is essentially two extensions that we put into the environment,

368
00:28:19,640 --> 00:28:27,320
which is it allows to play multiplayer in a sense that football is inherently a multiplayer game

369
00:28:28,440 --> 00:28:33,480
where you play against an opponent, which brings a completely new dynamic when you have

370
00:28:33,480 --> 00:28:40,360
somebody else acting in the same environment with adversarial goals to you. Similarly, the

371
00:28:40,360 --> 00:28:47,560
environment supports actual research into multi-agents where you could think of every single player

372
00:28:47,560 --> 00:28:51,720
has to make their own decision on what it's going to do, and now you have two teams, right?

373
00:28:51,720 --> 00:28:56,440
And they have to start cooperating. Players within the team have to start cooperating

374
00:28:56,440 --> 00:29:00,920
with each other to play against somebody else. And I think this is just a very broad

375
00:29:00,920 --> 00:29:08,120
range of topics that you can investigate. Similarly, we built into the engine that you can go

376
00:29:08,120 --> 00:29:14,200
and train on different representations or different types of observations, so you can either go

377
00:29:14,200 --> 00:29:20,200
and train about pixels, which may be computation expensive, or we can go at other representations

378
00:29:20,200 --> 00:29:27,640
of the observations, such as the actual positions of the players, right? Not all the data is images,

379
00:29:27,640 --> 00:29:34,200
and you can, again, do something which you cannot do in other environments.

380
00:29:35,240 --> 00:29:41,880
So the representations can be, in addition to just looking at the images, you can get the

381
00:29:42,840 --> 00:29:50,040
kind of more inherent state. What about on the control side? What are the options that you have

382
00:29:50,040 --> 00:29:56,760
on control of the player with the ball? Yeah, so there has been other reinforcement learning

383
00:29:56,760 --> 00:30:05,480
environments, which are more, which are also doing football. This is DeepMind Soccer and Robocup,

384
00:30:05,480 --> 00:30:11,560
the 3D simulation. These are more on the side of doing continuous control, where it's really about

385
00:30:11,560 --> 00:30:18,840
low level control of either the robot in Robocup or the simulator robot in Robocup that you control,

386
00:30:18,840 --> 00:30:27,400
or essentially a kind of players in this DeepM Soccer, where you really have to specify how much

387
00:30:27,400 --> 00:30:32,520
do you turn, how much do you go forward and back, which is much more focused on low level control.

388
00:30:32,520 --> 00:30:37,720
Here, the idea is more, you want to learn how to do these more high level control,

389
00:30:37,720 --> 00:30:43,560
like things like strategy, tactics, become much more important, here you control.

390
00:30:43,560 --> 00:30:48,360
Okay, as you would with a gamepad, essentially you can move right, left, top, bottom,

391
00:30:48,360 --> 00:30:55,880
you can sprint, you can pass, you can try to press when you're in defense, and so on.

392
00:30:56,840 --> 00:31:01,000
Again, the environment is open-source, so this can be modified, right?

393
00:31:01,560 --> 00:31:07,720
So it sounds like the way that you control the character is one of the key distinction

394
00:31:07,720 --> 00:31:12,120
between this environment and some of the others that are out there for this game.

395
00:31:12,120 --> 00:31:19,160
Are there other distinctions? Yeah, I think that is one of them, it's in particular to with regards

396
00:31:19,160 --> 00:31:24,120
to the other environments that do football. That is, for me, the key difference, I think to the

397
00:31:24,120 --> 00:31:29,640
other ones, the actual tasks that you're doing, the fact that you can go from different types of

398
00:31:29,640 --> 00:31:36,360
observations, that you can adjust the difficulty, that is open-source, these are all differences.

399
00:31:36,360 --> 00:31:40,760
I think one of the points here is also, I think there's also, in reinforcement, always a certain

400
00:31:40,760 --> 00:31:46,040
investment required in going and using an environment, and what I like here is, in one

401
00:31:46,040 --> 00:31:50,120
environment, you can essentially do a lot of different types of research.

402
00:31:51,080 --> 00:31:57,320
Maybe share a little bit about where your group goes from here. This has been published,

403
00:31:58,040 --> 00:32:03,400
out on GitHub. When did it go live? When was the environment published and the paper published?

404
00:32:03,400 --> 00:32:14,600
Yeah, so we put this kind of right. We finished a sprint just before ICML, where we first pushed

405
00:32:14,600 --> 00:32:21,640
us to GitHub with an accompanying paper, which describes the environment. Since then, we had a

406
00:32:21,640 --> 00:32:27,000
demonstration at ICML, at the Google booth, where we showed this to people, we got a lot of feedback,

407
00:32:27,000 --> 00:32:33,960
we've worked on the environment actively. Since then, we added a lot of cool features that I'm

408
00:32:33,960 --> 00:32:39,160
excited about, this multiplayer feature, where you can do self-play, for example. We've added

409
00:32:40,040 --> 00:32:47,400
multi-agent support. We've done optimizations of the game engine. It runs now through

410
00:32:47,400 --> 00:32:53,080
ExFaster, and we've just recently updated the paper and put it on archive with more experiments,

411
00:32:53,080 --> 00:32:59,480
where we investigate the first experiments into three different research directions,

412
00:32:59,480 --> 00:33:05,480
which we think are very promising in this environment. One of them is what happens if you do

413
00:33:05,480 --> 00:33:09,400
self-play, what happens second is what happens if you do this multi-agent setting where you

414
00:33:09,400 --> 00:33:15,960
control more players, and third one is what happens if you use different representations of the

415
00:33:15,960 --> 00:33:24,920
actual environment or the observations that the agent interacts with. From this point,

416
00:33:24,920 --> 00:33:30,520
does your group continue to work on the environment itself? Or do you start to

417
00:33:32,040 --> 00:33:37,240
branch off into doing more experimentation work on the environment? Where do you see your work going

418
00:33:37,240 --> 00:33:45,320
with this? Yes. I think there is, as you said, two things that we want to do. The first thing is

419
00:33:45,320 --> 00:33:52,920
we want to make sure it is a useful research or enforcement learning environment. We are

420
00:33:52,920 --> 00:34:00,120
definitely going to continue adding features, make it easier to use the environment. I think

421
00:34:00,120 --> 00:34:05,960
the potential that it has, especially with the multiplayer component or where you have to compete

422
00:34:05,960 --> 00:34:11,480
against other agents, we are definitely looking into options how to make this more accessible.

423
00:34:11,480 --> 00:34:16,360
At the same time, we are also super excited to do research on this environment. Yes.

424
00:34:16,920 --> 00:34:22,520
And also, I think one thing that we should highlight here, this has been a big team effort

425
00:34:22,520 --> 00:34:29,880
in the team here. So it has been over 10 people that have worked actively on this project, right?

426
00:34:31,320 --> 00:34:35,720
Now, I think it's also maybe a time where different people are going to explore different things

427
00:34:35,720 --> 00:34:42,520
in this environment. And so, Olivia, maybe what are three things that you personally learned

428
00:34:42,520 --> 00:34:50,680
in working on this project? Yes. So for me, this has been quite a cool experience. I didn't really

429
00:34:50,680 --> 00:34:56,200
do reinforcement learning research before I came to this project. So for me, I learned a ton

430
00:34:56,200 --> 00:35:03,640
about all different environments that are out there, about different algorithms. And I also learned

431
00:35:03,640 --> 00:35:10,520
that this kind of funny anecdote that even now these algorithms that we have, they're very good

432
00:35:10,520 --> 00:35:17,640
at exploiting kind of loopholes in the game engine. And like one example that people in our team

433
00:35:17,640 --> 00:35:22,680
found and the photo is pretty funny is if you have 11 players playing football and trying to

434
00:35:22,680 --> 00:35:28,360
score against a single goalkeeper, what is kind of the best strategy to do? And turns out in

435
00:35:28,360 --> 00:35:34,440
the version that we have the game that we had at that time, what the agent learned is it would

436
00:35:34,440 --> 00:35:42,200
go take the ball, kick it outside of the playing field so that the goalkeeper has to go to a throw-in

437
00:35:42,200 --> 00:35:45,880
where he doesn't have any teammates, so he's going to throw it somewhere where the other players

438
00:35:45,880 --> 00:35:51,240
have it, and now the players will score. And I think that really it is a pretty cool

439
00:35:52,360 --> 00:35:56,040
example that we found. How long did it take the agent to figure that out?

440
00:35:56,040 --> 00:36:01,480
I don't remember the details, but it's kind of like once it found this, it's a super smart strategy,

441
00:36:01,480 --> 00:36:06,280
right? And it's really exploiting kind of a loophole. We had a similar one where essentially when

442
00:36:06,280 --> 00:36:11,560
you train these agents in the beginning, the opposing team scores a lot and you're kind of learning

443
00:36:11,560 --> 00:36:19,960
not to admit any goals, right? And one thing I learned is for example that once the opposing

444
00:36:19,960 --> 00:36:24,840
player, for example, kicks the ball besides the goal, your goalkeeper when he's doing the kickoff,

445
00:36:24,840 --> 00:36:28,920
he can just wait indefinitely because the engine didn't include any mechanism to

446
00:36:28,920 --> 00:36:33,960
pray with that, and that is a good way to minimize the opponent's goaling goals if the game doesn't

447
00:36:33,960 --> 00:36:40,520
continue. I think these things are super funny, and I think it's also highlights one of the things

448
00:36:40,520 --> 00:36:48,200
which we didn't really talk about yet, but which is that doing research in this football environment

449
00:36:48,200 --> 00:36:53,800
also makes that research to some degree more accessible, right? A lot of people they care about

450
00:36:53,800 --> 00:37:01,720
football, they like watching football is very intuitive, and that's why I also, we hope that

451
00:37:01,720 --> 00:37:07,240
actually having a football environment also helps with actually people maybe demystifying a bit

452
00:37:07,240 --> 00:37:12,120
what is happening in this research so that people can see, oh, you know, this is a random agent

453
00:37:12,120 --> 00:37:17,080
that's doing this, right? And then suddenly it starts learning how not to admit goals, and then

454
00:37:17,080 --> 00:37:23,160
you see how it learns to score. I think people can very much identify with it, students that

455
00:37:23,160 --> 00:37:28,840
are doing master programs that want to learn about reinforcement learning. It's super cool,

456
00:37:28,840 --> 00:37:35,480
you can go with the environment, learn how to pass or kind of score against an empty goal,

457
00:37:35,480 --> 00:37:40,440
then two versus one, and so on. So I think that's maybe one of the benefits that we didn't really

458
00:37:40,440 --> 00:37:45,640
talk about, which is that it is a very accessible environment, right? You don't like like a lot of

459
00:37:45,640 --> 00:37:52,360
people know about football. And of course, you just recently published this, but are you aware of any

460
00:37:52,360 --> 00:38:01,720
efforts or plans to incorporate this into kind of learning curricula? So deep learning and reinforcement

461
00:38:01,720 --> 00:38:07,560
is very fast-moving field. So we chose the approach of releasing the environment and publish,

462
00:38:07,560 --> 00:38:14,280
putting online the paper, so this has not been published yet. It's on the web, everybody can

463
00:38:14,280 --> 00:38:19,640
use it, but we're definitely trying to publish this, but it has not been published yet. Got it. And

464
00:38:19,640 --> 00:38:28,600
the second question for curriculum learning, we did some very initial experiments. To be

465
00:38:28,600 --> 00:38:38,600
clear, more talking about DC application, well, you spoke to seeing applications of this environment

466
00:38:38,600 --> 00:38:46,200
in the education of humans and growing their own knowledge of reinforcement learning,

467
00:38:46,200 --> 00:38:51,880
and I was just curious if you were aware of any plans to formalize that and maybe build it into

468
00:38:53,240 --> 00:38:59,880
some online course or some school-based course to teach students about reinforcement learning

469
00:38:59,880 --> 00:39:05,800
using this metaphor that they're very familiar with. So I'm not aware of this, but I definitely

470
00:39:05,800 --> 00:39:11,960
hope that this happens. We've seen quite a lot of positive feedbacks on things like Twitter,

471
00:39:11,960 --> 00:39:17,320
and where people got very excited about that. So I definitely hope that helps people access

472
00:39:17,320 --> 00:39:25,240
research. Again, also given that it's open-source, it's very easy to use or to get started. If people

473
00:39:25,240 --> 00:39:30,040
are interested in them, even if your listeners are interested in, we go to our GitHub page,

474
00:39:30,680 --> 00:39:37,880
GitHub.com Google-research-football. It's very easy to get started. You can install the

475
00:39:37,880 --> 00:39:43,080
environment, there is the examples, which show you how to train a basic agent. So I'm all for

476
00:39:43,080 --> 00:39:50,280
making it as accessible as possible. You talked about the agents that you've played with finding

477
00:39:50,280 --> 00:39:57,160
these corner cases to exploit the environment. How do you address that? It strikes me that in some

478
00:39:57,160 --> 00:40:06,600
cases, the solution of that is maybe putting in more constraints that are like the human game of

479
00:40:06,600 --> 00:40:11,320
football or better modeling that environment. In other cases, you might need to do something

480
00:40:11,320 --> 00:40:20,680
that doesn't make sense in the context of the actual sport. It creates this unique computer

481
00:40:20,680 --> 00:40:27,960
football sport. Does that resonate at all? How have you addressed those kinds of corner case

482
00:40:27,960 --> 00:40:35,400
issues? We ran a lot of experiments. As you said, it is actually pretty hard to build a good

483
00:40:35,400 --> 00:40:41,320
reinforcement learning environment. I think now we are pretty confident that it works pretty well,

484
00:40:42,440 --> 00:40:49,080
but that doesn't exclude that you go, come with a new learning algorithm. It's really good.

485
00:40:49,080 --> 00:40:55,640
It finds another corner case. I think we then have to address this on a case-by-case basis.

486
00:40:55,640 --> 00:41:02,760
I think it also boils back to the underlying motivation. I think the motivation is not

487
00:41:02,760 --> 00:41:10,840
necessarily playing as well as possible in this environment. It's a primary goal, but the goal

488
00:41:10,840 --> 00:41:15,400
is to do research. If there are things that hind the research in this environment,

489
00:41:16,360 --> 00:41:22,280
then I guess we have to change it. On the other hand, I think there's also the general feeling

490
00:41:22,280 --> 00:41:28,280
our team. There is a benefit to having stability on the environment side, so that research and

491
00:41:28,280 --> 00:41:35,480
results stay comparable across different papers. We definitely don't want to change the

492
00:41:35,480 --> 00:41:40,760
environment too often, but if there is very obvious payload cases, which you can very exclude,

493
00:41:40,760 --> 00:41:47,400
then we might have to do this. It sounds like then that for the most part, these exploitable

494
00:41:47,400 --> 00:41:55,400
corner cases, you've considered them as bugs as opposed to inherent qualities of the game

495
00:41:55,400 --> 00:42:04,920
that you're trying to model. Does that make sense? Yes. I think the example of the goalkeeper

496
00:42:04,920 --> 00:42:08,760
having to go during the throw-in that really defeats the purpose of these scenarios we've

497
00:42:08,760 --> 00:42:14,200
defined. Similarly, the example that I gave where the goal doesn't kick it away. If you do this

498
00:42:14,200 --> 00:42:24,120
in a real game, you would get a delay of game penalty. Something would happen. I think that

499
00:42:24,120 --> 00:42:30,840
really shows, maybe this was really a bug in the implementation. In the other hand,

500
00:42:30,840 --> 00:42:35,960
it also boils back to whenever you're building an environment like this. You have to take some

501
00:42:35,960 --> 00:42:41,640
choices. We've done this with the best faith effort, and we think now it is ready to be used

502
00:42:41,640 --> 00:42:47,960
for research. Olivier, thanks so much for taking the time to share this project with us.

503
00:42:47,960 --> 00:42:58,120
Yeah, thanks for having me here. All right everyone, that's our show for today. If you like what

504
00:42:58,120 --> 00:43:03,160
you've heard, please do us a favor and tell your friends about the show, and if you haven't already

505
00:43:03,160 --> 00:43:08,280
hit that subscribe button yourself, make sure you do so you don't miss any of the great episodes we've

506
00:43:08,280 --> 00:43:21,800
got in store for you. As always, thanks so much for listening, and catch you next time.

