WEBVTT

00:00.000 --> 00:10.000
All right, everyone. I am here with Samir Singh. Samir is an assistant professor in the

00:10.000 --> 00:16.240
Department of Computer Science at UC Irvine. Samir, welcome to the Twimal AI podcast.

00:16.240 --> 00:22.400
Thank you, Simon. Excited to be here. I'm excited to chat with you. Why don't we get started

00:22.400 --> 00:26.800
by having you share a little bit about your background? What got you started working

00:26.800 --> 00:32.720
in machine learning? So yeah, I've been sort of thinking about AI as a goal for a long time,

00:32.720 --> 00:37.840
you know, since teenage years reading all these science fiction books and wanting to solve AI.

00:38.720 --> 00:45.120
And in my naivety, I thought the solution was to build robots initially. And so I didn't

00:45.120 --> 00:50.080
undergrad in electrical engineering, trying to do hardware sort of things, build manipulators,

00:50.080 --> 00:55.440
and sort of small vehicles and things like that. And then when I started reading research

00:55.440 --> 01:01.120
papers, I kind of realized that at least at that time, people were spending their whole PhDs

01:01.120 --> 01:07.680
just trying to grasp like a cup of something, right? And that didn't seem like AI to me.

01:07.680 --> 01:13.600
So I quickly sort of shifted more towards the software side of things. And at that time,

01:13.600 --> 01:20.400
again, a lot of AI was doing a star search and things like that, which seemed a lot more cooler.

01:20.400 --> 01:26.160
But didn't seem to quite get at the learning or the intelligence aspect of it. So slowly,

01:26.160 --> 01:32.160
I made my way and found machine learning and you know, been working in machine learning for a long

01:32.160 --> 01:39.600
time now. And within machine learning, I think NLP ended up being this sort of really attractive

01:39.600 --> 01:45.360
application for me. It seemed a good mixture of being very, very practical, especially with internet

01:45.360 --> 01:51.440
and everybody posting stuff online, but also addressing some of the fundamental things that excited

01:51.440 --> 01:56.480
me about AI being able to communicate, being able to understand humans and things like that. So

01:57.200 --> 02:00.880
that's kind of how I made my way into doing machine learning.

02:02.160 --> 02:06.720
Nice. Nice. There's some out there that think that embodied applications like robotics are

02:07.360 --> 02:14.240
really the only way we're going to get close to to AGI. You know, that may be only the

02:14.240 --> 02:19.760
roboticist that think that though. Well, what you're taking on that is that's, you know, that was

02:19.760 --> 02:26.560
the goal that you started out with. Yeah, I think embodied AI is such a like concrete thing that

02:26.560 --> 02:32.800
I feel like that there is something very attractive about that. But you know, everybody's struggling

02:32.800 --> 02:39.200
with what is AI? How do we define it and things like that? I think of learning being a key component

02:39.200 --> 02:47.440
and learning doesn't have to be grounded necessarily. And if something is learning from internet and

02:47.440 --> 02:54.800
from YouTube videos, audio streams, that might be sufficiently intelligent, right? Like who knows?

02:55.600 --> 03:00.960
Our kids right now really grounded or are they completely in the internet? And you know,

03:00.960 --> 03:05.840
it's difficult to say even us as humans how grounded we are going to be in the future.

03:05.840 --> 03:15.360
In the matrix, are we is anyone grounded? Exactly. So tell us a little bit about your research focus.

03:15.360 --> 03:19.760
What are some of the areas that you're spending your time on nowadays?

03:20.480 --> 03:27.680
Yeah, so for a long time, I was doing an LP, sort of a lot of different tasks in an LP. And this is

03:27.680 --> 03:34.240
sort of, you know, about, I guess I want to say eight to 10 years ago, where I was so focused on

03:34.240 --> 03:40.160
the task that I was working on that sort of the deep learning stuff kind of went by me. And I was

03:40.160 --> 03:45.840
working on sort of more graphical models and log linear kind of stuff. And by the time I started

03:45.840 --> 03:51.600
doing deep learning things, it was really, really good at everything I was doing. But it wasn't using

03:51.600 --> 03:56.800
any of the cool stuff that I was working on. So that made me feel like, okay, why is it doing what

03:56.800 --> 04:03.360
it is doing? Whether it is, it is actually doing what it is doing. So you could say it was almost

04:03.360 --> 04:09.760
by spite that my research agenda got started. But you know, sort of playing around with these

04:09.760 --> 04:16.480
models, you know, I did my postdoc at University of Washington with Carlos. And with one of his

04:16.480 --> 04:21.760
students at the time, we started looking at, okay, these deep learning models really seem to be

04:21.760 --> 04:28.560
quite good. But we have very little idea of what's going on in them. And so maybe let's try to

04:28.560 --> 04:34.640
find a way to get to some place that we are actually familiar with. So this took me back to my

04:34.640 --> 04:40.480
sort of linear classification or log linear model days where I used to spend a lot of time

04:40.480 --> 04:45.200
looking at feature weights and trying to figure out, okay, is there a problem here? Can I fix

04:45.200 --> 04:52.160
something? Or in my worst days, can I even change some of these weights to try and get it to do

04:52.160 --> 04:58.240
something? And I think, but since none of that was possible in deep learning, we sort of started

04:58.240 --> 05:04.480
thinking about how we could try to get into the internals of deep learning and at least provide

05:04.480 --> 05:09.600
some intuition as to what's going on. And that's sort of where I started working on explainability.

05:09.600 --> 05:15.920
And we came up with this line algorithm in order to explain black box models. And since then,

05:15.920 --> 05:21.600
I think, you know, explainability has been really interesting. But initially we were hoping for

05:21.600 --> 05:29.600
explainability to tell us why deep learning models are working. But often they ended up telling us

05:29.600 --> 05:36.960
why the deep learning models were not working. And so my tragically sort of has become a lot more

05:36.960 --> 05:44.880
on looking at why are these explanation methods telling us that they are not working when all

05:44.880 --> 05:50.000
of these leaderboards and New York Times articles are telling us that they are working. There seems

05:50.000 --> 05:56.080
to be this big mismatch. And so why does this mismatch exist? And what can we do about it?

05:56.720 --> 06:00.400
So in the recent couple of years, I've been thinking a lot more about evaluation,

06:00.400 --> 06:04.400
about debugging and things of that nature for machine learning.

06:04.400 --> 06:12.560
Nice. Nice. Yeah. I'll say that the line paper, which goes back four or five years,

06:13.520 --> 06:17.680
well, first of all, as you noted, we're still struggling with the same problem with deep learning

06:17.680 --> 06:26.400
generally. But the paper itself and that method itself seems to have had pretty strong staying power.

06:26.400 --> 06:33.680
Like people are still talking about it. People are still using it. By the time this show comes out,

06:33.680 --> 06:39.040
we will have had our recent event model explainability forum where we dig into

06:40.800 --> 06:47.120
a lot of the current issues in model explainability. But when people are evaluating explainability methods,

06:47.120 --> 06:56.240
Lyme is one that still comes up. Any thoughts or reflections on that work? Did you expect it to be

06:56.240 --> 07:01.760
as important as it has turned out to be when you were working on it? That's a good question.

07:01.760 --> 07:07.040
I guess you hope that every paper you're working on, I would hope more people think that every paper

07:07.040 --> 07:13.840
they're working on is actually going to be world changing and get people excited. But it usually

07:13.840 --> 07:21.600
doesn't happen with Lyme, of course, people got excited about it a lot more than we anticipated.

07:21.600 --> 07:28.640
And there are both good and the bads of it. We are close to Lyme, so we know what it's doing in a way

07:28.640 --> 07:34.160
that we know what it's capable of and what it's not capable of. And we've been more recently

07:34.160 --> 07:38.960
with the collaboration with Hema, who I think you had on your podcast a few weeks ago. We've also

07:38.960 --> 07:43.840
been working on going back and looking at Lyme and saying, okay, can it be manipulated if they were

07:43.840 --> 07:47.920
like bad agents and stuff? And we show that it's actually not that difficult to do that.

07:49.200 --> 07:56.400
And I think just like machine learning is considered to be this thing that will come and solve

07:56.400 --> 08:01.680
all your problems. Lyme seems to be well. Whatever problems machine learning has,

08:01.680 --> 08:05.840
now Lyme is going to solve it for you and it's not like that right now. It's one of the

08:05.840 --> 08:13.040
explainability techniques. It is a useful tool, I believe, especially for evaluation, but it's not

08:13.040 --> 08:19.040
something that has solved the model explainability problem. And if anything, it initiated the discussion,

08:19.040 --> 08:23.520
and I'm glad that people are talking about it because it's a really, really difficult problem,

08:23.520 --> 08:29.040
even to define what explainability is, much less solved with in a single paper.

08:29.040 --> 08:39.520
And so that paper set the stage for the paper we'll be spending some time talking about today,

08:39.520 --> 08:48.080
and that is beyond accuracy, behavioral testing of NLP models with checklists, which was the

08:48.080 --> 08:58.240
ACL 2020 best paper winner. Congratulations. Thank you. Tell us a little bit about that paper and how

08:58.240 --> 09:05.280
the some of your prior research led you to that problem. And so I think a lot of my research,

09:05.280 --> 09:11.360
especially in collaboration with Marco, has been sort of looking at this mismatch between what

09:11.360 --> 09:19.200
accuracy seems to suggest and what we know models are actually capable of. And accuracy is

09:19.200 --> 09:24.480
something that's very quantitative. It's on the leaderboard out there. You can say 10% better

09:24.480 --> 09:30.560
or worse or whatever it is. But on the other side, what models are good at and what models are

09:30.560 --> 09:37.040
bad at is not something that's quantified or formalized or even well-defined as far as the

09:37.680 --> 09:45.840
community is concerned. So what we wanted to do was try to look at this mismatch and say,

09:45.840 --> 09:52.720
okay, there should be some way of thinking about this that goes beyond just creating another leader

09:52.720 --> 09:59.040
board or introducing a different data set. So last couple of years, we this work started maybe two

09:59.040 --> 10:06.160
years ago with this work we did called Sears. Where the idea was very simple, what we're going to do

10:06.160 --> 10:11.920
is we're going to think of doing a serial attacks on models, but instead of doing this adding some

10:11.920 --> 10:16.240
noise that people don't understand, we're going to take the original instance and we're just going

10:16.240 --> 10:23.600
to paraphrase it. So it should be exactly the same sentence in terms of its meaning, but the actual

10:23.600 --> 10:29.040
form might be different. And we had an automated tool for doing this building upon a lot of other

10:29.040 --> 10:34.320
people who've been working on back translation and things like that. And paraphrasing was what you

10:34.320 --> 10:39.760
called the adversarial attack in this case? Yes, because it was a paraphrase that was getting the

10:39.760 --> 10:48.480
model to change its prediction. Got it, right? And so, you know, like, why did China take some action?

10:50.000 --> 11:00.320
Or let me think of a different. So what is that thing on the table versus what's that thing on the

11:00.320 --> 11:05.680
table? This is like a very simple version of a paraphrase. Of course, this means the same to us,

11:05.680 --> 11:10.480
but at that time, there were models that got, like, gave a different prediction for these two

11:10.480 --> 11:17.920
instances. So that was one way to start characterizing what the problems were in the models.

11:17.920 --> 11:22.800
We continue this work. So we can call this sort of robustness to paraphrases, I guess. We had

11:22.800 --> 11:30.320
another follow-up work that was looking at consistency. So if you had, is there a bird in the image?

11:30.320 --> 11:36.400
Or let's say how many birds are in this image? The model might say two. And then we say,

11:37.120 --> 11:43.280
are there two birds in this image? The model should clearly say yes, but the model did not often.

11:44.320 --> 11:49.520
And what that meant was model was not even being consistent with its own sort of what things

11:49.520 --> 11:57.360
are going on. And so that also sort of started thinking as about how can we characterize what the

11:57.360 --> 12:03.520
pitfalls in these models are. And that all sort of culminates in this work in checklist,

12:03.520 --> 12:11.120
where we kind of create a tool to allow users to think a lot more about testing in a structured way,

12:11.120 --> 12:22.240
essentially. Okay. Yeah. Identifying the pitfalls or possible failure modes is

12:22.240 --> 12:30.160
fairly different from identifying the causes of those pitfalls. How well do we understand

12:30.160 --> 12:34.560
for the various models that you're looking at what's actually causing the problems?

12:36.320 --> 12:44.000
Very little, I think. So you would hope that, so just to sort of clarify what checklist does is it

12:44.000 --> 12:49.120
creates essentially helps you create a bunch of tests for your model. And when the tests fail,

12:49.120 --> 12:53.760
similar to traditional software engineering, sometimes the test fails and you're like,

12:53.760 --> 13:00.720
oh, I forgot to use that variable instead. I use some other variable. But sometimes it fails and

13:00.720 --> 13:05.600
you're like, oh, wait, the whole my whole piece of code, the way I was structuring it is completely

13:05.600 --> 13:10.960
wrong. Right. And so even in machine learning, when something fails, either it could be some artifact

13:10.960 --> 13:18.320
of the training data, or it could be, it could lead to like a string of five research papers that

13:18.320 --> 13:23.920
eventually solve their problem, hopefully. Right. So, so for example, fairness is a good example

13:23.920 --> 13:31.200
of that. Right. If your model is being unfair, there are no easy. I'm going to replace all

13:31.200 --> 13:36.400
males with females or something like that and solve the problem. That doesn't do it. But

13:37.200 --> 13:42.560
checklist focuses more on at least do you know if this is a problem or not? And can you get an idea

13:42.560 --> 13:53.840
of how much this is a problem? Okay. So, you describe checklist as analogous to kind of behavioral

13:53.840 --> 14:02.240
testing in software engineering, you know, kind of riff on that analogy a little bit. Yeah. So,

14:02.240 --> 14:06.800
that's actually where we started. We were like, people have been building all these complex

14:06.800 --> 14:11.360
software systems. Machine learning is not the first one to come up and start predicting the

14:11.360 --> 14:17.360
sentiment. There are way other more complicated software that do much more complicated things.

14:17.360 --> 14:21.840
How do they approach testing? And this is something that that all of us know and we wanted to see

14:21.840 --> 14:28.160
how many of those lessons can be taken and applied to machine learning. So, the idea was we we create

14:28.160 --> 14:32.560
a bunch of different kinds of tests and I can sort of walk you through what they look like.

14:32.560 --> 14:39.600
But the easiest one to understand is something we call minimal functionality text at minimum functionality

14:39.600 --> 14:46.080
test where it basically looks like a unit test in software engineering, right? So, I'm not going to

14:46.080 --> 14:51.760
think too much about what the everything that the model is supposed to be doing, but I'm going to

14:51.760 --> 14:59.200
pick a single phenomena. So, say we are trying to do sentiment analysis and we just want a very simple

14:59.200 --> 15:09.680
like does it understand negation or not, right? If I say I this movie is not good, is my sentiment

15:09.680 --> 15:15.520
classifier able to do that or not? Right. So, that's a very simple unit test. That by itself,

15:15.520 --> 15:21.200
if the model does it or not can be useful, but we sort of have this whole templating engine that

15:21.200 --> 15:29.920
says I can say something like this blank is not blank where we fill blanks with the first

15:29.920 --> 15:35.680
blank with a bunch of nouns and the second blank with a bunch of positive adjectives and negative

15:35.680 --> 15:40.960
adjectives and then we have an expectation over whether the model will predict positive or

15:40.960 --> 15:46.960
negative review for it. And so, in this case, we create many test cases as they would say in software

15:46.960 --> 15:52.480
engineering and we test a machine learning model by just running the model output through these

15:52.480 --> 15:59.120
things. And what this does is it hasn't told you like does this model, if you pass all the test,

15:59.120 --> 16:05.600
you don't know whether the model has definitely learned how to do negation or not, but if it does

16:05.600 --> 16:11.040
fail most of this test, it would be a red flag and you would say okay, the model doesn't even

16:11.040 --> 16:16.560
understand such a simple negation, it probably doesn't understand negation. So, based on this intuition,

16:16.560 --> 16:20.800
we created a bunch of different kinds of tests and I can walk you through those a few things.

16:20.800 --> 16:30.080
Yeah, before we do that though, you gave sentiment analysis as an example, what is the scope of

16:30.080 --> 16:36.080
checklist? For example, we spent a lot of time talking about language models nowadays and

16:36.640 --> 16:44.480
transformers and things like that, does it address those kinds of tasks as well or is it limited to

16:44.480 --> 16:52.720
classification? What are the boundaries of this work? Yeah, so I think in the paper itself,

16:52.720 --> 16:58.240
we had a bunch of tasks that go beyond sentiment analysis or just simple classification, I guess.

16:58.240 --> 17:03.200
And just to sort of motivate why we picked sentiment analysis, this was also one of the system

17:03.200 --> 17:09.600
that one of the tasks that research papers are constantly looking at and trying to do better on

17:09.600 --> 17:16.240
and by some metrics, we are better than humans on sentiment analysis, which may not be a surprise

17:16.240 --> 17:21.840
to many people. But also, it was one of those tasks where there were a lot of commercial products.

17:21.840 --> 17:29.360
So, like Microsoft and Google and Amazon make a ready to purchase sentiment analyzer.

17:29.360 --> 17:35.120
And what we wanted to do was apply checklist, not just to some research models that we had lying

17:35.120 --> 17:39.840
around, which were all transformable based like Word and Roberta, but also apply checklist to

17:39.840 --> 17:47.520
these commercial systems to see what they were lacking in and sort of be able to compare across

17:47.520 --> 17:52.800
research and industrial models. But in the paper, we also did things like question answering,

17:52.800 --> 17:57.360
where you're given a question, you have to come up with an answer, given a paragraph,

17:57.360 --> 18:03.120
we did a paraphrase detection. So, there's a Korak question pair dataset where you have pairs of

18:03.120 --> 18:08.080
questions and you want to detect whether their paraphrases are not. We didn't directly do language

18:08.080 --> 18:13.520
modeling, but that's something that we've been working on. And it's a little bit trickier to

18:13.520 --> 18:19.840
define what even a test is for language models, but I think there are ways to use checklist for that as

18:19.840 --> 18:30.880
well. Okay. Okay. So, walk us through the kind of the array of tests. Yeah. So, the idea here is that

18:30.880 --> 18:38.640
you want to figure out what are the capabilities of the model. So, we conceptualize it as a matrix

18:38.640 --> 18:43.760
where there are a bunch of rows, where the rows are sort of what you want to test about the model.

18:43.760 --> 18:49.120
Right. So, suppose simple negation is something that you might want to test, and the easiest way

18:49.120 --> 18:56.640
to test that is to create a bunch of minimum functionality tests, like unit tests. For some other

18:56.640 --> 19:05.280
ones, we wanted to test things like robustness to location names. Right. So, my flight to US was great.

19:06.560 --> 19:13.920
I want to see how much, whether the model is robust to changes in the country name. If I replace US

19:13.920 --> 19:18.960
with a different country, or if it's a city, then replace it with a different city, the sentiment

19:18.960 --> 19:24.800
doesn't change, but other models robust to that or not. That doesn't quite fit into the unit test.

19:24.800 --> 19:30.880
So, we had a second category of tests, which we call invariance tests, where we are,

19:30.880 --> 19:36.400
and this is also something that's similar in software engineering, where we are essentially mutating

19:36.400 --> 19:42.240
the input or perturbing the input, in ways that we know shouldn't affect the sentiment.

19:42.960 --> 19:49.760
In this case, replacing US with China or any other country, and then trying to see how often

19:49.760 --> 19:55.280
does that change the actual output of the model. And this was one of the ones which was quite

19:55.280 --> 20:01.280
surprising to us, because many times when you change the location, the output of the model

20:02.000 --> 20:11.200
changes, which was surprising and disappointing. Some of the other kind of capabilities you might

20:11.200 --> 20:17.760
want to check was, you're like, okay, I don't care what the review was, but if I explicitly add a

20:17.760 --> 20:24.880
strongly negative statement to it. So, if I say blah, blah, blah review ends, I really hated it

20:24.880 --> 20:32.320
at the end. You would hope that the model will not become more positive towards this review,

20:32.320 --> 20:37.760
right? Now, this is also not invariance, because in case, in this case, you're changing the input,

20:37.760 --> 20:42.880
but you're trying to test the directionality of the output. So, this is more of a directional

20:42.880 --> 20:50.880
test. And we just want to make a test where you've got, you use a similar kind of strong,

20:50.880 --> 20:56.800
definitive statement, and you want to see if the model even picks up on that, even if there's

20:56.800 --> 21:03.760
other things that are more ambiguous or even positive. Is there, do you have a test kind of

21:03.760 --> 21:12.320
blown those on? Not really, because it's always like, we wanted to define tests where we wanted

21:12.320 --> 21:20.960
it was a failure rate of 0. We didn't want something where 80% would be still, sorry, 20% failure

21:20.960 --> 21:27.360
would still be okay, right? So, with these strong statements, you can never be sure whether

21:28.080 --> 21:34.720
they contradict enough with the review text, right? So, in this case, we wanted to keep it simple,

21:34.720 --> 21:40.960
right? Like, we wanted to say at least this thing, the model should be able to get 0% failure on.

21:40.960 --> 21:47.360
Right? So, it shouldn't become more positive is a very simple statement. That's why it was a

21:47.360 --> 21:54.000
surprise to us, where many times it did like more than a third of the time, it just became more

21:54.000 --> 21:59.920
positive when you added phrases like that, right? And these are some of these word sort of commercial

21:59.920 --> 22:08.000
systems as well. So, yeah, so these kind of tests are in some sense, the way we describe it is

22:08.000 --> 22:14.160
like that, I guess mathematically more of a necessary condition for a model to get deployed,

22:14.160 --> 22:19.920
just because you get 0% failure doesn't mean that the model is safe to deploy, just because

22:19.920 --> 22:24.720
it's similar to software testing, right? Just because all your tests pass doesn't mean you don't

22:24.720 --> 22:30.320
have any more work to do. You probably either need to write more tests or write more code and

22:30.320 --> 22:38.400
it's right, but at least if a test fails, there is a red flag and you know exactly what the model

22:38.400 --> 22:43.520
is not able to do. And just by breaking these things down, I think that's the main contribution

22:43.520 --> 22:47.760
of checklist is making, hopefully making people think a little bit more about these different

22:47.760 --> 22:58.560
dimensions of the problem than just the single number. Got it. Is there an analogy to code coverage

22:58.560 --> 23:08.080
in these types of tests? That's a good question. There has been some work that tries to do these

23:08.080 --> 23:15.120
things where they, the analogy to code coverage is to look at the neurons inside the transformer

23:15.120 --> 23:21.120
or whatever and try to make sure there are enough inputs in your test case or whatever,

23:21.120 --> 23:27.120
at the same time, whatever you're planning to do that go through all the neurons at some point.

23:27.120 --> 23:34.080
We haven't looked at things like that. Yeah, we are sort of going back to like it almost like a

23:34.080 --> 23:40.560
black box model way of thinking about this, right? You are someone who cares about sentiment analysis

23:40.560 --> 23:45.440
or you care about whatever you're trying to use machine learning for and presumably with that,

23:45.440 --> 23:53.040
you have a set of capabilities that you expect someone who's claiming it can do sentiment analysis

23:53.040 --> 23:58.480
should be able to do, right? So we are sort of, I guess, delegating that to the users,

23:58.480 --> 24:06.000
rather than thinking about the internals of the model. And so from a practical perspective,

24:06.000 --> 24:18.080
if I read the paper, download the code, are you suggesting that this is a kind of point

24:18.080 --> 24:24.880
to set your model and run it and it's a full, it kind of throws the book at your model and tells

24:24.880 --> 24:34.320
you where it's weak or do I have to adapt checklist with my particular model in mind and the things

24:34.320 --> 24:43.760
that maybe the things I'm concerned about or how engaged as a user have to be in taking advantage of

24:43.760 --> 24:49.920
this. Yeah, so that's a good question and we've been thinking a lot about this. And I guess the answer

24:49.920 --> 24:55.920
is it's somewhere in between depends specifically on what you're working on. So I guess the easiest one

24:55.920 --> 25:03.520
would be if you are creating a sentiment analysis system in your company, then it's trivial. You can

25:03.520 --> 25:10.480
just download our code and run it. If you are doing one of the tasks that are not already supported

25:10.480 --> 25:16.640
in checklist, then it's pretty easy and we've tried to like Mark was written really nice documentation

25:16.640 --> 25:21.200
and sort of walk through it. It's pretty easy to get started and start thinking about okay,

25:21.200 --> 25:27.920
what are the capabilities for this specific task and how do I write it down in a way that we can

25:27.920 --> 25:35.680
generate a lot of tests. And checklist is really, really useful for use cases like that. I would say

25:35.680 --> 25:39.600
we did some experiment also to sort of make sure that this is the case and we can talk about that

25:39.600 --> 25:46.800
a little later. But what we also want to try to do and we are sort of getting there slowly

25:46.800 --> 25:52.320
is to make it incredibly easy for people to contribute the tests or for us to keep

25:53.200 --> 25:59.600
growing the set of tests that are available in checklist. So if you're doing a task that is either

25:59.600 --> 26:05.360
slightly adjacent to what we already have. For example, you're doing paraphrasing but not question

26:05.360 --> 26:11.520
paraphrasing, then you may have additional tests and we make it really easy to include it back in

26:11.520 --> 26:18.320
checklist. We also want to make it really easy to evaluate any new model that comes out. So

26:18.320 --> 26:23.680
we want to, for example, if hugging phase has a new transformers model, we want to make it

26:23.680 --> 26:29.440
incredibly easy to also test and generate sort of almost a report card of okay. On negation,

26:29.440 --> 26:36.000
we got this much failure rate on something else, we got this much and so on. Got it. Yeah. The

26:36.000 --> 26:44.720
task that the user has to do to adapt it to their model is it writing tests or writing some kind

26:44.720 --> 26:51.200
of meta test or test recipe that checklist then uses to generate a bunch of tests. I kind of heard

26:51.200 --> 26:56.800
both in your description. Yeah, so it kind of supports both, but I think the way we've been

26:56.800 --> 27:02.160
approaching this is for the first part, you just have to think a lot. So you have to think about

27:02.160 --> 27:07.680
what are the different capabilities and checklist can not quite help you with that. Maybe it helps a

27:07.680 --> 27:14.640
little bit by some of the automated exploration tools, but the idea is so we can take any specific

27:15.440 --> 27:21.360
safe fairness or something like that. I want my model to be fair. Well, how do we answer something

27:21.360 --> 27:29.200
like this and say I'm doing question answering, right? So okay, how can I test whether the model is

27:29.200 --> 27:38.160
being fair and let's be more specific? Let's say gender fairness, right? So if I think the model

27:38.160 --> 27:43.360
is not fair, one way to test whether it's fair or not is to come up with a really simple

27:43.360 --> 27:53.840
context for the question answering system where we can say John is not a doctor, but Mary is

27:54.960 --> 28:02.160
and then ask the question, who is the doctor, right? This thing is something we can easily write out

28:02.160 --> 28:07.440
and we know the answer should be Mary because it's obvious from this sentence and that's a test,

28:07.440 --> 28:14.800
right? Now we can say, well, if the model fails or not, it could be dependent on the word choice

28:14.800 --> 28:21.360
of John or Mary. So I might maybe I want to replace John or Mary with any other names, right? So we

28:22.080 --> 28:27.440
John with any other male name, Mary with any other female name and checklist has some of these

28:27.440 --> 28:32.320
lexicon built in. So you can easily just say, okay, I'm going to create a template that says

28:32.320 --> 28:42.640
male first name is not a doctor, but female first name is, who is the doctor, female first name,

28:42.640 --> 28:47.920
right? That's one level of templating. Then you can say, well, why should it be just doctor?

28:47.920 --> 28:54.080
Maybe there are other professions that we also want to test. So we have a tool that just you

28:54.080 --> 28:59.200
can hide doctor and it suggests a bunch of professions and you can say, okay, I want to

28:59.200 --> 29:04.720
say pick a bunch of these as part of the test and by doing all of these things, you can essentially

29:04.720 --> 29:11.520
create thousands of use cases automatically, even just for the single statement that I talked about

29:11.520 --> 29:18.240
and then just quickly test it whether the model your model is actually passing them with a sufficient

29:18.240 --> 29:27.760
tolerance or not. Is there, do you envision a way to use checklist or maybe some future

29:27.760 --> 29:37.600
version or evolution of this work? You're not just to give you kind of a binary pass fail or

29:37.600 --> 29:45.360
sufficient or necessary, I forget which of those conditions you mentioned, but necessary.

29:46.960 --> 29:53.040
But rather to give you insights into your model that you can then turn around into the

29:53.040 --> 29:57.760
take into the training loop, the training process, for example, you mentioned these,

29:59.360 --> 30:10.080
you know, the gender bias example. Is there, can it tell you more than you have a problem and maybe

30:10.080 --> 30:16.080
how you might go about fixing the problem? Yeah, how you go about fixing a problem is a much more

30:16.080 --> 30:22.640
difficult step, but I would say the first step of going about fixing a problem is to know

30:22.640 --> 30:28.560
all the problems that your model might have, right? So yeah, I guess the short answer is no,

30:28.560 --> 30:35.600
that's not something we focus on. In the future, we are thinking of ways to do it. The tricky thing here

30:35.600 --> 30:44.560
is I think of these tests almost as like you should think of the test set in machine learning,

30:44.560 --> 30:49.520
right? So test set is supposed to be this hidden test set just because you get some error on the

30:49.520 --> 30:53.520
test set doesn't mean you just add those instances to your training data, right? If you do that,

30:53.520 --> 30:59.840
you've lost the value of the test, right? So in the in similar sense, these tests or what checklist

30:59.840 --> 31:06.400
is doing should be treated as something slightly external to the training process because you've come

31:06.400 --> 31:12.480
up with the one way of phrasing the negation and you want to just test that as a proxy for other

31:12.480 --> 31:18.400
ways of phrasing the negation. And so if you put this check this test somehow back into the

31:18.400 --> 31:24.320
training loop, you've lost that advantage you had and now you have to come up with either a different

31:24.320 --> 31:31.440
formulation of negation or assume that the model has learned negation and both of those might be

31:31.440 --> 31:37.280
difficult or wrong things to do. So I think at least for now, we are thinking of checklist a little bit

31:37.280 --> 31:45.600
as a test set that's a lot more fine grained and essentially customizable to your specific task.

31:45.600 --> 31:54.240
Got it, got it. Tell us a little bit about the evaluation process for as you built this out.

31:54.880 --> 32:03.200
Yeah, so we built this tool and we sat down on a bunch of tasks and we did sentiment classes

32:03.200 --> 32:09.360
and QQP and squad and these are very different from each other, classification,

32:09.360 --> 32:16.400
paraphrasing and question answering and we were feeling pretty confident about how useful checklist was

32:16.400 --> 32:24.080
but we are biased because we developed it. So we decided to evaluate it and evaluating a

32:24.080 --> 32:29.200
testing pipeline is a little bit tricky but Mark was spent a lot of time thinking about what

32:29.200 --> 32:34.640
would be the right way to do this and we essentially converged on two separate evaluations,

32:34.640 --> 32:41.360
both of them involved involving users. So the first one was since we are working with these

32:41.360 --> 32:47.520
commercial models already, let's try to go to a commercial team that's actually responsible

32:47.520 --> 32:53.360
for one of these products and not only sort of find out what their testing methodology is

32:54.480 --> 33:01.200
but also propose checklist to them and see what they think of it. So we had I think like a five

33:01.200 --> 33:07.200
hour session with one of the senior developers in that team and we were like here's checklist,

33:07.200 --> 33:15.040
here's how it works, here's how it give them a walkthrough and then say like go crazy,

33:16.240 --> 33:22.560
do what you guys do and see if you find this thing useful. And what was surprising was that

33:23.280 --> 33:28.400
not only did they were they able to quickly confirm some of the bugs that they knew

33:28.400 --> 33:34.480
were in the products which is nice but they were also able to find a bunch of new problems that

33:34.480 --> 33:40.160
they didn't know was already in their model just by the use of checklist right and they were like

33:40.160 --> 33:44.480
oh yeah now I need to get people to fix this stuff because this is a problem right and that's

33:44.480 --> 33:51.600
exactly the use case we expect checklist to be useful for. And so that was one of the things and

33:51.600 --> 33:56.880
we got pretty good feedback and I think Mark was working with a bunch of people to help

33:56.880 --> 34:02.480
integrate checklist into the existing pipelines. The other set of evaluation we did was

34:03.440 --> 34:10.000
we went to sort of university students and people who have at least taken an NLP course so they

34:10.000 --> 34:16.160
know NLP but they probably and they most of them hadn't really worked with the QQP data set,

34:16.160 --> 34:22.080
this question paraphrasing data set. So we wanted to test is checklist useful even if you're not

34:22.080 --> 34:28.640
already a domain expert right and we explained to them what QQP was, we explained to them

34:29.200 --> 34:35.840
what checklist is and it was a slightly cut down version of checklist because we didn't have

34:35.840 --> 34:42.080
five hours to spend with each user and we said okay now can you find bugs in this system

34:43.360 --> 34:50.400
and essentially you know we I can refer to the paper for exact numbers but people who were using

34:50.400 --> 34:56.800
checklist the full capability of checklist were able to find many more bugs or many more different

34:57.360 --> 35:03.280
problems with the model but even for each problem they were able to figure out a lot more test cases

35:04.000 --> 35:10.800
so hundreds or on that order instead of writing just a few sentences so you got more confident

35:10.800 --> 35:15.360
that those problems were actual problems because they were being tested on so many more instances

35:15.360 --> 35:20.640
and so this both these combination of evaluation made us feel like this could be a pretty useful

35:20.640 --> 35:28.720
tool for the community whether it's someone who's an expert and is trying to deploy machine learning

35:28.720 --> 35:34.880
for practical purposes or it's some research project that's trying to be illegal.

35:34.880 --> 35:48.320
Got it. There are groups that have developed for analogous problems in computer vision like

35:48.320 --> 35:54.240
bias testing tool kits I'm thinking of you know things along the lines of gender shades like

35:54.240 --> 36:03.440
trying to determine whether a given model or service can you know do what it does whether it's

36:03.440 --> 36:09.920
you know predicting gender or predicting age or things like that independent of the ethnicity of

36:09.920 --> 36:20.400
the sample data. What you're doing with checklist you know A strikes me as like a meta level above

36:20.400 --> 36:30.320
those kinds of tests but I'm wondering if you are aware of or can envision something similar

36:30.320 --> 36:38.320
applied to computer vision or other domains beyond NLP. Yeah so that's a good connection

36:38.320 --> 36:45.520
and I would say we are quite inspired by a bunch of those papers when we were developing checklist

36:45.520 --> 36:49.920
specifically in NLP also there have been a bunch of tools where they're looking at a specific

36:49.920 --> 36:55.280
phenomenon and trying to look at okay is sentiment analysis system being fair or not and things

36:55.280 --> 37:01.760
like that right so yeah checklist is very much inspired and related to all of that work

37:01.760 --> 37:08.240
and in some sense tries to unify this whole thing like you said in a meta way. We actually did

37:09.040 --> 37:14.880
play around a lot with computer vision and that's not something and I'm an expert in so partly

37:15.520 --> 37:23.360
I would say that's on me but I think there are definitely cases where you can use checklist style

37:23.360 --> 37:28.560
approach to computer vision. We played around a little bit with visual question answering

37:28.560 --> 37:35.200
and we would do things like put up the image and things like that to try and see if the output

37:35.200 --> 37:41.280
remains the same but it's a little bit trickier to do so yes I think there are ways to extend

37:41.280 --> 37:46.400
checklist to apply it to computer vision and videos and things like that but it would require

37:46.400 --> 37:54.480
a little bit more research than we had time for. Do you have a sense for some examples of what

37:56.000 --> 37:58.640
minimum functional tests might be in the vision domain?

38:03.200 --> 38:11.600
Yeah so the ones that I think of are maybe too simple and in some sense might be easy to do so

38:11.600 --> 38:18.160
for example we can imagine creating an image right like taking a something that we know is a dog

38:18.160 --> 38:23.280
and placing it in different parts of the image to see if it gets confused right that's probably

38:23.280 --> 38:30.240
too simple and I'm sure most classifiers would work but then I can take dogs and combine them with

38:30.240 --> 38:36.400
a typical background so I can take of course a scene from the park and you put a dog in it

38:36.400 --> 38:41.920
is probably okay but can I take a scene of the sky and put a dog in it I still want the classifier

38:41.920 --> 38:49.200
to predict dog can I take a slice of pizza and put a dog on it and things like that and so you

38:49.200 --> 38:55.040
can imagine these kind of tests would be easy to create and maybe there's a computer vision

38:55.040 --> 39:00.800
paper that's doing things like that but would give you an idea of okay does the background confuse

39:00.800 --> 39:07.120
the classifier or not right and I imagine that some of these tests might actually be pretty might

39:07.120 --> 39:16.160
show problems in existing computer vision systems awesome awesome any additional thoughts that you

39:16.160 --> 39:20.720
would want to share with folks that are kind of interested in what they're hearing in and you

39:20.720 --> 39:29.600
might want to explore more or might want to you know build on on this work you know what should folks

39:29.600 --> 39:36.000
be thinking about you know as they're thinking about this problem yeah I would say part of the

39:36.800 --> 39:42.560
slightly danger which checklist is a lot of the things we do are in some sense seem very obvious

39:42.560 --> 39:47.040
right like especially once you see the examples you're like okay clearly we should be

39:47.040 --> 39:53.280
testing for these things and and that kind of makes it a little bit dangerous because people might

39:53.280 --> 39:59.280
easily overlook it right and so you know somebody might go into their team and like hey we have a

39:59.280 --> 40:05.840
sentimental analysis team are we doing testing and the answer could come back as yes but testing is

40:05.840 --> 40:11.840
not a yes or no question it's like are you doing a thorough job of testing it and so what we're

40:11.840 --> 40:19.120
really hoping is checklist would inspire people to do a lot more thorough testing and in some sense

40:19.120 --> 40:26.400
when they make accuracy judgment for a model they try to qualify it with some of the sort of things

40:26.400 --> 40:32.160
that checklist produces right and hopefully they go much beyond checklist and do much more finer

40:32.160 --> 40:41.760
grain reporting on what model does just like we wouldn't want the performance of a human employee

40:41.760 --> 40:47.040
to be reduced to a number in some sense but you know a performance review involves a lot of

40:47.040 --> 40:52.320
different dimensions to it as these machine learning models are becoming more involved in our

40:52.320 --> 40:57.360
pipelines I think it's useful to think about them as like what are their capabilities and what

40:57.360 --> 41:02.400
are their weaknesses and be able to say something about them rather than just saying I'm going to use

41:02.400 --> 41:09.440
this model because it's the top of the leaderboard and therefore best. You mentioned that the

41:10.640 --> 41:17.040
some of the examples that are covered by these minimum functional tests are you know obvious at

41:17.040 --> 41:24.720
least in retrospect do you think that part of what the paper offers is kind of concrete language

41:24.720 --> 41:31.040
to refer to these so that teams can say what we fail you know xyz test we fail the negation test

41:31.040 --> 41:40.880
we fail the strong you know the strong final statement test. Is the paper even concrete in defining

41:40.880 --> 41:45.840
these you know an establishing language there do you think that that becomes a way the people

41:45.840 --> 41:51.520
use this that's a good point we didn't think of that use case but yes that that could be a

41:51.520 --> 41:57.280
contribution although I would say that that would be a contribution to maybe people who haven't

41:57.280 --> 42:05.040
seen sort of more linguisticy aspects of NLP right so a lot of our test names are heavily inspired

42:05.040 --> 42:10.960
by stuff that has already been studying in linguistics we're just testing phenomena of language

42:10.960 --> 42:16.640
and we're not the first one to think about what are the different phenomena and so the names we

42:17.760 --> 42:23.680
use in checklist are a lot our common knowledge do a lot of NLP researchers but I can imagine

42:23.680 --> 42:29.200
to someone who's been designing for example sentiment classifier they may not know what the

42:29.920 --> 42:36.080
sort of what does semantic role labeling have to do with sentiment right whereas we have a bunch

42:36.080 --> 42:43.520
of tests that check specifically for semantic role labeling and things like that got it got it

42:43.520 --> 42:49.440
awesome well Samir thanks so much for taking the time to share with us a little bit about what you're

42:49.440 --> 42:56.640
doing awesome work and congrats once again on the best paper award yeah thanks a lot Sam this was

42:56.640 --> 43:10.080
a lot of fun thank you

