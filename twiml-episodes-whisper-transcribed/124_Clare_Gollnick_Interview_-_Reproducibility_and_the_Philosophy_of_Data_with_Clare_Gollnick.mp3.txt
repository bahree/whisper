Hello and welcome to another episode of Twimble Talk, the podcast rye interview interesting
people doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington. In this episode I'm joined by Claire Galnick, CTO of Turbium
Labs, to discuss our thoughts on the reproducibility crisis currently haunting the scientific landscape.
For a little background, a nature survey in 2016 showed that more than 70% of researchers
have tried and failed to reproduce another scientist experiments, and more than half have failed
to reproduce their own experiments. Claire gives us her take on the situation and how it applies
to data science, along with some great nuggets about the philosophy of data and a few interesting
use cases as well. We also cover her thoughts on Bayesian versus frequentist techniques,
and while we're at it, the VIM versus EMAX debate. No, actually I'm just kidding on that last one,
but this was indeed a very fun conversation that I think you'll enjoy.
Okay, you all know I traveled to a ton of events each year, and the event season is just getting
started for me. On Friday, I'm on my way back to the Bay Area for the scaled ML conference and
video GTC. But the event I'm most excited about is my very own AI summit, the successor to the
awesome future of data summit I did last year. This year's event takes place on April 30th and
May 1st, and is once again being held in Las Vegas in conjunction with the Interrupt ITX conference.
This year's event is much more AI focused and is targeting enterprise line of business and
IT managers and leaders who want to get smart on AI very quickly. Think of it as a two-day no-fluff
technical MBA and machine learning and AI. I'll be presenting an ML and AI bootcamp,
and I'll have experts coming in to present many workshops on topics like computer vision,
natural language processing, conversational applications, machine learning and AI for IOT and
industrial applications, data management for AI, building an AI first culture, and operationalizing
machine learning and AI applications and systems. For more information on the program,
visit twimmolai.com slash AI summit. And now on to the show.
All right, everyone. I am on the line with Claire Galnick. Claire is CTO at Turbium Labs.
Claire, welcome to this week in machine learning and AI. Thank you. I'm happy to be here.
So you've got a background in neuroscience and biomedical engineering,
but you've ended up spending a lot of your time working in data science and machine learning.
Can you tell us a little bit about your path?
Definitely. So my background, my graduate school research was actually in information processing
and neural networks, but not artificial neural networks, not neural networks that data scientists
are usually talking about. And so it was definitely an interesting transition, leaving
leaving graduate school, spending all my time trying to figure out how the brain works,
and then coming into data science and hearing people say, you know, like my algorithm works
just like the brain, and I was like, wow, interesting. You should tell the neuroscientists how that works.
But it was, yeah, it was an interesting transition. It's definitely a different type of problem,
and a different way of using data to make the world better. So in science, you're trying to learn
so that you in particular in biomedical engineering to help cure diseases. And now I work in
cybersecurity. And so you're using data and fighting another hard problem. It's just a very different
type of problem. Can you tell us a little bit more about your graduate work? What was your research
there? Yes. So we studied how your brain processes sensory information. So after, you know,
light touches your retina or something touches your skin or your somatosensory system,
how is that information propagated through neural signals, through spike trains,
and how is it that eventually that turns into perception would be the big question.
And interestingly, with the engineering bent, the question was using this knowledge,
could we replace this sensory signal with artificial signals so that we can mimic sensory input
for people who have lost it? So for example, if you've lost a limb and you no longer have your
sense of touch, could we replace that, for example, was a long-term goal?
Interesting. And now when you're looking at this as a scientist, a neuroscientist,
are you exploring it from at the level of chemicals, or are you looking at it, or were you looking at
it more from a system level, electrical signals, or all of the above? Yeah. So we used a couple of
different signals, but broadly we're operating on the like spike train or neuron level.
And so I like to think of it as a, you know, a non-natural language. What is the language of the
brain? And the best current model for understanding how information is transmitted
in the brain is through neuron spiking. And these patterns of spiking and the different
neurons spiking at different times to represent different patterns that could create different
perceptions. Do you find it in working in data science and talking with data scientists that
there are things that, you know, things that you learned about the brain and how those types
of networks work that you feel would be more helpful, if that would be helpful, if they were more
broadly appreciated within data science? That is an interesting question. The thing that I find
most fascinating is how these two fields interact. One of the things that is true is that we don't
know how the brain works. And often we're using mathematical frameworks. And we force that onto
our model of the brain, as opposed, and we see if it's a good model for predicting, for understanding
the brain activity. And so it's a weird interplay where you would, you would in many cases prefer it
to be more from the brain to feeding the statistical modeling and the network modeling.
But often it goes the other way around. A new statistical estimation technique comes, uh, is developed
and then it is applied backwards on into neuroscience. It is an interesting dynamic because you would,
you would imagine or hope in some cases that it went more the other way. Right, right. Especially
going back to your comment, how people talk about artificial neural networks as being brain inspired
when, uh, it often happens the other way around. Yeah, it can. I definitely see it going the other
way around very often. Awesome. And so you've been recently spending some time thinking about,
kind of going back to your roots in science, the reproducibility crisis that has been
much talked about in that field and what data scientists can take away from that. Maybe start by
talking about that reproducibility crisis for folks that aren't familiar with it. Definitely.
So I would say that the reproducibility crisis is a little bit controversial. But the, the main,
the main observation is that a lot of the literature that has been published, a lot of the
experiments that have been summarized and the results that have been summarized in particular in
the fields that I was in. So in biology and neuroscience and in, and in psychology,
is not reproducible. Meaning that if you were to pick a paper out of the literature and try to
perform the experiment as exactly a set out in the methods, you likely would not achieve the same
results as the authors report. And you say it's, do you say it's controversial? Would you say it's
controversial? That it exists or because it exists? I think it is controversial because it implies
that the methods that scientists are using are not actually objective or perfect. And a lot of
scientists work very hard and their work is very close to their identity. And so the implication
that the body of work that a lot of very hard working and well-meeting scientists have put out
into the world is not reliable as a, as a body of knowledge. It's a lot of people really
core to their identity. And I think the, the way that it's controversial, it comes from, it comes
from that sense of ownership over this body of literature. Is it, is it generally accepted though
that it's a problem or is, is that still up for a debate in, in scientific corners?
I would say that the fact that there is a problem is no longer debated. The scale of the problem
is still up for debate. You know, when I've heard about, when the, when the topic of the reproducibility
crisis comes up, a lot of times, you hear people throwing around like p-values and, and the
implications of that, what's that all about? That's, that's very interesting. So I can talk
from my own experience about how I was taught about hypothesis testing and p-values being a part
of that. And I think it's interesting because I see a lot of parallels to the way that data
scientists taught now. When I was in high school, I learned about hypothesis testing and I thought
it was the coolest technology that anyone could possibly imagine because I so desperately wanted
to be a scientist and I knew that scientists needed to be objective. So I needed this objective
way to take an experiment that I ran and then know whether something was true, whether my experiment
was meaningful. And the hypothesis testing was just so compelling as a tool because it'll
basically allow me to just take this data and run it through an algorithm and then have an answer.
And what is, what is interesting about that is that now these hypothesis tests are the center
of the controversy over over the reproducibility crisis and a lot of people are attributing them.
And I agree that it's a major part of the cause of the reproducibility crisis. And one of the most
interesting things about it is that it's actually a problem of scale that p-values and hypothesis
testing applied across many scientists simultaneously or over and over and over again on the same
data set. That's a process we now call key hacking. It's actually re-analyzing the same data over
and over and over again until you find something that appears to be statistically significant.
And that's now considered one of the major problems and major causes of the reproducibility
crisis. And what's interesting is that really machine learning is exactly that. It's re-analyzing
the same data over and over and over again testing hypotheses in a very, very fast faster than
even any human or any scientist normally would be able to and then trying to pick the one that best
explains your data. And so there's a lot of parallels between these hypothesis testing and machine
learning. So with that in mind, how should it impact the way we approach data science?
That is an excellent question. So one of the things that is true about how machine learning is
taught is you're always taught to cross validate, which means that you you hypothesis generate on
a training set of data. You run all those models and then you test that data once. And that it's
interesting is that testing testing this model once on a new set of data is a lot like making a
very specific prediction and then running that experiment, which is what the scientific method is
and it's best formulation, make a very specific prediction that is unlikely to occur in less
your hypothesis is true and then run the experiment and see if that actually happened. And so in a
lot of ways cross validation is taught and is is meant to solve the problem of machine learning
being essentially pehacking. And what's interesting is when you think about it that way, you realize that
a person, a data scientist who is well-meaning and trying to make a model really, really work
could essentially start pehacking their data by repeating this training and testing,
training and testing, training and testing over and over and over again. Sometimes people call this
overfitting on your test set, but it's also just another formulation of pehacking.
Right. Right. Interesting. I guess what's kind of thrown me for about that description is that
it's also, I think, called data science, right? That's what people are fundamentally doing is
trying to manipulate models and model parameters through an iterative loop to try to come up with
something that works. Yeah. So what's really fascinating is so I became very frustrated with this
when I started researching the reproducibility crisis was the thing that I got me interested,
but it led me into this sort of academic deep dive into, wait, why? But why? But why? And I ended up
eventually studying the philosophy of data and why is it that we believe that we can learn from data
at all? And what I learned is that there's a couple important assumptions that you're making implicitly
when you think that you can learn things from data. One of the most important ones is that if you
think about it, 100% of the data that we have ever collected is about the past. Right. We don't have
data about the future. Right. But most of the time, we're trying to learn something about the future.
Right. We're trying to make predictions about the future. You're assuming some kind of predictive
value or dependence. Exactly. You're making the assumption that something about the past and the
future is shared or that there's a shared set of rules in the past and the future. That means that
you can learn something about the past and use it to predict the future. And one of the things
that's interesting is that if you think what is the counterfactual to that? What is the opposite
of having rules? Potentially, things just happen due to chance. Right. Perhaps things that happened
in the past just happened and there wasn't a rule. So when you choose to use data to try to make
predictions about the future, you're implicitly saying, I believe this system that I am studying
is determined by a set of rules. And I'm just trying to figure out what those rules are.
And what's really cool about, or what's really interesting is if you think about the ways and
machines, the ways and the problems in which machine learning has been the most successful,
they're all in the types of problems that are extremely rule based. Or more often,
we're humans determined the rules. For example, the first example is chess. Right. The very first
example of automated intelligence. Yes, is a entirely rule based game that they're playing.
And one of the great parts about it is that about making this attractable problem and making
p-hacking not a problem is that you know the game you're playing, you know the rules. And if you
someone tried to tell you, you know, oh, I'm going to move my pawn and I'm going to move it,
you know, seven spaces diagonally to the right. You'd say, oh, no, that just breaks the rules.
You can't do that. And because it's a rule based intractable like that is actually
the machine learning ends up being an automated intelligence, ends up being a much more powerful
tool in this type of problem than one in which the rules are not known to exist.
So what's an example of a system where the rules aren't known or known to exist and
you know, machine learning doesn't work as well? That's an excellent question. So I have a couple,
but one of my favorites to talk about is actually talking about moving a problem along a spectrum
between being very rule based and not rule based. It's not quite machine learning but it certainly
is predictive and that is predicting elections. Okay. Elections in the US are rule based or
they have been historically, which means, you know, one person, one vote. The definition of state
boundaries are determined by a governing body and the amount of electoral votes or number of
votes that these states get is determined in advance as a rule. And at the end of the day,
what you end up predicting or measuring and predicting is just a few parameters. Basically,
who will vote and who they'll vote for? And that makes it a very tractable problem for
predictive type analytics, whether it's machine learning or, you know, your Bayesian approaches
or however you want to go about it. An interesting thought experiment is to say, well, what happened?
What would happen? How would we predict elections if those rules didn't apply? For example,
if you didn't know how many votes each person got and you had to infer that as well,
or if you didn't know what the state boundaries were and you had to use data to try to guess
based on previous elections that you've seen as outcomes, what the state boundaries were,
how much harder would this problem be? Or worse, if you're not in a functioning democracy where
rules matter and the rules and votes can just be deleted and removed or added or stuffed arbitrarily
randomly, then what use is your predictive technology? What use is data? I'm figuring out what
the next outcome would be. And so what does it tell you to explore, you know, these alternate scenarios
where the rules don't apply? So my favorite use case, like practical use case of this philosophy
and this understanding is to think about what framing the problem means. So you'll often hear
people say, oh, it's all about how you frame the problem. Often if you ask them, well, what does
that mean? How do you frame the problem? It's hard to articulate what that is. For me,
what understanding about this problem of like needing a rule-based system in order for learning
from data to be a practical solution? You have to think about when you're framing the problem,
you're trying to frame it so that the rules apply. So if you have a choice, if you want to choose
potentially multiple different ways of framing your problem or setting up your model or your
features, you're trying to pick features that are representative of true rules about how the domain
actually works. It's basically an argument for domain expertise. It's not at its surface,
it's basically like understand the system that you're using because that's what you need in order
to make good models. I don't think that that's particularly controversial, but it does suggest
that you can't just add a bunch of data into a model and then press go and get the meaning of
the universe out on the other side. Is there an example from your experience at Turbium where
a given use case, maybe there was an initial temptation to approach it without thinking about
the underlying rules and kind of applying this philosophy and thinking about those rules,
it changed your approach to modeling and your outcome? Yeah, that's an excellent question.
So it's Turbium broadly works in the field of cybersecurity, so speaking a little more broadly
than Turbium specifically, this is a problem that people in cybersecurity come up against all the time,
which is that hackers are not rule-based entities. When they try to get into a system, they're trying
to break the rules. So sometimes you have a choice. You have a choice of trying to learn the rules
or you have a choice to try to set the rules in your system. And so for example,
you could choose to try to learn how a network or a given company's network works and then
try to detect anomalies in order to use machine learning to detect anomalies. And that approach
will work, but it's hard. It's hard because the way the network interacts is changing all the time,
the rules are changing, which computer is talking to, which computer are changing. But another way
to approach the same problem is to set really good policies on your network, essentially permissions
policies, and make those rules extremely formalized and designed by humans. Like this computer can
only talk to this one. Like this is the rule. And then when you start observing anomalies, when you
have a network that is really well designed, in which you have all the minimal access to any
particular resource, then data suddenly starts working a lot better at detecting bad actors on your
network for example. So you have a choice of I can just learn the rules as they exist or I can
create the rules and then data suddenly becomes much more powerful as a tool if you create the rules.
Right. It sounds to me like a combination between the previous point you're making about the
importance of domain knowledge, but also just the utility of constraints and making a problem
manageable, which I also don't think would be necessarily controversial as a point. But it's
interesting to kind of layer it on to this way of thinking about data. You mentioned that you
started researching the philosophy of data. Where did you find research about this or who's out
there philosophizing about data? There's a lot. It's actually fairly interesting. I'll tell you
because some of my favorites. So David Hume wrote about a long time ago about the law of induction
and a lot of these a lot of these are philosophers. And so instead of talking in the concept of
data, they're talking about thought experiments. That's their usual unit of work. And he posed a
famous thought experiment that asked how many times do you have to observe the sun rise to be
certain that it will rise again tomorrow? And this is a really compelling interesting question
for me because the question is essentially how much data can I collect before I'm certain?
And if you think about that question, right, it gives you this deep sense of like, wait, what?
Like does this actually work? So that's one that's one writer I really like. Another one is Karl
Popper. Karl Popper is famous for a lot of things, but one of the things that he did was come up with
the concept of falsifiability. So in science and in data driven endeavors in general, you never
prove anything true. You just demonstrate that things your best idea, your best model is not yet
false. And I was taught this in high school. And I thought it had existed since like, you know,
something like the beginning of time, you know, I thought that this was like, you know,
a happen had come up, you have thousands of years ago as like the nature of knowledge, but it turns
out that this concept of falsifiability that's so central is actually from like the 1960s.
Oh wow. It's much newer than you would think. Then you would think, yeah. And that just gave me
this feeling that like so many of the techniques that we've been using and the way we've been
thinking about data and this entire endeavor to learn about our world using like the systematic
collection of data is like very new in the world of like philosophy and how you how you learn from
data. So it was very fascinating. So those are two, those are two of my favorites, but I'm always
looking for more. Yeah, it's interesting. I mean, humor, obviously, I associate with philosophy,
but never would have connected that question to one of data explicitly, although it's obvious
now that you say it. If anyone else who's listening to this knows of some others, definitely send
them my way. It's super interesting. So reproducibility crisis kind of led you to thinking more broadly
about data and some of the, maybe some of the implicit and explicit assumptions we make about data.
One of the thoughts that I had when you were describing this, you know, the inherent
belief in, in, you know, that there are rules that govern our data is kind of brings me back
to the whole Bayesian versus frequentist thing like the implication is almost that, you know,
there's no such thing as a non Bayesian. Well, yes, the way I like to think of it is that Bayesian,
I think Bayesian statistics as an approach is much stronger than frequentist statistics,
but it doesn't actually solve the problem because in order to make a prior, in order to frame it,
you're still making a lot of assumptions about what matters in the problem. So what's fun to do
as a thought experiment? I think one again, Bayesian is just a much more powerful technique,
but it still requires you to make this like an initial guess on how, on how the world works. And
that's again, that rule-based thing like you have to assume that there are rules. So it's,
it's interesting. Yeah. And so did you, you know, to kind of come back to the implication
for data scientists about this reproducibility crisis? Like, do you have, and when you talk about
this, is there, is there like a prescriptive list at the end that you, you know, do A, B, and C,
and you'll be clear? You'll avoid the fate of these poor, you know, these poor, air-room-producible
scientists? Or is it not that clean? Well, so what I, what I say is data is not magic. And you
just have to be okay with that. At a certain extent, you have to think about data and inference as
a sum of the tools in your toolkit that you can use to approach a problem, but not necessarily
always the answer. The second thing that I say is, this is for, for people who are in the position
of making strategic investment in data products. So for example, V, C, these are even managers,
and you're trying to understand which of these solutions that someone has put in front of me
is most likely to actually generalize and actually work when put out into, you know, either as a
company or as a new model for your software or your platform or whatever it is. And at that
moment, when you have to make a choice, a really important question to ask is how did you make this
model? Like what was it that made it work? And a warning flag for me being in this position all
the time, like having to make a decision about where to invest our company's resources is to hear
a data scientist say, I just ran 17,000 different algorithms in different tasks. And I did a huge
grid service across like all of the perimeter space. And I found the one that performed the best.
Because to me, that sounds like p-hacking. And it has been proven out in my experience that
that's a riskier model to choose to, to put into production. Then alternatively, if someone comes
to you and they say, you know, how did you make this model happen? What was the thing that mattered?
And they come and they say, hey, like I thought really hard about the problem. And when I realized,
I realized that if you look at the problem differently, if you look at it this way as opposed to this
way, then everything made sense. Then my model that didn't work suddenly started working. And to me,
that sort of I reframed the problem. And then things I didn't have to work as hard to find the
right answer. I didn't have to p-hack. I didn't have to really push, you know, beat my data in order
to get an answer. That is a sign I think has of a model that's much more likely to work
when pushed into production. Or when you invest in a model.
Is there an implication there that you're not a fan of deep learning? A lot of the way I think
deep learning, the deep learning camp thinks about the world is like throw a huge amount of data
at this neural network and, you know, let the network figure it out. And we shouldn't have to
you know, in kind of the pure sense, like we shouldn't have to bring a lot of our priori knowledge
into the way that we, you know, build these networks or like, you know, process the data or what
have you. That's the role of the data and the network to figure that stuff out.
You have, you have me pegged at what I call a deep learning skeptic.
I would say that much like any tool, there are specific situations in which I think that could
be a really interesting approach, but that it is not a panacea to all problems. And I don't believe,
based on my understanding of philosophy, based on my understanding of the assumptions that go
into the process of learning from data, that there will ever be a modeling architecture or structure
that avoids this process or makes, that makes this this problem go away. Until like what would
it take, it would take a new system of logic, something different than our current definition of
inference. A solution to Hume's law of induction, so to speak, before whatever new algorithm or
new approach comes out, I would, I would think that it was a revolutionary change to how we
solve, solve these problems. I know it strikes me that you're also saying something about
you know, the role of optimization, like you can, you know, your example with like, you know,
grid searching your, your model to death, so to speak, like is this kind of philosophy? Does
it undergarative belief that, you know, that there's some inflection point where you're 90%
model that's based on fundamental principles, you know, is, you know, way better than your 95%
you know, performing model that is kind of brute forced when you, you know, try to actually put
it in a production. Yeah, so I mean, this is sort of, this is where it gets hard, right? So
what I would say is what you learn when you study philosophy of and learning from data is that
the pursuit of pure objectivity is a fool's errand in some way. But so why, why do you do data
science? Well, you do it. Why do you build these models? It's because they're useful. And what I do
think is true is that there's potentially diminishing returns in the last 90 to 95%. But that's not
true in all cases. In some cases, you're, you know, that's, I always say that you don't make
decisions based on probabilities. You make decisions based on expected value. If you have a
use case where failing is so catastrophically terrible that you can't risk that extra 2%,
then it might be worth your time and effort to try for the hopes that it actually does work. And
it is possible, you know, to stumble upon the answer, even if it's not a rigorous process,
it is possible to find it, right? It just means that it's not a robust way of going about it or
a reproducible way of going about it, which is the nature of the game. So it's really about
understanding, you know, what are you doing? Are you trying to sell like 5% more products?
Are you trying to prevent 5% more crime? Like how bad are your potential failures? And then,
and then deciding whether it's worth it to try to risk it to like, to do this brute force approach.
But if you have the option, like you only do that if you don't have the option to solve it by
understanding something fundamental or rule based about the problems, they should all, that should
always be, in my opinion, your first step in building a model. Now we've talked about the
reproducibility crisis in science and applying that, you know, the lessons learned there,
two data science. You also hear from time to time, you know, talk of reproducibility issues
in data science, meaning in the machine learning research, the difficulty going from a research
paper to a working implementation. Any thoughts on that? Yeah. That was a very heavy yeah.
My feeling about that is that it's probably my guess. I've seen the reports of this. I have not
myself tried to reproduce some of these these core central papers, but my, my guess if I were to
make a guess on what the cause of this is is sort of rooted in the same idea where if it's applied
to the right problem, a problem that has good, a good formulation of rules, it probably works the way
the person presented in the original paper. And then if you take the exact same use and you
tweet that problem even slightly, like sometimes that problem can just be like where you got your data
from. Because the rules no longer apply in quite the same way that they did because of how you
collected it or because of your bias or some amount of data that you're missing for whatever reason.
And that might explain some of the lack of reproducibility. In the same way,
you know, that that the reproducibility crisis in science might be explained in some part by people
not reproducing in exactly the same way, not using exactly the same materials, not doing things with
exactly the same technology. That that some of that variance might be explained. And what that's
really saying is that you were relying on rules that weren't actually core and central to your problem,
but they were artifacts. They were, you know, extra extra things like my technology works this way.
And so because so this worked in this case, as opposed to this other case, it didn't generalize
in the same way as I expected it to. Yeah. Yeah. Yeah. It strikes me that there are a number of
things going on there. I think there's the case that you mentioned where I'm trying to apply,
you know, this paper to my use case. And all of the things that you mentioned apply, it also ties
back to, I should say, the kind of issue that the Ali Rahimi brought up at in his nips presentation,
you may have heard of this where he kind of decried the lack of rigor in the field or at least
certain parts of the field, specifically, I think, talking about deep learning. And you know,
that kind of ties to reproducibility and that, you know, people will publish papers, you know, there's
not a set of kind of fundamental rules for how you got your architecture or your hyper parameters.
You just have them. If you don't share them with anyone, then they can't reproduce what you did
because they're like magic numbers that you just found in your grid search. Yeah.
I think if we could learn anything from how poorly the scientist are handling the reproducible
body crisis on a human level, I would suggest that when we try to solve these problems, we do our
best to not throw stones or name call or point out individuals and talk again about the problem
as like, hey, this is how data works. And we have to know this about the tool that we are using.
Going into it so that it doesn't feel like a direct attack on someone who's trying to make
things work with the tools as they understand them. I think a better solution to this problem is
to raise the awareness of the idea that, hey, like, this doesn't actually work. We all wanted to work,
you know, have data, magic formula, and then we know things. But it doesn't, and it's actually
harder than that. And we all have to be okay with that when we when we approach it.
Awesome. Any closing thoughts to kind of tie things up for us?
No. Which is a totally valid answer.
Yeah, I guess I guess I do have one closing thought is that another really great thought
experiment to look into is the infinite monkeys theorem or infinite monkeys thought experiment.
It's a really good demonstration of of what happens when you scale up inference.
And explain that. I love this thought experiment. So it goes it goes basically like this.
You walk into a room and there's a monkey and the monkey's on a typewriter.
And the monkey's begging around on the typewriter and you look at what the monkey is typing
and you see, hey, look, this monkey has typed all of Shakespeare's hamlet.
And you think, oh my gosh, like, this is so crazy.
Something must be up with this monkey. There has to be a rule to explain why this monkey happened.
Clearly, this monkey came from the alien overlords to like down to earth to rewrite Shakespeare's hamlet.
So you're very surprised by this. And then but then someone tells you, hey,
if you had gone over to that room over on your left or the one over on your right,
you would have seen another monkey. And in fact, there's not only one monkey, but there's actually
millions of monkeys or billions of monkeys. And they've been typing on these typewriters
for as long as anyone can remember. And every day we've been looking and we've been looking and
we've been looking, we've just trying to find that monkey that had typed something that we thought
was compelling that looked like there was evidence of a rule. And we finally found it.
And then you think, oh, well, it's not like it's not thinking about this monkey. It's not the fact
that this data happened or this data was collected, but how we came to find it.
And so one of the things I like to always say is when you're trying to evaluate how
strong your evidence is, you should ask how many monkeys you want to know how many times you had
to look to try to find it. Nice. You're going to have to find a way to work that into the title of
this podcast. How many months? Awesome. Well, Claire, you've definitely given us a lot to think about.
Thank you for taking the time to chat with me. Thank you. It's been great.
All right, everyone. That's our show for today. For more information on Claire or any of the
topics covered in this episode, you'll find the show notes online at twomla.com.
If you're new to the pod and like what you hear or you're a veteran listener and haven't
already done so, head on over to your podcast app of choice and leave us your most gracious
rating and review. It helps new listeners find us, which certainly helps us grow.
Thanks so much for listening and catch you next time.
