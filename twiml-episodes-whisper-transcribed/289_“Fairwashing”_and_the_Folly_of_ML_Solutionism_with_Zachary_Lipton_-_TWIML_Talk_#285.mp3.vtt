WEBVTT

00:00.000 --> 00:16.320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

00:16.320 --> 00:21.480
people, doing interesting things in machine learning and artificial intelligence.

00:21.480 --> 00:34.120
I'm your host Sam Charrington.

00:34.120 --> 00:39.920
As we approach Twimblecon AI platforms, I'd like to let you all in on our first major announcement

00:39.920 --> 00:41.760
from the conference.

00:41.760 --> 00:46.600
Now you all love this podcast for great guests and interviews and we're bringing that concept

00:46.600 --> 00:48.800
right to the Twimblecon stage.

00:48.800 --> 00:54.760
I am super excited to announce that Andrew Eng will be joining me on stage at Twimblecon

00:54.760 --> 00:57.720
for a live keynote interview.

00:57.720 --> 01:02.320
Many of you know Andrew from his work at Stanford, Coursera or his many other efforts in the

01:02.320 --> 01:06.640
industry including recently founding deeplearning.ai.

01:06.640 --> 01:11.520
Andrew and his work have been super impactful on my life and career and I know that's the

01:11.520 --> 01:13.920
case for many of you as well.

01:13.920 --> 01:18.880
In our conversation we'll be discussing the state of AI in the enterprise, the barriers

01:18.880 --> 01:24.000
to using deep learning and production and how to overcome them, his views on tooling and

01:24.000 --> 01:30.640
platforms for efficient AI delivery and other topics from his recently published AI Transformation

01:30.640 --> 01:32.440
Playbook.

01:32.440 --> 01:36.920
Be on the lookout for more great speaker announcements rolling out over the course of the next few

01:36.920 --> 01:37.920
weeks.

01:37.920 --> 01:39.920
You don't want to miss this event.

01:39.920 --> 01:47.440
Get your tickets now at twimblecon.com slash register.

01:47.440 --> 01:49.520
Alright everyone, I am on the line with Zach Lipton.

01:49.520 --> 01:54.680
Zach is an assistant professor in a temperate school of business and an affiliate faculty

01:54.680 --> 01:59.320
in the machine learning department and high school of public policy at CMU.

01:59.320 --> 02:01.760
Zach, welcome to this week a machine learning in AI.

02:01.760 --> 02:03.480
Thanks for having me.

02:03.480 --> 02:06.680
Let's get started by having you share a little bit about your background.

02:06.680 --> 02:15.400
How did you end up in this intersection of business, machine learning, public policy?

02:15.400 --> 02:16.800
Lots of different areas.

02:16.800 --> 02:19.640
That's a good question.

02:19.640 --> 02:26.480
I'm not sure completely, but I can't really be completely honest.

02:26.480 --> 02:28.240
I can't claim it was planned or anything.

02:28.240 --> 02:33.240
I think if anything maybe it came from sort of just doing what I want to do at every

02:33.240 --> 02:41.960
given point and not following a very specific path that was laid out and as a result wound

02:41.960 --> 02:46.280
up maybe some plays different than the standard thing.

02:46.280 --> 02:54.040
You recently transitioned into your professorship at CMU from your PhD.

02:54.040 --> 02:55.360
What was that in?

02:55.360 --> 02:57.320
So I did my PhD in computer science.

02:57.320 --> 03:06.720
I kind of had a circuitous path to PhD and out of PhD, but I did my undergraduate in

03:06.720 --> 03:08.800
math and economics at Columbia.

03:08.800 --> 03:14.960
That was a long time ago and I was a musician sort of before, during and after and that was

03:14.960 --> 03:15.960
my main thing.

03:15.960 --> 03:22.040
I was playing this saxophone and I wanted to be a hustling jazz musician which is a hard

03:22.040 --> 03:30.480
life even if you're the very best and luckiest, but certainly if you're not the luckiest.

03:30.480 --> 03:35.080
So I was doing that for a long time after I think I was happiest playing music when I was

03:35.080 --> 03:43.680
an undergrad and there was this kind of amazing balance about being at Columbia and having

03:43.680 --> 03:48.880
the academic side of my life be a little bit more on the technical side which felt more

03:48.880 --> 03:52.400
natural for me at least within academic environment.

03:52.400 --> 03:58.520
I enjoyed jazz music as sort of a folk music and there's something very organic about learning

03:58.520 --> 04:02.520
that music as an oral tradition and learning it by spending your nights out until I am

04:02.520 --> 04:08.160
playing it with people who speak that music or who play that music kind of natively.

04:08.160 --> 04:13.080
And there was something that I never quite loved about taking classes in jazz music

04:13.080 --> 04:17.040
like in a university or conservatory setting and it always felt a little bit artificial.

04:17.040 --> 04:22.480
I think maybe other other art forms might be more amenable to that.

04:22.480 --> 04:28.280
And so I guess after spending a certain amount of time playing music being outside I had

04:28.280 --> 04:33.000
some like personal setbacks so I was like hemorrhaging money and living in the lower

04:33.000 --> 04:37.680
east side and the only way I was able to live in the lower east side is I was in a rent

04:37.680 --> 04:43.840
control department and usually that means that unfortunately in New York was kind of neglected

04:43.840 --> 04:48.720
by the landlords and so it was like a moldy apartment as people vomiting on your sidewalk

04:48.720 --> 04:52.840
and all that and then I went out and visited a friend who was actually a musician who

04:52.840 --> 04:58.040
was doing a PhD in music composition so he was a jazz musician but he had he was an amazing

04:58.040 --> 05:05.160
pianist and has you know the kind of rounded chops that he could also make it in a composition

05:05.160 --> 05:06.160
program.

05:06.160 --> 05:10.320
He came out and started a graduate program at UC Santa Cruz so I came out and visited

05:10.320 --> 05:17.400
him and I didn't want to go to grad school for music but that experience of being out there

05:17.400 --> 05:25.000
and like after just kind of being in this sort of falling apart type state in New York

05:25.000 --> 05:32.480
city in this rundown apartment and feeling unhealthy and everything then I was out in Santa

05:32.480 --> 05:37.280
Cruz and Santa Cruz like the sun is shining it's the most beautiful place in the world.

05:37.280 --> 05:45.160
The 90 year olds, the 90 year olds look 30 years old and then they you know that has

05:45.160 --> 05:49.720
group of composers would get together every week and they had these listening sessions

05:49.720 --> 05:54.080
and it was almost like a reading group you know like they would like it was very different

05:54.080 --> 05:58.680
than maybe like my experience in music which is a being a social music was hard to have

05:58.680 --> 06:02.840
like a kind of critical intellectual discourse about it on the level because you sort of

06:02.840 --> 06:07.280
were it was very personal to people and you were trying to get gigs working with people

06:07.280 --> 06:13.840
and I thought there was this missing part of my life that you know it's like very academic

06:13.840 --> 06:19.360
slash New York thing and me that just wants to like have very like candid arguments with

06:19.360 --> 06:24.760
people about things and I was I was in Santa Cruz and it was just beautiful and I was amazing

06:24.760 --> 06:29.160
and these composers would get together once a week and someone would curate and they play

06:29.160 --> 06:32.560
a bunch of music and then people would just scream at each other about it and like really you

06:32.560 --> 06:37.880
know not not in an ad home in a way but just really you know express really really strong

06:37.880 --> 06:42.880
kind of critical opinions about it and there was something about this this environment that

06:42.880 --> 06:47.640
made me think you know like that oh that's it like it wasn't that I want to go to

06:47.640 --> 06:50.280
grad school for music but it was like oh I'm going to move to California I'm going to do

06:50.280 --> 06:55.000
a PhD I'm going to get into that kind of environment where I feel like I have that kind of intellectual

06:55.000 --> 07:02.480
life and so I went back home and I like got my landlord degree to let me break my lease

07:02.480 --> 07:10.160
early I asked my parents if I could steal our old like 2004 Toyota Corolla I took the sign

07:10.160 --> 07:14.560
up take the GREs and then I was like I'm going to go do a PhD and I haven't even decided

07:14.560 --> 07:19.080
like what the PhD was going to be in so then like the next step was like well what am I going

07:19.080 --> 07:24.120
to do and actually came together as whole like really just kind of like ridiculous you

07:24.120 --> 07:28.920
know like unqualified plan to do PhD came together extremely fast like two to three weeks

07:28.920 --> 07:34.840
it was just kind of all set up it just kind of I don't know why I didn't even know what

07:34.840 --> 07:39.560
machine learning really was but I knew I wanted to do PhD and I went I had a good friend who

07:39.560 --> 07:44.920
was sort of had been a you know a bit of a mentor for me who was a biophysics professor

07:44.920 --> 07:49.400
and I used to I built him I taught myself to program a little bit and I built him a website

07:49.400 --> 07:53.960
and so we would hang out and play chess sometimes and drink coffee and he's to invite me to

07:53.960 --> 07:57.400
the reading group so I had a little bit of a sense of what academic life was like even though

07:57.400 --> 08:02.600
I I don't really know biology or physics and he was just always for some reason he took

08:02.600 --> 08:06.440
an interest in me and we were we were close and so I talked to him I was like you know maybe

08:06.440 --> 08:12.200
I should do PhD in biology and he's like you know that's not fun you know it's like that's

08:12.200 --> 08:17.160
a top heavy basically it was like that's you know it's a great job and it's an amazing field

08:17.160 --> 08:22.840
if you're on top but he's like you know it's going to take you six years to figure out how to

08:22.840 --> 08:27.480
be a useful lab tech before you can even start doing anything creative and by that time you're

08:27.480 --> 08:31.400
going to be you know you know on your way to 40 years old like that's that's not the route

08:32.520 --> 08:37.080
and I thought about some more it's like what do I taught you know I've taken one only one or two

08:37.080 --> 08:42.520
undergraduate computer science classes but somehow that felt like the right thing like there was

08:42.520 --> 08:46.600
something just like really I think I think a lot of people had the first time you learn computer

08:46.600 --> 08:51.480
science you learn about algorithms and you suddenly start learning how to think about how to

08:51.480 --> 08:55.160
start formalizing things you see in the world where how would you model that computationally

08:55.160 --> 08:59.240
and then you start thinking about things like the structure in real world problems that

08:59.240 --> 09:02.760
makes them amenable to a efficient algorithm and something about that like kind of clicked with

09:02.760 --> 09:07.240
me when I was young so even if I hadn't sort of followed up on it just having these like two or

09:07.240 --> 09:11.240
three undergraduate computer science classes made this impression on me and I knew how to program

09:11.240 --> 09:17.720
just enough to cause trouble and so thought about it well you know that's that's the thing I

09:17.720 --> 09:22.840
could maybe sort of run with you know but that was you know it was a very thin thin basis to hang

09:22.840 --> 09:28.760
my hat on but fortunately fortunately when I applied to PhD there are some people who were

09:28.760 --> 09:34.760
willing to take a chance on me and and one of them was UC San Diego which is a absolutely fantastic

09:34.760 --> 09:39.960
school that you know everyone in the world should apply to if you're interested in not if you're

09:39.960 --> 09:46.680
interested in computer science or surfing or beautiful weather just building a new life

09:46.680 --> 09:51.640
sounds like you pattern match Santa Cruz pretty well except for maybe I don't know eight degrees

09:51.640 --> 09:57.640
warmer on average or something yeah it's a bit warmer it's you know they both I think have the year

09:57.640 --> 10:03.080
round moderate weather thing right kind of happened to both have the surfing I think Santa Cruz is

10:03.080 --> 10:10.200
a bit happier you know San Diego has multiple sides like you've got the La Jolla kind of stuffy

10:10.200 --> 10:17.000
Mitch Romney kind of side and then you've got the I got a lot Navy presence there so that that's a

10:17.000 --> 10:25.080
big player in San Diego but yeah you've got that that beautiful year round weather and that

10:25.080 --> 10:30.600
attracts something cool great food also and so did you just jump right in take some courses

10:30.600 --> 10:36.680
find your way to machine learning or what was that initial connection to the the world of ML

10:36.680 --> 10:43.560
well ML was just the thing from the start like that was my I didn't know any ML like literally

10:43.560 --> 10:48.680
if you told me to like write out you know explain to you on the whiteboard like a classic algorithm

10:48.680 --> 10:54.280
like logistic regression or something I would not have known I would not have been able to do it

10:54.280 --> 10:59.400
or you know explain gradient descent like I hadn't I hadn't ever implemented a machine learning

10:59.400 --> 11:07.160
algorithm but what I knew was that I spent one year in San Francisco 2012 so like when I made

11:07.160 --> 11:12.040
this plan you know that there's one problem with the whole grad school pipeline which is that

11:12.040 --> 11:17.400
it's a great pipeline or you know maybe maybe people have some fault with it or something but

11:17.400 --> 11:21.960
overall like it works well if you're already in the system so like if you're an undergrad and

11:21.960 --> 11:25.880
then you want you you know you want to go to masters you're like well you know I'm a junior now

11:25.880 --> 11:30.440
I got to start thinking about that and and and you do it and you're not like off the path

11:30.440 --> 11:36.280
while you're making that plan right but the the trick is if you're doing something completely

11:36.280 --> 11:43.160
different you know if you're like yo I'm I'm playing a saxophone at three in the morning for 40 bucks

11:43.160 --> 11:49.720
in some weird dive that's gonna close down it a couple months and I want to go do a PhD

11:49.720 --> 11:55.800
then it's like you're you're out of the system like what are you gonna so so so this was like

11:55.800 --> 12:03.400
spring two thousand like end of spring 2012 is what you need that's that's like your last moment

12:03.400 --> 12:09.400
to make a move like a serious like hard hard turn even if you play everything right to like

12:09.400 --> 12:14.760
get in the PhD for fall 2013 right so you have to kind of not just like have this thing come

12:14.760 --> 12:21.240
together but you then have to somehow stick with it despite not being like in that rhythm for

12:21.240 --> 12:25.880
some period of time so so so my move is basically I knew that if I stayed in New York I would just

12:25.880 --> 12:31.320
keep doing what I was doing so my move is to like break my lease sign up for the GREs get that part

12:31.320 --> 12:35.640
out of the way because that was the part that I knew about and then move to California and you

12:35.640 --> 12:42.120
know I was like well you know I'll I'll basically go to San Francisco maybe find a way to be uh

12:42.120 --> 12:49.000
be a lemming for a startup or something until um you know while while I'm putting that together

12:49.000 --> 12:52.680
at least I wouldn't be like in New York still hanging out till five in the morning like trying

12:52.680 --> 12:58.840
to hustle for gigs because I'd be I'd be out of you know and it just also felt natural like change

12:58.840 --> 13:03.320
change your life change your location like to wake up every morning in a new place so I moved to

13:03.320 --> 13:06.680
San Francisco actually I did everything where you know it was a really wild time like I moved I

13:06.680 --> 13:11.320
lived in one of these like totally could not possibly have been legal they call like hacker hostels

13:11.320 --> 13:16.440
or something oh yeah like I saw some I saw I have this like problem which is maybe part of why

13:16.440 --> 13:22.200
I have wound up in all kinds of weird situations but if you can cast like a situation as a choice

13:22.200 --> 13:26.920
to have an adventure or not have an adventure and that's like a valid lens on the situation

13:26.920 --> 13:32.440
then I'll choose the adventure and so and so I read and you know I was thinking like San Francisco

13:32.440 --> 13:36.920
like you know whatever Silicon Valley whatever whatever you know it didn't it was still a little

13:36.920 --> 13:41.720
more romantic at the time I think in 2012 it wasn't quite as like evil empires it is now but it

13:41.720 --> 13:46.680
was already pretty expensive and there was these articles in New York times about these weird

13:46.680 --> 13:50.440
hacker hostels we were like people would wrench them through Airbnb for like a month at a time

13:50.440 --> 13:55.480
and they were like packed in and so I went out and I lived like literally in a bunk bed with

13:56.680 --> 14:03.160
six people for a month until trying to make it big and Silicon Valley and yeah just trying

14:03.160 --> 14:07.560
uh yeah I don't know what it was just just found find out what it was I didn't know anyone out there

14:07.560 --> 14:13.480
so I really saw me some it was all like Germans met like a bunch of Germans who were hanging out

14:13.480 --> 14:20.200
for a minute yeah they love their hostels yeah so I moved to California bite the California coast

14:20.200 --> 14:25.080
and then set up in San Francisco and lived in this weird Airbnb for a minute and then I moved out

14:25.080 --> 14:29.880
to Oakland and that was great I actually ended up playing a lot of jazz again when I was in Oakland

14:29.880 --> 14:36.280
and getting to know that community and worked with the startup and then you know spent a significant

14:36.280 --> 14:40.600
portion of my time then I'm applying a PhD and I got lucky that someone took a chance on me

14:41.320 --> 14:49.960
nice nice and so you applied to ML programs you know not to be all CMU showblinist or something

14:49.960 --> 14:58.440
but I think CMU's unusual in having an ML program and was in the past was exceedingly unusual

14:58.440 --> 15:03.880
and having like a like an ML department like so I applied I applied to computer science and you

15:03.880 --> 15:08.200
know you check off maybe some interest areas or something but it's not like a a separate program

15:08.200 --> 15:12.600
you applied to computer science I think now things have gotten weird enough that you look at a

15:12.600 --> 15:18.040
a typical school has a has a typical university as a school of engineering within a department

15:18.040 --> 15:23.160
of computer science and within that some subgroup of people some working group that works on

15:23.160 --> 15:27.960
machine learning not as like a formal distinction although maybe there's some kind of committees or

15:27.960 --> 15:34.280
they band together for making hiring decisions or something but you know that's how CMU is very

15:34.280 --> 15:40.200
unusual a CMU has a school of computer science and within it a department of machine learning

15:40.200 --> 15:44.200
a department of robotics the department of a natural language you know it's called language

15:44.200 --> 15:49.480
technologies institute department of human computer action and stuff like that so you know at

15:49.480 --> 15:53.640
the time of applying most of the place you apply you just apply to CS and maybe lists or

15:53.640 --> 15:58.200
interest I think now things are getting weird because I think more places are copying the CMU model

15:58.200 --> 16:04.360
as places try to keep up with the band especially for like courses and in AI machine learning

16:05.800 --> 16:10.520
I think the other thing that's happening is just a lot of schools get I've read from colleagues

16:10.520 --> 16:16.360
who are professors elsewhere that you know at a lot of CS departments you'll get maybe CS will

16:16.360 --> 16:23.400
be like six faculty out of 50 or 40 or something but maybe 50 60 percent of the applications

16:23.400 --> 16:28.280
for grad school for PhD are people saying they want to do machine learning right so that creates

16:28.280 --> 16:33.080
a whole other dynamic where maybe they end up treating it even if it's not formally a separate

16:33.080 --> 16:36.520
application they have to throttle it a little bit because they're thinking well who are these

16:36.520 --> 16:40.200
people going to work with right right right and the alternatives you accept them all you know

16:40.200 --> 16:44.280
and then you know the half of them get the advice that they want and the other half end up doing

16:44.280 --> 16:51.160
compilers or something let's maybe talk a little bit about your your broad research interest nowadays

16:51.160 --> 16:57.640
what are you focusing on well you know I think the lens I mean okay there's a few a few kind of

16:57.640 --> 17:03.400
lenses that that I have in research right there's you know a lot of people are more applied a lot of

17:03.400 --> 17:07.560
people are more like theoretical or core algorithms and I kind of straddle that line a little bit

17:08.360 --> 17:15.560
and on the applied side my my biggest interest has been since before I started PhD and has

17:15.560 --> 17:21.240
continued to be throughout and as a young faculty member has been working a machine learning for

17:21.240 --> 17:29.080
healthcare so so that's kind of you know if I just step back and think about not you know papers

17:29.720 --> 17:34.280
in terms of like their aesthetic beauty or something but in terms of like

17:35.880 --> 17:40.040
if I could you know build something big like what would the grand vision be it would be I

17:40.040 --> 17:47.080
would like to have a positive impact in healthcare and I think there are opportunities to do it um

17:47.080 --> 17:53.240
but at the same time working on problems in healthcare I think it's also a great application

17:53.240 --> 17:59.400
not just because uh it's you know uh much better for yourself than working on advertisements

18:00.200 --> 18:06.280
but also because I think it just sort of puts you in touch with what's wrong right because you

18:06.280 --> 18:11.080
you basically you just can't afford like it's too important and the stakes are too high

18:11.720 --> 18:15.560
that if you're going to if you're gonna go out there and say this is how we should do decision-making

18:15.560 --> 18:20.440
or something or you know we could you people there's these sensational headlines your next doctor

18:20.440 --> 18:26.200
might be an AI and it's such absolute crap right but the reason why is because it really you know

18:26.200 --> 18:30.520
if you really think deeply about these things that puts you in touch with like the discrepancy

18:30.520 --> 18:36.920
between the tools that we uh have mastered and that we're building and in the actual real world

18:36.920 --> 18:44.040
problems we're claiming to to to make some impact on right so the the one hammer we have that

18:44.040 --> 18:49.000
everybody uh you know it is throwing all over the place wherever you can stick it in it's called

18:49.000 --> 18:53.000
supervised learning I'm sure you've talked about you know machine learning I guess you know

18:53.000 --> 18:57.880
you've run 9000 podcasts already and everyone who was talking about an actual working system

18:57.880 --> 19:01.880
probably was talking about supervised learning right they're they're limited exceptions or maybe

19:01.880 --> 19:07.720
someone just and with banded algorithms on advertisements but for the most part you know supervised

19:07.720 --> 19:13.000
learning is basically based on this idea that you're gonna get data that comes in right and it's

19:13.000 --> 19:18.600
gonna consist of inputs and corresponding outputs and you're gonna um try to predict the outputs

19:18.600 --> 19:25.320
based on the inputs and uh fortunately you know what makes it supervised is that you for the purposes

19:25.320 --> 19:30.280
of training are gonna have as large data set for which the outputs are known so it's like someone's

19:31.000 --> 19:37.000
standing over your shoulder and telling you what the right answer is and now the big big assumption

19:37.000 --> 19:43.720
is that the the data that you're then gonna see in uh in the real world you your training data

19:43.720 --> 19:49.480
was representative of it right so like basically the the historical data and the future data are

19:50.120 --> 19:54.440
assumed to be what called iid which means they're like independently sampled from the same exact

19:54.440 --> 20:00.600
distribution yeah and and that's just a oh oh wild assumption right when you then think like okay

20:00.600 --> 20:06.680
wait a minute so so what can we do it's a we can infer a likely output given an input

20:08.280 --> 20:15.880
um assuming that the future is in every like statistical way sort of you know the the the

20:15.880 --> 20:21.640
historical data is perfectly representative of the future in every you know important statistical

20:21.640 --> 20:25.960
way and that's just something that completely breaks down when you look at a lot of real world

20:25.960 --> 20:33.080
problems right so uh one thing that happens is that just well the the historical data is not

20:33.080 --> 20:38.680
representative so so this is a question that you know i think uh formally we talk about it in terms

20:38.680 --> 20:44.200
of uh we're called distribution shift and distribution shift could be kind of benign or not benign

20:44.200 --> 20:49.800
but um could it be kind of organic in a sense that it could be that hey you know one's day is different

20:49.800 --> 20:54.680
from Tuesday because it's because it's different from Tuesday right so uh if you're classifying

20:54.680 --> 20:59.720
news articles are somewhat people are you know different different stories are trending uh more

20:59.720 --> 21:03.960
you know if you're classifying by topic or something there's more sport stories you know today

21:04.920 --> 21:10.600
because Wimbledon is happening or something or you know the women's world cup and uh maybe there

21:10.600 --> 21:15.960
will be less uh one week from now who knows so that that's one way that things change but then

21:15.960 --> 21:23.240
there's more in serious ways that things change which is um the other key thing is that we often

21:23.240 --> 21:27.400
you know the the machine learning problem the formal statements all about making predictions

21:28.040 --> 21:33.560
but we're often not really concerned with making predictions we're concerned with um taking actions

21:33.560 --> 21:36.440
right it's all about driving decisions if you're accompanying your thumb out automation and machine

21:36.440 --> 21:41.640
learning is coming up as this multi billion dollar concern largely because of the hope that

21:41.640 --> 21:46.200
you know what makes technology that valuable it's it's something that you can do at scales not

21:46.200 --> 21:51.800
because it's just uh people are doing offline data analysis or you know trying to understand

21:51.800 --> 21:56.360
their customers qualitatively because they're trying to drive decisions and once you start making

21:56.360 --> 22:00.520
decisions now suddenly you're impacting the world and very often that very same environment that

22:00.520 --> 22:05.080
generates your future data and we just don't have great tools for understanding these kind of

22:05.080 --> 22:10.600
feedback loops right once we once you take the data extract information from it and then use it

22:10.600 --> 22:15.000
to change the way that you make decisions in a way that you know influences the world everything

22:15.000 --> 22:21.720
kind of falls apart and so I've done a lot of work recently um trying to look at well under what

22:21.720 --> 22:27.560
assumptions can you make models um one that are sort of guaranteed to be robust against certain

22:27.560 --> 22:35.000
kinds of distribution shift to short of that you know at least under what conditions uh what tools

22:35.000 --> 22:40.200
can you use to try to detect as efficiently as possible as quickly as possible when somehow your

22:40.200 --> 22:46.360
environment has changed um and beyond that to try to sort of gain some qualitative insight into

22:46.360 --> 22:51.560
is that is that shift pathological or not is this something that you expect to to break your model

22:51.560 --> 22:57.800
or or destroy the validity of your predictions anything about a medical setting right like you're

22:57.800 --> 23:01.960
trying to you're trying to you know you want to have the the doctor AI your next doctor is going

23:01.960 --> 23:06.040
to be an AI it's like presumably they have to be able to make treatment decisions not just uh

23:06.040 --> 23:11.480
uh you know uh predict what would have happened if a different doctor you know if if the doctor

23:11.480 --> 23:16.760
who would have treated you anyway uh had done their thing so that's that's a bit of a nuance

23:16.760 --> 23:22.200
how does that correspond to the distribution shift and the feedback loops that you were talking about

23:22.200 --> 23:28.200
because I think the fundamental premise of as to your point most everything we're doing here which

23:28.200 --> 23:34.040
is supervised learning is you know we're going to collect this data that represents the sage wisdom

23:34.040 --> 23:41.320
of all the best doctors and train our models on it and so then if you know our models making the

23:41.320 --> 23:46.920
the decisions you know that are close to what are you know the doctor that would have otherwise

23:46.920 --> 23:52.120
done them and does a good job at making those decisions and everything is good and rosy right

23:52.120 --> 23:58.920
the problem is that well there's a number of big problems but one is you know what one is that

23:58.920 --> 24:04.760
the act you know the world is changing naturally right and the actual doctor is has some understanding

24:04.760 --> 24:09.160
of of the biology of the disease and and is somewhat adaptable in this way right like when we

24:09.160 --> 24:13.560
look at the ways that machine learning models break because you you move the few pixels and an image

24:14.600 --> 24:19.800
that kind of stuff doesn't fool the humans so the humans are pretty robust and I think actually

24:19.800 --> 24:24.040
this is a and this is something that um my friend Jacob Steinhart and I talked about

24:24.040 --> 24:30.200
in in a paper about some kind of misleading trends or some problematic trends in scholarship

24:30.680 --> 24:35.480
is that there's this tendency in papers to to sort of make a kind of hyperbolic claim that is

24:35.480 --> 24:40.440
insubstantiated by by by the research and what one of the classic ways this happens is people

24:40.440 --> 24:45.160
talk about human level of performance right so the human level of performance it's actually not

24:45.160 --> 24:50.040
it's not quite the right compare if you're going to talk about sort of human capacity the human

24:50.040 --> 24:58.360
capacity isn't just for doing doing well in this very very constrained sort of like artificial

24:58.360 --> 25:05.320
environment that only exists when you truly have a randomized trained test split the human

25:05.320 --> 25:10.680
dermatologist is going to continue to be a good dermatologist even if the the light contrast

25:10.680 --> 25:15.240
slightly changes on the images that they're looking at right or even if the skin tone of the

25:15.240 --> 25:20.360
patients is different than it was in the training set problems where the the machine learning

25:20.360 --> 25:25.960
potentially is going to fall apart in a catastrophic way the other thing is right you know

25:27.000 --> 25:30.840
so there's there's an issue even with matching performance because like matching performance

25:30.840 --> 25:35.080
you know when we talk about performance which I'm about accuracy offense like well accuracy is

25:35.080 --> 25:40.360
itself a statistic right accuracy is only true it's only valid assuming a certain distribution

25:40.360 --> 25:45.400
of data and if that's something that could change like if you know the the the patients something

25:45.400 --> 25:49.000
something different happens you know the the patients start coming in with a different distribution

25:49.000 --> 25:54.680
of illnesses some disease starts becoming you know that there's an epidemic it's not quite you

25:54.680 --> 26:00.840
know pit not every patient you know you have to make adjustments to what what what illnesses or

26:00.840 --> 26:05.640
patients likely to have given their symptoms things like that the other side is that ultimately

26:05.640 --> 26:11.160
what you'd like to do is you'd like to not the dream of machine learning isn't in health care

26:11.160 --> 26:19.880
isn't that we're going to somehow replace a bunch of doctors and in the process keep health care

26:19.880 --> 26:24.520
like slightly worse but almost as good right like that's what you're talking about when you're like

26:24.520 --> 26:28.840
if you can predict what when you talk about this imitation type learning thing so so a bigger dream

26:28.840 --> 26:32.760
would be that you would actually be able to assist a decision-making process and so you'd be able

26:32.760 --> 26:37.400
to like like ultimately the thing that gets most people excited isn't just say hey we're going to

26:37.400 --> 26:43.480
automate doctors which you know already maybe misses the point about just how much of the work

26:43.480 --> 26:49.480
is involved is is not you know taking the the data that's already there and like trying to

26:49.480 --> 26:53.640
predict the doctor's decision but it was actually meeting with the patient and determining which

26:53.640 --> 26:57.720
tests to run in the first place you know which resulted in the data which actually was already

26:57.720 --> 27:02.600
all the work that the machine learning's not doing for you so you know that's another way that it's

27:02.600 --> 27:06.440
misleading you know if we we start the machine learning and we think we're doing what the doctor

27:06.440 --> 27:10.280
does but it's like well what about those test results where they can come from well because someone

27:10.280 --> 27:15.320
who ordered those tests so what you know so then what oh we also have to predict which test to

27:15.320 --> 27:18.920
order and then given the test we have to predict which disease given the disease we have to predict

27:18.920 --> 27:23.640
you know also which treatment they're going to recommend and so you know you start if you actually

27:23.640 --> 27:27.880
do a kind of fair analysis that puts together all the compounded errors that pop up and then

27:27.880 --> 27:32.200
account for a world that's constantly changing things get messy but then even on top of that

27:32.200 --> 27:36.920
it's like our goal isn't to just freeze medicine at this point in time get rid of all the doctors

27:36.920 --> 27:41.240
and then just like put into place machine learning algorithm that is based on what medicine looks

27:41.240 --> 27:47.880
like in 2018 what we'd like to do is be able to understand disease processes better and be able

27:47.880 --> 27:53.560
to understand be able to analyze data using say for example models that don't just make predictions

27:53.560 --> 27:58.840
but estimate treatment effects so that's something we call causal inference and ultimately right you

27:58.840 --> 28:03.400
like to be able to say hey if I intervene and do something different than what the doctor normally

28:03.400 --> 28:08.760
would do what do I expect the outcome would be or which patients should I assign which drugs can I

28:08.760 --> 28:12.920
can I you know can we make a dent in personalized medicine to do that we're not just trying to make

28:12.920 --> 28:16.680
predictions about what people would have otherwise done we're trying to figure out what are better

28:16.680 --> 28:21.080
things that we could do and that requires causal inference and causal inference actually now

28:21.080 --> 28:26.840
gives you gives you a whole other lens on the ways that humans and computers potentially need to

28:26.840 --> 28:31.080
combine what they're good at because causal inference is not something that you in general can just

28:31.080 --> 28:38.120
do given offline data and no prior information causal inference actually requires a certain

28:38.120 --> 28:43.400
inductive assumption certain assumptions about the causal graph or the mechanism that relates to data

28:43.400 --> 28:50.040
or which things listen to which things you know for example like the you know smoking causes cancer

28:50.040 --> 28:55.880
potentially but cancer probably doesn't cause smoking i'd be like a classic toy you know textbook

28:55.880 --> 29:02.440
example but building in certain certain kind of advanced knowledge together with offline data

29:02.440 --> 29:07.320
you know then we're able to to potentially identify a causal effect even without you know running

29:07.320 --> 29:13.720
experiments in the wild but we have to you know I think ultimately you know working on a task

29:13.720 --> 29:18.440
like medicine really exposes and thinking about what it would take to actually do something that

29:18.440 --> 29:22.200
you could actually run the wild exposes you you know makes you think about those things in a way that

29:22.200 --> 29:27.880
I think you know maybe you should think about it if you're doing recommender systems but the truth

29:27.880 --> 29:32.520
is that even if you don't think about it there's going to be a large company that's willing to

29:32.520 --> 29:37.560
throw it out in the world and see if they make more money or lose money and if they make money is

29:37.560 --> 29:42.360
kind of going to roll with it even if it's kind of doing the wrong thing right so we do that a lot

29:42.360 --> 29:46.040
with recommender systems where it's like what is the real task we're trying to solve I don't know

29:46.040 --> 29:52.920
curate interesting content but what do we actually do it's like we predict clicks because that's

29:52.920 --> 29:57.240
a data we capture so and that's another way we that the contract breaks right I've talked about

29:57.240 --> 30:02.520
the way contract breaks because the distribution changes because we actually interfere in the world

30:02.520 --> 30:07.800
and that kind of messes with all the future data we see is now from a different world the world

30:07.800 --> 30:13.240
in which you know customers are interacting with this this system that that changes everything

30:13.240 --> 30:17.320
but another way of the contract breaks is we're just predicting the wrong thing in the first place

30:17.320 --> 30:20.840
because the thing we really care about is something when we don't we don't capture a structure of

30:20.840 --> 30:26.120
data right if you think about that like with a lot of these issues potentially that that

30:26.120 --> 30:31.320
make people worried and concerned about problems uh regarding say for example racial or gender bias

30:31.320 --> 30:35.480
in in automated systems another area that's been a lot of time thinking about and working on

30:36.440 --> 30:41.000
you know one of the the clear failure mechanisms is the data that you capture that your model is

30:41.000 --> 30:45.800
the thing that's convenient not the not the the true the true data it's not the only way that things

30:45.800 --> 30:49.880
can fail but that's one of them and so you can imagine that you know you predict who's going

30:49.880 --> 30:55.880
who to hire based on who the people hired in the past but you know that that's what you measure

30:55.880 --> 31:00.520
is is who got hired in the past or who how are they rated by the interviewers in the past

31:00.520 --> 31:05.080
well you don't capture necessarily is the thing you actually care about which is you know how

31:05.080 --> 31:09.080
strong a candidate are they which is a more abstract concept so we end up relying on

31:09.080 --> 31:13.480
instinct that we have data for which is not the same thing and you know what what is what is

31:13.480 --> 31:18.520
that consequence of sort of optimizing the wrong thing and you know in the case of like YouTube

31:18.520 --> 31:23.800
recently they had a scandal where the consequence was sort of curating pedophilia or curating

31:23.800 --> 31:30.120
naked baby videos for people with this you know so yeah that's uh you know I think that that's a

31:30.120 --> 31:35.080
especially dark or scary uh consequence but you know this is something we we have to think about

31:35.080 --> 31:42.600
is is when the contract breaks um you know and I think by you know that sort of forces us to sort of

31:42.600 --> 31:48.200
go beyond just the narrow confines of doing and evaluating supervised learning models where

31:48.200 --> 31:54.040
mostly the technical content consists of people um you know just sort of the get squeezing out

31:54.040 --> 31:59.560
incremental predictive performance improvements assuming the task is the right task and sort of

31:59.560 --> 32:04.680
forces us to step back and think about either a more challenging or fundamental task like

32:04.680 --> 32:09.400
estimating causal effects or thinking about the consequences of some of these systems like

32:09.400 --> 32:13.160
you know using tools to say economic modeling and and that's one thing that's cool about sitting

32:13.160 --> 32:17.480
in the business school and having a social scientist and economists as colleagues.

32:18.360 --> 32:24.280
Well it talks me about some of the economic modeling uh tools and applications of those tools

32:24.280 --> 32:29.160
in the space. What are some examples of how that plays out in uh healthcare and other areas?

32:29.160 --> 32:36.280
I think the area where I personally am encountering economics the most like I came I got hired by

32:36.280 --> 32:44.040
you know kind of strangely early in PhD I got approached by the the Tepper school um was made

32:44.040 --> 32:49.560
looking to make um you know kind of uh move move in data science direct I think I was you know

32:49.560 --> 32:56.760
I was a little bit of an experiment um and uh for for a little while I was a bit separated I think

32:56.760 --> 33:02.360
um I wasn't like reading many economics papers or something I was wrapping up you know my kind of

33:02.360 --> 33:09.000
core like I see a milner of type work and and actually what two things that me recently have put

33:09.000 --> 33:14.120
me in touch with them is one you know uh starting to think a lot more about causality you take a

33:14.120 --> 33:21.000
step back and say well who's who's doing empirical causal effect modeling in in the world broadly

33:21.000 --> 33:26.920
and it's largely social scientists and you know like applied applied uh uh economists and

33:26.920 --> 33:31.640
econometrations right like this is our bread and butter is um there was some shock to this is

33:31.640 --> 33:37.480
dealing the is it true then increasing the minimum wage uh um decreases employment or is that

33:37.480 --> 33:43.320
not true right and then you have these various ways of trying to draw these inferences from from

33:43.880 --> 33:49.720
um you know shocks of the system in the natural world or you know various you know

33:49.720 --> 33:53.720
they're you know they're the you know the handful of tools that that they're kind of tried

33:53.720 --> 33:59.560
and true like instrumental variable analysis regression discontinuity um difference and

33:59.560 --> 34:02.920
difference some of these tools you know rely on some very strong assumptions and I think

34:02.920 --> 34:07.640
can sometimes drop you know if those assumptions are violated can lead to some wrong conclusions

34:07.640 --> 34:12.520
but that that's one way that I've kind of started crossing over and I think you know a lot of those

34:12.520 --> 34:17.080
tools are important because if you look at the work modeling you know when you look at the effect of

34:17.080 --> 34:22.440
this technology in the real world and you you want to start saying well what is the impact of um

34:23.960 --> 34:28.280
you know especially when you have these fuzzy systems right like uh risk scoring systems that

34:28.280 --> 34:33.320
have been driving policing decisions that are then influencing these outcomes and it's very hard

34:33.320 --> 34:37.640
you know from the machine learning standpoint if you there's big pile of papers that are just

34:37.640 --> 34:41.400
considering the classification aspect right I've got these people they belong to this group

34:41.400 --> 34:46.920
if you live belong to that group these are the these are the their inputs these are the um ground truth

34:46.920 --> 34:52.040
outputs these are the predictions let me let me do some uh kind of arithmetic on them but what

34:52.040 --> 34:56.840
you're not seeing is that like oh this system is generating a prediction this prediction is some kind

34:56.840 --> 35:01.800
of score the score is an input to a human who's making a decision and if you actually care about

35:01.800 --> 35:06.840
the downstream problem which is how is how is the introduction of this kind of system that is

35:06.840 --> 35:11.880
mining this information to make predictions driving decisions and influencing outcomes then you

35:11.880 --> 35:16.440
start crossing over into a land where actually the people who have the experience doing this are

35:16.440 --> 35:22.120
the social scientists of actually um looking at real world data and trying to figure out um you know

35:22.120 --> 35:26.920
what what what are what are what are these various effects um I think the other side actually

35:26.920 --> 35:32.120
if I could jump in there that's a really interesting point because I think we often hear from

35:32.120 --> 35:41.800
um the kind of the vendor community that's providing tools that are enabling uh these examples

35:41.800 --> 35:48.200
these use cases that you're describing that hey you know we're providing these tools to uh to feed

35:48.200 --> 35:54.040
into a human decision process as if you know that that somehow you know means that there's not

35:54.040 --> 35:58.680
going to be anything wrong with the system like the overall system and also that we don't need to

35:58.680 --> 36:04.760
further like study and understand that overall system with you know the impact of these new

36:04.760 --> 36:10.920
tools in it and I think you're pointing to that well actually studying you know the impact of things

36:10.920 --> 36:16.120
like this is you know something that we've been doing it for a while but just in other domains

36:16.920 --> 36:22.040
and these are techniques that we can apply to these systems yeah absolutely and I think

36:22.040 --> 36:29.480
I mean there's there's a lot to say about that right all in hand I feel like the easiest thing

36:29.480 --> 36:35.080
to say which um maybe is not it's like intellectually profound but like I think as like a public service

36:35.080 --> 36:40.520
announcement needs to be out there is like first of all like no um I think it's almost a bit

36:40.520 --> 36:44.760
or well it's like first of all no it's not okay just because there's an algorithm involved

36:44.760 --> 36:49.000
but at the same time it's also not just okay just it's not okay just because there's a human

36:49.000 --> 36:55.720
involved right right um and there's there's a lot of issues to sort through but both in terms

36:55.720 --> 37:00.120
of how we analyze these systems also in terms of you know I think one of the fundamental

37:00.120 --> 37:05.960
difficulties in the area is also figuring out you know what is what even what is our goal

37:05.960 --> 37:11.400
or what is what is the right thing to do here it's not always obvious um I think there are some cases

37:11.400 --> 37:18.120
that to us are like very obvious that something is wrong you know but it's not always obvious what

37:18.120 --> 37:21.720
is the right basis for making certain kinds of decisions especially when there's certain kinds

37:21.720 --> 37:28.280
of like intrinsic uh trade-offs okay so so so there's question about what's the right thing to do

37:28.280 --> 37:35.080
the other thing the other problem is it's also okay so the big meta point there I think also

37:35.080 --> 37:40.120
is that hey these aren't new problems that emerge just because we stuck an algorithm in there

37:40.840 --> 37:45.080
um but they sort of get seen through a new light and get a new kind of attention I think a lot of

37:45.080 --> 37:50.440
us in the machine learning community get stuck in this loop of sort of trying to re like you know

37:50.440 --> 37:55.800
you see a lot of people sort of talking about like AI ethics which is great it's great that people

37:55.800 --> 38:03.320
are trying to think about um what is right um and uh think about these sort of like social impacts

38:03.320 --> 38:08.600
of applying these automated systems but at the same time a lot of people are doing it because there's

38:08.600 --> 38:12.520
a little bit of a bandwagoning effect going on right where a lot of people are jumping into it

38:12.520 --> 38:18.120
kind of completely oblivious of the fact that people have been mulling over like what what are people

38:18.120 --> 38:24.520
you know what what are people's rights and what um what what is like an ethical way to to engage

38:24.520 --> 38:29.640
in a lot of these systems even before machine learning was introduced um and I was actually I

38:29.640 --> 38:38.200
the benefit of uh in early June Cynthia Dwork um who's uh an absolutely inspirational researcher

38:38.200 --> 38:43.000
um she's the inventor of differential privacy in one of the pioneers in the more algorithmic fairness

38:43.000 --> 38:51.400
world and Patricia Williams who's a professor at Columbia Law um and uh they they put together this

38:51.400 --> 38:58.040
fantastic um workshop at the Simon's Institute for theoretical computer science and so they've

38:58.040 --> 39:08.040
been branching out into some more interdisciplinary type workshops and this one was uh about sort of

39:08.040 --> 39:16.840
uh the you know race and data and and and and injustice and brought together people from I liked

39:16.840 --> 39:21.480
it because you know you normally have a spectrum that brings together people who are working in um

39:22.680 --> 39:29.160
computer science ML and then like computer science ML adjacent like you know the the I feel like

39:29.160 --> 39:33.960
there's a sweet spot that like the the spectrum for interdisciplinary interdisciplinary runs from

39:33.960 --> 39:41.160
um like uh information school type technical social scientist to theoretical computer scientist

39:41.160 --> 39:45.320
and that's the spectrum which is great that's still very broad and it's a wonderful set of people

39:45.320 --> 39:50.760
that I consider to be like a large part of my you know my home like the people who you will

39:50.760 --> 39:56.040
meet if you go to like a fat star right the fairness accountability transparency conference um

39:57.400 --> 40:01.720
but the cool thing about this workshop is a brought together people like uh Ruha Benjamin who

40:01.720 --> 40:09.240
is in uh Princeton and writes um you know teaches in like African-American studies and has studied

40:09.240 --> 40:15.480
very clearly um issues about race and technology in ways um that transcend not just machine learning

40:15.480 --> 40:23.320
or this moment in time but uh more broadly um kind of history of um ways of technology has been used

40:23.320 --> 40:29.160
as a as a tool that maybe exacerbates inequality you had a lot of people from public health and

40:29.160 --> 40:34.920
epidemiology who were there you had a lot of people who had um been involved on the ethical aspects

40:34.920 --> 40:40.600
from like the medical ethics community and uh when when genetics was like the big hot technology

40:40.600 --> 40:47.880
and there were all kinds of ways that genetics was being used in ways that um had kind of um maybe like

40:48.520 --> 40:55.960
ways that were expressed kind of dubiously in terms of um the you know ethical consequences

40:55.960 --> 41:02.040
like various things various sort of studies and population genetics that were sort of sort of

41:02.040 --> 41:10.600
sort of like neo eugenicist type uh work and um you know there there there's been a long history

41:10.600 --> 41:16.440
here that is outside machine learning I think is you know where we came from this and um being

41:16.440 --> 41:21.240
in touch with that and and you know sometimes it's actually a surprisingly you know when you come

41:21.240 --> 41:25.880
at it from the perspective of like us you know people a lot of people who are who are first thinking

41:25.880 --> 41:32.440
about these things and haven't like learned to um are only first thinking about it and in a relatively

41:32.440 --> 41:42.200
immature way and go to an area um people who've been looking at say criminal justice and um thinking

41:42.200 --> 41:48.840
about it and in a very mature way over a very long period of time and have a really deep understanding

41:48.840 --> 41:55.480
of the the kind of systemic problems in a way that goes beyond just um kind of some tried formalism

41:55.480 --> 42:01.480
that maybe fails to capture the the kind of like institutional level issues then um I think that

42:01.480 --> 42:06.280
you know there's a lot to learn there and I think you know this is also you know one of the big

42:06.280 --> 42:11.960
things that came out of it for me from that workshop that was I think um a really kind of profound

42:11.960 --> 42:18.280
insight was that we have a tendency in the technical community to try to reduce things to some kind

42:18.280 --> 42:24.840
of abstract problem but there's a big danger that we can actually I think a lot of us even a lot

42:24.840 --> 42:31.560
of us sort of purporting to work on problems of algorithmic fairness but through a technical lens

42:32.360 --> 42:36.280
by not understanding the broader context and not understanding the kind of systemic problems like

42:36.280 --> 42:40.120
it's not just the matter of making the false positive race equal between two groups or something

42:40.120 --> 42:45.560
like this but actually understanding um you know stepping back with like what does the data even

42:45.560 --> 42:52.360
mean right or where was this data collected or what you know um truly asking um the kind of

42:52.360 --> 42:59.160
foundational questions about the broader system in which the kind of this formalism is embedded

42:59.160 --> 43:04.840
we run the risk of of actually doing uh what some people are calling fairwashing

43:04.840 --> 43:10.600
which is that we could like take these fundamentally flawed systems where maybe the the precise way

43:10.600 --> 43:14.760
that machine learning is being used there there's something fundamentally wrong with it

43:14.760 --> 43:19.480
and we could go in and say oh you know I'm I'm into machine learning I'm into social good

43:19.480 --> 43:24.600
let me let me do some work at that area and come up with a little tweak on the you know there's

43:24.600 --> 43:31.640
a danger of coming up with just a small tweak on some existing algorithm right that sort of

43:31.640 --> 43:38.200
preserves precisely qualitatively what we're already doing but gives somehow the impression

43:38.200 --> 43:44.440
that you've made it like fair right I mean that's kind of reflective of a broader argument I

43:44.440 --> 43:51.800
guess that's happening in the ML research community at large that the extent to which you know

43:51.800 --> 43:58.280
the you know these kind of revolutionary advances versus you know incremental you know with

43:58.280 --> 44:04.440
regard to publishing papers like you know that we're you know a lot of the papers that we're seeing

44:04.440 --> 44:10.040
are kind of incremental application of some new training technique or something like that and

44:10.040 --> 44:17.800
there is some commentary that you know we're seeing kind of this huge exponential growth in

44:17.800 --> 44:23.160
the number of papers that are published but some are arguing that the field isn't advancing

44:23.160 --> 44:30.520
you know accordingly and so this is this kind of fairwashing argument you know in one hand

44:31.240 --> 44:37.480
you know the fairwashing element of it is different but it's also somewhat reflective of kind

44:37.480 --> 44:41.240
of the broader dynamic that's happening in the research community would you agree with that?

44:42.200 --> 44:46.600
Yeah I mean I think that the broader thing that it's kind of endemic is

44:50.040 --> 44:56.440
I think there's a lot of trends in the community largely due to the success of the field

44:56.440 --> 45:02.920
and the fact that it's grown in such a really I don't know if it's unprecedented in the

45:02.920 --> 45:08.840
scoop of all fields but it's unprecedented maybe in the scope of of of AI machine intelligence

45:09.560 --> 45:13.960
that has sort of resulted in this weird situation where the the review system is buckling a

45:13.960 --> 45:19.800
little bit the review system the pool of reviewers has had to grow in a way that maybe changes

45:19.800 --> 45:27.080
the standards of who's a qualified reviewer a lot of people are in a very volume driven I think

45:27.080 --> 45:32.280
what one thing that happens is as a community grows so fast not that many people are getting

45:32.280 --> 45:39.240
mentorship from you know true masters in the field but are really taking their cues from

45:39.240 --> 45:46.040
machine learning subreddit or something and you know are looking for looking for a splashy or

45:46.040 --> 45:52.840
just you know trying to get papers in or get citations so there's a lot of ways that the community

45:52.840 --> 45:59.480
may be struggling a little bit now in terms of quality control and I think that maybe is a natural

45:59.480 --> 46:04.440
thing that'll self correct is you know I think even this ICML was actually a step in the right

46:04.440 --> 46:11.240
direction then you know the other side is like the specific trend that you're talking about here

46:11.240 --> 46:19.880
which is maybe this particular thing that we talk about about maybe being a little bit abusive

46:19.880 --> 46:26.920
with language misusing language you know making one of the things about technical papers even

46:26.920 --> 46:31.240
when technical review works it has certain blind spots right like you you you submit a

46:31.240 --> 46:36.520
a nirips paper you submit an ICML paper you have an expectation there are certain things other

46:36.520 --> 46:44.120
reviewers are going to if the system works be very critical about and they are going to spot and

46:44.120 --> 46:52.200
among them you know you expect that they're going to spot if your your your notation makes no sense

46:52.200 --> 46:57.560
they're going to spot if your theorem is wrong hopefully they're going to spot if your experiments

46:57.560 --> 47:05.560
don't sort of support the kind of performance claim that you are making in a in a like absolute

47:05.560 --> 47:11.800
sense right but what they tend to miss and I think there's a blind spot in the review is the

47:11.800 --> 47:19.080
reviews are not generally critical about the this sort of you know they're not able to assess

47:19.080 --> 47:23.720
the critical arguments that are made and this is an area where you know the typical ml paper

47:23.720 --> 47:29.480
is a bit sloppy so typical ml paper will you know there's a caricature that you can make which is

47:29.480 --> 47:34.280
that your introduction is just kind of you know something is a problem and other people have worked

47:34.280 --> 47:40.360
on it or it's you know used to say some kind of generic mumbo jumbo there's more and more data

47:40.360 --> 47:44.600
and bigger and bigger computers and something something something and then you say something is

47:44.600 --> 47:49.720
a problem and right now people have to do the work and we can automate it with machine learning

47:49.720 --> 47:54.680
or something but it's basically in too many papers the introduction is not like an opportunity

47:55.320 --> 48:01.000
for for like some profound philosophical argument that that justifies the why you're solving

48:01.000 --> 48:06.280
a problem you are in the first place it's sort of a throw away piece that just is some kind of

48:06.280 --> 48:11.320
stage setting so you can move on to the meat of the paper which is here the equations and here

48:11.320 --> 48:17.240
are the quantitative results and that leaves you know is that the right bar for a paper a profound

48:17.240 --> 48:23.800
philosophical argument is that the case you know in other fields not for every paper but um you

48:23.800 --> 48:28.440
know well in other fields it is the contribution right now in other fields like the the argument is

48:28.440 --> 48:33.640
the substance of the paper I think a philosophy paper um depending on which area of you know say

48:33.640 --> 48:38.520
economics or something that that could be the essence of the paper can be the critical argument

48:38.520 --> 48:42.920
that you're making which is very different where where we sort of get to that I have a very

48:42.920 --> 48:47.320
well-formed machine learning problem and either I have an algorithm for it or I have a a smashing

48:47.320 --> 48:53.640
empirical result for it that is the that in the sweet spot I think for like a a NURPS or ICML

48:53.640 --> 48:58.280
paper like the sweet spot from a perspective of like most likely to get in is something like that

48:58.280 --> 49:02.200
I've got a very well-formed problem everyone's already decided it's important I don't have to argue

49:02.200 --> 49:09.240
for its validity and then you have the real sweet spot would be you had a just enough of a algorithmic

49:09.240 --> 49:13.800
contribution that there's a non-trivial theorem in there and then you also had some amount of

49:13.800 --> 49:17.400
experiments you know because there's one of these things like whoever it's whoever doesn't like

49:17.400 --> 49:22.360
your paper the most it probably is going to get it you know uh you know like there's some argument

49:22.360 --> 49:26.040
I don't know you know I don't know if this is truly true but you know there's maybe an

49:26.040 --> 49:29.800
ask a negative truth in it that like a paper doesn't get accepted it gets not rejected or

49:29.800 --> 49:35.800
something like that and so this way it's you know the person who would say ah you know if there's

49:35.800 --> 49:40.200
no real experiments I don't know it really works that that person is satisfied and the person who will

49:41.400 --> 49:47.320
their lazy way of accepting the paper as ads trivial they they get the they get the math that

49:47.320 --> 49:52.280
they're looking for than everyone is happy but but you know I think I think the language is a bit

49:52.280 --> 49:59.640
of a blind spot like for example it's extremely difficult to publish a position paper no matter

49:59.640 --> 50:05.320
how well argued it I think if you if you submitted a an eight-page essay that like really

50:06.120 --> 50:11.560
meticulously picks apart say the foundations of some kind of problem or some kind of application

50:11.560 --> 50:15.240
of machine learning in a way that it's extremely knowledgeable or well researched or whatever

50:15.240 --> 50:19.080
I think you'd have virtually no chance of getting it accepted at ICML or NURPS

50:20.440 --> 50:24.920
I think the NLP community is a bit better like some of their conferences will explicitly

50:24.920 --> 50:30.440
it if not you know say that's the number one priority and I don't think they should it's

50:30.440 --> 50:35.480
technical conference well these include position papers as like within the mandate of the conference

50:36.600 --> 50:39.960
where you do have a little bit more license for something like that is a lot of workshops

50:39.960 --> 50:45.160
are familiar to that kind of material but you know I think the danger is what you end up getting

50:45.160 --> 50:50.280
is what you get is a technical paper that maybe maybe it makes technical sense I mean even even

50:50.280 --> 50:55.560
there I think we have some weird issues but that's a whole other you know topic but you know even

50:55.560 --> 50:59.240
say the technical content of the paper makes sense what you have is a problem which is

50:59.240 --> 51:04.760
ostensibly valuable in part because of the virtue signaling that it is addressing some important

51:04.760 --> 51:13.800
real world problem right so so you start from that point but then the claim maybe you know the

51:13.800 --> 51:19.800
paper says like we make our algorithm fair by doing whatever you know this is the fair the

51:19.800 --> 51:26.280
fair DQN the fair GAN the fair whatever it is so the the claim like from the titles in the very

51:26.280 --> 51:30.840
title of the paper is the claim is somehow we have satisfied whatever it is this thing the

51:30.840 --> 51:35.000
thing called fairness fair and check you know we've made the ethical the ethical

51:36.280 --> 51:41.160
reinforcement learning agent or something like this so like the claim is sort of at the outset

51:41.160 --> 51:45.560
is not it's not like a firm technical mathematical claim is sort of like we solved ethics or something

51:45.560 --> 51:50.760
like this and then the introduction very often you know for for and I'm not saying every paper is

51:50.760 --> 51:55.640
like this but I think there's a pattern where the motivation might might either be you know on the

51:55.640 --> 51:59.800
worst side it could be you know there's plenty of words it's just like babbling but on the other

51:59.800 --> 52:07.320
side you could have work that is even intelligent but it's just that the the applicability to to

52:07.320 --> 52:12.680
that real world problem that you're claiming that you uh solved is just missing or it's not there

52:12.680 --> 52:18.840
or you know it's they're serious caveats and need to be stated um and then you know you have this

52:18.840 --> 52:23.240
these weird things that you like you have to argue that your your quantitative result is correct

52:23.240 --> 52:28.440
you have to argue you have to be meticulous about laying out like how you did your splits between

52:28.440 --> 52:36.520
train validation test data um how many runs did you do um you know make sure that your uh work is

52:36.520 --> 52:41.160
is reproducible by by communicating certain technical details you have to put the proofs for

52:41.160 --> 52:45.640
the theorems you can't just make a outlandish claim that some mathematical factors true you have

52:45.640 --> 52:50.120
to you have to prove it right but then there's things that you don't have to prove and I think these

52:50.120 --> 52:54.840
are subtle ways that that things go wrong like for example you don't have to argue for what you call

52:54.840 --> 52:59.960
something and some reviewers might get predicted they might be particular about language but I think

52:59.960 --> 53:07.000
a lot of people are willing to let that slide um and and this becomes I think especially a problem

53:07.000 --> 53:12.440
in this new dynamic where um the research isn't just among the research community right it's not

53:12.440 --> 53:19.240
just technical research is being read by PhD students there's this weird loop between research um

53:19.240 --> 53:26.840
industry uh governance and uh a huge part of that is the popular press um and so you wind up

53:26.840 --> 53:31.880
and assist more like people uh are maybe motivated I think I think you know people I don't think

53:31.880 --> 53:37.880
are malicious I think you wind up with someone maybe motivated by a genuine desire to uh do something

53:37.880 --> 53:45.400
that they feel has more um social you know sort of some kind of positive social impact or something

53:45.400 --> 53:51.560
like that but then they start working on and they think well um you know how am I get you know

53:51.560 --> 53:54.840
what would be a good title for this paper it's gonna get people to read it or something you know

53:54.840 --> 54:00.200
and and they're not necessarily thinking like hey I'm making a claim that this is this thing that

54:00.200 --> 54:04.760
solves this there's this real world problem and I'm making a claim that this algorithm that I'm

54:04.760 --> 54:12.440
putting which solves some very uh sort of like toy very reductive version of that problem um that

54:12.920 --> 54:18.200
uh you know that has some applicability to that that real world situation which it might not right

54:18.200 --> 54:24.200
and and then you know people you know like you know you put out this thing like um you know I'm gonna

54:24.200 --> 54:28.360
you know my thing will tell you if your algorithm is fair or whatever without even being clear about

54:28.360 --> 54:33.000
how we even address like the right ingredients or even speaking in the right language to be able

54:33.000 --> 54:38.680
to capture something like that and we even access the right data or any any of that and then it gets

54:38.680 --> 54:43.080
picked up in a story and say you know oh uh you know like for example like IBM has been a bit of a

54:44.760 --> 54:51.080
a bit of a like cavalier actor in this way right both with this both with medical and with fairness

54:51.080 --> 54:58.440
related things right so so IBM um did this thing where you know with Watson forever where they kind

54:58.440 --> 55:04.120
of made people think they were solving fundamental medical problems when they really didn't have

55:04.120 --> 55:07.640
they didn't have the goods to back it up and then now it's like oh we're getting ahead of the

55:07.640 --> 55:11.240
fairness thing which is you know it's great that on one hand you know the running is ad campaign

55:11.240 --> 55:14.920
they're raising awareness that's one thing but on the other hand they put out uh like an open

55:14.920 --> 55:19.240
source repo and they made it sound like hey you know here's a set of algorithms that'll make your

55:19.240 --> 55:24.360
stuff fair um and the big danger right is that someone would use that and think that like okay that

55:25.000 --> 55:29.800
that that'll clear us and and work it's even crazier and this may be ties into a paper that I

55:29.800 --> 55:34.600
wrote uh recently with Alex Cholde-Trova at the Heinz School of Public Policy and Julian McCauley

55:34.600 --> 55:41.880
from UCSD is that you know sometimes the algorithm actually could be super horrible from from any

55:41.880 --> 55:46.280
I think like reasonable ethical standpoint because people have miscast the problem right

55:46.280 --> 55:51.080
those simplifying decisions that you made that sort of seemed intuitive from setting up a toy

55:51.080 --> 55:55.800
problem when you weren't thinking about any real world data might actually result in something that's

55:55.800 --> 55:59.800
absolutely horrible um that you would never want to use right there's some examples of that

56:00.680 --> 56:05.880
so here's a problem in the in the United States our sort of perspective on like discrimination

56:05.880 --> 56:11.880
in a lot of context is is informed by um title seven of the Civil Rights Act of 1964

56:11.880 --> 56:16.040
now that gives I think our dominance in the research community means that maybe like the

56:16.040 --> 56:21.720
overall like fairness work is a little bit skewed by technical interpretations of United States law

56:21.720 --> 56:28.200
versus other countries which maybe have their own legal precedent um but that that you know that's

56:28.200 --> 56:35.320
that's where I think a lot of like thinking um comes from and and in that context you know there

56:35.320 --> 56:41.560
there's these basically the Civil Rights Act has these two um legal doctrines that have emerged

56:41.560 --> 56:46.040
from it and one is this notion of disparate treatment and the other is this notion of disparate

56:46.040 --> 56:53.080
impact and you know um these are motivated by discrimination and in housing and employment

56:53.080 --> 57:00.120
and a number of cases and you know disparate treatment is basically about intentional discrimination

57:00.920 --> 57:09.400
so it includes it subsumes discrimination based on like explicit consideration of something that

57:09.400 --> 57:15.720
is deemed uh what they call a protected um characteristic or like membership in a protected class

57:15.720 --> 57:21.640
right so this could be like um the what is someone's race what is someone's gender and in fact in

57:21.640 --> 57:27.240
the original Civil Rights Act of 1964 there's like some some some some listing of set of protected

57:27.240 --> 57:32.600
classes probably in 2019 there's some that we would want to include in things that we would think

57:32.600 --> 57:39.960
of as kind of like salient uh like wedges of discrimination in society that we would want to be

57:39.960 --> 57:45.000
especially productive of that were not included then like a sexual orientation or something but um

57:45.000 --> 57:49.080
I guess like 1964 America wasn't there yet so it's not part of the Civil Rights Act

57:50.200 --> 57:55.800
so disparate treatment is about intentional discrimination and um that includes but is not

57:55.800 --> 58:01.320
limited to like direct use of that characteristic so you know it's important to keep in in mind how

58:01.320 --> 58:06.120
the law works versus how math works with math we want to say I want to come up with a a crystal

58:06.120 --> 58:11.560
clear simple set of principles that's going to govern any kind of situation that I could possibly

58:11.560 --> 58:15.800
encounter right just it describes the world or you know physics or something like that we would

58:15.800 --> 58:24.360
kind of operate in this sort of way the law is more like a patchwork that is is plugging in laws

58:24.360 --> 58:28.520
in different places to try to deal with different things that are actually happening in the world

58:28.520 --> 58:32.920
right so they're there there's a set of drugs that are illegal there's a set of drugs that are not

58:32.920 --> 58:37.720
illegal but you know maybe you're qualitatively similar the reason why they're not illegal is

58:37.720 --> 58:42.600
because uh they haven't but they haven't been abused yet you know once they are then maybe

58:42.600 --> 58:46.840
they would become illegal right we would make a law we say oh there's a problem let's make a law

58:46.840 --> 58:51.480
and puts us in a weird situation for governing technology because technology might bring up new

58:51.480 --> 58:55.960
problems but our law is about dealing with uh certain when we think of discrimination when you're

58:55.960 --> 58:59.400
creating the law it's a career thinking of you're thinking of the racist person who has a sign

58:59.400 --> 59:05.320
out as we don't hire whoever that's one case right so let's so this retreatment I think and I

59:05.320 --> 59:11.080
and I want to be clear that I'm not a I'm not a legal scholar I'm just a uh a fake legal scholar or

59:11.080 --> 59:16.520
something um I'm a machine learning president who tries to be uh play one on TV I tries to be bilingual

59:17.160 --> 59:23.400
maybe more than is standard but um I don't you know I'm not like uh you know I'm sure that there's

59:23.400 --> 59:26.760
a number of people from from the other side that know this better but fortunately by by having

59:26.760 --> 59:31.880
dialogue with them you know we we've had a more refined we've been able to refine our you know

59:31.880 --> 59:36.440
um analysis on it right so that's this one thing this retreatment and the other side is disparate

59:36.440 --> 59:41.240
impact and disparate impact basically is trying to cover those situations whereby you can have

59:41.240 --> 59:47.160
what the law calls a facially neutral policy so facially neutral means like you know like on this

59:47.160 --> 59:55.080
it appears not to in any way like explicitly be um taking it you know using this kind of

59:55.080 --> 01:00:00.440
information and yet at the same time it can have an unjustified um disparate impact you know

01:00:00.440 --> 01:00:06.280
an unjustified disparity and so it's really important to think about like what's going on here

01:00:06.280 --> 01:00:09.880
and that on one hand we have disparate treatment disparate treatment is a doctrine that is

01:00:09.880 --> 01:00:16.040
concerned with intent right and then on the other side we have disparate uh impact uh a

01:00:16.040 --> 01:00:20.840
different legal doctrine which is concerned with um whether or not something is justified

01:00:21.560 --> 01:00:27.080
and then we're trying to uh people basically look to the law because they say hey well I want to

01:00:27.080 --> 01:00:32.760
make algorithms that you know don't do bad things so so let me look to the law because that that's

01:00:32.760 --> 01:00:37.960
like my existing body of work that I could draw from but the law talks about intention the law

01:00:37.960 --> 01:00:44.600
talks about justification and um you know I don't know what justification is in in the language of

01:00:44.600 --> 01:00:48.920
supervised learning but I'm pretty sure that basically it's not expressable in the like like

01:00:48.920 --> 01:00:52.680
basically we've got an insufficient language the the the way that we're talking about machine

01:00:52.680 --> 01:00:57.160
learning problems is too reductive and it doesn't doesn't possess the vocabulary to express

01:00:57.160 --> 01:01:00.680
this concept it's kind of like I know you've seen Judah Pearl if you read the book of lies kind of

01:01:00.680 --> 01:01:05.880
like that the way he's he's you know he's he's he's ringing that bell talking about um you know

01:01:05.880 --> 01:01:09.960
the all the long line of statisticians that didn't realize that causality was something that

01:01:09.960 --> 01:01:15.640
lied outside classical statistics and like no matter how hard you try there's not like just

01:01:15.640 --> 01:01:21.320
an expression in purely probability terms that you know tells you what causality is you have to

01:01:21.320 --> 01:01:26.440
introduce like external notation um I think it's a little bit like that you know we have the law is

01:01:26.440 --> 01:01:33.240
is coming from from this richer family of notions and and we don't know what it means to justify

01:01:33.240 --> 01:01:38.040
something because all we know how to do is basically say is something associated with something else

01:01:38.040 --> 01:01:42.360
we don't know why it's associated we don't know how to differentiate for example um

01:01:42.360 --> 01:01:51.320
you know uh why what is the difference between um so uh the difference between if you were to look

01:01:51.320 --> 01:01:55.400
at like educational outcomes right and say what what should what should institutes of higher learning

01:01:55.400 --> 01:02:01.720
do you know in their in their admissions processes vis-a-vis um black versus white Americans versus

01:02:01.720 --> 01:02:08.280
white you know versus Jewish Americans and those are two very different uh cases to think about and

01:02:08.280 --> 01:02:16.200
part of why they're different to think about is because um you know there's this whole um background

01:02:16.200 --> 01:02:21.640
of how that data was created it's not just is a group overrepresented versus another group

01:02:21.640 --> 01:02:26.440
but there's also you know what was the process you know in one case I think we have a very deep and

01:02:26.440 --> 01:02:33.560
well-documented knowledge of uh systematic you know like institutional um oppression of one group

01:02:33.560 --> 01:02:39.800
that you know created opportunities and um with held opportunities from the other and in the other

01:02:39.800 --> 01:02:45.400
case you sort of have a group that's maybe overrepresented despite being um discriminated against

01:02:46.040 --> 01:02:51.960
and and and you know these kind of point to different uh I think to most people's like ethical

01:02:51.960 --> 01:02:55.960
sensibilities different senses about you know how you should correct them but supervised learning

01:02:55.960 --> 01:02:59.400
doesn't give you those tools right supervised learning doesn't distinguish these them

01:02:59.400 --> 01:03:02.920
between these because it doesn't the the tools that we have in supervised learning for trying

01:03:02.920 --> 01:03:08.280
to address fairness don't don't look into where the data comes from so back back to the those

01:03:08.280 --> 01:03:12.680
two cases what people have done is they've tried to formalize you know disparate treatment and

01:03:12.680 --> 01:03:18.120
disparate impact as technical ideas that you can express just as simple statistical parodies

01:03:18.120 --> 01:03:21.640
and the reason why is because they want to build an algorithm right you want to we want to say hey

01:03:21.640 --> 01:03:27.960
here's a problem I can I can define it in technical terms I can fix it in technical terms

01:03:28.920 --> 01:03:33.960
so the way people interpret disparate treatment in technical terms is they say um uh disparate

01:03:33.960 --> 01:03:39.080
treatment says explicit use of the protected class or intentional use of the you know intentional

01:03:40.040 --> 01:03:45.560
kind of discrimination on that basis even if you know it's not you know used directly well we

01:03:45.560 --> 01:03:49.320
well let's throw out the intentional part because we don't really know how to model intent we

01:03:49.320 --> 01:03:52.680
don't know what that means it's not even clear if it means anything in the context of the

01:03:52.680 --> 01:03:57.160
supervised learning model you know who's intent are we talking about the human the model so you

01:03:57.160 --> 01:04:01.320
throw that part out and so then disparate treatment just becomes blindness so it means we just don't

01:04:01.320 --> 01:04:06.680
use that feature then disparate impact on the other hand we don't know um how to deal with the

01:04:06.680 --> 01:04:10.600
justification part because justification has we have to start thinking about where the data comes

01:04:10.600 --> 01:04:15.880
from we have to start thinking about what does it you know we you know that's two it's too philosophical

01:04:15.880 --> 01:04:20.760
so let's throw out the justification part let's just look at demographic parity right then

01:04:21.720 --> 01:04:28.200
among other things you know you say well um let's say that I wanted to minimize the demographic

01:04:28.200 --> 01:04:34.840
disparity between two groups while as much as possible maintaining the accuracy of the model right

01:04:35.400 --> 01:04:39.800
how would I do that and the answer is actually simple it's I would go in there and explicitly

01:04:41.880 --> 01:04:45.400
set the thresholds of the groups differently so that I would accept more people from one group

01:04:45.400 --> 01:04:49.560
is slightly less than the other than if I had just like run a supervised model instead of universal

01:04:49.560 --> 01:04:55.480
threshold and by doing that you would actually like optimally trade off you know the demographic

01:04:55.480 --> 01:05:01.160
parity versus the accuracy and then what people end up saying as well but we can't do that

01:05:01.160 --> 01:05:05.720
because we want to we don't want to have disparate treatment and the question is is it disparate

01:05:05.720 --> 01:05:11.800
treatment if it's in the service of diversity um and that actually becomes this actually you know

01:05:11.800 --> 01:05:17.320
it's so so the big problem like where we you know we actually we'll rent our own problem and got

01:05:17.320 --> 01:05:21.400
to work through this issue is that fortunately we thought instead of others we had friends who are

01:05:21.400 --> 01:05:25.400
in the legal community who work on these issues and are they passionate about them and are no

01:05:25.400 --> 01:05:29.160
hit when they're talked about the wrong way from technical community thinking back to us and said

01:05:29.160 --> 01:05:33.640
well you know you can't you can't have legal disparate treatment like what do you mean you're like

01:05:33.640 --> 01:05:38.040
well it's by definition if it's legal it's not disparate treatment right so there was this there was

01:05:38.040 --> 01:05:43.560
this kind of tension of so disparate treatment then doesn't actually mean blindness disparate treatment

01:05:43.560 --> 01:05:49.080
means it's unjustified right it's like disparate it's not like having having disparate treatment

01:05:49.080 --> 01:05:54.520
isn't just not being blind it's like having disparate treatment means not being blind in a scenario

01:05:54.520 --> 01:05:59.640
that's like illegal unless it's somehow over and considered to be legal so what we've done I think

01:05:59.640 --> 01:06:06.120
the big danger is we've we've overloaded these like reductive technical terms with the name of

01:06:06.120 --> 01:06:10.360
a legal doctrine so we've purported you know at the end of the day we say we've solved you know

01:06:10.360 --> 01:06:14.280
we've we've solved this for treatment we've solved this for an impact but we've solved this like

01:06:14.280 --> 01:06:19.000
these sort of toys statistical parodies we've characterized these trade offs it's useful work

01:06:19.000 --> 01:06:23.080
but the big danger I think is that we end up sort of misrepresenting the public that we've solved

01:06:23.080 --> 01:06:28.440
like a legal conundrum and we have it right so what we ended up looking at is so what then people do

01:06:28.440 --> 01:06:31.880
right is they they say well you can't have this for treatment because the loss is you can't even

01:06:31.880 --> 01:06:37.400
though actually you can potentially you know and this is I think people there's all kinds of

01:06:37.400 --> 01:06:41.560
other arguments like people are wary of doing anything that looks like affirmative action

01:06:41.560 --> 01:06:45.480
or calling it affirmative action whether or not they believe in affirmative action because

01:06:45.480 --> 01:06:50.840
there's a worry that you know even though affirmative action is legal that small changes in

01:06:51.560 --> 01:06:56.520
composition of the Supreme Court or something could or you know it could be in a precarious state

01:06:56.520 --> 01:07:00.040
so you want to come up with a solution that doesn't rely on that's a that's a separate issue for

01:07:00.040 --> 01:07:04.840
I think for more from like a strategic angle in terms of pitching policy then from a technical angle

01:07:06.200 --> 01:07:11.000
but what we end up doing is paper is basically look at this problem like one first how to how do

01:07:11.000 --> 01:07:17.960
these how do these technical notions relate to these legal ones but then even further when you

01:07:17.960 --> 01:07:24.360
try to trade off having you know this representational parity this demographic parity or or what

01:07:24.360 --> 01:07:30.520
we call you know what they call satisfying you know disparate impact or not having disparate impact but

01:07:30.520 --> 01:07:35.480
you know it's actually this like demographic parity in a statistical sense with accuracy and you

01:07:35.480 --> 01:07:39.560
try to do it without explicitly looking at the group membership you come up with these algorithms

01:07:39.560 --> 01:07:43.880
that have very very strange behavior and so we start analyzing you know what's the behavior of these

01:07:43.880 --> 01:07:50.120
algorithms and two really weird things happen one is that basically well you have all these features

01:07:50.120 --> 01:07:55.320
that are sort of predictive of whatever is the protected attribute right so correlated to yeah

01:07:55.320 --> 01:07:58.760
right which is sort of why you have a problem in the first place right well why disparate impact

01:07:58.760 --> 01:08:01.880
is the problem in the first place is that well it's not enough to just not look at that feature

01:08:01.880 --> 01:08:06.120
because it's associated with with a label it's associated with the other covariates like

01:08:06.120 --> 01:08:10.840
everything's all entangled if it's perfectly entangled like in the way that like you could just

01:08:10.840 --> 01:08:15.720
with a hundred percent accuracy recover that feature right then it turns out well because the

01:08:15.720 --> 01:08:21.160
optimal thing is to just set a class dependent threshold like and just move the threshold to sort

01:08:21.160 --> 01:08:26.040
of in a diversity promoting way that's what your model is going to do anyway right if your model's

01:08:26.040 --> 01:08:29.720
expressive enough that's explicitly what it's going to do so you know you've done all you've

01:08:29.720 --> 01:08:33.080
done through all these hoops to pretend that you're doing something else but when the feature is

01:08:33.080 --> 01:08:37.640
fully recoverable you're going to end up doing that exactly anyway so you're only kind of pretending

01:08:37.640 --> 01:08:44.200
to do something qualitatively different in which case why bother but then the the more troubling case

01:08:44.200 --> 01:08:49.800
is what happens when you know everything's correlated but not perfectly like you can only

01:08:50.440 --> 01:08:57.800
imperfectly infer someone's gender say from their non-protected traits and then the weird

01:08:57.800 --> 01:09:01.400
thing that happens is that if you take a lot of these algorithms off the shelf and you apply them

01:09:01.400 --> 01:09:06.040
on data so we looked at CS admissions data and we said what if we wanted to increase the number

01:09:06.040 --> 01:09:11.880
of a women admitted relative to men while without looking at the gender right so that's that's

01:09:11.880 --> 01:09:16.600
the that's the kind of problems that was adopted by a lot of problems is as we want to

01:09:17.400 --> 01:09:21.960
not have disparate impact but also not have disparate treatment you know keep in mind I'm like

01:09:21.960 --> 01:09:25.640
using a scare quotes because that's not you know they're not really talking about the legal doctrine

01:09:25.640 --> 01:09:31.720
but about the so we we tried to not use those terms so we called it impact disparity and treatment

01:09:31.720 --> 01:09:35.560
disparity to be like just you know it explicitly say in the introduction like you know that we

01:09:35.560 --> 01:09:39.880
use this to just be clear we're not talking about the the legal doctrine so if you say I don't want

01:09:39.880 --> 01:09:44.120
to have either of those and then subject to that I want to maximize accuracy what happens

01:09:44.680 --> 01:09:50.760
and what ends up happening is the model implicitly is trying to infer what is the what is the gender say

01:09:50.760 --> 01:09:56.200
so in our case it was gender was a sensitive trait and then it's trying to use the inferred implicit

01:09:56.200 --> 01:10:00.280
gender to flip decisions so basically you know you look at what was the threshold that you would

01:10:00.280 --> 01:10:03.720
have had anyway and you have some people that are a little bit above the threshold some people that

01:10:03.720 --> 01:10:08.200
are a little bit below the threshold um both among you know the men and above the women and what

01:10:08.200 --> 01:10:11.640
basically what the model is doing is it doesn't know which of the men are the women it knows which

01:10:11.640 --> 01:10:15.960
ones the things are more likely to be men or more likely to be women so end up being if you look

01:10:15.960 --> 01:10:20.760
at as compared to the unconstrained model whose decisions get flipped and it's well one because

01:10:20.760 --> 01:10:28.040
there there's an in-norded amount of men in the cs admissions data um it's it's largely just men

01:10:28.040 --> 01:10:32.360
having their decisions flipped from men who the model thinks are women having their models flip

01:10:32.360 --> 01:10:37.320
to positive and men who the model thinks are men having their very confidently having their decisions

01:10:37.320 --> 01:10:43.720
flipped to negative and then there's also some flips of women who and women who the model thinks are

01:10:43.720 --> 01:10:49.640
men might get hurt by the algorithm and women who the model thinks are most likely to be women but

01:10:49.640 --> 01:10:56.120
are a little bit below the threshold are benefited by it um and then you think well you know what you

01:10:56.120 --> 01:10:59.400
know if you step back if you think in terms of supervised learning and you just think hey what I

01:10:59.400 --> 01:11:04.600
care about is that there's some group level justice then you know in that through that lens you

01:11:04.600 --> 01:11:09.640
think you might still say this is you know this is worth it this is okay right even though it sounds

01:11:09.640 --> 01:11:13.320
acid on more it yeah it sounds a little bit weird especially when it's like what we could have

01:11:13.320 --> 01:11:17.480
just gone in there and just set the thresholds a little bit differently and just you know like our

01:11:17.480 --> 01:11:22.440
goal was we wanted to increase the representations in group like why aren't we just doing that but

01:11:22.440 --> 01:11:27.240
the weirder thing though I think that then this is the part where I think it kind of brings

01:11:27.240 --> 01:11:32.040
things full circle to like thinking a little bit more like say an economist or like a social scientist

01:11:32.040 --> 01:11:38.040
is you think not just about a prediction but a system of incentives right not just I'm making

01:11:38.040 --> 01:11:42.600
classifications but I'm making decisions and this is telling people what they should do to get

01:11:42.600 --> 01:11:47.160
into grad school right and it's like so if you start breaking it down you look into it and you say

01:11:47.160 --> 01:11:51.560
who are the people that the model thinks are most likely to be women I like say among the women

01:11:51.560 --> 01:11:56.520
like what what what are the the features that make the model think they're most likely to be women

01:11:56.520 --> 01:12:01.720
versus men and it turns out that there's discrepancy is based on who your requested advisor is which

01:12:01.720 --> 01:12:08.840
area you say you want to work in things like this right and so like to me that's like the scariest

01:12:08.840 --> 01:12:12.600
part about it it's not just that it's like a little bit like if it was just your injectors all

01:12:12.600 --> 01:12:18.600
these layers of unintended consequences and potentials for gaming and all kinds of stuff

01:12:18.600 --> 01:12:23.080
and even more than that right because it's not just like unintended consequences like who are the

01:12:23.080 --> 01:12:29.960
women like how do you how do you benefit from this policy as a woman you have to be in the field in

01:12:29.960 --> 01:12:36.200
which women are already well represented so that it's in the subfield so that the algorithm knows

01:12:36.200 --> 01:12:41.000
that you're a woman but the women who are actually hurt by it are the ones who are like in the field

01:12:41.000 --> 01:12:47.240
that is underrepresented those ones who are hurt because the model thinks they're men right so

01:12:48.040 --> 01:12:52.040
or things are more likely to be men and therefore things that's more likely to get more credit

01:12:52.040 --> 01:12:59.240
towards it's like you know equality constraint by rejecting them and so the danger here and so like

01:12:59.240 --> 01:13:04.520
I don't mean like I want to be very clear that one I think we need to do something to address these

01:13:04.520 --> 01:13:11.880
social issues and I think it's like a pressing concern of our time and I'm all for it I also

01:13:11.880 --> 01:13:18.680
think we need technical people working on it and thinking about these problems and even investigating

01:13:18.680 --> 01:13:24.440
the potential of technical solutions but I think you know the the danger is we need to one we need

01:13:24.440 --> 01:13:29.240
to be very clear about what is the problem we're solving and is this is this a solution anyone should

01:13:29.240 --> 01:13:33.640
even think about using off the shelf right because like these policy makers are saying we need an

01:13:33.640 --> 01:13:40.040
interpretability or explainability or or you know some kind of fairness whatever and when technologists

01:13:40.040 --> 01:13:44.280
are showing up because there's a bit of a career cookie to say hey I'm working on that and I'm

01:13:44.280 --> 01:13:49.160
I've made progress what about my research the big danger is well what what if what you're doing

01:13:49.160 --> 01:13:54.280
actually is really bad like would be really really bad for anyone to deploy that um compared to

01:13:54.280 --> 01:13:59.000
even doing something kind of naive right and and we have to be I think just really careful that we

01:13:59.000 --> 01:14:05.880
don't like sort of create this uh sense that like we've we've solved this societal problem in a way

01:14:05.880 --> 01:14:10.040
where like oh you know you could outsource the judgment to us we don't need we don't need the

01:14:10.840 --> 01:14:15.400
the kind of think about it in a much way it's like we we've actually turned this into a technical

01:14:15.400 --> 01:14:22.920
problem and solved it so so you know like you know outsource outsource the the like really really

01:14:24.360 --> 01:14:30.040
pertinent like debate that needs to happen to society to the technocrats or something

01:14:30.920 --> 01:14:36.040
so so right you know I think it's absolutely important work in an important area but the danger

01:14:36.040 --> 01:14:41.640
that I think we kind of highlight there is there is there is a huge risk of a certain kind of

01:14:41.640 --> 01:14:49.640
um folly of solutionism uh was that it was great uh getting a chance to finally catch up with you

01:14:49.640 --> 01:14:58.280
and definitely lots of important issues here and things to figure out but not via solutionism

01:14:58.280 --> 01:15:04.680
yeah yeah we gotta stay humble all right well thanks so much thanks for having me Sam

01:15:04.680 --> 01:15:13.800
all right everyone that's our show for today for more information on today's show

01:15:13.800 --> 01:15:21.960
visit twomolai.com slash shows make sure you head over to twomolcan.com to learn more about the

01:15:21.960 --> 01:15:37.880
twomolcan ai platforms conference as always thanks so much for listening and catch you next time

