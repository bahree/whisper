1
00:00:00,000 --> 00:00:04,880
The conversation you're about to hear was recorded live at Twomokon AI

2
00:00:04,880 --> 00:00:11,680
platforms. For more coverage of Twomokon, visit Twomokon.com-news or follow us

3
00:00:11,680 --> 00:00:20,040
on Twitter at Twomokon AI. But first a word from our sponsor. Big thanks to our

4
00:00:20,040 --> 00:00:26,880
friends at IBM for being a founding sponsor of Twomokon AI platforms. IBM

5
00:00:26,880 --> 00:00:31,080
Watson is the company's comprehensive suite of AI tools for the enterprise

6
00:00:31,080 --> 00:00:37,400
which includes Watson Studio, Watson Machine Learning, and Watson Open Scale. The

7
00:00:37,400 --> 00:00:41,960
IBM Watson Suite allows enterprises to build, deploy, and manage AI models in

8
00:00:41,960 --> 00:00:47,560
any environment including on-premises and in private and public clouds. We

9
00:00:47,560 --> 00:00:51,360
encourage you to check out the IBM Data Science and AI community by visiting

10
00:00:51,360 --> 00:00:57,040
Twomokon AI.com slash IBM. And if you join, you'll get a complimentary month of

11
00:00:57,040 --> 00:01:01,560
select IBM programs on Coursera.

12
00:01:03,600 --> 00:01:11,880
All right, so super excited to invite up our next guests. Fran Bell, Fran, runs a

13
00:01:11,880 --> 00:01:19,440
Data Science platform's team at Uber. She's got over 100 data scientists

14
00:01:19,440 --> 00:01:25,280
working on building tools that are a platform that's at a higher level of

15
00:01:25,280 --> 00:01:35,560
abstraction than Uber's already famous Michelangelo platform. Fran. Welcome to

16
00:01:35,560 --> 00:01:40,440
Twomokon. So I kind of paraphrased your role a little bit. Why don't you tell

17
00:01:40,440 --> 00:01:44,160
us a little bit about your team and your charter? Yeah, absolutely. Thank you so

18
00:01:44,160 --> 00:01:48,200
much for having me. It's really a pleasure to be here at the inaugural Twomokon.

19
00:01:48,200 --> 00:01:54,480
About the charter of the team, the vision really is to provide cutting-edge

20
00:01:54,480 --> 00:01:59,000
data science at the push of a button to anyone within the company. So that

21
00:01:59,000 --> 00:02:04,120
basically means that we're aiming to transform anyone within Uber into a

22
00:02:04,120 --> 00:02:08,680
data scientist. An example of this is forecasting. So forecasting obviously

23
00:02:08,680 --> 00:02:13,680
underpins a large number of use cases within Uber. And so the vision here is to

24
00:02:13,680 --> 00:02:18,800
provide the latest and greatest cutting-edge forecasts to folks at a push of a

25
00:02:18,800 --> 00:02:24,600
button via UI, for example, as integrated into a BI stack or programmatically

26
00:02:24,600 --> 00:02:30,160
accessible through our API. And so the only thing that our end users within Uber

27
00:02:30,160 --> 00:02:34,960
need to provide is historic data, whether it's for example in the form of a

28
00:02:34,960 --> 00:02:40,480
CSV file or query, and the forecast horizon. So how far you want to forecast

29
00:02:40,480 --> 00:02:45,320
out. And we do everything else automatically in the background. We scan over

30
00:02:45,320 --> 00:02:50,280
whole suite of forecasting algorithms, either those that we have integrated

31
00:02:50,280 --> 00:02:56,160
off the shelf into the platform, or also those that we have developed in-house

32
00:02:56,160 --> 00:03:01,760
proprietary. And we've gone way beyond forecasting in terms of data science

33
00:03:01,760 --> 00:03:05,920
areas, other areas that we're investing heavily in in platformization or

34
00:03:05,920 --> 00:03:11,320
anomaly detection, experimentation, more recently also conversational AI

35
00:03:11,320 --> 00:03:15,840
and natural language. And then personally I'm very excited about our proof of

36
00:03:15,840 --> 00:03:20,680
concepts of also platformizing and semi-automating intelligent insights

37
00:03:20,680 --> 00:03:26,640
generation and data exploration. So of those conversational AI seems like the

38
00:03:26,640 --> 00:03:31,640
I'd man out so to speak. How did you end up working on that one? Yeah so we see a

39
00:03:31,640 --> 00:03:37,160
lot of really great benefits. We have a lot of conversational AI data at Uber.

40
00:03:37,160 --> 00:03:41,640
One example is our customer obsession ticket assistant, which was one of our

41
00:03:41,640 --> 00:03:48,920
first use cases in this space. So here for example we wanted to aid customer

42
00:03:48,920 --> 00:03:53,800
service representatives in solving customer support tickets that are coming

43
00:03:53,800 --> 00:03:58,240
in. And as you can imagine given the size of our platform we get quite a few

44
00:03:58,240 --> 00:04:04,080
of those. And so using natural language and deep learning approaches we were

45
00:04:04,080 --> 00:04:09,200
able to build recommendations for our customer service representatives of what

46
00:04:09,200 --> 00:04:14,160
is the topic that folks are writing in about. What are potential actions that

47
00:04:14,160 --> 00:04:18,800
the customer service representative might want to take. And then also providing

48
00:04:18,800 --> 00:04:24,040
guard lines or guard rails basically around what could be the best starting

49
00:04:24,040 --> 00:04:29,800
point to actually address the response to the end consumer. Of course the

50
00:04:29,800 --> 00:04:34,480
customer service representative always has the final call and say on this. But we

51
00:04:34,480 --> 00:04:39,800
saw really great improvements in our customer care experience as a result of

52
00:04:39,800 --> 00:04:44,080
having this AI basically assisting our customer service representatives.

53
00:04:44,080 --> 00:04:49,600
Okay cool. So you've got this portfolio of platforms essentially that you're

54
00:04:49,600 --> 00:04:55,560
building to support these different use cases. How do you know when it's time

55
00:04:55,560 --> 00:05:00,720
to platform something? Yeah that's a really great question. And we're looking at

56
00:05:00,720 --> 00:05:04,440
multiple different dimensions here and very deliberately see which of the

57
00:05:04,440 --> 00:05:07,360
data science areas we want to platformize. It's obviously a heavily

58
00:05:07,360 --> 00:05:13,680
investment. And so we look at three items. The first one is can the

59
00:05:13,680 --> 00:05:17,600
platformization of this area really create step function improvements to our

60
00:05:17,600 --> 00:05:21,280
user experience and the business. And so sticking with the example of

61
00:05:21,280 --> 00:05:27,480
forecasting if we can forecast tightly accurately demand in a particular space

62
00:05:27,480 --> 00:05:32,080
and time we can create more magical user experiences. The second thing is

63
00:05:32,080 --> 00:05:37,560
really the wealth of use cases that exist across the company. Of course building

64
00:05:37,560 --> 00:05:42,080
a platform you want to be able to tackle many different use cases. And so with

65
00:05:42,080 --> 00:05:46,400
forecasting for example it does spend the entire enterprise ranging from

66
00:05:46,400 --> 00:05:50,880
marketing to obviously marketplace supply and demand financial aspects

67
00:05:50,880 --> 00:05:55,280
operations as well as our hardware. We still have a lot of hardware on premise

68
00:05:55,280 --> 00:05:59,080
and so accurately forecasting the hardware needs especially on high

69
00:05:59,080 --> 00:06:03,640
demand days and special days such as Halloween or New Year's Eve is really

70
00:06:03,640 --> 00:06:07,840
important. So that's the second aspect. And then thirdly. All we need is a

71
00:06:07,840 --> 00:06:15,960
special day at Uber. Yes that's it is. See lots of demand on that front. And then

72
00:06:15,960 --> 00:06:20,560
the third dimension really is the reusability of the models and

73
00:06:20,560 --> 00:06:25,640
methodologies that we apply. And again with forecasting you know a common

74
00:06:25,640 --> 00:06:29,560
framework that is needed to build forecasting algorithms is a backtesting

75
00:06:29,560 --> 00:06:34,040
framework. So understanding the accuracy of your forecasts and that really is

76
00:06:34,040 --> 00:06:40,560
needed for any step along the forecasting journey. And so having a common

77
00:06:40,560 --> 00:06:45,560
central paralyzed language extensible backtesting framework is something

78
00:06:45,560 --> 00:06:49,800
that's really important. So as your team out evangelizing the

79
00:06:49,800 --> 00:06:55,160
opportunity to platformize and looking for customers that are already working

80
00:06:55,160 --> 00:07:00,680
on things that meet these criteria or our folks coming to you saying hey we've

81
00:07:00,680 --> 00:07:04,760
got these problems help us all them. How does the relationship with your

82
00:07:04,760 --> 00:07:10,040
ultimate customer evolve? Yeah absolutely. So we have a lot of the product teams

83
00:07:10,040 --> 00:07:14,840
coming to us with use cases at the same time because we are this horizontal

84
00:07:14,840 --> 00:07:19,040
team that spans across the entire company across all lines of business we have

85
00:07:19,040 --> 00:07:23,720
a very unique vantage point as well. And so we can also gently nudge some of

86
00:07:23,720 --> 00:07:29,000
the product teams to come and join us in this journey as well. Okay and so when

87
00:07:29,000 --> 00:07:34,640
you identify a problem space that it makes sense to platformize how do you

88
00:07:34,640 --> 00:07:38,200
approach that you just jump in and start building, start coding or what is

89
00:07:38,200 --> 00:07:43,360
the methodology look like? Yeah that's a great question. So the way we build

90
00:07:43,360 --> 00:07:48,040
platforms is in a use case driven manner. So that basically means that with

91
00:07:48,040 --> 00:07:52,440
every use case that is strategically chosen we augment the platform and we

92
00:07:52,440 --> 00:07:57,200
reuse as much capabilities as possible from the platform. And that really

93
00:07:57,200 --> 00:08:02,040
allows us to have wins very early on. And learning from this we actually now

94
00:08:02,040 --> 00:08:06,240
have a three-faced approach to platformization. So step one is really

95
00:08:06,240 --> 00:08:11,120
consulting. So we have these deep domain experts in particular areas of data

96
00:08:11,120 --> 00:08:15,560
science on the team. And so we embed them with particular areas of the

97
00:08:15,560 --> 00:08:21,920
business where we see opportunities of having use cases in these areas. And so

98
00:08:21,920 --> 00:08:26,040
that has a couple of advantages. Firstly the domain experts learn more about the

99
00:08:26,040 --> 00:08:30,120
business, about the opportunities, the pain points, and really can bring back

100
00:08:30,120 --> 00:08:35,800
these learnings to then drive the best design for these platforms. It also

101
00:08:35,800 --> 00:08:40,920
allows us to tackle these use cases early on and really show wins and gain the

102
00:08:40,920 --> 00:08:45,880
trust of our partners and leadership on that front. That of course is not a

103
00:08:45,880 --> 00:08:50,400
scalable approach. This is why we set out to do platforms in the first place. But

104
00:08:50,400 --> 00:08:54,120
it's a very good starting point. And so the second thing that we usually do is

105
00:08:54,120 --> 00:08:58,960
templatization. So what I mean by this is we build recipes, whether it's form of

106
00:08:58,960 --> 00:09:04,480
documentation, example ipython notebooks, providing talks and educational

107
00:09:04,480 --> 00:09:08,920
aspects. And this really allows us now to have a one-to-many multiplicative

108
00:09:08,920 --> 00:09:14,760
effect throughout the organization, mostly to other data scientists that are

109
00:09:14,760 --> 00:09:20,160
dedicated to these business areas. And then over time as we're taking on more and

110
00:09:20,160 --> 00:09:24,160
more of these use cases, we really expand our platform to become more and

111
00:09:24,160 --> 00:09:28,960
more self-service and work towards that vision of really providing it at the

112
00:09:28,960 --> 00:09:33,200
push of a button without domain expertise required. Of course, including

113
00:09:33,200 --> 00:09:38,040
best practices and guardrails in the process. So in introducing you I mentioned

114
00:09:38,040 --> 00:09:43,560
Michelangelo, Uber's low-level machine learning infrastructure platform. Uber

115
00:09:43,560 --> 00:09:47,160
was one of the first companies to publish about what they were doing to

116
00:09:47,160 --> 00:09:53,520
automate machine learning. If I interpret your LinkedIn profile correctly, you

117
00:09:53,520 --> 00:09:58,400
were at Uber doing applied machine learning platforms before at least before

118
00:09:58,400 --> 00:10:03,840
that article hit possibly before the Michelangelo effort even started. What's

119
00:10:03,840 --> 00:10:09,040
the relationship between these two teams? Yeah, we have a fantastic working

120
00:10:09,040 --> 00:10:13,120
relationship with Michelangelo as well as the AI organization engineering branch

121
00:10:13,120 --> 00:10:18,120
that we also work with very closely to platformize. And we have three modes

122
00:10:18,120 --> 00:10:23,360
of interaction here. The first one is as the head of platform data science, I get

123
00:10:23,360 --> 00:10:28,000
pulled in into the strategic and vision setting when it comes to Michelangelo

124
00:10:28,000 --> 00:10:31,640
working closely with their engineering and product lead. So right from the

125
00:10:31,640 --> 00:10:35,880
start, there's a really great collaborative relationship that we can build on.

126
00:10:35,880 --> 00:10:41,360
And then we have two other modes that have evolved over time. The first one is

127
00:10:41,360 --> 00:10:47,440
Michelangelo was more decent. We deeply embedded folks from our teams into the

128
00:10:47,440 --> 00:10:51,840
Michelangelo group. So for example, with the customer obsession ticket assistant

129
00:10:51,840 --> 00:10:55,840
example that I mentioned earlier, this was actually the first deep learning

130
00:10:55,840 --> 00:11:00,240
algorithm that ran on Michelangelo. And so as you can imagine, a lot of the

131
00:11:00,240 --> 00:11:04,840
features required for doing deep learning were in a very nascent state at the

132
00:11:04,840 --> 00:11:08,280
time. And so having data scientists who were working on this particular

133
00:11:08,280 --> 00:11:12,680
problem deeply embedded in the Michelangelo group and working together with the

134
00:11:12,680 --> 00:11:17,800
engineers and product managers there to build all capabilities not only to solve

135
00:11:17,800 --> 00:11:22,600
the customer obsession ticket assistant case but also more generic aspects that

136
00:11:22,600 --> 00:11:28,320
then really benefited the community more at large to build deep learning, you

137
00:11:28,320 --> 00:11:33,640
know algorithms and frameworks was really important here. Of course, as

138
00:11:33,640 --> 00:11:38,560
Michelangelo has evolved over time and became more mature, we are becoming more

139
00:11:38,560 --> 00:11:43,720
of an end consumer of the platform and it becomes more of a self-service

140
00:11:43,720 --> 00:11:50,200
component especially with the onset of PyML. PyML has really provided step

141
00:11:50,200 --> 00:11:55,560
function improvements to what's PyML. So PyML basically allows us to write

142
00:11:55,560 --> 00:12:01,480
Python code and bring our own models that then basically via Michelangelo get

143
00:12:01,480 --> 00:12:06,760
deployed in a sandbox environment at scale. And so that really reduces the

144
00:12:06,760 --> 00:12:11,320
barrier to entry for data scientists to be less reliant on, you know, the

145
00:12:11,320 --> 00:12:16,360
native approaches that are already integrated in Michelangelo or software

146
00:12:16,360 --> 00:12:20,920
engineers that would help with productionization for example. And so this

147
00:12:20,920 --> 00:12:26,240
approach has become really prominent across Uber and has really opened up new

148
00:12:26,240 --> 00:12:34,000
avenues for self-service on Michelangelo. And so with your team pre-existing some

149
00:12:34,000 --> 00:12:41,640
of that effort, you've got platforms and use cases that you've stood up before

150
00:12:41,640 --> 00:12:46,000
they were mature and ready now that they're more mature and ready. Is it a

151
00:12:46,000 --> 00:12:51,280
dynamic relationship in the sense that, you know, there's a heavy migrated in any

152
00:12:51,280 --> 00:12:58,760
of those legacy models over to Michelangelo or, you know, if it's there and is

153
00:12:58,760 --> 00:13:03,600
working, you're going to leave it alone. Yeah, that's a really great question. So

154
00:13:03,600 --> 00:13:08,720
we have a couple of aspects here. One is platforms that are more recent. So for

155
00:13:08,720 --> 00:13:13,000
example, a conversational AI platform here from the get go we build on top of

156
00:13:13,000 --> 00:13:17,000
the Michelangelo capabilities and are actively utilizing this. But as you

157
00:13:17,000 --> 00:13:20,760
correctly pointed out, you know, some of the platforms were built well beyond

158
00:13:20,760 --> 00:13:25,640
before Michelangelo existed or was very recent. And so we have our own

159
00:13:25,640 --> 00:13:29,760
independent stacks on this front. But here it's really important to see the

160
00:13:29,760 --> 00:13:34,440
opportunities for integration and to see a timeline where some of these

161
00:13:34,440 --> 00:13:40,240
platforms are already or may be merging in the future. And then the third part

162
00:13:40,240 --> 00:13:44,560
is platforms that are currently standalone, but likely will never merge with

163
00:13:44,560 --> 00:13:48,800
Michelangelo. So for example, our experimentation platform, which has very

164
00:13:48,800 --> 00:13:52,960
different types of methodologies and workflows, I wouldn't imagine would be

165
00:13:52,960 --> 00:13:57,520
combined with Michelangelo. Can you elaborate on that? What about the methodologies

166
00:13:57,520 --> 00:14:04,360
and workflows makes them? You're not good fits. Yeah, absolutely. So here we use

167
00:14:04,360 --> 00:14:09,560
more statistical approaches. So hypothesis testing, multi-arm bandits, etc.

168
00:14:09,560 --> 00:14:14,600
versus the traditional machine learning approaches. And so for that reason we

169
00:14:14,600 --> 00:14:18,320
keep them separate. What are some of the key technical challenges that you

170
00:14:18,320 --> 00:14:24,840
face for this portfolio of use cases? Yeah, that's a great question. So each of

171
00:14:24,840 --> 00:14:31,240
the platforms is different. We have different users, different use cases, and so

172
00:14:31,240 --> 00:14:36,120
therefore also different requirements on the technology side. But I can go into

173
00:14:36,120 --> 00:14:40,800
two concrete examples here. The first one is for real-time anomaly detection.

174
00:14:40,800 --> 00:14:45,240
This was actually the first platform that I've built at Uber. And so here the

175
00:14:45,240 --> 00:14:49,680
idea is that we wanted to detect system outages as quickly as possible. So

176
00:14:49,680 --> 00:14:56,200
people not being able to sign in or sign up or perhaps trips being degraded,

177
00:14:56,200 --> 00:15:03,720
etc. And here we basically saw that this was still an open research problem.

178
00:15:03,720 --> 00:15:10,000
And we set out to build a new platform around it and also advance the space.

179
00:15:10,000 --> 00:15:14,880
We have a couple of patents now in that space as well. But here the key kind of

180
00:15:14,880 --> 00:15:19,640
requirement beyond the innovation component was that we needed to have

181
00:15:19,640 --> 00:15:24,320
extremely low latencies. So as you can imagine, because there's a real-time

182
00:15:24,320 --> 00:15:30,640
problem, we had basically those considerations to take care of on and

183
00:15:30,640 --> 00:15:35,800
extremely high QPS because we have hundreds of millions of signals

184
00:15:35,800 --> 00:15:40,840
basically back end as well as aggregate mobile signals that we're tracking in

185
00:15:40,840 --> 00:15:46,200
order to understand whether there's a system outage going on. Another one

186
00:15:46,200 --> 00:15:51,240
example is forecasting. As I mentioned earlier, we integrated our forecasting

187
00:15:51,240 --> 00:15:57,760
algorithms also into our BI stack for easy access through UIs. And so there's

188
00:15:57,760 --> 00:16:01,760
now a really great path where you can query a metric, you can visualize this

189
00:16:01,760 --> 00:16:06,160
metric using DashBuilder, an internal tool that we've built. And then you'll

190
00:16:06,160 --> 00:16:10,560
also have this little button that says, I want to forecast this metric. And so

191
00:16:10,560 --> 00:16:13,880
obviously we want to make sure that we have a good user experience here and that

192
00:16:13,880 --> 00:16:16,360
people don't have to wait, you know, minutes or hours.

193
00:16:16,360 --> 00:16:21,400
You're hearing theme here. Exactly, right. And so having low latency for some of

194
00:16:21,400 --> 00:16:24,760
these forecasting algorithms is really important. And so are some of these

195
00:16:24,760 --> 00:16:28,560
technical challenges ever a reason why you might build something from the

196
00:16:28,560 --> 00:16:33,640
ground up yourself, so to speak, as opposed to rely on what the Michelangelo

197
00:16:33,640 --> 00:16:39,440
team offers or is that not a consideration typically? Yes, it does flow into kind

198
00:16:39,440 --> 00:16:44,480
of our decision making process here in terms of what is already available, how

199
00:16:44,480 --> 00:16:49,240
easily it is extensible. And often the timing kind of component comes in as

200
00:16:49,240 --> 00:16:53,560
we discussed earlier in terms of, you know, where was Michelangelo when we

201
00:16:53,560 --> 00:16:58,560
started to build? And how can we evolve that in the future? Can you talk a little

202
00:16:58,560 --> 00:17:05,920
bit about the technology stack that your platforms tend to rely on? Do you have

203
00:17:05,920 --> 00:17:10,680
your own kind of not Michelangelo, but you know, you've got these higher level

204
00:17:10,680 --> 00:17:13,840
platforms that are very application or use case focused. You have your own

205
00:17:13,840 --> 00:17:18,600
kind of intermediate level of abstraction, or are you building kind of use

206
00:17:18,600 --> 00:17:26,720
cases more independently? Yeah, so when we don't build on top of Michelangelo, which

207
00:17:26,720 --> 00:17:32,160
is quite a few of our platforms, we build microservice architectures, we build

208
00:17:32,160 --> 00:17:38,520
them and go in Java actually for performance reasons. Databases are typically

209
00:17:38,520 --> 00:17:44,480
in my SQL. Then we run our instances typically on prem for efficiency

210
00:17:44,480 --> 00:17:50,400
reasons. And then several of our use cases are batch and offline, especially on

211
00:17:50,400 --> 00:17:55,120
the training side. But for those that we discussed earlier, where latency is

212
00:17:55,120 --> 00:18:00,200
something that we want to focus on, we use caching for optimization. And do

213
00:18:00,200 --> 00:18:04,480
you rely heavily on open source in this area or publish open source in this

214
00:18:04,480 --> 00:18:09,760
area? Yeah, both. So we're definitely building on the shoulder of giants and

215
00:18:09,760 --> 00:18:14,040
using open source wherever possible. I think we wouldn't have evolved as

216
00:18:14,040 --> 00:18:21,040
quickly if it wasn't for open source. And so utilizing the methodologies that

217
00:18:21,040 --> 00:18:25,360
have been developed by the communities is very essential. But we also are

218
00:18:25,360 --> 00:18:29,400
heavily invested in open sourcing ourselves as well. And we have quite a few

219
00:18:29,400 --> 00:18:34,680
open source projects. If we look at the data science domain, there are quite a

220
00:18:34,680 --> 00:18:40,200
few examples here as well. We have a pyro that was developed by the AI

221
00:18:40,200 --> 00:18:44,440
organization, which is a probabilistic programming language. We have horror

222
00:18:44,440 --> 00:18:50,000
vod that is distributed deep learning framework on TensorFlow that has gained a

223
00:18:50,000 --> 00:18:55,520
lot of popularity in the community. There is Ludwig that was also built by the

224
00:18:55,520 --> 00:19:00,480
AI organization, which allows for a deep learning framework where you actually

225
00:19:00,480 --> 00:19:06,480
don't have to write code anymore to deploy and train models. And then more

226
00:19:06,480 --> 00:19:11,440
recently, our org has worked together with the AI organization to develop

227
00:19:11,440 --> 00:19:18,280
Plato. This is a very flexible and use case rich platform that allows for

228
00:19:18,280 --> 00:19:24,480
conversational AI, especially in the research and prototyping phase. And then

229
00:19:24,480 --> 00:19:30,360
also our causal ML package is a Python package that we recently launched in

230
00:19:30,360 --> 00:19:36,960
collaboration with our marketing team that basically covers uplift modeling

231
00:19:36,960 --> 00:19:42,880
use cases as well as causal inference in combination with machine learning.

232
00:19:42,880 --> 00:19:47,280
So yeah, quite a lot of efforts in that direction. And we're considering to do

233
00:19:47,280 --> 00:19:52,960
more. With so much out and available in open source that you're incorporating

234
00:19:52,960 --> 00:19:58,440
into these systems that you're building, reminds me a little bit of podcasts

235
00:19:58,440 --> 00:20:03,480
we published not too long ago. Quote that the guest mentioned that became the

236
00:20:03,480 --> 00:20:08,560
title of the podcast was machine learning or AI was is a systems engineering

237
00:20:08,560 --> 00:20:13,360
problem. I wonder how much of what you're doing is systems engineering

238
00:20:13,360 --> 00:20:18,800
connecting pieces that you know in many cases exist already versus kind of

239
00:20:18,800 --> 00:20:23,640
pure innovation and building new stuff. Yeah, it's definitely a combination of

240
00:20:23,640 --> 00:20:27,760
both, especially because we want to be fast to market with a lot of these

241
00:20:27,760 --> 00:20:33,160
things. So we leverage whatever is already there. But often more often than not

242
00:20:33,160 --> 00:20:37,320
actually, we need to not only deploy the cutting edge but actually be and

243
00:20:37,320 --> 00:20:42,400
define the cutting edge as well. And as I was hinting a little bit earlier, one

244
00:20:42,400 --> 00:20:46,600
example is the real-time and non-intection platform that I built when I joined

245
00:20:46,600 --> 00:20:52,240
Uber about five years ago. And so here it became very quickly clear that with

246
00:20:52,240 --> 00:20:56,360
the scale that we're operating at, the real-time nature, the signal to noise

247
00:20:56,360 --> 00:21:00,840
ratio because we actually would be sending pager to the alerts that would wake

248
00:21:00,840 --> 00:21:04,600
people up potentially in the middle of the night. If our algorithm thought there

249
00:21:04,600 --> 00:21:09,800
was a system outage going on. And so we were able to break new ground very

250
00:21:09,800 --> 00:21:14,880
quickly in the space. And one other thing that really helped develop these

251
00:21:14,880 --> 00:21:20,360
algorithms to the precision recall that we needed was I put myself on call. I

252
00:21:20,360 --> 00:21:24,760
was very customer obsessed. We're in the pager. Sorry. Wearing the pager. Yes,

253
00:21:24,760 --> 00:21:29,440
wearing the pager for six teams, multiple consecutive weeks. I can tell you I

254
00:21:29,440 --> 00:21:35,000
did not sleep much. I was working up a lot during this time. But that also helped

255
00:21:35,000 --> 00:21:38,880
to improve the algorithms really quickly. May she want to get it right? Yes,

256
00:21:38,880 --> 00:21:45,840
exactly. Nice, nice. Your team is kind of building very use-case specific

257
00:21:45,840 --> 00:21:50,960
things, you know, very close to the end user, doing low-level kind of

258
00:21:50,960 --> 00:21:59,000
infrastructure as well to support all of this. How do you organize? How do you

259
00:21:59,000 --> 00:22:04,840
build an organization to support all of this? Yeah, so we are organized by

260
00:22:04,840 --> 00:22:09,480
data science expertise. So we have a forecasting team, a non-detection team,

261
00:22:09,480 --> 00:22:14,800
experimentation team, convi team, computer vision team, etc. basically. And then

262
00:22:14,800 --> 00:22:19,120
as many other companies we are cross-functionally organized. So we have

263
00:22:19,120 --> 00:22:23,960
data scientists and engineers, product managers, designing, working

264
00:22:23,960 --> 00:22:28,560
together, basically building all of these platforms. When we zoom into the

265
00:22:28,560 --> 00:22:33,960
data scientists, we tend to hire full stack data scientists for these roles

266
00:22:33,960 --> 00:22:38,640
exactly for the reason that you described. We see a lot of advantages to

267
00:22:38,640 --> 00:22:42,720
having folks who can write production-level code in addition to having this

268
00:22:42,720 --> 00:22:48,360
deep domain expertise in a particular data science area. And in some of the

269
00:22:48,360 --> 00:22:52,880
advantages we see here already start in the design phase. So having somebody

270
00:22:52,880 --> 00:22:58,680
has deep understanding about the constraints of the infrastructure stack. As

271
00:22:58,680 --> 00:23:03,120
we're developing a lot of these things at scale with latency constraints can

272
00:23:03,120 --> 00:23:07,920
be highly advantageous. Because things that might look good on paper might

273
00:23:07,920 --> 00:23:12,240
actually not work in the real world once we deploy it in these ecosystems.

274
00:23:12,240 --> 00:23:17,320
Do you have examples of that? Things that look good on paper that didn't

275
00:23:17,320 --> 00:23:20,760
actually work out? Yeah. So coming back again to the real-time

276
00:23:20,760 --> 00:23:26,320
anomaly detection framework. So at the beginning we were developing algorithms

277
00:23:26,320 --> 00:23:31,000
that would have extremely fine granular data points. So we obviously want to

278
00:23:31,000 --> 00:23:36,320
have near real-time signal. And so we wanted to have an understanding minute by

279
00:23:36,320 --> 00:23:41,600
minute or even in finer granularity what was going on. And so originally we

280
00:23:41,600 --> 00:23:47,080
designed an algorithm that would require multiple weeks of data to train one

281
00:23:47,080 --> 00:23:51,840
way of how we did this is to frame it as a forecasting problem. And then half

282
00:23:51,840 --> 00:23:55,880
that minute granularity. And that obviously would be an extremely large overhead

283
00:23:55,880 --> 00:24:02,480
onto our database systems on that front. And so we basically designed the

284
00:24:02,480 --> 00:24:07,760
algorithms such a way that we have lower course of granularity in more

285
00:24:07,760 --> 00:24:11,520
historic kind of aspects. And then very fine granularity is overlaid as a

286
00:24:11,520 --> 00:24:16,360
secondary step in order to still capture the variance on for example a minute

287
00:24:16,360 --> 00:24:21,400
by minute level. So just having that kind of constraint in mind made sure that

288
00:24:21,400 --> 00:24:27,600
we didn't require unnecessarily high kind of overheads on our databases and

289
00:24:27,600 --> 00:24:32,760
unnecessary infrastructure cost as a result. So that's one example on that

290
00:24:32,760 --> 00:24:38,400
front. Example another example of where we see a lot of advantages for having

291
00:24:38,400 --> 00:24:44,280
full stack data scientists is in the productionization step. So here we're

292
00:24:44,280 --> 00:24:49,160
trying to avoid handoffs in terms of a data scientist writing a script or a

293
00:24:49,160 --> 00:24:54,080
white paper and then you know providing it to a software engineer. We see a

294
00:24:54,080 --> 00:24:59,040
lot of opportunities for errors on that front logical errors in particular. And

295
00:24:59,040 --> 00:25:04,520
so having data scientists to also write the production level code without such a

296
00:25:04,520 --> 00:25:09,520
hand of step is really important to us. And then finally also developer

297
00:25:09,520 --> 00:25:16,760
velocity. So as we all know there is this prototyping step involved and we're

298
00:25:16,760 --> 00:25:20,640
trying to exceed some threshold criterion that we've set out in the

299
00:25:20,640 --> 00:25:24,640
beginning of the design phase. And it's of course not known when are we gonna

300
00:25:24,640 --> 00:25:30,080
exceed basically this threshold criterion. And so that can lead them to

301
00:25:30,080 --> 00:25:33,760
lag times once you have actually found an algorithm you want to productionize

302
00:25:33,760 --> 00:25:38,480
that software engineer might be busy with other things during that time. And so

303
00:25:38,480 --> 00:25:43,200
again having data scientists to can write production level code can really

304
00:25:43,200 --> 00:25:48,640
also help speed up that innovation cycle as well. You mentioned developer

305
00:25:48,640 --> 00:25:52,960
velocity and that makes me think of kind of velocity in a sense of agile

306
00:25:52,960 --> 00:25:58,040
methodologies. Do you mean it that concretely? And is there a methodology that

307
00:25:58,040 --> 00:26:03,240
you've kind of evolved to or developed that works well in the context of

308
00:26:03,240 --> 00:26:08,880
these types of problems? So the way we work is very closely in software

309
00:26:08,880 --> 00:26:13,480
engineering kind of principles. In both in terms of best practices in terms of

310
00:26:13,480 --> 00:26:18,040
our working structure we work on a daily basis hand-in-hand with software

311
00:26:18,040 --> 00:26:22,200
engineers. So we have developed a lot of this. But I think you bring up a

312
00:26:22,200 --> 00:26:25,960
really good point in terms of you know developer velocity and productivity.

313
00:26:25,960 --> 00:26:31,760
And so a big goal of the platform teams more holistically is to really speed

314
00:26:31,760 --> 00:26:35,800
up that innovation cycle while increasing accuracy of the various

315
00:26:35,800 --> 00:26:40,040
different methodologies employed right. And so the way we see it is we have

316
00:26:40,040 --> 00:26:44,840
these four major steps within the development cycle of a machine learning or

317
00:26:44,840 --> 00:26:49,160
broadly speaking data science problem. You have exploratory data analytics

318
00:26:49,160 --> 00:26:54,000
than this iterative prototyping phase productionization and then roll out

319
00:26:54,000 --> 00:26:58,840
and monitoring and that closes the loop for kind of a new cycle to start off.

320
00:26:58,840 --> 00:27:04,680
If there's you know a V2 that we want to progress in. And so building abstraction

321
00:27:04,680 --> 00:27:08,360
layers higher level abstraction layers and this is where you know a lot of the

322
00:27:08,360 --> 00:27:12,840
work that my team and collaborators are coming in to play really helps to

323
00:27:12,840 --> 00:27:17,400
facilitate not us only for us ourselves kind of this cycle but really for the

324
00:27:17,400 --> 00:27:23,600
entirety of the company to really go faster and faster around that loop. So you're

325
00:27:23,600 --> 00:27:27,880
primarily doing this by hiring full-stack engineer full-stack data scientists

326
00:27:27,880 --> 00:27:34,720
they're not easy to find. And we throw that time around like it defines

327
00:27:34,720 --> 00:27:41,800
concretely a specific set of skills but not you not every full-stack data

328
00:27:41,800 --> 00:27:47,320
scientist is going to have the same strengths. How do you grow your data

329
00:27:47,320 --> 00:27:52,640
scientists or if you find folks that need support in one or more areas or how do

330
00:27:52,640 --> 00:27:59,080
you manage the kind of learning cycle for your team? Yeah absolutely so when

331
00:27:59,080 --> 00:28:04,360
we hire for these roles we also hire complimentary of course right so there

332
00:28:04,360 --> 00:28:08,200
would be some folks on the team who lean more towards the research side and

333
00:28:08,200 --> 00:28:12,600
others who lean more towards the engineering and you know software development

334
00:28:12,600 --> 00:28:17,080
side and folks that sit in between. So that's one aspect and then there's a very

335
00:28:17,080 --> 00:28:21,880
strong learning and teaching culture at Uber really continuously striving to

336
00:28:21,880 --> 00:28:29,360
improve oneself and so we have a lot of programs even within Uber educational

337
00:28:29,360 --> 00:28:34,480
programs everything from introductory courses to machine learning all the way

338
00:28:34,480 --> 00:28:40,600
to domain experts basically who then give workshops and training sessions hands

339
00:28:40,600 --> 00:28:45,720
on basically courses. I've invested a lot in mentorship programs at Uber as

340
00:28:45,720 --> 00:28:50,720
well building out a community across all of the data science and analytics where

341
00:28:50,720 --> 00:28:55,960
we then partner folks and in sometimes we also do 20% projects similar to

342
00:28:55,960 --> 00:29:00,840
what Google for example does where we have people then immersed into various

343
00:29:00,840 --> 00:29:05,800
different teams to get hand on a Hans experience basically in some of these

344
00:29:05,800 --> 00:29:09,320
domains so yeah I think continuous learning and teaching is something that's

345
00:29:09,320 --> 00:29:14,200
really really important. Okay and what are you excited for going forward what's

346
00:29:14,200 --> 00:29:20,320
the future of data science platforms that Uber look like? Yeah absolutely so I

347
00:29:20,320 --> 00:29:26,360
see both at Uber as well as in the industry a big push towards these higher and

348
00:29:26,360 --> 00:29:31,640
higher level abstractions for platforms to really kind of commoditize it to

349
00:29:31,640 --> 00:29:36,640
make it available to a broader audience beyond data science machine learning

350
00:29:36,640 --> 00:29:41,320
engineers and engineers more broadly speaking and so coming back to that

351
00:29:41,320 --> 00:29:46,600
four-step data science workflow or machine learning workflow that it

352
00:29:46,600 --> 00:29:51,720
described earlier you know one of the gaps that we saw is that we didn't have

353
00:29:51,720 --> 00:29:56,320
any semi-automation or automation around a data exploration or insights

354
00:29:56,320 --> 00:30:00,360
generation as a whole and that's something that actually goes well beyond

355
00:30:00,360 --> 00:30:04,040
data science or machine learning workflows right? It's that first step of the

356
00:30:04,040 --> 00:30:09,560
four-step cycle exactly the first step right where there is still a lot of human

357
00:30:09,560 --> 00:30:14,720
hours that need to go into that to dig into the data to understand and explore the

358
00:30:14,720 --> 00:30:19,040
data and also for business analysts right a lot of the questions that they

359
00:30:19,040 --> 00:30:24,920
would be getting is I have an important business KPI and it moved up or down

360
00:30:24,920 --> 00:30:28,520
right to investigate you know all the various different slices and

361
00:30:28,520 --> 00:30:32,960
dices of the data on what might have happened right? And so what we have

362
00:30:32,960 --> 00:30:37,280
been starting to work on is a proof of concept to actually have an algorithm

363
00:30:37,280 --> 00:30:42,360
automatically scan through our data and to surface a potentially interesting

364
00:30:42,360 --> 00:30:47,920
insights to folks whether it's machine learning experts or business

365
00:30:47,920 --> 00:30:52,720
analysts for example and they obviously would go and dig into some of these

366
00:30:52,720 --> 00:30:58,120
suggestions and understand them more deeply but here we see a really large

367
00:30:58,120 --> 00:31:04,320
opportunity not only to save people a lot of time but also to really open up

368
00:31:04,320 --> 00:31:08,960
new insights that might not have been discovered previously and this is some of

369
00:31:08,960 --> 00:31:13,800
the feedback that we're getting from our early adopters in this field that the

370
00:31:13,800 --> 00:31:18,440
machine was able to come up with interesting suggestions that they say

371
00:31:18,440 --> 00:31:24,080
wouldn't have come up themselves and so I think that really will if successful

372
00:31:24,080 --> 00:31:28,000
will revolutionize how we do data analytics at Uber and I think more

373
00:31:28,000 --> 00:31:32,360
broadling the industry awesome awesome well friend thanks so much for joining us

374
00:31:32,360 --> 00:31:44,440
here at Twomalcon thanks for having me great speaking with you all right

375
00:31:44,440 --> 00:31:48,960
everyone I hope you enjoyed our show straight from the main stage at Twomalcon

376
00:31:48,960 --> 00:31:55,320
AI platforms for more information about today's show visit Twomalai.com and

377
00:31:55,320 --> 00:32:02,080
for more Twomalcon coverage visit Twomalcon.com slash news thanks so much

378
00:32:02,080 --> 00:32:05,680
for listening and catch you next time

